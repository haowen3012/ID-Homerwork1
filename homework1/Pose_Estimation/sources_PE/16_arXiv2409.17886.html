<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection</title>
<!--Generated on Thu Sep 26 14:07:25 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="3D gaze target detection gaze estimation human pose estimation upper-body pose depth map multimodal privacy-preserving" lang="en" name="keywords"/>
<base href="/html/2409.17886v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S1" title="In Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S2" title="In Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S2.SS1" title="In 2 Related Work â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Gaze Target Detection in 2D</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S2.SS2" title="In 2 Related Work â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Gaze Target Detection in 3D</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S2.SS3" title="In 2 Related Work â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Gaze Estimation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S3" title="In Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S3.SS1" title="In 3 Methodology â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Gaze Target Detection Pipeline</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S3.SS2" title="In 3 Methodology â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>The Skeleton-based Gaze Estimation Module</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S3.SS3" title="In 3 Methodology â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Privacy Preservation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S3.SS4" title="In 3 Methodology â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S3.SS5" title="In 3 Methodology â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Implementation Details</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S4" title="In Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S4.SS1" title="In 4 Experiments â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S4.SS2" title="In 4 Experiments â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Evaluation Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S4.SS3" title="In 4 Experiments â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Results of the Gaze Estimation Module</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S4.SS4" title="In 4 Experiments â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Results on the GFIE Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S4.SS5" title="In 4 Experiments â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Qualitative Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S4.SS6" title="In 4 Experiments â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6 </span>Ablation Study</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S5" title="In Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>
Dept. of Engineering for Innovation Medicine, University of Verona, Verona, Italy
</span></span></span><span class="ltx_note ltx_role_institutetext" id="id2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Dept. of Computer Science, University of Verona, Verona, Italy
<span class="ltx_note ltx_role_email" id="id2.1"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">email: </span>name.surname@univr.it</span></span></span>
<br class="ltx_break"/></span></span></span><span class="ltx_note ltx_role_institutetext" id="id3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span>QUALYCO S.r.l., Spin-off of the University of Verona, Verona, Italy
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id3.1"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">email: </span>marco.cristani@qualyco.com</span></span></span>
</span></span></span>
<h1 class="ltx_title ltx_title_document">Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Andrea Toaiari<span class="ltx_ERROR undefined" id="id1.1.id1">\orcidlink</span>0000-0002-3759-8865
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Vittorio Murino<span class="ltx_ERROR undefined" id="id2.1.id1">\orcidlink</span>0000-0002-8645-2328
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Marco Cristani<span class="ltx_ERROR undefined" id="id3.1.id1">\orcidlink</span>0000-0002-0523-6042
</span><span class="ltx_author_notes">1133</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Cigdem Beyan<span class="ltx_ERROR undefined" id="id4.1.id1">\orcidlink</span>0000-0002-9583-0087
</span><span class="ltx_author_notes">22</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id5.id1">Gaze Target Detection (GTD), <em class="ltx_emph ltx_font_italic" id="id5.id1.1">i.e</em>.<span class="ltx_text" id="id5.id1.2"></span>, determining where a person is looking within a scene from an external viewpoint, is a challenging task, particularly in 3D space. Existing approaches heavily rely on analyzing the personâ€™s appearance, primarily focusing on their face to predict the gaze target.
This paper presents a novel approach to tackle this problem by utilizing the personâ€™s upper-body pose and available depth maps to extract a 3D gaze direction and employing a multi-stage or an end-to-end pipeline to predict the gazed target.
When predicted accurately, the human body pose can provide valuable information about the head pose, which is a good approximation of the gaze direction, as well as the position of the arms and hands, which are linked to the activity the person is performing and the objects they are likely focusing on.
Consequently, in addition to performing gaze estimation in 3D, we are also able to perform GTD simultaneously.
We demonstrate state-of-the-art results on the most comprehensive publicly accessible 3D gaze target detection dataset without requiring images of the personâ€™s face, thus promoting privacy preservation in various application contexts.
The code is available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/intelligolabs/privacy-gtd-3D" title="">https://github.com/intelligolabs/privacy-gtd-3D</a>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>3D gaze target detection gaze estimation human pose estimation upper-body pose depth map multimodal privacy-preserving
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Human communication relies on a complex interplay of verbal and non-verbal cues. Among nonverbal cues, the gaze holds particular significance as it reveals where a person directs their visual attention, offering insights into their interests, intentions, or imminent actionsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib16" title="">16</a>]</cite>. This ability to interpret gaze is crucial for understanding the reciprocal focus and facilitating interactions like turn-taking in social contextsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib4" title="">4</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Gaze analysis spans a broad spectrum of disciplines, including, among others, human-computer interaction, neuroscience, social robotics, and organizational psychologyÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib7" title="">7</a>]</cite>.
An analytical approach to estimating the gaze direction or the location which is gazed at not only enhances our understanding of human behavior but also informs the development of technologies that rely on human-machine interaction, social understanding in robotics, and even psychological research into interpersonal dynamics and attentional processesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib3" title="">3</a>]</cite>.
Thus, the study of gaze represents a multidisciplinary endeavor that continues to evolve with technological and cognitive science advancements.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In computer vision, the study of gaze behavior primarily involves two main tasks: <em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">gaze estimation</em> and <em class="ltx_emph ltx_font_italic" id="S1.p3.1.2">gaze target detection</em>.
Gaze estimation typically implicates predicting where a person is looking in a three-dimensional space. This means determining the direction of a personâ€™s gaze in terms of yaw, pitch, and sometimes roll angles, which correspond to horizontal, vertical, and depth dimensions, respectively.
Gaze estimation can be performed from a single image or a sequence of images, often focusing on the personâ€™s eyes or the entire head regionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib28" title="">28</a>]</cite>.
Gaze target detectionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib39" title="">39</a>]</cite> (also referred to as gaze-following), on the other hand, focuses on identifying the specific point or region in the scene that the person is looking at. This is typically done in a two-dimensional space (2D), where the goal is to localize the gaze targetâ€™s exact coordinates <math alttext="(x,y)" class="ltx_Math" display="inline" id="S1.p3.1.m1.2"><semantics id="S1.p3.1.m1.2a"><mrow id="S1.p3.1.m1.2.3.2" xref="S1.p3.1.m1.2.3.1.cmml"><mo id="S1.p3.1.m1.2.3.2.1" stretchy="false" xref="S1.p3.1.m1.2.3.1.cmml">(</mo><mi id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml">x</mi><mo id="S1.p3.1.m1.2.3.2.2" xref="S1.p3.1.m1.2.3.1.cmml">,</mo><mi id="S1.p3.1.m1.2.2" xref="S1.p3.1.m1.2.2.cmml">y</mi><mo id="S1.p3.1.m1.2.3.2.3" stretchy="false" xref="S1.p3.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.2b"><interval closure="open" id="S1.p3.1.m1.2.3.1.cmml" xref="S1.p3.1.m1.2.3.2"><ci id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1">ğ‘¥</ci><ci id="S1.p3.1.m1.2.2.cmml" xref="S1.p3.1.m1.2.2">ğ‘¦</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.2c">(x,y)</annotation><annotation encoding="application/x-llamapun" id="S1.p3.1.m1.2d">( italic_x , italic_y )</annotation></semantics></math> within the image or video frame.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="223" id="S1.F1.g1" src="x1.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.3.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.4.2" style="font-size:90%;">We introduce a novel way to tackle the 3D gaze estimation and 3D gaze target detection problems that, differently from the previous approaches, does not use the head crops of the observed people but rather employs upper-body skeletons, depth maps, and scene images where the head is blurred out.
The new pipeline not only proved successful in tackling both tasks but offers a concrete way to preserve the identity of the involved people.
The currently available state-of-the-art method already used depth maps (in <span class="ltx_text" id="S1.F1.4.2.1" style="color:#FF00FF;">magenta</span>) in its pipeline, so we are not introducing additional information.
</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Traditional methods for gaze estimation historically relied on geometric eye models and regression functions to establish a mapping from eye or face images to gaze vectorsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib45" title="">45</a>]</cite>. These methods demonstrated effectiveness in controlled environments with consistent subject characteristics, head positions, and lighting conditions; however, their performance diminishes in more diverse and less controlled settingsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib49" title="">49</a>]</cite>.
In recent years, some methods incorporated broader facial features such as head pose and overall facial appearance to enhance accuracy and indicated that leveraging full-face information tends to yield more precise gaze estimates compared to methods focusing solely on eye imagesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib50" title="">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib30" title="">30</a>]</cite>. To the best of our knowledge, no research has explored the use of full-body or upper-body pose rather than head orientation or head/eye images for <span class="ltx_text ltx_font_bold" id="S1.p4.1.1">3D gaze estimation</span> in unconstrained scenes.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">When addressing <span class="ltx_text ltx_font_bold" id="S1.p5.1.1">gaze target detection</span>, the predominant approach has focused on performing this task within a 2D context, utilizing a dual-pathway deep architecture. This architecture typically incorporates inputs such as head crops and scene imagesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib40" title="">40</a>]</cite>.
Some studies have expanded upon this framework by introducing temporal data integration <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib12" title="">12</a>]</cite> or by incorporating a third pathway to handle depth maps of the scene <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib42" title="">42</a>]</cite>.
While numerous attempts have been made to perform gaze target detection in 2D, significantly fewer studies have focused on 3D gaze target detection.
However, it has been demonstrated that gaze following in a 3D scene with only a single camera is feasible <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib25" title="">25</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p6">
<p class="ltx_p" id="S1.p6.1"><span class="ltx_text ltx_font_bold" id="S1.p6.1.1">Our proposal.</span>
In this paper, we propose a methodology that not only solves the <span class="ltx_text ltx_font_bold" id="S1.p6.1.2">3D gaze estimation task</span> in a novel way, employing estimated upper-body skeletons and ground truth depth maps, but also provides a solution for both <span class="ltx_text ltx_font_bold" id="S1.p6.1.3">2D and 3D gaze target detection</span> while simultaneously preserving the identity of the observed subjects.
Inspired by the preliminary work carried out inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib21" title="">21</a>]</cite>, we build upon the framework and the dataset proposed byÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib25" title="">25</a>]</cite>, replacing their gaze estimation module based on head crops with a skeleton and depth-based module, limiting the treatment of scene and face images only to the pose estimation pre-processing phase, after which all these sensitive information can be discarded (see Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_tag">1</span></a>).
A single attention layer, applied to the combination of <span class="ltx_text ltx_font_bold" id="S1.p6.1.4">upper-body pose</span> features normalized on one anchor joint and convolutional features extracted from the scene depth maps, provided us with accurate 3D gaze vectors.
These vectors are then fed to a 3D gaze target estimation pipeline, which uses scene images where the peopleâ€™s faces have been blurred out.
The proposed method can be trained either in a <span class="ltx_text ltx_font_bold" id="S1.p6.1.5">multi-stage</span> fashion, pre-training the gaze estimation module, or in an <span class="ltx_text ltx_font_bold" id="S1.p6.1.6">end-to-end</span> manner.
This approach not only achieves state-of-the-art results on the GFIE dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib25" title="">25</a>]</cite>, the most comprehensive dataset available for 3D scenarios but also represents a novel approach to privacy preservation in the context of 3D gaze target detection.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p7">
<p class="ltx_p" id="S1.p7.1"><span class="ltx_text ltx_font_bold" id="S1.p7.1.1">Our contributions.</span>
The main contributions of our work can be summarized as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We present a novel module for <span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">3D gaze estimation</span> that utilizes upper-body skeleton data and the depth map of the scene. This approach achieves outstanding results, showcasing the richness of pose data in understanding a personâ€™s gaze direction in 3D.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We incorporate the above-mentioned new module into a pipeline for <span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">3D gaze target estimation</span>, achieving state-of-the-art results on the GFIE dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib25" title="">25</a>]</cite>, the most comprehensive dataset currently available for the task at hand.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">For the first time, we focus on the <span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">privacy-preserving</span> capabilities of a vision-based gaze estimation and gaze target detection approach in the 3D space, discarding information about the personâ€™s facial appearance and concentrating solely on their upper-body poses.
By doing so, our approach eliminates the need to use a head backbone pre-trained on a large dataset of head orientations.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">We provide an overview of the standard gaze target detection task, usually defined in the 2D space, in SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S2.SS1" title="2.1 Gaze Target Detection in 2D â€£ 2 Related Work â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_tag">2.1</span></a>, and the recent developments when it comes to tackling it in the 3D space in SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S2.SS2" title="2.2 Gaze Target Detection in 3D â€£ 2 Related Work â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_tag">2.2</span></a>.
Finally, in SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S2.SS3" title="2.3 Gaze Estimation â€£ 2 Related Work â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_tag">2.3</span></a>, we offer a brief panoramic of the related gaze estimation task.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Gaze Target Detection in 2D</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Gaze target detection is a complex vision task that consists of finding what an observed human subject is looking at, usually by analyzing only an image or a video of the scene taken from a third-person point of view.
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib39" title="">39</a>]</cite> formalized it, proposing a 2D dataset with multiple target annotations and a double pathway model that extracts features from a subjectâ€™s head appearance and the sceneâ€™s salient areas.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">This structure has been adopted by multiple following worksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib12" title="">12</a>]</cite>.
For instance,Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib12" title="">12</a>]</cite> proposed a spatiotemporal modelling of the gaze target detection problem and a way to classify whether the gaze target was inside or outside the scene image.
Among these, some introduced a third pathway to incorporate the depth map of the scene image, which is derived using a monocular depth estimatorÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib42" title="">42</a>]</cite>.
While the approaches outlined inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib42" title="">42</a>]</cite> have proven effective, they are unable to simultaneously detect gaze targets for all the individuals within a scene.
This limitation is addressed by Transformer-based gaze target detectors, as shown inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib43" title="">43</a>]</cite>. On the flip side, a notable drawback ofÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib43" title="">43</a>]</cite> is their requirement for relatively larger training datasets, causing them to underperform in a low-data regime.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">The work ofÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib21" title="">21</a>]</cite> presented the possibility of using skeleton data and depth maps, but their method underperformed compared to earlier work.
Note that their implementation uses skeleton data represented as images, whereas our pipeline utilizes normalized key points (<em class="ltx_emph ltx_font_italic" id="S2.SS1.p3.1.1">i.e</em>.<span class="ltx_text" id="S2.SS1.p3.1.2"></span>, not images).</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Gaze Target Detection in 3D</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Although various efforts have been made to extend 2D gaze target detection to the 3D spaceÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib23" title="">23</a>]</cite> by estimating depth maps and three-dimensional gaze cones, these methods have been tested only on datasetsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib12" title="">12</a>]</cite> that provide 2D data and annotations.
Similarly, the estimation of visual selective attentionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib41" title="">41</a>]</cite> involves mapping attention directly onto the 3D point cloud of a scene by analyzing video sequences of individuals moving within a constrained environment. This approach uses 2D inputs but performs its evaluation entirely in 3D space, focusing on how multiple subjectsâ€™ visual attention maps onto the scene over time.
Wei et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib46" title="">46</a>]</cite> is one of the first methods to deal with this task in the 3D scenario, addressing not only the attention prediction challenge but also investigating why the subject is looking there and what action is being performed.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib24" title="">24</a>]</cite>, an RGB-D dataset with 3D annotations has been proposed, offering manually annotated 3D gaze targets and adding a novel 3D pathway to the now standard saliency-head prediction pipeline.
Recently, Hu et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib25" title="">25</a>]</cite> presented an architecture with four modules for a) estimating gaze directions, b) perceiving the stereo field of view, c) generating a 2D gaze heatmap, and d) retrieving the 3D gaze target. This method utilizes a pre-trained ResNet50 as the backbone for the gaze direction estimation module, which takes a cropped head image as input and outputs a 3D gaze unit vector. From the 2D prediction, the 3D target is then retrieved in the evaluation phase. A dataset called GFIE is also presented in the paper, representing the most comprehensive data collection to date for gaze target detection in 3D.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Gaze Estimation</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">The field of computer vision has a longer history in gaze estimation compared to gaze target detection.
There are two main approaches to gaze estimation: geometric eye models and appearance models. The latter has become more prevalent with the rise of deep learning.
The most recent appearance-based models have shown increased interest in CNN-based solutions. For instance, Zhang et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib51" title="">51</a>]</cite> use both eye and facial features, while Krafka et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib30" title="">30</a>]</cite> developed a multi-channel network using eye images, full-face images, and face grid information.
Chen et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib8" title="">8</a>]</cite> introduced a model integrating statistical information within a CNN for eye images, utilizing dilated convolutions to retain high-level image features without reducing spatial resolution. Their GEDDNet model employs gaze decomposition with dilated convolutions for enhanced performance.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">Fischer et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib18" title="">18</a>]</cite> enhanced gaze prediction accuracy by incorporating the head pose vector with VGG CNN-extracted features from eye crops and using an ensemble scheme.
Cheng et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib9" title="">9</a>]</cite> proposed CA-Net, which first predicts primary gaze angles from a face image and refines them with residuals from eye crops, using a bi-gram model to integrate primary gaze and eye residuals.
Kellnhofer et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib28" title="">28</a>]</cite> employed an LSTM-based temporal model with 7-frame sequences and adapted pinball loss for joint gaze direction and error bounds regression to improve accuracy.</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">More recently, transformers with self-attention modules have been applied to gaze estimation. One approachÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib36" title="">36</a>]</cite> used a transformer to extract crucial gaze features from images with high variance, filtering irrelevant information using convolution projection and preserving detailed features with a deconvolution layer. AGE-NetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib5" title="">5</a>]</cite> proposed using two parallel networks for each eye image: one generating a feature vector with a CNN and the other generating a weight feature vector with an attention-based network.
Notably, no existing work addresses processing body or upper-body pose and depth maps for gaze estimation in 3D, as we target in this study.
Furthermore, the literature focusing on both gaze estimation and 2D/3D gaze target detection simultaneously (<em class="ltx_emph ltx_font_italic" id="S2.SS3.p3.1.1">e.g</em>.<span class="ltx_text" id="S2.SS3.p3.1.2"></span>,Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib25" title="">25</a>]</cite>), which is our aim, is very limited. For those interested in a deeper exploration, survey papers on gaze estimation are available for referenceÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib37" title="">37</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we introduce our upper-body and depth-based gaze estimation module and explain how it enables us to execute the entire 3D gaze target detection pipeline while preserving the privacy of the involved subjects.
The proposed architecture is presented in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S3.F2" title="Figure 2 â€£ 3.2 The Skeleton-based Gaze Estimation Module â€£ 3 Methodology â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_tag">2</span></a> and described in SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S3.SS1" title="3.1 Gaze Target Detection Pipeline â€£ 3 Methodology â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_tag">3.1</span></a>.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S3.SS2" title="3.2 The Skeleton-based Gaze Estimation Module â€£ 3 Methodology â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_tag">3.2</span></a> describes our novel gaze estimation method and how it fits inside the gaze target detection pipeline.
Privacy preservation and how it is enforced is discussed in SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S3.SS3" title="3.3 Privacy Preservation â€£ 3 Methodology â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_tag">3.3</span></a>, while in SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S3.SS4" title="3.4 Training â€£ 3 Methodology â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_tag">3.4</span></a> and SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S3.SS5" title="3.5 Implementation Details â€£ 3 Methodology â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_tag">3.5</span></a> we provide training and implementation details to enable appropriate reproducibility of the proposed approach.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Gaze Target Detection Pipeline</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.9">Given an image or video dataset which offers annotations about the ground-truth gaze vector and the 2D/3D gaze targets, the task of 3D gaze target detection consists of finding the 3D point <math alttext="t^{3D}=(t^{3D}_{x},t^{3D}_{y},t^{3D}_{z})" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.3"><semantics id="S3.SS1.p1.1.m1.3a"><mrow id="S3.SS1.p1.1.m1.3.3" xref="S3.SS1.p1.1.m1.3.3.cmml"><msup id="S3.SS1.p1.1.m1.3.3.5" xref="S3.SS1.p1.1.m1.3.3.5.cmml"><mi id="S3.SS1.p1.1.m1.3.3.5.2" xref="S3.SS1.p1.1.m1.3.3.5.2.cmml">t</mi><mrow id="S3.SS1.p1.1.m1.3.3.5.3" xref="S3.SS1.p1.1.m1.3.3.5.3.cmml"><mn id="S3.SS1.p1.1.m1.3.3.5.3.2" xref="S3.SS1.p1.1.m1.3.3.5.3.2.cmml">3</mn><mo id="S3.SS1.p1.1.m1.3.3.5.3.1" xref="S3.SS1.p1.1.m1.3.3.5.3.1.cmml">â¢</mo><mi id="S3.SS1.p1.1.m1.3.3.5.3.3" xref="S3.SS1.p1.1.m1.3.3.5.3.3.cmml">D</mi></mrow></msup><mo id="S3.SS1.p1.1.m1.3.3.4" xref="S3.SS1.p1.1.m1.3.3.4.cmml">=</mo><mrow id="S3.SS1.p1.1.m1.3.3.3.3" xref="S3.SS1.p1.1.m1.3.3.3.4.cmml"><mo id="S3.SS1.p1.1.m1.3.3.3.3.4" stretchy="false" xref="S3.SS1.p1.1.m1.3.3.3.4.cmml">(</mo><msubsup id="S3.SS1.p1.1.m1.1.1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.1.1.1.2.2" xref="S3.SS1.p1.1.m1.1.1.1.1.1.2.2.cmml">t</mi><mi id="S3.SS1.p1.1.m1.1.1.1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.1.1.1.3.cmml">x</mi><mrow id="S3.SS1.p1.1.m1.1.1.1.1.1.2.3" xref="S3.SS1.p1.1.m1.1.1.1.1.1.2.3.cmml"><mn id="S3.SS1.p1.1.m1.1.1.1.1.1.2.3.2" xref="S3.SS1.p1.1.m1.1.1.1.1.1.2.3.2.cmml">3</mn><mo id="S3.SS1.p1.1.m1.1.1.1.1.1.2.3.1" xref="S3.SS1.p1.1.m1.1.1.1.1.1.2.3.1.cmml">â¢</mo><mi id="S3.SS1.p1.1.m1.1.1.1.1.1.2.3.3" xref="S3.SS1.p1.1.m1.1.1.1.1.1.2.3.3.cmml">D</mi></mrow></msubsup><mo id="S3.SS1.p1.1.m1.3.3.3.3.5" xref="S3.SS1.p1.1.m1.3.3.3.4.cmml">,</mo><msubsup id="S3.SS1.p1.1.m1.2.2.2.2.2" xref="S3.SS1.p1.1.m1.2.2.2.2.2.cmml"><mi id="S3.SS1.p1.1.m1.2.2.2.2.2.2.2" xref="S3.SS1.p1.1.m1.2.2.2.2.2.2.2.cmml">t</mi><mi id="S3.SS1.p1.1.m1.2.2.2.2.2.3" xref="S3.SS1.p1.1.m1.2.2.2.2.2.3.cmml">y</mi><mrow id="S3.SS1.p1.1.m1.2.2.2.2.2.2.3" xref="S3.SS1.p1.1.m1.2.2.2.2.2.2.3.cmml"><mn id="S3.SS1.p1.1.m1.2.2.2.2.2.2.3.2" xref="S3.SS1.p1.1.m1.2.2.2.2.2.2.3.2.cmml">3</mn><mo id="S3.SS1.p1.1.m1.2.2.2.2.2.2.3.1" xref="S3.SS1.p1.1.m1.2.2.2.2.2.2.3.1.cmml">â¢</mo><mi id="S3.SS1.p1.1.m1.2.2.2.2.2.2.3.3" xref="S3.SS1.p1.1.m1.2.2.2.2.2.2.3.3.cmml">D</mi></mrow></msubsup><mo id="S3.SS1.p1.1.m1.3.3.3.3.6" xref="S3.SS1.p1.1.m1.3.3.3.4.cmml">,</mo><msubsup id="S3.SS1.p1.1.m1.3.3.3.3.3" xref="S3.SS1.p1.1.m1.3.3.3.3.3.cmml"><mi id="S3.SS1.p1.1.m1.3.3.3.3.3.2.2" xref="S3.SS1.p1.1.m1.3.3.3.3.3.2.2.cmml">t</mi><mi id="S3.SS1.p1.1.m1.3.3.3.3.3.3" xref="S3.SS1.p1.1.m1.3.3.3.3.3.3.cmml">z</mi><mrow id="S3.SS1.p1.1.m1.3.3.3.3.3.2.3" xref="S3.SS1.p1.1.m1.3.3.3.3.3.2.3.cmml"><mn id="S3.SS1.p1.1.m1.3.3.3.3.3.2.3.2" xref="S3.SS1.p1.1.m1.3.3.3.3.3.2.3.2.cmml">3</mn><mo id="S3.SS1.p1.1.m1.3.3.3.3.3.2.3.1" xref="S3.SS1.p1.1.m1.3.3.3.3.3.2.3.1.cmml">â¢</mo><mi id="S3.SS1.p1.1.m1.3.3.3.3.3.2.3.3" xref="S3.SS1.p1.1.m1.3.3.3.3.3.2.3.3.cmml">D</mi></mrow></msubsup><mo id="S3.SS1.p1.1.m1.3.3.3.3.7" stretchy="false" xref="S3.SS1.p1.1.m1.3.3.3.4.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.3b"><apply id="S3.SS1.p1.1.m1.3.3.cmml" xref="S3.SS1.p1.1.m1.3.3"><eq id="S3.SS1.p1.1.m1.3.3.4.cmml" xref="S3.SS1.p1.1.m1.3.3.4"></eq><apply id="S3.SS1.p1.1.m1.3.3.5.cmml" xref="S3.SS1.p1.1.m1.3.3.5"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.3.3.5.1.cmml" xref="S3.SS1.p1.1.m1.3.3.5">superscript</csymbol><ci id="S3.SS1.p1.1.m1.3.3.5.2.cmml" xref="S3.SS1.p1.1.m1.3.3.5.2">ğ‘¡</ci><apply id="S3.SS1.p1.1.m1.3.3.5.3.cmml" xref="S3.SS1.p1.1.m1.3.3.5.3"><times id="S3.SS1.p1.1.m1.3.3.5.3.1.cmml" xref="S3.SS1.p1.1.m1.3.3.5.3.1"></times><cn id="S3.SS1.p1.1.m1.3.3.5.3.2.cmml" type="integer" xref="S3.SS1.p1.1.m1.3.3.5.3.2">3</cn><ci id="S3.SS1.p1.1.m1.3.3.5.3.3.cmml" xref="S3.SS1.p1.1.m1.3.3.5.3.3">ğ·</ci></apply></apply><vector id="S3.SS1.p1.1.m1.3.3.3.4.cmml" xref="S3.SS1.p1.1.m1.3.3.3.3"><apply id="S3.SS1.p1.1.m1.1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1">subscript</csymbol><apply id="S3.SS1.p1.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.1.1.2.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1">superscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.1.1.1.2.2.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.2.2">ğ‘¡</ci><apply id="S3.SS1.p1.1.m1.1.1.1.1.1.2.3.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.2.3"><times id="S3.SS1.p1.1.m1.1.1.1.1.1.2.3.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.2.3.1"></times><cn id="S3.SS1.p1.1.m1.1.1.1.1.1.2.3.2.cmml" type="integer" xref="S3.SS1.p1.1.m1.1.1.1.1.1.2.3.2">3</cn><ci id="S3.SS1.p1.1.m1.1.1.1.1.1.2.3.3.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.2.3.3">ğ·</ci></apply></apply><ci id="S3.SS1.p1.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.3">ğ‘¥</ci></apply><apply id="S3.SS1.p1.1.m1.2.2.2.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.2.2.2.2.2.1.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.2">subscript</csymbol><apply id="S3.SS1.p1.1.m1.2.2.2.2.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.2.2.2.2.2.2.1.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.2">superscript</csymbol><ci id="S3.SS1.p1.1.m1.2.2.2.2.2.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.2.2.2">ğ‘¡</ci><apply id="S3.SS1.p1.1.m1.2.2.2.2.2.2.3.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.2.2.3"><times id="S3.SS1.p1.1.m1.2.2.2.2.2.2.3.1.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.2.2.3.1"></times><cn id="S3.SS1.p1.1.m1.2.2.2.2.2.2.3.2.cmml" type="integer" xref="S3.SS1.p1.1.m1.2.2.2.2.2.2.3.2">3</cn><ci id="S3.SS1.p1.1.m1.2.2.2.2.2.2.3.3.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.2.2.3.3">ğ·</ci></apply></apply><ci id="S3.SS1.p1.1.m1.2.2.2.2.2.3.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.2.3">ğ‘¦</ci></apply><apply id="S3.SS1.p1.1.m1.3.3.3.3.3.cmml" xref="S3.SS1.p1.1.m1.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.3.3.3.3.3.1.cmml" xref="S3.SS1.p1.1.m1.3.3.3.3.3">subscript</csymbol><apply id="S3.SS1.p1.1.m1.3.3.3.3.3.2.cmml" xref="S3.SS1.p1.1.m1.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.3.3.3.3.3.2.1.cmml" xref="S3.SS1.p1.1.m1.3.3.3.3.3">superscript</csymbol><ci id="S3.SS1.p1.1.m1.3.3.3.3.3.2.2.cmml" xref="S3.SS1.p1.1.m1.3.3.3.3.3.2.2">ğ‘¡</ci><apply id="S3.SS1.p1.1.m1.3.3.3.3.3.2.3.cmml" xref="S3.SS1.p1.1.m1.3.3.3.3.3.2.3"><times id="S3.SS1.p1.1.m1.3.3.3.3.3.2.3.1.cmml" xref="S3.SS1.p1.1.m1.3.3.3.3.3.2.3.1"></times><cn id="S3.SS1.p1.1.m1.3.3.3.3.3.2.3.2.cmml" type="integer" xref="S3.SS1.p1.1.m1.3.3.3.3.3.2.3.2">3</cn><ci id="S3.SS1.p1.1.m1.3.3.3.3.3.2.3.3.cmml" xref="S3.SS1.p1.1.m1.3.3.3.3.3.2.3.3">ğ·</ci></apply></apply><ci id="S3.SS1.p1.1.m1.3.3.3.3.3.3.cmml" xref="S3.SS1.p1.1.m1.3.3.3.3.3.3">ğ‘§</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.3c">t^{3D}=(t^{3D}_{x},t^{3D}_{y},t^{3D}_{z})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.3d">italic_t start_POSTSUPERSCRIPT 3 italic_D end_POSTSUPERSCRIPT = ( italic_t start_POSTSUPERSCRIPT 3 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT , italic_t start_POSTSUPERSCRIPT 3 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT , italic_t start_POSTSUPERSCRIPT 3 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT )</annotation></semantics></math> looked at by the subject, or subjects if extended to social gaze prediction, in the scene and their gaze direction represented by the normalized vector <math alttext="g=(g_{x},g_{y},g_{z})" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.3"><semantics id="S3.SS1.p1.2.m2.3a"><mrow id="S3.SS1.p1.2.m2.3.3" xref="S3.SS1.p1.2.m2.3.3.cmml"><mi id="S3.SS1.p1.2.m2.3.3.5" xref="S3.SS1.p1.2.m2.3.3.5.cmml">g</mi><mo id="S3.SS1.p1.2.m2.3.3.4" xref="S3.SS1.p1.2.m2.3.3.4.cmml">=</mo><mrow id="S3.SS1.p1.2.m2.3.3.3.3" xref="S3.SS1.p1.2.m2.3.3.3.4.cmml"><mo id="S3.SS1.p1.2.m2.3.3.3.3.4" stretchy="false" xref="S3.SS1.p1.2.m2.3.3.3.4.cmml">(</mo><msub id="S3.SS1.p1.2.m2.1.1.1.1.1" xref="S3.SS1.p1.2.m2.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.1.1.1.2" xref="S3.SS1.p1.2.m2.1.1.1.1.1.2.cmml">g</mi><mi id="S3.SS1.p1.2.m2.1.1.1.1.1.3" xref="S3.SS1.p1.2.m2.1.1.1.1.1.3.cmml">x</mi></msub><mo id="S3.SS1.p1.2.m2.3.3.3.3.5" xref="S3.SS1.p1.2.m2.3.3.3.4.cmml">,</mo><msub id="S3.SS1.p1.2.m2.2.2.2.2.2" xref="S3.SS1.p1.2.m2.2.2.2.2.2.cmml"><mi id="S3.SS1.p1.2.m2.2.2.2.2.2.2" xref="S3.SS1.p1.2.m2.2.2.2.2.2.2.cmml">g</mi><mi id="S3.SS1.p1.2.m2.2.2.2.2.2.3" xref="S3.SS1.p1.2.m2.2.2.2.2.2.3.cmml">y</mi></msub><mo id="S3.SS1.p1.2.m2.3.3.3.3.6" xref="S3.SS1.p1.2.m2.3.3.3.4.cmml">,</mo><msub id="S3.SS1.p1.2.m2.3.3.3.3.3" xref="S3.SS1.p1.2.m2.3.3.3.3.3.cmml"><mi id="S3.SS1.p1.2.m2.3.3.3.3.3.2" xref="S3.SS1.p1.2.m2.3.3.3.3.3.2.cmml">g</mi><mi id="S3.SS1.p1.2.m2.3.3.3.3.3.3" xref="S3.SS1.p1.2.m2.3.3.3.3.3.3.cmml">z</mi></msub><mo id="S3.SS1.p1.2.m2.3.3.3.3.7" stretchy="false" xref="S3.SS1.p1.2.m2.3.3.3.4.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.3b"><apply id="S3.SS1.p1.2.m2.3.3.cmml" xref="S3.SS1.p1.2.m2.3.3"><eq id="S3.SS1.p1.2.m2.3.3.4.cmml" xref="S3.SS1.p1.2.m2.3.3.4"></eq><ci id="S3.SS1.p1.2.m2.3.3.5.cmml" xref="S3.SS1.p1.2.m2.3.3.5">ğ‘”</ci><vector id="S3.SS1.p1.2.m2.3.3.3.4.cmml" xref="S3.SS1.p1.2.m2.3.3.3.3"><apply id="S3.SS1.p1.2.m2.1.1.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.1.2">ğ‘”</ci><ci id="S3.SS1.p1.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.1.3">ğ‘¥</ci></apply><apply id="S3.SS1.p1.2.m2.2.2.2.2.2.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.2.2.2.2.2.1.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.2.m2.2.2.2.2.2.2.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.2.2">ğ‘”</ci><ci id="S3.SS1.p1.2.m2.2.2.2.2.2.3.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.2.3">ğ‘¦</ci></apply><apply id="S3.SS1.p1.2.m2.3.3.3.3.3.cmml" xref="S3.SS1.p1.2.m2.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.3.3.3.3.3.1.cmml" xref="S3.SS1.p1.2.m2.3.3.3.3.3">subscript</csymbol><ci id="S3.SS1.p1.2.m2.3.3.3.3.3.2.cmml" xref="S3.SS1.p1.2.m2.3.3.3.3.3.2">ğ‘”</ci><ci id="S3.SS1.p1.2.m2.3.3.3.3.3.3.cmml" xref="S3.SS1.p1.2.m2.3.3.3.3.3.3">ğ‘§</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.3c">g=(g_{x},g_{y},g_{z})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.3d">italic_g = ( italic_g start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT , italic_g start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT , italic_g start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT )</annotation></semantics></math>.
The input data for each gaze prediction includes the image scene <math alttext="s" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><mi id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">ğ‘ </ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">italic_s</annotation></semantics></math>, the aligned depth map <math alttext="d" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4.1"><semantics id="S3.SS1.p1.4.m4.1a"><mi id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">d</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.1d">italic_d</annotation></semantics></math> with associated camera parameters <math alttext="k" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m5.1"><semantics id="S3.SS1.p1.5.m5.1a"><mi id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><ci id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.5.m5.1d">italic_k</annotation></semantics></math>, a binary mask for the face location <math alttext="m" class="ltx_Math" display="inline" id="S3.SS1.p1.6.m6.1"><semantics id="S3.SS1.p1.6.m6.1a"><mi id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><ci id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">ğ‘š</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">m</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.6.m6.1d">italic_m</annotation></semantics></math>, the 2D (<math alttext="e^{2D}" class="ltx_Math" display="inline" id="S3.SS1.p1.7.m7.1"><semantics id="S3.SS1.p1.7.m7.1a"><msup id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml"><mi id="S3.SS1.p1.7.m7.1.1.2" xref="S3.SS1.p1.7.m7.1.1.2.cmml">e</mi><mrow id="S3.SS1.p1.7.m7.1.1.3" xref="S3.SS1.p1.7.m7.1.1.3.cmml"><mn id="S3.SS1.p1.7.m7.1.1.3.2" xref="S3.SS1.p1.7.m7.1.1.3.2.cmml">2</mn><mo id="S3.SS1.p1.7.m7.1.1.3.1" xref="S3.SS1.p1.7.m7.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.p1.7.m7.1.1.3.3" xref="S3.SS1.p1.7.m7.1.1.3.3.cmml">D</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><apply id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m7.1.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">superscript</csymbol><ci id="S3.SS1.p1.7.m7.1.1.2.cmml" xref="S3.SS1.p1.7.m7.1.1.2">ğ‘’</ci><apply id="S3.SS1.p1.7.m7.1.1.3.cmml" xref="S3.SS1.p1.7.m7.1.1.3"><times id="S3.SS1.p1.7.m7.1.1.3.1.cmml" xref="S3.SS1.p1.7.m7.1.1.3.1"></times><cn id="S3.SS1.p1.7.m7.1.1.3.2.cmml" type="integer" xref="S3.SS1.p1.7.m7.1.1.3.2">2</cn><ci id="S3.SS1.p1.7.m7.1.1.3.3.cmml" xref="S3.SS1.p1.7.m7.1.1.3.3">ğ·</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">e^{2D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.7.m7.1d">italic_e start_POSTSUPERSCRIPT 2 italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>) and 3D (<math alttext="e^{3D}" class="ltx_Math" display="inline" id="S3.SS1.p1.8.m8.1"><semantics id="S3.SS1.p1.8.m8.1a"><msup id="S3.SS1.p1.8.m8.1.1" xref="S3.SS1.p1.8.m8.1.1.cmml"><mi id="S3.SS1.p1.8.m8.1.1.2" xref="S3.SS1.p1.8.m8.1.1.2.cmml">e</mi><mrow id="S3.SS1.p1.8.m8.1.1.3" xref="S3.SS1.p1.8.m8.1.1.3.cmml"><mn id="S3.SS1.p1.8.m8.1.1.3.2" xref="S3.SS1.p1.8.m8.1.1.3.2.cmml">3</mn><mo id="S3.SS1.p1.8.m8.1.1.3.1" xref="S3.SS1.p1.8.m8.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.p1.8.m8.1.1.3.3" xref="S3.SS1.p1.8.m8.1.1.3.3.cmml">D</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.1b"><apply id="S3.SS1.p1.8.m8.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.8.m8.1.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1">superscript</csymbol><ci id="S3.SS1.p1.8.m8.1.1.2.cmml" xref="S3.SS1.p1.8.m8.1.1.2">ğ‘’</ci><apply id="S3.SS1.p1.8.m8.1.1.3.cmml" xref="S3.SS1.p1.8.m8.1.1.3"><times id="S3.SS1.p1.8.m8.1.1.3.1.cmml" xref="S3.SS1.p1.8.m8.1.1.3.1"></times><cn id="S3.SS1.p1.8.m8.1.1.3.2.cmml" type="integer" xref="S3.SS1.p1.8.m8.1.1.3.2">3</cn><ci id="S3.SS1.p1.8.m8.1.1.3.3.cmml" xref="S3.SS1.p1.8.m8.1.1.3.3">ğ·</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">e^{3D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.8.m8.1d">italic_e start_POSTSUPERSCRIPT 3 italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>) annotations of the point between the eyes and the bounding box (<math alttext="box_{h}" class="ltx_Math" display="inline" id="S3.SS1.p1.9.m9.1"><semantics id="S3.SS1.p1.9.m9.1a"><mrow id="S3.SS1.p1.9.m9.1.1" xref="S3.SS1.p1.9.m9.1.1.cmml"><mi id="S3.SS1.p1.9.m9.1.1.2" xref="S3.SS1.p1.9.m9.1.1.2.cmml">b</mi><mo id="S3.SS1.p1.9.m9.1.1.1" xref="S3.SS1.p1.9.m9.1.1.1.cmml">â¢</mo><mi id="S3.SS1.p1.9.m9.1.1.3" xref="S3.SS1.p1.9.m9.1.1.3.cmml">o</mi><mo id="S3.SS1.p1.9.m9.1.1.1a" xref="S3.SS1.p1.9.m9.1.1.1.cmml">â¢</mo><msub id="S3.SS1.p1.9.m9.1.1.4" xref="S3.SS1.p1.9.m9.1.1.4.cmml"><mi id="S3.SS1.p1.9.m9.1.1.4.2" xref="S3.SS1.p1.9.m9.1.1.4.2.cmml">x</mi><mi id="S3.SS1.p1.9.m9.1.1.4.3" xref="S3.SS1.p1.9.m9.1.1.4.3.cmml">h</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m9.1b"><apply id="S3.SS1.p1.9.m9.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1"><times id="S3.SS1.p1.9.m9.1.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1.1"></times><ci id="S3.SS1.p1.9.m9.1.1.2.cmml" xref="S3.SS1.p1.9.m9.1.1.2">ğ‘</ci><ci id="S3.SS1.p1.9.m9.1.1.3.cmml" xref="S3.SS1.p1.9.m9.1.1.3">ğ‘œ</ci><apply id="S3.SS1.p1.9.m9.1.1.4.cmml" xref="S3.SS1.p1.9.m9.1.1.4"><csymbol cd="ambiguous" id="S3.SS1.p1.9.m9.1.1.4.1.cmml" xref="S3.SS1.p1.9.m9.1.1.4">subscript</csymbol><ci id="S3.SS1.p1.9.m9.1.1.4.2.cmml" xref="S3.SS1.p1.9.m9.1.1.4.2">ğ‘¥</ci><ci id="S3.SS1.p1.9.m9.1.1.4.3.cmml" xref="S3.SS1.p1.9.m9.1.1.4.3">â„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m9.1c">box_{h}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.9.m9.1d">italic_b italic_o italic_x start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT</annotation></semantics></math>) around the head.
Note that the head bounding box is not employed as a modality but is used to blur the individualâ€™s face in the scene image;
For further details, see SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S3.SS3" title="3.3 Privacy Preservation â€£ 3 Methodology â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_tag">3.3</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.15">We adopted the four-module pipeline presented inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib25" title="">25</a>]</cite> to extract the 2D and the 3D gaze targets from the 2D image and the 3D point cloud, respectively.
The gaze estimation module, Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S3.F2" title="Figure 2 â€£ 3.2 The Skeleton-based Gaze Estimation Module â€£ 3 Methodology â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_tag">2</span></a>a, computes the normalized three-dimensional gaze vector <math alttext="\hat{g}=(\hat{g_{x}},\hat{g_{y}},\hat{g_{z}})" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.3"><semantics id="S3.SS1.p2.1.m1.3a"><mrow id="S3.SS1.p2.1.m1.3.4" xref="S3.SS1.p2.1.m1.3.4.cmml"><mover accent="true" id="S3.SS1.p2.1.m1.3.4.2" xref="S3.SS1.p2.1.m1.3.4.2.cmml"><mi id="S3.SS1.p2.1.m1.3.4.2.2" xref="S3.SS1.p2.1.m1.3.4.2.2.cmml">g</mi><mo id="S3.SS1.p2.1.m1.3.4.2.1" xref="S3.SS1.p2.1.m1.3.4.2.1.cmml">^</mo></mover><mo id="S3.SS1.p2.1.m1.3.4.1" xref="S3.SS1.p2.1.m1.3.4.1.cmml">=</mo><mrow id="S3.SS1.p2.1.m1.3.4.3.2" xref="S3.SS1.p2.1.m1.3.4.3.1.cmml"><mo id="S3.SS1.p2.1.m1.3.4.3.2.1" stretchy="false" xref="S3.SS1.p2.1.m1.3.4.3.1.cmml">(</mo><mover accent="true" id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><msub id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2.2" xref="S3.SS1.p2.1.m1.1.1.2.2.cmml">g</mi><mi id="S3.SS1.p2.1.m1.1.1.2.3" xref="S3.SS1.p2.1.m1.1.1.2.3.cmml">x</mi></msub><mo id="S3.SS1.p2.1.m1.1.1.1" xref="S3.SS1.p2.1.m1.1.1.1.cmml">^</mo></mover><mo id="S3.SS1.p2.1.m1.3.4.3.2.2" xref="S3.SS1.p2.1.m1.3.4.3.1.cmml">,</mo><mover accent="true" id="S3.SS1.p2.1.m1.2.2" xref="S3.SS1.p2.1.m1.2.2.cmml"><msub id="S3.SS1.p2.1.m1.2.2.2" xref="S3.SS1.p2.1.m1.2.2.2.cmml"><mi id="S3.SS1.p2.1.m1.2.2.2.2" xref="S3.SS1.p2.1.m1.2.2.2.2.cmml">g</mi><mi id="S3.SS1.p2.1.m1.2.2.2.3" xref="S3.SS1.p2.1.m1.2.2.2.3.cmml">y</mi></msub><mo id="S3.SS1.p2.1.m1.2.2.1" xref="S3.SS1.p2.1.m1.2.2.1.cmml">^</mo></mover><mo id="S3.SS1.p2.1.m1.3.4.3.2.3" xref="S3.SS1.p2.1.m1.3.4.3.1.cmml">,</mo><mover accent="true" id="S3.SS1.p2.1.m1.3.3" xref="S3.SS1.p2.1.m1.3.3.cmml"><msub id="S3.SS1.p2.1.m1.3.3.2" xref="S3.SS1.p2.1.m1.3.3.2.cmml"><mi id="S3.SS1.p2.1.m1.3.3.2.2" xref="S3.SS1.p2.1.m1.3.3.2.2.cmml">g</mi><mi id="S3.SS1.p2.1.m1.3.3.2.3" xref="S3.SS1.p2.1.m1.3.3.2.3.cmml">z</mi></msub><mo id="S3.SS1.p2.1.m1.3.3.1" xref="S3.SS1.p2.1.m1.3.3.1.cmml">^</mo></mover><mo id="S3.SS1.p2.1.m1.3.4.3.2.4" stretchy="false" xref="S3.SS1.p2.1.m1.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.3b"><apply id="S3.SS1.p2.1.m1.3.4.cmml" xref="S3.SS1.p2.1.m1.3.4"><eq id="S3.SS1.p2.1.m1.3.4.1.cmml" xref="S3.SS1.p2.1.m1.3.4.1"></eq><apply id="S3.SS1.p2.1.m1.3.4.2.cmml" xref="S3.SS1.p2.1.m1.3.4.2"><ci id="S3.SS1.p2.1.m1.3.4.2.1.cmml" xref="S3.SS1.p2.1.m1.3.4.2.1">^</ci><ci id="S3.SS1.p2.1.m1.3.4.2.2.cmml" xref="S3.SS1.p2.1.m1.3.4.2.2">ğ‘”</ci></apply><vector id="S3.SS1.p2.1.m1.3.4.3.1.cmml" xref="S3.SS1.p2.1.m1.3.4.3.2"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><ci id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1">^</ci><apply id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.2.1.cmml" xref="S3.SS1.p2.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.2.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2.2">ğ‘”</ci><ci id="S3.SS1.p2.1.m1.1.1.2.3.cmml" xref="S3.SS1.p2.1.m1.1.1.2.3">ğ‘¥</ci></apply></apply><apply id="S3.SS1.p2.1.m1.2.2.cmml" xref="S3.SS1.p2.1.m1.2.2"><ci id="S3.SS1.p2.1.m1.2.2.1.cmml" xref="S3.SS1.p2.1.m1.2.2.1">^</ci><apply id="S3.SS1.p2.1.m1.2.2.2.cmml" xref="S3.SS1.p2.1.m1.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.2.2.2.1.cmml" xref="S3.SS1.p2.1.m1.2.2.2">subscript</csymbol><ci id="S3.SS1.p2.1.m1.2.2.2.2.cmml" xref="S3.SS1.p2.1.m1.2.2.2.2">ğ‘”</ci><ci id="S3.SS1.p2.1.m1.2.2.2.3.cmml" xref="S3.SS1.p2.1.m1.2.2.2.3">ğ‘¦</ci></apply></apply><apply id="S3.SS1.p2.1.m1.3.3.cmml" xref="S3.SS1.p2.1.m1.3.3"><ci id="S3.SS1.p2.1.m1.3.3.1.cmml" xref="S3.SS1.p2.1.m1.3.3.1">^</ci><apply id="S3.SS1.p2.1.m1.3.3.2.cmml" xref="S3.SS1.p2.1.m1.3.3.2"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.3.3.2.1.cmml" xref="S3.SS1.p2.1.m1.3.3.2">subscript</csymbol><ci id="S3.SS1.p2.1.m1.3.3.2.2.cmml" xref="S3.SS1.p2.1.m1.3.3.2.2">ğ‘”</ci><ci id="S3.SS1.p2.1.m1.3.3.2.3.cmml" xref="S3.SS1.p2.1.m1.3.3.2.3">ğ‘§</ci></apply></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.3c">\hat{g}=(\hat{g_{x}},\hat{g_{y}},\hat{g_{z}})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.3d">over^ start_ARG italic_g end_ARG = ( over^ start_ARG italic_g start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT end_ARG , over^ start_ARG italic_g start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT end_ARG , over^ start_ARG italic_g start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT end_ARG )</annotation></semantics></math>, starting from the annotated 3D eye position <math alttext="{e^{3D}=(e^{3D}_{x},e^{3D}_{y},e^{3D}_{z})}" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.3"><semantics id="S3.SS1.p2.2.m2.3a"><mrow id="S3.SS1.p2.2.m2.3.3" xref="S3.SS1.p2.2.m2.3.3.cmml"><msup id="S3.SS1.p2.2.m2.3.3.5" xref="S3.SS1.p2.2.m2.3.3.5.cmml"><mi id="S3.SS1.p2.2.m2.3.3.5.2" xref="S3.SS1.p2.2.m2.3.3.5.2.cmml">e</mi><mrow id="S3.SS1.p2.2.m2.3.3.5.3" xref="S3.SS1.p2.2.m2.3.3.5.3.cmml"><mn id="S3.SS1.p2.2.m2.3.3.5.3.2" xref="S3.SS1.p2.2.m2.3.3.5.3.2.cmml">3</mn><mo id="S3.SS1.p2.2.m2.3.3.5.3.1" xref="S3.SS1.p2.2.m2.3.3.5.3.1.cmml">â¢</mo><mi id="S3.SS1.p2.2.m2.3.3.5.3.3" xref="S3.SS1.p2.2.m2.3.3.5.3.3.cmml">D</mi></mrow></msup><mo id="S3.SS1.p2.2.m2.3.3.4" xref="S3.SS1.p2.2.m2.3.3.4.cmml">=</mo><mrow id="S3.SS1.p2.2.m2.3.3.3.3" xref="S3.SS1.p2.2.m2.3.3.3.4.cmml"><mo id="S3.SS1.p2.2.m2.3.3.3.3.4" stretchy="false" xref="S3.SS1.p2.2.m2.3.3.3.4.cmml">(</mo><msubsup id="S3.SS1.p2.2.m2.1.1.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.1.1.1.2.2" xref="S3.SS1.p2.2.m2.1.1.1.1.1.2.2.cmml">e</mi><mi id="S3.SS1.p2.2.m2.1.1.1.1.1.3" xref="S3.SS1.p2.2.m2.1.1.1.1.1.3.cmml">x</mi><mrow id="S3.SS1.p2.2.m2.1.1.1.1.1.2.3" xref="S3.SS1.p2.2.m2.1.1.1.1.1.2.3.cmml"><mn id="S3.SS1.p2.2.m2.1.1.1.1.1.2.3.2" xref="S3.SS1.p2.2.m2.1.1.1.1.1.2.3.2.cmml">3</mn><mo id="S3.SS1.p2.2.m2.1.1.1.1.1.2.3.1" xref="S3.SS1.p2.2.m2.1.1.1.1.1.2.3.1.cmml">â¢</mo><mi id="S3.SS1.p2.2.m2.1.1.1.1.1.2.3.3" xref="S3.SS1.p2.2.m2.1.1.1.1.1.2.3.3.cmml">D</mi></mrow></msubsup><mo id="S3.SS1.p2.2.m2.3.3.3.3.5" xref="S3.SS1.p2.2.m2.3.3.3.4.cmml">,</mo><msubsup id="S3.SS1.p2.2.m2.2.2.2.2.2" xref="S3.SS1.p2.2.m2.2.2.2.2.2.cmml"><mi id="S3.SS1.p2.2.m2.2.2.2.2.2.2.2" xref="S3.SS1.p2.2.m2.2.2.2.2.2.2.2.cmml">e</mi><mi id="S3.SS1.p2.2.m2.2.2.2.2.2.3" xref="S3.SS1.p2.2.m2.2.2.2.2.2.3.cmml">y</mi><mrow id="S3.SS1.p2.2.m2.2.2.2.2.2.2.3" xref="S3.SS1.p2.2.m2.2.2.2.2.2.2.3.cmml"><mn id="S3.SS1.p2.2.m2.2.2.2.2.2.2.3.2" xref="S3.SS1.p2.2.m2.2.2.2.2.2.2.3.2.cmml">3</mn><mo id="S3.SS1.p2.2.m2.2.2.2.2.2.2.3.1" xref="S3.SS1.p2.2.m2.2.2.2.2.2.2.3.1.cmml">â¢</mo><mi id="S3.SS1.p2.2.m2.2.2.2.2.2.2.3.3" xref="S3.SS1.p2.2.m2.2.2.2.2.2.2.3.3.cmml">D</mi></mrow></msubsup><mo id="S3.SS1.p2.2.m2.3.3.3.3.6" xref="S3.SS1.p2.2.m2.3.3.3.4.cmml">,</mo><msubsup id="S3.SS1.p2.2.m2.3.3.3.3.3" xref="S3.SS1.p2.2.m2.3.3.3.3.3.cmml"><mi id="S3.SS1.p2.2.m2.3.3.3.3.3.2.2" xref="S3.SS1.p2.2.m2.3.3.3.3.3.2.2.cmml">e</mi><mi id="S3.SS1.p2.2.m2.3.3.3.3.3.3" xref="S3.SS1.p2.2.m2.3.3.3.3.3.3.cmml">z</mi><mrow id="S3.SS1.p2.2.m2.3.3.3.3.3.2.3" xref="S3.SS1.p2.2.m2.3.3.3.3.3.2.3.cmml"><mn id="S3.SS1.p2.2.m2.3.3.3.3.3.2.3.2" xref="S3.SS1.p2.2.m2.3.3.3.3.3.2.3.2.cmml">3</mn><mo id="S3.SS1.p2.2.m2.3.3.3.3.3.2.3.1" xref="S3.SS1.p2.2.m2.3.3.3.3.3.2.3.1.cmml">â¢</mo><mi id="S3.SS1.p2.2.m2.3.3.3.3.3.2.3.3" xref="S3.SS1.p2.2.m2.3.3.3.3.3.2.3.3.cmml">D</mi></mrow></msubsup><mo id="S3.SS1.p2.2.m2.3.3.3.3.7" stretchy="false" xref="S3.SS1.p2.2.m2.3.3.3.4.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.3b"><apply id="S3.SS1.p2.2.m2.3.3.cmml" xref="S3.SS1.p2.2.m2.3.3"><eq id="S3.SS1.p2.2.m2.3.3.4.cmml" xref="S3.SS1.p2.2.m2.3.3.4"></eq><apply id="S3.SS1.p2.2.m2.3.3.5.cmml" xref="S3.SS1.p2.2.m2.3.3.5"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.3.3.5.1.cmml" xref="S3.SS1.p2.2.m2.3.3.5">superscript</csymbol><ci id="S3.SS1.p2.2.m2.3.3.5.2.cmml" xref="S3.SS1.p2.2.m2.3.3.5.2">ğ‘’</ci><apply id="S3.SS1.p2.2.m2.3.3.5.3.cmml" xref="S3.SS1.p2.2.m2.3.3.5.3"><times id="S3.SS1.p2.2.m2.3.3.5.3.1.cmml" xref="S3.SS1.p2.2.m2.3.3.5.3.1"></times><cn id="S3.SS1.p2.2.m2.3.3.5.3.2.cmml" type="integer" xref="S3.SS1.p2.2.m2.3.3.5.3.2">3</cn><ci id="S3.SS1.p2.2.m2.3.3.5.3.3.cmml" xref="S3.SS1.p2.2.m2.3.3.5.3.3">ğ·</ci></apply></apply><vector id="S3.SS1.p2.2.m2.3.3.3.4.cmml" xref="S3.SS1.p2.2.m2.3.3.3.3"><apply id="S3.SS1.p2.2.m2.1.1.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1">subscript</csymbol><apply id="S3.SS1.p2.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.1.1.1.2.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1">superscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.1.1.1.2.2.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.2.2">ğ‘’</ci><apply id="S3.SS1.p2.2.m2.1.1.1.1.1.2.3.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.2.3"><times id="S3.SS1.p2.2.m2.1.1.1.1.1.2.3.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.2.3.1"></times><cn id="S3.SS1.p2.2.m2.1.1.1.1.1.2.3.2.cmml" type="integer" xref="S3.SS1.p2.2.m2.1.1.1.1.1.2.3.2">3</cn><ci id="S3.SS1.p2.2.m2.1.1.1.1.1.2.3.3.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.2.3.3">ğ·</ci></apply></apply><ci id="S3.SS1.p2.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.3">ğ‘¥</ci></apply><apply id="S3.SS1.p2.2.m2.2.2.2.2.2.cmml" xref="S3.SS1.p2.2.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.2.2.2.2.2.1.cmml" xref="S3.SS1.p2.2.m2.2.2.2.2.2">subscript</csymbol><apply id="S3.SS1.p2.2.m2.2.2.2.2.2.2.cmml" xref="S3.SS1.p2.2.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.2.2.2.2.2.2.1.cmml" xref="S3.SS1.p2.2.m2.2.2.2.2.2">superscript</csymbol><ci id="S3.SS1.p2.2.m2.2.2.2.2.2.2.2.cmml" xref="S3.SS1.p2.2.m2.2.2.2.2.2.2.2">ğ‘’</ci><apply id="S3.SS1.p2.2.m2.2.2.2.2.2.2.3.cmml" xref="S3.SS1.p2.2.m2.2.2.2.2.2.2.3"><times id="S3.SS1.p2.2.m2.2.2.2.2.2.2.3.1.cmml" xref="S3.SS1.p2.2.m2.2.2.2.2.2.2.3.1"></times><cn id="S3.SS1.p2.2.m2.2.2.2.2.2.2.3.2.cmml" type="integer" xref="S3.SS1.p2.2.m2.2.2.2.2.2.2.3.2">3</cn><ci id="S3.SS1.p2.2.m2.2.2.2.2.2.2.3.3.cmml" xref="S3.SS1.p2.2.m2.2.2.2.2.2.2.3.3">ğ·</ci></apply></apply><ci id="S3.SS1.p2.2.m2.2.2.2.2.2.3.cmml" xref="S3.SS1.p2.2.m2.2.2.2.2.2.3">ğ‘¦</ci></apply><apply id="S3.SS1.p2.2.m2.3.3.3.3.3.cmml" xref="S3.SS1.p2.2.m2.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.3.3.3.3.3.1.cmml" xref="S3.SS1.p2.2.m2.3.3.3.3.3">subscript</csymbol><apply id="S3.SS1.p2.2.m2.3.3.3.3.3.2.cmml" xref="S3.SS1.p2.2.m2.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.3.3.3.3.3.2.1.cmml" xref="S3.SS1.p2.2.m2.3.3.3.3.3">superscript</csymbol><ci id="S3.SS1.p2.2.m2.3.3.3.3.3.2.2.cmml" xref="S3.SS1.p2.2.m2.3.3.3.3.3.2.2">ğ‘’</ci><apply id="S3.SS1.p2.2.m2.3.3.3.3.3.2.3.cmml" xref="S3.SS1.p2.2.m2.3.3.3.3.3.2.3"><times id="S3.SS1.p2.2.m2.3.3.3.3.3.2.3.1.cmml" xref="S3.SS1.p2.2.m2.3.3.3.3.3.2.3.1"></times><cn id="S3.SS1.p2.2.m2.3.3.3.3.3.2.3.2.cmml" type="integer" xref="S3.SS1.p2.2.m2.3.3.3.3.3.2.3.2">3</cn><ci id="S3.SS1.p2.2.m2.3.3.3.3.3.2.3.3.cmml" xref="S3.SS1.p2.2.m2.3.3.3.3.3.2.3.3">ğ·</ci></apply></apply><ci id="S3.SS1.p2.2.m2.3.3.3.3.3.3.cmml" xref="S3.SS1.p2.2.m2.3.3.3.3.3.3">ğ‘§</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.3c">{e^{3D}=(e^{3D}_{x},e^{3D}_{y},e^{3D}_{z})}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.3d">italic_e start_POSTSUPERSCRIPT 3 italic_D end_POSTSUPERSCRIPT = ( italic_e start_POSTSUPERSCRIPT 3 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT , italic_e start_POSTSUPERSCRIPT 3 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT , italic_e start_POSTSUPERSCRIPT 3 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT )</annotation></semantics></math> and describing the direction of where the person in the scene is looking.
The second module (Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S3.F2" title="Figure 2 â€£ 3.2 The Skeleton-based Gaze Estimation Module â€£ 3 Methodology â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_tag">2</span></a>b) creates a heatmap <math alttext="V" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3.1"><semantics id="S3.SS1.p2.3.m3.1a"><mi id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">ğ‘‰</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">V</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m3.1d">italic_V</annotation></semantics></math> defined as the dot product between the unprojected depth map <math alttext="d^{\prime}-e^{3D}" class="ltx_Math" display="inline" id="S3.SS1.p2.4.m4.1"><semantics id="S3.SS1.p2.4.m4.1a"><mrow id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><msup id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml"><mi id="S3.SS1.p2.4.m4.1.1.2.2" xref="S3.SS1.p2.4.m4.1.1.2.2.cmml">d</mi><mo id="S3.SS1.p2.4.m4.1.1.2.3" xref="S3.SS1.p2.4.m4.1.1.2.3.cmml">â€²</mo></msup><mo id="S3.SS1.p2.4.m4.1.1.1" xref="S3.SS1.p2.4.m4.1.1.1.cmml">âˆ’</mo><msup id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml"><mi id="S3.SS1.p2.4.m4.1.1.3.2" xref="S3.SS1.p2.4.m4.1.1.3.2.cmml">e</mi><mrow id="S3.SS1.p2.4.m4.1.1.3.3" xref="S3.SS1.p2.4.m4.1.1.3.3.cmml"><mn id="S3.SS1.p2.4.m4.1.1.3.3.2" xref="S3.SS1.p2.4.m4.1.1.3.3.2.cmml">3</mn><mo id="S3.SS1.p2.4.m4.1.1.3.3.1" xref="S3.SS1.p2.4.m4.1.1.3.3.1.cmml">â¢</mo><mi id="S3.SS1.p2.4.m4.1.1.3.3.3" xref="S3.SS1.p2.4.m4.1.1.3.3.3.cmml">D</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><minus id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1.1"></minus><apply id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.2.1.cmml" xref="S3.SS1.p2.4.m4.1.1.2">superscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.2.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2.2">ğ‘‘</ci><ci id="S3.SS1.p2.4.m4.1.1.2.3.cmml" xref="S3.SS1.p2.4.m4.1.1.2.3">â€²</ci></apply><apply id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.3.1.cmml" xref="S3.SS1.p2.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.3.2.cmml" xref="S3.SS1.p2.4.m4.1.1.3.2">ğ‘’</ci><apply id="S3.SS1.p2.4.m4.1.1.3.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3"><times id="S3.SS1.p2.4.m4.1.1.3.3.1.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.1"></times><cn id="S3.SS1.p2.4.m4.1.1.3.3.2.cmml" type="integer" xref="S3.SS1.p2.4.m4.1.1.3.3.2">3</cn><ci id="S3.SS1.p2.4.m4.1.1.3.3.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.3">ğ·</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">d^{\prime}-e^{3D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.4.m4.1d">italic_d start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT - italic_e start_POSTSUPERSCRIPT 3 italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>, corresponding to the point cloud of the scene centered in the 3D eye position <math alttext="e^{3D}" class="ltx_Math" display="inline" id="S3.SS1.p2.5.m5.1"><semantics id="S3.SS1.p2.5.m5.1a"><msup id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml"><mi id="S3.SS1.p2.5.m5.1.1.2" xref="S3.SS1.p2.5.m5.1.1.2.cmml">e</mi><mrow id="S3.SS1.p2.5.m5.1.1.3" xref="S3.SS1.p2.5.m5.1.1.3.cmml"><mn id="S3.SS1.p2.5.m5.1.1.3.2" xref="S3.SS1.p2.5.m5.1.1.3.2.cmml">3</mn><mo id="S3.SS1.p2.5.m5.1.1.3.1" xref="S3.SS1.p2.5.m5.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.p2.5.m5.1.1.3.3" xref="S3.SS1.p2.5.m5.1.1.3.3.cmml">D</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><apply id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.1.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">superscript</csymbol><ci id="S3.SS1.p2.5.m5.1.1.2.cmml" xref="S3.SS1.p2.5.m5.1.1.2">ğ‘’</ci><apply id="S3.SS1.p2.5.m5.1.1.3.cmml" xref="S3.SS1.p2.5.m5.1.1.3"><times id="S3.SS1.p2.5.m5.1.1.3.1.cmml" xref="S3.SS1.p2.5.m5.1.1.3.1"></times><cn id="S3.SS1.p2.5.m5.1.1.3.2.cmml" type="integer" xref="S3.SS1.p2.5.m5.1.1.3.2">3</cn><ci id="S3.SS1.p2.5.m5.1.1.3.3.cmml" xref="S3.SS1.p2.5.m5.1.1.3.3">ğ·</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">e^{3D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.5.m5.1d">italic_e start_POSTSUPERSCRIPT 3 italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>, and the estimated gaze vector <math alttext="\hat{g}" class="ltx_Math" display="inline" id="S3.SS1.p2.6.m6.1"><semantics id="S3.SS1.p2.6.m6.1a"><mover accent="true" id="S3.SS1.p2.6.m6.1.1" xref="S3.SS1.p2.6.m6.1.1.cmml"><mi id="S3.SS1.p2.6.m6.1.1.2" xref="S3.SS1.p2.6.m6.1.1.2.cmml">g</mi><mo id="S3.SS1.p2.6.m6.1.1.1" xref="S3.SS1.p2.6.m6.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><apply id="S3.SS1.p2.6.m6.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1"><ci id="S3.SS1.p2.6.m6.1.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1.1">^</ci><ci id="S3.SS1.p2.6.m6.1.1.2.cmml" xref="S3.SS1.p2.6.m6.1.1.2">ğ‘”</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">\hat{g}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.6.m6.1d">over^ start_ARG italic_g end_ARG</annotation></semantics></math>, with the maximum values representing the points in the point cloud with the highest probability of being looked at.
Intuitively, each unprojected (and translated) point represents a vector from <math alttext="e^{3D}" class="ltx_Math" display="inline" id="S3.SS1.p2.7.m7.1"><semantics id="S3.SS1.p2.7.m7.1a"><msup id="S3.SS1.p2.7.m7.1.1" xref="S3.SS1.p2.7.m7.1.1.cmml"><mi id="S3.SS1.p2.7.m7.1.1.2" xref="S3.SS1.p2.7.m7.1.1.2.cmml">e</mi><mrow id="S3.SS1.p2.7.m7.1.1.3" xref="S3.SS1.p2.7.m7.1.1.3.cmml"><mn id="S3.SS1.p2.7.m7.1.1.3.2" xref="S3.SS1.p2.7.m7.1.1.3.2.cmml">3</mn><mo id="S3.SS1.p2.7.m7.1.1.3.1" xref="S3.SS1.p2.7.m7.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.p2.7.m7.1.1.3.3" xref="S3.SS1.p2.7.m7.1.1.3.3.cmml">D</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.1b"><apply id="S3.SS1.p2.7.m7.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1">superscript</csymbol><ci id="S3.SS1.p2.7.m7.1.1.2.cmml" xref="S3.SS1.p2.7.m7.1.1.2">ğ‘’</ci><apply id="S3.SS1.p2.7.m7.1.1.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3"><times id="S3.SS1.p2.7.m7.1.1.3.1.cmml" xref="S3.SS1.p2.7.m7.1.1.3.1"></times><cn id="S3.SS1.p2.7.m7.1.1.3.2.cmml" type="integer" xref="S3.SS1.p2.7.m7.1.1.3.2">3</cn><ci id="S3.SS1.p2.7.m7.1.1.3.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3.3">ğ·</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.1c">e^{3D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.7.m7.1d">italic_e start_POSTSUPERSCRIPT 3 italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> to that point, and the heatmap role is to highlight the closest vectors to the estimated gaze direction.
A second heatmap <math alttext="\hat{V}" class="ltx_Math" display="inline" id="S3.SS1.p2.8.m8.1"><semantics id="S3.SS1.p2.8.m8.1a"><mover accent="true" id="S3.SS1.p2.8.m8.1.1" xref="S3.SS1.p2.8.m8.1.1.cmml"><mi id="S3.SS1.p2.8.m8.1.1.2" xref="S3.SS1.p2.8.m8.1.1.2.cmml">V</mi><mo id="S3.SS1.p2.8.m8.1.1.1" xref="S3.SS1.p2.8.m8.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m8.1b"><apply id="S3.SS1.p2.8.m8.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1"><ci id="S3.SS1.p2.8.m8.1.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1.1">^</ci><ci id="S3.SS1.p2.8.m8.1.1.2.cmml" xref="S3.SS1.p2.8.m8.1.1.2">ğ‘‰</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m8.1c">\hat{V}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.8.m8.1d">over^ start_ARG italic_V end_ARG</annotation></semantics></math> is computed as <math alttext="\hat{V}=ReLU(V)^{\alpha}" class="ltx_Math" display="inline" id="S3.SS1.p2.9.m9.1"><semantics id="S3.SS1.p2.9.m9.1a"><mrow id="S3.SS1.p2.9.m9.1.2" xref="S3.SS1.p2.9.m9.1.2.cmml"><mover accent="true" id="S3.SS1.p2.9.m9.1.2.2" xref="S3.SS1.p2.9.m9.1.2.2.cmml"><mi id="S3.SS1.p2.9.m9.1.2.2.2" xref="S3.SS1.p2.9.m9.1.2.2.2.cmml">V</mi><mo id="S3.SS1.p2.9.m9.1.2.2.1" xref="S3.SS1.p2.9.m9.1.2.2.1.cmml">^</mo></mover><mo id="S3.SS1.p2.9.m9.1.2.1" xref="S3.SS1.p2.9.m9.1.2.1.cmml">=</mo><mrow id="S3.SS1.p2.9.m9.1.2.3" xref="S3.SS1.p2.9.m9.1.2.3.cmml"><mi id="S3.SS1.p2.9.m9.1.2.3.2" xref="S3.SS1.p2.9.m9.1.2.3.2.cmml">R</mi><mo id="S3.SS1.p2.9.m9.1.2.3.1" xref="S3.SS1.p2.9.m9.1.2.3.1.cmml">â¢</mo><mi id="S3.SS1.p2.9.m9.1.2.3.3" xref="S3.SS1.p2.9.m9.1.2.3.3.cmml">e</mi><mo id="S3.SS1.p2.9.m9.1.2.3.1a" xref="S3.SS1.p2.9.m9.1.2.3.1.cmml">â¢</mo><mi id="S3.SS1.p2.9.m9.1.2.3.4" xref="S3.SS1.p2.9.m9.1.2.3.4.cmml">L</mi><mo id="S3.SS1.p2.9.m9.1.2.3.1b" xref="S3.SS1.p2.9.m9.1.2.3.1.cmml">â¢</mo><mi id="S3.SS1.p2.9.m9.1.2.3.5" xref="S3.SS1.p2.9.m9.1.2.3.5.cmml">U</mi><mo id="S3.SS1.p2.9.m9.1.2.3.1c" xref="S3.SS1.p2.9.m9.1.2.3.1.cmml">â¢</mo><msup id="S3.SS1.p2.9.m9.1.2.3.6" xref="S3.SS1.p2.9.m9.1.2.3.6.cmml"><mrow id="S3.SS1.p2.9.m9.1.2.3.6.2.2" xref="S3.SS1.p2.9.m9.1.2.3.6.cmml"><mo id="S3.SS1.p2.9.m9.1.2.3.6.2.2.1" stretchy="false" xref="S3.SS1.p2.9.m9.1.2.3.6.cmml">(</mo><mi id="S3.SS1.p2.9.m9.1.1" xref="S3.SS1.p2.9.m9.1.1.cmml">V</mi><mo id="S3.SS1.p2.9.m9.1.2.3.6.2.2.2" stretchy="false" xref="S3.SS1.p2.9.m9.1.2.3.6.cmml">)</mo></mrow><mi id="S3.SS1.p2.9.m9.1.2.3.6.3" xref="S3.SS1.p2.9.m9.1.2.3.6.3.cmml">Î±</mi></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.9.m9.1b"><apply id="S3.SS1.p2.9.m9.1.2.cmml" xref="S3.SS1.p2.9.m9.1.2"><eq id="S3.SS1.p2.9.m9.1.2.1.cmml" xref="S3.SS1.p2.9.m9.1.2.1"></eq><apply id="S3.SS1.p2.9.m9.1.2.2.cmml" xref="S3.SS1.p2.9.m9.1.2.2"><ci id="S3.SS1.p2.9.m9.1.2.2.1.cmml" xref="S3.SS1.p2.9.m9.1.2.2.1">^</ci><ci id="S3.SS1.p2.9.m9.1.2.2.2.cmml" xref="S3.SS1.p2.9.m9.1.2.2.2">ğ‘‰</ci></apply><apply id="S3.SS1.p2.9.m9.1.2.3.cmml" xref="S3.SS1.p2.9.m9.1.2.3"><times id="S3.SS1.p2.9.m9.1.2.3.1.cmml" xref="S3.SS1.p2.9.m9.1.2.3.1"></times><ci id="S3.SS1.p2.9.m9.1.2.3.2.cmml" xref="S3.SS1.p2.9.m9.1.2.3.2">ğ‘…</ci><ci id="S3.SS1.p2.9.m9.1.2.3.3.cmml" xref="S3.SS1.p2.9.m9.1.2.3.3">ğ‘’</ci><ci id="S3.SS1.p2.9.m9.1.2.3.4.cmml" xref="S3.SS1.p2.9.m9.1.2.3.4">ğ¿</ci><ci id="S3.SS1.p2.9.m9.1.2.3.5.cmml" xref="S3.SS1.p2.9.m9.1.2.3.5">ğ‘ˆ</ci><apply id="S3.SS1.p2.9.m9.1.2.3.6.cmml" xref="S3.SS1.p2.9.m9.1.2.3.6"><csymbol cd="ambiguous" id="S3.SS1.p2.9.m9.1.2.3.6.1.cmml" xref="S3.SS1.p2.9.m9.1.2.3.6">superscript</csymbol><ci id="S3.SS1.p2.9.m9.1.1.cmml" xref="S3.SS1.p2.9.m9.1.1">ğ‘‰</ci><ci id="S3.SS1.p2.9.m9.1.2.3.6.3.cmml" xref="S3.SS1.p2.9.m9.1.2.3.6.3">ğ›¼</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.9.m9.1c">\hat{V}=ReLU(V)^{\alpha}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.9.m9.1d">over^ start_ARG italic_V end_ARG = italic_R italic_e italic_L italic_U ( italic_V ) start_POSTSUPERSCRIPT italic_Î± end_POSTSUPERSCRIPT</annotation></semantics></math>, with <math alttext="\alpha=3" class="ltx_Math" display="inline" id="S3.SS1.p2.10.m10.1"><semantics id="S3.SS1.p2.10.m10.1a"><mrow id="S3.SS1.p2.10.m10.1.1" xref="S3.SS1.p2.10.m10.1.1.cmml"><mi id="S3.SS1.p2.10.m10.1.1.2" xref="S3.SS1.p2.10.m10.1.1.2.cmml">Î±</mi><mo id="S3.SS1.p2.10.m10.1.1.1" xref="S3.SS1.p2.10.m10.1.1.1.cmml">=</mo><mn id="S3.SS1.p2.10.m10.1.1.3" xref="S3.SS1.p2.10.m10.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.10.m10.1b"><apply id="S3.SS1.p2.10.m10.1.1.cmml" xref="S3.SS1.p2.10.m10.1.1"><eq id="S3.SS1.p2.10.m10.1.1.1.cmml" xref="S3.SS1.p2.10.m10.1.1.1"></eq><ci id="S3.SS1.p2.10.m10.1.1.2.cmml" xref="S3.SS1.p2.10.m10.1.1.2">ğ›¼</ci><cn id="S3.SS1.p2.10.m10.1.1.3.cmml" type="integer" xref="S3.SS1.p2.10.m10.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.10.m10.1c">\alpha=3</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.10.m10.1d">italic_Î± = 3</annotation></semantics></math>, to further highlight the most interesting points in the scene.
The scene image <math alttext="s" class="ltx_Math" display="inline" id="S3.SS1.p2.11.m11.1"><semantics id="S3.SS1.p2.11.m11.1a"><mi id="S3.SS1.p2.11.m11.1.1" xref="S3.SS1.p2.11.m11.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.11.m11.1b"><ci id="S3.SS1.p2.11.m11.1.1.cmml" xref="S3.SS1.p2.11.m11.1.1">ğ‘ </ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.11.m11.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.11.m11.1d">italic_s</annotation></semantics></math>, the heatmaps <math alttext="V" class="ltx_Math" display="inline" id="S3.SS1.p2.12.m12.1"><semantics id="S3.SS1.p2.12.m12.1a"><mi id="S3.SS1.p2.12.m12.1.1" xref="S3.SS1.p2.12.m12.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.12.m12.1b"><ci id="S3.SS1.p2.12.m12.1.1.cmml" xref="S3.SS1.p2.12.m12.1.1">ğ‘‰</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.12.m12.1c">V</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.12.m12.1d">italic_V</annotation></semantics></math> and <math alttext="\hat{V}" class="ltx_Math" display="inline" id="S3.SS1.p2.13.m13.1"><semantics id="S3.SS1.p2.13.m13.1a"><mover accent="true" id="S3.SS1.p2.13.m13.1.1" xref="S3.SS1.p2.13.m13.1.1.cmml"><mi id="S3.SS1.p2.13.m13.1.1.2" xref="S3.SS1.p2.13.m13.1.1.2.cmml">V</mi><mo id="S3.SS1.p2.13.m13.1.1.1" xref="S3.SS1.p2.13.m13.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.13.m13.1b"><apply id="S3.SS1.p2.13.m13.1.1.cmml" xref="S3.SS1.p2.13.m13.1.1"><ci id="S3.SS1.p2.13.m13.1.1.1.cmml" xref="S3.SS1.p2.13.m13.1.1.1">^</ci><ci id="S3.SS1.p2.13.m13.1.1.2.cmml" xref="S3.SS1.p2.13.m13.1.1.2">ğ‘‰</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.13.m13.1c">\hat{V}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.13.m13.1d">over^ start_ARG italic_V end_ARG</annotation></semantics></math>, and the head location mask <math alttext="m" class="ltx_Math" display="inline" id="S3.SS1.p2.14.m14.1"><semantics id="S3.SS1.p2.14.m14.1a"><mi id="S3.SS1.p2.14.m14.1.1" xref="S3.SS1.p2.14.m14.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.14.m14.1b"><ci id="S3.SS1.p2.14.m14.1.1.cmml" xref="S3.SS1.p2.14.m14.1.1">ğ‘š</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.14.m14.1c">m</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.14.m14.1d">italic_m</annotation></semantics></math> are then concatenated and fed to the third module (Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S3.F2" title="Figure 2 â€£ 3.2 The Skeleton-based Gaze Estimation Module â€£ 3 Methodology â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_tag">2</span></a>c), an encoder-decoder network, that generates the 2D gaze heatmap <math alttext="H" class="ltx_Math" display="inline" id="S3.SS1.p2.15.m15.1"><semantics id="S3.SS1.p2.15.m15.1a"><mi id="S3.SS1.p2.15.m15.1.1" xref="S3.SS1.p2.15.m15.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.15.m15.1b"><ci id="S3.SS1.p2.15.m15.1.1.cmml" xref="S3.SS1.p2.15.m15.1.1">ğ»</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.15.m15.1c">H</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.15.m15.1d">italic_H</annotation></semantics></math> used to extract the 2D gaze target.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.4">The last module, depicted in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S3.F2" title="Figure 2 â€£ 3.2 The Skeleton-based Gaze Estimation Module â€£ 3 Methodology â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_tag">2</span></a>d, is used only in the evaluation phase and is responsible for extracting the 3D gaze target from the original point cloud.
After extracting the 2D gaze target from the predicted heatmap <math alttext="H" class="ltx_Math" display="inline" id="S3.SS1.p3.1.m1.1"><semantics id="S3.SS1.p3.1.m1.1a"><mi id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><ci id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">ğ»</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">H</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.1.m1.1d">italic_H</annotation></semantics></math> as <span class="ltx_text" id="S3.SS1.p3.2.1"><math alttext="t^{2D}=\texttt{argmax}(H)" class="ltx_Math" display="inline" id="S3.SS1.p3.2.1.m1.1"><semantics id="S3.SS1.p3.2.1.m1.1a"><mrow id="S3.SS1.p3.2.1.m1.1.2" xref="S3.SS1.p3.2.1.m1.1.2.cmml"><msup id="S3.SS1.p3.2.1.m1.1.2.2" xref="S3.SS1.p3.2.1.m1.1.2.2.cmml"><mi id="S3.SS1.p3.2.1.m1.1.2.2.2" xref="S3.SS1.p3.2.1.m1.1.2.2.2.cmml">t</mi><mrow id="S3.SS1.p3.2.1.m1.1.2.2.3" xref="S3.SS1.p3.2.1.m1.1.2.2.3.cmml"><mn id="S3.SS1.p3.2.1.m1.1.2.2.3.2" xref="S3.SS1.p3.2.1.m1.1.2.2.3.2.cmml">2</mn><mo id="S3.SS1.p3.2.1.m1.1.2.2.3.1" xref="S3.SS1.p3.2.1.m1.1.2.2.3.1.cmml">â¢</mo><mi id="S3.SS1.p3.2.1.m1.1.2.2.3.3" xref="S3.SS1.p3.2.1.m1.1.2.2.3.3.cmml">D</mi></mrow></msup><mo id="S3.SS1.p3.2.1.m1.1.2.1" xref="S3.SS1.p3.2.1.m1.1.2.1.cmml">=</mo><mrow id="S3.SS1.p3.2.1.m1.1.2.3" xref="S3.SS1.p3.2.1.m1.1.2.3.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.SS1.p3.2.1.m1.1.2.3.2" xref="S3.SS1.p3.2.1.m1.1.2.3.2a.cmml">argmax</mtext><mo id="S3.SS1.p3.2.1.m1.1.2.3.1" xref="S3.SS1.p3.2.1.m1.1.2.3.1.cmml">â¢</mo><mrow id="S3.SS1.p3.2.1.m1.1.2.3.3.2" xref="S3.SS1.p3.2.1.m1.1.2.3.cmml"><mo id="S3.SS1.p3.2.1.m1.1.2.3.3.2.1" stretchy="false" xref="S3.SS1.p3.2.1.m1.1.2.3.cmml">(</mo><mi id="S3.SS1.p3.2.1.m1.1.1" xref="S3.SS1.p3.2.1.m1.1.1.cmml">H</mi><mo id="S3.SS1.p3.2.1.m1.1.2.3.3.2.2" stretchy="false" xref="S3.SS1.p3.2.1.m1.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.1.m1.1b"><apply id="S3.SS1.p3.2.1.m1.1.2.cmml" xref="S3.SS1.p3.2.1.m1.1.2"><eq id="S3.SS1.p3.2.1.m1.1.2.1.cmml" xref="S3.SS1.p3.2.1.m1.1.2.1"></eq><apply id="S3.SS1.p3.2.1.m1.1.2.2.cmml" xref="S3.SS1.p3.2.1.m1.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.p3.2.1.m1.1.2.2.1.cmml" xref="S3.SS1.p3.2.1.m1.1.2.2">superscript</csymbol><ci id="S3.SS1.p3.2.1.m1.1.2.2.2.cmml" xref="S3.SS1.p3.2.1.m1.1.2.2.2">ğ‘¡</ci><apply id="S3.SS1.p3.2.1.m1.1.2.2.3.cmml" xref="S3.SS1.p3.2.1.m1.1.2.2.3"><times id="S3.SS1.p3.2.1.m1.1.2.2.3.1.cmml" xref="S3.SS1.p3.2.1.m1.1.2.2.3.1"></times><cn id="S3.SS1.p3.2.1.m1.1.2.2.3.2.cmml" type="integer" xref="S3.SS1.p3.2.1.m1.1.2.2.3.2">2</cn><ci id="S3.SS1.p3.2.1.m1.1.2.2.3.3.cmml" xref="S3.SS1.p3.2.1.m1.1.2.2.3.3">ğ·</ci></apply></apply><apply id="S3.SS1.p3.2.1.m1.1.2.3.cmml" xref="S3.SS1.p3.2.1.m1.1.2.3"><times id="S3.SS1.p3.2.1.m1.1.2.3.1.cmml" xref="S3.SS1.p3.2.1.m1.1.2.3.1"></times><ci id="S3.SS1.p3.2.1.m1.1.2.3.2a.cmml" xref="S3.SS1.p3.2.1.m1.1.2.3.2"><mtext class="ltx_mathvariant_monospace" id="S3.SS1.p3.2.1.m1.1.2.3.2.cmml" xref="S3.SS1.p3.2.1.m1.1.2.3.2">argmax</mtext></ci><ci id="S3.SS1.p3.2.1.m1.1.1.cmml" xref="S3.SS1.p3.2.1.m1.1.1">ğ»</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.1.m1.1c">t^{2D}=\texttt{argmax}(H)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.2.1.m1.1d">italic_t start_POSTSUPERSCRIPT 2 italic_D end_POSTSUPERSCRIPT = argmax ( italic_H )</annotation></semantics></math></span>, a 3D volume centered in that point is built in the unprojected (and translated to the <math alttext="e^{3D}" class="ltx_Math" display="inline" id="S3.SS1.p3.3.m2.1"><semantics id="S3.SS1.p3.3.m2.1a"><msup id="S3.SS1.p3.3.m2.1.1" xref="S3.SS1.p3.3.m2.1.1.cmml"><mi id="S3.SS1.p3.3.m2.1.1.2" xref="S3.SS1.p3.3.m2.1.1.2.cmml">e</mi><mrow id="S3.SS1.p3.3.m2.1.1.3" xref="S3.SS1.p3.3.m2.1.1.3.cmml"><mn id="S3.SS1.p3.3.m2.1.1.3.2" xref="S3.SS1.p3.3.m2.1.1.3.2.cmml">3</mn><mo id="S3.SS1.p3.3.m2.1.1.3.1" xref="S3.SS1.p3.3.m2.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.p3.3.m2.1.1.3.3" xref="S3.SS1.p3.3.m2.1.1.3.3.cmml">D</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m2.1b"><apply id="S3.SS1.p3.3.m2.1.1.cmml" xref="S3.SS1.p3.3.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.3.m2.1.1.1.cmml" xref="S3.SS1.p3.3.m2.1.1">superscript</csymbol><ci id="S3.SS1.p3.3.m2.1.1.2.cmml" xref="S3.SS1.p3.3.m2.1.1.2">ğ‘’</ci><apply id="S3.SS1.p3.3.m2.1.1.3.cmml" xref="S3.SS1.p3.3.m2.1.1.3"><times id="S3.SS1.p3.3.m2.1.1.3.1.cmml" xref="S3.SS1.p3.3.m2.1.1.3.1"></times><cn id="S3.SS1.p3.3.m2.1.1.3.2.cmml" type="integer" xref="S3.SS1.p3.3.m2.1.1.3.2">3</cn><ci id="S3.SS1.p3.3.m2.1.1.3.3.cmml" xref="S3.SS1.p3.3.m2.1.1.3.3">ğ·</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m2.1c">e^{3D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.3.m2.1d">italic_e start_POSTSUPERSCRIPT 3 italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> position) vector space to enclose a portion of the available points.
The vector with the highest similarity to the predicted gaze vector inside the volume is selected as the new predicted gaze vector <math alttext="\hat{g}" class="ltx_Math" display="inline" id="S3.SS1.p3.4.m3.1"><semantics id="S3.SS1.p3.4.m3.1a"><mover accent="true" id="S3.SS1.p3.4.m3.1.1" xref="S3.SS1.p3.4.m3.1.1.cmml"><mi id="S3.SS1.p3.4.m3.1.1.2" xref="S3.SS1.p3.4.m3.1.1.2.cmml">g</mi><mo id="S3.SS1.p3.4.m3.1.1.1" xref="S3.SS1.p3.4.m3.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m3.1b"><apply id="S3.SS1.p3.4.m3.1.1.cmml" xref="S3.SS1.p3.4.m3.1.1"><ci id="S3.SS1.p3.4.m3.1.1.1.cmml" xref="S3.SS1.p3.4.m3.1.1.1">^</ci><ci id="S3.SS1.p3.4.m3.1.1.2.cmml" xref="S3.SS1.p3.4.m3.1.1.2">ğ‘”</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m3.1c">\hat{g}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.4.m3.1d">over^ start_ARG italic_g end_ARG</annotation></semantics></math>, and the corresponding point
is selected as the 3D gaze target from the original point cloud. Below, we discuss the critical ingredients and the processing of our approach and, as a result, the novelty of the pipeline in detail.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>The Skeleton-based Gaze Estimation Module</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">To effectively obtain a pipeline able to preserve the identity of the subjects and avoid using image data related to the appearance of the face, the first step we adopt consists of replacing the original gaze estimation method based on face cropsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib25" title="">25</a>]</cite>.
We, therefore, introduce a novel module for gaze estimation, <em class="ltx_emph ltx_font_italic" id="S3.SS2.p1.1.1">i.e</em>.<span class="ltx_text" id="S3.SS2.p1.1.2"></span>, the prediction of the gaze direction in the 3D space, that exploits the upper-body skeleton and the depth map as multi-modal information to extract a coherent gaze vector.
Considering the pose estimation as a pre-processing step, after which the whole gaze target detection pipeline doesnâ€™t require the face appearance, the proposed gaze estimation module has proven effective in correctly estimating the gaze direction despite having no access to either the face crop or the scene image.
The pose key points used in this study are from the hip upwards, for a total of 13 joints, including the arms and wrists, which are extracted using a pre-trained top-down pose estimatorÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib47" title="">47</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">The key points go through a normalization procedure that uses the neck position and the distance between the neck and hip joints to obtain a scale-invariant representation of the human pose.
A simple MLP with three layers processes them to expand the featuresâ€™ dimensionality. Concurrently, a convolutional encoder extracts the depth features from the resized depth map.
The upper-body and depth features are concatenated, and a single attention layer is applied to select the most helpful features for predicting the gaze direction.
Finally, two linear layers are applied to these features to extract the 3D gaze vector <math alttext="\hat{g}" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mover accent="true" id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">g</mi><mo id="S3.SS2.p2.1.m1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><ci id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1">^</ci><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">ğ‘”</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\hat{g}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">over^ start_ARG italic_g end_ARG</annotation></semantics></math>, which is normalized before being handed over to the next module.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="252" id="S3.F2.g1" src="x2.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.8.2.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S3.F2.2.1" style="font-size:90%;">Overview of the proposed approach. <span class="ltx_text ltx_font_bold" id="S3.F2.2.1.1">a)</span> The 3D Gaze Estimation Module predicts a gaze vector by exploiting the upper-body pose coordinates, processed by a simple MLP, and the convolutional features extracted from the depth map by a ResNet50Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib22" title="">22</a>]</cite>. A single multi-head attention layer is applied to the concatenated features before the final MLP.
<span class="ltx_text ltx_font_bold" id="S3.F2.2.1.2">b)</span> The perception module converts the depth map to the unprojected point cloud, from which the 3D eye coordinates are subtracted, and computes two heatmaps highlighting the most interesting part of the scene.
<span class="ltx_text ltx_font_bold" id="S3.F2.2.1.3">c)</span> The 2D gaze target heatmap is predicted by an encoder-decoder architecture operating on the concatenation of the scene image (with the blurred-out face), the head location mask and the two heatmaps.
<span class="ltx_text ltx_font_bold" id="S3.F2.2.1.4">d)</span> During the evaluation, the most similar vector to the predicted vector <math alttext="\hat{g}" class="ltx_Math" display="inline" id="S3.F2.2.1.m1.1"><semantics id="S3.F2.2.1.m1.1b"><mover accent="true" id="S3.F2.2.1.m1.1.1" xref="S3.F2.2.1.m1.1.1.cmml"><mi id="S3.F2.2.1.m1.1.1.2" xref="S3.F2.2.1.m1.1.1.2.cmml">g</mi><mo id="S3.F2.2.1.m1.1.1.1" xref="S3.F2.2.1.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.F2.2.1.m1.1c"><apply id="S3.F2.2.1.m1.1.1.cmml" xref="S3.F2.2.1.m1.1.1"><ci id="S3.F2.2.1.m1.1.1.1.cmml" xref="S3.F2.2.1.m1.1.1.1">^</ci><ci id="S3.F2.2.1.m1.1.1.2.cmml" xref="S3.F2.2.1.m1.1.1.2">ğ‘”</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.2.1.m1.1d">\hat{g}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.2.1.m1.1e">over^ start_ARG italic_g end_ARG</annotation></semantics></math> in the unprojected and translated point cloud is selected as the final gaze vector and the corresponding point in the original point cloud as the predicted 3D gaze target.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Privacy Preservation</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.2">The only step of the proposed pipeline where the face images are used is the pose estimation pre-processing phase, ensuring that the gaze target detection architecture can offer the claimed privacy-preserving feature.
In addition to the use of upper-body poses and depth maps to replace the need to use head crops, the entire scene image used as one of the inputs by the auto-encoder module for the prediction of the heatmap <math alttext="H" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">ğ»</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">H</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">italic_H</annotation></semantics></math> has been edited by applying Gaussian blur in the region defined by the head bounding box provided in the dataset, <math alttext="box_{h}" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2.1"><semantics id="S3.SS3.p1.2.m2.1a"><mrow id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml"><mi id="S3.SS3.p1.2.m2.1.1.2" xref="S3.SS3.p1.2.m2.1.1.2.cmml">b</mi><mo id="S3.SS3.p1.2.m2.1.1.1" xref="S3.SS3.p1.2.m2.1.1.1.cmml">â¢</mo><mi id="S3.SS3.p1.2.m2.1.1.3" xref="S3.SS3.p1.2.m2.1.1.3.cmml">o</mi><mo id="S3.SS3.p1.2.m2.1.1.1a" xref="S3.SS3.p1.2.m2.1.1.1.cmml">â¢</mo><msub id="S3.SS3.p1.2.m2.1.1.4" xref="S3.SS3.p1.2.m2.1.1.4.cmml"><mi id="S3.SS3.p1.2.m2.1.1.4.2" xref="S3.SS3.p1.2.m2.1.1.4.2.cmml">x</mi><mi id="S3.SS3.p1.2.m2.1.1.4.3" xref="S3.SS3.p1.2.m2.1.1.4.3.cmml">h</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><times id="S3.SS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1"></times><ci id="S3.SS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2">ğ‘</ci><ci id="S3.SS3.p1.2.m2.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3">ğ‘œ</ci><apply id="S3.SS3.p1.2.m2.1.1.4.cmml" xref="S3.SS3.p1.2.m2.1.1.4"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.4.1.cmml" xref="S3.SS3.p1.2.m2.1.1.4">subscript</csymbol><ci id="S3.SS3.p1.2.m2.1.1.4.2.cmml" xref="S3.SS3.p1.2.m2.1.1.4.2">ğ‘¥</ci><ci id="S3.SS3.p1.2.m2.1.1.4.3.cmml" xref="S3.SS3.p1.2.m2.1.1.4.3">â„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">box_{h}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m2.1d">italic_b italic_o italic_x start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT</annotation></semantics></math>, hence making sure that no module could use that information to improve the gaze target detection.
Examples of edited scene images and skeletons extracted by the pose estimation method are shown in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S4.F3" title="Figure 3 â€£ 4.5 Qualitative Results â€£ 4 Experiments â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Training</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.9">Excluding the pose estimation phase, where a pre-trained 2D pose estimator has been adopted, our gaze target detection pipeline can be trained in two ways: a) multi-stage training, and b) end-to-end manner.
For the multi-stage training, firstly, the gaze estimation module is trained standalone using only a cosine loss on the estimated gaze vector <math alttext="{\mathcal{L}_{gaze}=1-CosineSimilarity(g,\hat{g})}" class="ltx_Math" display="inline" id="S3.SS4.p1.1.m1.2"><semantics id="S3.SS4.p1.1.m1.2a"><mrow id="S3.SS4.p1.1.m1.2.3" xref="S3.SS4.p1.1.m1.2.3.cmml"><msub id="S3.SS4.p1.1.m1.2.3.2" xref="S3.SS4.p1.1.m1.2.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p1.1.m1.2.3.2.2" xref="S3.SS4.p1.1.m1.2.3.2.2.cmml">â„’</mi><mrow id="S3.SS4.p1.1.m1.2.3.2.3" xref="S3.SS4.p1.1.m1.2.3.2.3.cmml"><mi id="S3.SS4.p1.1.m1.2.3.2.3.2" xref="S3.SS4.p1.1.m1.2.3.2.3.2.cmml">g</mi><mo id="S3.SS4.p1.1.m1.2.3.2.3.1" xref="S3.SS4.p1.1.m1.2.3.2.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.1.m1.2.3.2.3.3" xref="S3.SS4.p1.1.m1.2.3.2.3.3.cmml">a</mi><mo id="S3.SS4.p1.1.m1.2.3.2.3.1a" xref="S3.SS4.p1.1.m1.2.3.2.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.1.m1.2.3.2.3.4" xref="S3.SS4.p1.1.m1.2.3.2.3.4.cmml">z</mi><mo id="S3.SS4.p1.1.m1.2.3.2.3.1b" xref="S3.SS4.p1.1.m1.2.3.2.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.1.m1.2.3.2.3.5" xref="S3.SS4.p1.1.m1.2.3.2.3.5.cmml">e</mi></mrow></msub><mo id="S3.SS4.p1.1.m1.2.3.1" xref="S3.SS4.p1.1.m1.2.3.1.cmml">=</mo><mrow id="S3.SS4.p1.1.m1.2.3.3" xref="S3.SS4.p1.1.m1.2.3.3.cmml"><mn id="S3.SS4.p1.1.m1.2.3.3.2" xref="S3.SS4.p1.1.m1.2.3.3.2.cmml">1</mn><mo id="S3.SS4.p1.1.m1.2.3.3.1" xref="S3.SS4.p1.1.m1.2.3.3.1.cmml">âˆ’</mo><mrow id="S3.SS4.p1.1.m1.2.3.3.3" xref="S3.SS4.p1.1.m1.2.3.3.3.cmml"><mi id="S3.SS4.p1.1.m1.2.3.3.3.2" xref="S3.SS4.p1.1.m1.2.3.3.3.2.cmml">C</mi><mo id="S3.SS4.p1.1.m1.2.3.3.3.1" xref="S3.SS4.p1.1.m1.2.3.3.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.1.m1.2.3.3.3.3" xref="S3.SS4.p1.1.m1.2.3.3.3.3.cmml">o</mi><mo id="S3.SS4.p1.1.m1.2.3.3.3.1a" xref="S3.SS4.p1.1.m1.2.3.3.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.1.m1.2.3.3.3.4" xref="S3.SS4.p1.1.m1.2.3.3.3.4.cmml">s</mi><mo id="S3.SS4.p1.1.m1.2.3.3.3.1b" xref="S3.SS4.p1.1.m1.2.3.3.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.1.m1.2.3.3.3.5" xref="S3.SS4.p1.1.m1.2.3.3.3.5.cmml">i</mi><mo id="S3.SS4.p1.1.m1.2.3.3.3.1c" xref="S3.SS4.p1.1.m1.2.3.3.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.1.m1.2.3.3.3.6" xref="S3.SS4.p1.1.m1.2.3.3.3.6.cmml">n</mi><mo id="S3.SS4.p1.1.m1.2.3.3.3.1d" xref="S3.SS4.p1.1.m1.2.3.3.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.1.m1.2.3.3.3.7" xref="S3.SS4.p1.1.m1.2.3.3.3.7.cmml">e</mi><mo id="S3.SS4.p1.1.m1.2.3.3.3.1e" xref="S3.SS4.p1.1.m1.2.3.3.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.1.m1.2.3.3.3.8" xref="S3.SS4.p1.1.m1.2.3.3.3.8.cmml">S</mi><mo id="S3.SS4.p1.1.m1.2.3.3.3.1f" xref="S3.SS4.p1.1.m1.2.3.3.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.1.m1.2.3.3.3.9" xref="S3.SS4.p1.1.m1.2.3.3.3.9.cmml">i</mi><mo id="S3.SS4.p1.1.m1.2.3.3.3.1g" xref="S3.SS4.p1.1.m1.2.3.3.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.1.m1.2.3.3.3.10" xref="S3.SS4.p1.1.m1.2.3.3.3.10.cmml">m</mi><mo id="S3.SS4.p1.1.m1.2.3.3.3.1h" xref="S3.SS4.p1.1.m1.2.3.3.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.1.m1.2.3.3.3.11" xref="S3.SS4.p1.1.m1.2.3.3.3.11.cmml">i</mi><mo id="S3.SS4.p1.1.m1.2.3.3.3.1i" xref="S3.SS4.p1.1.m1.2.3.3.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.1.m1.2.3.3.3.12" xref="S3.SS4.p1.1.m1.2.3.3.3.12.cmml">l</mi><mo id="S3.SS4.p1.1.m1.2.3.3.3.1j" xref="S3.SS4.p1.1.m1.2.3.3.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.1.m1.2.3.3.3.13" xref="S3.SS4.p1.1.m1.2.3.3.3.13.cmml">a</mi><mo id="S3.SS4.p1.1.m1.2.3.3.3.1k" xref="S3.SS4.p1.1.m1.2.3.3.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.1.m1.2.3.3.3.14" xref="S3.SS4.p1.1.m1.2.3.3.3.14.cmml">r</mi><mo id="S3.SS4.p1.1.m1.2.3.3.3.1l" xref="S3.SS4.p1.1.m1.2.3.3.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.1.m1.2.3.3.3.15" xref="S3.SS4.p1.1.m1.2.3.3.3.15.cmml">i</mi><mo id="S3.SS4.p1.1.m1.2.3.3.3.1m" xref="S3.SS4.p1.1.m1.2.3.3.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.1.m1.2.3.3.3.16" xref="S3.SS4.p1.1.m1.2.3.3.3.16.cmml">t</mi><mo id="S3.SS4.p1.1.m1.2.3.3.3.1n" xref="S3.SS4.p1.1.m1.2.3.3.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.1.m1.2.3.3.3.17" xref="S3.SS4.p1.1.m1.2.3.3.3.17.cmml">y</mi><mo id="S3.SS4.p1.1.m1.2.3.3.3.1o" xref="S3.SS4.p1.1.m1.2.3.3.3.1.cmml">â¢</mo><mrow id="S3.SS4.p1.1.m1.2.3.3.3.18.2" xref="S3.SS4.p1.1.m1.2.3.3.3.18.1.cmml"><mo id="S3.SS4.p1.1.m1.2.3.3.3.18.2.1" stretchy="false" xref="S3.SS4.p1.1.m1.2.3.3.3.18.1.cmml">(</mo><mi id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml">g</mi><mo id="S3.SS4.p1.1.m1.2.3.3.3.18.2.2" xref="S3.SS4.p1.1.m1.2.3.3.3.18.1.cmml">,</mo><mover accent="true" id="S3.SS4.p1.1.m1.2.2" xref="S3.SS4.p1.1.m1.2.2.cmml"><mi id="S3.SS4.p1.1.m1.2.2.2" xref="S3.SS4.p1.1.m1.2.2.2.cmml">g</mi><mo id="S3.SS4.p1.1.m1.2.2.1" xref="S3.SS4.p1.1.m1.2.2.1.cmml">^</mo></mover><mo id="S3.SS4.p1.1.m1.2.3.3.3.18.2.3" stretchy="false" xref="S3.SS4.p1.1.m1.2.3.3.3.18.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.2b"><apply id="S3.SS4.p1.1.m1.2.3.cmml" xref="S3.SS4.p1.1.m1.2.3"><eq id="S3.SS4.p1.1.m1.2.3.1.cmml" xref="S3.SS4.p1.1.m1.2.3.1"></eq><apply id="S3.SS4.p1.1.m1.2.3.2.cmml" xref="S3.SS4.p1.1.m1.2.3.2"><csymbol cd="ambiguous" id="S3.SS4.p1.1.m1.2.3.2.1.cmml" xref="S3.SS4.p1.1.m1.2.3.2">subscript</csymbol><ci id="S3.SS4.p1.1.m1.2.3.2.2.cmml" xref="S3.SS4.p1.1.m1.2.3.2.2">â„’</ci><apply id="S3.SS4.p1.1.m1.2.3.2.3.cmml" xref="S3.SS4.p1.1.m1.2.3.2.3"><times id="S3.SS4.p1.1.m1.2.3.2.3.1.cmml" xref="S3.SS4.p1.1.m1.2.3.2.3.1"></times><ci id="S3.SS4.p1.1.m1.2.3.2.3.2.cmml" xref="S3.SS4.p1.1.m1.2.3.2.3.2">ğ‘”</ci><ci id="S3.SS4.p1.1.m1.2.3.2.3.3.cmml" xref="S3.SS4.p1.1.m1.2.3.2.3.3">ğ‘</ci><ci id="S3.SS4.p1.1.m1.2.3.2.3.4.cmml" xref="S3.SS4.p1.1.m1.2.3.2.3.4">ğ‘§</ci><ci id="S3.SS4.p1.1.m1.2.3.2.3.5.cmml" xref="S3.SS4.p1.1.m1.2.3.2.3.5">ğ‘’</ci></apply></apply><apply id="S3.SS4.p1.1.m1.2.3.3.cmml" xref="S3.SS4.p1.1.m1.2.3.3"><minus id="S3.SS4.p1.1.m1.2.3.3.1.cmml" xref="S3.SS4.p1.1.m1.2.3.3.1"></minus><cn id="S3.SS4.p1.1.m1.2.3.3.2.cmml" type="integer" xref="S3.SS4.p1.1.m1.2.3.3.2">1</cn><apply id="S3.SS4.p1.1.m1.2.3.3.3.cmml" xref="S3.SS4.p1.1.m1.2.3.3.3"><times id="S3.SS4.p1.1.m1.2.3.3.3.1.cmml" xref="S3.SS4.p1.1.m1.2.3.3.3.1"></times><ci id="S3.SS4.p1.1.m1.2.3.3.3.2.cmml" xref="S3.SS4.p1.1.m1.2.3.3.3.2">ğ¶</ci><ci id="S3.SS4.p1.1.m1.2.3.3.3.3.cmml" xref="S3.SS4.p1.1.m1.2.3.3.3.3">ğ‘œ</ci><ci id="S3.SS4.p1.1.m1.2.3.3.3.4.cmml" xref="S3.SS4.p1.1.m1.2.3.3.3.4">ğ‘ </ci><ci id="S3.SS4.p1.1.m1.2.3.3.3.5.cmml" xref="S3.SS4.p1.1.m1.2.3.3.3.5">ğ‘–</ci><ci id="S3.SS4.p1.1.m1.2.3.3.3.6.cmml" xref="S3.SS4.p1.1.m1.2.3.3.3.6">ğ‘›</ci><ci id="S3.SS4.p1.1.m1.2.3.3.3.7.cmml" xref="S3.SS4.p1.1.m1.2.3.3.3.7">ğ‘’</ci><ci id="S3.SS4.p1.1.m1.2.3.3.3.8.cmml" xref="S3.SS4.p1.1.m1.2.3.3.3.8">ğ‘†</ci><ci id="S3.SS4.p1.1.m1.2.3.3.3.9.cmml" xref="S3.SS4.p1.1.m1.2.3.3.3.9">ğ‘–</ci><ci id="S3.SS4.p1.1.m1.2.3.3.3.10.cmml" xref="S3.SS4.p1.1.m1.2.3.3.3.10">ğ‘š</ci><ci id="S3.SS4.p1.1.m1.2.3.3.3.11.cmml" xref="S3.SS4.p1.1.m1.2.3.3.3.11">ğ‘–</ci><ci id="S3.SS4.p1.1.m1.2.3.3.3.12.cmml" xref="S3.SS4.p1.1.m1.2.3.3.3.12">ğ‘™</ci><ci id="S3.SS4.p1.1.m1.2.3.3.3.13.cmml" xref="S3.SS4.p1.1.m1.2.3.3.3.13">ğ‘</ci><ci id="S3.SS4.p1.1.m1.2.3.3.3.14.cmml" xref="S3.SS4.p1.1.m1.2.3.3.3.14">ğ‘Ÿ</ci><ci id="S3.SS4.p1.1.m1.2.3.3.3.15.cmml" xref="S3.SS4.p1.1.m1.2.3.3.3.15">ğ‘–</ci><ci id="S3.SS4.p1.1.m1.2.3.3.3.16.cmml" xref="S3.SS4.p1.1.m1.2.3.3.3.16">ğ‘¡</ci><ci id="S3.SS4.p1.1.m1.2.3.3.3.17.cmml" xref="S3.SS4.p1.1.m1.2.3.3.3.17">ğ‘¦</ci><interval closure="open" id="S3.SS4.p1.1.m1.2.3.3.3.18.1.cmml" xref="S3.SS4.p1.1.m1.2.3.3.3.18.2"><ci id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">ğ‘”</ci><apply id="S3.SS4.p1.1.m1.2.2.cmml" xref="S3.SS4.p1.1.m1.2.2"><ci id="S3.SS4.p1.1.m1.2.2.1.cmml" xref="S3.SS4.p1.1.m1.2.2.1">^</ci><ci id="S3.SS4.p1.1.m1.2.2.2.cmml" xref="S3.SS4.p1.1.m1.2.2.2">ğ‘”</ci></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.2c">{\mathcal{L}_{gaze}=1-CosineSimilarity(g,\hat{g})}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.1.m1.2d">caligraphic_L start_POSTSUBSCRIPT italic_g italic_a italic_z italic_e end_POSTSUBSCRIPT = 1 - italic_C italic_o italic_s italic_i italic_n italic_e italic_S italic_i italic_m italic_i italic_l italic_a italic_r italic_i italic_t italic_y ( italic_g , over^ start_ARG italic_g end_ARG )</annotation></semantics></math> on the same train-valid-test split as the complete architecture.
The module weights are then loaded into the same module, but this time as part of the full pipeline. The latter is then trained in an end-to-end fashion and supervised on the gaze vector estimation with a cosine loss <math alttext="\mathcal{L}_{gaze}" class="ltx_Math" display="inline" id="S3.SS4.p1.2.m2.1"><semantics id="S3.SS4.p1.2.m2.1a"><msub id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p1.2.m2.1.1.2" xref="S3.SS4.p1.2.m2.1.1.2.cmml">â„’</mi><mrow id="S3.SS4.p1.2.m2.1.1.3" xref="S3.SS4.p1.2.m2.1.1.3.cmml"><mi id="S3.SS4.p1.2.m2.1.1.3.2" xref="S3.SS4.p1.2.m2.1.1.3.2.cmml">g</mi><mo id="S3.SS4.p1.2.m2.1.1.3.1" xref="S3.SS4.p1.2.m2.1.1.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.2.m2.1.1.3.3" xref="S3.SS4.p1.2.m2.1.1.3.3.cmml">a</mi><mo id="S3.SS4.p1.2.m2.1.1.3.1a" xref="S3.SS4.p1.2.m2.1.1.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.2.m2.1.1.3.4" xref="S3.SS4.p1.2.m2.1.1.3.4.cmml">z</mi><mo id="S3.SS4.p1.2.m2.1.1.3.1b" xref="S3.SS4.p1.2.m2.1.1.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.2.m2.1.1.3.5" xref="S3.SS4.p1.2.m2.1.1.3.5.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.1b"><apply id="S3.SS4.p1.2.m2.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.2.m2.1.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.p1.2.m2.1.1.2.cmml" xref="S3.SS4.p1.2.m2.1.1.2">â„’</ci><apply id="S3.SS4.p1.2.m2.1.1.3.cmml" xref="S3.SS4.p1.2.m2.1.1.3"><times id="S3.SS4.p1.2.m2.1.1.3.1.cmml" xref="S3.SS4.p1.2.m2.1.1.3.1"></times><ci id="S3.SS4.p1.2.m2.1.1.3.2.cmml" xref="S3.SS4.p1.2.m2.1.1.3.2">ğ‘”</ci><ci id="S3.SS4.p1.2.m2.1.1.3.3.cmml" xref="S3.SS4.p1.2.m2.1.1.3.3">ğ‘</ci><ci id="S3.SS4.p1.2.m2.1.1.3.4.cmml" xref="S3.SS4.p1.2.m2.1.1.3.4">ğ‘§</ci><ci id="S3.SS4.p1.2.m2.1.1.3.5.cmml" xref="S3.SS4.p1.2.m2.1.1.3.5">ğ‘’</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.1c">\mathcal{L}_{gaze}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.2.m2.1d">caligraphic_L start_POSTSUBSCRIPT italic_g italic_a italic_z italic_e end_POSTSUBSCRIPT</annotation></semantics></math> and on the 2D heatmap <math alttext="H" class="ltx_Math" display="inline" id="S3.SS4.p1.3.m3.1"><semantics id="S3.SS4.p1.3.m3.1a"><mi id="S3.SS4.p1.3.m3.1.1" xref="S3.SS4.p1.3.m3.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.3.m3.1b"><ci id="S3.SS4.p1.3.m3.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1">ğ»</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.3.m3.1c">H</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.3.m3.1d">italic_H</annotation></semantics></math> creation with an L2 loss <math alttext="\mathcal{L}_{heat}" class="ltx_Math" display="inline" id="S3.SS4.p1.4.m4.1"><semantics id="S3.SS4.p1.4.m4.1a"><msub id="S3.SS4.p1.4.m4.1.1" xref="S3.SS4.p1.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p1.4.m4.1.1.2" xref="S3.SS4.p1.4.m4.1.1.2.cmml">â„’</mi><mrow id="S3.SS4.p1.4.m4.1.1.3" xref="S3.SS4.p1.4.m4.1.1.3.cmml"><mi id="S3.SS4.p1.4.m4.1.1.3.2" xref="S3.SS4.p1.4.m4.1.1.3.2.cmml">h</mi><mo id="S3.SS4.p1.4.m4.1.1.3.1" xref="S3.SS4.p1.4.m4.1.1.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.4.m4.1.1.3.3" xref="S3.SS4.p1.4.m4.1.1.3.3.cmml">e</mi><mo id="S3.SS4.p1.4.m4.1.1.3.1a" xref="S3.SS4.p1.4.m4.1.1.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.4.m4.1.1.3.4" xref="S3.SS4.p1.4.m4.1.1.3.4.cmml">a</mi><mo id="S3.SS4.p1.4.m4.1.1.3.1b" xref="S3.SS4.p1.4.m4.1.1.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.4.m4.1.1.3.5" xref="S3.SS4.p1.4.m4.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.4.m4.1b"><apply id="S3.SS4.p1.4.m4.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.4.m4.1.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS4.p1.4.m4.1.1.2.cmml" xref="S3.SS4.p1.4.m4.1.1.2">â„’</ci><apply id="S3.SS4.p1.4.m4.1.1.3.cmml" xref="S3.SS4.p1.4.m4.1.1.3"><times id="S3.SS4.p1.4.m4.1.1.3.1.cmml" xref="S3.SS4.p1.4.m4.1.1.3.1"></times><ci id="S3.SS4.p1.4.m4.1.1.3.2.cmml" xref="S3.SS4.p1.4.m4.1.1.3.2">â„</ci><ci id="S3.SS4.p1.4.m4.1.1.3.3.cmml" xref="S3.SS4.p1.4.m4.1.1.3.3">ğ‘’</ci><ci id="S3.SS4.p1.4.m4.1.1.3.4.cmml" xref="S3.SS4.p1.4.m4.1.1.3.4">ğ‘</ci><ci id="S3.SS4.p1.4.m4.1.1.3.5.cmml" xref="S3.SS4.p1.4.m4.1.1.3.5">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.4.m4.1c">\mathcal{L}_{heat}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.4.m4.1d">caligraphic_L start_POSTSUBSCRIPT italic_h italic_e italic_a italic_t end_POSTSUBSCRIPT</annotation></semantics></math>. The total loss is then computed as <math alttext="\mathcal{L}=w_{heat}\mathcal{L}_{heat}+w_{gaze}\mathcal{L}_{gaze}" class="ltx_Math" display="inline" id="S3.SS4.p1.5.m5.1"><semantics id="S3.SS4.p1.5.m5.1a"><mrow id="S3.SS4.p1.5.m5.1.1" xref="S3.SS4.p1.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p1.5.m5.1.1.2" xref="S3.SS4.p1.5.m5.1.1.2.cmml">â„’</mi><mo id="S3.SS4.p1.5.m5.1.1.1" xref="S3.SS4.p1.5.m5.1.1.1.cmml">=</mo><mrow id="S3.SS4.p1.5.m5.1.1.3" xref="S3.SS4.p1.5.m5.1.1.3.cmml"><mrow id="S3.SS4.p1.5.m5.1.1.3.2" xref="S3.SS4.p1.5.m5.1.1.3.2.cmml"><msub id="S3.SS4.p1.5.m5.1.1.3.2.2" xref="S3.SS4.p1.5.m5.1.1.3.2.2.cmml"><mi id="S3.SS4.p1.5.m5.1.1.3.2.2.2" xref="S3.SS4.p1.5.m5.1.1.3.2.2.2.cmml">w</mi><mrow id="S3.SS4.p1.5.m5.1.1.3.2.2.3" xref="S3.SS4.p1.5.m5.1.1.3.2.2.3.cmml"><mi id="S3.SS4.p1.5.m5.1.1.3.2.2.3.2" xref="S3.SS4.p1.5.m5.1.1.3.2.2.3.2.cmml">h</mi><mo id="S3.SS4.p1.5.m5.1.1.3.2.2.3.1" xref="S3.SS4.p1.5.m5.1.1.3.2.2.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.5.m5.1.1.3.2.2.3.3" xref="S3.SS4.p1.5.m5.1.1.3.2.2.3.3.cmml">e</mi><mo id="S3.SS4.p1.5.m5.1.1.3.2.2.3.1a" xref="S3.SS4.p1.5.m5.1.1.3.2.2.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.5.m5.1.1.3.2.2.3.4" xref="S3.SS4.p1.5.m5.1.1.3.2.2.3.4.cmml">a</mi><mo id="S3.SS4.p1.5.m5.1.1.3.2.2.3.1b" xref="S3.SS4.p1.5.m5.1.1.3.2.2.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.5.m5.1.1.3.2.2.3.5" xref="S3.SS4.p1.5.m5.1.1.3.2.2.3.5.cmml">t</mi></mrow></msub><mo id="S3.SS4.p1.5.m5.1.1.3.2.1" xref="S3.SS4.p1.5.m5.1.1.3.2.1.cmml">â¢</mo><msub id="S3.SS4.p1.5.m5.1.1.3.2.3" xref="S3.SS4.p1.5.m5.1.1.3.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p1.5.m5.1.1.3.2.3.2" xref="S3.SS4.p1.5.m5.1.1.3.2.3.2.cmml">â„’</mi><mrow id="S3.SS4.p1.5.m5.1.1.3.2.3.3" xref="S3.SS4.p1.5.m5.1.1.3.2.3.3.cmml"><mi id="S3.SS4.p1.5.m5.1.1.3.2.3.3.2" xref="S3.SS4.p1.5.m5.1.1.3.2.3.3.2.cmml">h</mi><mo id="S3.SS4.p1.5.m5.1.1.3.2.3.3.1" xref="S3.SS4.p1.5.m5.1.1.3.2.3.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.5.m5.1.1.3.2.3.3.3" xref="S3.SS4.p1.5.m5.1.1.3.2.3.3.3.cmml">e</mi><mo id="S3.SS4.p1.5.m5.1.1.3.2.3.3.1a" xref="S3.SS4.p1.5.m5.1.1.3.2.3.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.5.m5.1.1.3.2.3.3.4" xref="S3.SS4.p1.5.m5.1.1.3.2.3.3.4.cmml">a</mi><mo id="S3.SS4.p1.5.m5.1.1.3.2.3.3.1b" xref="S3.SS4.p1.5.m5.1.1.3.2.3.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.5.m5.1.1.3.2.3.3.5" xref="S3.SS4.p1.5.m5.1.1.3.2.3.3.5.cmml">t</mi></mrow></msub></mrow><mo id="S3.SS4.p1.5.m5.1.1.3.1" xref="S3.SS4.p1.5.m5.1.1.3.1.cmml">+</mo><mrow id="S3.SS4.p1.5.m5.1.1.3.3" xref="S3.SS4.p1.5.m5.1.1.3.3.cmml"><msub id="S3.SS4.p1.5.m5.1.1.3.3.2" xref="S3.SS4.p1.5.m5.1.1.3.3.2.cmml"><mi id="S3.SS4.p1.5.m5.1.1.3.3.2.2" xref="S3.SS4.p1.5.m5.1.1.3.3.2.2.cmml">w</mi><mrow id="S3.SS4.p1.5.m5.1.1.3.3.2.3" xref="S3.SS4.p1.5.m5.1.1.3.3.2.3.cmml"><mi id="S3.SS4.p1.5.m5.1.1.3.3.2.3.2" xref="S3.SS4.p1.5.m5.1.1.3.3.2.3.2.cmml">g</mi><mo id="S3.SS4.p1.5.m5.1.1.3.3.2.3.1" xref="S3.SS4.p1.5.m5.1.1.3.3.2.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.5.m5.1.1.3.3.2.3.3" xref="S3.SS4.p1.5.m5.1.1.3.3.2.3.3.cmml">a</mi><mo id="S3.SS4.p1.5.m5.1.1.3.3.2.3.1a" xref="S3.SS4.p1.5.m5.1.1.3.3.2.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.5.m5.1.1.3.3.2.3.4" xref="S3.SS4.p1.5.m5.1.1.3.3.2.3.4.cmml">z</mi><mo id="S3.SS4.p1.5.m5.1.1.3.3.2.3.1b" xref="S3.SS4.p1.5.m5.1.1.3.3.2.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.5.m5.1.1.3.3.2.3.5" xref="S3.SS4.p1.5.m5.1.1.3.3.2.3.5.cmml">e</mi></mrow></msub><mo id="S3.SS4.p1.5.m5.1.1.3.3.1" xref="S3.SS4.p1.5.m5.1.1.3.3.1.cmml">â¢</mo><msub id="S3.SS4.p1.5.m5.1.1.3.3.3" xref="S3.SS4.p1.5.m5.1.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p1.5.m5.1.1.3.3.3.2" xref="S3.SS4.p1.5.m5.1.1.3.3.3.2.cmml">â„’</mi><mrow id="S3.SS4.p1.5.m5.1.1.3.3.3.3" xref="S3.SS4.p1.5.m5.1.1.3.3.3.3.cmml"><mi id="S3.SS4.p1.5.m5.1.1.3.3.3.3.2" xref="S3.SS4.p1.5.m5.1.1.3.3.3.3.2.cmml">g</mi><mo id="S3.SS4.p1.5.m5.1.1.3.3.3.3.1" xref="S3.SS4.p1.5.m5.1.1.3.3.3.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.5.m5.1.1.3.3.3.3.3" xref="S3.SS4.p1.5.m5.1.1.3.3.3.3.3.cmml">a</mi><mo id="S3.SS4.p1.5.m5.1.1.3.3.3.3.1a" xref="S3.SS4.p1.5.m5.1.1.3.3.3.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.5.m5.1.1.3.3.3.3.4" xref="S3.SS4.p1.5.m5.1.1.3.3.3.3.4.cmml">z</mi><mo id="S3.SS4.p1.5.m5.1.1.3.3.3.3.1b" xref="S3.SS4.p1.5.m5.1.1.3.3.3.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.5.m5.1.1.3.3.3.3.5" xref="S3.SS4.p1.5.m5.1.1.3.3.3.3.5.cmml">e</mi></mrow></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.5.m5.1b"><apply id="S3.SS4.p1.5.m5.1.1.cmml" xref="S3.SS4.p1.5.m5.1.1"><eq id="S3.SS4.p1.5.m5.1.1.1.cmml" xref="S3.SS4.p1.5.m5.1.1.1"></eq><ci id="S3.SS4.p1.5.m5.1.1.2.cmml" xref="S3.SS4.p1.5.m5.1.1.2">â„’</ci><apply id="S3.SS4.p1.5.m5.1.1.3.cmml" xref="S3.SS4.p1.5.m5.1.1.3"><plus id="S3.SS4.p1.5.m5.1.1.3.1.cmml" xref="S3.SS4.p1.5.m5.1.1.3.1"></plus><apply id="S3.SS4.p1.5.m5.1.1.3.2.cmml" xref="S3.SS4.p1.5.m5.1.1.3.2"><times id="S3.SS4.p1.5.m5.1.1.3.2.1.cmml" xref="S3.SS4.p1.5.m5.1.1.3.2.1"></times><apply id="S3.SS4.p1.5.m5.1.1.3.2.2.cmml" xref="S3.SS4.p1.5.m5.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.SS4.p1.5.m5.1.1.3.2.2.1.cmml" xref="S3.SS4.p1.5.m5.1.1.3.2.2">subscript</csymbol><ci id="S3.SS4.p1.5.m5.1.1.3.2.2.2.cmml" xref="S3.SS4.p1.5.m5.1.1.3.2.2.2">ğ‘¤</ci><apply id="S3.SS4.p1.5.m5.1.1.3.2.2.3.cmml" xref="S3.SS4.p1.5.m5.1.1.3.2.2.3"><times id="S3.SS4.p1.5.m5.1.1.3.2.2.3.1.cmml" xref="S3.SS4.p1.5.m5.1.1.3.2.2.3.1"></times><ci id="S3.SS4.p1.5.m5.1.1.3.2.2.3.2.cmml" xref="S3.SS4.p1.5.m5.1.1.3.2.2.3.2">â„</ci><ci id="S3.SS4.p1.5.m5.1.1.3.2.2.3.3.cmml" xref="S3.SS4.p1.5.m5.1.1.3.2.2.3.3">ğ‘’</ci><ci id="S3.SS4.p1.5.m5.1.1.3.2.2.3.4.cmml" xref="S3.SS4.p1.5.m5.1.1.3.2.2.3.4">ğ‘</ci><ci id="S3.SS4.p1.5.m5.1.1.3.2.2.3.5.cmml" xref="S3.SS4.p1.5.m5.1.1.3.2.2.3.5">ğ‘¡</ci></apply></apply><apply id="S3.SS4.p1.5.m5.1.1.3.2.3.cmml" xref="S3.SS4.p1.5.m5.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.SS4.p1.5.m5.1.1.3.2.3.1.cmml" xref="S3.SS4.p1.5.m5.1.1.3.2.3">subscript</csymbol><ci id="S3.SS4.p1.5.m5.1.1.3.2.3.2.cmml" xref="S3.SS4.p1.5.m5.1.1.3.2.3.2">â„’</ci><apply id="S3.SS4.p1.5.m5.1.1.3.2.3.3.cmml" xref="S3.SS4.p1.5.m5.1.1.3.2.3.3"><times id="S3.SS4.p1.5.m5.1.1.3.2.3.3.1.cmml" xref="S3.SS4.p1.5.m5.1.1.3.2.3.3.1"></times><ci id="S3.SS4.p1.5.m5.1.1.3.2.3.3.2.cmml" xref="S3.SS4.p1.5.m5.1.1.3.2.3.3.2">â„</ci><ci id="S3.SS4.p1.5.m5.1.1.3.2.3.3.3.cmml" xref="S3.SS4.p1.5.m5.1.1.3.2.3.3.3">ğ‘’</ci><ci id="S3.SS4.p1.5.m5.1.1.3.2.3.3.4.cmml" xref="S3.SS4.p1.5.m5.1.1.3.2.3.3.4">ğ‘</ci><ci id="S3.SS4.p1.5.m5.1.1.3.2.3.3.5.cmml" xref="S3.SS4.p1.5.m5.1.1.3.2.3.3.5">ğ‘¡</ci></apply></apply></apply><apply id="S3.SS4.p1.5.m5.1.1.3.3.cmml" xref="S3.SS4.p1.5.m5.1.1.3.3"><times id="S3.SS4.p1.5.m5.1.1.3.3.1.cmml" xref="S3.SS4.p1.5.m5.1.1.3.3.1"></times><apply id="S3.SS4.p1.5.m5.1.1.3.3.2.cmml" xref="S3.SS4.p1.5.m5.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS4.p1.5.m5.1.1.3.3.2.1.cmml" xref="S3.SS4.p1.5.m5.1.1.3.3.2">subscript</csymbol><ci id="S3.SS4.p1.5.m5.1.1.3.3.2.2.cmml" xref="S3.SS4.p1.5.m5.1.1.3.3.2.2">ğ‘¤</ci><apply id="S3.SS4.p1.5.m5.1.1.3.3.2.3.cmml" xref="S3.SS4.p1.5.m5.1.1.3.3.2.3"><times id="S3.SS4.p1.5.m5.1.1.3.3.2.3.1.cmml" xref="S3.SS4.p1.5.m5.1.1.3.3.2.3.1"></times><ci id="S3.SS4.p1.5.m5.1.1.3.3.2.3.2.cmml" xref="S3.SS4.p1.5.m5.1.1.3.3.2.3.2">ğ‘”</ci><ci id="S3.SS4.p1.5.m5.1.1.3.3.2.3.3.cmml" xref="S3.SS4.p1.5.m5.1.1.3.3.2.3.3">ğ‘</ci><ci id="S3.SS4.p1.5.m5.1.1.3.3.2.3.4.cmml" xref="S3.SS4.p1.5.m5.1.1.3.3.2.3.4">ğ‘§</ci><ci id="S3.SS4.p1.5.m5.1.1.3.3.2.3.5.cmml" xref="S3.SS4.p1.5.m5.1.1.3.3.2.3.5">ğ‘’</ci></apply></apply><apply id="S3.SS4.p1.5.m5.1.1.3.3.3.cmml" xref="S3.SS4.p1.5.m5.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS4.p1.5.m5.1.1.3.3.3.1.cmml" xref="S3.SS4.p1.5.m5.1.1.3.3.3">subscript</csymbol><ci id="S3.SS4.p1.5.m5.1.1.3.3.3.2.cmml" xref="S3.SS4.p1.5.m5.1.1.3.3.3.2">â„’</ci><apply id="S3.SS4.p1.5.m5.1.1.3.3.3.3.cmml" xref="S3.SS4.p1.5.m5.1.1.3.3.3.3"><times id="S3.SS4.p1.5.m5.1.1.3.3.3.3.1.cmml" xref="S3.SS4.p1.5.m5.1.1.3.3.3.3.1"></times><ci id="S3.SS4.p1.5.m5.1.1.3.3.3.3.2.cmml" xref="S3.SS4.p1.5.m5.1.1.3.3.3.3.2">ğ‘”</ci><ci id="S3.SS4.p1.5.m5.1.1.3.3.3.3.3.cmml" xref="S3.SS4.p1.5.m5.1.1.3.3.3.3.3">ğ‘</ci><ci id="S3.SS4.p1.5.m5.1.1.3.3.3.3.4.cmml" xref="S3.SS4.p1.5.m5.1.1.3.3.3.3.4">ğ‘§</ci><ci id="S3.SS4.p1.5.m5.1.1.3.3.3.3.5.cmml" xref="S3.SS4.p1.5.m5.1.1.3.3.3.3.5">ğ‘’</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.5.m5.1c">\mathcal{L}=w_{heat}\mathcal{L}_{heat}+w_{gaze}\mathcal{L}_{gaze}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.5.m5.1d">caligraphic_L = italic_w start_POSTSUBSCRIPT italic_h italic_e italic_a italic_t end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_h italic_e italic_a italic_t end_POSTSUBSCRIPT + italic_w start_POSTSUBSCRIPT italic_g italic_a italic_z italic_e end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_g italic_a italic_z italic_e end_POSTSUBSCRIPT</annotation></semantics></math>, where <math alttext="w_{heat}" class="ltx_Math" display="inline" id="S3.SS4.p1.6.m6.1"><semantics id="S3.SS4.p1.6.m6.1a"><msub id="S3.SS4.p1.6.m6.1.1" xref="S3.SS4.p1.6.m6.1.1.cmml"><mi id="S3.SS4.p1.6.m6.1.1.2" xref="S3.SS4.p1.6.m6.1.1.2.cmml">w</mi><mrow id="S3.SS4.p1.6.m6.1.1.3" xref="S3.SS4.p1.6.m6.1.1.3.cmml"><mi id="S3.SS4.p1.6.m6.1.1.3.2" xref="S3.SS4.p1.6.m6.1.1.3.2.cmml">h</mi><mo id="S3.SS4.p1.6.m6.1.1.3.1" xref="S3.SS4.p1.6.m6.1.1.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.6.m6.1.1.3.3" xref="S3.SS4.p1.6.m6.1.1.3.3.cmml">e</mi><mo id="S3.SS4.p1.6.m6.1.1.3.1a" xref="S3.SS4.p1.6.m6.1.1.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.6.m6.1.1.3.4" xref="S3.SS4.p1.6.m6.1.1.3.4.cmml">a</mi><mo id="S3.SS4.p1.6.m6.1.1.3.1b" xref="S3.SS4.p1.6.m6.1.1.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.6.m6.1.1.3.5" xref="S3.SS4.p1.6.m6.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.6.m6.1b"><apply id="S3.SS4.p1.6.m6.1.1.cmml" xref="S3.SS4.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.6.m6.1.1.1.cmml" xref="S3.SS4.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS4.p1.6.m6.1.1.2.cmml" xref="S3.SS4.p1.6.m6.1.1.2">ğ‘¤</ci><apply id="S3.SS4.p1.6.m6.1.1.3.cmml" xref="S3.SS4.p1.6.m6.1.1.3"><times id="S3.SS4.p1.6.m6.1.1.3.1.cmml" xref="S3.SS4.p1.6.m6.1.1.3.1"></times><ci id="S3.SS4.p1.6.m6.1.1.3.2.cmml" xref="S3.SS4.p1.6.m6.1.1.3.2">â„</ci><ci id="S3.SS4.p1.6.m6.1.1.3.3.cmml" xref="S3.SS4.p1.6.m6.1.1.3.3">ğ‘’</ci><ci id="S3.SS4.p1.6.m6.1.1.3.4.cmml" xref="S3.SS4.p1.6.m6.1.1.3.4">ğ‘</ci><ci id="S3.SS4.p1.6.m6.1.1.3.5.cmml" xref="S3.SS4.p1.6.m6.1.1.3.5">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.6.m6.1c">w_{heat}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.6.m6.1d">italic_w start_POSTSUBSCRIPT italic_h italic_e italic_a italic_t end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="w_{gaze}" class="ltx_Math" display="inline" id="S3.SS4.p1.7.m7.1"><semantics id="S3.SS4.p1.7.m7.1a"><msub id="S3.SS4.p1.7.m7.1.1" xref="S3.SS4.p1.7.m7.1.1.cmml"><mi id="S3.SS4.p1.7.m7.1.1.2" xref="S3.SS4.p1.7.m7.1.1.2.cmml">w</mi><mrow id="S3.SS4.p1.7.m7.1.1.3" xref="S3.SS4.p1.7.m7.1.1.3.cmml"><mi id="S3.SS4.p1.7.m7.1.1.3.2" xref="S3.SS4.p1.7.m7.1.1.3.2.cmml">g</mi><mo id="S3.SS4.p1.7.m7.1.1.3.1" xref="S3.SS4.p1.7.m7.1.1.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.7.m7.1.1.3.3" xref="S3.SS4.p1.7.m7.1.1.3.3.cmml">a</mi><mo id="S3.SS4.p1.7.m7.1.1.3.1a" xref="S3.SS4.p1.7.m7.1.1.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.7.m7.1.1.3.4" xref="S3.SS4.p1.7.m7.1.1.3.4.cmml">z</mi><mo id="S3.SS4.p1.7.m7.1.1.3.1b" xref="S3.SS4.p1.7.m7.1.1.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.7.m7.1.1.3.5" xref="S3.SS4.p1.7.m7.1.1.3.5.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.7.m7.1b"><apply id="S3.SS4.p1.7.m7.1.1.cmml" xref="S3.SS4.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.7.m7.1.1.1.cmml" xref="S3.SS4.p1.7.m7.1.1">subscript</csymbol><ci id="S3.SS4.p1.7.m7.1.1.2.cmml" xref="S3.SS4.p1.7.m7.1.1.2">ğ‘¤</ci><apply id="S3.SS4.p1.7.m7.1.1.3.cmml" xref="S3.SS4.p1.7.m7.1.1.3"><times id="S3.SS4.p1.7.m7.1.1.3.1.cmml" xref="S3.SS4.p1.7.m7.1.1.3.1"></times><ci id="S3.SS4.p1.7.m7.1.1.3.2.cmml" xref="S3.SS4.p1.7.m7.1.1.3.2">ğ‘”</ci><ci id="S3.SS4.p1.7.m7.1.1.3.3.cmml" xref="S3.SS4.p1.7.m7.1.1.3.3">ğ‘</ci><ci id="S3.SS4.p1.7.m7.1.1.3.4.cmml" xref="S3.SS4.p1.7.m7.1.1.3.4">ğ‘§</ci><ci id="S3.SS4.p1.7.m7.1.1.3.5.cmml" xref="S3.SS4.p1.7.m7.1.1.3.5">ğ‘’</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.7.m7.1c">w_{gaze}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.7.m7.1d">italic_w start_POSTSUBSCRIPT italic_g italic_a italic_z italic_e end_POSTSUBSCRIPT</annotation></semantics></math> are two loss-specific weights defined as hyperparameters.
Specifically, <math alttext="w_{heat}" class="ltx_Math" display="inline" id="S3.SS4.p1.8.m8.1"><semantics id="S3.SS4.p1.8.m8.1a"><msub id="S3.SS4.p1.8.m8.1.1" xref="S3.SS4.p1.8.m8.1.1.cmml"><mi id="S3.SS4.p1.8.m8.1.1.2" xref="S3.SS4.p1.8.m8.1.1.2.cmml">w</mi><mrow id="S3.SS4.p1.8.m8.1.1.3" xref="S3.SS4.p1.8.m8.1.1.3.cmml"><mi id="S3.SS4.p1.8.m8.1.1.3.2" xref="S3.SS4.p1.8.m8.1.1.3.2.cmml">h</mi><mo id="S3.SS4.p1.8.m8.1.1.3.1" xref="S3.SS4.p1.8.m8.1.1.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.8.m8.1.1.3.3" xref="S3.SS4.p1.8.m8.1.1.3.3.cmml">e</mi><mo id="S3.SS4.p1.8.m8.1.1.3.1a" xref="S3.SS4.p1.8.m8.1.1.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.8.m8.1.1.3.4" xref="S3.SS4.p1.8.m8.1.1.3.4.cmml">a</mi><mo id="S3.SS4.p1.8.m8.1.1.3.1b" xref="S3.SS4.p1.8.m8.1.1.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.8.m8.1.1.3.5" xref="S3.SS4.p1.8.m8.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.8.m8.1b"><apply id="S3.SS4.p1.8.m8.1.1.cmml" xref="S3.SS4.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.8.m8.1.1.1.cmml" xref="S3.SS4.p1.8.m8.1.1">subscript</csymbol><ci id="S3.SS4.p1.8.m8.1.1.2.cmml" xref="S3.SS4.p1.8.m8.1.1.2">ğ‘¤</ci><apply id="S3.SS4.p1.8.m8.1.1.3.cmml" xref="S3.SS4.p1.8.m8.1.1.3"><times id="S3.SS4.p1.8.m8.1.1.3.1.cmml" xref="S3.SS4.p1.8.m8.1.1.3.1"></times><ci id="S3.SS4.p1.8.m8.1.1.3.2.cmml" xref="S3.SS4.p1.8.m8.1.1.3.2">â„</ci><ci id="S3.SS4.p1.8.m8.1.1.3.3.cmml" xref="S3.SS4.p1.8.m8.1.1.3.3">ğ‘’</ci><ci id="S3.SS4.p1.8.m8.1.1.3.4.cmml" xref="S3.SS4.p1.8.m8.1.1.3.4">ğ‘</ci><ci id="S3.SS4.p1.8.m8.1.1.3.5.cmml" xref="S3.SS4.p1.8.m8.1.1.3.5">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.8.m8.1c">w_{heat}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.8.m8.1d">italic_w start_POSTSUBSCRIPT italic_h italic_e italic_a italic_t end_POSTSUBSCRIPT</annotation></semantics></math> has a value of 10000, and <math alttext="w_{gaze}" class="ltx_Math" display="inline" id="S3.SS4.p1.9.m9.1"><semantics id="S3.SS4.p1.9.m9.1a"><msub id="S3.SS4.p1.9.m9.1.1" xref="S3.SS4.p1.9.m9.1.1.cmml"><mi id="S3.SS4.p1.9.m9.1.1.2" xref="S3.SS4.p1.9.m9.1.1.2.cmml">w</mi><mrow id="S3.SS4.p1.9.m9.1.1.3" xref="S3.SS4.p1.9.m9.1.1.3.cmml"><mi id="S3.SS4.p1.9.m9.1.1.3.2" xref="S3.SS4.p1.9.m9.1.1.3.2.cmml">g</mi><mo id="S3.SS4.p1.9.m9.1.1.3.1" xref="S3.SS4.p1.9.m9.1.1.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.9.m9.1.1.3.3" xref="S3.SS4.p1.9.m9.1.1.3.3.cmml">a</mi><mo id="S3.SS4.p1.9.m9.1.1.3.1a" xref="S3.SS4.p1.9.m9.1.1.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.9.m9.1.1.3.4" xref="S3.SS4.p1.9.m9.1.1.3.4.cmml">z</mi><mo id="S3.SS4.p1.9.m9.1.1.3.1b" xref="S3.SS4.p1.9.m9.1.1.3.1.cmml">â¢</mo><mi id="S3.SS4.p1.9.m9.1.1.3.5" xref="S3.SS4.p1.9.m9.1.1.3.5.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.9.m9.1b"><apply id="S3.SS4.p1.9.m9.1.1.cmml" xref="S3.SS4.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.9.m9.1.1.1.cmml" xref="S3.SS4.p1.9.m9.1.1">subscript</csymbol><ci id="S3.SS4.p1.9.m9.1.1.2.cmml" xref="S3.SS4.p1.9.m9.1.1.2">ğ‘¤</ci><apply id="S3.SS4.p1.9.m9.1.1.3.cmml" xref="S3.SS4.p1.9.m9.1.1.3"><times id="S3.SS4.p1.9.m9.1.1.3.1.cmml" xref="S3.SS4.p1.9.m9.1.1.3.1"></times><ci id="S3.SS4.p1.9.m9.1.1.3.2.cmml" xref="S3.SS4.p1.9.m9.1.1.3.2">ğ‘”</ci><ci id="S3.SS4.p1.9.m9.1.1.3.3.cmml" xref="S3.SS4.p1.9.m9.1.1.3.3">ğ‘</ci><ci id="S3.SS4.p1.9.m9.1.1.3.4.cmml" xref="S3.SS4.p1.9.m9.1.1.3.4">ğ‘§</ci><ci id="S3.SS4.p1.9.m9.1.1.3.5.cmml" xref="S3.SS4.p1.9.m9.1.1.3.5">ğ‘’</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.9.m9.1c">w_{gaze}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.9.m9.1d">italic_w start_POSTSUBSCRIPT italic_g italic_a italic_z italic_e end_POSTSUBSCRIPT</annotation></semantics></math> has a value of 10, similar to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib25" title="">25</a>]</cite>â€™s implementation.
The second training approach, the so-called end-to-end, involves training the entire gaze target detection pipeline, including the gaze estimation module, in a unified manner. This method does not rely on any pre-trained gaze estimation module from either a different dataset or the training split of the dataset used.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Implementation Details</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">The proposed approach modules are implemented in PyTorch and trained on a machine equipped with an Intel-i9 CPU and a single NVIDIA RTX 4090.

<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS5.p2">
<p class="ltx_p" id="S3.SS5.p2.5"><span class="ltx_text ltx_font_bold" id="S3.SS5.p2.5.1">Gaze estimation module.</span> The module that manages the estimation of the gaze direction processes depth maps resized to <math alttext="224\times 224" class="ltx_Math" display="inline" id="S3.SS5.p2.1.m1.1"><semantics id="S3.SS5.p2.1.m1.1a"><mrow id="S3.SS5.p2.1.m1.1.1" xref="S3.SS5.p2.1.m1.1.1.cmml"><mn id="S3.SS5.p2.1.m1.1.1.2" xref="S3.SS5.p2.1.m1.1.1.2.cmml">224</mn><mo id="S3.SS5.p2.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS5.p2.1.m1.1.1.1.cmml">Ã—</mo><mn id="S3.SS5.p2.1.m1.1.1.3" xref="S3.SS5.p2.1.m1.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.1.m1.1b"><apply id="S3.SS5.p2.1.m1.1.1.cmml" xref="S3.SS5.p2.1.m1.1.1"><times id="S3.SS5.p2.1.m1.1.1.1.cmml" xref="S3.SS5.p2.1.m1.1.1.1"></times><cn id="S3.SS5.p2.1.m1.1.1.2.cmml" type="integer" xref="S3.SS5.p2.1.m1.1.1.2">224</cn><cn id="S3.SS5.p2.1.m1.1.1.3.cmml" type="integer" xref="S3.SS5.p2.1.m1.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.1.m1.1c">224\times 224</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p2.1.m1.1d">224 Ã— 224</annotation></semantics></math> pixels using a ResNet50<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib22" title="">22</a>]</cite> and three linear layers with ReLU and Dropout. A three-layer Multi-Layer Perceptron (MLP) with ReLU activation function handles the <math alttext="13\times 2" class="ltx_Math" display="inline" id="S3.SS5.p2.2.m2.1"><semantics id="S3.SS5.p2.2.m2.1a"><mrow id="S3.SS5.p2.2.m2.1.1" xref="S3.SS5.p2.2.m2.1.1.cmml"><mn id="S3.SS5.p2.2.m2.1.1.2" xref="S3.SS5.p2.2.m2.1.1.2.cmml">13</mn><mo id="S3.SS5.p2.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS5.p2.2.m2.1.1.1.cmml">Ã—</mo><mn id="S3.SS5.p2.2.m2.1.1.3" xref="S3.SS5.p2.2.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.2.m2.1b"><apply id="S3.SS5.p2.2.m2.1.1.cmml" xref="S3.SS5.p2.2.m2.1.1"><times id="S3.SS5.p2.2.m2.1.1.1.cmml" xref="S3.SS5.p2.2.m2.1.1.1"></times><cn id="S3.SS5.p2.2.m2.1.1.2.cmml" type="integer" xref="S3.SS5.p2.2.m2.1.1.2">13</cn><cn id="S3.SS5.p2.2.m2.1.1.3.cmml" type="integer" xref="S3.SS5.p2.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.2.m2.1c">13\times 2</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p2.2.m2.1d">13 Ã— 2</annotation></semantics></math> upper-body pose data estimated from a pre-trained HRFormerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib47" title="">47</a>]</cite> and normalized on the neck position and the distance between the neck and hip joints.
The attention layer is implemented as a Transformer Encoder Layer with four heads and a feature size of <math alttext="256" class="ltx_Math" display="inline" id="S3.SS5.p2.3.m3.1"><semantics id="S3.SS5.p2.3.m3.1a"><mn id="S3.SS5.p2.3.m3.1.1" xref="S3.SS5.p2.3.m3.1.1.cmml">256</mn><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.3.m3.1b"><cn id="S3.SS5.p2.3.m3.1.1.cmml" type="integer" xref="S3.SS5.p2.3.m3.1.1">256</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.3.m3.1c">256</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p2.3.m3.1d">256</annotation></semantics></math>.
In the multi-stage training regime, the gaze estimation module is trained standalone with a batch size of 128 for 30 epochs, the AdamÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib29" title="">29</a>]</cite> optimizer and a learning rate and weight decay of <math alttext="1e-4" class="ltx_Math" display="inline" id="S3.SS5.p2.4.m4.1"><semantics id="S3.SS5.p2.4.m4.1a"><mrow id="S3.SS5.p2.4.m4.1.1" xref="S3.SS5.p2.4.m4.1.1.cmml"><mrow id="S3.SS5.p2.4.m4.1.1.2" xref="S3.SS5.p2.4.m4.1.1.2.cmml"><mn id="S3.SS5.p2.4.m4.1.1.2.2" xref="S3.SS5.p2.4.m4.1.1.2.2.cmml">1</mn><mo id="S3.SS5.p2.4.m4.1.1.2.1" xref="S3.SS5.p2.4.m4.1.1.2.1.cmml">â¢</mo><mi id="S3.SS5.p2.4.m4.1.1.2.3" xref="S3.SS5.p2.4.m4.1.1.2.3.cmml">e</mi></mrow><mo id="S3.SS5.p2.4.m4.1.1.1" xref="S3.SS5.p2.4.m4.1.1.1.cmml">âˆ’</mo><mn id="S3.SS5.p2.4.m4.1.1.3" xref="S3.SS5.p2.4.m4.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.4.m4.1b"><apply id="S3.SS5.p2.4.m4.1.1.cmml" xref="S3.SS5.p2.4.m4.1.1"><minus id="S3.SS5.p2.4.m4.1.1.1.cmml" xref="S3.SS5.p2.4.m4.1.1.1"></minus><apply id="S3.SS5.p2.4.m4.1.1.2.cmml" xref="S3.SS5.p2.4.m4.1.1.2"><times id="S3.SS5.p2.4.m4.1.1.2.1.cmml" xref="S3.SS5.p2.4.m4.1.1.2.1"></times><cn id="S3.SS5.p2.4.m4.1.1.2.2.cmml" type="integer" xref="S3.SS5.p2.4.m4.1.1.2.2">1</cn><ci id="S3.SS5.p2.4.m4.1.1.2.3.cmml" xref="S3.SS5.p2.4.m4.1.1.2.3">ğ‘’</ci></apply><cn id="S3.SS5.p2.4.m4.1.1.3.cmml" type="integer" xref="S3.SS5.p2.4.m4.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.4.m4.1c">1e-4</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p2.4.m4.1d">1 italic_e - 4</annotation></semantics></math>.
Given that prior work employs auxiliary large gaze estimation datasets for pre-training their gaze estimation module, we take a different approach for our multi-stage training.
Instead of using an additional dataset, we utilize the weights obtained from training our gaze estimation module solely on the training set of the GFIE datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib25" title="">25</a>]</cite> with the gaze loss <math alttext="\mathcal{L}_{gaze}" class="ltx_Math" display="inline" id="S3.SS5.p2.5.m5.1"><semantics id="S3.SS5.p2.5.m5.1a"><msub id="S3.SS5.p2.5.m5.1.1" xref="S3.SS5.p2.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS5.p2.5.m5.1.1.2" xref="S3.SS5.p2.5.m5.1.1.2.cmml">â„’</mi><mrow id="S3.SS5.p2.5.m5.1.1.3" xref="S3.SS5.p2.5.m5.1.1.3.cmml"><mi id="S3.SS5.p2.5.m5.1.1.3.2" xref="S3.SS5.p2.5.m5.1.1.3.2.cmml">g</mi><mo id="S3.SS5.p2.5.m5.1.1.3.1" xref="S3.SS5.p2.5.m5.1.1.3.1.cmml">â¢</mo><mi id="S3.SS5.p2.5.m5.1.1.3.3" xref="S3.SS5.p2.5.m5.1.1.3.3.cmml">a</mi><mo id="S3.SS5.p2.5.m5.1.1.3.1a" xref="S3.SS5.p2.5.m5.1.1.3.1.cmml">â¢</mo><mi id="S3.SS5.p2.5.m5.1.1.3.4" xref="S3.SS5.p2.5.m5.1.1.3.4.cmml">z</mi><mo id="S3.SS5.p2.5.m5.1.1.3.1b" xref="S3.SS5.p2.5.m5.1.1.3.1.cmml">â¢</mo><mi id="S3.SS5.p2.5.m5.1.1.3.5" xref="S3.SS5.p2.5.m5.1.1.3.5.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.5.m5.1b"><apply id="S3.SS5.p2.5.m5.1.1.cmml" xref="S3.SS5.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.5.m5.1.1.1.cmml" xref="S3.SS5.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS5.p2.5.m5.1.1.2.cmml" xref="S3.SS5.p2.5.m5.1.1.2">â„’</ci><apply id="S3.SS5.p2.5.m5.1.1.3.cmml" xref="S3.SS5.p2.5.m5.1.1.3"><times id="S3.SS5.p2.5.m5.1.1.3.1.cmml" xref="S3.SS5.p2.5.m5.1.1.3.1"></times><ci id="S3.SS5.p2.5.m5.1.1.3.2.cmml" xref="S3.SS5.p2.5.m5.1.1.3.2">ğ‘”</ci><ci id="S3.SS5.p2.5.m5.1.1.3.3.cmml" xref="S3.SS5.p2.5.m5.1.1.3.3">ğ‘</ci><ci id="S3.SS5.p2.5.m5.1.1.3.4.cmml" xref="S3.SS5.p2.5.m5.1.1.3.4">ğ‘§</ci><ci id="S3.SS5.p2.5.m5.1.1.3.5.cmml" xref="S3.SS5.p2.5.m5.1.1.3.5">ğ‘’</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.5.m5.1c">\mathcal{L}_{gaze}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p2.5.m5.1d">caligraphic_L start_POSTSUBSCRIPT italic_g italic_a italic_z italic_e end_POSTSUBSCRIPT</annotation></semantics></math> (corresponding results are given in SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S4.SS3" title="4.3 Results of the Gaze Estimation Module â€£ 4 Experiments â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_tag">4.3</span></a>) to initialize the module within the complete gaze target detection pipeline.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS5.p3">
<p class="ltx_p" id="S3.SS5.p3.7"><span class="ltx_text ltx_font_bold" id="S3.SS5.p3.7.1">Gaze target detection pipeline.</span> The full pipeline is trained for 50 epochs, with the AdamÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib29" title="">29</a>]</cite> optimizer, a batch size of 32, and a learning rate and weight decay of <math alttext="1e-4" class="ltx_Math" display="inline" id="S3.SS5.p3.1.m1.1"><semantics id="S3.SS5.p3.1.m1.1a"><mrow id="S3.SS5.p3.1.m1.1.1" xref="S3.SS5.p3.1.m1.1.1.cmml"><mrow id="S3.SS5.p3.1.m1.1.1.2" xref="S3.SS5.p3.1.m1.1.1.2.cmml"><mn id="S3.SS5.p3.1.m1.1.1.2.2" xref="S3.SS5.p3.1.m1.1.1.2.2.cmml">1</mn><mo id="S3.SS5.p3.1.m1.1.1.2.1" xref="S3.SS5.p3.1.m1.1.1.2.1.cmml">â¢</mo><mi id="S3.SS5.p3.1.m1.1.1.2.3" xref="S3.SS5.p3.1.m1.1.1.2.3.cmml">e</mi></mrow><mo id="S3.SS5.p3.1.m1.1.1.1" xref="S3.SS5.p3.1.m1.1.1.1.cmml">âˆ’</mo><mn id="S3.SS5.p3.1.m1.1.1.3" xref="S3.SS5.p3.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.1.m1.1b"><apply id="S3.SS5.p3.1.m1.1.1.cmml" xref="S3.SS5.p3.1.m1.1.1"><minus id="S3.SS5.p3.1.m1.1.1.1.cmml" xref="S3.SS5.p3.1.m1.1.1.1"></minus><apply id="S3.SS5.p3.1.m1.1.1.2.cmml" xref="S3.SS5.p3.1.m1.1.1.2"><times id="S3.SS5.p3.1.m1.1.1.2.1.cmml" xref="S3.SS5.p3.1.m1.1.1.2.1"></times><cn id="S3.SS5.p3.1.m1.1.1.2.2.cmml" type="integer" xref="S3.SS5.p3.1.m1.1.1.2.2">1</cn><ci id="S3.SS5.p3.1.m1.1.1.2.3.cmml" xref="S3.SS5.p3.1.m1.1.1.2.3">ğ‘’</ci></apply><cn id="S3.SS5.p3.1.m1.1.1.3.cmml" type="integer" xref="S3.SS5.p3.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.1.m1.1c">1e-4</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p3.1.m1.1d">1 italic_e - 4</annotation></semantics></math>.
The images and depth maps are resized to <math alttext="224\times 224" class="ltx_Math" display="inline" id="S3.SS5.p3.2.m2.1"><semantics id="S3.SS5.p3.2.m2.1a"><mrow id="S3.SS5.p3.2.m2.1.1" xref="S3.SS5.p3.2.m2.1.1.cmml"><mn id="S3.SS5.p3.2.m2.1.1.2" xref="S3.SS5.p3.2.m2.1.1.2.cmml">224</mn><mo id="S3.SS5.p3.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS5.p3.2.m2.1.1.1.cmml">Ã—</mo><mn id="S3.SS5.p3.2.m2.1.1.3" xref="S3.SS5.p3.2.m2.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.2.m2.1b"><apply id="S3.SS5.p3.2.m2.1.1.cmml" xref="S3.SS5.p3.2.m2.1.1"><times id="S3.SS5.p3.2.m2.1.1.1.cmml" xref="S3.SS5.p3.2.m2.1.1.1"></times><cn id="S3.SS5.p3.2.m2.1.1.2.cmml" type="integer" xref="S3.SS5.p3.2.m2.1.1.2">224</cn><cn id="S3.SS5.p3.2.m2.1.1.3.cmml" type="integer" xref="S3.SS5.p3.2.m2.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.2.m2.1c">224\times 224</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p3.2.m2.1d">224 Ã— 224</annotation></semantics></math> pixels.
Random crop, flip, saturation, brightness and contrast adjustments have been applied as augmentations.
The predicted perception heatmaps <math alttext="V" class="ltx_Math" display="inline" id="S3.SS5.p3.3.m3.1"><semantics id="S3.SS5.p3.3.m3.1a"><mi id="S3.SS5.p3.3.m3.1.1" xref="S3.SS5.p3.3.m3.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.3.m3.1b"><ci id="S3.SS5.p3.3.m3.1.1.cmml" xref="S3.SS5.p3.3.m3.1.1">ğ‘‰</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.3.m3.1c">V</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p3.3.m3.1d">italic_V</annotation></semantics></math> and <math alttext="\hat{V}" class="ltx_Math" display="inline" id="S3.SS5.p3.4.m4.1"><semantics id="S3.SS5.p3.4.m4.1a"><mover accent="true" id="S3.SS5.p3.4.m4.1.1" xref="S3.SS5.p3.4.m4.1.1.cmml"><mi id="S3.SS5.p3.4.m4.1.1.2" xref="S3.SS5.p3.4.m4.1.1.2.cmml">V</mi><mo id="S3.SS5.p3.4.m4.1.1.1" xref="S3.SS5.p3.4.m4.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.4.m4.1b"><apply id="S3.SS5.p3.4.m4.1.1.cmml" xref="S3.SS5.p3.4.m4.1.1"><ci id="S3.SS5.p3.4.m4.1.1.1.cmml" xref="S3.SS5.p3.4.m4.1.1.1">^</ci><ci id="S3.SS5.p3.4.m4.1.1.2.cmml" xref="S3.SS5.p3.4.m4.1.1.2">ğ‘‰</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.4.m4.1c">\hat{V}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p3.4.m4.1d">over^ start_ARG italic_V end_ARG</annotation></semantics></math> have a size of <math alttext="224\times 224" class="ltx_Math" display="inline" id="S3.SS5.p3.5.m5.1"><semantics id="S3.SS5.p3.5.m5.1a"><mrow id="S3.SS5.p3.5.m5.1.1" xref="S3.SS5.p3.5.m5.1.1.cmml"><mn id="S3.SS5.p3.5.m5.1.1.2" xref="S3.SS5.p3.5.m5.1.1.2.cmml">224</mn><mo id="S3.SS5.p3.5.m5.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS5.p3.5.m5.1.1.1.cmml">Ã—</mo><mn id="S3.SS5.p3.5.m5.1.1.3" xref="S3.SS5.p3.5.m5.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.5.m5.1b"><apply id="S3.SS5.p3.5.m5.1.1.cmml" xref="S3.SS5.p3.5.m5.1.1"><times id="S3.SS5.p3.5.m5.1.1.1.cmml" xref="S3.SS5.p3.5.m5.1.1.1"></times><cn id="S3.SS5.p3.5.m5.1.1.2.cmml" type="integer" xref="S3.SS5.p3.5.m5.1.1.2">224</cn><cn id="S3.SS5.p3.5.m5.1.1.3.cmml" type="integer" xref="S3.SS5.p3.5.m5.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.5.m5.1c">224\times 224</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p3.5.m5.1d">224 Ã— 224</annotation></semantics></math> pixels, while the final 2D target detection heatmap <math alttext="H" class="ltx_Math" display="inline" id="S3.SS5.p3.6.m6.1"><semantics id="S3.SS5.p3.6.m6.1a"><mi id="S3.SS5.p3.6.m6.1.1" xref="S3.SS5.p3.6.m6.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.6.m6.1b"><ci id="S3.SS5.p3.6.m6.1.1.cmml" xref="S3.SS5.p3.6.m6.1.1">ğ»</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.6.m6.1c">H</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p3.6.m6.1d">italic_H</annotation></semantics></math> has a size of <math alttext="64\times 64" class="ltx_Math" display="inline" id="S3.SS5.p3.7.m7.1"><semantics id="S3.SS5.p3.7.m7.1a"><mrow id="S3.SS5.p3.7.m7.1.1" xref="S3.SS5.p3.7.m7.1.1.cmml"><mn id="S3.SS5.p3.7.m7.1.1.2" xref="S3.SS5.p3.7.m7.1.1.2.cmml">64</mn><mo id="S3.SS5.p3.7.m7.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS5.p3.7.m7.1.1.1.cmml">Ã—</mo><mn id="S3.SS5.p3.7.m7.1.1.3" xref="S3.SS5.p3.7.m7.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.7.m7.1b"><apply id="S3.SS5.p3.7.m7.1.1.cmml" xref="S3.SS5.p3.7.m7.1.1"><times id="S3.SS5.p3.7.m7.1.1.1.cmml" xref="S3.SS5.p3.7.m7.1.1.1"></times><cn id="S3.SS5.p3.7.m7.1.1.2.cmml" type="integer" xref="S3.SS5.p3.7.m7.1.1.2">64</cn><cn id="S3.SS5.p3.7.m7.1.1.3.cmml" type="integer" xref="S3.SS5.p3.7.m7.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.7.m7.1c">64\times 64</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p3.7.m7.1d">64 Ã— 64</annotation></semantics></math> pixels. The ground truth heatmap is generated by a Gaussian centered in the gaze targetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib39" title="">39</a>]</cite>. The encoder is implemented as a ResNet50Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib22" title="">22</a>]</cite>, pre-trained on ImagenetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib14" title="">14</a>]</cite>, while the decoder consists of two convolutional layers and three deconvolutional layers.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we describe the dataset used (SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S4.SS1" title="4.1 Dataset â€£ 4 Experiments â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_tag">4.1</span></a>) for our study, detailing the evaluation setup along with the 2D and 3D metrics used for gaze estimation and gaze target detection (SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S4.SS2" title="4.2 Evaluation Metrics â€£ 4 Experiments â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_tag">4.2</span></a>). We then showcase the results of our gaze estimation module (SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S4.SS3" title="4.3 Results of the Gaze Estimation Module â€£ 4 Experiments â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_tag">4.3</span></a>), comparing them to the most related workÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib25" title="">25</a>]</cite>.
Next, we report the results of 2D/3D gaze target detection (SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S4.SS4" title="4.4 Results on the GFIE Dataset â€£ 4 Experiments â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_tag">4.4</span></a>), corresponding to the entire pipeline and comparing our findings against state-of-the-art methods. Finally, we present the qualitative results (SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S4.SS5" title="4.5 Qualitative Results â€£ 4 Experiments â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_tag">4.5</span></a>) and ablation study (SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S4.SS6" title="4.6 Ablation Study â€£ 4 Experiments â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_tag">4.6</span></a>).</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Dataset</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Datasets annotated for 3D gaze estimation and 2D/3D gaze target detection are particularly scarce. Consequently, we use the largest available dataset of this kind, GFIEÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib25" title="">25</a>]</cite>, to evaluate our proposed method. This dataset allows us to compare our results with state-of-the-art methods and conduct an ablation study to examine the impact of using only a portion of the skeleton and the effect of blurring the face.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">The GFIE datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib25" title="">25</a>]</cite> was captured by using a laser rangefinder and an Azure Kinect RGB-D camera mounted on a stable platform. The laser rangefinder projects a laser spot to guide the subjectâ€™s gaze, ensuring continuous fixation during recording. This way, the distance measurements from the laser rangefinder can be logged. Annotations encompass a) head bounding boxes, b) 2D/3D gaze targets, and c) 2D/3D eye locations. The annotations were performed semi-automatically. For example, a face detection methodÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib48" title="">48</a>]</cite> was used, followed by manual verification. For 2D gaze targets, after detecting laser spots in RGB images, a team of annotators validates the accuracy of these detections. Using the distance measurements obtained from the laser rangefinder, the positions of the gaze targets in 2D are mapped into corresponding 3D coordinates in the real-world space. Additionally, using a facial landmark detector, the locations of left and right eye landmarks were identified within cropped head images.
The 2D eye position corresponds to the midpoint between these landmarks, while the 3D eye position was computed by unprojecting from the depth mapâ€™s facial landmarks.
This dataset includes diverse gaze behaviors from 61 subjects (27 male and 34 female), covering various activities. It consists of a total of 71,799 frames. The dataset is divided into a training set containing 59,217 frames, a test set with 6,281 frames, and a validation set containing 6,281 frames.
Importantly, the subjects and scenes present in the training set were excluded from both the test and validation sets.
To validate our approach, we use the same evaluation protocol defined inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib25" title="">25</a>]</cite>, and during training, we use the annotations for the 2D and 3D gaze targets and the annotated 3D gaze vector as the ground truth.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Evaluation Metrics</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">While our primary focus remains on the 3D evaluation metrics and the 3D scene in general, we also report the results for the 2D metrics, usually adopted for the standard gaze target detection task.
The standard 3D metrics involve distance and the angle error defined as follows. <span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.1">3D Distance</span> measures the Euclidean distance between the predicted and actual gaze points in a 3D space. The unit is meters. <span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.2">Angle Error</span> quantifies the angle difference between the predicted and actual gaze directions. It is computed as the <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p1.1.3">arccos</span> of the cosine similarity between the normalized ground truth and predicted gaze vectors. The unit is degrees.
On the other hand, 2D metrics include the area under the curve and the 2D distance, described as follows. <span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.4">Area Under Curve (AUC)</span> utilizes the predicted heatmap to plot the ROC curve, as proposed byÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib27" title="">27</a>]</cite>. <span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.5">2D Distance</span> computes the Euclidean distance between the predicted and actual gaze points, using the <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p1.1.6">argmax</span> of the predicted 2D heatmap, assuming an image size of 1<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><mo id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><times id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">Ã—</annotation></semantics></math>1.
While angle error is a metric for 3D gaze estimation, all other metrics pertain to 2D/3D gaze target detection tasks.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Results of the Gaze Estimation Module</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Without injecting our proposed module for gaze estimation into our complete gaze target detection pipeline, we trained and tested it alone. We performed the same for the face crop-based approach inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib25" title="">25</a>]</cite>.
Differently from our module, Hu et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib25" title="">25</a>]</cite> pre-trained their network that gets the head crops as input on another gaze estimation dataset called Gaze360Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib28" title="">28</a>]</cite>.
However, we do not use any auxiliary datasets to pre-train our model.
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib25" title="">25</a>]</cite>, while performing very well on the training set with a 3D angle error close to 3Â°, when it comes to the testing set, it achieves an angle error of 19.7Â°, showing an overfitting characteristic. On the other hand, our training and testing performance is consistent, and we perform a 3D angle error of 16.5Â°, which is not only remarkably better but also overfitting-free.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Results on the GFIE Dataset</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">The results obtained by running our full pipeline on the GFIE datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib25" title="">25</a>]</cite> are reported in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S4.T1" title="Table 1 â€£ 4.4 Results on the GFIE Dataset â€£ 4 Experiments â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_tag">1</span></a>. We compare our method with several approaches for detecting gaze targets in both 2D and 3D, as summarized below.</p>
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><em class="ltx_emph ltx_font_italic" id="S4.I1.i1.p1.1.1">Random</em> approach refers to gaze targets chosen randomly within the image for 2D and within the point cloud for 3D space.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><em class="ltx_emph ltx_font_italic" id="S4.I1.i2.p1.1.1">Center</em> approach positions the gaze targets at the center of the image for 2D space and at the center of the point cloud for 3D space.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1">For the existing methods that focus on gaze target detection in 2D, such as <em class="ltx_emph ltx_font_italic" id="S4.I1.i3.p1.1.1">GazeFollow</em>Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib39" title="">39</a>]</cite>, <em class="ltx_emph ltx_font_italic" id="S4.I1.i3.p1.1.2">Lian et al.</em>Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib31" title="">31</a>]</cite>, and <em class="ltx_emph ltx_font_italic" id="S4.I1.i3.p1.1.3">Chong et al.</em>Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib12" title="">12</a>]</cite>, they have been extended to estimate 3D gaze targets by unprojecting the 2D gaze targets into 3D space inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib25" title="">25</a>]</cite>.
Additionally, <em class="ltx_emph ltx_font_italic" id="S4.I1.i3.p1.1.4">RT-Gene</em>Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib18" title="">18</a>]</cite> and <em class="ltx_emph ltx_font_italic" id="S4.I1.i3.p1.1.5">Gaze360</em>Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib28" title="">28</a>]</cite>, which estimate 3D gaze directions in unconstrained environments, have been adapted to the same evaluation protocol inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib25" title="">25</a>]</cite>. The most similar vector to the predicted gaze vector within the unregistered depth map has been selected as the actual gaze vector, and the point obtained by adding this vector to the eye coordinates is considered the 3D gaze target.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I1.i4.p1">
<p class="ltx_p" id="S4.I1.i4.p1.1"><em class="ltx_emph ltx_font_italic" id="S4.I1.i4.p1.1.1">Hu et al.</em>Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib25" title="">25</a>]</cite> is the most relevant work, which uses head crops, scene images (without blurring), and depth maps to build a point cloud and simultaneously estimate the gaze direction and location in both 2D and 3D.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">The baseline results, obtained with the <em class="ltx_emph ltx_font_italic" id="S4.SS4.p2.1.1">Random</em> and <em class="ltx_emph ltx_font_italic" id="S4.SS4.p2.1.2">Center</em> heuristics, do not offer particular insights about the problem, achieving comprehensively high error values in both scenarios.
The methods built explicitly for gaze estimation (<span class="ltx_text" id="S4.SS4.p2.1.3"><em class="ltx_emph ltx_font_italic" id="S4.SS4.p2.1.3.1">Rt-Gene</em>Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib18" title="">18</a>]</cite></span> and <em class="ltx_emph ltx_font_italic" id="S4.SS4.p2.1.4">Gaze360</em>Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib28" title="">28</a>]</cite>) perform well in estimating a suitable gaze vector, while obtaining poor results on the gaze target selection.</p>
</div>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.1">Our methodology, trained with a multi-stage strategy or an end-to-end strategy and based on a gaze estimation method that exploits the attention mechanism on the feature extracted from both upper-body poses and depth maps and operates on scene images with the faces blurred out, outperforms all state-of-the-art in three metrics out of four. More importantly, both the 3D metrics have been improved with respect toÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib25" title="">25</a>]</cite>, with a gain of 27 millimeters on the 3D distance and a 1.8Â°Â improvement on the 3D angle error achieved by the multi-stage strategy, with the gaze estimation module trained alone before the full pipeline training.
Improving gaze estimation is, therefore, of fundamental importance in solving the task of gaze target detection in 3D, even more than in 2D scenarios.
Since most of the modelsâ€™ inputs are sensory-acquired 3D data (depth maps, camera parameters, and eye positions), a better estimation of the gaze vector would allow focusing on a relatively small number of potential 3D targets.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.6.2.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S4.T1.2.1" style="font-size:90%;">Results on the GFIE datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib25" title="">25</a>]</cite> for the 2D and the 3D scenarios. The results of the prior work are taken fromÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib25" title="">25</a>]</cite>. <math alttext="Dist." class="ltx_Math" display="inline" id="S4.T1.2.1.m1.1"><semantics id="S4.T1.2.1.m1.1b"><mrow id="S4.T1.2.1.m1.1.1.1" xref="S4.T1.2.1.m1.1.1.1.1.cmml"><mrow id="S4.T1.2.1.m1.1.1.1.1" xref="S4.T1.2.1.m1.1.1.1.1.cmml"><mi id="S4.T1.2.1.m1.1.1.1.1.2" xref="S4.T1.2.1.m1.1.1.1.1.2.cmml">D</mi><mo id="S4.T1.2.1.m1.1.1.1.1.1" xref="S4.T1.2.1.m1.1.1.1.1.1.cmml">â¢</mo><mi id="S4.T1.2.1.m1.1.1.1.1.3" xref="S4.T1.2.1.m1.1.1.1.1.3.cmml">i</mi><mo id="S4.T1.2.1.m1.1.1.1.1.1b" xref="S4.T1.2.1.m1.1.1.1.1.1.cmml">â¢</mo><mi id="S4.T1.2.1.m1.1.1.1.1.4" xref="S4.T1.2.1.m1.1.1.1.1.4.cmml">s</mi><mo id="S4.T1.2.1.m1.1.1.1.1.1c" xref="S4.T1.2.1.m1.1.1.1.1.1.cmml">â¢</mo><mi id="S4.T1.2.1.m1.1.1.1.1.5" xref="S4.T1.2.1.m1.1.1.1.1.5.cmml">t</mi></mrow><mo id="S4.T1.2.1.m1.1.1.1.2" lspace="0em" xref="S4.T1.2.1.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.2.1.m1.1c"><apply id="S4.T1.2.1.m1.1.1.1.1.cmml" xref="S4.T1.2.1.m1.1.1.1"><times id="S4.T1.2.1.m1.1.1.1.1.1.cmml" xref="S4.T1.2.1.m1.1.1.1.1.1"></times><ci id="S4.T1.2.1.m1.1.1.1.1.2.cmml" xref="S4.T1.2.1.m1.1.1.1.1.2">ğ·</ci><ci id="S4.T1.2.1.m1.1.1.1.1.3.cmml" xref="S4.T1.2.1.m1.1.1.1.1.3">ğ‘–</ci><ci id="S4.T1.2.1.m1.1.1.1.1.4.cmml" xref="S4.T1.2.1.m1.1.1.1.1.4">ğ‘ </ci><ci id="S4.T1.2.1.m1.1.1.1.1.5.cmml" xref="S4.T1.2.1.m1.1.1.1.1.5">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.1.m1.1d">Dist.</annotation><annotation encoding="application/x-llamapun" id="S4.T1.2.1.m1.1e">italic_D italic_i italic_s italic_t .</annotation></semantics></math> stands for Distance.
The <span class="ltx_text ltx_font_bold" id="S4.T1.2.1.1">bold</span> numbers represent the best results. The second best is <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.2.1.2">underlined</span>.
H, D, UP, and S stand for head crop, depth map, upper-body pose, and scene, respectively.
</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.7" style="width:390.3pt;height:155.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-76.2pt,30.3pt) scale(0.719110236614551,0.719110236614551) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.7.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.7.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S4.T1.7.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.7.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.1.1.2.1">Original</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.7.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.1.1.3.1">Privacy</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T1.7.1.1.1.4"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S4.T1.7.1.1.1.4.1">3D Metrics</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T1.7.1.1.1.5"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S4.T1.7.1.1.1.5.1">2D Metrics</span></th>
</tr>
<tr class="ltx_tr" id="S4.T1.7.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" id="S4.T1.7.1.2.2.1"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.2.2.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.7.1.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.2.2.2.1">Modalities</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.7.1.2.2.3"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.2.2.3.1">Preserving</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.7.1.2.2.4"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.2.2.4.1">Dist.(â†“)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.7.1.2.2.5"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.2.2.5.1">Angle Error(â†“)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.7.1.2.2.6"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.2.2.6.1">AUC(â†‘)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.7.1.2.2.7"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.2.2.7.1">Dist.(â†“)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.7.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.7.1.3.1.1">Random</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.3.1.2">S</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.3.1.3"><span class="ltx_text" id="S4.T1.7.1.3.1.3.1" style="color:#FF0000;">âœ—</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.3.1.4">2.930</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.3.1.5">84.4Â°</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.3.1.6">0.585</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.1.3.1.7">0.425</td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.7.1.4.2.1">Center</th>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.4.2.2">S</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.4.2.3"><span class="ltx_text" id="S4.T1.7.1.4.2.3.1" style="color:#FF0000;">âœ—</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.4.2.4">2.510</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.4.2.5">87.2Â°</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.4.2.6">0.614</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.4.2.7">0.287</td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.7.1.5.3.1">GazeFollowÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib39" title="">39</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.5.3.2">H, S</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.5.3.3"><span class="ltx_text" id="S4.T1.7.1.5.3.3.1" style="color:#FF0000;">âœ—</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.5.3.4">0.856</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.5.3.5">41.5Â°</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.5.3.6">0.941</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.5.3.7">0.131</td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.7.1.6.4.1">Lian et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib31" title="">31</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.6.4.2">H, S</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.6.4.3"><span class="ltx_text" id="S4.T1.7.1.6.4.3.1" style="color:#FF0000;">âœ—</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.6.4.4">0.542</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.6.4.5">26.7Â°</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.6.4.6">0.962</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.6.4.7">0.091</td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.1.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.7.1.7.5.1">Chong et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib12" title="">12</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.7.5.2">H, S</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.7.5.3"><span class="ltx_text" id="S4.T1.7.1.7.5.3.1" style="color:#FF0000;">âœ—</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.7.5.4">0.455</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.7.5.5">20.8Â°</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.7.5.6">0.972</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.7.5.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.7.1.7.5.7.1">0.069</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.1.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.7.1.8.6.1">Rt-GeneÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib18" title="">18</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.8.6.2">H, S</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.8.6.3"><span class="ltx_text" id="S4.T1.7.1.8.6.3.1" style="color:#FF0000;">âœ—</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.8.6.4">0.552</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.8.6.5">21.0Â°</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.8.6.6">0.823</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.8.6.7">0.123</td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.1.9.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.7.1.9.7.1">Gaze360Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib28" title="">28</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.9.7.2">H, S</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.9.7.3"><span class="ltx_text" id="S4.T1.7.1.9.7.3.1" style="color:#FF0000;">âœ—</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.9.7.4">0.540</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.9.7.5">19.8Â°</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.9.7.6">0.821</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.9.7.7">0.130</td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.1.10.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.7.1.10.8.1">Hu et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib25" title="">25</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.10.8.2">H, S, D</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.10.8.3"><span class="ltx_text" id="S4.T1.7.1.10.8.3.1" style="color:#FF0000;">âœ—</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.10.8.4">0.311</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.10.8.5">17.7Â°</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.10.8.6">0.965</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.10.8.7"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.10.8.7.1">0.065</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.1.11.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.7.1.11.9.1"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.11.9.1.1">Ours (Multi-stage)</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.11.9.2">UP, D, S (blurred)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.11.9.3"><span class="ltx_text" id="S4.T1.7.1.11.9.3.1" style="color:#00FF00;">âœ“</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.11.9.4"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.11.9.4.1">0.284</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.11.9.5"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.11.9.5.1">15.9Â°</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.11.9.6"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.11.9.6.1">0.983</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.1.11.9.7">0.083</td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.1.12.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T1.7.1.12.10.1"><span class="ltx_text ltx_font_bold" id="S4.T1.7.1.12.10.1.1">Ours (End-to-end)</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.7.1.12.10.2">UP, D, S (blurred)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.7.1.12.10.3"><span class="ltx_text" id="S4.T1.7.1.12.10.3.1" style="color:#00FF00;">âœ“</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.7.1.12.10.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.7.1.12.10.4.1">0.292</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.7.1.12.10.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.7.1.12.10.5.1">16.2Â°</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.7.1.12.10.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.7.1.12.10.6.1">0.973</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.7.1.12.10.7">0.083</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Qualitative Results</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">In Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S4.F3" title="Figure 3 â€£ 4.5 Qualitative Results â€£ 4 Experiments â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_tag">3</span></a>, we present some qualitative results obtained on the GFIE datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib25" title="">25</a>]</cite>. As it can be seen from the analyzed samples, our method can estimate 3D gaze vectors and gaze targets (in <span class="ltx_text" id="S4.SS5.p1.1.1" style="color:#0000FF;">blue</span>) very close to the ground truth annotated ones (in <span class="ltx_text" id="S4.SS5.p1.1.2" style="color:#FF0000;">red</span>). More interestingly, our gaze estimation module seems to operate well regardless of the orientation of the person, with upper-body skeletons front facing the camera (fourth row), on the side (fifth row), or captured from the back (first row). Additionally, in the third row, a subject interacts with a robot, both looking and touching it, demonstrating the relationship between body position, the performed action, and, consequently, where the gaze is directed.</p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="570" id="S4.F3.g1" src="extracted/5882003/src/imgs/qualitatives_arial.png" width="509"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.8.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S4.F3.9.2" style="font-size:90%;">
Qualitative results of our method on the GFIE datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib25" title="">25</a>]</cite>. Each row represents a single sample. <span class="ltx_text ltx_font_bold" id="S4.F3.9.2.1">First Column:</span> We use resized depth maps as input for our gaze estimation module. <span class="ltx_text ltx_font_bold" id="S4.F3.9.2.2">Second Column:</span> The estimated upper-body skeletons are visualized on a black background before being normalized. <span class="ltx_text ltx_font_bold" id="S4.F3.9.2.3">Third Column:</span> Scene images where the faces are blurred out before being used by the encoder-decoder module to predict the 2D gaze target heatmap. <span class="ltx_text ltx_font_bold" id="S4.F3.9.2.4">Fourth Column:</span> The final point cloud of the scene, with the ground truth gaze vector in <span class="ltx_text" id="S4.F3.9.2.5" style="color:#FF0000;">red</span> and the estimated gaze vector in <span class="ltx_text" id="S4.F3.9.2.6" style="color:#0000FF;">blue</span>.
</span></figcaption>
</figure>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.6.2.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S4.T2.2.1" style="font-size:90%;">Ablation study in terms of modality and the type of input scene image (<em class="ltx_emph ltx_font_italic" id="S4.T2.2.1.1">i.e</em>.<span class="ltx_text" id="S4.T2.2.1.2"></span>, face blurred and not blurred) on the GFIE datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#bib.bib25" title="">25</a>]</cite>. <math alttext="Dist." class="ltx_Math" display="inline" id="S4.T2.2.1.m1.1"><semantics id="S4.T2.2.1.m1.1b"><mrow id="S4.T2.2.1.m1.1.1.1" xref="S4.T2.2.1.m1.1.1.1.1.cmml"><mrow id="S4.T2.2.1.m1.1.1.1.1" xref="S4.T2.2.1.m1.1.1.1.1.cmml"><mi id="S4.T2.2.1.m1.1.1.1.1.2" xref="S4.T2.2.1.m1.1.1.1.1.2.cmml">D</mi><mo id="S4.T2.2.1.m1.1.1.1.1.1" xref="S4.T2.2.1.m1.1.1.1.1.1.cmml">â¢</mo><mi id="S4.T2.2.1.m1.1.1.1.1.3" xref="S4.T2.2.1.m1.1.1.1.1.3.cmml">i</mi><mo id="S4.T2.2.1.m1.1.1.1.1.1b" xref="S4.T2.2.1.m1.1.1.1.1.1.cmml">â¢</mo><mi id="S4.T2.2.1.m1.1.1.1.1.4" xref="S4.T2.2.1.m1.1.1.1.1.4.cmml">s</mi><mo id="S4.T2.2.1.m1.1.1.1.1.1c" xref="S4.T2.2.1.m1.1.1.1.1.1.cmml">â¢</mo><mi id="S4.T2.2.1.m1.1.1.1.1.5" xref="S4.T2.2.1.m1.1.1.1.1.5.cmml">t</mi></mrow><mo id="S4.T2.2.1.m1.1.1.1.2" lspace="0em" xref="S4.T2.2.1.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.2.1.m1.1c"><apply id="S4.T2.2.1.m1.1.1.1.1.cmml" xref="S4.T2.2.1.m1.1.1.1"><times id="S4.T2.2.1.m1.1.1.1.1.1.cmml" xref="S4.T2.2.1.m1.1.1.1.1.1"></times><ci id="S4.T2.2.1.m1.1.1.1.1.2.cmml" xref="S4.T2.2.1.m1.1.1.1.1.2">ğ·</ci><ci id="S4.T2.2.1.m1.1.1.1.1.3.cmml" xref="S4.T2.2.1.m1.1.1.1.1.3">ğ‘–</ci><ci id="S4.T2.2.1.m1.1.1.1.1.4.cmml" xref="S4.T2.2.1.m1.1.1.1.1.4">ğ‘ </ci><ci id="S4.T2.2.1.m1.1.1.1.1.5.cmml" xref="S4.T2.2.1.m1.1.1.1.1.5">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.1.m1.1d">Dist.</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.1.m1.1e">italic_D italic_i italic_s italic_t .</annotation></semantics></math> stands for Distance. Note that depth maps and scene images are fundamental and must be included for the 3D gaze target detection to function properly; thus, their removal cannot be ablated.
</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.7" style="width:390.3pt;height:114pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-20.5pt,6.0pt) scale(0.904938383256822,0.904938383256822) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.7.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.7.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S4.T2.7.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.7.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T2.7.1.1.1.2.1">Head</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.7.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T2.7.1.1.1.3.1">Blurred</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.7.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T2.7.1.1.1.4.1">Full</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.7.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T2.7.1.1.1.5.1">Upper</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T2.7.1.1.1.6"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S4.T2.7.1.1.1.6.1">3D Metrics</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T2.7.1.1.1.7"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S4.T2.7.1.1.1.7.1">2D Metrics</span></th>
</tr>
<tr class="ltx_tr" id="S4.T2.7.1.2.2">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T2.7.1.2.2.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.7.1.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.T2.7.1.2.2.2.1">Crop</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.7.1.2.2.3"><span class="ltx_text ltx_font_bold" id="S4.T2.7.1.2.2.3.1">Face</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.7.1.2.2.4"><span class="ltx_text ltx_font_bold" id="S4.T2.7.1.2.2.4.1">Body</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.7.1.2.2.5"><span class="ltx_text ltx_font_bold" id="S4.T2.7.1.2.2.5.1">Body</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.7.1.2.2.6"><span class="ltx_text ltx_font_bold" id="S4.T2.7.1.2.2.6.1">Dist.(â†“)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.7.1.2.2.7"><span class="ltx_text ltx_font_bold" id="S4.T2.7.1.2.2.7.1">Angle Error(â†“)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.7.1.2.2.8"><span class="ltx_text ltx_font_bold" id="S4.T2.7.1.2.2.8.1">AUC(â†‘)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.7.1.2.2.9"><span class="ltx_text ltx_font_bold" id="S4.T2.7.1.2.2.9.1">Dist.(â†“)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.7.1.3.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T2.7.1.3.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.1.3.1.2"><span class="ltx_text" id="S4.T2.7.1.3.1.2.1" style="color:#00FF00;">âœ“</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.1.3.1.3"><span class="ltx_text" id="S4.T2.7.1.3.1.3.1" style="color:#FF0000;">âœ—</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.1.3.1.4"><span class="ltx_text" id="S4.T2.7.1.3.1.4.1" style="color:#FF0000;">âœ—</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.1.3.1.5"><span class="ltx_text" id="S4.T2.7.1.3.1.5.1" style="color:#FF0000;">âœ—</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.1.3.1.6">0.311</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.1.3.1.7">17.7Â°</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.1.3.1.8">0.965</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.1.3.1.9">0.065</td>
</tr>
<tr class="ltx_tr" id="S4.T2.7.1.4.2">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T2.7.1.4.2.1"></th>
<td class="ltx_td ltx_align_center" id="S4.T2.7.1.4.2.2"><span class="ltx_text" id="S4.T2.7.1.4.2.2.1" style="color:#FF0000;">âœ—</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.1.4.2.3"><span class="ltx_text" id="S4.T2.7.1.4.2.3.1" style="color:#FF0000;">âœ—</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.1.4.2.4"><span class="ltx_text" id="S4.T2.7.1.4.2.4.1" style="color:#FF0000;">âœ—</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.1.4.2.5"><span class="ltx_text" id="S4.T2.7.1.4.2.5.1" style="color:#00FF00;">âœ“</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.1.4.2.6">0.314</td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.1.4.2.7">16.1Â°</td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.1.4.2.8">0.981</td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.1.4.2.9">0.086</td>
</tr>
<tr class="ltx_tr" id="S4.T2.7.1.5.3">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T2.7.1.5.3.1"></th>
<td class="ltx_td ltx_align_center" id="S4.T2.7.1.5.3.2"><span class="ltx_text" id="S4.T2.7.1.5.3.2.1" style="color:#FF0000;">âœ—</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.1.5.3.3"><span class="ltx_text" id="S4.T2.7.1.5.3.3.1" style="color:#00FF00;">âœ“</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.1.5.3.4"><span class="ltx_text" id="S4.T2.7.1.5.3.4.1" style="color:#00FF00;">âœ“</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.1.5.3.5"><span class="ltx_text" id="S4.T2.7.1.5.3.5.1" style="color:#FF0000;">âœ—</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.1.5.3.6">0.310</td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.1.5.3.7">16.7Â°</td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.1.5.3.8">0.979</td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.1.5.3.9"><span class="ltx_text ltx_font_bold" id="S4.T2.7.1.5.3.9.1">0.059</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.7.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.7.1.6.4.1">Multi-stage</th>
<td class="ltx_td ltx_align_center" id="S4.T2.7.1.6.4.2"><span class="ltx_text" id="S4.T2.7.1.6.4.2.1" style="color:#FF0000;">âœ—</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.1.6.4.3"><span class="ltx_text" id="S4.T2.7.1.6.4.3.1" style="color:#00FF00;">âœ“</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.1.6.4.4"><span class="ltx_text" id="S4.T2.7.1.6.4.4.1" style="color:#FF0000;">âœ—</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.1.6.4.5"><span class="ltx_text" id="S4.T2.7.1.6.4.5.1" style="color:#00FF00;">âœ“</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.1.6.4.6"><span class="ltx_text ltx_font_bold" id="S4.T2.7.1.6.4.6.1">0.284</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.1.6.4.7"><span class="ltx_text ltx_font_bold" id="S4.T2.7.1.6.4.7.1">15.9Â°</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.1.6.4.8"><span class="ltx_text ltx_font_bold" id="S4.T2.7.1.6.4.8.1">0.983</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.1.6.4.9">0.083</td>
</tr>
<tr class="ltx_tr" id="S4.T2.7.1.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T2.7.1.7.5.1">End-to-end</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.7.1.7.5.2"><span class="ltx_text" id="S4.T2.7.1.7.5.2.1" style="color:#FF0000;">âœ—</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.7.1.7.5.3"><span class="ltx_text" id="S4.T2.7.1.7.5.3.1" style="color:#00FF00;">âœ“</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.7.1.7.5.4"><span class="ltx_text" id="S4.T2.7.1.7.5.4.1" style="color:#FF0000;">âœ—</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.7.1.7.5.5"><span class="ltx_text" id="S4.T2.7.1.7.5.5.1" style="color:#00FF00;">âœ“</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.7.1.7.5.6">0.292</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.7.1.7.5.7">16.2Â°</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.7.1.7.5.8">0.973</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.7.1.7.5.9">0.083</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Ablation Study</h3>
<div class="ltx_para" id="S4.SS6.p1">
<p class="ltx_p" id="S4.SS6.p1.2">We report ablation results (TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.17886v1#S4.T2" title="Table 2 â€£ 4.5 Qualitative Results â€£ 4 Experiments â€£ Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection"><span class="ltx_text ltx_ref_tag">2</span></a>) performed by removing or changing the used modalities for the proposed method.
We also present the results of the proposed method when trained using either the end-to-end approach or the multi-stage training process.
As seen, using the upper-body skeleton instead of the complete 17-joint skeleton is slightly beneficial for the 3D Distance compared to the baseline (third row).
Interestingly, using the entire skeleton leads to the best 2D distance score.
This can happen due to the optimization process of the two losses, <math alttext="\mathcal{L}_{gaze}" class="ltx_Math" display="inline" id="S4.SS6.p1.1.m1.1"><semantics id="S4.SS6.p1.1.m1.1a"><msub id="S4.SS6.p1.1.m1.1.1" xref="S4.SS6.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS6.p1.1.m1.1.1.2" xref="S4.SS6.p1.1.m1.1.1.2.cmml">â„’</mi><mrow id="S4.SS6.p1.1.m1.1.1.3" xref="S4.SS6.p1.1.m1.1.1.3.cmml"><mi id="S4.SS6.p1.1.m1.1.1.3.2" xref="S4.SS6.p1.1.m1.1.1.3.2.cmml">g</mi><mo id="S4.SS6.p1.1.m1.1.1.3.1" xref="S4.SS6.p1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S4.SS6.p1.1.m1.1.1.3.3" xref="S4.SS6.p1.1.m1.1.1.3.3.cmml">a</mi><mo id="S4.SS6.p1.1.m1.1.1.3.1a" xref="S4.SS6.p1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S4.SS6.p1.1.m1.1.1.3.4" xref="S4.SS6.p1.1.m1.1.1.3.4.cmml">z</mi><mo id="S4.SS6.p1.1.m1.1.1.3.1b" xref="S4.SS6.p1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S4.SS6.p1.1.m1.1.1.3.5" xref="S4.SS6.p1.1.m1.1.1.3.5.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS6.p1.1.m1.1b"><apply id="S4.SS6.p1.1.m1.1.1.cmml" xref="S4.SS6.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS6.p1.1.m1.1.1.1.cmml" xref="S4.SS6.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS6.p1.1.m1.1.1.2.cmml" xref="S4.SS6.p1.1.m1.1.1.2">â„’</ci><apply id="S4.SS6.p1.1.m1.1.1.3.cmml" xref="S4.SS6.p1.1.m1.1.1.3"><times id="S4.SS6.p1.1.m1.1.1.3.1.cmml" xref="S4.SS6.p1.1.m1.1.1.3.1"></times><ci id="S4.SS6.p1.1.m1.1.1.3.2.cmml" xref="S4.SS6.p1.1.m1.1.1.3.2">ğ‘”</ci><ci id="S4.SS6.p1.1.m1.1.1.3.3.cmml" xref="S4.SS6.p1.1.m1.1.1.3.3">ğ‘</ci><ci id="S4.SS6.p1.1.m1.1.1.3.4.cmml" xref="S4.SS6.p1.1.m1.1.1.3.4">ğ‘§</ci><ci id="S4.SS6.p1.1.m1.1.1.3.5.cmml" xref="S4.SS6.p1.1.m1.1.1.3.5">ğ‘’</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p1.1.m1.1c">\mathcal{L}_{gaze}</annotation><annotation encoding="application/x-llamapun" id="S4.SS6.p1.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_g italic_a italic_z italic_e end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\mathcal{L}_{heat}" class="ltx_Math" display="inline" id="S4.SS6.p1.2.m2.1"><semantics id="S4.SS6.p1.2.m2.1a"><msub id="S4.SS6.p1.2.m2.1.1" xref="S4.SS6.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS6.p1.2.m2.1.1.2" xref="S4.SS6.p1.2.m2.1.1.2.cmml">â„’</mi><mrow id="S4.SS6.p1.2.m2.1.1.3" xref="S4.SS6.p1.2.m2.1.1.3.cmml"><mi id="S4.SS6.p1.2.m2.1.1.3.2" xref="S4.SS6.p1.2.m2.1.1.3.2.cmml">h</mi><mo id="S4.SS6.p1.2.m2.1.1.3.1" xref="S4.SS6.p1.2.m2.1.1.3.1.cmml">â¢</mo><mi id="S4.SS6.p1.2.m2.1.1.3.3" xref="S4.SS6.p1.2.m2.1.1.3.3.cmml">e</mi><mo id="S4.SS6.p1.2.m2.1.1.3.1a" xref="S4.SS6.p1.2.m2.1.1.3.1.cmml">â¢</mo><mi id="S4.SS6.p1.2.m2.1.1.3.4" xref="S4.SS6.p1.2.m2.1.1.3.4.cmml">a</mi><mo id="S4.SS6.p1.2.m2.1.1.3.1b" xref="S4.SS6.p1.2.m2.1.1.3.1.cmml">â¢</mo><mi id="S4.SS6.p1.2.m2.1.1.3.5" xref="S4.SS6.p1.2.m2.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS6.p1.2.m2.1b"><apply id="S4.SS6.p1.2.m2.1.1.cmml" xref="S4.SS6.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS6.p1.2.m2.1.1.1.cmml" xref="S4.SS6.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS6.p1.2.m2.1.1.2.cmml" xref="S4.SS6.p1.2.m2.1.1.2">â„’</ci><apply id="S4.SS6.p1.2.m2.1.1.3.cmml" xref="S4.SS6.p1.2.m2.1.1.3"><times id="S4.SS6.p1.2.m2.1.1.3.1.cmml" xref="S4.SS6.p1.2.m2.1.1.3.1"></times><ci id="S4.SS6.p1.2.m2.1.1.3.2.cmml" xref="S4.SS6.p1.2.m2.1.1.3.2">â„</ci><ci id="S4.SS6.p1.2.m2.1.1.3.3.cmml" xref="S4.SS6.p1.2.m2.1.1.3.3">ğ‘’</ci><ci id="S4.SS6.p1.2.m2.1.1.3.4.cmml" xref="S4.SS6.p1.2.m2.1.1.3.4">ğ‘</ci><ci id="S4.SS6.p1.2.m2.1.1.3.5.cmml" xref="S4.SS6.p1.2.m2.1.1.3.5">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p1.2.m2.1c">\mathcal{L}_{heat}</annotation><annotation encoding="application/x-llamapun" id="S4.SS6.p1.2.m2.1d">caligraphic_L start_POSTSUBSCRIPT italic_h italic_e italic_a italic_t end_POSTSUBSCRIPT</annotation></semantics></math>.
In this case, it seems the latter was favored over the former.
The second row shows that using the model without blurring the faces benefits the 2D AUC metric.
The pipeline trained with a multi-stage policy, firstly training the gaze estimation module alone and loading its weights before launching the training on the full pipeline, achieved the best results on both the 3D metrics, with a minimum of 15.9Â°Â for the angle error.
However, if a faster training procedure is required, the end-to-end strategy still provides outstanding results and is preferable in such a situation.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">The gaze target detection task, often referred to as gaze-following, is a fundamental proxy task for multiple vision problems regarding the behavior and interaction of people between themselves and with the environment.
Understanding where and what a person is observing can give us hints about their current interests and future actions, whether they are interactions with humans, with objects in the environment, and, more recently, with collaborative robots that are increasingly present in public and workplaces.
In this paper, we propose a novel way to tackle 3D gaze estimation and 3D gaze target detection by focusing on the upper-body pose and the scene depth map, outperforming the previous state-of-the-art.
The combination of the skeleton-based gaze estimation module and the blurring applied on the face in the scene image allowed us to present an effective method that is also able to preserve the sensitive data of the person, such as the face appearance, generally used to solve these types of tasks.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text ltx_font_bold" id="S5.p2.1.1">Limitations and future work.</span>
In instances where pose estimation fails, our upper-body-based gaze estimation and target detection pipeline might not perform as effectively. However, this issue is also present in state-of-the-art methods that primarily rely on head crops. For the dataset used in this paper, we manually reviewed the body poses for a subset of the testing data and did not observe any failures with the pose estimation method employed. At present, there is a lack of multi-person 3D gaze target detection datasets. Consequently, our work is constrained by the complexity of scenes and the availability of a dataset that only features single individuals per scene. As more crowded datasets and corresponding gaze annotations become available, it will be important to test our approach on these datasets as well. However, we currently anticipate that our pipeline can be applied directly without requiring modifications. Eventually, in case of occlusions across multiple persons, we can adopt a denoising pose estimator to our pipeline. Furthermore, an interesting future direction is the inclusion of activity recognition as part of the pipeline, exploiting the gaze direction and target as a proxy for predicting the future intentions of one or more people.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This study was carried out within the PNRR research activities of the consortium iNEST (Interconnected North-Est Innovation Ecosystem) funded by the European Union Next-GenerationEU (Piano Nazionale di Ripresa e Resilienza (PNRR) â€“ Missione 4 Componente 2, Investimento 1.5 â€“ D.D. 1058 23/06/2022, ECS_00000043).
This manuscript reflects only the Authorsâ€™ views and opinions. Neither the European Union nor the European Commission can be considered responsible for them.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Admoni, H., Scassellati, B.: Social eye gaze in human-robot interaction: a review. Journal of Human-Robot Interaction <span class="ltx_text ltx_font_bold" id="bib.bib1.1.1">6</span>(1), 25â€“63 (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Bao, J., Liu, B., Yu, J.: Escnet: Gaze target detection with the understanding of 3d scenes. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 14126â€“14135 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Beyan, C., Katsageorgiou, V.M., Murino, V.: A sequential data analysis approach to detect emergent leaders in small groups. IEEE Transactions on Multimedia <span class="ltx_text ltx_font_bold" id="bib.bib3.1.1">21</span>(8), 2107â€“2116 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Beyan, C., Vinciarelli, A., DelÂ Bue, A.: Co-located human-human interaction analysis using nonverbal cues: A survey. ACM Computing Surveys (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Biswas, P., etÂ al.: Appearance-based gaze estimation using attention and difference mechanism. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 3143â€“3152 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Brau, E., Guan, J., Jeffries, T., Barnard, K.: Multiple-gaze geometry: Inferring novel 3d locations from gazes observed in monocular video. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 612â€“630 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Capozzi, F., Beyan, C., Pierro, A., Koul, A., Murino, V., Livi, S., Bayliss, A.P., Ristic, J., Becchio, C.: Tracking the leader: Gaze behavior in group interactions. Iscience <span class="ltx_text ltx_font_bold" id="bib.bib7.1.1">16</span>, 242â€“249 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Chen, Z., Shi, B.E.: Towards high performance low complexity calibration in appearance based gaze estimation. IEEE transactions on pattern analysis and machine intelligence <span class="ltx_text ltx_font_bold" id="bib.bib8.1.1">45</span>(1), 1174â€“1188 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Cheng, Y., Huang, S., Wang, F., Qian, C., Lu, F.: A coarse-to-fine adaptive network for appearance-based gaze estimation. In: Proceedings of the AAAI conference on artificial intelligence. vol.Â 34, pp. 10623â€“10630 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Cheng, Y., Lu, F., Zhang, X.: Appearance-based gaze estimation via evaluation-guided asymmetric regression. In: Proceedings of the European conference on computer vision (ECCV). pp. 100â€“115 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Chong, E., Ruiz, N., Wang, Y., Zhang, Y., Rozga, A., Rehg, J.M.: Connecting gaze, scene, and attention: Generalized attention estimation via joint modeling of gaze and scene saliency. In: Proceedings of the European conference on computer vision (ECCV). pp. 383â€“398 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Chong, E., Wang, Y., Ruiz, N., Rehg, J.M.: Detecting attended visual targets in video. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 5396â€“5406 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Dalton, K.M., Nacewicz, B.M., Johnstone, T., Schaefer, H.S., Gernsbacher, M.A., Goldsmith, H.H., Alexander, A.L., Davidson, R.J.: Gaze fixation and the neural circuitry of face processing in autism. Nature neuroscience <span class="ltx_text ltx_font_bold" id="bib.bib13.1.1">8</span>(4), 519â€“526 (2005)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: 2009 IEEE conference on computer vision and pattern recognition. pp. 248â€“255. IEEE (2009)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Edwards, S.G., Stephenson, L.J., Dalmaso, M., Bayliss, A.P.: Social orienting in gaze leading: a mechanism for shared attention. Proceedings of the Royal Society B: Biological Sciences <span class="ltx_text ltx_font_bold" id="bib.bib15.1.1">282</span>(1812) (2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Emery, N.J.: The eyes have it: the neuroethology, function and evolution of social gaze. Neuroscience &amp; biobehavioral reviews <span class="ltx_text ltx_font_bold" id="bib.bib16.1.1">24</span>(6), 581â€“604 (2000)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Fang, Y., Tang, J., Shen, W., Shen, W., Gu, X., Song, L., Zhai, G.: Dual attention guided gaze target detection in the wild. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 11390â€“11399 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Fischer, T., Chang, H.J., Demiris, Y.: Rt-gene: Real-time eye gaze estimation in natural environments. In: Proceedings of the European conference on computer vision (ECCV). pp. 334â€“352 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Ghosh, S., Dhall, A., Hayat, M., Knibbe, J., Ji, Q.: Automatic gaze analysis: A survey of deep learning based approaches. IEEE Transactions on Pattern Analysis and Machine Intelligence <span class="ltx_text ltx_font_bold" id="bib.bib19.1.1">46</span>(1), 61â€“84 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Guo, Z., Yuan, Z., Zhang, C., Chi, W., Ling, Y., Zhang, S.: Domain adaptation gaze estimation by embedding with prediction consistency. In: Proceedings of the Asian Conference on Computer Vision (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Gupta, A., Tafasca, S., Odobez, J.M.: A modular multimodal architecture for gaze target prediction: Application to privacy-sensitive settings. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5041â€“5050 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770â€“778 (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Horanyi, N., Zheng, L., Chong, E., Leonardis, A., Chang, H.J.: Where are they looking in the 3d space? In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2677â€“2686 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Hu, Z., Yang, D., Cheng, S., Zhou, L., Wu, S., Liu, J.: We know where they are looking at from the rgb-d camera: Gaze following in 3d. IEEE Transactions on Instrumentation and Measurement <span class="ltx_text ltx_font_bold" id="bib.bib24.1.1">71</span>, 1â€“14 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Hu, Z., Yang, Y., Zhai, X., Yang, D., Zhou, B., Liu, J.: Gfie: A dataset and baseline for gaze-following from 2d to 3d in indoor environments. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8907â€“8916 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Jin, T., Yu, Q., Zhu, S., Lin, Z., Ren, J., Zhou, Y., Song, W.: Depth-aware gaze-following via auxiliary networks for robotics. Engineering Applications of Artificial Intelligence <span class="ltx_text ltx_font_bold" id="bib.bib26.1.1">113</span>, 104924 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Judd, T., Ehinger, K., Durand, F., Torralba, A.: Learning to predict where humans look. In: 2009 IEEE 12th international conference on computer vision. pp. 2106â€“2113. IEEE (2009)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Kellnhofer, P., Recasens, A., Stent, S., Matusik, W., Torralba, A.: Gaze360: Physically unconstrained gaze estimation in the wild. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 6912â€“6921 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Krafka, K., Khosla, A., Kellnhofer, P., Kannan, H., Bhandarkar, S., Matusik, W., Torralba, A.: Eye tracking for everyone. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2176â€“2184 (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Lian, D., Yu, Z., Gao, S.: Believe it or not, we know what you are looking at! In: Asian Conference on Computer Vision. pp. 35â€“50. Springer (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Lu, F., Gao, Y., Chen, X.: Estimating 3d gaze directions using unlabeled eye images via synthetic iris appearance fitting. IEEE Transactions on Multimedia <span class="ltx_text ltx_font_bold" id="bib.bib32.1.1">18</span>(9), 1772â€“1782 (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Miao, Q., Hoai, M., Samaras, D.: Patch-level gaze distribution prediction for gaze following. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 880â€“889 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Nakazawa, A., Nitschke, C.: Point of gaze estimation through corneal surface reflection in an active illumination environment. In: Computer Visionâ€“ECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part II 12. pp. 159â€“172. Springer (2012)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Noureddin, B., Lawrence, P.D., Man, C.: A non-contact device for tracking gaze in a human computer interface. Computer Vision and Image Understanding <span class="ltx_text ltx_font_bold" id="bib.bib35.1.1">98</span>(1), 52â€“82 (2005)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
OÂ Oh, J., Chang, H.J., Choi, S.I.: Self-attention with convolution and deconvolution for efficient eye gaze estimation from a full face image. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4992â€“5000 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Pathirana, P., Senarath, S., Meedeniya, D., Jayarathna, S.: Eye gaze estimation: A survey on deep learning-based approaches. Expert Systems with Applications <span class="ltx_text ltx_font_bold" id="bib.bib37.1.1">199</span>, 116894 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Rayner, K.: Eye movements in reading and information processing: 20 years of research. Psychological bulletin <span class="ltx_text ltx_font_bold" id="bib.bib38.1.1">124</span>(3), Â 372 (1998)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Recasens, A., Khosla, A., Vondrick, C., Torralba, A.: Where are they looking? Advances in neural information processing systems <span class="ltx_text ltx_font_bold" id="bib.bib39.1.1">28</span> (2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Recasens, A., Vondrick, C., Khosla, A., Torralba, A.: Following gaze in video. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 1435â€“1443 (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Toaiari, A., Cunico, F., Taioli, F., Caputo, A., Menegaz, G., Giachetti, A., Farinella, G.M., Cristani, M.: Scene-pathy: Capturing the visual selective attention of people towards scene elements. In: International Conference on Image Analysis and Processing. pp. 352â€“363. Springer (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Tonini, F., Beyan, C., Ricci, E.: Multimodal across domains gaze target detection. In: Proceedings of the 2022 International Conference on Multimodal Interaction. pp. 420â€“431 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Tonini, F., Dallâ€™Asen, N., Beyan, C., Ricci, E.: Object-aware gaze target detection. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 21860â€“21869 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Tu, D., Min, X., Duan, H., Guo, G., Zhai, G., Shen, W.: End-to-end human-gaze-target detection with transformers. In: 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 2192â€“2200. IEEE (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Valenti, R., Sebe, N., Gevers, T.: Combining head pose and eye location information for gaze estimation. IEEE Transactions on Image Processing <span class="ltx_text ltx_font_bold" id="bib.bib45.1.1">21</span>(2), 802â€“815 (2011)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Wei, P., Liu, Y., Shu, T., Zheng, N., Zhu, S.C.: Where and why are they looking? jointly inferring human attention and intentions in complex tasks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 6801â€“6809 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Yuan, Y., Fu, R., Huang, L., Lin, W., Zhang, C., Chen, X., Wang, J.: Hrformer: High-resolution vision transformer for dense predict. Advances in neural information processing systems <span class="ltx_text ltx_font_bold" id="bib.bib47.1.1">34</span>, 7281â€“7293 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Zhang, S., Zhu, X., Lei, Z., Shi, H., Wang, X., Li, S.Z.: S3fd: Single shot scale-invariant face detector. In: Proceedings of the IEEE international conference on computer vision. pp. 192â€“201 (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Zhang, X., Sugano, Y., Bulling, A.: Evaluation of appearance-based methods and implications for gaze-based applications. In: Proceedings of the 2019 CHI conference on human factors in computing systems. pp. 1â€“13 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Zhang, X., Sugano, Y., Fritz, M., Bulling, A.: Itâ€™s written all over your face: Full-face appearance-based gaze estimation. In: Proceedings of the IEEE conference on computer vision and pattern recognition workshops. pp. 51â€“60 (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Zhang, X., Sugano, Y., Fritz, M., Bulling, A.: Mpiigaze: Real-world dataset and deep appearance-based gaze estimation. IEEE transactions on pattern analysis and machine intelligence <span class="ltx_text ltx_font_bold" id="bib.bib51.1.1">41</span>(1), 162â€“175 (2017)

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Sep 26 14:07:25 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
