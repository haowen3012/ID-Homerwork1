<!DOCTYPE html>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits</title>
<!--Generated on Wed Jan 10 14:55:01 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2401.05202v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S1" title="1 Introduction ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S2" title="2 Materials ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Materials</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S2.SS1" title="2.1 Data acquisition ‚Ä£ 2 Materials ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Data acquisition</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S2.SS2" title="2.2 Locomotion scoring ‚Ä£ 2 Materials ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Locomotion scoring</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S2.SS3" title="2.3 Observers reliability and agreement ‚Ä£ 2 Materials ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Observers reliability and agreement</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S2.SS3.SSS1" title="2.3.1 Inter-observer reliability and agreement ‚Ä£ 2.3 Observers reliability and agreement ‚Ä£ 2 Materials ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.1 </span>Inter-observer reliability and agreement</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S2.SS3.SSS2" title="2.3.2 Intra-observer reliability and agreement ‚Ä£ 2.3 Observers reliability and agreement ‚Ä£ 2 Materials ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.2 </span>Intra-observer reliability and agreement</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S2.SS4" title="2.4 Merging the locomotion scores ‚Ä£ 2 Materials ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Merging the locomotion scores</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S2.SS4.SSS1" title="2.4.1 Merging the scores from multiple observers ‚Ä£ 2.4 Merging the locomotion scores ‚Ä£ 2 Materials ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4.1 </span>Merging the scores from multiple observers</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S2.SS4.SSS2" title="2.4.2 Merging the levels of the scale ‚Ä£ 2.4 Merging the locomotion scores ‚Ä£ 2 Materials ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4.2 </span>Merging the levels of the scale</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S2.SS5" title="2.5 Overview of the Materials ‚Ä£ 2 Materials ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5 </span>Overview of the Materials</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S3" title="3 Methods ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S3.SS1" title="3.1 Pose estimation ‚Ä£ 3 Methods ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Pose estimation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S3.SS1.SSS1" title="3.1.1 Detect-and-crop ‚Ä£ 3.1 Pose estimation ‚Ä£ 3 Methods ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Detect-and-crop</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S3.SS1.SSS2" title="3.1.2 Keypoint detection ‚Ä£ 3.1 Pose estimation ‚Ä£ 3 Methods ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>Keypoint detection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S3.SS1.SSS3" title="3.1.3 Keypoint correction ‚Ä£ 3.1 Pose estimation ‚Ä£ 3 Methods ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.3 </span>Keypoint correction</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S3.SS2" title="3.2 Gait features extraction ‚Ä£ 3 Methods ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Gait features extraction</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S3.SS2.SSS1" title="3.2.1 Step detection ‚Ä£ 3.2 Gait features extraction ‚Ä£ 3 Methods ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Step detection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S3.SS2.SSS2" title="3.2.2 Step correction ‚Ä£ 3.2 Gait features extraction ‚Ä£ 3 Methods ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Step correction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S3.SS2.SSS3" title="3.2.3 Back posture measurement (BPM) ‚Ä£ 3.2 Gait features extraction ‚Ä£ 3 Methods ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.3 </span>Back posture measurement (BPM)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S3.SS2.SSS4" title="3.2.4 Head bobbing amplitude (HBA) ‚Ä£ 3.2 Gait features extraction ‚Ä£ 3 Methods ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.4 </span>Head bobbing amplitude (HBA)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S3.SS2.SSS5" title="3.2.5 Tracking distance (TRK) ‚Ä£ 3.2 Gait features extraction ‚Ä£ 3 Methods ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.5 </span>Tracking distance (TRK)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S3.SS2.SSS6" title="3.2.6 Stride length difference (STL) ‚Ä£ 3.2 Gait features extraction ‚Ä£ 3 Methods ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.6 </span>Stride length difference (STL)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S3.SS2.SSS7" title="3.2.7 Stance duration difference (STD) ‚Ä£ 3.2 Gait features extraction ‚Ä£ 3 Methods ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.7 </span>Stance duration difference (STD)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S3.SS2.SSS8" title="3.2.8 Swing duration difference (SWD) ‚Ä£ 3.2 Gait features extraction ‚Ä£ 3 Methods ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.8 </span>Swing duration difference (SWD)</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S3.SS3" title="3.3 Gait classification ‚Ä£ 3 Methods ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Gait classification</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S3.SS3.SSS1" title="3.3.1 Data preparation ‚Ä£ 3.3 Gait classification ‚Ä£ 3 Methods ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>Data preparation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S3.SS3.SSS2" title="3.3.2 Classification models ‚Ä£ 3.3 Gait classification ‚Ä£ 3 Methods ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.2 </span>Classification models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S3.SS3.SSS3" title="3.3.3 Evaluation metrics ‚Ä£ 3.3 Gait classification ‚Ä£ 3 Methods ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.3 </span>Evaluation metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S3.SS3.SSS4" title="3.3.4 Feature importance ‚Ä£ 3.3 Gait classification ‚Ä£ 3 Methods ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.4 </span>Feature importance</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S4" title="4 Results ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS1" title="4.1 Pose estimation ‚Ä£ 4 Results ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Pose estimation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS2" title="4.2 Gait score classification ‚Ä£ 4 Results ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Gait score classification</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS3" title="4.3 Feature importance ‚Ä£ 4 Results ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Feature importance</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S5" title="5 Discussion ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S5.SS1" title="5.1 Video processing ‚Ä£ 5 Discussion ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Video processing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S5.SS2" title="5.2 Locomotion scoring ‚Ä£ 5 Discussion ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Locomotion scoring</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S5.SS3" title="5.3 Gait score classification ‚Ä£ 5 Discussion ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Gait score classification</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S5.SS4" title="5.4 Feature importance ‚Ä£ 5 Discussion ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Feature importance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S5.SS5" title="5.5 Comparison with related work ‚Ä£ 5 Discussion ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Comparison with related work</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S6" title="6 Conclusion ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content"><div class="section" id="target-section"><div id="license-tr">License: CC BY 4.0</div><div id="watermark-tr">arXiv:2401.05202v1 [cs.CV] 10 Jan 2024</div></div>
<article class="ltx_document ltx_fleqn">
<h1 class="ltx_title ltx_title_document">Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Helena Russello
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rik van der Tol
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Menno Holzhauer
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Eldert J. van Henten
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gert Kootstra
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_address">Agricultural Biosystems Engineering group, Wageningen University &amp; Research, Wageningen, The Netherlands 
</span>
<span class="ltx_contact ltx_role_address">Ruminant Health Department, Royal GD AH, Deventer, The Netherlands
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">This study presents an automated lameness detection system that uses deep-learning image processing techniques to extract multiple locomotion traits associated with lameness.
Using the T-LEAP pose estimation model, the motion of nine keypoints was extracted from videos of walking cows.
The videos were recorded outdoors, with varying illumination conditions, and T-LEAP extracted 99.6% of correct keypoints.
The trajectories of the keypoints were then used to compute six locomotion traits: back posture measurement, head bobbing, tracking distance, stride length, stance duration, and swing duration.
The three most important traits were back posture measurement, head bobbing, and tracking distance.
For the ground truth, we showed that a thoughtful merging of the scores of the observers could improve intra-observer reliability and agreement.
We showed that including multiple locomotion traits improves the classification accuracy from 76.6% with only one trait to 79.9% with the three most important traits and to 80.1% with all six locomotion traits. ¬†
<br class="ltx_break"/></p>
</div>
<span class="ltx_ERROR undefined" id="id1">\useunder</span>
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1"><span class="ltx_text ltx_ulem_uline" id="p1.1.1"></span><span class="ltx_ERROR undefined" id="p1.1.2">\ul</span>
</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Lameness is a painful gait disorder in dairy cows and is often characterized by abnormal locomotion of the cow.
A recent literature review¬†<cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="#bib.bib1" title="">thomsen2023prevalence, </a>)</cite> estimated the global prevalence of lameness at 22.8%, with little change in the last 30 years.
Lameness has a negative impact on welfare¬†<cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="#bib.bib2" title="">whay2017impact, </a>)</cite> and leads to substantial economic losses¬†<cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="#bib.bib3" title="">enting1997economic, </a>)</cite> due to decreased milk production and reproduction¬†<cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="#bib.bib4" title="">huxley2013impact, </a>)</cite> as well as premature culling¬†<cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="#bib.bib3" title="">enting1997economic, </a>)</cite>.
While lameness is commonly assessed by trained observers performing visual locomotion scoring of the herd, the procedure is time-consuming and cannot realistically be performed on a regular basis.
Hence, dairy farms could benefit from automatic lameness detection.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">To date, a number of studies have investigated ways to automate locomotion scoring and lameness detection using camera systems. Video cameras are an attractive sensor for this application as they are relatively inexpensive, non-intrusive, and scale well with large herds.
A three-step approach is commonly taken to detect lameness from videos: (1) use computer vision methods to localize body parts of interest, (2) compute one or more locomotion traits from the extracted body parts, and (3) train a classifier to score lameness using the locomotion traits as features.
In the past, the body parts were localized using classical computer vision methods such as background subtraction¬†<cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="#bib.bib5" title="">song_automatic_2008, </a>; <a class="ltx_ref" href="#bib.bib6" title="">poursaberi_real-time_2010, </a>; <a class="ltx_ref" href="#bib.bib7" title="">zheng_cows_2023, </a>; <a class="ltx_ref" href="#bib.bib8" title="">zhao_automatic_2023, </a>)</cite>. These methods worked in experimental settings but were sensitive to changes in background and light, making them less applicable in practice.
Others placed physical markers (tags or paint marks) on the cows‚Äô body parts and tracked the markers with specialized software¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib9" title="">blackie_associations_2013 </a>; <a class="ltx_ref" href="#bib.bib10" title="">karoui_deep_2021 </a></cite>. In practical settings, however, physical markers don‚Äôt scale well to large herds as they need to be placed on each cow and cleaned regularly to remain visible.
More recently, with the emergence of deep neural networks, studies started using deep-learning-based object detection¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib11" title="">wu_lameness_2020 </a>; <a class="ltx_ref" href="#bib.bib12" title="">kang_accurate_2020 </a>; <a class="ltx_ref" href="#bib.bib13" title="">jiang_dairy_2022 </a>; <a class="ltx_ref" href="#bib.bib7" title="">zheng_cows_2023 </a></cite> to localize the legs or the back of the cows, object segmentation¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib14" title="">arazo_segmentation_2022 </a></cite> to extract the body contour from the background, or markerless (i.e., without physical markers) pose estimation¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib15" title="">mathis_deeplabcut_2018 </a>; <a class="ltx_ref" href="#bib.bib16" title="">russello_t-leap_2021 </a>; <a class="ltx_ref" href="#bib.bib8" title="">zhao_automatic_2023 </a>; <a class="ltx_ref" href="#bib.bib17" title="">barney2023deep </a>; <a class="ltx_ref" href="#bib.bib18" title="">taghavi2023cow </a></cite> to localize multiple body parts in videos. Although they typically require more data than classical approaches, the deep-learning methods cope well with complex background and light conditions and can sometimes even cope with occlusions such as fences¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib16" title="">russello_t-leap_2021 </a>; <a class="ltx_ref" href="#bib.bib18" title="">taghavi2023cow </a></cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Once localized in the images or video frames, the outline of the spine, for instance, can be used to compute the back posture¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib6" title="">poursaberi_real-time_2010 </a>; <a class="ltx_ref" href="#bib.bib19" title="">viazzi_analysis_2012 </a>; <a class="ltx_ref" href="#bib.bib20" title="">van_hertem_automatic_2014 </a>; <a class="ltx_ref" href="#bib.bib21" title="">viazzi_comparison_2014 </a>; <a class="ltx_ref" href="#bib.bib22" title="">van_hertem_implementation_2018 </a>; <a class="ltx_ref" href="#bib.bib13" title="">jiang_dairy_2022 </a>; <a class="ltx_ref" href="#bib.bib7" title="">zheng_cows_2023 </a></cite>, and the location of the legs to compute the tracking distance¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib5" title="">song_automatic_2008 </a>; <a class="ltx_ref" href="#bib.bib9" title="">blackie_associations_2013 </a></cite> or stride length¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib9" title="">blackie_associations_2013 </a>; <a class="ltx_ref" href="#bib.bib11" title="">wu_lameness_2020 </a>; <a class="ltx_ref" href="#bib.bib7" title="">zheng_cows_2023 </a></cite>.
To the best of our knowledge, almost all studies on lameness detection from videos use only one locomotion trait as a feature to score lameness, and only two studies¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib17" title="">barney2023deep </a>; <a class="ltx_ref" href="#bib.bib8" title="">zhao_automatic_2023 </a></cite> combined two locomotion traits, namely back posture and head bobbing.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Using the locomotion trait(s) as feature(s), supervised learning classifiers can then be trained to score lameness.
In supervised learning, classifiers learn from given examples, also known as ground truth or golden standard. Manual locomotion scores, that is, locomotion scores provided by one or more observers, make up the ground truth of lameness detection classifiers.
The subjective nature of manual locomotion scoring is a well-known problem¬†<cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="#bib.bib23" title="">schlageter-tello_effect_2014, </a>)</cite> and often leads to low intra- and inter-observer reliability and agreement.
However, a classifier can only be as good as its ground truth, so information about the reliability of the locomotion scale is necessary. However, observer reliability and agreement are seldom reported, let alone analyzed.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Three critical gaps emerge from the studies discussed so far:
(1) the use of obsolete image processing methods remains frequent,
(2) no one combined more than two locomotion traits for lameness classification,
and (3) the reliability of the ground truth is seldom reported.
This paper addresses the three gaps mentioned above and proposes a non-intrusive and fully automated approach to camera-based lameness detection that includes multiple locomotion traits.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">We used videos of walking cows that were scored on a 5-point locomotion scoring scale by four observers.
We first reported and discussed the intra- and inter-observer reliability and agreement of the ground truth.
We showed the effect of several approaches for merging scores from multiple observers and motivated the merging of the 5-point locomotion scale to a binary scale.
We then trained T-LEAP¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib16" title="">russello_t-leap_2021 </a></cite>, a deep-learning markerless pose estimation model, to automatically extract the motion of multiple body parts (later referred to as keypoints) from videos of walking cows.
The sequences of keypoints were used to compute six locomotion traits that are known to be correlated with locomotion scores¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib24" title="">schlageter-tello_relation_2015 </a></cite>, namely back posture measurement, head bobbing, tracking distance, stride length, stance duration, and swing duration.
Using the locomotion traits mentioned above as input features, we trained multiple machine-learning classifiers to score the gait on a 2-level scale (healthy/lame).
We evaluated the performance of each model and showed the impact of using different combinations of locomotion traits on the score classification.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Materials</h2>
<span class="ltx_ERROR undefined" id="S2.1">\@fb@secFB</span>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Data acquisition</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The data were collected in Tilburg, The Netherlands, at a commercial dairy farm whose herd contained about a hundred Holstein-Frisian cows.
The data were collected between 9 am and 4 pm on 8 different days between May and July 2019. The cows were filmed from the side while they walked freely through an outdoor passageway.
A ZED RGB-D stereo camera<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.stereolabs.com/zed-2/" title="">https://www.stereolabs.com/zed-2/</a></span></span></span> was placed 2 meters above the ground, at 4.5m from the fence of the passageway.
The camera directly faced the passageway and recorded in landscape mode at Full-HD (1080p) resolution at 30 frames per second.
The recordings were saved into short videos of about 7.6 seconds, which was the average time a cow needed to walk the visible part of the passageway (9.5 meters).
The same data acquisition campaign was used by¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib16" title="">russello_t-leap_2021 </a></cite> on the same farm.
In total, 1101 videos were collected, and a subset of 272 videos were selected according to the following criteria: there was only one cow on the passageway, and the cow walked from the left to the right without distraction or interruption.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">During the data collection, no process was set in place to automatically link the videos to an individual cow (e.g., by means of an RFID tag reader). The cows were, therefore, assigned a unique identifier at a later time by manually grouping the individual cows. We identified 98 unique cows, out of which 24 cows were present in the videos only once, 21 cows twice, 25 three times, 17 four times, 6 five times, 3 six times, 1 seven times, and 1 eight times. For the cows that were present multiple times, some were recorded at different times on the same day, and some on different days.</p>
</div>
<span class="ltx_ERROR undefined" id="S2.SS1.1">\@fb@secFB</span>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Locomotion scoring</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The locomotion scoring was performed using the 5-point discrete scale described by Sprecher et al. 1997¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib25" title="">sprecher_lameness_1997 </a></cite>, where a score of 1 corresponds to <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.1">normal gait</span>, 2 to <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.2">midly lame</span>, 3 to <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.3">moderately lame</span>, 4 to <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.4">lame</span> and 5 to <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.5">severely lame</span>.
The videos were scored by four observers: one expert (A) with 20 years of experience in visual locomotion scoring and 3 observers (B, C, D) with no prior experience in locomotion scoring but with a background in animal science and dairy farming.
The inexperienced observers were trained by the expert (A) before the scoring session.
During the scoring session, each video was played twice in a row to give enough time to observe the locomotion. To ensure consistency, the observers were asked to give the lowest score if they were hesitating between two scores.
All the videos were scored on the same day.
After the scoring session, the observers indicated no cow recognition, i.e., that they did not recognize the individual cows that appeared in multiple videos.
Table¬†<a class="ltx_ref" href="#S2.T1" title="Table 1 ‚Ä£ 2.2 Locomotion scoring ‚Ä£ 2 Materials ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_tag">1</span></a> shows the distribution of the scores assigned by the four observers.
The distribution of the scores was highly imbalanced and indicated a homogeneous herd, where most cows were distributed throughout the first two levels of the scale (normal, mildly lame), which is typical of herds with a low prevalence of lameness¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib26" title="">thomsen2008evaluation </a></cite>.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T1.2.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S2.T1.3.2" style="font-size:90%;">Distribution of the locomotion scores assigned by the observers</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T1.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.4.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T1.4.1.1.1" rowspan="2"><span class="ltx_text" id="S2.T1.4.1.1.1.1">Observer</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="5" id="S2.T1.4.1.1.2">Locomotion score</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.4.1.1.3" rowspan="2"><span class="ltx_text" id="S2.T1.4.1.1.3.1">Total</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.2.2">
<td class="ltx_td ltx_align_center" id="S2.T1.4.2.2.1">1</td>
<td class="ltx_td ltx_align_center" id="S2.T1.4.2.2.2">2</td>
<td class="ltx_td ltx_align_center" id="S2.T1.4.2.2.3">3</td>
<td class="ltx_td ltx_align_center" id="S2.T1.4.2.2.4">4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.4.2.2.5">5</td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T1.4.3.3.1">A</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.4.3.3.2">115</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.4.3.3.3">99</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.4.3.3.4">27</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.4.3.3.5">31</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.4.3.3.6">0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.4.3.3.7">272</td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S2.T1.4.4.4.1">B</th>
<td class="ltx_td ltx_align_center" id="S2.T1.4.4.4.2">109</td>
<td class="ltx_td ltx_align_center" id="S2.T1.4.4.4.3">80</td>
<td class="ltx_td ltx_align_center" id="S2.T1.4.4.4.4">54</td>
<td class="ltx_td ltx_align_center" id="S2.T1.4.4.4.5">26</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.4.4.4.6">3</td>
<td class="ltx_td ltx_align_center" id="S2.T1.4.4.4.7">272</td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S2.T1.4.5.5.1">C</th>
<td class="ltx_td ltx_align_center" id="S2.T1.4.5.5.2">101</td>
<td class="ltx_td ltx_align_center" id="S2.T1.4.5.5.3">119</td>
<td class="ltx_td ltx_align_center" id="S2.T1.4.5.5.4">34</td>
<td class="ltx_td ltx_align_center" id="S2.T1.4.5.5.5">15</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.4.5.5.6">3</td>
<td class="ltx_td ltx_align_center" id="S2.T1.4.5.5.7">272</td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S2.T1.4.6.6.1">D</th>
<td class="ltx_td ltx_align_center" id="S2.T1.4.6.6.2">141</td>
<td class="ltx_td ltx_align_center" id="S2.T1.4.6.6.3">80</td>
<td class="ltx_td ltx_align_center" id="S2.T1.4.6.6.4">38</td>
<td class="ltx_td ltx_align_center" id="S2.T1.4.6.6.5">12</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.4.6.6.6">1</td>
<td class="ltx_td ltx_align_center" id="S2.T1.4.6.6.7">272</td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.7.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.4.7.7.1">Distribution</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S2.T1.4.7.7.2">42.8%</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S2.T1.4.7.7.3">34.7%</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S2.T1.4.7.7.4">14.1%</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S2.T1.4.7.7.5">7.7%</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.4.7.7.6">0.6%</td>
<td class="ltx_td ltx_border_b ltx_border_t" id="S2.T1.4.7.7.7"></td>
</tr>
</tbody>
</table>
</figure><span class="ltx_ERROR undefined" id="S2.SS2.1">\@fb@secFB</span>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Observers reliability and agreement</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.3">Manual locomotion scoring is subjective¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib27" title="">flower_effect_2006 </a></cite>. Investigating the reliability and agreement between (inter-rater) and among (intra-rater) raters can inform on the quality of the data.
Reliability estimates the capability of the raters to differentiate between the different scores, whereas agreement assesses the capability of the raters to assign the same score to the same data point.
Reliability was measured with Krippendorff‚Äôs <math alttext="\alpha" class="ltx_Math" display="inline" id="S2.SS3.p1.1.m1.1"><semantics id="S2.SS3.p1.1.m1.1a"><mi id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml">Œ±</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.1b"><ci id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1">ùõº</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.1.m1.1d">italic_Œ±</annotation></semantics></math>¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib28" title="">krippendorff2011computing </a></cite> for ordinal values, and agreement was presented as the Percentage of Agreement (PA) and Specific Agreement (SA).
The commonly accepted thresholds are <math alttext="\alpha\geq 0.66" class="ltx_Math" display="inline" id="S2.SS3.p1.2.m2.1"><semantics id="S2.SS3.p1.2.m2.1a"><mrow id="S2.SS3.p1.2.m2.1.1" xref="S2.SS3.p1.2.m2.1.1.cmml"><mi id="S2.SS3.p1.2.m2.1.1.2" xref="S2.SS3.p1.2.m2.1.1.2.cmml">Œ±</mi><mo id="S2.SS3.p1.2.m2.1.1.1" xref="S2.SS3.p1.2.m2.1.1.1.cmml">‚â•</mo><mn id="S2.SS3.p1.2.m2.1.1.3" xref="S2.SS3.p1.2.m2.1.1.3.cmml">0.66</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.2.m2.1b"><apply id="S2.SS3.p1.2.m2.1.1.cmml" xref="S2.SS3.p1.2.m2.1.1"><geq id="S2.SS3.p1.2.m2.1.1.1.cmml" xref="S2.SS3.p1.2.m2.1.1.1"></geq><ci id="S2.SS3.p1.2.m2.1.1.2.cmml" xref="S2.SS3.p1.2.m2.1.1.2">ùõº</ci><cn id="S2.SS3.p1.2.m2.1.1.3.cmml" type="float" xref="S2.SS3.p1.2.m2.1.1.3">0.66</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.2.m2.1c">\alpha\geq 0.66</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.2.m2.1d">italic_Œ± ‚â• 0.66</annotation></semantics></math> for reliability¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib28" title="">krippendorff2011computing </a></cite>, and <math alttext="\text{PA}\geq 75\%" class="ltx_Math" display="inline" id="S2.SS3.p1.3.m3.1"><semantics id="S2.SS3.p1.3.m3.1a"><mrow id="S2.SS3.p1.3.m3.1.1" xref="S2.SS3.p1.3.m3.1.1.cmml"><mtext id="S2.SS3.p1.3.m3.1.1.2" xref="S2.SS3.p1.3.m3.1.1.2a.cmml">PA</mtext><mo id="S2.SS3.p1.3.m3.1.1.1" xref="S2.SS3.p1.3.m3.1.1.1.cmml">‚â•</mo><mrow id="S2.SS3.p1.3.m3.1.1.3" xref="S2.SS3.p1.3.m3.1.1.3.cmml"><mn id="S2.SS3.p1.3.m3.1.1.3.2" xref="S2.SS3.p1.3.m3.1.1.3.2.cmml">75</mn><mo id="S2.SS3.p1.3.m3.1.1.3.1" xref="S2.SS3.p1.3.m3.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.3.m3.1b"><apply id="S2.SS3.p1.3.m3.1.1.cmml" xref="S2.SS3.p1.3.m3.1.1"><geq id="S2.SS3.p1.3.m3.1.1.1.cmml" xref="S2.SS3.p1.3.m3.1.1.1"></geq><ci id="S2.SS3.p1.3.m3.1.1.2a.cmml" xref="S2.SS3.p1.3.m3.1.1.2"><mtext id="S2.SS3.p1.3.m3.1.1.2.cmml" xref="S2.SS3.p1.3.m3.1.1.2">PA</mtext></ci><apply id="S2.SS3.p1.3.m3.1.1.3.cmml" xref="S2.SS3.p1.3.m3.1.1.3"><csymbol cd="latexml" id="S2.SS3.p1.3.m3.1.1.3.1.cmml" xref="S2.SS3.p1.3.m3.1.1.3.1">percent</csymbol><cn id="S2.SS3.p1.3.m3.1.1.3.2.cmml" type="integer" xref="S2.SS3.p1.3.m3.1.1.3.2">75</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.3.m3.1c">\text{PA}\geq 75\%</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.3.m3.1d">PA ‚â• 75 %</annotation></semantics></math> for agreement¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib23" title="">schlageter-tello_effect_2014 </a></cite>.
The inter-observer and intra-observer measures are reported in the following sub-sections.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.1 </span>Inter-observer reliability and agreement</h4>
<div class="ltx_para" id="S2.SS3.SSS1.p1">
<p class="ltx_p" id="S2.SS3.SSS1.p1.1">The inter-observer reliability and agreement values are reported in Table¬†<a class="ltx_ref" href="#S2.T2" title="Table 2 ‚Ä£ 2.3.1 Inter-observer reliability and agreement ‚Ä£ 2.3 Observers reliability and agreement ‚Ä£ 2 Materials ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_tag">2</span></a>.
The <math alttext="\alpha" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p1.1.m1.1"><semantics id="S2.SS3.SSS1.p1.1.m1.1a"><mi id="S2.SS3.SSS1.p1.1.m1.1.1" xref="S2.SS3.SSS1.p1.1.m1.1.1.cmml">Œ±</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p1.1.m1.1b"><ci id="S2.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S2.SS3.SSS1.p1.1.m1.1.1">ùõº</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p1.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.p1.1.m1.1d">italic_Œ±</annotation></semantics></math> value was marginally lower than the commonly accepted threshold.
It meant that the observers agreed on 60% of the labels they were expected to disagree on by chance.
The percentage of agreement was also low. When looking at the specific agreement, score 5 had the lowest agreement. Observer A didn‚Äôt assign any score of 5, whereas the other observers assigned a score of 5 to at most three videos, hinting that the boundary between 4 and 5 was not clear to the inexperienced observers.</p>
</div>
<figure class="ltx_table" id="S2.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T2.5.2.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S2.T2.2.1" style="font-size:90%;">Inter-observer reliability (<math alttext="\alpha" class="ltx_Math" display="inline" id="S2.T2.2.1.m1.1"><semantics id="S2.T2.2.1.m1.1b"><mi id="S2.T2.2.1.m1.1.1" xref="S2.T2.2.1.m1.1.1.cmml">Œ±</mi><annotation-xml encoding="MathML-Content" id="S2.T2.2.1.m1.1c"><ci id="S2.T2.2.1.m1.1.1.cmml" xref="S2.T2.2.1.m1.1.1">ùõº</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.2.1.m1.1d">\alpha</annotation><annotation encoding="application/x-llamapun" id="S2.T2.2.1.m1.1e">italic_Œ±</annotation></semantics></math>), agreement (PA), and agreement per locomotion score (specific agreement) on the 5-point locomotion scale.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T2.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T2.3.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T2.3.1.2" rowspan="2"><span class="ltx_text" id="S2.T2.3.1.2.1">Levels</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T2.3.1.1" rowspan="2"><span class="ltx_text" id="S2.T2.3.1.1.1"><math alttext="\alpha" class="ltx_Math" display="inline" id="S2.T2.3.1.1.1.m1.1"><semantics id="S2.T2.3.1.1.1.m1.1a"><mi id="S2.T2.3.1.1.1.m1.1.1" xref="S2.T2.3.1.1.1.m1.1.1.cmml">Œ±</mi><annotation-xml encoding="MathML-Content" id="S2.T2.3.1.1.1.m1.1b"><ci id="S2.T2.3.1.1.1.m1.1.1.cmml" xref="S2.T2.3.1.1.1.m1.1.1">ùõº</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.3.1.1.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S2.T2.3.1.1.1.m1.1d">italic_Œ±</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T2.3.1.3" rowspan="2"><span class="ltx_text" id="S2.T2.3.1.3.1">PA</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="5" id="S2.T2.3.1.4">Specific Agreement</td>
</tr>
<tr class="ltx_tr" id="S2.T2.3.2.1">
<td class="ltx_td ltx_align_center" id="S2.T2.3.2.1.1">1</td>
<td class="ltx_td ltx_align_center" id="S2.T2.3.2.1.2">2</td>
<td class="ltx_td ltx_align_center" id="S2.T2.3.2.1.3">3</td>
<td class="ltx_td ltx_align_center" id="S2.T2.3.2.1.4">4</td>
<td class="ltx_td ltx_align_center" id="S2.T2.3.2.1.5">5</td>
</tr>
<tr class="ltx_tr" id="S2.T2.3.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" id="S2.T2.3.3.2.1">1-2-3-4-5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" id="S2.T2.3.3.2.2">0.602</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" id="S2.T2.3.3.2.3">55.8</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S2.T2.3.3.2.4">69.7</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S2.T2.3.3.2.5">49.4</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S2.T2.3.3.2.6">37.0</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S2.T2.3.3.2.7">44.4</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S2.T2.3.3.2.8">28.6</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.2 </span>Intra-observer reliability and agreement</h4>
<div class="ltx_para" id="S2.SS3.SSS2.p1">
<p class="ltx_p" id="S2.SS3.SSS2.p1.3">Intra-observer metrics are usually performed on repeated ratings from the same observer on the same data points.
Here, however, the videos were only scored once, so we could not compute intra-observer metrics the usual way.
Instead, we proposed the following approach to approximate the intra-observer reliability and agreement.
As mentioned in sub-section¬†<a class="ltx_ref" href="#S2.SS1" title="2.1 Data acquisition ‚Ä£ 2 Materials ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_tag">2.1</span></a>, some cows were present in several videos of the dataset.
Assuming that the locomotion score remained the same for a period of time <math alttext="T" class="ltx_Math" display="inline" id="S2.SS3.SSS2.p1.1.m1.1"><semantics id="S2.SS3.SSS2.p1.1.m1.1a"><mi id="S2.SS3.SSS2.p1.1.m1.1.1" xref="S2.SS3.SSS2.p1.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p1.1.m1.1b"><ci id="S2.SS3.SSS2.p1.1.m1.1.1.cmml" xref="S2.SS3.SSS2.p1.1.m1.1.1">ùëá</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p1.1.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS2.p1.1.m1.1d">italic_T</annotation></semantics></math>, we could consider videos of a cow recorded less than <math alttext="T" class="ltx_Math" display="inline" id="S2.SS3.SSS2.p1.2.m2.1"><semantics id="S2.SS3.SSS2.p1.2.m2.1a"><mi id="S2.SS3.SSS2.p1.2.m2.1.1" xref="S2.SS3.SSS2.p1.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p1.2.m2.1b"><ci id="S2.SS3.SSS2.p1.2.m2.1.1.cmml" xref="S2.SS3.SSS2.p1.2.m2.1.1">ùëá</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p1.2.m2.1c">T</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS2.p1.2.m2.1d">italic_T</annotation></semantics></math> hours apart to be the same data sample and should, therefore, be assigned the same score by the observers.
We set <math alttext="T=48" class="ltx_Math" display="inline" id="S2.SS3.SSS2.p1.3.m3.1"><semantics id="S2.SS3.SSS2.p1.3.m3.1a"><mrow id="S2.SS3.SSS2.p1.3.m3.1.1" xref="S2.SS3.SSS2.p1.3.m3.1.1.cmml"><mi id="S2.SS3.SSS2.p1.3.m3.1.1.2" xref="S2.SS3.SSS2.p1.3.m3.1.1.2.cmml">T</mi><mo id="S2.SS3.SSS2.p1.3.m3.1.1.1" xref="S2.SS3.SSS2.p1.3.m3.1.1.1.cmml">=</mo><mn id="S2.SS3.SSS2.p1.3.m3.1.1.3" xref="S2.SS3.SSS2.p1.3.m3.1.1.3.cmml">48</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p1.3.m3.1b"><apply id="S2.SS3.SSS2.p1.3.m3.1.1.cmml" xref="S2.SS3.SSS2.p1.3.m3.1.1"><eq id="S2.SS3.SSS2.p1.3.m3.1.1.1.cmml" xref="S2.SS3.SSS2.p1.3.m3.1.1.1"></eq><ci id="S2.SS3.SSS2.p1.3.m3.1.1.2.cmml" xref="S2.SS3.SSS2.p1.3.m3.1.1.2">ùëá</ci><cn id="S2.SS3.SSS2.p1.3.m3.1.1.3.cmml" type="integer" xref="S2.SS3.SSS2.p1.3.m3.1.1.3">48</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p1.3.m3.1c">T=48</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS2.p1.3.m3.1d">italic_T = 48</annotation></semantics></math> hours and found 55 pairs of videos of the same cows recorded at less than 48-hour intervals.
This data was then used to approximate the intra-observer metrics. We would like to emphasize that here, the intra-observer metrics were approximated because they were computed on a subset of the scores.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS2.p2">
<p class="ltx_p" id="S2.SS3.SSS2.p2.1">The intra-observer reliability and agreement values are reported per observer in table¬†<a class="ltx_ref" href="#S2.T3" title="Table 3 ‚Ä£ 2.3.2 Intra-observer reliability and agreement ‚Ä£ 2.3 Observers reliability and agreement ‚Ä£ 2 Materials ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_tag">3</span></a>. Out of the four observers, only observer A and observer C had the highest <math alttext="\alpha" class="ltx_Math" display="inline" id="S2.SS3.SSS2.p2.1.m1.1"><semantics id="S2.SS3.SSS2.p2.1.m1.1a"><mi id="S2.SS3.SSS2.p2.1.m1.1.1" xref="S2.SS3.SSS2.p2.1.m1.1.1.cmml">Œ±</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p2.1.m1.1b"><ci id="S2.SS3.SSS2.p2.1.m1.1.1.cmml" xref="S2.SS3.SSS2.p2.1.m1.1.1">ùõº</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p2.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS2.p2.1.m1.1d">italic_Œ±</annotation></semantics></math>, meaning that these observers were the best at distinguishing between the different levels of the scale. None of the observers reached an acceptable level of agreement, meaning that they gave the same score to the same cow less than 75% of the time.</p>
</div>
<figure class="ltx_table" id="S2.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T3.5.2.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S2.T3.2.1" style="font-size:90%;">Intra-observer reliability (<math alttext="\alpha" class="ltx_Math" display="inline" id="S2.T3.2.1.m1.1"><semantics id="S2.T3.2.1.m1.1b"><mi id="S2.T3.2.1.m1.1.1" xref="S2.T3.2.1.m1.1.1.cmml">Œ±</mi><annotation-xml encoding="MathML-Content" id="S2.T3.2.1.m1.1c"><ci id="S2.T3.2.1.m1.1.1.cmml" xref="S2.T3.2.1.m1.1.1">ùõº</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.2.1.m1.1d">\alpha</annotation><annotation encoding="application/x-llamapun" id="S2.T3.2.1.m1.1e">italic_Œ±</annotation></semantics></math>), agreement (PA), and agreement per locomotion score (specific agreement) on the 5-point locomotion scale.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T3.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T3.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T3.3.1.2" rowspan="2"><span class="ltx_text" id="S2.T3.3.1.2.1">Observer</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T3.3.1.1" rowspan="2"><span class="ltx_text" id="S2.T3.3.1.1.1"><math alttext="\alpha" class="ltx_Math" display="inline" id="S2.T3.3.1.1.1.m1.1"><semantics id="S2.T3.3.1.1.1.m1.1a"><mi id="S2.T3.3.1.1.1.m1.1.1" xref="S2.T3.3.1.1.1.m1.1.1.cmml">Œ±</mi><annotation-xml encoding="MathML-Content" id="S2.T3.3.1.1.1.m1.1b"><ci id="S2.T3.3.1.1.1.m1.1.1.cmml" xref="S2.T3.3.1.1.1.m1.1.1">ùõº</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.3.1.1.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S2.T3.3.1.1.1.m1.1d">italic_Œ±</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T3.3.1.3" rowspan="2"><span class="ltx_text ltx_font_bold" id="S2.T3.3.1.3.1">PA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="5" id="S2.T3.3.1.4">Specific Agreement</th>
</tr>
<tr class="ltx_tr" id="S2.T3.3.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T3.3.2.1.1">1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T3.3.2.1.2">2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T3.3.2.1.3">3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T3.3.2.1.4">4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T3.3.2.1.5">5</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T3.3.3.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T3.3.3.1.1">A</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T3.3.3.1.2">0.611</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T3.3.3.1.3">56.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.3.3.1.4">72.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.3.3.1.5">46.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.3.3.1.6">20.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.3.3.1.7">54.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.3.3.1.8">0.0</td>
</tr>
<tr class="ltx_tr" id="S2.T3.3.4.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T3.3.4.2.1">B</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T3.3.4.2.2">0.552</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T3.3.4.2.3">49.1</td>
<td class="ltx_td ltx_align_center" id="S2.T3.3.4.2.4">71.2</td>
<td class="ltx_td ltx_align_center" id="S2.T3.3.4.2.5">9.5</td>
<td class="ltx_td ltx_align_center" id="S2.T3.3.4.2.6">22.2</td>
<td class="ltx_td ltx_align_center" id="S2.T3.3.4.2.7">40.0</td>
<td class="ltx_td ltx_align_center" id="S2.T3.3.4.2.8">100.0</td>
</tr>
<tr class="ltx_tr" id="S2.T3.3.5.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T3.3.5.3.1">C</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T3.3.5.3.2">0.653</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T3.3.5.3.3">60.0</td>
<td class="ltx_td ltx_align_center" id="S2.T3.3.5.3.4">72.3</td>
<td class="ltx_td ltx_align_center" id="S2.T3.3.5.3.5">60.9</td>
<td class="ltx_td ltx_align_center" id="S2.T3.3.5.3.6">36.4</td>
<td class="ltx_td ltx_align_center" id="S2.T3.3.5.3.7">0.0</td>
<td class="ltx_td ltx_align_center" id="S2.T3.3.5.3.8">0.0</td>
</tr>
<tr class="ltx_tr" id="S2.T3.3.6.4">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S2.T3.3.6.4.1">D</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S2.T3.3.6.4.2">0.585</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S2.T3.3.6.4.3">58.2</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T3.3.6.4.4">76.9</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T3.3.6.4.5">32.0</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T3.3.6.4.6">30.8</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T3.3.6.4.7">33.3</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T3.3.6.4.8">0.0</td>
</tr>
</tbody>
</table>
</figure><span class="ltx_ERROR undefined" id="S2.SS3.SSS2.1">\@fb@secFB</span>
</section>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Merging the locomotion scores</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">The locomotion scores ranged from 1 to 5, and were provided by multiple observers.
Our task at hand, however, was a binary classification task, where the model was taught to distinguish between normal and lame gaits based on ground-truth examples.
The ground-truth consisted of one binary label (normal/lame) per sample (video).
Therefore, the locomotion scores needed to be merged in two ways: first, the scores from the multiple observers needed to be merged into one value; second, the five levels of the scale needed to be merged into a binary scale.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.1 </span>Merging the scores from multiple observers</h4>
<div class="ltx_para" id="S2.SS4.SSS1.p1">
<p class="ltx_p" id="S2.SS4.SSS1.p1.4">For a classification task, each sample (i.e., video) is assigned one ground-truth label or locomotion score based on the multiple ground-truth labels provided by the observers.
Common strategies for merging scores from multiple observers are mean, majority voting, and weighted voting.
In the case of a tie with voting, the highest or the lowest score is retained.
A drawback of these merging strategies is that if one or more observers have low reliability and agreement, chances are that their contributions would still add noise to the ground truth.
Using the scores of only one observer, e.g., the most reliable observer, could also be a valid strategy, but one is taking the risk of training the classifier with observer bias.
We therefore proposed an additional merging strategy: <math alttext="\tau" class="ltx_Math" display="inline" id="S2.SS4.SSS1.p1.1.m1.1"><semantics id="S2.SS4.SSS1.p1.1.m1.1a"><mi id="S2.SS4.SSS1.p1.1.m1.1.1" xref="S2.SS4.SSS1.p1.1.m1.1.1.cmml">œÑ</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p1.1.m1.1b"><ci id="S2.SS4.SSS1.p1.1.m1.1.1.cmml" xref="S2.SS4.SSS1.p1.1.m1.1.1">ùúè</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p1.1.m1.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS1.p1.1.m1.1d">italic_œÑ</annotation></semantics></math>-voting, where <math alttext="\tau" class="ltx_Math" display="inline" id="S2.SS4.SSS1.p1.2.m2.1"><semantics id="S2.SS4.SSS1.p1.2.m2.1a"><mi id="S2.SS4.SSS1.p1.2.m2.1.1" xref="S2.SS4.SSS1.p1.2.m2.1.1.cmml">œÑ</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p1.2.m2.1b"><ci id="S2.SS4.SSS1.p1.2.m2.1.1.cmml" xref="S2.SS4.SSS1.p1.2.m2.1.1">ùúè</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p1.2.m2.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS1.p1.2.m2.1d">italic_œÑ</annotation></semantics></math> defined a minimum reliability threshold.
The scores of an observer were then included in the vote if its intra-observer reliability was <math alttext="\geq\tau" class="ltx_Math" display="inline" id="S2.SS4.SSS1.p1.3.m3.1"><semantics id="S2.SS4.SSS1.p1.3.m3.1a"><mrow id="S2.SS4.SSS1.p1.3.m3.1.1" xref="S2.SS4.SSS1.p1.3.m3.1.1.cmml"><mi id="S2.SS4.SSS1.p1.3.m3.1.1.2" xref="S2.SS4.SSS1.p1.3.m3.1.1.2.cmml"></mi><mo id="S2.SS4.SSS1.p1.3.m3.1.1.1" xref="S2.SS4.SSS1.p1.3.m3.1.1.1.cmml">‚â•</mo><mi id="S2.SS4.SSS1.p1.3.m3.1.1.3" xref="S2.SS4.SSS1.p1.3.m3.1.1.3.cmml">œÑ</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p1.3.m3.1b"><apply id="S2.SS4.SSS1.p1.3.m3.1.1.cmml" xref="S2.SS4.SSS1.p1.3.m3.1.1"><geq id="S2.SS4.SSS1.p1.3.m3.1.1.1.cmml" xref="S2.SS4.SSS1.p1.3.m3.1.1.1"></geq><csymbol cd="latexml" id="S2.SS4.SSS1.p1.3.m3.1.1.2.cmml" xref="S2.SS4.SSS1.p1.3.m3.1.1.2">absent</csymbol><ci id="S2.SS4.SSS1.p1.3.m3.1.1.3.cmml" xref="S2.SS4.SSS1.p1.3.m3.1.1.3">ùúè</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p1.3.m3.1c">\geq\tau</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS1.p1.3.m3.1d">‚â• italic_œÑ</annotation></semantics></math>.
We set the threshold to the overall inter-observer reliability on the 5-level scale, so <math alttext="\tau=0.602" class="ltx_Math" display="inline" id="S2.SS4.SSS1.p1.4.m4.1"><semantics id="S2.SS4.SSS1.p1.4.m4.1a"><mrow id="S2.SS4.SSS1.p1.4.m4.1.1" xref="S2.SS4.SSS1.p1.4.m4.1.1.cmml"><mi id="S2.SS4.SSS1.p1.4.m4.1.1.2" xref="S2.SS4.SSS1.p1.4.m4.1.1.2.cmml">œÑ</mi><mo id="S2.SS4.SSS1.p1.4.m4.1.1.1" xref="S2.SS4.SSS1.p1.4.m4.1.1.1.cmml">=</mo><mn id="S2.SS4.SSS1.p1.4.m4.1.1.3" xref="S2.SS4.SSS1.p1.4.m4.1.1.3.cmml">0.602</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p1.4.m4.1b"><apply id="S2.SS4.SSS1.p1.4.m4.1.1.cmml" xref="S2.SS4.SSS1.p1.4.m4.1.1"><eq id="S2.SS4.SSS1.p1.4.m4.1.1.1.cmml" xref="S2.SS4.SSS1.p1.4.m4.1.1.1"></eq><ci id="S2.SS4.SSS1.p1.4.m4.1.1.2.cmml" xref="S2.SS4.SSS1.p1.4.m4.1.1.2">ùúè</ci><cn id="S2.SS4.SSS1.p1.4.m4.1.1.3.cmml" type="float" xref="S2.SS4.SSS1.p1.4.m4.1.1.3">0.602</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p1.4.m4.1c">\tau=0.602</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS1.p1.4.m4.1d">italic_œÑ = 0.602</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S2.SS4.SSS1.p2">
<p class="ltx_p" id="S2.SS4.SSS1.p2.3">Table¬†<a class="ltx_ref" href="#S2.T4" title="Table 4 ‚Ä£ 2.4.1 Merging the scores from multiple observers ‚Ä£ 2.4 Merging the locomotion scores ‚Ä£ 2 Materials ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_tag">4</span></a> shows the intra-reliability and intra-agreement values after applying the different merging strategies.
The <math alttext="\tau" class="ltx_Math" display="inline" id="S2.SS4.SSS1.p2.1.m1.1"><semantics id="S2.SS4.SSS1.p2.1.m1.1a"><mi id="S2.SS4.SSS1.p2.1.m1.1.1" xref="S2.SS4.SSS1.p2.1.m1.1.1.cmml">œÑ</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p2.1.m1.1b"><ci id="S2.SS4.SSS1.p2.1.m1.1.1.cmml" xref="S2.SS4.SSS1.p2.1.m1.1.1">ùúè</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p2.1.m1.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS1.p2.1.m1.1d">italic_œÑ</annotation></semantics></math>-vote strategy increased both metrics the most, where only scores provided by the two most reliable observers (A and C) were included in the majority voting.
Because there were only two observers included, majority voting was here equivalent to taking the lowest value of the two scores upon disagreement.
This approach aligned with the direction given in the scoring session to assign the lowest score if an observer is uncertain.
As shown in table¬†<a class="ltx_ref" href="#S2.T4" title="Table 4 ‚Ä£ 2.4.1 Merging the scores from multiple observers ‚Ä£ 2.4 Merging the locomotion scores ‚Ä£ 2 Materials ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_tag">4</span></a>, merging the scores largely improved the agreement compared to the individual observers and brought the reliability above the acceptable threshold when using majority voting and <math alttext="\tau" class="ltx_Math" display="inline" id="S2.SS4.SSS1.p2.2.m2.1"><semantics id="S2.SS4.SSS1.p2.2.m2.1a"><mi id="S2.SS4.SSS1.p2.2.m2.1.1" xref="S2.SS4.SSS1.p2.2.m2.1.1.cmml">œÑ</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p2.2.m2.1b"><ci id="S2.SS4.SSS1.p2.2.m2.1.1.cmml" xref="S2.SS4.SSS1.p2.2.m2.1.1">ùúè</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p2.2.m2.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS1.p2.2.m2.1d">italic_œÑ</annotation></semantics></math>-voting.
As a result, the locomotion scores were merged into one ground-truth value using the votes of observers A and C with <math alttext="\tau" class="ltx_Math" display="inline" id="S2.SS4.SSS1.p2.3.m3.1"><semantics id="S2.SS4.SSS1.p2.3.m3.1a"><mi id="S2.SS4.SSS1.p2.3.m3.1.1" xref="S2.SS4.SSS1.p2.3.m3.1.1.cmml">œÑ</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p2.3.m3.1b"><ci id="S2.SS4.SSS1.p2.3.m3.1.1.cmml" xref="S2.SS4.SSS1.p2.3.m3.1.1">ùúè</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p2.3.m3.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS1.p2.3.m3.1d">italic_œÑ</annotation></semantics></math>-voting.</p>
</div>
<figure class="ltx_table" id="S2.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T4.5.1.1" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text" id="S2.T4.6.2" style="font-size:90%;">Intra-observer reliability and agreement of the different voting strategies used for merging the scores from multiple observers.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T4.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T4.1.1.2" rowspan="2"><span class="ltx_text" id="S2.T4.1.1.2.1">Voting strategy</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T4.1.1.1" rowspan="2"><span class="ltx_text" id="S2.T4.1.1.1.1"><math alttext="\alpha" class="ltx_Math" display="inline" id="S2.T4.1.1.1.1.m1.1"><semantics id="S2.T4.1.1.1.1.m1.1a"><mi id="S2.T4.1.1.1.1.m1.1.1" xref="S2.T4.1.1.1.1.m1.1.1.cmml">Œ±</mi><annotation-xml encoding="MathML-Content" id="S2.T4.1.1.1.1.m1.1b"><ci id="S2.T4.1.1.1.1.m1.1.1.cmml" xref="S2.T4.1.1.1.1.m1.1.1">ùõº</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T4.1.1.1.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S2.T4.1.1.1.1.m1.1d">italic_Œ±</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T4.1.1.3" rowspan="2"><span class="ltx_text ltx_font_bold" id="S2.T4.1.1.3.1">PA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="5" id="S2.T4.1.1.4">Specific Agreement</th>
</tr>
<tr class="ltx_tr" id="S2.T4.3.4.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T4.3.4.1.1">1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T4.3.4.1.2">2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T4.3.4.1.3">3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T4.3.4.1.4">4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T4.3.4.1.5">5</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T4.3.5.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T4.3.5.1.1">Mean</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T4.3.5.1.2">0.614</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T4.3.5.1.3">58.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T4.3.5.1.4">68.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T4.3.5.1.5">53.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T4.3.5.1.6">22.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T4.3.5.1.7">66.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T4.3.5.1.8">0.0</td>
</tr>
<tr class="ltx_tr" id="S2.T4.3.6.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T4.3.6.2.1">Weighted vote</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T4.3.6.2.2">0.611</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T4.3.6.2.3">56.3</td>
<td class="ltx_td ltx_align_center" id="S2.T4.3.6.2.4">72.0</td>
<td class="ltx_td ltx_align_center" id="S2.T4.3.6.2.5">46.1</td>
<td class="ltx_td ltx_align_center" id="S2.T4.3.6.2.6">20.0</td>
<td class="ltx_td ltx_align_center" id="S2.T4.3.6.2.7">54.5</td>
<td class="ltx_td ltx_align_center" id="S2.T4.3.6.2.8">0.0</td>
</tr>
<tr class="ltx_tr" id="S2.T4.3.7.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T4.3.7.3.1">Majority vote</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T4.3.7.3.2">0.667</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T4.3.7.3.3">65.4</td>
<td class="ltx_td ltx_align_center" id="S2.T4.3.7.3.4">82.3</td>
<td class="ltx_td ltx_align_center" id="S2.T4.3.7.3.5">38.5</td>
<td class="ltx_td ltx_align_center" id="S2.T4.3.7.3.6">22.2</td>
<td class="ltx_td ltx_align_center" id="S2.T4.3.7.3.7">57.1</td>
<td class="ltx_td ltx_align_center" id="S2.T4.3.7.3.8">0.0</td>
</tr>
<tr class="ltx_tr" id="S2.T4.3.3">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S2.T4.3.3.2">
<math alttext="\tau" class="ltx_Math" display="inline" id="S2.T4.2.2.1.m1.1"><semantics id="S2.T4.2.2.1.m1.1a"><mi id="S2.T4.2.2.1.m1.1.1" xref="S2.T4.2.2.1.m1.1.1.cmml">œÑ</mi><annotation-xml encoding="MathML-Content" id="S2.T4.2.2.1.m1.1b"><ci id="S2.T4.2.2.1.m1.1.1.cmml" xref="S2.T4.2.2.1.m1.1.1">ùúè</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T4.2.2.1.m1.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S2.T4.2.2.1.m1.1d">italic_œÑ</annotation></semantics></math>-vote (<math alttext="\tau=0.6" class="ltx_Math" display="inline" id="S2.T4.3.3.2.m2.1"><semantics id="S2.T4.3.3.2.m2.1a"><mrow id="S2.T4.3.3.2.m2.1.1" xref="S2.T4.3.3.2.m2.1.1.cmml"><mi id="S2.T4.3.3.2.m2.1.1.2" xref="S2.T4.3.3.2.m2.1.1.2.cmml">œÑ</mi><mo id="S2.T4.3.3.2.m2.1.1.1" xref="S2.T4.3.3.2.m2.1.1.1.cmml">=</mo><mn id="S2.T4.3.3.2.m2.1.1.3" xref="S2.T4.3.3.2.m2.1.1.3.cmml">0.6</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.T4.3.3.2.m2.1b"><apply id="S2.T4.3.3.2.m2.1.1.cmml" xref="S2.T4.3.3.2.m2.1.1"><eq id="S2.T4.3.3.2.m2.1.1.1.cmml" xref="S2.T4.3.3.2.m2.1.1.1"></eq><ci id="S2.T4.3.3.2.m2.1.1.2.cmml" xref="S2.T4.3.3.2.m2.1.1.2">ùúè</ci><cn id="S2.T4.3.3.2.m2.1.1.3.cmml" type="float" xref="S2.T4.3.3.2.m2.1.1.3">0.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T4.3.3.2.m2.1c">\tau=0.6</annotation><annotation encoding="application/x-llamapun" id="S2.T4.3.3.2.m2.1d">italic_œÑ = 0.6</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S2.T4.3.3.3">0.695</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S2.T4.3.3.4">70.9</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T4.3.3.5">83.1</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T4.3.3.6">58.1</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T4.3.3.7">44.4</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T4.3.3.8">40.0</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T4.3.3.9">0.0</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="S2.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.2 </span>Merging the levels of the scale</h4>
<div class="ltx_para" id="S2.SS4.SSS2.p1">
<p class="ltx_p" id="S2.SS4.SSS2.p1.1">The majority of the studies on lameness detection focus on 2-level (normal, lame) or 3-level (normal, moderately lame, lame) locomotion scores rather than 5-level scale <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib25" title="">sprecher_lameness_1997 </a>; <a class="ltx_ref" href="#bib.bib27" title="">flower_effect_2006 </a></cite>.
There are two primary motivations for resorting to smaller resolutions in locomotion scores.
First, severely lame cows are rare to find, as most of them get treatment or are culled before they reach this level of lameness¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib29" title="">engel_assessment_2003 </a></cite>. This results in a heavily unbalanced score distribution, most scores being levels 1, 2, and 3. It is then challenging to train a classifier on unbalanced datasets, especially when little examples are available for some classes.
Second, visual locomotion scoring is subjective and often yields low intra- and inter-observer agreement and reliability measures.
<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib23" title="">schlageter-tello_effect_2014 </a></cite> studied the effects of merging the levels of the locomotion scoring scale and showed that while the agreement and reliability measures were shown to be low for 5-level scales, they only exceeded the acceptable threshold for 2-level scales.
We then followed the same practice as¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib23" title="">schlageter-tello_effect_2014 </a></cite>, and merged our 5-level scale to a 2-level (i.e., binary) scale, where level 1 indicates a <span class="ltx_text ltx_font_italic" id="S2.SS4.SSS2.p1.1.1">normal</span> gait, and levels 2,3,4 and 5 indicate a <span class="ltx_text ltx_font_italic" id="S2.SS4.SSS2.p1.1.2">lame</span> gait.
The levels of the scale were merged into a binary scale <span class="ltx_text ltx_font_italic" id="S2.SS4.SSS2.p1.1.3">after</span> merging the scores from the multiple observers.
This resulted in an intra-observer agreement of 80%, and reliability of 0.590.
Note that reliability metrics such as Krippendorff‚Äôs <math alttext="\alpha" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p1.1.m1.1"><semantics id="S2.SS4.SSS2.p1.1.m1.1a"><mi id="S2.SS4.SSS2.p1.1.m1.1.1" xref="S2.SS4.SSS2.p1.1.m1.1.1.cmml">Œ±</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.1.m1.1b"><ci id="S2.SS4.SSS2.p1.1.m1.1.1.cmml" xref="S2.SS4.SSS2.p1.1.m1.1.1">ùõº</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p1.1.m1.1d">italic_Œ±</annotation></semantics></math> can decrease when the scoring scale is smaller because the chance of agreement is larger.</p>
</div>
<span class="ltx_ERROR undefined" id="S2.SS4.SSS2.1">\@fb@secFB</span>
</section>
</section>
<section class="ltx_subsection" id="S2.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span>Overview of the Materials</h3>
<div class="ltx_para" id="S2.SS5.p1">
<p class="ltx_p" id="S2.SS5.p1.1">To summarize, the data used for this study consisted of 272 videos of walking cows, with 98 unique cows.
For each video, there was one binary ground-truth label or locomotion score. In total, 143 videos were labeled as <span class="ltx_text ltx_font_italic" id="S2.SS5.p1.1.1">normal</span>, and 129 videos were labeled as <span class="ltx_text ltx_font_italic" id="S2.SS5.p1.1.2">lame</span>.
</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methods</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Our methodology consisted of three main parts: pose estimation, gait features extraction, and gait classification. These parts are described in detail in the following subsections, and a graphical summary of the methods is provided in Figure¬†<a class="ltx_ref" href="#S3.F1" title="Figure 1 ‚Ä£ 3 Methods ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F1">
<br class="ltx_break ltx_centering"/><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="645" id="S3.F1.g1" src="x1.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S3.F1.3.2" style="font-size:90%;">Summary of the video processing procedure.</span></figcaption>
</figure><span class="ltx_ERROR undefined" id="S3.1">\@fb@secFB</span>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Pose estimation</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Pose estimation models can be used to predict the position of keypoints (body parts) in images and videos without requiring physical markers.
T-LEAP is a recent, deep-learning-based, temporal pose estimation model that was trained to detect keypoints on the body of cows in videos¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib16" title="">russello_t-leap_2021 </a></cite>.
The model used sequences of successive frames to predict the coordinate of the keypoints, and was shown to perform better than static approaches in the presence of occlusions (such as fences).
In this study, we used T-LEAP to extract nine keypoint coordinates from the video frames (Figure¬†<a class="ltx_ref" href="#S3.F2" title="Figure 2 ‚Ä£ 3.1.2 Keypoint detection ‚Ä£ 3.1 Pose estimation ‚Ä£ 3 Methods ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_tag">2</span></a>).
In the next paragraphs, we describe the steps necessary for image cropping, pose estimation, and correction.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Detect-and-crop</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">The T-LEAP model required the input frames to be square and cropped around the cow‚Äôs body.
The cows were automatically localized in the video frames using the Faster Region-based Convolutional Neural Network (Faster R-CNN), an object-detection model that returns the coordinates of a bounding box (bbox) around each object of interest (here, cows).
We used the Faster R-CNN model (with ResNeXt-101 backbone) trained on the COCO-2017 dataset from the <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS1.p1.1.1">Detectron2</span> library¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib30" title="">wu2019detectron2 </a></cite>.
The COCO-2017 dataset contained 118K training images with annotations for 80 categories of objects, among which 8014 bounding-box annotations of cows.
The Faster R-CNN model from <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS1.p1.1.2">Detectron2</span> worked out of the box and could detect the cows in our video frames without fine-tuning.
Each frame of each video was fed to the object-detection model, which returned a list of bounding boxes, one for each detected cow.
For each frame, the bounding box was made square by extending the top and bottom coordinates to match the width while keeping the cow vertically centered.
A 100-pixel padding was added to all four sides to ensure that the body of the cow was fully visible in the cropped area.
The image was cropped to the coordinates of the extended bounding box and re-scaled to a size of <math alttext="200\times 200" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p1.1.m1.1"><semantics id="S3.SS1.SSS1.p1.1.m1.1a"><mrow id="S3.SS1.SSS1.p1.1.m1.1.1" xref="S3.SS1.SSS1.p1.1.m1.1.1.cmml"><mn id="S3.SS1.SSS1.p1.1.m1.1.1.2" xref="S3.SS1.SSS1.p1.1.m1.1.1.2.cmml">200</mn><mo id="S3.SS1.SSS1.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.SSS1.p1.1.m1.1.1.1.cmml">√ó</mo><mn id="S3.SS1.SSS1.p1.1.m1.1.1.3" xref="S3.SS1.SSS1.p1.1.m1.1.1.3.cmml">200</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.1.m1.1b"><apply id="S3.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1"><times id="S3.SS1.SSS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1.1"></times><cn id="S3.SS1.SSS1.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS1.SSS1.p1.1.m1.1.1.2">200</cn><cn id="S3.SS1.SSS1.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS1.SSS1.p1.1.m1.1.1.3">200</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.1.m1.1c">200\times 200</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p1.1.m1.1d">200 √ó 200</annotation></semantics></math> pixels.
The coordinates of the cropping bounding box were saved to transform the keypoint predictions back to the true coordinates for the video frame.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Keypoint detection</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">We trained T-LEAP to predict the location of 9 keypoints. They represented the location of the following anatomical landmarks: Nose, Forehead, Withers, Sacrum, Caudal thoracic vertebrae, and the four Hooves (Figure¬†<a class="ltx_ref" href="#S3.F2" title="Figure 2 ‚Ä£ 3.1.2 Keypoint detection ‚Ä£ 3.1 Pose estimation ‚Ä£ 3 Methods ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_tag">2</span></a>).
The location of these nine keypoints was needed for extracting the gait features described in subsection¬†<a class="ltx_ref" href="#S3.SS2" title="3.2 Gait features extraction ‚Ä£ 3 Methods ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_tag">3.2</span></a>.
T-LEAP was trained with sequences of 2 consecutive frames as input because the authors reported the best performance with T=2¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib16" title="">russello_t-leap_2021 </a></cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p2">
<p class="ltx_p" id="S3.SS1.SSS2.p2.1">A pose estimation dataset was created for training and evaluating T-LEAP, using 28 videos of unique cows randomly selected from of the 272 available videos.
The coordinates of the nine keypoints were annotated for each frame of the 28 videos and divided into 968 non-overlapping sequences of 2 frames. We refer to each set of consecutive frames as a sample.
T-LEAP was trained with a random subset of 80% of the samples (i.e., 774 training samples) and evaluated on the remaining 20% of the samples (i.e., 194 test samples).
We used the same training procedure and hyper-parameters settings as described in the original T-LEAP paper¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib16" title="">russello_t-leap_2021 </a></cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p3">
<p class="ltx_p" id="S3.SS1.SSS2.p3.2">The trained T-LEAP model was then used to predict the location of the nine keypoints on all 272 videos of walking cows, including the 28 videos used for training.
Each video frame was cropped around the body of the cow, and sequences of 2 consecutive frames were fed to the pose estimation model.
The keypoint coordinates predicted by the model were then transformed to the true coordinates of the video.
For each video, this resulted in the coordinates <math alttext="(x_{t},y_{t})" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p3.1.m1.2"><semantics id="S3.SS1.SSS2.p3.1.m1.2a"><mrow id="S3.SS1.SSS2.p3.1.m1.2.2.2" xref="S3.SS1.SSS2.p3.1.m1.2.2.3.cmml"><mo id="S3.SS1.SSS2.p3.1.m1.2.2.2.3" stretchy="false" xref="S3.SS1.SSS2.p3.1.m1.2.2.3.cmml">(</mo><msub id="S3.SS1.SSS2.p3.1.m1.1.1.1.1" xref="S3.SS1.SSS2.p3.1.m1.1.1.1.1.cmml"><mi id="S3.SS1.SSS2.p3.1.m1.1.1.1.1.2" xref="S3.SS1.SSS2.p3.1.m1.1.1.1.1.2.cmml">x</mi><mi id="S3.SS1.SSS2.p3.1.m1.1.1.1.1.3" xref="S3.SS1.SSS2.p3.1.m1.1.1.1.1.3.cmml">t</mi></msub><mo id="S3.SS1.SSS2.p3.1.m1.2.2.2.4" xref="S3.SS1.SSS2.p3.1.m1.2.2.3.cmml">,</mo><msub id="S3.SS1.SSS2.p3.1.m1.2.2.2.2" xref="S3.SS1.SSS2.p3.1.m1.2.2.2.2.cmml"><mi id="S3.SS1.SSS2.p3.1.m1.2.2.2.2.2" xref="S3.SS1.SSS2.p3.1.m1.2.2.2.2.2.cmml">y</mi><mi id="S3.SS1.SSS2.p3.1.m1.2.2.2.2.3" xref="S3.SS1.SSS2.p3.1.m1.2.2.2.2.3.cmml">t</mi></msub><mo id="S3.SS1.SSS2.p3.1.m1.2.2.2.5" stretchy="false" xref="S3.SS1.SSS2.p3.1.m1.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p3.1.m1.2b"><interval closure="open" id="S3.SS1.SSS2.p3.1.m1.2.2.3.cmml" xref="S3.SS1.SSS2.p3.1.m1.2.2.2"><apply id="S3.SS1.SSS2.p3.1.m1.1.1.1.1.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p3.1.m1.1.1.1.1.1.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p3.1.m1.1.1.1.1.2.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.1.1.1.2">ùë•</ci><ci id="S3.SS1.SSS2.p3.1.m1.1.1.1.1.3.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.1.1.1.3">ùë°</ci></apply><apply id="S3.SS1.SSS2.p3.1.m1.2.2.2.2.cmml" xref="S3.SS1.SSS2.p3.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p3.1.m1.2.2.2.2.1.cmml" xref="S3.SS1.SSS2.p3.1.m1.2.2.2.2">subscript</csymbol><ci id="S3.SS1.SSS2.p3.1.m1.2.2.2.2.2.cmml" xref="S3.SS1.SSS2.p3.1.m1.2.2.2.2.2">ùë¶</ci><ci id="S3.SS1.SSS2.p3.1.m1.2.2.2.2.3.cmml" xref="S3.SS1.SSS2.p3.1.m1.2.2.2.2.3">ùë°</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p3.1.m1.2c">(x_{t},y_{t})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p3.1.m1.2d">( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )</annotation></semantics></math> of each keypoint for each frame <math alttext="t" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p3.2.m2.1"><semantics id="S3.SS1.SSS2.p3.2.m2.1a"><mi id="S3.SS1.SSS2.p3.2.m2.1.1" xref="S3.SS1.SSS2.p3.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p3.2.m2.1b"><ci id="S3.SS1.SSS2.p3.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p3.2.m2.1.1">ùë°</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p3.2.m2.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p3.2.m2.1d">italic_t</annotation></semantics></math>.
We refer to the collection of keypoints of one video as "keypoints trajectories". In essence, these trajectories represent the motion of the anatomical landmarks localized by the pose-estimator in the 2D image plane.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="287" id="S3.F2.g1" src="x2.png" width="287"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S3.F2.3.2" style="font-size:90%;">The 9 keypoints (anatomical landmarks) as described in¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib16" title="">russello_t-leap_2021 </a></cite>. The keypoints are named as follows: 1: Left-hind hoof, 2: Right-hind hoof, 3: Left-front hoof, 4: Right-front hoof 5: Nose, 6: Forehead, 7: Withers, 8: Sacrum, 9: Caudal thoracic vertebrae.</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3 </span>Keypoint correction</h4>
<div class="ltx_para" id="S3.SS1.SSS3.p1">
<p class="ltx_p" id="S3.SS1.SSS3.p1.2">In our set of 272 videos, we identified 98 individual cows.
There were 28 videos of unique cows included in training the pose estimation model, and thus 70 cows that the pose estimation model did not see.
In their generalization experiment, the authors of T-LEAP reported a percentage of correct keypoints (PCKh@0.2) of <math alttext="93.8\%" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p1.1.m1.1"><semantics id="S3.SS1.SSS3.p1.1.m1.1a"><mrow id="S3.SS1.SSS3.p1.1.m1.1.1" xref="S3.SS1.SSS3.p1.1.m1.1.1.cmml"><mn id="S3.SS1.SSS3.p1.1.m1.1.1.2" xref="S3.SS1.SSS3.p1.1.m1.1.1.2.cmml">93.8</mn><mo id="S3.SS1.SSS3.p1.1.m1.1.1.1" xref="S3.SS1.SSS3.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.1.m1.1b"><apply id="S3.SS1.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS3.p1.1.m1.1.1"><csymbol cd="latexml" id="S3.SS1.SSS3.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS3.p1.1.m1.1.1.1">percent</csymbol><cn id="S3.SS1.SSS3.p1.1.m1.1.1.2.cmml" type="float" xref="S3.SS1.SSS3.p1.1.m1.1.1.2">93.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.1.m1.1c">93.8\%</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p1.1.m1.1d">93.8 %</annotation></semantics></math> on known cows (i.e., cows included in the training set) and a performance of <math alttext="87.6\%" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p1.2.m2.1"><semantics id="S3.SS1.SSS3.p1.2.m2.1a"><mrow id="S3.SS1.SSS3.p1.2.m2.1.1" xref="S3.SS1.SSS3.p1.2.m2.1.1.cmml"><mn id="S3.SS1.SSS3.p1.2.m2.1.1.2" xref="S3.SS1.SSS3.p1.2.m2.1.1.2.cmml">87.6</mn><mo id="S3.SS1.SSS3.p1.2.m2.1.1.1" xref="S3.SS1.SSS3.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.2.m2.1b"><apply id="S3.SS1.SSS3.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS3.p1.2.m2.1.1"><csymbol cd="latexml" id="S3.SS1.SSS3.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS3.p1.2.m2.1.1.1">percent</csymbol><cn id="S3.SS1.SSS3.p1.2.m2.1.1.2.cmml" type="float" xref="S3.SS1.SSS3.p1.2.m2.1.1.2">87.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.2.m2.1c">87.6\%</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p1.2.m2.1d">87.6 %</annotation></semantics></math> on unknown cows (i.e., cows not included in the training set).
It was, therefore, expected to have errors in the predicted keypoint trajectories.
To deal with that, we developed a method for correcting the keypoints.
First, to identify and correct large outliers in the trajectories, we used a Median-Absolute-Deviation (MAD) filter with a temporal window of size 3. We then applied a Savitzky‚ÄìGolay filter¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib31" title="">savitzky1964smoothing </a></cite> (window=10, order=3) to smooth the trajectories temporally.
Figure¬†<a class="ltx_ref" href="#S3.F3" title="Figure 3 ‚Ä£ 3.1.3 Keypoint correction ‚Ä£ 3.1 Pose estimation ‚Ä£ 3 Methods ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_tag">3</span></a> shows examples of trajectories with outliers before and after applying the filters.</p>
</div>
<figure class="ltx_figure" id="S3.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S3.F2.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="467" id="S3.F2.sf1.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S3.F2.sf1.3.2" style="font-size:90%;">Normal gait, unfiltered</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S3.F2.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="467" id="S3.F2.sf2.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S3.F2.sf2.3.2" style="font-size:90%;">Normal gait, filtered</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S3.F2.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="467" id="S3.F2.sf3.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.sf3.2.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S3.F2.sf3.3.2" style="font-size:90%;">Lame gait, unfiltered</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S3.F2.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="467" id="S3.F2.sf4.g1" src="x6.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.sf4.2.1.1" style="font-size:90%;">(d)</span> </span><span class="ltx_text" id="S3.F2.sf4.3.2" style="font-size:90%;">Lame gait, filtered</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.3.2" style="font-size:90%;">Example of the keypoint trajectories extracted with T-LEAP (left), and after filtering (right) for a normal gait (top) and a lame gait (bottom).</span></figcaption>
</figure><span class="ltx_ERROR undefined" id="S3.SS1.SSS3.1">\@fb@secFB</span>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Gait features extraction</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Using the keypoint trajectories, we computed six locomotion traits that were shown to be correlated with locomotion scores¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib24" title="">schlageter-tello_relation_2015 </a></cite>, namely Back Posture Measurement (BPM), Head Bobbing Amplitude (HBA), Tracking distance (TRK), Stride Length (STL), Stance Duration (STD) and Swing Duration (SWD).
All features relied on step detection, that is, knowing when each hoof was moving (swing phase) or remained still (stance phase).
Hence, in the following paragraphs, we first describe the implementation of the step detection, followed by the implementation of the gait features.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Step detection</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">For each leg, the horizontal movement (x-coordinate) of the hoof was used to detect the stance and swing phases.
The stance phase starts when a hoof lands on the floor and ends when the hoof moves forward again.
At that moment, the swing phase starts.
The hoof continues moving forward for the whole duration of the swing phase until it lands and remains still for another stance phase.
The start and end frames of the stance phases were detected by finding when the x-coordinates of the hoof remained the same, that is, by finding plateaus of at least 10 frames where the absolute difference in x-coordinates between two frames was <math alttext="\leq" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.1.m1.1"><semantics id="S3.SS2.SSS1.p1.1.m1.1a"><mo id="S3.SS2.SSS1.p1.1.m1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.cmml">‚â§</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.1.m1.1b"><leq id="S3.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.1.m1.1c">\leq</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.1.m1.1d">‚â§</annotation></semantics></math> 10 pixels, to account for small jitters.
We define mid-swing as a frame between the liftoff and landing of the hoof, just before the hoof starts to slow down.
The mid-swing moments were detected by finding the peaks of the acceleration of the x-coordinates.
The horizontal acceleration of the hoof was computed by taking the second-order derivative of the x-coordinates and then passed through a uniform filter of size 3.
An example of the x-coordinate trajectories is shown in Figure¬†<a class="ltx_ref" href="#S3.F4" title="Figure 4 ‚Ä£ 3.2.1 Step detection ‚Ä£ 3.2 Gait features extraction ‚Ä£ 3 Methods ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_tag">4</span></a>, with the stance and mid-swing phases identified by the step detection.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="498" id="S3.F4.g1" src="x7.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S3.F4.3.2" style="font-size:90%;">Example of the step detection, using the trajectories of the x-coordinates of the hooves. The vertical lines mark the beginning and end of the stance phase. The crosses mark the peak of the swing phase.</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Step correction</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">The step detection was automatically controlled and corrected using the following procedure: for any given leg, mid-swings must happen before or after the stance phases, and the mid-swings must happen during the supporting phase of the opposite leg (left-right).
When the step detection failed to meet these requirements, this indicated that the keypoint predictions were too noisy on that hoof.
The frames with problematic steps were then removed from the keypoint trajectories, resulting in trajectories with one or several gaps.
The trajectories were then trimmed to the part with the most remaining frames.
Using this method, only four videos were found to have problematic step detection, and only one of them had less than two stance phases per leg. The latter was then discarded from the dataset, as at least two stance phases are needed to compute some of the features.
As a result, the final dataset included 271 videos.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Back posture measurement (BPM)</h4>
<div class="ltx_para" id="S3.SS2.SSS3.p1">
<p class="ltx_p" id="S3.SS2.SSS3.p1.2">To estimate the back posture, or curvature of the back, a similar approach as described in¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib6" title="">poursaberi_real-time_2010 </a></cite> was taken.
A circle was fitted through the three keypoints on the spine. The curvature of a circle can be found by taking the inverse of its radius.
The radius (<math alttext="r" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p1.1.m1.1"><semantics id="S3.SS2.SSS3.p1.1.m1.1a"><mi id="S3.SS2.SSS3.p1.1.m1.1.1" xref="S3.SS2.SSS3.p1.1.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.1.m1.1b"><ci id="S3.SS2.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1">ùëü</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.1.m1.1c">r</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p1.1.m1.1d">italic_r</annotation></semantics></math>) of the fitted circle was normalized with the head length (<math alttext="h" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p1.2.m2.1"><semantics id="S3.SS2.SSS3.p1.2.m2.1a"><mi id="S3.SS2.SSS3.p1.2.m2.1.1" xref="S3.SS2.SSS3.p1.2.m2.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.2.m2.1b"><ci id="S3.SS2.SSS3.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS3.p1.2.m2.1.1">‚Ñé</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.2.m2.1c">h</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p1.2.m2.1d">italic_h</annotation></semantics></math>) of the cow (in pixels), as the length of cows can differ.
The head length was taken as the Euclidean distance between the keypoints on the forehead and the nose.
The BPM was then calculated as follows:</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS3.p2">
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="Sx2.EGx1">
<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span class="ltx_text ltx_markedasmath" id="S3.E1.2.1.1.1">BPM</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{1}{r/h}=\frac{h}{r}" class="ltx_Math" display="inline" id="S3.E1.m2.1"><semantics id="S3.E1.m2.1a"><mrow id="S3.E1.m2.1.1" xref="S3.E1.m2.1.1.cmml"><mi id="S3.E1.m2.1.1.2" xref="S3.E1.m2.1.1.2.cmml"></mi><mo id="S3.E1.m2.1.1.3" xref="S3.E1.m2.1.1.3.cmml">=</mo><mstyle displaystyle="true" id="S3.E1.m2.1.1.4" xref="S3.E1.m2.1.1.4.cmml"><mfrac id="S3.E1.m2.1.1.4a" xref="S3.E1.m2.1.1.4.cmml"><mn id="S3.E1.m2.1.1.4.2" xref="S3.E1.m2.1.1.4.2.cmml">1</mn><mrow id="S3.E1.m2.1.1.4.3" xref="S3.E1.m2.1.1.4.3.cmml"><mi id="S3.E1.m2.1.1.4.3.2" xref="S3.E1.m2.1.1.4.3.2.cmml">r</mi><mo id="S3.E1.m2.1.1.4.3.1" xref="S3.E1.m2.1.1.4.3.1.cmml">/</mo><mi id="S3.E1.m2.1.1.4.3.3" xref="S3.E1.m2.1.1.4.3.3.cmml">h</mi></mrow></mfrac></mstyle><mo id="S3.E1.m2.1.1.5" xref="S3.E1.m2.1.1.5.cmml">=</mo><mstyle displaystyle="true" id="S3.E1.m2.1.1.6" xref="S3.E1.m2.1.1.6.cmml"><mfrac id="S3.E1.m2.1.1.6a" xref="S3.E1.m2.1.1.6.cmml"><mi id="S3.E1.m2.1.1.6.2" xref="S3.E1.m2.1.1.6.2.cmml">h</mi><mi id="S3.E1.m2.1.1.6.3" xref="S3.E1.m2.1.1.6.3.cmml">r</mi></mfrac></mstyle></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m2.1b"><apply id="S3.E1.m2.1.1.cmml" xref="S3.E1.m2.1.1"><and id="S3.E1.m2.1.1a.cmml" xref="S3.E1.m2.1.1"></and><apply id="S3.E1.m2.1.1b.cmml" xref="S3.E1.m2.1.1"><eq id="S3.E1.m2.1.1.3.cmml" xref="S3.E1.m2.1.1.3"></eq><csymbol cd="latexml" id="S3.E1.m2.1.1.2.cmml" xref="S3.E1.m2.1.1.2">absent</csymbol><apply id="S3.E1.m2.1.1.4.cmml" xref="S3.E1.m2.1.1.4"><divide id="S3.E1.m2.1.1.4.1.cmml" xref="S3.E1.m2.1.1.4"></divide><cn id="S3.E1.m2.1.1.4.2.cmml" type="integer" xref="S3.E1.m2.1.1.4.2">1</cn><apply id="S3.E1.m2.1.1.4.3.cmml" xref="S3.E1.m2.1.1.4.3"><divide id="S3.E1.m2.1.1.4.3.1.cmml" xref="S3.E1.m2.1.1.4.3.1"></divide><ci id="S3.E1.m2.1.1.4.3.2.cmml" xref="S3.E1.m2.1.1.4.3.2">ùëü</ci><ci id="S3.E1.m2.1.1.4.3.3.cmml" xref="S3.E1.m2.1.1.4.3.3">‚Ñé</ci></apply></apply></apply><apply id="S3.E1.m2.1.1c.cmml" xref="S3.E1.m2.1.1"><eq id="S3.E1.m2.1.1.5.cmml" xref="S3.E1.m2.1.1.5"></eq><share href="#S3.E1.m2.1.1.4.cmml" id="S3.E1.m2.1.1d.cmml" xref="S3.E1.m2.1.1"></share><apply id="S3.E1.m2.1.1.6.cmml" xref="S3.E1.m2.1.1.6"><divide id="S3.E1.m2.1.1.6.1.cmml" xref="S3.E1.m2.1.1.6"></divide><ci id="S3.E1.m2.1.1.6.2.cmml" xref="S3.E1.m2.1.1.6.2">‚Ñé</ci><ci id="S3.E1.m2.1.1.6.3.cmml" xref="S3.E1.m2.1.1.6.3">ùëü</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m2.1c">\displaystyle=\frac{1}{r/h}=\frac{h}{r}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m2.1d">= divide start_ARG 1 end_ARG start_ARG italic_r / italic_h end_ARG = divide start_ARG italic_h end_ARG start_ARG italic_r end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS3.p2.1">For each leg, the BPM was computed at each mid-swing phase.
If there were multiple swing phases, the median BPM value was kept for that leg.
The largest BPM over all four legs was used as the final BPM value.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.4 </span>Head bobbing amplitude (HBA)</h4>
<div class="ltx_para" id="S3.SS2.SSS4.p1">
<p class="ltx_p" id="S3.SS2.SSS4.p1.6">Head bobbing is defined as an exaggerated movement of the head when an affected limb lands and lifts from the ground¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib24" title="">schlageter-tello_relation_2015 </a>; <a class="ltx_ref" href="#bib.bib9" title="">blackie_associations_2013 </a></cite>.
Hence, in the presence of head bobbing, the head moves significantly up and down cyclically (at least once per gait cycle).
Sound subjects are expected to have a more steady head stance.
Examples of a noticeable head bob and steady head stance are shown in Figure¬†<a class="ltx_ref" href="#S3.F5" title="Figure 5 ‚Ä£ 3.2.4 Head bobbing amplitude (HBA) ‚Ä£ 3.2 Gait features extraction ‚Ä£ 3 Methods ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_tag">5</span></a>.
The amplitude of the vertical movement (y-signal) of the forehead keypoint was used as a measure of head bobbing.
The amplitude of the y-signal was computed with fast Fourier transforms¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib32" title="">cooley1965algorithm </a></cite> as follows: let <math alttext="N_{v}" class="ltx_Math" display="inline" id="S3.SS2.SSS4.p1.1.m1.1"><semantics id="S3.SS2.SSS4.p1.1.m1.1a"><msub id="S3.SS2.SSS4.p1.1.m1.1.1" xref="S3.SS2.SSS4.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS4.p1.1.m1.1.1.2" xref="S3.SS2.SSS4.p1.1.m1.1.1.2.cmml">N</mi><mi id="S3.SS2.SSS4.p1.1.m1.1.1.3" xref="S3.SS2.SSS4.p1.1.m1.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p1.1.m1.1b"><apply id="S3.SS2.SSS4.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS4.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS4.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS4.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS4.p1.1.m1.1.1.2">ùëÅ</ci><ci id="S3.SS2.SSS4.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS4.p1.1.m1.1.1.3">ùë£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p1.1.m1.1c">N_{v}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS4.p1.1.m1.1d">italic_N start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT</annotation></semantics></math> be the number of frames in a video, let <math alttext="N_{g}" class="ltx_Math" display="inline" id="S3.SS2.SSS4.p1.2.m2.1"><semantics id="S3.SS2.SSS4.p1.2.m2.1a"><msub id="S3.SS2.SSS4.p1.2.m2.1.1" xref="S3.SS2.SSS4.p1.2.m2.1.1.cmml"><mi id="S3.SS2.SSS4.p1.2.m2.1.1.2" xref="S3.SS2.SSS4.p1.2.m2.1.1.2.cmml">N</mi><mi id="S3.SS2.SSS4.p1.2.m2.1.1.3" xref="S3.SS2.SSS4.p1.2.m2.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p1.2.m2.1b"><apply id="S3.SS2.SSS4.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS4.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS4.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS4.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS4.p1.2.m2.1.1.2">ùëÅ</ci><ci id="S3.SS2.SSS4.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS4.p1.2.m2.1.1.3">ùëî</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p1.2.m2.1c">N_{g}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS4.p1.2.m2.1d">italic_N start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT</annotation></semantics></math> be the number of frames per gait cycle in a video, <math alttext="k\in[1,N_{v}]" class="ltx_Math" display="inline" id="S3.SS2.SSS4.p1.3.m3.2"><semantics id="S3.SS2.SSS4.p1.3.m3.2a"><mrow id="S3.SS2.SSS4.p1.3.m3.2.2" xref="S3.SS2.SSS4.p1.3.m3.2.2.cmml"><mi id="S3.SS2.SSS4.p1.3.m3.2.2.3" xref="S3.SS2.SSS4.p1.3.m3.2.2.3.cmml">k</mi><mo id="S3.SS2.SSS4.p1.3.m3.2.2.2" xref="S3.SS2.SSS4.p1.3.m3.2.2.2.cmml">‚àà</mo><mrow id="S3.SS2.SSS4.p1.3.m3.2.2.1.1" xref="S3.SS2.SSS4.p1.3.m3.2.2.1.2.cmml"><mo id="S3.SS2.SSS4.p1.3.m3.2.2.1.1.2" stretchy="false" xref="S3.SS2.SSS4.p1.3.m3.2.2.1.2.cmml">[</mo><mn id="S3.SS2.SSS4.p1.3.m3.1.1" xref="S3.SS2.SSS4.p1.3.m3.1.1.cmml">1</mn><mo id="S3.SS2.SSS4.p1.3.m3.2.2.1.1.3" xref="S3.SS2.SSS4.p1.3.m3.2.2.1.2.cmml">,</mo><msub id="S3.SS2.SSS4.p1.3.m3.2.2.1.1.1" xref="S3.SS2.SSS4.p1.3.m3.2.2.1.1.1.cmml"><mi id="S3.SS2.SSS4.p1.3.m3.2.2.1.1.1.2" xref="S3.SS2.SSS4.p1.3.m3.2.2.1.1.1.2.cmml">N</mi><mi id="S3.SS2.SSS4.p1.3.m3.2.2.1.1.1.3" xref="S3.SS2.SSS4.p1.3.m3.2.2.1.1.1.3.cmml">v</mi></msub><mo id="S3.SS2.SSS4.p1.3.m3.2.2.1.1.4" stretchy="false" xref="S3.SS2.SSS4.p1.3.m3.2.2.1.2.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p1.3.m3.2b"><apply id="S3.SS2.SSS4.p1.3.m3.2.2.cmml" xref="S3.SS2.SSS4.p1.3.m3.2.2"><in id="S3.SS2.SSS4.p1.3.m3.2.2.2.cmml" xref="S3.SS2.SSS4.p1.3.m3.2.2.2"></in><ci id="S3.SS2.SSS4.p1.3.m3.2.2.3.cmml" xref="S3.SS2.SSS4.p1.3.m3.2.2.3">ùëò</ci><interval closure="closed" id="S3.SS2.SSS4.p1.3.m3.2.2.1.2.cmml" xref="S3.SS2.SSS4.p1.3.m3.2.2.1.1"><cn id="S3.SS2.SSS4.p1.3.m3.1.1.cmml" type="integer" xref="S3.SS2.SSS4.p1.3.m3.1.1">1</cn><apply id="S3.SS2.SSS4.p1.3.m3.2.2.1.1.1.cmml" xref="S3.SS2.SSS4.p1.3.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS4.p1.3.m3.2.2.1.1.1.1.cmml" xref="S3.SS2.SSS4.p1.3.m3.2.2.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS4.p1.3.m3.2.2.1.1.1.2.cmml" xref="S3.SS2.SSS4.p1.3.m3.2.2.1.1.1.2">ùëÅ</ci><ci id="S3.SS2.SSS4.p1.3.m3.2.2.1.1.1.3.cmml" xref="S3.SS2.SSS4.p1.3.m3.2.2.1.1.1.3">ùë£</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p1.3.m3.2c">k\in[1,N_{v}]</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS4.p1.3.m3.2d">italic_k ‚àà [ 1 , italic_N start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ]</annotation></semantics></math> the frequency, <math alttext="X" class="ltx_Math" display="inline" id="S3.SS2.SSS4.p1.4.m4.1"><semantics id="S3.SS2.SSS4.p1.4.m4.1a"><mi id="S3.SS2.SSS4.p1.4.m4.1.1" xref="S3.SS2.SSS4.p1.4.m4.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p1.4.m4.1b"><ci id="S3.SS2.SSS4.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS4.p1.4.m4.1.1">ùëã</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p1.4.m4.1c">X</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS4.p1.4.m4.1d">italic_X</annotation></semantics></math> the Fourier transform of the signal, and <math alttext="A_{k}" class="ltx_Math" display="inline" id="S3.SS2.SSS4.p1.5.m5.1"><semantics id="S3.SS2.SSS4.p1.5.m5.1a"><msub id="S3.SS2.SSS4.p1.5.m5.1.1" xref="S3.SS2.SSS4.p1.5.m5.1.1.cmml"><mi id="S3.SS2.SSS4.p1.5.m5.1.1.2" xref="S3.SS2.SSS4.p1.5.m5.1.1.2.cmml">A</mi><mi id="S3.SS2.SSS4.p1.5.m5.1.1.3" xref="S3.SS2.SSS4.p1.5.m5.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p1.5.m5.1b"><apply id="S3.SS2.SSS4.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS4.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS4.p1.5.m5.1.1.1.cmml" xref="S3.SS2.SSS4.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.SSS4.p1.5.m5.1.1.2.cmml" xref="S3.SS2.SSS4.p1.5.m5.1.1.2">ùê¥</ci><ci id="S3.SS2.SSS4.p1.5.m5.1.1.3.cmml" xref="S3.SS2.SSS4.p1.5.m5.1.1.3">ùëò</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p1.5.m5.1c">A_{k}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS4.p1.5.m5.1d">italic_A start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> the amplitude at frequency <math alttext="k" class="ltx_Math" display="inline" id="S3.SS2.SSS4.p1.6.m6.1"><semantics id="S3.SS2.SSS4.p1.6.m6.1a"><mi id="S3.SS2.SSS4.p1.6.m6.1.1" xref="S3.SS2.SSS4.p1.6.m6.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p1.6.m6.1b"><ci id="S3.SS2.SSS4.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS4.p1.6.m6.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p1.6.m6.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS4.p1.6.m6.1d">italic_k</annotation></semantics></math>.
The value of the HBA was then assigned as the largest amplitude in a gait cycle:</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS4.p2">
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="Sx2.EGx2">
<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle A_{k}" class="ltx_Math" display="inline" id="S3.E2.m1.1"><semantics id="S3.E2.m1.1a"><msub id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><mi id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml">A</mi><mi id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2">ùê¥</ci><ci id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3">ùëò</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\displaystyle A_{k}</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.1d">italic_A start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{|X_{k}|}{N_{v}}" class="ltx_Math" display="inline" id="S3.E2.m2.1"><semantics id="S3.E2.m2.1a"><mrow id="S3.E2.m2.1.2" xref="S3.E2.m2.1.2.cmml"><mi id="S3.E2.m2.1.2.2" xref="S3.E2.m2.1.2.2.cmml"></mi><mo id="S3.E2.m2.1.2.1" xref="S3.E2.m2.1.2.1.cmml">=</mo><mstyle displaystyle="true" id="S3.E2.m2.1.1" xref="S3.E2.m2.1.1.cmml"><mfrac id="S3.E2.m2.1.1a" xref="S3.E2.m2.1.1.cmml"><mrow id="S3.E2.m2.1.1.1.1" xref="S3.E2.m2.1.1.1.2.cmml"><mo id="S3.E2.m2.1.1.1.1.2" stretchy="false" xref="S3.E2.m2.1.1.1.2.1.cmml">|</mo><msub id="S3.E2.m2.1.1.1.1.1" xref="S3.E2.m2.1.1.1.1.1.cmml"><mi id="S3.E2.m2.1.1.1.1.1.2" xref="S3.E2.m2.1.1.1.1.1.2.cmml">X</mi><mi id="S3.E2.m2.1.1.1.1.1.3" xref="S3.E2.m2.1.1.1.1.1.3.cmml">k</mi></msub><mo id="S3.E2.m2.1.1.1.1.3" stretchy="false" xref="S3.E2.m2.1.1.1.2.1.cmml">|</mo></mrow><msub id="S3.E2.m2.1.1.3" xref="S3.E2.m2.1.1.3.cmml"><mi id="S3.E2.m2.1.1.3.2" xref="S3.E2.m2.1.1.3.2.cmml">N</mi><mi id="S3.E2.m2.1.1.3.3" xref="S3.E2.m2.1.1.3.3.cmml">v</mi></msub></mfrac></mstyle></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m2.1b"><apply id="S3.E2.m2.1.2.cmml" xref="S3.E2.m2.1.2"><eq id="S3.E2.m2.1.2.1.cmml" xref="S3.E2.m2.1.2.1"></eq><csymbol cd="latexml" id="S3.E2.m2.1.2.2.cmml" xref="S3.E2.m2.1.2.2">absent</csymbol><apply id="S3.E2.m2.1.1.cmml" xref="S3.E2.m2.1.1"><divide id="S3.E2.m2.1.1.2.cmml" xref="S3.E2.m2.1.1"></divide><apply id="S3.E2.m2.1.1.1.2.cmml" xref="S3.E2.m2.1.1.1.1"><abs id="S3.E2.m2.1.1.1.2.1.cmml" xref="S3.E2.m2.1.1.1.1.2"></abs><apply id="S3.E2.m2.1.1.1.1.1.cmml" xref="S3.E2.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m2.1.1.1.1.1.1.cmml" xref="S3.E2.m2.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m2.1.1.1.1.1.2.cmml" xref="S3.E2.m2.1.1.1.1.1.2">ùëã</ci><ci id="S3.E2.m2.1.1.1.1.1.3.cmml" xref="S3.E2.m2.1.1.1.1.1.3">ùëò</ci></apply></apply><apply id="S3.E2.m2.1.1.3.cmml" xref="S3.E2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m2.1.1.3.1.cmml" xref="S3.E2.m2.1.1.3">subscript</csymbol><ci id="S3.E2.m2.1.1.3.2.cmml" xref="S3.E2.m2.1.1.3.2">ùëÅ</ci><ci id="S3.E2.m2.1.1.3.3.cmml" xref="S3.E2.m2.1.1.3.3">ùë£</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m2.1c">\displaystyle=\frac{|X_{k}|}{N_{v}}</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m2.1d">= divide start_ARG | italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | end_ARG start_ARG italic_N start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span class="ltx_text ltx_markedasmath" id="S3.E3.2.1.1.1">HBA</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\max^{N_{g}}_{k=0}(A_{k})" class="ltx_Math" display="inline" id="S3.E3.m2.2"><semantics id="S3.E3.m2.2a"><mrow id="S3.E3.m2.2.2" xref="S3.E3.m2.2.2.cmml"><mi id="S3.E3.m2.2.2.4" xref="S3.E3.m2.2.2.4.cmml"></mi><mo id="S3.E3.m2.2.2.3" xref="S3.E3.m2.2.2.3.cmml">=</mo><mrow id="S3.E3.m2.2.2.2.2" xref="S3.E3.m2.2.2.2.3.cmml"><munderover id="S3.E3.m2.1.1.1.1.1" xref="S3.E3.m2.1.1.1.1.1.cmml"><mi id="S3.E3.m2.1.1.1.1.1.2.2" xref="S3.E3.m2.1.1.1.1.1.2.2.cmml">max</mi><mrow id="S3.E3.m2.1.1.1.1.1.3" xref="S3.E3.m2.1.1.1.1.1.3.cmml"><mi id="S3.E3.m2.1.1.1.1.1.3.2" xref="S3.E3.m2.1.1.1.1.1.3.2.cmml">k</mi><mo id="S3.E3.m2.1.1.1.1.1.3.1" xref="S3.E3.m2.1.1.1.1.1.3.1.cmml">=</mo><mn id="S3.E3.m2.1.1.1.1.1.3.3" xref="S3.E3.m2.1.1.1.1.1.3.3.cmml">0</mn></mrow><msub id="S3.E3.m2.1.1.1.1.1.2.3" xref="S3.E3.m2.1.1.1.1.1.2.3.cmml"><mi id="S3.E3.m2.1.1.1.1.1.2.3.2" xref="S3.E3.m2.1.1.1.1.1.2.3.2.cmml">N</mi><mi id="S3.E3.m2.1.1.1.1.1.2.3.3" xref="S3.E3.m2.1.1.1.1.1.2.3.3.cmml">g</mi></msub></munderover><mo id="S3.E3.m2.2.2.2.2a" xref="S3.E3.m2.2.2.2.3.cmml">‚Å°</mo><mrow id="S3.E3.m2.2.2.2.2.2" xref="S3.E3.m2.2.2.2.3.cmml"><mo id="S3.E3.m2.2.2.2.2.2.2" stretchy="false" xref="S3.E3.m2.2.2.2.3.cmml">(</mo><msub id="S3.E3.m2.2.2.2.2.2.1" xref="S3.E3.m2.2.2.2.2.2.1.cmml"><mi id="S3.E3.m2.2.2.2.2.2.1.2" xref="S3.E3.m2.2.2.2.2.2.1.2.cmml">A</mi><mi id="S3.E3.m2.2.2.2.2.2.1.3" xref="S3.E3.m2.2.2.2.2.2.1.3.cmml">k</mi></msub><mo id="S3.E3.m2.2.2.2.2.2.3" stretchy="false" xref="S3.E3.m2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m2.2b"><apply id="S3.E3.m2.2.2.cmml" xref="S3.E3.m2.2.2"><eq id="S3.E3.m2.2.2.3.cmml" xref="S3.E3.m2.2.2.3"></eq><csymbol cd="latexml" id="S3.E3.m2.2.2.4.cmml" xref="S3.E3.m2.2.2.4">absent</csymbol><apply id="S3.E3.m2.2.2.2.3.cmml" xref="S3.E3.m2.2.2.2.2"><apply id="S3.E3.m2.1.1.1.1.1.cmml" xref="S3.E3.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m2.1.1.1.1.1.1.cmml" xref="S3.E3.m2.1.1.1.1.1">subscript</csymbol><apply id="S3.E3.m2.1.1.1.1.1.2.cmml" xref="S3.E3.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m2.1.1.1.1.1.2.1.cmml" xref="S3.E3.m2.1.1.1.1.1">superscript</csymbol><max id="S3.E3.m2.1.1.1.1.1.2.2.cmml" xref="S3.E3.m2.1.1.1.1.1.2.2"></max><apply id="S3.E3.m2.1.1.1.1.1.2.3.cmml" xref="S3.E3.m2.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E3.m2.1.1.1.1.1.2.3.1.cmml" xref="S3.E3.m2.1.1.1.1.1.2.3">subscript</csymbol><ci id="S3.E3.m2.1.1.1.1.1.2.3.2.cmml" xref="S3.E3.m2.1.1.1.1.1.2.3.2">ùëÅ</ci><ci id="S3.E3.m2.1.1.1.1.1.2.3.3.cmml" xref="S3.E3.m2.1.1.1.1.1.2.3.3">ùëî</ci></apply></apply><apply id="S3.E3.m2.1.1.1.1.1.3.cmml" xref="S3.E3.m2.1.1.1.1.1.3"><eq id="S3.E3.m2.1.1.1.1.1.3.1.cmml" xref="S3.E3.m2.1.1.1.1.1.3.1"></eq><ci id="S3.E3.m2.1.1.1.1.1.3.2.cmml" xref="S3.E3.m2.1.1.1.1.1.3.2">ùëò</ci><cn id="S3.E3.m2.1.1.1.1.1.3.3.cmml" type="integer" xref="S3.E3.m2.1.1.1.1.1.3.3">0</cn></apply></apply><apply id="S3.E3.m2.2.2.2.2.2.1.cmml" xref="S3.E3.m2.2.2.2.2.2.1"><csymbol cd="ambiguous" id="S3.E3.m2.2.2.2.2.2.1.1.cmml" xref="S3.E3.m2.2.2.2.2.2.1">subscript</csymbol><ci id="S3.E3.m2.2.2.2.2.2.1.2.cmml" xref="S3.E3.m2.2.2.2.2.2.1.2">ùê¥</ci><ci id="S3.E3.m2.2.2.2.2.2.1.3.cmml" xref="S3.E3.m2.2.2.2.2.2.1.3">ùëò</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m2.2c">\displaystyle=\max^{N_{g}}_{k=0}(A_{k})</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m2.2d">= roman_max start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k = 0 end_POSTSUBSCRIPT ( italic_A start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<figure class="ltx_figure" id="S3.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S3.F4.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="623" id="S3.F4.sf1.g1" src="x8.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S3.F4.sf1.3.2" style="font-size:90%;">Y-signal of the head keypoint without noticeable head bob.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S3.F4.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="623" id="S3.F4.sf2.g1" src="x9.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S3.F4.sf2.3.2" style="font-size:90%;">Y-signal of the head keypoint with noticeable head bob.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S3.F5.3.2" style="font-size:90%;">Example of y-signal with and without head bobbing.</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.5 </span>Tracking distance (TRK)</h4>
<div class="ltx_para" id="S3.SS2.SSS5.p1">
<p class="ltx_p" id="S3.SS2.SSS5.p1.7">The tracking distance is defined as the horizontal distance (x-coordinate) between the landing position of the front hoof and the subsequent landing position of the hind hoof of the same side.
If the hind hoof lands at the same location as the front hoof, it indicates no serious walking problem¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib5" title="">song_automatic_2008 </a></cite>, and the TRK value is equal (or close) to 0.
The tracking distance was measured on the left (TRK<math alttext="{}^{L}" class="ltx_Math" display="inline" id="S3.SS2.SSS5.p1.1.m1.1"><semantics id="S3.SS2.SSS5.p1.1.m1.1a"><msup id="S3.SS2.SSS5.p1.1.m1.1.1" xref="S3.SS2.SSS5.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS5.p1.1.m1.1.1a" xref="S3.SS2.SSS5.p1.1.m1.1.1.cmml"></mi><mi id="S3.SS2.SSS5.p1.1.m1.1.1.1" xref="S3.SS2.SSS5.p1.1.m1.1.1.1.cmml">L</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS5.p1.1.m1.1b"><apply id="S3.SS2.SSS5.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS5.p1.1.m1.1.1"><ci id="S3.SS2.SSS5.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS5.p1.1.m1.1.1.1">ùêø</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS5.p1.1.m1.1c">{}^{L}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS5.p1.1.m1.1d">start_FLOATSUPERSCRIPT italic_L end_FLOATSUPERSCRIPT</annotation></semantics></math>) and right (TRK<math alttext="{}^{R}" class="ltx_Math" display="inline" id="S3.SS2.SSS5.p1.2.m2.1"><semantics id="S3.SS2.SSS5.p1.2.m2.1a"><msup id="S3.SS2.SSS5.p1.2.m2.1.1" xref="S3.SS2.SSS5.p1.2.m2.1.1.cmml"><mi id="S3.SS2.SSS5.p1.2.m2.1.1a" xref="S3.SS2.SSS5.p1.2.m2.1.1.cmml"></mi><mi id="S3.SS2.SSS5.p1.2.m2.1.1.1" xref="S3.SS2.SSS5.p1.2.m2.1.1.1.cmml">R</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS5.p1.2.m2.1b"><apply id="S3.SS2.SSS5.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS5.p1.2.m2.1.1"><ci id="S3.SS2.SSS5.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS5.p1.2.m2.1.1.1">ùëÖ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS5.p1.2.m2.1c">{}^{R}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS5.p1.2.m2.1d">start_FLOATSUPERSCRIPT italic_R end_FLOATSUPERSCRIPT</annotation></semantics></math>) side of the cow and was normalized to the head length (<math alttext="h" class="ltx_Math" display="inline" id="S3.SS2.SSS5.p1.3.m3.1"><semantics id="S3.SS2.SSS5.p1.3.m3.1a"><mi id="S3.SS2.SSS5.p1.3.m3.1.1" xref="S3.SS2.SSS5.p1.3.m3.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS5.p1.3.m3.1b"><ci id="S3.SS2.SSS5.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS5.p1.3.m3.1.1">‚Ñé</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS5.p1.3.m3.1c">h</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS5.p1.3.m3.1d">italic_h</annotation></semantics></math>) as follows:
for any given side (left, right), let <math alttext="x_{f}" class="ltx_Math" display="inline" id="S3.SS2.SSS5.p1.4.m4.1"><semantics id="S3.SS2.SSS5.p1.4.m4.1a"><msub id="S3.SS2.SSS5.p1.4.m4.1.1" xref="S3.SS2.SSS5.p1.4.m4.1.1.cmml"><mi id="S3.SS2.SSS5.p1.4.m4.1.1.2" xref="S3.SS2.SSS5.p1.4.m4.1.1.2.cmml">x</mi><mi id="S3.SS2.SSS5.p1.4.m4.1.1.3" xref="S3.SS2.SSS5.p1.4.m4.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS5.p1.4.m4.1b"><apply id="S3.SS2.SSS5.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS5.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS5.p1.4.m4.1.1.1.cmml" xref="S3.SS2.SSS5.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.SSS5.p1.4.m4.1.1.2.cmml" xref="S3.SS2.SSS5.p1.4.m4.1.1.2">ùë•</ci><ci id="S3.SS2.SSS5.p1.4.m4.1.1.3.cmml" xref="S3.SS2.SSS5.p1.4.m4.1.1.3">ùëì</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS5.p1.4.m4.1c">x_{f}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS5.p1.4.m4.1d">italic_x start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="x_{h}" class="ltx_Math" display="inline" id="S3.SS2.SSS5.p1.5.m5.1"><semantics id="S3.SS2.SSS5.p1.5.m5.1a"><msub id="S3.SS2.SSS5.p1.5.m5.1.1" xref="S3.SS2.SSS5.p1.5.m5.1.1.cmml"><mi id="S3.SS2.SSS5.p1.5.m5.1.1.2" xref="S3.SS2.SSS5.p1.5.m5.1.1.2.cmml">x</mi><mi id="S3.SS2.SSS5.p1.5.m5.1.1.3" xref="S3.SS2.SSS5.p1.5.m5.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS5.p1.5.m5.1b"><apply id="S3.SS2.SSS5.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS5.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS5.p1.5.m5.1.1.1.cmml" xref="S3.SS2.SSS5.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.SSS5.p1.5.m5.1.1.2.cmml" xref="S3.SS2.SSS5.p1.5.m5.1.1.2">ùë•</ci><ci id="S3.SS2.SSS5.p1.5.m5.1.1.3.cmml" xref="S3.SS2.SSS5.p1.5.m5.1.1.3">‚Ñé</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS5.p1.5.m5.1c">x_{h}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS5.p1.5.m5.1d">italic_x start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT</annotation></semantics></math> be the x-coordinates of the front and hind hooves,
Let <math alttext="s" class="ltx_Math" display="inline" id="S3.SS2.SSS5.p1.6.m6.1"><semantics id="S3.SS2.SSS5.p1.6.m6.1a"><mi id="S3.SS2.SSS5.p1.6.m6.1.1" xref="S3.SS2.SSS5.p1.6.m6.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS5.p1.6.m6.1b"><ci id="S3.SS2.SSS5.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS5.p1.6.m6.1.1">ùë†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS5.p1.6.m6.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS5.p1.6.m6.1d">italic_s</annotation></semantics></math> be the start frame of a stance phase on the front hoof, and <math alttext="s+1" class="ltx_Math" display="inline" id="S3.SS2.SSS5.p1.7.m7.1"><semantics id="S3.SS2.SSS5.p1.7.m7.1a"><mrow id="S3.SS2.SSS5.p1.7.m7.1.1" xref="S3.SS2.SSS5.p1.7.m7.1.1.cmml"><mi id="S3.SS2.SSS5.p1.7.m7.1.1.2" xref="S3.SS2.SSS5.p1.7.m7.1.1.2.cmml">s</mi><mo id="S3.SS2.SSS5.p1.7.m7.1.1.1" xref="S3.SS2.SSS5.p1.7.m7.1.1.1.cmml">+</mo><mn id="S3.SS2.SSS5.p1.7.m7.1.1.3" xref="S3.SS2.SSS5.p1.7.m7.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS5.p1.7.m7.1b"><apply id="S3.SS2.SSS5.p1.7.m7.1.1.cmml" xref="S3.SS2.SSS5.p1.7.m7.1.1"><plus id="S3.SS2.SSS5.p1.7.m7.1.1.1.cmml" xref="S3.SS2.SSS5.p1.7.m7.1.1.1"></plus><ci id="S3.SS2.SSS5.p1.7.m7.1.1.2.cmml" xref="S3.SS2.SSS5.p1.7.m7.1.1.2">ùë†</ci><cn id="S3.SS2.SSS5.p1.7.m7.1.1.3.cmml" type="integer" xref="S3.SS2.SSS5.p1.7.m7.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS5.p1.7.m7.1c">s+1</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS5.p1.7.m7.1d">italic_s + 1</annotation></semantics></math> the start frame of the subsequent stance phase on the hind hoof.
When there was more than one value per side, the median TRK value of that side was returned.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS5.p2">
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="\text{TRK}=\frac{x_{f_{s}}-x_{h_{s+1}}}{h}" class="ltx_Math" display="block" id="S3.E4.m1.1"><semantics id="S3.E4.m1.1a"><mrow id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml"><mtext id="S3.E4.m1.1.1.2" xref="S3.E4.m1.1.1.2a.cmml">TRK</mtext><mo id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.cmml">=</mo><mfrac id="S3.E4.m1.1.1.3" xref="S3.E4.m1.1.1.3.cmml"><mrow id="S3.E4.m1.1.1.3.2" xref="S3.E4.m1.1.1.3.2.cmml"><msub id="S3.E4.m1.1.1.3.2.2" xref="S3.E4.m1.1.1.3.2.2.cmml"><mi id="S3.E4.m1.1.1.3.2.2.2" xref="S3.E4.m1.1.1.3.2.2.2.cmml">x</mi><msub id="S3.E4.m1.1.1.3.2.2.3" xref="S3.E4.m1.1.1.3.2.2.3.cmml"><mi id="S3.E4.m1.1.1.3.2.2.3.2" xref="S3.E4.m1.1.1.3.2.2.3.2.cmml">f</mi><mi id="S3.E4.m1.1.1.3.2.2.3.3" xref="S3.E4.m1.1.1.3.2.2.3.3.cmml">s</mi></msub></msub><mo id="S3.E4.m1.1.1.3.2.1" xref="S3.E4.m1.1.1.3.2.1.cmml">‚àí</mo><msub id="S3.E4.m1.1.1.3.2.3" xref="S3.E4.m1.1.1.3.2.3.cmml"><mi id="S3.E4.m1.1.1.3.2.3.2" xref="S3.E4.m1.1.1.3.2.3.2.cmml">x</mi><msub id="S3.E4.m1.1.1.3.2.3.3" xref="S3.E4.m1.1.1.3.2.3.3.cmml"><mi id="S3.E4.m1.1.1.3.2.3.3.2" xref="S3.E4.m1.1.1.3.2.3.3.2.cmml">h</mi><mrow id="S3.E4.m1.1.1.3.2.3.3.3" xref="S3.E4.m1.1.1.3.2.3.3.3.cmml"><mi id="S3.E4.m1.1.1.3.2.3.3.3.2" xref="S3.E4.m1.1.1.3.2.3.3.3.2.cmml">s</mi><mo id="S3.E4.m1.1.1.3.2.3.3.3.1" xref="S3.E4.m1.1.1.3.2.3.3.3.1.cmml">+</mo><mn id="S3.E4.m1.1.1.3.2.3.3.3.3" xref="S3.E4.m1.1.1.3.2.3.3.3.3.cmml">1</mn></mrow></msub></msub></mrow><mi id="S3.E4.m1.1.1.3.3" xref="S3.E4.m1.1.1.3.3.cmml">h</mi></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1"><eq id="S3.E4.m1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"></eq><ci id="S3.E4.m1.1.1.2a.cmml" xref="S3.E4.m1.1.1.2"><mtext id="S3.E4.m1.1.1.2.cmml" xref="S3.E4.m1.1.1.2">TRK</mtext></ci><apply id="S3.E4.m1.1.1.3.cmml" xref="S3.E4.m1.1.1.3"><divide id="S3.E4.m1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.3"></divide><apply id="S3.E4.m1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.3.2"><minus id="S3.E4.m1.1.1.3.2.1.cmml" xref="S3.E4.m1.1.1.3.2.1"></minus><apply id="S3.E4.m1.1.1.3.2.2.cmml" xref="S3.E4.m1.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.2.2.1.cmml" xref="S3.E4.m1.1.1.3.2.2">subscript</csymbol><ci id="S3.E4.m1.1.1.3.2.2.2.cmml" xref="S3.E4.m1.1.1.3.2.2.2">ùë•</ci><apply id="S3.E4.m1.1.1.3.2.2.3.cmml" xref="S3.E4.m1.1.1.3.2.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.2.2.3.1.cmml" xref="S3.E4.m1.1.1.3.2.2.3">subscript</csymbol><ci id="S3.E4.m1.1.1.3.2.2.3.2.cmml" xref="S3.E4.m1.1.1.3.2.2.3.2">ùëì</ci><ci id="S3.E4.m1.1.1.3.2.2.3.3.cmml" xref="S3.E4.m1.1.1.3.2.2.3.3">ùë†</ci></apply></apply><apply id="S3.E4.m1.1.1.3.2.3.cmml" xref="S3.E4.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.2.3.1.cmml" xref="S3.E4.m1.1.1.3.2.3">subscript</csymbol><ci id="S3.E4.m1.1.1.3.2.3.2.cmml" xref="S3.E4.m1.1.1.3.2.3.2">ùë•</ci><apply id="S3.E4.m1.1.1.3.2.3.3.cmml" xref="S3.E4.m1.1.1.3.2.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.2.3.3.1.cmml" xref="S3.E4.m1.1.1.3.2.3.3">subscript</csymbol><ci id="S3.E4.m1.1.1.3.2.3.3.2.cmml" xref="S3.E4.m1.1.1.3.2.3.3.2">‚Ñé</ci><apply id="S3.E4.m1.1.1.3.2.3.3.3.cmml" xref="S3.E4.m1.1.1.3.2.3.3.3"><plus id="S3.E4.m1.1.1.3.2.3.3.3.1.cmml" xref="S3.E4.m1.1.1.3.2.3.3.3.1"></plus><ci id="S3.E4.m1.1.1.3.2.3.3.3.2.cmml" xref="S3.E4.m1.1.1.3.2.3.3.3.2">ùë†</ci><cn id="S3.E4.m1.1.1.3.2.3.3.3.3.cmml" type="integer" xref="S3.E4.m1.1.1.3.2.3.3.3.3">1</cn></apply></apply></apply></apply><ci id="S3.E4.m1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.3.3">‚Ñé</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">\text{TRK}=\frac{x_{f_{s}}-x_{h_{s+1}}}{h}</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.1d">TRK = divide start_ARG italic_x start_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT - italic_x start_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_s + 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_ARG start_ARG italic_h end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.6 </span>Stride length difference (STL)</h4>
<div class="ltx_para" id="S3.SS2.SSS6.p1">
<p class="ltx_p" id="S3.SS2.SSS6.p1.5">The stride length is defined as the horizontal distance between two successive landings of the same hoof.
The stride length (<math alttext="l" class="ltx_Math" display="inline" id="S3.SS2.SSS6.p1.1.m1.1"><semantics id="S3.SS2.SSS6.p1.1.m1.1a"><mi id="S3.SS2.SSS6.p1.1.m1.1.1" xref="S3.SS2.SSS6.p1.1.m1.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS6.p1.1.m1.1b"><ci id="S3.SS2.SSS6.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS6.p1.1.m1.1.1">ùëô</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS6.p1.1.m1.1c">l</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS6.p1.1.m1.1d">italic_l</annotation></semantics></math>) was measured for each hoof, between each successive stance phase (<math alttext="s" class="ltx_Math" display="inline" id="S3.SS2.SSS6.p1.2.m2.1"><semantics id="S3.SS2.SSS6.p1.2.m2.1a"><mi id="S3.SS2.SSS6.p1.2.m2.1.1" xref="S3.SS2.SSS6.p1.2.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS6.p1.2.m2.1b"><ci id="S3.SS2.SSS6.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS6.p1.2.m2.1.1">ùë†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS6.p1.2.m2.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS6.p1.2.m2.1d">italic_s</annotation></semantics></math>), and normalized to the head length (<math alttext="h" class="ltx_Math" display="inline" id="S3.SS2.SSS6.p1.3.m3.1"><semantics id="S3.SS2.SSS6.p1.3.m3.1a"><mi id="S3.SS2.SSS6.p1.3.m3.1.1" xref="S3.SS2.SSS6.p1.3.m3.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS6.p1.3.m3.1b"><ci id="S3.SS2.SSS6.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS6.p1.3.m3.1.1">‚Ñé</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS6.p1.3.m3.1c">h</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS6.p1.3.m3.1d">italic_h</annotation></semantics></math>).
If there was more than one stride length per hoof, the median value was kept.
We measured the difference in stride length between the left and right sides for the hind (STL<math alttext="{}^{H}" class="ltx_Math" display="inline" id="S3.SS2.SSS6.p1.4.m4.1"><semantics id="S3.SS2.SSS6.p1.4.m4.1a"><msup id="S3.SS2.SSS6.p1.4.m4.1.1" xref="S3.SS2.SSS6.p1.4.m4.1.1.cmml"><mi id="S3.SS2.SSS6.p1.4.m4.1.1a" xref="S3.SS2.SSS6.p1.4.m4.1.1.cmml"></mi><mi id="S3.SS2.SSS6.p1.4.m4.1.1.1" xref="S3.SS2.SSS6.p1.4.m4.1.1.1.cmml">H</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS6.p1.4.m4.1b"><apply id="S3.SS2.SSS6.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS6.p1.4.m4.1.1"><ci id="S3.SS2.SSS6.p1.4.m4.1.1.1.cmml" xref="S3.SS2.SSS6.p1.4.m4.1.1.1">ùêª</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS6.p1.4.m4.1c">{}^{H}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS6.p1.4.m4.1d">start_FLOATSUPERSCRIPT italic_H end_FLOATSUPERSCRIPT</annotation></semantics></math>) and front (STL<math alttext="{}^{F}" class="ltx_Math" display="inline" id="S3.SS2.SSS6.p1.5.m5.1"><semantics id="S3.SS2.SSS6.p1.5.m5.1a"><msup id="S3.SS2.SSS6.p1.5.m5.1.1" xref="S3.SS2.SSS6.p1.5.m5.1.1.cmml"><mi id="S3.SS2.SSS6.p1.5.m5.1.1a" xref="S3.SS2.SSS6.p1.5.m5.1.1.cmml"></mi><mi id="S3.SS2.SSS6.p1.5.m5.1.1.1" xref="S3.SS2.SSS6.p1.5.m5.1.1.1.cmml">F</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS6.p1.5.m5.1b"><apply id="S3.SS2.SSS6.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS6.p1.5.m5.1.1"><ci id="S3.SS2.SSS6.p1.5.m5.1.1.1.cmml" xref="S3.SS2.SSS6.p1.5.m5.1.1.1">ùêπ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS6.p1.5.m5.1c">{}^{F}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS6.p1.5.m5.1d">start_FLOATSUPERSCRIPT italic_F end_FLOATSUPERSCRIPT</annotation></semantics></math>) legs as follows:</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS6.p2">
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="Sx2.EGx3">
<tbody id="S3.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle l_{s}" class="ltx_Math" display="inline" id="S3.E5.m1.1"><semantics id="S3.E5.m1.1a"><msub id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml"><mi id="S3.E5.m1.1.1.2" xref="S3.E5.m1.1.1.2.cmml">l</mi><mi id="S3.E5.m1.1.1.3" xref="S3.E5.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.E5.m1.1b"><apply id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.cmml" xref="S3.E5.m1.1.1">subscript</csymbol><ci id="S3.E5.m1.1.1.2.cmml" xref="S3.E5.m1.1.1.2">ùëô</ci><ci id="S3.E5.m1.1.1.3.cmml" xref="S3.E5.m1.1.1.3">ùë†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.1c">\displaystyle l_{s}</annotation><annotation encoding="application/x-llamapun" id="S3.E5.m1.1d">italic_l start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{x_{s}-x_{s-1}}{h}" class="ltx_Math" display="inline" id="S3.E5.m2.1"><semantics id="S3.E5.m2.1a"><mrow id="S3.E5.m2.1.1" xref="S3.E5.m2.1.1.cmml"><mi id="S3.E5.m2.1.1.2" xref="S3.E5.m2.1.1.2.cmml"></mi><mo id="S3.E5.m2.1.1.1" xref="S3.E5.m2.1.1.1.cmml">=</mo><mstyle displaystyle="true" id="S3.E5.m2.1.1.3" xref="S3.E5.m2.1.1.3.cmml"><mfrac id="S3.E5.m2.1.1.3a" xref="S3.E5.m2.1.1.3.cmml"><mrow id="S3.E5.m2.1.1.3.2" xref="S3.E5.m2.1.1.3.2.cmml"><msub id="S3.E5.m2.1.1.3.2.2" xref="S3.E5.m2.1.1.3.2.2.cmml"><mi id="S3.E5.m2.1.1.3.2.2.2" xref="S3.E5.m2.1.1.3.2.2.2.cmml">x</mi><mi id="S3.E5.m2.1.1.3.2.2.3" xref="S3.E5.m2.1.1.3.2.2.3.cmml">s</mi></msub><mo id="S3.E5.m2.1.1.3.2.1" xref="S3.E5.m2.1.1.3.2.1.cmml">‚àí</mo><msub id="S3.E5.m2.1.1.3.2.3" xref="S3.E5.m2.1.1.3.2.3.cmml"><mi id="S3.E5.m2.1.1.3.2.3.2" xref="S3.E5.m2.1.1.3.2.3.2.cmml">x</mi><mrow id="S3.E5.m2.1.1.3.2.3.3" xref="S3.E5.m2.1.1.3.2.3.3.cmml"><mi id="S3.E5.m2.1.1.3.2.3.3.2" xref="S3.E5.m2.1.1.3.2.3.3.2.cmml">s</mi><mo id="S3.E5.m2.1.1.3.2.3.3.1" xref="S3.E5.m2.1.1.3.2.3.3.1.cmml">‚àí</mo><mn id="S3.E5.m2.1.1.3.2.3.3.3" xref="S3.E5.m2.1.1.3.2.3.3.3.cmml">1</mn></mrow></msub></mrow><mi id="S3.E5.m2.1.1.3.3" xref="S3.E5.m2.1.1.3.3.cmml">h</mi></mfrac></mstyle></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m2.1b"><apply id="S3.E5.m2.1.1.cmml" xref="S3.E5.m2.1.1"><eq id="S3.E5.m2.1.1.1.cmml" xref="S3.E5.m2.1.1.1"></eq><csymbol cd="latexml" id="S3.E5.m2.1.1.2.cmml" xref="S3.E5.m2.1.1.2">absent</csymbol><apply id="S3.E5.m2.1.1.3.cmml" xref="S3.E5.m2.1.1.3"><divide id="S3.E5.m2.1.1.3.1.cmml" xref="S3.E5.m2.1.1.3"></divide><apply id="S3.E5.m2.1.1.3.2.cmml" xref="S3.E5.m2.1.1.3.2"><minus id="S3.E5.m2.1.1.3.2.1.cmml" xref="S3.E5.m2.1.1.3.2.1"></minus><apply id="S3.E5.m2.1.1.3.2.2.cmml" xref="S3.E5.m2.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E5.m2.1.1.3.2.2.1.cmml" xref="S3.E5.m2.1.1.3.2.2">subscript</csymbol><ci id="S3.E5.m2.1.1.3.2.2.2.cmml" xref="S3.E5.m2.1.1.3.2.2.2">ùë•</ci><ci id="S3.E5.m2.1.1.3.2.2.3.cmml" xref="S3.E5.m2.1.1.3.2.2.3">ùë†</ci></apply><apply id="S3.E5.m2.1.1.3.2.3.cmml" xref="S3.E5.m2.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E5.m2.1.1.3.2.3.1.cmml" xref="S3.E5.m2.1.1.3.2.3">subscript</csymbol><ci id="S3.E5.m2.1.1.3.2.3.2.cmml" xref="S3.E5.m2.1.1.3.2.3.2">ùë•</ci><apply id="S3.E5.m2.1.1.3.2.3.3.cmml" xref="S3.E5.m2.1.1.3.2.3.3"><minus id="S3.E5.m2.1.1.3.2.3.3.1.cmml" xref="S3.E5.m2.1.1.3.2.3.3.1"></minus><ci id="S3.E5.m2.1.1.3.2.3.3.2.cmml" xref="S3.E5.m2.1.1.3.2.3.3.2">ùë†</ci><cn id="S3.E5.m2.1.1.3.2.3.3.3.cmml" type="integer" xref="S3.E5.m2.1.1.3.2.3.3.3">1</cn></apply></apply></apply><ci id="S3.E5.m2.1.1.3.3.cmml" xref="S3.E5.m2.1.1.3.3">‚Ñé</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m2.1c">\displaystyle=\frac{x_{s}-x_{s-1}}{h}</annotation><annotation encoding="application/x-llamapun" id="S3.E5.m2.1d">= divide start_ARG italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT - italic_x start_POSTSUBSCRIPT italic_s - 1 end_POSTSUBSCRIPT end_ARG start_ARG italic_h end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
<tbody id="S3.E6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span class="ltx_text ltx_markedasmath" id="S3.E6.2.1.1.1">STL</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=|l^{\text{right}}-l^{\text{left}}|" class="ltx_Math" display="inline" id="S3.E6.m2.1"><semantics id="S3.E6.m2.1a"><mrow id="S3.E6.m2.1.1" xref="S3.E6.m2.1.1.cmml"><mi id="S3.E6.m2.1.1.3" xref="S3.E6.m2.1.1.3.cmml"></mi><mo id="S3.E6.m2.1.1.2" xref="S3.E6.m2.1.1.2.cmml">=</mo><mrow id="S3.E6.m2.1.1.1.1" xref="S3.E6.m2.1.1.1.2.cmml"><mo id="S3.E6.m2.1.1.1.1.2" stretchy="false" xref="S3.E6.m2.1.1.1.2.1.cmml">|</mo><mrow id="S3.E6.m2.1.1.1.1.1" xref="S3.E6.m2.1.1.1.1.1.cmml"><msup id="S3.E6.m2.1.1.1.1.1.2" xref="S3.E6.m2.1.1.1.1.1.2.cmml"><mi id="S3.E6.m2.1.1.1.1.1.2.2" xref="S3.E6.m2.1.1.1.1.1.2.2.cmml">l</mi><mtext id="S3.E6.m2.1.1.1.1.1.2.3" xref="S3.E6.m2.1.1.1.1.1.2.3a.cmml">right</mtext></msup><mo id="S3.E6.m2.1.1.1.1.1.1" xref="S3.E6.m2.1.1.1.1.1.1.cmml">‚àí</mo><msup id="S3.E6.m2.1.1.1.1.1.3" xref="S3.E6.m2.1.1.1.1.1.3.cmml"><mi id="S3.E6.m2.1.1.1.1.1.3.2" xref="S3.E6.m2.1.1.1.1.1.3.2.cmml">l</mi><mtext id="S3.E6.m2.1.1.1.1.1.3.3" xref="S3.E6.m2.1.1.1.1.1.3.3a.cmml">left</mtext></msup></mrow><mo id="S3.E6.m2.1.1.1.1.3" stretchy="false" xref="S3.E6.m2.1.1.1.2.1.cmml">|</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m2.1b"><apply id="S3.E6.m2.1.1.cmml" xref="S3.E6.m2.1.1"><eq id="S3.E6.m2.1.1.2.cmml" xref="S3.E6.m2.1.1.2"></eq><csymbol cd="latexml" id="S3.E6.m2.1.1.3.cmml" xref="S3.E6.m2.1.1.3">absent</csymbol><apply id="S3.E6.m2.1.1.1.2.cmml" xref="S3.E6.m2.1.1.1.1"><abs id="S3.E6.m2.1.1.1.2.1.cmml" xref="S3.E6.m2.1.1.1.1.2"></abs><apply id="S3.E6.m2.1.1.1.1.1.cmml" xref="S3.E6.m2.1.1.1.1.1"><minus id="S3.E6.m2.1.1.1.1.1.1.cmml" xref="S3.E6.m2.1.1.1.1.1.1"></minus><apply id="S3.E6.m2.1.1.1.1.1.2.cmml" xref="S3.E6.m2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m2.1.1.1.1.1.2.1.cmml" xref="S3.E6.m2.1.1.1.1.1.2">superscript</csymbol><ci id="S3.E6.m2.1.1.1.1.1.2.2.cmml" xref="S3.E6.m2.1.1.1.1.1.2.2">ùëô</ci><ci id="S3.E6.m2.1.1.1.1.1.2.3a.cmml" xref="S3.E6.m2.1.1.1.1.1.2.3"><mtext id="S3.E6.m2.1.1.1.1.1.2.3.cmml" mathsize="70%" xref="S3.E6.m2.1.1.1.1.1.2.3">right</mtext></ci></apply><apply id="S3.E6.m2.1.1.1.1.1.3.cmml" xref="S3.E6.m2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E6.m2.1.1.1.1.1.3.1.cmml" xref="S3.E6.m2.1.1.1.1.1.3">superscript</csymbol><ci id="S3.E6.m2.1.1.1.1.1.3.2.cmml" xref="S3.E6.m2.1.1.1.1.1.3.2">ùëô</ci><ci id="S3.E6.m2.1.1.1.1.1.3.3a.cmml" xref="S3.E6.m2.1.1.1.1.1.3.3"><mtext id="S3.E6.m2.1.1.1.1.1.3.3.cmml" mathsize="70%" xref="S3.E6.m2.1.1.1.1.1.3.3">left</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m2.1c">\displaystyle=|l^{\text{right}}-l^{\text{left}}|</annotation><annotation encoding="application/x-llamapun" id="S3.E6.m2.1d">= | italic_l start_POSTSUPERSCRIPT right end_POSTSUPERSCRIPT - italic_l start_POSTSUPERSCRIPT left end_POSTSUPERSCRIPT |</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS7">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.7 </span>Stance duration difference (STD)</h4>
<div class="ltx_para" id="S3.SS2.SSS7.p1">
<p class="ltx_p" id="S3.SS2.SSS7.p1.6">We define the stance duration as the number of frames between the start (<math alttext="a" class="ltx_Math" display="inline" id="S3.SS2.SSS7.p1.1.m1.1"><semantics id="S3.SS2.SSS7.p1.1.m1.1a"><mi id="S3.SS2.SSS7.p1.1.m1.1.1" xref="S3.SS2.SSS7.p1.1.m1.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS7.p1.1.m1.1b"><ci id="S3.SS2.SSS7.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS7.p1.1.m1.1.1">ùëé</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS7.p1.1.m1.1c">a</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS7.p1.1.m1.1d">italic_a</annotation></semantics></math>) and end (<math alttext="b" class="ltx_Math" display="inline" id="S3.SS2.SSS7.p1.2.m2.1"><semantics id="S3.SS2.SSS7.p1.2.m2.1a"><mi id="S3.SS2.SSS7.p1.2.m2.1.1" xref="S3.SS2.SSS7.p1.2.m2.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS7.p1.2.m2.1b"><ci id="S3.SS2.SSS7.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS7.p1.2.m2.1.1">ùëè</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS7.p1.2.m2.1c">b</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS7.p1.2.m2.1d">italic_b</annotation></semantics></math>) of each stance phase.
The stance duration (<math alttext="t" class="ltx_Math" display="inline" id="S3.SS2.SSS7.p1.3.m3.1"><semantics id="S3.SS2.SSS7.p1.3.m3.1a"><mi id="S3.SS2.SSS7.p1.3.m3.1.1" xref="S3.SS2.SSS7.p1.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS7.p1.3.m3.1b"><ci id="S3.SS2.SSS7.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS7.p1.3.m3.1.1">ùë°</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS7.p1.3.m3.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS7.p1.3.m3.1d">italic_t</annotation></semantics></math>) was measured per hoof for each stance phase (<math alttext="s" class="ltx_Math" display="inline" id="S3.SS2.SSS7.p1.4.m4.1"><semantics id="S3.SS2.SSS7.p1.4.m4.1a"><mi id="S3.SS2.SSS7.p1.4.m4.1.1" xref="S3.SS2.SSS7.p1.4.m4.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS7.p1.4.m4.1b"><ci id="S3.SS2.SSS7.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS7.p1.4.m4.1.1">ùë†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS7.p1.4.m4.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS7.p1.4.m4.1d">italic_s</annotation></semantics></math>).
If a leg had more than one stance phase, the median duration was used.
We measured the difference in duration between the left and right sides for the hind (STD<math alttext="{}^{H}" class="ltx_Math" display="inline" id="S3.SS2.SSS7.p1.5.m5.1"><semantics id="S3.SS2.SSS7.p1.5.m5.1a"><msup id="S3.SS2.SSS7.p1.5.m5.1.1" xref="S3.SS2.SSS7.p1.5.m5.1.1.cmml"><mi id="S3.SS2.SSS7.p1.5.m5.1.1a" xref="S3.SS2.SSS7.p1.5.m5.1.1.cmml"></mi><mi id="S3.SS2.SSS7.p1.5.m5.1.1.1" xref="S3.SS2.SSS7.p1.5.m5.1.1.1.cmml">H</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS7.p1.5.m5.1b"><apply id="S3.SS2.SSS7.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS7.p1.5.m5.1.1"><ci id="S3.SS2.SSS7.p1.5.m5.1.1.1.cmml" xref="S3.SS2.SSS7.p1.5.m5.1.1.1">ùêª</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS7.p1.5.m5.1c">{}^{H}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS7.p1.5.m5.1d">start_FLOATSUPERSCRIPT italic_H end_FLOATSUPERSCRIPT</annotation></semantics></math>) and front (STD<math alttext="{}^{F}" class="ltx_Math" display="inline" id="S3.SS2.SSS7.p1.6.m6.1"><semantics id="S3.SS2.SSS7.p1.6.m6.1a"><msup id="S3.SS2.SSS7.p1.6.m6.1.1" xref="S3.SS2.SSS7.p1.6.m6.1.1.cmml"><mi id="S3.SS2.SSS7.p1.6.m6.1.1a" xref="S3.SS2.SSS7.p1.6.m6.1.1.cmml"></mi><mi id="S3.SS2.SSS7.p1.6.m6.1.1.1" xref="S3.SS2.SSS7.p1.6.m6.1.1.1.cmml">F</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS7.p1.6.m6.1b"><apply id="S3.SS2.SSS7.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS7.p1.6.m6.1.1"><ci id="S3.SS2.SSS7.p1.6.m6.1.1.1.cmml" xref="S3.SS2.SSS7.p1.6.m6.1.1.1">ùêπ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS7.p1.6.m6.1c">{}^{F}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS7.p1.6.m6.1d">start_FLOATSUPERSCRIPT italic_F end_FLOATSUPERSCRIPT</annotation></semantics></math>) legs as follows:</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS7.p2">
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="Sx2.EGx4">
<tbody id="S3.E7"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle t_{s}" class="ltx_Math" display="inline" id="S3.E7.m1.1"><semantics id="S3.E7.m1.1a"><msub id="S3.E7.m1.1.1" xref="S3.E7.m1.1.1.cmml"><mi id="S3.E7.m1.1.1.2" xref="S3.E7.m1.1.1.2.cmml">t</mi><mi id="S3.E7.m1.1.1.3" xref="S3.E7.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.E7.m1.1b"><apply id="S3.E7.m1.1.1.cmml" xref="S3.E7.m1.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.cmml" xref="S3.E7.m1.1.1">subscript</csymbol><ci id="S3.E7.m1.1.1.2.cmml" xref="S3.E7.m1.1.1.2">ùë°</ci><ci id="S3.E7.m1.1.1.3.cmml" xref="S3.E7.m1.1.1.3">ùë†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m1.1c">\displaystyle t_{s}</annotation><annotation encoding="application/x-llamapun" id="S3.E7.m1.1d">italic_t start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=b_{s}-a_{s}" class="ltx_Math" display="inline" id="S3.E7.m2.1"><semantics id="S3.E7.m2.1a"><mrow id="S3.E7.m2.1.1" xref="S3.E7.m2.1.1.cmml"><mi id="S3.E7.m2.1.1.2" xref="S3.E7.m2.1.1.2.cmml"></mi><mo id="S3.E7.m2.1.1.1" xref="S3.E7.m2.1.1.1.cmml">=</mo><mrow id="S3.E7.m2.1.1.3" xref="S3.E7.m2.1.1.3.cmml"><msub id="S3.E7.m2.1.1.3.2" xref="S3.E7.m2.1.1.3.2.cmml"><mi id="S3.E7.m2.1.1.3.2.2" xref="S3.E7.m2.1.1.3.2.2.cmml">b</mi><mi id="S3.E7.m2.1.1.3.2.3" xref="S3.E7.m2.1.1.3.2.3.cmml">s</mi></msub><mo id="S3.E7.m2.1.1.3.1" xref="S3.E7.m2.1.1.3.1.cmml">‚àí</mo><msub id="S3.E7.m2.1.1.3.3" xref="S3.E7.m2.1.1.3.3.cmml"><mi id="S3.E7.m2.1.1.3.3.2" xref="S3.E7.m2.1.1.3.3.2.cmml">a</mi><mi id="S3.E7.m2.1.1.3.3.3" xref="S3.E7.m2.1.1.3.3.3.cmml">s</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m2.1b"><apply id="S3.E7.m2.1.1.cmml" xref="S3.E7.m2.1.1"><eq id="S3.E7.m2.1.1.1.cmml" xref="S3.E7.m2.1.1.1"></eq><csymbol cd="latexml" id="S3.E7.m2.1.1.2.cmml" xref="S3.E7.m2.1.1.2">absent</csymbol><apply id="S3.E7.m2.1.1.3.cmml" xref="S3.E7.m2.1.1.3"><minus id="S3.E7.m2.1.1.3.1.cmml" xref="S3.E7.m2.1.1.3.1"></minus><apply id="S3.E7.m2.1.1.3.2.cmml" xref="S3.E7.m2.1.1.3.2"><csymbol cd="ambiguous" id="S3.E7.m2.1.1.3.2.1.cmml" xref="S3.E7.m2.1.1.3.2">subscript</csymbol><ci id="S3.E7.m2.1.1.3.2.2.cmml" xref="S3.E7.m2.1.1.3.2.2">ùëè</ci><ci id="S3.E7.m2.1.1.3.2.3.cmml" xref="S3.E7.m2.1.1.3.2.3">ùë†</ci></apply><apply id="S3.E7.m2.1.1.3.3.cmml" xref="S3.E7.m2.1.1.3.3"><csymbol cd="ambiguous" id="S3.E7.m2.1.1.3.3.1.cmml" xref="S3.E7.m2.1.1.3.3">subscript</csymbol><ci id="S3.E7.m2.1.1.3.3.2.cmml" xref="S3.E7.m2.1.1.3.3.2">ùëé</ci><ci id="S3.E7.m2.1.1.3.3.3.cmml" xref="S3.E7.m2.1.1.3.3.3">ùë†</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m2.1c">\displaystyle=b_{s}-a_{s}</annotation><annotation encoding="application/x-llamapun" id="S3.E7.m2.1d">= italic_b start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT - italic_a start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
<tbody id="S3.E8"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span class="ltx_text ltx_markedasmath" id="S3.E8.2.1.1.1">STD</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=|t^{\text{right}}-t^{\text{left}}|" class="ltx_Math" display="inline" id="S3.E8.m2.1"><semantics id="S3.E8.m2.1a"><mrow id="S3.E8.m2.1.1" xref="S3.E8.m2.1.1.cmml"><mi id="S3.E8.m2.1.1.3" xref="S3.E8.m2.1.1.3.cmml"></mi><mo id="S3.E8.m2.1.1.2" xref="S3.E8.m2.1.1.2.cmml">=</mo><mrow id="S3.E8.m2.1.1.1.1" xref="S3.E8.m2.1.1.1.2.cmml"><mo id="S3.E8.m2.1.1.1.1.2" stretchy="false" xref="S3.E8.m2.1.1.1.2.1.cmml">|</mo><mrow id="S3.E8.m2.1.1.1.1.1" xref="S3.E8.m2.1.1.1.1.1.cmml"><msup id="S3.E8.m2.1.1.1.1.1.2" xref="S3.E8.m2.1.1.1.1.1.2.cmml"><mi id="S3.E8.m2.1.1.1.1.1.2.2" xref="S3.E8.m2.1.1.1.1.1.2.2.cmml">t</mi><mtext id="S3.E8.m2.1.1.1.1.1.2.3" xref="S3.E8.m2.1.1.1.1.1.2.3a.cmml">right</mtext></msup><mo id="S3.E8.m2.1.1.1.1.1.1" xref="S3.E8.m2.1.1.1.1.1.1.cmml">‚àí</mo><msup id="S3.E8.m2.1.1.1.1.1.3" xref="S3.E8.m2.1.1.1.1.1.3.cmml"><mi id="S3.E8.m2.1.1.1.1.1.3.2" xref="S3.E8.m2.1.1.1.1.1.3.2.cmml">t</mi><mtext id="S3.E8.m2.1.1.1.1.1.3.3" xref="S3.E8.m2.1.1.1.1.1.3.3a.cmml">left</mtext></msup></mrow><mo id="S3.E8.m2.1.1.1.1.3" stretchy="false" xref="S3.E8.m2.1.1.1.2.1.cmml">|</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E8.m2.1b"><apply id="S3.E8.m2.1.1.cmml" xref="S3.E8.m2.1.1"><eq id="S3.E8.m2.1.1.2.cmml" xref="S3.E8.m2.1.1.2"></eq><csymbol cd="latexml" id="S3.E8.m2.1.1.3.cmml" xref="S3.E8.m2.1.1.3">absent</csymbol><apply id="S3.E8.m2.1.1.1.2.cmml" xref="S3.E8.m2.1.1.1.1"><abs id="S3.E8.m2.1.1.1.2.1.cmml" xref="S3.E8.m2.1.1.1.1.2"></abs><apply id="S3.E8.m2.1.1.1.1.1.cmml" xref="S3.E8.m2.1.1.1.1.1"><minus id="S3.E8.m2.1.1.1.1.1.1.cmml" xref="S3.E8.m2.1.1.1.1.1.1"></minus><apply id="S3.E8.m2.1.1.1.1.1.2.cmml" xref="S3.E8.m2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E8.m2.1.1.1.1.1.2.1.cmml" xref="S3.E8.m2.1.1.1.1.1.2">superscript</csymbol><ci id="S3.E8.m2.1.1.1.1.1.2.2.cmml" xref="S3.E8.m2.1.1.1.1.1.2.2">ùë°</ci><ci id="S3.E8.m2.1.1.1.1.1.2.3a.cmml" xref="S3.E8.m2.1.1.1.1.1.2.3"><mtext id="S3.E8.m2.1.1.1.1.1.2.3.cmml" mathsize="70%" xref="S3.E8.m2.1.1.1.1.1.2.3">right</mtext></ci></apply><apply id="S3.E8.m2.1.1.1.1.1.3.cmml" xref="S3.E8.m2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E8.m2.1.1.1.1.1.3.1.cmml" xref="S3.E8.m2.1.1.1.1.1.3">superscript</csymbol><ci id="S3.E8.m2.1.1.1.1.1.3.2.cmml" xref="S3.E8.m2.1.1.1.1.1.3.2">ùë°</ci><ci id="S3.E8.m2.1.1.1.1.1.3.3a.cmml" xref="S3.E8.m2.1.1.1.1.1.3.3"><mtext id="S3.E8.m2.1.1.1.1.1.3.3.cmml" mathsize="70%" xref="S3.E8.m2.1.1.1.1.1.3.3">left</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E8.m2.1c">\displaystyle=|t^{\text{right}}-t^{\text{left}}|</annotation><annotation encoding="application/x-llamapun" id="S3.E8.m2.1d">= | italic_t start_POSTSUPERSCRIPT right end_POSTSUPERSCRIPT - italic_t start_POSTSUPERSCRIPT left end_POSTSUPERSCRIPT |</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS8">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.8 </span>Swing duration difference (SWD)</h4>
<div class="ltx_para" id="S3.SS2.SSS8.p1">
<p class="ltx_p" id="S3.SS2.SSS8.p1.6">We define the swing duration as the number of frames between the (<math alttext="a" class="ltx_Math" display="inline" id="S3.SS2.SSS8.p1.1.m1.1"><semantics id="S3.SS2.SSS8.p1.1.m1.1a"><mi id="S3.SS2.SSS8.p1.1.m1.1.1" xref="S3.SS2.SSS8.p1.1.m1.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS8.p1.1.m1.1b"><ci id="S3.SS2.SSS8.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS8.p1.1.m1.1.1">ùëé</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS8.p1.1.m1.1c">a</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS8.p1.1.m1.1d">italic_a</annotation></semantics></math>) and end (<math alttext="b" class="ltx_Math" display="inline" id="S3.SS2.SSS8.p1.2.m2.1"><semantics id="S3.SS2.SSS8.p1.2.m2.1a"><mi id="S3.SS2.SSS8.p1.2.m2.1.1" xref="S3.SS2.SSS8.p1.2.m2.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS8.p1.2.m2.1b"><ci id="S3.SS2.SSS8.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS8.p1.2.m2.1.1">ùëè</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS8.p1.2.m2.1c">b</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS8.p1.2.m2.1d">italic_b</annotation></semantics></math>) of each swing phase.
The swing duration (<math alttext="w" class="ltx_Math" display="inline" id="S3.SS2.SSS8.p1.3.m3.1"><semantics id="S3.SS2.SSS8.p1.3.m3.1a"><mi id="S3.SS2.SSS8.p1.3.m3.1.1" xref="S3.SS2.SSS8.p1.3.m3.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS8.p1.3.m3.1b"><ci id="S3.SS2.SSS8.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS8.p1.3.m3.1.1">ùë§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS8.p1.3.m3.1c">w</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS8.p1.3.m3.1d">italic_w</annotation></semantics></math>) was measured per hoof for each swing phase (<math alttext="s" class="ltx_Math" display="inline" id="S3.SS2.SSS8.p1.4.m4.1"><semantics id="S3.SS2.SSS8.p1.4.m4.1a"><mi id="S3.SS2.SSS8.p1.4.m4.1.1" xref="S3.SS2.SSS8.p1.4.m4.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS8.p1.4.m4.1b"><ci id="S3.SS2.SSS8.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS8.p1.4.m4.1.1">ùë†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS8.p1.4.m4.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS8.p1.4.m4.1d">italic_s</annotation></semantics></math>).
If a leg had more than one swing phase, the median duration was used.
We measured the difference in duration between the left and right sides for the hind (SWD<math alttext="{}^{H}" class="ltx_Math" display="inline" id="S3.SS2.SSS8.p1.5.m5.1"><semantics id="S3.SS2.SSS8.p1.5.m5.1a"><msup id="S3.SS2.SSS8.p1.5.m5.1.1" xref="S3.SS2.SSS8.p1.5.m5.1.1.cmml"><mi id="S3.SS2.SSS8.p1.5.m5.1.1a" xref="S3.SS2.SSS8.p1.5.m5.1.1.cmml"></mi><mi id="S3.SS2.SSS8.p1.5.m5.1.1.1" xref="S3.SS2.SSS8.p1.5.m5.1.1.1.cmml">H</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS8.p1.5.m5.1b"><apply id="S3.SS2.SSS8.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS8.p1.5.m5.1.1"><ci id="S3.SS2.SSS8.p1.5.m5.1.1.1.cmml" xref="S3.SS2.SSS8.p1.5.m5.1.1.1">ùêª</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS8.p1.5.m5.1c">{}^{H}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS8.p1.5.m5.1d">start_FLOATSUPERSCRIPT italic_H end_FLOATSUPERSCRIPT</annotation></semantics></math>) and front (SWD<math alttext="{}^{F}" class="ltx_Math" display="inline" id="S3.SS2.SSS8.p1.6.m6.1"><semantics id="S3.SS2.SSS8.p1.6.m6.1a"><msup id="S3.SS2.SSS8.p1.6.m6.1.1" xref="S3.SS2.SSS8.p1.6.m6.1.1.cmml"><mi id="S3.SS2.SSS8.p1.6.m6.1.1a" xref="S3.SS2.SSS8.p1.6.m6.1.1.cmml"></mi><mi id="S3.SS2.SSS8.p1.6.m6.1.1.1" xref="S3.SS2.SSS8.p1.6.m6.1.1.1.cmml">F</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS8.p1.6.m6.1b"><apply id="S3.SS2.SSS8.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS8.p1.6.m6.1.1"><ci id="S3.SS2.SSS8.p1.6.m6.1.1.1.cmml" xref="S3.SS2.SSS8.p1.6.m6.1.1.1">ùêπ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS8.p1.6.m6.1c">{}^{F}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS8.p1.6.m6.1d">start_FLOATSUPERSCRIPT italic_F end_FLOATSUPERSCRIPT</annotation></semantics></math>) legs.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS8.p2">
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="Sx2.EGx5">
<tbody id="S3.E9"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle w_{s}" class="ltx_Math" display="inline" id="S3.E9.m1.1"><semantics id="S3.E9.m1.1a"><msub id="S3.E9.m1.1.1" xref="S3.E9.m1.1.1.cmml"><mi id="S3.E9.m1.1.1.2" xref="S3.E9.m1.1.1.2.cmml">w</mi><mi id="S3.E9.m1.1.1.3" xref="S3.E9.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.E9.m1.1b"><apply id="S3.E9.m1.1.1.cmml" xref="S3.E9.m1.1.1"><csymbol cd="ambiguous" id="S3.E9.m1.1.1.1.cmml" xref="S3.E9.m1.1.1">subscript</csymbol><ci id="S3.E9.m1.1.1.2.cmml" xref="S3.E9.m1.1.1.2">ùë§</ci><ci id="S3.E9.m1.1.1.3.cmml" xref="S3.E9.m1.1.1.3">ùë†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E9.m1.1c">\displaystyle w_{s}</annotation><annotation encoding="application/x-llamapun" id="S3.E9.m1.1d">italic_w start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=b_{s}-a_{s}" class="ltx_Math" display="inline" id="S3.E9.m2.1"><semantics id="S3.E9.m2.1a"><mrow id="S3.E9.m2.1.1" xref="S3.E9.m2.1.1.cmml"><mi id="S3.E9.m2.1.1.2" xref="S3.E9.m2.1.1.2.cmml"></mi><mo id="S3.E9.m2.1.1.1" xref="S3.E9.m2.1.1.1.cmml">=</mo><mrow id="S3.E9.m2.1.1.3" xref="S3.E9.m2.1.1.3.cmml"><msub id="S3.E9.m2.1.1.3.2" xref="S3.E9.m2.1.1.3.2.cmml"><mi id="S3.E9.m2.1.1.3.2.2" xref="S3.E9.m2.1.1.3.2.2.cmml">b</mi><mi id="S3.E9.m2.1.1.3.2.3" xref="S3.E9.m2.1.1.3.2.3.cmml">s</mi></msub><mo id="S3.E9.m2.1.1.3.1" xref="S3.E9.m2.1.1.3.1.cmml">‚àí</mo><msub id="S3.E9.m2.1.1.3.3" xref="S3.E9.m2.1.1.3.3.cmml"><mi id="S3.E9.m2.1.1.3.3.2" xref="S3.E9.m2.1.1.3.3.2.cmml">a</mi><mi id="S3.E9.m2.1.1.3.3.3" xref="S3.E9.m2.1.1.3.3.3.cmml">s</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E9.m2.1b"><apply id="S3.E9.m2.1.1.cmml" xref="S3.E9.m2.1.1"><eq id="S3.E9.m2.1.1.1.cmml" xref="S3.E9.m2.1.1.1"></eq><csymbol cd="latexml" id="S3.E9.m2.1.1.2.cmml" xref="S3.E9.m2.1.1.2">absent</csymbol><apply id="S3.E9.m2.1.1.3.cmml" xref="S3.E9.m2.1.1.3"><minus id="S3.E9.m2.1.1.3.1.cmml" xref="S3.E9.m2.1.1.3.1"></minus><apply id="S3.E9.m2.1.1.3.2.cmml" xref="S3.E9.m2.1.1.3.2"><csymbol cd="ambiguous" id="S3.E9.m2.1.1.3.2.1.cmml" xref="S3.E9.m2.1.1.3.2">subscript</csymbol><ci id="S3.E9.m2.1.1.3.2.2.cmml" xref="S3.E9.m2.1.1.3.2.2">ùëè</ci><ci id="S3.E9.m2.1.1.3.2.3.cmml" xref="S3.E9.m2.1.1.3.2.3">ùë†</ci></apply><apply id="S3.E9.m2.1.1.3.3.cmml" xref="S3.E9.m2.1.1.3.3"><csymbol cd="ambiguous" id="S3.E9.m2.1.1.3.3.1.cmml" xref="S3.E9.m2.1.1.3.3">subscript</csymbol><ci id="S3.E9.m2.1.1.3.3.2.cmml" xref="S3.E9.m2.1.1.3.3.2">ùëé</ci><ci id="S3.E9.m2.1.1.3.3.3.cmml" xref="S3.E9.m2.1.1.3.3.3">ùë†</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E9.m2.1c">\displaystyle=b_{s}-a_{s}</annotation><annotation encoding="application/x-llamapun" id="S3.E9.m2.1d">= italic_b start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT - italic_a start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
<tbody id="S3.E10"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span class="ltx_text ltx_markedasmath" id="S3.E10.2.1.1.1">SWD</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=|w^{\text{right}}-w^{\text{left}}|" class="ltx_Math" display="inline" id="S3.E10.m2.1"><semantics id="S3.E10.m2.1a"><mrow id="S3.E10.m2.1.1" xref="S3.E10.m2.1.1.cmml"><mi id="S3.E10.m2.1.1.3" xref="S3.E10.m2.1.1.3.cmml"></mi><mo id="S3.E10.m2.1.1.2" xref="S3.E10.m2.1.1.2.cmml">=</mo><mrow id="S3.E10.m2.1.1.1.1" xref="S3.E10.m2.1.1.1.2.cmml"><mo id="S3.E10.m2.1.1.1.1.2" stretchy="false" xref="S3.E10.m2.1.1.1.2.1.cmml">|</mo><mrow id="S3.E10.m2.1.1.1.1.1" xref="S3.E10.m2.1.1.1.1.1.cmml"><msup id="S3.E10.m2.1.1.1.1.1.2" xref="S3.E10.m2.1.1.1.1.1.2.cmml"><mi id="S3.E10.m2.1.1.1.1.1.2.2" xref="S3.E10.m2.1.1.1.1.1.2.2.cmml">w</mi><mtext id="S3.E10.m2.1.1.1.1.1.2.3" xref="S3.E10.m2.1.1.1.1.1.2.3a.cmml">right</mtext></msup><mo id="S3.E10.m2.1.1.1.1.1.1" xref="S3.E10.m2.1.1.1.1.1.1.cmml">‚àí</mo><msup id="S3.E10.m2.1.1.1.1.1.3" xref="S3.E10.m2.1.1.1.1.1.3.cmml"><mi id="S3.E10.m2.1.1.1.1.1.3.2" xref="S3.E10.m2.1.1.1.1.1.3.2.cmml">w</mi><mtext id="S3.E10.m2.1.1.1.1.1.3.3" xref="S3.E10.m2.1.1.1.1.1.3.3a.cmml">left</mtext></msup></mrow><mo id="S3.E10.m2.1.1.1.1.3" stretchy="false" xref="S3.E10.m2.1.1.1.2.1.cmml">|</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E10.m2.1b"><apply id="S3.E10.m2.1.1.cmml" xref="S3.E10.m2.1.1"><eq id="S3.E10.m2.1.1.2.cmml" xref="S3.E10.m2.1.1.2"></eq><csymbol cd="latexml" id="S3.E10.m2.1.1.3.cmml" xref="S3.E10.m2.1.1.3">absent</csymbol><apply id="S3.E10.m2.1.1.1.2.cmml" xref="S3.E10.m2.1.1.1.1"><abs id="S3.E10.m2.1.1.1.2.1.cmml" xref="S3.E10.m2.1.1.1.1.2"></abs><apply id="S3.E10.m2.1.1.1.1.1.cmml" xref="S3.E10.m2.1.1.1.1.1"><minus id="S3.E10.m2.1.1.1.1.1.1.cmml" xref="S3.E10.m2.1.1.1.1.1.1"></minus><apply id="S3.E10.m2.1.1.1.1.1.2.cmml" xref="S3.E10.m2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E10.m2.1.1.1.1.1.2.1.cmml" xref="S3.E10.m2.1.1.1.1.1.2">superscript</csymbol><ci id="S3.E10.m2.1.1.1.1.1.2.2.cmml" xref="S3.E10.m2.1.1.1.1.1.2.2">ùë§</ci><ci id="S3.E10.m2.1.1.1.1.1.2.3a.cmml" xref="S3.E10.m2.1.1.1.1.1.2.3"><mtext id="S3.E10.m2.1.1.1.1.1.2.3.cmml" mathsize="70%" xref="S3.E10.m2.1.1.1.1.1.2.3">right</mtext></ci></apply><apply id="S3.E10.m2.1.1.1.1.1.3.cmml" xref="S3.E10.m2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E10.m2.1.1.1.1.1.3.1.cmml" xref="S3.E10.m2.1.1.1.1.1.3">superscript</csymbol><ci id="S3.E10.m2.1.1.1.1.1.3.2.cmml" xref="S3.E10.m2.1.1.1.1.1.3.2">ùë§</ci><ci id="S3.E10.m2.1.1.1.1.1.3.3a.cmml" xref="S3.E10.m2.1.1.1.1.1.3.3"><mtext id="S3.E10.m2.1.1.1.1.1.3.3.cmml" mathsize="70%" xref="S3.E10.m2.1.1.1.1.1.3.3">left</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E10.m2.1c">\displaystyle=|w^{\text{right}}-w^{\text{left}}|</annotation><annotation encoding="application/x-llamapun" id="S3.E10.m2.1d">= | italic_w start_POSTSUPERSCRIPT right end_POSTSUPERSCRIPT - italic_w start_POSTSUPERSCRIPT left end_POSTSUPERSCRIPT |</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(10)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS2.SSS8.p3">
<p class="ltx_p" id="S3.SS2.SSS8.p3.1">A summary of the features extracted is listed in Table¬†<a class="ltx_ref" href="#S3.T5" title="Table 5 ‚Ä£ 3.2.8 Swing duration difference (SWD) ‚Ä£ 3.2 Gait features extraction ‚Ä£ 3 Methods ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_tag">5</span></a>, and Figure¬†<a class="ltx_ref" href="#S3.F6" title="Figure 6 ‚Ä£ 3.2.8 Swing duration difference (SWD) ‚Ä£ 3.2 Gait features extraction ‚Ä£ 3 Methods ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_tag">6</span></a> presents the distribution of the values of each feature per lameness class.</p>
</div>
<figure class="ltx_table" id="S3.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T5.10.1.1" style="font-size:90%;">Table 5</span>: </span><span class="ltx_text" id="S3.T5.11.2" style="font-size:90%;">List of the features extracted from the keypoint trajectories.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T5.8">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T5.8.9.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T5.8.9.1.1"><span class="ltx_text ltx_font_bold" id="S3.T5.8.9.1.1.1">Feature</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T5.8.9.1.2"><span class="ltx_text ltx_font_bold" id="S3.T5.8.9.1.2.1">Description</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T5.8.10.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T5.8.10.1.1">BPM</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T5.8.10.1.2">Back posture measurement</td>
</tr>
<tr class="ltx_tr" id="S3.T5.8.11.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T5.8.11.2.1">HBA</th>
<td class="ltx_td ltx_align_left" id="S3.T5.8.11.2.2">Head bobbing amplitude</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T5.1.1.1">TRK<math alttext="{}^{L}" class="ltx_Math" display="inline" id="S3.T5.1.1.1.m1.1"><semantics id="S3.T5.1.1.1.m1.1a"><msup id="S3.T5.1.1.1.m1.1.1" xref="S3.T5.1.1.1.m1.1.1.cmml"><mi id="S3.T5.1.1.1.m1.1.1a" xref="S3.T5.1.1.1.m1.1.1.cmml"></mi><mi id="S3.T5.1.1.1.m1.1.1.1" xref="S3.T5.1.1.1.m1.1.1.1.cmml">L</mi></msup><annotation-xml encoding="MathML-Content" id="S3.T5.1.1.1.m1.1b"><apply id="S3.T5.1.1.1.m1.1.1.cmml" xref="S3.T5.1.1.1.m1.1.1"><ci id="S3.T5.1.1.1.m1.1.1.1.cmml" xref="S3.T5.1.1.1.m1.1.1.1">ùêø</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.1.1.1.m1.1c">{}^{L}</annotation><annotation encoding="application/x-llamapun" id="S3.T5.1.1.1.m1.1d">start_FLOATSUPERSCRIPT italic_L end_FLOATSUPERSCRIPT</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left" id="S3.T5.1.1.2">Tracking distance on the left side</td>
</tr>
<tr class="ltx_tr" id="S3.T5.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T5.2.2.1">TRK<math alttext="{}^{R}" class="ltx_Math" display="inline" id="S3.T5.2.2.1.m1.1"><semantics id="S3.T5.2.2.1.m1.1a"><msup id="S3.T5.2.2.1.m1.1.1" xref="S3.T5.2.2.1.m1.1.1.cmml"><mi id="S3.T5.2.2.1.m1.1.1a" xref="S3.T5.2.2.1.m1.1.1.cmml"></mi><mi id="S3.T5.2.2.1.m1.1.1.1" xref="S3.T5.2.2.1.m1.1.1.1.cmml">R</mi></msup><annotation-xml encoding="MathML-Content" id="S3.T5.2.2.1.m1.1b"><apply id="S3.T5.2.2.1.m1.1.1.cmml" xref="S3.T5.2.2.1.m1.1.1"><ci id="S3.T5.2.2.1.m1.1.1.1.cmml" xref="S3.T5.2.2.1.m1.1.1.1">ùëÖ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.2.2.1.m1.1c">{}^{R}</annotation><annotation encoding="application/x-llamapun" id="S3.T5.2.2.1.m1.1d">start_FLOATSUPERSCRIPT italic_R end_FLOATSUPERSCRIPT</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left" id="S3.T5.2.2.2">Tracking distance on the right side</td>
</tr>
<tr class="ltx_tr" id="S3.T5.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T5.3.3.1">STL<math alttext="{}^{F}" class="ltx_Math" display="inline" id="S3.T5.3.3.1.m1.1"><semantics id="S3.T5.3.3.1.m1.1a"><msup id="S3.T5.3.3.1.m1.1.1" xref="S3.T5.3.3.1.m1.1.1.cmml"><mi id="S3.T5.3.3.1.m1.1.1a" xref="S3.T5.3.3.1.m1.1.1.cmml"></mi><mi id="S3.T5.3.3.1.m1.1.1.1" xref="S3.T5.3.3.1.m1.1.1.1.cmml">F</mi></msup><annotation-xml encoding="MathML-Content" id="S3.T5.3.3.1.m1.1b"><apply id="S3.T5.3.3.1.m1.1.1.cmml" xref="S3.T5.3.3.1.m1.1.1"><ci id="S3.T5.3.3.1.m1.1.1.1.cmml" xref="S3.T5.3.3.1.m1.1.1.1">ùêπ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.3.3.1.m1.1c">{}^{F}</annotation><annotation encoding="application/x-llamapun" id="S3.T5.3.3.1.m1.1d">start_FLOATSUPERSCRIPT italic_F end_FLOATSUPERSCRIPT</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left" id="S3.T5.3.3.2">Stride length difference between left- and right-front hooves</td>
</tr>
<tr class="ltx_tr" id="S3.T5.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T5.4.4.1">STL<math alttext="{}^{H}" class="ltx_Math" display="inline" id="S3.T5.4.4.1.m1.1"><semantics id="S3.T5.4.4.1.m1.1a"><msup id="S3.T5.4.4.1.m1.1.1" xref="S3.T5.4.4.1.m1.1.1.cmml"><mi id="S3.T5.4.4.1.m1.1.1a" xref="S3.T5.4.4.1.m1.1.1.cmml"></mi><mi id="S3.T5.4.4.1.m1.1.1.1" xref="S3.T5.4.4.1.m1.1.1.1.cmml">H</mi></msup><annotation-xml encoding="MathML-Content" id="S3.T5.4.4.1.m1.1b"><apply id="S3.T5.4.4.1.m1.1.1.cmml" xref="S3.T5.4.4.1.m1.1.1"><ci id="S3.T5.4.4.1.m1.1.1.1.cmml" xref="S3.T5.4.4.1.m1.1.1.1">ùêª</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.4.4.1.m1.1c">{}^{H}</annotation><annotation encoding="application/x-llamapun" id="S3.T5.4.4.1.m1.1d">start_FLOATSUPERSCRIPT italic_H end_FLOATSUPERSCRIPT</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left" id="S3.T5.4.4.2">Stride length difference between left- and right-hind hooves</td>
</tr>
<tr class="ltx_tr" id="S3.T5.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T5.5.5.1">STD<math alttext="{}^{F}" class="ltx_Math" display="inline" id="S3.T5.5.5.1.m1.1"><semantics id="S3.T5.5.5.1.m1.1a"><msup id="S3.T5.5.5.1.m1.1.1" xref="S3.T5.5.5.1.m1.1.1.cmml"><mi id="S3.T5.5.5.1.m1.1.1a" xref="S3.T5.5.5.1.m1.1.1.cmml"></mi><mi id="S3.T5.5.5.1.m1.1.1.1" xref="S3.T5.5.5.1.m1.1.1.1.cmml">F</mi></msup><annotation-xml encoding="MathML-Content" id="S3.T5.5.5.1.m1.1b"><apply id="S3.T5.5.5.1.m1.1.1.cmml" xref="S3.T5.5.5.1.m1.1.1"><ci id="S3.T5.5.5.1.m1.1.1.1.cmml" xref="S3.T5.5.5.1.m1.1.1.1">ùêπ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.5.5.1.m1.1c">{}^{F}</annotation><annotation encoding="application/x-llamapun" id="S3.T5.5.5.1.m1.1d">start_FLOATSUPERSCRIPT italic_F end_FLOATSUPERSCRIPT</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left" id="S3.T5.5.5.2">Stance duration difference between left- and right-front hooves</td>
</tr>
<tr class="ltx_tr" id="S3.T5.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T5.6.6.1">STD<math alttext="{}^{H}" class="ltx_Math" display="inline" id="S3.T5.6.6.1.m1.1"><semantics id="S3.T5.6.6.1.m1.1a"><msup id="S3.T5.6.6.1.m1.1.1" xref="S3.T5.6.6.1.m1.1.1.cmml"><mi id="S3.T5.6.6.1.m1.1.1a" xref="S3.T5.6.6.1.m1.1.1.cmml"></mi><mi id="S3.T5.6.6.1.m1.1.1.1" xref="S3.T5.6.6.1.m1.1.1.1.cmml">H</mi></msup><annotation-xml encoding="MathML-Content" id="S3.T5.6.6.1.m1.1b"><apply id="S3.T5.6.6.1.m1.1.1.cmml" xref="S3.T5.6.6.1.m1.1.1"><ci id="S3.T5.6.6.1.m1.1.1.1.cmml" xref="S3.T5.6.6.1.m1.1.1.1">ùêª</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.6.6.1.m1.1c">{}^{H}</annotation><annotation encoding="application/x-llamapun" id="S3.T5.6.6.1.m1.1d">start_FLOATSUPERSCRIPT italic_H end_FLOATSUPERSCRIPT</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left" id="S3.T5.6.6.2">Stance duration difference between left- and right-hind hooves</td>
</tr>
<tr class="ltx_tr" id="S3.T5.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T5.7.7.1">SWD<math alttext="{}^{F}" class="ltx_Math" display="inline" id="S3.T5.7.7.1.m1.1"><semantics id="S3.T5.7.7.1.m1.1a"><msup id="S3.T5.7.7.1.m1.1.1" xref="S3.T5.7.7.1.m1.1.1.cmml"><mi id="S3.T5.7.7.1.m1.1.1a" xref="S3.T5.7.7.1.m1.1.1.cmml"></mi><mi id="S3.T5.7.7.1.m1.1.1.1" xref="S3.T5.7.7.1.m1.1.1.1.cmml">F</mi></msup><annotation-xml encoding="MathML-Content" id="S3.T5.7.7.1.m1.1b"><apply id="S3.T5.7.7.1.m1.1.1.cmml" xref="S3.T5.7.7.1.m1.1.1"><ci id="S3.T5.7.7.1.m1.1.1.1.cmml" xref="S3.T5.7.7.1.m1.1.1.1">ùêπ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.7.7.1.m1.1c">{}^{F}</annotation><annotation encoding="application/x-llamapun" id="S3.T5.7.7.1.m1.1d">start_FLOATSUPERSCRIPT italic_F end_FLOATSUPERSCRIPT</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left" id="S3.T5.7.7.2">Swing duration difference between left- and right-front hooves</td>
</tr>
<tr class="ltx_tr" id="S3.T5.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T5.8.8.1">SWD<math alttext="{}^{H}" class="ltx_Math" display="inline" id="S3.T5.8.8.1.m1.1"><semantics id="S3.T5.8.8.1.m1.1a"><msup id="S3.T5.8.8.1.m1.1.1" xref="S3.T5.8.8.1.m1.1.1.cmml"><mi id="S3.T5.8.8.1.m1.1.1a" xref="S3.T5.8.8.1.m1.1.1.cmml"></mi><mi id="S3.T5.8.8.1.m1.1.1.1" xref="S3.T5.8.8.1.m1.1.1.1.cmml">H</mi></msup><annotation-xml encoding="MathML-Content" id="S3.T5.8.8.1.m1.1b"><apply id="S3.T5.8.8.1.m1.1.1.cmml" xref="S3.T5.8.8.1.m1.1.1"><ci id="S3.T5.8.8.1.m1.1.1.1.cmml" xref="S3.T5.8.8.1.m1.1.1.1">ùêª</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.8.8.1.m1.1c">{}^{H}</annotation><annotation encoding="application/x-llamapun" id="S3.T5.8.8.1.m1.1d">start_FLOATSUPERSCRIPT italic_H end_FLOATSUPERSCRIPT</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T5.8.8.2">Swing duration difference between left- and right-hind hooves</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="968" id="S3.F6.g1" src="x10.png" width="605"/>
<br class="ltx_break ltx_centering"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F6.2.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S3.F6.3.2" style="font-size:90%;">Distribution of the features per lameness score, where 0 corresponds to healthy, and 1 to lame.</span></figcaption>
</figure><span class="ltx_ERROR undefined" id="S3.SS2.SSS8.1">\@fb@secFB</span>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Gait classification</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">The layout of our machine-learning experiments is described in the next paragraphs.
We first split the data into training and validation sets using cross-validation.
We then trained and evaluated different classifiers to score the gait using all the extracted features.
Lastly, we investigated the importance of features on classification performance.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Data preparation</h4>
<div class="ltx_para" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.1">Considering the relatively small dataset size (271 videos), the dataset was split into training and validation sets using a 5-fold cross-validation (CV) with stratified grouping.
In order to prevent data leakage, the grouping was performed on the cow IDs to ensure that, in each fold, there was no overlap of cow IDs between the training and the validation set.
Given this non-overlapping constraint, the stratification creates folds that retain, as much as possible, the same class distribution¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib33" title="">buitinck2013api </a></cite>.
To ensure a balanced class distribution during training, we applied the Synthetic Minority Oversampling Technique (SMOTE)¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib34" title="">chawla2002smote </a></cite> to the minority classes in the training sets.
SMOTE generates new training samples whose feature values are close to the other samples in the minority class.
Lastly, the features were re-scaled as machine-learning models often require the features to be on a similar scale.
The range of the features was re-scaled using Robust Scaling¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib35" title="">scikit-learn </a></cite>, which uses statistics that are robust to outliers for scaling the data.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Classification models</h4>
<div class="ltx_para" id="S3.SS3.SSS2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.p1.1">We compared the performance of the following six classifiers: Logistic Regression (LR), Random Forest (RF), Support Vector with a linear kernel (SVL) and with a radial kernel (SVR), Multi-Layer Perceptron (MLP) and Gradient Boosting Machines (GB).
These classifiers were selected as they showed good performance in previous research on lameness detection¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib8" title="">zhao_automatic_2023 </a>; <a class="ltx_ref" href="#bib.bib7" title="">zheng_cows_2023 </a>; <a class="ltx_ref" href="#bib.bib14" title="">arazo_segmentation_2022 </a></cite>.
We used a flat cross-validation approach to tune the hyper-parameters and train the models, as it is computationally less expensive than nested cross-validation, and generally results in the selection of an algorithm of similar quality to that selected via nested cross-validation¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib36" title="">wainer2021nested </a></cite>.
The hyper-parameters of the classifiers were first optimized using a random cross-validated search of 100 iterations over the 5-folds.
The classifiers were then re-trained on the 5-folds with the best set of hyper-parameters.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.3 </span>Evaluation metrics</h4>
<div class="ltx_para" id="S3.SS3.SSS3.p1">
<p class="ltx_p" id="S3.SS3.SSS3.p1.1">The performance of the classification models was evaluated with the following metrics: accuracy, F1-score, sensitivity, and specificity. The F1-score was macro-averaged; that is, the metric was calculated per class and then averaged. The macro-average is especially useful with imbalanced datasets, as all classes contribute equally to the metric.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.4 </span>Feature importance</h4>
<div class="ltx_para" id="S3.SS3.SSS4.p1">
<p class="ltx_p" id="S3.SS3.SSS4.p1.1">An additional experiment was run to investigate whether including multiple features could lead to improvements in gait scoring.
The predictive value of a feature was evaluated by measuring the feature importance, that is, how much a feature contributed to a correct classification.
To measure the feature importance, we selected the permutation importance method¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib37" title="">breiman2001random </a></cite> as it can be applied to any classifier.
The importance of features was evaluated on the best-performing classifier among the 6 classifiers that were trained with all the features.
The permutation importance method was performed as follows:
For each cross-validation fold, the model was fitted on the training dataset and evaluated on the F1-score on the validation set.
Then, a feature column from the validation set was randomly shuffled, and the model was evaluated again.
The importance score was then the difference between the F1-score on the non-shuffled and the shuffled validation data.
The permutations were repeated 100 times for each feature.
The features were then ranked in the order of their mean importance score.
To estimate whether including multiple features could lead to improvements in the gait scoring, the classifier was then retrained with the most important feature, the two most important features, and so on, gradually adding one feature in the order of their importance.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>
<span class="ltx_ERROR undefined" id="S4.1">\@fb@secFB</span>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Pose estimation</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The test results of T-LEAP are presented in Table¬†<a class="ltx_ref" href="#S4.T6" title="Table 6 ‚Ä£ 4.1 Pose estimation ‚Ä£ 4 Results ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_tag">6</span></a>. On average, there were 99.6% of correctly detected keypoints (PCKh@0.2). In other words, the Euclidian distance between the predicted keypoint and its ground truth was smaller than 20% of the head length in 99.6% of the cases. This is in line with the results presented in the original paper¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib16" title="">russello_t-leap_2021 </a></cite>, where they achieved a 99.0% detection rate on the same model with 17 keypoints.
The keypoint correction and filtering were run on all 272 videos, and the MAD filter (of window size 3) identified 0.21% of outlier keypoints, whose coordinates were then corrected to the median value of the temporal window.
Because of the lack of keypoint annotations on all videos, the keypoint correction could only be assessed qualitatively.
The trajectories of the keypoints before and after the filtering were plotted for each video and controlled visually.
The quality of the filtered trajectories was deemed balanced, in that most of the outliers could be corrected and the trajectories appeared smooth, without over-correction or flattening.
The outliers that could not be corrected sufficiently led to a wrong step detection. These steps were then discarded from trajectories, as detailed in section¬†<a class="ltx_ref" href="#S3.SS2" title="3.2 Gait features extraction ‚Ä£ 3 Methods ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_tag">3.2</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T6.2.1.1" style="font-size:90%;">Table 6</span>: </span><span class="ltx_text" id="S4.T6.3.2" style="font-size:90%;">Percentage of Correct Keypoints (PCKh@0.2) of T-LEAP on the test set.
The keypoints are named as follows: 1: Left-hind hoof, 2: Right-hind hoof, 3: Left-front hoof, 4: Right-front hoof 5: Nose, 6: Forehead, 7: Withers, 8: Sacrum, 9: Caudal thoracic vertebrae.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T6.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T6.4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T6.4.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T6.4.1.1.1.1">Keypoint</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T6.4.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T6.4.1.1.2.1">1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T6.4.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T6.4.1.1.3.1">2</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T6.4.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T6.4.1.1.4.1">3</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T6.4.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T6.4.1.1.5.1">4</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T6.4.1.1.6"><span class="ltx_text ltx_font_bold" id="S4.T6.4.1.1.6.1">5</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T6.4.1.1.7"><span class="ltx_text ltx_font_bold" id="S4.T6.4.1.1.7.1">6</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T6.4.1.1.8"><span class="ltx_text ltx_font_bold" id="S4.T6.4.1.1.8.1">7</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T6.4.1.1.9"><span class="ltx_text ltx_font_bold" id="S4.T6.4.1.1.9.1">8</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T6.4.1.1.10"><span class="ltx_text ltx_font_bold" id="S4.T6.4.1.1.10.1">9</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T6.4.1.1.11"><span class="ltx_text ltx_font_bold" id="S4.T6.4.1.1.11.1">Mean</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T6.4.2.1">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T6.4.2.1.1">PCKh@0.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.4.2.1.2">98.45</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.4.2.1.3">1</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.4.2.1.4">99.48</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.4.2.1.5">98.45</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.4.2.1.6">100</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.4.2.1.7">100</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.4.2.1.8">100</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.4.2.1.9">100</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.4.2.1.10">100</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.4.2.1.11">99.60</td>
</tr>
</tbody>
</table>
</figure><span class="ltx_ERROR undefined" id="S4.SS1.1">\@fb@secFB</span>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Gait score classification</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">The classification results of the different classifiers are listed in Table¬†<a class="ltx_ref" href="#S4.T7" title="Table 7 ‚Ä£ 4.2 Gait score classification ‚Ä£ 4 Results ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_tag">7</span></a>.
The SVM with radial kernel, Random Forests, and Gradient Boosting classifiers performed best, with an accuracy above 79%.
SVM-R had a higher specificity, while the Random Forests and Gradient Boosting had a higher sensitivity.
The logistic regression, the SVM with linear kernel, and the Multi-Layer Perceptron performed slightly worse.</p>
</div>
<figure class="ltx_table" id="S4.T7">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T7.2.1.1" style="font-size:90%;">Table 7</span>: </span><span class="ltx_text" id="S4.T7.3.2" style="font-size:90%;">Results of the classifiers using all the features. Values are expressed in %. The best results are highlighted in bold.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T7.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T7.4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T7.4.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T7.4.1.1.1.1" style="font-size:90%;">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T7.4.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T7.4.1.1.2.1" style="font-size:90%;">Accuracy</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T7.4.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T7.4.1.1.3.1" style="font-size:90%;">F1-score</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T7.4.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T7.4.1.1.4.1" style="font-size:90%;">Sensitivity</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T7.4.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T7.4.1.1.5.1" style="font-size:90%;">Specificity</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T7.4.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T7.4.2.1.1">Logistic Regression</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.4.2.1.2">78.49</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.4.2.1.3">77.26</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.4.2.1.4">77.33</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.4.2.1.5">77.90</td>
</tr>
<tr class="ltx_tr" id="S4.T7.4.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T7.4.3.2.1">SVM linear kernel</th>
<td class="ltx_td ltx_align_center" id="S4.T7.4.3.2.2">77.25</td>
<td class="ltx_td ltx_align_center" id="S4.T7.4.3.2.3">76.31</td>
<td class="ltx_td ltx_align_center" id="S4.T7.4.3.2.4">75.39</td>
<td class="ltx_td ltx_align_center" id="S4.T7.4.3.2.5">77.90</td>
</tr>
<tr class="ltx_tr" id="S4.T7.4.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T7.4.4.3.1">SVM radial kernel</th>
<td class="ltx_td ltx_align_center" id="S4.T7.4.4.3.2"><span class="ltx_text ltx_font_bold" id="S4.T7.4.4.3.2.1">80.07</span></td>
<td class="ltx_td ltx_align_center" id="S4.T7.4.4.3.3"><span class="ltx_text ltx_font_bold" id="S4.T7.4.4.3.3.1">78.70</span></td>
<td class="ltx_td ltx_align_center" id="S4.T7.4.4.3.4">76.78</td>
<td class="ltx_td ltx_align_center" id="S4.T7.4.4.3.5"><span class="ltx_text ltx_font_bold" id="S4.T7.4.4.3.5.1">81.15</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.4.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T7.4.5.4.1">Random Forests</th>
<td class="ltx_td ltx_align_center" id="S4.T7.4.5.4.2">79.66</td>
<td class="ltx_td ltx_align_center" id="S4.T7.4.5.4.3">78.44</td>
<td class="ltx_td ltx_align_center" id="S4.T7.4.5.4.4">83.68</td>
<td class="ltx_td ltx_align_center" id="S4.T7.4.5.4.5">74.64</td>
</tr>
<tr class="ltx_tr" id="S4.T7.4.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T7.4.6.5.1">Gradient Boosting</th>
<td class="ltx_td ltx_align_center" id="S4.T7.4.6.5.2">79.12</td>
<td class="ltx_td ltx_align_center" id="S4.T7.4.6.5.3">77.79</td>
<td class="ltx_td ltx_align_center" id="S4.T7.4.6.5.4"><span class="ltx_text ltx_font_bold" id="S4.T7.4.6.5.4.1">84.60</span></td>
<td class="ltx_td ltx_align_center" id="S4.T7.4.6.5.5">72.05</td>
</tr>
<tr class="ltx_tr" id="S4.T7.4.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T7.4.7.6.1">Multi-Layer Perceptron</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T7.4.7.6.2">78.97</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T7.4.7.6.3">77.60</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T7.4.7.6.4">80.74</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T7.4.7.6.5">74.59</td>
</tr>
</tbody>
</table>
</figure><span class="ltx_ERROR undefined" id="S4.SS2.1">\@fb@secFB</span>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Feature importance</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">A plot with the scores returned by the permutation importance is shown in Figure¬†<a class="ltx_ref" href="#S4.F7" title="Figure 7 ‚Ä£ 4.3 Feature importance ‚Ä£ 4 Results ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_tag">7</span></a>.
For each feature, the score indicates how much a random permutation of the feature values impacted the prediction scores, averaged over 100 permutations.
The Back Posture Measurement (BPM) had the highest permutation score, followed by the Head Bobbing Amplitude (HBA) and Left Tracking Distance (TRK_L).
The remaining features showed less importance.</p>
</div>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="581" id="S4.F7.g1" src="x11.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F7.2.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text" id="S4.F7.3.2" style="font-size:90%;">Results of the feature importance over 100 random permutations.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">Using the permutation importance results, the SVM classifier with radial kernel (SVM-R) was then retrained by gradually adding one feature, in the order of their importance.
The classification results of the classifier using these different combinations of features are presented in Table¬†<a class="ltx_ref" href="#S4.T8" title="Table 8 ‚Ä£ 4.3 Feature importance ‚Ä£ 4 Results ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_tag">8</span></a>.
In terms of accuracy and F1-score, using two or more features improves the classification results compared to only using BPM.
The best classification scores are reached by using combinations of 3 and 6 features.</p>
</div>
<figure class="ltx_table" id="S4.T8">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T8.2.1.1" style="font-size:90%;">Table 8</span>: </span><span class="ltx_text" id="S4.T8.3.2" style="font-size:90%;">Results (in %) of the SVM-R classifier after gradually adding one feature per order of their importance score.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T8.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T8.4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T8.4.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T8.4.1.1.1.1" style="font-size:90%;">SVM-R Features</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T8.4.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T8.4.1.1.2.1" style="font-size:90%;">Accuracy</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T8.4.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T8.4.1.1.3.1" style="font-size:90%;">F1-score</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T8.4.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T8.4.1.1.4.1" style="font-size:90%;">Sensitivity</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T8.4.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T8.4.1.1.5.1" style="font-size:90%;">Specificity</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T8.4.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T8.4.2.1.1"><span class="ltx_text" id="S4.T8.4.2.1.1.1" style="font-size:70%;">BPM</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.4.2.1.2">76.66</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.4.2.1.3">74.81</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.4.2.1.4">63.26</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.4.2.1.5">86.69</td>
</tr>
<tr class="ltx_tr" id="S4.T8.4.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T8.4.3.2.1"><span class="ltx_text" id="S4.T8.4.3.2.1.1" style="font-size:70%;">BPM, HBA</span></th>
<td class="ltx_td ltx_align_center" id="S4.T8.4.3.2.2">79.31</td>
<td class="ltx_td ltx_align_center" id="S4.T8.4.3.2.3">77.50</td>
<td class="ltx_td ltx_align_center" id="S4.T8.4.3.2.4">77.42</td>
<td class="ltx_td ltx_align_center" id="S4.T8.4.3.2.5">77.32</td>
</tr>
<tr class="ltx_tr" id="S4.T8.4.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T8.4.4.3.1"><span class="ltx_text" id="S4.T8.4.4.3.1.1" style="font-size:70%;">BPM, HBA, TRK</span></th>
<td class="ltx_td ltx_align_center" id="S4.T8.4.4.3.2">79.87</td>
<td class="ltx_td ltx_align_center" id="S4.T8.4.4.3.3">78.22</td>
<td class="ltx_td ltx_align_center" id="S4.T8.4.4.3.4">76.35</td>
<td class="ltx_td ltx_align_center" id="S4.T8.4.4.3.5">80.14</td>
</tr>
<tr class="ltx_tr" id="S4.T8.4.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T8.4.5.4.1"><span class="ltx_text" id="S4.T8.4.5.4.1.1" style="font-size:70%;">BPM, HBA, TRK, STD</span></th>
<td class="ltx_td ltx_align_center" id="S4.T8.4.5.4.2">79.47</td>
<td class="ltx_td ltx_align_center" id="S4.T8.4.5.4.3">77.87</td>
<td class="ltx_td ltx_align_center" id="S4.T8.4.5.4.4">77.09</td>
<td class="ltx_td ltx_align_center" id="S4.T8.4.5.4.5">78.89</td>
</tr>
<tr class="ltx_tr" id="S4.T8.4.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T8.4.6.5.1"><span class="ltx_text" id="S4.T8.4.6.5.1.1" style="font-size:70%;">BPM, HBA, TRK, STD, STL</span></th>
<td class="ltx_td ltx_align_center" id="S4.T8.4.6.5.2">79.18</td>
<td class="ltx_td ltx_align_center" id="S4.T8.4.6.5.3">78.03</td>
<td class="ltx_td ltx_align_center" id="S4.T8.4.6.5.4">78.31</td>
<td class="ltx_td ltx_align_center" id="S4.T8.4.6.5.5">79.17</td>
</tr>
<tr class="ltx_tr" id="S4.T8.4.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T8.4.7.6.1"><span class="ltx_text" id="S4.T8.4.7.6.1.1" style="font-size:70%;">BPM, HBA, TRK, STD, STL, SWD</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T8.4.7.6.2">80.07</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T8.4.7.6.3">78.70</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T8.4.7.6.4">76.78</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T8.4.7.6.5">81.15</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2>
<span class="ltx_ERROR undefined" id="S5.1">\@fb@secFB</span>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Video processing</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">The video processing consisted of the following steps: using Faster-R-CNN to detect and isolate the cows from the video frames, using T-LEAP to extract time-series of keypoint locations, and using the MAD and Savitzky‚ÄìGolay filters to reduce noise from the keypoint predictions.
For our set of videos, the pre-trained Faster-R-CNN worked out of the box and detected the location of the cows in each video frame.
The performance of T-LEAP was on par with the results described in the original paper¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib16" title="">russello_t-leap_2021 </a></cite>, and it would require little effort to be transferred to videos recorded in new farms, as¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib18" title="">taghavi2023cow </a></cite> showed that little new training data was needed to fine-tune the T-LEAP model.
However, some keypoint mis-detections needed to be corrected.
The parameters for the MAD outlier filter and the smoothing Savitzky‚ÄìGolay filter had to be tuned manually until a good trade-off was found between under- and over-correction.
With no or insufficient correction of the keypoint trajectories, the features could give erroneous values.
While with over-correction, one would run the risk of removing the true signal of keypoint trajectories, and the extracted features wouldn‚Äôt be discriminatory. For instance, if the signal of the forehead would be too flattened, the head bobbing would be systematically missed.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">The videos were selected such that there was only one cow at a time in the field of view. This constraint makes the gait analysis more reliable in two ways.
First, having a single cow in the field of view ensures that the cows don‚Äôt occlude each other‚Äôs body parts, making the pose estimation more reliable.
Second, a single cow in the field of view ensures enough space between the cows such that they can walk at their own pace and display a voluntary gait.
In practice, this constraint could be implemented by skipping the videos where the Faster-R-CNN (or any other object detector) detects more than one cow, or as done in¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib17" title="">barney2023deep </a></cite>, by implementing a tracking algorithm that follows each cow through the video.</p>
</div>
<span class="ltx_ERROR undefined" id="S5.SS1.1">\@fb@secFB</span>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Locomotion scoring</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">A classifier learns to classify samples from a set of labeled examples, also known as ground-truth or golden-standard.
Because a classifier can only be as accurate as its golden-standard¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib24" title="">schlageter-tello_relation_2015 </a></cite>, a reliable locomotion scale is necessary.
Here, the initial inter- and intra-observer reliability was under par.
It is worth noting that the reliability is usually lower in homogeneous data because the probability of agreement by chance is higher when scores are not equally distributed¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib23" title="">schlageter-tello_effect_2014 </a></cite>.
It is unlikely that scoring from live observations instead of from videos would have improved the scores, as¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib38" title="">schlageter2015comparison </a></cite> showed no difference in the reliability of inexperienced observers between live and video scoring and showed improved reliability of experienced observers when scoring from video.
The quality of the ground-truth could perhaps have been further improved by organizing additional locomotion scoring sessions or by having shorter scoring sessions over multiple days.
However, given that the availability of the observers was limited and that a perfect golden standard was not necessary nor likely achievable, we took other steps to address the problem of low reliability and agreement.
First, because we had multiple observers, we could discard the votes from the least reliable observers.
Second, we addressed the problem of class (score) imbalance by merging the levels of the scale to a binary score: normal and lame.
By doing so, we then increased the quality of our golden standard to an acceptable level for running the experiments.</p>
</div>
<span class="ltx_ERROR undefined" id="S5.SS2.1">\@fb@secFB</span>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Gait score classification</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">The gait-score classification task was binary (<span class="ltx_text ltx_font_italic" id="S5.SS3.p1.1.1">normal</span> vs. <span class="ltx_text ltx_font_italic" id="S5.SS3.p1.1.2">lame</span>) and therefore focused on lameness detection rather than fine-grained gait scoring.
Fine-grained locomotion scoring is left for future research as it would require collecting more video footage with sufficient examples of gait scores of 3 and above.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">The performance of the linear classifiers (i.e., logistic regression and SVM with linear kernel) was lower than the performance of the non-linear classifiers.
This implies that when combining all the features, the decision boundary between the <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.1">normal</span> and <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.2">lame</span> classes is non-linear.
The Multi-Layer Perceptron didn‚Äôt perform as well as the other non-linear classifiers, most likely because of the relatively small dataset.
The performance of the three best classifiers SVM-R, RF, and BG, aligns with the conclusions of¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib39" title="">wainer2016comparison </a></cite> and¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib36" title="">wainer2021nested </a></cite>: they found these three binary classifiers to perform the best on 115 open-source datasets tackling a variety of real-world problems in medicine and biology (but not related to lameness detection).
Although, on this dataset, the SVM classifier with radial kernel achieved the best performance in terms of accuracy and F1-score, it might not be the case for other datasets. This is a well-known machine-learning challenge, also known as the ‚Äúno-free-lunch‚Äù theorem, that suggests that no algorithm can outperform all others for all problems¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib40" title="">wolpert1996lack </a></cite>.
Our recommendation would then be to try several classifiers, and the SVMs with radial kernel, random forests, and gradient boosting classifiers provide a good starting point.</p>
</div>
<span class="ltx_ERROR undefined" id="S5.SS3.1">\@fb@secFB</span>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Feature importance</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">Multiple studies investigated the relationship between individual locomotion traits and locomotion scores¬†<cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="#bib.bib27" title="">flower_effect_2006, </a>; <a class="ltx_ref" href="#bib.bib41" title="">borderas2008effect, </a>; <a class="ltx_ref" href="#bib.bib42" title="">chapinal2009using, </a>; <a class="ltx_ref" href="#bib.bib24" title="">schlageter-tello_relation_2015, </a>)</cite>.
They found that, when scored individually, the traits arched back, asymmetric gait, head bobbing, reluctance to bear weight and tracking-up were highly correlated with the locomotion score.
The features selected in this study were designed to measure the same traits. The arched back was measured by the Back Curvature Measurement (HBA), the asymmetric gait by the Stride Length (STL) difference between left and right limbs, the head bobbing by the Head Bobbing Amplitude (HBA), the reluctance to bear weight by the Stance Duration (STD) and Swing Duration (SWD), and the tracking up was measured by the Tracking distance (TRK).</p>
</div>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.1">The BPM, HBA, and TRK features returned the highest scores in the permutation importance test.
BPM and HBA displayed a clear demarcation between the normal and lame classes in Figure¬†<a class="ltx_ref" href="#S3.F6" title="Figure 6 ‚Ä£ 3.2.8 Swing duration difference (SWD) ‚Ä£ 3.2 Gait features extraction ‚Ä£ 3 Methods ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_tag">6</span></a>.
As reported by¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib27" title="">flower_effect_2006 </a></cite>,¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib41" title="">borderas2008effect </a></cite> and¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib42" title="">chapinal2009using </a></cite>, it suggests that the back posture, head bobbing, and tracking-up are, for human observers, easier to recognize than an asymmetric gait (e.g. stride length).
The tracking distance on the left side (TRK-L) had a higher importance than the one on the right side (TRK-R).
This could indicate that, in our dataset, there were more cows tracking-up on the left than on the right side.</p>
</div>
<div class="ltx_para" id="S5.SS4.p3">
<p class="ltx_p" id="S5.SS4.p3.1">Both for the Stance Duration (STD) and the Swing Duration (SWD) on the hind legs (Fig.¬†<a class="ltx_ref" href="#S3.F6" title="Figure 6 ‚Ä£ 3.2.8 Swing duration difference (SWD) ‚Ä£ 3.2 Gait features extraction ‚Ä£ 3 Methods ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_tag">6</span></a>), one can see a clear difference in the duration of the stance/swing phases between the classes, whereas classes differences are less obvious on the front legs. This could be explained by the fact that lameness happens more often on the hind legs¬†<cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="#bib.bib6" title="">poursaberi_real-time_2010, </a>; <a class="ltx_ref" href="#bib.bib27" title="">flower_effect_2006, </a>)</cite>.
Including SWD as a feature increased the classification performance, even though SWD had the lowest importance score.
In contrast, STD had a larger importance score than SWD, but adding the STD feature to the input of the classifier led to a small decrease in accuracy and F1-score.
This could indicate multi-collinearity with other features.</p>
</div>
<div class="ltx_para" id="S5.SS4.p4">
<p class="ltx_p" id="S5.SS4.p4.1">The STL features had the second lowest importance score and the class separation was harder to distinguish in Figure¬†<a class="ltx_ref" href="#S3.F6" title="Figure 6 ‚Ä£ 3.2.8 Swing duration difference (SWD) ‚Ä£ 3.2 Gait features extraction ‚Ä£ 3 Methods ‚Ä£ Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits"><span class="ltx_text ltx_ref_tag">6</span></a>.
Interestingly, the F1-score, sensitivity, and specificity were higher when the STL features were included.
This suggests that the stride length can be informative when used in combination with other features.
It is worth noting that if the cows have bilateral lameness, i.e., are lame on left and right limbs, then the stride length would show little to no difference¬†<cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="#bib.bib9" title="">blackie_associations_2013, </a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS4.p5">
<p class="ltx_p" id="S5.SS4.p5.1">Overall, combining multiple locomotion traits led to a better classification performance than using a single trait.
Using a combination of 3 and 6 traits led to the best accuracy and F1-scores on the SVM classifier with a radial kernel.
Even though additional traits could be extracted from the keypoint trajectories, it is unknown whether they would lead to significant improvements in the gait classification.
Our recommendation would be to include at least the following locomotion traits in an automatic lameness detection system: back posture, head bobbing, and tracking distance, as they demonstrated good overall classification metrics, and these features have been shown to be highly correlated with the locomotion scores¬†<cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="#bib.bib27" title="">flower_effect_2006, </a>; <a class="ltx_ref" href="#bib.bib41" title="">borderas2008effect, </a>; <a class="ltx_ref" href="#bib.bib42" title="">chapinal2009using, </a>)</cite>.
</p>
</div>
<span class="ltx_ERROR undefined" id="S5.SS4.1">\@fb@secFB</span>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Comparison with related work</h3>
<div class="ltx_para" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1">Directly comparing the performance of our lameness classifiers against related work is not straightforward, because even though the task at hand (i.e., detecting lameness from videos) is the same, there is a large variation in the material, methods, and evaluations used in papers that address it.
Furthermore, a comprehensive literature review is out of the scope of this paper, and we refer the reader to¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib43" title="">nejati2023technology </a></cite> for an overview of past and current advances in bovine gait analysis.
We will here compare our results and contrast our findings with previous work that we deem directly related to ours.</p>
</div>
<div class="ltx_para" id="S5.SS5.p2">
<p class="ltx_p" id="S5.SS5.p2.1">The Back Posture Measurement (BPM) was first introduced by¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib6" title="">poursaberi_real-time_2010 </a></cite> and curvature of the back has since then been used in numerous studies¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib6" title="">poursaberi_real-time_2010 </a>; <a class="ltx_ref" href="#bib.bib19" title="">viazzi_analysis_2012 </a>; <a class="ltx_ref" href="#bib.bib20" title="">van_hertem_automatic_2014 </a>; <a class="ltx_ref" href="#bib.bib21" title="">viazzi_comparison_2014 </a>; <a class="ltx_ref" href="#bib.bib22" title="">van_hertem_implementation_2018 </a>; <a class="ltx_ref" href="#bib.bib13" title="">jiang_dairy_2022 </a>; <a class="ltx_ref" href="#bib.bib7" title="">zheng_cows_2023 </a>; <a class="ltx_ref" href="#bib.bib8" title="">zhao_automatic_2023 </a>; <a class="ltx_ref" href="#bib.bib17" title="">barney2023deep </a></cite>.
The BPM is commonly measured during the supporting phase of the hind hooves, and not during the supporting phase of the front hooves because lameness is more common on the hind hooves than on the front ones.
However, this practice could lead to front lameness cases being systematically missed by the algorithm.
To prevent this, we computed the BPM based on the supporting phase of the four legs.
When using BPM as a single locomotion trait, the accuracy of lameness classification ranged from 76%¬†<cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="#bib.bib19" title="">viazzi_analysis_2012, </a>)</cite> to 96%¬†<cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="#bib.bib13" title="">jiang_dairy_2022, </a>)</cite>.
When only including the BPM trait in our SVM-R classifier, we reached an accuracy of 76.6%, which is in line with the literature.</p>
</div>
<div class="ltx_para" id="S5.SS5.p3">
<p class="ltx_p" id="S5.SS5.p3.1">The work presented in¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib8" title="">zhao_automatic_2023 </a></cite> is perhaps the most closely related to this study.
In <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib8" title="">zhao_automatic_2023 </a></cite>, the authors used a combination of traditional and deep-learning-based computer vision to develop a lameness detection system.
They used DeepLabCut¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib15" title="">mathis_deeplabcut_2018 </a></cite>, a deep-learning model that was trained to track the location of the hoofs and the head in videos of walking cows without physical markers.
However, a pixel-level background subtraction method was used for extracting the outline of the spine, which might not be robust to varying backgrounds and light.
The videos where the keypoint predictions were too erroneous were manually discarded.
In total, they used 212 videos of walking cows, where cows that were given a score of 1 or 2 were classified as <span class="ltx_text ltx_font_italic" id="S5.SS5.p3.1.1">normal</span>, and a score of 3 or 4 as <span class="ltx_text ltx_font_italic" id="S5.SS5.p3.1.2">lame</span>.
The back curvature was computed from the outline of the spine, and the keypoints on the hooves and on the neck were used to extract the following features: head bobbing, stride length asymmetry, tracking up, landing speed, supporting phase asymmetry, and moving speed.
The feature selection was performed as follows: a Chi-square test was run on the whole dataset.
The test revealed that back posture measurement and head bobbing were the most important features.
Several classifiers were trained with the back curvature and head bobbing, and the logistic regression classifier returned the best results, with a classification accuracy of 87.3%.
They reported that no other combination of features performed better than back curvature and head bobbing.
In contrast, we found that adding tracking-up to the other two features led to better results on our dataset.
This could mean that, in their dataset, lame subjects were not tracking up.
Another explanation could be that with increasing the number of traits, the complexity of the data increases, and a non-linear classifier, such as SVM-R, would be needed.</p>
</div>
<div class="ltx_para" id="S5.SS5.p4">
<p class="ltx_p" id="S5.SS5.p4.1">In¬†<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib17" title="">barney2023deep </a></cite>, a fully automated multi-cow lameness detection system was developed.
They used a Mask-R-CNN, a deep-learning model, to simultaneously perform object-detection of the cows, and pose estimation of 7 keypoints located on the back neck and head.
In total, they used 250 videos of 10 different cows.
The keypoints were used to extract the back curvature and head position locomotion traits.
Each locomotion trait was extracted per video frame and aggregated per video into statistical features such as the mean, median, standard deviation, min, and max values.
They trained the CatBoost gradient boosting classifier and achieved a 98% accuracy on binary lameness detection, and 94% accuracy on a 4-point scale lameness scoring.
In our work, although we included four more locomotion traits, we only aggregated the values into the median value of the video.
In light of the excellent performance of their classifiers, a promising direction for extending our work would then be to extract more statistical features from the locomotion traits, such as mean, standard deviation, and min and max values, to further improve our classification performance.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we developed a fully automated lameness detection system.
Using the T-LEAP pose estimation model, the motion of nine keypoints was extracted from videos of walking cows.
The trajectories of the keypoints were then used to compute six locomotion traits, namely back posture measurement, head bobbing, tracking distance, stride length, stance duration, and swing duration.
We found that the three most important traits were back posture measurement, head bobbing, and tracking distance and that including multiple locomotion traits led to a better classification than with a single locomotion trait.
For the ground truth, we showed that a thoughtful merging of the scores of the observers could improve intra-observer reliability and agreement.
Future work should evaluate the system in a less constrained environment, for instance, with multiple cows in the field of view.
Another area for future research could focus on leveraging the temporal essence of the videos, by for instance, including more statistical features per locomotion traits.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Aknowledgements</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This publication is part of the project Deep Learning for Human and Animal Health (with project number EDL P16-25-P5) of the research program Efficient Deep Learning (<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://efficientdeeplearning.nl" title="">https://efficientdeeplearning.nl</a>) which is (partly) financed by the Dutch Research Council (NWO).</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Declaration of interests</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
P.¬†T. Thomsen, J.¬†K. Shearer, H.¬†Houe, Prevalence of lameness in dairy cows,
The Veterinary Journal (2023) 105975.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(2)</span>
<span class="ltx_bibblock">
H.¬†R. Whay, J.¬†K. Shearer, The impact of lameness on welfare of the dairy cow,
Veterinary Clinics: Food Animal Practice 33¬†(2) (2017) 153‚Äì164.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(3)</span>
<span class="ltx_bibblock">
H.¬†Enting, D.¬†Kooij, A.¬†Dijkhuizen, R.¬†Huirne, E.¬†Noordhuizen-Stassen, Economic
losses due to clinical lameness in dairy cattle, Livestock production science
49¬†(3) (1997) 259‚Äì267.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(4)</span>
<span class="ltx_bibblock">
J.¬†Huxley, Impact of lameness and claw lesions in cows on health and
production, Livestock Science 156¬†(1-3) (2013) 64‚Äì70.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(5)</span>
<span class="ltx_bibblock">
X.¬†Song, T.¬†Leroy, E.¬†Vranken, W.¬†Maertens, B.¬†Sonck, D.¬†Berckmans, Automatic
detection of lameness in dairy cattle-Vision-based trackway analysis in
cow‚Äôs locomotion, Computers and Electronics in Agriculture 64¬†(1) (2008)
39‚Äì44, iSBN: 0168-1699 _eprint: 9809069v1.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.compag.2008.05.016" title=""><span class="ltx_ref ltx_path ltx_font_typewriter">doi:10.1016/j.compag.2008.05.016</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(6)</span>
<span class="ltx_bibblock">
A.¬†Poursaberi, C.¬†Bahr, A.¬†Pluk, A.¬†V. Nuffel, D.¬†Berckmans, A.¬†Van¬†Nuffel,
D.¬†Berckmans, <a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1016/j.compag.2010.07.004" title="">Real-time
automatic lameness detection based on back posture extraction in dairy
cattle: Shape analysis of cow with image processing techniques</a>, Computers
and Electronics in Agriculture 74¬†(1) (2010) 110‚Äì119, iSBN: 0168-1699
Publisher: Elsevier B.V.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.compag.2010.07.004" title=""><span class="ltx_ref ltx_path ltx_font_typewriter">doi:10.1016/j.compag.2010.07.004</span></a>.

<br class="ltx_break"/>URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1016/j.compag.2010.07.004" title="">http://dx.doi.org/10.1016/j.compag.2010.07.004</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(7)</span>
<span class="ltx_bibblock">
Z.¬†Zheng, X.¬†Zhang, L.¬†Qin, S.¬†Yue, P.¬†Zeng,
<a class="ltx_ref ltx_href" href="https://www.sciencedirect.com/science/article/pii/S0168169923000066" title="">Cows‚Äô
legs tracking and lameness detection in dairy cattle using video analysis and
Siamese neural networks</a>, Computers and Electronics in Agriculture 205
(2023) 107618.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.compag.2023.107618" title=""><span class="ltx_ref ltx_path ltx_font_typewriter">doi:10.1016/j.compag.2023.107618</span></a>.

<br class="ltx_break"/>URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S0168169923000066" title="">https://www.sciencedirect.com/science/article/pii/S0168169923000066</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(8)</span>
<span class="ltx_bibblock">
K.¬†Zhao, M.¬†Zhang, J.¬†Ji, R.¬†Zhang, J.¬†M. Bewley,
<a class="ltx_ref ltx_href" href="https://www.sciencedirect.com/science/article/pii/S153751102300106X" title="">Automatic
lameness scoring of dairy cows based on the analysis of head- and back-hoof
linkage features using machine learning methods</a>, Biosystems Engineering 230
(2023) 424‚Äì441.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.biosystemseng.2023.05.003" title=""><span class="ltx_ref ltx_path ltx_font_typewriter">doi:10.1016/j.biosystemseng.2023.05.003</span></a>.

<br class="ltx_break"/>URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S153751102300106X" title="">https://www.sciencedirect.com/science/article/pii/S153751102300106X</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(9)</span>
<span class="ltx_bibblock">
N.¬†Blackie, E.¬†Bleach, J.¬†Amory, J.¬†Scaife,
<a class="ltx_ref ltx_href" href="http://linkinghub.elsevier.com/retrieve/pii/S0022030213002282" title="">Associations
between locomotion score and kinematic measures in dairy cows with varying
hoof lesion types</a>, Journal of Dairy Science 96¬†(6) (2013) 3564‚Äì3572, iSBN:
0022-0302 Publisher: Elsevier.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3168/jds.2012-5597" title=""><span class="ltx_ref ltx_path ltx_font_typewriter">doi:10.3168/jds.2012-5597</span></a>.

<br class="ltx_break"/>URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://linkinghub.elsevier.com/retrieve/pii/S0022030213002282" title="">http://linkinghub.elsevier.com/retrieve/pii/S0022030213002282</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(10)</span>
<span class="ltx_bibblock">
Y.¬†Karoui, A.¬†A.¬†B. Jacques, A.¬†B. Diallo, E.¬†Shepley, E.¬†Vasseur,
<a class="ltx_ref ltx_href" href="https://ojs.aaai.org/index.php/AAAI/article/view/17902" title="">A Deep
Learning Framework for Improving Lameness Identification in Dairy
Cattle</a>, Proceedings of the AAAI Conference on Artificial Intelligence
35¬†(18) (2021) 15811‚Äì15812, number: 18.

<br class="ltx_break"/>URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ojs.aaai.org/index.php/AAAI/article/view/17902" title="">https://ojs.aaai.org/index.php/AAAI/article/view/17902</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(11)</span>
<span class="ltx_bibblock">
D.¬†Wu, Q.¬†Wu, X.¬†Yin, B.¬†Jiang, H.¬†Wang, D.¬†He, H.¬†Song,
<a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.biosystemseng.2019.11.017" title="">Lameness detection
of dairy cows based on the YOLOv3 deep learning algorithm and a relative
step size characteristic vector</a>, Biosystems Engineering 189 (2020) 150‚Äì163,
publisher: Elsevier Ltd.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.biosystemseng.2019.11.017" title=""><span class="ltx_ref ltx_path ltx_font_typewriter">doi:10.1016/j.biosystemseng.2019.11.017</span></a>.

<br class="ltx_break"/>URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1016/j.biosystemseng.2019.11.017" title="">https://doi.org/10.1016/j.biosystemseng.2019.11.017</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(12)</span>
<span class="ltx_bibblock">
X.¬†Kang, X.¬†D. Zhang, G.¬†Liu,
<a class="ltx_ref ltx_href" href="https://www-journalofdairyscience-org.ezproxy.library.wur.nl/article/S0022-0302(20)30713-X/abstract" title="">Accurate
detection of lameness in dairy cattle with computer vision: A new and
individualized detection strategy based on the analysis of the supporting
phase</a>, Journal of Dairy Science 103¬†(11) (2020) 10628‚Äì10638, publisher:
Elsevier.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3168/jds.2020-18288" title=""><span class="ltx_ref ltx_path ltx_font_typewriter">doi:10.3168/jds.2020-18288</span></a>.

<br class="ltx_break"/>URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www-journalofdairyscience-org.ezproxy.library.wur.nl/article/S0022-0302(20)30713-X/abstract" title="">https://www-journalofdairyscience-org.ezproxy.library.wur.nl/article/S0022-0302(20)30713-X/abstract</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(13)</span>
<span class="ltx_bibblock">
B.¬†Jiang, H.¬†Song, H.¬†Wang, C.¬†Li,
<a class="ltx_ref ltx_href" href="https://www.sciencedirect.com/science/article/pii/S0168169922000461" title="">Dairy
cow lameness detection using a back curvature feature</a>, Computers and
Electronics in Agriculture 194 (2022) 106729.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.compag.2022.106729" title=""><span class="ltx_ref ltx_path ltx_font_typewriter">doi:10.1016/j.compag.2022.106729</span></a>.

<br class="ltx_break"/>URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S0168169922000461" title="">https://www.sciencedirect.com/science/article/pii/S0168169922000461</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(14)</span>
<span class="ltx_bibblock">
E.¬†Arazo, R.¬†Aly, K.¬†McGuinness,
<a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2206.04449" title="">Segmentation Enhanced Lameness
Detection in Dairy Cows from RGB and Depth Video</a>,
arXiv:2206.04449 [cs] (Jun. 2022).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2206.04449" title=""><span class="ltx_ref ltx_path ltx_font_typewriter">doi:10.48550/arXiv.2206.04449</span></a>.

<br class="ltx_break"/>URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2206.04449" title="">http://arxiv.org/abs/2206.04449</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(15)</span>
<span class="ltx_bibblock">
A.¬†Mathis, P.¬†Mamidanna, K.¬†M. Cury, T.¬†Abe, V.¬†N. Murthy, M.¬†W. Mathis,
M.¬†Bethge,
<a class="ltx_ref ltx_href" href="https://www.nature.com/articles/s41593-018-0209-y." title="">DeepLabCut:
markerless pose estimation of user-defined body parts with deep learning</a>,
Nature Neuroscience 21¬†(9) (2018) 1281‚Äì1289, number: 9 Publisher: Nature
Publishing Group.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1038/s41593-018-0209-y" title=""><span class="ltx_ref ltx_path ltx_font_typewriter">doi:10.1038/s41593-018-0209-y</span></a>.

<br class="ltx_break"/>URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nature.com/articles/s41593-018-0209-y." title="">https://www.nature.com/articles/s41593-018-0209-y.</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(16)</span>
<span class="ltx_bibblock">
H.¬†Russello, R.¬†van¬†der Tol, G.¬†Kootstra,
<a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2104.08029" title="">T-LEAP: occlusion-robust pose
estimation of walking cows using temporal information</a>, arXiv:2104.08029
[cs]ArXiv: 2104.08029 (Apr. 2021).

<br class="ltx_break"/>URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2104.08029" title="">http://arxiv.org/abs/2104.08029</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(17)</span>
<span class="ltx_bibblock">
S.¬†Barney, S.¬†Dlay, A.¬†Crowe, I.¬†Kyriazakis, M.¬†Leach, Deep learning pose
estimation for multi-cattle lameness detection, Scientific Reports 13¬†(1)
(2023) 4499.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(18)</span>
<span class="ltx_bibblock">
M.¬†Taghavi, H.¬†Russello, W.¬†Ouweltjes, C.¬†Kamphuis, I.¬†Adriaens, Cow key point
detection in indoor housing conditions with a deep learning model, Journal of
Dairy Science (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(19)</span>
<span class="ltx_bibblock">
S.¬†Viazzi, C.¬†Bahr, A.¬†Schlageter-Tello, T.¬†Van¬†Hertem, C.¬†Romanini, A.¬†Pluk,
I.¬†Halachmi, C.¬†Lokhorst, D.¬†Berckmans,
<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.3168/jds.2012-5806" title="">Analysis of individual
classification of lameness using automatic measurement of back posture in
dairy cattle</a>, Journal of Dairy Science 96¬†(1) (2012) 257‚Äì266, publisher:
Elsevier.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3168/jds.2012-5806" title=""><span class="ltx_ref ltx_path ltx_font_typewriter">doi:10.3168/jds.2012-5806</span></a>.

<br class="ltx_break"/>URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.3168/jds.2012-5806" title="">http://dx.doi.org/10.3168/jds.2012-5806</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(20)</span>
<span class="ltx_bibblock">
T.¬†Van¬†Hertem, S.¬†Viazzi, M.¬†Steensels, E.¬†Maltz, A.¬†Antler, V.¬†Alchanatis,
A.¬†A. Schlageter-Tello, K.¬†Lokhorst, E.¬†C. Romanini, C.¬†Bahr, D.¬†Berckmans,
I.¬†Halachmi,
<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1016/j.biosystemseng.2014.01.009" title="">Automatic
lameness detection based on consecutive 3D-video recordings</a>, Biosystems
Engineering 119 (2014) 108‚Äì116, iSBN: 9789088263330 Publisher: IAgrE.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.biosystemseng.2014.01.009" title=""><span class="ltx_ref ltx_path ltx_font_typewriter">doi:10.1016/j.biosystemseng.2014.01.009</span></a>.

<br class="ltx_break"/>URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1016/j.biosystemseng.2014.01.009" title="">http://dx.doi.org/10.1016/j.biosystemseng.2014.01.009</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(21)</span>
<span class="ltx_bibblock">
S.¬†Viazzi, C.¬†Bahr, T.¬†Van¬†Hertem, A.¬†Schlageter-Tello, C.¬†E.¬†B. Romanini,
I.¬†Halachmi, C.¬†Lokhorst, D.¬†Berckmans,
<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1016/j.compag.2013.11.005" title="">Comparison of a
three-dimensional and two-dimensional camera system for automated measurement
of back posture in dairy cows</a>, Computers and Electronics in Agriculture 100
(2014) 139‚Äì147, iSBN: 0168-1699 Publisher: Elsevier B.V.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.compag.2013.11.005" title=""><span class="ltx_ref ltx_path ltx_font_typewriter">doi:10.1016/j.compag.2013.11.005</span></a>.

<br class="ltx_break"/>URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1016/j.compag.2013.11.005" title="">http://dx.doi.org/10.1016/j.compag.2013.11.005</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(22)</span>
<span class="ltx_bibblock">
T.¬†Van¬†Hertem, A.¬†S. Tello, S.¬†Viazzi, M.¬†Steensels, C.¬†Bahr, C.¬†E.¬†B.
Romanini, K.¬†Lokhorst, E.¬†Maltz, I.¬†Halachmi, D.¬†Berckmans,
A.¬†Schlageter¬†Tello, S.¬†Viazzi, M.¬†Steensels, C.¬†Bahr, C.¬†E.¬†B. Romanini,
K.¬†Lokhorst, E.¬†Maltz, I.¬†Halachmi, D.¬†Berckmans, Implementation of an
automatic 3D vision monitor for dairy cow locomotion in a commercial farm,
Biosystems Engineering 173 (2018) 166‚Äì175, iSBN: 1537-5110 Publisher:
Elsevier.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.biosystemseng.2017.08.011" title=""><span class="ltx_ref ltx_path ltx_font_typewriter">doi:10.1016/j.biosystemseng.2017.08.011</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(23)</span>
<span class="ltx_bibblock">
A.¬†Schlageter-Tello, E.¬†A. Bokkers, P.¬†W. Groot¬†Koerkamp, T.¬†Van¬†Hertem,
S.¬†Viazzi, C.¬†E. Romanini, I.¬†Halachmi, C.¬†Bahr, D.¬†Berckmans, K.¬†Lokhorst,
<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.3168/jds.2014-8129" title="">Effect of merging levels of
locomotion scores for dairy cows on intra- and interrater reliability and
agreement</a>, Journal of Dairy Science 97¬†(9) (2014) 5533‚Äì5542, publisher:
Elsevier.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3168/jds.2014-8129" title=""><span class="ltx_ref ltx_path ltx_font_typewriter">doi:10.3168/jds.2014-8129</span></a>.

<br class="ltx_break"/>URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.3168/jds.2014-8129" title="">http://dx.doi.org/10.3168/jds.2014-8129</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(24)</span>
<span class="ltx_bibblock">
A.¬†Schlageter-Tello, E.¬†A. Bokkers, P.¬†W. Groot¬†Koerkamp, T.¬†Van¬†Hertem,
S.¬†Viazzi, C.¬†E. Romanini, I.¬†Halachmi, C.¬†Bahr, D.¬†Berckmans, K.¬†Lokhorst,
<a class="ltx_ref ltx_href" href="https://linkinghub.elsevier.com/retrieve/pii/S0022030215006633" title="">Relation
between observed locomotion traits and locomotion score in dairy cows</a>,
Journal of Dairy Science 98¬†(12) (2015) 8623‚Äì8633.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3168/jds.2014-9059" title=""><span class="ltx_ref ltx_path ltx_font_typewriter">doi:10.3168/jds.2014-9059</span></a>.

<br class="ltx_break"/>URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://linkinghub.elsevier.com/retrieve/pii/S0022030215006633" title="">https://linkinghub.elsevier.com/retrieve/pii/S0022030215006633</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(25)</span>
<span class="ltx_bibblock">
D.¬†Sprecher, D.¬†Hostetler, J.¬†Kaneene, A LAMENESS SCORING SYSTEM THAT
USES POSTURE AND GAIT TO PREDICT DAIRY CATTLE REPRODUCTIVE
PERFORMANCE, Science¬†(97) (1997).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(26)</span>
<span class="ltx_bibblock">
P.¬†Thomsen, L.¬†Munksgaard, F.¬†T√∏gersen, Evaluation of a lameness scoring
system for dairy cows, Journal of dairy science 91¬†(1) (2008) 119‚Äì126.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(27)</span>
<span class="ltx_bibblock">
F.¬†C. Flower, D.¬†M. Weary,
<a class="ltx_ref ltx_href" href="https://www.sciencedirect.com/science/article/pii/S002203020672077X" title="">Effect
of Hoof Pathologies on Subjective Assessments of Dairy Cow
Gait</a>, Journal of Dairy Science 89¬†(1) (2006) 139‚Äì146.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3168/jds.S0022-0302(06)72077-X" title=""><span class="ltx_ref ltx_path ltx_font_typewriter">doi:10.3168/jds.S0022-0302(06)72077-X</span></a>.

<br class="ltx_break"/>URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S002203020672077X" title="">https://www.sciencedirect.com/science/article/pii/S002203020672077X</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(28)</span>
<span class="ltx_bibblock">
K.¬†Krippendorff, Computing krippendorff‚Äôs alpha-reliability (2011).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(29)</span>
<span class="ltx_bibblock">
B.¬†Engel, G.¬†Bruin, G.¬†Andre, W.¬†Buist,
<a class="ltx_ref ltx_href" href="http://www.cambridge.org/core/journals/journal-of-agricultural-science/article/assessment-of-observer-performance-in-a-subjective-scoring-system-visual-classification-of-the-gait-of-cows/A4C2BDAAE4803FE2DFE34013FC8F6DE9#access-block" title="">Assessment
of observer performance in a subjective scoring system: visual classification
of the gait of cows</a>, The Journal of Agricultural Science 140¬†(3) (2003)
317‚Äì333, publisher: Cambridge University Press.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1017/S0021859603002983" title=""><span class="ltx_ref ltx_path ltx_font_typewriter">doi:10.1017/S0021859603002983</span></a>.

<br class="ltx_break"/>URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.cambridge.org/core/journals/journal-of-agricultural-science/article/assessment-of-observer-performance-in-a-subjective-scoring-system-visual-classification-of-the-gait-of-cows/A4C2BDAAE4803FE2DFE34013FC8F6DE9#access-block" title="">http://www.cambridge.org/core/journals/journal-of-agricultural-science/article/assessment-of-observer-performance-in-a-subjective-scoring-system-visual-classification-of-the-gait-of-cows/A4C2BDAAE4803FE2DFE34013FC8F6DE9#access-block</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(30)</span>
<span class="ltx_bibblock">
Y.¬†Wu, A.¬†Kirillov, F.¬†Massa, W.-Y. Lo, R.¬†Girshick, Detectron2,
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/facebookresearch/detectron2" title="">https://github.com/facebookresearch/detectron2</a> (2019).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(31)</span>
<span class="ltx_bibblock">
A.¬†Savitzky, M.¬†J. Golay, Smoothing and differentiation of data by simplified
least squares procedures., Analytical chemistry 36¬†(8) (1964) 1627‚Äì1639.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(32)</span>
<span class="ltx_bibblock">
J.¬†W. Cooley, J.¬†W. Tukey, An algorithm for the machine calculation of complex
fourier series, Mathematics of computation 19¬†(90) (1965) 297‚Äì301.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(33)</span>
<span class="ltx_bibblock">
L.¬†Buitinck, G.¬†Louppe, M.¬†Blondel, F.¬†Pedregosa, A.¬†Mueller, O.¬†Grisel,
V.¬†Niculae, P.¬†Prettenhofer, A.¬†Gramfort, J.¬†Grobler, et¬†al., Api design for
machine learning software: experiences from the scikit-learn project, arXiv
preprint arXiv:1309.0238 (2013).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(34)</span>
<span class="ltx_bibblock">
N.¬†V. Chawla, K.¬†W. Bowyer, L.¬†O. Hall, W.¬†P. Kegelmeyer, Smote: synthetic
minority over-sampling technique, Journal of artificial intelligence research
16 (2002) 321‚Äì357.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(35)</span>
<span class="ltx_bibblock">
F.¬†Pedregosa, G.¬†Varoquaux, A.¬†Gramfort, V.¬†Michel, B.¬†Thirion, O.¬†Grisel,
M.¬†Blondel, P.¬†Prettenhofer, R.¬†Weiss, V.¬†Dubourg, J.¬†Vanderplas, A.¬†Passos,
D.¬†Cournapeau, M.¬†Brucher, M.¬†Perrot, E.¬†Duchesnay, Scikit-learn: Machine
learning in Python, Journal of Machine Learning Research 12 (2011)
2825‚Äì2830.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(36)</span>
<span class="ltx_bibblock">
J.¬†Wainer, G.¬†Cawley, Nested cross-validation when selecting classifiers is
overzealous for most practical applications, Expert Systems with Applications
182 (2021) 115222.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(37)</span>
<span class="ltx_bibblock">
L.¬†Breiman, Random forests, Machine learning 45 (2001) 5‚Äì32.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(38)</span>
<span class="ltx_bibblock">
A.¬†Schlageter-Tello, E.¬†Bokkers, P.¬†G. Koerkamp, T.¬†Van¬†Hertem, S.¬†Viazzi,
C.¬†Romanini, I.¬†Halachmi, C.¬†Bahr, D.¬†Berckmans, K.¬†Lokhorst, Comparison of
locomotion scoring for dairy cows by experienced and inexperienced raters
using live or video observation methods, Animal Welfare 24¬†(1) (2015) 69‚Äì79.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(39)</span>
<span class="ltx_bibblock">
J.¬†Wainer, Comparison of 14 different families of classification algorithms on
115 binary datasets, arXiv preprint arXiv:1606.00930 (2016).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(40)</span>
<span class="ltx_bibblock">
D.¬†H. Wolpert, The lack of a priori distinctions between learning algorithms,
Neural computation 8¬†(7) (1996) 1341‚Äì1390.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(41)</span>
<span class="ltx_bibblock">
T.¬†Borderas, A.¬†Fournier, J.¬†Rushen, A.¬†De¬†Passille, Effect of lameness on
dairy cows‚Äô visits to automatic milking systems, Canadian Journal of Animal
Science 88¬†(1) (2008) 1‚Äì8.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(42)</span>
<span class="ltx_bibblock">
N.¬†Chapinal, A.¬†De¬†Passille, D.¬†Weary, M.¬†Von¬†Keyserlingk, J.¬†Rushen, Using
gait score, walking speed, and lying behavior to detect hoof lesions in dairy
cows, Journal of dairy science 92¬†(9) (2009) 4365‚Äì4374.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(43)</span>
<span class="ltx_bibblock">
A.¬†Nejati, A.¬†Bradtmueller, E.¬†Shepley, E.¬†Vasseur, Technology applications in
bovine gait analysis: A scoping review, Plos one 18¬†(1) (2023) e0266287.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Jan 10 14:55:01 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
