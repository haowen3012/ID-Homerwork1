<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation</title>
<!--Generated on Thu Aug 15 15:54:57 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2408.08234v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#S1" title="In Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#S2" title="In Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#S3" title="In Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Benchmark</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#S4" title="In Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>3D Object Reconstruction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#S5" title="In Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experimental Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#S6" title="In Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A1" title="In Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Dataset</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A1.SS1" title="In Appendix A Dataset ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Data Collection Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A1.SS2" title="In Appendix A Dataset ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Data Release</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A2" title="In Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Evaluation Setup</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A2.SS1" title="In Appendix B Evaluation Setup ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Reconstruction Implementation Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A2.SS2" title="In Appendix B Evaluation Setup ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span>Pose Estimators: FoundationPose and Megapose</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A2.SS3" title="In Appendix B Evaluation Setup ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.3 </span>BOP Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A2.SS4" title="In Appendix B Evaluation Setup ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.4 </span>Object Categorization</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3" title="In Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Additional Results</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.SS1" title="In Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.1 </span>Pose Evaluation: Average Recall</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.SS2" title="In Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2 </span>Reconstruction with varying training size</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.SS3" title="In Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.3 </span>Object Categories</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.SS4" title="In Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.4 </span>Influence of the Texturing Algorithm</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.SS5" title="In Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.5 </span>Per-Object Pose Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.SS6" title="In Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.6 </span>Qualitative Results</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Varun Burde   Assia Benbihi<sup class="ltx_sup" id="id2.2.id1">∗</sup>   Pavel Burget   Torsten Sattler 
<br class="ltx_break"/>Czech Institute of Informatics, Robotics and Cybernetics, Czech Technical University in Prague
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id3.3.id2">firstname.lastname@cvut.cz</span>
</span><span class="ltx_author_notes">Equal Contribution.</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id4.id1">Object pose estimation is essential to many industrial applications involving robotic manipulation, navigation, and augmented reality. Current generalizable object pose estimators, i.e., approaches that do not need to be trained per object, rely on accurate 3D models. Predominantly, CAD models are used, which can be hard to obtain in practice. At the same time, it is often possible to acquire images of an object. Naturally, this leads to the question whether 3D models reconstructed from images are sufficient to facilitate accurate object pose estimation. We aim to answer this question by proposing a novel benchmark for measuring the impact of 3D reconstruction quality on pose estimation accuracy. Our benchmark provides calibrated images for object reconstruction registered with the test images of the YCB-V dataset for pose evaluation under the BOP benchmark format. Detailed experiments with multiple state-of-the-art 3D reconstruction and object pose estimation approaches show that the geometry produced by modern reconstruction methods is often sufficient for accurate pose estimation. Our experiments lead to interesting observations: (1) Standard metrics for measuring 3D reconstruction quality are not necessarily indicative of pose estimation accuracy, which shows the need for dedicated benchmarks such as ours. (2) Classical, non-learning-based approaches can perform on par with modern learning-based reconstruction techniques and can even offer a better reconstruction time-pose accuracy tradeoff. (3) There is still a sizable gap between performance with reconstructed and with CAD models. To foster research on closing this gap, our benchmark is publicly available at <a class="ltx_ref ltx_href ltx_font_typewriter" href="https://github.com/VarunBurde/reconstruction_pose_benchmark" title="">github.com/VarunBurde/reconstruction_pose_benchmark</a>.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Object pose estimators predict the position and orientation of an object in a given image.
Object pose estimation is a prerequisite to various applications including robotic manipulation and virtual tasks such as highlighting individual components of large machinery in AR applications.
Existing methods can be categorized into CAD-based methods, <em class="ltx_emph ltx_font_italic" id="S1.p1.1.1">e.g</em>.<span class="ltx_text" id="S1.p1.1.2"></span>, approaches using feature matching <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib67" title="">67</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib84" title="">84</a>]</cite>, templates <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib40" title="">40</a>]</cite>,
regression <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib107" title="">107</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib35" title="">35</a>]</cite>, or
render-and-compare strategies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib58" title="">58</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib93" title="">93</a>]</cite>,
and CAD-free methods, <em class="ltx_emph ltx_font_italic" id="S1.p1.1.3">e.g</em>.<span class="ltx_text" id="S1.p1.1.4"></span>, methods using pose regression <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib111" title="">111</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib62" title="">62</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib37" title="">37</a>]</cite>,
analysis-by-synthesis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib19" title="">19</a>]</cite>, object coordinate
regression <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib79" title="">79</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib16" title="">16</a>]</cite>,
feature matching <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib99" title="">99</a>]</cite>, or render-and-compare <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib78" title="">78</a>]</cite>.
CAD-based methods usually
lead the BOP benchmark
for object pose estimation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib2" title="">2</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The 3D models involved are often generated by CAD applications in complex setups or reconstructed via range scanners <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib26" title="">26</a>]</cite> that produce highly accurate meshes.
Naturally, this process can be time-consuming.
Also, in many practical settings, <em class="ltx_emph ltx_font_italic" id="S1.p2.1.1">e.g</em>.<span class="ltx_text" id="S1.p2.1.2"></span>, service robots, it is infeasible to obtain such a 3D model for each object class and object instance that might be encountered.
A more practical approach in such scenarios would be to reconstruct 3D object models on the fly from images.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Generalization is another factor limiting the deployment of pose estimators: in the example where a robot faces a new object and generates an on-the-fly 3D model, most of the estimators still require training or finetuning on the new object.
This induces a computational overhead not only because of the network training but also because the robot must capture enough views for the training to converge.
Instead, deploying CAD-based estimators that generalize to unseen objects <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib12" title="">12</a>]</cite> is more efficient since it can be used without finetuning and 3D reconstruction can work even with sparse views.
Two such estimators have recently set the <span class="ltx_glossaryref" title="">State-of-the-Art (SotA)</span> with a CAD-based render-and-compare strategy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib12" title="">12</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this paper, we investigate using 3D reconstructions from RGB images for object pose estimation.
In particular, we want to quantify the object pose accuracy gap between using high-quality CAD models and 3D models obtained from images inside
generalizable object pose estimators <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib12" title="">12</a>]</cite>.
For this purpose, we propose a new benchmark. Compared to existing benchmarks for 3D reconstruction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib90" title="">90</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib102" title="">102</a>]</cite>, our benchmark does not treat 3D reconstruction as a task unto itself but rather evaluates the resulting 3D models inside a higher-level task (namely object pose estimation).
Thus, we mainly measure 3D reconstruction performance by the accuracy of the estimated poses rather than the accuracy of the resulting 3D models itself.
Our benchmark is based on the YCB-V <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib111" title="">111</a>]</cite> dataset for object pose evaluation: it is made of 21 objects selected from the YCB dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib17" title="">17</a>]</cite>, including small objects, objects with low texture, complex shapes, high reflectance, and multiple symmetries.
For each object, our benchmark consists of two components:
<span class="ltx_text ltx_font_bold" id="S1.p4.1.1">(1)</span> Images of the object captured by a robot arm
that can be used for 3D reconstruction.
We provide image sets of different sizes as, in certain applications, data capture and 3D reconstruction times can be important.
Providing smaller subsets of images allows us to simulate such scenarios.
<span class="ltx_text ltx_font_bold" id="S1.p4.1.2">(2)</span> A set of test images for object pose estimation together with ground truth poses registered with the image sets from (1).
Compared to the data for the first component, which we captured ourselves, we use the test images provided by the YCB-V <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib111" title="">111</a>]</cite> dataset for the second component.
Given the two components, we evaluate multiple state-of-the-art 3D reconstruction methods, including both classical methods based on multi-view stereo <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib88" title="">88</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib82" title="">82</a>]</cite> and recent learning-based approaches using neural implicit representations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib113" title="">113</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib116" title="">116</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib114" title="">114</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib75" title="">75</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib109" title="">109</a>]</cite>.
Our experiments show that standard metrics for 3D reconstruction quality (both geometry-based and appearance-based) do not necessarily directly correlate with object pose estimation accuracy.
This clearly shows a need for benchmarks such as ours that directly measure the impact of 3D reconstruction algorithms on object pose estimation’s accuracy.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">In summary, this paper makes the following contributions: <span class="ltx_text ltx_font_bold" id="S1.p5.1.1">(1)</span> We propose a new benchmark to evaluate 3D reconstruction algorithms in the context of object pose estimation.
Our benchmark aims to measure to what degree 3D models reconstructed from images instead of CAD models can be used for object pose estimation.
<span class="ltx_text ltx_font_bold" id="S1.p5.1.2">(2)</span> We evaluate a wide range of state-of-the-art 3D reconstruction algorithms on our benchmark in terms of object pose accuracy, data requirements to reach high accuracy, and reconstruction times.
The most important insights are:
(a) There is a clear need for benchmarks that evaluate 3D reconstruction algorithms in the context of object pose estimation.
(b) Classical, non-learning-based approaches can perform similar to modern learning-based reconstruction techniques while offering faster reconstruction times.
(c) The top-performing reconstruction methods are the same for both object pose estimators considered in our benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib12" title="">12</a>]</cite>, suggesting that all object pose estimators might potentially benefit from progress in the area of 3D reconstruction.
(d) While it is possible to obtain a similar performance as with CAD models for some types of objects, there is still a sizable gap in performance for other types of objects, especially for objects with fine details and reflective surfaces.
Clearly, the problem of 3D object reconstruction is not yet solved.
<span class="ltx_text ltx_font_bold" id="S1.p5.1.3">(3)</span> The benchmark data is available at <a class="ltx_ref ltx_href ltx_font_typewriter" href="https://github.com/VarunBurde/reconstruction_pose_benchmark" title="">github.com/VarunBurde/reconstruction_pose_benchmark</a>.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">Object pose estimation. </span>
The main approaches include
feature matching <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib67" title="">67</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib84" title="">84</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib66" title="">66</a>]</cite>, template matching <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib110" title="">110</a>]</cite>,
pose regression <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib111" title="">111</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib62" title="">62</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib37" title="">37</a>]</cite>,
3D-coordinate-regression <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib107" title="">107</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib79" title="">79</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib103" title="">103</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib16" title="">16</a>]</cite>,
and render-and-compare <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib58" title="">58</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib93" title="">93</a>]</cite>.
One key property for the deployment of pose estimators is their generalization that falls into three categories: no generalization, generalization to object categories seen at training time <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib61" title="">61</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib108" title="">108</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib66" title="">66</a>]</cite>, and generalization to any object <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib73" title="">73</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib76" title="">76</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib70" title="">70</a>]</cite>.
Pose estimators that generalize to any objects are the most suitable in practical scenarios as it is unrealistic to train the estimators on any object instance or category that might be encountered during deployment.
However, they often rely on CAD models and
it is also impractical to generate CAD models for all objects beforehand.
This prompts recent contributions on generalizable pose estimators to use 3D models generated on the fly for the pose estimation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib99" title="">99</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib78" title="">78</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib73" title="">73</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib12" title="">12</a>]</cite>.
At the time of the writing, FoundationPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib12" title="">12</a>]</cite> sets the state-of-the-art
which motivates its integration in the benchmark to evaluate the 3D reconstructions.
Whenever a CAD model is not available, it generates a 3D model and use it in a render-and-compare fashion.
We also evaluate the 3D reconstructions with Megapose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib55" title="">55</a>]</cite>, another <span class="ltx_glossaryref" title="">SotA</span> render-and-compare method that operates on RGB images.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">3D Reconstruction. </span>
Given a set of calibrated images with <span class="ltx_text ltx_font_bold" id="S2.p2.1.2">known camera poses</span>,
<span class="ltx_glossaryref" title="">Multi-View Stereo (MVS)</span> generates a dense 3D reconstruction of the scene with <span class="ltx_text ltx_font_bold" id="S2.p2.1.3">explicit</span> representations such as dense point clouds <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib57" title="">57</a>]</cite>, depth maps <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib97" title="">97</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib88" title="">88</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib50" title="">50</a>]</cite>
surface hulls <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib94" title="">94</a>]</cite>, or voxels <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib91" title="">91</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib95" title="">95</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib105" title="">105</a>]</cite>.
One advantage of depth-based approaches is the recovery of fine details at a reasonable memory budget contrary to surface or voxel based methods which memory requirements usually increase with the resolution of the 3D reconstruction.
This motivates the benchmark to focus on depth-based methods with the open-source COLMAP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib87" title="">87</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib88" title="">88</a>]</cite> and its industrial counterpart Reality Capture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib82" title="">82</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Alternatives to explicit reconstructions are <span class="ltx_text ltx_font_bold" id="S2.p3.1.1">implicit</span> representations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib72" title="">72</a>]</cite>, <em class="ltx_emph ltx_font_italic" id="S2.p3.1.2">i.e</em>.<span class="ltx_text" id="S2.p3.1.3"></span>, continuous functions defined over the 3D space that embed properties about the scene.
An example is the <span class="ltx_glossaryref" title="">Signed Distance Function (SDF)</span> that outputs the distance between a given 3D point and the closest surface.
Defining implicit functions is often not tractable so they are instead approximated, often with neural networks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib68" title="">68</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib77" title="">77</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib109" title="">109</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib96" title="">96</a>]</cite>.
With the advent of differentiable rendering, several implicit functions relevant for 3D reconstruction can be learned simply from calibrated-image-supervision including volumetric radiance field <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib69" title="">69</a>]</cite>, occupancy network <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib75" title="">75</a>]</cite>, and <span class="ltx_glossaryref" title="">SDF</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib112" title="">112</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib74" title="">74</a>]</cite>.
Volumetric methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib69" title="">69</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib63" title="">63</a>]</cite>
represent the scene as a colored volume where a network outputs a volume density and a color for each point in the space.
Rendering the volume density produces depth maps suitable to generate 3D meshes using Poisson surface resconstruction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib52" title="">52</a>]</cite>.
Volumetric methods are optimized for novel view synthesis only whereas optimizing SDF networks involves more geometric constraints <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib34" title="">34</a>]</cite> to ensure that the network exhibits SDF properties such as 0 values on the surface and smooth surfaces.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">Given the expansive literature on neural implicit representations, it is unrealistic to evaluate them all.
Instead, we select the methods representative of new optimization paradigms:
UniSURF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib75" title="">75</a>]</cite> as an occupancy-grid method;
Neus <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib109" title="">109</a>]</cite> and
VolSDF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib113" title="">113</a>]</cite> for jointly using volumetric and surface representations; MonoSDF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib116" title="">116</a>]</cite> for integrating 3D priors in the SDF learning;
iNGP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib71" title="">71</a>]</cite> as a representative of fast feature-grids-based methods;
NeuralAngelo <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib59" title="">59</a>]</cite> for integrating feature-grids into the SDF learning;
BakedSDF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib114" title="">114</a>]</cite> that jointly optimizes for the geometry and the texture of the scene;
and the Nerfacto <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib100" title="">100</a>]</cite> implementation Nerf <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib69" title="">69</a>]</cite> that integrates several contributions, including <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib71" title="">71</a>]</cite>, that the authors have found to work well in real-case scenarios.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p5">
<p class="ltx_p" id="S2.p5.1"><span class="ltx_text ltx_font_bold" id="S2.p5.1.1">Benchmarking 3D reconstruction algorithms. </span>
Existing datasets and benchmarks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib98" title="">98</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib90" title="">90</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib92" title="">92</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib86" title="">86</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib85" title="">85</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib9" title="">9</a>]</cite>
are tailored to measure the accuracy of the 3D scene geometry itself.
In contrast, we are interested in determining to what degree 3D reconstruction algorithms can be used to aid object pose estimation.
Thus, we evaluate the performance of a 3D reconstruction algorithm by the accuracy of the object pose estimates it facilitates.
We also evaluate how reconstruction performance (as measured by existing benchmarks) correlates to the object pose performance.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p6">
<p class="ltx_p" id="S2.p6.1"><span class="ltx_text ltx_font_bold" id="S2.p6.1.1">Object pose benchmarks. </span>
Pose evaluation datasets provide object CAD models, images depicting those objects in various positions
and the object poses <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib104" title="">104</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib111" title="">111</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib83" title="">83</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib101" title="">101</a>]</cite>.
These many datasets and benchmarks are unified in the common BOP benchmark by Hodan et al  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib47" title="">47</a>]</cite> that unifies data format, metrics, and evaluation tools.
We follow the guidelines defined in the BOP benchmark to evaluate the object pose estimation.
Given the ample set of datasets, we focus the benchmark on the YCB-V dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib111" title="">111</a>]</cite>, made out of the YCB dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib17" title="">17</a>]</cite> and that encompasses objects most representative of daily-life scenarios and robotic manipulation challenges.
Contrary to the BOP benchmark that focuses on the pose estimators in themselves, this benchmark focuses on the 3D reconstruction methods and how to improve them as to make the deployment of existing pose estimation algorithms easier in practice.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Benchmark</h2>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">The proposed benchmark answers the question of how well current 3D reconstruction algorithms can replace the CAD models commonly used in object pose estimation.
To this end, we compare the performance of <span class="ltx_glossaryref" title="">SotA</span> object pose estimators <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib12" title="">12</a>]</cite> when using the 3D reconstructed models against the use of classical CAD models.
We follow the standard object pose evaluation guidelines defined in the BOP
benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib47" title="">47</a>]</cite>
that provides accurate CAD models used as a reference for the reconstruction evaluation and to estimate the baseline object poses.
The BOP benchmark also releases images of the YCB-V <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib111" title="">111</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib17" title="">17</a>]</cite> objects that could be used for 3D reconstruction but these are captured with carefully controlled light and calibrated support.
Instead, we aim to evaluate the 3D reconstructions from images captured in setups close to real-life applications.
This motivates the automatic capture of new images with a camera mounted on a robotic arm in an uncontrolled indoor environment.
The reconstruction algorithms run on these calibrated images and the resulting
meshes are compared to the reference CAD models on their use for object
pose estimation.
Our benchmark thus consists of two components: (1) a set of novel images of a
variety of different objects with known intrinsic and extrinsic parameters
that can be used to build 3D models; (2) The set of BOP test images to evaluate the meshes on their integration into the object pose estimation pipeline. The test images show the objects in
different scenes and provide ground truth poses for the objects used to evaluate object pose estimation.
We adopt the YCB-V <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib111" title="">111</a>]</cite> objects for their variety and their
recurrence in the everyday life <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib17" title="">17</a>]</cite>.
<span class="ltx_text ltx_font_bold" id="S3.p1.1.1">Released Data:</span> The dataset contains calibrated and undistorted images for all 21 objects, object masks, and object reconstructions.
The rest of this section describes the benchmark setup.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p2">
<p class="ltx_p" id="S3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.p2.1.1">Data Acquisition. </span>
The images are collected with a Basler Ace camera
acA2440-20gc <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib1" title="">1</a>]</cite>
mounted on the flange of a 7 <span class="ltx_glossaryref" title="">Degrees-of-Freedom (DoF)</span> KUKA LBR IIWA 14 R820 (Supp-A).
The image resolution is 2448x2048 pixels and the field of view covers the whole object.
The acquisition is done in an open-floor indoor environment exposed to neon
lighting and varying sunlight through ceiling-high windows, which is typical
of daily life and industrial scenarios.
The camera exposure remains fixed during the object’s scan.
The camera and the hand-eye calibrations are run with OpenCV <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib15" title="">15</a>]</cite> and MoveIt <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib4" title="">4</a>]</cite>
with an error of up to 5mm in translation and 0.1<sup class="ltx_sup" id="S3.p2.1.2"><span class="ltx_text ltx_font_italic" id="S3.p2.1.2.1">∘</span></sup> in rotation.
The resulting camera calibration is used to undistort the images using
COLMAP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib87" title="">87</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p3">
<p class="ltx_p" id="S3.p3.7">As illustrated in (Supp-Fig.1), the objects are positioned on a
tripod such that the centroid of the object is approximately at the center of
the reachability domain of the robot.
For each object, dense outside-in views are captured by positioning
the camera on a sphere centered on the object’s centroid and with a radius <math alttext="\sim" class="ltx_Math" display="inline" id="S3.p3.1.m1.1"><semantics id="S3.p3.1.m1.1a"><mo id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><csymbol cd="latexml" id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.p3.1.m1.1d">∼</annotation></semantics></math>30cm.
The positions are evenly distributed on the sphere with latitude and longitude
spanning over [0<sup class="ltx_sup" id="S3.p3.7.1"><span class="ltx_text ltx_font_italic" id="S3.p3.7.1.1">∘</span></sup>,150<sup class="ltx_sup" id="S3.p3.7.2"><span class="ltx_text ltx_font_italic" id="S3.p3.7.2.1">∘</span></sup>] and [0<sup class="ltx_sup" id="S3.p3.7.3"><span class="ltx_text ltx_font_italic" id="S3.p3.7.3.1">∘</span></sup>,360<sup class="ltx_sup" id="S3.p3.7.4"><span class="ltx_text ltx_font_italic" id="S3.p3.7.4.1">∘</span></sup>]
respectively, with intervals of 10<sup class="ltx_sup" id="S3.p3.7.5"><span class="ltx_text ltx_font_italic" id="S3.p3.7.5.1">∘</span></sup>.
The lowest camera position, at latitude 150<sup class="ltx_sup" id="S3.p3.7.6"><span class="ltx_text ltx_font_italic" id="S3.p3.7.6.1">∘</span></sup> allows to
capture the objects from all sides.
Since the object is a source of collisions, the number of
reachable views vary
depending on the object’s shape and size. The number of images per object
varies between 397 and 505 images with an average of 480.
The motion planning and the collision avoidance are operated with
the IIWA stack <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib38" title="">38</a>]</cite> and
MoveIt <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib20" title="">20</a>]</cite> with the CHOMP planning
algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib80" title="">80</a>]</cite>.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>3D Object Reconstruction</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We evaluate 3D reconstructions based on the performance of the object pose estimation when replacing the traditional CAD models with the reconstructions.
This section describes the benchmark setup including the evaluated reconstructions, their registration with the YCB-V <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib111" title="">111</a>]</cite> CAD models,
the reconstruction metrics and the pose metrics, and the implementation details.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p2">
<p class="ltx_p" id="S4.p2.1"><span class="ltx_text ltx_font_bold" id="S4.p2.1.1">Methods. </span>
We evaluate the <span class="ltx_text ltx_font_italic" id="S4.p2.1.2">MVS methods</span> Colmap <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib87" title="">87</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib88" title="">88</a>]</cite> and Reality Capture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib82" title="">82</a>]</cite>: these methods exploit feature matching and multi-view geometry to derive a sparse 3D structure of the object later used to derive depth maps <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib11" title="">11</a>]</cite>.
The <span class="ltx_text ltx_font_italic" id="S4.p2.1.3">neural volumetric implicit representations</span> Nerfacto <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib100" title="">100</a>]</cite> and iNGP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib71" title="">71</a>]</cite> also output depth maps: a tiny MLP takes an input a position, a direction, and a learnable feature encoding to output a volume density and a color.
The depth map for a given calibrated image is generated by integrating the volumetric density along rays from the camera center to the pixels <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib49" title="">49</a>]</cite>.
Nerfacto <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib100" title="">100</a>]</cite> also includes adaptive sampling along the ray that typically helps the training focus on the useful parts of the scene, <em class="ltx_emph ltx_font_italic" id="S4.p2.1.4">i.e</em>.<span class="ltx_text" id="S4.p2.1.5"></span>, where the object is located.
For all these methods, a mesh is obtained from the depth maps using the Poisson surface reconstruction algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib52" title="">52</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p3">
<p class="ltx_p" id="S4.p3.1"><span class="ltx_text ltx_font_italic" id="S4.p3.1.1">Neural surface implicit representations</span> train a network to approximate an SDF.
UniSURF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib75" title="">75</a>]</cite> approximates the SDF by an occupancy network which root values define the surface whereas Neus <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib109" title="">109</a>]</cite> approximates the SDF directly with a neural network to reach better reconstruction accuracy.
MonoSDF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib116" title="">116</a>]</cite> integrates geometric and depth cues in the SDF learning to further improve the reconstruction quality.
In parallel, VolSDF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib113" title="">113</a>]</cite> bridges the gap between volumetric and surface representations by training a volume density network as a function of an SDF to better constrain the learning of the scene’s geometry.
BakedSDF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib114" title="">114</a>]</cite> builds on top of VolSDF and optimizes a baked version of it, <em class="ltx_emph ltx_font_italic" id="S4.p3.1.2">i.e</em>.<span class="ltx_text" id="S4.p3.1.3"></span>, pre-computed values, which improves both the geometry and the texture and accelerates the rendering.
NeuralAngelo <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib59" title="">59</a>]</cite> also addresses the runtime challenges of SDF rendering by training an SDF on learnable features represented by the hash grid introduced by the fast iNGP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib71" title="">71</a>]</cite> for volumetric representations. A mesh is obtained from the SDF using the Marching Cube algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib64" title="">64</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p4">
<p class="ltx_p" id="S4.p4.1">We also experiment with Plenoxels <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib30" title="">30</a>]</cite> that adopts a different representation where the scene is a feature grid with density values and spherical harmonics optimized with differentiable rendering.
The absence of a neural network makes the training faster than the traditional NeRF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib69" title="">69</a>]</cite> methods.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p5">
<p class="ltx_p" id="S4.p5.1">The meshes thus obtained are used in place of the original YCB-V <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib111" title="">111</a>]</cite> CAD models to generate object renderings used in the render-and-compare pose estimators.
Another way to generate rendering from implicit representations is to synthesize novel views directly from the trained network.
While this has the advantage of bypassing the mesh generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib64" title="">64</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib52" title="">52</a>]</cite>, it remains, for now, slower than physical rendering for which efficient implementations are readily available.
Still, we evaluate how well can implicit renderings serve pose estimation with iNGP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib71" title="">71</a>]</cite> for its fast rendering compared to the prohibitive runtime of other methods (<em class="ltx_emph ltx_font_italic" id="S4.p5.1.1">e.g</em>.<span class="ltx_text" id="S4.p5.1.2"></span>, the combination of Megapose and Nerfacto <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib100" title="">100</a>]</cite> takes close to 2 minutes on A100 GPU for a single image).</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p6">
<p class="ltx_p" id="S4.p6.1"><span class="ltx_text ltx_font_bold" id="S4.p6.1.1">Pose Refinement. </span>
All reconstructions are run on the undistorted images with the camera intrinsics and extrinsics collected during the data acquisition (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#S3" title="3 Benchmark ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">3</span></a>).
The camera extrinsics are obtained from the kinematic chain of the robot which can be noisy so we refine them in two steps: first with the iNGP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib71" title="">71</a>]</cite> pose refinement then with bundle adjustment <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib87" title="">87</a>]</cite> (Supp-A1).
After this step, the camera extrinsics remain fixed.
Even though some of the evaluated methods have pose refinement capabilities, we chose to turn them off in the main paper to focus on the evaluation of the reconstruction.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p7">
<p class="ltx_p" id="S4.p7.1"><span class="ltx_text ltx_font_bold" id="S4.p7.1.1">Reconstruction and Texturing. </span>
We use the default parameters for COLMAP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib87" title="">87</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib88" title="">88</a>]</cite>, which are already optimized for reconstruction purposes.
Before running the Poisson surface reconstruction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib52" title="">52</a>]</cite>, we filter out the room’s background by roughly cropping the dense point cloud around the object. A similar processing is applied to Reality Capture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib82" title="">82</a>]</cite>.
The learning-based methods undergo the pre-processings described by their respective authors (see Supp-B1).
The texture of learning-based methods is generated by querying the network for the color at the position of the mesh’s faces and Reality Capture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib82" title="">82</a>]</cite> uses proprietary code.
For methods that do not have texturing capabilities (COLMAP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib87" title="">87</a>]</cite> and iNGP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib71" title="">71</a>]</cite>), we evaluate the colored mesh since this leads to better pose estimates than if the mesh was textured with an off-the-shelf texturing software <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib106" title="">106</a>]</cite> (see Supp-C4).</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p8">
<p class="ltx_p" id="S4.p8.1"><span class="ltx_text ltx_font_bold" id="S4.p8.1.1">Mesh Registration. </span>
To enable the pose evaluation on the YCB-V <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib111" title="">111</a>]</cite> images, the reconstructions must be registered to the original YCB-V meshes because the ground-truth object poses are defined in the coordinate frame of the original meshes.
The rigid transform between the coordinate frame of the original mesh and the world coordinate frame where we placed the captured objects is estimated with
<span class="ltx_glossaryref" title="">Iterative Closest Point (ICP)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib10" title="">10</a>]</cite> initialized with a user-defined coarse alignment.
Once the 3D models are aligned, we further crop them to filter out any remaining background in the reconstructions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p9">
<p class="ltx_p" id="S4.p9.1">Some of the original YCB-V <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib111" title="">111</a>]</cite> objects were discontinued so we use the alternatives recommended by the YCB benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib5" title="">5</a>]</cite>.
Thus, the objects collected in this benchmark may differ in texture and scale,
with an average scale change of 4% of the object’s dimensions.
Given the small object’s dimensions, such changes often fall below the distance thresholds used in the metrics. Therefore, we choose to keep the scale of the collected objects to be consistent with the current object’s reality.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p10">
<p class="ltx_p" id="S4.p10.1"><span class="ltx_text ltx_font_bold" id="S4.p10.1.1">Implementation Details. </span>
The Nerfstudio <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib100" title="">100</a>]</cite> and SdfStudio <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib115" title="">115</a>]</cite> codebases
are used to run Nerfacto <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib100" title="">100</a>]</cite>, MonoSDF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib116" title="">116</a>]</cite>, VolSDF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib113" title="">113</a>]</cite>, UniSURF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib75" title="">75</a>]</cite>, Neus <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib109" title="">109</a>]</cite>, and BakedSDF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib114" title="">114</a>]</cite>.
We use the author’s codebase for all other methods and for the pose estimation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib12" title="">12</a>]</cite>, the MVS-texturing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib106" title="">106</a>]</cite>, the reconstruction evaluation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib89" title="">89</a>]</cite> and the pose evaluation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib3" title="">3</a>]</cite>.
The mesh registration and post-processing are run with Open3D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib118" title="">118</a>]</cite>.
All methods run on a single NVIDIA A100 GPU except for RealityCapture that runs on a GeForce RTX 3060.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental Evaluation</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We recall the metrics to evaluate the reconstructions and the object poses estimated with those. The baseline is made of poses estimated with the original CAD models (<span class="ltx_text ltx_font_bold" id="S5.p1.1.1">Oracle</span>).
We then analyze how the quality of the estimated poses is impacted by the object properties, the texturing, and the amount of images used in the reconstruction.</p>
</div>
<figure class="ltx_figure" id="S5.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="970" id="S5.F1.1.g1" src="x1.png" width="683"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F1.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="944" id="S5.F1.2.g1" src="x2.png" width="832"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F1.4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="116" id="S5.F1.3.g1" src="x3.png" width="166"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="135" id="S5.F1.4.g2" src="x4.png" width="160"/></div>
</div>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
<span class="ltx_text ltx_font_bold" id="S5.F1.6.1">3D Reconstruction Evaluation.</span>
Left: Most reconstructions exhibit high geometric quality.
Center: 3D reconstructions evaluated with the Average Recall (AR) of Megapose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib55" title="">55</a>]</cite> (dashed and dotted) and FoundationPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib12" title="">12</a>]</cite> (uniform) when replacing the CAD models with the reconstructions.
The pose evaluation is more discriminative than the geometric one (left) and the relative ranking of the methods is consistent between the two pose estimators.
Right: Runtime, mesh appearance quality and pose quality <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib55" title="">55</a>]</cite>. Nerfacto <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib100" title="">100</a>]</cite> and Reality Capture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib82" title="">82</a>]</cite> achieve the best peformances.
</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text ltx_font_bold" id="S5.p2.1.1">Reconstruction Metrics. </span>
The reconstructed geometries are evaluated with the completeness and
accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib89" title="">89</a>]</cite> over distance thresholds ranging from 1mm to 2cm.
The <span class="ltx_text ltx_font_italic" id="S5.p2.1.2">completeness</span> measures how much of the reference mesh is modeled by the
reconstructed one: it is the ratio of reference points which nearest-neighbor
in the reconstructed mesh is within a distance threshold.
The <span class="ltx_text ltx_font_italic" id="S5.p2.1.3">accuracy</span> assesses how close the reconstructed mesh is to the reference one with the ratio of reconstructed points within a distance threshold to the reference points.
These metrics are sensitive to the mesh’s density so we first
subsample the reconstructed meshes to a fixed-size set of points before evaluation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p3">
<p class="ltx_p" id="S5.p3.1"><span class="ltx_text ltx_font_bold" id="S5.p3.1.1">Object Pose Estimation and Metrics. </span>
The object poses are estimated with the <span class="ltx_glossaryref" title="">SotA</span> RGB Megapose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib55" title="">55</a>]</cite> and RGB-D FoundationPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib12" title="">12</a>]</cite>.
The evaluation follows the protocol of the standard BOP benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib47" title="">47</a>]</cite> and reports the <span class="ltx_text ltx_font_bold" id="S5.p3.1.2">Average Recall (AR)</span> on the three errors defined next.
These errors do not measure the object pose translation and rotation errors but measure the misalignment of the object when it is positioned with the estimated pose and with the ground-truth pose.
Such assessment not only accounts for the poses’ discrepancy but also the impact of said discrepancy on the perception or manipulation of the object, which is a relevant indicator in industrial applications.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">The <span class="ltx_glossaryref" title="">Visible Surface Discrepancy (VSD)</span> measures the distance between the surfaces of the object at the ground-truth and estimated poses.
The <span class="ltx_glossaryref" title="">Maximum Symmetry-Aware Surface Distance (MSSD)</span> measures the maximum surface distance
between the two poses.
This metric is symmetry-aware, <em class="ltx_emph ltx_font_italic" id="S5.p4.1.1">i.e</em>.<span class="ltx_text" id="S5.p4.1.2"></span>, it does not penalize the estimated pose for ambiguous symmetries.
The <span class="ltx_glossaryref" title="">Maximum Symmetry-Aware Projection Distance (MSPD)</span> measures the visual discrepancy in pixel distance induced by an incorrect pose when rendering the object.
The fraction of images for which the errors are below a given threshold defines the recall. It is averaged over several error thresholds and over images to form the <span class="ltx_text ltx_font_bold" id="S5.p4.1.3">AR</span> for each error. The resulting three ARs are averaged to form the global AR that we report next.
<span class="ltx_text ltx_font_italic" id="S5.p4.1.4">Interpretation: </span>
the VSD and MSSD are good indicators of the object pose quality for navigation or object-manipulation applications while the MSPD is better suited for applications that exploit object rendering, such as Augmented and Virtual Reality.</p>
</div>
<figure class="ltx_figure" id="S5.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="180" id="S5.F2.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span class="ltx_text ltx_font_bold" id="S5.F2.2.1">Pose Evaluation on Object Categories.</span>
Average Recall (AR) of Megapose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib55" title="">55</a>]</cite> (dashed and dotted bar) and FoundationPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib12" title="">12</a>]</cite> (uniform bar) when replacing the CAD models with the reconstructions.
Some categories expected to be challenging (uniform-texture objects) are not whereas
small and shiny objects are more challenging.
</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.p5">
<p class="ltx_p" id="S5.p5.1"><span class="ltx_text ltx_font_bold" id="S5.p5.1.1">Reconstruction and Pose Scores. </span>

Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#S5.F1" title="Figure 1 ‣ 5 Experimental Evaluation ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">1</span></a> shows the geometric quality of the reconstructions
(left), their performance when integrated into FoundationPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib12" title="">12</a>]</cite> and Megapose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib55" title="">55</a>]</cite> (center), and the correlation between the estimated poses <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib55" title="">55</a>]</cite>, the appearance quality of the mesh, and the reconstruction runtime (right).
The results demonstrate the practical advantage of the MVS-based Reality Capture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib82" title="">82</a>]</cite> that achieves <span class="ltx_glossaryref" title="">SotA</span> reconstruction performance under the shortest runtime (recall that RealityCapture is run on a weaker GPU).
Overall, there remains a 10-20% gap in quality between CAD models and the best 3D reconstructions
(Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#S5.F1" title="Figure 1 ‣ 5 Experimental Evaluation ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">1</span></a>-center).
Given that the relative pose performances of the reconstructions are consistent between the two pose estimators, one can expect advances in 3D reconstruction to close the gap for other pose estimators.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p6">
<p class="ltx_p" id="S5.p6.1">The reconstructions best for pose estimation usually exhibit high geometric and appearance quality.
Still, there is no direct correlation between the reconstruction metrics and the pose scores: for example the geometric completeness of UniSURF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib75" title="">75</a>]</cite> and iNGP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib71" title="">71</a>]</cite>
is relatively low, which is reflected in the pose performance.
At the same time, RealityCapture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib82" title="">82</a>]</cite> performs comparably to BakedSDF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib114" title="">114</a>]</cite> in terms of completeness and accuracy but leads to slightly worse poses.
Conversely, the reconstructed geometry of Nerfacto <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib100" title="">100</a>]</cite> is worse than MonoSDF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib116" title="">116</a>]</cite> but their pose evaluation is on par.
One interesting observation is that the geometric evaluation (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#S5.F1" title="Figure 1 ‣ 5 Experimental Evaluation ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">1</span></a>-left) does not discriminate between VolSDF and MonoSDF (superimposed red and cyan curves) whereas the pose evaluation does.
This validates the proposed reconstruction evaluation that not only measures the accuracy of the reconstruction itself but also the quality of the pose estimation it enables.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p7">
<p class="ltx_p" id="S5.p7.1">There is also no immediate correlation between the quality of the mesh’s appearance and the pose quality (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#S5.F1" title="Figure 1 ‣ 5 Experimental Evaluation ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">1</span></a>-right).
For example, Reality Capture reaches a higher PSNR than BakedSDF yet the latter achieves better pose performances.
As for Colmap <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib87" title="">87</a>]</cite> and NeuralAngelo <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib59" title="">59</a>]</cite>, they exhibit similar PSNR but the first leads to much worse pose estimation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p8">
<p class="ltx_p" id="S5.p8.1">Another remarkable result is the high performance when Megapose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib55" title="">55</a>]</cite> uses the iNGP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib71" title="">71</a>]</cite> implicit renderings instead of the iNGP mesh’s ones.
This shows that an off-the-shelf implicit representation can be integrated with a pose estimator, which completes recent contributions that train pose estimation together with implicit renderings <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib18" title="">18</a>]</cite>.
This also suggests that pose estimators can perform well even on objects hard to reconstruct as long as the novel view synthesis is good.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p9">
<p class="ltx_p" id="S5.p9.1"><span class="ltx_text ltx_font_bold" id="S5.p9.1.1">Finer Pose Evaluation. </span>
The BOP benchmark defines multiple metrics to assess how suitable the pose estimator, hence the 3D reconstruction, is for a given application such as augmented and virtual reality (MSPD), navigation (VSD), and object manipulation (MSSD).
Due to the page limit, we report detailed results in Supp-C1.
The VSD recall remains close to the baseline so objects are overall well-positioned, which is useful for global object reasoning such as object avoidance and navigation.
However, there is a drop in MSSD recall that is particularly sensitive to small misplacements when objects exhibit high curvature regions.
This is typical of grasping points so this signals a limitation when using reconstructed meshes for object manipulation.
Another limitation relates to virtual reality because of the MSPD recall drop that indicates visual dissonance when rendering objects.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p10">
<p class="ltx_p" id="S5.p10.1"><span class="ltx_text ltx_font_bold" id="S5.p10.1.1">Object Properties.</span>
We split the YCB-V <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib111" title="">111</a>]</cite> objects into 8 groups based on their shape, texture, material properties, and the degree of change between the collected objects and the original ones. We show the results for 3 groups in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#S5.F2" title="Figure 2 ‣ 5 Experimental Evaluation ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">2</span></a> and the rest in Supp-C.3. The pose performance variations across groups are telling of some of the current 3D reconstruction challenges.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p11">
<p class="ltx_p" id="S5.p11.1">For simple objects such as large objects with lambertian texture, whether uniform or not, 3D reconstructions can replace CAD models without a significant drop in performance for both pose estimators.
These results on objects with little or uniform texture suggest that the reconstructed geometry is reliable for pose estimation: since the texture has little information, it marginally contributes to the pose estimation contrary to the geometry.
Also, this suggests that imperfect texturing is not necessarily prohibitive for pose estimation on objects with little texture.</p>
</div>
<figure class="ltx_figure" id="S5.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F3.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="424" id="S5.F3.1.g1" src="x6.png" width="830"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F3.3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="248" id="S5.F3.2.g1" src="x7.png" width="831"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="488" id="S5.F3.3.g2" src="x8.png" width="827"/></div>
</div>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>
<span class="ltx_text ltx_font_bold" id="S5.F3.6.1">Reconstruction from image sets with increasing size</span> (left).
Average Recall (AR) of Megapose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib55" title="">55</a>]</cite> when replacing the object’s CAD model with the reconstruction; and
geometric evaluation of the reconstruction with completeness under a 5mm error threshold.
Note that the performance of RealityCapture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib82" title="">82</a>]</cite> remains stable when the number of images goes down and runs extremely fast: on average, it takes less than 1 min. on 25 images and 2 min. on 50 images.
<span class="ltx_text ltx_font_bold" id="S5.F3.7.2">Subpar reconstructions and good pose estimation</span> (right).
The reconstructed bowls <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib82" title="">82</a>]</cite>
both miss parts yet the left one reaches oracle performance (AR=38) whereas the one on the right is subpar (AR=15).
Both wood blocks
(AR=39) outperform the oracle (AR=27) even though the left texture is incorrect.
</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.p12">
<p class="ltx_p" id="S5.p12.1">Conversely, the reconstruction of objects with shiny or small texture elements performs worse than other categories, independently of the pose estimators.
Several factors contribute to this drop.
First, several of the objects in these categories have been discontinued or got their texture updated. The texture inconsistency between the test images and our reconstructions impedes the pose estimation, which is confirmed by the higher pose performance on the ‘legacy’ objects‘ than on the updated ones.
Another factor
is that small and shiny textures are harder to generate than Lambertian ones.
When it comes to small objects (<math alttext="\leq" class="ltx_Math" display="inline" id="S5.p12.1.m1.1"><semantics id="S5.p12.1.m1.1a"><mo id="S5.p12.1.m1.1.1" xref="S5.p12.1.m1.1.1.cmml">≤</mo><annotation-xml encoding="MathML-Content" id="S5.p12.1.m1.1b"><leq id="S5.p12.1.m1.1.1.cmml" xref="S5.p12.1.m1.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S5.p12.1.m1.1c">\leq</annotation><annotation encoding="application/x-llamapun" id="S5.p12.1.m1.1d">≤</annotation></semantics></math> 8cm), the reconstruction generates a low-resolution texture, which decreases the amount of visual information relevant to disambiguating the estimated poses.
This is all the more critical as those objects exhibit symmetries that can only be differentiated through their texture (07-pudding-box, 06-tuna-fish-can).
As for shiny objects, the light reflection induces a visual discrepancy between the target texture and the reconstructed one.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p13">
<p class="ltx_p" id="S5.p13.1">None of the top-performing methods exhibit an advantage over the others for the more challenging object types.
Given their ability to model view-dependent effects such as reflections, one would have expected NeRF-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib100" title="">100</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib71" title="">71</a>]</cite> to significantly outperform RealityCapture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib82" title="">82</a>]</cite> which does not model view-dependent effects, but this is not the case.
These results suggest that the texture generation of the 3D reconstruction has room for improvement.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p14">
<p class="ltx_p" id="S5.p14.1"><span class="ltx_text ltx_font_bold" id="S5.p14.1.1">Number of images.</span>
Besides the quality of the pose estimation, an important property of the reconstruction methods is their data requirement.
Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.F8" title="Figure 8 ‣ C.2 Reconstruction with varying training size ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">8</span></a>-left shows the geometric and Megapose evaluation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib55" title="">55</a>]</cite> of reconstructions generated from varying numbers of images sampled uniformly around the object <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib33" title="">33</a>]</cite>. As expected, the results get better with the number of images and 75-100 images is enough to generate reconstructions close to the best ones.
However, we observe that using sparse views can hinder the convergence of some reconstructions.
We also observe that the performance does not increase linearly with the data
which we attribute to non-uniform noise in the camera poses.
These results show that another avenue for improvement includes robustness to noisy calibration and reconstructions from limited views.
In parallel, qualitative results in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.F8" title="Figure 8 ‣ C.2 Reconstruction with varying training size ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">8</span></a>-right illustrate examples where reconstructions are visually subpar yet informative enough for satisfying pose estimation.
This suggests that incomplete meshes can be relevant for object pose estimation as long as the discriminative parts of the objects are reconstructed such as the outer edge of the bowl.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions</h2>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this work, we have considered the problem of reconstructing 3D models from images such that they can be used for object pose estimation.
In particular, we aimed to answer the question of whether state-of-the-art 3D reconstruction algorithms can be readily used to remove the need for CAD models during training and testing of object pose estimators.
We have created a new benchmark, based on the YCB-V <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib111" title="">111</a>]</cite> dataset, that can be used to measure the impact of replacing CAD models with reconstructed meshes.
We have evaluated multiple state-of-the-art reconstruction methods and our results show that reconstructed models or their implicit render can often readily replace CAD models.
Still, there remains a considerable gap between CAD models and image-based meshes for certain types of objects that are hard to reconstruct, <em class="ltx_emph ltx_font_italic" id="S6.p1.1.1">e.g</em>.<span class="ltx_text" id="S6.p1.1.2"></span>, small and / or shiny objects.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Several interesting observations can be drawn from our results:
(1) The classical, handcrafted Reality Capture often performs similarly to state-of-the-art learning-based approaches.
It is also very efficient and able to reconstruct scenes within a few minutes, which makes it a strong baseline for future work.
(2) The ranking between different 3D reconstruction approaches is quite consistent between the two evaluated pose estimators.
There thus is hope that all pose estimators will automatically benefit from advances in the area of 3D reconstruction.
(3) Incomplete reconstructions, where parts of the object cannot be reconstructed, are not necessarily problematic as the resulting models can still enable accurate pose estimation as long as enough parts are reconstructed.
This observation validates our idea of evaluating reconstruction systems inside a higher-level task as classical reconstruction-based metrics, <em class="ltx_emph ltx_font_italic" id="S6.p2.1.1">e.g</em>.<span class="ltx_text" id="S6.p2.1.2"></span>, accuracy and completeness, are falsely indicating the resulting models as low-quality reconstructions.
(4) There is not a clear relation between the quality of the estimated 3D scene geometry and pose accuracy. For example, the renderings produced directly by the iNGP network achieve quite good performance with Megapose whereas the renderings of the iNGP mesh fall behind.
This is a promising result as it suggests that objects hard to reconstruct can still allow accurate pose estimation if the novel view synthesis performs well.
(5) In practice, one would ideally use as few images as possible for the reconstruction to minimize both capture and reconstruction time.
Our results indicate room for improvement in the few-image regime.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">While our results show that using reconstructed models is a promising direction for removing the need for CAD models, we also observe that the problem is far from solved.
We believe that our benchmark will help drive research on approaches that can accurately reconstruct challenging objects from as few images as possible.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para ltx_noindent" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This work was supported by the Czech Science Foundation (GACR) EXPRO (grant no. 23-07973X).
This work was supported by the Ministry of Education, Youth and Sports of the Czech Republic through the e-INFRA CZ (ID:90254).
The authors would like to acknowledge the “Research and Innovation Centre on Advanced Industrial Production (RICAIP)“ supported by European Structural and Investment Funds and Operational Programme Research, Development and Education via the Ministry of Education, Youth and Sports of the Czech Republic (ID: CZ.02.1.01/0.0/0.0/17_043/0010085) and from the European Union’s Horizon 2020 research and innovation program under grant agreement 857306.
This work was supported by the Grant Agency of the Czech Technical University in Prague, grant No.SGS23/172/OHK3/3T/13.
This work was co-funded by European Union under the project Robotics and advanced industrial production - ROBOPROX (reg. no. CZ.02.01.01/00/22_008/0004590).</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_logical-block ltx_align_bottom" id="Sx1.1">
<div class="ltx_para" id="Sx1.1.p1">
<p class="ltx_p" id="Sx1.1.p1.1"><span class="ltx_rule" style="width:100%;height:4.0pt;background:black;display:inline-block;"> </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold ltx_align_center" id="Sx1.1.p1.1.1">Supplementary: Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation</span></p>
<br class="ltx_break"/>
<p class="ltx_p" id="Sx1.1.p1.2"><span class="ltx_rule" style="width:100%;height:1.0pt;background:black;display:inline-block;"> </span>
<br class="ltx_break"/></p>
</div>
</div>
<div class="ltx_para ltx_noindent" id="Sx1.p2">
<p class="ltx_p" id="Sx1.p2.1">The supplementary material is organized as follows: Sec. <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A1" title="Appendix A Dataset ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">A</span></a> describes the data acquisition and the information related to the release as announced in Sec.3 of the main paper.
Sec. <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A2" title="Appendix B Evaluation Setup ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">B</span></a> provides implementation details relative to the reconstructions, the pose estimation, and the evaluation, as announced in Sec. 4 of the main paper.
Sec.<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3" title="Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">C</span></a> reports results complementary to Sec. 5 of the main paper.</p>
</div>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Dataset</h2>
<div class="ltx_para ltx_noindent" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">The proposed benchmark answers the question to what degree 3D models reconstructed by current 3D reconstruction algorithms can replace the CAD models commonly used for object pose estimation.
To this end, we compare the performance of two <span class="ltx_glossaryref" title="">SotA</span> object pose estimators, FoundationPose and Megapose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib12" title="">12</a>]</cite>, when using the 3D reconstructed models against the use of classical CAD models.
Thus, we mainly measure 3D reconstruction performance by the accuracy of the estimated poses rather than the accuracy of the resulting 3D models itself.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.p2">
<p class="ltx_p" id="A1.p2.1">The evaluation of the pose estimators runs on the classic YCB-V <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib111" title="">111</a>]</cite> pose estimation dataset and follows the standard guidelines defined in the BOP
benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib47" title="">47</a>]</cite>.
The YCB-V dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib111" title="">111</a>]</cite> is made of 21 objects (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A1.F5" title="Figure 5 ‣ A.2 Data Release ‣ Appendix A Dataset ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">5</span></a>) selected from the YCB dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib17" title="">17</a>]</cite>, including small objects, objects with low texture, complex shapes, high reflectance, and multiple symmetries.
For each object, our benchmark consists of two components:
<span class="ltx_text ltx_font_bold" id="A1.p2.1.1">(1)</span> Images of the object captured by a robot arm that can be used for 3D reconstruction.
<span class="ltx_text ltx_font_bold" id="A1.p2.1.2">(2)</span> A set of test images for object pose estimation with ground truth poses registered with the image sets from (1).
Compared to the data for the first component, which we captured ourselves, we use the test images provided by the YCB-V <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib111" title="">111</a>]</cite> dataset for the second component.
Given the two components, we evaluate multiple state-of-the-art 3D reconstruction methods.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.p3">
<p class="ltx_p" id="A1.p3.1">In this section, we describe how the images in <span class="ltx_text ltx_font_bold" id="A1.p3.1.1">(1)</span> were collected (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A1.SS1" title="A.1 Data Collection Setup ‣ Appendix A Dataset ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">A.1</span></a>) and how the data is released (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A1.SS2" title="A.2 Data Release ‣ Appendix A Dataset ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">A.2</span></a>).</p>
</div>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Data Collection Setup</h3>
<div class="ltx_para ltx_noindent" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.p1.1.1">Data Acquisition. </span>
The images are collected with a Basler Ace camera
acA2440-20gc <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://docs.baslerweb.com/aca2440-20gc</span></span></span>
mounted on the flange of a 7 <span class="ltx_glossaryref" title="">DoF</span> KUKA LBR IIWA 14 R820 (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A1.F4" title="Figure 4 ‣ A.1 Data Collection Setup ‣ Appendix A Dataset ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">4</span></a>).
The image resolution prior to undistortion is 2448x2048 pixels and the field of view covers the whole object.
The acquisition is done in an open-floor indoor environment exposed to neon lighting and varying sunlight through ceiling-high windows, which is typical of daily life and industrial scenarios.
The camera exposure remains fixed during the object’s scan.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS1.p2">
<p class="ltx_p" id="A1.SS1.p2.1">The camera is calibrated using open-source solutions: the OpenCV routine for the camera calibration <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib15" title="">15</a>]</cite> and the MoveIt <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib20" title="">20</a>]</cite> routine for the hand-eye calibration <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://github.com/ros-planning/moveit_calibration</span></span></span>.
Afterward, the camera can be positioned with an error of up to 5mm in translation and 0.1<sup class="ltx_sup" id="A1.SS1.p2.1.1"><span class="ltx_text ltx_font_italic" id="A1.SS1.p2.1.1.1">∘</span></sup> in rotation.
The resulting camera calibration is used to undistort the images using the open-source COLMAP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib87" title="">87</a>]</cite> library.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS1.p3">
<p class="ltx_p" id="A1.SS1.p3.7">The objects are positioned on a tripod such that the centroid of the object is approximately at the center of the reachability domain of the robot.
For each object, dense outside-in views are captured by positioning the camera on a sphere centered on the object’s centroid and with a radius <math alttext="\sim" class="ltx_Math" display="inline" id="A1.SS1.p3.1.m1.1"><semantics id="A1.SS1.p3.1.m1.1a"><mo id="A1.SS1.p3.1.m1.1.1" xref="A1.SS1.p3.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="A1.SS1.p3.1.m1.1b"><csymbol cd="latexml" id="A1.SS1.p3.1.m1.1.1.cmml" xref="A1.SS1.p3.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p3.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p3.1.m1.1d">∼</annotation></semantics></math>30cm.
The positions are evenly distributed on the sphere with latitude and longitude spanning over [0<sup class="ltx_sup" id="A1.SS1.p3.7.1"><span class="ltx_text ltx_font_italic" id="A1.SS1.p3.7.1.1">∘</span></sup>,150<sup class="ltx_sup" id="A1.SS1.p3.7.2"><span class="ltx_text ltx_font_italic" id="A1.SS1.p3.7.2.1">∘</span></sup>] and [0<sup class="ltx_sup" id="A1.SS1.p3.7.3"><span class="ltx_text ltx_font_italic" id="A1.SS1.p3.7.3.1">∘</span></sup>,360<sup class="ltx_sup" id="A1.SS1.p3.7.4"><span class="ltx_text ltx_font_italic" id="A1.SS1.p3.7.4.1">∘</span></sup>] respectively, with intervals of 10<sup class="ltx_sup" id="A1.SS1.p3.7.5"><span class="ltx_text ltx_font_italic" id="A1.SS1.p3.7.5.1">∘</span></sup>.
The lowest camera position at latitude 150<sup class="ltx_sup" id="A1.SS1.p3.7.6"><span class="ltx_text ltx_font_italic" id="A1.SS1.p3.7.6.1">∘</span></sup> allows to capture views of the objects from all sides.
Since the object is a source of collisions, the number of reachable views varies depending on the object’s shape and size.
The number of images per object varies between 397 and 505 images with an average of 480.
The motion planning and the collision avoidance are operated with the proprietary IIWA stack <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib38" title="">38</a>]</cite> and the open-source MoveIt <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib20" title="">20</a>]</cite> framework using the CHOMP planning algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib80" title="">80</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="A1.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="251" id="A1.F4.g1" src="extracted/5794104/robot_capture_cropped_anon.png" width="359"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span class="ltx_text ltx_font_bold" id="A1.F4.2.1">Data Acquisition.</span>
The objects are mounted on a tripod.
A Basler Ace camera mounted on a 7DoF KUKA arm autonomously captures views from all sides of the YCB-V <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib111" title="">111</a>]</cite> objects.
</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="A1.SS1.p4">
<p class="ltx_p" id="A1.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.p4.1.1">Pose Refinement. </span>
The camera extrinsics of the collected images are obtained from the kinematic chain of the robot that can be relatively noisy.
One source of such noise is in the robotic arm’s positional encoders: they are known to be noisier in configurations that are close to degenerate, <em class="ltx_emph ltx_font_italic" id="A1.SS1.p4.1.2">e.g</em>.<span class="ltx_text" id="A1.SS1.p4.1.3"></span>, when the arm is extended.
This can affect the quality of the reconstruction methods so we refine the camera extrinsics in two steps:
i) we use sequentially the pose refinement capabilities of iNGP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib71" title="">71</a>]</cite> that update the poses consistently with the trained 3D reconstruction; ii) we then use the iNGP-refined poses to generate a sparse 3D reconstruction and refine the camera poses again with bundle adjustment using COLMAP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib87" title="">87</a>]</cite>.
After this step, the camera extrinsics remain fixed.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS1.p5">
<p class="ltx_p" id="A1.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.p5.1.1">Image Registration. </span>
The YCB-V dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib111" title="">111</a>]</cite> provides test images depicting the objects and their ground-truth poses.
These poses are the rotation and the translation between the camera coordinate frame and the object’s coordinate frame, which is the coordinate frame of the original CAD model.
This coordinate frame is different from the ‘world’ coordinate frame where we collect the images and reconstruct the objects.
Thus, the YCB-V ground-truth poses are not compatible with the coordinate frame of the 3D reconstructions.
Consequently, we register the collected images to the original CAD model for each object.
To do so, we first generate 3D reconstructions for all objects in their coordinate frame, and then estimate the rigid transform between the reconstruction’s coordinate frame and the CAD model’s coordinate frame with <span class="ltx_glossaryref" title="">ICP</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib118" title="">118</a>]</cite>.
The ICP is initialized with user-defined coarse alignments.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS1.p6">
<p class="ltx_p" id="A1.SS1.p6.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.p6.1.1">Object masks. </span>
Once the images are registered with the original YCB-V <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib111" title="">111</a>]</cite>’s mesh, we generate object masks by projecting the mesh onto the image.
We sample a dense set of points on the mesh and project them on the image using the intrinsics and the registered extrinsics.
The pixels where the projected points fall form the object’s mask.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS1.p7">
<p class="ltx_p" id="A1.SS1.p7.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.p7.1.1">Subset Generation. </span>
In addition to the full set of captured images, we define image sets of different sizes as, in certain applications, data capture and 3D reconstruction times can be important.
Providing smaller subsets of images allows us to simulate such scenarios.
We provide subsets of the following sizes: 25, 50, 75, 100, 150, and 300.
The subsets are generated from the full set of images with Fibonacci sampling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib33" title="">33</a>]</cite> to ensure that the subset views cover the object uniformly.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Data Release</h3>
<div class="ltx_para ltx_noindent" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">The released data is available at</p>
</div>
<div class="ltx_para" id="A1.SS2.p2">
<p class="ltx_p ltx_align_center" id="A1.SS2.p2.1"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://github.com/VarunBurde/reconstruction_pose_benchmark" title="">github.com/VarunBurde/reconstruction_pose_benchmark</a></p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS2.p3">
<p class="ltx_p" id="A1.SS2.p3.1">The webpage contains instructions on how to download the data from the Apache server, either through a web page or a simple command line tool, <em class="ltx_emph ltx_font_italic" id="A1.SS2.p3.1.1">e.g</em>.<span class="ltx_text" id="A1.SS2.p3.1.2"></span>, <span class="ltx_text ltx_font_typewriter" id="A1.SS2.p3.1.3">wget</span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS2.p4">
<p class="ltx_p" id="A1.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="A1.SS2.p4.1.1">Released Data and Format. </span> We release the calibrated undistorted images of the 21 YCB-V <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib111" title="">111</a>]</cite> objects collected in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A1.SS1" title="A.1 Data Collection Setup ‣ Appendix A Dataset ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">A.1</span></a>, the objects’ masks, and the meshes generated by the reconstruction methods evaluated in the benchmark.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS2.p5">
<p class="ltx_p" id="A1.SS2.p5.1">For each object, the undistorted images, their extrinsics, the camera intrinsics, and the object masks are packed in a single zip file.
The images and the masks are in <span class="ltx_text ltx_font_typewriter" id="A1.SS2.p5.1.1">png</span> format, the extrinsics and intrinsics are released under both the Nerfstudio <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib100" title="">100</a>]</cite> camera pose convention <span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://docs.nerf.studio/quickstart/data_conventions.html" title="">docs.nerf.studio/quickstart/data_conventions.html</a></span></span></span>
and the COLMAP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib87" title="">87</a>]</cite> camera pose convention <span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://colmap.github.io/format.html#text-format" title="">colmap.github.io/format.html#text-format</a></span></span></span>
These are two of the most commonly used camera pose conventions for 3D reconstruction.
Dataparsers for these two formats are available at the respective repositories.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS2.p6">
<p class="ltx_p" id="A1.SS2.p6.1">For each reconstruction method, a single zip file contains all reconstructed objects.
The 3D reconstructions are saved under the standard <span class="ltx_text ltx_font_typewriter" id="A1.SS2.p6.1.1">obj</span> format usually used by 3D processing libraries, <em class="ltx_emph ltx_font_italic" id="A1.SS2.p6.1.2">e.g</em>.<span class="ltx_text" id="A1.SS2.p6.1.3"></span>, Meshlab <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib21" title="">21</a>]</cite>, Open3D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib118" title="">118</a>]</cite>, trimesh <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib24" title="">24</a>]</cite>, and Pytorch3D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib81" title="">81</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="A1.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="352" id="A1.F5.g1" src="x9.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span class="ltx_text ltx_font_bold" id="A1.F5.2.1">The 21 YCB-V <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib111" title="">111</a>]</cite> objects</span> rendered from original CAD models.
</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="A1.SS2.p7">
<p class="ltx_p" id="A1.SS2.p7.1"><span class="ltx_text ltx_font_bold" id="A1.SS2.p7.1.1">License. </span>
The collected images and the reconstructed meshes are released under the CC BY 4.0 license: the license is conditioned on the license of the YCB <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib17" title="">17</a>]</cite> objects depicted in the images.
The YCB objects are released under the Creative Commons Attribution 4.0 International (CC BY 4.0) <span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><span class="ltx_text ltx_font_typewriter" id="footnote5.1">http://ycb-benchmarks.s3-website-us-east-1.amazonaws.com/</span></span></span></span>, so the data is released under the same license.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS2.p8">
<p class="ltx_p" id="A1.SS2.p8.1"><span class="ltx_text ltx_font_bold" id="A1.SS2.p8.1.1">Benchmark Reproducibility. </span>
The benchmark uses open-source code for the reconstructions, the pose estimation, and the evaluation.
The only exception may be the code of the reconstruction method RealityCapture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib82" title="">82</a>]</cite>, which is free except for companies with high earnings <span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.capturingreality.com/pricing-changes" title="">https://www.capturingreality.com/pricing-changes</a></span></span></span>.
Unless mentioned otherwise in Sec.B1, the default parameters of the reconstruction methods are used.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Evaluation Setup</h2>
<div class="ltx_para ltx_noindent" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">Compared to existing benchmarks for 3D reconstruction, our benchmark does not treat 3D reconstruction as a task unto itself but rather evaluates the resulting 3D models inside a higher-level task, <em class="ltx_emph ltx_font_italic" id="A2.p1.1.1">i.e</em>.<span class="ltx_text" id="A2.p1.1.2"></span>, object pose estimation.
Thus, we mainly measure 3D reconstruction performance by the accuracy of the estimated poses rather than the accuracy of the resulting 3D models itself.
To this end, we compare the performance of two <span class="ltx_glossaryref" title="">SotA</span> object pose estimators <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib12" title="">12</a>]</cite> when using the 3D reconstructed models against the use of classical CAD models.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.p2">
<p class="ltx_p" id="A2.p2.1">In the rest of this section, we report the implementation details relative to the evaluated 3D reconstructions (Sec <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A2.SS1" title="B.1 Reconstruction Implementation Details ‣ Appendix B Evaluation Setup ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">B.1</span></a>), the pose estimators used for the evaluation (Sec <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A2.SS2" title="B.2 Pose Estimators: FoundationPose and Megapose ‣ Appendix B Evaluation Setup ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">B.2</span></a>), the pose evaluation metrics (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A2.SS3" title="B.3 BOP Evaluation ‣ Appendix B Evaluation Setup ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">B.3</span></a>), and the nature of the objects in the evaluation dataset (Sec <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A2.SS4" title="B.4 Object Categorization ‣ Appendix B Evaluation Setup ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">B.4</span></a>).</p>
</div>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Reconstruction Implementation Details</h3>
<div class="ltx_para ltx_noindent" id="A2.SS1.p1">
<p class="ltx_p" id="A2.SS1.p1.1">We describe the experimental setup related to the 3D reconstruction.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS1.p2">
<p class="ltx_p" id="A2.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="A2.SS1.p2.1.1">Colmap <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib87" title="">87</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib88" title="">88</a>]</cite>. </span>
We use the default parameters of COLMAP’s triangulation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib87" title="">87</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib88" title="">88</a>]</cite>, which are already optimized for the purpose of reconstruction.
The feature extraction returns RootSIFT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib65" title="">65</a>]</cite> features and the feature matching runs only between covisible image pairs: two images are covisible if they form an angle smaller than 45<sup class="ltx_sup" id="A2.SS1.p2.1.2"><span class="ltx_text ltx_font_italic" id="A2.SS1.p2.1.2.1">∘</span></sup> with respect to the object’s center.
Before the Poisson <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib52" title="">52</a>]</cite> mesh reconstruction, we filter out the room’s background by roughly cropping the dense point cloud around the object.
We run the Poisson surface reconstruction with parameters <span class="ltx_text ltx_font_typewriter" id="A2.SS1.p2.1.3">depth=10, trim=1</span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS1.p3">
<p class="ltx_p" id="A2.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="A2.SS1.p3.1.1">Reality Capture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib82" title="">82</a>]</cite>. </span>
Given the known camera extrinsics and intrinsics, we first obtain a sparse point cloud model using the "draft" mode, which downsamples the images by a factor of two.
For the following processing steps, a reconstruction region is defined as a bounding box (roughly) around the camera positions.
All scene parts outside the region are ignored.
A 3D mesh is then computed for this region in "normal detail", which also uses images downsampled by a factor of two.
This reconstruction stage first computes depth maps to obtain a dense point cloud, from which a mesh is then extracted.
Typically, there are artifacts in the form of additional connected components.
We only keep the largest connected component, which in all cases corresponded to (parts of) the object.
The remaining mesh is then textured.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS1.p4">
<p class="ltx_p" id="A2.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="A2.SS1.p4.1.1">Implicit Methods. </span>
Nerfacto <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib100" title="">100</a>]</cite> is trained for 30K iterations without pose refinement, with normal prediction, and with the default parameters set by Nerfstudio <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib100" title="">100</a>]</cite>.
iNGP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib71" title="">71</a>]</cite> is trained with the default parameters set by the authors for 30K iterations.
A mesh is extracted from the resulting models using the Poisson surface reconstruction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib52" title="">52</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS1.p5">
<p class="ltx_p" id="A2.SS1.p5.1">MonoSDF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib116" title="">116</a>]</cite>, VolSDF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib113" title="">113</a>]</cite>, BakedSDF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib114" title="">114</a>]</cite>, and Neus <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib109" title="">109</a>]</cite> are trained with the default configuration set in SdfStudio <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib115" title="">115</a>]</cite>, NeuralAngelo <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib59" title="">59</a>]</cite> and Plenoxel <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib30" title="">30</a>]</cite> with the configurations set by the authors.
VolSDF and Neus are trained for 100k steps, MonoSDF for 200k iterations, BakedSDF for 250k iterations, NeuralAngelo for 500K iterations, and Plexoxel for 128K iterations, as recommended by their respective authors.
These methods output an <span class="ltx_glossaryref" title="">SDF</span> from which the mesh is extracted using the marching cube algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib64" title="">64</a>]</cite> with SDF values sampled on a 1024x1024x1024 grid and a subsampling factor of 8.
The NeuralAngelo <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib59" title="">59</a>]</cite> mesh is extracted with a 2048-resolution grid, and the Plexoxel <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib30" title="">30</a>]</cite> one with a 256-resolution grid,
as recommended by their respective authors. For all methods, the SDF is initialized with a unit sphere.
The same setup is adopted for Unisurf <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib75" title="">75</a>]</cite>, except that it outputs an occupancy grid instead of an SDF.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS1.p6">
<p class="ltx_p" id="A2.SS1.p6.1"><span class="ltx_text ltx_font_bold" id="A2.SS1.p6.1.1">iNGP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib71" title="">71</a>]</cite> implicit rendering. </span>
In addition to evaluating the mesh derived with iNGP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib71" title="">71</a>]</cite>, we also evaluate how well the novel view synthesis of iNGP can replace the rendering of CAD models for pose estimation.
We used the default settings of the iNGP renderer, except for the number of samples per pixel that is decreased to 1.
We edit the Megapose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib55" title="">55</a>]</cite> codebase to replace the CAD renderings with the iNGP implicit renderings.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS1.p7">
<p class="ltx_p" id="A2.SS1.p7.1"><span class="ltx_text ltx_font_bold" id="A2.SS1.p7.1.1">Hardware. </span>
To provide fair runtime comparisons, all reconstruction methods are run on a single NVIDIA
A100 GPU, 32x Intel Xeon-SC 8628, 24 cores, 2,9 GHz with 256GB of RAM.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS1.p8">
<p class="ltx_p" id="A2.SS1.p8.1">One exception though is for RealityCapture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib82" title="">82</a>]</cite> that runs on a GeForce RTX 3060 with 12 GB of RAM, <em class="ltx_emph ltx_font_italic" id="A2.SS1.p8.1.1">i.e</em>.<span class="ltx_text" id="A2.SS1.p8.1.2"></span>, a weaker GPU than the other methods.
This exception is because RealityCapture required a Windows installation that we had available only on a machine equipped with GeForce.</p>
</div>
</section>
<section class="ltx_subsection" id="A2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Pose Estimators: FoundationPose and Megapose</h3>
<div class="ltx_para ltx_noindent" id="A2.SS2.p1">
<p class="ltx_p" id="A2.SS2.p1.1">We recall how the two pose estimators used in the evaluation operate and refer the reader to their respective papers for more details.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS2.p2">
<p class="ltx_p" id="A2.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="A2.SS2.p2.1.1">FoundationPose </span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib12" title="">12</a>]</cite> <span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>https://github.com/NVlabs/FoundationPose</span></span></span> is a generalizable render-and-compare pose estimator that takes as input an RGBD image with an object of interest and a 3D representation of that object to output a pose.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS2.p3">
<p class="ltx_p" id="A2.SS2.p3.1">The pose estimation proceeds in 2 steps: it first generates a set of pose hypotheses all around the object that are later refined by a network.
This refinement network is given both the RGBD test image and the RGBD renderings from the pose hypotheses generated with the 3D representation.
A second network then ranks the refined pose hypotheses using the RGBD test image and the RGBD rendering produced from the refined poses.
The best-ranked pose is the final output.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS2.p4">
<p class="ltx_p" id="A2.SS2.p4.1">One of the main contributions of FoundationPose is that the 3D representation can take two forms as long as color and depth renderings can be generated from it: a traditional CAD model or an implicit representation.
The pose estimation is agnostic to the 3D representation as it only uses the RGBD renderings.
Also, it is jointly trained with renderings from the CAD models and the implicit representations, which reduces any possible distribution shift between the two types of renderings.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS2.p5">
<p class="ltx_p" id="A2.SS2.p5.1">Whenever the 3D representation of the object at hand is not available, FoundationPose generates a 3D implicit representation of the object at test time.
Thus FoundationPose can be deployed on any object even when the 3D representation is not known beforehand, which reflects the practical conditions in which pose estimators are deployed.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS2.p6">
<p class="ltx_p" id="A2.SS2.p6.1">The 3D implicit model can generate RGBD renderings in two ways: either with novel-view synthesis directly or by first extracting a mesh out of the implicit representation and then rastering the mesh.
The latter is computationally more efficient, as observed by the FoundationPose authors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib12" title="">12</a>]</cite>.
We adopt a similar derivation where we extract a mesh out of the evaluated implicit 3D reconstruction methods to produce RGBD renderings.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS2.p7">
<p class="ltx_p" id="A2.SS2.p7.5"><span class="ltx_text ltx_font_bold" id="A2.SS2.p7.5.1">Megapose <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_medium" id="A2.SS2.p7.5.1.1.1">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib55" title="">55</a><span class="ltx_text ltx_font_medium" id="A2.SS2.p7.5.1.2.2">]</span></cite></span> <span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>https://github.com/megapose6d/megapose6d</span></span></span> is a also generalizable render-and-compare pose estimator that takes as input an RGB image depicting an object and a 3D model of that object and outputs the pose of the object.
Megapose is made of two modules: a coarse pose estimator and a pose refiner.
Given a test image cropped around the object, <span class="ltx_text ltx_font_bold" id="A2.SS2.p7.5.2">the coarse module</span> generates <math alttext="n" class="ltx_Math" display="inline" id="A2.SS2.p7.1.m1.1"><semantics id="A2.SS2.p7.1.m1.1a"><mi id="A2.SS2.p7.1.m1.1.1" xref="A2.SS2.p7.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="A2.SS2.p7.1.m1.1b"><ci id="A2.SS2.p7.1.m1.1.1.cmml" xref="A2.SS2.p7.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p7.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="A2.SS2.p7.1.m1.1d">italic_n</annotation></semantics></math> pose hypothesis all around the objects from which renderings are generated.
The concatenation of the test image and the renderings are fed to a classification network which logits are interpreted as a score for each pose hypothesis.
The top-<math alttext="K" class="ltx_Math" display="inline" id="A2.SS2.p7.2.m2.1"><semantics id="A2.SS2.p7.2.m2.1a"><mi id="A2.SS2.p7.2.m2.1.1" xref="A2.SS2.p7.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="A2.SS2.p7.2.m2.1b"><ci id="A2.SS2.p7.2.m2.1.1.cmml" xref="A2.SS2.p7.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p7.2.m2.1c">K</annotation><annotation encoding="application/x-llamapun" id="A2.SS2.p7.2.m2.1d">italic_K</annotation></semantics></math> coarse poses are kept and refined: for each coarse pose, the refiner renders the object’s mesh from that coarse pose and from three additional views.
The views aim at creating parallax and are generated by moving the camera laterally while ensuring that the camera-z axis goes through the object’s centroid.
<span class="ltx_text ltx_font_bold" id="A2.SS2.p7.5.3">The refiner network</span> is a regression network that takes as input the test image and the four renders defined previously and outputs a pose update to apply to the coarse pose.
The final pose is the composition of the coarse pose with the pose update. This final pose can also be refined by repeating the refinement step.
We adopt the author’s recommendation and set the number of refinement iterations to 5.
We feed the coarse module with the maximal number of initial pose hypothesis <math alttext="n=576" class="ltx_Math" display="inline" id="A2.SS2.p7.3.m3.1"><semantics id="A2.SS2.p7.3.m3.1a"><mrow id="A2.SS2.p7.3.m3.1.1" xref="A2.SS2.p7.3.m3.1.1.cmml"><mi id="A2.SS2.p7.3.m3.1.1.2" xref="A2.SS2.p7.3.m3.1.1.2.cmml">n</mi><mo id="A2.SS2.p7.3.m3.1.1.1" xref="A2.SS2.p7.3.m3.1.1.1.cmml">=</mo><mn id="A2.SS2.p7.3.m3.1.1.3" xref="A2.SS2.p7.3.m3.1.1.3.cmml">576</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.SS2.p7.3.m3.1b"><apply id="A2.SS2.p7.3.m3.1.1.cmml" xref="A2.SS2.p7.3.m3.1.1"><eq id="A2.SS2.p7.3.m3.1.1.1.cmml" xref="A2.SS2.p7.3.m3.1.1.1"></eq><ci id="A2.SS2.p7.3.m3.1.1.2.cmml" xref="A2.SS2.p7.3.m3.1.1.2">𝑛</ci><cn id="A2.SS2.p7.3.m3.1.1.3.cmml" type="integer" xref="A2.SS2.p7.3.m3.1.1.3">576</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p7.3.m3.1c">n=576</annotation><annotation encoding="application/x-llamapun" id="A2.SS2.p7.3.m3.1d">italic_n = 576</annotation></semantics></math> and we keep the top-1 coarse pose, <em class="ltx_emph ltx_font_italic" id="A2.SS2.p7.5.4">i.e</em>.<span class="ltx_text" id="A2.SS2.p7.5.5"></span>, setting <math alttext="K=1" class="ltx_Math" display="inline" id="A2.SS2.p7.4.m4.1"><semantics id="A2.SS2.p7.4.m4.1a"><mrow id="A2.SS2.p7.4.m4.1.1" xref="A2.SS2.p7.4.m4.1.1.cmml"><mi id="A2.SS2.p7.4.m4.1.1.2" xref="A2.SS2.p7.4.m4.1.1.2.cmml">K</mi><mo id="A2.SS2.p7.4.m4.1.1.1" xref="A2.SS2.p7.4.m4.1.1.1.cmml">=</mo><mn id="A2.SS2.p7.4.m4.1.1.3" xref="A2.SS2.p7.4.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.SS2.p7.4.m4.1b"><apply id="A2.SS2.p7.4.m4.1.1.cmml" xref="A2.SS2.p7.4.m4.1.1"><eq id="A2.SS2.p7.4.m4.1.1.1.cmml" xref="A2.SS2.p7.4.m4.1.1.1"></eq><ci id="A2.SS2.p7.4.m4.1.1.2.cmml" xref="A2.SS2.p7.4.m4.1.1.2">𝐾</ci><cn id="A2.SS2.p7.4.m4.1.1.3.cmml" type="integer" xref="A2.SS2.p7.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p7.4.m4.1c">K=1</annotation><annotation encoding="application/x-llamapun" id="A2.SS2.p7.4.m4.1d">italic_K = 1</annotation></semantics></math>, since we observe marginal improvement with <math alttext="K=5" class="ltx_Math" display="inline" id="A2.SS2.p7.5.m5.1"><semantics id="A2.SS2.p7.5.m5.1a"><mrow id="A2.SS2.p7.5.m5.1.1" xref="A2.SS2.p7.5.m5.1.1.cmml"><mi id="A2.SS2.p7.5.m5.1.1.2" xref="A2.SS2.p7.5.m5.1.1.2.cmml">K</mi><mo id="A2.SS2.p7.5.m5.1.1.1" xref="A2.SS2.p7.5.m5.1.1.1.cmml">=</mo><mn id="A2.SS2.p7.5.m5.1.1.3" xref="A2.SS2.p7.5.m5.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.SS2.p7.5.m5.1b"><apply id="A2.SS2.p7.5.m5.1.1.cmml" xref="A2.SS2.p7.5.m5.1.1"><eq id="A2.SS2.p7.5.m5.1.1.1.cmml" xref="A2.SS2.p7.5.m5.1.1.1"></eq><ci id="A2.SS2.p7.5.m5.1.1.2.cmml" xref="A2.SS2.p7.5.m5.1.1.2">𝐾</ci><cn id="A2.SS2.p7.5.m5.1.1.3.cmml" type="integer" xref="A2.SS2.p7.5.m5.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p7.5.m5.1c">K=5</annotation><annotation encoding="application/x-llamapun" id="A2.SS2.p7.5.m5.1d">italic_K = 5</annotation></semantics></math> (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A2.F6" title="Figure 6 ‣ B.2 Pose Estimators: FoundationPose and Megapose ‣ Appendix B Evaluation Setup ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">6</span></a>) and it runs faster.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS2.p8">
<p class="ltx_p" id="A2.SS2.p8.1">Before being fed to the network, the image is cropped around the object of interest.
One can either use a box detector or the ground-truth box to define the region of interest and we use the latter for both Megapose and FoudationPose.</p>
</div>
<figure class="ltx_figure" id="A2.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="205" id="A2.F6.g1" src="x10.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>
<span class="ltx_text ltx_font_bold" id="A2.F6.2.1">Megapose Parameter Tuning</span>: comparison of the pose performance when keeping the top-1 (uniform bar) and top-5 (crossed bar) pose hypothesis for pose refinement.
The experiment runs on two of the best-performing reconstruction methods RealityCapture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib82" title="">82</a>]</cite> and
Nerfacto <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib100" title="">100</a>]</cite>.
The results show that using a higher number of coarse hypotheses marginally
improves the scores for RealityCapture (+3%) and barely affects Nerfacto.
</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.3 </span>BOP Evaluation</h3>
<div class="ltx_para ltx_noindent" id="A2.SS3.p1">
<p class="ltx_p" id="A2.SS3.p1.1">As is custom for object pose estimation, we report the three standard pose errors defined in the BOP benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib47" title="">47</a>]</cite>.
The metrics reported are the Visible Surface Discrepancy (VSD), the Maximum Symmetry-Aware Surface Distance (MSSD), and the Maximum Symmetry-Aware Projection Distance (MSPD).
We refer the reader to the well-documented BOP benchmark methodology for details<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>https://bop.felk.cvut.cz/challenges/bop-challenge-2019/#evaluationmethodology</span></span></span>.
We recall the definitions here for the sake of completeness.
<span class="ltx_text ltx_font_bold" id="A2.SS3.p1.1.1">Note that all these errors measure the pose error, hence the lower, the better.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS3.p2">
<p class="ltx_p" id="A2.SS3.p2.6">The <span class="ltx_text ltx_font_bold" id="A2.SS3.p2.6.1">Maximum Symmetry-Aware Surface Distance (MSSD)</span>
measures the maximum surface misalignment between the surface of the object when it is positioned with the ground-truth pose and when it is positioned with the estimated pose. In both cases, the object is positioned with respect to the camera coordinate frame.
As indicated in the name, the MSSD is symmetry-aware, <em class="ltx_emph ltx_font_italic" id="A2.SS3.p2.6.2">i.e</em>.<span class="ltx_text" id="A2.SS3.p2.6.3"></span>, it does not penalize
the estimated pose for ambiguous symmetries (<em class="ltx_emph ltx_font_italic" id="A2.SS3.p2.6.4">e.g</em>.<span class="ltx_text" id="A2.SS3.p2.6.5"></span> a textureless sphere has an
infinite number of non-resolvable symmetries).
<span class="ltx_text ltx_font_italic" id="A2.SS3.p2.6.6">Derivation details:</span>
Let <math alttext="M" class="ltx_Math" display="inline" id="A2.SS3.p2.1.m1.1"><semantics id="A2.SS3.p2.1.m1.1a"><mi id="A2.SS3.p2.1.m1.1.1" xref="A2.SS3.p2.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="A2.SS3.p2.1.m1.1b"><ci id="A2.SS3.p2.1.m1.1.1.cmml" xref="A2.SS3.p2.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p2.1.m1.1c">M</annotation><annotation encoding="application/x-llamapun" id="A2.SS3.p2.1.m1.1d">italic_M</annotation></semantics></math> be the CAD object model, <math alttext="\hat{\mathbf{P}}" class="ltx_Math" display="inline" id="A2.SS3.p2.2.m2.1"><semantics id="A2.SS3.p2.2.m2.1a"><mover accent="true" id="A2.SS3.p2.2.m2.1.1" xref="A2.SS3.p2.2.m2.1.1.cmml"><mi id="A2.SS3.p2.2.m2.1.1.2" xref="A2.SS3.p2.2.m2.1.1.2.cmml">𝐏</mi><mo id="A2.SS3.p2.2.m2.1.1.1" xref="A2.SS3.p2.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="A2.SS3.p2.2.m2.1b"><apply id="A2.SS3.p2.2.m2.1.1.cmml" xref="A2.SS3.p2.2.m2.1.1"><ci id="A2.SS3.p2.2.m2.1.1.1.cmml" xref="A2.SS3.p2.2.m2.1.1.1">^</ci><ci id="A2.SS3.p2.2.m2.1.1.2.cmml" xref="A2.SS3.p2.2.m2.1.1.2">𝐏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p2.2.m2.1c">\hat{\mathbf{P}}</annotation><annotation encoding="application/x-llamapun" id="A2.SS3.p2.2.m2.1d">over^ start_ARG bold_P end_ARG</annotation></semantics></math> the estimated pose, <math alttext="\bar{\mathbf{P}}" class="ltx_Math" display="inline" id="A2.SS3.p2.3.m3.1"><semantics id="A2.SS3.p2.3.m3.1a"><mover accent="true" id="A2.SS3.p2.3.m3.1.1" xref="A2.SS3.p2.3.m3.1.1.cmml"><mi id="A2.SS3.p2.3.m3.1.1.2" xref="A2.SS3.p2.3.m3.1.1.2.cmml">𝐏</mi><mo id="A2.SS3.p2.3.m3.1.1.1" xref="A2.SS3.p2.3.m3.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="A2.SS3.p2.3.m3.1b"><apply id="A2.SS3.p2.3.m3.1.1.cmml" xref="A2.SS3.p2.3.m3.1.1"><ci id="A2.SS3.p2.3.m3.1.1.1.cmml" xref="A2.SS3.p2.3.m3.1.1.1">¯</ci><ci id="A2.SS3.p2.3.m3.1.1.2.cmml" xref="A2.SS3.p2.3.m3.1.1.2">𝐏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p2.3.m3.1c">\bar{\mathbf{P}}</annotation><annotation encoding="application/x-llamapun" id="A2.SS3.p2.3.m3.1d">over¯ start_ARG bold_P end_ARG</annotation></semantics></math> the ground-truth pose, <math alttext="V_{M}" class="ltx_Math" display="inline" id="A2.SS3.p2.4.m4.1"><semantics id="A2.SS3.p2.4.m4.1a"><msub id="A2.SS3.p2.4.m4.1.1" xref="A2.SS3.p2.4.m4.1.1.cmml"><mi id="A2.SS3.p2.4.m4.1.1.2" xref="A2.SS3.p2.4.m4.1.1.2.cmml">V</mi><mi id="A2.SS3.p2.4.m4.1.1.3" xref="A2.SS3.p2.4.m4.1.1.3.cmml">M</mi></msub><annotation-xml encoding="MathML-Content" id="A2.SS3.p2.4.m4.1b"><apply id="A2.SS3.p2.4.m4.1.1.cmml" xref="A2.SS3.p2.4.m4.1.1"><csymbol cd="ambiguous" id="A2.SS3.p2.4.m4.1.1.1.cmml" xref="A2.SS3.p2.4.m4.1.1">subscript</csymbol><ci id="A2.SS3.p2.4.m4.1.1.2.cmml" xref="A2.SS3.p2.4.m4.1.1.2">𝑉</ci><ci id="A2.SS3.p2.4.m4.1.1.3.cmml" xref="A2.SS3.p2.4.m4.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p2.4.m4.1c">V_{M}</annotation><annotation encoding="application/x-llamapun" id="A2.SS3.p2.4.m4.1d">italic_V start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT</annotation></semantics></math> a set of points sampled on the reference CAD model <math alttext="M" class="ltx_Math" display="inline" id="A2.SS3.p2.5.m5.1"><semantics id="A2.SS3.p2.5.m5.1a"><mi id="A2.SS3.p2.5.m5.1.1" xref="A2.SS3.p2.5.m5.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="A2.SS3.p2.5.m5.1b"><ci id="A2.SS3.p2.5.m5.1.1.cmml" xref="A2.SS3.p2.5.m5.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p2.5.m5.1c">M</annotation><annotation encoding="application/x-llamapun" id="A2.SS3.p2.5.m5.1d">italic_M</annotation></semantics></math> (<em class="ltx_emph ltx_font_italic" id="A2.SS3.p2.6.7">i.e</em>.<span class="ltx_text" id="A2.SS3.p2.6.8"></span> the YCB-V <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib111" title="">111</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib17" title="">17</a>]</cite> one), <math alttext="S_{M}" class="ltx_Math" display="inline" id="A2.SS3.p2.6.m6.1"><semantics id="A2.SS3.p2.6.m6.1a"><msub id="A2.SS3.p2.6.m6.1.1" xref="A2.SS3.p2.6.m6.1.1.cmml"><mi id="A2.SS3.p2.6.m6.1.1.2" xref="A2.SS3.p2.6.m6.1.1.2.cmml">S</mi><mi id="A2.SS3.p2.6.m6.1.1.3" xref="A2.SS3.p2.6.m6.1.1.3.cmml">M</mi></msub><annotation-xml encoding="MathML-Content" id="A2.SS3.p2.6.m6.1b"><apply id="A2.SS3.p2.6.m6.1.1.cmml" xref="A2.SS3.p2.6.m6.1.1"><csymbol cd="ambiguous" id="A2.SS3.p2.6.m6.1.1.1.cmml" xref="A2.SS3.p2.6.m6.1.1">subscript</csymbol><ci id="A2.SS3.p2.6.m6.1.1.2.cmml" xref="A2.SS3.p2.6.m6.1.1.2">𝑆</ci><ci id="A2.SS3.p2.6.m6.1.1.3.cmml" xref="A2.SS3.p2.6.m6.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p2.6.m6.1c">S_{M}</annotation><annotation encoding="application/x-llamapun" id="A2.SS3.p2.6.m6.1d">italic_S start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT</annotation></semantics></math> the set of symmetries of the object. The MSSD is computed as:</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS3.p3">
<table class="ltx_equation ltx_eqn_table" id="A2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="e_{\text{MSSD}}(\hat{\mathbf{P}},\bar{\mathbf{P}},S_{M},V_{M})=\text{min}_{%
\mathbf{S}\in S_{M}}\text{max}_{\mathbf{x}\in V_{M}}\|\hat{\mathbf{P}}\mathbf{%
x}-\bar{\mathbf{P}}\mathbf{S}\mathbf{x}\|_{2}\enspace." class="ltx_Math" display="block" id="A2.E1.m1.3"><semantics id="A2.E1.m1.3a"><mrow id="A2.E1.m1.3.3.1" xref="A2.E1.m1.3.3.1.1.cmml"><mrow id="A2.E1.m1.3.3.1.1" xref="A2.E1.m1.3.3.1.1.cmml"><mrow id="A2.E1.m1.3.3.1.1.2" xref="A2.E1.m1.3.3.1.1.2.cmml"><msub id="A2.E1.m1.3.3.1.1.2.4" xref="A2.E1.m1.3.3.1.1.2.4.cmml"><mi id="A2.E1.m1.3.3.1.1.2.4.2" xref="A2.E1.m1.3.3.1.1.2.4.2.cmml">e</mi><mtext id="A2.E1.m1.3.3.1.1.2.4.3" xref="A2.E1.m1.3.3.1.1.2.4.3a.cmml">MSSD</mtext></msub><mo id="A2.E1.m1.3.3.1.1.2.3" xref="A2.E1.m1.3.3.1.1.2.3.cmml">⁢</mo><mrow id="A2.E1.m1.3.3.1.1.2.2.2" xref="A2.E1.m1.3.3.1.1.2.2.3.cmml"><mo id="A2.E1.m1.3.3.1.1.2.2.2.3" stretchy="false" xref="A2.E1.m1.3.3.1.1.2.2.3.cmml">(</mo><mover accent="true" id="A2.E1.m1.1.1" xref="A2.E1.m1.1.1.cmml"><mi id="A2.E1.m1.1.1.2" xref="A2.E1.m1.1.1.2.cmml">𝐏</mi><mo id="A2.E1.m1.1.1.1" xref="A2.E1.m1.1.1.1.cmml">^</mo></mover><mo id="A2.E1.m1.3.3.1.1.2.2.2.4" xref="A2.E1.m1.3.3.1.1.2.2.3.cmml">,</mo><mover accent="true" id="A2.E1.m1.2.2" xref="A2.E1.m1.2.2.cmml"><mi id="A2.E1.m1.2.2.2" xref="A2.E1.m1.2.2.2.cmml">𝐏</mi><mo id="A2.E1.m1.2.2.1" xref="A2.E1.m1.2.2.1.cmml">¯</mo></mover><mo id="A2.E1.m1.3.3.1.1.2.2.2.5" xref="A2.E1.m1.3.3.1.1.2.2.3.cmml">,</mo><msub id="A2.E1.m1.3.3.1.1.1.1.1.1" xref="A2.E1.m1.3.3.1.1.1.1.1.1.cmml"><mi id="A2.E1.m1.3.3.1.1.1.1.1.1.2" xref="A2.E1.m1.3.3.1.1.1.1.1.1.2.cmml">S</mi><mi id="A2.E1.m1.3.3.1.1.1.1.1.1.3" xref="A2.E1.m1.3.3.1.1.1.1.1.1.3.cmml">M</mi></msub><mo id="A2.E1.m1.3.3.1.1.2.2.2.6" xref="A2.E1.m1.3.3.1.1.2.2.3.cmml">,</mo><msub id="A2.E1.m1.3.3.1.1.2.2.2.2" xref="A2.E1.m1.3.3.1.1.2.2.2.2.cmml"><mi id="A2.E1.m1.3.3.1.1.2.2.2.2.2" xref="A2.E1.m1.3.3.1.1.2.2.2.2.2.cmml">V</mi><mi id="A2.E1.m1.3.3.1.1.2.2.2.2.3" xref="A2.E1.m1.3.3.1.1.2.2.2.2.3.cmml">M</mi></msub><mo id="A2.E1.m1.3.3.1.1.2.2.2.7" stretchy="false" xref="A2.E1.m1.3.3.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="A2.E1.m1.3.3.1.1.4" xref="A2.E1.m1.3.3.1.1.4.cmml">=</mo><mrow id="A2.E1.m1.3.3.1.1.3" xref="A2.E1.m1.3.3.1.1.3.cmml"><msub id="A2.E1.m1.3.3.1.1.3.3" xref="A2.E1.m1.3.3.1.1.3.3.cmml"><mtext id="A2.E1.m1.3.3.1.1.3.3.2" xref="A2.E1.m1.3.3.1.1.3.3.2a.cmml">min</mtext><mrow id="A2.E1.m1.3.3.1.1.3.3.3" xref="A2.E1.m1.3.3.1.1.3.3.3.cmml"><mi id="A2.E1.m1.3.3.1.1.3.3.3.2" xref="A2.E1.m1.3.3.1.1.3.3.3.2.cmml">𝐒</mi><mo id="A2.E1.m1.3.3.1.1.3.3.3.1" xref="A2.E1.m1.3.3.1.1.3.3.3.1.cmml">∈</mo><msub id="A2.E1.m1.3.3.1.1.3.3.3.3" xref="A2.E1.m1.3.3.1.1.3.3.3.3.cmml"><mi id="A2.E1.m1.3.3.1.1.3.3.3.3.2" xref="A2.E1.m1.3.3.1.1.3.3.3.3.2.cmml">S</mi><mi id="A2.E1.m1.3.3.1.1.3.3.3.3.3" xref="A2.E1.m1.3.3.1.1.3.3.3.3.3.cmml">M</mi></msub></mrow></msub><mo id="A2.E1.m1.3.3.1.1.3.2" xref="A2.E1.m1.3.3.1.1.3.2.cmml">⁢</mo><msub id="A2.E1.m1.3.3.1.1.3.4" xref="A2.E1.m1.3.3.1.1.3.4.cmml"><mtext id="A2.E1.m1.3.3.1.1.3.4.2" xref="A2.E1.m1.3.3.1.1.3.4.2a.cmml">max</mtext><mrow id="A2.E1.m1.3.3.1.1.3.4.3" xref="A2.E1.m1.3.3.1.1.3.4.3.cmml"><mi id="A2.E1.m1.3.3.1.1.3.4.3.2" xref="A2.E1.m1.3.3.1.1.3.4.3.2.cmml">𝐱</mi><mo id="A2.E1.m1.3.3.1.1.3.4.3.1" xref="A2.E1.m1.3.3.1.1.3.4.3.1.cmml">∈</mo><msub id="A2.E1.m1.3.3.1.1.3.4.3.3" xref="A2.E1.m1.3.3.1.1.3.4.3.3.cmml"><mi id="A2.E1.m1.3.3.1.1.3.4.3.3.2" xref="A2.E1.m1.3.3.1.1.3.4.3.3.2.cmml">V</mi><mi id="A2.E1.m1.3.3.1.1.3.4.3.3.3" xref="A2.E1.m1.3.3.1.1.3.4.3.3.3.cmml">M</mi></msub></mrow></msub><mo id="A2.E1.m1.3.3.1.1.3.2a" xref="A2.E1.m1.3.3.1.1.3.2.cmml">⁢</mo><msub id="A2.E1.m1.3.3.1.1.3.1" xref="A2.E1.m1.3.3.1.1.3.1.cmml"><mrow id="A2.E1.m1.3.3.1.1.3.1.1.1" xref="A2.E1.m1.3.3.1.1.3.1.1.2.cmml"><mo id="A2.E1.m1.3.3.1.1.3.1.1.1.2" stretchy="false" xref="A2.E1.m1.3.3.1.1.3.1.1.2.1.cmml">‖</mo><mrow id="A2.E1.m1.3.3.1.1.3.1.1.1.1" xref="A2.E1.m1.3.3.1.1.3.1.1.1.1.cmml"><mrow id="A2.E1.m1.3.3.1.1.3.1.1.1.1.2" xref="A2.E1.m1.3.3.1.1.3.1.1.1.1.2.cmml"><mover accent="true" id="A2.E1.m1.3.3.1.1.3.1.1.1.1.2.2" xref="A2.E1.m1.3.3.1.1.3.1.1.1.1.2.2.cmml"><mi id="A2.E1.m1.3.3.1.1.3.1.1.1.1.2.2.2" xref="A2.E1.m1.3.3.1.1.3.1.1.1.1.2.2.2.cmml">𝐏</mi><mo id="A2.E1.m1.3.3.1.1.3.1.1.1.1.2.2.1" xref="A2.E1.m1.3.3.1.1.3.1.1.1.1.2.2.1.cmml">^</mo></mover><mo id="A2.E1.m1.3.3.1.1.3.1.1.1.1.2.1" xref="A2.E1.m1.3.3.1.1.3.1.1.1.1.2.1.cmml">⁢</mo><mi id="A2.E1.m1.3.3.1.1.3.1.1.1.1.2.3" xref="A2.E1.m1.3.3.1.1.3.1.1.1.1.2.3.cmml">𝐱</mi></mrow><mo id="A2.E1.m1.3.3.1.1.3.1.1.1.1.1" xref="A2.E1.m1.3.3.1.1.3.1.1.1.1.1.cmml">−</mo><mrow id="A2.E1.m1.3.3.1.1.3.1.1.1.1.3" xref="A2.E1.m1.3.3.1.1.3.1.1.1.1.3.cmml"><mover accent="true" id="A2.E1.m1.3.3.1.1.3.1.1.1.1.3.2" xref="A2.E1.m1.3.3.1.1.3.1.1.1.1.3.2.cmml"><mi id="A2.E1.m1.3.3.1.1.3.1.1.1.1.3.2.2" xref="A2.E1.m1.3.3.1.1.3.1.1.1.1.3.2.2.cmml">𝐏</mi><mo id="A2.E1.m1.3.3.1.1.3.1.1.1.1.3.2.1" xref="A2.E1.m1.3.3.1.1.3.1.1.1.1.3.2.1.cmml">¯</mo></mover><mo id="A2.E1.m1.3.3.1.1.3.1.1.1.1.3.1" xref="A2.E1.m1.3.3.1.1.3.1.1.1.1.3.1.cmml">⁢</mo><mi id="A2.E1.m1.3.3.1.1.3.1.1.1.1.3.3" xref="A2.E1.m1.3.3.1.1.3.1.1.1.1.3.3.cmml">𝐒𝐱</mi></mrow></mrow><mo id="A2.E1.m1.3.3.1.1.3.1.1.1.3" stretchy="false" xref="A2.E1.m1.3.3.1.1.3.1.1.2.1.cmml">‖</mo></mrow><mn id="A2.E1.m1.3.3.1.1.3.1.3" xref="A2.E1.m1.3.3.1.1.3.1.3.cmml">2</mn></msub></mrow></mrow><mo id="A2.E1.m1.3.3.1.2" lspace="0em" xref="A2.E1.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.E1.m1.3b"><apply id="A2.E1.m1.3.3.1.1.cmml" xref="A2.E1.m1.3.3.1"><eq id="A2.E1.m1.3.3.1.1.4.cmml" xref="A2.E1.m1.3.3.1.1.4"></eq><apply id="A2.E1.m1.3.3.1.1.2.cmml" xref="A2.E1.m1.3.3.1.1.2"><times id="A2.E1.m1.3.3.1.1.2.3.cmml" xref="A2.E1.m1.3.3.1.1.2.3"></times><apply id="A2.E1.m1.3.3.1.1.2.4.cmml" xref="A2.E1.m1.3.3.1.1.2.4"><csymbol cd="ambiguous" id="A2.E1.m1.3.3.1.1.2.4.1.cmml" xref="A2.E1.m1.3.3.1.1.2.4">subscript</csymbol><ci id="A2.E1.m1.3.3.1.1.2.4.2.cmml" xref="A2.E1.m1.3.3.1.1.2.4.2">𝑒</ci><ci id="A2.E1.m1.3.3.1.1.2.4.3a.cmml" xref="A2.E1.m1.3.3.1.1.2.4.3"><mtext id="A2.E1.m1.3.3.1.1.2.4.3.cmml" mathsize="70%" xref="A2.E1.m1.3.3.1.1.2.4.3">MSSD</mtext></ci></apply><vector id="A2.E1.m1.3.3.1.1.2.2.3.cmml" xref="A2.E1.m1.3.3.1.1.2.2.2"><apply id="A2.E1.m1.1.1.cmml" xref="A2.E1.m1.1.1"><ci id="A2.E1.m1.1.1.1.cmml" xref="A2.E1.m1.1.1.1">^</ci><ci id="A2.E1.m1.1.1.2.cmml" xref="A2.E1.m1.1.1.2">𝐏</ci></apply><apply id="A2.E1.m1.2.2.cmml" xref="A2.E1.m1.2.2"><ci id="A2.E1.m1.2.2.1.cmml" xref="A2.E1.m1.2.2.1">¯</ci><ci id="A2.E1.m1.2.2.2.cmml" xref="A2.E1.m1.2.2.2">𝐏</ci></apply><apply id="A2.E1.m1.3.3.1.1.1.1.1.1.cmml" xref="A2.E1.m1.3.3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="A2.E1.m1.3.3.1.1.1.1.1.1.1.cmml" xref="A2.E1.m1.3.3.1.1.1.1.1.1">subscript</csymbol><ci id="A2.E1.m1.3.3.1.1.1.1.1.1.2.cmml" xref="A2.E1.m1.3.3.1.1.1.1.1.1.2">𝑆</ci><ci id="A2.E1.m1.3.3.1.1.1.1.1.1.3.cmml" xref="A2.E1.m1.3.3.1.1.1.1.1.1.3">𝑀</ci></apply><apply id="A2.E1.m1.3.3.1.1.2.2.2.2.cmml" xref="A2.E1.m1.3.3.1.1.2.2.2.2"><csymbol cd="ambiguous" id="A2.E1.m1.3.3.1.1.2.2.2.2.1.cmml" xref="A2.E1.m1.3.3.1.1.2.2.2.2">subscript</csymbol><ci id="A2.E1.m1.3.3.1.1.2.2.2.2.2.cmml" xref="A2.E1.m1.3.3.1.1.2.2.2.2.2">𝑉</ci><ci id="A2.E1.m1.3.3.1.1.2.2.2.2.3.cmml" xref="A2.E1.m1.3.3.1.1.2.2.2.2.3">𝑀</ci></apply></vector></apply><apply id="A2.E1.m1.3.3.1.1.3.cmml" xref="A2.E1.m1.3.3.1.1.3"><times id="A2.E1.m1.3.3.1.1.3.2.cmml" xref="A2.E1.m1.3.3.1.1.3.2"></times><apply id="A2.E1.m1.3.3.1.1.3.3.cmml" xref="A2.E1.m1.3.3.1.1.3.3"><csymbol cd="ambiguous" id="A2.E1.m1.3.3.1.1.3.3.1.cmml" xref="A2.E1.m1.3.3.1.1.3.3">subscript</csymbol><ci id="A2.E1.m1.3.3.1.1.3.3.2a.cmml" xref="A2.E1.m1.3.3.1.1.3.3.2"><mtext id="A2.E1.m1.3.3.1.1.3.3.2.cmml" xref="A2.E1.m1.3.3.1.1.3.3.2">min</mtext></ci><apply id="A2.E1.m1.3.3.1.1.3.3.3.cmml" xref="A2.E1.m1.3.3.1.1.3.3.3"><in id="A2.E1.m1.3.3.1.1.3.3.3.1.cmml" xref="A2.E1.m1.3.3.1.1.3.3.3.1"></in><ci id="A2.E1.m1.3.3.1.1.3.3.3.2.cmml" xref="A2.E1.m1.3.3.1.1.3.3.3.2">𝐒</ci><apply id="A2.E1.m1.3.3.1.1.3.3.3.3.cmml" xref="A2.E1.m1.3.3.1.1.3.3.3.3"><csymbol cd="ambiguous" id="A2.E1.m1.3.3.1.1.3.3.3.3.1.cmml" xref="A2.E1.m1.3.3.1.1.3.3.3.3">subscript</csymbol><ci id="A2.E1.m1.3.3.1.1.3.3.3.3.2.cmml" xref="A2.E1.m1.3.3.1.1.3.3.3.3.2">𝑆</ci><ci id="A2.E1.m1.3.3.1.1.3.3.3.3.3.cmml" xref="A2.E1.m1.3.3.1.1.3.3.3.3.3">𝑀</ci></apply></apply></apply><apply id="A2.E1.m1.3.3.1.1.3.4.cmml" xref="A2.E1.m1.3.3.1.1.3.4"><csymbol cd="ambiguous" id="A2.E1.m1.3.3.1.1.3.4.1.cmml" xref="A2.E1.m1.3.3.1.1.3.4">subscript</csymbol><ci id="A2.E1.m1.3.3.1.1.3.4.2a.cmml" xref="A2.E1.m1.3.3.1.1.3.4.2"><mtext id="A2.E1.m1.3.3.1.1.3.4.2.cmml" xref="A2.E1.m1.3.3.1.1.3.4.2">max</mtext></ci><apply id="A2.E1.m1.3.3.1.1.3.4.3.cmml" xref="A2.E1.m1.3.3.1.1.3.4.3"><in id="A2.E1.m1.3.3.1.1.3.4.3.1.cmml" xref="A2.E1.m1.3.3.1.1.3.4.3.1"></in><ci id="A2.E1.m1.3.3.1.1.3.4.3.2.cmml" xref="A2.E1.m1.3.3.1.1.3.4.3.2">𝐱</ci><apply id="A2.E1.m1.3.3.1.1.3.4.3.3.cmml" xref="A2.E1.m1.3.3.1.1.3.4.3.3"><csymbol cd="ambiguous" id="A2.E1.m1.3.3.1.1.3.4.3.3.1.cmml" xref="A2.E1.m1.3.3.1.1.3.4.3.3">subscript</csymbol><ci id="A2.E1.m1.3.3.1.1.3.4.3.3.2.cmml" xref="A2.E1.m1.3.3.1.1.3.4.3.3.2">𝑉</ci><ci id="A2.E1.m1.3.3.1.1.3.4.3.3.3.cmml" xref="A2.E1.m1.3.3.1.1.3.4.3.3.3">𝑀</ci></apply></apply></apply><apply id="A2.E1.m1.3.3.1.1.3.1.cmml" xref="A2.E1.m1.3.3.1.1.3.1"><csymbol cd="ambiguous" id="A2.E1.m1.3.3.1.1.3.1.2.cmml" xref="A2.E1.m1.3.3.1.1.3.1">subscript</csymbol><apply id="A2.E1.m1.3.3.1.1.3.1.1.2.cmml" xref="A2.E1.m1.3.3.1.1.3.1.1.1"><csymbol cd="latexml" id="A2.E1.m1.3.3.1.1.3.1.1.2.1.cmml" xref="A2.E1.m1.3.3.1.1.3.1.1.1.2">norm</csymbol><apply id="A2.E1.m1.3.3.1.1.3.1.1.1.1.cmml" xref="A2.E1.m1.3.3.1.1.3.1.1.1.1"><minus id="A2.E1.m1.3.3.1.1.3.1.1.1.1.1.cmml" xref="A2.E1.m1.3.3.1.1.3.1.1.1.1.1"></minus><apply id="A2.E1.m1.3.3.1.1.3.1.1.1.1.2.cmml" xref="A2.E1.m1.3.3.1.1.3.1.1.1.1.2"><times id="A2.E1.m1.3.3.1.1.3.1.1.1.1.2.1.cmml" xref="A2.E1.m1.3.3.1.1.3.1.1.1.1.2.1"></times><apply id="A2.E1.m1.3.3.1.1.3.1.1.1.1.2.2.cmml" xref="A2.E1.m1.3.3.1.1.3.1.1.1.1.2.2"><ci id="A2.E1.m1.3.3.1.1.3.1.1.1.1.2.2.1.cmml" xref="A2.E1.m1.3.3.1.1.3.1.1.1.1.2.2.1">^</ci><ci id="A2.E1.m1.3.3.1.1.3.1.1.1.1.2.2.2.cmml" xref="A2.E1.m1.3.3.1.1.3.1.1.1.1.2.2.2">𝐏</ci></apply><ci id="A2.E1.m1.3.3.1.1.3.1.1.1.1.2.3.cmml" xref="A2.E1.m1.3.3.1.1.3.1.1.1.1.2.3">𝐱</ci></apply><apply id="A2.E1.m1.3.3.1.1.3.1.1.1.1.3.cmml" xref="A2.E1.m1.3.3.1.1.3.1.1.1.1.3"><times id="A2.E1.m1.3.3.1.1.3.1.1.1.1.3.1.cmml" xref="A2.E1.m1.3.3.1.1.3.1.1.1.1.3.1"></times><apply id="A2.E1.m1.3.3.1.1.3.1.1.1.1.3.2.cmml" xref="A2.E1.m1.3.3.1.1.3.1.1.1.1.3.2"><ci id="A2.E1.m1.3.3.1.1.3.1.1.1.1.3.2.1.cmml" xref="A2.E1.m1.3.3.1.1.3.1.1.1.1.3.2.1">¯</ci><ci id="A2.E1.m1.3.3.1.1.3.1.1.1.1.3.2.2.cmml" xref="A2.E1.m1.3.3.1.1.3.1.1.1.1.3.2.2">𝐏</ci></apply><ci id="A2.E1.m1.3.3.1.1.3.1.1.1.1.3.3.cmml" xref="A2.E1.m1.3.3.1.1.3.1.1.1.1.3.3">𝐒𝐱</ci></apply></apply></apply><cn id="A2.E1.m1.3.3.1.1.3.1.3.cmml" type="integer" xref="A2.E1.m1.3.3.1.1.3.1.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.E1.m1.3c">e_{\text{MSSD}}(\hat{\mathbf{P}},\bar{\mathbf{P}},S_{M},V_{M})=\text{min}_{%
\mathbf{S}\in S_{M}}\text{max}_{\mathbf{x}\in V_{M}}\|\hat{\mathbf{P}}\mathbf{%
x}-\bar{\mathbf{P}}\mathbf{S}\mathbf{x}\|_{2}\enspace.</annotation><annotation encoding="application/x-llamapun" id="A2.E1.m1.3d">italic_e start_POSTSUBSCRIPT MSSD end_POSTSUBSCRIPT ( over^ start_ARG bold_P end_ARG , over¯ start_ARG bold_P end_ARG , italic_S start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT , italic_V start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT ) = min start_POSTSUBSCRIPT bold_S ∈ italic_S start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT end_POSTSUBSCRIPT max start_POSTSUBSCRIPT bold_x ∈ italic_V start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT end_POSTSUBSCRIPT ∥ over^ start_ARG bold_P end_ARG bold_x - over¯ start_ARG bold_P end_ARG bold_Sx ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="A2.SS3.p3.2">The metric is computed over the set of points <math alttext="V_{M}" class="ltx_Math" display="inline" id="A2.SS3.p3.1.m1.1"><semantics id="A2.SS3.p3.1.m1.1a"><msub id="A2.SS3.p3.1.m1.1.1" xref="A2.SS3.p3.1.m1.1.1.cmml"><mi id="A2.SS3.p3.1.m1.1.1.2" xref="A2.SS3.p3.1.m1.1.1.2.cmml">V</mi><mi id="A2.SS3.p3.1.m1.1.1.3" xref="A2.SS3.p3.1.m1.1.1.3.cmml">M</mi></msub><annotation-xml encoding="MathML-Content" id="A2.SS3.p3.1.m1.1b"><apply id="A2.SS3.p3.1.m1.1.1.cmml" xref="A2.SS3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="A2.SS3.p3.1.m1.1.1.1.cmml" xref="A2.SS3.p3.1.m1.1.1">subscript</csymbol><ci id="A2.SS3.p3.1.m1.1.1.2.cmml" xref="A2.SS3.p3.1.m1.1.1.2">𝑉</ci><ci id="A2.SS3.p3.1.m1.1.1.3.cmml" xref="A2.SS3.p3.1.m1.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p3.1.m1.1c">V_{M}</annotation><annotation encoding="application/x-llamapun" id="A2.SS3.p3.1.m1.1d">italic_V start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT</annotation></semantics></math> sampled on the reference mesh positioned with the ground-truth pose and with the estimated pose.
Corresponding points form pairs between which the distance is computed.
The MSSD reports the maximum distance over all pairs.
To account for symmetries, the derivation computes several distances for each pair of points where the ground-truth point is additionally transformed with an isometry to account for the symmetry (<math alttext="\text{min}_{\mathbf{S}\in S_{M}}" class="ltx_Math" display="inline" id="A2.SS3.p3.2.m2.1"><semantics id="A2.SS3.p3.2.m2.1a"><msub id="A2.SS3.p3.2.m2.1.1" xref="A2.SS3.p3.2.m2.1.1.cmml"><mtext id="A2.SS3.p3.2.m2.1.1.2" xref="A2.SS3.p3.2.m2.1.1.2a.cmml">min</mtext><mrow id="A2.SS3.p3.2.m2.1.1.3" xref="A2.SS3.p3.2.m2.1.1.3.cmml"><mi id="A2.SS3.p3.2.m2.1.1.3.2" xref="A2.SS3.p3.2.m2.1.1.3.2.cmml">𝐒</mi><mo id="A2.SS3.p3.2.m2.1.1.3.1" xref="A2.SS3.p3.2.m2.1.1.3.1.cmml">∈</mo><msub id="A2.SS3.p3.2.m2.1.1.3.3" xref="A2.SS3.p3.2.m2.1.1.3.3.cmml"><mi id="A2.SS3.p3.2.m2.1.1.3.3.2" xref="A2.SS3.p3.2.m2.1.1.3.3.2.cmml">S</mi><mi id="A2.SS3.p3.2.m2.1.1.3.3.3" xref="A2.SS3.p3.2.m2.1.1.3.3.3.cmml">M</mi></msub></mrow></msub><annotation-xml encoding="MathML-Content" id="A2.SS3.p3.2.m2.1b"><apply id="A2.SS3.p3.2.m2.1.1.cmml" xref="A2.SS3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="A2.SS3.p3.2.m2.1.1.1.cmml" xref="A2.SS3.p3.2.m2.1.1">subscript</csymbol><ci id="A2.SS3.p3.2.m2.1.1.2a.cmml" xref="A2.SS3.p3.2.m2.1.1.2"><mtext id="A2.SS3.p3.2.m2.1.1.2.cmml" xref="A2.SS3.p3.2.m2.1.1.2">min</mtext></ci><apply id="A2.SS3.p3.2.m2.1.1.3.cmml" xref="A2.SS3.p3.2.m2.1.1.3"><in id="A2.SS3.p3.2.m2.1.1.3.1.cmml" xref="A2.SS3.p3.2.m2.1.1.3.1"></in><ci id="A2.SS3.p3.2.m2.1.1.3.2.cmml" xref="A2.SS3.p3.2.m2.1.1.3.2">𝐒</ci><apply id="A2.SS3.p3.2.m2.1.1.3.3.cmml" xref="A2.SS3.p3.2.m2.1.1.3.3"><csymbol cd="ambiguous" id="A2.SS3.p3.2.m2.1.1.3.3.1.cmml" xref="A2.SS3.p3.2.m2.1.1.3.3">subscript</csymbol><ci id="A2.SS3.p3.2.m2.1.1.3.3.2.cmml" xref="A2.SS3.p3.2.m2.1.1.3.3.2">𝑆</ci><ci id="A2.SS3.p3.2.m2.1.1.3.3.3.cmml" xref="A2.SS3.p3.2.m2.1.1.3.3.3">𝑀</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p3.2.m2.1c">\text{min}_{\mathbf{S}\in S_{M}}</annotation><annotation encoding="application/x-llamapun" id="A2.SS3.p3.2.m2.1d">min start_POSTSUBSCRIPT bold_S ∈ italic_S start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math>).
The actual distance between two points in the minimum over all symmetries.
The symmetries are provided by the evaluation dataset as annotations.
<span class="ltx_text ltx_font_bold" id="A2.SS3.p3.2.1">Interpretation:</span> as an upper bound on the surface misalignment, the MSSD is very sensitive as even a small pose error can induce high surface discrepancies around high-curvature
regions of the object.
In practice, such high-curvature regions are suitable grasping points for <span class="ltx_text ltx_font_bold" id="A2.SS3.p3.2.2">robotic manipulation</span>, so a high AR-MSSD (Average Recall-MSSD, see below) suggests that the object pose estimation is suitable for robotic grasping.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS3.p4">
<p class="ltx_p" id="A2.SS3.p4.6">The <span class="ltx_text ltx_font_bold" id="A2.SS3.p4.6.1">Maximum Symmetry-Aware Projection Distance (MSPD)</span>
measures the maximum pixel displacement induced by an inaccurate
pose estimate when rendering the object.
<span class="ltx_text ltx_font_italic" id="A2.SS3.p4.6.2">Derivation details:</span>
The notation is the same as for the MSSD metric.
Let <math alttext="M" class="ltx_Math" display="inline" id="A2.SS3.p4.1.m1.1"><semantics id="A2.SS3.p4.1.m1.1a"><mi id="A2.SS3.p4.1.m1.1.1" xref="A2.SS3.p4.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="A2.SS3.p4.1.m1.1b"><ci id="A2.SS3.p4.1.m1.1.1.cmml" xref="A2.SS3.p4.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p4.1.m1.1c">M</annotation><annotation encoding="application/x-llamapun" id="A2.SS3.p4.1.m1.1d">italic_M</annotation></semantics></math> be the CAD object model, <math alttext="\hat{\mathbf{P}}" class="ltx_Math" display="inline" id="A2.SS3.p4.2.m2.1"><semantics id="A2.SS3.p4.2.m2.1a"><mover accent="true" id="A2.SS3.p4.2.m2.1.1" xref="A2.SS3.p4.2.m2.1.1.cmml"><mi id="A2.SS3.p4.2.m2.1.1.2" xref="A2.SS3.p4.2.m2.1.1.2.cmml">𝐏</mi><mo id="A2.SS3.p4.2.m2.1.1.1" xref="A2.SS3.p4.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="A2.SS3.p4.2.m2.1b"><apply id="A2.SS3.p4.2.m2.1.1.cmml" xref="A2.SS3.p4.2.m2.1.1"><ci id="A2.SS3.p4.2.m2.1.1.1.cmml" xref="A2.SS3.p4.2.m2.1.1.1">^</ci><ci id="A2.SS3.p4.2.m2.1.1.2.cmml" xref="A2.SS3.p4.2.m2.1.1.2">𝐏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p4.2.m2.1c">\hat{\mathbf{P}}</annotation><annotation encoding="application/x-llamapun" id="A2.SS3.p4.2.m2.1d">over^ start_ARG bold_P end_ARG</annotation></semantics></math> the estimated pose, <math alttext="\bar{\mathbf{P}}" class="ltx_Math" display="inline" id="A2.SS3.p4.3.m3.1"><semantics id="A2.SS3.p4.3.m3.1a"><mover accent="true" id="A2.SS3.p4.3.m3.1.1" xref="A2.SS3.p4.3.m3.1.1.cmml"><mi id="A2.SS3.p4.3.m3.1.1.2" xref="A2.SS3.p4.3.m3.1.1.2.cmml">𝐏</mi><mo id="A2.SS3.p4.3.m3.1.1.1" xref="A2.SS3.p4.3.m3.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="A2.SS3.p4.3.m3.1b"><apply id="A2.SS3.p4.3.m3.1.1.cmml" xref="A2.SS3.p4.3.m3.1.1"><ci id="A2.SS3.p4.3.m3.1.1.1.cmml" xref="A2.SS3.p4.3.m3.1.1.1">¯</ci><ci id="A2.SS3.p4.3.m3.1.1.2.cmml" xref="A2.SS3.p4.3.m3.1.1.2">𝐏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p4.3.m3.1c">\bar{\mathbf{P}}</annotation><annotation encoding="application/x-llamapun" id="A2.SS3.p4.3.m3.1d">over¯ start_ARG bold_P end_ARG</annotation></semantics></math> the ground-truth pose, <math alttext="V_{M}" class="ltx_Math" display="inline" id="A2.SS3.p4.4.m4.1"><semantics id="A2.SS3.p4.4.m4.1a"><msub id="A2.SS3.p4.4.m4.1.1" xref="A2.SS3.p4.4.m4.1.1.cmml"><mi id="A2.SS3.p4.4.m4.1.1.2" xref="A2.SS3.p4.4.m4.1.1.2.cmml">V</mi><mi id="A2.SS3.p4.4.m4.1.1.3" xref="A2.SS3.p4.4.m4.1.1.3.cmml">M</mi></msub><annotation-xml encoding="MathML-Content" id="A2.SS3.p4.4.m4.1b"><apply id="A2.SS3.p4.4.m4.1.1.cmml" xref="A2.SS3.p4.4.m4.1.1"><csymbol cd="ambiguous" id="A2.SS3.p4.4.m4.1.1.1.cmml" xref="A2.SS3.p4.4.m4.1.1">subscript</csymbol><ci id="A2.SS3.p4.4.m4.1.1.2.cmml" xref="A2.SS3.p4.4.m4.1.1.2">𝑉</ci><ci id="A2.SS3.p4.4.m4.1.1.3.cmml" xref="A2.SS3.p4.4.m4.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p4.4.m4.1c">V_{M}</annotation><annotation encoding="application/x-llamapun" id="A2.SS3.p4.4.m4.1d">italic_V start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT</annotation></semantics></math> a set of points sampled on the reference CAD model <math alttext="M" class="ltx_Math" display="inline" id="A2.SS3.p4.5.m5.1"><semantics id="A2.SS3.p4.5.m5.1a"><mi id="A2.SS3.p4.5.m5.1.1" xref="A2.SS3.p4.5.m5.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="A2.SS3.p4.5.m5.1b"><ci id="A2.SS3.p4.5.m5.1.1.cmml" xref="A2.SS3.p4.5.m5.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p4.5.m5.1c">M</annotation><annotation encoding="application/x-llamapun" id="A2.SS3.p4.5.m5.1d">italic_M</annotation></semantics></math> (<em class="ltx_emph ltx_font_italic" id="A2.SS3.p4.6.3">i.e</em>.<span class="ltx_text" id="A2.SS3.p4.6.4"></span> the YCB-V <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib111" title="">111</a>]</cite> one), and <math alttext="S_{M}" class="ltx_Math" display="inline" id="A2.SS3.p4.6.m6.1"><semantics id="A2.SS3.p4.6.m6.1a"><msub id="A2.SS3.p4.6.m6.1.1" xref="A2.SS3.p4.6.m6.1.1.cmml"><mi id="A2.SS3.p4.6.m6.1.1.2" xref="A2.SS3.p4.6.m6.1.1.2.cmml">S</mi><mi id="A2.SS3.p4.6.m6.1.1.3" xref="A2.SS3.p4.6.m6.1.1.3.cmml">M</mi></msub><annotation-xml encoding="MathML-Content" id="A2.SS3.p4.6.m6.1b"><apply id="A2.SS3.p4.6.m6.1.1.cmml" xref="A2.SS3.p4.6.m6.1.1"><csymbol cd="ambiguous" id="A2.SS3.p4.6.m6.1.1.1.cmml" xref="A2.SS3.p4.6.m6.1.1">subscript</csymbol><ci id="A2.SS3.p4.6.m6.1.1.2.cmml" xref="A2.SS3.p4.6.m6.1.1.2">𝑆</ci><ci id="A2.SS3.p4.6.m6.1.1.3.cmml" xref="A2.SS3.p4.6.m6.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p4.6.m6.1c">S_{M}</annotation><annotation encoding="application/x-llamapun" id="A2.SS3.p4.6.m6.1d">italic_S start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT</annotation></semantics></math> the set of symmetries of the object.
The MSPD is computed as:</p>
<table class="ltx_equation ltx_eqn_table" id="A2.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="e_{\text{MSPD}}(\hat{\mathbf{P}},\bar{\mathbf{P}},S_{M},V_{M})=\\
\text{min}_{\mathbf{S}\in S_{M}}\text{max}_{\mathbf{x}\in V_{M}}\|\text{proj}(%
\hat{\mathbf{P}}\mathbf{x})-\text{proj}(\bar{\mathbf{P}}\mathbf{S}\mathbf{x})%
\|_{2}\enspace." class="ltx_Math" display="block" id="A2.E2.m1.3"><semantics id="A2.E2.m1.3a"><mrow id="A2.E2.m1.3.3.1" xref="A2.E2.m1.3.3.1.1.cmml"><mrow id="A2.E2.m1.3.3.1.1" xref="A2.E2.m1.3.3.1.1.cmml"><mrow id="A2.E2.m1.3.3.1.1.2" xref="A2.E2.m1.3.3.1.1.2.cmml"><msub id="A2.E2.m1.3.3.1.1.2.4" xref="A2.E2.m1.3.3.1.1.2.4.cmml"><mi id="A2.E2.m1.3.3.1.1.2.4.2" xref="A2.E2.m1.3.3.1.1.2.4.2.cmml">e</mi><mtext id="A2.E2.m1.3.3.1.1.2.4.3" xref="A2.E2.m1.3.3.1.1.2.4.3a.cmml">MSPD</mtext></msub><mo id="A2.E2.m1.3.3.1.1.2.3" xref="A2.E2.m1.3.3.1.1.2.3.cmml">⁢</mo><mrow id="A2.E2.m1.3.3.1.1.2.2.2" xref="A2.E2.m1.3.3.1.1.2.2.3.cmml"><mo id="A2.E2.m1.3.3.1.1.2.2.2.3" stretchy="false" xref="A2.E2.m1.3.3.1.1.2.2.3.cmml">(</mo><mover accent="true" id="A2.E2.m1.1.1" xref="A2.E2.m1.1.1.cmml"><mi id="A2.E2.m1.1.1.2" xref="A2.E2.m1.1.1.2.cmml">𝐏</mi><mo id="A2.E2.m1.1.1.1" xref="A2.E2.m1.1.1.1.cmml">^</mo></mover><mo id="A2.E2.m1.3.3.1.1.2.2.2.4" xref="A2.E2.m1.3.3.1.1.2.2.3.cmml">,</mo><mover accent="true" id="A2.E2.m1.2.2" xref="A2.E2.m1.2.2.cmml"><mi id="A2.E2.m1.2.2.2" xref="A2.E2.m1.2.2.2.cmml">𝐏</mi><mo id="A2.E2.m1.2.2.1" xref="A2.E2.m1.2.2.1.cmml">¯</mo></mover><mo id="A2.E2.m1.3.3.1.1.2.2.2.5" xref="A2.E2.m1.3.3.1.1.2.2.3.cmml">,</mo><msub id="A2.E2.m1.3.3.1.1.1.1.1.1" xref="A2.E2.m1.3.3.1.1.1.1.1.1.cmml"><mi id="A2.E2.m1.3.3.1.1.1.1.1.1.2" xref="A2.E2.m1.3.3.1.1.1.1.1.1.2.cmml">S</mi><mi id="A2.E2.m1.3.3.1.1.1.1.1.1.3" xref="A2.E2.m1.3.3.1.1.1.1.1.1.3.cmml">M</mi></msub><mo id="A2.E2.m1.3.3.1.1.2.2.2.6" xref="A2.E2.m1.3.3.1.1.2.2.3.cmml">,</mo><msub id="A2.E2.m1.3.3.1.1.2.2.2.2" xref="A2.E2.m1.3.3.1.1.2.2.2.2.cmml"><mi id="A2.E2.m1.3.3.1.1.2.2.2.2.2" xref="A2.E2.m1.3.3.1.1.2.2.2.2.2.cmml">V</mi><mi id="A2.E2.m1.3.3.1.1.2.2.2.2.3" xref="A2.E2.m1.3.3.1.1.2.2.2.2.3.cmml">M</mi></msub><mo id="A2.E2.m1.3.3.1.1.2.2.2.7" stretchy="false" xref="A2.E2.m1.3.3.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="A2.E2.m1.3.3.1.1.4" xref="A2.E2.m1.3.3.1.1.4.cmml">=</mo><mrow id="A2.E2.m1.3.3.1.1.3" xref="A2.E2.m1.3.3.1.1.3.cmml"><msub id="A2.E2.m1.3.3.1.1.3.3" xref="A2.E2.m1.3.3.1.1.3.3.cmml"><mtext id="A2.E2.m1.3.3.1.1.3.3.2" xref="A2.E2.m1.3.3.1.1.3.3.2a.cmml">min</mtext><mrow id="A2.E2.m1.3.3.1.1.3.3.3" xref="A2.E2.m1.3.3.1.1.3.3.3.cmml"><mi id="A2.E2.m1.3.3.1.1.3.3.3.2" xref="A2.E2.m1.3.3.1.1.3.3.3.2.cmml">𝐒</mi><mo id="A2.E2.m1.3.3.1.1.3.3.3.1" xref="A2.E2.m1.3.3.1.1.3.3.3.1.cmml">∈</mo><msub id="A2.E2.m1.3.3.1.1.3.3.3.3" xref="A2.E2.m1.3.3.1.1.3.3.3.3.cmml"><mi id="A2.E2.m1.3.3.1.1.3.3.3.3.2" xref="A2.E2.m1.3.3.1.1.3.3.3.3.2.cmml">S</mi><mi id="A2.E2.m1.3.3.1.1.3.3.3.3.3" xref="A2.E2.m1.3.3.1.1.3.3.3.3.3.cmml">M</mi></msub></mrow></msub><mo id="A2.E2.m1.3.3.1.1.3.2" xref="A2.E2.m1.3.3.1.1.3.2.cmml">⁢</mo><msub id="A2.E2.m1.3.3.1.1.3.4" xref="A2.E2.m1.3.3.1.1.3.4.cmml"><mtext id="A2.E2.m1.3.3.1.1.3.4.2" xref="A2.E2.m1.3.3.1.1.3.4.2a.cmml">max</mtext><mrow id="A2.E2.m1.3.3.1.1.3.4.3" xref="A2.E2.m1.3.3.1.1.3.4.3.cmml"><mi id="A2.E2.m1.3.3.1.1.3.4.3.2" xref="A2.E2.m1.3.3.1.1.3.4.3.2.cmml">𝐱</mi><mo id="A2.E2.m1.3.3.1.1.3.4.3.1" xref="A2.E2.m1.3.3.1.1.3.4.3.1.cmml">∈</mo><msub id="A2.E2.m1.3.3.1.1.3.4.3.3" xref="A2.E2.m1.3.3.1.1.3.4.3.3.cmml"><mi id="A2.E2.m1.3.3.1.1.3.4.3.3.2" xref="A2.E2.m1.3.3.1.1.3.4.3.3.2.cmml">V</mi><mi id="A2.E2.m1.3.3.1.1.3.4.3.3.3" xref="A2.E2.m1.3.3.1.1.3.4.3.3.3.cmml">M</mi></msub></mrow></msub><mo id="A2.E2.m1.3.3.1.1.3.2a" xref="A2.E2.m1.3.3.1.1.3.2.cmml">⁢</mo><msub id="A2.E2.m1.3.3.1.1.3.1" xref="A2.E2.m1.3.3.1.1.3.1.cmml"><mrow id="A2.E2.m1.3.3.1.1.3.1.1.1" xref="A2.E2.m1.3.3.1.1.3.1.1.2.cmml"><mo id="A2.E2.m1.3.3.1.1.3.1.1.1.2" stretchy="false" xref="A2.E2.m1.3.3.1.1.3.1.1.2.1.cmml">‖</mo><mrow id="A2.E2.m1.3.3.1.1.3.1.1.1.1" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.cmml"><mrow id="A2.E2.m1.3.3.1.1.3.1.1.1.1.1" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.cmml"><mtext id="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.3" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.3a.cmml">proj</mtext><mo id="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.2" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.2.cmml">⁢</mo><mrow id="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.1.1" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.1.1.1.cmml"><mo id="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.1.1.2" stretchy="false" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.1.1.1" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.1.1.1.2" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.1.1.1.2.cmml"><mi id="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.1.1.1.2.2" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.1.1.1.2.2.cmml">𝐏</mi><mo id="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.1.1.1.2.1" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mo id="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.1.1.1.1" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.1.1.1.3" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.1.1.1.3.cmml">𝐱</mi></mrow><mo id="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.1.1.3" stretchy="false" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="A2.E2.m1.3.3.1.1.3.1.1.1.1.3" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.3.cmml">−</mo><mrow id="A2.E2.m1.3.3.1.1.3.1.1.1.1.2" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.cmml"><mtext id="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.3" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.3a.cmml">proj</mtext><mo id="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.2" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.2.cmml">⁢</mo><mrow id="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.1.1" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.1.1.1.cmml"><mo id="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.1.1.2" stretchy="false" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.1.1.1.cmml">(</mo><mrow id="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.1.1.1" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.1.1.1.cmml"><mover accent="true" id="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.1.1.1.2" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.1.1.1.2.cmml"><mi id="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.1.1.1.2.2" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.1.1.1.2.2.cmml">𝐏</mi><mo id="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.1.1.1.2.1" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.1.1.1.2.1.cmml">¯</mo></mover><mo id="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.1.1.1.1" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.1.1.1.1.cmml">⁢</mo><mi id="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.1.1.1.3" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.1.1.1.3.cmml">𝐒𝐱</mi></mrow><mo id="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.1.1.3" stretchy="false" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="A2.E2.m1.3.3.1.1.3.1.1.1.3" stretchy="false" xref="A2.E2.m1.3.3.1.1.3.1.1.2.1.cmml">‖</mo></mrow><mn id="A2.E2.m1.3.3.1.1.3.1.3" xref="A2.E2.m1.3.3.1.1.3.1.3.cmml">2</mn></msub></mrow></mrow><mo id="A2.E2.m1.3.3.1.2" lspace="0em" xref="A2.E2.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.E2.m1.3b"><apply id="A2.E2.m1.3.3.1.1.cmml" xref="A2.E2.m1.3.3.1"><eq id="A2.E2.m1.3.3.1.1.4.cmml" xref="A2.E2.m1.3.3.1.1.4"></eq><apply id="A2.E2.m1.3.3.1.1.2.cmml" xref="A2.E2.m1.3.3.1.1.2"><times id="A2.E2.m1.3.3.1.1.2.3.cmml" xref="A2.E2.m1.3.3.1.1.2.3"></times><apply id="A2.E2.m1.3.3.1.1.2.4.cmml" xref="A2.E2.m1.3.3.1.1.2.4"><csymbol cd="ambiguous" id="A2.E2.m1.3.3.1.1.2.4.1.cmml" xref="A2.E2.m1.3.3.1.1.2.4">subscript</csymbol><ci id="A2.E2.m1.3.3.1.1.2.4.2.cmml" xref="A2.E2.m1.3.3.1.1.2.4.2">𝑒</ci><ci id="A2.E2.m1.3.3.1.1.2.4.3a.cmml" xref="A2.E2.m1.3.3.1.1.2.4.3"><mtext id="A2.E2.m1.3.3.1.1.2.4.3.cmml" mathsize="70%" xref="A2.E2.m1.3.3.1.1.2.4.3">MSPD</mtext></ci></apply><vector id="A2.E2.m1.3.3.1.1.2.2.3.cmml" xref="A2.E2.m1.3.3.1.1.2.2.2"><apply id="A2.E2.m1.1.1.cmml" xref="A2.E2.m1.1.1"><ci id="A2.E2.m1.1.1.1.cmml" xref="A2.E2.m1.1.1.1">^</ci><ci id="A2.E2.m1.1.1.2.cmml" xref="A2.E2.m1.1.1.2">𝐏</ci></apply><apply id="A2.E2.m1.2.2.cmml" xref="A2.E2.m1.2.2"><ci id="A2.E2.m1.2.2.1.cmml" xref="A2.E2.m1.2.2.1">¯</ci><ci id="A2.E2.m1.2.2.2.cmml" xref="A2.E2.m1.2.2.2">𝐏</ci></apply><apply id="A2.E2.m1.3.3.1.1.1.1.1.1.cmml" xref="A2.E2.m1.3.3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="A2.E2.m1.3.3.1.1.1.1.1.1.1.cmml" xref="A2.E2.m1.3.3.1.1.1.1.1.1">subscript</csymbol><ci id="A2.E2.m1.3.3.1.1.1.1.1.1.2.cmml" xref="A2.E2.m1.3.3.1.1.1.1.1.1.2">𝑆</ci><ci id="A2.E2.m1.3.3.1.1.1.1.1.1.3.cmml" xref="A2.E2.m1.3.3.1.1.1.1.1.1.3">𝑀</ci></apply><apply id="A2.E2.m1.3.3.1.1.2.2.2.2.cmml" xref="A2.E2.m1.3.3.1.1.2.2.2.2"><csymbol cd="ambiguous" id="A2.E2.m1.3.3.1.1.2.2.2.2.1.cmml" xref="A2.E2.m1.3.3.1.1.2.2.2.2">subscript</csymbol><ci id="A2.E2.m1.3.3.1.1.2.2.2.2.2.cmml" xref="A2.E2.m1.3.3.1.1.2.2.2.2.2">𝑉</ci><ci id="A2.E2.m1.3.3.1.1.2.2.2.2.3.cmml" xref="A2.E2.m1.3.3.1.1.2.2.2.2.3">𝑀</ci></apply></vector></apply><apply id="A2.E2.m1.3.3.1.1.3.cmml" xref="A2.E2.m1.3.3.1.1.3"><times id="A2.E2.m1.3.3.1.1.3.2.cmml" xref="A2.E2.m1.3.3.1.1.3.2"></times><apply id="A2.E2.m1.3.3.1.1.3.3.cmml" xref="A2.E2.m1.3.3.1.1.3.3"><csymbol cd="ambiguous" id="A2.E2.m1.3.3.1.1.3.3.1.cmml" xref="A2.E2.m1.3.3.1.1.3.3">subscript</csymbol><ci id="A2.E2.m1.3.3.1.1.3.3.2a.cmml" xref="A2.E2.m1.3.3.1.1.3.3.2"><mtext id="A2.E2.m1.3.3.1.1.3.3.2.cmml" xref="A2.E2.m1.3.3.1.1.3.3.2">min</mtext></ci><apply id="A2.E2.m1.3.3.1.1.3.3.3.cmml" xref="A2.E2.m1.3.3.1.1.3.3.3"><in id="A2.E2.m1.3.3.1.1.3.3.3.1.cmml" xref="A2.E2.m1.3.3.1.1.3.3.3.1"></in><ci id="A2.E2.m1.3.3.1.1.3.3.3.2.cmml" xref="A2.E2.m1.3.3.1.1.3.3.3.2">𝐒</ci><apply id="A2.E2.m1.3.3.1.1.3.3.3.3.cmml" xref="A2.E2.m1.3.3.1.1.3.3.3.3"><csymbol cd="ambiguous" id="A2.E2.m1.3.3.1.1.3.3.3.3.1.cmml" xref="A2.E2.m1.3.3.1.1.3.3.3.3">subscript</csymbol><ci id="A2.E2.m1.3.3.1.1.3.3.3.3.2.cmml" xref="A2.E2.m1.3.3.1.1.3.3.3.3.2">𝑆</ci><ci id="A2.E2.m1.3.3.1.1.3.3.3.3.3.cmml" xref="A2.E2.m1.3.3.1.1.3.3.3.3.3">𝑀</ci></apply></apply></apply><apply id="A2.E2.m1.3.3.1.1.3.4.cmml" xref="A2.E2.m1.3.3.1.1.3.4"><csymbol cd="ambiguous" id="A2.E2.m1.3.3.1.1.3.4.1.cmml" xref="A2.E2.m1.3.3.1.1.3.4">subscript</csymbol><ci id="A2.E2.m1.3.3.1.1.3.4.2a.cmml" xref="A2.E2.m1.3.3.1.1.3.4.2"><mtext id="A2.E2.m1.3.3.1.1.3.4.2.cmml" xref="A2.E2.m1.3.3.1.1.3.4.2">max</mtext></ci><apply id="A2.E2.m1.3.3.1.1.3.4.3.cmml" xref="A2.E2.m1.3.3.1.1.3.4.3"><in id="A2.E2.m1.3.3.1.1.3.4.3.1.cmml" xref="A2.E2.m1.3.3.1.1.3.4.3.1"></in><ci id="A2.E2.m1.3.3.1.1.3.4.3.2.cmml" xref="A2.E2.m1.3.3.1.1.3.4.3.2">𝐱</ci><apply id="A2.E2.m1.3.3.1.1.3.4.3.3.cmml" xref="A2.E2.m1.3.3.1.1.3.4.3.3"><csymbol cd="ambiguous" id="A2.E2.m1.3.3.1.1.3.4.3.3.1.cmml" xref="A2.E2.m1.3.3.1.1.3.4.3.3">subscript</csymbol><ci id="A2.E2.m1.3.3.1.1.3.4.3.3.2.cmml" xref="A2.E2.m1.3.3.1.1.3.4.3.3.2">𝑉</ci><ci id="A2.E2.m1.3.3.1.1.3.4.3.3.3.cmml" xref="A2.E2.m1.3.3.1.1.3.4.3.3.3">𝑀</ci></apply></apply></apply><apply id="A2.E2.m1.3.3.1.1.3.1.cmml" xref="A2.E2.m1.3.3.1.1.3.1"><csymbol cd="ambiguous" id="A2.E2.m1.3.3.1.1.3.1.2.cmml" xref="A2.E2.m1.3.3.1.1.3.1">subscript</csymbol><apply id="A2.E2.m1.3.3.1.1.3.1.1.2.cmml" xref="A2.E2.m1.3.3.1.1.3.1.1.1"><csymbol cd="latexml" id="A2.E2.m1.3.3.1.1.3.1.1.2.1.cmml" xref="A2.E2.m1.3.3.1.1.3.1.1.1.2">norm</csymbol><apply id="A2.E2.m1.3.3.1.1.3.1.1.1.1.cmml" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1"><minus id="A2.E2.m1.3.3.1.1.3.1.1.1.1.3.cmml" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.3"></minus><apply id="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.cmml" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.1"><times id="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.2.cmml" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.2"></times><ci id="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.3a.cmml" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.3"><mtext id="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.3.cmml" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.3">proj</mtext></ci><apply id="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.1.1.1.cmml" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.1.1"><times id="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.1.1.1.1.cmml" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.1.1.1.1"></times><apply id="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.1.1.1.2.cmml" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.1.1.1.2"><ci id="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.1.1.1.2.1.cmml" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.1.1.1.2.1">^</ci><ci id="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.1.1.1.2.2.cmml" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.1.1.1.2.2">𝐏</ci></apply><ci id="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.1.1.1.3.cmml" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.1.1.1.1.3">𝐱</ci></apply></apply><apply id="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.cmml" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.2"><times id="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.2.cmml" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.2"></times><ci id="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.3a.cmml" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.3"><mtext id="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.3.cmml" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.3">proj</mtext></ci><apply id="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.1.1.1.cmml" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.1.1"><times id="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.1.1.1.1.cmml" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.1.1.1.1"></times><apply id="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.1.1.1.2.cmml" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.1.1.1.2"><ci id="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.1.1.1.2.1.cmml" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.1.1.1.2.1">¯</ci><ci id="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.1.1.1.2.2.cmml" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.1.1.1.2.2">𝐏</ci></apply><ci id="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.1.1.1.3.cmml" xref="A2.E2.m1.3.3.1.1.3.1.1.1.1.2.1.1.1.3">𝐒𝐱</ci></apply></apply></apply></apply><cn id="A2.E2.m1.3.3.1.1.3.1.3.cmml" type="integer" xref="A2.E2.m1.3.3.1.1.3.1.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.E2.m1.3c">e_{\text{MSPD}}(\hat{\mathbf{P}},\bar{\mathbf{P}},S_{M},V_{M})=\\
\text{min}_{\mathbf{S}\in S_{M}}\text{max}_{\mathbf{x}\in V_{M}}\|\text{proj}(%
\hat{\mathbf{P}}\mathbf{x})-\text{proj}(\bar{\mathbf{P}}\mathbf{S}\mathbf{x})%
\|_{2}\enspace.</annotation><annotation encoding="application/x-llamapun" id="A2.E2.m1.3d">italic_e start_POSTSUBSCRIPT MSPD end_POSTSUBSCRIPT ( over^ start_ARG bold_P end_ARG , over¯ start_ARG bold_P end_ARG , italic_S start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT , italic_V start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT ) = min start_POSTSUBSCRIPT bold_S ∈ italic_S start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT end_POSTSUBSCRIPT max start_POSTSUBSCRIPT bold_x ∈ italic_V start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT end_POSTSUBSCRIPT ∥ proj ( over^ start_ARG bold_P end_ARG bold_x ) - proj ( over¯ start_ARG bold_P end_ARG bold_Sx ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="A2.SS3.p4.7">As for the MSSD, the metric is computed on the set of points <math alttext="V_{M}" class="ltx_Math" display="inline" id="A2.SS3.p4.7.m1.1"><semantics id="A2.SS3.p4.7.m1.1a"><msub id="A2.SS3.p4.7.m1.1.1" xref="A2.SS3.p4.7.m1.1.1.cmml"><mi id="A2.SS3.p4.7.m1.1.1.2" xref="A2.SS3.p4.7.m1.1.1.2.cmml">V</mi><mi id="A2.SS3.p4.7.m1.1.1.3" xref="A2.SS3.p4.7.m1.1.1.3.cmml">M</mi></msub><annotation-xml encoding="MathML-Content" id="A2.SS3.p4.7.m1.1b"><apply id="A2.SS3.p4.7.m1.1.1.cmml" xref="A2.SS3.p4.7.m1.1.1"><csymbol cd="ambiguous" id="A2.SS3.p4.7.m1.1.1.1.cmml" xref="A2.SS3.p4.7.m1.1.1">subscript</csymbol><ci id="A2.SS3.p4.7.m1.1.1.2.cmml" xref="A2.SS3.p4.7.m1.1.1.2">𝑉</ci><ci id="A2.SS3.p4.7.m1.1.1.3.cmml" xref="A2.SS3.p4.7.m1.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p4.7.m1.1c">V_{M}</annotation><annotation encoding="application/x-llamapun" id="A2.SS3.p4.7.m1.1d">italic_V start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT</annotation></semantics></math> sampled on the reference CAD model when it is positioned with the ground-truth pose and with the estimated poses, respectively.
The points are projected onto the image plane and the MSPD reports the maximum distance between the projected points over all pairs.
<span class="ltx_text ltx_font_bold" id="A2.SS3.p4.7.1">Interpretation:</span>
Note that the projective nature of the MSPD makes it unsuitable for applications that require physical interactions with the world.
Instead, this perceptual error is better suited for vision applications such as <span class="ltx_text ltx_font_bold" id="A2.SS3.p4.7.2">Augmented and Virtual Reality</span> that exploit the rendering of positioned objects.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS3.p5">
<p class="ltx_p" id="A2.SS3.p5.10">The <span class="ltx_text ltx_font_bold" id="A2.SS3.p5.10.1">Visible Surface Discrepancy (VSD)</span> measures the average misalignment of the visible surface of the object along the camera-z axis.
<span class="ltx_text ltx_font_italic" id="A2.SS3.p5.10.2">Derivation details:</span>
Let <math alttext="\hat{V}" class="ltx_Math" display="inline" id="A2.SS3.p5.1.m1.1"><semantics id="A2.SS3.p5.1.m1.1a"><mover accent="true" id="A2.SS3.p5.1.m1.1.1" xref="A2.SS3.p5.1.m1.1.1.cmml"><mi id="A2.SS3.p5.1.m1.1.1.2" xref="A2.SS3.p5.1.m1.1.1.2.cmml">V</mi><mo id="A2.SS3.p5.1.m1.1.1.1" xref="A2.SS3.p5.1.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="A2.SS3.p5.1.m1.1b"><apply id="A2.SS3.p5.1.m1.1.1.cmml" xref="A2.SS3.p5.1.m1.1.1"><ci id="A2.SS3.p5.1.m1.1.1.1.cmml" xref="A2.SS3.p5.1.m1.1.1.1">^</ci><ci id="A2.SS3.p5.1.m1.1.1.2.cmml" xref="A2.SS3.p5.1.m1.1.1.2">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p5.1.m1.1c">\hat{V}</annotation><annotation encoding="application/x-llamapun" id="A2.SS3.p5.1.m1.1d">over^ start_ARG italic_V end_ARG</annotation></semantics></math> and <math alttext="\bar{V}" class="ltx_Math" display="inline" id="A2.SS3.p5.2.m2.1"><semantics id="A2.SS3.p5.2.m2.1a"><mover accent="true" id="A2.SS3.p5.2.m2.1.1" xref="A2.SS3.p5.2.m2.1.1.cmml"><mi id="A2.SS3.p5.2.m2.1.1.2" xref="A2.SS3.p5.2.m2.1.1.2.cmml">V</mi><mo id="A2.SS3.p5.2.m2.1.1.1" xref="A2.SS3.p5.2.m2.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="A2.SS3.p5.2.m2.1b"><apply id="A2.SS3.p5.2.m2.1.1.cmml" xref="A2.SS3.p5.2.m2.1.1"><ci id="A2.SS3.p5.2.m2.1.1.1.cmml" xref="A2.SS3.p5.2.m2.1.1.1">¯</ci><ci id="A2.SS3.p5.2.m2.1.1.2.cmml" xref="A2.SS3.p5.2.m2.1.1.2">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p5.2.m2.1c">\bar{V}</annotation><annotation encoding="application/x-llamapun" id="A2.SS3.p5.2.m2.1d">over¯ start_ARG italic_V end_ARG</annotation></semantics></math> be visibility masks, <em class="ltx_emph ltx_font_italic" id="A2.SS3.p5.10.3">i.e</em>.<span class="ltx_text" id="A2.SS3.p5.10.4"></span>, the set of pixels where the reference model <math alttext="M" class="ltx_Math" display="inline" id="A2.SS3.p5.3.m3.1"><semantics id="A2.SS3.p5.3.m3.1a"><mi id="A2.SS3.p5.3.m3.1.1" xref="A2.SS3.p5.3.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="A2.SS3.p5.3.m3.1b"><ci id="A2.SS3.p5.3.m3.1.1.cmml" xref="A2.SS3.p5.3.m3.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p5.3.m3.1c">M</annotation><annotation encoding="application/x-llamapun" id="A2.SS3.p5.3.m3.1d">italic_M</annotation></semantics></math> is visible in the image when the model is positioned at the estimated pose <math alttext="\hat{P}" class="ltx_Math" display="inline" id="A2.SS3.p5.4.m4.1"><semantics id="A2.SS3.p5.4.m4.1a"><mover accent="true" id="A2.SS3.p5.4.m4.1.1" xref="A2.SS3.p5.4.m4.1.1.cmml"><mi id="A2.SS3.p5.4.m4.1.1.2" xref="A2.SS3.p5.4.m4.1.1.2.cmml">P</mi><mo id="A2.SS3.p5.4.m4.1.1.1" xref="A2.SS3.p5.4.m4.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="A2.SS3.p5.4.m4.1b"><apply id="A2.SS3.p5.4.m4.1.1.cmml" xref="A2.SS3.p5.4.m4.1.1"><ci id="A2.SS3.p5.4.m4.1.1.1.cmml" xref="A2.SS3.p5.4.m4.1.1.1">^</ci><ci id="A2.SS3.p5.4.m4.1.1.2.cmml" xref="A2.SS3.p5.4.m4.1.1.2">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p5.4.m4.1c">\hat{P}</annotation><annotation encoding="application/x-llamapun" id="A2.SS3.p5.4.m4.1d">over^ start_ARG italic_P end_ARG</annotation></semantics></math> and the ground-truth pose <math alttext="\bar{P}" class="ltx_Math" display="inline" id="A2.SS3.p5.5.m5.1"><semantics id="A2.SS3.p5.5.m5.1a"><mover accent="true" id="A2.SS3.p5.5.m5.1.1" xref="A2.SS3.p5.5.m5.1.1.cmml"><mi id="A2.SS3.p5.5.m5.1.1.2" xref="A2.SS3.p5.5.m5.1.1.2.cmml">P</mi><mo id="A2.SS3.p5.5.m5.1.1.1" xref="A2.SS3.p5.5.m5.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="A2.SS3.p5.5.m5.1b"><apply id="A2.SS3.p5.5.m5.1.1.cmml" xref="A2.SS3.p5.5.m5.1.1"><ci id="A2.SS3.p5.5.m5.1.1.1.cmml" xref="A2.SS3.p5.5.m5.1.1.1">¯</ci><ci id="A2.SS3.p5.5.m5.1.1.2.cmml" xref="A2.SS3.p5.5.m5.1.1.2">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p5.5.m5.1c">\bar{P}</annotation><annotation encoding="application/x-llamapun" id="A2.SS3.p5.5.m5.1d">over¯ start_ARG italic_P end_ARG</annotation></semantics></math>.
Let <math alttext="\hat{D}" class="ltx_Math" display="inline" id="A2.SS3.p5.6.m6.1"><semantics id="A2.SS3.p5.6.m6.1a"><mover accent="true" id="A2.SS3.p5.6.m6.1.1" xref="A2.SS3.p5.6.m6.1.1.cmml"><mi id="A2.SS3.p5.6.m6.1.1.2" xref="A2.SS3.p5.6.m6.1.1.2.cmml">D</mi><mo id="A2.SS3.p5.6.m6.1.1.1" xref="A2.SS3.p5.6.m6.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="A2.SS3.p5.6.m6.1b"><apply id="A2.SS3.p5.6.m6.1.1.cmml" xref="A2.SS3.p5.6.m6.1.1"><ci id="A2.SS3.p5.6.m6.1.1.1.cmml" xref="A2.SS3.p5.6.m6.1.1.1">^</ci><ci id="A2.SS3.p5.6.m6.1.1.2.cmml" xref="A2.SS3.p5.6.m6.1.1.2">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p5.6.m6.1c">\hat{D}</annotation><annotation encoding="application/x-llamapun" id="A2.SS3.p5.6.m6.1d">over^ start_ARG italic_D end_ARG</annotation></semantics></math> and <math alttext="\bar{D}" class="ltx_Math" display="inline" id="A2.SS3.p5.7.m7.1"><semantics id="A2.SS3.p5.7.m7.1a"><mover accent="true" id="A2.SS3.p5.7.m7.1.1" xref="A2.SS3.p5.7.m7.1.1.cmml"><mi id="A2.SS3.p5.7.m7.1.1.2" xref="A2.SS3.p5.7.m7.1.1.2.cmml">D</mi><mo id="A2.SS3.p5.7.m7.1.1.1" xref="A2.SS3.p5.7.m7.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="A2.SS3.p5.7.m7.1b"><apply id="A2.SS3.p5.7.m7.1.1.cmml" xref="A2.SS3.p5.7.m7.1.1"><ci id="A2.SS3.p5.7.m7.1.1.1.cmml" xref="A2.SS3.p5.7.m7.1.1.1">¯</ci><ci id="A2.SS3.p5.7.m7.1.1.2.cmml" xref="A2.SS3.p5.7.m7.1.1.2">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p5.7.m7.1c">\bar{D}</annotation><annotation encoding="application/x-llamapun" id="A2.SS3.p5.7.m7.1d">over¯ start_ARG italic_D end_ARG</annotation></semantics></math> be the distance maps obtained by rendering the reference object model <math alttext="M" class="ltx_Math" display="inline" id="A2.SS3.p5.8.m8.1"><semantics id="A2.SS3.p5.8.m8.1a"><mi id="A2.SS3.p5.8.m8.1.1" xref="A2.SS3.p5.8.m8.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="A2.SS3.p5.8.m8.1b"><ci id="A2.SS3.p5.8.m8.1.1.cmml" xref="A2.SS3.p5.8.m8.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p5.8.m8.1c">M</annotation><annotation encoding="application/x-llamapun" id="A2.SS3.p5.8.m8.1d">italic_M</annotation></semantics></math> in the estimated pose <math alttext="\hat{P}" class="ltx_Math" display="inline" id="A2.SS3.p5.9.m9.1"><semantics id="A2.SS3.p5.9.m9.1a"><mover accent="true" id="A2.SS3.p5.9.m9.1.1" xref="A2.SS3.p5.9.m9.1.1.cmml"><mi id="A2.SS3.p5.9.m9.1.1.2" xref="A2.SS3.p5.9.m9.1.1.2.cmml">P</mi><mo id="A2.SS3.p5.9.m9.1.1.1" xref="A2.SS3.p5.9.m9.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="A2.SS3.p5.9.m9.1b"><apply id="A2.SS3.p5.9.m9.1.1.cmml" xref="A2.SS3.p5.9.m9.1.1"><ci id="A2.SS3.p5.9.m9.1.1.1.cmml" xref="A2.SS3.p5.9.m9.1.1.1">^</ci><ci id="A2.SS3.p5.9.m9.1.1.2.cmml" xref="A2.SS3.p5.9.m9.1.1.2">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p5.9.m9.1c">\hat{P}</annotation><annotation encoding="application/x-llamapun" id="A2.SS3.p5.9.m9.1d">over^ start_ARG italic_P end_ARG</annotation></semantics></math> and the ground-truth pose <math alttext="\bar{P}" class="ltx_Math" display="inline" id="A2.SS3.p5.10.m10.1"><semantics id="A2.SS3.p5.10.m10.1a"><mover accent="true" id="A2.SS3.p5.10.m10.1.1" xref="A2.SS3.p5.10.m10.1.1.cmml"><mi id="A2.SS3.p5.10.m10.1.1.2" xref="A2.SS3.p5.10.m10.1.1.2.cmml">P</mi><mo id="A2.SS3.p5.10.m10.1.1.1" xref="A2.SS3.p5.10.m10.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="A2.SS3.p5.10.m10.1b"><apply id="A2.SS3.p5.10.m10.1.1.cmml" xref="A2.SS3.p5.10.m10.1.1"><ci id="A2.SS3.p5.10.m10.1.1.1.cmml" xref="A2.SS3.p5.10.m10.1.1.1">¯</ci><ci id="A2.SS3.p5.10.m10.1.1.2.cmml" xref="A2.SS3.p5.10.m10.1.1.2">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p5.10.m10.1c">\bar{P}</annotation><annotation encoding="application/x-llamapun" id="A2.SS3.p5.10.m10.1d">over¯ start_ARG italic_P end_ARG</annotation></semantics></math>, respectively. The VSD is computed as:</p>
<table class="ltx_equation ltx_eqn_table" id="A2.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="e_{\text{VSD}}(\hat{D},\bar{D},\hat{V},\bar{V},\tau)=\mathrm{avg}_{p\epsilon%
\hat{V}\cup\bar{V}}\\
\begin{cases}0&amp;\text{if}~{}p\in\hat{V}\cap\bar{V}\wedge\|\hat{D}(p)-\bar{D}(p)%
\|\leq\tau\\
1&amp;\text{otherwise }\end{cases}\enspace." class="ltx_Math" display="block" id="A2.E3.m1.10"><semantics id="A2.E3.m1.10a"><mrow id="A2.E3.m1.10.10.1" xref="A2.E3.m1.10.10.1.1.cmml"><mrow id="A2.E3.m1.10.10.1.1" xref="A2.E3.m1.10.10.1.1.cmml"><mrow id="A2.E3.m1.10.10.1.1.2" xref="A2.E3.m1.10.10.1.1.2.cmml"><msub id="A2.E3.m1.10.10.1.1.2.2" xref="A2.E3.m1.10.10.1.1.2.2.cmml"><mi id="A2.E3.m1.10.10.1.1.2.2.2" xref="A2.E3.m1.10.10.1.1.2.2.2.cmml">e</mi><mtext id="A2.E3.m1.10.10.1.1.2.2.3" xref="A2.E3.m1.10.10.1.1.2.2.3a.cmml">VSD</mtext></msub><mo id="A2.E3.m1.10.10.1.1.2.1" xref="A2.E3.m1.10.10.1.1.2.1.cmml">⁢</mo><mrow id="A2.E3.m1.10.10.1.1.2.3.2" xref="A2.E3.m1.10.10.1.1.2.3.1.cmml"><mo id="A2.E3.m1.10.10.1.1.2.3.2.1" stretchy="false" xref="A2.E3.m1.10.10.1.1.2.3.1.cmml">(</mo><mover accent="true" id="A2.E3.m1.5.5" xref="A2.E3.m1.5.5.cmml"><mi id="A2.E3.m1.5.5.2" xref="A2.E3.m1.5.5.2.cmml">D</mi><mo id="A2.E3.m1.5.5.1" xref="A2.E3.m1.5.5.1.cmml">^</mo></mover><mo id="A2.E3.m1.10.10.1.1.2.3.2.2" xref="A2.E3.m1.10.10.1.1.2.3.1.cmml">,</mo><mover accent="true" id="A2.E3.m1.6.6" xref="A2.E3.m1.6.6.cmml"><mi id="A2.E3.m1.6.6.2" xref="A2.E3.m1.6.6.2.cmml">D</mi><mo id="A2.E3.m1.6.6.1" xref="A2.E3.m1.6.6.1.cmml">¯</mo></mover><mo id="A2.E3.m1.10.10.1.1.2.3.2.3" xref="A2.E3.m1.10.10.1.1.2.3.1.cmml">,</mo><mover accent="true" id="A2.E3.m1.7.7" xref="A2.E3.m1.7.7.cmml"><mi id="A2.E3.m1.7.7.2" xref="A2.E3.m1.7.7.2.cmml">V</mi><mo id="A2.E3.m1.7.7.1" xref="A2.E3.m1.7.7.1.cmml">^</mo></mover><mo id="A2.E3.m1.10.10.1.1.2.3.2.4" xref="A2.E3.m1.10.10.1.1.2.3.1.cmml">,</mo><mover accent="true" id="A2.E3.m1.8.8" xref="A2.E3.m1.8.8.cmml"><mi id="A2.E3.m1.8.8.2" xref="A2.E3.m1.8.8.2.cmml">V</mi><mo id="A2.E3.m1.8.8.1" xref="A2.E3.m1.8.8.1.cmml">¯</mo></mover><mo id="A2.E3.m1.10.10.1.1.2.3.2.5" xref="A2.E3.m1.10.10.1.1.2.3.1.cmml">,</mo><mi id="A2.E3.m1.9.9" xref="A2.E3.m1.9.9.cmml">τ</mi><mo id="A2.E3.m1.10.10.1.1.2.3.2.6" stretchy="false" xref="A2.E3.m1.10.10.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="A2.E3.m1.10.10.1.1.1" xref="A2.E3.m1.10.10.1.1.1.cmml">=</mo><mrow id="A2.E3.m1.10.10.1.1.3" xref="A2.E3.m1.10.10.1.1.3.cmml"><msub id="A2.E3.m1.10.10.1.1.3.2" xref="A2.E3.m1.10.10.1.1.3.2.cmml"><mi id="A2.E3.m1.10.10.1.1.3.2.2" xref="A2.E3.m1.10.10.1.1.3.2.2.cmml">avg</mi><mrow id="A2.E3.m1.10.10.1.1.3.2.3" xref="A2.E3.m1.10.10.1.1.3.2.3.cmml"><mrow id="A2.E3.m1.10.10.1.1.3.2.3.2" xref="A2.E3.m1.10.10.1.1.3.2.3.2.cmml"><mi id="A2.E3.m1.10.10.1.1.3.2.3.2.2" xref="A2.E3.m1.10.10.1.1.3.2.3.2.2.cmml">p</mi><mo id="A2.E3.m1.10.10.1.1.3.2.3.2.1" xref="A2.E3.m1.10.10.1.1.3.2.3.2.1.cmml">⁢</mo><mi id="A2.E3.m1.10.10.1.1.3.2.3.2.3" xref="A2.E3.m1.10.10.1.1.3.2.3.2.3.cmml">ϵ</mi><mo id="A2.E3.m1.10.10.1.1.3.2.3.2.1a" xref="A2.E3.m1.10.10.1.1.3.2.3.2.1.cmml">⁢</mo><mover accent="true" id="A2.E3.m1.10.10.1.1.3.2.3.2.4" xref="A2.E3.m1.10.10.1.1.3.2.3.2.4.cmml"><mi id="A2.E3.m1.10.10.1.1.3.2.3.2.4.2" xref="A2.E3.m1.10.10.1.1.3.2.3.2.4.2.cmml">V</mi><mo id="A2.E3.m1.10.10.1.1.3.2.3.2.4.1" xref="A2.E3.m1.10.10.1.1.3.2.3.2.4.1.cmml">^</mo></mover></mrow><mo id="A2.E3.m1.10.10.1.1.3.2.3.1" xref="A2.E3.m1.10.10.1.1.3.2.3.1.cmml">∪</mo><mover accent="true" id="A2.E3.m1.10.10.1.1.3.2.3.3" xref="A2.E3.m1.10.10.1.1.3.2.3.3.cmml"><mi id="A2.E3.m1.10.10.1.1.3.2.3.3.2" xref="A2.E3.m1.10.10.1.1.3.2.3.3.2.cmml">V</mi><mo id="A2.E3.m1.10.10.1.1.3.2.3.3.1" xref="A2.E3.m1.10.10.1.1.3.2.3.3.1.cmml">¯</mo></mover></mrow></msub><mo id="A2.E3.m1.10.10.1.1.3.1" xref="A2.E3.m1.10.10.1.1.3.1.cmml">⁢</mo><mrow id="A2.E3.m1.4.4" xref="A2.E3.m1.10.10.1.1.3.3.1.cmml"><mo id="A2.E3.m1.4.4.5" xref="A2.E3.m1.10.10.1.1.3.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" id="A2.E3.m1.4.4.4" rowspacing="0pt" xref="A2.E3.m1.10.10.1.1.3.3.1.cmml"><mtr id="A2.E3.m1.4.4.4a" xref="A2.E3.m1.10.10.1.1.3.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="A2.E3.m1.4.4.4b" xref="A2.E3.m1.10.10.1.1.3.3.1.cmml"><mn id="A2.E3.m1.1.1.1.1.1.1" xref="A2.E3.m1.1.1.1.1.1.1.cmml">0</mn></mtd><mtd class="ltx_align_left" columnalign="left" id="A2.E3.m1.4.4.4c" xref="A2.E3.m1.10.10.1.1.3.3.1.cmml"><mrow id="A2.E3.m1.2.2.2.2.2.1" xref="A2.E3.m1.2.2.2.2.2.1.cmml"><mrow id="A2.E3.m1.2.2.2.2.2.1.5" xref="A2.E3.m1.2.2.2.2.2.1.5.cmml"><mtext id="A2.E3.m1.2.2.2.2.2.1.5.2" xref="A2.E3.m1.2.2.2.2.2.1.5.2a.cmml">if</mtext><mo id="A2.E3.m1.2.2.2.2.2.1.5.1" lspace="0.330em" xref="A2.E3.m1.2.2.2.2.2.1.5.1.cmml">⁢</mo><mi id="A2.E3.m1.2.2.2.2.2.1.5.3" xref="A2.E3.m1.2.2.2.2.2.1.5.3.cmml">p</mi></mrow><mo id="A2.E3.m1.2.2.2.2.2.1.6" xref="A2.E3.m1.2.2.2.2.2.1.6.cmml">∈</mo><mrow id="A2.E3.m1.2.2.2.2.2.1.3" xref="A2.E3.m1.2.2.2.2.2.1.3.cmml"><mrow id="A2.E3.m1.2.2.2.2.2.1.3.3" xref="A2.E3.m1.2.2.2.2.2.1.3.3.cmml"><mover accent="true" id="A2.E3.m1.2.2.2.2.2.1.3.3.2" xref="A2.E3.m1.2.2.2.2.2.1.3.3.2.cmml"><mi id="A2.E3.m1.2.2.2.2.2.1.3.3.2.2" xref="A2.E3.m1.2.2.2.2.2.1.3.3.2.2.cmml">V</mi><mo id="A2.E3.m1.2.2.2.2.2.1.3.3.2.1" xref="A2.E3.m1.2.2.2.2.2.1.3.3.2.1.cmml">^</mo></mover><mo id="A2.E3.m1.2.2.2.2.2.1.3.3.1" xref="A2.E3.m1.2.2.2.2.2.1.3.3.1.cmml">∩</mo><mover accent="true" id="A2.E3.m1.2.2.2.2.2.1.3.3.3" xref="A2.E3.m1.2.2.2.2.2.1.3.3.3.cmml"><mi id="A2.E3.m1.2.2.2.2.2.1.3.3.3.2" xref="A2.E3.m1.2.2.2.2.2.1.3.3.3.2.cmml">V</mi><mo id="A2.E3.m1.2.2.2.2.2.1.3.3.3.1" xref="A2.E3.m1.2.2.2.2.2.1.3.3.3.1.cmml">¯</mo></mover></mrow><mo id="A2.E3.m1.2.2.2.2.2.1.3.2" xref="A2.E3.m1.2.2.2.2.2.1.3.2.cmml">∧</mo><mrow id="A2.E3.m1.2.2.2.2.2.1.3.1.1" xref="A2.E3.m1.2.2.2.2.2.1.3.1.2.cmml"><mo id="A2.E3.m1.2.2.2.2.2.1.3.1.1.2" stretchy="false" xref="A2.E3.m1.2.2.2.2.2.1.3.1.2.1.cmml">‖</mo><mrow id="A2.E3.m1.2.2.2.2.2.1.3.1.1.1" xref="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.cmml"><mrow id="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.2" xref="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.2.cmml"><mover accent="true" id="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.2.2" xref="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.2.2.cmml"><mi id="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.2.2.2" xref="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.2.2.2.cmml">D</mi><mo id="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.2.2.1" xref="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.2.2.1.cmml">^</mo></mover><mo id="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.2.1" xref="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.2.1.cmml">⁢</mo><mrow id="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.2.3.2" xref="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.2.cmml"><mo id="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.2.3.2.1" stretchy="false" xref="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.2.cmml">(</mo><mi id="A2.E3.m1.2.2.2.2.2.1.1" xref="A2.E3.m1.2.2.2.2.2.1.1.cmml">p</mi><mo id="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.2.3.2.2" stretchy="false" xref="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.1" xref="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.1.cmml">−</mo><mrow id="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.3" xref="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.3.cmml"><mover accent="true" id="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.3.2" xref="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.3.2.cmml"><mi id="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.3.2.2" xref="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.3.2.2.cmml">D</mi><mo id="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.3.2.1" xref="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.3.2.1.cmml">¯</mo></mover><mo id="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.3.1" xref="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.3.1.cmml">⁢</mo><mrow id="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.3.3.2" xref="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.3.cmml"><mo id="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.3.3.2.1" stretchy="false" xref="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.3.cmml">(</mo><mi id="A2.E3.m1.2.2.2.2.2.1.2" xref="A2.E3.m1.2.2.2.2.2.1.2.cmml">p</mi><mo id="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.3.3.2.2" stretchy="false" xref="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo id="A2.E3.m1.2.2.2.2.2.1.3.1.1.3" stretchy="false" xref="A2.E3.m1.2.2.2.2.2.1.3.1.2.1.cmml">‖</mo></mrow></mrow><mo id="A2.E3.m1.2.2.2.2.2.1.7" xref="A2.E3.m1.2.2.2.2.2.1.7.cmml">≤</mo><mi id="A2.E3.m1.2.2.2.2.2.1.8" xref="A2.E3.m1.2.2.2.2.2.1.8.cmml">τ</mi></mrow></mtd></mtr><mtr id="A2.E3.m1.4.4.4d" xref="A2.E3.m1.10.10.1.1.3.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="A2.E3.m1.4.4.4e" xref="A2.E3.m1.10.10.1.1.3.3.1.cmml"><mn id="A2.E3.m1.3.3.3.3.1.1" xref="A2.E3.m1.3.3.3.3.1.1.cmml">1</mn></mtd><mtd class="ltx_align_left" columnalign="left" id="A2.E3.m1.4.4.4f" xref="A2.E3.m1.10.10.1.1.3.3.1.cmml"><mtext id="A2.E3.m1.4.4.4.4.2.1" xref="A2.E3.m1.4.4.4.4.2.1a.cmml">otherwise </mtext></mtd></mtr></mtable></mrow></mrow></mrow><mo id="A2.E3.m1.10.10.1.2" lspace="0em" xref="A2.E3.m1.10.10.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.E3.m1.10b"><apply id="A2.E3.m1.10.10.1.1.cmml" xref="A2.E3.m1.10.10.1"><eq id="A2.E3.m1.10.10.1.1.1.cmml" xref="A2.E3.m1.10.10.1.1.1"></eq><apply id="A2.E3.m1.10.10.1.1.2.cmml" xref="A2.E3.m1.10.10.1.1.2"><times id="A2.E3.m1.10.10.1.1.2.1.cmml" xref="A2.E3.m1.10.10.1.1.2.1"></times><apply id="A2.E3.m1.10.10.1.1.2.2.cmml" xref="A2.E3.m1.10.10.1.1.2.2"><csymbol cd="ambiguous" id="A2.E3.m1.10.10.1.1.2.2.1.cmml" xref="A2.E3.m1.10.10.1.1.2.2">subscript</csymbol><ci id="A2.E3.m1.10.10.1.1.2.2.2.cmml" xref="A2.E3.m1.10.10.1.1.2.2.2">𝑒</ci><ci id="A2.E3.m1.10.10.1.1.2.2.3a.cmml" xref="A2.E3.m1.10.10.1.1.2.2.3"><mtext id="A2.E3.m1.10.10.1.1.2.2.3.cmml" mathsize="70%" xref="A2.E3.m1.10.10.1.1.2.2.3">VSD</mtext></ci></apply><vector id="A2.E3.m1.10.10.1.1.2.3.1.cmml" xref="A2.E3.m1.10.10.1.1.2.3.2"><apply id="A2.E3.m1.5.5.cmml" xref="A2.E3.m1.5.5"><ci id="A2.E3.m1.5.5.1.cmml" xref="A2.E3.m1.5.5.1">^</ci><ci id="A2.E3.m1.5.5.2.cmml" xref="A2.E3.m1.5.5.2">𝐷</ci></apply><apply id="A2.E3.m1.6.6.cmml" xref="A2.E3.m1.6.6"><ci id="A2.E3.m1.6.6.1.cmml" xref="A2.E3.m1.6.6.1">¯</ci><ci id="A2.E3.m1.6.6.2.cmml" xref="A2.E3.m1.6.6.2">𝐷</ci></apply><apply id="A2.E3.m1.7.7.cmml" xref="A2.E3.m1.7.7"><ci id="A2.E3.m1.7.7.1.cmml" xref="A2.E3.m1.7.7.1">^</ci><ci id="A2.E3.m1.7.7.2.cmml" xref="A2.E3.m1.7.7.2">𝑉</ci></apply><apply id="A2.E3.m1.8.8.cmml" xref="A2.E3.m1.8.8"><ci id="A2.E3.m1.8.8.1.cmml" xref="A2.E3.m1.8.8.1">¯</ci><ci id="A2.E3.m1.8.8.2.cmml" xref="A2.E3.m1.8.8.2">𝑉</ci></apply><ci id="A2.E3.m1.9.9.cmml" xref="A2.E3.m1.9.9">𝜏</ci></vector></apply><apply id="A2.E3.m1.10.10.1.1.3.cmml" xref="A2.E3.m1.10.10.1.1.3"><times id="A2.E3.m1.10.10.1.1.3.1.cmml" xref="A2.E3.m1.10.10.1.1.3.1"></times><apply id="A2.E3.m1.10.10.1.1.3.2.cmml" xref="A2.E3.m1.10.10.1.1.3.2"><csymbol cd="ambiguous" id="A2.E3.m1.10.10.1.1.3.2.1.cmml" xref="A2.E3.m1.10.10.1.1.3.2">subscript</csymbol><ci id="A2.E3.m1.10.10.1.1.3.2.2.cmml" xref="A2.E3.m1.10.10.1.1.3.2.2">avg</ci><apply id="A2.E3.m1.10.10.1.1.3.2.3.cmml" xref="A2.E3.m1.10.10.1.1.3.2.3"><union id="A2.E3.m1.10.10.1.1.3.2.3.1.cmml" xref="A2.E3.m1.10.10.1.1.3.2.3.1"></union><apply id="A2.E3.m1.10.10.1.1.3.2.3.2.cmml" xref="A2.E3.m1.10.10.1.1.3.2.3.2"><times id="A2.E3.m1.10.10.1.1.3.2.3.2.1.cmml" xref="A2.E3.m1.10.10.1.1.3.2.3.2.1"></times><ci id="A2.E3.m1.10.10.1.1.3.2.3.2.2.cmml" xref="A2.E3.m1.10.10.1.1.3.2.3.2.2">𝑝</ci><ci id="A2.E3.m1.10.10.1.1.3.2.3.2.3.cmml" xref="A2.E3.m1.10.10.1.1.3.2.3.2.3">italic-ϵ</ci><apply id="A2.E3.m1.10.10.1.1.3.2.3.2.4.cmml" xref="A2.E3.m1.10.10.1.1.3.2.3.2.4"><ci id="A2.E3.m1.10.10.1.1.3.2.3.2.4.1.cmml" xref="A2.E3.m1.10.10.1.1.3.2.3.2.4.1">^</ci><ci id="A2.E3.m1.10.10.1.1.3.2.3.2.4.2.cmml" xref="A2.E3.m1.10.10.1.1.3.2.3.2.4.2">𝑉</ci></apply></apply><apply id="A2.E3.m1.10.10.1.1.3.2.3.3.cmml" xref="A2.E3.m1.10.10.1.1.3.2.3.3"><ci id="A2.E3.m1.10.10.1.1.3.2.3.3.1.cmml" xref="A2.E3.m1.10.10.1.1.3.2.3.3.1">¯</ci><ci id="A2.E3.m1.10.10.1.1.3.2.3.3.2.cmml" xref="A2.E3.m1.10.10.1.1.3.2.3.3.2">𝑉</ci></apply></apply></apply><apply id="A2.E3.m1.10.10.1.1.3.3.1.cmml" xref="A2.E3.m1.4.4"><csymbol cd="latexml" id="A2.E3.m1.10.10.1.1.3.3.1.1.cmml" xref="A2.E3.m1.4.4.5">cases</csymbol><cn id="A2.E3.m1.1.1.1.1.1.1.cmml" type="integer" xref="A2.E3.m1.1.1.1.1.1.1">0</cn><apply id="A2.E3.m1.2.2.2.2.2.1.cmml" xref="A2.E3.m1.2.2.2.2.2.1"><and id="A2.E3.m1.2.2.2.2.2.1a.cmml" xref="A2.E3.m1.2.2.2.2.2.1"></and><apply id="A2.E3.m1.2.2.2.2.2.1b.cmml" xref="A2.E3.m1.2.2.2.2.2.1"><in id="A2.E3.m1.2.2.2.2.2.1.6.cmml" xref="A2.E3.m1.2.2.2.2.2.1.6"></in><apply id="A2.E3.m1.2.2.2.2.2.1.5.cmml" xref="A2.E3.m1.2.2.2.2.2.1.5"><times id="A2.E3.m1.2.2.2.2.2.1.5.1.cmml" xref="A2.E3.m1.2.2.2.2.2.1.5.1"></times><ci id="A2.E3.m1.2.2.2.2.2.1.5.2a.cmml" xref="A2.E3.m1.2.2.2.2.2.1.5.2"><mtext id="A2.E3.m1.2.2.2.2.2.1.5.2.cmml" xref="A2.E3.m1.2.2.2.2.2.1.5.2">if</mtext></ci><ci id="A2.E3.m1.2.2.2.2.2.1.5.3.cmml" xref="A2.E3.m1.2.2.2.2.2.1.5.3">𝑝</ci></apply><apply id="A2.E3.m1.2.2.2.2.2.1.3.cmml" xref="A2.E3.m1.2.2.2.2.2.1.3"><and id="A2.E3.m1.2.2.2.2.2.1.3.2.cmml" xref="A2.E3.m1.2.2.2.2.2.1.3.2"></and><apply id="A2.E3.m1.2.2.2.2.2.1.3.3.cmml" xref="A2.E3.m1.2.2.2.2.2.1.3.3"><intersect id="A2.E3.m1.2.2.2.2.2.1.3.3.1.cmml" xref="A2.E3.m1.2.2.2.2.2.1.3.3.1"></intersect><apply id="A2.E3.m1.2.2.2.2.2.1.3.3.2.cmml" xref="A2.E3.m1.2.2.2.2.2.1.3.3.2"><ci id="A2.E3.m1.2.2.2.2.2.1.3.3.2.1.cmml" xref="A2.E3.m1.2.2.2.2.2.1.3.3.2.1">^</ci><ci id="A2.E3.m1.2.2.2.2.2.1.3.3.2.2.cmml" xref="A2.E3.m1.2.2.2.2.2.1.3.3.2.2">𝑉</ci></apply><apply id="A2.E3.m1.2.2.2.2.2.1.3.3.3.cmml" xref="A2.E3.m1.2.2.2.2.2.1.3.3.3"><ci id="A2.E3.m1.2.2.2.2.2.1.3.3.3.1.cmml" xref="A2.E3.m1.2.2.2.2.2.1.3.3.3.1">¯</ci><ci id="A2.E3.m1.2.2.2.2.2.1.3.3.3.2.cmml" xref="A2.E3.m1.2.2.2.2.2.1.3.3.3.2">𝑉</ci></apply></apply><apply id="A2.E3.m1.2.2.2.2.2.1.3.1.2.cmml" xref="A2.E3.m1.2.2.2.2.2.1.3.1.1"><csymbol cd="latexml" id="A2.E3.m1.2.2.2.2.2.1.3.1.2.1.cmml" xref="A2.E3.m1.2.2.2.2.2.1.3.1.1.2">norm</csymbol><apply id="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.cmml" xref="A2.E3.m1.2.2.2.2.2.1.3.1.1.1"><minus id="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.1.cmml" xref="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.1"></minus><apply id="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.2.cmml" xref="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.2"><times id="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.2.1.cmml" xref="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.2.1"></times><apply id="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.2.2.cmml" xref="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.2.2"><ci id="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.2.2.1.cmml" xref="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.2.2.1">^</ci><ci id="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.2.2.2.cmml" xref="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.2.2.2">𝐷</ci></apply><ci id="A2.E3.m1.2.2.2.2.2.1.1.cmml" xref="A2.E3.m1.2.2.2.2.2.1.1">𝑝</ci></apply><apply id="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.3.cmml" xref="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.3"><times id="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.3.1.cmml" xref="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.3.1"></times><apply id="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.3.2.cmml" xref="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.3.2"><ci id="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.3.2.1.cmml" xref="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.3.2.1">¯</ci><ci id="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.3.2.2.cmml" xref="A2.E3.m1.2.2.2.2.2.1.3.1.1.1.3.2.2">𝐷</ci></apply><ci id="A2.E3.m1.2.2.2.2.2.1.2.cmml" xref="A2.E3.m1.2.2.2.2.2.1.2">𝑝</ci></apply></apply></apply></apply></apply><apply id="A2.E3.m1.2.2.2.2.2.1c.cmml" xref="A2.E3.m1.2.2.2.2.2.1"><leq id="A2.E3.m1.2.2.2.2.2.1.7.cmml" xref="A2.E3.m1.2.2.2.2.2.1.7"></leq><share href="https://arxiv.org/html/2408.08234v1#A2.E3.m1.2.2.2.2.2.1.3.cmml" id="A2.E3.m1.2.2.2.2.2.1d.cmml" xref="A2.E3.m1.2.2.2.2.2.1"></share><ci id="A2.E3.m1.2.2.2.2.2.1.8.cmml" xref="A2.E3.m1.2.2.2.2.2.1.8">𝜏</ci></apply></apply><cn id="A2.E3.m1.3.3.3.3.1.1.cmml" type="integer" xref="A2.E3.m1.3.3.3.3.1.1">1</cn><ci id="A2.E3.m1.4.4.4.4.2.1a.cmml" xref="A2.E3.m1.4.4.4.4.2.1"><mtext id="A2.E3.m1.4.4.4.4.2.1.cmml" xref="A2.E3.m1.4.4.4.4.2.1">otherwise </mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.E3.m1.10c">e_{\text{VSD}}(\hat{D},\bar{D},\hat{V},\bar{V},\tau)=\mathrm{avg}_{p\epsilon%
\hat{V}\cup\bar{V}}\\
\begin{cases}0&amp;\text{if}~{}p\in\hat{V}\cap\bar{V}\wedge\|\hat{D}(p)-\bar{D}(p)%
\|\leq\tau\\
1&amp;\text{otherwise }\end{cases}\enspace.</annotation><annotation encoding="application/x-llamapun" id="A2.E3.m1.10d">italic_e start_POSTSUBSCRIPT VSD end_POSTSUBSCRIPT ( over^ start_ARG italic_D end_ARG , over¯ start_ARG italic_D end_ARG , over^ start_ARG italic_V end_ARG , over¯ start_ARG italic_V end_ARG , italic_τ ) = roman_avg start_POSTSUBSCRIPT italic_p italic_ϵ over^ start_ARG italic_V end_ARG ∪ over¯ start_ARG italic_V end_ARG end_POSTSUBSCRIPT { start_ROW start_CELL 0 end_CELL start_CELL if italic_p ∈ over^ start_ARG italic_V end_ARG ∩ over¯ start_ARG italic_V end_ARG ∧ ∥ over^ start_ARG italic_D end_ARG ( italic_p ) - over¯ start_ARG italic_D end_ARG ( italic_p ) ∥ ≤ italic_τ end_CELL end_ROW start_ROW start_CELL 1 end_CELL start_CELL otherwise end_CELL end_ROW .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="A2.SS3.p5.11">The VSD is also a perceptual metric but the definition of the distance is slightly different.
It measures a distance in 3D but contrary to the MSSD, it does not measure it between the corresponding points sampled on the mesh.
Instead, the pairs are formed by taking a visible point from each mesh that projects onto the same pixel.
In practice, the CAD model is rendered from the two poses to generate distance maps (<em class="ltx_emph ltx_font_italic" id="A2.SS3.p5.11.1">i.e</em>.<span class="ltx_text" id="A2.SS3.p5.11.2"></span>, depth maps) and the VSD
corresponds to the average number of misplaced rendered pixels.
Misplacement can either mean an incorrect distance map value or an incorrect position in the image, <em class="ltx_emph ltx_font_italic" id="A2.SS3.p5.11.3">e.g</em>.<span class="ltx_text" id="A2.SS3.p5.11.4"></span>, if the estimated pose translates the object too much in the camera-x or camera-y directions, the rendered pixels will be disjoint on the image space.
<span class="ltx_text ltx_font_bold" id="A2.SS3.p5.11.5">Interpretation:</span> a low VSD indicates that the object is
well positioned, yet can miss to report errors in the estimated pose’s rotation.
If the object is mostly symmetric, a low VSD can indicate that the centroid of the object is well estimated but it provides less information on the rotation’s error.
For example, take the 14-mug object and assume that the estimated pose is the ground-truth pose flipped so that the mug is upside-down.
Then the position of the CAD models positioned with the ground-truth pose and the estimated pose differ only on the side in which the 14-mug handle is.
Instead, the surface of the 14-mug’s "body" will be aligned.
Thus, the distance maps of these points will be mostly equal and will dominate the metric, indicating a good position.
Thus, the VSD is better suited <span class="ltx_text ltx_font_bold" id="A2.SS3.p5.11.6">for applications with tolerance to such edge-cases such as robotic navigation.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS3.p6">
<p class="ltx_p" id="A2.SS3.p6.1"><span class="ltx_text ltx_font_bold" id="A2.SS3.p6.1.1">Throughout the evaluation, the metrics are derived on the same model <math alttext="M" class="ltx_Math" display="inline" id="A2.SS3.p6.1.1.m1.1"><semantics id="A2.SS3.p6.1.1.m1.1a"><mi id="A2.SS3.p6.1.1.m1.1.1" xref="A2.SS3.p6.1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="A2.SS3.p6.1.1.m1.1b"><ci id="A2.SS3.p6.1.1.m1.1.1.cmml" xref="A2.SS3.p6.1.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p6.1.1.m1.1c">M</annotation><annotation encoding="application/x-llamapun" id="A2.SS3.p6.1.1.m1.1d">italic_M</annotation></semantics></math> which is the original CAD model provided in the YCB-V <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib111" title="">111</a>]</cite> dataset, <em class="ltx_emph ltx_font_italic" id="A2.SS3.p6.1.1.1">i.e</em>.<span class="ltx_text" id="A2.SS3.p6.1.1.2"></span>, the reconstructed meshes are not used when computing the metrics.</span>
This ensures that the results for poses obtained from different meshes are comparable.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS3.p7">
<p class="ltx_p" id="A2.SS3.p7.1">Note that the BOP benchmark does not report these errors as is but reports the
<span class="ltx_text ltx_font_bold" id="A2.SS3.p7.1.1">Average Recall (AR)</span> on these errors. Given an error threshold, the recall is the fraction of the estimated poses for which the
error falls below the threshold.
<span class="ltx_text ltx_font_bold" id="A2.SS3.p7.1.2">Since these metrics quantify the pose error, the lower, the better.</span>
The recall is computed separately over each metric and as usual <span class="ltx_text ltx_font_bold" id="A2.SS3.p7.1.3">for recalls, the higher is better</span>.
The Average Recall (AR) for a given metric, which we report as AR-VSD, AR-MSPD,
AR-MSSD, is the average of several recalls computed over a range of error
thresholds.
Finally, we also report the <span class="ltx_text ltx_font_bold" id="A2.SS3.p7.1.4">global average recall AR</span> computed as the average
of the three average recalls AR-VSD, AR-MSPD, AR-MSSD. As specified in the BOP
benchmark, it measures a method’s overall performance.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS3.p8">
<p class="ltx_p" id="A2.SS3.p8.1">The recall averaging used in the benchmark slightly differs from the averaging of the BOP benchmark.
Instead of averaging the metrics over the set of all test images, we averaged the recall over the test images depicting a given object instance (<em class="ltx_emph ltx_font_italic" id="A2.SS3.p8.1.1">e.g</em>.<span class="ltx_text" id="A2.SS3.p8.1.2"></span> 02-cracker-box) to output the object-specific AR-MSPD, AR-MSSD, AR-VSD, AR.
The global performance of the method is the average of these metrics over the set of objects.
The performance of the methods of specific object categories is the average of the subset of objects that make that category.
Because of the change in the averaging order, the metrics we report can be slightly lower than the ones reported in the BOP benchmark by about 5% on average.
This edit is not critical as we are interested in the relative pose performances rather than the absolute ones.</p>
</div>
<figure class="ltx_table" id="A2.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A2.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A2.T1.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="A2.T1.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T1.1.1.1.1.1">
<span class="ltx_p" id="A2.T1.1.1.1.1.1.1" style="width:108.4pt;">Category</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="A2.T1.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="A2.T1.1.1.1.2.1">
<span class="ltx_p" id="A2.T1.1.1.1.2.1.1" style="width:252.9pt;">Objects</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T1.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T1.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T1.1.2.1.1.1">
<span class="ltx_p" id="A2.T1.1.2.1.1.1.1" style="width:108.4pt;">Lambertian-Textured-Large</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T1.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="A2.T1.1.2.1.2.1">
<span class="ltx_p" id="A2.T1.1.2.1.2.1.1" style="width:252.9pt;">02-cracker-box, 03-sugar-box, 16-wooden-block</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T1.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T1.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T1.1.3.2.1.1">
<span class="ltx_p" id="A2.T1.1.3.2.1.1.1" style="width:108.4pt;">Lambertian-Textured-Small</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T1.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="A2.T1.1.3.2.2.1">
<span class="ltx_p" id="A2.T1.1.3.2.2.1.1" style="width:252.9pt;">07-pudding-box, 08-gelatin-box, 18-large-marker</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T1.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T1.1.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T1.1.4.3.1.1">
<span class="ltx_p" id="A2.T1.1.4.3.1.1.1" style="width:108.4pt;">Shiny-Textured</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T1.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="A2.T1.1.4.3.2.1">
<span class="ltx_p" id="A2.T1.1.4.3.2.1.1" style="width:252.9pt;">01-master-chef-can, 04-tomato-soup-can, 06-tuna-fish-can, 09-potted-meat-can,13-bowl, 14-mug</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T1.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T1.1.5.4.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T1.1.5.4.1.1">
<span class="ltx_p" id="A2.T1.1.5.4.1.1.1" style="width:108.4pt;">Uniform-Texture</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T1.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="A2.T1.1.5.4.2.1">
<span class="ltx_p" id="A2.T1.1.5.4.2.1.1" style="width:252.9pt;">10-banana, 11-pitcher-base, 17-scissors, 19-large-clamp, 20-extra-large-clamp, 21-foam-brick</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T1.1.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T1.1.6.5.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T1.1.6.5.1.1">
<span class="ltx_p" id="A2.T1.1.6.5.1.1.1" style="width:108.4pt;">Low-Texture</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T1.1.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="A2.T1.1.6.5.2.1">
<span class="ltx_p" id="A2.T1.1.6.5.2.1.1" style="width:252.9pt;">05-mustard-bottle, 12-bleach-cleanser</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T1.1.7.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T1.1.7.6.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T1.1.7.6.1.1">
<span class="ltx_p" id="A2.T1.1.7.6.1.1.1" style="width:108.4pt;">Scissors-Like</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T1.1.7.6.2">
<span class="ltx_inline-block ltx_align_top" id="A2.T1.1.7.6.2.1">
<span class="ltx_p" id="A2.T1.1.7.6.2.1.1" style="width:252.9pt;">17-scissors, 19-large-clamp, 20-extra-large-clamp</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T1.1.8.7">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="A2.T1.1.8.7.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T1.1.8.7.1.1">
<span class="ltx_p" id="A2.T1.1.8.7.1.1.1" style="width:108.4pt;">Legacy-Objects</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="A2.T1.1.8.7.2">
<span class="ltx_inline-block ltx_align_top" id="A2.T1.1.8.7.2.1">
<span class="ltx_p" id="A2.T1.1.8.7.2.1.1" style="width:252.9pt;">03-sugar-box, 04-tomato-soup-can, 10-banana, 12-bleach-cleaner, 13-bowl, 14-mug, 16-wooden-block, 17-scissors, 18-large-marker, 19-large-clamp, 20-extra-large-clamp, 21-foam-brick</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T1.1.9.8">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="A2.T1.1.9.8.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T1.1.9.8.1.1">
<span class="ltx_p" id="A2.T1.1.9.8.1.1.1" style="width:108.4pt;">Updated-Objects</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="A2.T1.1.9.8.2">
<span class="ltx_inline-block ltx_align_top" id="A2.T1.1.9.8.2.1">
<span class="ltx_p" id="A2.T1.1.9.8.2.1.1" style="width:252.9pt;">01-master-chef-can, 02-cracker-box, 05-mustard-bottle, 06-tuna-fish-can, 07-pudding-box, 08-gelatin-box, 09-potted-meat-can, 11-pitcher-base, 15-power-drill</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span><span class="ltx_text ltx_font_bold" id="A2.T1.3.1">Object Categories.</span> The categories are characteristic of
object properties such as size, shape, texture, and materials.
We also distinguish between the objects that are the same between the YCB-V datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib111" title="">111</a>]</cite> and our data collection (‘legacy’) and the ones that got updated by their manufacturer (’updated’).
</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.4 </span>Object Categorization</h3>
<div class="ltx_para ltx_noindent" id="A2.SS4.p1">
<p class="ltx_p" id="A2.SS4.p1.1">We split the YCB-V objects into categories with specific properties depending on their size, their texture and their materials (Tab. <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A2.T1" title="Table 1 ‣ B.3 BOP Evaluation ‣ Appendix B Evaluation Setup ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS4.p2">
<p class="ltx_p" id="A2.SS4.p2.1"><span class="ltx_text ltx_font_bold" id="A2.SS4.p2.1.1">Lambertian-Textured-Large / Small</span> refers to large / small objects strongly textured, with diffuse reflections.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS4.p3">
<p class="ltx_p" id="A2.SS4.p3.1"><span class="ltx_text ltx_font_bold" id="A2.SS4.p3.1.1">Shiny-Textured</span> objects are medium and small objects which material has high reflectivity.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS4.p4">
<p class="ltx_p" id="A2.SS4.p4.1"><span class="ltx_text ltx_font_bold" id="A2.SS4.p4.1.1">Uniform-Texture</span> objects have no texture, <em class="ltx_emph ltx_font_italic" id="A2.SS4.p4.1.2">i.e</em>.<span class="ltx_text" id="A2.SS4.p4.1.3"></span>, they are color-uniform whereas <span class="ltx_text ltx_font_bold" id="A2.SS4.p4.1.4">Low-texture</span> objects have both high-texture areas and textureless areas.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS4.p5">
<p class="ltx_p" id="A2.SS4.p5.1"><span class="ltx_text ltx_font_bold" id="A2.SS4.p5.1.1">Scissors-like</span>: Objects with scissors-like shapes that may have holes inside the mesh and have symmetries from different views.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS4.p6">
<p class="ltx_p" id="A2.SS4.p6.1"><span class="ltx_text ltx_font_bold" id="A2.SS4.p6.1.1">Updated Objects:</span>
The YCB-V <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib111" title="">111</a>]</cite> are made of objects commonly found in grocery stores.
However, brands regularly update their product packaging, which means that the object’s texture gets updated.
Some of the original YCB-V <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib111" title="">111</a>]</cite> objects are not available on the market anymore so we use the alternatives suggested by the official YCB <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib17" title="">17</a>]</cite> website<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>https://www.ycbbenchmarks.com/object-set/</span></span></span>.
The newer versions of the objects differ in texture and scale, with an average change estimated at 4% of the object’s dimensions.
As for the texture update, the 01-master-chef-can, 11-pitcher-base, and 15-power-drill underwent full texture updates and other objects have only minor updates.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS4.p7">
<p class="ltx_p" id="A2.SS4.p7.1"><span class="ltx_text ltx_font_bold" id="A2.SS4.p7.1.1">Legacy Objects</span> are the objects that are the same between the YCB-V <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib111" title="">111</a>]</cite> dataset and our data collection, <em class="ltx_emph ltx_font_italic" id="A2.SS4.p7.1.2">i.e</em>.<span class="ltx_text" id="A2.SS4.p7.1.3"></span>, all objects minus the updated objects.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Additional Results</h2>
<div class="ltx_para ltx_noindent" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">We complete the experimental results of the main paper with
the average recall on each of the three pose errors (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.SS1" title="C.1 Pose Evaluation: Average Recall ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">C.1</span></a>),
the influence of the size of the training data on the performance (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.SS2" title="C.2 Reconstruction with varying training size ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">C.2</span></a>),
further results on the relation between the object categories and the pose performance (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.SS3" title="C.3 Object Categories ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">C.3</span></a>),
the importance of the texture for pose estimation (Sec <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.SS4" title="C.4 Influence of the Texturing Algorithm ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">C.4</span></a>),
the detail of the pose performances per objects (Sec <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.SS5" title="C.5 Per-Object Pose Evaluation ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">C.5</span></a>),
and qualitative results showing renderings from the reconstructed meshes (Sec <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.SS6" title="C.6 Qualitative Results ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">C.6</span></a>).</p>
</div>
<section class="ltx_subsection" id="A3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Pose Evaluation: Average Recall</h3>
<figure class="ltx_figure" id="A3.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="463" id="A3.F7.g1" src="x11.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>
<span class="ltx_text ltx_font_bold" id="A3.F7.4.1">Detailed Pose Evaluation of the 3D reconstructions.</span>
We measure the performance of FoundationPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib12" title="">12</a>]</cite> (uniform bar) and Megapose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib55" title="">55</a>]</cite> (lined bar) when replacing the CAD models with the 3D reconstructions.
The blue lines indicate the performance of the pose estimators when using the original CAD models (FoundationPose: <span class="ltx_text ltx_font_bold" id="A3.F7.5.2" style="color:#0080FF;">full line</span>, Megapose: <span class="ltx_text ltx_font_bold" id="A3.F7.6.3" style="color:#0080FF;">dashed line</span>).
Overall, a gap remains between CAD models and 3D reconstructions for pose estimation.
This gap is smaller on the VSD, which indicates that 3D reconstructions can be suitable for pose estimation in applications with tolerance to pose errors.
</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="A3.SS1.p1">
<p class="ltx_p" id="A3.SS1.p1.1">The BOP benchmark defines multiple errors relevant to assess how suitable a reconstruction is for a given application such as augmented and virtual reality (MSPD), navigation (VSD), and object manipulation (MSSD).
In the main paper, the pose performances are measured with the Average Recall (AR) averaged over all three VSD, MSSD, and MSPD errors
We now report the AR for each error separately (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.F7" title="Figure 7 ‣ C.1 Pose Evaluation: Average Recall ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">7</span></a>) to support the conclusions of the main paper (Sec.5 "Finer Pose Evaluation").</p>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS1.p2">
<p class="ltx_p" id="A3.SS1.p2.1">The performance gap between Megapose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib55" title="">55</a>]</cite> and FoundationPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib12" title="">12</a>]</cite> when they are evaluated with the original CAD models is relatively smaller for the MSPD than for the other errors.
One interpretation is the improvement of FoundationPose over Megapose is particularly useful for 3D spatial tasks, as measured by the VSD and the MSSD, <em class="ltx_emph ltx_font_italic" id="A3.SS1.p2.1.1">e.g</em>.<span class="ltx_text" id="A3.SS1.p2.1.2"></span>, for navigation and object manipulation.
The improvement is smaller for tasks involving object rendering for which Megapose is close to FoundationPose.</p>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS1.p3">
<p class="ltx_p" id="A3.SS1.p3.1">The main paper mentions the performance drop induced when replacing the original CAD models with the reconstructed ones and how it is consitent between the two pose estimators for the best reconstruction methods, <em class="ltx_emph ltx_font_italic" id="A3.SS1.p3.1.1">i.e</em>.<span class="ltx_text" id="A3.SS1.p3.1.2"></span>, RealityCapture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib82" title="">82</a>]</cite>, Nerfacto <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib100" title="">100</a>]</cite>, VolSDF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib113" title="">113</a>]</cite>, MonoSDF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib116" title="">116</a>]</cite>, BakedSDF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib114" title="">114</a>]</cite>, and NeuralAngelo <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib59" title="">59</a>]</cite>.
This remains the case for the individual ARs and we also observe that the relative performances of the reconstruction methods are consistent across the different errors.
So one can expect that improving the 3D reconstructions would benefit all three pose estimation errors, hence various applications involving object pose estimation.</p>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS1.p4">
<p class="ltx_p" id="A3.SS1.p4.1">One interesting observation is the gap between the original CAD models and the best reconstruction methods is relatively small on the VSD compared to the other errors.
The VSD indicates that the object is overall well positioned so this suggests that 3D reconstructions can be suitable replacements for the CAD model when the pose estimator is used in tasks that are tolerant to the pose errors, <em class="ltx_emph ltx_font_italic" id="A3.SS1.p4.1.1">e.g</em>.<span class="ltx_text" id="A3.SS1.p4.1.2"></span>, visual navigation.
However, the gaps on the MSSD and MSPD remain large so there is room for improvement for 3D reconstructions to replace CAD models in tasks with low tolerance in the pose errors, <em class="ltx_emph ltx_font_italic" id="A3.SS1.p4.1.3">e.g</em>.<span class="ltx_text" id="A3.SS1.p4.1.4"></span>, object manipulation.</p>
</div>
</section>
<section class="ltx_subsection" id="A3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>Reconstruction with varying training size</h3>
<div class="ltx_para ltx_noindent" id="A3.SS2.p1">
<p class="ltx_p" id="A3.SS2.p1.1">Besides the quality of the pose estimation, an important property of the reconstruction methods is their data requirement.
To evaluate the ‘data-greediness’ of the reconstruction methods, we reconstruct the YCB-V <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib111" title="">111</a>]</cite> objects from subsets of images sampled uniformly around the object <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib33" title="">33</a>]</cite>.
Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.F8" title="Figure 8 ‣ C.2 Reconstruction with varying training size ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">8</span></a> complete the results in the main paper with the pose performance of FoundationPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib12" title="">12</a>]</cite> with reconstructions from image subsets and the 3D reconstruction’s geometric accuracy.</p>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS2.p2">
<p class="ltx_p" id="A3.SS2.p2.1">As for Megapose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib55" title="">55</a>]</cite>, the performance of FoundationPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib12" title="">12</a>]</cite> improves as more images are used for the 3D reconstruction (top-left).
Nerfacto <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib100" title="">100</a>]</cite> plateaus with only 75-100 images and the performance of RealityCapture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib82" title="">82</a>]</cite> remains relatively stable as the number of images decreases to 25.
This further demonstrates the practical advantage of RealityCapture that is both fast and data-efficient over other reconstruction methods: the reconstruction takes less than 1 min. on 25 images and <math alttext="\sim" class="ltx_Math" display="inline" id="A3.SS2.p2.1.m1.1"><semantics id="A3.SS2.p2.1.m1.1a"><mo id="A3.SS2.p2.1.m1.1.1" xref="A3.SS2.p2.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="A3.SS2.p2.1.m1.1b"><csymbol cd="latexml" id="A3.SS2.p2.1.m1.1.1.cmml" xref="A3.SS2.p2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A3.SS2.p2.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="A3.SS2.p2.1.m1.1d">∼</annotation></semantics></math>2 min. on 50 images.
Still, there remains a gap between the 3D reconstructions and the original CAD models for pose estimation that calls for further improvements in 3D reconstruction.</p>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS2.p3">
<p class="ltx_p" id="A3.SS2.p3.1">We observe that FoundationPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib12" title="">12</a>]</cite> seems more resilient to innacurate 3D reconstructions than Megapose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib55" title="">55</a>]</cite>.
For example, the geometric quality of Nerfacto <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib100" title="">100</a>]</cite> drops slightly at 150 images which is reflected in the Megapose performance but not in the FoundationPose ones.
A similar observation holds for BakedSDF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib114" title="">114</a>]</cite> at 100 and 150 images.</p>
</div>
<figure class="ltx_figure" id="A3.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="552" id="A3.F8.g1" src="x12.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>
<span class="ltx_text ltx_font_bold" id="A3.F8.7.1">Pose Evaluation of the 3D reconstructions from subsets of images.</span>
<span class="ltx_text ltx_font_bold" id="A3.F8.8.2">Top:</span> We measure the performance of FoundationPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib12" title="">12</a>]</cite> and Megapose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib55" title="">55</a>]</cite> when replacing the CAD models with the 3D reconstructions generated from image sets of varying size.
The blue lines indicate the performance of the pose estimators when using the original CAD models (FoundationPose: <span class="ltx_text ltx_font_bold" id="A3.F8.9.3" style="color:#0080FF;">full line</span>, Megapose: <span class="ltx_text ltx_font_bold" id="A3.F8.10.4" style="color:#0080FF;">dashed line</span>).
As expected, the more data the better the results and 75-100 images are enough to get reasonable reconstructions.
The performance of Reality Capture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib82" title="">82</a>]</cite> is relatively stable as the number of images decreases: <span class="ltx_text ltx_font_bold" id="A3.F8.11.5">the reconstruction from 50 images takes as little as 2 minutes</span> and is already close to the best RealityCapture performance.
<span class="ltx_text ltx_font_bold" id="A3.F8.12.6">Bottom:</span> Evaluation of the reconstructed geometry with the traditional completeness and accuracy metrics.
</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.3 </span>Object Categories</h3>
<div class="ltx_para ltx_noindent" id="A3.SS3.p1">
<p class="ltx_p" id="A3.SS3.p1.1">We split the YCB-V <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib111" title="">111</a>]</cite> objects into 8 groups defined in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A2.SS4" title="B.4 Object Categorization ‣ Appendix B Evaluation Setup ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">B.4</span></a> based on their shape, texture, material properties, and the degree of change between the collected objects and the original ones.
We report results for all object categories in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.F9" title="Figure 9 ‣ C.3 Object Categories ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">9</span></a> (including the 3 reported in the main paper) and observe that the relative pose performance across object categories is telling of some of the current challenges in 3D reconstruction.</p>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS3.p2">
<p class="ltx_p" id="A3.SS3.p2.1">The variation in pose performance is relatively large between the most simple categories and the most challenging ones.
Simple object categories include large objects with Lambertian texture and objects with little or uniform texture.
For these objects, replacing the CAD model with the 3D reconstructions has little or no impact on the performances.
We also note that some of the reconstruction methods that perform relatively lower than others, <em class="ltx_emph ltx_font_italic" id="A3.SS3.p2.1.1">e.g</em>.<span class="ltx_text" id="A3.SS3.p2.1.2"></span> Neus <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib109" title="">109</a>]</cite>, NeuralAngelo <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib59" title="">59</a>]</cite>, achieve very good results on some of the simple object categories.
Another interesting result is the high performance of the MVS-based RealityCapture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib82" title="">82</a>]</cite> on objects with little texture: one could have expected lower results since MVS relies on feature matching which accuracy usually drops when the image content exhibits little texture.</p>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS3.p3">
<p class="ltx_p" id="A3.SS3.p3.1">As mentioned in the main paper, the most challenging object categories are small textured objects and objects with a ‘shiny’ texture, <em class="ltx_emph ltx_font_italic" id="A3.SS3.p3.1.1">i.e</em>.<span class="ltx_text" id="A3.SS3.p3.1.2"></span>, a texture with high reflectance.
For these categories, the performance bottleneck often lies in the texture.
The resolution of small reconstructed objects is relatively lower than for larger objects and that loss of information can impede the pose estimation.
As for shiny objects, the reflection of light sources on the object during the data collection can cause visual discrepancies between the reconstruction and the object in the test images.</p>
</div>
<figure class="ltx_figure" id="A3.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="631" id="A3.F9.g1" src="x13.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>
<span class="ltx_text ltx_font_bold" id="A3.F9.4.1">Pose Evaluation of the 3D reconstructions for various Object Categories.</span>
We measure the performance of FoundationPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib12" title="">12</a>]</cite> (uniform bar) and Megapose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib55" title="">55</a>]</cite> (lined bar) when replacing the CAD models with the 3D reconstructions.
The blue lines indicate the performance of the pose estimators when using the original CAD models (FoundationPose: <span class="ltx_text ltx_font_bold" id="A3.F9.5.2" style="color:#0080FF;">full line</span>, Megapose: <span class="ltx_text ltx_font_bold" id="A3.F9.6.3" style="color:#0080FF;">dashed line</span>).
Some categories we expected to be challenging, such as objects with uniform texture are not.
Instead, the most difficult categories are small and shiny objects.
</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="A3.SS3.p4">
<p class="ltx_p" id="A3.SS3.p4.1">Since the YCB-V dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib111" title="">111</a>]</cite> is made of objects commonly found in grocery stores, whenever a brand updates its packaging, the texture gets updated.
The YCB-V objects we captured hence fall into two categories: the ‘legacy’ objects which texture did not change and the ‘updated’ objects which texture has changed.
Only the coffee box ‘01-master-chef-can’, the ’11-pitcher-base, and the ’15-power-drill’ have undergone a full texture update and the other objects have undergone relatively minor updates.
Still, the gap between the 3D reconstructions and the CAD models is lower on the ‘legacy’ objects than the ‘updated ones’.
BakedSDF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib114" title="">114</a>]</cite> is even on par with the original CAD models with Megapose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib55" title="">55</a>]</cite>.
There still remains a gap between 3D reconstructions and CAD models though.
Note that the integration of a given 3D reconstruction with FoundationPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib12" title="">12</a>]</cite> often outperforms or is on par with Megapose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib55" title="">55</a>]</cite> running with the original CAD model: this suggests that the gap between CAD models and 3D reconstructions could be closed not only by improving 3D reconstructions but also pose estimators.</p>
</div>
</section>
<section class="ltx_subsection" id="A3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.4 </span>Influence of the Texturing Algorithm</h3>
<figure class="ltx_figure" id="A3.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="661" id="A3.F10.g1" src="x14.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>
<span class="ltx_text ltx_font_bold" id="A3.F10.4.1">Pose Evaluation of the 3D reconstructions under different texturings.</span>
We measure the performance of Megapose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib55" title="">55</a>]</cite> when replacing the CAD models with 3D reconstructions.
The reconstructions are compared to the original CAD models with texture <span class="ltx_text ltx_font_bold" id="A3.F10.5.2" style="color:#0080FF;">(dashed line)</span> and without texture <span class="ltx_text ltx_font_bold" id="A3.F10.6.3" style="color:#0080FF;">(dotted line)</span>.
The performances of the textureless CAD models and the textureless 3D reconstructions are on par, which suggests that the 3D reconstructions have geometry suitable for pose estimation.
</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="A3.SS4.p1">
<p class="ltx_p" id="A3.SS4.p1.1">Previously, we observed that the performance of the pose estimation drops on small objects or shiny objects and we argued that the performance bottleneck lies in the texturing.
So we now analyze the influence of the texture on pose estimation (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.F10" title="Figure 10 ‣ C.4 Influence of the Texturing Algorithm ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">10</span></a>).
In the first experiment, we compare the <span class="ltx_text ltx_font_bold" id="A3.SS4.p1.1.1">pose performance of the 3D reconstructions with and without texture</span>.
A second experiment assesses the influence of the texturing algorithm by <span class="ltx_text ltx_font_bold" id="A3.SS4.p1.1.2">comparing the 3D reconstruction with native texturing and MVS-texturing</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib106" title="">106</a>]</cite>.
These reconstructions are compared to the original CAD models with texture <span class="ltx_text ltx_font_bold" id="A3.SS4.p1.1.3" style="color:#0080FF;">(dashed blue line)</span> and without texture <span class="ltx_text ltx_font_bold" id="A3.SS4.p1.1.4" style="color:#0080FF;">(dotted blue line)</span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS4.p2">
<p class="ltx_p" id="A3.SS4.p2.1">We run this ablation on Megapose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib55" title="">55</a>]</cite> with a subset of methods:
Nerfacto <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib100" title="">100</a>]</cite>, VolSDF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib113" title="">113</a>]</cite>, MonoSDF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib116" title="">116</a>]</cite> and BakedSDF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib114" title="">114</a>]</cite>, the colored but textureless reconstructions from COLMAP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib87" title="">87</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib88" title="">88</a>]</cite>, Neus <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib109" title="">109</a>]</cite> and UniSURF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib75" title="">75</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS4.p3">
<p class="ltx_p" id="A3.SS4.p3.1">We compare the reconstructions under three texturings: the texturing <span class="ltx_text ltx_font_bold" id="A3.SS4.p3.1.1">native</span> to each reconstruction method, <span class="ltx_text ltx_font_bold" id="A3.SS4.p3.1.2">no texturing</span> at all, and texturing with the <span class="ltx_text ltx_font_bold" id="A3.SS4.p3.1.3">MVS-texturing</span> algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib106" title="">106</a>]</cite>.
Note that COLMAP does not texture the models but only provides vertex colors so for COLMAP we compare the mesh colored with the point cloud colors, the colorless mesh, and the mesh textured with MVS-texturing.
Native texturing refers to the texturing algorithm as implemented in NerfStudio <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib100" title="">100</a>]</cite> and SDFStudio <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib115" title="">115</a>]</cite>: the UV map is generated by querying the network for the color at the position of the mesh’s faces.
The textureless mesh has exactly the same geometry as the textured one but without the texture map and with no color.
Finally, the MVS-textured mesh is the output of the MVS-texturing algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib106" title="">106</a>]</cite> run on the geometry of the reconstructed mesh.</p>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS4.p4">
<p class="ltx_p" id="A3.SS4.p4.1"><span class="ltx_text ltx_font_bold" id="A3.SS4.p4.1.1">With vs. Without Texture (circles). </span>
For all object categories, the gap between the textureless CAD model (dotted line) and the textureless reconstruction (circle bars) is relatively small or non-existent.
This is the case even for the object categories that are the most challenging under the regular pose evaluation (small and shiny), which supports the assumption that the performance bottleneck for these categories is the texture.
This suggests that the geometric quality of the 3D reconstructions and the CAD models are comparable as measured by pose estimation performance.
This is in line with the results obtained with traditional geometric evaluation of the 3D reconstructions measured with the completeness and accuracy metrics.</p>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS4.p5">
<p class="ltx_p" id="A3.SS4.p5.1">When the texture is removed, whether from the CAD models or the 3D reconstructions, the pose performance drops less for objects with low or uniform texture than for textured objects.
A reasonable explanation is that the pose estimation relies more on the geometry than on the texture for objects with little texture information.
Hence, the absence of texture or even incorrect texturing may not be prohibitive for pose estimation on such objects as long as the reconstructed geometry is good enough.</p>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS4.p6">
<p class="ltx_p" id="A3.SS4.p6.1"><span class="ltx_text ltx_font_bold" id="A3.SS4.p6.1.1">Native vs. MVS-Texturing. </span>
The texturing native to each method (uniform bar) leads to either comparable or better results than MVS-texturing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib106" title="">106</a>]</cite> (crossed bar).
Even the textureless but colored meshes of COLMAP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib87" title="">87</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib88" title="">88</a>]</cite> leads to better pose performance than with the MVS-texturing.
This may appear counter-intuitive since MVS-Texturing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib106" title="">106</a>]</cite> produces sharper and more visually appealing textures (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.F11" title="Figure 11 ‣ C.4 Influence of the Texturing Algorithm ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">11</span></a>-right).
However, render-and-compare methods usually operate on low-resolution renderings so that they can be fed efficiently to the network, so the lower resolution of the native texturing is not penalized.
Also, MVS-Texturing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib106" title="">106</a>]</cite> can generate strong color artifacts, such as color saturation or color mismatch that decrease the texture’s quality (see the green tint as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.F11" title="Figure 11 ‣ C.4 Influence of the Texturing Algorithm ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">11</span></a>-right).
That could be due to multiple reasons:
noise in the camera pose which leads to blurred texture (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.F11" title="Figure 11 ‣ C.4 Influence of the Texturing Algorithm ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">11</span></a>-left), the surface extracted from SDFs may not been extracted with the optimum isosurface value, or MVS-Texturing can not handle well reflections.</p>
</div>
<figure class="ltx_figure" id="A3.F11">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" height="168" id="A3.F11.g1" src="extracted/5794104/repetative_text.png" width="107"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="186" id="A3.F11.g2" src="extracted/5794104/texrecon_artifact.png" width="443"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Examples of texturing artifacts generated with MVS-texturing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib106" title="">106</a>]</cite>.
<span class="ltx_text ltx_font_bold" id="A3.F11.5.1">Left</span>: Illustration of MVS-Texturing mirage artifact.
The text in the 03-sugar-box is distorted and repeated. We also observe color saturation on the blue separation line between the top of the box and the bottom.
<span class="ltx_text ltx_font_bold" id="A3.F11.6.2">Right</span>: Illustration of the MVS-Texturing color artifacts. The top
of the 02-cracker-box on the left (COLMAP + MVS-Texturing) exhibit green areas that do not exist in the original model and are not produced by native texturing
of other methods (<em class="ltx_emph ltx_font_italic" id="A3.F11.7.3">e.g</em>.<span class="ltx_text" id="A3.F11.8.4"></span> Nerfacto on the right).</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.5 </span>Per-Object Pose Evaluation</h3>
<div class="ltx_para ltx_noindent" id="A3.SS5.p1">
<p class="ltx_p" id="A3.SS5.p1.1">We report the pose performances separately for each YCB-V object <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib111" title="">111</a>]</cite>, 3D reconstruction, and pose estimators in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.F12" title="Figure 12 ‣ C.5 Per-Object Pose Evaluation ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">12</span></a> and Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.F13" title="Figure 13 ‣ C.5 Per-Object Pose Evaluation ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">13</span></a>.
The results are consistent with the previous conclusions that the most challenging objects are the small ones or shiny ones, <em class="ltx_emph ltx_font_italic" id="A3.SS5.p1.1.1">e.g</em>.<span class="ltx_text" id="A3.SS5.p1.1.2"></span>, 06-tuna-fish-can or the 18-large-marker.</p>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS5.p2">
<p class="ltx_p" id="A3.SS5.p2.1">As observed previously, the combination of FoundationPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib12" title="">12</a>]</cite> and the 3D reconstructions is often better than Megapose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib55" title="">55</a>]</cite> with the original CAD models, <em class="ltx_emph ltx_font_italic" id="A3.SS5.p2.1.1">e.g</em>.<span class="ltx_text" id="A3.SS5.p2.1.2"></span>, 02-cracker-box, 10-banana.
This suggests that bridging the gap between 3D reconstructions and CAD models for pose estimations has two directions for improvements: improving the 3D reconstruction itself and improving pose estimators.</p>
</div>
<figure class="ltx_figure" id="A3.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="969" id="A3.F12.g1" src="x15.png" width="605"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span><span class="ltx_text ltx_font_bold" id="A3.F12.8.1">Pose Evaluation of the 3D reconstructions for each YCB-V object <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib111" title="">111</a>]</cite> (1/2).</span>
The 3D reconstructions are evaluated based on the performance of two pose estimators, <span class="ltx_text ltx_font_bold" id="A3.F12.9.2">FoundationPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib12" title="">12</a>]</cite> (uniform bar)</span> and <span class="ltx_text ltx_font_bold" id="A3.F12.10.3">Megapose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib55" title="">55</a>]</cite> (lines bar)</span>, when the CAD model is replaced with 3D reconstructions.
The blue vertical lines indicate the performance of the pose estimators when using the original CAD models (FoundationPose: <span class="ltx_text ltx_font_bold" id="A3.F12.11.4" style="color:#0080FF;">full line</span>, Megapose: <span class="ltx_text ltx_font_bold" id="A3.F12.12.5" style="color:#0080FF;">dashed line</span>).
The objects for which it is the most challenging to replace the CAD model are the small and / or shiny objects, <em class="ltx_emph ltx_font_italic" id="A3.F12.13.6">e.g</em>.<span class="ltx_text" id="A3.F12.14.7"></span>, 06-tuna-fish-can, 07-pudding-box.
For a given 3D reconstruction, the use of a high-performance pose estimator can compensate for some of the limitations of the 3D reconstruction: for example, FoundationPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib12" title="">12</a>]</cite> induces a strong boost on 07-pudding-box compared to Megapose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib55" title="">55</a>]</cite>.
</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="895" id="A3.F13.g1" src="x16.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span><span class="ltx_text ltx_font_bold" id="A3.F13.6.1">Pose Evaluation of the 3D reconstructions for each YCB-V object <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib111" title="">111</a>]</cite> (1/2).</span>
The 3D reconstructions are evaluated based on the performance of two pose estimators, <span class="ltx_text ltx_font_bold" id="A3.F13.7.2">FoundationPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib12" title="">12</a>]</cite> (uniform bar)</span> and <span class="ltx_text ltx_font_bold" id="A3.F13.8.3">Megapose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib55" title="">55</a>]</cite> (lines bar)</span>, when the CAD model is replaced with 3D reconstructions.
The blue vertical lines indicate the performance of the pose estimators when using the original CAD models (FoundationPose: <span class="ltx_text ltx_font_bold" id="A3.F13.9.4" style="color:#0080FF;">full line</span>, Megapose: <span class="ltx_text ltx_font_bold" id="A3.F13.10.5" style="color:#0080FF;">dashed line</span>).
</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A3.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.6 </span>Qualitative Results</h3>
<div class="ltx_para ltx_noindent" id="A3.SS6.p1">
<p class="ltx_p" id="A3.SS6.p1.1">The SSIM, PSNR, and LPIPS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#bib.bib117" title="">117</a>]</cite> metrics are evaluated by comparing the rendering of the CAD model and the masked YCB-V images, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.F14" title="Figure 14 ‣ C.6 Qualitative Results ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">14</span></a>. We report the average values across all test images.</p>
</div>
<figure class="ltx_figure" id="A3.F14"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="698" id="A3.F14.g1" src="extracted/5794104/psnr_ssim_lpips_evaluation.jpg" width="558"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Methodology for evaluating SSIM, PSNR, and LPIPS metrics. On the left is the test YCB-V image, in the middle is the masked YCB-V object, and on the right is the rendering of the reconstructed mesh overlay on the masked image.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="A3.SS6.p2">
<p class="ltx_p" id="A3.SS6.p2.1">The 3D reconstructions are displayed in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.F15" title="Figure 15 ‣ C.6 Qualitative Results ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">15</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.F16" title="Figure 16 ‣ C.6 Qualitative Results ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">16</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.F17" title="Figure 17 ‣ C.6 Qualitative Results ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">17</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.F18" title="Figure 18 ‣ C.6 Qualitative Results ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">18</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.F19" title="Figure 19 ‣ C.6 Qualitative Results ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">19</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.F20" title="Figure 20 ‣ C.6 Qualitative Results ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">20</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.F21" title="Figure 21 ‣ C.6 Qualitative Results ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">21</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.F22" title="Figure 22 ‣ C.6 Qualitative Results ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">22</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.F23" title="Figure 23 ‣ C.6 Qualitative Results ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">23</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.F24" title="Figure 24 ‣ C.6 Qualitative Results ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">24</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.F25" title="Figure 25 ‣ C.6 Qualitative Results ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">25</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.F26" title="Figure 26 ‣ C.6 Qualitative Results ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">26</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.F27" title="Figure 27 ‣ C.6 Qualitative Results ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">27</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.F28" title="Figure 28 ‣ C.6 Qualitative Results ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">28</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.F29" title="Figure 29 ‣ C.6 Qualitative Results ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">29</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.F30" title="Figure 30 ‣ C.6 Qualitative Results ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">30</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.F31" title="Figure 31 ‣ C.6 Qualitative Results ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">31</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.F32" title="Figure 32 ‣ C.6 Qualitative Results ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">32</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.F33" title="Figure 33 ‣ C.6 Qualitative Results ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">33</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.F34" title="Figure 34 ‣ C.6 Qualitative Results ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">34</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08234v1#A3.F35" title="Figure 35 ‣ C.6 Qualitative Results ‣ Appendix C Additional Results ‣ Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation"><span class="ltx_text ltx_ref_tag">35</span></a>. Each figure showcases the object’s reconstructions, the pose performances and the texture quality for various reconstruction methods.</p>
</div>
<figure class="ltx_figure" id="A3.F15"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="673" id="A3.F15.g1" src="extracted/5794104/01_master_chef_can.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>Pose performance and texture scores (left) and object renderings (right) of 01-master-chef-can reconstructed with various reconstructed methods.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F16"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="673" id="A3.F16.g1" src="extracted/5794104/02_cracker_box.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>Pose performance and texture scores (left) and object renderings (right) of 02-cracker-box reconstructed with various reconstructed methods.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F17"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="673" id="A3.F17.g1" src="extracted/5794104/03_sugar_box.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 17: </span>Pose performance and texture scores (left) and object renderings (right) of 03-sugar-box reconstructed with various reconstructed methods.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F18"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="673" id="A3.F18.g1" src="extracted/5794104/04_tomatoe_soup_can.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 18: </span>Pose performance and texture scores (left) and object renderings (right) of 04-tomatoe-soup-can reconstructed with various reconstructed methods.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F19"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="673" id="A3.F19.g1" src="extracted/5794104/05_mustard_bottle.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 19: </span>Pose performance and texture scores (left) and object renderings (right) of 05-mustard-bottle reconstructed with various reconstructed methods.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F20"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="673" id="A3.F20.g1" src="extracted/5794104/06_tuna_fish_can.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 20: </span>Pose performance and texture scores (left) and object renderings (right) of 06-tuna-fish-can reconstructed with various reconstructed methods.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F21"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="673" id="A3.F21.g1" src="extracted/5794104/07_pudding_box.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 21: </span>Pose performance and texture scores (left) and object renderings (right) of 07-pudding-box reconstructed with various reconstructed methods.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F22"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="673" id="A3.F22.g1" src="extracted/5794104/08_gelatin_box.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 22: </span>Pose performance and texture scores (left) and object renderings (right) of 08-gelatin-box reconstructed with various reconstructed methods.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F23"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="673" id="A3.F23.g1" src="extracted/5794104/09_potted_meat_can.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 23: </span>Pose performance and texture scores (left) and object renderings (right) of 09-potted-meat-can reconstructed with various reconstructed methods.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F24"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="673" id="A3.F24.g1" src="extracted/5794104/10_banana.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 24: </span>Pose performance and texture scores (left) and object renderings (right) of 10-banana reconstructed with various reconstructed methods.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F25"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="673" id="A3.F25.g1" src="extracted/5794104/11_pitcher_base.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 25: </span>Pose performance and texture scores (left) and object renderings (right) of 11-pitcher-base reconstructed with various reconstructed methods.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F26"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="673" id="A3.F26.g1" src="extracted/5794104/12_bleach_cleanser.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 26: </span>Pose performance and texture scores (left) and object renderings (right) of 12-bleach-cleanser reconstructed with various reconstructed methods.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F27"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="673" id="A3.F27.g1" src="extracted/5794104/13_bowl.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 27: </span>Pose performance and texture scores (left) and object renderings (right) of 13-bowl reconstructed with various reconstructed methods.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F28"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="673" id="A3.F28.g1" src="extracted/5794104/14_mug.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 28: </span>Pose performance and texture scores (left) and object renderings (right) of 14-mug reconstructed with various reconstructed methods.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F29"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="673" id="A3.F29.g1" src="extracted/5794104/15_power_drill.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 29: </span>Pose performance and texture scores (left) and object renderings (right) of 15-power-drill reconstructed with various reconstructed methods.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F30"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="673" id="A3.F30.g1" src="extracted/5794104/16_wood_block.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 30: </span>Pose performance and texture scores (left) and object renderings (right) of 16-wood-block reconstructed with various reconstructed methods.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F31"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="673" id="A3.F31.g1" src="extracted/5794104/17_scissors.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 31: </span>Pose performance and texture scores (left) and object renderings (right) of 17-scissors reconstructed with various reconstructed methods.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F32"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="673" id="A3.F32.g1" src="extracted/5794104/18_large_marker.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 32: </span>Pose performance and texture scores (left) and object renderings (right) of 18-large-marker reconstructed with various reconstructed methods.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F33"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="673" id="A3.F33.g1" src="extracted/5794104/19_large_clamp.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 33: </span>Pose performance and texture scores (left) and object renderings (right) of 19-large-clamp reconstructed with various reconstructed methods.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F34"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="673" id="A3.F34.g1" src="extracted/5794104/20_extra_large_clamp.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 34: </span>Pose performance and texture scores (left) and object renderings (right) of 20-extra-large-clamp reconstructed with various reconstructed methods.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F35"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="673" id="A3.F35.g1" src="extracted/5794104/21_foam_brick.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 35: </span>Pose performance and texture scores (left) and object renderings (right) of 21-foam-brick reconstructed with various reconstructed methods.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Basler ace camera aca2440-20gc.

</span>
<span class="ltx_bibblock">https://docs.baslerweb.com/aca2440-20gc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">bop [a]</span>
<span class="ltx_bibblock">
Bop: Benchmark for 6d object pose estimation.

</span>
<span class="ltx_bibblock">https://bop.felk.cvut.cz/home/, a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">bop [b]</span>
<span class="ltx_bibblock">
Bop toolkit.

</span>
<span class="ltx_bibblock">https://github.com/thodan/bop_toolkit/, b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Moveit hand-eye calibration.

</span>
<span class="ltx_bibblock">https://github.com/ros-planning/moveit_calibrationc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Ycb benchmark.

</span>
<span class="ltx_bibblock">https://www.ycbbenchmarks.com/object-set/.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">lin [2024]</span>
<span class="ltx_bibblock">
Sam-6d: Segment anything model meets zero-shot 6d object pose estimation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahmadyan et al. [2021]</span>
<span class="ltx_bibblock">
Adel Ahmadyan, Liangkai Zhang, Artsiom Ablavatski, Jianing Wei, and Matthias Grundmann.

</span>
<span class="ltx_bibblock">Objectron: A large scale dataset of object-centric videos in the wild with pose annotations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pages 7822–7831, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arandjelović and Zisserman [2012]</span>
<span class="ltx_bibblock">
Relja Arandjelović and Andrew Zisserman.

</span>
<span class="ltx_bibblock">Three things everyone should know to improve object retrieval.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">2012 IEEE conference on computer vision and pattern recognition</em>, pages 2911–2918. IEEE, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aravecchia et al. [2024]</span>
<span class="ltx_bibblock">
Stéphanie Aravecchia, Marianne Clausel, and Cédric Pradalier.

</span>
<span class="ltx_bibblock">Comparing metrics for evaluating 3d map quality in natural environments.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Robotics and Autonomous Systems</em>, 173:104617, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Besl and McKay [1992]</span>
<span class="ltx_bibblock">
Paul J Besl and Neil D McKay.

</span>
<span class="ltx_bibblock">Method for registration of 3-d shapes.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Sensor fusion IV: control paradigms and data structures</em>, volume 1611, pages 586–606. Spie, 1992.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bleyer et al. [2011]</span>
<span class="ltx_bibblock">
Michael Bleyer, Christoph Rhemann, and Carsten Rother.

</span>
<span class="ltx_bibblock">Patchmatch stereo-stereo matching with slanted support windows.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Bmvc</em>, volume 11, pages 1–11, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bowen Wen [2024]</span>
<span class="ltx_bibblock">
Jan Kautz Stan Birchfield Bowen Wen, Wei Yang.

</span>
<span class="ltx_bibblock">FoundationPose: Unified 6d pose estimation and tracking of novel objects.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">CVPR</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brachmann et al. [2014]</span>
<span class="ltx_bibblock">
Eric Brachmann, Alexander Krull, Frank Michel, Stefan Gumhold, Jamie Shotton, and Carsten Rother.

</span>
<span class="ltx_bibblock">Learning 6d object pose estimation using 3d object coordinates.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part II 13</em>, pages 536–551. Springer, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brachmann et al. [2016]</span>
<span class="ltx_bibblock">
Eric Brachmann, Frank Michel, Alexander Krull, Michael Ying Yang, Stefan Gumhold, et al.

</span>
<span class="ltx_bibblock">Uncertainty-driven 6d pose estimation of objects and scenes from a single rgb image.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 3364–3372, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bradski et al. [2000]</span>
<span class="ltx_bibblock">
Gary Bradski, Adrian Kaehler, et al.

</span>
<span class="ltx_bibblock">Opencv.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Dr. Dobb’s journal of software tools</em>, 3(2), 2000.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai and Reid [2020]</span>
<span class="ltx_bibblock">
Ming Cai and Ian Reid.

</span>
<span class="ltx_bibblock">Reconstruct locally, localize globally: A model free method for object pose estimation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 3153–3163, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Calli et al. [2015]</span>
<span class="ltx_bibblock">
Berk Calli, Arjun Singh, Aaron Walsman, Siddhartha Srinivasa, Pieter Abbeel, and Aaron M Dollar.

</span>
<span class="ltx_bibblock">The ycb object and model set: Towards common benchmarks for manipulation research.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">2015 international conference on advanced robotics (ICAR)</em>, pages 510–517. IEEE, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2023]</span>
<span class="ltx_bibblock">
Hanzhi Chen, Fabian Manhardt, Nassir Navab, and Benjamin Busam.

</span>
<span class="ltx_bibblock">Texpose: Neural texture learning for self-supervised 6d object pose estimation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 4841–4852, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2020]</span>
<span class="ltx_bibblock">
Xu Chen, Zijian Dong, Jie Song, Andreas Geiger, and Otmar Hilliges.

</span>
<span class="ltx_bibblock">Category level object pose estimation via neural analysis-by-synthesis.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXVI 16</em>, pages 139–156. Springer, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chitta et al. [2012]</span>
<span class="ltx_bibblock">
Sachin Chitta, Ioan Sucan, and Steve Cousins.

</span>
<span class="ltx_bibblock">Moveit![ros topics].

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">IEEE Robotics &amp; Automation Magazine</em>, 19(1):18–19, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cignoni et al. [2008]</span>
<span class="ltx_bibblock">
Paolo Cignoni, Marco Callieri, Massimiliano Corsini, Matteo Dellepiane, Fabio Ganovelli, and Guido Ranzuglia.

</span>
<span class="ltx_bibblock">MeshLab: an Open-Source Mesh Processing Tool.

</span>
<span class="ltx_bibblock">In Vittorio Scarano, Rosario De Chiara, and Ugo Erra, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Eurographics Italian Chapter Conference</em>. The Eurographics Association, 2008.

</span>
<span class="ltx_bibblock">ISBN 978-3-905673-68-5.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.2312/LocalChapterEvents/ItalChap/ItalianChapConf2008/129-136</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Collet et al. [2009]</span>
<span class="ltx_bibblock">
Alvaro Collet, Dmitry Berenson, Siddhartha S Srinivasa, and Dave Ferguson.

</span>
<span class="ltx_bibblock">Object recognition and full pose registration from a single image for robotic manipulation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">2009 IEEE International Conference on Robotics and Automation</em>, pages 48–55. IEEE, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Curless and Levoy [1996]</span>
<span class="ltx_bibblock">
Brian Curless and Marc Levoy.

</span>
<span class="ltx_bibblock">A volumetric method for building complex models from range images.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 23rd annual conference on Computer graphics and interactive techniques</em>, pages 303–312, 1996.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Dawson-Haggerty et al.

</span>
<span class="ltx_bibblock">trimesh.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://trimesh.org/" title="">https://trimesh.org/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Doumanoglou et al. [2016]</span>
<span class="ltx_bibblock">
Andreas Doumanoglou, Rigas Kouskouridas, Sotiris Malassiotis, and Tae-Kyun Kim.

</span>
<span class="ltx_bibblock">Recovering 6d object pose and predicting next-best-view in the crowd.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 3583–3592, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Downs et al. [2022]</span>
<span class="ltx_bibblock">
Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas B McHugh, and Vincent Vanhoucke.

</span>
<span class="ltx_bibblock">Google scanned objects: A high-quality dataset of 3d scanned household items.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">2022 International Conference on Robotics and Automation (ICRA)</em>, pages 2553–2560. IEEE, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Drost et al. [2010]</span>
<span class="ltx_bibblock">
Bertram Drost, Markus Ulrich, Nassir Navab, and Slobodan Ilic.

</span>
<span class="ltx_bibblock">Model globally, match locally: Efficient and robust 3d object recognition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">2010 IEEE computer society conference on computer vision and pattern recognition</em>, pages 998–1005. Ieee, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Drost et al. [2017]</span>
<span class="ltx_bibblock">
Bertram Drost, Markus Ulrich, Paul Bergmann, Philipp Hartinger, and Carsten Steger.

</span>
<span class="ltx_bibblock">Introducing mvtec itodd-a dataset for 3d object recognition in industry.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the IEEE international conference on computer vision workshops</em>, pages 2200–2208, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Faugeras and Keriven [1997]</span>
<span class="ltx_bibblock">
Olivier Faugeras and Renaud Keriven.

</span>
<span class="ltx_bibblock">Level set methods and the stereo problem.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Scale-Space Theory in Computer Vision: First International Conference, Scale-Space’97 Utrecht, The Netherlands, July 2–4, 1997 Proceedings 1</em>, pages 272–283. Springer, 1997.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fridovich-Keil et al. [2022]</span>
<span class="ltx_bibblock">
Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa.

</span>
<span class="ltx_bibblock">Plenoxels: Radiance fields without neural networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 5501–5510, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Furukawa and Ponce [2010]</span>
<span class="ltx_bibblock">
Y Furukawa and J Ponce.

</span>
<span class="ltx_bibblock">Accurate, dense, and robust multiview stereopsis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 32(8):1362–1376, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Furukawa and Ponce [2006]</span>
<span class="ltx_bibblock">
Yasutaka Furukawa and Jean Ponce.

</span>
<span class="ltx_bibblock">Carved visual hulls for image-based modeling.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Computer Vision–ECCV 2006: 9th European Conference on Computer Vision, Graz, Austria, May 7-13, 2006. Proceedings, Part I 9</em>, pages 564–577. Springer, 2006.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">González [2010]</span>
<span class="ltx_bibblock">
Álvaro González.

</span>
<span class="ltx_bibblock">Measurement of areas on a sphere using fibonacci and latitude–longitude lattices.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Mathematical Geosciences</em>, 42:49–64, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gropp et al. [2020]</span>
<span class="ltx_bibblock">
Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and Yaron Lipman.

</span>
<span class="ltx_bibblock">Implicit geometric regularization for learning shapes.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the 37th International Conference on Machine Learning</em>, pages 3789–3799, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haugaard and Buch [2022]</span>
<span class="ltx_bibblock">
Rasmus Laurvig Haugaard and Anders Glent Buch.

</span>
<span class="ltx_bibblock">Surfemb: Dense and continuous correspondence distributions for object pose estimation with learnt surface embeddings.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 6749–6758, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. [2022a]</span>
<span class="ltx_bibblock">
Xingyi He, Jiaming Sun, Yuang Wang, Di Huang, Hujun Bao, and Xiaowei Zhou.

</span>
<span class="ltx_bibblock">Onepose++: Keypoint-free one-shot object pose estimation without cad models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Advances in Neural Information Processing Systems</em>, 35:35103–35115, 2022a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. [2022b]</span>
<span class="ltx_bibblock">
Yisheng He, Yao Wang, Haoqiang Fan, Jian Sun, and Qifeng Chen.

</span>
<span class="ltx_bibblock">Fs6d: Few-shot 6d pose estimation of novel objects.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 6814–6824, 2022b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hennersperger et al. [2017]</span>
<span class="ltx_bibblock">
Christoph Hennersperger, Bernhard Fuerst, Salvatore Virga, Oliver Zettinig, Benjamin Frisch, Thomas Neff, and Nassir Navab.

</span>
<span class="ltx_bibblock">Towards mri-based autonomous robotic us acquisitions: a first feasibility study.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">IEEE transactions on medical imaging</em>, 36(2):538–548, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hinterstoisser et al. [2011a]</span>
<span class="ltx_bibblock">
Stefan Hinterstoisser, Cedric Cagniart, Slobodan Ilic, Peter Sturm, Nassir Navab, Pascal Fua, and Vincent Lepetit.

</span>
<span class="ltx_bibblock">Gradient response maps for real-time detection of textureless objects.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">IEEE transactions on pattern analysis and machine intelligence</em>, 34(5):876–888, 2011a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hinterstoisser et al. [2011b]</span>
<span class="ltx_bibblock">
Stefan Hinterstoisser, Stefan Holzer, Cedric Cagniart, Slobodan Ilic, Kurt Konolige, Nassir Navab, and Vincent Lepetit.

</span>
<span class="ltx_bibblock">Multimodal templates for real-time detection of texture-less objects in heavily cluttered scenes.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">2011 international conference on computer vision</em>, pages 858–865. IEEE, 2011b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hinterstoisser et al. [2013]</span>
<span class="ltx_bibblock">
Stefan Hinterstoisser, Vincent Lepetit, Slobodan Ilic, Stefan Holzer, Gary Bradski, Kurt Konolige, and Nassir Navab.

</span>
<span class="ltx_bibblock">Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Computer Vision–ACCV 2012: 11th Asian Conference on Computer Vision, Daejeon, Korea, November 5-9, 2012, Revised Selected Papers, Part I 11</em>, pages 548–562. Springer, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hinterstoisser et al. [2016]</span>
<span class="ltx_bibblock">
Stefan Hinterstoisser, Vincent Lepetit, Naresh Rajkumar, and Kurt Konolige.

</span>
<span class="ltx_bibblock">Going further with point pair features.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III 14</em>, pages 834–848. Springer, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hirschmuller and Scharstein [2007]</span>
<span class="ltx_bibblock">
Heiko Hirschmuller and Daniel Scharstein.

</span>
<span class="ltx_bibblock">Evaluation of cost functions for stereo matching.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">2007 IEEE conference on computer vision and pattern recognition</em>, pages 1–8. IEEE, 2007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hodan et al. [2017]</span>
<span class="ltx_bibblock">
Tomáš Hodan, Pavel Haluza, Štepán Obdržálek, Jiri Matas, Manolis Lourakis, and Xenophon Zabulis.

</span>
<span class="ltx_bibblock">T-less: An rgb-d dataset for 6d pose estimation of texture-less objects.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">2017 IEEE Winter Conference on Applications of Computer Vision (WACV)</em>, pages 880–888. IEEE, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hodan et al. [2018]</span>
<span class="ltx_bibblock">
Tomas Hodan, Frank Michel, Eric Brachmann, Wadim Kehl, Anders GlentBuch, Dirk Kraft, Bertram Drost, Joel Vidal, Stephan Ihrke, Xenophon Zabulis, et al.

</span>
<span class="ltx_bibblock">Bop: Benchmark for 6d object pose estimation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Proceedings of the European conference on computer vision (ECCV)</em>, pages 19–34, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hodan et al. [2020]</span>
<span class="ltx_bibblock">
Tomas Hodan, Daniel Barath, and Jiri Matas.

</span>
<span class="ltx_bibblock">Epos: Estimating 6d pose of objects with symmetries.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pages 11703–11712, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hodaň et al. [2020]</span>
<span class="ltx_bibblock">
Tomáš Hodaň, Martin Sundermeyer, Bertram Drost, Yann Labbé, Eric Brachmann, Frank Michel, Carsten Rother, and Jiří Matas.

</span>
<span class="ltx_bibblock">Bop challenge 2020 on 6d object localization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Computer Vision–ECCV 2020 Workshops: Glasgow, UK, August 23–28, 2020, Proceedings, Part II 16</em>, pages 577–594. Springer, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jensen et al. [2014]</span>
<span class="ltx_bibblock">
Rasmus Jensen, Anders Dahl, George Vogiatzis, Engin Tola, and Henrik Aanæs.

</span>
<span class="ltx_bibblock">Large scale multi-view stereopsis evaluation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 406–413, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kajiya and Von Herzen [1984]</span>
<span class="ltx_bibblock">
James T Kajiya and Brian P Von Herzen.

</span>
<span class="ltx_bibblock">Ray tracing volume densities.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">ACM SIGGRAPH computer graphics</em>, 18(3):165–174, 1984.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kanade et al. [1995]</span>
<span class="ltx_bibblock">
Takeo Kanade, Hiroshi Kano, Shigeru Kimura, Atsushi Yoshida, and Kazuo Oda.

</span>
<span class="ltx_bibblock">Development of a video-rate stereo machine.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Proceedings 1995 IEEE/RSJ International Conference on Intelligent Robots and Systems. Human Robot Interaction and Cooperative Robots</em>, volume 3, pages 95–100. IEEE, 1995.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaskman et al. [2019]</span>
<span class="ltx_bibblock">
Roman Kaskman, Sergey Zakharov, Ivan Shugurov, and Slobodan Ilic.

</span>
<span class="ltx_bibblock">Homebreweddb: Rgb-d dataset for 6d pose estimation of 3d objects.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</em>, pages 0–0, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kazhdan and Hoppe [2013]</span>
<span class="ltx_bibblock">
Michael Kazhdan and Hugues Hoppe.

</span>
<span class="ltx_bibblock">Screened poisson surface reconstruction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">ACM Transactions on Graphics (ToG)</em>, 32(3):1–13, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Knapitsch et al. [2017]</span>
<span class="ltx_bibblock">
Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun.

</span>
<span class="ltx_bibblock">Tanks and temples: Benchmarking large-scale scene reconstruction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">ACM Transactions on Graphics (ToG)</em>, 36(4):1–13, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kutulakos and Seitz [2000]</span>
<span class="ltx_bibblock">
Kiriakos N Kutulakos and Steven M Seitz.

</span>
<span class="ltx_bibblock">A theory of shape by space carving.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">International journal of computer vision</em>, 38:199–218, 2000.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Yann Labbé, Lucas Manuelli, Arsalan Mousavian, Stephen Tyree, Stan Birchfield, Jonathan Tremblay, Justin Carpentier, Mathieu Aubry, Dieter Fox, and Josef Sivic.

</span>
<span class="ltx_bibblock">MegaPose: 6D Pose Estimation of Novel Objects via Render &amp; Compare.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">CoRL</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Labbé et al. [2020]</span>
<span class="ltx_bibblock">
Yann Labbé, Justin Carpentier, Mathieu Aubry, and Josef Sivic.

</span>
<span class="ltx_bibblock">Cosypose: Consistent multi-view multi-object 6d pose estimation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XVII 16</em>, pages 574–591. Springer, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lhuillier and Quan [2005]</span>
<span class="ltx_bibblock">
Maxime Lhuillier and Long Quan.

</span>
<span class="ltx_bibblock">A quasi-dense approach to surface reconstruction from uncalibrated images.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">IEEE transactions on pattern analysis and machine intelligence</em>, 27(3):418–433, 2005.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2018]</span>
<span class="ltx_bibblock">
Yi Li, Gu Wang, Xiangyang Ji, Yu Xiang, and Dieter Fox.

</span>
<span class="ltx_bibblock">Deepim: Deep iterative matching for 6d pose estimation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Proceedings of the European Conference on Computer Vision (ECCV)</em>, pages 683–698, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023]</span>
<span class="ltx_bibblock">
Zhaoshuo Li, Thomas Müller, Alex Evans, Russell H Taylor, Mathias Unberath, Ming-Yu Liu, and Chen-Hsuan Lin.

</span>
<span class="ltx_bibblock">Neuralangelo: High-fidelity neural surface reconstruction.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 8456–8465, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2019]</span>
<span class="ltx_bibblock">
Zhigang Li, Gu Wang, and Xiangyang Ji.

</span>
<span class="ltx_bibblock">Cdpn: Coordinates-based disentangled pose network for real-time rgb-based 6-dof object pose estimation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pages 7678–7687, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2022]</span>
<span class="ltx_bibblock">
Yunzhi Lin, Jonathan Tremblay, Stephen Tyree, Patricio A Vela, and Stan Birchfield.

</span>
<span class="ltx_bibblock">Single-stage keypoint-based category-level object pose estimation from an rgb image.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">2022 International Conference on Robotics and Automation (ICRA)</em>, pages 1547–1553. IEEE, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2022]</span>
<span class="ltx_bibblock">
Yuan Liu, Yilin Wen, Sida Peng, Cheng Lin, Xiaoxiao Long, Taku Komura, and Wenping Wang.

</span>
<span class="ltx_bibblock">Gen6d: Generalizable model-free 6-dof object pose estimation from rgb images.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">European Conference on Computer Vision</em>, pages 298–315. Springer, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lombardi et al. [2019]</span>
<span class="ltx_bibblock">
Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, and Yaser Sheikh.

</span>
<span class="ltx_bibblock">Neural volumes: learning dynamic renderable volumes from images.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">ACM Transactions on Graphics (TOG)</em>, 38(4):1–14, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lorensen and Cline [1987]</span>
<span class="ltx_bibblock">
William E Lorensen and Harvey E Cline.

</span>
<span class="ltx_bibblock">Marching cubes: A high resolution 3d surface construction algorithm.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">ACM SIGGRAPH Computer Graphics</em>, 21(4):163–169, 1987.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lowe [2004]</span>
<span class="ltx_bibblock">
David G. Lowe.

</span>
<span class="ltx_bibblock">Distinctive Image Features from Scale-Invariant Keypoints.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">IJCV</em>, 2004.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Manuelli et al. [2019]</span>
<span class="ltx_bibblock">
Lucas Manuelli, Wei Gao, Peter Florence, and Russ Tedrake.

</span>
<span class="ltx_bibblock">kpam: Keypoint affordances for category-level robotic manipulation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">The International Symposium of Robotics Research</em>, pages 132–157. Springer, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Martinez et al. [2010]</span>
<span class="ltx_bibblock">
Manuel Martinez, Alvaro Collet, and Siddhartha S Srinivasa.

</span>
<span class="ltx_bibblock">Moped: A scalable and low latency object recognition and pose estimation system.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">2010 IEEE International Conference on Robotics and Automation</em>, pages 2043–2049. IEEE, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mescheder et al. [2019]</span>
<span class="ltx_bibblock">
Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger.

</span>
<span class="ltx_bibblock">Occupancy networks: Learning 3d reconstruction in function space.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pages 4460–4470, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mildenhall et al. [2020]</span>
<span class="ltx_bibblock">
B Mildenhall, PP Srinivasan, M Tancik, JT Barron, R Ramamoorthi, and R Ng.

</span>
<span class="ltx_bibblock">Nerf: Representing scenes as neural radiance fields for view synthesis.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">European conference on computer vision</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moon et al. [2024]</span>
<span class="ltx_bibblock">
Sungphill Moon, Hyeontae Son, Dongcheol Hur, and Sangwook Kim.

</span>
<span class="ltx_bibblock">Genflow: Generalizable recurrent flow for 6d pose refinement of novel objects.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Müller et al. [2022]</span>
<span class="ltx_bibblock">
Thomas Müller, Alex Evans, Christoph Schied, and Alexander Keller.

</span>
<span class="ltx_bibblock">Instant neural graphics primitives with a multiresolution hash encoding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">ACM Transactions on Graphics (ToG)</em>, 41(4):1–15, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Newcombe et al. [2011]</span>
<span class="ltx_bibblock">
Richard A Newcombe, Shahram Izadi, Otmar Hilliges, David Molyneaux, David Kim, Andrew J Davison, Pushmeet Kohi, Jamie Shotton, Steve Hodges, and Andrew Fitzgibbon.

</span>
<span class="ltx_bibblock">Kinectfusion: Real-time dense surface mapping and tracking.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">2011 10th IEEE international symposium on mixed and augmented reality</em>, pages 127–136. IEEE, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. [2024]</span>
<span class="ltx_bibblock">
Van Nguyen Nguyen, Thibault Groueix, Mathieu Salzmann, and Vincent Lepetit.

</span>
<span class="ltx_bibblock">Gigapose: Fast and robust novel object pose estimation via one correspondence.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Niemeyer et al. [2020]</span>
<span class="ltx_bibblock">
Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger.

</span>
<span class="ltx_bibblock">Differentiable volumetric rendering: Learning implicit 3d representations without 3d supervision.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 3504–3515, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oechsle et al. [2021]</span>
<span class="ltx_bibblock">
Michael Oechsle, Songyou Peng, and Andreas Geiger.

</span>
<span class="ltx_bibblock">Unisurf: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pages 5589–5599, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Örnek et al. [2024]</span>
<span class="ltx_bibblock">
Evin Pınar Örnek, Yann Labbé, Bugra Tekin, Lingni Ma, Cem Keskin, Christian Forster, and Tomas Hodan.

</span>
<span class="ltx_bibblock">Foundpose: Unseen object pose estimation with foundation features.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al. [2019]</span>
<span class="ltx_bibblock">
Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove.

</span>
<span class="ltx_bibblock">Deepsdf: Learning continuous signed distance functions for shape representation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pages 165–174, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al. [2020]</span>
<span class="ltx_bibblock">
Keunhong Park, Arsalan Mousavian, Yu Xiang, and Dieter Fox.

</span>
<span class="ltx_bibblock">Latentfusion: End-to-end differentiable reconstruction and rendering for unseen object pose estimation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pages 10710–10719, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rad and Lepetit [2017]</span>
<span class="ltx_bibblock">
Mahdi Rad and Vincent Lepetit.

</span>
<span class="ltx_bibblock">Bb8: A scalable, accurate, robust to partial occlusion method for predicting the 3d poses of challenging objects without using depth.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib79.1.1">Proceedings of the IEEE international conference on computer vision</em>, pages 3828–3836, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ratliff et al. [2009]</span>
<span class="ltx_bibblock">
Nathan Ratliff, Matt Zucker, J Andrew Bagnell, and Siddhartha Srinivasa.

</span>
<span class="ltx_bibblock">Chomp: Gradient optimization techniques for efficient motion planning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">2009 IEEE international conference on robotics and automation</em>, pages 489–494. IEEE, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ravi et al. [2020]</span>
<span class="ltx_bibblock">
Nikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor Gordon, Wan-Yen Lo, Justin Johnson, and Georgia Gkioxari.

</span>
<span class="ltx_bibblock">Accelerating 3d deep learning with pytorch3d.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">arXiv:2007.08501</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">RealityCapture2023 [2023]</span>
<span class="ltx_bibblock">
RealityCapture2023.

</span>
<span class="ltx_bibblock">RealityCapture, 4 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.capturingreality.com/" title="">https://www.capturingreality.com/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rennie et al. [2016]</span>
<span class="ltx_bibblock">
Colin Rennie, Rahul Shome, Kostas E Bekris, and Alberto F De Souza.

</span>
<span class="ltx_bibblock">A dataset for improved rgbd-based object detection and pose estimation for warehouse pick-and-place.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib83.1.1">IEEE Robotics and Automation Letters</em>, 1(2):1179–1185, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rothganger et al. [2006]</span>
<span class="ltx_bibblock">
Fred Rothganger, Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce.

</span>
<span class="ltx_bibblock">3d object modeling and recognition using local affine-invariant image descriptors and multi-view spatial constraints.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">International journal of computer vision</em>, 66:231–259, 2006.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scharstein and Szeliski [2002]</span>
<span class="ltx_bibblock">
Daniel Scharstein and Richard Szeliski.

</span>
<span class="ltx_bibblock">A taxonomy and evaluation of dense two-frame stereo correspondence algorithms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">International journal of computer vision</em>, 47:7–42, 2002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scharstein et al. [2014]</span>
<span class="ltx_bibblock">
Daniel Scharstein, Heiko Hirschmüller, York Kitajima, Greg Krathwohl, Nera Nešić, Xi Wang, and Porter Westling.

</span>
<span class="ltx_bibblock">High-resolution stereo datasets with subpixel-accurate ground truth.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib86.1.1">Pattern Recognition: 36th German Conference, GCPR 2014, Münster, Germany, September 2-5, 2014, Proceedings 36</em>, pages 31–42. Springer, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schonberger and Frahm [2016]</span>
<span class="ltx_bibblock">
Johannes L Schonberger and Jan-Michael Frahm.

</span>
<span class="ltx_bibblock">Structure-from-motion revisited.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib87.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 4104–4113, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schönberger et al. [2016]</span>
<span class="ltx_bibblock">
Johannes L Schönberger, Enliang Zheng, Jan-Michael Frahm, and Marc Pollefeys.

</span>
<span class="ltx_bibblock">Pixelwise view selection for unstructured multi-view stereo.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib88.1.1">Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III 14</em>, pages 501–518. Springer, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schops et al. [2017]</span>
<span class="ltx_bibblock">
Thomas Schops, Johannes L Schonberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, and Andreas Geiger.

</span>
<span class="ltx_bibblock">A multi-view stereo benchmark with high-resolution images and multi-camera videos.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib89.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 3260–3269, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schöps et al. [2019]</span>
<span class="ltx_bibblock">
Thomas Schöps, Torsten Sattler, and Marc Pollefeys.

</span>
<span class="ltx_bibblock">BAD SLAM: Bundle adjusted direct RGB-D SLAM.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib90.1.1">Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Seitz and Dyer [1999]</span>
<span class="ltx_bibblock">
Steven M Seitz and Charles R Dyer.

</span>
<span class="ltx_bibblock">Photorealistic scene reconstruction by voxel coloring.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib91.1.1">International Journal of Computer Vision</em>, 35:151–173, 1999.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Seitz et al. [2006]</span>
<span class="ltx_bibblock">
Steven M Seitz, Brian Curless, James Diebel, Daniel Scharstein, and Richard Szeliski.

</span>
<span class="ltx_bibblock">A comparison and evaluation of multi-view stereo reconstruction algorithms.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib92.1.1">2006 IEEE computer society conference on computer vision and pattern recognition (CVPR’06)</em>, volume 1, pages 519–528. IEEE, 2006.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shugurov et al. [2022]</span>
<span class="ltx_bibblock">
Ivan Shugurov, Fu Li, Benjamin Busam, and Slobodan Ilic.

</span>
<span class="ltx_bibblock">Osop: A multi-stage one shot object pose estimation framework.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib93.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 6835–6844, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sinha and Pollefeys [2004]</span>
<span class="ltx_bibblock">
Sudipta N Sinha and Marc Pollefeys.

</span>
<span class="ltx_bibblock">Visual-hull reconstruction from uncalibrated and unsynchronized video streams.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib94.1.1">Proceedings. 2nd International Symposium on 3D Data Processing, Visualization and Transmission, 2004. 3DPVT 2004.</em>, pages 349–356. IEEE, 2004.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sinha et al. [2007]</span>
<span class="ltx_bibblock">
Sudipta N Sinha, Philippos Mordohai, and Marc Pollefeys.

</span>
<span class="ltx_bibblock">Multi-view stereo via graph cuts on the dual of an adaptive tetrahedral mesh.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib95.1.1">2007 IEEE 11th international conference on computer vision</em>, pages 1–8. IEEE, 2007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sitzmann et al. [2019]</span>
<span class="ltx_bibblock">
Vincent Sitzmann, Michael Zollhöfer, and Gordon Wetzstein.

</span>
<span class="ltx_bibblock">Scene representation networks: Continuous 3d-structure-aware neural scene representations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib96.1.1">Advances in Neural Information Processing Systems</em>, 32, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Strecha et al. [2006]</span>
<span class="ltx_bibblock">
Christoph Strecha, Rik Fransens, and Luc Van Gool.

</span>
<span class="ltx_bibblock">Combined depth and outlier estimation in multi-view stereo.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib97.1.1">2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06)</em>, volume 2, pages 2394–2401. IEEE, 2006.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Strecha et al. [2008]</span>
<span class="ltx_bibblock">
Christoph Strecha, Wolfgang Von Hansen, Luc Van Gool, Pascal Fua, and Ulrich Thoennessen.

</span>
<span class="ltx_bibblock">On benchmarking camera calibration and multi-view stereo for high resolution imagery.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib98.1.1">2008 IEEE conference on computer vision and pattern recognition</em>, pages 1–8. Ieee, 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. [2022]</span>
<span class="ltx_bibblock">
Jiaming Sun, Zihao Wang, Siyu Zhang, Xingyi He, Hongcheng Zhao, Guofeng Zhang, and Xiaowei Zhou.

</span>
<span class="ltx_bibblock">Onepose: One-shot object pose estimation without cad models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib99.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 6825–6834, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tancik et al. [2023]</span>
<span class="ltx_bibblock">
Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, Brent Yi, Justin Kerr, Terrance Wang, Alexander Kristoffersen, Jake Austin, Kamyar Salahi, Abhik Ahuja, David McAllister, and Angjoo Kanazawa.

</span>
<span class="ltx_bibblock">Nerfstudio: A modular framework for neural radiance field development.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib100.1.1">ACM SIGGRAPH 2023 Conference Proceedings</em>, SIGGRAPH ’23, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tejani et al. [2014]</span>
<span class="ltx_bibblock">
Alykhan Tejani, Danhang Tang, Rigas Kouskouridas, and Tae-Kyun Kim.

</span>
<span class="ltx_bibblock">Latent-class hough forests for 3d object detection and pose estimation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib101.1.1">Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13</em>, pages 462–477. Springer, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tola et al. [2012]</span>
<span class="ltx_bibblock">
Engin Tola, Christoph Strecha, and Pascal Fua.

</span>
<span class="ltx_bibblock">Efficient large-scale multi-view stereo for ultra high-resolution image sets.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib102.1.1">Machine Vision and Applications</em>, 23:903–920, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tremblay et al. [2018]</span>
<span class="ltx_bibblock">
Jonathan Tremblay, Thang To, Balakumar Sundaralingam, Yu Xiang, Dieter Fox, and Stan Birchfield.

</span>
<span class="ltx_bibblock">Deep object pose estimation for semantic robotic grasping of household objects.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib103.1.1">Conference on Robot Learning</em>, pages 306–316. PMLR, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tyree et al. [2022]</span>
<span class="ltx_bibblock">
Stephen Tyree, Jonathan Tremblay, Thang To, Jia Cheng, Terry Mosier, Jeffrey Smith, and Stan Birchfield.

</span>
<span class="ltx_bibblock">6-dof pose estimation of household objects for robotic manipulation: An accessible dataset and benchmark.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib104.1.1">2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, pages 13081–13088. IEEE, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ulusoy et al. [2017]</span>
<span class="ltx_bibblock">
Ali Osman Ulusoy, Michael J Black, and Andreas Geiger.

</span>
<span class="ltx_bibblock">Semantic multi-view stereo: Jointly estimating objects and voxels.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib105.1.1">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pages 4531–4540. IEEE, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Waechter et al. [2014]</span>
<span class="ltx_bibblock">
Michael Waechter, Nils Moehrle, and Michael Goesele.

</span>
<span class="ltx_bibblock">Let there be color! — Large-scale texturing of 3D reconstructions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib106.1.1">Proceedings of the European Conference on Computer Vision</em>. Springer, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2021a]</span>
<span class="ltx_bibblock">
Gu Wang, Fabian Manhardt, Federico Tombari, and Xiangyang Ji.

</span>
<span class="ltx_bibblock">Gdr-net: Geometry-guided direct regression network for monocular 6d object pose estimation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib107.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 16611–16621, 2021a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2019]</span>
<span class="ltx_bibblock">
He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, and Leonidas J Guibas.

</span>
<span class="ltx_bibblock">Normalized object coordinate space for category-level 6d object pose and size estimation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib108.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 2642–2651, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2021b]</span>
<span class="ltx_bibblock">
Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang.

</span>
<span class="ltx_bibblock">Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib109.1.1">Advances in Neural Information Processing Systems</em>, 34:27171–27183, 2021b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wohlhart and Lepetit [2015]</span>
<span class="ltx_bibblock">
Paul Wohlhart and Vincent Lepetit.

</span>
<span class="ltx_bibblock">Learning descriptors for object recognition and 3d pose estimation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib110.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 3109–3118, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiang et al. [2018]</span>
<span class="ltx_bibblock">
Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, and Dieter Fox.

</span>
<span class="ltx_bibblock">Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes.

</span>
<span class="ltx_bibblock">2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yariv et al. [2020]</span>
<span class="ltx_bibblock">
Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman.

</span>
<span class="ltx_bibblock">Multiview neural surface reconstruction by disentangling geometry and appearance.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib112.1.1">Advances in Neural Information Processing Systems</em>, 33:2492–2502, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yariv et al. [2021]</span>
<span class="ltx_bibblock">
Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman.

</span>
<span class="ltx_bibblock">Volume rendering of neural implicit surfaces.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib113.1.1">Advances in Neural Information Processing Systems</em>, 34:4805–4815, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yariv et al. [2023]</span>
<span class="ltx_bibblock">
Lior Yariv, Peter Hedman, Christian Reiser, Dor Verbin, Pratul P Srinivasan, Richard Szeliski, Jonathan T Barron, and Ben Mildenhall.

</span>
<span class="ltx_bibblock">Bakedsdf: Meshing neural sdfs for real-time view synthesis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib114.1.1">ACM Transactions on Graphics</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. [2022a]</span>
<span class="ltx_bibblock">
Zehao Yu, Anpei Chen, Bozidar Antic, Songyou Peng, Apratim Bhattacharyya, Michael Niemeyer, Siyu Tang, Torsten Sattler, and Andreas Geiger.

</span>
<span class="ltx_bibblock">Sdfstudio: A unified framework for surface reconstruction, 2022a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/autonomousvision/sdfstudio" title="">https://github.com/autonomousvision/sdfstudio</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. [2022b]</span>
<span class="ltx_bibblock">
Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sattler, and Andreas Geiger.

</span>
<span class="ltx_bibblock">Monosdf: Exploring monocular geometric cues for neural implicit surface reconstruction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib116.1.1">Advances in neural information processing systems</em>, 35:25018–25032, 2022b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2018]</span>
<span class="ltx_bibblock">
Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang.

</span>
<span class="ltx_bibblock">The unreasonable effectiveness of deep features as a perceptual metric.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib117.1.1">CVPR</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. [2018]</span>
<span class="ltx_bibblock">
Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun.

</span>
<span class="ltx_bibblock">Open3D: A modern library for 3D data processing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib118.1.1">arXiv:1801.09847</em>, 2018.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Aug 15 15:54:57 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
