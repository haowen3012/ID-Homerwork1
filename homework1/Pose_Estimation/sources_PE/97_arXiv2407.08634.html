<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation</title>
<!--Generated on Thu Jul 11 16:03:24 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2407.08634v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#S1" title="In RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#S2" title="In RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#S2.SS0.SSS0.Px1" title="In 2 Related Work ‣ RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation"><span class="ltx_text ltx_ref_title">Top-down Approaches.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#S2.SS0.SSS0.Px2" title="In 2 Related Work ‣ RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation"><span class="ltx_text ltx_ref_title">Coordinate Classification.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#S2.SS0.SSS0.Px3" title="In 2 Related Work ‣ RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation"><span class="ltx_text ltx_ref_title">3D pose estimation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#S3" title="In RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Model Architecture and Training</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#S3.SS1" title="In 3 Model Architecture and Training ‣ RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>RTMW</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#S3.SS1.SSS1" title="In 3.1 RTMW ‣ 3 Model Architecture and Training ‣ RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Task Limitation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#S3.SS1.SSS2" title="In 3.1 RTMW ‣ 3 Model Architecture and Training ‣ RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>Model Architecture</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#S3.SS1.SSS2.Px1" title="In 3.1.2 Model Architecture ‣ 3.1 RTMW ‣ 3 Model Architecture and Training ‣ RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation"><span class="ltx_text ltx_ref_title">PAFPN</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#S3.SS1.SSS2.Px2" title="In 3.1.2 Model Architecture ‣ 3.1 RTMW ‣ 3 Model Architecture and Training ‣ RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation"><span class="ltx_text ltx_ref_title">HEM (Hierarchical Encoding Module)</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#S3.SS1.SSS3" title="In 3.1 RTMW ‣ 3 Model Architecture and Training ‣ RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.3 </span>Training Techniques</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#S3.SS2" title="In 3 Model Architecture and Training ‣ RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>RTMW3D</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#S3.SS2.SSS0.Px1" title="In 3.2 RTMW3D ‣ 3 Model Architecture and Training ‣ RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation"><span class="ltx_text ltx_ref_title">Task definition</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#S3.SS2.SSS0.Px2" title="In 3.2 RTMW3D ‣ 3 Model Architecture and Training ‣ RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation"><span class="ltx_text ltx_ref_title">Data process</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#S4" title="In RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#S4.SS1" title="In 4 Experiments ‣ RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#S4.SS2" title="In 4 Experiments ‣ RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#S4.SS2.SSS0.Px1" title="In 4.2 Results ‣ 4 Experiments ‣ RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation"><span class="ltx_text ltx_ref_title">COCO-Wholebody</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#S4.SS2.SSS0.Px2" title="In 4.2 Results ‣ 4 Experiments ‣ RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation"><span class="ltx_text ltx_ref_title">H3WB</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#S4.SS2.SSS0.Px3" title="In 4.2 Results ‣ 4 Experiments ‣ RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation"><span class="ltx_text ltx_ref_title">Inference speed</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#S4.SS3" title="In 4 Experiments ‣ RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Ablation study</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#S4.SS3.SSS1" title="In 4.3 Ablation study ‣ 4 Experiments ‣ RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.1 </span>Ablation on RTMW</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#S4.SS4" title="In 4 Experiments ‣ RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Visualization Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#S5" title="In RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tao Jiang<sup class="ltx_sup" id="id4.4.id1"><span class="ltx_text ltx_font_italic" id="id4.4.id1.1">∗</span></sup>    Xinchen Xie<sup class="ltx_sup" id="id5.5.id2"><span class="ltx_text ltx_font_italic" id="id5.5.id2.1">∗</span></sup>    Yining Li 
<br class="ltx_break"/>
Shanghai AI Laboratory 
<br class="ltx_break"/><math alttext="\left\{\text{jiangtao, xiexinchen, liyining}\right\}" class="ltx_Math" display="inline" id="id3.3.m3.1"><semantics id="id3.3.m3.1a"><mrow id="id3.3.m3.1.2.2" xref="id3.3.m3.1.2.1.cmml"><mo id="id3.3.m3.1.2.2.1" xref="id3.3.m3.1.2.1.cmml">{</mo><mtext class="ltx_mathvariant_monospace" id="id3.3.m3.1.1" mathsize="90%" xref="id3.3.m3.1.1a.cmml">jiangtao, xiexinchen, liyining</mtext><mo id="id3.3.m3.1.2.2.2" xref="id3.3.m3.1.2.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="id3.3.m3.1b"><set id="id3.3.m3.1.2.1.cmml" xref="id3.3.m3.1.2.2"><ci id="id3.3.m3.1.1a.cmml" xref="id3.3.m3.1.1"><mtext class="ltx_mathvariant_monospace" id="id3.3.m3.1.1.cmml" mathsize="90%" xref="id3.3.m3.1.1">jiangtao, xiexinchen, liyining</mtext></ci></set></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.1c">\left\{\text{jiangtao, xiexinchen, liyining}\right\}</annotation><annotation encoding="application/x-llamapun" id="id3.3.m3.1d">{ jiangtao, xiexinchen, liyining }</annotation></semantics></math><span class="ltx_text ltx_font_typewriter" id="id6.6.id3" style="font-size:90%;">@pjlab.org.cn</span>
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id7.id1">Whole-body pose estimation is a challenging task that requires simultaneous prediction of keypoints for the body, hands, face, and feet.
Whole-body pose estimation aims to predict fine-grained pose information for the human body, including the face, torso, hands, and feet, which plays an important role in the study of human-centric perception and generation and in various applications.
In this work, we present RTMW (Real-Time Multi-person Whole-body pose estimation models), a series of high-performance models for 2D/3D whole-body pose estimation.
We incorporate RTMPose model architecture with FPN and HEM (Hierarchical Encoding Module) to better capture pose information from different body parts with various scales. The model is trained with a rich collection of open-source human keypoint datasets with manually aligned annotations and further enhanced via a two-stage distillation strategy.
RTMW demonstrates strong performance on multiple whole-body pose estimation benchmarks while maintaining high inference efficiency and deployment friendliness. We release three sizes: m/l/x, with RTMW-l achieving a 70.2 mAP on the COCO-Wholebody benchmark, making it the first open-source model to exceed 70 mAP on this benchmark. Meanwhile, we explored the performance of RTMW in the task of 3D whole-body pose estimation, conducting image-based monocular 3D whole-body pose estimation in a coordinate classification manner.
We hope this work can benefit both academic research and industrial applications. The code and models have been made publicly available at: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/open-mmlab/mmpose/tree/main/projects/rtmpose" title="">https://github.com/open-mmlab/mmpose/tree/main/projects/rtmpose</a></p>
</div>
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">{NoHyper}</span><span class="ltx_note ltx_role_footnotetext" id="footnotex1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotetext: </span><sup class="ltx_sup" id="footnotex1.1"><span class="ltx_text ltx_font_italic" id="footnotex1.1.1">∗</span></sup> Equal Contribution.</span></span></span>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Whole-body pose estimation is an essential component in advancing the capabilities of human-centric artificial intelligence systems. It can be utilized in human-computer interaction, virtual avatar animation, and the film industry. In the context of AIGC (AI Generated Content) applications, the outcomes of whole-body pose estimation are also employed to control character generation. As the emergence of downstream tasks and industrial applications empowered by whole-body pose estimation, designing a model that is highly accurate, low-latency, and easy to deploy is extremely valuable.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In early research on human pose estimation, due to the complexity of the task and limitations in computational power and data, researchers divided the human body into separate parts for independent pose estimation studies. With the relentless efforts of predecessors, remarkable results have been achieved in these singular, part-specific 2D pose estimation tasks.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Previous work, such as OpenPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib3" title="">3</a>]</cite>, could obtain whole-body pose estimation results by combining the outcomes of these separate parts. However, this straightforward combination approach faced high computational costs and significant performance limitations. While lightweight tools like MediaPipe <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib22" title="">22</a>]</cite> offer high real-time performance and ease of deployment, their accuracy is not entirely satisfactory.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Our MMPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib5" title="">5</a>]</cite> team released the RTMPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib10" title="">10</a>]</cite> model last year, which achieved an excellent balance between accuracy and real-time performance. Subsequently, on this foundation, DWPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib40" title="">40</a>]</cite> further enhanced the performance of RTMPose in whole-body pose estimation tasks by incorporating a two-stage distillation technique and integrating a new dataset, UBody <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib17" title="">17</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">The structural design of RTMPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib10" title="">10</a>]</cite> initially only considered the body posture. However, in whole-body pose estimation tasks, feature resolution is crucial for accuracy in facial, hand, and foot pose estimation. Therefore, we introduced two techniques, PAFPN (Part-Aggregation Feature Pyramid Network) and HEM (High-Efficiency Multi-Scale Feature Fusion), to enhance feature resolution. Experimental results have confirmed that these two modules significantly improve the localization accuracy of fine-grained body parts.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">At the same time, the scarcity of open-source whole-body pose estimation datasets greatly limits the performance of open-source models. To fully use datasets focusing on different body parts, we manually aligned the key point definitions of 14 open-source datasets (3 for whole-body keypoints, 6 for body keypoints, 4 for facial keypoints, and 1 for hand keypoints), which are jointly used to train RTMW.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">In the realm of 3D whole-body pose estimation, the academic community has predominantly embraced two main methodologies: Lifting <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib42" title="">42</a>]</cite> and regression <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib32" title="">32</a>]</cite> approaches. There has been a notable absence of scholarly inquiry into methods grounded in the SimCC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib16" title="">16</a>]</cite> technique. Our research endeavors have ventured into uncharted territory by applying the RTMW architecture to the 3D whole-body pose estimation task. Our experimental findings indicate that the SimCC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib16" title="">16</a>]</cite> method holds its own and delivers commendable performance in this domain.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Top-down Approaches.</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">Top-down algorithms use off-the-shelf detectors to provide bounding boxes and crop the human to a uniform scale for pose estimation. Algorithms  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib39" title="">39</a>]</cite> of the top-down paradigm have dominated public benchmarks. The two-stage inference paradigm allows both the human detector and the pose estimator to use relatively small input resolutions, allowing them to outperform bottom-up algorithms in speed and accuracy in non-extreme scenarios (i.e. when the number of people in the image is no more than 6). Additionally, most previous work has focused on achieving state-of-the-art performance on public datasets. In contrast, our work aims to design models with better speed-accuracy trade-offs to meet the needs of industrial applications.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Coordinate Classification.</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">Previous pose estimation approaches usually regard keypoint localization as either coordinate regression (e.g.  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib25" title="">25</a>]</cite>) or heatmap regression (e.g.  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib39" title="">39</a>]</cite>). SimCC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib16" title="">16</a>]</cite> introduces a new scheme that formulates keypoint prediction as classification from sub-pixel bins for horizontal and vertical coordinates respectively, which brings about several advantages. First, SimCC is freed from the dependence on high-resolution heatmaps, thus allowing for a very compact architecture that requires neither high-resolution intermediate representations  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib31" title="">31</a>]</cite> nor costly upscaling layers  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib37" title="">37</a>]</cite>. Second, SimCC flattens the final feature map for classification instead of involving global pooling  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib33" title="">33</a>]</cite> and, therefore, avoids the loss of spatial information. Third, the quantization error can be effectively alleviated by coordinate classification at the sub-pixel scale without needing extra refinement post-processing  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib41" title="">41</a>]</cite>. These qualities make SimCC attractive for building lightweight pose estimation models. RTMO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib21" title="">21</a>]</cite> introduced the coordinate classification method into the one-stage pose estimation, which has achieved significant performance improvement and also confirmed the great potential of the SimCC method in pose estimation tasks. In this work, we further exploit the coordinate classification scheme with optimizations on model architecture and training strategy.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">3D pose estimation</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p1.1">3D pose estimation is a vibrant research domain with extensive industry applications. The landscape of contemporary methodologies is dominated by two primary approaches: lifting methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib42" title="">42</a>]</cite> that leverage 2D keypoints and regression methods grounded in image analysis. The lifting methods, which input 2D coordinates into a neural network to directly predict their spatial coordinates, are distinguished by their swift computational pace. However, this efficiency comes at the cost of scene information, as these algorithms are devoid of image input, leading to a reliance on the annotation of training data for the range of their predictive outcomes.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px3.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p2.1">Conversely, image-based regression methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib32" title="">32</a>]</cite>, while incorporating rich visual data, face the challenges of sluggish inference speeds and increased task complexity. These factors contribute to difficulty in training the models and achieving high accuracy. Our proposed method, RTMW3D, diverges from these conventional approaches by employing a classification strategy based on the Simcc technique <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib16" title="">16</a>]</cite> to refine the final spatial coordinates through post-processing. Our experimental findings underscore the effectiveness of this innovative approach.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Model Architecture and Training</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Although the RTMPose we proposed before did not have a special design for the whole-body keypoint estimation task, after experiments, we found that its performance is comparable to the current state-of-the-art method, ZoomNas <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib38" title="">38</a>]</cite>. However, during the experimental process, we found that RTMPose has a certain performance bottleneck in the whole-body pose estimation task. As the scale of parameters increases, the model performance does not improve with the increase in the number of parameters. On the other hand, with the research of other research teams, such as the DWPose team, although they added new datasets during the training of RTMPose and adopted more efficient two-stage distillation training technology, which effectively improved the accuracy of RTMPose, they still cannot avoid the problem of the inherent performance bottleneck of RTMPose as the number of parameters increases.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>RTMW</h3>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Task Limitation</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">We first analyze some of the unsolved problems in the whole-body pose estimation task. RTMW was designed to address these challenges. The first issue is the resolution limit of local areas. In an image, parts of the human body, such as the face, hands, and feet, occupy a very small proportion of the human body and the image. For a model, the input resolution of these areas will directly affect the accuracy of the model’s prediction of the keypoints in these parts. The second problem is that the difficulty of learning keypoints from different human body parts varies. For example, the keypoints on the face can be regarded as some points attached to the face, a rigid body, and since the deformation of the face is very small, the model will find it easier to learn to predict these facial keypoints. On the contrary, hand keypoints have a higher degree of freedom due to the rotation and movement of the fingers and wrists, making them much more difficult to predict.
The third problem lies in the loss function. Commonly used loss functions such as the KL divergence and the regression error are calculated point-by-point. Thus, they tend to be dominated by body parts with more keypoints and larger spatial proportions, like the torso and face, paying insufficient attention to small but complex parts like hands and feet. This typically results in imbalanced convergence, meaning the model archives lower average error but exhibits poor accuracy on complex body parts. The last point is that there are very few open-source whole-body pose estimation datasets, significantly limiting our research on the model’s capabilities. In response to the above limitations, we have designed a set of targeted optimization solutions for RTMPose and proposed our RTMW model.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Model Architecture</h4>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="104" id="S3.F1.g1" src="extracted/5724567/figs/rtmw_arch.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The RTMW arch.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">The RTMW model structure is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#S3.F1" title="Figure 1 ‣ 3.1.2 Model Architecture ‣ 3.1 RTMW ‣ 3 Model Architecture and Training ‣ RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation"><span class="ltx_text ltx_ref_tag">1</span></a>, designed based on RTMPose, incorporating PAFPN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib19" title="">19</a>]</cite> and HEM modules to enhance feature resolution. Among these, FPN (Feature Pyramid Network) is an effective technique for improving feature resolution and is widely used in the model structures of dense prediction tasks. In addition to this, inspired by the hierarchical encoding concept proposed in the VQVAE-2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib30" title="">30</a>]</cite> paper, we introduced a HEM (Hierarchical Encoding Module) module. Experimental results show that the simultaneous introduction of these two modules can significantly enhance the performance of RTMPose in whole-body pose estimation tasks.</p>
</div>
<section class="ltx_paragraph" id="S3.SS1.SSS2.Px1">
<h5 class="ltx_title ltx_title_paragraph">PAFPN</h5>
<div class="ltx_para" id="S3.SS1.SSS2.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS2.Px1.p1.1">In the previous analysis, since we emphasized the impact of the input resolution of various body parts on the model’s prediction accuracy, we naturally introduced an FPN module. This technique is very common in other vision tasks, such as object detection tasks. Details of this module can be referred to in the original paper <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib19" title="">19</a>]</cite> and another improved version used in RTMDet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib24" title="">24</a>]</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS2.Px2">
<h5 class="ltx_title ltx_title_paragraph">HEM (Hierarchical Encoding Module)</h5>
<div class="ltx_para" id="S3.SS1.SSS2.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.Px2.p1.1">The inspiration for this module comes from the work VQVAEv2, which introduced a hierarchical concept on top of the original Encoder-Decoder architecture. In the original VQVAE work, the generated images lacked clear and rich details. Therefore, in the second version, they added hierarchical encoding to features of different resolutions and decoded these hierarchical features separately during the decoding process, which enriched the details in the generated images. We were inspired by this idea in the design of RTMW, creating the HEM (Hierarchical Encoding Module), which performs SimCC encoding on the features output by PAFPN in a hierarchical manner, then merges them after the hierarchical encoding and finally decodes them in the decoder. Experiments have shown that this design can improve the prediction accuracy of low-resolution body parts in human pose estimation.</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3 </span>Training Techniques</h4>
<div class="ltx_para" id="S3.SS1.SSS3.p1">
<p class="ltx_p" id="S3.SS1.SSS3.p1.1">Due to the lack of open-source whole-body pose estimation datasets, we manually aligned 14 open-source datasets that include whole-body, torso, hand, and facial keypoints for pose estimation. We used these 14 datasets for joint training. At the same time, to maximize the model’s performance, we adopted the two-stage distillation technique used by DWPose during the model training process to further enhance the model’s performance.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>RTMW3D</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We further explore extending RTMW architecture to 3D whole-body pose estimation, which is challenging due to its ill-posed nature. Specifically, while it is straightforward to ascertain the x and y coordinates of a keypoint from a single RGB image, the ambiguity of the z-axis introduces complexity in the model’s learning process. Furthermore, the scarcity of open-source 3D human pose datasets, especially those with keypoint annotations, and the limited scene diversity of available datasets severely hamper the model’s performance and generalization ability. To address these challenges, RTMW3D has been tailored to excel in monocular 3D pose estimation by refining the 3D dataset, redefining the z-axis, and mitigating the learning difficulty of predicting z-axis coordinates. Structurally, RTMW3D closely mirrors RTMW, with the notable addition of a z-axis prediction branch to the decoder head.</p>
</div>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Task definition</h5>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.1">RTMW3D adheres to the principles of 2D pose estimation, directly forecasting the x and y coordinates of keypoints on the SimCC coordinate space. Following the design of the SimCC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib16" title="">16</a>]</cite> method, as shown in figure <a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#S3.F2" title="Figure 2 ‣ Task definition ‣ 3.2 RTMW3D ‣ 3 Model Architecture and Training ‣ RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation"><span class="ltx_text ltx_ref_tag">2</span></a>, for the z-axis, we eschew the direct use of annotated z-axis coordinates from the dataset. Instead, we establish a root point for the human skeleton and calculate the z-axis offsets of keypoints relative to this root point. This innovative approach standardizes the z-axis across various datasets and simplifies the model’s learning challenge.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="179" id="S3.F2.g1" src="extracted/5724567/figs/3d_example.png" width="180"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The task definition of 3d pose estimation.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Data process</h5>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p1.1">Given the lack of 3D human pose datasets centered on keypoints, we implemented a synergistic training strategy that integrates both 2D and 3D datasets. This approach enriches the data with diverse scenarios, bolstering the model’s performance on 2D pose estimation tasks. To address the issue of the lack of z-axis annotations in 2D keypoint datasets, we have incorporated a z-axis mask into the 2D keypoint data. This modification facilitates a unified training protocol for both data types, enabling the model to accurately predict the x and y coordinates of keypoints and enhance its capacity for 2D pose estimation.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">During the training phase, we continued to use all the training techniques employed in RTMPose and adopted the two-stage distillation technology proposed by DWPose. Due to the limited availability of open-source whole-body pose estimation datasets, we integrated 14 datasets from different parts, manually aligned the keypoint definitions, and uniformly mapped them to the 133-point definition of COCO-Wholebody. In the monocular 3D whole-body pose estimation task, due to the lack of open-source 3D datasets, we combined 14 existing 2D datasets with three open-source 3D datasets for joint training using a total of 17 datasets.
The datasets we used are as follows:</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">3 whole-body datasets: COCO-Wholebody <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib38" title="">38</a>]</cite>, UBody <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib17" title="">17</a>]</cite>, Halpe <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib6" title="">6</a>]</cite></p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">6 human body datasets: AIC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib7" title="">7</a>]</cite>, CrowdPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib15" title="">15</a>]</cite>, MPII <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib1" title="">1</a>]</cite>, sub-JHMDB <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib9" title="">9</a>]</cite>, PoseTrack18 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib37" title="">37</a>]</cite>, Human-Art <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib13" title="">13</a>]</cite></p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1">4 face datasets: WFLW <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib36" title="">36</a>]</cite>, 300W, COFW, LaPa <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib20" title="">20</a>]</cite></p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i4.p1">
<p class="ltx_p" id="S4.I1.i4.p1.1">1 hand dataset: InterHand <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib27" title="">27</a>]</cite></p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i5.p1">
<p class="ltx_p" id="S4.I1.i5.p1.1">3 3D whole-body point datasets: H3WB <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib43" title="">43</a>]</cite>, UBody <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib17" title="">17</a>]</cite>, DNA-rendering <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib4" title="">4</a>]</cite></p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">For detailed mapping relationships, please refer to the appendix.
We carried out a two-stage distillation of RTMW according to the process proposed by DWPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib40" title="">40</a>]</cite>, and the relevant hyperparameters followed the settings in DWPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib40" title="">40</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Results</h3>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">COCO-Wholebody</h5>
<div class="ltx_para" id="S4.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px1.p1.1">We validate the proposed RTMW model on the whole-body pose estimation task with COCO-WholeBody <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib12" title="">12</a>]</cite> V1.0 dataset. As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#S4.T1" title="Table 1 ‣ COCO-Wholebody ‣ 4.2 Results ‣ 4 Experiments ‣ RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation"><span class="ltx_text ltx_ref_tag">1</span></a>, RTMW achieves superior performance and well balances accuracy and complexity. In addition, our proposed RTMW3D also demonstrates good performance on COCO-WholeBody.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Whole-body pose estimation results on COCO-WholeBody <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib12" title="">12</a>]</cite> V1.0 dataset. “*” denotes the model is pre-trained on AIC+COCO. “<sup class="ltx_sup" id="S4.T1.31.1"><span class="ltx_text ltx_font_italic" id="S4.T1.31.1.1">†</span></sup>” denotes the model in trained on the combined dataset introduced at <a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#S4" title="4 Experiments ‣ RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation"><span class="ltx_text ltx_ref_tag">4</span></a>.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.29" style="width:346.9pt;height:169.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-232.1pt,113.3pt) scale(0.42764039743275,0.42764039743275) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.29.27">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.29.27.28.1">
<td class="ltx_td ltx_border_r ltx_border_tt" id="S4.T1.29.27.28.1.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T1.29.27.28.1.2">Method</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.29.27.28.1.3">Input Size</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S4.T1.29.27.28.1.4">whole-body</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S4.T1.29.27.28.1.5">body</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S4.T1.29.27.28.1.6">foot</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S4.T1.29.27.28.1.7">face</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T1.29.27.28.1.8">hand</td>
</tr>
<tr class="ltx_tr" id="S4.T1.29.27.29.2">
<td class="ltx_td ltx_border_r" id="S4.T1.29.27.29.2.1"></td>
<td class="ltx_td ltx_border_r" id="S4.T1.29.27.29.2.2"></td>
<td class="ltx_td ltx_border_r" id="S4.T1.29.27.29.2.3"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.29.27.29.2.4">AP</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.29.27.29.2.5">AR</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.29.27.29.2.6">AP</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.29.27.29.2.7">AR</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.29.27.29.2.8">AP</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.29.27.29.2.9">AR</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.29.27.29.2.10">AP</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.29.27.29.2.11">AR</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.29.27.29.2.12">AP</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.29.27.29.2.13">AR</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.1.1">
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T1.3.1.1.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.1.1.3">DeepPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib33" title="">33</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.3.1.1.1">384<math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.3.1.1.1.m1.1"><semantics id="S4.T1.3.1.1.1.m1.1a"><mo id="S4.T1.3.1.1.1.m1.1.1" xref="S4.T1.3.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.1.1.1.m1.1b"><times id="S4.T1.3.1.1.1.m1.1.1.cmml" xref="S4.T1.3.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.3.1.1.1.m1.1d">×</annotation></semantics></math>288</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.1.1.4">33.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.3.1.1.5">45.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.1.1.6">42.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.3.1.1.7">58.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.1.1.8">9.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.3.1.1.9">36.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.1.1.10">64.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.3.1.1.11">69.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.1.1.12">40.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.1.1.13">58.0</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.2.2">
<td class="ltx_td ltx_border_r" id="S4.T1.4.2.2.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.4.2.2.3">SimpleBaseline <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib37" title="">37</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.4.2.2.1">384<math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.4.2.2.1.m1.1"><semantics id="S4.T1.4.2.2.1.m1.1a"><mo id="S4.T1.4.2.2.1.m1.1.1" xref="S4.T1.4.2.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.4.2.2.1.m1.1b"><times id="S4.T1.4.2.2.1.m1.1.1.cmml" xref="S4.T1.4.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.2.2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.4.2.2.1.m1.1d">×</annotation></semantics></math>288</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.2.2.4">57.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.4.2.2.5">67.1</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.2.2.6">66.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.4.2.2.7">74.7</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.2.2.8">63.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.4.2.2.9">76.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.2.2.10">73.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.4.2.2.11">81.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.2.2.12">53.7</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.2.2.13">64.7</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.3.3">
<td class="ltx_td ltx_border_r" id="S4.T1.5.3.3.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.5.3.3.3">HRNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib31" title="">31</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.5.3.3.1">384<math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.5.3.3.1.m1.1"><semantics id="S4.T1.5.3.3.1.m1.1a"><mo id="S4.T1.5.3.3.1.m1.1.1" xref="S4.T1.5.3.3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.5.3.3.1.m1.1b"><times id="S4.T1.5.3.3.1.m1.1.1.cmml" xref="S4.T1.5.3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.3.3.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.5.3.3.1.m1.1d">×</annotation></semantics></math>288</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.3.3.4">58.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.5.3.3.5">67.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.3.3.6">70.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.5.3.3.7">77.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.3.3.8">58.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.5.3.3.9">69.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.3.3.10">72.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.5.3.3.11">78.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.3.3.12">51.6</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.3.3.13">60.4</td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.4.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.4.4.2">Top-Down</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.6.4.4.3">PVT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib35" title="">35</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.4.4.1">384<math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.6.4.4.1.m1.1"><semantics id="S4.T1.6.4.4.1.m1.1a"><mo id="S4.T1.6.4.4.1.m1.1.1" xref="S4.T1.6.4.4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.6.4.4.1.m1.1b"><times id="S4.T1.6.4.4.1.m1.1.1.cmml" xref="S4.T1.6.4.4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.4.4.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.6.4.4.1.m1.1d">×</annotation></semantics></math>288</td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.4.4.4">58.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.4.4.5">68.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.4.4.6">67.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.4.4.7">76.1</td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.4.4.8">66.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.4.4.9">79.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.4.4.10">74.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.4.4.11">82.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.4.4.12">54.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.4.4.13">65.4</td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.5.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.7.5.5.2">Methods</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.7.5.5.3">FastPose50-dcn-si <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib6" title="">6</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.7.5.5.1">256<math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.7.5.5.1.m1.1"><semantics id="S4.T1.7.5.5.1.m1.1a"><mo id="S4.T1.7.5.5.1.m1.1.1" xref="S4.T1.7.5.5.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.7.5.5.1.m1.1b"><times id="S4.T1.7.5.5.1.m1.1.1.cmml" xref="S4.T1.7.5.5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.5.5.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.7.5.5.1.m1.1d">×</annotation></semantics></math>192</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.5.5.4">59.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.7.5.5.5">66.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.5.5.6">70.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.7.5.5.7">75.6</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.5.5.8">70.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.7.5.5.9">77.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.5.5.10">77.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.7.5.5.11">82.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.5.5.12">45.7</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.5.5.13">53.9</td>
</tr>
<tr class="ltx_tr" id="S4.T1.8.6.6">
<td class="ltx_td ltx_border_r" id="S4.T1.8.6.6.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.8.6.6.3">ZoomNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib11" title="">11</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.8.6.6.1">384<math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.8.6.6.1.m1.1"><semantics id="S4.T1.8.6.6.1.m1.1a"><mo id="S4.T1.8.6.6.1.m1.1.1" xref="S4.T1.8.6.6.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.8.6.6.1.m1.1b"><times id="S4.T1.8.6.6.1.m1.1.1.cmml" xref="S4.T1.8.6.6.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.6.6.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.8.6.6.1.m1.1d">×</annotation></semantics></math>288</td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.6.6.4">63.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.8.6.6.5">74.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.6.6.6">74.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.8.6.6.7">81.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.6.6.8">60.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.8.6.6.9">70.8</td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.6.6.10">88.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.8.6.6.11">92.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.6.6.12">57.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.6.6.13">73.4</td>
</tr>
<tr class="ltx_tr" id="S4.T1.9.7.7">
<td class="ltx_td ltx_border_r" id="S4.T1.9.7.7.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.9.7.7.3">ZoomNAS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib38" title="">38</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.9.7.7.1">384<math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.9.7.7.1.m1.1"><semantics id="S4.T1.9.7.7.1.m1.1a"><mo id="S4.T1.9.7.7.1.m1.1.1" xref="S4.T1.9.7.7.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.9.7.7.1.m1.1b"><times id="S4.T1.9.7.7.1.m1.1.1.cmml" xref="S4.T1.9.7.7.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.9.7.7.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.9.7.7.1.m1.1d">×</annotation></semantics></math>288</td>
<td class="ltx_td ltx_align_center" id="S4.T1.9.7.7.4">65.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.9.7.7.5">74.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.9.7.7.6">74.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.9.7.7.7">80.7</td>
<td class="ltx_td ltx_align_center" id="S4.T1.9.7.7.8">61.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.9.7.7.9">71.8</td>
<td class="ltx_td ltx_align_center" id="S4.T1.9.7.7.10">88.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.9.7.7.11">93.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.9.7.7.12">62.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.9.7.7.13">74.0</td>
</tr>
<tr class="ltx_tr" id="S4.T1.10.8.8">
<td class="ltx_td ltx_border_r" id="S4.T1.10.8.8.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.10.8.8.3">DWPose-l <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib40" title="">40</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.10.8.8.1">384<math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.10.8.8.1.m1.1"><semantics id="S4.T1.10.8.8.1.m1.1a"><mo id="S4.T1.10.8.8.1.m1.1.1" xref="S4.T1.10.8.8.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.10.8.8.1.m1.1b"><times id="S4.T1.10.8.8.1.m1.1.1.cmml" xref="S4.T1.10.8.8.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.10.8.8.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.10.8.8.1.m1.1d">×</annotation></semantics></math>288</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.8.8.4">66.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.10.8.8.5">74.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.8.8.6">72.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.10.8.8.7">78.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.8.8.8">70.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.10.8.8.9">81.6</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.8.8.10">88.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.10.8.8.11">92.1</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.8.8.12">62.1</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.8.8.13">72.0</td>
</tr>
<tr class="ltx_tr" id="S4.T1.11.9.9">
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T1.11.9.9.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.11.9.9.3">RTMPose-m*</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.11.9.9.1">256<math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.11.9.9.1.m1.1"><semantics id="S4.T1.11.9.9.1.m1.1a"><mo id="S4.T1.11.9.9.1.m1.1.1" xref="S4.T1.11.9.9.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.11.9.9.1.m1.1b"><times id="S4.T1.11.9.9.1.m1.1.1.cmml" xref="S4.T1.11.9.9.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.11.9.9.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.11.9.9.1.m1.1d">×</annotation></semantics></math>192</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.11.9.9.4">58.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.11.9.9.5">67.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.11.9.9.6">67.3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.11.9.9.7">75.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.11.9.9.8">61.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.11.9.9.9">75.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.11.9.9.10">81.3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.11.9.9.11">87.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.11.9.9.12">47.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.11.9.9.13">58.9</td>
</tr>
<tr class="ltx_tr" id="S4.T1.12.10.10">
<td class="ltx_td ltx_border_r" id="S4.T1.12.10.10.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.12.10.10.3">RTMPose-l*</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.12.10.10.1">256<math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.12.10.10.1.m1.1"><semantics id="S4.T1.12.10.10.1.m1.1a"><mo id="S4.T1.12.10.10.1.m1.1.1" xref="S4.T1.12.10.10.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.12.10.10.1.m1.1b"><times id="S4.T1.12.10.10.1.m1.1.1.cmml" xref="S4.T1.12.10.10.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.12.10.10.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.12.10.10.1.m1.1d">×</annotation></semantics></math>192</td>
<td class="ltx_td ltx_align_center" id="S4.T1.12.10.10.4">61.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.12.10.10.5">70.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.12.10.10.6">69.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.12.10.10.7">76.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.12.10.10.8">65.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.12.10.10.9">78.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.12.10.10.10">83.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.12.10.10.11">88.7</td>
<td class="ltx_td ltx_align_center" id="S4.T1.12.10.10.12">51.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.12.10.10.13">62.8</td>
</tr>
<tr class="ltx_tr" id="S4.T1.13.11.11">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.13.11.11.2">RTMPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib10" title="">10</a>]</cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.13.11.11.3">RTMPose-l*</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.13.11.11.1">384<math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.13.11.11.1.m1.1"><semantics id="S4.T1.13.11.11.1.m1.1a"><mo id="S4.T1.13.11.11.1.m1.1.1" xref="S4.T1.13.11.11.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.13.11.11.1.m1.1b"><times id="S4.T1.13.11.11.1.m1.1.1.cmml" xref="S4.T1.13.11.11.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.13.11.11.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.13.11.11.1.m1.1d">×</annotation></semantics></math>288</td>
<td class="ltx_td ltx_align_center" id="S4.T1.13.11.11.4">64.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.13.11.11.5">73.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.13.11.11.6">71.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.13.11.11.7">78.1</td>
<td class="ltx_td ltx_align_center" id="S4.T1.13.11.11.8">69.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.13.11.11.9">81.1</td>
<td class="ltx_td ltx_align_center" id="S4.T1.13.11.11.10">88.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.13.11.11.11">91.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.13.11.11.12">57.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.13.11.11.13">67.7</td>
</tr>
<tr class="ltx_tr" id="S4.T1.14.12.12">
<td class="ltx_td ltx_border_r" id="S4.T1.14.12.12.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.14.12.12.3">RTMPose-x*</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.14.12.12.1">384<math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.14.12.12.1.m1.1"><semantics id="S4.T1.14.12.12.1.m1.1a"><mo id="S4.T1.14.12.12.1.m1.1.1" xref="S4.T1.14.12.12.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.14.12.12.1.m1.1b"><times id="S4.T1.14.12.12.1.m1.1.1.cmml" xref="S4.T1.14.12.12.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.14.12.12.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.14.12.12.1.m1.1d">×</annotation></semantics></math>288</td>
<td class="ltx_td ltx_align_center" id="S4.T1.14.12.12.4">65.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.14.12.12.5">73.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.14.12.12.6">71.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.14.12.12.7">78.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.14.12.12.8">68.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.14.12.12.9">80.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.14.12.12.10">89.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.14.12.12.11">92.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.14.12.12.12">59.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.14.12.12.13">68.7</td>
</tr>
<tr class="ltx_tr" id="S4.T1.15.13.13">
<td class="ltx_td ltx_border_r" id="S4.T1.15.13.13.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.15.13.13.3">RTMPose-x*</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.15.13.13.1">384<math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.15.13.13.1.m1.1"><semantics id="S4.T1.15.13.13.1.m1.1a"><mo id="S4.T1.15.13.13.1.m1.1.1" xref="S4.T1.15.13.13.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.15.13.13.1.m1.1b"><times id="S4.T1.15.13.13.1.m1.1.1.cmml" xref="S4.T1.15.13.13.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.15.13.13.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.15.13.13.1.m1.1d">×</annotation></semantics></math>288</td>
<td class="ltx_td ltx_align_center" id="S4.T1.15.13.13.4">65.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.15.13.13.5">73.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.15.13.13.6">71.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.15.13.13.7">78.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.15.13.13.8">69.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.15.13.13.9">81.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.15.13.13.10">88.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.15.13.13.11">92.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.15.13.13.12">59.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.15.13.13.13">68.5</td>
</tr>
<tr class="ltx_tr" id="S4.T1.17.15.15">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.17.15.15.3">RTMW3D</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.16.14.14.1">RTMW3D-l<sup class="ltx_sup" id="S4.T1.16.14.14.1.1"><span class="ltx_text ltx_font_italic" id="S4.T1.16.14.14.1.1.1">†</span></sup>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.17.15.15.2">384<math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.17.15.15.2.m1.1"><semantics id="S4.T1.17.15.15.2.m1.1a"><mo id="S4.T1.17.15.15.2.m1.1.1" xref="S4.T1.17.15.15.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.17.15.15.2.m1.1b"><times id="S4.T1.17.15.15.2.m1.1.1.cmml" xref="S4.T1.17.15.15.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.17.15.15.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.17.15.15.2.m1.1d">×</annotation></semantics></math>288</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.17.15.15.4">67.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.17.15.15.5">75.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.17.15.15.6">74.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.17.15.15.7">80.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.17.15.15.8">77.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.17.15.15.9">86.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.17.15.15.10">88.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.17.15.15.11">91.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.17.15.15.12">61.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.17.15.15.13">70.6</td>
</tr>
<tr class="ltx_tr" id="S4.T1.19.17.17">
<td class="ltx_td ltx_border_r" id="S4.T1.19.17.17.3"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.18.16.16.1">RTMW3D-x<sup class="ltx_sup" id="S4.T1.18.16.16.1.1"><span class="ltx_text ltx_font_italic" id="S4.T1.18.16.16.1.1.1">†</span></sup>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.19.17.17.2">384<math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.19.17.17.2.m1.1"><semantics id="S4.T1.19.17.17.2.m1.1a"><mo id="S4.T1.19.17.17.2.m1.1.1" xref="S4.T1.19.17.17.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.19.17.17.2.m1.1b"><times id="S4.T1.19.17.17.2.m1.1.1.cmml" xref="S4.T1.19.17.17.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.19.17.17.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.19.17.17.2.m1.1d">×</annotation></semantics></math>288</td>
<td class="ltx_td ltx_align_center" id="S4.T1.19.17.17.4">68.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.19.17.17.5">75.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.19.17.17.6">73.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.19.17.17.7">80.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.19.17.17.8">76.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.19.17.17.9">86.8</td>
<td class="ltx_td ltx_align_center" id="S4.T1.19.17.17.10">88.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.19.17.17.11">92.1</td>
<td class="ltx_td ltx_align_center" id="S4.T1.19.17.17.12">62.7</td>
<td class="ltx_td ltx_align_center" id="S4.T1.19.17.17.13">72.2</td>
</tr>
<tr class="ltx_tr" id="S4.T1.21.19.19">
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T1.21.19.19.3"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.20.18.18.1">RTMW-m<sup class="ltx_sup" id="S4.T1.20.18.18.1.1"><span class="ltx_text ltx_font_italic" id="S4.T1.20.18.18.1.1.1">†</span></sup>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.21.19.19.2">256<math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.21.19.19.2.m1.1"><semantics id="S4.T1.21.19.19.2.m1.1a"><mo id="S4.T1.21.19.19.2.m1.1.1" xref="S4.T1.21.19.19.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.21.19.19.2.m1.1b"><times id="S4.T1.21.19.19.2.m1.1.1.cmml" xref="S4.T1.21.19.19.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.21.19.19.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.21.19.19.2.m1.1d">×</annotation></semantics></math>192</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.21.19.19.4">58.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.21.19.19.5">67.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.21.19.19.6">67.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.21.19.19.7">74.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.21.19.19.8">67.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.21.19.19.9">79.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.21.19.19.10">78.3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.21.19.19.11">85.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.21.19.19.12">49.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.21.19.19.13">60.4</td>
</tr>
<tr class="ltx_tr" id="S4.T1.23.21.21">
<td class="ltx_td ltx_border_r" id="S4.T1.23.21.21.3"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.22.20.20.1">RTMW-l<sup class="ltx_sup" id="S4.T1.22.20.20.1.1"><span class="ltx_text ltx_font_italic" id="S4.T1.22.20.20.1.1.1">†</span></sup>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.23.21.21.2">256<math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.23.21.21.2.m1.1"><semantics id="S4.T1.23.21.21.2.m1.1a"><mo id="S4.T1.23.21.21.2.m1.1.1" xref="S4.T1.23.21.21.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.23.21.21.2.m1.1b"><times id="S4.T1.23.21.21.2.m1.1.1.cmml" xref="S4.T1.23.21.21.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.23.21.21.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.23.21.21.2.m1.1d">×</annotation></semantics></math>192</td>
<td class="ltx_td ltx_align_center" id="S4.T1.23.21.21.4">66.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.23.21.21.5">74.6</td>
<td class="ltx_td ltx_align_center" id="S4.T1.23.21.21.6">74.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.23.21.21.7">80.7</td>
<td class="ltx_td ltx_align_center" id="S4.T1.23.21.21.8">76.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.23.21.21.9">86.8</td>
<td class="ltx_td ltx_align_center" id="S4.T1.23.21.21.10">83.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.23.21.21.11">88.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.23.21.21.12">59.8</td>
<td class="ltx_td ltx_align_center" id="S4.T1.23.21.21.13">70.1</td>
</tr>
<tr class="ltx_tr" id="S4.T1.25.23.23">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.25.23.23.3">RTMW</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.24.22.22.1">RTMW-x<sup class="ltx_sup" id="S4.T1.24.22.22.1.1"><span class="ltx_text ltx_font_italic" id="S4.T1.24.22.22.1.1.1">†</span></sup>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.25.23.23.2">256<math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.25.23.23.2.m1.1"><semantics id="S4.T1.25.23.23.2.m1.1a"><mo id="S4.T1.25.23.23.2.m1.1.1" xref="S4.T1.25.23.23.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.25.23.23.2.m1.1b"><times id="S4.T1.25.23.23.2.m1.1.1.cmml" xref="S4.T1.25.23.23.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.25.23.23.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.25.23.23.2.m1.1d">×</annotation></semantics></math>192</td>
<td class="ltx_td ltx_align_center" id="S4.T1.25.23.23.4">67.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.25.23.23.5">75.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.25.23.23.6">74.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.25.23.23.7">80.8</td>
<td class="ltx_td ltx_align_center" id="S4.T1.25.23.23.8">77.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.25.23.23.9">86.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.25.23.23.10">84.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.25.23.23.11">89.6</td>
<td class="ltx_td ltx_align_center" id="S4.T1.25.23.23.12">61.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.25.23.23.13">71.0</td>
</tr>
<tr class="ltx_tr" id="S4.T1.27.25.25">
<td class="ltx_td ltx_border_r" id="S4.T1.27.25.25.3"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.26.24.24.1">RTMW-l<sup class="ltx_sup" id="S4.T1.26.24.24.1.1"><span class="ltx_text ltx_font_italic" id="S4.T1.26.24.24.1.1.1">†</span></sup>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.27.25.25.2">384<math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.27.25.25.2.m1.1"><semantics id="S4.T1.27.25.25.2.m1.1a"><mo id="S4.T1.27.25.25.2.m1.1.1" xref="S4.T1.27.25.25.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.27.25.25.2.m1.1b"><times id="S4.T1.27.25.25.2.m1.1.1.cmml" xref="S4.T1.27.25.25.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.27.25.25.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.27.25.25.2.m1.1d">×</annotation></semantics></math>288</td>
<td class="ltx_td ltx_align_center" id="S4.T1.27.25.25.4">70.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.27.25.25.5">78.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.27.25.25.6">76.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.27.25.25.7">82.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.27.25.25.8">79.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.27.25.25.9">88.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.27.25.25.10">88.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.27.25.25.11">92.1</td>
<td class="ltx_td ltx_align_center" id="S4.T1.27.25.25.12">66.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.27.25.25.13">75.2</td>
</tr>
<tr class="ltx_tr" id="S4.T1.29.27.27">
<td class="ltx_td ltx_border_bb ltx_border_r" id="S4.T1.29.27.27.3"></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T1.28.26.26.1">RTMW-x<sup class="ltx_sup" id="S4.T1.28.26.26.1.1"><span class="ltx_text ltx_font_italic" id="S4.T1.28.26.26.1.1.1">†</span></sup>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T1.29.27.27.2">384<math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.29.27.27.2.m1.1"><semantics id="S4.T1.29.27.27.2.m1.1a"><mo id="S4.T1.29.27.27.2.m1.1.1" xref="S4.T1.29.27.27.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.29.27.27.2.m1.1b"><times id="S4.T1.29.27.27.2.m1.1.1.cmml" xref="S4.T1.29.27.27.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.29.27.27.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.29.27.27.2.m1.1d">×</annotation></semantics></math>288</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.29.27.27.4">70.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T1.29.27.27.5">78.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.29.27.27.6">76.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T1.29.27.27.7">82.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.29.27.27.8">79.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T1.29.27.27.9">88.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.29.27.27.10">88.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T1.29.27.27.11">92.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.29.27.27.12">66.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.29.27.27.13">75.5</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">H3WB</h5>
<div class="ltx_para" id="S4.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p1.1">H3WB <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib43" title="">43</a>]</cite> is currently the only open-source 3D whole-body pose estimation dataset that provides a test set. We evaluated the performance of RTMW3D on the test set of H3WB, and the results are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#S4.T2" title="Table 2 ‣ H3WB ‣ 4.2 Results ‣ 4 Experiments ‣ RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation"><span class="ltx_text ltx_ref_tag">2</span></a>. Similarly, RTMW3D has achieved excellent performance on the H3WB dataset.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Whole-body 3D pose estimation results on H3WB <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib43" title="">43</a>]</cite> dataset. “*” denotes the results are borrowed from H3WB paper <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib43" title="">43</a>]</cite>.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.1" style="width:390.3pt;height:109.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(2.8pt,-0.8pt) scale(1.01475554733107,1.01475554733107) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T2.1.1.1.1.1">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T2.1.1.1.1.2">Input Size</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.1.3">MPJPE</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.1.1.2.1.1">CanonPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib34" title="">34</a>]</cite> with 3D supervision*</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.2.1.2">N/A</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.2.1.3">0.117</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.1.1.3.2.1">Large SimpleBaseline* <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib37" title="">37</a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.1.3.2.2">N/A</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.3.2.3">0.112</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.1.1.4.3.1">JointFormer* <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib23" title="">23</a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.1.4.3.2">N/A</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.4.3.3">0.088</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.1.1.5.4.1">RTMW3D-l</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.1.5.4.2">384x288</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.5.4.3">0.056</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T2.1.1.6.5.1">RTMW3D-x</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.1.1.6.5.2">384x288</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.1.6.5.3">0.057</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Inference speed</h5>
<div class="ltx_para" id="S4.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px3.p1.1">As our primary focus is on the development of a real-time model, the inference speed emerges as a critical performance metric. The evaluation of RTMW models’ inference speed is detailed in the accompanying table <a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#S4.T3" title="Table 3 ‣ Inference speed ‣ 4.2 Results ‣ 4 Experiments ‣ RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation"><span class="ltx_text ltx_ref_tag">3</span></a>. Although RTMW, which incorporates an additional module relative to RTMPose, exhibits a marginally reduced inference speed, it offers a substantial enhancement in accuracy. This trade-off is deemed justifiable in the context of our objectives.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Inference speed on CPU. RTMPose models are deployed and tested using ONNXRuntime. Flip test is not used in this table. </figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.7.7" style="width:433.6pt;height:148.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(6.7pt,-2.3pt) scale(1.03185507206172,1.03185507206172) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T3.7.7.7">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.7.7.7.8.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" colspan="2" id="S4.T3.7.7.7.8.1.1">Results</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T3.7.7.7.8.1.2">Input Size</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.7.7.7.8.1.3">GFLOPs</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.7.7.7.8.1.4">AP</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.7.7.7.8.1.5">CPU(ms)</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.1.1.1.1.2" rowspan="3"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.1.1.1.1.3">HRNet-w32+DARK</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.1.1.1.1.1"><math alttext="256\times 192" class="ltx_Math" display="inline" id="S4.T3.1.1.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.1.1.m1.1a"><mrow id="S4.T3.1.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.1.m1.1.1.cmml"><mn id="S4.T3.1.1.1.1.1.m1.1.1.2" xref="S4.T3.1.1.1.1.1.m1.1.1.2.cmml">256</mn><mo id="S4.T3.1.1.1.1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T3.1.1.1.1.1.m1.1.1.1.cmml">×</mo><mn id="S4.T3.1.1.1.1.1.m1.1.1.3" xref="S4.T3.1.1.1.1.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.1.m1.1b"><apply id="S4.T3.1.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.1.m1.1.1"><times id="S4.T3.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T3.1.1.1.1.1.m1.1.1.1"></times><cn id="S4.T3.1.1.1.1.1.m1.1.1.2.cmml" type="integer" xref="S4.T3.1.1.1.1.1.m1.1.1.2">256</cn><cn id="S4.T3.1.1.1.1.1.m1.1.1.3.cmml" type="integer" xref="S4.T3.1.1.1.1.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.1.m1.1c">256\times 192</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.1.1.m1.1d">256 × 192</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.1.1.1.4">7.72</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.1.1.1.5">57.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.1.1.6">39.051</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.2.2.2.2.2">RTMPose-m</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.2.2.2.2.1"><math alttext="256\times 192" class="ltx_Math" display="inline" id="S4.T3.2.2.2.2.1.m1.1"><semantics id="S4.T3.2.2.2.2.1.m1.1a"><mrow id="S4.T3.2.2.2.2.1.m1.1.1" xref="S4.T3.2.2.2.2.1.m1.1.1.cmml"><mn id="S4.T3.2.2.2.2.1.m1.1.1.2" xref="S4.T3.2.2.2.2.1.m1.1.1.2.cmml">256</mn><mo id="S4.T3.2.2.2.2.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T3.2.2.2.2.1.m1.1.1.1.cmml">×</mo><mn id="S4.T3.2.2.2.2.1.m1.1.1.3" xref="S4.T3.2.2.2.2.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.2.1.m1.1b"><apply id="S4.T3.2.2.2.2.1.m1.1.1.cmml" xref="S4.T3.2.2.2.2.1.m1.1.1"><times id="S4.T3.2.2.2.2.1.m1.1.1.1.cmml" xref="S4.T3.2.2.2.2.1.m1.1.1.1"></times><cn id="S4.T3.2.2.2.2.1.m1.1.1.2.cmml" type="integer" xref="S4.T3.2.2.2.2.1.m1.1.1.2">256</cn><cn id="S4.T3.2.2.2.2.1.m1.1.1.3.cmml" type="integer" xref="S4.T3.2.2.2.2.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.2.1.m1.1c">256\times 192</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.2.2.2.1.m1.1d">256 × 192</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.2.2.3">2.22</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.2.2.4">59.1</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.2.5"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.2.2.5.1">13.496</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.3.3.3.3.2">RTMPose-l</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.3.3.3.3.1"><math alttext="256\times 192" class="ltx_Math" display="inline" id="S4.T3.3.3.3.3.1.m1.1"><semantics id="S4.T3.3.3.3.3.1.m1.1a"><mrow id="S4.T3.3.3.3.3.1.m1.1.1" xref="S4.T3.3.3.3.3.1.m1.1.1.cmml"><mn id="S4.T3.3.3.3.3.1.m1.1.1.2" xref="S4.T3.3.3.3.3.1.m1.1.1.2.cmml">256</mn><mo id="S4.T3.3.3.3.3.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T3.3.3.3.3.1.m1.1.1.1.cmml">×</mo><mn id="S4.T3.3.3.3.3.1.m1.1.1.3" xref="S4.T3.3.3.3.3.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.3.1.m1.1b"><apply id="S4.T3.3.3.3.3.1.m1.1.1.cmml" xref="S4.T3.3.3.3.3.1.m1.1.1"><times id="S4.T3.3.3.3.3.1.m1.1.1.1.cmml" xref="S4.T3.3.3.3.3.1.m1.1.1.1"></times><cn id="S4.T3.3.3.3.3.1.m1.1.1.2.cmml" type="integer" xref="S4.T3.3.3.3.3.1.m1.1.1.2">256</cn><cn id="S4.T3.3.3.3.3.1.m1.1.1.3.cmml" type="integer" xref="S4.T3.3.3.3.3.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.3.1.m1.1c">256\times 192</annotation><annotation encoding="application/x-llamapun" id="S4.T3.3.3.3.3.1.m1.1d">256 × 192</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.3.3.3.3.3">4.52</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.3.3.3.3.4">62.2</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.3.3.3.5">23.410</td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.4.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.4.4.4.4.2">COCO-</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.4.4.4.4.3">HRNet-w48+DARK</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.4.4.4.4.1"><math alttext="384\times 288" class="ltx_Math" display="inline" id="S4.T3.4.4.4.4.1.m1.1"><semantics id="S4.T3.4.4.4.4.1.m1.1a"><mrow id="S4.T3.4.4.4.4.1.m1.1.1" xref="S4.T3.4.4.4.4.1.m1.1.1.cmml"><mn id="S4.T3.4.4.4.4.1.m1.1.1.2" xref="S4.T3.4.4.4.4.1.m1.1.1.2.cmml">384</mn><mo id="S4.T3.4.4.4.4.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T3.4.4.4.4.1.m1.1.1.1.cmml">×</mo><mn id="S4.T3.4.4.4.4.1.m1.1.1.3" xref="S4.T3.4.4.4.4.1.m1.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.4.4.1.m1.1b"><apply id="S4.T3.4.4.4.4.1.m1.1.1.cmml" xref="S4.T3.4.4.4.4.1.m1.1.1"><times id="S4.T3.4.4.4.4.1.m1.1.1.1.cmml" xref="S4.T3.4.4.4.4.1.m1.1.1.1"></times><cn id="S4.T3.4.4.4.4.1.m1.1.1.2.cmml" type="integer" xref="S4.T3.4.4.4.4.1.m1.1.1.2">384</cn><cn id="S4.T3.4.4.4.4.1.m1.1.1.3.cmml" type="integer" xref="S4.T3.4.4.4.4.1.m1.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.4.4.1.m1.1c">384\times 288</annotation><annotation encoding="application/x-llamapun" id="S4.T3.4.4.4.4.1.m1.1d">384 × 288</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.4.4.4.4">35.52</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.4.4.4.5">65.3</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.4.4.4.6">150.765</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.5.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.5.5.5.5.2">WholeBody <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib11" title="">11</a>]</cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.5.5.5.5.3">RTMPose-l</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.5.5.5.5.1"><math alttext="384\times 288" class="ltx_Math" display="inline" id="S4.T3.5.5.5.5.1.m1.1"><semantics id="S4.T3.5.5.5.5.1.m1.1a"><mrow id="S4.T3.5.5.5.5.1.m1.1.1" xref="S4.T3.5.5.5.5.1.m1.1.1.cmml"><mn id="S4.T3.5.5.5.5.1.m1.1.1.2" xref="S4.T3.5.5.5.5.1.m1.1.1.2.cmml">384</mn><mo id="S4.T3.5.5.5.5.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T3.5.5.5.5.1.m1.1.1.1.cmml">×</mo><mn id="S4.T3.5.5.5.5.1.m1.1.1.3" xref="S4.T3.5.5.5.5.1.m1.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.5.5.1.m1.1b"><apply id="S4.T3.5.5.5.5.1.m1.1.1.cmml" xref="S4.T3.5.5.5.5.1.m1.1.1"><times id="S4.T3.5.5.5.5.1.m1.1.1.1.cmml" xref="S4.T3.5.5.5.5.1.m1.1.1.1"></times><cn id="S4.T3.5.5.5.5.1.m1.1.1.2.cmml" type="integer" xref="S4.T3.5.5.5.5.1.m1.1.1.2">384</cn><cn id="S4.T3.5.5.5.5.1.m1.1.1.3.cmml" type="integer" xref="S4.T3.5.5.5.5.1.m1.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.5.5.1.m1.1c">384\times 288</annotation><annotation encoding="application/x-llamapun" id="S4.T3.5.5.5.5.1.m1.1d">384 × 288</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.5.5.5.4">10.07</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.5.5.5.5">66.1</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.5.6">44.581</td>
</tr>
<tr class="ltx_tr" id="S4.T3.6.6.6.6">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S4.T3.6.6.6.6.2"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.6.6.6.6.3">RTMW-l</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.6.6.6.6.1"><math alttext="256\times 192" class="ltx_Math" display="inline" id="S4.T3.6.6.6.6.1.m1.1"><semantics id="S4.T3.6.6.6.6.1.m1.1a"><mrow id="S4.T3.6.6.6.6.1.m1.1.1" xref="S4.T3.6.6.6.6.1.m1.1.1.cmml"><mn id="S4.T3.6.6.6.6.1.m1.1.1.2" xref="S4.T3.6.6.6.6.1.m1.1.1.2.cmml">256</mn><mo id="S4.T3.6.6.6.6.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T3.6.6.6.6.1.m1.1.1.1.cmml">×</mo><mn id="S4.T3.6.6.6.6.1.m1.1.1.3" xref="S4.T3.6.6.6.6.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.6.6.6.6.1.m1.1b"><apply id="S4.T3.6.6.6.6.1.m1.1.1.cmml" xref="S4.T3.6.6.6.6.1.m1.1.1"><times id="S4.T3.6.6.6.6.1.m1.1.1.1.cmml" xref="S4.T3.6.6.6.6.1.m1.1.1.1"></times><cn id="S4.T3.6.6.6.6.1.m1.1.1.2.cmml" type="integer" xref="S4.T3.6.6.6.6.1.m1.1.1.2">256</cn><cn id="S4.T3.6.6.6.6.1.m1.1.1.3.cmml" type="integer" xref="S4.T3.6.6.6.6.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.6.6.6.1.m1.1c">256\times 192</annotation><annotation encoding="application/x-llamapun" id="S4.T3.6.6.6.6.1.m1.1d">256 × 192</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.6.6.6.6.4">7.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.6.6.6.6.5">66.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.6.6.6">35.322</td>
</tr>
<tr class="ltx_tr" id="S4.T3.7.7.7.7">
<th class="ltx_td ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T3.7.7.7.7.2"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T3.7.7.7.7.3">RTMW-l</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T3.7.7.7.7.1"><math alttext="384\times 288" class="ltx_Math" display="inline" id="S4.T3.7.7.7.7.1.m1.1"><semantics id="S4.T3.7.7.7.7.1.m1.1a"><mrow id="S4.T3.7.7.7.7.1.m1.1.1" xref="S4.T3.7.7.7.7.1.m1.1.1.cmml"><mn id="S4.T3.7.7.7.7.1.m1.1.1.2" xref="S4.T3.7.7.7.7.1.m1.1.1.2.cmml">384</mn><mo id="S4.T3.7.7.7.7.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T3.7.7.7.7.1.m1.1.1.1.cmml">×</mo><mn id="S4.T3.7.7.7.7.1.m1.1.1.3" xref="S4.T3.7.7.7.7.1.m1.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.7.7.7.7.1.m1.1b"><apply id="S4.T3.7.7.7.7.1.m1.1.1.cmml" xref="S4.T3.7.7.7.7.1.m1.1.1"><times id="S4.T3.7.7.7.7.1.m1.1.1.1.cmml" xref="S4.T3.7.7.7.7.1.m1.1.1.1"></times><cn id="S4.T3.7.7.7.7.1.m1.1.1.2.cmml" type="integer" xref="S4.T3.7.7.7.7.1.m1.1.1.2">384</cn><cn id="S4.T3.7.7.7.7.1.m1.1.1.3.cmml" type="integer" xref="S4.T3.7.7.7.7.1.m1.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.7.7.7.7.1.m1.1c">384\times 288</annotation><annotation encoding="application/x-llamapun" id="S4.T3.7.7.7.7.1.m1.1d">384 × 288</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.7.7.7.7.4">17.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.7.7.7.7.5"><span class="ltx_text ltx_font_bold" id="S4.T3.7.7.7.7.5.1">70.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.7.7.7.7.6">47.618</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Ablation on RTMW. Models are trained on COCO-WholeBody+UBody+Halpe.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T4.1" style="width:474.0pt;height:98.1pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-26.3pt,5.4pt) scale(0.9,0.9) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T4.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.1.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T4.1.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T4.1.1.1.1.2">PAFPN</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T4.1.1.1.1.3">HEM</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T4.1.1.1.1.4">GAU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T4.1.1.1.1.5">Whole</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T4.1.1.1.1.6">Body</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T4.1.1.1.1.7">Foot</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T4.1.1.1.1.8">Face</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.1.1.9">Hand</th>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T4.1.1.2.2.1">RTMPose-l</th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T4.1.1.2.2.2"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T4.1.1.2.2.3"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T4.1.1.2.2.4">✓</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T4.1.1.2.2.5">63.8</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T4.1.1.2.2.6">71.7</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T4.1.1.2.2.7">71.5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T4.1.1.2.2.8">83.3</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T4.1.1.2.2.9">56.4</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.1.1.3.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.1.1.3.1.1">w/ PAFPN</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.3.1.2">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.1.3.1.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.3.1.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.3.1.5">64.3 (<span class="ltx_text" id="S4.T4.1.1.3.1.5.1" style="color:#0000FF;">+0.5</span>)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.3.1.6">71.8 (<span class="ltx_text" id="S4.T4.1.1.3.1.6.1" style="color:#0000FF;">+0.1</span>)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.3.1.7">71.3 (<span class="ltx_text" id="S4.T4.1.1.3.1.7.1" style="color:#FF0000;">-0.2</span>)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.3.1.8">84.0 (<span class="ltx_text" id="S4.T4.1.1.3.1.8.1" style="color:#00FF00;">+0.7</span>)</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.1.1.3.1.9">57.7 (<span class="ltx_text" id="S4.T4.1.1.3.1.9.1" style="color:#00FF00;">+1.3</span>)</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.4.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.1.1.4.2.1">w/ HEM</td>
<td class="ltx_td ltx_border_r" id="S4.T4.1.1.4.2.2"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.4.2.3">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.4.2.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.4.2.5">64.1 (<span class="ltx_text" id="S4.T4.1.1.4.2.5.1" style="color:#0000FF;">+0.3</span>)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.4.2.6">72.0 (<span class="ltx_text" id="S4.T4.1.1.4.2.6.1" style="color:#0000FF;">+0.3</span>)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.4.2.7">71.9 (<span class="ltx_text" id="S4.T4.1.1.4.2.7.1" style="color:#0000FF;">+0.4</span>)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.4.2.8">83.4 (<span class="ltx_text" id="S4.T4.1.1.4.2.8.1" style="color:#0000FF;">+0.1</span>)</td>
<td class="ltx_td ltx_align_left" id="S4.T4.1.1.4.2.9">57.1 (<span class="ltx_text" id="S4.T4.1.1.4.2.9.1" style="color:#00FF00;">+0.7</span>)</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.5.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.1.1.5.3.1">w/ PAFPN + HEM</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.5.3.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.5.3.3">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.5.3.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.5.3.5">65.3 (<span class="ltx_text" id="S4.T4.1.1.5.3.5.1" style="color:#00FF00;">+1.5</span>)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.5.3.6">72.7 (<span class="ltx_text" id="S4.T4.1.1.5.3.6.1" style="color:#00FF00;">+1.0</span>)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.5.3.7">74.8 (<span class="ltx_text" id="S4.T4.1.1.5.3.7.1" style="color:#00FF00;">+3.3</span>)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.5.3.8">84.0 (<span class="ltx_text" id="S4.T4.1.1.5.3.8.1" style="color:#00FF00;">+0.7</span>)</td>
<td class="ltx_td ltx_align_left" id="S4.T4.1.1.5.3.9">59.9 (<span class="ltx_text" id="S4.T4.1.1.5.3.9.1" style="color:#00FF00;">+3.5</span>)</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.6.4">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T4.1.1.6.4.1">w/o GAU</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T4.1.1.6.4.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T4.1.1.6.4.3">✓</td>
<td class="ltx_td ltx_border_bb ltx_border_r" id="S4.T4.1.1.6.4.4"></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T4.1.1.6.4.5">64.8 (<span class="ltx_text" id="S4.T4.1.1.6.4.5.1" style="color:#00FF00;">+1.0</span>)</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T4.1.1.6.4.6">71.9 (<span class="ltx_text" id="S4.T4.1.1.6.4.6.1" style="color:#0000FF;">+0.2</span>)</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T4.1.1.6.4.7">72.2 (<span class="ltx_text" id="S4.T4.1.1.6.4.7.1" style="color:#00FF00;">+0.7</span>)</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T4.1.1.6.4.8">84.1 (<span class="ltx_text" id="S4.T4.1.1.6.4.8.1" style="color:#00FF00;">+0.8</span>)</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T4.1.1.6.4.9">59.5 (<span class="ltx_text" id="S4.T4.1.1.6.4.9.1" style="color:#00FF00;">+3.1</span>)</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Ablation study</h3>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Ablation on RTMW</h4>
<div class="ltx_para" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.1">We tested the impact of each module of RTMW on the performance, and the results are shown in table <a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#S4.T4" title="Table 4 ‣ Inference speed ‣ 4.2 Results ‣ 4 Experiments ‣ RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation"><span class="ltx_text ltx_ref_tag">4</span></a>. The models are trained on COCO-Wholebody <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib12" title="">12</a>]</cite>, Ubody <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib17" title="">17</a>]</cite>, and Halpe <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib6" title="">6</a>]</cite> datasets, and the input shape is <math alttext="256\times 192" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p1.1.m1.1"><semantics id="S4.SS3.SSS1.p1.1.m1.1a"><mrow id="S4.SS3.SSS1.p1.1.m1.1.1" xref="S4.SS3.SSS1.p1.1.m1.1.1.cmml"><mn id="S4.SS3.SSS1.p1.1.m1.1.1.2" xref="S4.SS3.SSS1.p1.1.m1.1.1.2.cmml">256</mn><mo id="S4.SS3.SSS1.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS3.SSS1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS3.SSS1.p1.1.m1.1.1.3" xref="S4.SS3.SSS1.p1.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.1.m1.1b"><apply id="S4.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1"><times id="S4.SS3.SSS1.p1.1.m1.1.1.1.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1.1"></times><cn id="S4.SS3.SSS1.p1.1.m1.1.1.2.cmml" type="integer" xref="S4.SS3.SSS1.p1.1.m1.1.1.2">256</cn><cn id="S4.SS3.SSS1.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.SS3.SSS1.p1.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.1.m1.1c">256\times 192</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p1.1.m1.1d">256 × 192</annotation></semantics></math>. The experimental results indicate that the PAFPN and HEM modules we employed can significantly enhance the prediction accuracy, particularly for body parts like hands and feet with lower input resolutions, where the improvement in predictive accuracy is substantial.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Visualization Results</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#S4.F3" title="Figure 3 ‣ 4.4 Visualization Results ‣ 4 Experiments ‣ RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation"><span class="ltx_text ltx_ref_tag">3</span></a> and Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#S4.F4" title="Figure 4 ‣ 4.4 Visualization Results ‣ 4 Experiments ‣ RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation"><span class="ltx_text ltx_ref_tag">4</span></a> present visual representations of the inference outcomes for the RTMW and RTMW3D models. These illustrations demonstrate that the performance of both RTMW and RTMW3D models is commendably high, corroborating the quantitative assessment data.</p>
</div>
<figure class="ltx_figure" id="S4.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S4.F3.1" style="width:130.1pt;">
<p class="ltx_p ltx_align_center" id="S4.F3.1.1"><span class="ltx_text" id="S4.F3.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="727" id="S4.F3.1.1.1.g1" src="extracted/5724567/figs/hrnet_results/000000001648.jpg" width="538"/></span></p>
<p class="ltx_p ltx_align_center" id="S4.F3.1.2"><span class="ltx_text" id="S4.F3.1.2.1">HRNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib31" title="">31</a>]</cite></span></p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S4.F3.2" style="width:130.1pt;">
<p class="ltx_p ltx_align_center" id="S4.F3.2.1"><span class="ltx_text" id="S4.F3.2.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="727" id="S4.F3.2.1.1.g1" src="extracted/5724567/figs/dw_results/000000001648.jpg" width="538"/></span></p>
<p class="ltx_p ltx_align_center" id="S4.F3.2.2"><span class="ltx_text" id="S4.F3.2.2.1">DWPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.08634v1#bib.bib40" title="">40</a>]</cite></span></p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S4.F3.3" style="width:130.1pt;">
<p class="ltx_p ltx_align_center" id="S4.F3.3.1"><span class="ltx_text" id="S4.F3.3.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="734" id="S4.F3.3.1.1.g1" src="extracted/5724567/figs/rtmw2d_results/000000001648.jpg" width="521"/></span></p>
<p class="ltx_p ltx_align_center" id="S4.F3.3.2"><span class="ltx_text" id="S4.F3.3.2.1">ours</span></p>
</div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>2D visualization results</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="233" id="S4.F4.g1" src="extracted/5724567/figs/rtmw3d_results/060754485.jpg" width="479"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="162" id="S4.F4.g2" src="extracted/5724567/figs/rtmw3d_results/ubody_Olympic_S12_Trim1_000019.png" width="479"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="217" id="S4.F4.g3" src="extracted/5724567/figs/rtmw3d_results/S1_Directions_1.54138969_000001.jpg" width="479"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>RTMW3D inference results (The 2D and 3D keypoints are both obtained from the RTMW3D model after a single inference.)</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">This paper expands the existing scholarly work by critically examining the intricacies and challenges inherent in whole-body pose estimation. Leveraging our established RTMPose model as a foundation, we introduce an enhanced, high-performance model, RTMW/RTMW3D, for real-time whole-body pose estimation. Our model has demonstrated unparalleled performance among all open-source alternatives and possesses a distinct monocular 3D pose estimation capability. We anticipate that the proposed algorithm and its open-source availability will address several practical requirements within the industry for robust pose estimation solutions.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.1.1" style="font-size:90%;">
Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and Bernt Schiele.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.2.1" style="font-size:90%;">2d human pose estimation: New benchmark and state of the art analysis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib1.4.2" style="font-size:90%;">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span><span class="ltx_text" id="bib.bib1.5.3" style="font-size:90%;">, June 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.1.1" style="font-size:90%;">
Yuanhao Cai, Zhicheng Wang, Zhengxiong Luo, Binyi Yin, Angang Du, Haoqian Wang, Xiangyu Zhang, Xinyu Zhou, Erjin Zhou, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.2.1" style="font-size:90%;">Learning delicate local representations for multi-person pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib2.4.2" style="font-size:90%;">ECCV</span><span class="ltx_text" id="bib.bib2.5.3" style="font-size:90%;">, pages 455–472. Springer, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.1.1" style="font-size:90%;">
Z. Cao, G. Hidalgo Martinez, T. Simon, S. Wei, and Y. A. Sheikh.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.2.1" style="font-size:90%;">Openpose: Realtime multi-person 2d pose estimation using part affinity fields.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.3.1" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span class="ltx_text" id="bib.bib3.4.2" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.1.1" style="font-size:90%;">
Wei Cheng, Ruixiang Chen, Wanqi Yin, Siming Fan, Keyu Chen, Honglin He, Huiwen Luo, Zhongang Cai, Jingbo Wang, Yang Gao, Zhengming Yu, Zhengyu Lin, Daxuan Ren, Lei Yang, Ziwei Liu, Chen Change Loy, Chen Qian, Wayne Wu, Dahua Lin, Bo Dai, and Kwan-Yee Lin.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.2.1" style="font-size:90%;">Dna-rendering: A diverse neural actor repository for high-fidelity human-centric rendering.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.3.1" style="font-size:90%;">arXiv preprint</span><span class="ltx_text" id="bib.bib4.4.2" style="font-size:90%;">, arXiv:2307.10173, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.1.1" style="font-size:90%;">
MMPose Contributors.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.2.1" style="font-size:90%;">Openmmlab pose estimation toolbox and benchmark.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/open-mmlab/mmpose" style="font-size:90%;" title="">https://github.com/open-mmlab/mmpose</a><span class="ltx_text" id="bib.bib5.3.1" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.1.1" style="font-size:90%;">
Hao-Shu Fang, Jiefeng Li, Hongyang Tang, Chao Xu, Haoyi Zhu, Yuliang Xiu, Yong-Lu Li, and Cewu Lu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.2.1" style="font-size:90%;">Alphapose: Whole-body regional multi-person pose estimation and tracking in real-time.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.3.1" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span class="ltx_text" id="bib.bib6.4.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.1.1" style="font-size:90%;">
Federico Fulgeri, Matteo Fabbri, Stefano Alletto, Simone Calderara, and Rita Cucchiara.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.2.1" style="font-size:90%;">Can adversarial networks hallucinate occluded people with a plausible aspect?
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.3.1" style="font-size:90%;">arXiv preprint arXiv:1901.08097</span><span class="ltx_text" id="bib.bib7.4.2" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.1.1" style="font-size:90%;">
Junjie Huang, Zheng Zhu, Feng Guo, and Guan Huang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.2.1" style="font-size:90%;">The devil is in the details: Delving into unbiased data processing for human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib8.4.2" style="font-size:90%;">CVPR</span><span class="ltx_text" id="bib.bib8.5.3" style="font-size:90%;">, pages 5700–5709, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.1.1" style="font-size:90%;">
H. Jhuang, J. Gall, S. Zuffi, C. Schmid, and M. J. Black.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.2.1" style="font-size:90%;">Towards understanding action recognition.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib9.4.2" style="font-size:90%;">International Conf. on Computer Vision (ICCV)</span><span class="ltx_text" id="bib.bib9.5.3" style="font-size:90%;">, pages 3192–3199, Dec. 2013.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.1.1" style="font-size:90%;">
Tao Jiang, Peng Lu, Li Zhang, Ningsheng Ma, Rui Han, Chengqi Lyu, Yining Li, and Kai Chen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.2.1" style="font-size:90%;">Rtmpose: Real-time multi-person pose estimation based on mmpose, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.1.1" style="font-size:90%;">
Sheng Jin, Lumin Xu, Jin Xu, Can Wang, Wentao Liu, Chen Qian, Wanli Ouyang, and Ping Luo.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.2.1" style="font-size:90%;">Whole-body human pose estimation in the wild, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.1.1" style="font-size:90%;">
Sheng Jin, Lumin Xu, Jin Xu, Can Wang, Wentao Liu, Chen Qian, Wanli Ouyang, and Ping Luo.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.2.1" style="font-size:90%;">Whole-body human pose estimation in the wild.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib12.4.2" style="font-size:90%;">Proceedings of the European Conference on Computer Vision (ECCV)</span><span class="ltx_text" id="bib.bib12.5.3" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.1.1" style="font-size:90%;">
Xuan Ju, Ailing Zeng, Jianan Wang, Qiang Xu, and Lei Zhang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.2.1" style="font-size:90%;">Human-art: A versatile human-centric dataset bridging natural and artificial scenes.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib13.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib13.5.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.1.1" style="font-size:90%;">
Jiefeng Li, Siyuan Bian, Ailing Zeng, Can Wang, Bo Pang, Wentao Liu, and Cewu Lu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.2.1" style="font-size:90%;">Human pose regression with residual log-likelihood estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib14.4.2" style="font-size:90%;">ICCV</span><span class="ltx_text" id="bib.bib14.5.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.1.1" style="font-size:90%;">
Jiefeng Li, Can Wang, Hao Zhu, Yihuan Mao, Hao-Shu Fang, and Cewu Lu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.2.1" style="font-size:90%;">Crowdpose: Efficient crowded scenes pose estimation and a new benchmark.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib15.3.1" style="font-size:90%;">arXiv preprint arXiv:1812.00324</span><span class="ltx_text" id="bib.bib15.4.2" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.1.1" style="font-size:90%;">
Yanjie Li, Sen Yang, Peidong Liu, Shoukui Zhang, Yunxiao Wang, Zhicheng Wang, Wankou Yang, and Shu-Tao Xia.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.2.1" style="font-size:90%;">Simcc: a simple coordinate classification perspective for human pose estimation, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.1.1" style="font-size:90%;">
Jing Lin, Ailing Zeng, Haoqian Wang, Lei Zhang, and Yu Li.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.2.1" style="font-size:90%;">One-stage 3d whole-body mesh recovery with component aware transformer.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib17.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib17.5.3" style="font-size:90%;">, pages 21159–21168, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.1.1" style="font-size:90%;">
Huajun Liu, Fuqiang Liu, Xinyi Fan, and Dong Huang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.2.1" style="font-size:90%;">Polarized self-attention: towards high-quality pixel-wise regression.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.3.1" style="font-size:90%;">arXiv preprint arXiv:2107.00782</span><span class="ltx_text" id="bib.bib18.4.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.1.1" style="font-size:90%;">
Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.2.1" style="font-size:90%;">Path aggregation network for instance segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib19.4.2" style="font-size:90%;">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span><span class="ltx_text" id="bib.bib19.5.3" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.1.1" style="font-size:90%;">
Yinglu Liu, Hailin Shi, Hao Shen, Yue Si, Xiaobo Wang, and Tao Mei.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.2.1" style="font-size:90%;">A new dataset and boundary-attention semantic segmentation for face parsing.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib20.4.2" style="font-size:90%;">AAAI</span><span class="ltx_text" id="bib.bib20.5.3" style="font-size:90%;">, pages 11637–11644, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.1.1" style="font-size:90%;">
Peng Lu, Tao Jiang, Yining Li, Xiangtai Li, Kai Chen, and Wenming Yang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.2.1" style="font-size:90%;">RTMO: Towards high-performance one-stage real-time multi-person pose estimation, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.1.1" style="font-size:90%;">
Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris McClanahan, Esha Uboweja, Michael Hays, Fan Zhang, Chuo-Ling Chang, Ming Guang Yong, Juhyun Lee, Wan-Teh Chang, Wei Hua, Manfred Georg, and Matthias Grundmann.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.2.1" style="font-size:90%;">Mediapipe: A framework for building perception pipelines, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.1.1" style="font-size:90%;">
Sebastian Lutz, Richard Blythman, Koustav Ghostal, Moynihan Matthew, Ciaran Simms, and Aljosa Smolic.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.2.1" style="font-size:90%;">Jointformer: Single-frame lifting transformer with error prediction and refinement for 3d human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib23.3.1" style="font-size:90%;">26TH International Conference on Pattern Recognition, ICPR 2022</span><span class="ltx_text" id="bib.bib23.4.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.1.1" style="font-size:90%;">
Chengqi Lyu, Wenwei Zhang, Haian Huang, Yue Zhou, Yudong Wang, Yanyi Liu, Shilong Zhang, and Kai Chen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.2.1" style="font-size:90%;">Rtmdet: An empirical study of designing real-time object detectors, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.1.1" style="font-size:90%;">
Weian Mao, Yongtao Ge, Chunhua Shen, Zhi Tian, Xinlong Wang, Zhibin Wang, and Anton van den Hengel.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.2.1" style="font-size:90%;">Poseur: Direct human pose regression with transformers.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib25.4.2" style="font-size:90%;">European Conference on Computer Vision</span><span class="ltx_text" id="bib.bib25.5.3" style="font-size:90%;">, pages 72–88. Springer, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.1.1" style="font-size:90%;">
Julieta Martinez, Rayat Hossain, Javier Romero, and James J. Little.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.2.1" style="font-size:90%;">A simple yet effective baseline for 3d human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib26.4.2" style="font-size:90%;">ICCV</span><span class="ltx_text" id="bib.bib26.5.3" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.1.1" style="font-size:90%;">
Gyeongsik Moon, Shoou-I Yu, He Wen, Takaaki Shiratori, and Kyoung Mu Lee.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.2.1" style="font-size:90%;">Interhand2.6m: A dataset and baseline for 3d interacting hand pose estimation from a single rgb image.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib27.4.2" style="font-size:90%;">European Conference on Computer Vision (ECCV)</span><span class="ltx_text" id="bib.bib27.5.3" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.1.1" style="font-size:90%;">
Georgios Pavlakos, Xiaowei Zhou, Konstantinos G Derpanis, and Kostas Daniilidis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.2.1" style="font-size:90%;">Coarse-to-fine volumetric prediction for single-image 3D human pose.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib28.4.2" style="font-size:90%;">Computer Vision and Pattern Recognition (CVPR)</span><span class="ltx_text" id="bib.bib28.5.3" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.1.1" style="font-size:90%;">
Dario Pavllo, Christoph Feichtenhofer, David Grangier, and Michael Auli.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.2.1" style="font-size:90%;">3d human pose estimation in video with temporal convolutions and semi-supervised training.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib29.4.2" style="font-size:90%;">Conference on Computer Vision and Pattern Recognition (CVPR)</span><span class="ltx_text" id="bib.bib29.5.3" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.1.1" style="font-size:90%;">
Ali Razavi, Aaron van den Oord, and Oriol Vinyals.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.2.1" style="font-size:90%;">Generating diverse high-fidelity images with vq-vae-2, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.1.1" style="font-size:90%;">
Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.2.1" style="font-size:90%;">Deep high-resolution representation learning for human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib31.4.2" style="font-size:90%;">CVPR</span><span class="ltx_text" id="bib.bib31.5.3" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.1.1" style="font-size:90%;">
Xiao Sun, Bin Xiao, Shuang Liang, and Yichen Wei.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.2.1" style="font-size:90%;">Integral human pose regression.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib32.3.1" style="font-size:90%;">arXiv preprint arXiv:1711.08229</span><span class="ltx_text" id="bib.bib32.4.2" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.1.1" style="font-size:90%;">
Alexander Toshev and Christian Szegedy.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.2.1" style="font-size:90%;">Deeppose: Human pose estimation via deep neural networks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib33.4.2" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span><span class="ltx_text" id="bib.bib33.5.3" style="font-size:90%;">, June 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.1.1" style="font-size:90%;">
Bastian Wandt, Marco Rudolph, Petrissa Zell, Helge Rhodin, and Bodo Rosenhahn.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.2.1" style="font-size:90%;">Canonpose: Self-supervised monocular 3d human pose estimation in the wild.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib34.4.2" style="font-size:90%;">Computer Vision and Pattern Recognition (CVPR)</span><span class="ltx_text" id="bib.bib34.5.3" style="font-size:90%;">, June 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.1.1" style="font-size:90%;">
Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.2.1" style="font-size:90%;">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib35.3.1" style="font-size:90%;">arXiv: Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib35.4.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.1.1" style="font-size:90%;">
Wayne Wu, Chen Qian, Shuo Yang, Quan Wang, Yici Cai, and Qiang Zhou.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.2.1" style="font-size:90%;">Look at boundary: A boundary-aware face alignment algorithm.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib36.4.2" style="font-size:90%;">CVPR</span><span class="ltx_text" id="bib.bib36.5.3" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.1.1" style="font-size:90%;">
Bin Xiao, Haiping Wu, and Yichen Wei.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.2.1" style="font-size:90%;">Simple baselines for human pose estimation and tracking.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib37.4.2" style="font-size:90%;">European Conference on Computer Vision (ECCV)</span><span class="ltx_text" id="bib.bib37.5.3" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.1.1" style="font-size:90%;">
Lumin Xu, Sheng Jin, Wentao Liu, Chen Qian, Wanli Ouyang, Ping Luo, and Xiaogang Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.2.1" style="font-size:90%;">Zoomnas: Searching for whole-body human pose estimation in the wild.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib38.3.1" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span class="ltx_text" id="bib.bib38.4.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.1.1" style="font-size:90%;">
Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.2.1" style="font-size:90%;">Vitpose: Simple vision transformer baselines for human pose estimation, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.1.1" style="font-size:90%;">
Zhendong Yang, Ailing Zeng, Chun Yuan, and Yu Li.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.2.1" style="font-size:90%;">Effective whole-body pose estimation with two-stages distillation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib40.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</span><span class="ltx_text" id="bib.bib40.5.3" style="font-size:90%;">, pages 4210–4220, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.1.1" style="font-size:90%;">
Feng Zhang, Xiatian Zhu, Hanbin Dai, Mao Ye, and Ce Zhu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.2.1" style="font-size:90%;">Distribution-aware coordinate representation for human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib41.4.2" style="font-size:90%;">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span><span class="ltx_text" id="bib.bib41.5.3" style="font-size:90%;">, June 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.1.1" style="font-size:90%;">
Wentao Zhu, Xiaoxuan Ma, Zhaoyang Liu, Libin Liu, Wayne Wu, and Yizhou Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.2.1" style="font-size:90%;">Motionbert: A unified perspective on learning human motion representations.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib42.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</span><span class="ltx_text" id="bib.bib42.5.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.1.1" style="font-size:90%;">
Yue Zhu, Nermin Samet, and David Picard.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.2.1" style="font-size:90%;">H3wb: Human3.6m 3d wholebody dataset and benchmark.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib43.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</span><span class="ltx_text" id="bib.bib43.5.3" style="font-size:90%;">, pages 20166–20177, October 2023.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Jul 11 16:03:24 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
