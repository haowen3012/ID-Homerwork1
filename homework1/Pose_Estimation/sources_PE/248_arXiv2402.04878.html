<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation</title>
<!--Generated on Tue Jul 23 14:17:37 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2402.04878v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S1" title="In Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S2" title="In Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S2.SS1" title="In 2 Related Work ‣ Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>2D Object Detection and 6D Pose Estimation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S2.SS2" title="In 2 Related Work ‣ Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Data Representation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S3" title="In Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Induction of Shape Bias</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S4" title="In Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Data Rendering for Inducing a Shape Bias</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S5" title="In Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S5.SS1" title="In 5 Experiments ‣ Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S5.SS1.SSS1" title="In 5.1 Experimental Setup ‣ 5 Experiments ‣ Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.1 </span>Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S5.SS1.SSS2" title="In 5.1 Experimental Setup ‣ 5 Experiments ‣ Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.2 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S5.SS1.SSS3" title="In 5.1 Experimental Setup ‣ 5 Experiments ‣ Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.3 </span>Metrics</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S5.SS2" title="In 5 Experiments ‣ Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>UV-Mapping versus Style Transfer</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S5.SS3" title="In 5 Experiments ‣ Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Object Detection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S5.SS4" title="In 5 Experiments ‣ Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Object Pose Estimation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S5.SS5" title="In 5 Experiments ‣ Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Robustness to Image Perturbations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S5.SS6" title="In 5 Experiments ‣ Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6 </span>Ablative Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S5.SS6.SSS1" title="In 5.6 Ablative Experiments ‣ 5 Experiments ‣ Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6.1 </span>Number of Random Textures</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S5.SS6.SSS2" title="In 5.6 Ablative Experiments ‣ 5 Experiments ‣ Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6.2 </span>Superposition of Shape Bias and Data Augmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S5.SS6.SSS3" title="In 5.6 Ablative Experiments ‣ 5 Experiments ‣ Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6.3 </span>Influence of the Mesh Origin</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S6" title="In Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Peter Hönig<sup class="ltx_sup" id="id1.1.id1">1</sup>  Stefan Thalhammer<sup class="ltx_sup" id="id2.2.id2">2</sup>  Jean-Baptiste Weibel<sup class="ltx_sup" id="id3.3.id3">1</sup>
<br class="ltx_break"/>Matthias Hirschmanner<sup class="ltx_sup" id="id4.4.id4">1</sup>  Markus Vincze<sup class="ltx_sup" id="id5.5.id5">1</sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id6.6.id6">1</sup>TU Wien, <sup class="ltx_sup" id="id7.7.id7">2</sup>UAS Technikum Vienna
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id8.id1">Recent advances in machine learning have greatly benefited object detection and 6D pose estimation.
However, textureless and metallic objects
still pose a significant challenge due to few visual cues and the texture bias of CNNs.
To address this issue, we propose a strategy for inducing a shape bias to CNN training.
In particular, by randomizing textures applied to object surfaces during data rendering, we create training data without consistent textural cues.
This methodology allows for seamless integration into existing data rendering engines, and results in negligible computational overhead for data rendering and network training.
Our findings demonstrate that the shape bias we induce via randomized texturing, improves over existing approaches using style transfer.
We evaluate with three detectors and two pose estimators.
For the most recent object detector and for pose estimation in general, estimation accuracy improves for textureless and metallic objects.
Additionally we show that our approach increases the pose estimation accuracy in the presence of image noise and strong illumination changes.
Code and datasets are publicly available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="github.com/hoenigpeter/randomized_texturing" title="">github.com/hoenigpeter/randomized_texturing</a>.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Object detection and 6D pose estimation are the foundation of robotic manipulation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib1" title="">1</a>]</cite> and scene understanding <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib31" title="">31</a>]</cite>.
Computational perception and reasoning are generally more challenging for textureless and metallic objects, as they have minimal or no texture cues <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib30" title="">30</a>]</cite>.
These objects suffer from inferior detection and pose estimation accuracy due to varying illumination conditions resulting in significantly altered surface appearances.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The standard method for solving this problem is to train with a large amount of synthetic training data, assuming that the data distribution to be expected at runtime is adequately represented <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib35" title="">35</a>]</cite>.
However, this approach does not consider ambiguities caused by the interaction of the light with the object surface. Consequently, pose estimation approaches designed for such challenging materials focus on retrieving geometrical object representations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib36" title="">36</a>]</cite>.
This strategy is supported by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib7" title="">7</a>]</cite>, where the authors demonstrate that CNNs exhibit a texture bias and that ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib29" title="">29</a>]</cite> classification improves when a shape bias is induced.
This fact actually exacerbates the aforementioned phenomenon of increased perception difficulty when processing textureless and metallic objects.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="210" id="S1.F1.g1" src="extracted/5750059/images/improvementpic.jpg" width="299"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.3.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text ltx_font_bold" id="S1.F1.4.2" style="font-size:90%;">Induction of Shape Bias.<span class="ltx_text ltx_font_medium" id="S1.F1.4.2.1"> 6D pose of a textureless object is visualized with 3D bounding boxes; the ground truth (blue) and estimate (green) of GDR-Net trained with conventional data (top) and when inducing a shape bias (bottom).</span></span></figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">We show that inducing a shape bias during training will be particularly effective for textureless and metallic objects.
For this purpose we conceptualize texture randomization as a UV-mapping approach.
The surface textures of objects are randomized to render scene-level training data.
This approach offers three key advantages over <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib7" title="">7</a>]</cite>:
a) UV-mapping of the texture provides geometrically accurate visible cues for observing the object geometry, b) since encoded representations are agnostic to texture, the trained models are more robust with respect to changes in appearance due to illumination, and c) training data is effectively repurposed for multiple vision problems, such as detection, segmentation, and pose estimation.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Experimental results demonstrate that our strategy improves over <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib7" title="">7</a>]</cite> for the classification of textureless objects.
Experiments were conducted with three object detectors, YOLOx <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib6" title="">6</a>]</cite>, Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib28" title="">28</a>]</cite>, and RetinaNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib23" title="">23</a>]</cite>, and with two pose estimators, GDR-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib35" title="">35</a>]</cite> and Pix2Pose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib27" title="">27</a>]</cite>. The experiments provide precise information regarding the improvements and limitations for textureless and metallic objects.
Furthermore, experiments are presented showing enhanced robustness against image perturbations and regarding the object mesh origin.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">We summarize our contributions as follows.</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We show that the induction of a shape bias to learned representations through the application of our UV-mapping approach improves textureless object classification outperforming style transfer.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">The shape bias can be applied to any detection and pose estimation method and we report improvements for three object detection and two pose estimation approaches for textureless and metallic objects.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Our strategy provides a partial alleviation for the need for online data augmentation. In particular, when recognizing textureless objects with YOLOx, the shape distortion leads to an object recognition accuracy comparable to that achieved by a grid search using the hyperparameters of the online data augmentation.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">The induction of the shape bias also robustifies the estimation of the object pose against typical real-world image perturbations, such as noise and significant illumination alterations.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">The paper is structured as follows: Section <a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S2" title="2 Related Work ‣ Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_tag">2</span></a> provides an overview of related literature, Section <a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S3" title="3 Induction of Shape Bias ‣ Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_tag">3</span></a> introduces the rationale behind the induction of a shape bias, followed by the comprehensive description of the implementation and the experimental setup. Section <a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S5" title="5 Experiments ‣ Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_tag">5</span></a> presents the empirical evidence supporting our findings, while Section <a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S6" title="6 Conclusion ‣ Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_tag">6</span></a> summarizes the observations made and outlines future work.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">This section summarizes the state of the art for object detection, pose estimation, and data representation.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>2D Object Detection and 6D Pose Estimation</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.4">Throughout this paper, the term object detection defines the 2D detection of an object, defined by the class label <math alttext="c" class="ltx_Math" display="inline" id="S2.SS1.p1.1.m1.1"><semantics id="S2.SS1.p1.1.m1.1a"><mi id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">c</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.1.m1.1d">italic_c</annotation></semantics></math> and bounding box <math alttext="\mathbf{b}" class="ltx_Math" display="inline" id="S2.SS1.p1.2.m2.1"><semantics id="S2.SS1.p1.2.m2.1a"><mi id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml">𝐛</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><ci id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">𝐛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">\mathbf{b}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.2.m2.1d">bold_b</annotation></semantics></math>.
Pose estimation refers to the instance-level 6D pose estimation of an object, defined by the translation <math alttext="\mathbf{T}" class="ltx_Math" display="inline" id="S2.SS1.p1.3.m3.1"><semantics id="S2.SS1.p1.3.m3.1a"><mi id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml">𝐓</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><ci id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">𝐓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">\mathbf{T}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.3.m3.1d">bold_T</annotation></semantics></math> and rotation <math alttext="\mathbf{R}" class="ltx_Math" display="inline" id="S2.SS1.p1.4.m4.1"><semantics id="S2.SS1.p1.4.m4.1a"><mi id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml">𝐑</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><ci id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1">𝐑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">\mathbf{R}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.4.m4.1d">bold_R</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">The Benchmark for 6D Object Pose Estimation (BOP) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib30" title="">30</a>]</cite> challenge ranks the best-performing object detection and pose estimation methods, evaluated with standardized metrics and datasets.
Successful object detectors from the last years’ challenges include two-stage models such as Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib9" title="">9</a>]</cite> and Faster R-CNN, and single-stage ones, e.g. YOLOx.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">Pose estimators are either composed of a single stage <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib32" title="">32</a>]</cite> or multiple stages <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib35" title="">35</a>]</cite>, where subsequent stages build upon location priors, provided by arbitrary object detectors.
For each location prior, the subsequent pose estimator performs feature extraction, learns 2D-3D correspondences, and regresses translation and rotation of the object pose.
Successful CNN-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib27" title="">27</a>]</cite> from the last years’ challenges use multi-stage approaches employing one of the abovementioned object detectors.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Data Representation</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">State-of-the-art benchmarking datasets for object detection and pose estimation of textureless and metallic objects include TLESS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib16" title="">16</a>]</cite> and ITODD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib4" title="">4</a>]</cite> which consist of physically-based-rendered (PBR) training sets and real-world test sets.
Training with synthetic and evaluating on real-world data leaves a domain gap <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib34" title="">34</a>]</cite>.
Data rendering and model training offer different techniques for bridging this domain gap.
During rendering, domain randomization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib34" title="">34</a>]</cite> can be applied, which includes the variation of backgrounds, camera views, object poses, and lighting.
After rendering, during the training procedure data augmentation can be applied online, either with pre-trained augmentations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib2" title="">2</a>]</cite>, style-transfer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib8" title="">8</a>]</cite> or handcrafted combinations of varying image perturbation types.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">The domain gap is also influenced by the quality of the available object geometry and texture.
Depending on the target domain, meshes for the datasets are either provided via CAD models or reconstructions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib37" title="">37</a>]</cite>.
TLESS and ITODD, for example, feature plastic and metallic objects for industrial purposes, where CAD models are usually available.
CAD models are geometrically accurate but often lack texture information, while reconstructions have added texture information but less geometric accuracy due to reconstruction noise.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">While existing methods for bridging the domain gap focus on textured objects with distinct visual cues, less attention has been paid to textureless and metallic objects.
Since lighting, reflections and shadows alter the appearance of these objects, they do not have continuous textures in the real-world domain.
We hypothesize that texture should be treated as an unknown and object detection as well as pose estimation models should be biased toward shape.
We therefore want to explore the randomization of object textures, to force CNNs to learn exclusively from object geometries from 2D RGB images only.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="223" id="S2.F2.g1" src="extracted/5750059/images/pipeline.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.3.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text ltx_font_bold" id="S2.F2.4.2" style="font-size:90%;">Synthetic Data Generation with Randomized Texturing Pipeline.<span class="ltx_text ltx_font_medium" id="S2.F2.4.2.1"> Instead of sampling color values from a uniform color distribution we download textures from a creative-commons database and perform low-cost UV-mapping, a method that we call texture randomization. While the figure shows the scene composition and rendering steps with five exemplary textures, the texture randomization method can be used with an arbitrary number of texture files.</span></span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Induction of Shape Bias</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">This section presents the rationale of biasing representation learning toward shape.
The authors of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib7" title="">7</a>]</cite> present evidence that CNNs exhibit a bias toward texture.
By employing style transfer with AdaIN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib5" title="">5</a>]</cite> the learned representations of the network are shape biased, thereby enhancing classification on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib29" title="">29</a>]</cite>.
Particularly for textureless and metallic objects, where the texture is sparse the phenomenon of texture bias is disadvantageous.
This has also been observed in the context of object pose estimation for textureless and metallic objects.
As a result, these objects are typically handled by estimating their shape, for example by estimating edges and silhouettes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib36" title="">36</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">The capacity of networks to generalize from synthetic to real object primitives with random color has been demonstrated by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib34" title="">34</a>]</cite>.
The authors consider various scene composition parameters, including background texture and camera, object, and light source poses as random variables.
Similarly <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib3" title="">3</a>]</cite> observed that replacing the scene background with textures is an effective method for foreground-background separation in various vision problems.
This strategy is considered the standard for creating training data for object pose estimation.
We hypothesize that the application of random textures to objects themselves results in encoding the object shape in feature spaces, a process analog to that observed by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib7" title="">7</a>]</cite>.
This hypothesis is tested through the formulation of a learning strategy that biases estimators toward shape through rendering, thus:</p>
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">Encoding geometrically accurate cues.</span> Localization tasks, such as object detection and pose estimation, are susceptible to geometry and illumination effects such as shadows <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib20" title="">20</a>]</cite>. Rendering allows us to accurately model and control the distribution of such effects.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">Being robust against strong surface appearance changes.</span> Metallic objects present a particular challenging case as their appearance is strongly influenced by the nature of the incident light. By learning shape representations, it is possible to achieve a certain degree of agnosticism with regard to surface effects.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.1">Obtaining training data for different problems.</span> Although the authors of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib7" title="">7</a>]</cite> demonstrate that style transfer is a viable strategy for object classification, we present a strategy for inducing a shape bias for classification, detection and pose estimation. While not yet tested, our strategy can theoretically be applied to the same set of vision problems as that of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib3" title="">3</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i4.p1.1.1">Being more universally applicable.</span> A significant limitation of instance-level approaches is that the trained models lack the capacity to generalize effectively to changes of the object. While this remains unsolved, our approach offers a degree of mitigation. As our trained models are color and texture agnostic, we are able to accommodate instances where the object color is unknown.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">Next, we present the experimental setup and implementation detail to validate our hypothesis.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.9.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text ltx_font_bold" id="S3.T1.10.2" style="font-size:90%;">Domain Randomization Overview.<span class="ltx_text ltx_font_medium" id="S3.T1.10.2.1"> Parameters and functions denoting the rendering specifications for the TLESS and ITODD objects</span></span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.6.7.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.6.7.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.6.7.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.6.7.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.6.7.1.2.1">Parameter</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.6.7.1.3"><span class="ltx_text ltx_font_bold" id="S3.T1.6.7.1.3.1">Function</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.2" rowspan="3"><span class="ltx_text" id="S3.T1.1.1.2.1">
<span class="ltx_inline-block" id="S3.T1.1.1.2.1.1">
<span class="ltx_p" id="S3.T1.1.1.2.1.1.1">Texture</span>
<span class="ltx_p" id="S3.T1.1.1.2.1.1.2">Bias</span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.3">Specularity</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1"><math alttext="S\sim\mathcal{U}(0,1)" class="ltx_Math" display="inline" id="S3.T1.1.1.1.m1.2"><semantics id="S3.T1.1.1.1.m1.2a"><mrow id="S3.T1.1.1.1.m1.2.3" xref="S3.T1.1.1.1.m1.2.3.cmml"><mi id="S3.T1.1.1.1.m1.2.3.2" xref="S3.T1.1.1.1.m1.2.3.2.cmml">S</mi><mo id="S3.T1.1.1.1.m1.2.3.1" xref="S3.T1.1.1.1.m1.2.3.1.cmml">∼</mo><mrow id="S3.T1.1.1.1.m1.2.3.3" xref="S3.T1.1.1.1.m1.2.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.T1.1.1.1.m1.2.3.3.2" xref="S3.T1.1.1.1.m1.2.3.3.2.cmml">𝒰</mi><mo id="S3.T1.1.1.1.m1.2.3.3.1" xref="S3.T1.1.1.1.m1.2.3.3.1.cmml">⁢</mo><mrow id="S3.T1.1.1.1.m1.2.3.3.3.2" xref="S3.T1.1.1.1.m1.2.3.3.3.1.cmml"><mo id="S3.T1.1.1.1.m1.2.3.3.3.2.1" stretchy="false" xref="S3.T1.1.1.1.m1.2.3.3.3.1.cmml">(</mo><mn id="S3.T1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.m1.1.1.cmml">0</mn><mo id="S3.T1.1.1.1.m1.2.3.3.3.2.2" xref="S3.T1.1.1.1.m1.2.3.3.3.1.cmml">,</mo><mn id="S3.T1.1.1.1.m1.2.2" xref="S3.T1.1.1.1.m1.2.2.cmml">1</mn><mo id="S3.T1.1.1.1.m1.2.3.3.3.2.3" stretchy="false" xref="S3.T1.1.1.1.m1.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.m1.2b"><apply id="S3.T1.1.1.1.m1.2.3.cmml" xref="S3.T1.1.1.1.m1.2.3"><csymbol cd="latexml" id="S3.T1.1.1.1.m1.2.3.1.cmml" xref="S3.T1.1.1.1.m1.2.3.1">similar-to</csymbol><ci id="S3.T1.1.1.1.m1.2.3.2.cmml" xref="S3.T1.1.1.1.m1.2.3.2">𝑆</ci><apply id="S3.T1.1.1.1.m1.2.3.3.cmml" xref="S3.T1.1.1.1.m1.2.3.3"><times id="S3.T1.1.1.1.m1.2.3.3.1.cmml" xref="S3.T1.1.1.1.m1.2.3.3.1"></times><ci id="S3.T1.1.1.1.m1.2.3.3.2.cmml" xref="S3.T1.1.1.1.m1.2.3.3.2">𝒰</ci><interval closure="open" id="S3.T1.1.1.1.m1.2.3.3.3.1.cmml" xref="S3.T1.1.1.1.m1.2.3.3.3.2"><cn id="S3.T1.1.1.1.m1.1.1.cmml" type="integer" xref="S3.T1.1.1.1.m1.1.1">0</cn><cn id="S3.T1.1.1.1.m1.2.2.cmml" type="integer" xref="S3.T1.1.1.1.m1.2.2">1</cn></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.m1.2c">S\sim\mathcal{U}(0,1)</annotation><annotation encoding="application/x-llamapun" id="S3.T1.1.1.1.m1.2d">italic_S ∼ caligraphic_U ( 0 , 1 )</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.2">
<td class="ltx_td ltx_align_center" id="S3.T1.2.2.2">Roughness</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.2.1"><math alttext="R\sim\mathcal{U}(0,1)" class="ltx_Math" display="inline" id="S3.T1.2.2.1.m1.2"><semantics id="S3.T1.2.2.1.m1.2a"><mrow id="S3.T1.2.2.1.m1.2.3" xref="S3.T1.2.2.1.m1.2.3.cmml"><mi id="S3.T1.2.2.1.m1.2.3.2" xref="S3.T1.2.2.1.m1.2.3.2.cmml">R</mi><mo id="S3.T1.2.2.1.m1.2.3.1" xref="S3.T1.2.2.1.m1.2.3.1.cmml">∼</mo><mrow id="S3.T1.2.2.1.m1.2.3.3" xref="S3.T1.2.2.1.m1.2.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.T1.2.2.1.m1.2.3.3.2" xref="S3.T1.2.2.1.m1.2.3.3.2.cmml">𝒰</mi><mo id="S3.T1.2.2.1.m1.2.3.3.1" xref="S3.T1.2.2.1.m1.2.3.3.1.cmml">⁢</mo><mrow id="S3.T1.2.2.1.m1.2.3.3.3.2" xref="S3.T1.2.2.1.m1.2.3.3.3.1.cmml"><mo id="S3.T1.2.2.1.m1.2.3.3.3.2.1" stretchy="false" xref="S3.T1.2.2.1.m1.2.3.3.3.1.cmml">(</mo><mn id="S3.T1.2.2.1.m1.1.1" xref="S3.T1.2.2.1.m1.1.1.cmml">0</mn><mo id="S3.T1.2.2.1.m1.2.3.3.3.2.2" xref="S3.T1.2.2.1.m1.2.3.3.3.1.cmml">,</mo><mn id="S3.T1.2.2.1.m1.2.2" xref="S3.T1.2.2.1.m1.2.2.cmml">1</mn><mo id="S3.T1.2.2.1.m1.2.3.3.3.2.3" stretchy="false" xref="S3.T1.2.2.1.m1.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.1.m1.2b"><apply id="S3.T1.2.2.1.m1.2.3.cmml" xref="S3.T1.2.2.1.m1.2.3"><csymbol cd="latexml" id="S3.T1.2.2.1.m1.2.3.1.cmml" xref="S3.T1.2.2.1.m1.2.3.1">similar-to</csymbol><ci id="S3.T1.2.2.1.m1.2.3.2.cmml" xref="S3.T1.2.2.1.m1.2.3.2">𝑅</ci><apply id="S3.T1.2.2.1.m1.2.3.3.cmml" xref="S3.T1.2.2.1.m1.2.3.3"><times id="S3.T1.2.2.1.m1.2.3.3.1.cmml" xref="S3.T1.2.2.1.m1.2.3.3.1"></times><ci id="S3.T1.2.2.1.m1.2.3.3.2.cmml" xref="S3.T1.2.2.1.m1.2.3.3.2">𝒰</ci><interval closure="open" id="S3.T1.2.2.1.m1.2.3.3.3.1.cmml" xref="S3.T1.2.2.1.m1.2.3.3.3.2"><cn id="S3.T1.2.2.1.m1.1.1.cmml" type="integer" xref="S3.T1.2.2.1.m1.1.1">0</cn><cn id="S3.T1.2.2.1.m1.2.2.cmml" type="integer" xref="S3.T1.2.2.1.m1.2.2">1</cn></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.1.m1.2c">R\sim\mathcal{U}(0,1)</annotation><annotation encoding="application/x-llamapun" id="S3.T1.2.2.1.m1.2d">italic_R ∼ caligraphic_U ( 0 , 1 )</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.3">
<td class="ltx_td ltx_align_center" id="S3.T1.3.3.2">Color</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.3.1"><math alttext="(R,G,B)\sim\mathcal{U}(0.1,0.9)" class="ltx_Math" display="inline" id="S3.T1.3.3.1.m1.5"><semantics id="S3.T1.3.3.1.m1.5a"><mrow id="S3.T1.3.3.1.m1.5.6" xref="S3.T1.3.3.1.m1.5.6.cmml"><mrow id="S3.T1.3.3.1.m1.5.6.2.2" xref="S3.T1.3.3.1.m1.5.6.2.1.cmml"><mo id="S3.T1.3.3.1.m1.5.6.2.2.1" stretchy="false" xref="S3.T1.3.3.1.m1.5.6.2.1.cmml">(</mo><mi id="S3.T1.3.3.1.m1.1.1" xref="S3.T1.3.3.1.m1.1.1.cmml">R</mi><mo id="S3.T1.3.3.1.m1.5.6.2.2.2" xref="S3.T1.3.3.1.m1.5.6.2.1.cmml">,</mo><mi id="S3.T1.3.3.1.m1.2.2" xref="S3.T1.3.3.1.m1.2.2.cmml">G</mi><mo id="S3.T1.3.3.1.m1.5.6.2.2.3" xref="S3.T1.3.3.1.m1.5.6.2.1.cmml">,</mo><mi id="S3.T1.3.3.1.m1.3.3" xref="S3.T1.3.3.1.m1.3.3.cmml">B</mi><mo id="S3.T1.3.3.1.m1.5.6.2.2.4" stretchy="false" xref="S3.T1.3.3.1.m1.5.6.2.1.cmml">)</mo></mrow><mo id="S3.T1.3.3.1.m1.5.6.1" xref="S3.T1.3.3.1.m1.5.6.1.cmml">∼</mo><mrow id="S3.T1.3.3.1.m1.5.6.3" xref="S3.T1.3.3.1.m1.5.6.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.T1.3.3.1.m1.5.6.3.2" xref="S3.T1.3.3.1.m1.5.6.3.2.cmml">𝒰</mi><mo id="S3.T1.3.3.1.m1.5.6.3.1" xref="S3.T1.3.3.1.m1.5.6.3.1.cmml">⁢</mo><mrow id="S3.T1.3.3.1.m1.5.6.3.3.2" xref="S3.T1.3.3.1.m1.5.6.3.3.1.cmml"><mo id="S3.T1.3.3.1.m1.5.6.3.3.2.1" stretchy="false" xref="S3.T1.3.3.1.m1.5.6.3.3.1.cmml">(</mo><mn id="S3.T1.3.3.1.m1.4.4" xref="S3.T1.3.3.1.m1.4.4.cmml">0.1</mn><mo id="S3.T1.3.3.1.m1.5.6.3.3.2.2" xref="S3.T1.3.3.1.m1.5.6.3.3.1.cmml">,</mo><mn id="S3.T1.3.3.1.m1.5.5" xref="S3.T1.3.3.1.m1.5.5.cmml">0.9</mn><mo id="S3.T1.3.3.1.m1.5.6.3.3.2.3" stretchy="false" xref="S3.T1.3.3.1.m1.5.6.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.1.m1.5b"><apply id="S3.T1.3.3.1.m1.5.6.cmml" xref="S3.T1.3.3.1.m1.5.6"><csymbol cd="latexml" id="S3.T1.3.3.1.m1.5.6.1.cmml" xref="S3.T1.3.3.1.m1.5.6.1">similar-to</csymbol><vector id="S3.T1.3.3.1.m1.5.6.2.1.cmml" xref="S3.T1.3.3.1.m1.5.6.2.2"><ci id="S3.T1.3.3.1.m1.1.1.cmml" xref="S3.T1.3.3.1.m1.1.1">𝑅</ci><ci id="S3.T1.3.3.1.m1.2.2.cmml" xref="S3.T1.3.3.1.m1.2.2">𝐺</ci><ci id="S3.T1.3.3.1.m1.3.3.cmml" xref="S3.T1.3.3.1.m1.3.3">𝐵</ci></vector><apply id="S3.T1.3.3.1.m1.5.6.3.cmml" xref="S3.T1.3.3.1.m1.5.6.3"><times id="S3.T1.3.3.1.m1.5.6.3.1.cmml" xref="S3.T1.3.3.1.m1.5.6.3.1"></times><ci id="S3.T1.3.3.1.m1.5.6.3.2.cmml" xref="S3.T1.3.3.1.m1.5.6.3.2">𝒰</ci><interval closure="open" id="S3.T1.3.3.1.m1.5.6.3.3.1.cmml" xref="S3.T1.3.3.1.m1.5.6.3.3.2"><cn id="S3.T1.3.3.1.m1.4.4.cmml" type="float" xref="S3.T1.3.3.1.m1.4.4">0.1</cn><cn id="S3.T1.3.3.1.m1.5.5.cmml" type="float" xref="S3.T1.3.3.1.m1.5.5">0.9</cn></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.1.m1.5c">(R,G,B)\sim\mathcal{U}(0.1,0.9)</annotation><annotation encoding="application/x-llamapun" id="S3.T1.3.3.1.m1.5d">( italic_R , italic_G , italic_B ) ∼ caligraphic_U ( 0.1 , 0.9 )</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.4">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.4.4.2" rowspan="3"><span class="ltx_text" id="S3.T1.4.4.2.1">
<span class="ltx_inline-block" id="S3.T1.4.4.2.1.1">
<span class="ltx_p" id="S3.T1.4.4.2.1.1.1">Shape</span>
<span class="ltx_p" id="S3.T1.4.4.2.1.1.2">Bias (ours)</span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.4.4.3">Specularity</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.4.4.1"><math alttext="S\sim\mathcal{U}(0,1)" class="ltx_Math" display="inline" id="S3.T1.4.4.1.m1.2"><semantics id="S3.T1.4.4.1.m1.2a"><mrow id="S3.T1.4.4.1.m1.2.3" xref="S3.T1.4.4.1.m1.2.3.cmml"><mi id="S3.T1.4.4.1.m1.2.3.2" xref="S3.T1.4.4.1.m1.2.3.2.cmml">S</mi><mo id="S3.T1.4.4.1.m1.2.3.1" xref="S3.T1.4.4.1.m1.2.3.1.cmml">∼</mo><mrow id="S3.T1.4.4.1.m1.2.3.3" xref="S3.T1.4.4.1.m1.2.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.T1.4.4.1.m1.2.3.3.2" xref="S3.T1.4.4.1.m1.2.3.3.2.cmml">𝒰</mi><mo id="S3.T1.4.4.1.m1.2.3.3.1" xref="S3.T1.4.4.1.m1.2.3.3.1.cmml">⁢</mo><mrow id="S3.T1.4.4.1.m1.2.3.3.3.2" xref="S3.T1.4.4.1.m1.2.3.3.3.1.cmml"><mo id="S3.T1.4.4.1.m1.2.3.3.3.2.1" stretchy="false" xref="S3.T1.4.4.1.m1.2.3.3.3.1.cmml">(</mo><mn id="S3.T1.4.4.1.m1.1.1" xref="S3.T1.4.4.1.m1.1.1.cmml">0</mn><mo id="S3.T1.4.4.1.m1.2.3.3.3.2.2" xref="S3.T1.4.4.1.m1.2.3.3.3.1.cmml">,</mo><mn id="S3.T1.4.4.1.m1.2.2" xref="S3.T1.4.4.1.m1.2.2.cmml">1</mn><mo id="S3.T1.4.4.1.m1.2.3.3.3.2.3" stretchy="false" xref="S3.T1.4.4.1.m1.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.1.m1.2b"><apply id="S3.T1.4.4.1.m1.2.3.cmml" xref="S3.T1.4.4.1.m1.2.3"><csymbol cd="latexml" id="S3.T1.4.4.1.m1.2.3.1.cmml" xref="S3.T1.4.4.1.m1.2.3.1">similar-to</csymbol><ci id="S3.T1.4.4.1.m1.2.3.2.cmml" xref="S3.T1.4.4.1.m1.2.3.2">𝑆</ci><apply id="S3.T1.4.4.1.m1.2.3.3.cmml" xref="S3.T1.4.4.1.m1.2.3.3"><times id="S3.T1.4.4.1.m1.2.3.3.1.cmml" xref="S3.T1.4.4.1.m1.2.3.3.1"></times><ci id="S3.T1.4.4.1.m1.2.3.3.2.cmml" xref="S3.T1.4.4.1.m1.2.3.3.2">𝒰</ci><interval closure="open" id="S3.T1.4.4.1.m1.2.3.3.3.1.cmml" xref="S3.T1.4.4.1.m1.2.3.3.3.2"><cn id="S3.T1.4.4.1.m1.1.1.cmml" type="integer" xref="S3.T1.4.4.1.m1.1.1">0</cn><cn id="S3.T1.4.4.1.m1.2.2.cmml" type="integer" xref="S3.T1.4.4.1.m1.2.2">1</cn></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.1.m1.2c">S\sim\mathcal{U}(0,1)</annotation><annotation encoding="application/x-llamapun" id="S3.T1.4.4.1.m1.2d">italic_S ∼ caligraphic_U ( 0 , 1 )</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S3.T1.5.5">
<td class="ltx_td ltx_align_center" id="S3.T1.5.5.2">Roughness</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.5.1"><math alttext="R\sim\mathcal{U}(0,1)" class="ltx_Math" display="inline" id="S3.T1.5.5.1.m1.2"><semantics id="S3.T1.5.5.1.m1.2a"><mrow id="S3.T1.5.5.1.m1.2.3" xref="S3.T1.5.5.1.m1.2.3.cmml"><mi id="S3.T1.5.5.1.m1.2.3.2" xref="S3.T1.5.5.1.m1.2.3.2.cmml">R</mi><mo id="S3.T1.5.5.1.m1.2.3.1" xref="S3.T1.5.5.1.m1.2.3.1.cmml">∼</mo><mrow id="S3.T1.5.5.1.m1.2.3.3" xref="S3.T1.5.5.1.m1.2.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.T1.5.5.1.m1.2.3.3.2" xref="S3.T1.5.5.1.m1.2.3.3.2.cmml">𝒰</mi><mo id="S3.T1.5.5.1.m1.2.3.3.1" xref="S3.T1.5.5.1.m1.2.3.3.1.cmml">⁢</mo><mrow id="S3.T1.5.5.1.m1.2.3.3.3.2" xref="S3.T1.5.5.1.m1.2.3.3.3.1.cmml"><mo id="S3.T1.5.5.1.m1.2.3.3.3.2.1" stretchy="false" xref="S3.T1.5.5.1.m1.2.3.3.3.1.cmml">(</mo><mn id="S3.T1.5.5.1.m1.1.1" xref="S3.T1.5.5.1.m1.1.1.cmml">0</mn><mo id="S3.T1.5.5.1.m1.2.3.3.3.2.2" xref="S3.T1.5.5.1.m1.2.3.3.3.1.cmml">,</mo><mn id="S3.T1.5.5.1.m1.2.2" xref="S3.T1.5.5.1.m1.2.2.cmml">1</mn><mo id="S3.T1.5.5.1.m1.2.3.3.3.2.3" stretchy="false" xref="S3.T1.5.5.1.m1.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.1.m1.2b"><apply id="S3.T1.5.5.1.m1.2.3.cmml" xref="S3.T1.5.5.1.m1.2.3"><csymbol cd="latexml" id="S3.T1.5.5.1.m1.2.3.1.cmml" xref="S3.T1.5.5.1.m1.2.3.1">similar-to</csymbol><ci id="S3.T1.5.5.1.m1.2.3.2.cmml" xref="S3.T1.5.5.1.m1.2.3.2">𝑅</ci><apply id="S3.T1.5.5.1.m1.2.3.3.cmml" xref="S3.T1.5.5.1.m1.2.3.3"><times id="S3.T1.5.5.1.m1.2.3.3.1.cmml" xref="S3.T1.5.5.1.m1.2.3.3.1"></times><ci id="S3.T1.5.5.1.m1.2.3.3.2.cmml" xref="S3.T1.5.5.1.m1.2.3.3.2">𝒰</ci><interval closure="open" id="S3.T1.5.5.1.m1.2.3.3.3.1.cmml" xref="S3.T1.5.5.1.m1.2.3.3.3.2"><cn id="S3.T1.5.5.1.m1.1.1.cmml" type="integer" xref="S3.T1.5.5.1.m1.1.1">0</cn><cn id="S3.T1.5.5.1.m1.2.2.cmml" type="integer" xref="S3.T1.5.5.1.m1.2.2">1</cn></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.1.m1.2c">R\sim\mathcal{U}(0,1)</annotation><annotation encoding="application/x-llamapun" id="S3.T1.5.5.1.m1.2d">italic_R ∼ caligraphic_U ( 0 , 1 )</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.6">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.6.6.2">Color</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.6.6.1"><math alttext="(R,G,B)=T_{n\sim\mathcal{U}(1,1226)}" class="ltx_Math" display="inline" id="S3.T1.6.6.1.m1.5"><semantics id="S3.T1.6.6.1.m1.5a"><mrow id="S3.T1.6.6.1.m1.5.6" xref="S3.T1.6.6.1.m1.5.6.cmml"><mrow id="S3.T1.6.6.1.m1.5.6.2.2" xref="S3.T1.6.6.1.m1.5.6.2.1.cmml"><mo id="S3.T1.6.6.1.m1.5.6.2.2.1" stretchy="false" xref="S3.T1.6.6.1.m1.5.6.2.1.cmml">(</mo><mi id="S3.T1.6.6.1.m1.3.3" xref="S3.T1.6.6.1.m1.3.3.cmml">R</mi><mo id="S3.T1.6.6.1.m1.5.6.2.2.2" xref="S3.T1.6.6.1.m1.5.6.2.1.cmml">,</mo><mi id="S3.T1.6.6.1.m1.4.4" xref="S3.T1.6.6.1.m1.4.4.cmml">G</mi><mo id="S3.T1.6.6.1.m1.5.6.2.2.3" xref="S3.T1.6.6.1.m1.5.6.2.1.cmml">,</mo><mi id="S3.T1.6.6.1.m1.5.5" xref="S3.T1.6.6.1.m1.5.5.cmml">B</mi><mo id="S3.T1.6.6.1.m1.5.6.2.2.4" stretchy="false" xref="S3.T1.6.6.1.m1.5.6.2.1.cmml">)</mo></mrow><mo id="S3.T1.6.6.1.m1.5.6.1" xref="S3.T1.6.6.1.m1.5.6.1.cmml">=</mo><msub id="S3.T1.6.6.1.m1.5.6.3" xref="S3.T1.6.6.1.m1.5.6.3.cmml"><mi id="S3.T1.6.6.1.m1.5.6.3.2" xref="S3.T1.6.6.1.m1.5.6.3.2.cmml">T</mi><mrow id="S3.T1.6.6.1.m1.2.2.2" xref="S3.T1.6.6.1.m1.2.2.2.cmml"><mi id="S3.T1.6.6.1.m1.2.2.2.4" xref="S3.T1.6.6.1.m1.2.2.2.4.cmml">n</mi><mo id="S3.T1.6.6.1.m1.2.2.2.3" xref="S3.T1.6.6.1.m1.2.2.2.3.cmml">∼</mo><mrow id="S3.T1.6.6.1.m1.2.2.2.5" xref="S3.T1.6.6.1.m1.2.2.2.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.T1.6.6.1.m1.2.2.2.5.2" xref="S3.T1.6.6.1.m1.2.2.2.5.2.cmml">𝒰</mi><mo id="S3.T1.6.6.1.m1.2.2.2.5.1" xref="S3.T1.6.6.1.m1.2.2.2.5.1.cmml">⁢</mo><mrow id="S3.T1.6.6.1.m1.2.2.2.5.3.2" xref="S3.T1.6.6.1.m1.2.2.2.5.3.1.cmml"><mo id="S3.T1.6.6.1.m1.2.2.2.5.3.2.1" stretchy="false" xref="S3.T1.6.6.1.m1.2.2.2.5.3.1.cmml">(</mo><mn id="S3.T1.6.6.1.m1.1.1.1.1" xref="S3.T1.6.6.1.m1.1.1.1.1.cmml">1</mn><mo id="S3.T1.6.6.1.m1.2.2.2.5.3.2.2" xref="S3.T1.6.6.1.m1.2.2.2.5.3.1.cmml">,</mo><mn id="S3.T1.6.6.1.m1.2.2.2.2" xref="S3.T1.6.6.1.m1.2.2.2.2.cmml">1226</mn><mo id="S3.T1.6.6.1.m1.2.2.2.5.3.2.3" stretchy="false" xref="S3.T1.6.6.1.m1.2.2.2.5.3.1.cmml">)</mo></mrow></mrow></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.6.6.1.m1.5b"><apply id="S3.T1.6.6.1.m1.5.6.cmml" xref="S3.T1.6.6.1.m1.5.6"><eq id="S3.T1.6.6.1.m1.5.6.1.cmml" xref="S3.T1.6.6.1.m1.5.6.1"></eq><vector id="S3.T1.6.6.1.m1.5.6.2.1.cmml" xref="S3.T1.6.6.1.m1.5.6.2.2"><ci id="S3.T1.6.6.1.m1.3.3.cmml" xref="S3.T1.6.6.1.m1.3.3">𝑅</ci><ci id="S3.T1.6.6.1.m1.4.4.cmml" xref="S3.T1.6.6.1.m1.4.4">𝐺</ci><ci id="S3.T1.6.6.1.m1.5.5.cmml" xref="S3.T1.6.6.1.m1.5.5">𝐵</ci></vector><apply id="S3.T1.6.6.1.m1.5.6.3.cmml" xref="S3.T1.6.6.1.m1.5.6.3"><csymbol cd="ambiguous" id="S3.T1.6.6.1.m1.5.6.3.1.cmml" xref="S3.T1.6.6.1.m1.5.6.3">subscript</csymbol><ci id="S3.T1.6.6.1.m1.5.6.3.2.cmml" xref="S3.T1.6.6.1.m1.5.6.3.2">𝑇</ci><apply id="S3.T1.6.6.1.m1.2.2.2.cmml" xref="S3.T1.6.6.1.m1.2.2.2"><csymbol cd="latexml" id="S3.T1.6.6.1.m1.2.2.2.3.cmml" xref="S3.T1.6.6.1.m1.2.2.2.3">similar-to</csymbol><ci id="S3.T1.6.6.1.m1.2.2.2.4.cmml" xref="S3.T1.6.6.1.m1.2.2.2.4">𝑛</ci><apply id="S3.T1.6.6.1.m1.2.2.2.5.cmml" xref="S3.T1.6.6.1.m1.2.2.2.5"><times id="S3.T1.6.6.1.m1.2.2.2.5.1.cmml" xref="S3.T1.6.6.1.m1.2.2.2.5.1"></times><ci id="S3.T1.6.6.1.m1.2.2.2.5.2.cmml" xref="S3.T1.6.6.1.m1.2.2.2.5.2">𝒰</ci><interval closure="open" id="S3.T1.6.6.1.m1.2.2.2.5.3.1.cmml" xref="S3.T1.6.6.1.m1.2.2.2.5.3.2"><cn id="S3.T1.6.6.1.m1.1.1.1.1.cmml" type="integer" xref="S3.T1.6.6.1.m1.1.1.1.1">1</cn><cn id="S3.T1.6.6.1.m1.2.2.2.2.cmml" type="integer" xref="S3.T1.6.6.1.m1.2.2.2.2">1226</cn></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.6.1.m1.5c">(R,G,B)=T_{n\sim\mathcal{U}(1,1226)}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.6.6.1.m1.5d">( italic_R , italic_G , italic_B ) = italic_T start_POSTSUBSCRIPT italic_n ∼ caligraphic_U ( 1 , 1226 ) end_POSTSUBSCRIPT</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Data Rendering for Inducing a Shape Bias</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We employ physically based rendering (PBR) to accurately model and project the object geometry to the image space and cast realistic shadows to induce a shape bias in estimators.
This concept has been demonstrated to be beneficial for detection and pose estimation under the synthetic-to-real setting <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib30" title="">30</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">The standard tool for rendering data for object pose estimation is BlenderProc <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib3" title="">3</a>]</cite>.
The procedure is delineated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S2.F2" title="Figure 2 ‣ 2.2 Data Representation ‣ 2 Related Work ‣ Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_tag">2</span></a>.
Conventional approaches either assume the availability of a texture; obtained through object reconstruction or by a texture file, or a color prior for textureless and metallic objects.
Instead of relying on these strategies, we hypothesize that they are detrimental to the representations learned during training.
Therefore, we uniformly sample from a set of <math alttext="n" class="ltx_Math" display="inline" id="S4.p2.1.m1.1"><semantics id="S4.p2.1.m1.1a"><mi id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><ci id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S4.p2.1.m1.1d">italic_n</annotation></semantics></math> textures and apply those via UV-mapping for PBR rendering of the training data.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.13">Scene composition and rendering are executed using <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib3" title="">3</a>]</cite> with the standard configuration of BOP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib30" title="">30</a>]</cite>.
Table <a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S3.T1" title="Table 1 ‣ 3 Induction of Shape Bias ‣ Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the list of object material parameters and their ranges utilized for scene composition.
The specular and roughness values are assigned identical values to those used within the standard configuration<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/DLR-RM/BlenderProc/tree/main/examples/datasets/bop_challenge" title="">https://github.com/DLR-RM/BlenderProc/tree/main/examples/datasets/bop_challenge</a></span></span></span>. However, to induce a shape bias, color randomization is replaced with our own texture randomization strategy.
Let <math alttext="T=\{T_{1},T_{2},\dots,T_{n}\}" class="ltx_Math" display="inline" id="S4.p3.1.m1.4"><semantics id="S4.p3.1.m1.4a"><mrow id="S4.p3.1.m1.4.4" xref="S4.p3.1.m1.4.4.cmml"><mi id="S4.p3.1.m1.4.4.5" xref="S4.p3.1.m1.4.4.5.cmml">T</mi><mo id="S4.p3.1.m1.4.4.4" xref="S4.p3.1.m1.4.4.4.cmml">=</mo><mrow id="S4.p3.1.m1.4.4.3.3" xref="S4.p3.1.m1.4.4.3.4.cmml"><mo id="S4.p3.1.m1.4.4.3.3.4" stretchy="false" xref="S4.p3.1.m1.4.4.3.4.cmml">{</mo><msub id="S4.p3.1.m1.2.2.1.1.1" xref="S4.p3.1.m1.2.2.1.1.1.cmml"><mi id="S4.p3.1.m1.2.2.1.1.1.2" xref="S4.p3.1.m1.2.2.1.1.1.2.cmml">T</mi><mn id="S4.p3.1.m1.2.2.1.1.1.3" xref="S4.p3.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S4.p3.1.m1.4.4.3.3.5" xref="S4.p3.1.m1.4.4.3.4.cmml">,</mo><msub id="S4.p3.1.m1.3.3.2.2.2" xref="S4.p3.1.m1.3.3.2.2.2.cmml"><mi id="S4.p3.1.m1.3.3.2.2.2.2" xref="S4.p3.1.m1.3.3.2.2.2.2.cmml">T</mi><mn id="S4.p3.1.m1.3.3.2.2.2.3" xref="S4.p3.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S4.p3.1.m1.4.4.3.3.6" xref="S4.p3.1.m1.4.4.3.4.cmml">,</mo><mi id="S4.p3.1.m1.1.1" mathvariant="normal" xref="S4.p3.1.m1.1.1.cmml">…</mi><mo id="S4.p3.1.m1.4.4.3.3.7" xref="S4.p3.1.m1.4.4.3.4.cmml">,</mo><msub id="S4.p3.1.m1.4.4.3.3.3" xref="S4.p3.1.m1.4.4.3.3.3.cmml"><mi id="S4.p3.1.m1.4.4.3.3.3.2" xref="S4.p3.1.m1.4.4.3.3.3.2.cmml">T</mi><mi id="S4.p3.1.m1.4.4.3.3.3.3" xref="S4.p3.1.m1.4.4.3.3.3.3.cmml">n</mi></msub><mo id="S4.p3.1.m1.4.4.3.3.8" stretchy="false" xref="S4.p3.1.m1.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.4b"><apply id="S4.p3.1.m1.4.4.cmml" xref="S4.p3.1.m1.4.4"><eq id="S4.p3.1.m1.4.4.4.cmml" xref="S4.p3.1.m1.4.4.4"></eq><ci id="S4.p3.1.m1.4.4.5.cmml" xref="S4.p3.1.m1.4.4.5">𝑇</ci><set id="S4.p3.1.m1.4.4.3.4.cmml" xref="S4.p3.1.m1.4.4.3.3"><apply id="S4.p3.1.m1.2.2.1.1.1.cmml" xref="S4.p3.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.p3.1.m1.2.2.1.1.1.1.cmml" xref="S4.p3.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S4.p3.1.m1.2.2.1.1.1.2.cmml" xref="S4.p3.1.m1.2.2.1.1.1.2">𝑇</ci><cn id="S4.p3.1.m1.2.2.1.1.1.3.cmml" type="integer" xref="S4.p3.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S4.p3.1.m1.3.3.2.2.2.cmml" xref="S4.p3.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.p3.1.m1.3.3.2.2.2.1.cmml" xref="S4.p3.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S4.p3.1.m1.3.3.2.2.2.2.cmml" xref="S4.p3.1.m1.3.3.2.2.2.2">𝑇</ci><cn id="S4.p3.1.m1.3.3.2.2.2.3.cmml" type="integer" xref="S4.p3.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1">…</ci><apply id="S4.p3.1.m1.4.4.3.3.3.cmml" xref="S4.p3.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S4.p3.1.m1.4.4.3.3.3.1.cmml" xref="S4.p3.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S4.p3.1.m1.4.4.3.3.3.2.cmml" xref="S4.p3.1.m1.4.4.3.3.3.2">𝑇</ci><ci id="S4.p3.1.m1.4.4.3.3.3.3.cmml" xref="S4.p3.1.m1.4.4.3.3.3.3">𝑛</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.4c">T=\{T_{1},T_{2},\dots,T_{n}\}</annotation><annotation encoding="application/x-llamapun" id="S4.p3.1.m1.4d">italic_T = { italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_T start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_T start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }</annotation></semantics></math> be a set of textures and <math alttext="M=\{M_{1},M_{2},\dots,M_{k}\}" class="ltx_Math" display="inline" id="S4.p3.2.m2.4"><semantics id="S4.p3.2.m2.4a"><mrow id="S4.p3.2.m2.4.4" xref="S4.p3.2.m2.4.4.cmml"><mi id="S4.p3.2.m2.4.4.5" xref="S4.p3.2.m2.4.4.5.cmml">M</mi><mo id="S4.p3.2.m2.4.4.4" xref="S4.p3.2.m2.4.4.4.cmml">=</mo><mrow id="S4.p3.2.m2.4.4.3.3" xref="S4.p3.2.m2.4.4.3.4.cmml"><mo id="S4.p3.2.m2.4.4.3.3.4" stretchy="false" xref="S4.p3.2.m2.4.4.3.4.cmml">{</mo><msub id="S4.p3.2.m2.2.2.1.1.1" xref="S4.p3.2.m2.2.2.1.1.1.cmml"><mi id="S4.p3.2.m2.2.2.1.1.1.2" xref="S4.p3.2.m2.2.2.1.1.1.2.cmml">M</mi><mn id="S4.p3.2.m2.2.2.1.1.1.3" xref="S4.p3.2.m2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S4.p3.2.m2.4.4.3.3.5" xref="S4.p3.2.m2.4.4.3.4.cmml">,</mo><msub id="S4.p3.2.m2.3.3.2.2.2" xref="S4.p3.2.m2.3.3.2.2.2.cmml"><mi id="S4.p3.2.m2.3.3.2.2.2.2" xref="S4.p3.2.m2.3.3.2.2.2.2.cmml">M</mi><mn id="S4.p3.2.m2.3.3.2.2.2.3" xref="S4.p3.2.m2.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S4.p3.2.m2.4.4.3.3.6" xref="S4.p3.2.m2.4.4.3.4.cmml">,</mo><mi id="S4.p3.2.m2.1.1" mathvariant="normal" xref="S4.p3.2.m2.1.1.cmml">…</mi><mo id="S4.p3.2.m2.4.4.3.3.7" xref="S4.p3.2.m2.4.4.3.4.cmml">,</mo><msub id="S4.p3.2.m2.4.4.3.3.3" xref="S4.p3.2.m2.4.4.3.3.3.cmml"><mi id="S4.p3.2.m2.4.4.3.3.3.2" xref="S4.p3.2.m2.4.4.3.3.3.2.cmml">M</mi><mi id="S4.p3.2.m2.4.4.3.3.3.3" xref="S4.p3.2.m2.4.4.3.3.3.3.cmml">k</mi></msub><mo id="S4.p3.2.m2.4.4.3.3.8" stretchy="false" xref="S4.p3.2.m2.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.4b"><apply id="S4.p3.2.m2.4.4.cmml" xref="S4.p3.2.m2.4.4"><eq id="S4.p3.2.m2.4.4.4.cmml" xref="S4.p3.2.m2.4.4.4"></eq><ci id="S4.p3.2.m2.4.4.5.cmml" xref="S4.p3.2.m2.4.4.5">𝑀</ci><set id="S4.p3.2.m2.4.4.3.4.cmml" xref="S4.p3.2.m2.4.4.3.3"><apply id="S4.p3.2.m2.2.2.1.1.1.cmml" xref="S4.p3.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.p3.2.m2.2.2.1.1.1.1.cmml" xref="S4.p3.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S4.p3.2.m2.2.2.1.1.1.2.cmml" xref="S4.p3.2.m2.2.2.1.1.1.2">𝑀</ci><cn id="S4.p3.2.m2.2.2.1.1.1.3.cmml" type="integer" xref="S4.p3.2.m2.2.2.1.1.1.3">1</cn></apply><apply id="S4.p3.2.m2.3.3.2.2.2.cmml" xref="S4.p3.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.p3.2.m2.3.3.2.2.2.1.cmml" xref="S4.p3.2.m2.3.3.2.2.2">subscript</csymbol><ci id="S4.p3.2.m2.3.3.2.2.2.2.cmml" xref="S4.p3.2.m2.3.3.2.2.2.2">𝑀</ci><cn id="S4.p3.2.m2.3.3.2.2.2.3.cmml" type="integer" xref="S4.p3.2.m2.3.3.2.2.2.3">2</cn></apply><ci id="S4.p3.2.m2.1.1.cmml" xref="S4.p3.2.m2.1.1">…</ci><apply id="S4.p3.2.m2.4.4.3.3.3.cmml" xref="S4.p3.2.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="S4.p3.2.m2.4.4.3.3.3.1.cmml" xref="S4.p3.2.m2.4.4.3.3.3">subscript</csymbol><ci id="S4.p3.2.m2.4.4.3.3.3.2.cmml" xref="S4.p3.2.m2.4.4.3.3.3.2">𝑀</ci><ci id="S4.p3.2.m2.4.4.3.3.3.3.cmml" xref="S4.p3.2.m2.4.4.3.3.3.3">𝑘</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.4c">M=\{M_{1},M_{2},\dots,M_{k}\}</annotation><annotation encoding="application/x-llamapun" id="S4.p3.2.m2.4d">italic_M = { italic_M start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_M start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_M start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT }</annotation></semantics></math> a set of objects.
The random texture is drawn from <math alttext="T" class="ltx_Math" display="inline" id="S4.p3.3.m3.1"><semantics id="S4.p3.3.m3.1a"><mi id="S4.p3.3.m3.1.1" xref="S4.p3.3.m3.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.p3.3.m3.1b"><ci id="S4.p3.3.m3.1.1.cmml" xref="S4.p3.3.m3.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.3.m3.1c">T</annotation><annotation encoding="application/x-llamapun" id="S4.p3.3.m3.1d">italic_T</annotation></semantics></math>, with <math alttext="n" class="ltx_Math" display="inline" id="S4.p3.4.m4.1"><semantics id="S4.p3.4.m4.1a"><mi id="S4.p3.4.m4.1.1" xref="S4.p3.4.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.p3.4.m4.1b"><ci id="S4.p3.4.m4.1.1.cmml" xref="S4.p3.4.m4.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.4.m4.1c">n</annotation><annotation encoding="application/x-llamapun" id="S4.p3.4.m4.1d">italic_n</annotation></semantics></math> being a hyperparameter denoting the number of textures.
The training dataset for <math alttext="M" class="ltx_Math" display="inline" id="S4.p3.5.m5.1"><semantics id="S4.p3.5.m5.1a"><mi id="S4.p3.5.m5.1.1" xref="S4.p3.5.m5.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.p3.5.m5.1b"><ci id="S4.p3.5.m5.1.1.cmml" xref="S4.p3.5.m5.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.5.m5.1c">M</annotation><annotation encoding="application/x-llamapun" id="S4.p3.5.m5.1d">italic_M</annotation></semantics></math> is generated according to the number of scenes <math alttext="N=\{N_{1},N_{2},\dots,N_{j}\}" class="ltx_Math" display="inline" id="S4.p3.6.m6.4"><semantics id="S4.p3.6.m6.4a"><mrow id="S4.p3.6.m6.4.4" xref="S4.p3.6.m6.4.4.cmml"><mi id="S4.p3.6.m6.4.4.5" xref="S4.p3.6.m6.4.4.5.cmml">N</mi><mo id="S4.p3.6.m6.4.4.4" xref="S4.p3.6.m6.4.4.4.cmml">=</mo><mrow id="S4.p3.6.m6.4.4.3.3" xref="S4.p3.6.m6.4.4.3.4.cmml"><mo id="S4.p3.6.m6.4.4.3.3.4" stretchy="false" xref="S4.p3.6.m6.4.4.3.4.cmml">{</mo><msub id="S4.p3.6.m6.2.2.1.1.1" xref="S4.p3.6.m6.2.2.1.1.1.cmml"><mi id="S4.p3.6.m6.2.2.1.1.1.2" xref="S4.p3.6.m6.2.2.1.1.1.2.cmml">N</mi><mn id="S4.p3.6.m6.2.2.1.1.1.3" xref="S4.p3.6.m6.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S4.p3.6.m6.4.4.3.3.5" xref="S4.p3.6.m6.4.4.3.4.cmml">,</mo><msub id="S4.p3.6.m6.3.3.2.2.2" xref="S4.p3.6.m6.3.3.2.2.2.cmml"><mi id="S4.p3.6.m6.3.3.2.2.2.2" xref="S4.p3.6.m6.3.3.2.2.2.2.cmml">N</mi><mn id="S4.p3.6.m6.3.3.2.2.2.3" xref="S4.p3.6.m6.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S4.p3.6.m6.4.4.3.3.6" xref="S4.p3.6.m6.4.4.3.4.cmml">,</mo><mi id="S4.p3.6.m6.1.1" mathvariant="normal" xref="S4.p3.6.m6.1.1.cmml">…</mi><mo id="S4.p3.6.m6.4.4.3.3.7" xref="S4.p3.6.m6.4.4.3.4.cmml">,</mo><msub id="S4.p3.6.m6.4.4.3.3.3" xref="S4.p3.6.m6.4.4.3.3.3.cmml"><mi id="S4.p3.6.m6.4.4.3.3.3.2" xref="S4.p3.6.m6.4.4.3.3.3.2.cmml">N</mi><mi id="S4.p3.6.m6.4.4.3.3.3.3" xref="S4.p3.6.m6.4.4.3.3.3.3.cmml">j</mi></msub><mo id="S4.p3.6.m6.4.4.3.3.8" stretchy="false" xref="S4.p3.6.m6.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.6.m6.4b"><apply id="S4.p3.6.m6.4.4.cmml" xref="S4.p3.6.m6.4.4"><eq id="S4.p3.6.m6.4.4.4.cmml" xref="S4.p3.6.m6.4.4.4"></eq><ci id="S4.p3.6.m6.4.4.5.cmml" xref="S4.p3.6.m6.4.4.5">𝑁</ci><set id="S4.p3.6.m6.4.4.3.4.cmml" xref="S4.p3.6.m6.4.4.3.3"><apply id="S4.p3.6.m6.2.2.1.1.1.cmml" xref="S4.p3.6.m6.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.p3.6.m6.2.2.1.1.1.1.cmml" xref="S4.p3.6.m6.2.2.1.1.1">subscript</csymbol><ci id="S4.p3.6.m6.2.2.1.1.1.2.cmml" xref="S4.p3.6.m6.2.2.1.1.1.2">𝑁</ci><cn id="S4.p3.6.m6.2.2.1.1.1.3.cmml" type="integer" xref="S4.p3.6.m6.2.2.1.1.1.3">1</cn></apply><apply id="S4.p3.6.m6.3.3.2.2.2.cmml" xref="S4.p3.6.m6.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.p3.6.m6.3.3.2.2.2.1.cmml" xref="S4.p3.6.m6.3.3.2.2.2">subscript</csymbol><ci id="S4.p3.6.m6.3.3.2.2.2.2.cmml" xref="S4.p3.6.m6.3.3.2.2.2.2">𝑁</ci><cn id="S4.p3.6.m6.3.3.2.2.2.3.cmml" type="integer" xref="S4.p3.6.m6.3.3.2.2.2.3">2</cn></apply><ci id="S4.p3.6.m6.1.1.cmml" xref="S4.p3.6.m6.1.1">…</ci><apply id="S4.p3.6.m6.4.4.3.3.3.cmml" xref="S4.p3.6.m6.4.4.3.3.3"><csymbol cd="ambiguous" id="S4.p3.6.m6.4.4.3.3.3.1.cmml" xref="S4.p3.6.m6.4.4.3.3.3">subscript</csymbol><ci id="S4.p3.6.m6.4.4.3.3.3.2.cmml" xref="S4.p3.6.m6.4.4.3.3.3.2">𝑁</ci><ci id="S4.p3.6.m6.4.4.3.3.3.3.cmml" xref="S4.p3.6.m6.4.4.3.3.3.3">𝑗</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.6.m6.4c">N=\{N_{1},N_{2},\dots,N_{j}\}</annotation><annotation encoding="application/x-llamapun" id="S4.p3.6.m6.4d">italic_N = { italic_N start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_N start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_N start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT }</annotation></semantics></math> and that for the rendered views per scene.
For each sampled scene <math alttext="O_{m}" class="ltx_Math" display="inline" id="S4.p3.7.m7.1"><semantics id="S4.p3.7.m7.1a"><msub id="S4.p3.7.m7.1.1" xref="S4.p3.7.m7.1.1.cmml"><mi id="S4.p3.7.m7.1.1.2" xref="S4.p3.7.m7.1.1.2.cmml">O</mi><mi id="S4.p3.7.m7.1.1.3" xref="S4.p3.7.m7.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p3.7.m7.1b"><apply id="S4.p3.7.m7.1.1.cmml" xref="S4.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S4.p3.7.m7.1.1.1.cmml" xref="S4.p3.7.m7.1.1">subscript</csymbol><ci id="S4.p3.7.m7.1.1.2.cmml" xref="S4.p3.7.m7.1.1.2">𝑂</ci><ci id="S4.p3.7.m7.1.1.3.cmml" xref="S4.p3.7.m7.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.7.m7.1c">O_{m}</annotation><annotation encoding="application/x-llamapun" id="S4.p3.7.m7.1d">italic_O start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT</annotation></semantics></math> and object of <math alttext="M" class="ltx_Math" display="inline" id="S4.p3.8.m8.1"><semantics id="S4.p3.8.m8.1a"><mi id="S4.p3.8.m8.1.1" xref="S4.p3.8.m8.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.p3.8.m8.1b"><ci id="S4.p3.8.m8.1.1.cmml" xref="S4.p3.8.m8.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.8.m8.1c">M</annotation><annotation encoding="application/x-llamapun" id="S4.p3.8.m8.1d">italic_M</annotation></semantics></math> the texture is drawn from a uniform distribution <math alttext="\mathcal{U}_{1,n}" class="ltx_Math" display="inline" id="S4.p3.9.m9.2"><semantics id="S4.p3.9.m9.2a"><msub id="S4.p3.9.m9.2.3" xref="S4.p3.9.m9.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p3.9.m9.2.3.2" xref="S4.p3.9.m9.2.3.2.cmml">𝒰</mi><mrow id="S4.p3.9.m9.2.2.2.4" xref="S4.p3.9.m9.2.2.2.3.cmml"><mn id="S4.p3.9.m9.1.1.1.1" xref="S4.p3.9.m9.1.1.1.1.cmml">1</mn><mo id="S4.p3.9.m9.2.2.2.4.1" xref="S4.p3.9.m9.2.2.2.3.cmml">,</mo><mi id="S4.p3.9.m9.2.2.2.2" xref="S4.p3.9.m9.2.2.2.2.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p3.9.m9.2b"><apply id="S4.p3.9.m9.2.3.cmml" xref="S4.p3.9.m9.2.3"><csymbol cd="ambiguous" id="S4.p3.9.m9.2.3.1.cmml" xref="S4.p3.9.m9.2.3">subscript</csymbol><ci id="S4.p3.9.m9.2.3.2.cmml" xref="S4.p3.9.m9.2.3.2">𝒰</ci><list id="S4.p3.9.m9.2.2.2.3.cmml" xref="S4.p3.9.m9.2.2.2.4"><cn id="S4.p3.9.m9.1.1.1.1.cmml" type="integer" xref="S4.p3.9.m9.1.1.1.1">1</cn><ci id="S4.p3.9.m9.2.2.2.2.cmml" xref="S4.p3.9.m9.2.2.2.2">𝑛</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.9.m9.2c">\mathcal{U}_{1,n}</annotation><annotation encoding="application/x-llamapun" id="S4.p3.9.m9.2d">caligraphic_U start_POSTSUBSCRIPT 1 , italic_n end_POSTSUBSCRIPT</annotation></semantics></math> over <math alttext="T" class="ltx_Math" display="inline" id="S4.p3.10.m10.1"><semantics id="S4.p3.10.m10.1a"><mi id="S4.p3.10.m10.1.1" xref="S4.p3.10.m10.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.p3.10.m10.1b"><ci id="S4.p3.10.m10.1.1.cmml" xref="S4.p3.10.m10.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.10.m10.1c">T</annotation><annotation encoding="application/x-llamapun" id="S4.p3.10.m10.1d">italic_T</annotation></semantics></math>, for each sampled object of <math alttext="M_{k}" class="ltx_Math" display="inline" id="S4.p3.11.m11.1"><semantics id="S4.p3.11.m11.1a"><msub id="S4.p3.11.m11.1.1" xref="S4.p3.11.m11.1.1.cmml"><mi id="S4.p3.11.m11.1.1.2" xref="S4.p3.11.m11.1.1.2.cmml">M</mi><mi id="S4.p3.11.m11.1.1.3" xref="S4.p3.11.m11.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p3.11.m11.1b"><apply id="S4.p3.11.m11.1.1.cmml" xref="S4.p3.11.m11.1.1"><csymbol cd="ambiguous" id="S4.p3.11.m11.1.1.1.cmml" xref="S4.p3.11.m11.1.1">subscript</csymbol><ci id="S4.p3.11.m11.1.1.2.cmml" xref="S4.p3.11.m11.1.1.2">𝑀</ci><ci id="S4.p3.11.m11.1.1.3.cmml" xref="S4.p3.11.m11.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.11.m11.1c">M_{k}</annotation><annotation encoding="application/x-llamapun" id="S4.p3.11.m11.1d">italic_M start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>.
For every scene <math alttext="O_{m}" class="ltx_Math" display="inline" id="S4.p3.12.m12.1"><semantics id="S4.p3.12.m12.1a"><msub id="S4.p3.12.m12.1.1" xref="S4.p3.12.m12.1.1.cmml"><mi id="S4.p3.12.m12.1.1.2" xref="S4.p3.12.m12.1.1.2.cmml">O</mi><mi id="S4.p3.12.m12.1.1.3" xref="S4.p3.12.m12.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p3.12.m12.1b"><apply id="S4.p3.12.m12.1.1.cmml" xref="S4.p3.12.m12.1.1"><csymbol cd="ambiguous" id="S4.p3.12.m12.1.1.1.cmml" xref="S4.p3.12.m12.1.1">subscript</csymbol><ci id="S4.p3.12.m12.1.1.2.cmml" xref="S4.p3.12.m12.1.1.2">𝑂</ci><ci id="S4.p3.12.m12.1.1.3.cmml" xref="S4.p3.12.m12.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.12.m12.1c">O_{m}</annotation><annotation encoding="application/x-llamapun" id="S4.p3.12.m12.1d">italic_O start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="l" class="ltx_Math" display="inline" id="S4.p3.13.m13.1"><semantics id="S4.p3.13.m13.1a"><mi id="S4.p3.13.m13.1.1" xref="S4.p3.13.m13.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S4.p3.13.m13.1b"><ci id="S4.p3.13.m13.1.1.cmml" xref="S4.p3.13.m13.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.13.m13.1c">l</annotation><annotation encoding="application/x-llamapun" id="S4.p3.13.m13.1d">italic_l</annotation></semantics></math> images are rendered with randomized object and camera poses, specular, roughness, and color values or textures.</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.6">According to the standard configuration of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib30" title="">30</a>]</cite> the obtained set of <math alttext="I" class="ltx_Math" display="inline" id="S4.p4.1.m1.1"><semantics id="S4.p4.1.m1.1a"><mi id="S4.p4.1.m1.1.1" xref="S4.p4.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S4.p4.1.m1.1b"><ci id="S4.p4.1.m1.1.1.cmml" xref="S4.p4.1.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.1.m1.1c">I</annotation><annotation encoding="application/x-llamapun" id="S4.p4.1.m1.1d">italic_I</annotation></semantics></math> is <math alttext="50,000" class="ltx_Math" display="inline" id="S4.p4.2.m2.2"><semantics id="S4.p4.2.m2.2a"><mrow id="S4.p4.2.m2.2.3.2" xref="S4.p4.2.m2.2.3.1.cmml"><mn id="S4.p4.2.m2.1.1" xref="S4.p4.2.m2.1.1.cmml">50</mn><mo id="S4.p4.2.m2.2.3.2.1" xref="S4.p4.2.m2.2.3.1.cmml">,</mo><mn id="S4.p4.2.m2.2.2" xref="S4.p4.2.m2.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.2.m2.2b"><list id="S4.p4.2.m2.2.3.1.cmml" xref="S4.p4.2.m2.2.3.2"><cn id="S4.p4.2.m2.1.1.cmml" type="integer" xref="S4.p4.2.m2.1.1">50</cn><cn id="S4.p4.2.m2.2.2.cmml" type="integer" xref="S4.p4.2.m2.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.2.m2.2c">50,000</annotation><annotation encoding="application/x-llamapun" id="S4.p4.2.m2.2d">50 , 000</annotation></semantics></math>, obtained from <math alttext="l=25" class="ltx_Math" display="inline" id="S4.p4.3.m3.1"><semantics id="S4.p4.3.m3.1a"><mrow id="S4.p4.3.m3.1.1" xref="S4.p4.3.m3.1.1.cmml"><mi id="S4.p4.3.m3.1.1.2" xref="S4.p4.3.m3.1.1.2.cmml">l</mi><mo id="S4.p4.3.m3.1.1.1" xref="S4.p4.3.m3.1.1.1.cmml">=</mo><mn id="S4.p4.3.m3.1.1.3" xref="S4.p4.3.m3.1.1.3.cmml">25</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.3.m3.1b"><apply id="S4.p4.3.m3.1.1.cmml" xref="S4.p4.3.m3.1.1"><eq id="S4.p4.3.m3.1.1.1.cmml" xref="S4.p4.3.m3.1.1.1"></eq><ci id="S4.p4.3.m3.1.1.2.cmml" xref="S4.p4.3.m3.1.1.2">𝑙</ci><cn id="S4.p4.3.m3.1.1.3.cmml" type="integer" xref="S4.p4.3.m3.1.1.3">25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.3.m3.1c">l=25</annotation><annotation encoding="application/x-llamapun" id="S4.p4.3.m3.1d">italic_l = 25</annotation></semantics></math> images for each of the <math alttext="m=2000" class="ltx_Math" display="inline" id="S4.p4.4.m4.1"><semantics id="S4.p4.4.m4.1a"><mrow id="S4.p4.4.m4.1.1" xref="S4.p4.4.m4.1.1.cmml"><mi id="S4.p4.4.m4.1.1.2" xref="S4.p4.4.m4.1.1.2.cmml">m</mi><mo id="S4.p4.4.m4.1.1.1" xref="S4.p4.4.m4.1.1.1.cmml">=</mo><mn id="S4.p4.4.m4.1.1.3" xref="S4.p4.4.m4.1.1.3.cmml">2000</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.4.m4.1b"><apply id="S4.p4.4.m4.1.1.cmml" xref="S4.p4.4.m4.1.1"><eq id="S4.p4.4.m4.1.1.1.cmml" xref="S4.p4.4.m4.1.1.1"></eq><ci id="S4.p4.4.m4.1.1.2.cmml" xref="S4.p4.4.m4.1.1.2">𝑚</ci><cn id="S4.p4.4.m4.1.1.3.cmml" type="integer" xref="S4.p4.4.m4.1.1.3">2000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.4.m4.1c">m=2000</annotation><annotation encoding="application/x-llamapun" id="S4.p4.4.m4.1d">italic_m = 2000</annotation></semantics></math> scenes in <math alttext="O" class="ltx_Math" display="inline" id="S4.p4.5.m5.1"><semantics id="S4.p4.5.m5.1a"><mi id="S4.p4.5.m5.1.1" xref="S4.p4.5.m5.1.1.cmml">O</mi><annotation-xml encoding="MathML-Content" id="S4.p4.5.m5.1b"><ci id="S4.p4.5.m5.1.1.cmml" xref="S4.p4.5.m5.1.1">𝑂</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.5.m5.1c">O</annotation><annotation encoding="application/x-llamapun" id="S4.p4.5.m5.1d">italic_O</annotation></semantics></math>.
Textures are taken from a Creative Commons online texture database (cc-textures)<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://cc0-textures.com/" title="">https://cc0-textures.com/</a></span></span></span>.
The number of random textures is <math alttext="n=1226" class="ltx_Math" display="inline" id="S4.p4.6.m6.1"><semantics id="S4.p4.6.m6.1a"><mrow id="S4.p4.6.m6.1.1" xref="S4.p4.6.m6.1.1.cmml"><mi id="S4.p4.6.m6.1.1.2" xref="S4.p4.6.m6.1.1.2.cmml">n</mi><mo id="S4.p4.6.m6.1.1.1" xref="S4.p4.6.m6.1.1.1.cmml">=</mo><mn id="S4.p4.6.m6.1.1.3" xref="S4.p4.6.m6.1.1.3.cmml">1226</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.6.m6.1b"><apply id="S4.p4.6.m6.1.1.cmml" xref="S4.p4.6.m6.1.1"><eq id="S4.p4.6.m6.1.1.1.cmml" xref="S4.p4.6.m6.1.1.1"></eq><ci id="S4.p4.6.m6.1.1.2.cmml" xref="S4.p4.6.m6.1.1.2">𝑛</ci><cn id="S4.p4.6.m6.1.1.3.cmml" type="integer" xref="S4.p4.6.m6.1.1.3">1226</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.6.m6.1c">n=1226</annotation><annotation encoding="application/x-llamapun" id="S4.p4.6.m6.1d">italic_n = 1226</annotation></semantics></math>.
This number represents the complete set of available opaque textures in the database and has negligible computational overhead in comparison to a smaller number.
As test sets, we use the ones provided by BOP.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">This section presents the empirical validation of our contributions.
The experiments on object classification demonstrate the advantage of biasing CNNs toward shape with our approach over that of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib8" title="">8</a>]</cite>.
Following this, an analysis regarding the advantages and limitations of our approach for object detection and pose estimation of textureless and metallic objects is presented.
Subsequently, the robustness to typical real-world image perturbations is demonstrated.
Ultimately, a series of ablations are presented to ascertain the significance of the number of textures utilized for rendering, to effectively integrate the presented shape bias with online data augmentation, and to investigate the influence of the mesh origin, texture, and color.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Experimental Setup</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">The following paragraphs present the set of methods and the datasets utilized for the validation our hypothesis, and the metrics employed for comparison.</p>
</div>
<section class="ltx_subsubsection" id="S5.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1 </span>Methods</h4>
<div class="ltx_para" id="S5.SS1.SSS1.p1">
<p class="ltx_p" id="S5.SS1.SSS1.p1.1">The classification experiments are conducted using ResNet50 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib10" title="">10</a>]</cite>.
The object detection results are provided with Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib28" title="">28</a>]</cite>, RetinaNet<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib23" title="">23</a>]</cite>, and YOLOx <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib6" title="">6</a>]</cite>.
These three approaches were considered the state of the art at different points in time over the last few years.
The same applies to our object pose estimation experiments where results are presented with GDR-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib35" title="">35</a>]</cite> and Pix2Pose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib27" title="">27</a>]</cite>.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T2.4.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text ltx_font_bold" id="S5.T2.5.2" style="font-size:90%;">Object Detection.<span class="ltx_text ltx_font_medium" id="S5.T2.5.2.1"> mAP<sup class="ltx_sup" id="S5.T2.5.2.1.1">0.50:0.95</sup> scores for Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib28" title="">28</a>]</cite>, RetinaNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib23" title="">23</a>]</cite>, and YOLOx <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib6" title="">6</a>]</cite> trained on TLESS and ITODD. Results are provided for learning a texture and a shape bias, with and without tuned online data augmentations.</span></span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T2.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.6.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T2.6.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T2.6.1.1.1.1">Dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.6.1.1.2" rowspan="2"><span class="ltx_text" id="S5.T2.6.1.1.2.1">
<span class="ltx_inline-block" id="S5.T2.6.1.1.2.1.1">
<span class="ltx_p" id="S5.T2.6.1.1.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.6.1.1.2.1.1.1.1">Data</span></span>
<span class="ltx_p" id="S5.T2.6.1.1.2.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T2.6.1.1.2.1.1.2.1">Augmentation</span></span>
</span></span></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T2.6.1.1.3"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T2.6.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T2.6.1.1.4.1">Faster R-CNN</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T2.6.1.1.5"><span class="ltx_text ltx_font_bold" id="S5.T2.6.1.1.5.1">RetinaNet</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T2.6.1.1.6"><span class="ltx_text ltx_font_bold" id="S5.T2.6.1.1.6.1">YOLOx</span></th>
</tr>
<tr class="ltx_tr" id="S5.T2.6.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T2.6.2.2.1">Bias:</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.6.2.2.2">texture</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T2.6.2.2.3"><span class="ltx_text ltx_font_bold" id="S5.T2.6.2.2.3.1">shape (ours)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.6.2.2.4">texture</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T2.6.2.2.5"><span class="ltx_text ltx_font_bold" id="S5.T2.6.2.2.5.1">shape (ours)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.6.2.2.6">texture</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.6.2.2.7"><span class="ltx_text ltx_font_bold" id="S5.T2.6.2.2.7.1">shape (ours)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.6.3.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T2.6.3.1.1" rowspan="2"><span class="ltx_text" id="S5.T2.6.3.1.1.1">TLESS</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.3.1.2">default</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T2.6.3.1.3"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.3.1.4">20.20</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.6.3.1.5"><span class="ltx_text ltx_font_bold" id="S5.T2.6.3.1.5.1">70.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.3.1.6">23.10</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.6.3.1.7"><span class="ltx_text ltx_font_bold" id="S5.T2.6.3.1.7.1">69.50</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.3.1.8">52.60</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.3.1.9"><span class="ltx_text ltx_font_bold" id="S5.T2.6.3.1.9.1">79.70</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.4.2">
<td class="ltx_td ltx_align_center" id="S5.T2.6.4.2.1">tuned</td>
<td class="ltx_td ltx_border_r" id="S5.T2.6.4.2.2"></td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.4.2.3"><span class="ltx_text ltx_font_bold" id="S5.T2.6.4.2.3.1">80.50</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.4.2.4">77.90</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.4.2.5"><span class="ltx_text ltx_font_bold" id="S5.T2.6.4.2.5.1">78.40</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.4.2.6">74.10</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.4.2.7">80.20</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.4.2.8"><span class="ltx_text ltx_font_bold" id="S5.T2.6.4.2.8.1">80.60</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.5.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S5.T2.6.5.3.1" rowspan="2"><span class="ltx_text" id="S5.T2.6.5.3.1.1">ITODD</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.5.3.2">default</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T2.6.5.3.3"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.5.3.4">21.10</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.6.5.3.5"><span class="ltx_text ltx_font_bold" id="S5.T2.6.5.3.5.1">23.50</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.5.3.6"><span class="ltx_text ltx_font_bold" id="S5.T2.6.5.3.6.1">15.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.6.5.3.7">13.90</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.5.3.8">25.70</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.5.3.9"><span class="ltx_text ltx_font_bold" id="S5.T2.6.5.3.9.1">41.20</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.4">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.6.6.4.1">tuned</td>
<td class="ltx_td ltx_border_bb ltx_border_r" id="S5.T2.6.6.4.2"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.6.6.4.3">46.20</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T2.6.6.4.4"><span class="ltx_text ltx_font_bold" id="S5.T2.6.6.4.4.1">46.60</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.6.6.4.5">41.50</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T2.6.6.4.6"><span class="ltx_text ltx_font_bold" id="S5.T2.6.6.4.6.1">44.20</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.6.6.4.7">53.10</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.6.6.4.8"><span class="ltx_text ltx_font_bold" id="S5.T2.6.6.4.8.1">56.30</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="S5.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.2 </span>Datasets</h4>
<div class="ltx_para" id="S5.SS1.SSS2.p1">
<p class="ltx_p" id="S5.SS1.SSS2.p1.2">In order to validate our hypothesis that training models for a shape bias is particularly beneficial for textureless and metallic objects, evaluations on TLESS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib16" title="">16</a>]</cite> and ITODD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib4" title="">4</a>]</cite> are presented.
The TLESS dataset comprises <math alttext="30" class="ltx_Math" display="inline" id="S5.SS1.SSS2.p1.1.m1.1"><semantics id="S5.SS1.SSS2.p1.1.m1.1a"><mn id="S5.SS1.SSS2.p1.1.m1.1.1" xref="S5.SS1.SSS2.p1.1.m1.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS2.p1.1.m1.1b"><cn id="S5.SS1.SSS2.p1.1.m1.1.1.cmml" type="integer" xref="S5.SS1.SSS2.p1.1.m1.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS2.p1.1.m1.1c">30</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS2.p1.1.m1.1d">30</annotation></semantics></math> highly symmetrical textureless industrial objects.
The ITODD dataset contains <math alttext="28" class="ltx_Math" display="inline" id="S5.SS1.SSS2.p1.2.m2.1"><semantics id="S5.SS1.SSS2.p1.2.m2.1a"><mn id="S5.SS1.SSS2.p1.2.m2.1.1" xref="S5.SS1.SSS2.p1.2.m2.1.1.cmml">28</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS2.p1.2.m2.1b"><cn id="S5.SS1.SSS2.p1.2.m2.1.1.cmml" type="integer" xref="S5.SS1.SSS2.p1.2.m2.1.1">28</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS2.p1.2.m2.1c">28</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS2.p1.2.m2.1d">28</annotation></semantics></math> metallic industrial objects.
In both cases, the BOP test sets are employed for the evaluation, and the results for the texture bias baselines are obtained using the PBR training sets of the BOP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib30" title="">30</a>]</cite>.
These were generated using the rendering hyperparameters outlined in Section <a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S4" title="4 Data Rendering for Inducing a Shape Bias ‣ Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_tag">4</span></a>.
The ground truth of the ITODD test set is not available to prohibit users from optimizing their data augmentation hyperparameters for the test data.
Thus, evaluations can only be performed with the online evaluation tool of the BOP, which only provides metrics for object detection and pose estimation.
Consequently, experiments for classification and the ablations are done using TLESS.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.3 </span>Metrics</h4>
<div class="ltx_para" id="S5.SS1.SSS3.p1">
<p class="ltx_p" id="S5.SS1.SSS3.p1.1">For the classification experiments the top-1 and the top-5 accuracy in percentage are reported.
The COCO evaluation metric <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib22" title="">22</a>]</cite>, mean average precision (mAP), is reported for object detection. The intersection over union (IoU) range emploed is <math alttext="0.5:0.95" class="ltx_Math" display="inline" id="S5.SS1.SSS3.p1.1.m1.1"><semantics id="S5.SS1.SSS3.p1.1.m1.1a"><mrow id="S5.SS1.SSS3.p1.1.m1.1.1" xref="S5.SS1.SSS3.p1.1.m1.1.1.cmml"><mn id="S5.SS1.SSS3.p1.1.m1.1.1.2" xref="S5.SS1.SSS3.p1.1.m1.1.1.2.cmml">0.5</mn><mo id="S5.SS1.SSS3.p1.1.m1.1.1.1" lspace="0.278em" rspace="0.278em" xref="S5.SS1.SSS3.p1.1.m1.1.1.1.cmml">:</mo><mn id="S5.SS1.SSS3.p1.1.m1.1.1.3" xref="S5.SS1.SSS3.p1.1.m1.1.1.3.cmml">0.95</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS3.p1.1.m1.1b"><apply id="S5.SS1.SSS3.p1.1.m1.1.1.cmml" xref="S5.SS1.SSS3.p1.1.m1.1.1"><ci id="S5.SS1.SSS3.p1.1.m1.1.1.1.cmml" xref="S5.SS1.SSS3.p1.1.m1.1.1.1">:</ci><cn id="S5.SS1.SSS3.p1.1.m1.1.1.2.cmml" type="float" xref="S5.SS1.SSS3.p1.1.m1.1.1.2">0.5</cn><cn id="S5.SS1.SSS3.p1.1.m1.1.1.3.cmml" type="float" xref="S5.SS1.SSS3.p1.1.m1.1.1.3">0.95</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS3.p1.1.m1.1c">0.5:0.95</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS3.p1.1.m1.1d">0.5 : 0.95</annotation></semantics></math>.
For pose estimation, we present the Average Recall (AR) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib13" title="">13</a>]</cite>, the combination of the visible surface discrepancy (VSD) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib13" title="">13</a>]</cite>, maximum symmetry-aware surface distance (MSSD) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib15" title="">15</a>]</cite>, and the maximum symmetry-aware projection distance (MSPD) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib15" title="">15</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>UV-Mapping versus Style Transfer</h3>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T3.3.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text ltx_font_bold" id="S5.T3.4.2" style="font-size:90%;">UV-Mapping versus Style Transfer.<span class="ltx_text ltx_font_medium" id="S5.T3.4.2.1"> Comparing top-1 and top-5 accuracy (Acc.) of style transfer for shape bias induction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib7" title="">7</a>]</cite> and our approach on the TLESS dataset.</span></span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.5.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T3.5.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T3.5.1.1.1.1">Bias</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.5.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T3.5.1.1.2.1">Top-1 Acc. (%)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.5.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T3.5.1.1.3.1">Top-5 Acc. (%)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.5.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.5.2.1.1">texture</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.5.2.1.2">63.22</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.5.2.1.3">86.27</td>
</tr>
<tr class="ltx_tr" id="S5.T3.5.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T3.5.3.2.1">shape<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib7" title="">7</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T3.5.3.2.2">75.88</td>
<td class="ltx_td ltx_align_center" id="S5.T3.5.3.2.3">91.16</td>
</tr>
<tr class="ltx_tr" id="S5.T3.5.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T3.5.4.3.1">shape (ours)</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.5.4.3.2"><span class="ltx_text ltx_font_bold" id="S5.T3.5.4.3.2.1">87.27</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.5.4.3.3"><span class="ltx_text ltx_font_bold" id="S5.T3.5.4.3.3.1">97.37</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.3">We conduct an experiment to verify our hypothesis that our UV-mapping approach is improving over style transfer for learning a shape bias.
Table <a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S5.T3" title="Table 3 ‣ 5.2 UV-Mapping versus Style Transfer ‣ 5 Experiments ‣ Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_tag">3</span></a> presents results for ResNet50 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib10" title="">10</a>]</cite> trained for object classification on TLESS.
For classification, all object instances of the test set are cropped using the ground truth bounding box with a scaling factor of <math alttext="1.0" class="ltx_Math" display="inline" id="S5.SS2.p1.1.m1.1"><semantics id="S5.SS2.p1.1.m1.1a"><mn id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml">1.0</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><cn id="S5.SS2.p1.1.m1.1.1.cmml" type="float" xref="S5.SS2.p1.1.m1.1.1">1.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">1.0</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.1.m1.1d">1.0</annotation></semantics></math>.
This value correlates with the cropping ratio of ImageNet, which <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib7" title="">7</a>]</cite> has been designed for.
Our experiments show that learning classification with a texture bias, using uniform color sampling, results in the worst top-1 and top-5 accuracy.
Using AdaIN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib19" title="">19</a>]</cite> for learning a shape bias with style transfer, as done by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib7" title="">7</a>]</cite> improves over the texture bias variant.
However, inducing a shape bias through our UV-mapping approach results in a <math alttext="15.01\%" class="ltx_Math" display="inline" id="S5.SS2.p1.2.m2.1"><semantics id="S5.SS2.p1.2.m2.1a"><mrow id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml"><mn id="S5.SS2.p1.2.m2.1.1.2" xref="S5.SS2.p1.2.m2.1.1.2.cmml">15.01</mn><mo id="S5.SS2.p1.2.m2.1.1.1" xref="S5.SS2.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.1b"><apply id="S5.SS2.p1.2.m2.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1"><csymbol cd="latexml" id="S5.SS2.p1.2.m2.1.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1.1">percent</csymbol><cn id="S5.SS2.p1.2.m2.1.1.2.cmml" type="float" xref="S5.SS2.p1.2.m2.1.1.2">15.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.1c">15.01\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.2.m2.1d">15.01 %</annotation></semantics></math> relative top-1 and a <math alttext="6.81\%" class="ltx_Math" display="inline" id="S5.SS2.p1.3.m3.1"><semantics id="S5.SS2.p1.3.m3.1a"><mrow id="S5.SS2.p1.3.m3.1.1" xref="S5.SS2.p1.3.m3.1.1.cmml"><mn id="S5.SS2.p1.3.m3.1.1.2" xref="S5.SS2.p1.3.m3.1.1.2.cmml">6.81</mn><mo id="S5.SS2.p1.3.m3.1.1.1" xref="S5.SS2.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.3.m3.1b"><apply id="S5.SS2.p1.3.m3.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1"><csymbol cd="latexml" id="S5.SS2.p1.3.m3.1.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1.1">percent</csymbol><cn id="S5.SS2.p1.3.m3.1.1.2.cmml" type="float" xref="S5.SS2.p1.3.m3.1.1.2">6.81</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.3.m3.1c">6.81\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.3.m3.1d">6.81 %</annotation></semantics></math> relative top-5 accuracy improvement over <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib8" title="">8</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Object Detection</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">Experiments using Faster R-CNN, RetinaNet, and YOLOx for object detection on TLESS and ITODD are conducted to identify the advantages and limitations of our method for shape bias induction.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">For Faster R-CNN and RetinaNet the Image-Net pre-trained ResNet50 backbone is used.
For YOLOx the l-variant is used.
All three object detectors are trained for 30 epochs, since validation accuracy converges in all cases until then.
The remaining training parameters, are all identical to the default settings of MMDetection<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/open-mmlab/mmdetection" title="">https://github.com/open-mmlab/mmdetection</a></span></span></span>, which are true to the original papers.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1">All experiments are conducted using the standard configuration for online data augmentation, the data augmentation of the winner of the BOP <math alttext="2022" class="ltx_Math" display="inline" id="S5.SS3.p3.1.m1.1"><semantics id="S5.SS3.p3.1.m1.1a"><mn id="S5.SS3.p3.1.m1.1.1" xref="S5.SS3.p3.1.m1.1.1.cmml">2022</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.1.m1.1b"><cn id="S5.SS3.p3.1.m1.1.1.cmml" type="integer" xref="S5.SS3.p3.1.m1.1.1">2022</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.1.m1.1c">2022</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p3.1.m1.1d">2022</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib24" title="">24</a>]</cite> when learning a texture bias.
Tuned data augmentation is used to maximize the detection accuracy at the superposition with shape bias.
More detail on the superposition of our shape bias and data augmentation is provided in Section <a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S5.SS6.SSS2" title="5.6.2 Superposition of Shape Bias and Data Augmentation ‣ 5.6 Ablative Experiments ‣ 5 Experiments ‣ Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_tag">5.6.2</span></a>.</p>
</div>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="332" id="S5.F3.g1" src="extracted/5750059/images/detection_results.jpg" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F3.5.2.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text ltx_font_bold" id="S5.F3.2.1" style="font-size:90%;">Detection Example.<span class="ltx_text ltx_font_medium" id="S5.F3.2.1.1"> Examplary test images from TLESS and ITODD, with ground truth (blue) and predicted (green) bounding boxes using YOLOx. YOLOx yields more accurate bounding boxes, better recall and precision rates, when inducing a shape bias; detection and IoU thresholds are both set to <math alttext="0.5" class="ltx_Math" display="inline" id="S5.F3.2.1.1.m1.1"><semantics id="S5.F3.2.1.1.m1.1b"><mn id="S5.F3.2.1.1.m1.1.1" xref="S5.F3.2.1.1.m1.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S5.F3.2.1.1.m1.1c"><cn id="S5.F3.2.1.1.m1.1.1.cmml" type="float" xref="S5.F3.2.1.1.m1.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F3.2.1.1.m1.1d">0.5</annotation><annotation encoding="application/x-llamapun" id="S5.F3.2.1.1.m1.1e">0.5</annotation></semantics></math>.</span></span></figcaption>
</figure>
<figure class="ltx_table" id="S5.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T4.3.1.1" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text ltx_font_bold" id="S5.T4.4.2" style="font-size:90%;">Pose estimation performance.<span class="ltx_text ltx_font_medium" id="S5.T4.4.2.1"> Pose estimation of GDR-Net and Pix2Pose on TLESS and ITODD. Compared is the AR training with object color priors (default) and for texture agnosticity (none), for object detection (O.D.) and pose estimation (P.E.); YOLOx is used for object detection.</span></span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T4.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.5.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.5.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T4.5.1.1.1.1">Dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T4.5.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T4.5.1.1.2.1">Bias</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4" id="S5.T4.5.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T4.5.1.1.3.1">GDR-Net</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4" id="S5.T4.5.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T4.5.1.1.4.1">Pix2Pose</span></th>
</tr>
<tr class="ltx_tr" id="S5.T4.5.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.5.2.2.1">O.D.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T4.5.2.2.2">P.E.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.5.2.2.3">AR<sub class="ltx_sub" id="S5.T4.5.2.2.3.1">MSPD</sub>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.5.2.2.4">AR<sub class="ltx_sub" id="S5.T4.5.2.2.4.1">MSSD</sub>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.5.2.2.5">AR<sub class="ltx_sub" id="S5.T4.5.2.2.5.1">VSD</sub>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T4.5.2.2.6"><span class="ltx_text ltx_font_bold" id="S5.T4.5.2.2.6.1">AR</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.5.2.2.7">AR<sub class="ltx_sub" id="S5.T4.5.2.2.7.1">MSPD</sub>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.5.2.2.8">AR<sub class="ltx_sub" id="S5.T4.5.2.2.8.1">MSSD</sub>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.5.2.2.9">AR<sub class="ltx_sub" id="S5.T4.5.2.2.9.1">VSD</sub>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.5.2.2.10"><span class="ltx_text ltx_font_bold" id="S5.T4.5.2.2.10.1">AR</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.5.3.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.5.3.1.1" rowspan="2"><span class="ltx_text" id="S5.T4.5.3.1.1.1">
<span class="ltx_inline-block" id="S5.T4.5.3.1.1.1.1">
<span class="ltx_p" id="S5.T4.5.3.1.1.1.1.1">TLESS</span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.5.3.1.2">texture</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.5.3.1.3">texture</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.5.3.1.4">72.66</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.5.3.1.5">50.52</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.5.3.1.6">45.70</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.5.3.1.7">56.29</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.5.3.1.8">68.35</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.5.3.1.9">37.85</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.5.3.1.10">33.84</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.5.3.1.11">46.68</td>
</tr>
<tr class="ltx_tr" id="S5.T4.5.4.2">
<td class="ltx_td ltx_align_center" id="S5.T4.5.4.2.1">texture</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.5.4.2.2"><span class="ltx_text ltx_font_bold" id="S5.T4.5.4.2.2.1">shape (ours)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.5.4.2.3">73.54</td>
<td class="ltx_td ltx_align_center" id="S5.T4.5.4.2.4">52.94</td>
<td class="ltx_td ltx_align_center" id="S5.T4.5.4.2.5">48.09</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.5.4.2.6">58.19</td>
<td class="ltx_td ltx_align_center" id="S5.T4.5.4.2.7">67.71</td>
<td class="ltx_td ltx_align_center" id="S5.T4.5.4.2.8">38.25</td>
<td class="ltx_td ltx_align_center" id="S5.T4.5.4.2.9">34.10</td>
<td class="ltx_td ltx_align_center" id="S5.T4.5.4.2.10">46.69</td>
</tr>
<tr class="ltx_tr" id="S5.T4.5.5.3">
<td class="ltx_td ltx_align_center" id="S5.T4.5.5.3.1">
<span class="ltx_ERROR undefined" id="S5.T4.5.5.3.1.1">\cdashline</span>2-11</td>
<td class="ltx_td ltx_align_center" id="S5.T4.5.5.3.2"><span class="ltx_text ltx_font_bold" id="S5.T4.5.5.3.2.1">shape (ours)</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.5.5.3.3"><span class="ltx_text ltx_font_bold" id="S5.T4.5.5.3.3.1">shape (ours)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.5.5.3.4"><span class="ltx_text ltx_font_bold" id="S5.T4.5.5.3.4.1">73.73</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.5.5.3.5"><span class="ltx_text ltx_font_bold" id="S5.T4.5.5.3.5.1">53.05</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.5.5.3.6"><span class="ltx_text ltx_font_bold" id="S5.T4.5.5.3.6.1">48.17</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.5.5.3.7"><span class="ltx_text ltx_font_bold" id="S5.T4.5.5.3.7.1">58.32</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.5.5.3.8"><span class="ltx_text ltx_font_bold" id="S5.T4.5.5.3.8.1">71.22</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.5.5.3.9"><span class="ltx_text ltx_font_bold" id="S5.T4.5.5.3.9.1">41.86</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.5.5.3.10"><span class="ltx_text ltx_font_bold" id="S5.T4.5.5.3.10.1">37.30</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.5.5.3.11"><span class="ltx_text ltx_font_bold" id="S5.T4.5.5.3.11.1">50.13</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.5.6.4">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.5.6.4.1" rowspan="2"><span class="ltx_text" id="S5.T4.5.6.4.1.1">
<span class="ltx_inline-block" id="S5.T4.5.6.4.1.1.1">
<span class="ltx_p" id="S5.T4.5.6.4.1.1.1.1">ITODD</span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.5.6.4.2">texture</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.5.6.4.3">texture</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.5.6.4.4">14.30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.5.6.4.5">10.10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.5.6.4.6">7.80</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.5.6.4.7">10.70</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.5.6.4.8">22.50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.5.6.4.9">8.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.5.6.4.10">8.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.5.6.4.11">12.90</td>
</tr>
<tr class="ltx_tr" id="S5.T4.5.7.5">
<td class="ltx_td ltx_align_center" id="S5.T4.5.7.5.1">texture</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.5.7.5.2"><span class="ltx_text ltx_font_bold" id="S5.T4.5.7.5.2.1">shape (ours)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.5.7.5.3">14.30</td>
<td class="ltx_td ltx_align_center" id="S5.T4.5.7.5.4">10.10</td>
<td class="ltx_td ltx_align_center" id="S5.T4.5.7.5.5">7.80</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.5.7.5.6">10.70</td>
<td class="ltx_td ltx_align_center" id="S5.T4.5.7.5.7">22.70</td>
<td class="ltx_td ltx_align_center" id="S5.T4.5.7.5.8">10.10</td>
<td class="ltx_td ltx_align_center" id="S5.T4.5.7.5.9">10.30</td>
<td class="ltx_td ltx_align_center" id="S5.T4.5.7.5.10">14.30</td>
</tr>
<tr class="ltx_tr" id="S5.T4.5.8.6">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.5.8.6.1">
<span class="ltx_ERROR undefined" id="S5.T4.5.8.6.1.1">\cdashline</span>2-11</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.5.8.6.2"><span class="ltx_text ltx_font_bold" id="S5.T4.5.8.6.2.1">shape (ours)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T4.5.8.6.3"><span class="ltx_text ltx_font_bold" id="S5.T4.5.8.6.3.1">shape (ours)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.5.8.6.4"><span class="ltx_text ltx_font_bold" id="S5.T4.5.8.6.4.1">15.30</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.5.8.6.5"><span class="ltx_text ltx_font_bold" id="S5.T4.5.8.6.5.1">10.20</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.5.8.6.6"><span class="ltx_text ltx_font_bold" id="S5.T4.5.8.6.6.1">7.90</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T4.5.8.6.7"><span class="ltx_text ltx_font_bold" id="S5.T4.5.8.6.7.1">11.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.5.8.6.8"><span class="ltx_text ltx_font_bold" id="S5.T4.5.8.6.8.1">23.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.5.8.6.9"><span class="ltx_text ltx_font_bold" id="S5.T4.5.8.6.9.1">11.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.5.8.6.10"><span class="ltx_text ltx_font_bold" id="S5.T4.5.8.6.10.1">11.70</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.5.8.6.11"><span class="ltx_text ltx_font_bold" id="S5.T4.5.8.6.11.1">15.30</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S5.SS3.p4">
<p class="ltx_p" id="S5.SS3.p4.8">Object detection results are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S5.T2" title="Table 2 ‣ 5.1.1 Methods ‣ 5.1 Experimental Setup ‣ 5 Experiments ‣ Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_tag">2</span></a>.
For the default two-staged Faster R-CNN and the single-staged RetinaNet our shape bias improves by <math alttext="247.03" class="ltx_Math" display="inline" id="S5.SS3.p4.1.m1.1"><semantics id="S5.SS3.p4.1.m1.1a"><mn id="S5.SS3.p4.1.m1.1.1" xref="S5.SS3.p4.1.m1.1.1.cmml">247.03</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.1.m1.1b"><cn id="S5.SS3.p4.1.m1.1.1.cmml" type="float" xref="S5.SS3.p4.1.m1.1.1">247.03</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.1.m1.1c">247.03</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.1.m1.1d">247.03</annotation></semantics></math> and <math alttext="200.87" class="ltx_Math" display="inline" id="S5.SS3.p4.2.m2.1"><semantics id="S5.SS3.p4.2.m2.1a"><mn id="S5.SS3.p4.2.m2.1.1" xref="S5.SS3.p4.2.m2.1.1.cmml">200.87</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.2.m2.1b"><cn id="S5.SS3.p4.2.m2.1.1.cmml" type="float" xref="S5.SS3.p4.2.m2.1.1">200.87</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.2.m2.1c">200.87</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.2.m2.1d">200.87</annotation></semantics></math> relative percent on TLESS, respectively.
On ITODD the improvement in mAP for Faster R-CNN is <math alttext="11.37\%" class="ltx_Math" display="inline" id="S5.SS3.p4.3.m3.1"><semantics id="S5.SS3.p4.3.m3.1a"><mrow id="S5.SS3.p4.3.m3.1.1" xref="S5.SS3.p4.3.m3.1.1.cmml"><mn id="S5.SS3.p4.3.m3.1.1.2" xref="S5.SS3.p4.3.m3.1.1.2.cmml">11.37</mn><mo id="S5.SS3.p4.3.m3.1.1.1" xref="S5.SS3.p4.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.3.m3.1b"><apply id="S5.SS3.p4.3.m3.1.1.cmml" xref="S5.SS3.p4.3.m3.1.1"><csymbol cd="latexml" id="S5.SS3.p4.3.m3.1.1.1.cmml" xref="S5.SS3.p4.3.m3.1.1.1">percent</csymbol><cn id="S5.SS3.p4.3.m3.1.1.2.cmml" type="float" xref="S5.SS3.p4.3.m3.1.1.2">11.37</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.3.m3.1c">11.37\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.3.m3.1d">11.37 %</annotation></semantics></math>.
However, the mAP of RetinaNet decreases by <math alttext="7.91\%" class="ltx_Math" display="inline" id="S5.SS3.p4.4.m4.1"><semantics id="S5.SS3.p4.4.m4.1a"><mrow id="S5.SS3.p4.4.m4.1.1" xref="S5.SS3.p4.4.m4.1.1.cmml"><mn id="S5.SS3.p4.4.m4.1.1.2" xref="S5.SS3.p4.4.m4.1.1.2.cmml">7.91</mn><mo id="S5.SS3.p4.4.m4.1.1.1" xref="S5.SS3.p4.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.4.m4.1b"><apply id="S5.SS3.p4.4.m4.1.1.cmml" xref="S5.SS3.p4.4.m4.1.1"><csymbol cd="latexml" id="S5.SS3.p4.4.m4.1.1.1.cmml" xref="S5.SS3.p4.4.m4.1.1.1">percent</csymbol><cn id="S5.SS3.p4.4.m4.1.1.2.cmml" type="float" xref="S5.SS3.p4.4.m4.1.1.2">7.91</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.4.m4.1c">7.91\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.4.m4.1d">7.91 %</annotation></semantics></math> when learning a shape bias for object detection on ITODD.
Similarly, when using tuned data augmentations for TLESS our shape prior reduces the mAP by <math alttext="3.34\%" class="ltx_Math" display="inline" id="S5.SS3.p4.5.m5.1"><semantics id="S5.SS3.p4.5.m5.1a"><mrow id="S5.SS3.p4.5.m5.1.1" xref="S5.SS3.p4.5.m5.1.1.cmml"><mn id="S5.SS3.p4.5.m5.1.1.2" xref="S5.SS3.p4.5.m5.1.1.2.cmml">3.34</mn><mo id="S5.SS3.p4.5.m5.1.1.1" xref="S5.SS3.p4.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.5.m5.1b"><apply id="S5.SS3.p4.5.m5.1.1.cmml" xref="S5.SS3.p4.5.m5.1.1"><csymbol cd="latexml" id="S5.SS3.p4.5.m5.1.1.1.cmml" xref="S5.SS3.p4.5.m5.1.1.1">percent</csymbol><cn id="S5.SS3.p4.5.m5.1.1.2.cmml" type="float" xref="S5.SS3.p4.5.m5.1.1.2">3.34</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.5.m5.1c">3.34\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.5.m5.1d">3.34 %</annotation></semantics></math> for Faster R-CNN and <math alttext="5.80\%" class="ltx_Math" display="inline" id="S5.SS3.p4.6.m6.1"><semantics id="S5.SS3.p4.6.m6.1a"><mrow id="S5.SS3.p4.6.m6.1.1" xref="S5.SS3.p4.6.m6.1.1.cmml"><mn id="S5.SS3.p4.6.m6.1.1.2" xref="S5.SS3.p4.6.m6.1.1.2.cmml">5.80</mn><mo id="S5.SS3.p4.6.m6.1.1.1" xref="S5.SS3.p4.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.6.m6.1b"><apply id="S5.SS3.p4.6.m6.1.1.cmml" xref="S5.SS3.p4.6.m6.1.1"><csymbol cd="latexml" id="S5.SS3.p4.6.m6.1.1.1.cmml" xref="S5.SS3.p4.6.m6.1.1.1">percent</csymbol><cn id="S5.SS3.p4.6.m6.1.1.2.cmml" type="float" xref="S5.SS3.p4.6.m6.1.1.2">5.80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.6.m6.1c">5.80\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.6.m6.1d">5.80 %</annotation></semantics></math> for RetinaNet.
On ITODD the relative mAP improvement is <math alttext="0.87\%" class="ltx_Math" display="inline" id="S5.SS3.p4.7.m7.1"><semantics id="S5.SS3.p4.7.m7.1a"><mrow id="S5.SS3.p4.7.m7.1.1" xref="S5.SS3.p4.7.m7.1.1.cmml"><mn id="S5.SS3.p4.7.m7.1.1.2" xref="S5.SS3.p4.7.m7.1.1.2.cmml">0.87</mn><mo id="S5.SS3.p4.7.m7.1.1.1" xref="S5.SS3.p4.7.m7.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.7.m7.1b"><apply id="S5.SS3.p4.7.m7.1.1.cmml" xref="S5.SS3.p4.7.m7.1.1"><csymbol cd="latexml" id="S5.SS3.p4.7.m7.1.1.1.cmml" xref="S5.SS3.p4.7.m7.1.1.1">percent</csymbol><cn id="S5.SS3.p4.7.m7.1.1.2.cmml" type="float" xref="S5.SS3.p4.7.m7.1.1.2">0.87</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.7.m7.1c">0.87\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.7.m7.1d">0.87 %</annotation></semantics></math> for Faster R-CNN and <math alttext="6.51\%" class="ltx_Math" display="inline" id="S5.SS3.p4.8.m8.1"><semantics id="S5.SS3.p4.8.m8.1a"><mrow id="S5.SS3.p4.8.m8.1.1" xref="S5.SS3.p4.8.m8.1.1.cmml"><mn id="S5.SS3.p4.8.m8.1.1.2" xref="S5.SS3.p4.8.m8.1.1.2.cmml">6.51</mn><mo id="S5.SS3.p4.8.m8.1.1.1" xref="S5.SS3.p4.8.m8.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.8.m8.1b"><apply id="S5.SS3.p4.8.m8.1.1.cmml" xref="S5.SS3.p4.8.m8.1.1"><csymbol cd="latexml" id="S5.SS3.p4.8.m8.1.1.1.cmml" xref="S5.SS3.p4.8.m8.1.1.1">percent</csymbol><cn id="S5.SS3.p4.8.m8.1.1.2.cmml" type="float" xref="S5.SS3.p4.8.m8.1.1.2">6.51</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.8.m8.1c">6.51\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.8.m8.1d">6.51 %</annotation></semantics></math> for RetinaNet.</p>
</div>
<div class="ltx_para" id="S5.SS3.p5">
<p class="ltx_p" id="S5.SS3.p5.3">On the more recent YOLOx model, which is arguably the current standard for object detection, our shape bias improves the mAP for TLESS and ITODD using the default and the tuned data augmentation version.
Comparing the default versions, our shape bias improves by a relative percentage of <math alttext="51.52" class="ltx_Math" display="inline" id="S5.SS3.p5.1.m1.1"><semantics id="S5.SS3.p5.1.m1.1a"><mn id="S5.SS3.p5.1.m1.1.1" xref="S5.SS3.p5.1.m1.1.1.cmml">51.52</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p5.1.m1.1b"><cn id="S5.SS3.p5.1.m1.1.1.cmml" type="float" xref="S5.SS3.p5.1.m1.1.1">51.52</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p5.1.m1.1c">51.52</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p5.1.m1.1d">51.52</annotation></semantics></math> for TLESS.
The improvements on TLESS using a superposition of our shape bias and tuned online data augmentations are negligible to that of using the online data augmentations of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib24" title="">24</a>]</cite> for training for a texture bias.
However, on the metallic objects of ITODD our relative improvement is <math alttext="60.31\%" class="ltx_Math" display="inline" id="S5.SS3.p5.2.m2.1"><semantics id="S5.SS3.p5.2.m2.1a"><mrow id="S5.SS3.p5.2.m2.1.1" xref="S5.SS3.p5.2.m2.1.1.cmml"><mn id="S5.SS3.p5.2.m2.1.1.2" xref="S5.SS3.p5.2.m2.1.1.2.cmml">60.31</mn><mo id="S5.SS3.p5.2.m2.1.1.1" xref="S5.SS3.p5.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p5.2.m2.1b"><apply id="S5.SS3.p5.2.m2.1.1.cmml" xref="S5.SS3.p5.2.m2.1.1"><csymbol cd="latexml" id="S5.SS3.p5.2.m2.1.1.1.cmml" xref="S5.SS3.p5.2.m2.1.1.1">percent</csymbol><cn id="S5.SS3.p5.2.m2.1.1.2.cmml" type="float" xref="S5.SS3.p5.2.m2.1.1.2">60.31</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p5.2.m2.1c">60.31\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p5.2.m2.1d">60.31 %</annotation></semantics></math>, using the default, and <math alttext="6.03\%" class="ltx_Math" display="inline" id="S5.SS3.p5.3.m3.1"><semantics id="S5.SS3.p5.3.m3.1a"><mrow id="S5.SS3.p5.3.m3.1.1" xref="S5.SS3.p5.3.m3.1.1.cmml"><mn id="S5.SS3.p5.3.m3.1.1.2" xref="S5.SS3.p5.3.m3.1.1.2.cmml">6.03</mn><mo id="S5.SS3.p5.3.m3.1.1.1" xref="S5.SS3.p5.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p5.3.m3.1b"><apply id="S5.SS3.p5.3.m3.1.1.cmml" xref="S5.SS3.p5.3.m3.1.1"><csymbol cd="latexml" id="S5.SS3.p5.3.m3.1.1.1.cmml" xref="S5.SS3.p5.3.m3.1.1.1">percent</csymbol><cn id="S5.SS3.p5.3.m3.1.1.2.cmml" type="float" xref="S5.SS3.p5.3.m3.1.1.2">6.03</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p5.3.m3.1c">6.03\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p5.3.m3.1d">6.03 %</annotation></semantics></math> using the tuned training configuration.
Interestingly, on TLESS using our method for learning a shape bias with all tested detectors is almost on par with using tuned data augmentations.
As such, a valid alternative when detecting textureless objects.
Metallic object detection on ITODD is more challenging, yet reaching the maximum mAP requires all tested detectors to learn a shape bias.
The experiments demonstrates that YOLOx exhibits enhanced accuracy in comparison to the other two detectors.
We conclude that approaches with higher learning capacities
benefit more from the induced shape bias.</p>
</div>
<div class="ltx_para" id="S5.SS3.p6">
<p class="ltx_p" id="S5.SS3.p6.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S5.F3" title="Figure 3 ‣ 5.3 Object Detection ‣ 5 Experiments ‣ Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_tag">3</span></a> shows example images of TLESS and ITODD with bounding boxes predicted by YOLOx, with texture and shape bias.
On TLESS the bounding box corner accuracy and the true positive rate are similar, yet the false negative rate is lower for the shape bias.
On ITODD, the shape-biased YOLOx shows improved accuracy in bounding box corners, higher true positive rate, and lower false negative rate.</p>
</div>
<div class="ltx_para" id="S5.SS3.p7">
<p class="ltx_p" id="S5.SS3.p7.1">In conclusion, inducing object detectors with a shape bias not only improves object detection in the majority of the cases, but also presents a viable alternative to color augmentation tuning for such textureless objects as those of TLESS. For the industrial metallic objects of ITODD the combination of tuned online color augmentation and geometry bias is consistently demonstrated to be the optimal approach.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Object Pose Estimation</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.6">This section demonstrates the influence on object pose estimation of textureless and metallic objects when training for a shape bias.
Results are presented for GDR-Net and Pix2Pose, integrated with the detections of YOLOx.
The default training protocols as reported in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib35" title="">35</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib27" title="">27</a>]</cite> are employed, with GDR-Net using ResNet34 and Pix2Pose using ResNet50 as backbones.
In both cases, one pose estimator is trained for each object.
Table <a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S5.T4" title="Table 4 ‣ 5.3 Object Detection ‣ 5 Experiments ‣ Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_tag">4</span></a> shows that our shape-biased GDR-Net and Pix2Pose improve in AR using the texture-biased version of YOLOx for object detection.
While the improvement with Pix2Pose is negligible, the improvement of GDR-Net is <math alttext="3.38\%" class="ltx_Math" display="inline" id="S5.SS4.p1.1.m1.1"><semantics id="S5.SS4.p1.1.m1.1a"><mrow id="S5.SS4.p1.1.m1.1.1" xref="S5.SS4.p1.1.m1.1.1.cmml"><mn id="S5.SS4.p1.1.m1.1.1.2" xref="S5.SS4.p1.1.m1.1.1.2.cmml">3.38</mn><mo id="S5.SS4.p1.1.m1.1.1.1" xref="S5.SS4.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.1.m1.1b"><apply id="S5.SS4.p1.1.m1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1"><csymbol cd="latexml" id="S5.SS4.p1.1.m1.1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1.1">percent</csymbol><cn id="S5.SS4.p1.1.m1.1.1.2.cmml" type="float" xref="S5.SS4.p1.1.m1.1.1.2">3.38</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.1.m1.1c">3.38\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.1.m1.1d">3.38 %</annotation></semantics></math>.
Further integrating the shape-biased YOLOx for detection enhances improvements to <math alttext="3.61\%" class="ltx_Math" display="inline" id="S5.SS4.p1.2.m2.1"><semantics id="S5.SS4.p1.2.m2.1a"><mrow id="S5.SS4.p1.2.m2.1.1" xref="S5.SS4.p1.2.m2.1.1.cmml"><mn id="S5.SS4.p1.2.m2.1.1.2" xref="S5.SS4.p1.2.m2.1.1.2.cmml">3.61</mn><mo id="S5.SS4.p1.2.m2.1.1.1" xref="S5.SS4.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.2.m2.1b"><apply id="S5.SS4.p1.2.m2.1.1.cmml" xref="S5.SS4.p1.2.m2.1.1"><csymbol cd="latexml" id="S5.SS4.p1.2.m2.1.1.1.cmml" xref="S5.SS4.p1.2.m2.1.1.1">percent</csymbol><cn id="S5.SS4.p1.2.m2.1.1.2.cmml" type="float" xref="S5.SS4.p1.2.m2.1.1.2">3.61</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.2.m2.1c">3.61\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.2.m2.1d">3.61 %</annotation></semantics></math> for GDR-Net, and <math alttext="7.39\%" class="ltx_Math" display="inline" id="S5.SS4.p1.3.m3.1"><semantics id="S5.SS4.p1.3.m3.1a"><mrow id="S5.SS4.p1.3.m3.1.1" xref="S5.SS4.p1.3.m3.1.1.cmml"><mn id="S5.SS4.p1.3.m3.1.1.2" xref="S5.SS4.p1.3.m3.1.1.2.cmml">7.39</mn><mo id="S5.SS4.p1.3.m3.1.1.1" xref="S5.SS4.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.3.m3.1b"><apply id="S5.SS4.p1.3.m3.1.1.cmml" xref="S5.SS4.p1.3.m3.1.1"><csymbol cd="latexml" id="S5.SS4.p1.3.m3.1.1.1.cmml" xref="S5.SS4.p1.3.m3.1.1.1">percent</csymbol><cn id="S5.SS4.p1.3.m3.1.1.2.cmml" type="float" xref="S5.SS4.p1.3.m3.1.1.2">7.39</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.3.m3.1c">7.39\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.3.m3.1d">7.39 %</annotation></semantics></math> for Pix2Pose.
On ITODD, GDR-Net results in the same AR with texture and with shape bias.
Integrating the detections of the shape-biased YOLOx improves AR by <math alttext="3.74\%" class="ltx_Math" display="inline" id="S5.SS4.p1.4.m4.1"><semantics id="S5.SS4.p1.4.m4.1a"><mrow id="S5.SS4.p1.4.m4.1.1" xref="S5.SS4.p1.4.m4.1.1.cmml"><mn id="S5.SS4.p1.4.m4.1.1.2" xref="S5.SS4.p1.4.m4.1.1.2.cmml">3.74</mn><mo id="S5.SS4.p1.4.m4.1.1.1" xref="S5.SS4.p1.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.4.m4.1b"><apply id="S5.SS4.p1.4.m4.1.1.cmml" xref="S5.SS4.p1.4.m4.1.1"><csymbol cd="latexml" id="S5.SS4.p1.4.m4.1.1.1.cmml" xref="S5.SS4.p1.4.m4.1.1.1">percent</csymbol><cn id="S5.SS4.p1.4.m4.1.1.2.cmml" type="float" xref="S5.SS4.p1.4.m4.1.1.2">3.74</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.4.m4.1c">3.74\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.4.m4.1d">3.74 %</annotation></semantics></math>.
Using the texture-biased YOLOx, Pix2Pose improves by <math alttext="10.85\%" class="ltx_Math" display="inline" id="S5.SS4.p1.5.m5.1"><semantics id="S5.SS4.p1.5.m5.1a"><mrow id="S5.SS4.p1.5.m5.1.1" xref="S5.SS4.p1.5.m5.1.1.cmml"><mn id="S5.SS4.p1.5.m5.1.1.2" xref="S5.SS4.p1.5.m5.1.1.2.cmml">10.85</mn><mo id="S5.SS4.p1.5.m5.1.1.1" xref="S5.SS4.p1.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.5.m5.1b"><apply id="S5.SS4.p1.5.m5.1.1.cmml" xref="S5.SS4.p1.5.m5.1.1"><csymbol cd="latexml" id="S5.SS4.p1.5.m5.1.1.1.cmml" xref="S5.SS4.p1.5.m5.1.1.1">percent</csymbol><cn id="S5.SS4.p1.5.m5.1.1.2.cmml" type="float" xref="S5.SS4.p1.5.m5.1.1.2">10.85</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.5.m5.1c">10.85\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.5.m5.1d">10.85 %</annotation></semantics></math>.
Integrating the detections of the shape-biased YOLOx the AR improvement is further enhanced to <math alttext="18.60\%" class="ltx_Math" display="inline" id="S5.SS4.p1.6.m6.1"><semantics id="S5.SS4.p1.6.m6.1a"><mrow id="S5.SS4.p1.6.m6.1.1" xref="S5.SS4.p1.6.m6.1.1.cmml"><mn id="S5.SS4.p1.6.m6.1.1.2" xref="S5.SS4.p1.6.m6.1.1.2.cmml">18.60</mn><mo id="S5.SS4.p1.6.m6.1.1.1" xref="S5.SS4.p1.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.6.m6.1b"><apply id="S5.SS4.p1.6.m6.1.1.cmml" xref="S5.SS4.p1.6.m6.1.1"><csymbol cd="latexml" id="S5.SS4.p1.6.m6.1.1.1.cmml" xref="S5.SS4.p1.6.m6.1.1.1">percent</csymbol><cn id="S5.SS4.p1.6.m6.1.1.2.cmml" type="float" xref="S5.SS4.p1.6.m6.1.1.2">18.60</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.6.m6.1c">18.60\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.6.m6.1d">18.60 %</annotation></semantics></math>.
Pix2Pose shows a greater degree of accuracy improvement than GDR-Net.
Analogous to our observations on object detection, Pix2Pose’ results improve more due to the higher capacity of the feature extractor.</p>
</div>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S5.F4" title="Figure 4 ‣ 5.4 Object Pose Estimation ‣ 5 Experiments ‣ Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_tag">4</span></a> shows qualitative pose estimation results using YOLOx as detector and GDR-Net for pose estimation.
On both, TLESS and ITODD, the alignment of the 3D bounding box, re-projected using the estimated pose, with the object improves.
For TLESS the ground truth is additionally visualized in blue.</p>
</div>
<div class="ltx_para" id="S5.SS4.p3">
<p class="ltx_p" id="S5.SS4.p3.1">In conclusion, biasing representations learned by pose estimators for shape is either on par, or improves accuracy as compared to the standard, texture-biased versions.
Integrating shape-biased object detectors with pose estimators consistently results in pose estimation accuracy improvement.</p>
</div>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="167" id="S5.F4.g1" src="extracted/5750059/images/pose_estimation_examples_v2.jpg" width="240"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F4.3.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text ltx_font_bold" id="S5.F4.4.2" style="font-size:90%;">Pose Estimation Examples.<span class="ltx_text ltx_font_medium" id="S5.F4.4.2.1"> Example images from the TLESS and ITODD datasets, showcasing the increased performance for occluded, dark and reflective object pose estimation; GDR-Net for pose estimation and YOLOx without color prior for detection; no detection ground truth for ITODD available.</span></span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Robustness to Image Perturbations</h3>
<div class="ltx_para" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1">To assess the robustness of pose estimators trained for shape bias, experiments with different common image perturbations are conducted.
The perturbations are applied to the test images of TLESS, using ground-truth detections.
Poses are estimated using GDR-Net.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S5.F6" title="Figure 6 ‣ 5.6.1 Number of Random Textures ‣ 5.6 Ablative Experiments ‣ 5 Experiments ‣ Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_tag">6</span></a> demonstrates that the shape bias yields higher robustness to Gaussian and Shot image noise.
Pose estimates remain highly accurate even in the presence of considerable noise.
In general, shape bias has little to no influence on the pose estimation accuracy when image blur, such as motion and Gaussian blur, is present.
However, the presented shape bias increases robustness in the regime of high brightness changes, which is potentially useful for cases such as pose estimation in space <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib18" title="">18</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.6 </span>Ablative Experiments</h3>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="122" id="S5.F5.g1" src="extracted/5750059/images/ablation_texture_count.jpg" width="165"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F5.3.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text ltx_font_bold" id="S5.F5.4.2" style="font-size:90%;">Number of Textures.<span class="ltx_text ltx_font_medium" id="S5.F5.4.2.1"> Ablation on the influence of the number of random textures for YOLOx on TLESS</span></span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS6.p1">
<p class="ltx_p" id="S5.SS6.p1.1">This section presents ablative studies.
Reported are the influence of the number of random textures <math alttext="n" class="ltx_Math" display="inline" id="S5.SS6.p1.1.m1.1"><semantics id="S5.SS6.p1.1.m1.1a"><mi id="S5.SS6.p1.1.m1.1.1" xref="S5.SS6.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.SS6.p1.1.m1.1b"><ci id="S5.SS6.p1.1.m1.1.1.cmml" xref="S5.SS6.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.p1.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S5.SS6.p1.1.m1.1d">italic_n</annotation></semantics></math>, object detection and pose estimation at the superposition of our shape bias and online data augmentations, and the influence of the object geometry prior.</p>
</div>
<section class="ltx_subsubsection" id="S5.SS6.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.6.1 </span>Number of Random Textures</h4>
<div class="ltx_para" id="S5.SS6.SSS1.p1">
<p class="ltx_p" id="S5.SS6.SSS1.p1.4">Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S5.F5" title="Figure 5 ‣ 5.6 Ablative Experiments ‣ 5 Experiments ‣ Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_tag">5</span></a> shows the trend of YOLOx’ mAP on TLESS when inducing a shape bias with a different number of random textures.
With the availability of more textures mAP improves.
Ultimately, using the <math alttext="n=1226" class="ltx_Math" display="inline" id="S5.SS6.SSS1.p1.1.m1.1"><semantics id="S5.SS6.SSS1.p1.1.m1.1a"><mrow id="S5.SS6.SSS1.p1.1.m1.1.1" xref="S5.SS6.SSS1.p1.1.m1.1.1.cmml"><mi id="S5.SS6.SSS1.p1.1.m1.1.1.2" xref="S5.SS6.SSS1.p1.1.m1.1.1.2.cmml">n</mi><mo id="S5.SS6.SSS1.p1.1.m1.1.1.1" xref="S5.SS6.SSS1.p1.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS6.SSS1.p1.1.m1.1.1.3" xref="S5.SS6.SSS1.p1.1.m1.1.1.3.cmml">1226</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS6.SSS1.p1.1.m1.1b"><apply id="S5.SS6.SSS1.p1.1.m1.1.1.cmml" xref="S5.SS6.SSS1.p1.1.m1.1.1"><eq id="S5.SS6.SSS1.p1.1.m1.1.1.1.cmml" xref="S5.SS6.SSS1.p1.1.m1.1.1.1"></eq><ci id="S5.SS6.SSS1.p1.1.m1.1.1.2.cmml" xref="S5.SS6.SSS1.p1.1.m1.1.1.2">𝑛</ci><cn id="S5.SS6.SSS1.p1.1.m1.1.1.3.cmml" type="integer" xref="S5.SS6.SSS1.p1.1.m1.1.1.3">1226</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.SSS1.p1.1.m1.1c">n=1226</annotation><annotation encoding="application/x-llamapun" id="S5.SS6.SSS1.p1.1.m1.1d">italic_n = 1226</annotation></semantics></math> available cc-textures yields the highest precision.
However, using five random textures already results in <math alttext="93.85\%" class="ltx_Math" display="inline" id="S5.SS6.SSS1.p1.2.m2.1"><semantics id="S5.SS6.SSS1.p1.2.m2.1a"><mrow id="S5.SS6.SSS1.p1.2.m2.1.1" xref="S5.SS6.SSS1.p1.2.m2.1.1.cmml"><mn id="S5.SS6.SSS1.p1.2.m2.1.1.2" xref="S5.SS6.SSS1.p1.2.m2.1.1.2.cmml">93.85</mn><mo id="S5.SS6.SSS1.p1.2.m2.1.1.1" xref="S5.SS6.SSS1.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS6.SSS1.p1.2.m2.1b"><apply id="S5.SS6.SSS1.p1.2.m2.1.1.cmml" xref="S5.SS6.SSS1.p1.2.m2.1.1"><csymbol cd="latexml" id="S5.SS6.SSS1.p1.2.m2.1.1.1.cmml" xref="S5.SS6.SSS1.p1.2.m2.1.1.1">percent</csymbol><cn id="S5.SS6.SSS1.p1.2.m2.1.1.2.cmml" type="float" xref="S5.SS6.SSS1.p1.2.m2.1.1.2">93.85</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.SSS1.p1.2.m2.1c">93.85\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS6.SSS1.p1.2.m2.1d">93.85 %</annotation></semantics></math> and <math alttext="100" class="ltx_Math" display="inline" id="S5.SS6.SSS1.p1.3.m3.1"><semantics id="S5.SS6.SSS1.p1.3.m3.1a"><mn id="S5.SS6.SSS1.p1.3.m3.1.1" xref="S5.SS6.SSS1.p1.3.m3.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S5.SS6.SSS1.p1.3.m3.1b"><cn id="S5.SS6.SSS1.p1.3.m3.1.1.cmml" type="integer" xref="S5.SS6.SSS1.p1.3.m3.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.SSS1.p1.3.m3.1c">100</annotation><annotation encoding="application/x-llamapun" id="S5.SS6.SSS1.p1.3.m3.1d">100</annotation></semantics></math> textures achieve <math alttext="99.63\%" class="ltx_Math" display="inline" id="S5.SS6.SSS1.p1.4.m4.1"><semantics id="S5.SS6.SSS1.p1.4.m4.1a"><mrow id="S5.SS6.SSS1.p1.4.m4.1.1" xref="S5.SS6.SSS1.p1.4.m4.1.1.cmml"><mn id="S5.SS6.SSS1.p1.4.m4.1.1.2" xref="S5.SS6.SSS1.p1.4.m4.1.1.2.cmml">99.63</mn><mo id="S5.SS6.SSS1.p1.4.m4.1.1.1" xref="S5.SS6.SSS1.p1.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS6.SSS1.p1.4.m4.1b"><apply id="S5.SS6.SSS1.p1.4.m4.1.1.cmml" xref="S5.SS6.SSS1.p1.4.m4.1.1"><csymbol cd="latexml" id="S5.SS6.SSS1.p1.4.m4.1.1.1.cmml" xref="S5.SS6.SSS1.p1.4.m4.1.1.1">percent</csymbol><cn id="S5.SS6.SSS1.p1.4.m4.1.1.2.cmml" type="float" xref="S5.SS6.SSS1.p1.4.m4.1.1.2">99.63</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.SSS1.p1.4.m4.1c">99.63\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS6.SSS1.p1.4.m4.1d">99.63 %</annotation></semantics></math> of the maximally achieved pose estimation accuracy.</p>
</div>
<figure class="ltx_figure" id="S5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="128" id="S5.F6.g1" src="extracted/5750059/images/perturbations.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F6.3.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text ltx_font_bold" id="S5.F6.4.2" style="font-size:90%;">Shape Bias Robustness to Image Perturbations<span class="ltx_text ltx_font_medium" id="S5.F6.4.2.1"> Influence of various image perturbations on AR scores with varying severity; GDR-Net with one pose estimation model trained per object; ground truth detections used for pose estimation</span></span></figcaption>
</figure>
<figure class="ltx_table" id="S5.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T5.24.2.1" style="font-size:90%;">Table 5</span>: </span><span class="ltx_text ltx_font_bold" id="S5.T5.2.1" style="font-size:90%;">Data Augmentation.<span class="ltx_text ltx_font_medium" id="S5.T5.2.1.1"> Baselines for object detection and pose estimation; <math alttext="p" class="ltx_Math" display="inline" id="S5.T5.2.1.1.m1.1"><semantics id="S5.T5.2.1.1.m1.1b"><mi id="S5.T5.2.1.1.m1.1.1" xref="S5.T5.2.1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S5.T5.2.1.1.m1.1c"><ci id="S5.T5.2.1.1.m1.1.1.cmml" xref="S5.T5.2.1.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.2.1.1.m1.1d">p</annotation><annotation encoding="application/x-llamapun" id="S5.T5.2.1.1.m1.1e">italic_p</annotation></semantics></math> denotes augmentation probability</span></span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T5.21">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T5.21.20.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T5.21.20.1.1"><span class="ltx_text ltx_font_bold" id="S5.T5.21.20.1.1.1">Object Detection</span></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S5.T5.21.20.1.2"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T5.21.20.1.3"><span class="ltx_text ltx_font_bold" id="S5.T5.21.20.1.3.1">Pose Estimation</span></th>
</tr>
<tr class="ltx_tr" id="S5.T5.4.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.4.2.3">Augmentation</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.3.1.1"><math alttext="p" class="ltx_Math" display="inline" id="S5.T5.3.1.1.m1.1"><semantics id="S5.T5.3.1.1.m1.1a"><mi id="S5.T5.3.1.1.m1.1.1" xref="S5.T5.3.1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S5.T5.3.1.1.m1.1b"><ci id="S5.T5.3.1.1.m1.1.1.cmml" xref="S5.T5.3.1.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.3.1.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="S5.T5.3.1.1.m1.1d">italic_p</annotation></semantics></math></th>
<th class="ltx_td ltx_th ltx_th_column" id="S5.T5.4.2.4"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.4.2.5">Augmentation</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.4.2.2"><math alttext="p" class="ltx_Math" display="inline" id="S5.T5.4.2.2.m1.1"><semantics id="S5.T5.4.2.2.m1.1a"><mi id="S5.T5.4.2.2.m1.1.1" xref="S5.T5.4.2.2.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S5.T5.4.2.2.m1.1b"><ci id="S5.T5.4.2.2.m1.1.1.cmml" xref="S5.T5.4.2.2.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.4.2.2.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="S5.T5.4.2.2.m1.1d">italic_p</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.6.4">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.6.4.3">Coarse Dropout</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.5.3.1"><math alttext="0.5" class="ltx_Math" display="inline" id="S5.T5.5.3.1.m1.1"><semantics id="S5.T5.5.3.1.m1.1a"><mn id="S5.T5.5.3.1.m1.1.1" xref="S5.T5.5.3.1.m1.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S5.T5.5.3.1.m1.1b"><cn id="S5.T5.5.3.1.m1.1.1.cmml" type="float" xref="S5.T5.5.3.1.m1.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.5.3.1.m1.1c">0.5</annotation><annotation encoding="application/x-llamapun" id="S5.T5.5.3.1.m1.1d">0.5</annotation></semantics></math></td>
<td class="ltx_td ltx_border_t" id="S5.T5.6.4.4"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.6.4.5">Coarse Dropout</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.6.4.2"><math alttext="0.5" class="ltx_Math" display="inline" id="S5.T5.6.4.2.m1.1"><semantics id="S5.T5.6.4.2.m1.1a"><mn id="S5.T5.6.4.2.m1.1.1" xref="S5.T5.6.4.2.m1.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S5.T5.6.4.2.m1.1b"><cn id="S5.T5.6.4.2.m1.1.1.cmml" type="float" xref="S5.T5.6.4.2.m1.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.6.4.2.m1.1c">0.5</annotation><annotation encoding="application/x-llamapun" id="S5.T5.6.4.2.m1.1d">0.5</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S5.T5.8.6">
<td class="ltx_td ltx_align_center" id="S5.T5.8.6.3">Gaussian Blur</td>
<td class="ltx_td ltx_align_center" id="S5.T5.7.5.1"><math alttext="0.4" class="ltx_Math" display="inline" id="S5.T5.7.5.1.m1.1"><semantics id="S5.T5.7.5.1.m1.1a"><mn id="S5.T5.7.5.1.m1.1.1" xref="S5.T5.7.5.1.m1.1.1.cmml">0.4</mn><annotation-xml encoding="MathML-Content" id="S5.T5.7.5.1.m1.1b"><cn id="S5.T5.7.5.1.m1.1.1.cmml" type="float" xref="S5.T5.7.5.1.m1.1.1">0.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.7.5.1.m1.1c">0.4</annotation><annotation encoding="application/x-llamapun" id="S5.T5.7.5.1.m1.1d">0.4</annotation></semantics></math></td>
<td class="ltx_td" id="S5.T5.8.6.4"></td>
<td class="ltx_td ltx_align_center" id="S5.T5.8.6.5">Gaussian Blur</td>
<td class="ltx_td ltx_align_center" id="S5.T5.8.6.2"><math alttext="0.5" class="ltx_Math" display="inline" id="S5.T5.8.6.2.m1.1"><semantics id="S5.T5.8.6.2.m1.1a"><mn id="S5.T5.8.6.2.m1.1.1" xref="S5.T5.8.6.2.m1.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S5.T5.8.6.2.m1.1b"><cn id="S5.T5.8.6.2.m1.1.1.cmml" type="float" xref="S5.T5.8.6.2.m1.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.8.6.2.m1.1c">0.5</annotation><annotation encoding="application/x-llamapun" id="S5.T5.8.6.2.m1.1d">0.5</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S5.T5.10.8">
<td class="ltx_td ltx_align_center" id="S5.T5.10.8.3">Enhance Sharpness</td>
<td class="ltx_td ltx_align_center" id="S5.T5.9.7.1"><math alttext="0.3" class="ltx_Math" display="inline" id="S5.T5.9.7.1.m1.1"><semantics id="S5.T5.9.7.1.m1.1a"><mn id="S5.T5.9.7.1.m1.1.1" xref="S5.T5.9.7.1.m1.1.1.cmml">0.3</mn><annotation-xml encoding="MathML-Content" id="S5.T5.9.7.1.m1.1b"><cn id="S5.T5.9.7.1.m1.1.1.cmml" type="float" xref="S5.T5.9.7.1.m1.1.1">0.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.9.7.1.m1.1c">0.3</annotation><annotation encoding="application/x-llamapun" id="S5.T5.9.7.1.m1.1d">0.3</annotation></semantics></math></td>
<td class="ltx_td" id="S5.T5.10.8.4"></td>
<td class="ltx_td ltx_align_center" id="S5.T5.10.8.5">Add</td>
<td class="ltx_td ltx_align_center" id="S5.T5.10.8.2"><math alttext="0.5" class="ltx_Math" display="inline" id="S5.T5.10.8.2.m1.1"><semantics id="S5.T5.10.8.2.m1.1a"><mn id="S5.T5.10.8.2.m1.1.1" xref="S5.T5.10.8.2.m1.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S5.T5.10.8.2.m1.1b"><cn id="S5.T5.10.8.2.m1.1.1.cmml" type="float" xref="S5.T5.10.8.2.m1.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.10.8.2.m1.1c">0.5</annotation><annotation encoding="application/x-llamapun" id="S5.T5.10.8.2.m1.1d">0.5</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S5.T5.12.10">
<td class="ltx_td ltx_align_center" id="S5.T5.12.10.3">Enhance Contrast</td>
<td class="ltx_td ltx_align_center" id="S5.T5.11.9.1"><math alttext="0.3" class="ltx_Math" display="inline" id="S5.T5.11.9.1.m1.1"><semantics id="S5.T5.11.9.1.m1.1a"><mn id="S5.T5.11.9.1.m1.1.1" xref="S5.T5.11.9.1.m1.1.1.cmml">0.3</mn><annotation-xml encoding="MathML-Content" id="S5.T5.11.9.1.m1.1b"><cn id="S5.T5.11.9.1.m1.1.1.cmml" type="float" xref="S5.T5.11.9.1.m1.1.1">0.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.11.9.1.m1.1c">0.3</annotation><annotation encoding="application/x-llamapun" id="S5.T5.11.9.1.m1.1d">0.3</annotation></semantics></math></td>
<td class="ltx_td" id="S5.T5.12.10.4"></td>
<td class="ltx_td ltx_align_center" id="S5.T5.12.10.5">Invert</td>
<td class="ltx_td ltx_align_center" id="S5.T5.12.10.2"><math alttext="0.3" class="ltx_Math" display="inline" id="S5.T5.12.10.2.m1.1"><semantics id="S5.T5.12.10.2.m1.1a"><mn id="S5.T5.12.10.2.m1.1.1" xref="S5.T5.12.10.2.m1.1.1.cmml">0.3</mn><annotation-xml encoding="MathML-Content" id="S5.T5.12.10.2.m1.1b"><cn id="S5.T5.12.10.2.m1.1.1.cmml" type="float" xref="S5.T5.12.10.2.m1.1.1">0.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.12.10.2.m1.1c">0.3</annotation><annotation encoding="application/x-llamapun" id="S5.T5.12.10.2.m1.1d">0.3</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S5.T5.14.12">
<td class="ltx_td ltx_align_center" id="S5.T5.14.12.3">Enhance Brightness</td>
<td class="ltx_td ltx_align_center" id="S5.T5.13.11.1"><math alttext="0.5" class="ltx_Math" display="inline" id="S5.T5.13.11.1.m1.1"><semantics id="S5.T5.13.11.1.m1.1a"><mn id="S5.T5.13.11.1.m1.1.1" xref="S5.T5.13.11.1.m1.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S5.T5.13.11.1.m1.1b"><cn id="S5.T5.13.11.1.m1.1.1.cmml" type="float" xref="S5.T5.13.11.1.m1.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.13.11.1.m1.1c">0.5</annotation><annotation encoding="application/x-llamapun" id="S5.T5.13.11.1.m1.1d">0.5</annotation></semantics></math></td>
<td class="ltx_td" id="S5.T5.14.12.4"></td>
<td class="ltx_td ltx_align_center" id="S5.T5.14.12.5">Multiply</td>
<td class="ltx_td ltx_align_center" id="S5.T5.14.12.2"><math alttext="0.5" class="ltx_Math" display="inline" id="S5.T5.14.12.2.m1.1"><semantics id="S5.T5.14.12.2.m1.1a"><mn id="S5.T5.14.12.2.m1.1.1" xref="S5.T5.14.12.2.m1.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S5.T5.14.12.2.m1.1b"><cn id="S5.T5.14.12.2.m1.1.1.cmml" type="float" xref="S5.T5.14.12.2.m1.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.14.12.2.m1.1c">0.5</annotation><annotation encoding="application/x-llamapun" id="S5.T5.14.12.2.m1.1d">0.5</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S5.T5.16.14">
<td class="ltx_td ltx_align_center" id="S5.T5.16.14.3">Enhance Color</td>
<td class="ltx_td ltx_align_center" id="S5.T5.15.13.1"><math alttext="0.3" class="ltx_Math" display="inline" id="S5.T5.15.13.1.m1.1"><semantics id="S5.T5.15.13.1.m1.1a"><mn id="S5.T5.15.13.1.m1.1.1" xref="S5.T5.15.13.1.m1.1.1.cmml">0.3</mn><annotation-xml encoding="MathML-Content" id="S5.T5.15.13.1.m1.1b"><cn id="S5.T5.15.13.1.m1.1.1.cmml" type="float" xref="S5.T5.15.13.1.m1.1.1">0.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.15.13.1.m1.1c">0.3</annotation><annotation encoding="application/x-llamapun" id="S5.T5.15.13.1.m1.1d">0.3</annotation></semantics></math></td>
<td class="ltx_td" id="S5.T5.16.14.4"></td>
<td class="ltx_td ltx_align_center" id="S5.T5.16.14.5">Linear Contrast</td>
<td class="ltx_td ltx_align_center" id="S5.T5.16.14.2"><math alttext="0.5" class="ltx_Math" display="inline" id="S5.T5.16.14.2.m1.1"><semantics id="S5.T5.16.14.2.m1.1a"><mn id="S5.T5.16.14.2.m1.1.1" xref="S5.T5.16.14.2.m1.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S5.T5.16.14.2.m1.1b"><cn id="S5.T5.16.14.2.m1.1.1.cmml" type="float" xref="S5.T5.16.14.2.m1.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.16.14.2.m1.1c">0.5</annotation><annotation encoding="application/x-llamapun" id="S5.T5.16.14.2.m1.1d">0.5</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S5.T5.17.15">
<td class="ltx_td ltx_align_center" id="S5.T5.17.15.2">Add</td>
<td class="ltx_td ltx_align_center" id="S5.T5.17.15.1"><math alttext="0.5" class="ltx_Math" display="inline" id="S5.T5.17.15.1.m1.1"><semantics id="S5.T5.17.15.1.m1.1a"><mn id="S5.T5.17.15.1.m1.1.1" xref="S5.T5.17.15.1.m1.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S5.T5.17.15.1.m1.1b"><cn id="S5.T5.17.15.1.m1.1.1.cmml" type="float" xref="S5.T5.17.15.1.m1.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.17.15.1.m1.1c">0.5</annotation><annotation encoding="application/x-llamapun" id="S5.T5.17.15.1.m1.1d">0.5</annotation></semantics></math></td>
<td class="ltx_td" id="S5.T5.17.15.3"></td>
<td class="ltx_td" id="S5.T5.17.15.4"></td>
<td class="ltx_td" id="S5.T5.17.15.5"></td>
</tr>
<tr class="ltx_tr" id="S5.T5.18.16">
<td class="ltx_td ltx_align_center" id="S5.T5.18.16.2">Invert</td>
<td class="ltx_td ltx_align_center" id="S5.T5.18.16.1"><math alttext="0.3" class="ltx_Math" display="inline" id="S5.T5.18.16.1.m1.1"><semantics id="S5.T5.18.16.1.m1.1a"><mn id="S5.T5.18.16.1.m1.1.1" xref="S5.T5.18.16.1.m1.1.1.cmml">0.3</mn><annotation-xml encoding="MathML-Content" id="S5.T5.18.16.1.m1.1b"><cn id="S5.T5.18.16.1.m1.1.1.cmml" type="float" xref="S5.T5.18.16.1.m1.1.1">0.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.18.16.1.m1.1c">0.3</annotation><annotation encoding="application/x-llamapun" id="S5.T5.18.16.1.m1.1d">0.3</annotation></semantics></math></td>
<td class="ltx_td" id="S5.T5.18.16.3"></td>
<td class="ltx_td" id="S5.T5.18.16.4"></td>
<td class="ltx_td" id="S5.T5.18.16.5"></td>
</tr>
<tr class="ltx_tr" id="S5.T5.19.17">
<td class="ltx_td ltx_align_center" id="S5.T5.19.17.2">Multiply</td>
<td class="ltx_td ltx_align_center" id="S5.T5.19.17.1"><math alttext="0.5" class="ltx_Math" display="inline" id="S5.T5.19.17.1.m1.1"><semantics id="S5.T5.19.17.1.m1.1a"><mn id="S5.T5.19.17.1.m1.1.1" xref="S5.T5.19.17.1.m1.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S5.T5.19.17.1.m1.1b"><cn id="S5.T5.19.17.1.m1.1.1.cmml" type="float" xref="S5.T5.19.17.1.m1.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.19.17.1.m1.1c">0.5</annotation><annotation encoding="application/x-llamapun" id="S5.T5.19.17.1.m1.1d">0.5</annotation></semantics></math></td>
<td class="ltx_td" id="S5.T5.19.17.3"></td>
<td class="ltx_td" id="S5.T5.19.17.4"></td>
<td class="ltx_td" id="S5.T5.19.17.5"></td>
</tr>
<tr class="ltx_tr" id="S5.T5.20.18">
<td class="ltx_td ltx_align_center" id="S5.T5.20.18.2">Gaussian Noise</td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.18.1"><math alttext="0.1" class="ltx_Math" display="inline" id="S5.T5.20.18.1.m1.1"><semantics id="S5.T5.20.18.1.m1.1a"><mn id="S5.T5.20.18.1.m1.1.1" xref="S5.T5.20.18.1.m1.1.1.cmml">0.1</mn><annotation-xml encoding="MathML-Content" id="S5.T5.20.18.1.m1.1b"><cn id="S5.T5.20.18.1.m1.1.1.cmml" type="float" xref="S5.T5.20.18.1.m1.1.1">0.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.20.18.1.m1.1c">0.1</annotation><annotation encoding="application/x-llamapun" id="S5.T5.20.18.1.m1.1d">0.1</annotation></semantics></math></td>
<td class="ltx_td" id="S5.T5.20.18.3"></td>
<td class="ltx_td" id="S5.T5.20.18.4"></td>
<td class="ltx_td" id="S5.T5.20.18.5"></td>
</tr>
<tr class="ltx_tr" id="S5.T5.21.19">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.21.19.2">Linear Contrast</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.21.19.1"><math alttext="0.5" class="ltx_Math" display="inline" id="S5.T5.21.19.1.m1.1"><semantics id="S5.T5.21.19.1.m1.1a"><mn id="S5.T5.21.19.1.m1.1.1" xref="S5.T5.21.19.1.m1.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S5.T5.21.19.1.m1.1b"><cn id="S5.T5.21.19.1.m1.1.1.cmml" type="float" xref="S5.T5.21.19.1.m1.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.21.19.1.m1.1c">0.5</annotation><annotation encoding="application/x-llamapun" id="S5.T5.21.19.1.m1.1d">0.5</annotation></semantics></math></td>
<td class="ltx_td ltx_border_bb" id="S5.T5.21.19.3"></td>
<td class="ltx_td ltx_border_bb" id="S5.T5.21.19.4"></td>
<td class="ltx_td ltx_border_bb" id="S5.T5.21.19.5"></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S5.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="135" id="S5.F7.g1" src="extracted/5750059/images/augmentation_probability.png" width="299"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F7.3.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text ltx_font_bold" id="S5.F7.4.2" style="font-size:90%;">Online Data Augmentation Severity.<span class="ltx_text ltx_font_medium" id="S5.F7.4.2.1"> Ablation study on the severity of online augmentation,
for Pix2Pose and YOLOx on TLESS; ground truth detection priors for Pix2Pose.</span></span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S5.SS6.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.6.2 </span>Superposition of Shape Bias and Data Augmentation</h4>
<div class="ltx_para" id="S5.SS6.SSS2.p1">
<p class="ltx_p" id="S5.SS6.SSS2.p1.5">This section provides a detailed analysis of the interplay of our shape bias with online data augmentation.
We perform a grid search over the augmentation probabilities used by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib24" title="">24</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib35" title="">35</a>]</cite>.
Table <a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S5.T5" title="Table 5 ‣ 5.6.1 Number of Random Textures ‣ 5.6 Ablative Experiments ‣ 5 Experiments ‣ Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_tag">5</span></a> lists the types and probabilities of augmentations applied to object detection and pose estimation.
As done by GDR-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib35" title="">35</a>]</cite>, and adopted for YOLOx by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib24" title="">24</a>]</cite>, a coefficient <math alttext="\lambda\in[0,1]" class="ltx_Math" display="inline" id="S5.SS6.SSS2.p1.1.m1.2"><semantics id="S5.SS6.SSS2.p1.1.m1.2a"><mrow id="S5.SS6.SSS2.p1.1.m1.2.3" xref="S5.SS6.SSS2.p1.1.m1.2.3.cmml"><mi id="S5.SS6.SSS2.p1.1.m1.2.3.2" xref="S5.SS6.SSS2.p1.1.m1.2.3.2.cmml">λ</mi><mo id="S5.SS6.SSS2.p1.1.m1.2.3.1" xref="S5.SS6.SSS2.p1.1.m1.2.3.1.cmml">∈</mo><mrow id="S5.SS6.SSS2.p1.1.m1.2.3.3.2" xref="S5.SS6.SSS2.p1.1.m1.2.3.3.1.cmml"><mo id="S5.SS6.SSS2.p1.1.m1.2.3.3.2.1" stretchy="false" xref="S5.SS6.SSS2.p1.1.m1.2.3.3.1.cmml">[</mo><mn id="S5.SS6.SSS2.p1.1.m1.1.1" xref="S5.SS6.SSS2.p1.1.m1.1.1.cmml">0</mn><mo id="S5.SS6.SSS2.p1.1.m1.2.3.3.2.2" xref="S5.SS6.SSS2.p1.1.m1.2.3.3.1.cmml">,</mo><mn id="S5.SS6.SSS2.p1.1.m1.2.2" xref="S5.SS6.SSS2.p1.1.m1.2.2.cmml">1</mn><mo id="S5.SS6.SSS2.p1.1.m1.2.3.3.2.3" stretchy="false" xref="S5.SS6.SSS2.p1.1.m1.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS6.SSS2.p1.1.m1.2b"><apply id="S5.SS6.SSS2.p1.1.m1.2.3.cmml" xref="S5.SS6.SSS2.p1.1.m1.2.3"><in id="S5.SS6.SSS2.p1.1.m1.2.3.1.cmml" xref="S5.SS6.SSS2.p1.1.m1.2.3.1"></in><ci id="S5.SS6.SSS2.p1.1.m1.2.3.2.cmml" xref="S5.SS6.SSS2.p1.1.m1.2.3.2">𝜆</ci><interval closure="closed" id="S5.SS6.SSS2.p1.1.m1.2.3.3.1.cmml" xref="S5.SS6.SSS2.p1.1.m1.2.3.3.2"><cn id="S5.SS6.SSS2.p1.1.m1.1.1.cmml" type="integer" xref="S5.SS6.SSS2.p1.1.m1.1.1">0</cn><cn id="S5.SS6.SSS2.p1.1.m1.2.2.cmml" type="integer" xref="S5.SS6.SSS2.p1.1.m1.2.2">1</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.SSS2.p1.1.m1.2c">\lambda\in[0,1]</annotation><annotation encoding="application/x-llamapun" id="S5.SS6.SSS2.p1.1.m1.2d">italic_λ ∈ [ 0 , 1 ]</annotation></semantics></math> for scaling the probability of the applied data augmentations is defined.
The left part of Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S5.F7" title="Figure 7 ‣ 5.6.1 Number of Random Textures ‣ 5.6 Ablative Experiments ‣ 5 Experiments ‣ Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_tag">7</span></a> demonstrates results for object pose estimation with Pix2Pose, the right part shows that for detection with YOLOx on TLESS.
For pose estimation a <math alttext="\lambda" class="ltx_Math" display="inline" id="S5.SS6.SSS2.p1.2.m2.1"><semantics id="S5.SS6.SSS2.p1.2.m2.1a"><mi id="S5.SS6.SSS2.p1.2.m2.1.1" xref="S5.SS6.SSS2.p1.2.m2.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S5.SS6.SSS2.p1.2.m2.1b"><ci id="S5.SS6.SSS2.p1.2.m2.1.1.cmml" xref="S5.SS6.SSS2.p1.2.m2.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.SSS2.p1.2.m2.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S5.SS6.SSS2.p1.2.m2.1d">italic_λ</annotation></semantics></math> of <math alttext="0.2\leq\lambda\leq 0.4" class="ltx_Math" display="inline" id="S5.SS6.SSS2.p1.3.m3.1"><semantics id="S5.SS6.SSS2.p1.3.m3.1a"><mrow id="S5.SS6.SSS2.p1.3.m3.1.1" xref="S5.SS6.SSS2.p1.3.m3.1.1.cmml"><mn id="S5.SS6.SSS2.p1.3.m3.1.1.2" xref="S5.SS6.SSS2.p1.3.m3.1.1.2.cmml">0.2</mn><mo id="S5.SS6.SSS2.p1.3.m3.1.1.3" xref="S5.SS6.SSS2.p1.3.m3.1.1.3.cmml">≤</mo><mi id="S5.SS6.SSS2.p1.3.m3.1.1.4" xref="S5.SS6.SSS2.p1.3.m3.1.1.4.cmml">λ</mi><mo id="S5.SS6.SSS2.p1.3.m3.1.1.5" xref="S5.SS6.SSS2.p1.3.m3.1.1.5.cmml">≤</mo><mn id="S5.SS6.SSS2.p1.3.m3.1.1.6" xref="S5.SS6.SSS2.p1.3.m3.1.1.6.cmml">0.4</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS6.SSS2.p1.3.m3.1b"><apply id="S5.SS6.SSS2.p1.3.m3.1.1.cmml" xref="S5.SS6.SSS2.p1.3.m3.1.1"><and id="S5.SS6.SSS2.p1.3.m3.1.1a.cmml" xref="S5.SS6.SSS2.p1.3.m3.1.1"></and><apply id="S5.SS6.SSS2.p1.3.m3.1.1b.cmml" xref="S5.SS6.SSS2.p1.3.m3.1.1"><leq id="S5.SS6.SSS2.p1.3.m3.1.1.3.cmml" xref="S5.SS6.SSS2.p1.3.m3.1.1.3"></leq><cn id="S5.SS6.SSS2.p1.3.m3.1.1.2.cmml" type="float" xref="S5.SS6.SSS2.p1.3.m3.1.1.2">0.2</cn><ci id="S5.SS6.SSS2.p1.3.m3.1.1.4.cmml" xref="S5.SS6.SSS2.p1.3.m3.1.1.4">𝜆</ci></apply><apply id="S5.SS6.SSS2.p1.3.m3.1.1c.cmml" xref="S5.SS6.SSS2.p1.3.m3.1.1"><leq id="S5.SS6.SSS2.p1.3.m3.1.1.5.cmml" xref="S5.SS6.SSS2.p1.3.m3.1.1.5"></leq><share href="https://arxiv.org/html/2402.04878v2#S5.SS6.SSS2.p1.3.m3.1.1.4.cmml" id="S5.SS6.SSS2.p1.3.m3.1.1d.cmml" xref="S5.SS6.SSS2.p1.3.m3.1.1"></share><cn id="S5.SS6.SSS2.p1.3.m3.1.1.6.cmml" type="float" xref="S5.SS6.SSS2.p1.3.m3.1.1.6">0.4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.SSS2.p1.3.m3.1c">0.2\leq\lambda\leq 0.4</annotation><annotation encoding="application/x-llamapun" id="S5.SS6.SSS2.p1.3.m3.1d">0.2 ≤ italic_λ ≤ 0.4</annotation></semantics></math> results in the most accurate estimates.
For detection a <math alttext="\lambda" class="ltx_Math" display="inline" id="S5.SS6.SSS2.p1.4.m4.1"><semantics id="S5.SS6.SSS2.p1.4.m4.1a"><mi id="S5.SS6.SSS2.p1.4.m4.1.1" xref="S5.SS6.SSS2.p1.4.m4.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S5.SS6.SSS2.p1.4.m4.1b"><ci id="S5.SS6.SSS2.p1.4.m4.1.1.cmml" xref="S5.SS6.SSS2.p1.4.m4.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.SSS2.p1.4.m4.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S5.SS6.SSS2.p1.4.m4.1d">italic_λ</annotation></semantics></math> of <math alttext="0.4" class="ltx_Math" display="inline" id="S5.SS6.SSS2.p1.5.m5.1"><semantics id="S5.SS6.SSS2.p1.5.m5.1a"><mn id="S5.SS6.SSS2.p1.5.m5.1.1" xref="S5.SS6.SSS2.p1.5.m5.1.1.cmml">0.4</mn><annotation-xml encoding="MathML-Content" id="S5.SS6.SSS2.p1.5.m5.1b"><cn id="S5.SS6.SSS2.p1.5.m5.1.1.cmml" type="float" xref="S5.SS6.SSS2.p1.5.m5.1.1">0.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.SSS2.p1.5.m5.1c">0.4</annotation><annotation encoding="application/x-llamapun" id="S5.SS6.SSS2.p1.5.m5.1d">0.4</annotation></semantics></math> achieves the best results.
Generally, while shape-biased pose estimation improves with any set of online data augmentations, the data augmentation for object detection requires tuning to result in the highest mAP when training for a shape bias.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS6.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.6.3 </span>Influence of the Mesh Origin</h4>
<div class="ltx_para" id="S5.SS6.SSS3.p1">
<p class="ltx_p" id="S5.SS6.SSS3.p1.1">This sections ablates the influence of the mesh origin and the type connected bias.
Table <a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S5.T6" title="Table 6 ‣ 5.6.3 Influence of the Mesh Origin ‣ 5.6 Ablative Experiments ‣ 5 Experiments ‣ Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_tag">6</span></a> reports the AR when training Pix2Pose for TLESS.
Ground truth detections are used as image location priors.
The results show that inducing a shape bias with reconstructed objects improves over using the object prior and its reconstructed texture.
The accurate geometry prior of CAD models is in general preferable over the reconstruction, yet again, training for a shape bias improves over training for a color bias; color bias refers to sampling object colors in grayscale, according to the parameters in Table <a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#S3.T1" title="Table 1 ‣ 3 Induction of Shape Bias ‣ Shape-biased Texture Agnostic Representations for Improved Textureless and Metallic Object Detection and 6D Pose Estimation"><span class="ltx_text ltx_ref_tag">1</span></a> and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.04878v2#bib.bib30" title="">30</a>]</cite>.</p>
</div>
<figure class="ltx_table" id="S5.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T6.5.2.1" style="font-size:90%;">Table 6</span>: </span><span class="ltx_text ltx_font_bold" id="S5.T6.2.1" style="font-size:90%;">Mesh Origin.<span class="ltx_text ltx_font_medium" id="S5.T6.2.1.1"> Pose estimation accuracy using Pix2Pose with diverse combination of object geometry and texture, on TLESS. The ground truth detections and an augmentation probability scaling factor of <math alttext="\lambda=0.2" class="ltx_Math" display="inline" id="S5.T6.2.1.1.m1.1"><semantics id="S5.T6.2.1.1.m1.1b"><mrow id="S5.T6.2.1.1.m1.1.1" xref="S5.T6.2.1.1.m1.1.1.cmml"><mi id="S5.T6.2.1.1.m1.1.1.2" xref="S5.T6.2.1.1.m1.1.1.2.cmml">λ</mi><mo id="S5.T6.2.1.1.m1.1.1.1" xref="S5.T6.2.1.1.m1.1.1.1.cmml">=</mo><mn id="S5.T6.2.1.1.m1.1.1.3" xref="S5.T6.2.1.1.m1.1.1.3.cmml">0.2</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T6.2.1.1.m1.1c"><apply id="S5.T6.2.1.1.m1.1.1.cmml" xref="S5.T6.2.1.1.m1.1.1"><eq id="S5.T6.2.1.1.m1.1.1.1.cmml" xref="S5.T6.2.1.1.m1.1.1.1"></eq><ci id="S5.T6.2.1.1.m1.1.1.2.cmml" xref="S5.T6.2.1.1.m1.1.1.2">𝜆</ci><cn id="S5.T6.2.1.1.m1.1.1.3.cmml" type="float" xref="S5.T6.2.1.1.m1.1.1.3">0.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.2.1.1.m1.1d">\lambda=0.2</annotation><annotation encoding="application/x-llamapun" id="S5.T6.2.1.1.m1.1e">italic_λ = 0.2</annotation></semantics></math> are used.</span></span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T6.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T6.6.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T6.6.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T6.6.1.1.1.1">Mesh Origin</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T6.6.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T6.6.1.1.2.1">Bias</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T6.6.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T6.6.1.1.3.1">AR</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T6.6.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T6.6.2.1.1">Reconstructed</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.6.2.1.2">texture</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.6.2.1.3">42.64</td>
</tr>
<tr class="ltx_tr" id="S5.T6.6.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T6.6.3.2.1">Reconstructed</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.6.3.2.2"><span class="ltx_text ltx_font_bold" id="S5.T6.6.3.2.2.1">none (ours)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.6.3.2.3">46.57</td>
</tr>
<tr class="ltx_tr" id="S5.T6.6.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T6.6.4.3.1">
<span class="ltx_ERROR undefined" id="S5.T6.6.4.3.1.1">\cdashline</span>1-3
CAD</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.6.4.3.2">color</td>
<td class="ltx_td ltx_align_center" id="S5.T6.6.4.3.3">59.24</td>
</tr>
<tr class="ltx_tr" id="S5.T6.6.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S5.T6.6.5.4.1">CAD</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T6.6.5.4.2"><span class="ltx_text ltx_font_bold" id="S5.T6.6.5.4.2.1">none (ours)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.6.5.4.3"><span class="ltx_text ltx_font_bold" id="S5.T6.6.5.4.3.1">62.95</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">This paper presents a simple and easily applicable strategy for improving pose estimation of textureless and metallic objects.
Inducing a texture bias to estimators by randomizing object textures during data rendering results in negligible computational overhead to the standard strategy.
The improvements, however, are manifold.
Object detection improves in most of the tested cases, particularly for the most recent detector, pose estimation is on par or improves for both tested object detectors, the pose estimators are more robust to image perturbations.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Future studies will investigate adversarial learning for biasing estimators for shape, in order to further improve detection and pose estimation accuracy and robustness.
The presented study does not consider Vision Transformers and Diffusion models.
Future work will account for them.
Furthermore, since ITODD does not provide ground truth annotations, future work will provide more extensive evaluations for metallic objects.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgement</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">We gratefully acknowledge the support of the EU-program EC Horizon 2020 for Research and Innovation under grant agreement No. 101017089, project TraceBot, the Austrian Science Fund (FWF), under project No. I 6114, project iChores, and the city of Vienna (MA23 – Economic Affairs, Labour and Statistics) through the research project AIAV (MA23 project 26-04).</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.1.1" style="font-size:90%;">
Dominik Bauer, Timothy Patten, and Markus Vincze.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.2.1" style="font-size:90%;">VeREFINE: Integrating object pose verification with physics-guided iterative refinement.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.3.1" style="font-size:90%;">IEEE Robotics and Automation Letters</span><span class="ltx_text" id="bib.bib1.4.2" style="font-size:90%;">, 5(3):4289–4296, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.1.1" style="font-size:90%;">
Sima Behpour, Kris M. Kitani, and Brian D. Ziebart.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.2.1" style="font-size:90%;">Ada: Adversarial data augmentation for object detection.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib2.4.2" style="font-size:90%;">Preceedings of the IEEE Winter Conference on Applications of Computer Vision</span><span class="ltx_text" id="bib.bib2.5.3" style="font-size:90%;">, pages 1243–1252, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.1.1" style="font-size:90%;">
Maximilian Denninger et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.2.1" style="font-size:90%;">Blenderproc.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.3.1" style="font-size:90%;">CoRR</span><span class="ltx_text" id="bib.bib3.4.2" style="font-size:90%;">, abs/1911.01911, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.1.1" style="font-size:90%;">
Bertram Drost, Markus Ulrich, Paul Bergmann, Philipp Hartinger, and Carsten Steger.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.2.1" style="font-size:90%;">Introducing MVTec ITODD - A dataset for 3D object recognition in industry.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib4.4.2" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer Vision Workshops</span><span class="ltx_text" id="bib.bib4.5.3" style="font-size:90%;">, pages 2200–2208, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.1.1" style="font-size:90%;">
Leon A Gatys, Alexander S Ecker, and Matthias Bethge.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.2.1" style="font-size:90%;">Image style transfer using convolutional neural networks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib5.4.2" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib5.5.3" style="font-size:90%;">, pages 2414–2423, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.1.1" style="font-size:90%;">
Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.2.1" style="font-size:90%;">YOLOX: Exceeding YOLO series in 2021.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.3.1" style="font-size:90%;">arXiv preprint arXiv:2107.08430</span><span class="ltx_text" id="bib.bib6.4.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.1.1" style="font-size:90%;">
Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and Wieland Brendel.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.2.1" style="font-size:90%;">Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib7.4.2" style="font-size:90%;">Proceedings of the International Conference on Learning Representations</span><span class="ltx_text" id="bib.bib7.5.3" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.1.1" style="font-size:90%;">
Robert Geirhos, Carlos RM Temme, Jonas Rauber, Heiko H Schütt, Matthias Bethge, and Felix A Wichmann.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.2.1" style="font-size:90%;">Generalisation in humans and deep neural networks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib8.3.1" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span class="ltx_text" id="bib.bib8.4.2" style="font-size:90%;">, 31, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.1.1" style="font-size:90%;">
Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.2.1" style="font-size:90%;">Mask R-CNN.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib9.4.2" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer Vision</span><span class="ltx_text" id="bib.bib9.5.3" style="font-size:90%;">, pages 2961–2969, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.1.1" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.2.1" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib10.4.2" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib10.5.3" style="font-size:90%;">, pages 770–778, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.1.1" style="font-size:90%;">
Zaixing He, Yue Chao, Mengtian Wu, Yilong Hu, and Xinyue Zhao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.2.1" style="font-size:90%;">G-GOP: Generative pose estimation of reflective texture-less metal parts with global-observation-point priors.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.3.1" style="font-size:90%;">IEEE/ASME Transactions on Mechatronics</span><span class="ltx_text" id="bib.bib11.4.2" style="font-size:90%;">, 29(1):154–165, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.1.1" style="font-size:90%;">
Zaixing He et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.2.1" style="font-size:90%;">ContourPose: Monocular 6-D pose estimation method for reflective textureless metal parts.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.3.1" style="font-size:90%;">IEEE Transactions on Robotics</span><span class="ltx_text" id="bib.bib12.4.2" style="font-size:90%;">, 39(5):4037–4050, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.1.1" style="font-size:90%;">
Tomáš Hodaň et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.2.1" style="font-size:90%;">BOP: Benchmark for 6D object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib13.4.2" style="font-size:90%;">Proceedings of the European Conference on Computer Vision</span><span class="ltx_text" id="bib.bib13.5.3" style="font-size:90%;">, pages 19–34, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.1.1" style="font-size:90%;">
Tomáš Hodaň et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.2.1" style="font-size:90%;">Photorealistic image synthesis for object instance detection.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib14.4.2" style="font-size:90%;">Proceedings of the IEEE International Conference on Image Processing</span><span class="ltx_text" id="bib.bib14.5.3" style="font-size:90%;">, pages 66–70. IEEE, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.1.1" style="font-size:90%;">
Tomáš Hodaň et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.2.1" style="font-size:90%;">BOP challenge 2020 on 6D object localization.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib15.3.1" style="font-size:90%;">Proceedings of the European Conference on Compututer Vision Workshops</span><span class="ltx_text" id="bib.bib15.4.2" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.1.1" style="font-size:90%;">
Tomáš Hodaň, Pavel Haluza, Štepán Obdržálek, Jiri Matas, Manolis Lourakis, and Xenophon Zabulis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.2.1" style="font-size:90%;">T-LESS: An RGB-D dataset for 6D pose estimation of texture-less objects.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib16.4.2" style="font-size:90%;">Proceedings of the IEEE Winter Conference on Applications of Computer Vision</span><span class="ltx_text" id="bib.bib16.5.3" style="font-size:90%;">, pages 880–888, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.1.1" style="font-size:90%;">
Yinlin Hu, Pascal Fua, and Mathieu Salzmann.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.2.1" style="font-size:90%;">Perspective flow aggregation for data-limited 6d object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib17.4.2" style="font-size:90%;">Proceedings of the European Conference on Computer Vision</span><span class="ltx_text" id="bib.bib17.5.3" style="font-size:90%;">, pages 89–106. Springer, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.1.1" style="font-size:90%;">
Yinlin Hu, Sebastien Speierer, Wenzel Jakob, Pascal Fua, and Mathieu Salzmann.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.2.1" style="font-size:90%;">Wide-depth-range 6D object pose estimation in space.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib18.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib18.5.3" style="font-size:90%;">, pages 15870–15879, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.1.1" style="font-size:90%;">
Xun Huang and Serge Belongie.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.2.1" style="font-size:90%;">Arbitrary style transfer in real-time with adaptive instance normalization.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib19.4.2" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer Vision</span><span class="ltx_text" id="bib.bib19.5.3" style="font-size:90%;">, pages 1501–1510, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.1.1" style="font-size:90%;">
Roman Kaskman, Sergey Zakharov, Ivan Shugurov, and Slobodan Ilic.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.2.1" style="font-size:90%;">HomebrewedDB: RGB-D dataset for 6D pose estimation of 3D objects.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib20.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</span><span class="ltx_text" id="bib.bib20.5.3" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.1.1" style="font-size:90%;">
Yann Labbé, Justin Carpentier, Mathieu Aubry, and Josef Sivic.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.2.1" style="font-size:90%;">Cosypose: Consistent multi-view multi-object 6d pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib21.4.2" style="font-size:90%;">Proceedings of the European Conference on Computer Vision</span><span class="ltx_text" id="bib.bib21.5.3" style="font-size:90%;">, pages 574–591. Springer, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.1.1" style="font-size:90%;">
Tsung-Yi Lin et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.2.1" style="font-size:90%;">Microsoft COCO: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib22.4.2" style="font-size:90%;">Proceedings of the European Conference on Computer Vision</span><span class="ltx_text" id="bib.bib22.5.3" style="font-size:90%;">, pages 740–755, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.1.1" style="font-size:90%;">
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.2.1" style="font-size:90%;">Focal loss for dense object detection.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib23.4.2" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer Vision</span><span class="ltx_text" id="bib.bib23.5.3" style="font-size:90%;">, pages 2980–2988, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.1.1" style="font-size:90%;">
Xingyu Liu, Ruida Zhang, Chenyangguang Zhang, Bowen Fu, Jiwen Tang, Xiquan Liang, Jingyi Tang, Xiaotian Cheng, Yukang Zhang, Gu Wang, and Xiangyang Ji.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.2.1" style="font-size:90%;">Gdrnpp.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/shanice-l/gdrnpp_bop2022" style="font-size:90%;" title="">https://github.com/shanice-l/gdrnpp_bop2022</a><span class="ltx_text" id="bib.bib24.3.1" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.1.1" style="font-size:90%;">
Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.2.1" style="font-size:90%;">NeRF: Representing scenes as neural radiance fields for view synthesis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib25.3.1" style="font-size:90%;">Communications of the ACM</span><span class="ltx_text" id="bib.bib25.4.2" style="font-size:90%;">, 65(1):99–106, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.1.1" style="font-size:90%;">
Yinyu Nie, Xiaoguang Han, Shihui Guo, Yujian Zheng, Jian Chang, and Jian Jun Zhang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.2.1" style="font-size:90%;">Total3DUnderstanding: Joint layout, object pose and mesh reconstruction for indoor scenes from a single image.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib26.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib26.5.3" style="font-size:90%;">, pages 55–64, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.1.1" style="font-size:90%;">
Kiru Park, Timothy Patten, and Markus Vincze.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.2.1" style="font-size:90%;">Pix2Pose: Pixel-wise coordinate regression of objects for 6D pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib27.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</span><span class="ltx_text" id="bib.bib27.5.3" style="font-size:90%;">, Oct 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.1.1" style="font-size:90%;">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.2.1" style="font-size:90%;">Faster R-CNN: Towards real-time object detection with region proposal networks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib28.3.1" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span class="ltx_text" id="bib.bib28.4.2" style="font-size:90%;">, 28, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.1.1" style="font-size:90%;">
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.2.1" style="font-size:90%;">ImageNet Large Scale Visual Recognition Challenge.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib29.3.1" style="font-size:90%;">International Journal of Computer Vision</span><span class="ltx_text" id="bib.bib29.4.2" style="font-size:90%;">, 115(3):211–252, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.1.1" style="font-size:90%;">
Martin Sundermeyer et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.2.1" style="font-size:90%;">Bop challenge 2022 on detection, segmentation and pose estimation of specific rigid objects.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib30.3.1" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib30.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.1.1" style="font-size:90%;">
Stefan Thalhammer, Dominik Bauer, Peter Hönig, Jean-Baptiste Weibel, José García-Rodríguez, and Markus Vincze.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.2.1" style="font-size:90%;">Challenges for monocular 6D object pose estimation in robotics.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib31.3.1" style="font-size:90%;">arXiv preprint arXiv:2307.12172</span><span class="ltx_text" id="bib.bib31.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.1.1" style="font-size:90%;">
Stefan Thalhammer, Markus Leitner, Timothy Patten, and Markus Vincze.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.2.1" style="font-size:90%;">PyraPose: Feature pyramids for fast and accurate object pose estimation under domain shift.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib32.4.2" style="font-size:90%;">IEEE International Conference on Robotics and Automation</span><span class="ltx_text" id="bib.bib32.5.3" style="font-size:90%;">, pages 13909–13915, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.1.1" style="font-size:90%;">
Stefan Thalhammer, Timothy Patten, and Markus Vincze.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.2.1" style="font-size:90%;">COPE: End-to-end trainable constant runtime object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib33.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</span><span class="ltx_text" id="bib.bib33.5.3" style="font-size:90%;">, pages 2859–2869, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.1.1" style="font-size:90%;">
Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.2.1" style="font-size:90%;">Domain randomization for transferring deep neural networks from simulation to the real world.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib34.4.2" style="font-size:90%;">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems</span><span class="ltx_text" id="bib.bib34.5.3" style="font-size:90%;">, pages 23–30, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.1.1" style="font-size:90%;">
Gu Wang, Fabian Manhardt, Federico Tombari, and Xiangyang Ji.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.2.1" style="font-size:90%;">GDR-Net: Geometry-guided direct regression network for monocular 6D object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib35.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib35.5.3" style="font-size:90%;">, pages 16611–16621, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.1.1" style="font-size:90%;">
Jun Yang, Wenjie Xue, Sahar Ghavidel, and Steven L. Waslander.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.2.1" style="font-size:90%;">6D pose estimation for textureless objects on RGB frames using multi-view optimization.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib36.4.2" style="font-size:90%;">Proceedings of the IEEE International Conference on Robotics and Automation</span><span class="ltx_text" id="bib.bib36.5.3" style="font-size:90%;">, pages 2905–2912, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.1.1" style="font-size:90%;">
Long Yang, Qingan Yan, Yanping Fu, and Chunxia Xiao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.2.1" style="font-size:90%;">Surface reconstruction via fusing sparse-sequence of depth images.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib37.3.1" style="font-size:90%;">IEEE Transactions on Visualization and Computer Graphics</span><span class="ltx_text" id="bib.bib37.4.2" style="font-size:90%;">, 24(2):1190–1203, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.1.1" style="font-size:90%;">
Haixin Yu, Shoujie Li, Houde Liu, Chongkun Xia, Wenbo Ding, and Bin Liang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.2.1" style="font-size:90%;">TGF-Net: Sim2Real transparent object 6D pose estimation based on geometric fusion.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib38.3.1" style="font-size:90%;">IEEE Robotics and Automation Letters</span><span class="ltx_text" id="bib.bib38.4.2" style="font-size:90%;">, 8(6):3868–3875, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.1.1" style="font-size:90%;">
Barret Zoph, Ekin D Cubuk, Golnaz Ghiasi, Tsung-Yi Lin, Jonathon Shlens, and Quoc V Le.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.2.1" style="font-size:90%;">Learning data augmentation strategies for object detection.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib39.4.2" style="font-size:90%;">Proceedings of the European Conference on Computer Vision</span><span class="ltx_text" id="bib.bib39.5.3" style="font-size:90%;">, pages 566–583. Springer, 2020.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jul 23 14:17:37 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
