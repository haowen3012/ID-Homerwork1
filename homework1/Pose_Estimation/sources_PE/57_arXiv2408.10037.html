<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition</title>
<!--Generated on Mon Aug 19 14:24:34 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Egocentric 3D hand pose Action recognition." lang="en" name="keywords"/>
<base href="/html/2408.10037v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S1" title="In SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S2" title="In SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S2.SS0.SSS0.Px1" title="In 2 Related Work ‣ SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_title">Egocentric Hand Pose Estimation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S2.SS0.SSS0.Px2" title="In 2 Related Work ‣ SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_title">Egocentric Action Recognition</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S2.SS0.SSS0.Px3" title="In 2 Related Work ‣ SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_title">Depth Estimation from Single RGB Image</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S2.SS0.SSS0.Px4" title="In 2 Related Work ‣ SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_title">What distinguishes our work</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S3" title="In SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Egocentric 3D Hand Pose Estimation and Action Recognition Enforced With Pseudo Depth</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S3.SS1" title="In 3 Egocentric 3D Hand Pose Estimation and Action Recognition Enforced With Pseudo Depth ‣ SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Egocentric 3D Hand Pose with Pseudo-Depth Segmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S3.SS2" title="In 3 Egocentric 3D Hand Pose Estimation and Action Recognition Enforced With Pseudo Depth ‣ SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Egocentric Action Recognition based on 3D Hand Pose</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S4" title="In SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S4.SS1" title="In 4 Experiments ‣ SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Datasets</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S4.SS1.SSS0.Px1" title="In 4.1 Datasets ‣ 4 Experiments ‣ SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_title">H2O Dataset</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S4.SS2" title="In 4 Experiments ‣ SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S4.SS3" title="In 4 Experiments ‣ SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Experiment setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S4.SS3.SSS0.Px1" title="In 4.3 Experiment setup ‣ 4 Experiments ‣ SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_title">3D Hand Pose Estimation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S4.SS3.SSS0.Px2" title="In 4.3 Experiment setup ‣ 4 Experiments ‣ SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_title">Action Recognition</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S4.SS4" title="In 4 Experiments ‣ SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Comparison with State of the Art</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S4.SS5" title="In 4 Experiments ‣ SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Ablation Studies</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S4.SS5.SSS0.Px1" title="In 4.5 Ablation Studies ‣ 4 Experiments ‣ SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_title">The range of human arms in training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S4.SS5.SSS0.Px2" title="In 4.5 Ablation Studies ‣ 4 Experiments ‣ SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_title">The range of human arms in inference</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S4.SS5.SSS0.Px3" title="In 4.5 Ablation Studies ‣ 4 Experiments ‣ SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_title">Pseudo-depth-based SHARP module</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S4.SS5.SSS0.Px4" title="In 4.5 Ablation Studies ‣ 4 Experiments ‣ SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_title">Oracle depth-based SHARP module</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S4.SS5.SSS0.Px5" title="In 4.5 Ablation Studies ‣ 4 Experiments ‣ SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_title">De-sharpening of segmentation mask</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S5" title="In SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S5.SS0.SSS1" title="In 5 Conclusion ‣ SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.0.1 </span>Acknowledgements</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Computer Vision Lab, TU Wien, Favoritenstr. 9/193-1, 1040 Vienna, Austria <span class="ltx_note ltx_role_email" id="id1.1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>{wiktor.mucha,martin.kampel}@tuwien.ac.at</span></span></span></span></span></span><span class="ltx_note ltx_role_institutetext" id="id2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>University of Bristol 
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id2.1"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">email: </span>michael.wray@bristol.ac.uk</span></span></span>
<br class="ltx_break"/></span></span></span>
<h1 class="ltx_title ltx_title_document">SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wiktor Mucha 
</span><span class="ltx_author_notes">11
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-6048-3425" title="ORCID identifier">0000-0002-6048-3425</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Michael Wray
</span><span class="ltx_author_notes">22
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0001-5918-9029" title="ORCID identifier">0000-0001-5918-9029</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Martin Kampel
</span><span class="ltx_author_notes">11
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-5217-2854" title="ORCID identifier">0000-0002-5217-2854</a></span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Hand pose represents key information for action recognition in the egocentric perspective, where the user is interacting with objects. We propose to improve egocentric 3D hand pose estimation based on RGB frames only by using pseudo-depth images. Incorporating state-of-the-art single RGB image depth estimation techniques, we generate pseudo-depth representations of the frames and use distance knowledge to segment irrelevant parts of the scene. The resulting depth maps are then used as segmentation masks for the RGB frames. Experimental results on <span class="ltx_text ltx_font_italic" id="id1.id1.1">H2O Dataset</span> confirm the high accuracy of the estimated pose with our method in an action recognition task.
The 3D hand pose, together with information from object detection, is processed by a transformer-based action recognition network, resulting in an accuracy of 91.73%, outperforming all state-of-the-art methods. Estimations of 3D hand pose result in competitive performance with existing methods with a mean pose error of 28.66 mm. This method opens up new possibilities for employing distance information in egocentric 3D hand pose estimation without relying on depth sensors. The code is available under <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/wiktormucha/SHARP" title="">https://github.com/wiktormucha/SHARP</a></p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>
Egocentric 3D hand pose Action recognition.
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="291" id="S1.F1.g1" src="extracted/5800341/images/icpr_teaser.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overview of our method. In the sequence of input frames <math alttext="f_{1},f_{2},f_{3}\dots f_{n}" class="ltx_Math" display="inline" id="S1.F1.5.m1.3"><semantics id="S1.F1.5.m1.3b"><mrow id="S1.F1.5.m1.3.3.3" xref="S1.F1.5.m1.3.3.4.cmml"><msub id="S1.F1.5.m1.1.1.1.1" xref="S1.F1.5.m1.1.1.1.1.cmml"><mi id="S1.F1.5.m1.1.1.1.1.2" xref="S1.F1.5.m1.1.1.1.1.2.cmml">f</mi><mn id="S1.F1.5.m1.1.1.1.1.3" xref="S1.F1.5.m1.1.1.1.1.3.cmml">1</mn></msub><mo id="S1.F1.5.m1.3.3.3.4" xref="S1.F1.5.m1.3.3.4.cmml">,</mo><msub id="S1.F1.5.m1.2.2.2.2" xref="S1.F1.5.m1.2.2.2.2.cmml"><mi id="S1.F1.5.m1.2.2.2.2.2" xref="S1.F1.5.m1.2.2.2.2.2.cmml">f</mi><mn id="S1.F1.5.m1.2.2.2.2.3" xref="S1.F1.5.m1.2.2.2.2.3.cmml">2</mn></msub><mo id="S1.F1.5.m1.3.3.3.5" xref="S1.F1.5.m1.3.3.4.cmml">,</mo><mrow id="S1.F1.5.m1.3.3.3.3" xref="S1.F1.5.m1.3.3.3.3.cmml"><msub id="S1.F1.5.m1.3.3.3.3.2" xref="S1.F1.5.m1.3.3.3.3.2.cmml"><mi id="S1.F1.5.m1.3.3.3.3.2.2" xref="S1.F1.5.m1.3.3.3.3.2.2.cmml">f</mi><mn id="S1.F1.5.m1.3.3.3.3.2.3" xref="S1.F1.5.m1.3.3.3.3.2.3.cmml">3</mn></msub><mo id="S1.F1.5.m1.3.3.3.3.1" xref="S1.F1.5.m1.3.3.3.3.1.cmml">⁢</mo><mi id="S1.F1.5.m1.3.3.3.3.3" mathvariant="normal" xref="S1.F1.5.m1.3.3.3.3.3.cmml">…</mi><mo id="S1.F1.5.m1.3.3.3.3.1b" xref="S1.F1.5.m1.3.3.3.3.1.cmml">⁢</mo><msub id="S1.F1.5.m1.3.3.3.3.4" xref="S1.F1.5.m1.3.3.3.3.4.cmml"><mi id="S1.F1.5.m1.3.3.3.3.4.2" xref="S1.F1.5.m1.3.3.3.3.4.2.cmml">f</mi><mi id="S1.F1.5.m1.3.3.3.3.4.3" xref="S1.F1.5.m1.3.3.3.3.4.3.cmml">n</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.5.m1.3c"><list id="S1.F1.5.m1.3.3.4.cmml" xref="S1.F1.5.m1.3.3.3"><apply id="S1.F1.5.m1.1.1.1.1.cmml" xref="S1.F1.5.m1.1.1.1.1"><csymbol cd="ambiguous" id="S1.F1.5.m1.1.1.1.1.1.cmml" xref="S1.F1.5.m1.1.1.1.1">subscript</csymbol><ci id="S1.F1.5.m1.1.1.1.1.2.cmml" xref="S1.F1.5.m1.1.1.1.1.2">𝑓</ci><cn id="S1.F1.5.m1.1.1.1.1.3.cmml" type="integer" xref="S1.F1.5.m1.1.1.1.1.3">1</cn></apply><apply id="S1.F1.5.m1.2.2.2.2.cmml" xref="S1.F1.5.m1.2.2.2.2"><csymbol cd="ambiguous" id="S1.F1.5.m1.2.2.2.2.1.cmml" xref="S1.F1.5.m1.2.2.2.2">subscript</csymbol><ci id="S1.F1.5.m1.2.2.2.2.2.cmml" xref="S1.F1.5.m1.2.2.2.2.2">𝑓</ci><cn id="S1.F1.5.m1.2.2.2.2.3.cmml" type="integer" xref="S1.F1.5.m1.2.2.2.2.3">2</cn></apply><apply id="S1.F1.5.m1.3.3.3.3.cmml" xref="S1.F1.5.m1.3.3.3.3"><times id="S1.F1.5.m1.3.3.3.3.1.cmml" xref="S1.F1.5.m1.3.3.3.3.1"></times><apply id="S1.F1.5.m1.3.3.3.3.2.cmml" xref="S1.F1.5.m1.3.3.3.3.2"><csymbol cd="ambiguous" id="S1.F1.5.m1.3.3.3.3.2.1.cmml" xref="S1.F1.5.m1.3.3.3.3.2">subscript</csymbol><ci id="S1.F1.5.m1.3.3.3.3.2.2.cmml" xref="S1.F1.5.m1.3.3.3.3.2.2">𝑓</ci><cn id="S1.F1.5.m1.3.3.3.3.2.3.cmml" type="integer" xref="S1.F1.5.m1.3.3.3.3.2.3">3</cn></apply><ci id="S1.F1.5.m1.3.3.3.3.3.cmml" xref="S1.F1.5.m1.3.3.3.3.3">…</ci><apply id="S1.F1.5.m1.3.3.3.3.4.cmml" xref="S1.F1.5.m1.3.3.3.3.4"><csymbol cd="ambiguous" id="S1.F1.5.m1.3.3.3.3.4.1.cmml" xref="S1.F1.5.m1.3.3.3.3.4">subscript</csymbol><ci id="S1.F1.5.m1.3.3.3.3.4.2.cmml" xref="S1.F1.5.m1.3.3.3.3.4.2">𝑓</ci><ci id="S1.F1.5.m1.3.3.3.3.4.3.cmml" xref="S1.F1.5.m1.3.3.3.3.4.3">𝑛</ci></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.5.m1.3d">f_{1},f_{2},f_{3}\dots f_{n}</annotation><annotation encoding="application/x-llamapun" id="S1.F1.5.m1.3e">italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT … italic_f start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> representing the action, <span class="ltx_text ltx_font_italic" id="S1.F1.11.1">SHARP</span> improves the estimation of the 3D hand pose <math alttext="Ph^{3D}_{L,R,n}" class="ltx_Math" display="inline" id="S1.F1.6.m2.3"><semantics id="S1.F1.6.m2.3b"><mrow id="S1.F1.6.m2.3.4" xref="S1.F1.6.m2.3.4.cmml"><mi id="S1.F1.6.m2.3.4.2" xref="S1.F1.6.m2.3.4.2.cmml">P</mi><mo id="S1.F1.6.m2.3.4.1" xref="S1.F1.6.m2.3.4.1.cmml">⁢</mo><msubsup id="S1.F1.6.m2.3.4.3" xref="S1.F1.6.m2.3.4.3.cmml"><mi id="S1.F1.6.m2.3.4.3.2.2" xref="S1.F1.6.m2.3.4.3.2.2.cmml">h</mi><mrow id="S1.F1.6.m2.3.3.3.5" xref="S1.F1.6.m2.3.3.3.4.cmml"><mi id="S1.F1.6.m2.1.1.1.1" xref="S1.F1.6.m2.1.1.1.1.cmml">L</mi><mo id="S1.F1.6.m2.3.3.3.5.1" xref="S1.F1.6.m2.3.3.3.4.cmml">,</mo><mi id="S1.F1.6.m2.2.2.2.2" xref="S1.F1.6.m2.2.2.2.2.cmml">R</mi><mo id="S1.F1.6.m2.3.3.3.5.2" xref="S1.F1.6.m2.3.3.3.4.cmml">,</mo><mi id="S1.F1.6.m2.3.3.3.3" xref="S1.F1.6.m2.3.3.3.3.cmml">n</mi></mrow><mrow id="S1.F1.6.m2.3.4.3.2.3" xref="S1.F1.6.m2.3.4.3.2.3.cmml"><mn id="S1.F1.6.m2.3.4.3.2.3.2" xref="S1.F1.6.m2.3.4.3.2.3.2.cmml">3</mn><mo id="S1.F1.6.m2.3.4.3.2.3.1" xref="S1.F1.6.m2.3.4.3.2.3.1.cmml">⁢</mo><mi id="S1.F1.6.m2.3.4.3.2.3.3" xref="S1.F1.6.m2.3.4.3.2.3.3.cmml">D</mi></mrow></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.6.m2.3c"><apply id="S1.F1.6.m2.3.4.cmml" xref="S1.F1.6.m2.3.4"><times id="S1.F1.6.m2.3.4.1.cmml" xref="S1.F1.6.m2.3.4.1"></times><ci id="S1.F1.6.m2.3.4.2.cmml" xref="S1.F1.6.m2.3.4.2">𝑃</ci><apply id="S1.F1.6.m2.3.4.3.cmml" xref="S1.F1.6.m2.3.4.3"><csymbol cd="ambiguous" id="S1.F1.6.m2.3.4.3.1.cmml" xref="S1.F1.6.m2.3.4.3">subscript</csymbol><apply id="S1.F1.6.m2.3.4.3.2.cmml" xref="S1.F1.6.m2.3.4.3"><csymbol cd="ambiguous" id="S1.F1.6.m2.3.4.3.2.1.cmml" xref="S1.F1.6.m2.3.4.3">superscript</csymbol><ci id="S1.F1.6.m2.3.4.3.2.2.cmml" xref="S1.F1.6.m2.3.4.3.2.2">ℎ</ci><apply id="S1.F1.6.m2.3.4.3.2.3.cmml" xref="S1.F1.6.m2.3.4.3.2.3"><times id="S1.F1.6.m2.3.4.3.2.3.1.cmml" xref="S1.F1.6.m2.3.4.3.2.3.1"></times><cn id="S1.F1.6.m2.3.4.3.2.3.2.cmml" type="integer" xref="S1.F1.6.m2.3.4.3.2.3.2">3</cn><ci id="S1.F1.6.m2.3.4.3.2.3.3.cmml" xref="S1.F1.6.m2.3.4.3.2.3.3">𝐷</ci></apply></apply><list id="S1.F1.6.m2.3.3.3.4.cmml" xref="S1.F1.6.m2.3.3.3.5"><ci id="S1.F1.6.m2.1.1.1.1.cmml" xref="S1.F1.6.m2.1.1.1.1">𝐿</ci><ci id="S1.F1.6.m2.2.2.2.2.cmml" xref="S1.F1.6.m2.2.2.2.2">𝑅</ci><ci id="S1.F1.6.m2.3.3.3.3.cmml" xref="S1.F1.6.m2.3.3.3.3">𝑛</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.6.m2.3d">Ph^{3D}_{L,R,n}</annotation><annotation encoding="application/x-llamapun" id="S1.F1.6.m2.3e">italic_P italic_h start_POSTSUPERSCRIPT 3 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_L , italic_R , italic_n end_POSTSUBSCRIPT</annotation></semantics></math>. The bounding box of the manipulated objects <math alttext="Po^{2D}_{n}" class="ltx_Math" display="inline" id="S1.F1.7.m3.1"><semantics id="S1.F1.7.m3.1b"><mrow id="S1.F1.7.m3.1.1" xref="S1.F1.7.m3.1.1.cmml"><mi id="S1.F1.7.m3.1.1.2" xref="S1.F1.7.m3.1.1.2.cmml">P</mi><mo id="S1.F1.7.m3.1.1.1" xref="S1.F1.7.m3.1.1.1.cmml">⁢</mo><msubsup id="S1.F1.7.m3.1.1.3" xref="S1.F1.7.m3.1.1.3.cmml"><mi id="S1.F1.7.m3.1.1.3.2.2" xref="S1.F1.7.m3.1.1.3.2.2.cmml">o</mi><mi id="S1.F1.7.m3.1.1.3.3" xref="S1.F1.7.m3.1.1.3.3.cmml">n</mi><mrow id="S1.F1.7.m3.1.1.3.2.3" xref="S1.F1.7.m3.1.1.3.2.3.cmml"><mn id="S1.F1.7.m3.1.1.3.2.3.2" xref="S1.F1.7.m3.1.1.3.2.3.2.cmml">2</mn><mo id="S1.F1.7.m3.1.1.3.2.3.1" xref="S1.F1.7.m3.1.1.3.2.3.1.cmml">⁢</mo><mi id="S1.F1.7.m3.1.1.3.2.3.3" xref="S1.F1.7.m3.1.1.3.2.3.3.cmml">D</mi></mrow></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.7.m3.1c"><apply id="S1.F1.7.m3.1.1.cmml" xref="S1.F1.7.m3.1.1"><times id="S1.F1.7.m3.1.1.1.cmml" xref="S1.F1.7.m3.1.1.1"></times><ci id="S1.F1.7.m3.1.1.2.cmml" xref="S1.F1.7.m3.1.1.2">𝑃</ci><apply id="S1.F1.7.m3.1.1.3.cmml" xref="S1.F1.7.m3.1.1.3"><csymbol cd="ambiguous" id="S1.F1.7.m3.1.1.3.1.cmml" xref="S1.F1.7.m3.1.1.3">subscript</csymbol><apply id="S1.F1.7.m3.1.1.3.2.cmml" xref="S1.F1.7.m3.1.1.3"><csymbol cd="ambiguous" id="S1.F1.7.m3.1.1.3.2.1.cmml" xref="S1.F1.7.m3.1.1.3">superscript</csymbol><ci id="S1.F1.7.m3.1.1.3.2.2.cmml" xref="S1.F1.7.m3.1.1.3.2.2">𝑜</ci><apply id="S1.F1.7.m3.1.1.3.2.3.cmml" xref="S1.F1.7.m3.1.1.3.2.3"><times id="S1.F1.7.m3.1.1.3.2.3.1.cmml" xref="S1.F1.7.m3.1.1.3.2.3.1"></times><cn id="S1.F1.7.m3.1.1.3.2.3.2.cmml" type="integer" xref="S1.F1.7.m3.1.1.3.2.3.2">2</cn><ci id="S1.F1.7.m3.1.1.3.2.3.3.cmml" xref="S1.F1.7.m3.1.1.3.2.3.3">𝐷</ci></apply></apply><ci id="S1.F1.7.m3.1.1.3.3.cmml" xref="S1.F1.7.m3.1.1.3.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.7.m3.1d">Po^{2D}_{n}</annotation><annotation encoding="application/x-llamapun" id="S1.F1.7.m3.1e">italic_P italic_o start_POSTSUPERSCRIPT 2 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> with their labels <math alttext="Po_{l}" class="ltx_Math" display="inline" id="S1.F1.8.m4.1"><semantics id="S1.F1.8.m4.1b"><mrow id="S1.F1.8.m4.1.1" xref="S1.F1.8.m4.1.1.cmml"><mi id="S1.F1.8.m4.1.1.2" xref="S1.F1.8.m4.1.1.2.cmml">P</mi><mo id="S1.F1.8.m4.1.1.1" xref="S1.F1.8.m4.1.1.1.cmml">⁢</mo><msub id="S1.F1.8.m4.1.1.3" xref="S1.F1.8.m4.1.1.3.cmml"><mi id="S1.F1.8.m4.1.1.3.2" xref="S1.F1.8.m4.1.1.3.2.cmml">o</mi><mi id="S1.F1.8.m4.1.1.3.3" xref="S1.F1.8.m4.1.1.3.3.cmml">l</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.8.m4.1c"><apply id="S1.F1.8.m4.1.1.cmml" xref="S1.F1.8.m4.1.1"><times id="S1.F1.8.m4.1.1.1.cmml" xref="S1.F1.8.m4.1.1.1"></times><ci id="S1.F1.8.m4.1.1.2.cmml" xref="S1.F1.8.m4.1.1.2">𝑃</ci><apply id="S1.F1.8.m4.1.1.3.cmml" xref="S1.F1.8.m4.1.1.3"><csymbol cd="ambiguous" id="S1.F1.8.m4.1.1.3.1.cmml" xref="S1.F1.8.m4.1.1.3">subscript</csymbol><ci id="S1.F1.8.m4.1.1.3.2.cmml" xref="S1.F1.8.m4.1.1.3.2">𝑜</ci><ci id="S1.F1.8.m4.1.1.3.3.cmml" xref="S1.F1.8.m4.1.1.3.3">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.8.m4.1d">Po_{l}</annotation><annotation encoding="application/x-llamapun" id="S1.F1.8.m4.1e">italic_P italic_o start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT</annotation></semantics></math> are retrieved using <span class="ltx_text ltx_font_italic" id="S1.F1.12.2">YOLOv7</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib27" title="">27</a>]</cite>. Pose information is embedded in a vector describing each frame. The sequence of vectors is processed by the transformer-based network to predict action.</figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In recent years, one of the growing research areas in computer vision has been egocentric vision, as evidenced by the increasing number and size of published datasets <span class="ltx_text ltx_font_italic" id="S1.p1.1.1">EPIC-KITCHENS</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib6" title="">6</a>]</cite>, <span class="ltx_text ltx_font_italic" id="S1.p1.1.2">Ego4D</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib14" title="">14</a>]</cite>, <span class="ltx_text ltx_font_italic" id="S1.p1.1.3">H2O</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib16" title="">16</a>]</cite> and release of devices like Ray-Ban Stories, Apple Vision Pro or HoloLens.
One of the challenges in egocentric vision is understanding human-object interaction based on hand pose estimation and action recognition <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib16" title="">16</a>]</cite>.
The hand pose estimation task is described as the challenge of estimating the position of key points representing the joints of a human hand in two or three-dimensional space. Estimated positions are a valuable source of information for recognising the actions performed by a camera wearer, linking these two tasks.
Egocentric action recognition research is of great importance in various domains, including augmented and virtual reality, nutritional behaviour analysis, and Active Assisted Living (AAL) technologies for lifestyle analysis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib21" title="">21</a>]</cite> or assistance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib17" title="">17</a>]</cite>. As AAL technologies mainly target Activities of Daily Living (ADLs) such as drinking, eating and food preparation, which are inherently manual and involve object manipulation, there’s a growing interest in research focused on hand-based action recognition.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Current work on egocentric hand-based action recognition focuses on 3D hand pose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib16" title="">16</a>]</cite> using a single RGB camera. As a result, these studies regress <math alttext="z" class="ltx_Math" display="inline" id="S1.p2.1.m1.1"><semantics id="S1.p2.1.m1.1a"><mi id="S1.p2.1.m1.1.1" xref="S1.p2.1.m1.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.1b"><ci id="S1.p2.1.m1.1.1.cmml" xref="S1.p2.1.m1.1.1">𝑧</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.1.m1.1c">z</annotation><annotation encoding="application/x-llamapun" id="S1.p2.1.m1.1d">italic_z</annotation></semantics></math> coordinate from RGB frames, which introduces complexity and results in pose prediction errors of around 40 mm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib16" title="">16</a>]</cite> (equivalent to a 20.5% error considering an average human hand size of 18 cm), which is far from the desired performance, especially considering that publicly available datasets for egocentric hand pose are captured in a laboratory environment. Accurate pose prediction is essential for hand-based action recognition <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib18" title="">18</a>]</cite>. The improvement in 3D prediction could be further enhanced by the use of a depth sensor, but there’s currently no portable depth sensor on the market. Despite market availability, an additional sensor would add undesired costs due to power and processing requirements. Data growth for training and research is another constraint, as labelling key points in 3D space is difficult and requires, for example, a laboratory multi-view camera setup <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib22" title="">22</a>]</cite>. All these circumstances create a need and motivate our research to explore new techniques and solutions to improve egocentric 3D pose estimation based on RGB images only.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Our study proposes the use of pseudo-depth images, depth images generated from a single RGB image using state-of-the-art depth estimation methods. The resulting distance representation of the scene does not contain real depth values, but it allows for the removal of non-relevant information in the scene depending on the distance. In an egocentric perspective, human arms have a constant maximum distance from the camera because the camera is mounted in a fixed position on the human body. This characteristic allows for the removal of the values representing the parts of the scene beyond this distance, leaving the input image of a hand pose estimation network with only hands and manipulated objects visible. We call this process Segmentation of Hands and Arms by Range using Pseudo-depth (SHARP). This solution requires no additional sensors; it can be applied to any RGB input data; no additional training of the depth estimation model is required; and compared to background subtraction based on image sequences, only a single RGB image is required. These advantages are confirmed by a performance improvement of 7 mm, reducing the mean pose error from 35.48 mm to 28.66 mm from the baseline. The overview of the method is presented in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_tag">1</span></a>. Our contribution can be listed as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">Inspired by superior egocentric hand pose estimation in 2D over other methods, we extend the state-of-the-art <span class="ltx_text ltx_font_italic" id="S1.I1.i1.p1.1.1">EffHandEgoNet</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib18" title="">18</a>]</cite> to 3D pose estimation, resulting in a new architecture called <span class="ltx_text ltx_font_italic" id="S1.I1.i1.p1.1.2">EffHandEgoNet3D</span>.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">On the top of <span class="ltx_text ltx_font_italic" id="S1.I1.i2.p1.1.1">EffHandEgoNet3D</span> we propose <span class="ltx_text ltx_font_italic" id="S1.I1.i2.p1.1.2">SHARP module</span>, a novel idea for egocentric scene segmentation to improve hand-object interaction understanding. A state-of-the-art depth estimation model is used to generate a pseudo-depth scene representation. Furthermore, the generated distance knowledge is used to remove irrelevant information in the scene with a fixed distance over the range of the human arms, resulting in the preservation of the human arms and the interacting object. <span class="ltx_text ltx_font_italic" id="S1.I1.i2.p1.1.3">SHARP</span> requires no additional training and can be applied to any egocentric RGB data. The proposed architecture outperforms several state-of-the-art studies, achieving a mean error of 28.66 mm on the <span class="ltx_text ltx_font_italic" id="S1.I1.i2.p1.1.4">H2O Dataset</span>.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We implement an action recognition network based on a transformer architecture. It uses previously estimated 3D hand pose and 2D object detection information as input. The network outperforms the state-of-the-art on the <span class="ltx_text ltx_font_italic" id="S1.I1.i3.p1.1.1">H2O Dataset</span>, including methods that use more information e.g. 6D object pose, reaching 91.73% action recognition accuracy.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">We present extensive experiments and ablations performed on <span class="ltx_text ltx_font_italic" id="S1.I1.i4.p1.1.1">H2O Dataset</span>, showing the influence of the proposed scene segmentation method on the performance of 3D hand pose estimation in the egocentric perspective.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">The structure of the paper is as follows: In section <a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S2" title="2 Related Work ‣ SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_tag">2</span></a>, we review related research on egocentric 3D hand keypoint estimation, hand-based action recognition, and depth estimation using a single RGB image, and identify opportunities for improvement. Section <a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S3" title="3 Egocentric 3D Hand Pose Estimation and Action Recognition Enforced With Pseudo Depth ‣ SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_tag">3</span></a> details our approach and its implementation. Our evaluation and experimental results are presented in section <a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S4" title="4 Experiments ‣ SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_tag">4</span></a>. Finally, section <a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S5" title="5 Conclusion ‣ SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_tag">5</span></a> concludes the study, summarising its main findings and limitations.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">Egocentric Hand Pose Estimation</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">Hand pose estimation in egocentric vision faces challenges such as self-occlusion, limited field of view, and diverse perspectives, which hinder effective generalisation. Some approaches overcome these obstacles by using RGB-D sensors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib11" title="">11</a>]</cite>. However, the adoption of depth sensors is hampered by limited market availability, directing towards self-made solutions and increasing computing and power costs. Due to device limitations, several studies estimate 3D keypoints from RGB images only by using neural networks that estimate the z coordinate representing depth along x and y, followed by a conversion from 2D to 3D space using intrinsic camera parameters <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib16" title="">16</a>]</cite>.
For example, Tekin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib26" title="">26</a>]</cite> compute the 3D pose of a hand directly from a single RGB image using a convolutional neural network (CNN) that outputs a 3D grid with the probability of target pose values in each cell. Similarly, Kwon et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib16" title="">16</a>]</cite> extend this approach to estimate poses for both hands. However, these methods report a mean end-point error (EPE) of 37 mm for hand pose estimation in the <span class="ltx_text ltx_font_italic" id="S2.SS0.SSS0.Px1.p1.1.1">H2O dataset</span>, suggesting room for improvement given the average human hand size of 18 cm. Cho et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib5" title="">5</a>]</cite> use CNNs with transformer-based networks for 3D pose reconstruction on a frame-by-frame basis, while Wen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib30" title="">30</a>]</cite> propose a sequence-based approach for depth reconstruction that addresses occlusion challenges.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">Egocentric Action Recognition</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">A common strategy for action recognition involves the joint processing of hand and object information. Cartas et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib3" title="">3</a>]</cite> proposes CNN-based object detectors to estimate the positions of primary regions (hands) and secondary regions (objects). Temporal information from these regions is then processed by a Long Short-Term Memory (LSTM) network. Nguyen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib20" title="">20</a>]</cite> Transition from bounding box information to 2D skeletons of a single hand estimated by CNN from RGB and depth images. The joints of these skeletons are aggregated using spatial and temporal Gaussian aggregation, and action recognition is performed using a learnable Symmetric Positive Definite (SPD) matrix. With the rise of 3D-based hand pose estimation algorithms, the scientific community has increasingly focused on egocentric action understanding using 3D information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib16" title="">16</a>]</cite>. Tekin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib26" title="">26</a>]</cite> estimate 3D hand and object poses from a single RGB frame using a CNN, embedding temporal information to predict action classes using an LSTM. Other techniques use graph networks, such as Das et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib7" title="">7</a>]</cite>, who present a spatio-temporal graph CNN architecture that describes finger motion using separate subgraphs. Kwon et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib16" title="">16</a>]</cite> construct sub-graphs for each hand and object, which are merged into a multigraph model, allowing learning of interactions between these components. Wen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib30" title="">30</a>]</cite> use a transformer-based model with estimated 3D hand pose and object label input. Cho et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib5" title="">5</a>]</cite> enrich the transformer inputs with object pose and hand-object contact information. However, these studies do not make use of depth data. Instead, they estimate points in 3D space using neural networks and intrinsic camera parameters <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib5" title="">5</a>]</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px3">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">Depth Estimation from Single RGB Image</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p1.1">Recent advances in depth estimation have relied on CNNs for direct regression of scene depth from input images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib9" title="">9</a>]</cite>. These methods often struggle to generalise to unconstrained scenes due to the limited diversity and size of the training data. Garg et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib12" title="">12</a>]</cite> proposed the use of calibrated stereo cameras for self-supervision, which simplifies data acquisition but maintains constraints on specific data regimes. Despite subsequent self-supervised approaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib13" title="">13</a>]</cite>, challenges remain, particularly for dynamic scenes. Efforts to overcome these limitations include crowd-sourced annotation of ordinal relationships <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib4" title="">4</a>]</cite>, but existing datasets are often biased or lack dynamic objects, making it difficult to generalise to less constrained environments. In response, Ranftl et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib24" title="">24</a>]</cite> propose tools for mixing multiple datasets, even with incompatible annotations. Their approach incorporates a robust training objective, principled multi-objective learning, and emphasises pre-training of encoders on ancillary tasks. By training on five different sources, including a rich dataset of 3D movies, they outperform state-of-the-art depth estimation models in zero-shot cross-dataset performance.
As an extension of this work, Ranftl et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib23" title="">23</a>]</cite> present <span class="ltx_text ltx_font_italic" id="S2.SS0.SSS0.Px3.p1.1.1">DPT-Hybrid</span> and <span class="ltx_text ltx_font_italic" id="S2.SS0.SSS0.Px3.p1.1.2">DPT-Large</span> architectures enhanced with dense prediction transformers, which use vision transformers instead of CNNs, further improving the performance of depth estimation.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px4">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">
What distinguishes our work</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px4.p1.1">from other studies of egocentric 3D hand pose is the use of a depth estimation that we incorporate into <span class="ltx_text ltx_font_italic" id="S2.SS0.SSS0.Px4.p1.1.1">SHARP</span> module. Using state-of-the-art single RGB image depth estimation techniques, we generate a pseudo-depth representation of the image without any additional equipment. Knowing that the distance of the human arms from the camera in an egocentric view is constant, we then use this generated depth image to segment irrelevant information from the scene using a fixed distance threshold, thereby unifying the dataset for hand pose estimation. This methodology ensures that the hand pose estimation model only considers hands and manipulated objects, thereby increasing accuracy and efficiency, and can be applied to any RGB dataset.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Egocentric 3D Hand Pose Estimation and Action Recognition Enforced With Pseudo Depth</h2>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="224" id="S3.F2.g1" src="extracted/5800341/images/icpr_hp_overview.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>
Overview of the proposed egocentric 3D hand pose estimation method. First, the RGB image is processed with the <span class="ltx_text ltx_font_italic" id="S3.F2.7.1">SHARP</span> module. Within <span class="ltx_text ltx_font_italic" id="S3.F2.8.2">SHARP</span>, the pseudo-depth image is generated using the <span class="ltx_text ltx_font_italic" id="S3.F2.9.3">DPT-Hybrid</span>. This distance representation is used to remove irrelevant scene information using a fixed threshold of the human arm range <math alttext="t" class="ltx_Math" display="inline" id="S3.F2.2.m1.1"><semantics id="S3.F2.2.m1.1b"><mi id="S3.F2.2.m1.1.1" xref="S3.F2.2.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.F2.2.m1.1c"><ci id="S3.F2.2.m1.1.1.cmml" xref="S3.F2.2.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.2.m1.1d">t</annotation><annotation encoding="application/x-llamapun" id="S3.F2.2.m1.1e">italic_t</annotation></semantics></math>. Secondly, the <span class="ltx_text ltx_font_italic" id="S3.F2.10.4">SHARP</span> output is passed through a 3D hand pose estimation network.
</figcaption>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">The study considers the tasks of egocentric 3D hand pose estimation and action recognition. These two tasks are correlated but significantly different, so the methodology is described separately for each.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Egocentric 3D Hand Pose with Pseudo-Depth Segmentation</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.10">In the first stage, each RGB frame <math alttext="f_{n}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><msub id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">f</mi><mi id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">𝑓</ci><ci id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">f_{n}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">italic_f start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> undergoes processing with <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.10.1">SHARP</span> module which consists of a depth estimation model <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.10.2">DPT-Hybrid</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib23" title="">23</a>]</cite>, yielding a pseudo-depth representation <math alttext="I^{D}_{n}" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><msubsup id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2.2" xref="S3.SS1.p1.2.m2.1.1.2.2.cmml">I</mi><mi id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">n</mi><mi id="S3.SS1.p1.2.m2.1.1.2.3" xref="S3.SS1.p1.2.m2.1.1.2.3.cmml">D</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">subscript</csymbol><apply id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.2.1.cmml" xref="S3.SS1.p1.2.m2.1.1">superscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2.2">𝐼</ci><ci id="S3.SS1.p1.2.m2.1.1.2.3.cmml" xref="S3.SS1.p1.2.m2.1.1.2.3">𝐷</ci></apply><ci id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">I^{D}_{n}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">italic_I start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> of the frame <math alttext="f_{n}" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><msub id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">f</mi><mi id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">𝑓</ci><ci id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">f_{n}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">italic_f start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math>. This pseudo-depth map is then normalised with its maximum value <math alttext="{max(I^{D}_{n})}" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4.1"><semantics id="S3.SS1.p1.4.m4.1a"><mrow id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mi id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3.cmml">m</mi><mo id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml">⁢</mo><mi id="S3.SS1.p1.4.m4.1.1.4" xref="S3.SS1.p1.4.m4.1.1.4.cmml">a</mi><mo id="S3.SS1.p1.4.m4.1.1.2a" xref="S3.SS1.p1.4.m4.1.1.2.cmml">⁢</mo><mi id="S3.SS1.p1.4.m4.1.1.5" xref="S3.SS1.p1.4.m4.1.1.5.cmml">x</mi><mo id="S3.SS1.p1.4.m4.1.1.2b" xref="S3.SS1.p1.4.m4.1.1.2.cmml">⁢</mo><mrow id="S3.SS1.p1.4.m4.1.1.1.1" xref="S3.SS1.p1.4.m4.1.1.1.1.1.cmml"><mo id="S3.SS1.p1.4.m4.1.1.1.1.2" stretchy="false" xref="S3.SS1.p1.4.m4.1.1.1.1.1.cmml">(</mo><msubsup id="S3.SS1.p1.4.m4.1.1.1.1.1" xref="S3.SS1.p1.4.m4.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.4.m4.1.1.1.1.1.2.2" xref="S3.SS1.p1.4.m4.1.1.1.1.1.2.2.cmml">I</mi><mi id="S3.SS1.p1.4.m4.1.1.1.1.1.3" xref="S3.SS1.p1.4.m4.1.1.1.1.1.3.cmml">n</mi><mi id="S3.SS1.p1.4.m4.1.1.1.1.1.2.3" xref="S3.SS1.p1.4.m4.1.1.1.1.1.2.3.cmml">D</mi></msubsup><mo id="S3.SS1.p1.4.m4.1.1.1.1.3" stretchy="false" xref="S3.SS1.p1.4.m4.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><times id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2"></times><ci id="S3.SS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3">𝑚</ci><ci id="S3.SS1.p1.4.m4.1.1.4.cmml" xref="S3.SS1.p1.4.m4.1.1.4">𝑎</ci><ci id="S3.SS1.p1.4.m4.1.1.5.cmml" xref="S3.SS1.p1.4.m4.1.1.5">𝑥</ci><apply id="S3.SS1.p1.4.m4.1.1.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1.1.1">subscript</csymbol><apply id="S3.SS1.p1.4.m4.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.1.1.1.2.1.cmml" xref="S3.SS1.p1.4.m4.1.1.1.1">superscript</csymbol><ci id="S3.SS1.p1.4.m4.1.1.1.1.1.2.2.cmml" xref="S3.SS1.p1.4.m4.1.1.1.1.1.2.2">𝐼</ci><ci id="S3.SS1.p1.4.m4.1.1.1.1.1.2.3.cmml" xref="S3.SS1.p1.4.m4.1.1.1.1.1.2.3">𝐷</ci></apply><ci id="S3.SS1.p1.4.m4.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.1.1.1.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">{max(I^{D}_{n})}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.1d">italic_m italic_a italic_x ( italic_I start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT )</annotation></semantics></math>. As human arms have a constant maximum range we utilise this characteristic. Subsequently, a fixed threshold <math alttext="t" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m5.1"><semantics id="S3.SS1.p1.5.m5.1a"><mi id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><ci id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.5.m5.1d">italic_t</annotation></semantics></math> is applied to the pseudo depth map <math alttext="I^{D}_{n}" class="ltx_Math" display="inline" id="S3.SS1.p1.6.m6.1"><semantics id="S3.SS1.p1.6.m6.1a"><msubsup id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml"><mi id="S3.SS1.p1.6.m6.1.1.2.2" xref="S3.SS1.p1.6.m6.1.1.2.2.cmml">I</mi><mi id="S3.SS1.p1.6.m6.1.1.3" xref="S3.SS1.p1.6.m6.1.1.3.cmml">n</mi><mi id="S3.SS1.p1.6.m6.1.1.2.3" xref="S3.SS1.p1.6.m6.1.1.2.3.cmml">D</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><apply id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">subscript</csymbol><apply id="S3.SS1.p1.6.m6.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.1.1.2.1.cmml" xref="S3.SS1.p1.6.m6.1.1">superscript</csymbol><ci id="S3.SS1.p1.6.m6.1.1.2.2.cmml" xref="S3.SS1.p1.6.m6.1.1.2.2">𝐼</ci><ci id="S3.SS1.p1.6.m6.1.1.2.3.cmml" xref="S3.SS1.p1.6.m6.1.1.2.3">𝐷</ci></apply><ci id="S3.SS1.p1.6.m6.1.1.3.cmml" xref="S3.SS1.p1.6.m6.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">I^{D}_{n}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.6.m6.1d">italic_I start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> to remove the non-relevant scene part. The resultant depth map, devoid of background interference, serves as a segmentation mask for the <math alttext="f_{n}" class="ltx_Math" display="inline" id="S3.SS1.p1.7.m7.1"><semantics id="S3.SS1.p1.7.m7.1a"><msub id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml"><mi id="S3.SS1.p1.7.m7.1.1.2" xref="S3.SS1.p1.7.m7.1.1.2.cmml">f</mi><mi id="S3.SS1.p1.7.m7.1.1.3" xref="S3.SS1.p1.7.m7.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><apply id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m7.1.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.p1.7.m7.1.1.2.cmml" xref="S3.SS1.p1.7.m7.1.1.2">𝑓</ci><ci id="S3.SS1.p1.7.m7.1.1.3.cmml" xref="S3.SS1.p1.7.m7.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">f_{n}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.7.m7.1d">italic_f start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math>. Segmentation of <math alttext="f_{n}" class="ltx_Math" display="inline" id="S3.SS1.p1.8.m8.1"><semantics id="S3.SS1.p1.8.m8.1a"><msub id="S3.SS1.p1.8.m8.1.1" xref="S3.SS1.p1.8.m8.1.1.cmml"><mi id="S3.SS1.p1.8.m8.1.1.2" xref="S3.SS1.p1.8.m8.1.1.2.cmml">f</mi><mi id="S3.SS1.p1.8.m8.1.1.3" xref="S3.SS1.p1.8.m8.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.1b"><apply id="S3.SS1.p1.8.m8.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.8.m8.1.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1">subscript</csymbol><ci id="S3.SS1.p1.8.m8.1.1.2.cmml" xref="S3.SS1.p1.8.m8.1.1.2">𝑓</ci><ci id="S3.SS1.p1.8.m8.1.1.3.cmml" xref="S3.SS1.p1.8.m8.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">f_{n}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.8.m8.1d">italic_f start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> with <math alttext="I^{D}_{n}" class="ltx_Math" display="inline" id="S3.SS1.p1.9.m9.1"><semantics id="S3.SS1.p1.9.m9.1a"><msubsup id="S3.SS1.p1.9.m9.1.1" xref="S3.SS1.p1.9.m9.1.1.cmml"><mi id="S3.SS1.p1.9.m9.1.1.2.2" xref="S3.SS1.p1.9.m9.1.1.2.2.cmml">I</mi><mi id="S3.SS1.p1.9.m9.1.1.3" xref="S3.SS1.p1.9.m9.1.1.3.cmml">n</mi><mi id="S3.SS1.p1.9.m9.1.1.2.3" xref="S3.SS1.p1.9.m9.1.1.2.3.cmml">D</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m9.1b"><apply id="S3.SS1.p1.9.m9.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.9.m9.1.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1">subscript</csymbol><apply id="S3.SS1.p1.9.m9.1.1.2.cmml" xref="S3.SS1.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.9.m9.1.1.2.1.cmml" xref="S3.SS1.p1.9.m9.1.1">superscript</csymbol><ci id="S3.SS1.p1.9.m9.1.1.2.2.cmml" xref="S3.SS1.p1.9.m9.1.1.2.2">𝐼</ci><ci id="S3.SS1.p1.9.m9.1.1.2.3.cmml" xref="S3.SS1.p1.9.m9.1.1.2.3">𝐷</ci></apply><ci id="S3.SS1.p1.9.m9.1.1.3.cmml" xref="S3.SS1.p1.9.m9.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m9.1c">I^{D}_{n}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.9.m9.1d">italic_I start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> results in <math alttext="I^{SEG}_{n}" class="ltx_Math" display="inline" id="S3.SS1.p1.10.m10.1"><semantics id="S3.SS1.p1.10.m10.1a"><msubsup id="S3.SS1.p1.10.m10.1.1" xref="S3.SS1.p1.10.m10.1.1.cmml"><mi id="S3.SS1.p1.10.m10.1.1.2.2" xref="S3.SS1.p1.10.m10.1.1.2.2.cmml">I</mi><mi id="S3.SS1.p1.10.m10.1.1.3" xref="S3.SS1.p1.10.m10.1.1.3.cmml">n</mi><mrow id="S3.SS1.p1.10.m10.1.1.2.3" xref="S3.SS1.p1.10.m10.1.1.2.3.cmml"><mi id="S3.SS1.p1.10.m10.1.1.2.3.2" xref="S3.SS1.p1.10.m10.1.1.2.3.2.cmml">S</mi><mo id="S3.SS1.p1.10.m10.1.1.2.3.1" xref="S3.SS1.p1.10.m10.1.1.2.3.1.cmml">⁢</mo><mi id="S3.SS1.p1.10.m10.1.1.2.3.3" xref="S3.SS1.p1.10.m10.1.1.2.3.3.cmml">E</mi><mo id="S3.SS1.p1.10.m10.1.1.2.3.1a" xref="S3.SS1.p1.10.m10.1.1.2.3.1.cmml">⁢</mo><mi id="S3.SS1.p1.10.m10.1.1.2.3.4" xref="S3.SS1.p1.10.m10.1.1.2.3.4.cmml">G</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.10.m10.1b"><apply id="S3.SS1.p1.10.m10.1.1.cmml" xref="S3.SS1.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.10.m10.1.1.1.cmml" xref="S3.SS1.p1.10.m10.1.1">subscript</csymbol><apply id="S3.SS1.p1.10.m10.1.1.2.cmml" xref="S3.SS1.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.10.m10.1.1.2.1.cmml" xref="S3.SS1.p1.10.m10.1.1">superscript</csymbol><ci id="S3.SS1.p1.10.m10.1.1.2.2.cmml" xref="S3.SS1.p1.10.m10.1.1.2.2">𝐼</ci><apply id="S3.SS1.p1.10.m10.1.1.2.3.cmml" xref="S3.SS1.p1.10.m10.1.1.2.3"><times id="S3.SS1.p1.10.m10.1.1.2.3.1.cmml" xref="S3.SS1.p1.10.m10.1.1.2.3.1"></times><ci id="S3.SS1.p1.10.m10.1.1.2.3.2.cmml" xref="S3.SS1.p1.10.m10.1.1.2.3.2">𝑆</ci><ci id="S3.SS1.p1.10.m10.1.1.2.3.3.cmml" xref="S3.SS1.p1.10.m10.1.1.2.3.3">𝐸</ci><ci id="S3.SS1.p1.10.m10.1.1.2.3.4.cmml" xref="S3.SS1.p1.10.m10.1.1.2.3.4">𝐺</ci></apply></apply><ci id="S3.SS1.p1.10.m10.1.1.3.cmml" xref="S3.SS1.p1.10.m10.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.10.m10.1c">I^{SEG}_{n}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.10.m10.1d">italic_I start_POSTSUPERSCRIPT italic_S italic_E italic_G end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> where the RGB image contains only human arms and a manipulated object.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.14">The processed <math alttext="I^{SEG}_{n}\in\mathbb{R}^{3\times w\times h},w,h=512" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.3"><semantics id="S3.SS1.p2.1.m1.3a"><mrow id="S3.SS1.p2.1.m1.3.3.2" xref="S3.SS1.p2.1.m1.3.3.3.cmml"><mrow id="S3.SS1.p2.1.m1.2.2.1.1" xref="S3.SS1.p2.1.m1.2.2.1.1.cmml"><msubsup id="S3.SS1.p2.1.m1.2.2.1.1.3" xref="S3.SS1.p2.1.m1.2.2.1.1.3.cmml"><mi id="S3.SS1.p2.1.m1.2.2.1.1.3.2.2" xref="S3.SS1.p2.1.m1.2.2.1.1.3.2.2.cmml">I</mi><mi id="S3.SS1.p2.1.m1.2.2.1.1.3.3" xref="S3.SS1.p2.1.m1.2.2.1.1.3.3.cmml">n</mi><mrow id="S3.SS1.p2.1.m1.2.2.1.1.3.2.3" xref="S3.SS1.p2.1.m1.2.2.1.1.3.2.3.cmml"><mi id="S3.SS1.p2.1.m1.2.2.1.1.3.2.3.2" xref="S3.SS1.p2.1.m1.2.2.1.1.3.2.3.2.cmml">S</mi><mo id="S3.SS1.p2.1.m1.2.2.1.1.3.2.3.1" xref="S3.SS1.p2.1.m1.2.2.1.1.3.2.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.1.m1.2.2.1.1.3.2.3.3" xref="S3.SS1.p2.1.m1.2.2.1.1.3.2.3.3.cmml">E</mi><mo id="S3.SS1.p2.1.m1.2.2.1.1.3.2.3.1a" xref="S3.SS1.p2.1.m1.2.2.1.1.3.2.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.1.m1.2.2.1.1.3.2.3.4" xref="S3.SS1.p2.1.m1.2.2.1.1.3.2.3.4.cmml">G</mi></mrow></msubsup><mo id="S3.SS1.p2.1.m1.2.2.1.1.2" xref="S3.SS1.p2.1.m1.2.2.1.1.2.cmml">∈</mo><mrow id="S3.SS1.p2.1.m1.2.2.1.1.1.1" xref="S3.SS1.p2.1.m1.2.2.1.1.1.2.cmml"><msup id="S3.SS1.p2.1.m1.2.2.1.1.1.1.1" xref="S3.SS1.p2.1.m1.2.2.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.2.2.1.1.1.1.1.2" xref="S3.SS1.p2.1.m1.2.2.1.1.1.1.1.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.1.m1.2.2.1.1.1.1.1.3" xref="S3.SS1.p2.1.m1.2.2.1.1.1.1.1.3.cmml"><mn id="S3.SS1.p2.1.m1.2.2.1.1.1.1.1.3.2" xref="S3.SS1.p2.1.m1.2.2.1.1.1.1.1.3.2.cmml">3</mn><mo id="S3.SS1.p2.1.m1.2.2.1.1.1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p2.1.m1.2.2.1.1.1.1.1.3.1.cmml">×</mo><mi id="S3.SS1.p2.1.m1.2.2.1.1.1.1.1.3.3" xref="S3.SS1.p2.1.m1.2.2.1.1.1.1.1.3.3.cmml">w</mi><mo id="S3.SS1.p2.1.m1.2.2.1.1.1.1.1.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p2.1.m1.2.2.1.1.1.1.1.3.1.cmml">×</mo><mi id="S3.SS1.p2.1.m1.2.2.1.1.1.1.1.3.4" xref="S3.SS1.p2.1.m1.2.2.1.1.1.1.1.3.4.cmml">h</mi></mrow></msup><mo id="S3.SS1.p2.1.m1.2.2.1.1.1.1.2" xref="S3.SS1.p2.1.m1.2.2.1.1.1.2.cmml">,</mo><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">w</mi></mrow></mrow><mo id="S3.SS1.p2.1.m1.3.3.2.3" xref="S3.SS1.p2.1.m1.3.3.3a.cmml">,</mo><mrow id="S3.SS1.p2.1.m1.3.3.2.2" xref="S3.SS1.p2.1.m1.3.3.2.2.cmml"><mi id="S3.SS1.p2.1.m1.3.3.2.2.2" xref="S3.SS1.p2.1.m1.3.3.2.2.2.cmml">h</mi><mo id="S3.SS1.p2.1.m1.3.3.2.2.1" xref="S3.SS1.p2.1.m1.3.3.2.2.1.cmml">=</mo><mn id="S3.SS1.p2.1.m1.3.3.2.2.3" xref="S3.SS1.p2.1.m1.3.3.2.2.3.cmml">512</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.3b"><apply id="S3.SS1.p2.1.m1.3.3.3.cmml" xref="S3.SS1.p2.1.m1.3.3.2"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.3.3.3a.cmml" xref="S3.SS1.p2.1.m1.3.3.2.3">formulae-sequence</csymbol><apply id="S3.SS1.p2.1.m1.2.2.1.1.cmml" xref="S3.SS1.p2.1.m1.2.2.1.1"><in id="S3.SS1.p2.1.m1.2.2.1.1.2.cmml" xref="S3.SS1.p2.1.m1.2.2.1.1.2"></in><apply id="S3.SS1.p2.1.m1.2.2.1.1.3.cmml" xref="S3.SS1.p2.1.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.2.2.1.1.3.1.cmml" xref="S3.SS1.p2.1.m1.2.2.1.1.3">subscript</csymbol><apply id="S3.SS1.p2.1.m1.2.2.1.1.3.2.cmml" xref="S3.SS1.p2.1.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.2.2.1.1.3.2.1.cmml" xref="S3.SS1.p2.1.m1.2.2.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.1.m1.2.2.1.1.3.2.2.cmml" xref="S3.SS1.p2.1.m1.2.2.1.1.3.2.2">𝐼</ci><apply id="S3.SS1.p2.1.m1.2.2.1.1.3.2.3.cmml" xref="S3.SS1.p2.1.m1.2.2.1.1.3.2.3"><times id="S3.SS1.p2.1.m1.2.2.1.1.3.2.3.1.cmml" xref="S3.SS1.p2.1.m1.2.2.1.1.3.2.3.1"></times><ci id="S3.SS1.p2.1.m1.2.2.1.1.3.2.3.2.cmml" xref="S3.SS1.p2.1.m1.2.2.1.1.3.2.3.2">𝑆</ci><ci id="S3.SS1.p2.1.m1.2.2.1.1.3.2.3.3.cmml" xref="S3.SS1.p2.1.m1.2.2.1.1.3.2.3.3">𝐸</ci><ci id="S3.SS1.p2.1.m1.2.2.1.1.3.2.3.4.cmml" xref="S3.SS1.p2.1.m1.2.2.1.1.3.2.3.4">𝐺</ci></apply></apply><ci id="S3.SS1.p2.1.m1.2.2.1.1.3.3.cmml" xref="S3.SS1.p2.1.m1.2.2.1.1.3.3">𝑛</ci></apply><list id="S3.SS1.p2.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.2.2.1.1.1.1"><apply id="S3.SS1.p2.1.m1.2.2.1.1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.2.2.1.1.1.1.1">superscript</csymbol><ci id="S3.SS1.p2.1.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.2.2.1.1.1.1.1.2">ℝ</ci><apply id="S3.SS1.p2.1.m1.2.2.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.2.2.1.1.1.1.1.3"><times id="S3.SS1.p2.1.m1.2.2.1.1.1.1.1.3.1.cmml" xref="S3.SS1.p2.1.m1.2.2.1.1.1.1.1.3.1"></times><cn id="S3.SS1.p2.1.m1.2.2.1.1.1.1.1.3.2.cmml" type="integer" xref="S3.SS1.p2.1.m1.2.2.1.1.1.1.1.3.2">3</cn><ci id="S3.SS1.p2.1.m1.2.2.1.1.1.1.1.3.3.cmml" xref="S3.SS1.p2.1.m1.2.2.1.1.1.1.1.3.3">𝑤</ci><ci id="S3.SS1.p2.1.m1.2.2.1.1.1.1.1.3.4.cmml" xref="S3.SS1.p2.1.m1.2.2.1.1.1.1.1.3.4">ℎ</ci></apply></apply><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝑤</ci></list></apply><apply id="S3.SS1.p2.1.m1.3.3.2.2.cmml" xref="S3.SS1.p2.1.m1.3.3.2.2"><eq id="S3.SS1.p2.1.m1.3.3.2.2.1.cmml" xref="S3.SS1.p2.1.m1.3.3.2.2.1"></eq><ci id="S3.SS1.p2.1.m1.3.3.2.2.2.cmml" xref="S3.SS1.p2.1.m1.3.3.2.2.2">ℎ</ci><cn id="S3.SS1.p2.1.m1.3.3.2.2.3.cmml" type="integer" xref="S3.SS1.p2.1.m1.3.3.2.2.3">512</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.3c">I^{SEG}_{n}\in\mathbb{R}^{3\times w\times h},w,h=512</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.3d">italic_I start_POSTSUPERSCRIPT italic_S italic_E italic_G end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT 3 × italic_w × italic_h end_POSTSUPERSCRIPT , italic_w , italic_h = 512</annotation></semantics></math> is then inputted into a 3D hand pose estimation network, named <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.14.1">EffHandEgoNet3D</span>, which is an extension of the state-of-the-art 2D egocentric hand pose network, <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.14.2">EffHandEgoNet</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib18" title="">18</a>]</cite>, tailored for 3D estimation. <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.14.3">EffHandEgoNet3D</span> comprises an <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.14.4">EfficientNetV2-S</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib25" title="">25</a>]</cite> backbone which extract feature map representation of <math alttext="I^{SEG}_{n}" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><msubsup id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.2.2" xref="S3.SS1.p2.2.m2.1.1.2.2.cmml">I</mi><mi id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml">n</mi><mrow id="S3.SS1.p2.2.m2.1.1.2.3" xref="S3.SS1.p2.2.m2.1.1.2.3.cmml"><mi id="S3.SS1.p2.2.m2.1.1.2.3.2" xref="S3.SS1.p2.2.m2.1.1.2.3.2.cmml">S</mi><mo id="S3.SS1.p2.2.m2.1.1.2.3.1" xref="S3.SS1.p2.2.m2.1.1.2.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.2.m2.1.1.2.3.3" xref="S3.SS1.p2.2.m2.1.1.2.3.3.cmml">E</mi><mo id="S3.SS1.p2.2.m2.1.1.2.3.1a" xref="S3.SS1.p2.2.m2.1.1.2.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.2.m2.1.1.2.3.4" xref="S3.SS1.p2.2.m2.1.1.2.3.4.cmml">G</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">subscript</csymbol><apply id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.2.1.cmml" xref="S3.SS1.p2.2.m2.1.1">superscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.2.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2.2">𝐼</ci><apply id="S3.SS1.p2.2.m2.1.1.2.3.cmml" xref="S3.SS1.p2.2.m2.1.1.2.3"><times id="S3.SS1.p2.2.m2.1.1.2.3.1.cmml" xref="S3.SS1.p2.2.m2.1.1.2.3.1"></times><ci id="S3.SS1.p2.2.m2.1.1.2.3.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2.3.2">𝑆</ci><ci id="S3.SS1.p2.2.m2.1.1.2.3.3.cmml" xref="S3.SS1.p2.2.m2.1.1.2.3.3">𝐸</ci><ci id="S3.SS1.p2.2.m2.1.1.2.3.4.cmml" xref="S3.SS1.p2.2.m2.1.1.2.3.4">𝐺</ci></apply></apply><ci id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">I^{SEG}_{n}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">italic_I start_POSTSUPERSCRIPT italic_S italic_E italic_G end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> <math alttext="F_{M}\in\mathbb{R}^{1280\times 16\times 16}" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3.1"><semantics id="S3.SS1.p2.3.m3.1a"><mrow id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml"><msub id="S3.SS1.p2.3.m3.1.1.2" xref="S3.SS1.p2.3.m3.1.1.2.cmml"><mi id="S3.SS1.p2.3.m3.1.1.2.2" xref="S3.SS1.p2.3.m3.1.1.2.2.cmml">F</mi><mi id="S3.SS1.p2.3.m3.1.1.2.3" xref="S3.SS1.p2.3.m3.1.1.2.3.cmml">M</mi></msub><mo id="S3.SS1.p2.3.m3.1.1.1" xref="S3.SS1.p2.3.m3.1.1.1.cmml">∈</mo><msup id="S3.SS1.p2.3.m3.1.1.3" xref="S3.SS1.p2.3.m3.1.1.3.cmml"><mi id="S3.SS1.p2.3.m3.1.1.3.2" xref="S3.SS1.p2.3.m3.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.3.m3.1.1.3.3" xref="S3.SS1.p2.3.m3.1.1.3.3.cmml"><mn id="S3.SS1.p2.3.m3.1.1.3.3.2" xref="S3.SS1.p2.3.m3.1.1.3.3.2.cmml">1280</mn><mo id="S3.SS1.p2.3.m3.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p2.3.m3.1.1.3.3.1.cmml">×</mo><mn id="S3.SS1.p2.3.m3.1.1.3.3.3" xref="S3.SS1.p2.3.m3.1.1.3.3.3.cmml">16</mn><mo id="S3.SS1.p2.3.m3.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p2.3.m3.1.1.3.3.1.cmml">×</mo><mn id="S3.SS1.p2.3.m3.1.1.3.3.4" xref="S3.SS1.p2.3.m3.1.1.3.3.4.cmml">16</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1"><in id="S3.SS1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1.1"></in><apply id="S3.SS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.2.1.cmml" xref="S3.SS1.p2.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS1.p2.3.m3.1.1.2.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2.2">𝐹</ci><ci id="S3.SS1.p2.3.m3.1.1.2.3.cmml" xref="S3.SS1.p2.3.m3.1.1.2.3">𝑀</ci></apply><apply id="S3.SS1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.3.1.cmml" xref="S3.SS1.p2.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.3.m3.1.1.3.2.cmml" xref="S3.SS1.p2.3.m3.1.1.3.2">ℝ</ci><apply id="S3.SS1.p2.3.m3.1.1.3.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3.3"><times id="S3.SS1.p2.3.m3.1.1.3.3.1.cmml" xref="S3.SS1.p2.3.m3.1.1.3.3.1"></times><cn id="S3.SS1.p2.3.m3.1.1.3.3.2.cmml" type="integer" xref="S3.SS1.p2.3.m3.1.1.3.3.2">1280</cn><cn id="S3.SS1.p2.3.m3.1.1.3.3.3.cmml" type="integer" xref="S3.SS1.p2.3.m3.1.1.3.3.3">16</cn><cn id="S3.SS1.p2.3.m3.1.1.3.3.4.cmml" type="integer" xref="S3.SS1.p2.3.m3.1.1.3.3.4">16</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">F_{M}\in\mathbb{R}^{1280\times 16\times 16}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m3.1d">italic_F start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT 1280 × 16 × 16 end_POSTSUPERSCRIPT</annotation></semantics></math>. Extracted feature map <math alttext="F_{M}" class="ltx_Math" display="inline" id="S3.SS1.p2.4.m4.1"><semantics id="S3.SS1.p2.4.m4.1a"><msub id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><mi id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml">F</mi><mi id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml">M</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2">𝐹</ci><ci id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">F_{M}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.4.m4.1d">italic_F start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT</annotation></semantics></math> is handed to two independent upsamplers for each of the hands and <math alttext="MLP^{Z}_{L,R}" class="ltx_Math" display="inline" id="S3.SS1.p2.5.m5.2"><semantics id="S3.SS1.p2.5.m5.2a"><mrow id="S3.SS1.p2.5.m5.2.3" xref="S3.SS1.p2.5.m5.2.3.cmml"><mi id="S3.SS1.p2.5.m5.2.3.2" xref="S3.SS1.p2.5.m5.2.3.2.cmml">M</mi><mo id="S3.SS1.p2.5.m5.2.3.1" xref="S3.SS1.p2.5.m5.2.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.5.m5.2.3.3" xref="S3.SS1.p2.5.m5.2.3.3.cmml">L</mi><mo id="S3.SS1.p2.5.m5.2.3.1a" xref="S3.SS1.p2.5.m5.2.3.1.cmml">⁢</mo><msubsup id="S3.SS1.p2.5.m5.2.3.4" xref="S3.SS1.p2.5.m5.2.3.4.cmml"><mi id="S3.SS1.p2.5.m5.2.3.4.2.2" xref="S3.SS1.p2.5.m5.2.3.4.2.2.cmml">P</mi><mrow id="S3.SS1.p2.5.m5.2.2.2.4" xref="S3.SS1.p2.5.m5.2.2.2.3.cmml"><mi id="S3.SS1.p2.5.m5.1.1.1.1" xref="S3.SS1.p2.5.m5.1.1.1.1.cmml">L</mi><mo id="S3.SS1.p2.5.m5.2.2.2.4.1" xref="S3.SS1.p2.5.m5.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p2.5.m5.2.2.2.2" xref="S3.SS1.p2.5.m5.2.2.2.2.cmml">R</mi></mrow><mi id="S3.SS1.p2.5.m5.2.3.4.2.3" xref="S3.SS1.p2.5.m5.2.3.4.2.3.cmml">Z</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.2b"><apply id="S3.SS1.p2.5.m5.2.3.cmml" xref="S3.SS1.p2.5.m5.2.3"><times id="S3.SS1.p2.5.m5.2.3.1.cmml" xref="S3.SS1.p2.5.m5.2.3.1"></times><ci id="S3.SS1.p2.5.m5.2.3.2.cmml" xref="S3.SS1.p2.5.m5.2.3.2">𝑀</ci><ci id="S3.SS1.p2.5.m5.2.3.3.cmml" xref="S3.SS1.p2.5.m5.2.3.3">𝐿</ci><apply id="S3.SS1.p2.5.m5.2.3.4.cmml" xref="S3.SS1.p2.5.m5.2.3.4"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.2.3.4.1.cmml" xref="S3.SS1.p2.5.m5.2.3.4">subscript</csymbol><apply id="S3.SS1.p2.5.m5.2.3.4.2.cmml" xref="S3.SS1.p2.5.m5.2.3.4"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.2.3.4.2.1.cmml" xref="S3.SS1.p2.5.m5.2.3.4">superscript</csymbol><ci id="S3.SS1.p2.5.m5.2.3.4.2.2.cmml" xref="S3.SS1.p2.5.m5.2.3.4.2.2">𝑃</ci><ci id="S3.SS1.p2.5.m5.2.3.4.2.3.cmml" xref="S3.SS1.p2.5.m5.2.3.4.2.3">𝑍</ci></apply><list id="S3.SS1.p2.5.m5.2.2.2.3.cmml" xref="S3.SS1.p2.5.m5.2.2.2.4"><ci id="S3.SS1.p2.5.m5.1.1.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1.1.1">𝐿</ci><ci id="S3.SS1.p2.5.m5.2.2.2.2.cmml" xref="S3.SS1.p2.5.m5.2.2.2.2">𝑅</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.2c">MLP^{Z}_{L,R}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.5.m5.2d">italic_M italic_L italic_P start_POSTSUPERSCRIPT italic_Z end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_L , italic_R end_POSTSUBSCRIPT</annotation></semantics></math> estimating keypoints’ depth. Despite pose estimation, the handness modules responsible for predicting each hand’s presence <math alttext="h_{L},h_{R}\in\mathbb{R}^{2}" class="ltx_Math" display="inline" id="S3.SS1.p2.6.m6.2"><semantics id="S3.SS1.p2.6.m6.2a"><mrow id="S3.SS1.p2.6.m6.2.2" xref="S3.SS1.p2.6.m6.2.2.cmml"><mrow id="S3.SS1.p2.6.m6.2.2.2.2" xref="S3.SS1.p2.6.m6.2.2.2.3.cmml"><msub id="S3.SS1.p2.6.m6.1.1.1.1.1" xref="S3.SS1.p2.6.m6.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.6.m6.1.1.1.1.1.2" xref="S3.SS1.p2.6.m6.1.1.1.1.1.2.cmml">h</mi><mi id="S3.SS1.p2.6.m6.1.1.1.1.1.3" xref="S3.SS1.p2.6.m6.1.1.1.1.1.3.cmml">L</mi></msub><mo id="S3.SS1.p2.6.m6.2.2.2.2.3" xref="S3.SS1.p2.6.m6.2.2.2.3.cmml">,</mo><msub id="S3.SS1.p2.6.m6.2.2.2.2.2" xref="S3.SS1.p2.6.m6.2.2.2.2.2.cmml"><mi id="S3.SS1.p2.6.m6.2.2.2.2.2.2" xref="S3.SS1.p2.6.m6.2.2.2.2.2.2.cmml">h</mi><mi id="S3.SS1.p2.6.m6.2.2.2.2.2.3" xref="S3.SS1.p2.6.m6.2.2.2.2.2.3.cmml">R</mi></msub></mrow><mo id="S3.SS1.p2.6.m6.2.2.3" xref="S3.SS1.p2.6.m6.2.2.3.cmml">∈</mo><msup id="S3.SS1.p2.6.m6.2.2.4" xref="S3.SS1.p2.6.m6.2.2.4.cmml"><mi id="S3.SS1.p2.6.m6.2.2.4.2" xref="S3.SS1.p2.6.m6.2.2.4.2.cmml">ℝ</mi><mn id="S3.SS1.p2.6.m6.2.2.4.3" xref="S3.SS1.p2.6.m6.2.2.4.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.2b"><apply id="S3.SS1.p2.6.m6.2.2.cmml" xref="S3.SS1.p2.6.m6.2.2"><in id="S3.SS1.p2.6.m6.2.2.3.cmml" xref="S3.SS1.p2.6.m6.2.2.3"></in><list id="S3.SS1.p2.6.m6.2.2.2.3.cmml" xref="S3.SS1.p2.6.m6.2.2.2.2"><apply id="S3.SS1.p2.6.m6.1.1.1.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p2.6.m6.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.6.m6.1.1.1.1.1.2">ℎ</ci><ci id="S3.SS1.p2.6.m6.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.6.m6.1.1.1.1.1.3">𝐿</ci></apply><apply id="S3.SS1.p2.6.m6.2.2.2.2.2.cmml" xref="S3.SS1.p2.6.m6.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.2.2.2.2.2.1.cmml" xref="S3.SS1.p2.6.m6.2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p2.6.m6.2.2.2.2.2.2.cmml" xref="S3.SS1.p2.6.m6.2.2.2.2.2.2">ℎ</ci><ci id="S3.SS1.p2.6.m6.2.2.2.2.2.3.cmml" xref="S3.SS1.p2.6.m6.2.2.2.2.2.3">𝑅</ci></apply></list><apply id="S3.SS1.p2.6.m6.2.2.4.cmml" xref="S3.SS1.p2.6.m6.2.2.4"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.2.2.4.1.cmml" xref="S3.SS1.p2.6.m6.2.2.4">superscript</csymbol><ci id="S3.SS1.p2.6.m6.2.2.4.2.cmml" xref="S3.SS1.p2.6.m6.2.2.4.2">ℝ</ci><cn id="S3.SS1.p2.6.m6.2.2.4.3.cmml" type="integer" xref="S3.SS1.p2.6.m6.2.2.4.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.2c">h_{L},h_{R}\in\mathbb{R}^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.6.m6.2d">italic_h start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT , italic_h start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> are built from another <math alttext="MLP^{H}_{L,R}" class="ltx_Math" display="inline" id="S3.SS1.p2.7.m7.2"><semantics id="S3.SS1.p2.7.m7.2a"><mrow id="S3.SS1.p2.7.m7.2.3" xref="S3.SS1.p2.7.m7.2.3.cmml"><mi id="S3.SS1.p2.7.m7.2.3.2" xref="S3.SS1.p2.7.m7.2.3.2.cmml">M</mi><mo id="S3.SS1.p2.7.m7.2.3.1" xref="S3.SS1.p2.7.m7.2.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.7.m7.2.3.3" xref="S3.SS1.p2.7.m7.2.3.3.cmml">L</mi><mo id="S3.SS1.p2.7.m7.2.3.1a" xref="S3.SS1.p2.7.m7.2.3.1.cmml">⁢</mo><msubsup id="S3.SS1.p2.7.m7.2.3.4" xref="S3.SS1.p2.7.m7.2.3.4.cmml"><mi id="S3.SS1.p2.7.m7.2.3.4.2.2" xref="S3.SS1.p2.7.m7.2.3.4.2.2.cmml">P</mi><mrow id="S3.SS1.p2.7.m7.2.2.2.4" xref="S3.SS1.p2.7.m7.2.2.2.3.cmml"><mi id="S3.SS1.p2.7.m7.1.1.1.1" xref="S3.SS1.p2.7.m7.1.1.1.1.cmml">L</mi><mo id="S3.SS1.p2.7.m7.2.2.2.4.1" xref="S3.SS1.p2.7.m7.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p2.7.m7.2.2.2.2" xref="S3.SS1.p2.7.m7.2.2.2.2.cmml">R</mi></mrow><mi id="S3.SS1.p2.7.m7.2.3.4.2.3" xref="S3.SS1.p2.7.m7.2.3.4.2.3.cmml">H</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.2b"><apply id="S3.SS1.p2.7.m7.2.3.cmml" xref="S3.SS1.p2.7.m7.2.3"><times id="S3.SS1.p2.7.m7.2.3.1.cmml" xref="S3.SS1.p2.7.m7.2.3.1"></times><ci id="S3.SS1.p2.7.m7.2.3.2.cmml" xref="S3.SS1.p2.7.m7.2.3.2">𝑀</ci><ci id="S3.SS1.p2.7.m7.2.3.3.cmml" xref="S3.SS1.p2.7.m7.2.3.3">𝐿</ci><apply id="S3.SS1.p2.7.m7.2.3.4.cmml" xref="S3.SS1.p2.7.m7.2.3.4"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.2.3.4.1.cmml" xref="S3.SS1.p2.7.m7.2.3.4">subscript</csymbol><apply id="S3.SS1.p2.7.m7.2.3.4.2.cmml" xref="S3.SS1.p2.7.m7.2.3.4"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.2.3.4.2.1.cmml" xref="S3.SS1.p2.7.m7.2.3.4">superscript</csymbol><ci id="S3.SS1.p2.7.m7.2.3.4.2.2.cmml" xref="S3.SS1.p2.7.m7.2.3.4.2.2">𝑃</ci><ci id="S3.SS1.p2.7.m7.2.3.4.2.3.cmml" xref="S3.SS1.p2.7.m7.2.3.4.2.3">𝐻</ci></apply><list id="S3.SS1.p2.7.m7.2.2.2.3.cmml" xref="S3.SS1.p2.7.m7.2.2.2.4"><ci id="S3.SS1.p2.7.m7.1.1.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1.1.1">𝐿</ci><ci id="S3.SS1.p2.7.m7.2.2.2.2.cmml" xref="S3.SS1.p2.7.m7.2.2.2.2">𝑅</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.2c">MLP^{H}_{L,R}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.7.m7.2d">italic_M italic_L italic_P start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_L , italic_R end_POSTSUBSCRIPT</annotation></semantics></math>. The upsamplers consist of three transposed convolutions with batch normalisation and ReLU activation except the last layer followed by a pointwise convolution. Output results are heatmaps <math alttext="\mathit{H}_{L,R}\in\mathbb{R}^{J\times w\times h}" class="ltx_Math" display="inline" id="S3.SS1.p2.8.m8.2"><semantics id="S3.SS1.p2.8.m8.2a"><mrow id="S3.SS1.p2.8.m8.2.3" xref="S3.SS1.p2.8.m8.2.3.cmml"><msub id="S3.SS1.p2.8.m8.2.3.2" xref="S3.SS1.p2.8.m8.2.3.2.cmml"><mi id="S3.SS1.p2.8.m8.2.3.2.2" xref="S3.SS1.p2.8.m8.2.3.2.2.cmml">H</mi><mrow id="S3.SS1.p2.8.m8.2.2.2.4" xref="S3.SS1.p2.8.m8.2.2.2.3.cmml"><mi id="S3.SS1.p2.8.m8.1.1.1.1" xref="S3.SS1.p2.8.m8.1.1.1.1.cmml">L</mi><mo id="S3.SS1.p2.8.m8.2.2.2.4.1" xref="S3.SS1.p2.8.m8.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p2.8.m8.2.2.2.2" xref="S3.SS1.p2.8.m8.2.2.2.2.cmml">R</mi></mrow></msub><mo id="S3.SS1.p2.8.m8.2.3.1" xref="S3.SS1.p2.8.m8.2.3.1.cmml">∈</mo><msup id="S3.SS1.p2.8.m8.2.3.3" xref="S3.SS1.p2.8.m8.2.3.3.cmml"><mi id="S3.SS1.p2.8.m8.2.3.3.2" xref="S3.SS1.p2.8.m8.2.3.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.8.m8.2.3.3.3" xref="S3.SS1.p2.8.m8.2.3.3.3.cmml"><mi id="S3.SS1.p2.8.m8.2.3.3.3.2" xref="S3.SS1.p2.8.m8.2.3.3.3.2.cmml">J</mi><mo id="S3.SS1.p2.8.m8.2.3.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p2.8.m8.2.3.3.3.1.cmml">×</mo><mi id="S3.SS1.p2.8.m8.2.3.3.3.3" xref="S3.SS1.p2.8.m8.2.3.3.3.3.cmml">w</mi><mo id="S3.SS1.p2.8.m8.2.3.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p2.8.m8.2.3.3.3.1.cmml">×</mo><mi id="S3.SS1.p2.8.m8.2.3.3.3.4" xref="S3.SS1.p2.8.m8.2.3.3.3.4.cmml">h</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m8.2b"><apply id="S3.SS1.p2.8.m8.2.3.cmml" xref="S3.SS1.p2.8.m8.2.3"><in id="S3.SS1.p2.8.m8.2.3.1.cmml" xref="S3.SS1.p2.8.m8.2.3.1"></in><apply id="S3.SS1.p2.8.m8.2.3.2.cmml" xref="S3.SS1.p2.8.m8.2.3.2"><csymbol cd="ambiguous" id="S3.SS1.p2.8.m8.2.3.2.1.cmml" xref="S3.SS1.p2.8.m8.2.3.2">subscript</csymbol><ci id="S3.SS1.p2.8.m8.2.3.2.2.cmml" xref="S3.SS1.p2.8.m8.2.3.2.2">𝐻</ci><list id="S3.SS1.p2.8.m8.2.2.2.3.cmml" xref="S3.SS1.p2.8.m8.2.2.2.4"><ci id="S3.SS1.p2.8.m8.1.1.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1.1.1">𝐿</ci><ci id="S3.SS1.p2.8.m8.2.2.2.2.cmml" xref="S3.SS1.p2.8.m8.2.2.2.2">𝑅</ci></list></apply><apply id="S3.SS1.p2.8.m8.2.3.3.cmml" xref="S3.SS1.p2.8.m8.2.3.3"><csymbol cd="ambiguous" id="S3.SS1.p2.8.m8.2.3.3.1.cmml" xref="S3.SS1.p2.8.m8.2.3.3">superscript</csymbol><ci id="S3.SS1.p2.8.m8.2.3.3.2.cmml" xref="S3.SS1.p2.8.m8.2.3.3.2">ℝ</ci><apply id="S3.SS1.p2.8.m8.2.3.3.3.cmml" xref="S3.SS1.p2.8.m8.2.3.3.3"><times id="S3.SS1.p2.8.m8.2.3.3.3.1.cmml" xref="S3.SS1.p2.8.m8.2.3.3.3.1"></times><ci id="S3.SS1.p2.8.m8.2.3.3.3.2.cmml" xref="S3.SS1.p2.8.m8.2.3.3.3.2">𝐽</ci><ci id="S3.SS1.p2.8.m8.2.3.3.3.3.cmml" xref="S3.SS1.p2.8.m8.2.3.3.3.3">𝑤</ci><ci id="S3.SS1.p2.8.m8.2.3.3.3.4.cmml" xref="S3.SS1.p2.8.m8.2.3.3.3.4">ℎ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m8.2c">\mathit{H}_{L,R}\in\mathbb{R}^{J\times w\times h}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.8.m8.2d">italic_H start_POSTSUBSCRIPT italic_L , italic_R end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_J × italic_w × italic_h end_POSTSUPERSCRIPT</annotation></semantics></math> where each cell represents the probability of joint <math alttext="J" class="ltx_Math" display="inline" id="S3.SS1.p2.9.m9.1"><semantics id="S3.SS1.p2.9.m9.1a"><mi id="S3.SS1.p2.9.m9.1.1" xref="S3.SS1.p2.9.m9.1.1.cmml">J</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.9.m9.1b"><ci id="S3.SS1.p2.9.m9.1.1.cmml" xref="S3.SS1.p2.9.m9.1.1">𝐽</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.9.m9.1c">J</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.9.m9.1d">italic_J</annotation></semantics></math> occurrence for each hand. In the next step they are transformed into <math alttext="\mathit{P}^{2D}_{L,R}" class="ltx_Math" display="inline" id="S3.SS1.p2.10.m10.2"><semantics id="S3.SS1.p2.10.m10.2a"><msubsup id="S3.SS1.p2.10.m10.2.3" xref="S3.SS1.p2.10.m10.2.3.cmml"><mi id="S3.SS1.p2.10.m10.2.3.2.2" xref="S3.SS1.p2.10.m10.2.3.2.2.cmml">P</mi><mrow id="S3.SS1.p2.10.m10.2.2.2.4" xref="S3.SS1.p2.10.m10.2.2.2.3.cmml"><mi id="S3.SS1.p2.10.m10.1.1.1.1" xref="S3.SS1.p2.10.m10.1.1.1.1.cmml">L</mi><mo id="S3.SS1.p2.10.m10.2.2.2.4.1" xref="S3.SS1.p2.10.m10.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p2.10.m10.2.2.2.2" xref="S3.SS1.p2.10.m10.2.2.2.2.cmml">R</mi></mrow><mrow id="S3.SS1.p2.10.m10.2.3.2.3" xref="S3.SS1.p2.10.m10.2.3.2.3.cmml"><mn id="S3.SS1.p2.10.m10.2.3.2.3.2" xref="S3.SS1.p2.10.m10.2.3.2.3.2.cmml">2</mn><mo id="S3.SS1.p2.10.m10.2.3.2.3.1" xref="S3.SS1.p2.10.m10.2.3.2.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.10.m10.2.3.2.3.3" xref="S3.SS1.p2.10.m10.2.3.2.3.3.cmml">D</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.10.m10.2b"><apply id="S3.SS1.p2.10.m10.2.3.cmml" xref="S3.SS1.p2.10.m10.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.10.m10.2.3.1.cmml" xref="S3.SS1.p2.10.m10.2.3">subscript</csymbol><apply id="S3.SS1.p2.10.m10.2.3.2.cmml" xref="S3.SS1.p2.10.m10.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.10.m10.2.3.2.1.cmml" xref="S3.SS1.p2.10.m10.2.3">superscript</csymbol><ci id="S3.SS1.p2.10.m10.2.3.2.2.cmml" xref="S3.SS1.p2.10.m10.2.3.2.2">𝑃</ci><apply id="S3.SS1.p2.10.m10.2.3.2.3.cmml" xref="S3.SS1.p2.10.m10.2.3.2.3"><times id="S3.SS1.p2.10.m10.2.3.2.3.1.cmml" xref="S3.SS1.p2.10.m10.2.3.2.3.1"></times><cn id="S3.SS1.p2.10.m10.2.3.2.3.2.cmml" type="integer" xref="S3.SS1.p2.10.m10.2.3.2.3.2">2</cn><ci id="S3.SS1.p2.10.m10.2.3.2.3.3.cmml" xref="S3.SS1.p2.10.m10.2.3.2.3.3">𝐷</ci></apply></apply><list id="S3.SS1.p2.10.m10.2.2.2.3.cmml" xref="S3.SS1.p2.10.m10.2.2.2.4"><ci id="S3.SS1.p2.10.m10.1.1.1.1.cmml" xref="S3.SS1.p2.10.m10.1.1.1.1">𝐿</ci><ci id="S3.SS1.p2.10.m10.2.2.2.2.cmml" xref="S3.SS1.p2.10.m10.2.2.2.2">𝑅</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.10.m10.2c">\mathit{P}^{2D}_{L,R}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.10.m10.2d">italic_P start_POSTSUPERSCRIPT 2 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_L , italic_R end_POSTSUBSCRIPT</annotation></semantics></math> and concatenated with estimated corresponding <math alttext="z" class="ltx_Math" display="inline" id="S3.SS1.p2.11.m11.1"><semantics id="S3.SS1.p2.11.m11.1a"><mi id="S3.SS1.p2.11.m11.1.1" xref="S3.SS1.p2.11.m11.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.11.m11.1b"><ci id="S3.SS1.p2.11.m11.1.1.cmml" xref="S3.SS1.p2.11.m11.1.1">𝑧</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.11.m11.1c">z</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.11.m11.1d">italic_z</annotation></semantics></math> values resulting in <math alttext="\mathit{P}^{2.5D}_{L,R}" class="ltx_Math" display="inline" id="S3.SS1.p2.12.m12.2"><semantics id="S3.SS1.p2.12.m12.2a"><msubsup id="S3.SS1.p2.12.m12.2.3" xref="S3.SS1.p2.12.m12.2.3.cmml"><mi id="S3.SS1.p2.12.m12.2.3.2.2" xref="S3.SS1.p2.12.m12.2.3.2.2.cmml">P</mi><mrow id="S3.SS1.p2.12.m12.2.2.2.4" xref="S3.SS1.p2.12.m12.2.2.2.3.cmml"><mi id="S3.SS1.p2.12.m12.1.1.1.1" xref="S3.SS1.p2.12.m12.1.1.1.1.cmml">L</mi><mo id="S3.SS1.p2.12.m12.2.2.2.4.1" xref="S3.SS1.p2.12.m12.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p2.12.m12.2.2.2.2" xref="S3.SS1.p2.12.m12.2.2.2.2.cmml">R</mi></mrow><mrow id="S3.SS1.p2.12.m12.2.3.2.3" xref="S3.SS1.p2.12.m12.2.3.2.3.cmml"><mn id="S3.SS1.p2.12.m12.2.3.2.3.2" xref="S3.SS1.p2.12.m12.2.3.2.3.2.cmml">2.5</mn><mo id="S3.SS1.p2.12.m12.2.3.2.3.1" xref="S3.SS1.p2.12.m12.2.3.2.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.12.m12.2.3.2.3.3" xref="S3.SS1.p2.12.m12.2.3.2.3.3.cmml">D</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.12.m12.2b"><apply id="S3.SS1.p2.12.m12.2.3.cmml" xref="S3.SS1.p2.12.m12.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.12.m12.2.3.1.cmml" xref="S3.SS1.p2.12.m12.2.3">subscript</csymbol><apply id="S3.SS1.p2.12.m12.2.3.2.cmml" xref="S3.SS1.p2.12.m12.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.12.m12.2.3.2.1.cmml" xref="S3.SS1.p2.12.m12.2.3">superscript</csymbol><ci id="S3.SS1.p2.12.m12.2.3.2.2.cmml" xref="S3.SS1.p2.12.m12.2.3.2.2">𝑃</ci><apply id="S3.SS1.p2.12.m12.2.3.2.3.cmml" xref="S3.SS1.p2.12.m12.2.3.2.3"><times id="S3.SS1.p2.12.m12.2.3.2.3.1.cmml" xref="S3.SS1.p2.12.m12.2.3.2.3.1"></times><cn id="S3.SS1.p2.12.m12.2.3.2.3.2.cmml" type="float" xref="S3.SS1.p2.12.m12.2.3.2.3.2">2.5</cn><ci id="S3.SS1.p2.12.m12.2.3.2.3.3.cmml" xref="S3.SS1.p2.12.m12.2.3.2.3.3">𝐷</ci></apply></apply><list id="S3.SS1.p2.12.m12.2.2.2.3.cmml" xref="S3.SS1.p2.12.m12.2.2.2.4"><ci id="S3.SS1.p2.12.m12.1.1.1.1.cmml" xref="S3.SS1.p2.12.m12.1.1.1.1">𝐿</ci><ci id="S3.SS1.p2.12.m12.2.2.2.2.cmml" xref="S3.SS1.p2.12.m12.2.2.2.2">𝑅</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.12.m12.2c">\mathit{P}^{2.5D}_{L,R}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.12.m12.2d">italic_P start_POSTSUPERSCRIPT 2.5 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_L , italic_R end_POSTSUBSCRIPT</annotation></semantics></math>. The final step utilises camera intrinsic parameters to transform <math alttext="\mathit{P}^{2.5D}_{L,R}" class="ltx_Math" display="inline" id="S3.SS1.p2.13.m13.2"><semantics id="S3.SS1.p2.13.m13.2a"><msubsup id="S3.SS1.p2.13.m13.2.3" xref="S3.SS1.p2.13.m13.2.3.cmml"><mi id="S3.SS1.p2.13.m13.2.3.2.2" xref="S3.SS1.p2.13.m13.2.3.2.2.cmml">P</mi><mrow id="S3.SS1.p2.13.m13.2.2.2.4" xref="S3.SS1.p2.13.m13.2.2.2.3.cmml"><mi id="S3.SS1.p2.13.m13.1.1.1.1" xref="S3.SS1.p2.13.m13.1.1.1.1.cmml">L</mi><mo id="S3.SS1.p2.13.m13.2.2.2.4.1" xref="S3.SS1.p2.13.m13.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p2.13.m13.2.2.2.2" xref="S3.SS1.p2.13.m13.2.2.2.2.cmml">R</mi></mrow><mrow id="S3.SS1.p2.13.m13.2.3.2.3" xref="S3.SS1.p2.13.m13.2.3.2.3.cmml"><mn id="S3.SS1.p2.13.m13.2.3.2.3.2" xref="S3.SS1.p2.13.m13.2.3.2.3.2.cmml">2.5</mn><mo id="S3.SS1.p2.13.m13.2.3.2.3.1" xref="S3.SS1.p2.13.m13.2.3.2.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.13.m13.2.3.2.3.3" xref="S3.SS1.p2.13.m13.2.3.2.3.3.cmml">D</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.13.m13.2b"><apply id="S3.SS1.p2.13.m13.2.3.cmml" xref="S3.SS1.p2.13.m13.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.13.m13.2.3.1.cmml" xref="S3.SS1.p2.13.m13.2.3">subscript</csymbol><apply id="S3.SS1.p2.13.m13.2.3.2.cmml" xref="S3.SS1.p2.13.m13.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.13.m13.2.3.2.1.cmml" xref="S3.SS1.p2.13.m13.2.3">superscript</csymbol><ci id="S3.SS1.p2.13.m13.2.3.2.2.cmml" xref="S3.SS1.p2.13.m13.2.3.2.2">𝑃</ci><apply id="S3.SS1.p2.13.m13.2.3.2.3.cmml" xref="S3.SS1.p2.13.m13.2.3.2.3"><times id="S3.SS1.p2.13.m13.2.3.2.3.1.cmml" xref="S3.SS1.p2.13.m13.2.3.2.3.1"></times><cn id="S3.SS1.p2.13.m13.2.3.2.3.2.cmml" type="float" xref="S3.SS1.p2.13.m13.2.3.2.3.2">2.5</cn><ci id="S3.SS1.p2.13.m13.2.3.2.3.3.cmml" xref="S3.SS1.p2.13.m13.2.3.2.3.3">𝐷</ci></apply></apply><list id="S3.SS1.p2.13.m13.2.2.2.3.cmml" xref="S3.SS1.p2.13.m13.2.2.2.4"><ci id="S3.SS1.p2.13.m13.1.1.1.1.cmml" xref="S3.SS1.p2.13.m13.1.1.1.1">𝐿</ci><ci id="S3.SS1.p2.13.m13.2.2.2.2.cmml" xref="S3.SS1.p2.13.m13.2.2.2.2">𝑅</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.13.m13.2c">\mathit{P}^{2.5D}_{L,R}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.13.m13.2d">italic_P start_POSTSUPERSCRIPT 2.5 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_L , italic_R end_POSTSUBSCRIPT</annotation></semantics></math> using the pinhole camera model to camera space resulting in <math alttext="\mathit{P}^{3D}_{L,R}" class="ltx_Math" display="inline" id="S3.SS1.p2.14.m14.2"><semantics id="S3.SS1.p2.14.m14.2a"><msubsup id="S3.SS1.p2.14.m14.2.3" xref="S3.SS1.p2.14.m14.2.3.cmml"><mi id="S3.SS1.p2.14.m14.2.3.2.2" xref="S3.SS1.p2.14.m14.2.3.2.2.cmml">P</mi><mrow id="S3.SS1.p2.14.m14.2.2.2.4" xref="S3.SS1.p2.14.m14.2.2.2.3.cmml"><mi id="S3.SS1.p2.14.m14.1.1.1.1" xref="S3.SS1.p2.14.m14.1.1.1.1.cmml">L</mi><mo id="S3.SS1.p2.14.m14.2.2.2.4.1" xref="S3.SS1.p2.14.m14.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p2.14.m14.2.2.2.2" xref="S3.SS1.p2.14.m14.2.2.2.2.cmml">R</mi></mrow><mrow id="S3.SS1.p2.14.m14.2.3.2.3" xref="S3.SS1.p2.14.m14.2.3.2.3.cmml"><mn id="S3.SS1.p2.14.m14.2.3.2.3.2" xref="S3.SS1.p2.14.m14.2.3.2.3.2.cmml">3</mn><mo id="S3.SS1.p2.14.m14.2.3.2.3.1" xref="S3.SS1.p2.14.m14.2.3.2.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.14.m14.2.3.2.3.3" xref="S3.SS1.p2.14.m14.2.3.2.3.3.cmml">D</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.14.m14.2b"><apply id="S3.SS1.p2.14.m14.2.3.cmml" xref="S3.SS1.p2.14.m14.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.14.m14.2.3.1.cmml" xref="S3.SS1.p2.14.m14.2.3">subscript</csymbol><apply id="S3.SS1.p2.14.m14.2.3.2.cmml" xref="S3.SS1.p2.14.m14.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.14.m14.2.3.2.1.cmml" xref="S3.SS1.p2.14.m14.2.3">superscript</csymbol><ci id="S3.SS1.p2.14.m14.2.3.2.2.cmml" xref="S3.SS1.p2.14.m14.2.3.2.2">𝑃</ci><apply id="S3.SS1.p2.14.m14.2.3.2.3.cmml" xref="S3.SS1.p2.14.m14.2.3.2.3"><times id="S3.SS1.p2.14.m14.2.3.2.3.1.cmml" xref="S3.SS1.p2.14.m14.2.3.2.3.1"></times><cn id="S3.SS1.p2.14.m14.2.3.2.3.2.cmml" type="integer" xref="S3.SS1.p2.14.m14.2.3.2.3.2">3</cn><ci id="S3.SS1.p2.14.m14.2.3.2.3.3.cmml" xref="S3.SS1.p2.14.m14.2.3.2.3.3">𝐷</ci></apply></apply><list id="S3.SS1.p2.14.m14.2.2.2.3.cmml" xref="S3.SS1.p2.14.m14.2.2.2.4"><ci id="S3.SS1.p2.14.m14.1.1.1.1.cmml" xref="S3.SS1.p2.14.m14.1.1.1.1">𝐿</ci><ci id="S3.SS1.p2.14.m14.2.2.2.2.cmml" xref="S3.SS1.p2.14.m14.2.2.2.2">𝑅</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.14.m14.2c">\mathit{P}^{3D}_{L,R}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.14.m14.2d">italic_P start_POSTSUPERSCRIPT 3 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_L , italic_R end_POSTSUBSCRIPT</annotation></semantics></math>. The overview of the complete method is visible in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S3.F2" title="Figure 2 ‣ 3 Egocentric 3D Hand Pose Estimation and Action Recognition Enforced With Pseudo Depth ‣ SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="131" id="S3.F3.g1" src="extracted/5800341/images/icpr_ar_overview.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>
Our action recognition procedure. From the sequence of frames <math alttext="f_{1},f_{2},f_{3}...f_{n}" class="ltx_Math" display="inline" id="S3.F3.7.m1.3"><semantics id="S3.F3.7.m1.3b"><mrow id="S3.F3.7.m1.3.3.3" xref="S3.F3.7.m1.3.3.4.cmml"><msub id="S3.F3.7.m1.1.1.1.1" xref="S3.F3.7.m1.1.1.1.1.cmml"><mi id="S3.F3.7.m1.1.1.1.1.2" xref="S3.F3.7.m1.1.1.1.1.2.cmml">f</mi><mn id="S3.F3.7.m1.1.1.1.1.3" xref="S3.F3.7.m1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.F3.7.m1.3.3.3.4" xref="S3.F3.7.m1.3.3.4.cmml">,</mo><msub id="S3.F3.7.m1.2.2.2.2" xref="S3.F3.7.m1.2.2.2.2.cmml"><mi id="S3.F3.7.m1.2.2.2.2.2" xref="S3.F3.7.m1.2.2.2.2.2.cmml">f</mi><mn id="S3.F3.7.m1.2.2.2.2.3" xref="S3.F3.7.m1.2.2.2.2.3.cmml">2</mn></msub><mo id="S3.F3.7.m1.3.3.3.5" xref="S3.F3.7.m1.3.3.4.cmml">,</mo><mrow id="S3.F3.7.m1.3.3.3.3" xref="S3.F3.7.m1.3.3.3.3.cmml"><msub id="S3.F3.7.m1.3.3.3.3.2" xref="S3.F3.7.m1.3.3.3.3.2.cmml"><mi id="S3.F3.7.m1.3.3.3.3.2.2" xref="S3.F3.7.m1.3.3.3.3.2.2.cmml">f</mi><mn id="S3.F3.7.m1.3.3.3.3.2.3" xref="S3.F3.7.m1.3.3.3.3.2.3.cmml">3</mn></msub><mo id="S3.F3.7.m1.3.3.3.3.1" xref="S3.F3.7.m1.3.3.3.3.1.cmml">⁢</mo><mi id="S3.F3.7.m1.3.3.3.3.3" mathvariant="normal" xref="S3.F3.7.m1.3.3.3.3.3.cmml">…</mi><mo id="S3.F3.7.m1.3.3.3.3.1b" xref="S3.F3.7.m1.3.3.3.3.1.cmml">⁢</mo><msub id="S3.F3.7.m1.3.3.3.3.4" xref="S3.F3.7.m1.3.3.3.3.4.cmml"><mi id="S3.F3.7.m1.3.3.3.3.4.2" xref="S3.F3.7.m1.3.3.3.3.4.2.cmml">f</mi><mi id="S3.F3.7.m1.3.3.3.3.4.3" xref="S3.F3.7.m1.3.3.3.3.4.3.cmml">n</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.F3.7.m1.3c"><list id="S3.F3.7.m1.3.3.4.cmml" xref="S3.F3.7.m1.3.3.3"><apply id="S3.F3.7.m1.1.1.1.1.cmml" xref="S3.F3.7.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.F3.7.m1.1.1.1.1.1.cmml" xref="S3.F3.7.m1.1.1.1.1">subscript</csymbol><ci id="S3.F3.7.m1.1.1.1.1.2.cmml" xref="S3.F3.7.m1.1.1.1.1.2">𝑓</ci><cn id="S3.F3.7.m1.1.1.1.1.3.cmml" type="integer" xref="S3.F3.7.m1.1.1.1.1.3">1</cn></apply><apply id="S3.F3.7.m1.2.2.2.2.cmml" xref="S3.F3.7.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.F3.7.m1.2.2.2.2.1.cmml" xref="S3.F3.7.m1.2.2.2.2">subscript</csymbol><ci id="S3.F3.7.m1.2.2.2.2.2.cmml" xref="S3.F3.7.m1.2.2.2.2.2">𝑓</ci><cn id="S3.F3.7.m1.2.2.2.2.3.cmml" type="integer" xref="S3.F3.7.m1.2.2.2.2.3">2</cn></apply><apply id="S3.F3.7.m1.3.3.3.3.cmml" xref="S3.F3.7.m1.3.3.3.3"><times id="S3.F3.7.m1.3.3.3.3.1.cmml" xref="S3.F3.7.m1.3.3.3.3.1"></times><apply id="S3.F3.7.m1.3.3.3.3.2.cmml" xref="S3.F3.7.m1.3.3.3.3.2"><csymbol cd="ambiguous" id="S3.F3.7.m1.3.3.3.3.2.1.cmml" xref="S3.F3.7.m1.3.3.3.3.2">subscript</csymbol><ci id="S3.F3.7.m1.3.3.3.3.2.2.cmml" xref="S3.F3.7.m1.3.3.3.3.2.2">𝑓</ci><cn id="S3.F3.7.m1.3.3.3.3.2.3.cmml" type="integer" xref="S3.F3.7.m1.3.3.3.3.2.3">3</cn></apply><ci id="S3.F3.7.m1.3.3.3.3.3.cmml" xref="S3.F3.7.m1.3.3.3.3.3">…</ci><apply id="S3.F3.7.m1.3.3.3.3.4.cmml" xref="S3.F3.7.m1.3.3.3.3.4"><csymbol cd="ambiguous" id="S3.F3.7.m1.3.3.3.3.4.1.cmml" xref="S3.F3.7.m1.3.3.3.3.4">subscript</csymbol><ci id="S3.F3.7.m1.3.3.3.3.4.2.cmml" xref="S3.F3.7.m1.3.3.3.3.4.2">𝑓</ci><ci id="S3.F3.7.m1.3.3.3.3.4.3.cmml" xref="S3.F3.7.m1.3.3.3.3.4.3">𝑛</ci></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.7.m1.3d">f_{1},f_{2},f_{3}...f_{n}</annotation><annotation encoding="application/x-llamapun" id="S3.F3.7.m1.3e">italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT … italic_f start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> the hand pose <math alttext="Ph^{3D}_{L,R}" class="ltx_Math" display="inline" id="S3.F3.8.m2.2"><semantics id="S3.F3.8.m2.2b"><mrow id="S3.F3.8.m2.2.3" xref="S3.F3.8.m2.2.3.cmml"><mi id="S3.F3.8.m2.2.3.2" xref="S3.F3.8.m2.2.3.2.cmml">P</mi><mo id="S3.F3.8.m2.2.3.1" xref="S3.F3.8.m2.2.3.1.cmml">⁢</mo><msubsup id="S3.F3.8.m2.2.3.3" xref="S3.F3.8.m2.2.3.3.cmml"><mi id="S3.F3.8.m2.2.3.3.2.2" xref="S3.F3.8.m2.2.3.3.2.2.cmml">h</mi><mrow id="S3.F3.8.m2.2.2.2.4" xref="S3.F3.8.m2.2.2.2.3.cmml"><mi id="S3.F3.8.m2.1.1.1.1" xref="S3.F3.8.m2.1.1.1.1.cmml">L</mi><mo id="S3.F3.8.m2.2.2.2.4.1" xref="S3.F3.8.m2.2.2.2.3.cmml">,</mo><mi id="S3.F3.8.m2.2.2.2.2" xref="S3.F3.8.m2.2.2.2.2.cmml">R</mi></mrow><mrow id="S3.F3.8.m2.2.3.3.2.3" xref="S3.F3.8.m2.2.3.3.2.3.cmml"><mn id="S3.F3.8.m2.2.3.3.2.3.2" xref="S3.F3.8.m2.2.3.3.2.3.2.cmml">3</mn><mo id="S3.F3.8.m2.2.3.3.2.3.1" xref="S3.F3.8.m2.2.3.3.2.3.1.cmml">⁢</mo><mi id="S3.F3.8.m2.2.3.3.2.3.3" xref="S3.F3.8.m2.2.3.3.2.3.3.cmml">D</mi></mrow></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.F3.8.m2.2c"><apply id="S3.F3.8.m2.2.3.cmml" xref="S3.F3.8.m2.2.3"><times id="S3.F3.8.m2.2.3.1.cmml" xref="S3.F3.8.m2.2.3.1"></times><ci id="S3.F3.8.m2.2.3.2.cmml" xref="S3.F3.8.m2.2.3.2">𝑃</ci><apply id="S3.F3.8.m2.2.3.3.cmml" xref="S3.F3.8.m2.2.3.3"><csymbol cd="ambiguous" id="S3.F3.8.m2.2.3.3.1.cmml" xref="S3.F3.8.m2.2.3.3">subscript</csymbol><apply id="S3.F3.8.m2.2.3.3.2.cmml" xref="S3.F3.8.m2.2.3.3"><csymbol cd="ambiguous" id="S3.F3.8.m2.2.3.3.2.1.cmml" xref="S3.F3.8.m2.2.3.3">superscript</csymbol><ci id="S3.F3.8.m2.2.3.3.2.2.cmml" xref="S3.F3.8.m2.2.3.3.2.2">ℎ</ci><apply id="S3.F3.8.m2.2.3.3.2.3.cmml" xref="S3.F3.8.m2.2.3.3.2.3"><times id="S3.F3.8.m2.2.3.3.2.3.1.cmml" xref="S3.F3.8.m2.2.3.3.2.3.1"></times><cn id="S3.F3.8.m2.2.3.3.2.3.2.cmml" type="integer" xref="S3.F3.8.m2.2.3.3.2.3.2">3</cn><ci id="S3.F3.8.m2.2.3.3.2.3.3.cmml" xref="S3.F3.8.m2.2.3.3.2.3.3">𝐷</ci></apply></apply><list id="S3.F3.8.m2.2.2.2.3.cmml" xref="S3.F3.8.m2.2.2.2.4"><ci id="S3.F3.8.m2.1.1.1.1.cmml" xref="S3.F3.8.m2.1.1.1.1">𝐿</ci><ci id="S3.F3.8.m2.2.2.2.2.cmml" xref="S3.F3.8.m2.2.2.2.2">𝑅</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.8.m2.2d">Ph^{3D}_{L,R}</annotation><annotation encoding="application/x-llamapun" id="S3.F3.8.m2.2e">italic_P italic_h start_POSTSUPERSCRIPT 3 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_L , italic_R end_POSTSUBSCRIPT</annotation></semantics></math> is estimated with <span class="ltx_text ltx_font_italic" id="S3.F3.16.1">SHARP</span> and <span class="ltx_text ltx_font_italic" id="S3.F3.17.2">EffHandEgoNet3D</span> model and the object pose <math alttext="Po^{2D}" class="ltx_Math" display="inline" id="S3.F3.9.m3.1"><semantics id="S3.F3.9.m3.1b"><mrow id="S3.F3.9.m3.1.1" xref="S3.F3.9.m3.1.1.cmml"><mi id="S3.F3.9.m3.1.1.2" xref="S3.F3.9.m3.1.1.2.cmml">P</mi><mo id="S3.F3.9.m3.1.1.1" xref="S3.F3.9.m3.1.1.1.cmml">⁢</mo><msup id="S3.F3.9.m3.1.1.3" xref="S3.F3.9.m3.1.1.3.cmml"><mi id="S3.F3.9.m3.1.1.3.2" xref="S3.F3.9.m3.1.1.3.2.cmml">o</mi><mrow id="S3.F3.9.m3.1.1.3.3" xref="S3.F3.9.m3.1.1.3.3.cmml"><mn id="S3.F3.9.m3.1.1.3.3.2" xref="S3.F3.9.m3.1.1.3.3.2.cmml">2</mn><mo id="S3.F3.9.m3.1.1.3.3.1" xref="S3.F3.9.m3.1.1.3.3.1.cmml">⁢</mo><mi id="S3.F3.9.m3.1.1.3.3.3" xref="S3.F3.9.m3.1.1.3.3.3.cmml">D</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.F3.9.m3.1c"><apply id="S3.F3.9.m3.1.1.cmml" xref="S3.F3.9.m3.1.1"><times id="S3.F3.9.m3.1.1.1.cmml" xref="S3.F3.9.m3.1.1.1"></times><ci id="S3.F3.9.m3.1.1.2.cmml" xref="S3.F3.9.m3.1.1.2">𝑃</ci><apply id="S3.F3.9.m3.1.1.3.cmml" xref="S3.F3.9.m3.1.1.3"><csymbol cd="ambiguous" id="S3.F3.9.m3.1.1.3.1.cmml" xref="S3.F3.9.m3.1.1.3">superscript</csymbol><ci id="S3.F3.9.m3.1.1.3.2.cmml" xref="S3.F3.9.m3.1.1.3.2">𝑜</ci><apply id="S3.F3.9.m3.1.1.3.3.cmml" xref="S3.F3.9.m3.1.1.3.3"><times id="S3.F3.9.m3.1.1.3.3.1.cmml" xref="S3.F3.9.m3.1.1.3.3.1"></times><cn id="S3.F3.9.m3.1.1.3.3.2.cmml" type="integer" xref="S3.F3.9.m3.1.1.3.3.2">2</cn><ci id="S3.F3.9.m3.1.1.3.3.3.cmml" xref="S3.F3.9.m3.1.1.3.3.3">𝐷</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.9.m3.1d">Po^{2D}</annotation><annotation encoding="application/x-llamapun" id="S3.F3.9.m3.1e">italic_P italic_o start_POSTSUPERSCRIPT 2 italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="Po_{l}" class="ltx_Math" display="inline" id="S3.F3.10.m4.1"><semantics id="S3.F3.10.m4.1b"><mrow id="S3.F3.10.m4.1.1" xref="S3.F3.10.m4.1.1.cmml"><mi id="S3.F3.10.m4.1.1.2" xref="S3.F3.10.m4.1.1.2.cmml">P</mi><mo id="S3.F3.10.m4.1.1.1" xref="S3.F3.10.m4.1.1.1.cmml">⁢</mo><msub id="S3.F3.10.m4.1.1.3" xref="S3.F3.10.m4.1.1.3.cmml"><mi id="S3.F3.10.m4.1.1.3.2" xref="S3.F3.10.m4.1.1.3.2.cmml">o</mi><mi id="S3.F3.10.m4.1.1.3.3" xref="S3.F3.10.m4.1.1.3.3.cmml">l</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.F3.10.m4.1c"><apply id="S3.F3.10.m4.1.1.cmml" xref="S3.F3.10.m4.1.1"><times id="S3.F3.10.m4.1.1.1.cmml" xref="S3.F3.10.m4.1.1.1"></times><ci id="S3.F3.10.m4.1.1.2.cmml" xref="S3.F3.10.m4.1.1.2">𝑃</ci><apply id="S3.F3.10.m4.1.1.3.cmml" xref="S3.F3.10.m4.1.1.3"><csymbol cd="ambiguous" id="S3.F3.10.m4.1.1.3.1.cmml" xref="S3.F3.10.m4.1.1.3">subscript</csymbol><ci id="S3.F3.10.m4.1.1.3.2.cmml" xref="S3.F3.10.m4.1.1.3.2">𝑜</ci><ci id="S3.F3.10.m4.1.1.3.3.cmml" xref="S3.F3.10.m4.1.1.3.3">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.10.m4.1d">Po_{l}</annotation><annotation encoding="application/x-llamapun" id="S3.F3.10.m4.1e">italic_P italic_o start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT</annotation></semantics></math> is extracted with <span class="ltx_text ltx_font_italic" id="S3.F3.18.3">YOLOv7</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib27" title="">27</a>]</cite>. Each sequence frame <math alttext="f_{n}" class="ltx_Math" display="inline" id="S3.F3.11.m5.1"><semantics id="S3.F3.11.m5.1b"><msub id="S3.F3.11.m5.1.1" xref="S3.F3.11.m5.1.1.cmml"><mi id="S3.F3.11.m5.1.1.2" xref="S3.F3.11.m5.1.1.2.cmml">f</mi><mi id="S3.F3.11.m5.1.1.3" xref="S3.F3.11.m5.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F3.11.m5.1c"><apply id="S3.F3.11.m5.1.1.cmml" xref="S3.F3.11.m5.1.1"><csymbol cd="ambiguous" id="S3.F3.11.m5.1.1.1.cmml" xref="S3.F3.11.m5.1.1">subscript</csymbol><ci id="S3.F3.11.m5.1.1.2.cmml" xref="S3.F3.11.m5.1.1.2">𝑓</ci><ci id="S3.F3.11.m5.1.1.3.cmml" xref="S3.F3.11.m5.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.11.m5.1d">f_{n}</annotation><annotation encoding="application/x-llamapun" id="S3.F3.11.m5.1e">italic_f start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> is linearised and positional embedding and classification tokens are added. Next, this sequence is passed to a transformer encoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib8" title="">8</a>]</cite> repeated <math alttext="\times 2" class="ltx_Math" display="inline" id="S3.F3.12.m6.1"><semantics id="S3.F3.12.m6.1b"><mrow id="S3.F3.12.m6.1.1" xref="S3.F3.12.m6.1.1.cmml"><mi id="S3.F3.12.m6.1.1.2" xref="S3.F3.12.m6.1.1.2.cmml"></mi><mo id="S3.F3.12.m6.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.F3.12.m6.1.1.1.cmml">×</mo><mn id="S3.F3.12.m6.1.1.3" xref="S3.F3.12.m6.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.F3.12.m6.1c"><apply id="S3.F3.12.m6.1.1.cmml" xref="S3.F3.12.m6.1.1"><times id="S3.F3.12.m6.1.1.1.cmml" xref="S3.F3.12.m6.1.1.1"></times><csymbol cd="latexml" id="S3.F3.12.m6.1.1.2.cmml" xref="S3.F3.12.m6.1.1.2">absent</csymbol><cn id="S3.F3.12.m6.1.1.3.cmml" type="integer" xref="S3.F3.12.m6.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.12.m6.1d">\times 2</annotation><annotation encoding="application/x-llamapun" id="S3.F3.12.m6.1e">× 2</annotation></semantics></math> times, which embeds the temporal information. Finally, the MLP predicts one of the 36 action labels.
</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Egocentric Action Recognition based on 3D Hand Pose</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.3">We perform egocentric action recognition from image sequences using estimated 3D hand pose and 2D information about interacting object. The actions considered in this study are those in which humans manipulate objects with one or both hands, such as <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.3.1">pouring milk</span> or <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.3.2">opening a bottle</span>. An overview of the pipeline is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S3.F3" title="Figure 3 ‣ 3.1 Egocentric 3D Hand Pose with Pseudo-Depth Segmentation ‣ 3 Egocentric 3D Hand Pose Estimation and Action Recognition Enforced With Pseudo Depth ‣ SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_tag">3</span></a>. It consists of three distinct components: object detection, 3D hand pose estimation, and finally action recognition using a transformer encoder and a classification MLP. The architecture improves egocentric action recognition based on the 2D hand pose introduced in <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.3.3">EffHandEgoNet</span> study <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib18" title="">18</a>]</cite>. The first step in the pipeline is object detection, which is carried out employing the pre-trained <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.3.4">YOLOv7</span> network <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib27" title="">27</a>]</cite>. In each frame, denoted as <math alttext="\mathit{f}_{n}" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><msub id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">f</mi><mi id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">𝑓</ci><ci id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\mathit{f}_{n}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_f start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math>, the interacting object is represented by <math alttext="\mathit{Po}_{2D}(x,y)\in\mathbb{R}^{4\times 2}" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.2"><semantics id="S3.SS2.p1.2.m2.2a"><mrow id="S3.SS2.p1.2.m2.2.3" xref="S3.SS2.p1.2.m2.2.3.cmml"><mrow id="S3.SS2.p1.2.m2.2.3.2" xref="S3.SS2.p1.2.m2.2.3.2.cmml"><msub id="S3.SS2.p1.2.m2.2.3.2.2" xref="S3.SS2.p1.2.m2.2.3.2.2.cmml"><mi id="S3.SS2.p1.2.m2.2.3.2.2.2" xref="S3.SS2.p1.2.m2.2.3.2.2.2.cmml">𝑃𝑜</mi><mrow id="S3.SS2.p1.2.m2.2.3.2.2.3" xref="S3.SS2.p1.2.m2.2.3.2.2.3.cmml"><mn id="S3.SS2.p1.2.m2.2.3.2.2.3.2" xref="S3.SS2.p1.2.m2.2.3.2.2.3.2.cmml">2</mn><mo id="S3.SS2.p1.2.m2.2.3.2.2.3.1" xref="S3.SS2.p1.2.m2.2.3.2.2.3.1.cmml">⁢</mo><mi id="S3.SS2.p1.2.m2.2.3.2.2.3.3" xref="S3.SS2.p1.2.m2.2.3.2.2.3.3.cmml">D</mi></mrow></msub><mo id="S3.SS2.p1.2.m2.2.3.2.1" xref="S3.SS2.p1.2.m2.2.3.2.1.cmml">⁢</mo><mrow id="S3.SS2.p1.2.m2.2.3.2.3.2" xref="S3.SS2.p1.2.m2.2.3.2.3.1.cmml"><mo id="S3.SS2.p1.2.m2.2.3.2.3.2.1" stretchy="false" xref="S3.SS2.p1.2.m2.2.3.2.3.1.cmml">(</mo><mi id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">x</mi><mo id="S3.SS2.p1.2.m2.2.3.2.3.2.2" xref="S3.SS2.p1.2.m2.2.3.2.3.1.cmml">,</mo><mi id="S3.SS2.p1.2.m2.2.2" xref="S3.SS2.p1.2.m2.2.2.cmml">y</mi><mo id="S3.SS2.p1.2.m2.2.3.2.3.2.3" stretchy="false" xref="S3.SS2.p1.2.m2.2.3.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.SS2.p1.2.m2.2.3.1" xref="S3.SS2.p1.2.m2.2.3.1.cmml">∈</mo><msup id="S3.SS2.p1.2.m2.2.3.3" xref="S3.SS2.p1.2.m2.2.3.3.cmml"><mi id="S3.SS2.p1.2.m2.2.3.3.2" xref="S3.SS2.p1.2.m2.2.3.3.2.cmml">ℝ</mi><mrow id="S3.SS2.p1.2.m2.2.3.3.3" xref="S3.SS2.p1.2.m2.2.3.3.3.cmml"><mn id="S3.SS2.p1.2.m2.2.3.3.3.2" xref="S3.SS2.p1.2.m2.2.3.3.3.2.cmml">4</mn><mo id="S3.SS2.p1.2.m2.2.3.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.2.m2.2.3.3.3.1.cmml">×</mo><mn id="S3.SS2.p1.2.m2.2.3.3.3.3" xref="S3.SS2.p1.2.m2.2.3.3.3.3.cmml">2</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.2b"><apply id="S3.SS2.p1.2.m2.2.3.cmml" xref="S3.SS2.p1.2.m2.2.3"><in id="S3.SS2.p1.2.m2.2.3.1.cmml" xref="S3.SS2.p1.2.m2.2.3.1"></in><apply id="S3.SS2.p1.2.m2.2.3.2.cmml" xref="S3.SS2.p1.2.m2.2.3.2"><times id="S3.SS2.p1.2.m2.2.3.2.1.cmml" xref="S3.SS2.p1.2.m2.2.3.2.1"></times><apply id="S3.SS2.p1.2.m2.2.3.2.2.cmml" xref="S3.SS2.p1.2.m2.2.3.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.2.3.2.2.1.cmml" xref="S3.SS2.p1.2.m2.2.3.2.2">subscript</csymbol><ci id="S3.SS2.p1.2.m2.2.3.2.2.2.cmml" xref="S3.SS2.p1.2.m2.2.3.2.2.2">𝑃𝑜</ci><apply id="S3.SS2.p1.2.m2.2.3.2.2.3.cmml" xref="S3.SS2.p1.2.m2.2.3.2.2.3"><times id="S3.SS2.p1.2.m2.2.3.2.2.3.1.cmml" xref="S3.SS2.p1.2.m2.2.3.2.2.3.1"></times><cn id="S3.SS2.p1.2.m2.2.3.2.2.3.2.cmml" type="integer" xref="S3.SS2.p1.2.m2.2.3.2.2.3.2">2</cn><ci id="S3.SS2.p1.2.m2.2.3.2.2.3.3.cmml" xref="S3.SS2.p1.2.m2.2.3.2.2.3.3">𝐷</ci></apply></apply><interval closure="open" id="S3.SS2.p1.2.m2.2.3.2.3.1.cmml" xref="S3.SS2.p1.2.m2.2.3.2.3.2"><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">𝑥</ci><ci id="S3.SS2.p1.2.m2.2.2.cmml" xref="S3.SS2.p1.2.m2.2.2">𝑦</ci></interval></apply><apply id="S3.SS2.p1.2.m2.2.3.3.cmml" xref="S3.SS2.p1.2.m2.2.3.3"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.2.3.3.1.cmml" xref="S3.SS2.p1.2.m2.2.3.3">superscript</csymbol><ci id="S3.SS2.p1.2.m2.2.3.3.2.cmml" xref="S3.SS2.p1.2.m2.2.3.3.2">ℝ</ci><apply id="S3.SS2.p1.2.m2.2.3.3.3.cmml" xref="S3.SS2.p1.2.m2.2.3.3.3"><times id="S3.SS2.p1.2.m2.2.3.3.3.1.cmml" xref="S3.SS2.p1.2.m2.2.3.3.3.1"></times><cn id="S3.SS2.p1.2.m2.2.3.3.3.2.cmml" type="integer" xref="S3.SS2.p1.2.m2.2.3.3.3.2">4</cn><cn id="S3.SS2.p1.2.m2.2.3.3.3.3.cmml" type="integer" xref="S3.SS2.p1.2.m2.2.3.3.3.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.2c">\mathit{Po}_{2D}(x,y)\in\mathbb{R}^{4\times 2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.2d">italic_Po start_POSTSUBSCRIPT 2 italic_D end_POSTSUBSCRIPT ( italic_x , italic_y ) ∈ blackboard_R start_POSTSUPERSCRIPT 4 × 2 end_POSTSUPERSCRIPT</annotation></semantics></math>, where each point corresponds to the corners of its bounding box. Additionally, <math alttext="\mathit{Po}_{l}\in\mathbb{R}^{1}" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3.1"><semantics id="S3.SS2.p1.3.m3.1a"><mrow id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml"><msub id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml"><mi id="S3.SS2.p1.3.m3.1.1.2.2" xref="S3.SS2.p1.3.m3.1.1.2.2.cmml">𝑃𝑜</mi><mi id="S3.SS2.p1.3.m3.1.1.2.3" xref="S3.SS2.p1.3.m3.1.1.2.3.cmml">l</mi></msub><mo id="S3.SS2.p1.3.m3.1.1.1" xref="S3.SS2.p1.3.m3.1.1.1.cmml">∈</mo><msup id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml"><mi id="S3.SS2.p1.3.m3.1.1.3.2" xref="S3.SS2.p1.3.m3.1.1.3.2.cmml">ℝ</mi><mn id="S3.SS2.p1.3.m3.1.1.3.3" xref="S3.SS2.p1.3.m3.1.1.3.3.cmml">1</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1"><in id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1.1"></in><apply id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.2.1.cmml" xref="S3.SS2.p1.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS2.p1.3.m3.1.1.2.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2.2">𝑃𝑜</ci><ci id="S3.SS2.p1.3.m3.1.1.2.3.cmml" xref="S3.SS2.p1.3.m3.1.1.2.3">𝑙</ci></apply><apply id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.3.1.cmml" xref="S3.SS2.p1.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS2.p1.3.m3.1.1.3.2.cmml" xref="S3.SS2.p1.3.m3.1.1.3.2">ℝ</ci><cn id="S3.SS2.p1.3.m3.1.1.3.3.cmml" type="integer" xref="S3.SS2.p1.3.m3.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">\mathit{Po}_{l}\in\mathbb{R}^{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.3.m3.1d">italic_Po start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT</annotation></semantics></math> represents object’s label.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.10">The representation of each action sequence consists of frames <math alttext="\mathit{[}f_{1},f_{2},f_{3},...,f_{n}]" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.5"><semantics id="S3.SS2.p2.1.m1.5a"><mrow id="S3.SS2.p2.1.m1.5.5.4" xref="S3.SS2.p2.1.m1.5.5.5.cmml"><mo id="S3.SS2.p2.1.m1.5.5.4.5" stretchy="false" xref="S3.SS2.p2.1.m1.5.5.5.cmml">[</mo><msub id="S3.SS2.p2.1.m1.2.2.1.1" xref="S3.SS2.p2.1.m1.2.2.1.1.cmml"><mi id="S3.SS2.p2.1.m1.2.2.1.1.2" xref="S3.SS2.p2.1.m1.2.2.1.1.2.cmml">f</mi><mn id="S3.SS2.p2.1.m1.2.2.1.1.3" xref="S3.SS2.p2.1.m1.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS2.p2.1.m1.5.5.4.6" xref="S3.SS2.p2.1.m1.5.5.5.cmml">,</mo><msub id="S3.SS2.p2.1.m1.3.3.2.2" xref="S3.SS2.p2.1.m1.3.3.2.2.cmml"><mi id="S3.SS2.p2.1.m1.3.3.2.2.2" xref="S3.SS2.p2.1.m1.3.3.2.2.2.cmml">f</mi><mn id="S3.SS2.p2.1.m1.3.3.2.2.3" xref="S3.SS2.p2.1.m1.3.3.2.2.3.cmml">2</mn></msub><mo id="S3.SS2.p2.1.m1.5.5.4.7" xref="S3.SS2.p2.1.m1.5.5.5.cmml">,</mo><msub id="S3.SS2.p2.1.m1.4.4.3.3" xref="S3.SS2.p2.1.m1.4.4.3.3.cmml"><mi id="S3.SS2.p2.1.m1.4.4.3.3.2" xref="S3.SS2.p2.1.m1.4.4.3.3.2.cmml">f</mi><mn id="S3.SS2.p2.1.m1.4.4.3.3.3" xref="S3.SS2.p2.1.m1.4.4.3.3.3.cmml">3</mn></msub><mo id="S3.SS2.p2.1.m1.5.5.4.8" xref="S3.SS2.p2.1.m1.5.5.5.cmml">,</mo><mi id="S3.SS2.p2.1.m1.1.1" mathvariant="normal" xref="S3.SS2.p2.1.m1.1.1.cmml">…</mi><mo id="S3.SS2.p2.1.m1.5.5.4.9" xref="S3.SS2.p2.1.m1.5.5.5.cmml">,</mo><msub id="S3.SS2.p2.1.m1.5.5.4.4" xref="S3.SS2.p2.1.m1.5.5.4.4.cmml"><mi id="S3.SS2.p2.1.m1.5.5.4.4.2" xref="S3.SS2.p2.1.m1.5.5.4.4.2.cmml">f</mi><mi id="S3.SS2.p2.1.m1.5.5.4.4.3" xref="S3.SS2.p2.1.m1.5.5.4.4.3.cmml">n</mi></msub><mo id="S3.SS2.p2.1.m1.5.5.4.10" stretchy="false" xref="S3.SS2.p2.1.m1.5.5.5.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.5b"><list id="S3.SS2.p2.1.m1.5.5.5.cmml" xref="S3.SS2.p2.1.m1.5.5.4"><apply id="S3.SS2.p2.1.m1.2.2.1.1.cmml" xref="S3.SS2.p2.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.2.2.1.1.1.cmml" xref="S3.SS2.p2.1.m1.2.2.1.1">subscript</csymbol><ci id="S3.SS2.p2.1.m1.2.2.1.1.2.cmml" xref="S3.SS2.p2.1.m1.2.2.1.1.2">𝑓</ci><cn id="S3.SS2.p2.1.m1.2.2.1.1.3.cmml" type="integer" xref="S3.SS2.p2.1.m1.2.2.1.1.3">1</cn></apply><apply id="S3.SS2.p2.1.m1.3.3.2.2.cmml" xref="S3.SS2.p2.1.m1.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.3.3.2.2.1.cmml" xref="S3.SS2.p2.1.m1.3.3.2.2">subscript</csymbol><ci id="S3.SS2.p2.1.m1.3.3.2.2.2.cmml" xref="S3.SS2.p2.1.m1.3.3.2.2.2">𝑓</ci><cn id="S3.SS2.p2.1.m1.3.3.2.2.3.cmml" type="integer" xref="S3.SS2.p2.1.m1.3.3.2.2.3">2</cn></apply><apply id="S3.SS2.p2.1.m1.4.4.3.3.cmml" xref="S3.SS2.p2.1.m1.4.4.3.3"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.4.4.3.3.1.cmml" xref="S3.SS2.p2.1.m1.4.4.3.3">subscript</csymbol><ci id="S3.SS2.p2.1.m1.4.4.3.3.2.cmml" xref="S3.SS2.p2.1.m1.4.4.3.3.2">𝑓</ci><cn id="S3.SS2.p2.1.m1.4.4.3.3.3.cmml" type="integer" xref="S3.SS2.p2.1.m1.4.4.3.3.3">3</cn></apply><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">…</ci><apply id="S3.SS2.p2.1.m1.5.5.4.4.cmml" xref="S3.SS2.p2.1.m1.5.5.4.4"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.5.5.4.4.1.cmml" xref="S3.SS2.p2.1.m1.5.5.4.4">subscript</csymbol><ci id="S3.SS2.p2.1.m1.5.5.4.4.2.cmml" xref="S3.SS2.p2.1.m1.5.5.4.4.2">𝑓</ci><ci id="S3.SS2.p2.1.m1.5.5.4.4.3.cmml" xref="S3.SS2.p2.1.m1.5.5.4.4.3">𝑛</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.5c">\mathit{[}f_{1},f_{2},f_{3},...,f_{n}]</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.5d">[ italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , … , italic_f start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ]</annotation></semantics></math>, where <math alttext="n\in[1..N]" class="ltx_math_unparsed" display="inline" id="S3.SS2.p2.2.m2.1"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1b"><mi id="S3.SS2.p2.2.m2.1.1">n</mi><mo id="S3.SS2.p2.2.m2.1.2">∈</mo><mrow id="S3.SS2.p2.2.m2.1.3"><mo id="S3.SS2.p2.2.m2.1.3.1" stretchy="false">[</mo><mn id="S3.SS2.p2.2.m2.1.3.2">1</mn><mo id="S3.SS2.p2.2.m2.1.3.3" lspace="0em" rspace="0.0835em">.</mo><mo id="S3.SS2.p2.2.m2.1.3.4" lspace="0.0835em" rspace="0.167em">.</mo><mi id="S3.SS2.p2.2.m2.1.3.5">N</mi><mo id="S3.SS2.p2.2.m2.1.3.6" stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">n\in[1..N]</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m2.1d">italic_n ∈ [ 1 . . italic_N ]</annotation></semantics></math> and <math alttext="N=20" class="ltx_Math" display="inline" id="S3.SS2.p2.3.m3.1"><semantics id="S3.SS2.p2.3.m3.1a"><mrow id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><mi id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml">N</mi><mo id="S3.SS2.p2.3.m3.1.1.1" xref="S3.SS2.p2.3.m3.1.1.1.cmml">=</mo><mn id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><eq id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1.1"></eq><ci id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2">𝑁</ci><cn id="S3.SS2.p2.3.m3.1.1.3.cmml" type="integer" xref="S3.SS2.p2.3.m3.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">N=20</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m3.1d">italic_N = 20</annotation></semantics></math> following <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib18" title="">18</a>]</cite>. These frames embed flattened poses of hands <math alttext="Ph^{3D}_{L,R}" class="ltx_Math" display="inline" id="S3.SS2.p2.4.m4.2"><semantics id="S3.SS2.p2.4.m4.2a"><mrow id="S3.SS2.p2.4.m4.2.3" xref="S3.SS2.p2.4.m4.2.3.cmml"><mi id="S3.SS2.p2.4.m4.2.3.2" xref="S3.SS2.p2.4.m4.2.3.2.cmml">P</mi><mo id="S3.SS2.p2.4.m4.2.3.1" xref="S3.SS2.p2.4.m4.2.3.1.cmml">⁢</mo><msubsup id="S3.SS2.p2.4.m4.2.3.3" xref="S3.SS2.p2.4.m4.2.3.3.cmml"><mi id="S3.SS2.p2.4.m4.2.3.3.2.2" xref="S3.SS2.p2.4.m4.2.3.3.2.2.cmml">h</mi><mrow id="S3.SS2.p2.4.m4.2.2.2.4" xref="S3.SS2.p2.4.m4.2.2.2.3.cmml"><mi id="S3.SS2.p2.4.m4.1.1.1.1" xref="S3.SS2.p2.4.m4.1.1.1.1.cmml">L</mi><mo id="S3.SS2.p2.4.m4.2.2.2.4.1" xref="S3.SS2.p2.4.m4.2.2.2.3.cmml">,</mo><mi id="S3.SS2.p2.4.m4.2.2.2.2" xref="S3.SS2.p2.4.m4.2.2.2.2.cmml">R</mi></mrow><mrow id="S3.SS2.p2.4.m4.2.3.3.2.3" xref="S3.SS2.p2.4.m4.2.3.3.2.3.cmml"><mn id="S3.SS2.p2.4.m4.2.3.3.2.3.2" xref="S3.SS2.p2.4.m4.2.3.3.2.3.2.cmml">3</mn><mo id="S3.SS2.p2.4.m4.2.3.3.2.3.1" xref="S3.SS2.p2.4.m4.2.3.3.2.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.4.m4.2.3.3.2.3.3" xref="S3.SS2.p2.4.m4.2.3.3.2.3.3.cmml">D</mi></mrow></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.2b"><apply id="S3.SS2.p2.4.m4.2.3.cmml" xref="S3.SS2.p2.4.m4.2.3"><times id="S3.SS2.p2.4.m4.2.3.1.cmml" xref="S3.SS2.p2.4.m4.2.3.1"></times><ci id="S3.SS2.p2.4.m4.2.3.2.cmml" xref="S3.SS2.p2.4.m4.2.3.2">𝑃</ci><apply id="S3.SS2.p2.4.m4.2.3.3.cmml" xref="S3.SS2.p2.4.m4.2.3.3"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.2.3.3.1.cmml" xref="S3.SS2.p2.4.m4.2.3.3">subscript</csymbol><apply id="S3.SS2.p2.4.m4.2.3.3.2.cmml" xref="S3.SS2.p2.4.m4.2.3.3"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.2.3.3.2.1.cmml" xref="S3.SS2.p2.4.m4.2.3.3">superscript</csymbol><ci id="S3.SS2.p2.4.m4.2.3.3.2.2.cmml" xref="S3.SS2.p2.4.m4.2.3.3.2.2">ℎ</ci><apply id="S3.SS2.p2.4.m4.2.3.3.2.3.cmml" xref="S3.SS2.p2.4.m4.2.3.3.2.3"><times id="S3.SS2.p2.4.m4.2.3.3.2.3.1.cmml" xref="S3.SS2.p2.4.m4.2.3.3.2.3.1"></times><cn id="S3.SS2.p2.4.m4.2.3.3.2.3.2.cmml" type="integer" xref="S3.SS2.p2.4.m4.2.3.3.2.3.2">3</cn><ci id="S3.SS2.p2.4.m4.2.3.3.2.3.3.cmml" xref="S3.SS2.p2.4.m4.2.3.3.2.3.3">𝐷</ci></apply></apply><list id="S3.SS2.p2.4.m4.2.2.2.3.cmml" xref="S3.SS2.p2.4.m4.2.2.2.4"><ci id="S3.SS2.p2.4.m4.1.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1">𝐿</ci><ci id="S3.SS2.p2.4.m4.2.2.2.2.cmml" xref="S3.SS2.p2.4.m4.2.2.2.2">𝑅</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.2c">Ph^{3D}_{L,R}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.4.m4.2d">italic_P italic_h start_POSTSUPERSCRIPT 3 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_L , italic_R end_POSTSUBSCRIPT</annotation></semantics></math> and object <math alttext="Po_{2D}" class="ltx_Math" display="inline" id="S3.SS2.p2.5.m5.1"><semantics id="S3.SS2.p2.5.m5.1a"><mrow id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml"><mi id="S3.SS2.p2.5.m5.1.1.2" xref="S3.SS2.p2.5.m5.1.1.2.cmml">P</mi><mo id="S3.SS2.p2.5.m5.1.1.1" xref="S3.SS2.p2.5.m5.1.1.1.cmml">⁢</mo><msub id="S3.SS2.p2.5.m5.1.1.3" xref="S3.SS2.p2.5.m5.1.1.3.cmml"><mi id="S3.SS2.p2.5.m5.1.1.3.2" xref="S3.SS2.p2.5.m5.1.1.3.2.cmml">o</mi><mrow id="S3.SS2.p2.5.m5.1.1.3.3" xref="S3.SS2.p2.5.m5.1.1.3.3.cmml"><mn id="S3.SS2.p2.5.m5.1.1.3.3.2" xref="S3.SS2.p2.5.m5.1.1.3.3.2.cmml">2</mn><mo id="S3.SS2.p2.5.m5.1.1.3.3.1" xref="S3.SS2.p2.5.m5.1.1.3.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.5.m5.1.1.3.3.3" xref="S3.SS2.p2.5.m5.1.1.3.3.3.cmml">D</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><apply id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1"><times id="S3.SS2.p2.5.m5.1.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1.1"></times><ci id="S3.SS2.p2.5.m5.1.1.2.cmml" xref="S3.SS2.p2.5.m5.1.1.2">𝑃</ci><apply id="S3.SS2.p2.5.m5.1.1.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m5.1.1.3.1.cmml" xref="S3.SS2.p2.5.m5.1.1.3">subscript</csymbol><ci id="S3.SS2.p2.5.m5.1.1.3.2.cmml" xref="S3.SS2.p2.5.m5.1.1.3.2">𝑜</ci><apply id="S3.SS2.p2.5.m5.1.1.3.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3.3"><times id="S3.SS2.p2.5.m5.1.1.3.3.1.cmml" xref="S3.SS2.p2.5.m5.1.1.3.3.1"></times><cn id="S3.SS2.p2.5.m5.1.1.3.3.2.cmml" type="integer" xref="S3.SS2.p2.5.m5.1.1.3.3.2">2</cn><ci id="S3.SS2.p2.5.m5.1.1.3.3.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3.3.3">𝐷</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">Po_{2D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.5.m5.1d">italic_P italic_o start_POSTSUBSCRIPT 2 italic_D end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="Po_{l}" class="ltx_Math" display="inline" id="S3.SS2.p2.6.m6.1"><semantics id="S3.SS2.p2.6.m6.1a"><mrow id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml"><mi id="S3.SS2.p2.6.m6.1.1.2" xref="S3.SS2.p2.6.m6.1.1.2.cmml">P</mi><mo id="S3.SS2.p2.6.m6.1.1.1" xref="S3.SS2.p2.6.m6.1.1.1.cmml">⁢</mo><msub id="S3.SS2.p2.6.m6.1.1.3" xref="S3.SS2.p2.6.m6.1.1.3.cmml"><mi id="S3.SS2.p2.6.m6.1.1.3.2" xref="S3.SS2.p2.6.m6.1.1.3.2.cmml">o</mi><mi id="S3.SS2.p2.6.m6.1.1.3.3" xref="S3.SS2.p2.6.m6.1.1.3.3.cmml">l</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><apply id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1"><times id="S3.SS2.p2.6.m6.1.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1.1"></times><ci id="S3.SS2.p2.6.m6.1.1.2.cmml" xref="S3.SS2.p2.6.m6.1.1.2">𝑃</ci><apply id="S3.SS2.p2.6.m6.1.1.3.cmml" xref="S3.SS2.p2.6.m6.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.6.m6.1.1.3.1.cmml" xref="S3.SS2.p2.6.m6.1.1.3">subscript</csymbol><ci id="S3.SS2.p2.6.m6.1.1.3.2.cmml" xref="S3.SS2.p2.6.m6.1.1.3.2">𝑜</ci><ci id="S3.SS2.p2.6.m6.1.1.3.3.cmml" xref="S3.SS2.p2.6.m6.1.1.3.3">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">Po_{l}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.6.m6.1d">italic_P italic_o start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT</annotation></semantics></math>. If fewer than <math alttext="N" class="ltx_Math" display="inline" id="S3.SS2.p2.7.m7.1"><semantics id="S3.SS2.p2.7.m7.1a"><mi id="S3.SS2.p2.7.m7.1.1" xref="S3.SS2.p2.7.m7.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m7.1b"><ci id="S3.SS2.p2.7.m7.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m7.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.7.m7.1d">italic_N</annotation></semantics></math> frames represent an action, zero padding is applied, while actions longer than <math alttext="N" class="ltx_Math" display="inline" id="S3.SS2.p2.8.m8.1"><semantics id="S3.SS2.p2.8.m8.1a"><mi id="S3.SS2.p2.8.m8.1.1" xref="S3.SS2.p2.8.m8.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.8.m8.1b"><ci id="S3.SS2.p2.8.m8.1.1.cmml" xref="S3.SS2.p2.8.m8.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.8.m8.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.8.m8.1d">italic_N</annotation></semantics></math> frames are sub-sampled.
The input vector <math alttext="\mathit{V}_{seq}" class="ltx_Math" display="inline" id="S3.SS2.p2.9.m9.1"><semantics id="S3.SS2.p2.9.m9.1a"><msub id="S3.SS2.p2.9.m9.1.1" xref="S3.SS2.p2.9.m9.1.1.cmml"><mi id="S3.SS2.p2.9.m9.1.1.2" xref="S3.SS2.p2.9.m9.1.1.2.cmml">V</mi><mrow id="S3.SS2.p2.9.m9.1.1.3" xref="S3.SS2.p2.9.m9.1.1.3.cmml"><mi id="S3.SS2.p2.9.m9.1.1.3.2" xref="S3.SS2.p2.9.m9.1.1.3.2.cmml">s</mi><mo id="S3.SS2.p2.9.m9.1.1.3.1" xref="S3.SS2.p2.9.m9.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.9.m9.1.1.3.3" xref="S3.SS2.p2.9.m9.1.1.3.3.cmml">e</mi><mo id="S3.SS2.p2.9.m9.1.1.3.1a" xref="S3.SS2.p2.9.m9.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.9.m9.1.1.3.4" xref="S3.SS2.p2.9.m9.1.1.3.4.cmml">q</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.9.m9.1b"><apply id="S3.SS2.p2.9.m9.1.1.cmml" xref="S3.SS2.p2.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.9.m9.1.1.1.cmml" xref="S3.SS2.p2.9.m9.1.1">subscript</csymbol><ci id="S3.SS2.p2.9.m9.1.1.2.cmml" xref="S3.SS2.p2.9.m9.1.1.2">𝑉</ci><apply id="S3.SS2.p2.9.m9.1.1.3.cmml" xref="S3.SS2.p2.9.m9.1.1.3"><times id="S3.SS2.p2.9.m9.1.1.3.1.cmml" xref="S3.SS2.p2.9.m9.1.1.3.1"></times><ci id="S3.SS2.p2.9.m9.1.1.3.2.cmml" xref="S3.SS2.p2.9.m9.1.1.3.2">𝑠</ci><ci id="S3.SS2.p2.9.m9.1.1.3.3.cmml" xref="S3.SS2.p2.9.m9.1.1.3.3">𝑒</ci><ci id="S3.SS2.p2.9.m9.1.1.3.4.cmml" xref="S3.SS2.p2.9.m9.1.1.3.4">𝑞</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.9.m9.1c">\mathit{V}_{seq}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.9.m9.1d">italic_V start_POSTSUBSCRIPT italic_s italic_e italic_q end_POSTSUBSCRIPT</annotation></semantics></math> is a concatenation of frames <math alttext="\mathit{f}_{n}\in\mathbb{R}^{135}" class="ltx_Math" display="inline" id="S3.SS2.p2.10.m10.1"><semantics id="S3.SS2.p2.10.m10.1a"><mrow id="S3.SS2.p2.10.m10.1.1" xref="S3.SS2.p2.10.m10.1.1.cmml"><msub id="S3.SS2.p2.10.m10.1.1.2" xref="S3.SS2.p2.10.m10.1.1.2.cmml"><mi id="S3.SS2.p2.10.m10.1.1.2.2" xref="S3.SS2.p2.10.m10.1.1.2.2.cmml">f</mi><mi id="S3.SS2.p2.10.m10.1.1.2.3" xref="S3.SS2.p2.10.m10.1.1.2.3.cmml">n</mi></msub><mo id="S3.SS2.p2.10.m10.1.1.1" xref="S3.SS2.p2.10.m10.1.1.1.cmml">∈</mo><msup id="S3.SS2.p2.10.m10.1.1.3" xref="S3.SS2.p2.10.m10.1.1.3.cmml"><mi id="S3.SS2.p2.10.m10.1.1.3.2" xref="S3.SS2.p2.10.m10.1.1.3.2.cmml">ℝ</mi><mn id="S3.SS2.p2.10.m10.1.1.3.3" xref="S3.SS2.p2.10.m10.1.1.3.3.cmml">135</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.10.m10.1b"><apply id="S3.SS2.p2.10.m10.1.1.cmml" xref="S3.SS2.p2.10.m10.1.1"><in id="S3.SS2.p2.10.m10.1.1.1.cmml" xref="S3.SS2.p2.10.m10.1.1.1"></in><apply id="S3.SS2.p2.10.m10.1.1.2.cmml" xref="S3.SS2.p2.10.m10.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.10.m10.1.1.2.1.cmml" xref="S3.SS2.p2.10.m10.1.1.2">subscript</csymbol><ci id="S3.SS2.p2.10.m10.1.1.2.2.cmml" xref="S3.SS2.p2.10.m10.1.1.2.2">𝑓</ci><ci id="S3.SS2.p2.10.m10.1.1.2.3.cmml" xref="S3.SS2.p2.10.m10.1.1.2.3">𝑛</ci></apply><apply id="S3.SS2.p2.10.m10.1.1.3.cmml" xref="S3.SS2.p2.10.m10.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.10.m10.1.1.3.1.cmml" xref="S3.SS2.p2.10.m10.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.10.m10.1.1.3.2.cmml" xref="S3.SS2.p2.10.m10.1.1.3.2">ℝ</ci><cn id="S3.SS2.p2.10.m10.1.1.3.3.cmml" type="integer" xref="S3.SS2.p2.10.m10.1.1.3.3">135</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.10.m10.1c">\mathit{f}_{n}\in\mathbb{R}^{135}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.10.m10.1d">italic_f start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT 135 end_POSTSUPERSCRIPT</annotation></semantics></math>.</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="f_{n}=[Ph^{3D}_{L},Ph^{3D}_{R},Po_{2D},Po_{l}]" class="ltx_Math" display="block" id="S3.E1.m1.4"><semantics id="S3.E1.m1.4a"><mrow id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml"><msub id="S3.E1.m1.4.4.6" xref="S3.E1.m1.4.4.6.cmml"><mi id="S3.E1.m1.4.4.6.2" xref="S3.E1.m1.4.4.6.2.cmml">f</mi><mi id="S3.E1.m1.4.4.6.3" xref="S3.E1.m1.4.4.6.3.cmml">n</mi></msub><mo id="S3.E1.m1.4.4.5" xref="S3.E1.m1.4.4.5.cmml">=</mo><mrow id="S3.E1.m1.4.4.4.4" xref="S3.E1.m1.4.4.4.5.cmml"><mo id="S3.E1.m1.4.4.4.4.5" stretchy="false" xref="S3.E1.m1.4.4.4.5.cmml">[</mo><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.2.cmml">P</mi><mo id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml">⁢</mo><msubsup id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.3.2.2" xref="S3.E1.m1.1.1.1.1.1.3.2.2.cmml">h</mi><mi id="S3.E1.m1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.3.3.cmml">L</mi><mrow id="S3.E1.m1.1.1.1.1.1.3.2.3" xref="S3.E1.m1.1.1.1.1.1.3.2.3.cmml"><mn id="S3.E1.m1.1.1.1.1.1.3.2.3.2" xref="S3.E1.m1.1.1.1.1.1.3.2.3.2.cmml">3</mn><mo id="S3.E1.m1.1.1.1.1.1.3.2.3.1" xref="S3.E1.m1.1.1.1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.1.3.2.3.3" xref="S3.E1.m1.1.1.1.1.1.3.2.3.3.cmml">D</mi></mrow></msubsup></mrow><mo id="S3.E1.m1.4.4.4.4.6" xref="S3.E1.m1.4.4.4.5.cmml">,</mo><mrow id="S3.E1.m1.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.cmml"><mi id="S3.E1.m1.2.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.2.cmml">P</mi><mo id="S3.E1.m1.2.2.2.2.2.1" xref="S3.E1.m1.2.2.2.2.2.1.cmml">⁢</mo><msubsup id="S3.E1.m1.2.2.2.2.2.3" xref="S3.E1.m1.2.2.2.2.2.3.cmml"><mi id="S3.E1.m1.2.2.2.2.2.3.2.2" xref="S3.E1.m1.2.2.2.2.2.3.2.2.cmml">h</mi><mi id="S3.E1.m1.2.2.2.2.2.3.3" xref="S3.E1.m1.2.2.2.2.2.3.3.cmml">R</mi><mrow id="S3.E1.m1.2.2.2.2.2.3.2.3" xref="S3.E1.m1.2.2.2.2.2.3.2.3.cmml"><mn id="S3.E1.m1.2.2.2.2.2.3.2.3.2" xref="S3.E1.m1.2.2.2.2.2.3.2.3.2.cmml">3</mn><mo id="S3.E1.m1.2.2.2.2.2.3.2.3.1" xref="S3.E1.m1.2.2.2.2.2.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.2.2.2.2.2.3.2.3.3" xref="S3.E1.m1.2.2.2.2.2.3.2.3.3.cmml">D</mi></mrow></msubsup></mrow><mo id="S3.E1.m1.4.4.4.4.7" xref="S3.E1.m1.4.4.4.5.cmml">,</mo><mrow id="S3.E1.m1.3.3.3.3.3" xref="S3.E1.m1.3.3.3.3.3.cmml"><mi id="S3.E1.m1.3.3.3.3.3.2" xref="S3.E1.m1.3.3.3.3.3.2.cmml">P</mi><mo id="S3.E1.m1.3.3.3.3.3.1" xref="S3.E1.m1.3.3.3.3.3.1.cmml">⁢</mo><msub id="S3.E1.m1.3.3.3.3.3.3" xref="S3.E1.m1.3.3.3.3.3.3.cmml"><mi id="S3.E1.m1.3.3.3.3.3.3.2" xref="S3.E1.m1.3.3.3.3.3.3.2.cmml">o</mi><mrow id="S3.E1.m1.3.3.3.3.3.3.3" xref="S3.E1.m1.3.3.3.3.3.3.3.cmml"><mn id="S3.E1.m1.3.3.3.3.3.3.3.2" xref="S3.E1.m1.3.3.3.3.3.3.3.2.cmml">2</mn><mo id="S3.E1.m1.3.3.3.3.3.3.3.1" xref="S3.E1.m1.3.3.3.3.3.3.3.1.cmml">⁢</mo><mi id="S3.E1.m1.3.3.3.3.3.3.3.3" xref="S3.E1.m1.3.3.3.3.3.3.3.3.cmml">D</mi></mrow></msub></mrow><mo id="S3.E1.m1.4.4.4.4.8" xref="S3.E1.m1.4.4.4.5.cmml">,</mo><mrow id="S3.E1.m1.4.4.4.4.4" xref="S3.E1.m1.4.4.4.4.4.cmml"><mi id="S3.E1.m1.4.4.4.4.4.2" xref="S3.E1.m1.4.4.4.4.4.2.cmml">P</mi><mo id="S3.E1.m1.4.4.4.4.4.1" xref="S3.E1.m1.4.4.4.4.4.1.cmml">⁢</mo><msub id="S3.E1.m1.4.4.4.4.4.3" xref="S3.E1.m1.4.4.4.4.4.3.cmml"><mi id="S3.E1.m1.4.4.4.4.4.3.2" xref="S3.E1.m1.4.4.4.4.4.3.2.cmml">o</mi><mi id="S3.E1.m1.4.4.4.4.4.3.3" xref="S3.E1.m1.4.4.4.4.4.3.3.cmml">l</mi></msub></mrow><mo id="S3.E1.m1.4.4.4.4.9" stretchy="false" xref="S3.E1.m1.4.4.4.5.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.4b"><apply id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4"><eq id="S3.E1.m1.4.4.5.cmml" xref="S3.E1.m1.4.4.5"></eq><apply id="S3.E1.m1.4.4.6.cmml" xref="S3.E1.m1.4.4.6"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.6.1.cmml" xref="S3.E1.m1.4.4.6">subscript</csymbol><ci id="S3.E1.m1.4.4.6.2.cmml" xref="S3.E1.m1.4.4.6.2">𝑓</ci><ci id="S3.E1.m1.4.4.6.3.cmml" xref="S3.E1.m1.4.4.6.3">𝑛</ci></apply><list id="S3.E1.m1.4.4.4.5.cmml" xref="S3.E1.m1.4.4.4.4"><apply id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"></times><ci id="S3.E1.m1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.2">𝑃</ci><apply id="S3.E1.m1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.3">subscript</csymbol><apply id="S3.E1.m1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.3.2.2">ℎ</ci><apply id="S3.E1.m1.1.1.1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.3.2.3"><times id="S3.E1.m1.1.1.1.1.1.3.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.3.2.3.1"></times><cn id="S3.E1.m1.1.1.1.1.1.3.2.3.2.cmml" type="integer" xref="S3.E1.m1.1.1.1.1.1.3.2.3.2">3</cn><ci id="S3.E1.m1.1.1.1.1.1.3.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.3.2.3.3">𝐷</ci></apply></apply><ci id="S3.E1.m1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.3.3">𝐿</ci></apply></apply><apply id="S3.E1.m1.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2"><times id="S3.E1.m1.2.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.2.2.1"></times><ci id="S3.E1.m1.2.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2">𝑃</ci><apply id="S3.E1.m1.2.2.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.2.3.1.cmml" xref="S3.E1.m1.2.2.2.2.2.3">subscript</csymbol><apply id="S3.E1.m1.2.2.2.2.2.3.2.cmml" xref="S3.E1.m1.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.2.3.2.1.cmml" xref="S3.E1.m1.2.2.2.2.2.3">superscript</csymbol><ci id="S3.E1.m1.2.2.2.2.2.3.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.3.2.2">ℎ</ci><apply id="S3.E1.m1.2.2.2.2.2.3.2.3.cmml" xref="S3.E1.m1.2.2.2.2.2.3.2.3"><times id="S3.E1.m1.2.2.2.2.2.3.2.3.1.cmml" xref="S3.E1.m1.2.2.2.2.2.3.2.3.1"></times><cn id="S3.E1.m1.2.2.2.2.2.3.2.3.2.cmml" type="integer" xref="S3.E1.m1.2.2.2.2.2.3.2.3.2">3</cn><ci id="S3.E1.m1.2.2.2.2.2.3.2.3.3.cmml" xref="S3.E1.m1.2.2.2.2.2.3.2.3.3">𝐷</ci></apply></apply><ci id="S3.E1.m1.2.2.2.2.2.3.3.cmml" xref="S3.E1.m1.2.2.2.2.2.3.3">𝑅</ci></apply></apply><apply id="S3.E1.m1.3.3.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3.3"><times id="S3.E1.m1.3.3.3.3.3.1.cmml" xref="S3.E1.m1.3.3.3.3.3.1"></times><ci id="S3.E1.m1.3.3.3.3.3.2.cmml" xref="S3.E1.m1.3.3.3.3.3.2">𝑃</ci><apply id="S3.E1.m1.3.3.3.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.3.3.3.3.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3">subscript</csymbol><ci id="S3.E1.m1.3.3.3.3.3.3.2.cmml" xref="S3.E1.m1.3.3.3.3.3.3.2">𝑜</ci><apply id="S3.E1.m1.3.3.3.3.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3.3.3.3"><times id="S3.E1.m1.3.3.3.3.3.3.3.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3.3.1"></times><cn id="S3.E1.m1.3.3.3.3.3.3.3.2.cmml" type="integer" xref="S3.E1.m1.3.3.3.3.3.3.3.2">2</cn><ci id="S3.E1.m1.3.3.3.3.3.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3.3.3.3.3">𝐷</ci></apply></apply></apply><apply id="S3.E1.m1.4.4.4.4.4.cmml" xref="S3.E1.m1.4.4.4.4.4"><times id="S3.E1.m1.4.4.4.4.4.1.cmml" xref="S3.E1.m1.4.4.4.4.4.1"></times><ci id="S3.E1.m1.4.4.4.4.4.2.cmml" xref="S3.E1.m1.4.4.4.4.4.2">𝑃</ci><apply id="S3.E1.m1.4.4.4.4.4.3.cmml" xref="S3.E1.m1.4.4.4.4.4.3"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.4.4.4.3.1.cmml" xref="S3.E1.m1.4.4.4.4.4.3">subscript</csymbol><ci id="S3.E1.m1.4.4.4.4.4.3.2.cmml" xref="S3.E1.m1.4.4.4.4.4.3.2">𝑜</ci><ci id="S3.E1.m1.4.4.4.4.4.3.3.cmml" xref="S3.E1.m1.4.4.4.4.4.3.3">𝑙</ci></apply></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.4c">f_{n}=[Ph^{3D}_{L},Ph^{3D}_{R},Po_{2D},Po_{l}]</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.4d">italic_f start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = [ italic_P italic_h start_POSTSUPERSCRIPT 3 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT , italic_P italic_h start_POSTSUPERSCRIPT 3 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT , italic_P italic_o start_POSTSUBSCRIPT 2 italic_D end_POSTSUBSCRIPT , italic_P italic_o start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="V_{seq}=[f_{1},f_{2}..f_{n}],\;n\in[1..N]" class="ltx_math_unparsed" display="block" id="S3.E2.m1.1"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1b"><msub id="S3.E2.m1.1.1"><mi id="S3.E2.m1.1.1.2">V</mi><mrow id="S3.E2.m1.1.1.3"><mi id="S3.E2.m1.1.1.3.2">s</mi><mo id="S3.E2.m1.1.1.3.1">⁢</mo><mi id="S3.E2.m1.1.1.3.3">e</mi><mo id="S3.E2.m1.1.1.3.1a">⁢</mo><mi id="S3.E2.m1.1.1.3.4">q</mi></mrow></msub><mo id="S3.E2.m1.1.2">=</mo><mrow id="S3.E2.m1.1.3"><mo id="S3.E2.m1.1.3.1" stretchy="false">[</mo><msub id="S3.E2.m1.1.3.2"><mi id="S3.E2.m1.1.3.2.2">f</mi><mn id="S3.E2.m1.1.3.2.3">1</mn></msub><mo id="S3.E2.m1.1.3.3">,</mo><msub id="S3.E2.m1.1.3.4"><mi id="S3.E2.m1.1.3.4.2">f</mi><mn id="S3.E2.m1.1.3.4.3">2</mn></msub><mo id="S3.E2.m1.1.3.5" lspace="0em" rspace="0.0835em">.</mo><mo id="S3.E2.m1.1.3.6" lspace="0.0835em" rspace="0.167em">.</mo><msub id="S3.E2.m1.1.3.7"><mi id="S3.E2.m1.1.3.7.2">f</mi><mi id="S3.E2.m1.1.3.7.3">n</mi></msub><mo id="S3.E2.m1.1.3.8" stretchy="false">]</mo></mrow><mo id="S3.E2.m1.1.4" rspace="0.447em">,</mo><mi id="S3.E2.m1.1.5">n</mi><mo id="S3.E2.m1.1.6">∈</mo><mrow id="S3.E2.m1.1.7"><mo id="S3.E2.m1.1.7.1" stretchy="false">[</mo><mn id="S3.E2.m1.1.7.2">1</mn><mo id="S3.E2.m1.1.7.3" lspace="0em" rspace="0.0835em">.</mo><mo id="S3.E2.m1.1.7.4" lspace="0.0835em" rspace="0.167em">.</mo><mi id="S3.E2.m1.1.7.5">N</mi><mo id="S3.E2.m1.1.7.6" stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.E2.m1.1c">V_{seq}=[f_{1},f_{2}..f_{n}],\;n\in[1..N]</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.1d">italic_V start_POSTSUBSCRIPT italic_s italic_e italic_q end_POSTSUBSCRIPT = [ italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT . . italic_f start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ] , italic_n ∈ [ 1 . . italic_N ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.4">The sequence vector representing an action <math alttext="\mathit{V}_{seq}" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.1"><semantics id="S3.SS2.p3.1.m1.1a"><msub id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml">V</mi><mrow id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3.cmml"><mi id="S3.SS2.p3.1.m1.1.1.3.2" xref="S3.SS2.p3.1.m1.1.1.3.2.cmml">s</mi><mo id="S3.SS2.p3.1.m1.1.1.3.1" xref="S3.SS2.p3.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p3.1.m1.1.1.3.3" xref="S3.SS2.p3.1.m1.1.1.3.3.cmml">e</mi><mo id="S3.SS2.p3.1.m1.1.1.3.1a" xref="S3.SS2.p3.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p3.1.m1.1.1.3.4" xref="S3.SS2.p3.1.m1.1.1.3.4.cmml">q</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2">𝑉</ci><apply id="S3.SS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3"><times id="S3.SS2.p3.1.m1.1.1.3.1.cmml" xref="S3.SS2.p3.1.m1.1.1.3.1"></times><ci id="S3.SS2.p3.1.m1.1.1.3.2.cmml" xref="S3.SS2.p3.1.m1.1.1.3.2">𝑠</ci><ci id="S3.SS2.p3.1.m1.1.1.3.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3.3">𝑒</ci><ci id="S3.SS2.p3.1.m1.1.1.3.4.cmml" xref="S3.SS2.p3.1.m1.1.1.3.4">𝑞</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">\mathit{V}_{seq}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.1d">italic_V start_POSTSUBSCRIPT italic_s italic_e italic_q end_POSTSUBSCRIPT</annotation></semantics></math> is processed to embed temporal information with a transformer encoder block following <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib18" title="">18</a>]</cite>. First, <math alttext="\mathit{V}_{seq}" class="ltx_Math" display="inline" id="S3.SS2.p3.2.m2.1"><semantics id="S3.SS2.p3.2.m2.1a"><msub id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml"><mi id="S3.SS2.p3.2.m2.1.1.2" xref="S3.SS2.p3.2.m2.1.1.2.cmml">V</mi><mrow id="S3.SS2.p3.2.m2.1.1.3" xref="S3.SS2.p3.2.m2.1.1.3.cmml"><mi id="S3.SS2.p3.2.m2.1.1.3.2" xref="S3.SS2.p3.2.m2.1.1.3.2.cmml">s</mi><mo id="S3.SS2.p3.2.m2.1.1.3.1" xref="S3.SS2.p3.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p3.2.m2.1.1.3.3" xref="S3.SS2.p3.2.m2.1.1.3.3.cmml">e</mi><mo id="S3.SS2.p3.2.m2.1.1.3.1a" xref="S3.SS2.p3.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p3.2.m2.1.1.3.4" xref="S3.SS2.p3.2.m2.1.1.3.4.cmml">q</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2">𝑉</ci><apply id="S3.SS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3"><times id="S3.SS2.p3.2.m2.1.1.3.1.cmml" xref="S3.SS2.p3.2.m2.1.1.3.1"></times><ci id="S3.SS2.p3.2.m2.1.1.3.2.cmml" xref="S3.SS2.p3.2.m2.1.1.3.2">𝑠</ci><ci id="S3.SS2.p3.2.m2.1.1.3.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3.3">𝑒</ci><ci id="S3.SS2.p3.2.m2.1.1.3.4.cmml" xref="S3.SS2.p3.2.m2.1.1.3.4">𝑞</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">\mathit{V}_{seq}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.2.m2.1d">italic_V start_POSTSUBSCRIPT italic_s italic_e italic_q end_POSTSUBSCRIPT</annotation></semantics></math> is linearised using a fully connected layer to <math alttext="\mathit{x}_{lin}" class="ltx_Math" display="inline" id="S3.SS2.p3.3.m3.1"><semantics id="S3.SS2.p3.3.m3.1a"><msub id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml"><mi id="S3.SS2.p3.3.m3.1.1.2" xref="S3.SS2.p3.3.m3.1.1.2.cmml">x</mi><mrow id="S3.SS2.p3.3.m3.1.1.3" xref="S3.SS2.p3.3.m3.1.1.3.cmml"><mi id="S3.SS2.p3.3.m3.1.1.3.2" xref="S3.SS2.p3.3.m3.1.1.3.2.cmml">l</mi><mo id="S3.SS2.p3.3.m3.1.1.3.1" xref="S3.SS2.p3.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p3.3.m3.1.1.3.3" xref="S3.SS2.p3.3.m3.1.1.3.3.cmml">i</mi><mo id="S3.SS2.p3.3.m3.1.1.3.1a" xref="S3.SS2.p3.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p3.3.m3.1.1.3.4" xref="S3.SS2.p3.3.m3.1.1.3.4.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><apply id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m3.1.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p3.3.m3.1.1.2.cmml" xref="S3.SS2.p3.3.m3.1.1.2">𝑥</ci><apply id="S3.SS2.p3.3.m3.1.1.3.cmml" xref="S3.SS2.p3.3.m3.1.1.3"><times id="S3.SS2.p3.3.m3.1.1.3.1.cmml" xref="S3.SS2.p3.3.m3.1.1.3.1"></times><ci id="S3.SS2.p3.3.m3.1.1.3.2.cmml" xref="S3.SS2.p3.3.m3.1.1.3.2">𝑙</ci><ci id="S3.SS2.p3.3.m3.1.1.3.3.cmml" xref="S3.SS2.p3.3.m3.1.1.3.3">𝑖</ci><ci id="S3.SS2.p3.3.m3.1.1.3.4.cmml" xref="S3.SS2.p3.3.m3.1.1.3.4">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">\mathit{x}_{lin}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.3.m3.1d">italic_x start_POSTSUBSCRIPT italic_l italic_i italic_n end_POSTSUBSCRIPT</annotation></semantics></math>. The resulting <math alttext="\mathit{x}_{lin}" class="ltx_Math" display="inline" id="S3.SS2.p3.4.m4.1"><semantics id="S3.SS2.p3.4.m4.1a"><msub id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml"><mi id="S3.SS2.p3.4.m4.1.1.2" xref="S3.SS2.p3.4.m4.1.1.2.cmml">x</mi><mrow id="S3.SS2.p3.4.m4.1.1.3" xref="S3.SS2.p3.4.m4.1.1.3.cmml"><mi id="S3.SS2.p3.4.m4.1.1.3.2" xref="S3.SS2.p3.4.m4.1.1.3.2.cmml">l</mi><mo id="S3.SS2.p3.4.m4.1.1.3.1" xref="S3.SS2.p3.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p3.4.m4.1.1.3.3" xref="S3.SS2.p3.4.m4.1.1.3.3.cmml">i</mi><mo id="S3.SS2.p3.4.m4.1.1.3.1a" xref="S3.SS2.p3.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p3.4.m4.1.1.3.4" xref="S3.SS2.p3.4.m4.1.1.3.4.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b"><apply id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.4.m4.1.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p3.4.m4.1.1.2.cmml" xref="S3.SS2.p3.4.m4.1.1.2">𝑥</ci><apply id="S3.SS2.p3.4.m4.1.1.3.cmml" xref="S3.SS2.p3.4.m4.1.1.3"><times id="S3.SS2.p3.4.m4.1.1.3.1.cmml" xref="S3.SS2.p3.4.m4.1.1.3.1"></times><ci id="S3.SS2.p3.4.m4.1.1.3.2.cmml" xref="S3.SS2.p3.4.m4.1.1.3.2">𝑙</ci><ci id="S3.SS2.p3.4.m4.1.1.3.3.cmml" xref="S3.SS2.p3.4.m4.1.1.3.3">𝑖</ci><ci id="S3.SS2.p3.4.m4.1.1.3.4.cmml" xref="S3.SS2.p3.4.m4.1.1.3.4">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">\mathit{x}_{lin}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.4.m4.1d">italic_x start_POSTSUBSCRIPT italic_l italic_i italic_n end_POSTSUBSCRIPT</annotation></semantics></math> is combined with a classification token and a positional embedding.
The embedded sequence is passed to MLP for classifying the action.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">In this evaluation, we focus exclusively on the <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.1">H2O Dataset</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib16" title="">16</a>]</cite> due to its suitability for our research objectives. This dataset captures human actions from an egocentric perspective, providing labels for action recognition and 3D hand pose of both hands. At the time of this study, there are only two other publicly available datasets with similar characteristics required for our study, such as <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.2">AssemblyHands</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib22" title="">22</a>]</cite> and <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.3">HoloAssist</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib29" title="">29</a>]</cite>. While <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.4">HoloAssist</span> is potentially valuable, the hand pose labels have not yet been released. <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.5">AssemblyHands</span> is excluded due to images captured by infrared cameras, which are incompatible with the <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.6">DPT-Hybrid</span> depth estimation model designed for RGB input.</p>
</div>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_font_bold ltx_font_italic ltx_title_paragraph">H2O Dataset</h5>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1">is a comprehensive resource for analysing hand-based actions and object interactions involving two hands. It includes multi-view RGB-D images annotated with action labels covering 36 different classes derived from verb and object labels. It also includes 3D poses for both hands, resulting in <math alttext="j=2\times 21" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.1.m1.1"><semantics id="S4.SS1.SSS0.Px1.p1.1.m1.1a"><mrow id="S4.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml">j</mi><mo id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml">=</mo><mrow id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml"><mn id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.2" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.2.cmml">2</mn><mo id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.1.cmml">×</mo><mn id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.3" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.3.cmml">21</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.1.m1.1b"><apply id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1"><eq id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1"></eq><ci id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2">𝑗</ci><apply id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3"><times id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.1"></times><cn id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.2.cmml" type="integer" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.2">2</cn><cn id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.3.cmml" type="integer" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.3">21</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.1.m1.1c">j=2\times 21</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px1.p1.1.m1.1d">italic_j = 2 × 21</annotation></semantics></math> points, and 6D poses and meshes for the manipulated objects. Ground truth camera poses and scene point clouds further enrich the dataset. The actions captured in the dataset were performed by four people. For both the action recognition and hand pose estimation tasks, the dataset provides training, validation and test subsets. The action recognition subset contains 569 clips for training, 122 for validation and 242 for testing.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Metrics</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">To evaluate the hand pose estimation and compare our work with the state of the art, we calculate the Mean Per Joint Position Error (MPJPE) in millimetres over 21 keypoints <math alttext="J" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">J</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">𝐽</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">J</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">italic_J</annotation></semantics></math> representing the human hand. This error metric quantifies the Euclidean distance between the predicted and ground truth values. For action recognition, we use the top-1 accuracy measure, where the model’s prediction must exactly match the expected ground truth to be considered accurate.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Experiment setup</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">For both learning processes, each run is repeated three times to reduce the effect of random initialisation of the network, and mean results with standard deviations are reported.</p>
</div>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px1">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">3D Hand Pose Estimation</h5>
<div class="ltx_para" id="S4.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS3.SSS0.Px1.p1.8">is trained and evaluated on <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS0.Px1.p1.8.1">H2O Dataset</span>. The optimisation is done using Stochastic Gradient Descent (SGD) over the summarised loss function including Intersection over Union (IoU) for each upsampler and <math alttext="L1" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p1.1.m1.1"><semantics id="S4.SS3.SSS0.Px1.p1.1.m1.1a"><mrow id="S4.SS3.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.2" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.2.cmml">L</mi><mo id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.1" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.1.cmml">⁢</mo><mn id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px1.p1.1.m1.1b"><apply id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1"><times id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.1"></times><ci id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.2">𝐿</ci><cn id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px1.p1.1.m1.1c">L1</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS0.Px1.p1.1.m1.1d">italic_L 1</annotation></semantics></math> loss for predicted corresponding depth values. The process starts with a learning rate <math alttext="l_{r}=0.1" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p1.2.m2.1"><semantics id="S4.SS3.SSS0.Px1.p1.2.m2.1a"><mrow id="S4.SS3.SSS0.Px1.p1.2.m2.1.1" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.cmml"><msub id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.2" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.2.cmml"><mi id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.2.2" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.2.2.cmml">l</mi><mi id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.2.3" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.2.3.cmml">r</mi></msub><mo id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.1" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.3" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px1.p1.2.m2.1b"><apply id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1"><eq id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.1"></eq><apply id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.2.1.cmml" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.2">subscript</csymbol><ci id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.2.2.cmml" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.2.2">𝑙</ci><ci id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.2.3.cmml" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.2.3">𝑟</ci></apply><cn id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.3.cmml" type="float" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px1.p1.2.m2.1c">l_{r}=0.1</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS0.Px1.p1.2.m2.1d">italic_l start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT = 0.1</annotation></semantics></math> and momentum equal to <math alttext="m=0.9" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p1.3.m3.1"><semantics id="S4.SS3.SSS0.Px1.p1.3.m3.1a"><mrow id="S4.SS3.SSS0.Px1.p1.3.m3.1.1" xref="S4.SS3.SSS0.Px1.p1.3.m3.1.1.cmml"><mi id="S4.SS3.SSS0.Px1.p1.3.m3.1.1.2" xref="S4.SS3.SSS0.Px1.p1.3.m3.1.1.2.cmml">m</mi><mo id="S4.SS3.SSS0.Px1.p1.3.m3.1.1.1" xref="S4.SS3.SSS0.Px1.p1.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS3.SSS0.Px1.p1.3.m3.1.1.3" xref="S4.SS3.SSS0.Px1.p1.3.m3.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px1.p1.3.m3.1b"><apply id="S4.SS3.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.3.m3.1.1"><eq id="S4.SS3.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.3.m3.1.1.1"></eq><ci id="S4.SS3.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S4.SS3.SSS0.Px1.p1.3.m3.1.1.2">𝑚</ci><cn id="S4.SS3.SSS0.Px1.p1.3.m3.1.1.3.cmml" type="float" xref="S4.SS3.SSS0.Px1.p1.3.m3.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px1.p1.3.m3.1c">m=0.9</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS0.Px1.p1.3.m3.1d">italic_m = 0.9</annotation></semantics></math>. Over time <math alttext="l_{r}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p1.4.m4.1"><semantics id="S4.SS3.SSS0.Px1.p1.4.m4.1a"><msub id="S4.SS3.SSS0.Px1.p1.4.m4.1.1" xref="S4.SS3.SSS0.Px1.p1.4.m4.1.1.cmml"><mi id="S4.SS3.SSS0.Px1.p1.4.m4.1.1.2" xref="S4.SS3.SSS0.Px1.p1.4.m4.1.1.2.cmml">l</mi><mi id="S4.SS3.SSS0.Px1.p1.4.m4.1.1.3" xref="S4.SS3.SSS0.Px1.p1.4.m4.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px1.p1.4.m4.1b"><apply id="S4.SS3.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS0.Px1.p1.4.m4.1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.4.m4.1.1">subscript</csymbol><ci id="S4.SS3.SSS0.Px1.p1.4.m4.1.1.2.cmml" xref="S4.SS3.SSS0.Px1.p1.4.m4.1.1.2">𝑙</ci><ci id="S4.SS3.SSS0.Px1.p1.4.m4.1.1.3.cmml" xref="S4.SS3.SSS0.Px1.p1.4.m4.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px1.p1.4.m4.1c">l_{r}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS0.Px1.p1.4.m4.1d">italic_l start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math> is reduced by <math alttext="\alpha=0.5" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p1.5.m5.1"><semantics id="S4.SS3.SSS0.Px1.p1.5.m5.1a"><mrow id="S4.SS3.SSS0.Px1.p1.5.m5.1.1" xref="S4.SS3.SSS0.Px1.p1.5.m5.1.1.cmml"><mi id="S4.SS3.SSS0.Px1.p1.5.m5.1.1.2" xref="S4.SS3.SSS0.Px1.p1.5.m5.1.1.2.cmml">α</mi><mo id="S4.SS3.SSS0.Px1.p1.5.m5.1.1.1" xref="S4.SS3.SSS0.Px1.p1.5.m5.1.1.1.cmml">=</mo><mn id="S4.SS3.SSS0.Px1.p1.5.m5.1.1.3" xref="S4.SS3.SSS0.Px1.p1.5.m5.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px1.p1.5.m5.1b"><apply id="S4.SS3.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.5.m5.1.1"><eq id="S4.SS3.SSS0.Px1.p1.5.m5.1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.5.m5.1.1.1"></eq><ci id="S4.SS3.SSS0.Px1.p1.5.m5.1.1.2.cmml" xref="S4.SS3.SSS0.Px1.p1.5.m5.1.1.2">𝛼</ci><cn id="S4.SS3.SSS0.Px1.p1.5.m5.1.1.3.cmml" type="float" xref="S4.SS3.SSS0.Px1.p1.5.m5.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px1.p1.5.m5.1c">\alpha=0.5</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS0.Px1.p1.5.m5.1d">italic_α = 0.5</annotation></semantics></math> every <math alttext="10^{th}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p1.6.m6.1"><semantics id="S4.SS3.SSS0.Px1.p1.6.m6.1a"><msup id="S4.SS3.SSS0.Px1.p1.6.m6.1.1" xref="S4.SS3.SSS0.Px1.p1.6.m6.1.1.cmml"><mn id="S4.SS3.SSS0.Px1.p1.6.m6.1.1.2" xref="S4.SS3.SSS0.Px1.p1.6.m6.1.1.2.cmml">10</mn><mrow id="S4.SS3.SSS0.Px1.p1.6.m6.1.1.3" xref="S4.SS3.SSS0.Px1.p1.6.m6.1.1.3.cmml"><mi id="S4.SS3.SSS0.Px1.p1.6.m6.1.1.3.2" xref="S4.SS3.SSS0.Px1.p1.6.m6.1.1.3.2.cmml">t</mi><mo id="S4.SS3.SSS0.Px1.p1.6.m6.1.1.3.1" xref="S4.SS3.SSS0.Px1.p1.6.m6.1.1.3.1.cmml">⁢</mo><mi id="S4.SS3.SSS0.Px1.p1.6.m6.1.1.3.3" xref="S4.SS3.SSS0.Px1.p1.6.m6.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px1.p1.6.m6.1b"><apply id="S4.SS3.SSS0.Px1.p1.6.m6.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS0.Px1.p1.6.m6.1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.6.m6.1.1">superscript</csymbol><cn id="S4.SS3.SSS0.Px1.p1.6.m6.1.1.2.cmml" type="integer" xref="S4.SS3.SSS0.Px1.p1.6.m6.1.1.2">10</cn><apply id="S4.SS3.SSS0.Px1.p1.6.m6.1.1.3.cmml" xref="S4.SS3.SSS0.Px1.p1.6.m6.1.1.3"><times id="S4.SS3.SSS0.Px1.p1.6.m6.1.1.3.1.cmml" xref="S4.SS3.SSS0.Px1.p1.6.m6.1.1.3.1"></times><ci id="S4.SS3.SSS0.Px1.p1.6.m6.1.1.3.2.cmml" xref="S4.SS3.SSS0.Px1.p1.6.m6.1.1.3.2">𝑡</ci><ci id="S4.SS3.SSS0.Px1.p1.6.m6.1.1.3.3.cmml" xref="S4.SS3.SSS0.Px1.p1.6.m6.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px1.p1.6.m6.1c">10^{th}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS0.Px1.p1.6.m6.1d">10 start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT</annotation></semantics></math> epoch starting from the <math alttext="50^{th}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p1.7.m7.1"><semantics id="S4.SS3.SSS0.Px1.p1.7.m7.1a"><msup id="S4.SS3.SSS0.Px1.p1.7.m7.1.1" xref="S4.SS3.SSS0.Px1.p1.7.m7.1.1.cmml"><mn id="S4.SS3.SSS0.Px1.p1.7.m7.1.1.2" xref="S4.SS3.SSS0.Px1.p1.7.m7.1.1.2.cmml">50</mn><mrow id="S4.SS3.SSS0.Px1.p1.7.m7.1.1.3" xref="S4.SS3.SSS0.Px1.p1.7.m7.1.1.3.cmml"><mi id="S4.SS3.SSS0.Px1.p1.7.m7.1.1.3.2" xref="S4.SS3.SSS0.Px1.p1.7.m7.1.1.3.2.cmml">t</mi><mo id="S4.SS3.SSS0.Px1.p1.7.m7.1.1.3.1" xref="S4.SS3.SSS0.Px1.p1.7.m7.1.1.3.1.cmml">⁢</mo><mi id="S4.SS3.SSS0.Px1.p1.7.m7.1.1.3.3" xref="S4.SS3.SSS0.Px1.p1.7.m7.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px1.p1.7.m7.1b"><apply id="S4.SS3.SSS0.Px1.p1.7.m7.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS0.Px1.p1.7.m7.1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.7.m7.1.1">superscript</csymbol><cn id="S4.SS3.SSS0.Px1.p1.7.m7.1.1.2.cmml" type="integer" xref="S4.SS3.SSS0.Px1.p1.7.m7.1.1.2">50</cn><apply id="S4.SS3.SSS0.Px1.p1.7.m7.1.1.3.cmml" xref="S4.SS3.SSS0.Px1.p1.7.m7.1.1.3"><times id="S4.SS3.SSS0.Px1.p1.7.m7.1.1.3.1.cmml" xref="S4.SS3.SSS0.Px1.p1.7.m7.1.1.3.1"></times><ci id="S4.SS3.SSS0.Px1.p1.7.m7.1.1.3.2.cmml" xref="S4.SS3.SSS0.Px1.p1.7.m7.1.1.3.2">𝑡</ci><ci id="S4.SS3.SSS0.Px1.p1.7.m7.1.1.3.3.cmml" xref="S4.SS3.SSS0.Px1.p1.7.m7.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px1.p1.7.m7.1c">50^{th}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS0.Px1.p1.7.m7.1d">50 start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT</annotation></semantics></math> epoch. the data is augmented with random cropping, horizontal flipping, vertical flipping, resizing, rotating and blurring. The batch size is equal to <math alttext="b_{s}=32" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p1.8.m8.1"><semantics id="S4.SS3.SSS0.Px1.p1.8.m8.1a"><mrow id="S4.SS3.SSS0.Px1.p1.8.m8.1.1" xref="S4.SS3.SSS0.Px1.p1.8.m8.1.1.cmml"><msub id="S4.SS3.SSS0.Px1.p1.8.m8.1.1.2" xref="S4.SS3.SSS0.Px1.p1.8.m8.1.1.2.cmml"><mi id="S4.SS3.SSS0.Px1.p1.8.m8.1.1.2.2" xref="S4.SS3.SSS0.Px1.p1.8.m8.1.1.2.2.cmml">b</mi><mi id="S4.SS3.SSS0.Px1.p1.8.m8.1.1.2.3" xref="S4.SS3.SSS0.Px1.p1.8.m8.1.1.2.3.cmml">s</mi></msub><mo id="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1" xref="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.cmml">=</mo><mn id="S4.SS3.SSS0.Px1.p1.8.m8.1.1.3" xref="S4.SS3.SSS0.Px1.p1.8.m8.1.1.3.cmml">32</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px1.p1.8.m8.1b"><apply id="S4.SS3.SSS0.Px1.p1.8.m8.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.8.m8.1.1"><eq id="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1"></eq><apply id="S4.SS3.SSS0.Px1.p1.8.m8.1.1.2.cmml" xref="S4.SS3.SSS0.Px1.p1.8.m8.1.1.2"><csymbol cd="ambiguous" id="S4.SS3.SSS0.Px1.p1.8.m8.1.1.2.1.cmml" xref="S4.SS3.SSS0.Px1.p1.8.m8.1.1.2">subscript</csymbol><ci id="S4.SS3.SSS0.Px1.p1.8.m8.1.1.2.2.cmml" xref="S4.SS3.SSS0.Px1.p1.8.m8.1.1.2.2">𝑏</ci><ci id="S4.SS3.SSS0.Px1.p1.8.m8.1.1.2.3.cmml" xref="S4.SS3.SSS0.Px1.p1.8.m8.1.1.2.3">𝑠</ci></apply><cn id="S4.SS3.SSS0.Px1.p1.8.m8.1.1.3.cmml" type="integer" xref="S4.SS3.SSS0.Px1.p1.8.m8.1.1.3">32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px1.p1.8.m8.1c">b_{s}=32</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS0.Px1.p1.8.m8.1d">italic_b start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT = 32</annotation></semantics></math>. Model weights are saved for the smallest MPJPE in the validation subset.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px2">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">Action Recognition</h5>
<div class="ltx_para" id="S4.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS3.SSS0.Px2.p1.3">module requires object detection. For this, we fine-tune YOLOv7 on the <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS0.Px2.p1.3.1">H2O Dataset</span> using the open-source strategy reported by the authors. The training of the action recognition includes the augmentation of the sequence vectors with keypoints using random rotation and an additional strategy with random masking of either the hand, the object positions or the label. This is done by setting the corresponding values of the hand or object in the frame <math alttext="\mathit{f}_{n}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px2.p1.1.m1.1"><semantics id="S4.SS3.SSS0.Px2.p1.1.m1.1a"><msub id="S4.SS3.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS3.SSS0.Px2.p1.1.m1.1.1.cmml"><mi id="S4.SS3.SSS0.Px2.p1.1.m1.1.1.2" xref="S4.SS3.SSS0.Px2.p1.1.m1.1.1.2.cmml">f</mi><mi id="S4.SS3.SSS0.Px2.p1.1.m1.1.1.3" xref="S4.SS3.SSS0.Px2.p1.1.m1.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px2.p1.1.m1.1b"><apply id="S4.SS3.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS0.Px2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S4.SS3.SSS0.Px2.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S4.SS3.SSS0.Px2.p1.1.m1.1.1.2">𝑓</ci><ci id="S4.SS3.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S4.SS3.SSS0.Px2.p1.1.m1.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px2.p1.1.m1.1c">\mathit{f}_{n}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS0.Px2.p1.1.m1.1d">italic_f start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> to zero. We follow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib32" title="">32</a>]</cite> and use given poses in training. Input sequence frames are randomly sub-sampled during training and uniformly sub-sampled for validation and testing. Models are trained with a batch size <math alttext="b_{s}=64" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px2.p1.2.m2.1"><semantics id="S4.SS3.SSS0.Px2.p1.2.m2.1a"><mrow id="S4.SS3.SSS0.Px2.p1.2.m2.1.1" xref="S4.SS3.SSS0.Px2.p1.2.m2.1.1.cmml"><msub id="S4.SS3.SSS0.Px2.p1.2.m2.1.1.2" xref="S4.SS3.SSS0.Px2.p1.2.m2.1.1.2.cmml"><mi id="S4.SS3.SSS0.Px2.p1.2.m2.1.1.2.2" xref="S4.SS3.SSS0.Px2.p1.2.m2.1.1.2.2.cmml">b</mi><mi id="S4.SS3.SSS0.Px2.p1.2.m2.1.1.2.3" xref="S4.SS3.SSS0.Px2.p1.2.m2.1.1.2.3.cmml">s</mi></msub><mo id="S4.SS3.SSS0.Px2.p1.2.m2.1.1.1" xref="S4.SS3.SSS0.Px2.p1.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS3.SSS0.Px2.p1.2.m2.1.1.3" xref="S4.SS3.SSS0.Px2.p1.2.m2.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px2.p1.2.m2.1b"><apply id="S4.SS3.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S4.SS3.SSS0.Px2.p1.2.m2.1.1"><eq id="S4.SS3.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S4.SS3.SSS0.Px2.p1.2.m2.1.1.1"></eq><apply id="S4.SS3.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S4.SS3.SSS0.Px2.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S4.SS3.SSS0.Px2.p1.2.m2.1.1.2.1.cmml" xref="S4.SS3.SSS0.Px2.p1.2.m2.1.1.2">subscript</csymbol><ci id="S4.SS3.SSS0.Px2.p1.2.m2.1.1.2.2.cmml" xref="S4.SS3.SSS0.Px2.p1.2.m2.1.1.2.2">𝑏</ci><ci id="S4.SS3.SSS0.Px2.p1.2.m2.1.1.2.3.cmml" xref="S4.SS3.SSS0.Px2.p1.2.m2.1.1.2.3">𝑠</ci></apply><cn id="S4.SS3.SSS0.Px2.p1.2.m2.1.1.3.cmml" type="integer" xref="S4.SS3.SSS0.Px2.p1.2.m2.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px2.p1.2.m2.1c">b_{s}=64</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS0.Px2.p1.2.m2.1d">italic_b start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT = 64</annotation></semantics></math>, AdamW optimiser, cross-entropy loss function, and a learning rate <math alttext="l_{r}=0.001" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px2.p1.3.m3.1"><semantics id="S4.SS3.SSS0.Px2.p1.3.m3.1a"><mrow id="S4.SS3.SSS0.Px2.p1.3.m3.1.1" xref="S4.SS3.SSS0.Px2.p1.3.m3.1.1.cmml"><msub id="S4.SS3.SSS0.Px2.p1.3.m3.1.1.2" xref="S4.SS3.SSS0.Px2.p1.3.m3.1.1.2.cmml"><mi id="S4.SS3.SSS0.Px2.p1.3.m3.1.1.2.2" xref="S4.SS3.SSS0.Px2.p1.3.m3.1.1.2.2.cmml">l</mi><mi id="S4.SS3.SSS0.Px2.p1.3.m3.1.1.2.3" xref="S4.SS3.SSS0.Px2.p1.3.m3.1.1.2.3.cmml">r</mi></msub><mo id="S4.SS3.SSS0.Px2.p1.3.m3.1.1.1" xref="S4.SS3.SSS0.Px2.p1.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS3.SSS0.Px2.p1.3.m3.1.1.3" xref="S4.SS3.SSS0.Px2.p1.3.m3.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px2.p1.3.m3.1b"><apply id="S4.SS3.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S4.SS3.SSS0.Px2.p1.3.m3.1.1"><eq id="S4.SS3.SSS0.Px2.p1.3.m3.1.1.1.cmml" xref="S4.SS3.SSS0.Px2.p1.3.m3.1.1.1"></eq><apply id="S4.SS3.SSS0.Px2.p1.3.m3.1.1.2.cmml" xref="S4.SS3.SSS0.Px2.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S4.SS3.SSS0.Px2.p1.3.m3.1.1.2.1.cmml" xref="S4.SS3.SSS0.Px2.p1.3.m3.1.1.2">subscript</csymbol><ci id="S4.SS3.SSS0.Px2.p1.3.m3.1.1.2.2.cmml" xref="S4.SS3.SSS0.Px2.p1.3.m3.1.1.2.2">𝑙</ci><ci id="S4.SS3.SSS0.Px2.p1.3.m3.1.1.2.3.cmml" xref="S4.SS3.SSS0.Px2.p1.3.m3.1.1.2.3">𝑟</ci></apply><cn id="S4.SS3.SSS0.Px2.p1.3.m3.1.1.3.cmml" type="float" xref="S4.SS3.SSS0.Px2.p1.3.m3.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px2.p1.3.m3.1c">l_{r}=0.001</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS0.Px2.p1.3.m3.1d">italic_l start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT = 0.001</annotation></semantics></math> reduced by a factor of 0.5 every 200 epochs after 500 epochs. Hyperparameters and augmentations are selected based on the best-performing set in the validation subset. Weights are stored for best validation accuracy.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Comparison with State of the Art</h3>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Results of 3D hand pose estimation provided in <span class="ltx_text ltx_font_italic" id="S4.T1.5.1">mm</span> in camera space.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T1.3.3.4">Method:</th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T1.3.3.5">Year</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1">MPJPE Left <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T1.1.1.1.m1.1"><semantics id="S4.T1.1.1.1.m1.1a"><mo id="S4.T1.1.1.1.m1.1.1" stretchy="false" xref="S4.T1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.1.1.1.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.2.2.2">MPJPE Right <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T1.2.2.2.m1.1"><semantics id="S4.T1.2.2.2.m1.1a"><mo id="S4.T1.2.2.2.m1.1.1" stretchy="false" xref="S4.T1.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.m1.1b"><ci id="S4.T1.2.2.2.m1.1.1.cmml" xref="S4.T1.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.2.2.2.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.3.3.3">MPJPE Both <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T1.3.3.3.m1.1"><semantics id="S4.T1.3.3.3.m1.1a"><mo id="S4.T1.3.3.3.m1.1.1" stretchy="false" xref="S4.T1.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.m1.1b"><ci id="S4.T1.3.3.3.m1.1.1.cmml" xref="S4.T1.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.3.3.3.m1.1d">↓</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.3.4.1.1">LPC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib15" title="">15</a>]</cite>
</th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T1.3.4.1.2">2020</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.4.1.3">39.56</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.4.1.4">41.87</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.4.1.5">40.72</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.3.5.2.1">H+O <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib26" title="">26</a>]</cite>
</th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.5.2.2">2019</th>
<td class="ltx_td ltx_align_center" id="S4.T1.3.5.2.3">41.42</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.5.2.4">38.86</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.5.2.5">40.14</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.3.6.3.1">H2O <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib16" title="">16</a>]</cite>
</th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.6.3.2">2021</th>
<td class="ltx_td ltx_align_center" id="S4.T1.3.6.3.3">41.45</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.6.3.4">37.21</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.6.3.5">39.33</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.3.7.4.1">HTT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib30" title="">30</a>]</cite>
</th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.7.4.2">2023</th>
<td class="ltx_td ltx_align_center" id="S4.T1.3.7.4.3">35.02</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.7.4.4">35.63</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.7.4.5">35.33</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.8.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.3.8.5.1">H2OTR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib5" title="">5</a>]</cite>
</th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.8.5.2">2023</th>
<td class="ltx_td ltx_align_center" id="S4.T1.3.8.5.3">24.40</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.8.5.4">25.80</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.8.5.5"><span class="ltx_text ltx_font_bold" id="S4.T1.3.8.5.5.1">25.10</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.9.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.3.9.6.1">THOR-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib1" title="">1</a>]</cite>
</th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.9.6.2">2023</th>
<td class="ltx_td ltx_align_center" id="S4.T1.3.9.6.3">36.80</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.9.6.4">36.50</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.9.6.5">36.65</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.10.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T1.3.10.7.1"><span class="ltx_text ltx_font_bold" id="S4.T1.3.10.7.1.1">Ours</span></th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S4.T1.3.10.7.2">Now</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.3.10.7.3">30.31</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.3.10.7.4">27.02</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.3.10.7.5">28.66</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="217" id="S4.F4.g1" src="extracted/5800341/images/icpr_qualitative.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>
Qualitative results of our method in 2D and 3D space. Green skeletons represent the <span class="ltx_text" id="S4.F4.6.1" style="color:#00E000;">ground truth hand pose</span>, red estimations <span class="ltx_text ltx_font_bold" id="S4.F4.7.2" style="color:#FF0000;">without<span class="ltx_text ltx_font_medium" id="S4.F4.7.2.1"> <span class="ltx_text ltx_font_italic" id="S4.F4.7.2.1.1">SHARP</span></span></span> and blue estimations <span class="ltx_text ltx_font_bold" id="S4.F4.8.3" style="color:#0000FF;">with<span class="ltx_text ltx_font_medium" id="S4.F4.8.3.1"> <span class="ltx_text ltx_font_italic" id="S4.F4.8.3.1.1">SHARP</span></span></span>. Images are annotated with a predicted action label for the represented sequences. Two examples from the left show that <span class="ltx_text ltx_font_italic" id="S4.F4.9.4">SHARP</span> improves 3D pose estimation. On the right, the 3D error increases as <span class="ltx_text ltx_font_italic" id="S4.F4.10.5">SHARP</span> partially loses the right hand.
</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="306" id="S4.F5.g1" src="extracted/5800341/images/inference_time.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Inference time for 3D hand pose estimation per single frame and action recognition accuracy per single action of state-of-the-art methods on <span class="ltx_text ltx_font_italic" id="S4.F5.5.1">H2O Dataset</span>. Each method is visualised as a circle whose size represents the number of trainable parameters. <span class="ltx_text ltx_font_italic" id="S4.F5.6.2">SHARP</span> inference is <math alttext="\approx\times 2.5" class="ltx_math_unparsed" display="inline" id="S4.F5.2.m1.2"><semantics id="S4.F5.2.m1.2b"><mrow id="S4.F5.2.m1.2c"><mo id="S4.F5.2.m1.1.1" rspace="0em">≈</mo><mo id="S4.F5.2.m1.2.2" lspace="0em" rspace="0.222em">×</mo><mn id="S4.F5.2.m1.2.3">2.5</mn></mrow><annotation encoding="application/x-tex" id="S4.F5.2.m1.2d">\approx\times 2.5</annotation><annotation encoding="application/x-llamapun" id="S4.F5.2.m1.2e">≈ × 2.5</annotation></semantics></math> faster than H2OTR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib5" title="">5</a>]</cite> with better action recognition.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Results in accuracy of action recognition methods on <span class="ltx_text ltx_font_italic" id="S4.T2.7.1">H2O Dataset</span>. Inputs of methods are: <span class="ltx_text ltx_font_italic" id="S4.T2.8.2">Img</span> stands for semantic features extracted from an image using CNN network, <span class="ltx_text ltx_font_italic" id="S4.T2.9.3">Hand Pose</span> and <span class="ltx_text ltx_font_italic" id="S4.T2.10.4">Obj Ppose</span> stand for pose information type for hands and objects, and <span class="ltx_text ltx_font_italic" id="S4.T2.11.5">Obj Label</span> stands for object label. Results origin from referenced studies.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T2.1.1.2">Method:</th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T2.1.1.3">Year</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.4">Img</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.5">Hand Pose</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.6">Obj Pose</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.7">Obj Label</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1">Acc. <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.m1.1a"><mo id="S4.T2.1.1.1.m1.1.1" stretchy="false" xref="S4.T2.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.2.1.1">C2D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib28" title="">28</a>]</cite>
</th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.2.1.2">2018</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.3">✓</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.4">✗</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.5">✗</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.6">✗</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.7">70.66</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.3.2.1">I3D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib2" title="">2</a>]</cite>
</th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.3.2.2">2017</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.3">✓</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.4">✗</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.5">✗</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.6">✗</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.7">75.21</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.4.3.1">SlowFast <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib10" title="">10</a>]</cite>
</th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.4.3.2">2019</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3.3">✓</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3.4">✗</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3.5">✗</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3.6">✗</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3.7">77.69</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.5.4.1">H+O <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib26" title="">26</a>]</cite>
</th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.5.4.2">2019</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.4.3">✗</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.4.4">3D</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.4.5">6D</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.4.6">✓</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.4.7">68.88</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.6.5.1">ST-GCN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib32" title="">32</a>]</cite>
</th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.6.5.2">2018</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.5.3">✗</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.5.4">3D</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.5.5">6D</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.5.6">✓</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.5.7">73.86</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.7.6.1">TA-GCN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib16" title="">16</a>]</cite>
</th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.7.6.2">2021</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.7.6.3">✗</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.7.6.4">3D</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.7.6.5">6D</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.7.6.6">✓</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.7.6.7">79.25</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.8.7.1">HTT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib30" title="">30</a>]</cite>
</th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.8.7.2">2023</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.8.7.3">✓</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.8.7.4">3D</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.8.7.5">✗</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.8.7.6">✓</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.8.7.7">86.36</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.9.8.1">H2OTR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib5" title="">5</a>]</cite>
</th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.9.8.2">2023</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.9.8.3">✗</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.9.8.4">3D</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.9.8.5">6D</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.9.8.6">✓</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.9.8.7">90.90</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.10.9.1">EffHandEgoNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib18" title="">18</a>]</cite>
</th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.10.9.2">2024</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.10.9.3">✗</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.10.9.4">2D</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.10.9.5">2D</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.10.9.6">✓</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.10.9.7">91.32</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T2.1.11.10.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.11.10.1.1">Ours</span></th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S4.T2.1.11.10.2">Now</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.11.10.3">✗</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.11.10.4">3D</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.11.10.5">2D</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.11.10.6">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.11.10.7"><span class="ltx_text ltx_font_bold" id="S4.T2.1.11.10.7.1">91.73</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.2">Our architecture with <span class="ltx_text ltx_font_italic" id="S4.SS4.p1.2.1">SHARP</span> gives an average MPJPE in hand pose of <math alttext="29.61\pm 0.71" class="ltx_Math" display="inline" id="S4.SS4.p1.1.m1.1"><semantics id="S4.SS4.p1.1.m1.1a"><mrow id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml"><mn id="S4.SS4.p1.1.m1.1.1.2" xref="S4.SS4.p1.1.m1.1.1.2.cmml">29.61</mn><mo id="S4.SS4.p1.1.m1.1.1.1" xref="S4.SS4.p1.1.m1.1.1.1.cmml">±</mo><mn id="S4.SS4.p1.1.m1.1.1.3" xref="S4.SS4.p1.1.m1.1.1.3.cmml">0.71</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><apply id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS4.p1.1.m1.1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.SS4.p1.1.m1.1.1.2.cmml" type="float" xref="S4.SS4.p1.1.m1.1.1.2">29.61</cn><cn id="S4.SS4.p1.1.m1.1.1.3.cmml" type="float" xref="S4.SS4.p1.1.m1.1.1.3">0.71</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">29.61\pm 0.71</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.1.m1.1d">29.61 ± 0.71</annotation></semantics></math> mm in three consecutive runs with the best run MPJPE equal to 28.66 mm. The qualitative results shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S4.F4" title="Figure 4 ‣ 4.4 Comparison with State of the Art ‣ 4 Experiments ‣ SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_tag">4</span></a> confirm the improvement in 3D hand pose estimation when using <span class="ltx_text ltx_font_italic" id="S4.SS4.p1.2.2">SHARP</span>, but also show that <span class="ltx_text ltx_font_italic" id="S4.SS4.p1.2.3">SHARP</span> can lead to a degradation in performance if too much information is reduced from the scene. Further, we employ the estimated 3D hand pose using <span class="ltx_text ltx_font_italic" id="S4.SS4.p1.2.4">SHARP</span> in the proposed action recognition architecture. It yields an average of 90.90%<math alttext="\pm" class="ltx_Math" display="inline" id="S4.SS4.p1.2.m2.1"><semantics id="S4.SS4.p1.2.m2.1a"><mo id="S4.SS4.p1.2.m2.1.1" xref="S4.SS4.p1.2.m2.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.2.m2.1b"><csymbol cd="latexml" id="S4.SS4.p1.2.m2.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.2.m2.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.2.m2.1d">±</annotation></semantics></math>0.67 over three runs, with the best model yielding an accuracy of 91.73%.
Comparison with state of the art for egocentric 3D hand pose estimation is presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S4.T1" title="Table 1 ‣ 4.4 Comparison with State of the Art ‣ 4 Experiments ‣ SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_tag">1</span></a>.
Table <a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S4.T2" title="Table 2 ‣ 4.4 Comparison with State of the Art ‣ 4 Experiments ‣ SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_tag">2</span></a> presents a comparison of state-of-the-art action recognition methods and their results on the <span class="ltx_text ltx_font_italic" id="S4.SS4.p1.2.5">H2O Dataset</span> reported by the authors. To ensure a fair comparison, the table provides details regarding the inputs of the action recognition modules.
For both tasks, we follow other studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib18" title="">18</a>]</cite> and report our best results.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">We measure the inference times of our methods for the hand pose estimation task for a single frame and for a complete action recognition pipeline for a single action. The evaluation is performed by averaging the inference times over 1000 trials on the NVIDIA GeForce RTX3090 GPU for reliability. The results are shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S4.F5" title="Figure 5 ‣ 4.4 Comparison with State of the Art ‣ 4 Experiments ‣ SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_tag">5</span></a>, where the upper part shows the hand pose performance and the lower part shows the action recognition. Our methods are compared with HTT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib30" title="">30</a>]</cite> and H2OTR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib5" title="">5</a>]</cite> as they are the only open-source implementations that allow such a comparison on the <span class="ltx_text ltx_font_italic" id="S4.SS4.p2.1.1">H2O Dataset</span> at the time of this study.</p>
</div>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.2"><span class="ltx_text ltx_font_italic" id="S4.SS4.p3.2.1">SHARP</span> estimates the egocentric 3D hand pose with the second best result, being faster <math alttext="\approx\times 2.4" class="ltx_math_unparsed" display="inline" id="S4.SS4.p3.1.m1.2"><semantics id="S4.SS4.p3.1.m1.2a"><mrow id="S4.SS4.p3.1.m1.2b"><mo id="S4.SS4.p3.1.m1.1.1" rspace="0em">≈</mo><mo id="S4.SS4.p3.1.m1.2.2" lspace="0em" rspace="0.222em">×</mo><mn id="S4.SS4.p3.1.m1.2.3">2.4</mn></mrow><annotation encoding="application/x-tex" id="S4.SS4.p3.1.m1.2c">\approx\times 2.4</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.1.m1.2d">≈ × 2.4</annotation></semantics></math> than the best H2OTR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib5" title="">5</a>]</cite> with 13M fewer parameters and only a 3mm performance penalty. Our action recognition outperforms all state-of-the-art methods and infers <math alttext="\approx\times 2.6" class="ltx_math_unparsed" display="inline" id="S4.SS4.p3.2.m2.2"><semantics id="S4.SS4.p3.2.m2.2a"><mrow id="S4.SS4.p3.2.m2.2b"><mo id="S4.SS4.p3.2.m2.1.1" rspace="0em">≈</mo><mo id="S4.SS4.p3.2.m2.2.2" lspace="0em" rspace="0.222em">×</mo><mn id="S4.SS4.p3.2.m2.2.3">2.6</mn></mrow><annotation encoding="application/x-tex" id="S4.SS4.p3.2.m2.2c">\approx\times 2.6</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.2.m2.2d">≈ × 2.6</annotation></semantics></math> faster with 12M fewer parameters than the second best H2OTR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#bib.bib5" title="">5</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Ablation Studies</h3>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="150" id="S4.F6.g1" src="extracted/5800341/images/ablation_both.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Figures showing the results of the 3D hand pose estimation error in MPJPE as a function of the segmentation threshold <math alttext="t" class="ltx_Math" display="inline" id="S4.F6.4.m1.1"><semantics id="S4.F6.4.m1.1b"><mi id="S4.F6.4.m1.1.1" xref="S4.F6.4.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.F6.4.m1.1c"><ci id="S4.F6.4.m1.1.1.cmml" xref="S4.F6.4.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.4.m1.1d">t</annotation><annotation encoding="application/x-llamapun" id="S4.F6.4.m1.1e">italic_t</annotation></semantics></math>. The left figure shows the performance with different thresholds used for training and the right figure shows the performance for the best trained model with <math alttext="t=0.47" class="ltx_Math" display="inline" id="S4.F6.5.m2.1"><semantics id="S4.F6.5.m2.1b"><mrow id="S4.F6.5.m2.1.1" xref="S4.F6.5.m2.1.1.cmml"><mi id="S4.F6.5.m2.1.1.2" xref="S4.F6.5.m2.1.1.2.cmml">t</mi><mo id="S4.F6.5.m2.1.1.1" xref="S4.F6.5.m2.1.1.1.cmml">=</mo><mn id="S4.F6.5.m2.1.1.3" xref="S4.F6.5.m2.1.1.3.cmml">0.47</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F6.5.m2.1c"><apply id="S4.F6.5.m2.1.1.cmml" xref="S4.F6.5.m2.1.1"><eq id="S4.F6.5.m2.1.1.1.cmml" xref="S4.F6.5.m2.1.1.1"></eq><ci id="S4.F6.5.m2.1.1.2.cmml" xref="S4.F6.5.m2.1.1.2">𝑡</ci><cn id="S4.F6.5.m2.1.1.3.cmml" type="float" xref="S4.F6.5.m2.1.1.3">0.47</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.5.m2.1d">t=0.47</annotation><annotation encoding="application/x-llamapun" id="S4.F6.5.m2.1e">italic_t = 0.47</annotation></semantics></math> and different <math alttext="t" class="ltx_Math" display="inline" id="S4.F6.6.m3.1"><semantics id="S4.F6.6.m3.1b"><mi id="S4.F6.6.m3.1.1" xref="S4.F6.6.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.F6.6.m3.1c"><ci id="S4.F6.6.m3.1.1.cmml" xref="S4.F6.6.m3.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.6.m3.1d">t</annotation><annotation encoding="application/x-llamapun" id="S4.F6.6.m3.1e">italic_t</annotation></semantics></math> during inference.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">To further evaluate our approach, we conduct extensive ablation studies. All experiments are performed with a fixed number of seeds to ensure reproducibility by eliminating the effect of random initialisation.</p>
</div>
<section class="ltx_paragraph" id="S4.SS5.SSS0.Px1">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">The range of human arms in training</h5>
<div class="ltx_para" id="S4.SS5.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS5.SSS0.Px1.p1.6">The most important part of our architecture is the pseudo-depth-based distance segmentation, which aims to remove irrelevant information from the processed scene, except for the human hands and the manipulated object. It raises the key question of what value of distance should be used as the threshold <math alttext="t" class="ltx_Math" display="inline" id="S4.SS5.SSS0.Px1.p1.1.m1.1"><semantics id="S4.SS5.SSS0.Px1.p1.1.m1.1a"><mi id="S4.SS5.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS0.Px1.p1.1.m1.1b"><ci id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS0.Px1.p1.1.m1.1c">t</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.SSS0.Px1.p1.1.m1.1d">italic_t</annotation></semantics></math>. In the case of pseudo depth obtained with <span class="ltx_text ltx_font_italic" id="S4.SS5.SSS0.Px1.p1.6.1">DPT-Hybrid</span>, the depth values are normalised, where <math alttext="t\in&lt;0,1&gt;" class="ltx_math_unparsed" display="inline" id="S4.SS5.SSS0.Px1.p1.2.m2.2"><semantics id="S4.SS5.SSS0.Px1.p1.2.m2.2a"><mrow id="S4.SS5.SSS0.Px1.p1.2.m2.2b"><mi id="S4.SS5.SSS0.Px1.p1.2.m2.2.3">t</mi><mo id="S4.SS5.SSS0.Px1.p1.2.m2.2.4" rspace="0em">∈</mo><mo id="S4.SS5.SSS0.Px1.p1.2.m2.2.5" lspace="0em">&lt;</mo><mn id="S4.SS5.SSS0.Px1.p1.2.m2.1.1">0</mn><mo id="S4.SS5.SSS0.Px1.p1.2.m2.2.6">,</mo><mn id="S4.SS5.SSS0.Px1.p1.2.m2.2.2">1</mn><mo id="S4.SS5.SSS0.Px1.p1.2.m2.2.7">&gt;</mo></mrow><annotation encoding="application/x-tex" id="S4.SS5.SSS0.Px1.p1.2.m2.2c">t\in&lt;0,1&gt;</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.SSS0.Px1.p1.2.m2.2d">italic_t ∈ &lt; 0 , 1 &gt;</annotation></semantics></math>. To select <math alttext="t" class="ltx_Math" display="inline" id="S4.SS5.SSS0.Px1.p1.3.m3.1"><semantics id="S4.SS5.SSS0.Px1.p1.3.m3.1a"><mi id="S4.SS5.SSS0.Px1.p1.3.m3.1.1" xref="S4.SS5.SSS0.Px1.p1.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS0.Px1.p1.3.m3.1b"><ci id="S4.SS5.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S4.SS5.SSS0.Px1.p1.3.m3.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS0.Px1.p1.3.m3.1c">t</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.SSS0.Px1.p1.3.m3.1d">italic_t</annotation></semantics></math>, we first observe the dataset samples and choose values that lead to the preservation of hands and objects only. However, as it is based on estimation, the behaviour is not the same for all samples for the same <math alttext="t" class="ltx_Math" display="inline" id="S4.SS5.SSS0.Px1.p1.4.m4.1"><semantics id="S4.SS5.SSS0.Px1.p1.4.m4.1a"><mi id="S4.SS5.SSS0.Px1.p1.4.m4.1.1" xref="S4.SS5.SSS0.Px1.p1.4.m4.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS0.Px1.p1.4.m4.1b"><ci id="S4.SS5.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S4.SS5.SSS0.Px1.p1.4.m4.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS0.Px1.p1.4.m4.1c">t</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.SSS0.Px1.p1.4.m4.1d">italic_t</annotation></semantics></math> and none of these values can be considered good without being proven with performance. In the second step, we search for the best performance by retraining the architecture for each of these <math alttext="t\in\{0.35,0.39,0.43,0.47,0.51\}" class="ltx_Math" display="inline" id="S4.SS5.SSS0.Px1.p1.5.m5.5"><semantics id="S4.SS5.SSS0.Px1.p1.5.m5.5a"><mrow id="S4.SS5.SSS0.Px1.p1.5.m5.5.6" xref="S4.SS5.SSS0.Px1.p1.5.m5.5.6.cmml"><mi id="S4.SS5.SSS0.Px1.p1.5.m5.5.6.2" xref="S4.SS5.SSS0.Px1.p1.5.m5.5.6.2.cmml">t</mi><mo id="S4.SS5.SSS0.Px1.p1.5.m5.5.6.1" xref="S4.SS5.SSS0.Px1.p1.5.m5.5.6.1.cmml">∈</mo><mrow id="S4.SS5.SSS0.Px1.p1.5.m5.5.6.3.2" xref="S4.SS5.SSS0.Px1.p1.5.m5.5.6.3.1.cmml"><mo id="S4.SS5.SSS0.Px1.p1.5.m5.5.6.3.2.1" stretchy="false" xref="S4.SS5.SSS0.Px1.p1.5.m5.5.6.3.1.cmml">{</mo><mn id="S4.SS5.SSS0.Px1.p1.5.m5.1.1" xref="S4.SS5.SSS0.Px1.p1.5.m5.1.1.cmml">0.35</mn><mo id="S4.SS5.SSS0.Px1.p1.5.m5.5.6.3.2.2" xref="S4.SS5.SSS0.Px1.p1.5.m5.5.6.3.1.cmml">,</mo><mn id="S4.SS5.SSS0.Px1.p1.5.m5.2.2" xref="S4.SS5.SSS0.Px1.p1.5.m5.2.2.cmml">0.39</mn><mo id="S4.SS5.SSS0.Px1.p1.5.m5.5.6.3.2.3" xref="S4.SS5.SSS0.Px1.p1.5.m5.5.6.3.1.cmml">,</mo><mn id="S4.SS5.SSS0.Px1.p1.5.m5.3.3" xref="S4.SS5.SSS0.Px1.p1.5.m5.3.3.cmml">0.43</mn><mo id="S4.SS5.SSS0.Px1.p1.5.m5.5.6.3.2.4" xref="S4.SS5.SSS0.Px1.p1.5.m5.5.6.3.1.cmml">,</mo><mn id="S4.SS5.SSS0.Px1.p1.5.m5.4.4" xref="S4.SS5.SSS0.Px1.p1.5.m5.4.4.cmml">0.47</mn><mo id="S4.SS5.SSS0.Px1.p1.5.m5.5.6.3.2.5" xref="S4.SS5.SSS0.Px1.p1.5.m5.5.6.3.1.cmml">,</mo><mn id="S4.SS5.SSS0.Px1.p1.5.m5.5.5" xref="S4.SS5.SSS0.Px1.p1.5.m5.5.5.cmml">0.51</mn><mo id="S4.SS5.SSS0.Px1.p1.5.m5.5.6.3.2.6" stretchy="false" xref="S4.SS5.SSS0.Px1.p1.5.m5.5.6.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS0.Px1.p1.5.m5.5b"><apply id="S4.SS5.SSS0.Px1.p1.5.m5.5.6.cmml" xref="S4.SS5.SSS0.Px1.p1.5.m5.5.6"><in id="S4.SS5.SSS0.Px1.p1.5.m5.5.6.1.cmml" xref="S4.SS5.SSS0.Px1.p1.5.m5.5.6.1"></in><ci id="S4.SS5.SSS0.Px1.p1.5.m5.5.6.2.cmml" xref="S4.SS5.SSS0.Px1.p1.5.m5.5.6.2">𝑡</ci><set id="S4.SS5.SSS0.Px1.p1.5.m5.5.6.3.1.cmml" xref="S4.SS5.SSS0.Px1.p1.5.m5.5.6.3.2"><cn id="S4.SS5.SSS0.Px1.p1.5.m5.1.1.cmml" type="float" xref="S4.SS5.SSS0.Px1.p1.5.m5.1.1">0.35</cn><cn id="S4.SS5.SSS0.Px1.p1.5.m5.2.2.cmml" type="float" xref="S4.SS5.SSS0.Px1.p1.5.m5.2.2">0.39</cn><cn id="S4.SS5.SSS0.Px1.p1.5.m5.3.3.cmml" type="float" xref="S4.SS5.SSS0.Px1.p1.5.m5.3.3">0.43</cn><cn id="S4.SS5.SSS0.Px1.p1.5.m5.4.4.cmml" type="float" xref="S4.SS5.SSS0.Px1.p1.5.m5.4.4">0.47</cn><cn id="S4.SS5.SSS0.Px1.p1.5.m5.5.5.cmml" type="float" xref="S4.SS5.SSS0.Px1.p1.5.m5.5.5">0.51</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS0.Px1.p1.5.m5.5c">t\in\{0.35,0.39,0.43,0.47,0.51\}</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.SSS0.Px1.p1.5.m5.5d">italic_t ∈ { 0.35 , 0.39 , 0.43 , 0.47 , 0.51 }</annotation></semantics></math>. The results highlight <math alttext="t=0.47" class="ltx_Math" display="inline" id="S4.SS5.SSS0.Px1.p1.6.m6.1"><semantics id="S4.SS5.SSS0.Px1.p1.6.m6.1a"><mrow id="S4.SS5.SSS0.Px1.p1.6.m6.1.1" xref="S4.SS5.SSS0.Px1.p1.6.m6.1.1.cmml"><mi id="S4.SS5.SSS0.Px1.p1.6.m6.1.1.2" xref="S4.SS5.SSS0.Px1.p1.6.m6.1.1.2.cmml">t</mi><mo id="S4.SS5.SSS0.Px1.p1.6.m6.1.1.1" xref="S4.SS5.SSS0.Px1.p1.6.m6.1.1.1.cmml">=</mo><mn id="S4.SS5.SSS0.Px1.p1.6.m6.1.1.3" xref="S4.SS5.SSS0.Px1.p1.6.m6.1.1.3.cmml">0.47</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS0.Px1.p1.6.m6.1b"><apply id="S4.SS5.SSS0.Px1.p1.6.m6.1.1.cmml" xref="S4.SS5.SSS0.Px1.p1.6.m6.1.1"><eq id="S4.SS5.SSS0.Px1.p1.6.m6.1.1.1.cmml" xref="S4.SS5.SSS0.Px1.p1.6.m6.1.1.1"></eq><ci id="S4.SS5.SSS0.Px1.p1.6.m6.1.1.2.cmml" xref="S4.SS5.SSS0.Px1.p1.6.m6.1.1.2">𝑡</ci><cn id="S4.SS5.SSS0.Px1.p1.6.m6.1.1.3.cmml" type="float" xref="S4.SS5.SSS0.Px1.p1.6.m6.1.1.3">0.47</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS0.Px1.p1.6.m6.1c">t=0.47</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.SSS0.Px1.p1.6.m6.1d">italic_t = 0.47</annotation></semantics></math> as the highest performance value and we observe the performance decrease above and below this value, proving the usability of the proposed method. All results are presented in the left sub-figure of Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S4.F6" title="Figure 6 ‣ 4.5 Ablation Studies ‣ 4 Experiments ‣ SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS5.SSS0.Px2">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">The range of human arms in inference</h5>
<div class="ltx_para" id="S4.SS5.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS5.SSS0.Px2.p1.4">Following the choice of <math alttext="t" class="ltx_Math" display="inline" id="S4.SS5.SSS0.Px2.p1.1.m1.1"><semantics id="S4.SS5.SSS0.Px2.p1.1.m1.1a"><mi id="S4.SS5.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS5.SSS0.Px2.p1.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS0.Px2.p1.1.m1.1b"><ci id="S4.SS5.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS5.SSS0.Px2.p1.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS0.Px2.p1.1.m1.1c">t</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.SSS0.Px2.p1.1.m1.1d">italic_t</annotation></semantics></math> in training, we examine the choice of t in testing for the best-performing model with <math alttext="t=0.47" class="ltx_Math" display="inline" id="S4.SS5.SSS0.Px2.p1.2.m2.1"><semantics id="S4.SS5.SSS0.Px2.p1.2.m2.1a"><mrow id="S4.SS5.SSS0.Px2.p1.2.m2.1.1" xref="S4.SS5.SSS0.Px2.p1.2.m2.1.1.cmml"><mi id="S4.SS5.SSS0.Px2.p1.2.m2.1.1.2" xref="S4.SS5.SSS0.Px2.p1.2.m2.1.1.2.cmml">t</mi><mo id="S4.SS5.SSS0.Px2.p1.2.m2.1.1.1" xref="S4.SS5.SSS0.Px2.p1.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS5.SSS0.Px2.p1.2.m2.1.1.3" xref="S4.SS5.SSS0.Px2.p1.2.m2.1.1.3.cmml">0.47</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS0.Px2.p1.2.m2.1b"><apply id="S4.SS5.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S4.SS5.SSS0.Px2.p1.2.m2.1.1"><eq id="S4.SS5.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S4.SS5.SSS0.Px2.p1.2.m2.1.1.1"></eq><ci id="S4.SS5.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S4.SS5.SSS0.Px2.p1.2.m2.1.1.2">𝑡</ci><cn id="S4.SS5.SSS0.Px2.p1.2.m2.1.1.3.cmml" type="float" xref="S4.SS5.SSS0.Px2.p1.2.m2.1.1.3">0.47</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS0.Px2.p1.2.m2.1c">t=0.47</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.SSS0.Px2.p1.2.m2.1d">italic_t = 0.47</annotation></semantics></math> in training. We run tests for <math alttext="t\in\{0.35,0.39,0.43,0.47,0.51\}" class="ltx_Math" display="inline" id="S4.SS5.SSS0.Px2.p1.3.m3.5"><semantics id="S4.SS5.SSS0.Px2.p1.3.m3.5a"><mrow id="S4.SS5.SSS0.Px2.p1.3.m3.5.6" xref="S4.SS5.SSS0.Px2.p1.3.m3.5.6.cmml"><mi id="S4.SS5.SSS0.Px2.p1.3.m3.5.6.2" xref="S4.SS5.SSS0.Px2.p1.3.m3.5.6.2.cmml">t</mi><mo id="S4.SS5.SSS0.Px2.p1.3.m3.5.6.1" xref="S4.SS5.SSS0.Px2.p1.3.m3.5.6.1.cmml">∈</mo><mrow id="S4.SS5.SSS0.Px2.p1.3.m3.5.6.3.2" xref="S4.SS5.SSS0.Px2.p1.3.m3.5.6.3.1.cmml"><mo id="S4.SS5.SSS0.Px2.p1.3.m3.5.6.3.2.1" stretchy="false" xref="S4.SS5.SSS0.Px2.p1.3.m3.5.6.3.1.cmml">{</mo><mn id="S4.SS5.SSS0.Px2.p1.3.m3.1.1" xref="S4.SS5.SSS0.Px2.p1.3.m3.1.1.cmml">0.35</mn><mo id="S4.SS5.SSS0.Px2.p1.3.m3.5.6.3.2.2" xref="S4.SS5.SSS0.Px2.p1.3.m3.5.6.3.1.cmml">,</mo><mn id="S4.SS5.SSS0.Px2.p1.3.m3.2.2" xref="S4.SS5.SSS0.Px2.p1.3.m3.2.2.cmml">0.39</mn><mo id="S4.SS5.SSS0.Px2.p1.3.m3.5.6.3.2.3" xref="S4.SS5.SSS0.Px2.p1.3.m3.5.6.3.1.cmml">,</mo><mn id="S4.SS5.SSS0.Px2.p1.3.m3.3.3" xref="S4.SS5.SSS0.Px2.p1.3.m3.3.3.cmml">0.43</mn><mo id="S4.SS5.SSS0.Px2.p1.3.m3.5.6.3.2.4" xref="S4.SS5.SSS0.Px2.p1.3.m3.5.6.3.1.cmml">,</mo><mn id="S4.SS5.SSS0.Px2.p1.3.m3.4.4" xref="S4.SS5.SSS0.Px2.p1.3.m3.4.4.cmml">0.47</mn><mo id="S4.SS5.SSS0.Px2.p1.3.m3.5.6.3.2.5" xref="S4.SS5.SSS0.Px2.p1.3.m3.5.6.3.1.cmml">,</mo><mn id="S4.SS5.SSS0.Px2.p1.3.m3.5.5" xref="S4.SS5.SSS0.Px2.p1.3.m3.5.5.cmml">0.51</mn><mo id="S4.SS5.SSS0.Px2.p1.3.m3.5.6.3.2.6" stretchy="false" xref="S4.SS5.SSS0.Px2.p1.3.m3.5.6.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS0.Px2.p1.3.m3.5b"><apply id="S4.SS5.SSS0.Px2.p1.3.m3.5.6.cmml" xref="S4.SS5.SSS0.Px2.p1.3.m3.5.6"><in id="S4.SS5.SSS0.Px2.p1.3.m3.5.6.1.cmml" xref="S4.SS5.SSS0.Px2.p1.3.m3.5.6.1"></in><ci id="S4.SS5.SSS0.Px2.p1.3.m3.5.6.2.cmml" xref="S4.SS5.SSS0.Px2.p1.3.m3.5.6.2">𝑡</ci><set id="S4.SS5.SSS0.Px2.p1.3.m3.5.6.3.1.cmml" xref="S4.SS5.SSS0.Px2.p1.3.m3.5.6.3.2"><cn id="S4.SS5.SSS0.Px2.p1.3.m3.1.1.cmml" type="float" xref="S4.SS5.SSS0.Px2.p1.3.m3.1.1">0.35</cn><cn id="S4.SS5.SSS0.Px2.p1.3.m3.2.2.cmml" type="float" xref="S4.SS5.SSS0.Px2.p1.3.m3.2.2">0.39</cn><cn id="S4.SS5.SSS0.Px2.p1.3.m3.3.3.cmml" type="float" xref="S4.SS5.SSS0.Px2.p1.3.m3.3.3">0.43</cn><cn id="S4.SS5.SSS0.Px2.p1.3.m3.4.4.cmml" type="float" xref="S4.SS5.SSS0.Px2.p1.3.m3.4.4">0.47</cn><cn id="S4.SS5.SSS0.Px2.p1.3.m3.5.5.cmml" type="float" xref="S4.SS5.SSS0.Px2.p1.3.m3.5.5">0.51</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS0.Px2.p1.3.m3.5c">t\in\{0.35,0.39,0.43,0.47,0.51\}</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.SSS0.Px2.p1.3.m3.5d">italic_t ∈ { 0.35 , 0.39 , 0.43 , 0.47 , 0.51 }</annotation></semantics></math>. All results are shown in the right subplot of Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S4.F6" title="Figure 6 ‣ 4.5 Ablation Studies ‣ 4 Experiments ‣ SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_tag">6</span></a>. The effect of <math alttext="t" class="ltx_Math" display="inline" id="S4.SS5.SSS0.Px2.p1.4.m4.1"><semantics id="S4.SS5.SSS0.Px2.p1.4.m4.1a"><mi id="S4.SS5.SSS0.Px2.p1.4.m4.1.1" xref="S4.SS5.SSS0.Px2.p1.4.m4.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS0.Px2.p1.4.m4.1b"><ci id="S4.SS5.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S4.SS5.SSS0.Px2.p1.4.m4.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS0.Px2.p1.4.m4.1c">t</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.SSS0.Px2.p1.4.m4.1d">italic_t</annotation></semantics></math> is significantly lower than in training and does not affect performance much.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS5.SSS0.Px3">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">Pseudo-depth-based SHARP module</h5>
<figure class="ltx_figure" id="S4.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F7.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="204" id="S4.F7.1.g1" src="x1.png" width="727"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F7.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="204" id="S4.F7.2.g1" src="x2.png" width="175"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span> On the left, frame processed with <span class="ltx_text ltx_font_italic" id="S4.F7.9.1">SHARP</span> and different values of <math alttext="t" class="ltx_Math" display="inline" id="S4.F7.5.m1.1"><semantics id="S4.F7.5.m1.1b"><mi id="S4.F7.5.m1.1.1" xref="S4.F7.5.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.F7.5.m1.1c"><ci id="S4.F7.5.m1.1.1.cmml" xref="S4.F7.5.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.5.m1.1d">t</annotation><annotation encoding="application/x-llamapun" id="S4.F7.5.m1.1e">italic_t</annotation></semantics></math>. On the right, the same frame processed with <span class="ltx_text ltx_font_italic" id="S4.F7.10.2">SHARP</span>, <math alttext="t=0.47" class="ltx_Math" display="inline" id="S4.F7.6.m2.1"><semantics id="S4.F7.6.m2.1b"><mrow id="S4.F7.6.m2.1.1" xref="S4.F7.6.m2.1.1.cmml"><mi id="S4.F7.6.m2.1.1.2" xref="S4.F7.6.m2.1.1.2.cmml">t</mi><mo id="S4.F7.6.m2.1.1.1" xref="S4.F7.6.m2.1.1.1.cmml">=</mo><mn id="S4.F7.6.m2.1.1.3" xref="S4.F7.6.m2.1.1.3.cmml">0.47</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F7.6.m2.1c"><apply id="S4.F7.6.m2.1.1.cmml" xref="S4.F7.6.m2.1.1"><eq id="S4.F7.6.m2.1.1.1.cmml" xref="S4.F7.6.m2.1.1.1"></eq><ci id="S4.F7.6.m2.1.1.2.cmml" xref="S4.F7.6.m2.1.1.2">𝑡</ci><cn id="S4.F7.6.m2.1.1.3.cmml" type="float" xref="S4.F7.6.m2.1.1.3">0.47</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.6.m2.1d">t=0.47</annotation><annotation encoding="application/x-llamapun" id="S4.F7.6.m2.1e">italic_t = 0.47</annotation></semantics></math> and with de-sharpening applied.
</figcaption>
</figure>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Results of ablations studies with different depth image types used in <span class="ltx_text ltx_font_italic" id="S4.T3.6.1">SHARP</span>. All results provided in <span class="ltx_text ltx_font_italic" id="S4.T3.7.2">mm</span> in camera space for left, right and both hands.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.3.3">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S4.T3.3.3.4"></th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.3.3.5">Depth</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1">MPJPE Left <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.m1.1a"><mo id="S4.T3.1.1.1.m1.1.1" stretchy="false" xref="S4.T3.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.2.2.2">MPJPE Right <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.2.2.2.m1.1"><semantics id="S4.T3.2.2.2.m1.1a"><mo id="S4.T3.2.2.2.m1.1.1" stretchy="false" xref="S4.T3.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.m1.1b"><ci id="S4.T3.2.2.2.m1.1.1.cmml" xref="S4.T3.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.2.2.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.3.3.3">MPJPE Both <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.3.3.3.m1.1"><semantics id="S4.T3.3.3.3.m1.1a"><mo id="S4.T3.3.3.3.m1.1.1" stretchy="false" xref="S4.T3.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.m1.1b"><ci id="S4.T3.3.3.3.m1.1.1.cmml" xref="S4.T3.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.3.3.3.m1.1d">↓</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.3.4.1.1">Ours</th>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T3.3.4.1.2">Estimated</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.4.1.3">30.31</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.4.1.4">27.02</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.4.1.5">28.66</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.3.5.2.1">Ablation I</th>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T3.3.5.2.2">✗</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.5.2.3">32.95</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.5.2.4">38.01</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.5.2.5">35.48</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.3.6.3.1">Ablation II</th>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T3.3.6.3.2">Ground Truth</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.6.3.3">21.31</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.6.3.4">28.86</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.6.3.5">25.09</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T3.3.7.4.1">Ablation III</th>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S4.T3.3.7.4.2">Est.+De-sharpen</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.3.7.4.3">39.49</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.3.7.4.4">35.01</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.3.7.4.5">37.25</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS5.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS5.SSS0.Px3.p1.2">We evaluate <span class="ltx_text ltx_font_italic" id="S4.SS5.SSS0.Px3.p1.2.1">SHARP</span>’s impact on the egocentric 3D hand pose estimation performance. The proposed architecture is retrained according to the previously described process without the <span class="ltx_text ltx_font_italic" id="S4.SS5.SSS0.Px3.p1.2.2">SHARP</span> module, using only unsegmented RGB images representing the full scene. The network reduced by the <span class="ltx_text ltx_font_italic" id="S4.SS5.SSS0.Px3.p1.2.3">SHARP</span> module in a fixed seed run achieves an MPJPE of 35.48 mm compared to 28.66 mm obtained with <span class="ltx_text ltx_font_italic" id="S4.SS5.SSS0.Px3.p1.2.4">SHARP</span>. The result is referenced in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S4.T3" title="Table 3 ‣ Pseudo-depth-based SHARP module ‣ 4.5 Ablation Studies ‣ 4 Experiments ‣ SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_tag">3</span></a> as <span class="ltx_text ltx_font_italic" id="S4.SS5.SSS0.Px3.p1.2.5">Ablation I</span>. The process is repeated three times to reduce the random effect of network initialisation and to strengthen the justification of the idea. The average of the three runs without the <span class="ltx_text ltx_font_italic" id="S4.SS5.SSS0.Px3.p1.2.6">SHARP</span> module is <math alttext="35.34\pm 0.17" class="ltx_Math" display="inline" id="S4.SS5.SSS0.Px3.p1.1.m1.1"><semantics id="S4.SS5.SSS0.Px3.p1.1.m1.1a"><mrow id="S4.SS5.SSS0.Px3.p1.1.m1.1.1" xref="S4.SS5.SSS0.Px3.p1.1.m1.1.1.cmml"><mn id="S4.SS5.SSS0.Px3.p1.1.m1.1.1.2" xref="S4.SS5.SSS0.Px3.p1.1.m1.1.1.2.cmml">35.34</mn><mo id="S4.SS5.SSS0.Px3.p1.1.m1.1.1.1" xref="S4.SS5.SSS0.Px3.p1.1.m1.1.1.1.cmml">±</mo><mn id="S4.SS5.SSS0.Px3.p1.1.m1.1.1.3" xref="S4.SS5.SSS0.Px3.p1.1.m1.1.1.3.cmml">0.17</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS0.Px3.p1.1.m1.1b"><apply id="S4.SS5.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S4.SS5.SSS0.Px3.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS5.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S4.SS5.SSS0.Px3.p1.1.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.SS5.SSS0.Px3.p1.1.m1.1.1.2.cmml" type="float" xref="S4.SS5.SSS0.Px3.p1.1.m1.1.1.2">35.34</cn><cn id="S4.SS5.SSS0.Px3.p1.1.m1.1.1.3.cmml" type="float" xref="S4.SS5.SSS0.Px3.p1.1.m1.1.1.3">0.17</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS0.Px3.p1.1.m1.1c">35.34\pm 0.17</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.SSS0.Px3.p1.1.m1.1d">35.34 ± 0.17</annotation></semantics></math>, while with <span class="ltx_text ltx_font_italic" id="S4.SS5.SSS0.Px3.p1.2.7">SHARP</span>, the performance improves to <math alttext="29.61\pm 0.71" class="ltx_Math" display="inline" id="S4.SS5.SSS0.Px3.p1.2.m2.1"><semantics id="S4.SS5.SSS0.Px3.p1.2.m2.1a"><mrow id="S4.SS5.SSS0.Px3.p1.2.m2.1.1" xref="S4.SS5.SSS0.Px3.p1.2.m2.1.1.cmml"><mn id="S4.SS5.SSS0.Px3.p1.2.m2.1.1.2" xref="S4.SS5.SSS0.Px3.p1.2.m2.1.1.2.cmml">29.61</mn><mo id="S4.SS5.SSS0.Px3.p1.2.m2.1.1.1" xref="S4.SS5.SSS0.Px3.p1.2.m2.1.1.1.cmml">±</mo><mn id="S4.SS5.SSS0.Px3.p1.2.m2.1.1.3" xref="S4.SS5.SSS0.Px3.p1.2.m2.1.1.3.cmml">0.71</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS0.Px3.p1.2.m2.1b"><apply id="S4.SS5.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S4.SS5.SSS0.Px3.p1.2.m2.1.1"><csymbol cd="latexml" id="S4.SS5.SSS0.Px3.p1.2.m2.1.1.1.cmml" xref="S4.SS5.SSS0.Px3.p1.2.m2.1.1.1">plus-or-minus</csymbol><cn id="S4.SS5.SSS0.Px3.p1.2.m2.1.1.2.cmml" type="float" xref="S4.SS5.SSS0.Px3.p1.2.m2.1.1.2">29.61</cn><cn id="S4.SS5.SSS0.Px3.p1.2.m2.1.1.3.cmml" type="float" xref="S4.SS5.SSS0.Px3.p1.2.m2.1.1.3">0.71</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS0.Px3.p1.2.m2.1c">29.61\pm 0.71</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.SSS0.Px3.p1.2.m2.1d">29.61 ± 0.71</annotation></semantics></math> mm, demonstrating the high importance of the proposed architecture.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS5.SSS0.Px4">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">Oracle depth-based SHARP module</h5>
<div class="ltx_para" id="S4.SS5.SSS0.Px4.p1">
<p class="ltx_p" id="S4.SS5.SSS0.Px4.p1.1"><span class="ltx_text ltx_font_italic" id="S4.SS5.SSS0.Px4.p1.1.1">SHARP</span> uses the state-of-the-art depth estimation network <span class="ltx_text ltx_font_italic" id="S4.SS5.SSS0.Px4.p1.1.2">DPT-Hybrid</span>. Like any deep learning architecture, this model is prone to errors. On the other hand, with progress in architecture development, depth estimation networks will improve in the future, leading to an improvement in the performance of our method. To highlight this potential, we retrain the network with an oracle ground truth depth image provided in the <span class="ltx_text ltx_font_italic" id="S4.SS5.SSS0.Px4.p1.1.3">H2O Dataset</span>. The depth image represents the distance in mm from a camera. For this reason, we choose <math alttext="t=700" class="ltx_Math" display="inline" id="S4.SS5.SSS0.Px4.p1.1.m1.1"><semantics id="S4.SS5.SSS0.Px4.p1.1.m1.1a"><mrow id="S4.SS5.SSS0.Px4.p1.1.m1.1.1" xref="S4.SS5.SSS0.Px4.p1.1.m1.1.1.cmml"><mi id="S4.SS5.SSS0.Px4.p1.1.m1.1.1.2" xref="S4.SS5.SSS0.Px4.p1.1.m1.1.1.2.cmml">t</mi><mo id="S4.SS5.SSS0.Px4.p1.1.m1.1.1.1" xref="S4.SS5.SSS0.Px4.p1.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS5.SSS0.Px4.p1.1.m1.1.1.3" xref="S4.SS5.SSS0.Px4.p1.1.m1.1.1.3.cmml">700</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS0.Px4.p1.1.m1.1b"><apply id="S4.SS5.SSS0.Px4.p1.1.m1.1.1.cmml" xref="S4.SS5.SSS0.Px4.p1.1.m1.1.1"><eq id="S4.SS5.SSS0.Px4.p1.1.m1.1.1.1.cmml" xref="S4.SS5.SSS0.Px4.p1.1.m1.1.1.1"></eq><ci id="S4.SS5.SSS0.Px4.p1.1.m1.1.1.2.cmml" xref="S4.SS5.SSS0.Px4.p1.1.m1.1.1.2">𝑡</ci><cn id="S4.SS5.SSS0.Px4.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.SS5.SSS0.Px4.p1.1.m1.1.1.3">700</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS0.Px4.p1.1.m1.1c">t=700</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.SSS0.Px4.p1.1.m1.1d">italic_t = 700</annotation></semantics></math> mm. The results are superior, achieving an MPJPE of 25.09 mm, better than any state-of-the-art method at the time of this study. The experiment is referred as <span class="ltx_text ltx_font_italic" id="S4.SS5.SSS0.Px4.p1.1.4">Ablation II</span> in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S4.T3" title="Table 3 ‣ Pseudo-depth-based SHARP module ‣ 4.5 Ablation Studies ‣ 4 Experiments ‣ SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_tag">3</span></a>. This performance demonstrates the potential of our approach when fed with less noisy pseudo-depth data.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS5.SSS0.Px5">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">De-sharpening of segmentation mask</h5>
<div class="ltx_para" id="S4.SS5.SSS0.Px5.p1">
<p class="ltx_p" id="S4.SS5.SSS0.Px5.p1.1">The segmentation mask, derived from a pseudo-depth scene representation, consists of sharp edges surrounding the human arms and the manipulated object, based on a distance. Depth estimation is prone to error, and in some scenes, this sharp-edge segmentation leads to the loss of parts of the image that represent relevant information, e.g. human hand. This negative effect can be reduced in two ways, by changing the segmentation threshold as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S4.F7" title="Figure 7 ‣ Pseudo-depth-based SHARP module ‣ 4.5 Ablation Studies ‣ 4 Experiments ‣ SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_tag">7</span></a> or by de-sharpening the edges. The effect of the de-sharpening process is presented in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.10037v1#S4.F7" title="Figure 7 ‣ Pseudo-depth-based SHARP module ‣ 4.5 Ablation Studies ‣ 4 Experiments ‣ SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition"><span class="ltx_text ltx_ref_tag">7</span></a>. In this ablation, we observe the effect of edge de-sharpening by blurring the mask derived from the pseudo-depth scene representation. Performance drops to 37.25 mm, highlighting the usefulness of the <span class="ltx_text ltx_font_italic" id="S4.SS5.SSS0.Px5.p1.1.1">SHARP</span> module only with accurate masking.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this study, a 3D hand pose estimation model has been developed for the egocentric perspective. The novelty of the proposed architecture lies in the <span class="ltx_text ltx_font_italic" id="S5.p1.1.1">SHARP</span> module, which uses pseudo-depth scene representation obtained through a monocular depth estimation model. Thanks to the characteristic of a fixed camera to a user in the egocentric perspective and a constant range of human arms, the distance information is used to remove irrelevant information from the scene. Experiments with our network showed an improvement in performance of 7 mm in the MPJPE metric when using <span class="ltx_text ltx_font_italic" id="S5.p1.1.2">SHARP</span>, with the best result of MPJPE equal to 28.66 mm placing as the second best result on the <span class="ltx_text ltx_font_italic" id="S5.p1.1.3">H2O Dataset</span>. The further potential of the <span class="ltx_text ltx_font_italic" id="S5.p1.1.4">SHARP</span> module was confirmed with the use of the ground truth depth image, resulting in the best result of all state-of-the-art methods equal to 25.09 mm. Furthermore, estimated 3D hand poses were used alongside object detection as input for the action recognition model, where each frame is described by a vector containing the 3D hand pose and the object bounding box, and their sequence is embedded using a transformer-based network. The results obtained on <span class="ltx_text ltx_font_italic" id="S5.p1.1.5">H2O Dataset</span>, which includes actions where one hand or two hands interact with objects, resulted in 91.73% accuracy, outperforming the state-of-the-art.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Our study shows that using pseudo depth to remove irrelevant information in the egocentric scene with current state-of-the-art monocular depth estimation methods improves 3D hand pose performance. The quality of pseudo depth correlates with pose estimation error and requires a sharp and accurate representation of human hands in the scene. In the future, with the advancement of depth estimation networks, this approach has a chance to improve hand pose estimation tasks further, leading to more accurate action recognition.</p>
</div>
<section class="ltx_subsubsection" id="S5.SS0.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.0.1 </span>Acknowledgements</h4>
<div class="ltx_para" id="S5.SS0.SSS1.p1">
<p class="ltx_p" id="S5.SS0.SSS1.p1.1">Part of this work was conducted during Wiktor’s research secondment at the University of Bristol within the Machine Learning and Computer Vision Research Group (MaVi). We thank the group for their support and resources. This research was supported by VisuAAL ITN H2020 (grant agreement no. 861091) and the Austrian Research Promotion Agency (grant agreement no. 49450173).</p>
</div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Aboukhadra, A., Malik, J., Elhayek, A., Robertini, N., Stricker, D.: THOR-Net: End-to-end Graformer-based Realistic Two Hands and Object Reconstruction with Self-supervision. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 1001–1010 (01 2023). https://doi.org/10.1109/WACV56688.2023.00106

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Carreira, J., Zisserman, A.: Quo Vadis, Action Recognition? a New Model and the Kinetics Dataset. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 6299–6308 (2017). https://doi.org/10.1109/CVPR.2017.502

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Cartas, A., Radeva, P., Dimiccoli, M.: Contextually Driven First-Person Action Recognition from Videos. In: Presentation at EPIC@ ICCV2017 Workshop. p. 8 (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Chen, W., Fu, Z., Yang, D., Deng, J.: Single-Image Depth Perception in the Wild. Advances in Neural Information Processing Systems <span class="ltx_text ltx_font_bold" id="bib.bib4.1.1">29</span> (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Cho, H., Kim, C., Kim, J., Lee, S., Ismayilzada, E., Baek, S.: Transformer-Based Unified Recognition of Two Hands Manipulating Objects. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4769–4778 (2023). https://doi.org/10.1109/CVPR52729.2023.00462

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Damen, D., Doughty, H., Farinella, G.M., Fidler, S., Furnari, A., Kazakos, E., Moltisanti, D., Munro, J., Perrett, T., Price, W., Wray, M.: Scaling Egocentric Vision: The EPIC-KITCHENS Dataset. In: European Conference on Computer Vision (ECCV) (2018). https://doi.org/10.1007/978-3-030-01225-0_44

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Das, P., Ortega, A.: Symmetric Sub-graph Spatio-temporal Graph Convolution and its Application in Complex Activity Recognition. In: ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 3215–3219. IEEE (2021). https://doi.org/10.1109/ICASSP39728.2021.9413833

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In: International Conference on Learning Representations (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Eigen, D., Puhrsch, C., Fergus, R.: Depth Map Prediction from a Single Image using a Multi-Scale Deep Network. Advances in Neural Information Processing Systems <span class="ltx_text ltx_font_bold" id="bib.bib9.1.1">27</span> (2014)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Feichtenhofer, C., Fan, H., Malik, J., He, K.: Slowfast Networks for Video Recognition. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 6202–6211 (2019). https://doi.org/10.1109/ICCV.2019.00630

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Garcia-Hernando, G., Yuan, S., Baek, S., Kim, T.K.: First-Person Hand Action Benchmark With RGB-D Videos and 3D Hand Pose Annotations. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 409–419 (2018). https://doi.org/10.1109/CVPR.2018.00050

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Garg, R., Bg, V.K., Carneiro, G., Reid, I.: Unsupervised CNN for Single View Depth Estimation: Geometry to the Rescue. In: Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14. pp. 740–756. Springer (2016). https://doi.org/10.1007/978-3-319-46484-8_45

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Godard, C., Mac Aodha, O., Firman, M., Brostow, G.J.: Digging into self-supervised monocular depth estimation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 3828–3838 (2019). https://doi.org/10.1109/ICCV.2019.00393

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Grauman, K., Westbury, A., Byrne, E., Chavis, Z., Furnari, A., Girdhar, R., Hamburger, J., Jiang, H., Liu, M., Liu, X., et al.: Ego4D: Around the World in 3,000 Hours of Egocentric Video. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 18995–19012 (2022). https://doi.org/10.1109/CVPR52688.2022.01842

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Hasson, Y., Tekin, B., Bogo, F., Laptev, I., Pollefeys, M., Schmid, C.: Leveraging Photometric Consistency over Time for Sparsely Supervised Hand-Object Reconstruction. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 571–580 (2020). https://doi.org/10.1109/CVPR42600.2020.00065

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Kwon, T., Tekin, B., Stühmer, J., Bogo, F., Pollefeys, M.: H2O: Two Hands Manipulating Objects for First Person Interaction Recognition. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 10138–10148 (October 2021). https://doi.org/10.1109/iccv48922.2021.00998

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Mucha, W., Cuconasu, F., Etori, N.A., Kalokyri, V., Trappolini, G.: TEXT2TASTE: A Versatile Egocentric Vision System for Intelligent Reading Assistance Using Large Language Model. In: Computers Helping People with Special Needs. pp. 285–291. Springer Nature Switzerland, Cham (2024). https://doi.org/10.1007/978-3-031-62849-8_35

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Mucha, W., Kampel, M.: In My Perspective, in My Hands: Accurate Egocentric 2D Hand Pose and Action Recognition. In: 2024 IEEE 18th International Conference on Automatic Face and Gesture Recognition (FG). pp. 1–9 (2024). https://doi.org/10.1109/FG59268.2024.10582035

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Mueller, F., Mehta, D., Sotnychenko, O., Sridhar, S., Casas, D., Theobalt, C.: Real-time Hand Tracking Under Occlusion From an Egocentric RGB-D Sensor. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 1154–1163 (2017). https://doi.org/10.1109/CVPR.2019.01231

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Nguyen, X.S., Brun, L., Lézoray, O., Bougleux, S.: A Neural Network Based on SPD Manifold Learning for Skeleton-based Hand Gesture Recognition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12036–12045 (2019). https://doi.org/10.1109/CVPR.2019.01231

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Núñez-Marcos, A., Azkune, G., Arganda-Carreras, I.: Egocentric Vision-based Action Recognition: a Survey. Neurocomputing <span class="ltx_text ltx_font_bold" id="bib.bib21.1.1">472</span>, 175–197 (2022). https://doi.org/10.1016/j.neucom.2021.11.081

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Ohkawa, T., He, K., Sener, F., Hodan, T., Tran, L., Keskin, C.: AssemblyHands: towards egocentric activity understanding via 3d hand pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 12999–13008 (2023). https://doi.org/10.1109/CVPR52729.2023.01249

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Ranftl, R., Bochkovskiy, A., Koltun, V.: Vision Transformers for Dense Prediction. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 12179–12188 (2021). https://doi.org/10.1109/ICCV48922.2021.01196

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Ranftl, R., Lasinger, K., Hafner, D., Schindler, K., Koltun, V.: Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer. IEEE Transactions on Pattern Analysis and Machine Intelligence <span class="ltx_text ltx_font_bold" id="bib.bib24.1.1">44</span>(3), 1623–1637 (2020). https://doi.org/10.1109/TPAMI.2020.3019967

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Tan, M., Le, Q.: Efficientnetv2: Smaller Models and Faster Training. In: International Conference on Machine Learning. pp. 10096–10106. PMLR (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Tekin, B., Bogo, F., Pollefeys, M.: H+O: Unified Egocentric Recognition of 3D Hand-object Poses and Interactions. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4511–4520 (2019). https://doi.org/10.1109/CVPR.2019.00464

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Wang, C.Y., Bochkovskiy, A., Liao, H.Y.M.: YOLOv7: Trainable Bag-of-Freebies Sets New State-of-the-Art for Real-Time Object Detectors. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 7464–7475 (2023). https://doi.org/10.48550/arXiv.2207.02696

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Wang, X., Girshick, R., Gupta, A., He, K.: Non-local Neural Networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 7794–7803 (2018). https://doi.org/10.1109/CVPR.2018.00813

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Wang, X., Kwon, T., Rad, M., Pan, B., Chakraborty, I., Andrist, S., Bohus, D., Feniello, A., Tekin, B., Frujeri, F.V., Joshi, N., Pollefeys, M.: HoloAssist: an Egocentric Human Interaction Dataset for Interactive AI Assistants in the Real World. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 20270–20281 (October 2023). https://doi.org/10.1109/ICCV51070.2023.01854

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Wen, Y., Pan, H., Yang, L., Pan, J., Komura, T., Wang, W.: Hierarchical Temporal Transformer for 3D Hand Pose Estimation and Action Recognition from Egocentric RGB Videos. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 21243–21253 (2023). https://doi.org/10.1109/CVPR52729.2023.02035

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Yamazaki, W., Ding, M., Takamatsu, J., Ogasawara, T.: Hand Pose Estimation and Motion Recognition Using Egocentric RGB-D Video. In: 2017 IEEE International Conference on Robotics and Biomimetics (ROBIO). pp. 147–152. IEEE (2017). https://doi.org/10.1109/ROBIO.2017.8324409

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Yan, S., Xiong, Y., Lin, D.: Spatial Temporal Graph Convolutional Networks for Skeleton-based Action Recognition. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 32 (2018). https://doi.org/10.1609/aaai.v32i1.12328

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Aug 19 14:24:34 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
