<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Realistic Data Generation for 6D Pose Estimation of Surgical Instruments</title>
<!--Generated on Tue Jun 11 14:48:19 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2406.07328v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#S1" title="In Realistic Data Generation for 6D Pose Estimation of Surgical Instruments"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#S2" title="In Realistic Data Generation for 6D Pose Estimation of Surgical Instruments"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Work</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#S2.SS1" title="In II Related Work ‣ Realistic Data Generation for 6D Pose Estimation of Surgical Instruments"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Offline Simulation Data Generation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#S2.SS2" title="In II Related Work ‣ Realistic Data Generation for 6D Pose Estimation of Surgical Instruments"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Robot Simulator &amp; Digital Twins</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#S3" title="In Realistic Data Generation for 6D Pose Estimation of Surgical Instruments"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Methodology</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#S3.SS1" title="In III Methodology ‣ Realistic Data Generation for 6D Pose Estimation of Surgical Instruments"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Data generation pipeline</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#S3.SS1.SSS1" title="In III-A Data generation pipeline ‣ III Methodology ‣ Realistic Data Generation for 6D Pose Estimation of Surgical Instruments"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span>1 </span>Format for generated data</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#S3.SS2" title="In III Methodology ‣ Realistic Data Generation for 6D Pose Estimation of Surgical Instruments"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Improvements of virtual scene</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#S3.SS3" title="In III Methodology ‣ Realistic Data Generation for 6D Pose Estimation of Surgical Instruments"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Generated dataset for surgical needles pose detection</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#S3.SS4" title="In III Methodology ‣ Realistic Data Generation for 6D Pose Estimation of Surgical Instruments"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span> </span><span class="ltx_text ltx_font_italic">Selected deep learning model for 6D pose estimation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#S3.SS5" title="In III Methodology ‣ Realistic Data Generation for 6D Pose Estimation of Surgical Instruments"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-E</span> </span><span class="ltx_text ltx_font_italic">Evaluation metrics for the pose estimates</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#S4" title="In Realistic Data Generation for 6D Pose Estimation of Surgical Instruments"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Experiments and Results</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#S5" title="In Realistic Data Generation for 6D Pose Estimation of Surgical Instruments"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Discussion and Future Work</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Realistic Data Generation for 6D Pose Estimation of Surgical Instruments</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Juan Antonio Barragan<sup class="ltx_sup" id="id8.8.id1"><span class="ltx_text ltx_font_italic" id="id8.8.id1.1">1</span></sup>, Jintan Zhang<sup class="ltx_sup" id="id9.9.id2"><span class="ltx_text ltx_font_italic" id="id9.9.id2.1">1</span></sup>, Haoying Zhou<sup class="ltx_sup" id="id10.10.id3"><span class="ltx_text ltx_font_italic" id="id10.10.id3.1">2</span></sup>,
<br class="ltx_break"/>Adnan Munawar<sup class="ltx_sup" id="id11.11.id4">1</sup>, and Peter Kazanzides<sup class="ltx_sup" id="id12.12.id5">1</sup>
</span><span class="ltx_author_notes"><sup class="ltx_sup" id="id13.13.id1">1</sup>Department of Computer Science, Johns Hopkins University, Baltimore, MD 21218, USA.
Email: <span class="ltx_text ltx_font_typewriter" id="id14.14.id2">jbarrag3@jhu.edu, jzhan247@jhu.edu</span><sup class="ltx_sup" id="id15.15.id1">2</sup>Department of Robotics Engineering, Worcester Polytechnic Institute, Worcester, MA 01608, USA.
Email: <span class="ltx_text ltx_font_typewriter" id="id16.16.id2">hzhou6@wpi.edu</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id17.id1">Automation in surgical robotics has the potential to improve patient safety and surgical efficiency, but it is difficult to achieve due to the need for robust perception algorithms. In particular, 6D pose estimation of surgical instruments is critical to enable the automatic execution of surgical maneuvers based on visual feedback. <span class="ltx_text" id="id17.id1.1" style="color:#000000;">In recent years, supervised deep learning algorithms have shown increasingly better performance at 6D pose estimation tasks; yet, their success depends on the availability of large amounts of annotated data. In household and industrial settings, synthetic data, generated with 3D computer graphics software, has been shown as an alternative to minimize annotation costs of 6D pose datasets. However, this strategy does not translate well to surgical domains as commercial graphics software have limited tools to generate images depicting realistic instrument-tissue interactions.</span> To address these limitations, we propose an improved simulation environment for surgical robotics that enables the automatic generation of large and diverse datasets for 6D pose estimation of surgical instruments. Among the improvements, we developed an automated data generation pipeline and an improved surgical scene. To show the applicability of our system, we generated a dataset of 7.5k images with pose annotations of a surgical needle that was used to evaluate a state-of-the-art pose estimation network. The trained model obtained a mean translational error of 2.59 mm on a challenging dataset that presented varying levels of occlusion. These results highlight our pipeline’s success in training and evaluating novel vision algorithms for surgical robotics applications.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.2.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In minimally invasive robotic surgery, automation of time-consuming and repetitive surgical subtasks has the potential to reduce the surgeon’s mental demands and improve the overall efficiency of surgery <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib1" title="">1</a>]</cite>. Automation of surgical subtasks has been extensively studied by the research community, leading to autonomous algorithms for suturing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib5" title="">5</a>]</cite>, blood suction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib7" title="">7</a>]</cite>, and tissue retraction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib8" title="">8</a>]</cite>, among others. <span class="ltx_text" id="S1.p1.1.1" style="color:#000000;">One key challenge of surgical automation is developing perception algorithms to compensate for the robot’s kinematic inaccuracies and execution failures. This requires estimating the 6D pose of rigid and articulated instruments from endoscopic video to modify autonomous motions based on visual feedback. </span></p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1"><span class="ltx_text" id="S1.p2.1.1" style="color:#000000;">In the task of 6D pose estimation, the goal is to estimate the translation and rotation of the object of interest with respect to the camera coordinate frame</span>. This task has traditionally been approached by extracting 2D visual features from RGB images and then matching them with corresponding 3D features on the object’s model. <span class="ltx_text" id="S1.p2.1.2" style="color:#000000;">These 2d-3d correspondences can then be used as an input to a Perspective-n-Point <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib9" title="">9</a>]</cite> solver to retrieve the object’s pose.</span></p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">More recently, end-to-end deep neural networks have demonstrated superior performance for 6D object pose estimation tasks than traditional point-pair feature approaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib10" title="">10</a>]</cite>. The main drawback of these deep learning approaches is the need to generate large amounts of annotated training data, which, for 6D pose estimation tasks, is prohibitively expensive to obtain. As a solution, it has been shown that high-fidelity synthetic data of models of physical objects can be used to train pose networks that perform well in locating their real counterparts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib11" title="">11</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In the surgical context, synthetic data generation is a more challenging endeavor as it is important to generate samples that portray sensible instrument motions and realistic tissue-instrument interaction. Currently available simulation environments such as Vision Blender <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib12" title="">12</a>]</cite> or BlenderProc <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib13" title="">13</a>]</cite> can be utilized to render annotated images of surgical instruments in surgical backgrounds; however, they offer limited capabilities on how the objects can be moved or interact with each other in the scene. Furthermore, they do not offer good support to work with articulated robotic instruments.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">In previous work, an open-source platform for surgical suturing was introduced to address some of the limitations of surgical robotics simulation platforms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib14" title="">14</a>]</cite>. In particular, this work, built with the Asynchronous Multi-Body Framework (AMBF)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib15" title="">15</a>]</cite>, introduced improved robot control algorithms and teleoperation capabilities, and provided access to ground-truth imaging data. Although it enabled the collection of realistic suturing motions via teleoperation, it still lacked the capabilities to automatically generate the large-scale datasets needed for neural network training. Furthermore, the scene provided a simplified phantom, not resembling any real physical phantom, which complicated the task of physically reproducing the virtual environment.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">To address these limitations, we have developed an automated data generation pipeline on top of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib14" title="">14</a>]</cite> to produce large-scale and diverse datasets from pre-recorded trajectories generated with teleoperation. Moreover, we scanned and added a commercially available training suturing pad to our simulation environment to improve realism and to ensure that the virtual scene could be physically reproduced. The goal of these improvements was to facilitate the creation of the large-scale datasets needed for deep learning-based algorithms.</p>
</div>
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="337" id="S1.1.g1" src="extracted/5657361/figures/main_diagram_improved.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">Proposed data collection pipeline. Realistic data generation with our proposed system requires two steps. First, trajectories of the robotic manipulators are collected using a teleoperation device. Second, trajectories are replayed automatically multiple times from different camera viewpoints to generate diverse set of images. While replaying, our pipeline stores depth and segmentation maps and the ground truth pose of all the objects with respect to the camera.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1"><span class="ltx_text" id="S1.p7.1.1" style="color:#000000;">In this paper, we showcase an application of our pipeline by generating data to train a state-of-the-art 6D pose estimation network to predict the pose of a needle while performing suturing maneuvers.</span> Regarding the network architecture, the GDR-Network <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib16" title="">16</a>]</cite> was chosen as it is one of the first fully differentiable networks for the task of pose estimation from monocular RGB. We envision that our work will significantly benefit the community of surgical robotics researchers as we provide a standardized platform for generating, evaluating, and deploying novel vision algorithms. Lastly, we highlight that thanks to the modular nature of the base simulation engine, our data generation pipeline can be used for a wide range of tasks and objects in surgical robotics setups.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">In summary, this work presents the following contributions:</p>
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">An automated data generation pipeline for 6D pose estimation of surgical instruments.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">A realistic simulation environment for surgical suturing based on a commercially available suturing pad model.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">A dataset of 7.5k images with 6D pose annotations for a simulated surgical needle.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1"><span class="ltx_text" id="S1.I1.i4.p1.1.1" style="color:#000000;">Evaluations on a state-of-the-art 6D pose estimation neural network on the task of surgical needle pose estimation.</span></p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related Work</span>
</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.5.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.6.2">Offline Simulation Data Generation</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Vision Blender <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib12" title="">12</a>]</cite> is a Blender add-on for generating synthetic computer vision data such as RGB, depth, segmentation, optical flow, and surface normals. Designed as a tool to efficiently generate data for surgical robotic system development, Vision Blender supports converting generated data to Robot Operating System (ROS) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib17" title="">17</a>]</cite> messages. Another modular procedural pipeline for generating simulation data based on Blender is BlenderProc <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib13" title="">13</a>]</cite>. It shares data generation capabilities with Vision Blender, with the added feature of bounding box generation. BlenderProc also supports importing data in URDF format, expanding its capability for modeling complex robotics systems. Compared to using the native Blender Python, which demands a deep understanding of the Blender infrastructure, BlenderProc offers an intuitive Python interface to simplify the scene-building and data-acquisition process.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.5.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.6.2">Robot Simulator &amp; Digital Twins</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">NVIDIA® Isaac Sim is a scalable robotics simulator and synthetic data generator.
Powered by the GPU-accelerated physics simulation engine PhysX and physically-based rendering technology Iray,
Isaac Sim is capable of simulating physically accurate virtual environments and generating photorealistic data.
Isaac Sim offers support for Universal Scene Descriptor (USD) and Unified Robot Description Format (URDF),
enabling developers to seamlessly import intricate 3D environment definitions and robot configurations with ease.
In addition, Isaac Sim allows developers to establish connections between Isaac Sim and their custom robot applications
via integrated Robot Operating System (ROS1 &amp; ROS2) interfaces.
With extensions such as Isaac Gym <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib18" title="">18</a>]</cite> and Isaac Orbit <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib19" title="">19</a>]</cite>,
developers can efficiently test and refine their robotic systems and robot learning algorithms.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Defining multi-body robots using formats like URDF or Standard Description Format (SDF) can lead to ambiguous definitions in cases of densely connected, sparsely connected, or unconnected bodies.
To address this constraint, Munawar et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib15" title="">15</a>]</cite> introduced the Asynchronous Multi-Body Framework (AMBF), an innovative front-end description format for multi-body simulation, aimed at simulating complex closed-loop robots.
AMBF leverages Bullet Physics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib20" title="">20</a>]</cite> for its physics simulation and CHAI-3D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib21" title="">21</a>]</cite> for graphics rendering and haptic volume rendering. Within AMBF, every object features a custom OpenGL shader, facilitating diverse data generation capabilities, including RGB, depth, and segmentation maps.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">Building upon AMBF, many research efforts have emerged to advance robot-assisted surgeries. These include using AMBF to design image-guided feedback modalities<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib22" title="">22</a>]</cite>, a novel framework for skull base surgeries featuring high-precision optical tracking and real-time simulation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib23" title="">23</a>]</cite>, and a causality-driven robot tool segmentation algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib24" title="">24</a>]</cite>. In this work, we selected AMBF simulation over other simulation alternatives due to its support of a broad array of input devices, which facilitates the collection of realistic motions of the surgical instruments. In particular, its tight integration with the da Vinci Research Kit (dVRK) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib25" title="">25</a>]</cite> allows collecting robotic surgical motions with a similar setup to what is used in surgery.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Methodology</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">The primary motivation of this work was to provide a data generation tool for 6D pose estimation of surgical instruments. In this regard, we adapted an open-source simulation environment to automatically generate sequences of images of robotic-assisted surgical actions with their corresponding ground-truth maps. The proposed data generation pipeline
(See figure <a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#S1.F1" title="Figure 1 ‣ I Introduction ‣ Realistic Data Generation for 6D Pose Estimation of Surgical Instruments"><span class="ltx_text ltx_ref_tag">1</span></a>) was developed with the goal of generating programmatically large and diverse datasets. The methodology section is divided as follows. In section <a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#S3.SS1" title="III-A Data generation pipeline ‣ III Methodology ‣ Realistic Data Generation for 6D Pose Estimation of Surgical Instruments"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a>, we present the pipeline for automatic data generation. Section <a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#S3.SS2" title="III-B Improvements of virtual scene ‣ III Methodology ‣ Realistic Data Generation for 6D Pose Estimation of Surgical Instruments"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a> shows the improvements in the surgical virtual scene. Section <a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#S3.SS3" title="III-C Generated dataset for surgical needles pose detection ‣ III Methodology ‣ Realistic Data Generation for 6D Pose Estimation of Surgical Instruments"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-C</span></span></a> describes the generation of a dataset for the task of needle pose estimation. Section <a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#S3.SS4" title="III-D Selected deep learning model for 6D pose estimation ‣ III Methodology ‣ Realistic Data Generation for 6D Pose Estimation of Surgical Instruments"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-D</span></span></a> describes the pose estimation deep learning model trained to estimated the needle’s pose. Lastly, section <a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#S3.SS5" title="III-E Evaluation metrics for the pose estimates ‣ III Methodology ‣ Realistic Data Generation for 6D Pose Estimation of Surgical Instruments"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-E</span></span></a>, describes the evaluation metrics used for the predictions of the trained neural network.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Data generation pipeline</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Our proposed data generation pipeline is composed of two stages: a recording step, and a processing and generation step. During the data recording step, a teleoperation device is used to move the virtual robotic manipulators to perform the surgical task. While teleoperating, joint and Cartesian positions of the robotic manipulators, and the poses of other objects in the simulation are stored in a rosbag file<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>A rosbag is a file format used to store messages, from the Robot Operating System (ROS) middleware. It is ideal for storing trajectories from a robot.</span></span></span>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">During the processing and generation step, the stored robotic trajectories are replayed multiple times under different camera viewpoints and lighting conditions. While replaying the trajectories, a collection script stores the resulting monocular or stereoscopic RGB images with their corresponding ground-truth information, i.e., depth map, segmented images, camera intrinsic parameters, and pose of objects expressed with respect to the camera coordinate frame.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS1.SSS1.5.1.1">III-A</span>1 </span>Format for generated data</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">To store the data, it was decided to use the Benchmark for 6D Object Pose Estimation (BOP) format <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib10" title="">10</a>]</cite>. This is a standardized format adopted by several benchmark 6D pose estimation datasets such as HOPE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib26" title="">26</a>]</cite>, YCB <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib27" title="">27</a>]</cite>, and others. Moreover, it is a standard format used for an annual 6D pose competition <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib10" title="">10</a>]</cite>. In the BOP format, related data are grouped under a <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS1.p1.1.1">scene_id</span>. For our pipeline, data from each trajectory replay was stored in a different <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS1.p1.1.2">scene_id</span>.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="156" id="S3.F2.g1" src="extracted/5657361/figures/phantom_collisions.png" width="287"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S3.F2.3.2" style="font-size:90%;">(a) Visual mesh of the 3-Dmed phantom after preprocessing. (b) Simplified collision mesh composed of multiple convex subcomponents assembled into a single mesh. The collision mesh was only provided for a single ridge of the phantom.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.6.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.7.2">Improvements of virtual scene</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">To improve the realism of our virtual scenes, a commercially available suturing pad (3-Dmed, Franklin, OH, US) was added to the simulation. The suturing pad was initially MRI scanned to obtain a mesh that was preprocessed using 3D Slicer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib28" title="">28</a>]</cite> and Meshlab <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib29" title="">29</a>]</cite>. The MRI scanning was selected over other modalities as it provides higher contrast for soft tissue phantoms<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib30" title="">30</a>]</cite>. Using the resulting mesh, an AMBF Description File (ADF) is made by utilizing the Blender-AMBF addon plugin <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib14" title="">14</a>]</cite>. As observed in figure <a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#S3.F2" title="Figure 2 ‣ III-A1 Format for generated data ‣ III-A Data generation pipeline ‣ III Methodology ‣ Realistic Data Generation for 6D Pose Estimation of Surgical Instruments"><span class="ltx_text ltx_ref_tag">2</span></a>, the full-resolution suturing pad is used for visualization, while a simplified mesh made of convex subshapes is used for collision. Small corridors are left on the collision mesh to allow for needle insertions similar to the scene developed in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib14" title="">14</a>]</cite>. Collision meshes are simplified to optimize the simulation’s performance.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="214" id="S3.F3.g1" src="extracted/5657361/figures/plots/plot_ground_truth_distrubtion.png" width="269"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.3.2" style="font-size:90%;">Ground-truth distribution of the collected needle 6DoF detection dataset.</span></figcaption>
</figure>
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="192" id="S3.SS2.1.g1" src="extracted/5657361/figures/pred_vis.png" width="509"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S3.F4.3.2" style="font-size:90%;">Test set sample frames and corresponding pose prediction visualizations. Colored images show samples from the test dataset. Masks in the grayscale images are generated by projecting the needle model to the image with the ground-truth (blue mask) and the network’s estimated pose (green mask). Higher overlaps between the green and blue masks are indicative of better pose estimates.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.5.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.6.2">Generated dataset for surgical needles pose detection</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Using our improved simulation environment, we collected a dataset for the task of 6D pose estimation of an 18.65 mm surgical needle. First, we collected 6 rosbag recordings of suturing motions using a dVRK robot’s surgical console <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib25" title="">25</a>]</cite>. Two recordings were done in scene 1 and four in scene 2 (See figure <a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#S1.F1" title="Figure 1 ‣ I Introduction ‣ Realistic Data Generation for 6D Pose Estimation of Surgical Instruments"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">Each of the 6 collected recordings was then replayed on the simulator 20 times, each time from different camera positions and view angles. Data from 4 recordings were used as a training set and 2 for the testing set. Camera positions were specified in the joint space of a virtual endoscopic camera manipulator (ECM) provided by the base simulation environment. To produce unique viewpoints with every replayed recording, a small random offset was added to the selected ECM joints.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">After filtering images where the needle was not present, 6430 training and 1500 testing images with a 640x480 resolution were obtained. As observed in figure <a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#S3.F3" title="Figure 3 ‣ III-B Improvements of virtual scene ‣ III Methodology ‣ Realistic Data Generation for 6D Pose Estimation of Surgical Instruments"><span class="ltx_text ltx_ref_tag">3</span></a>, the resulting dataset is more challenging and realistic than the one presented in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib4" title="">4</a>]</cite> as the needles in the images present varying levels of occlusion and distance to the camera. The visibility fraction is calculated with</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{visibility}=\frac{\text{area of visible mask}}{\text{area of projected %
mask}}" class="ltx_Math" display="block" id="S3.E1.m1.1"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><mtext id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2a.cmml">visibility</mtext><mo id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml">=</mo><mfrac id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><mtext id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2a.cmml">area of visible mask</mtext><mtext id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3a.cmml">area of projected mask</mtext></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><eq id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"></eq><ci id="S3.E1.m1.1.1.2a.cmml" xref="S3.E1.m1.1.1.2"><mtext id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2">visibility</mtext></ci><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><divide id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3"></divide><ci id="S3.E1.m1.1.1.3.2a.cmml" xref="S3.E1.m1.1.1.3.2"><mtext id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2">area of visible mask</mtext></ci><ci id="S3.E1.m1.1.1.3.3a.cmml" xref="S3.E1.m1.1.1.3.3"><mtext id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3">area of projected mask</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\text{visibility}=\frac{\text{area of visible mask}}{\text{area of projected %
mask}}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.1d">visibility = divide start_ARG area of visible mask end_ARG start_ARG area of projected mask end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p5">
<p class="ltx_p" id="S3.SS3.p5.1"><span class="ltx_text" id="S3.SS3.p5.1.1" style="color:#000000;">where the visible mask is the set of pixels in the RGB image that correspond to the needle and the projected mask is the set of pixels obtained by projecting the needle’s CAD model to the image plane using the ground-truth pose.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS4.5.1.1">III-D</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS4.6.2">Selected deep learning model for 6D pose estimation</span>
</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Using the dataset described in section <a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#S3.SS3" title="III-C Generated dataset for surgical needles pose detection ‣ III Methodology ‣ Realistic Data Generation for 6D Pose Estimation of Surgical Instruments"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-C</span></span></a>, we trained the state-of-the-art network for 6D pose estimation GDR-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib16" title="">16</a>]</cite>. This network was selected as it is one of the first fully differentiable pose estimation methods in the literature and the winner of the BOP pose estimation competition of 2022 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib10" title="">10</a>]</cite>. This network receives as an input a 2D RGB region, where the object of interest is located, and outputs three intermediate geometric feature maps: the <span class="ltx_text ltx_font_italic" id="S3.SS4.p1.1.1">visible object mask</span>, a map of 2d-3d dense correspondences, and a surface region attention map. These intermediate maps are concatenated and then given as input to a fully-differentiable Patch-PnP module that regresses the final rotation and translation of the object.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">Training of this network can be performed in an end-to-end manner and only requires the RGB image and the object CAD model to generate ground truth for the intermediate geometric maps. As mentioned above, the network requires a region of interest (ROI) where the object is located. To obtain these ROIs during our experiments, the off-the-shelf detector YOLOX <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib31" title="">31</a>]</cite> was also trained with our dataset for the task of 2D bounding box detection for the needle.</p>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="280" id="S3.F5.g1" src="extracted/5657361/figures/plots/plot_gdrnet_test_results_hist.png" width="269"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S3.F5.3.2" style="font-size:90%;">Error distribution for the best GDRNet model on the test dataset. The x-axis represents the different error metrics, and the y-axis is the number of samples within each bin. The red dotted line indicates the median performance. The x-axes of the histograms were truncated respectively at 70 mm, 15 deg and 10 mm for visualization purposes.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS5.5.1.1">III-E</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS5.6.2">Evaluation metrics for the pose estimates</span>
</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.9">Pose estimations from the neural network were evaluated using three common error metrics: (1) translation error (<math alttext="e_{TE}" class="ltx_Math" display="inline" id="S3.SS5.p1.1.m1.1"><semantics id="S3.SS5.p1.1.m1.1a"><msub id="S3.SS5.p1.1.m1.1.1" xref="S3.SS5.p1.1.m1.1.1.cmml"><mi id="S3.SS5.p1.1.m1.1.1.2" xref="S3.SS5.p1.1.m1.1.1.2.cmml">e</mi><mrow id="S3.SS5.p1.1.m1.1.1.3" xref="S3.SS5.p1.1.m1.1.1.3.cmml"><mi id="S3.SS5.p1.1.m1.1.1.3.2" xref="S3.SS5.p1.1.m1.1.1.3.2.cmml">T</mi><mo id="S3.SS5.p1.1.m1.1.1.3.1" xref="S3.SS5.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS5.p1.1.m1.1.1.3.3" xref="S3.SS5.p1.1.m1.1.1.3.3.cmml">E</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.1.m1.1b"><apply id="S3.SS5.p1.1.m1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.1.m1.1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS5.p1.1.m1.1.1.2.cmml" xref="S3.SS5.p1.1.m1.1.1.2">𝑒</ci><apply id="S3.SS5.p1.1.m1.1.1.3.cmml" xref="S3.SS5.p1.1.m1.1.1.3"><times id="S3.SS5.p1.1.m1.1.1.3.1.cmml" xref="S3.SS5.p1.1.m1.1.1.3.1"></times><ci id="S3.SS5.p1.1.m1.1.1.3.2.cmml" xref="S3.SS5.p1.1.m1.1.1.3.2">𝑇</ci><ci id="S3.SS5.p1.1.m1.1.1.3.3.cmml" xref="S3.SS5.p1.1.m1.1.1.3.3">𝐸</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.1.m1.1c">e_{TE}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.1.m1.1d">italic_e start_POSTSUBSCRIPT italic_T italic_E end_POSTSUBSCRIPT</annotation></semantics></math>), (2) rotation error (<math alttext="e_{RE}" class="ltx_Math" display="inline" id="S3.SS5.p1.2.m2.1"><semantics id="S3.SS5.p1.2.m2.1a"><msub id="S3.SS5.p1.2.m2.1.1" xref="S3.SS5.p1.2.m2.1.1.cmml"><mi id="S3.SS5.p1.2.m2.1.1.2" xref="S3.SS5.p1.2.m2.1.1.2.cmml">e</mi><mrow id="S3.SS5.p1.2.m2.1.1.3" xref="S3.SS5.p1.2.m2.1.1.3.cmml"><mi id="S3.SS5.p1.2.m2.1.1.3.2" xref="S3.SS5.p1.2.m2.1.1.3.2.cmml">R</mi><mo id="S3.SS5.p1.2.m2.1.1.3.1" xref="S3.SS5.p1.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S3.SS5.p1.2.m2.1.1.3.3" xref="S3.SS5.p1.2.m2.1.1.3.3.cmml">E</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.2.m2.1b"><apply id="S3.SS5.p1.2.m2.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.2.m2.1.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS5.p1.2.m2.1.1.2.cmml" xref="S3.SS5.p1.2.m2.1.1.2">𝑒</ci><apply id="S3.SS5.p1.2.m2.1.1.3.cmml" xref="S3.SS5.p1.2.m2.1.1.3"><times id="S3.SS5.p1.2.m2.1.1.3.1.cmml" xref="S3.SS5.p1.2.m2.1.1.3.1"></times><ci id="S3.SS5.p1.2.m2.1.1.3.2.cmml" xref="S3.SS5.p1.2.m2.1.1.3.2">𝑅</ci><ci id="S3.SS5.p1.2.m2.1.1.3.3.cmml" xref="S3.SS5.p1.2.m2.1.1.3.3">𝐸</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.2.m2.1c">e_{RE}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.2.m2.1d">italic_e start_POSTSUBSCRIPT italic_R italic_E end_POSTSUBSCRIPT</annotation></semantics></math>)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib32" title="">32</a>]</cite>, and (3) Maximum Symmetry-Aware Surface Distance (<math alttext="e_{MSSD}" class="ltx_Math" display="inline" id="S3.SS5.p1.3.m3.1"><semantics id="S3.SS5.p1.3.m3.1a"><msub id="S3.SS5.p1.3.m3.1.1" xref="S3.SS5.p1.3.m3.1.1.cmml"><mi id="S3.SS5.p1.3.m3.1.1.2" xref="S3.SS5.p1.3.m3.1.1.2.cmml">e</mi><mrow id="S3.SS5.p1.3.m3.1.1.3" xref="S3.SS5.p1.3.m3.1.1.3.cmml"><mi id="S3.SS5.p1.3.m3.1.1.3.2" xref="S3.SS5.p1.3.m3.1.1.3.2.cmml">M</mi><mo id="S3.SS5.p1.3.m3.1.1.3.1" xref="S3.SS5.p1.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.SS5.p1.3.m3.1.1.3.3" xref="S3.SS5.p1.3.m3.1.1.3.3.cmml">S</mi><mo id="S3.SS5.p1.3.m3.1.1.3.1a" xref="S3.SS5.p1.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.SS5.p1.3.m3.1.1.3.4" xref="S3.SS5.p1.3.m3.1.1.3.4.cmml">S</mi><mo id="S3.SS5.p1.3.m3.1.1.3.1b" xref="S3.SS5.p1.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.SS5.p1.3.m3.1.1.3.5" xref="S3.SS5.p1.3.m3.1.1.3.5.cmml">D</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.3.m3.1b"><apply id="S3.SS5.p1.3.m3.1.1.cmml" xref="S3.SS5.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.3.m3.1.1.1.cmml" xref="S3.SS5.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS5.p1.3.m3.1.1.2.cmml" xref="S3.SS5.p1.3.m3.1.1.2">𝑒</ci><apply id="S3.SS5.p1.3.m3.1.1.3.cmml" xref="S3.SS5.p1.3.m3.1.1.3"><times id="S3.SS5.p1.3.m3.1.1.3.1.cmml" xref="S3.SS5.p1.3.m3.1.1.3.1"></times><ci id="S3.SS5.p1.3.m3.1.1.3.2.cmml" xref="S3.SS5.p1.3.m3.1.1.3.2">𝑀</ci><ci id="S3.SS5.p1.3.m3.1.1.3.3.cmml" xref="S3.SS5.p1.3.m3.1.1.3.3">𝑆</ci><ci id="S3.SS5.p1.3.m3.1.1.3.4.cmml" xref="S3.SS5.p1.3.m3.1.1.3.4">𝑆</ci><ci id="S3.SS5.p1.3.m3.1.1.3.5.cmml" xref="S3.SS5.p1.3.m3.1.1.3.5">𝐷</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.3.m3.1c">e_{MSSD}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.3.m3.1d">italic_e start_POSTSUBSCRIPT italic_M italic_S italic_S italic_D end_POSTSUBSCRIPT</annotation></semantics></math>) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib33" title="">33</a>]</cite>. <span class="ltx_text" id="S3.SS5.p1.9.1" style="color:#000000;">Metric 1 measures the translational error using the Euclidean distance. Metric 2 measures the rotational error using the axis angle representation of rotation matrices. Lastly, metric 3 measures the maximum distance between a vertex of the object model transformed with the ground truth and estimated pose.</span> Given a ground truth pose <math alttext="\bar{P}=(\bar{R},\bar{t}" class="ltx_math_unparsed" display="inline" id="S3.SS5.p1.4.m4.2"><semantics id="S3.SS5.p1.4.m4.2a"><mrow id="S3.SS5.p1.4.m4.2b"><mover accent="true" id="S3.SS5.p1.4.m4.2.3"><mi id="S3.SS5.p1.4.m4.2.3.2">P</mi><mo id="S3.SS5.p1.4.m4.2.3.1">¯</mo></mover><mo id="S3.SS5.p1.4.m4.2.4">=</mo><mrow id="S3.SS5.p1.4.m4.2.5"><mo id="S3.SS5.p1.4.m4.2.5.1" stretchy="false">(</mo><mover accent="true" id="S3.SS5.p1.4.m4.1.1"><mi id="S3.SS5.p1.4.m4.1.1.2">R</mi><mo id="S3.SS5.p1.4.m4.1.1.1">¯</mo></mover><mo id="S3.SS5.p1.4.m4.2.5.2">,</mo><mover accent="true" id="S3.SS5.p1.4.m4.2.2"><mi id="S3.SS5.p1.4.m4.2.2.2">t</mi><mo id="S3.SS5.p1.4.m4.2.2.1">¯</mo></mover></mrow></mrow><annotation encoding="application/x-tex" id="S3.SS5.p1.4.m4.2c">\bar{P}=(\bar{R},\bar{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.4.m4.2d">over¯ start_ARG italic_P end_ARG = ( over¯ start_ARG italic_R end_ARG , over¯ start_ARG italic_t end_ARG</annotation></semantics></math>), an estimated pose <math alttext="\hat{P}=(\hat{R},\hat{t})" class="ltx_Math" display="inline" id="S3.SS5.p1.5.m5.2"><semantics id="S3.SS5.p1.5.m5.2a"><mrow id="S3.SS5.p1.5.m5.2.3" xref="S3.SS5.p1.5.m5.2.3.cmml"><mover accent="true" id="S3.SS5.p1.5.m5.2.3.2" xref="S3.SS5.p1.5.m5.2.3.2.cmml"><mi id="S3.SS5.p1.5.m5.2.3.2.2" xref="S3.SS5.p1.5.m5.2.3.2.2.cmml">P</mi><mo id="S3.SS5.p1.5.m5.2.3.2.1" xref="S3.SS5.p1.5.m5.2.3.2.1.cmml">^</mo></mover><mo id="S3.SS5.p1.5.m5.2.3.1" xref="S3.SS5.p1.5.m5.2.3.1.cmml">=</mo><mrow id="S3.SS5.p1.5.m5.2.3.3.2" xref="S3.SS5.p1.5.m5.2.3.3.1.cmml"><mo id="S3.SS5.p1.5.m5.2.3.3.2.1" stretchy="false" xref="S3.SS5.p1.5.m5.2.3.3.1.cmml">(</mo><mover accent="true" id="S3.SS5.p1.5.m5.1.1" xref="S3.SS5.p1.5.m5.1.1.cmml"><mi id="S3.SS5.p1.5.m5.1.1.2" xref="S3.SS5.p1.5.m5.1.1.2.cmml">R</mi><mo id="S3.SS5.p1.5.m5.1.1.1" xref="S3.SS5.p1.5.m5.1.1.1.cmml">^</mo></mover><mo id="S3.SS5.p1.5.m5.2.3.3.2.2" xref="S3.SS5.p1.5.m5.2.3.3.1.cmml">,</mo><mover accent="true" id="S3.SS5.p1.5.m5.2.2" xref="S3.SS5.p1.5.m5.2.2.cmml"><mi id="S3.SS5.p1.5.m5.2.2.2" xref="S3.SS5.p1.5.m5.2.2.2.cmml">t</mi><mo id="S3.SS5.p1.5.m5.2.2.1" xref="S3.SS5.p1.5.m5.2.2.1.cmml">^</mo></mover><mo id="S3.SS5.p1.5.m5.2.3.3.2.3" stretchy="false" xref="S3.SS5.p1.5.m5.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.5.m5.2b"><apply id="S3.SS5.p1.5.m5.2.3.cmml" xref="S3.SS5.p1.5.m5.2.3"><eq id="S3.SS5.p1.5.m5.2.3.1.cmml" xref="S3.SS5.p1.5.m5.2.3.1"></eq><apply id="S3.SS5.p1.5.m5.2.3.2.cmml" xref="S3.SS5.p1.5.m5.2.3.2"><ci id="S3.SS5.p1.5.m5.2.3.2.1.cmml" xref="S3.SS5.p1.5.m5.2.3.2.1">^</ci><ci id="S3.SS5.p1.5.m5.2.3.2.2.cmml" xref="S3.SS5.p1.5.m5.2.3.2.2">𝑃</ci></apply><interval closure="open" id="S3.SS5.p1.5.m5.2.3.3.1.cmml" xref="S3.SS5.p1.5.m5.2.3.3.2"><apply id="S3.SS5.p1.5.m5.1.1.cmml" xref="S3.SS5.p1.5.m5.1.1"><ci id="S3.SS5.p1.5.m5.1.1.1.cmml" xref="S3.SS5.p1.5.m5.1.1.1">^</ci><ci id="S3.SS5.p1.5.m5.1.1.2.cmml" xref="S3.SS5.p1.5.m5.1.1.2">𝑅</ci></apply><apply id="S3.SS5.p1.5.m5.2.2.cmml" xref="S3.SS5.p1.5.m5.2.2"><ci id="S3.SS5.p1.5.m5.2.2.1.cmml" xref="S3.SS5.p1.5.m5.2.2.1">^</ci><ci id="S3.SS5.p1.5.m5.2.2.2.cmml" xref="S3.SS5.p1.5.m5.2.2.2">𝑡</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.5.m5.2c">\hat{P}=(\hat{R},\hat{t})</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.5.m5.2d">over^ start_ARG italic_P end_ARG = ( over^ start_ARG italic_R end_ARG , over^ start_ARG italic_t end_ARG )</annotation></semantics></math>, and a set of vertices <math alttext="V_{M}" class="ltx_Math" display="inline" id="S3.SS5.p1.6.m6.1"><semantics id="S3.SS5.p1.6.m6.1a"><msub id="S3.SS5.p1.6.m6.1.1" xref="S3.SS5.p1.6.m6.1.1.cmml"><mi id="S3.SS5.p1.6.m6.1.1.2" xref="S3.SS5.p1.6.m6.1.1.2.cmml">V</mi><mi id="S3.SS5.p1.6.m6.1.1.3" xref="S3.SS5.p1.6.m6.1.1.3.cmml">M</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.6.m6.1b"><apply id="S3.SS5.p1.6.m6.1.1.cmml" xref="S3.SS5.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.6.m6.1.1.1.cmml" xref="S3.SS5.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS5.p1.6.m6.1.1.2.cmml" xref="S3.SS5.p1.6.m6.1.1.2">𝑉</ci><ci id="S3.SS5.p1.6.m6.1.1.3.cmml" xref="S3.SS5.p1.6.m6.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.6.m6.1c">V_{M}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.6.m6.1d">italic_V start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT</annotation></semantics></math> belonging to the object model, the metrics <math alttext="e_{TE}" class="ltx_Math" display="inline" id="S3.SS5.p1.7.m7.1"><semantics id="S3.SS5.p1.7.m7.1a"><msub id="S3.SS5.p1.7.m7.1.1" xref="S3.SS5.p1.7.m7.1.1.cmml"><mi id="S3.SS5.p1.7.m7.1.1.2" xref="S3.SS5.p1.7.m7.1.1.2.cmml">e</mi><mrow id="S3.SS5.p1.7.m7.1.1.3" xref="S3.SS5.p1.7.m7.1.1.3.cmml"><mi id="S3.SS5.p1.7.m7.1.1.3.2" xref="S3.SS5.p1.7.m7.1.1.3.2.cmml">T</mi><mo id="S3.SS5.p1.7.m7.1.1.3.1" xref="S3.SS5.p1.7.m7.1.1.3.1.cmml">⁢</mo><mi id="S3.SS5.p1.7.m7.1.1.3.3" xref="S3.SS5.p1.7.m7.1.1.3.3.cmml">E</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.7.m7.1b"><apply id="S3.SS5.p1.7.m7.1.1.cmml" xref="S3.SS5.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.7.m7.1.1.1.cmml" xref="S3.SS5.p1.7.m7.1.1">subscript</csymbol><ci id="S3.SS5.p1.7.m7.1.1.2.cmml" xref="S3.SS5.p1.7.m7.1.1.2">𝑒</ci><apply id="S3.SS5.p1.7.m7.1.1.3.cmml" xref="S3.SS5.p1.7.m7.1.1.3"><times id="S3.SS5.p1.7.m7.1.1.3.1.cmml" xref="S3.SS5.p1.7.m7.1.1.3.1"></times><ci id="S3.SS5.p1.7.m7.1.1.3.2.cmml" xref="S3.SS5.p1.7.m7.1.1.3.2">𝑇</ci><ci id="S3.SS5.p1.7.m7.1.1.3.3.cmml" xref="S3.SS5.p1.7.m7.1.1.3.3">𝐸</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.7.m7.1c">e_{TE}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.7.m7.1d">italic_e start_POSTSUBSCRIPT italic_T italic_E end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="e_{RE}" class="ltx_Math" display="inline" id="S3.SS5.p1.8.m8.1"><semantics id="S3.SS5.p1.8.m8.1a"><msub id="S3.SS5.p1.8.m8.1.1" xref="S3.SS5.p1.8.m8.1.1.cmml"><mi id="S3.SS5.p1.8.m8.1.1.2" xref="S3.SS5.p1.8.m8.1.1.2.cmml">e</mi><mrow id="S3.SS5.p1.8.m8.1.1.3" xref="S3.SS5.p1.8.m8.1.1.3.cmml"><mi id="S3.SS5.p1.8.m8.1.1.3.2" xref="S3.SS5.p1.8.m8.1.1.3.2.cmml">R</mi><mo id="S3.SS5.p1.8.m8.1.1.3.1" xref="S3.SS5.p1.8.m8.1.1.3.1.cmml">⁢</mo><mi id="S3.SS5.p1.8.m8.1.1.3.3" xref="S3.SS5.p1.8.m8.1.1.3.3.cmml">E</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.8.m8.1b"><apply id="S3.SS5.p1.8.m8.1.1.cmml" xref="S3.SS5.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.8.m8.1.1.1.cmml" xref="S3.SS5.p1.8.m8.1.1">subscript</csymbol><ci id="S3.SS5.p1.8.m8.1.1.2.cmml" xref="S3.SS5.p1.8.m8.1.1.2">𝑒</ci><apply id="S3.SS5.p1.8.m8.1.1.3.cmml" xref="S3.SS5.p1.8.m8.1.1.3"><times id="S3.SS5.p1.8.m8.1.1.3.1.cmml" xref="S3.SS5.p1.8.m8.1.1.3.1"></times><ci id="S3.SS5.p1.8.m8.1.1.3.2.cmml" xref="S3.SS5.p1.8.m8.1.1.3.2">𝑅</ci><ci id="S3.SS5.p1.8.m8.1.1.3.3.cmml" xref="S3.SS5.p1.8.m8.1.1.3.3">𝐸</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.8.m8.1c">e_{RE}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.8.m8.1d">italic_e start_POSTSUBSCRIPT italic_R italic_E end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="e_{MSSD}" class="ltx_Math" display="inline" id="S3.SS5.p1.9.m9.1"><semantics id="S3.SS5.p1.9.m9.1a"><msub id="S3.SS5.p1.9.m9.1.1" xref="S3.SS5.p1.9.m9.1.1.cmml"><mi id="S3.SS5.p1.9.m9.1.1.2" xref="S3.SS5.p1.9.m9.1.1.2.cmml">e</mi><mrow id="S3.SS5.p1.9.m9.1.1.3" xref="S3.SS5.p1.9.m9.1.1.3.cmml"><mi id="S3.SS5.p1.9.m9.1.1.3.2" xref="S3.SS5.p1.9.m9.1.1.3.2.cmml">M</mi><mo id="S3.SS5.p1.9.m9.1.1.3.1" xref="S3.SS5.p1.9.m9.1.1.3.1.cmml">⁢</mo><mi id="S3.SS5.p1.9.m9.1.1.3.3" xref="S3.SS5.p1.9.m9.1.1.3.3.cmml">S</mi><mo id="S3.SS5.p1.9.m9.1.1.3.1a" xref="S3.SS5.p1.9.m9.1.1.3.1.cmml">⁢</mo><mi id="S3.SS5.p1.9.m9.1.1.3.4" xref="S3.SS5.p1.9.m9.1.1.3.4.cmml">S</mi><mo id="S3.SS5.p1.9.m9.1.1.3.1b" xref="S3.SS5.p1.9.m9.1.1.3.1.cmml">⁢</mo><mi id="S3.SS5.p1.9.m9.1.1.3.5" xref="S3.SS5.p1.9.m9.1.1.3.5.cmml">D</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.9.m9.1b"><apply id="S3.SS5.p1.9.m9.1.1.cmml" xref="S3.SS5.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.9.m9.1.1.1.cmml" xref="S3.SS5.p1.9.m9.1.1">subscript</csymbol><ci id="S3.SS5.p1.9.m9.1.1.2.cmml" xref="S3.SS5.p1.9.m9.1.1.2">𝑒</ci><apply id="S3.SS5.p1.9.m9.1.1.3.cmml" xref="S3.SS5.p1.9.m9.1.1.3"><times id="S3.SS5.p1.9.m9.1.1.3.1.cmml" xref="S3.SS5.p1.9.m9.1.1.3.1"></times><ci id="S3.SS5.p1.9.m9.1.1.3.2.cmml" xref="S3.SS5.p1.9.m9.1.1.3.2">𝑀</ci><ci id="S3.SS5.p1.9.m9.1.1.3.3.cmml" xref="S3.SS5.p1.9.m9.1.1.3.3">𝑆</ci><ci id="S3.SS5.p1.9.m9.1.1.3.4.cmml" xref="S3.SS5.p1.9.m9.1.1.3.4">𝑆</ci><ci id="S3.SS5.p1.9.m9.1.1.3.5.cmml" xref="S3.SS5.p1.9.m9.1.1.3.5">𝐷</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.9.m9.1c">e_{MSSD}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.9.m9.1d">italic_e start_POSTSUBSCRIPT italic_M italic_S italic_S italic_D end_POSTSUBSCRIPT</annotation></semantics></math> can be calculated with</p>
</div>
<div class="ltx_para" id="S3.SS5.p2">
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="e_{TE}=||\bar{t}-\hat{t}||" class="ltx_Math" display="block" id="S3.E2.m1.1"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><msub id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.3.2" xref="S3.E2.m1.1.1.3.2.cmml">e</mi><mrow id="S3.E2.m1.1.1.3.3" xref="S3.E2.m1.1.1.3.3.cmml"><mi id="S3.E2.m1.1.1.3.3.2" xref="S3.E2.m1.1.1.3.3.2.cmml">T</mi><mo id="S3.E2.m1.1.1.3.3.1" xref="S3.E2.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E2.m1.1.1.3.3.3" xref="S3.E2.m1.1.1.3.3.3.cmml">E</mi></mrow></msub><mo id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.2.cmml"><mo id="S3.E2.m1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.cmml"><mover accent="true" id="S3.E2.m1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.1.1.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.1.2.2.cmml">t</mi><mo id="S3.E2.m1.1.1.1.1.1.2.1" xref="S3.E2.m1.1.1.1.1.1.2.1.cmml">¯</mo></mover><mo id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml">−</mo><mover accent="true" id="S3.E2.m1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.3.2.cmml">t</mi><mo id="S3.E2.m1.1.1.1.1.1.3.1" xref="S3.E2.m1.1.1.1.1.1.3.1.cmml">^</mo></mover></mrow><mo id="S3.E2.m1.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.1.1.1.2.1.cmml">‖</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><eq id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2"></eq><apply id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.3.2">𝑒</ci><apply id="S3.E2.m1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.3.3"><times id="S3.E2.m1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.3.3.1"></times><ci id="S3.E2.m1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.3.3.2">𝑇</ci><ci id="S3.E2.m1.1.1.3.3.3.cmml" xref="S3.E2.m1.1.1.3.3.3">𝐸</ci></apply></apply><apply id="S3.E2.m1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2">norm</csymbol><apply id="S3.E2.m1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"><minus id="S3.E2.m1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1"></minus><apply id="S3.E2.m1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.2"><ci id="S3.E2.m1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.2.1">¯</ci><ci id="S3.E2.m1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.2.2">𝑡</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.3"><ci id="S3.E2.m1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.3.1">^</ci><ci id="S3.E2.m1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.3.2">𝑡</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">e_{TE}=||\bar{t}-\hat{t}||</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.1d">italic_e start_POSTSUBSCRIPT italic_T italic_E end_POSTSUBSCRIPT = | | over¯ start_ARG italic_t end_ARG - over^ start_ARG italic_t end_ARG | |</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS5.p3">
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="e_{RE}=arccos((Tr(\bar{R}\hat{R}^{-1}-1)/2)" class="ltx_math_unparsed" display="block" id="S3.E3.m1.1"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1b"><msub id="S3.E3.m1.1.1"><mi id="S3.E3.m1.1.1.2">e</mi><mrow id="S3.E3.m1.1.1.3"><mi id="S3.E3.m1.1.1.3.2">R</mi><mo id="S3.E3.m1.1.1.3.1">⁢</mo><mi id="S3.E3.m1.1.1.3.3">E</mi></mrow></msub><mo id="S3.E3.m1.1.2">=</mo><mi id="S3.E3.m1.1.3">a</mi><mi id="S3.E3.m1.1.4">r</mi><mi id="S3.E3.m1.1.5">c</mi><mi id="S3.E3.m1.1.6">c</mi><mi id="S3.E3.m1.1.7">o</mi><mi id="S3.E3.m1.1.8">s</mi><mrow id="S3.E3.m1.1.9"><mo id="S3.E3.m1.1.9.1" stretchy="false">(</mo><mrow id="S3.E3.m1.1.9.2"><mo id="S3.E3.m1.1.9.2.1" stretchy="false">(</mo><mi id="S3.E3.m1.1.9.2.2">T</mi><mi id="S3.E3.m1.1.9.2.3">r</mi><mrow id="S3.E3.m1.1.9.2.4"><mo id="S3.E3.m1.1.9.2.4.1" stretchy="false">(</mo><mover accent="true" id="S3.E3.m1.1.9.2.4.2"><mi id="S3.E3.m1.1.9.2.4.2.2">R</mi><mo id="S3.E3.m1.1.9.2.4.2.1">¯</mo></mover><msup id="S3.E3.m1.1.9.2.4.3"><mover accent="true" id="S3.E3.m1.1.9.2.4.3.2"><mi id="S3.E3.m1.1.9.2.4.3.2.2">R</mi><mo id="S3.E3.m1.1.9.2.4.3.2.1">^</mo></mover><mrow id="S3.E3.m1.1.9.2.4.3.3"><mo id="S3.E3.m1.1.9.2.4.3.3a">−</mo><mn id="S3.E3.m1.1.9.2.4.3.3.2">1</mn></mrow></msup><mo id="S3.E3.m1.1.9.2.4.4">−</mo><mn id="S3.E3.m1.1.9.2.4.5">1</mn><mo id="S3.E3.m1.1.9.2.4.6" stretchy="false">)</mo></mrow><mo id="S3.E3.m1.1.9.2.5">/</mo><mn id="S3.E3.m1.1.9.2.6">2</mn><mo id="S3.E3.m1.1.9.2.7" stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex" id="S3.E3.m1.1c">e_{RE}=arccos((Tr(\bar{R}\hat{R}^{-1}-1)/2)</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.1d">italic_e start_POSTSUBSCRIPT italic_R italic_E end_POSTSUBSCRIPT = italic_a italic_r italic_c italic_c italic_o italic_s ( ( italic_T italic_r ( over¯ start_ARG italic_R end_ARG over^ start_ARG italic_R end_ARG start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT - 1 ) / 2 )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS5.p4">
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="e_{MSSD}=\min_{\mathbf{S}\in S_{M}}\max_{\mathbf{x}\in V_{M}}\|\hat{\mathbf{P}%
}\mathbf{x}-\overline{\mathbf{P}}\mathbf{Sx}\|_{2}" class="ltx_Math" display="block" id="S3.E4.m1.1"><semantics id="S3.E4.m1.1a"><mrow id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml"><msub id="S3.E4.m1.1.1.3" xref="S3.E4.m1.1.1.3.cmml"><mi id="S3.E4.m1.1.1.3.2" xref="S3.E4.m1.1.1.3.2.cmml">e</mi><mrow id="S3.E4.m1.1.1.3.3" xref="S3.E4.m1.1.1.3.3.cmml"><mi id="S3.E4.m1.1.1.3.3.2" xref="S3.E4.m1.1.1.3.3.2.cmml">M</mi><mo id="S3.E4.m1.1.1.3.3.1" xref="S3.E4.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E4.m1.1.1.3.3.3" xref="S3.E4.m1.1.1.3.3.3.cmml">S</mi><mo id="S3.E4.m1.1.1.3.3.1a" xref="S3.E4.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E4.m1.1.1.3.3.4" xref="S3.E4.m1.1.1.3.3.4.cmml">S</mi><mo id="S3.E4.m1.1.1.3.3.1b" xref="S3.E4.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E4.m1.1.1.3.3.5" xref="S3.E4.m1.1.1.3.3.5.cmml">D</mi></mrow></msub><mo id="S3.E4.m1.1.1.2" xref="S3.E4.m1.1.1.2.cmml">=</mo><mrow id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.cmml"><munder id="S3.E4.m1.1.1.1.2" xref="S3.E4.m1.1.1.1.2.cmml"><mi id="S3.E4.m1.1.1.1.2.2" xref="S3.E4.m1.1.1.1.2.2.cmml">min</mi><mrow id="S3.E4.m1.1.1.1.2.3" xref="S3.E4.m1.1.1.1.2.3.cmml"><mi id="S3.E4.m1.1.1.1.2.3.2" xref="S3.E4.m1.1.1.1.2.3.2.cmml">𝐒</mi><mo id="S3.E4.m1.1.1.1.2.3.1" xref="S3.E4.m1.1.1.1.2.3.1.cmml">∈</mo><msub id="S3.E4.m1.1.1.1.2.3.3" xref="S3.E4.m1.1.1.1.2.3.3.cmml"><mi id="S3.E4.m1.1.1.1.2.3.3.2" xref="S3.E4.m1.1.1.1.2.3.3.2.cmml">S</mi><mi id="S3.E4.m1.1.1.1.2.3.3.3" xref="S3.E4.m1.1.1.1.2.3.3.3.cmml">M</mi></msub></mrow></munder><mo id="S3.E4.m1.1.1.1a" lspace="0.167em" xref="S3.E4.m1.1.1.1.cmml">⁡</mo><mrow id="S3.E4.m1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml"><munder id="S3.E4.m1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.2.cmml"><mi id="S3.E4.m1.1.1.1.1.2.2" xref="S3.E4.m1.1.1.1.1.2.2.cmml">max</mi><mrow id="S3.E4.m1.1.1.1.1.2.3" xref="S3.E4.m1.1.1.1.1.2.3.cmml"><mi id="S3.E4.m1.1.1.1.1.2.3.2" xref="S3.E4.m1.1.1.1.1.2.3.2.cmml">𝐱</mi><mo id="S3.E4.m1.1.1.1.1.2.3.1" xref="S3.E4.m1.1.1.1.1.2.3.1.cmml">∈</mo><msub id="S3.E4.m1.1.1.1.1.2.3.3" xref="S3.E4.m1.1.1.1.1.2.3.3.cmml"><mi id="S3.E4.m1.1.1.1.1.2.3.3.2" xref="S3.E4.m1.1.1.1.1.2.3.3.2.cmml">V</mi><mi id="S3.E4.m1.1.1.1.1.2.3.3.3" xref="S3.E4.m1.1.1.1.1.2.3.3.3.cmml">M</mi></msub></mrow></munder><mo id="S3.E4.m1.1.1.1.1a" xref="S3.E4.m1.1.1.1.1.cmml">⁡</mo><msub id="S3.E4.m1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.cmml"><mrow id="S3.E4.m1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.2.cmml"><mo id="S3.E4.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E4.m1.1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.E4.m1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E4.m1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2.cmml"><mover accent="true" id="S3.E4.m1.1.1.1.1.1.1.1.1.2.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2.2.2.cmml">𝐏</mi><mo id="S3.E4.m1.1.1.1.1.1.1.1.1.2.2.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2.2.1.cmml">^</mo></mover><mo id="S3.E4.m1.1.1.1.1.1.1.1.1.2.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.2.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2.3.cmml">𝐱</mi></mrow><mo id="S3.E4.m1.1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml">−</mo><mrow id="S3.E4.m1.1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.cmml"><mover accent="true" id="S3.E4.m1.1.1.1.1.1.1.1.1.3.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.2.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.3.2.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.2.2.cmml">𝐏</mi><mo id="S3.E4.m1.1.1.1.1.1.1.1.1.3.2.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.2.1.cmml">¯</mo></mover><mo id="S3.E4.m1.1.1.1.1.1.1.1.1.3.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.3.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.3.cmml">𝐒𝐱</mi></mrow></mrow><mo id="S3.E4.m1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E4.m1.1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S3.E4.m1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.3.cmml">2</mn></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1"><eq id="S3.E4.m1.1.1.2.cmml" xref="S3.E4.m1.1.1.2"></eq><apply id="S3.E4.m1.1.1.3.cmml" xref="S3.E4.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.3.2">𝑒</ci><apply id="S3.E4.m1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.3.3"><times id="S3.E4.m1.1.1.3.3.1.cmml" xref="S3.E4.m1.1.1.3.3.1"></times><ci id="S3.E4.m1.1.1.3.3.2.cmml" xref="S3.E4.m1.1.1.3.3.2">𝑀</ci><ci id="S3.E4.m1.1.1.3.3.3.cmml" xref="S3.E4.m1.1.1.3.3.3">𝑆</ci><ci id="S3.E4.m1.1.1.3.3.4.cmml" xref="S3.E4.m1.1.1.3.3.4">𝑆</ci><ci id="S3.E4.m1.1.1.3.3.5.cmml" xref="S3.E4.m1.1.1.3.3.5">𝐷</ci></apply></apply><apply id="S3.E4.m1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"><apply id="S3.E4.m1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.1.2">subscript</csymbol><min id="S3.E4.m1.1.1.1.2.2.cmml" xref="S3.E4.m1.1.1.1.2.2"></min><apply id="S3.E4.m1.1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.1.2.3"><in id="S3.E4.m1.1.1.1.2.3.1.cmml" xref="S3.E4.m1.1.1.1.2.3.1"></in><ci id="S3.E4.m1.1.1.1.2.3.2.cmml" xref="S3.E4.m1.1.1.1.2.3.2">𝐒</ci><apply id="S3.E4.m1.1.1.1.2.3.3.cmml" xref="S3.E4.m1.1.1.1.2.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.2.3.3.1.cmml" xref="S3.E4.m1.1.1.1.2.3.3">subscript</csymbol><ci id="S3.E4.m1.1.1.1.2.3.3.2.cmml" xref="S3.E4.m1.1.1.1.2.3.3.2">𝑆</ci><ci id="S3.E4.m1.1.1.1.2.3.3.3.cmml" xref="S3.E4.m1.1.1.1.2.3.3.3">𝑀</ci></apply></apply></apply><apply id="S3.E4.m1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1"><apply id="S3.E4.m1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.1.1.2">subscript</csymbol><max id="S3.E4.m1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.1.1.1.1.2.2"></max><apply id="S3.E4.m1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.1.1.2.3"><in id="S3.E4.m1.1.1.1.1.2.3.1.cmml" xref="S3.E4.m1.1.1.1.1.2.3.1"></in><ci id="S3.E4.m1.1.1.1.1.2.3.2.cmml" xref="S3.E4.m1.1.1.1.1.2.3.2">𝐱</ci><apply id="S3.E4.m1.1.1.1.1.2.3.3.cmml" xref="S3.E4.m1.1.1.1.1.2.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.2.3.3.1.cmml" xref="S3.E4.m1.1.1.1.1.2.3.3">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.2.3.3.2.cmml" xref="S3.E4.m1.1.1.1.1.2.3.3.2">𝑉</ci><ci id="S3.E4.m1.1.1.1.1.2.3.3.3.cmml" xref="S3.E4.m1.1.1.1.1.2.3.3.3">𝑀</ci></apply></apply></apply><apply id="S3.E4.m1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1">subscript</csymbol><apply id="S3.E4.m1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E4.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1"><minus id="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1"></minus><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2"><times id="S3.E4.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2.1"></times><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2.2"><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2.2.1">^</ci><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2.2.2">𝐏</ci></apply><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2.3">𝐱</ci></apply><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3"><times id="S3.E4.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.1"></times><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.2"><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.2.1">¯</ci><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.2.2">𝐏</ci></apply><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.3">𝐒𝐱</ci></apply></apply></apply><cn id="S3.E4.m1.1.1.1.1.1.3.cmml" type="integer" xref="S3.E4.m1.1.1.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">e_{MSSD}=\min_{\mathbf{S}\in S_{M}}\max_{\mathbf{x}\in V_{M}}\|\hat{\mathbf{P}%
}\mathbf{x}-\overline{\mathbf{P}}\mathbf{Sx}\|_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.1d">italic_e start_POSTSUBSCRIPT italic_M italic_S italic_S italic_D end_POSTSUBSCRIPT = roman_min start_POSTSUBSCRIPT bold_S ∈ italic_S start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_max start_POSTSUBSCRIPT bold_x ∈ italic_V start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT end_POSTSUBSCRIPT ∥ over^ start_ARG bold_P end_ARG bold_x - over¯ start_ARG bold_P end_ARG bold_Sx ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS5.p4.1">where <math alttext="S_{M}" class="ltx_Math" display="inline" id="S3.SS5.p4.1.m1.1"><semantics id="S3.SS5.p4.1.m1.1a"><msub id="S3.SS5.p4.1.m1.1.1" xref="S3.SS5.p4.1.m1.1.1.cmml"><mi id="S3.SS5.p4.1.m1.1.1.2" xref="S3.SS5.p4.1.m1.1.1.2.cmml">S</mi><mi id="S3.SS5.p4.1.m1.1.1.3" xref="S3.SS5.p4.1.m1.1.1.3.cmml">M</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p4.1.m1.1b"><apply id="S3.SS5.p4.1.m1.1.1.cmml" xref="S3.SS5.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS5.p4.1.m1.1.1.1.cmml" xref="S3.SS5.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS5.p4.1.m1.1.1.2.cmml" xref="S3.SS5.p4.1.m1.1.1.2">𝑆</ci><ci id="S3.SS5.p4.1.m1.1.1.3.cmml" xref="S3.SS5.p4.1.m1.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p4.1.m1.1c">S_{M}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p4.1.m1.1d">italic_S start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT</annotation></semantics></math> is a set of symmetry transformations for the object whose pose is being estimated.</p>
</div>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="269" id="S3.F6.g1" src="extracted/5657361/figures/blender_rendering.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F6.2.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S3.F6.3.2" style="font-size:90%;">Rendering quality comparison between AMBF (left), Eevee (center), and Cycles (right). Each row represents the same scene. Shadow quality and metal shininess are superior in Cycles due to more comprehensive and exhaustive ray tracing, while the needle is less glossy and the shadow is unrealistically uniform in Eevee. Nevertheless, both Eevee and Cycles produce significantly higher fidelity rendering than AMBF.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Experiments and Results</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">For the evaluation experiments, first, the YOLOX and GDR-Net networks were trained with the generated training dataset. YOLOX was trained for 30 epochs using the Ranger Optimizer<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib34" title="">34</a>]</cite>, a batch size of 16 and a learning rate of 1e-3. GDR-Net was trained with the Ranger Optimizer for 450 epochs, a batch size of 48 images and a learning rate of 8e-4. This training setup was similar to the one used in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib16" title="">16</a>]</cite>. At test time, the trained bounding box detector was used to predict a single region of interest for each image. This region of interest was then used as input for the GDR-Net. Only images where at least 30 percent of the needle was visible were used for evaluation. Some sample images from the test set with their corresponding pose predictions can be observed in figure <a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#S3.F4" title="Figure 4 ‣ III-B Improvements of virtual scene ‣ III Methodology ‣ Realistic Data Generation for 6D Pose Estimation of Surgical Instruments"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">Final pose errors can be seen in table <a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#S4.T1" title="TABLE I ‣ IV Experiments and Results ‣ Realistic Data Generation for 6D Pose Estimation of Surgical Instruments"><span class="ltx_text ltx_ref_tag">I</span></a>. On the evaluated images, GDR-Net obtained a median rotational error of 7.74 degrees and a median translation error of 1.49 mm (less than 20 percent of the needle’s diameter). These results are comparable to the pose detection results of non-occluded needles presented in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib4" title="">4</a>]</cite> even though needles in our test set present varying levels of occlusion. Lastly, the median MSSD error is 1.43 mm. Pose error distribution in figure <a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#S3.F5" title="Figure 5 ‣ III-D Selected deep learning model for 6D pose estimation ‣ III Methodology ‣ Realistic Data Generation for 6D Pose Estimation of Surgical Instruments"><span class="ltx_text ltx_ref_tag">5</span></a> indicates that the network can have sporadic predictions with significantly higher errors. <span class="ltx_text" id="S4.p2.1.1" style="color:#000000;">High pose errors can be mainly attributed to images where several needle poses cannot be distinguished from each other.</span></p>
</div>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T1.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.3.4.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="4" id="S4.T1.3.4.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.3.4.1.1.1">Test set results (N=1458)</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.3">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.3.3.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.1">
<math alttext="\mathbf{e_{RE}}" class="ltx_Math" display="inline" id="S4.T1.1.1.1.m1.1"><semantics id="S4.T1.1.1.1.m1.1a"><msub id="S4.T1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.m1.1.1.cmml"><mi id="S4.T1.1.1.1.m1.1.1.2" xref="S4.T1.1.1.1.m1.1.1.2.cmml">𝐞</mi><mi id="S4.T1.1.1.1.m1.1.1.3" xref="S4.T1.1.1.1.m1.1.1.3.cmml">𝐑𝐄</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><apply id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.1.1.1.m1.1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T1.1.1.1.m1.1.1.2.cmml" xref="S4.T1.1.1.1.m1.1.1.2">𝐞</ci><ci id="S4.T1.1.1.1.m1.1.1.3.cmml" xref="S4.T1.1.1.1.m1.1.1.3">𝐑𝐄</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">\mathbf{e_{RE}}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.1.1.1.m1.1d">bold_e start_POSTSUBSCRIPT bold_RE end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1"> (deg)</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.2">
<math alttext="\mathbf{e_{TE}}" class="ltx_Math" display="inline" id="S4.T1.2.2.2.m1.1"><semantics id="S4.T1.2.2.2.m1.1a"><msub id="S4.T1.2.2.2.m1.1.1" xref="S4.T1.2.2.2.m1.1.1.cmml"><mi id="S4.T1.2.2.2.m1.1.1.2" xref="S4.T1.2.2.2.m1.1.1.2.cmml">𝐞</mi><mi id="S4.T1.2.2.2.m1.1.1.3" xref="S4.T1.2.2.2.m1.1.1.3.cmml">𝐓𝐄</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.m1.1b"><apply id="S4.T1.2.2.2.m1.1.1.cmml" xref="S4.T1.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.2.2.2.m1.1.1.1.cmml" xref="S4.T1.2.2.2.m1.1.1">subscript</csymbol><ci id="S4.T1.2.2.2.m1.1.1.2.cmml" xref="S4.T1.2.2.2.m1.1.1.2">𝐞</ci><ci id="S4.T1.2.2.2.m1.1.1.3.cmml" xref="S4.T1.2.2.2.m1.1.1.3">𝐓𝐄</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.m1.1c">\mathbf{e_{TE}}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.2.2.2.m1.1d">bold_e start_POSTSUBSCRIPT bold_TE end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.2.1"> (mm)</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.3.3.3">
<math alttext="\mathbf{e_{MSSD}}" class="ltx_Math" display="inline" id="S4.T1.3.3.3.m1.1"><semantics id="S4.T1.3.3.3.m1.1a"><msub id="S4.T1.3.3.3.m1.1.1" xref="S4.T1.3.3.3.m1.1.1.cmml"><mi id="S4.T1.3.3.3.m1.1.1.2" xref="S4.T1.3.3.3.m1.1.1.2.cmml">𝐞</mi><mi id="S4.T1.3.3.3.m1.1.1.3" xref="S4.T1.3.3.3.m1.1.1.3.cmml">𝐌𝐒𝐒𝐃</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.m1.1b"><apply id="S4.T1.3.3.3.m1.1.1.cmml" xref="S4.T1.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.3.3.3.m1.1.1.1.cmml" xref="S4.T1.3.3.3.m1.1.1">subscript</csymbol><ci id="S4.T1.3.3.3.m1.1.1.2.cmml" xref="S4.T1.3.3.3.m1.1.1.2">𝐞</ci><ci id="S4.T1.3.3.3.m1.1.1.3.cmml" xref="S4.T1.3.3.3.m1.1.1.3">𝐌𝐒𝐒𝐃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.m1.1c">\mathbf{e_{MSSD}}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.3.3.3.m1.1d">bold_e start_POSTSUBSCRIPT bold_MSSD end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S4.T1.3.3.3.1"> (mm)</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.5.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.3.5.2.1"><span class="ltx_text ltx_font_bold" id="S4.T1.3.5.2.1.1">mean</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S4.T1.3.5.2.2">11.85</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S4.T1.3.5.2.3">2.59</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.3.5.2.4">2.09</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.6.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.3.6.3.1"><span class="ltx_text ltx_font_bold" id="S4.T1.3.6.3.1.1">std</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T1.3.6.3.2">17.52</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T1.3.6.3.3">3.41</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.3.6.3.4">2.43</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.7.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.3.7.4.1"><span class="ltx_text ltx_font_bold" id="S4.T1.3.7.4.1.1">median</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T1.3.7.4.2">7.74</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T1.3.7.4.3">1.49</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.3.7.4.4">1.43</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.8.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T1.3.8.5.1"><span class="ltx_text ltx_font_bold" id="S4.T1.3.8.5.1.1">min</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T1.3.8.5.2">0.33</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T1.3.8.5.3">0.04</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.3.8.5.4">0.06</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.9.6">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r" id="S4.T1.3.9.6.1"><span class="ltx_text ltx_font_bold" id="S4.T1.3.9.6.1.1">max</span></td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_r" id="S4.T1.3.9.6.2">170.5</td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_r" id="S4.T1.3.9.6.3">33.01</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T1.3.9.6.4">20.91</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.5.1.1" style="font-size:90%;">TABLE I</span>: </span><span class="ltx_text" id="S4.T1.6.2" style="font-size:90%;">GDRNET test set results. Only images where at least 30 percent of the needle was visible were included in the evaluation. The diameter of the detected needle was 18.65 mm.</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Discussion and Future Work</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this work, we developed a data generation pipeline for 6D estimation tasks of surgical instruments on top of the simulation framework AMBF. The proposed pipeline generates monocular or stereoscopic RGB images, and pose annotations for any rigid or articulated instrument in the scene. Moreover, each generated RGB image is accompanied by its corresponding depth and segmentation maps.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">The focus of the work was to enable the automatic generation of large and diverse datasets showing realistic tissue-instrument interaction and sensible trajectories for robotic manipulators. In this regard, we divide our data generation pipeline into two steps: (1) a data recording step where robotic trajectories of a surgical task are recorded, and (2) a processing and generation step where each collected trajectory is replayed multiple times from different camera view angles and lighting conditions.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">To showcase the applicability of our pipeline, we generated a dataset of 7.5k images with pose annotations for a surgical needle to evaluate a state-of-the-art pose estimation neural network. After training, the network had translation and rotation errors comparable to previous works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib5" title="">5</a>]</cite> while being tested on a challenging dataset where the needle could be partially occluded by the instruments and the tissue.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1"><span class="ltx_text" id="S5.p4.1.1" style="color:#000000;">Although the network showed good performance on average, it is important to remember that the model makes predictions solely based on the visual appearance of the object, and therefore cases where multiple poses are indistinguishable from each other will result in high pose errors. Specifically for surgical needles, there are two main scenarios leading to pose ambiguities: (1) images where both the needle’s tail and tip are occluded and (2) images where the needle’s curvature cannot be observed, i.e., the needle appears as a straight line. As a solution, pose ambiguities could be resolved by using the network’s predictions with a model-based tracker that uses additional priors, such as the robot’s kinematic motion or the pose of the needle in previous frames.</span></p>
</div>
<div class="ltx_para" id="S5.p5">
<p class="ltx_p" id="S5.p5.1">In future work, we will leverage our data generation pipeline to study different techniques for transferring pose detection models from simulation to reality, a problem that is often referred to in the literature as the “domain gap” <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib35" title="">35</a>]</cite>. As noted by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib10" title="">10</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib36" title="">36</a>]</cite>, rendering realism plays an important role in transferring neural networks from synthetic to real objects. This hints that models trained based on our current synthetic data (generated with the simpler Blinn-Phong shading technique <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#bib.bib37" title="">37</a>]</cite> ) might suffer from degraded performance when applied to data from the physical surgical platform.</p>
</div>
<div class="ltx_para" id="S5.p6">
<p class="ltx_p" id="S5.p6.1">To mitigate this limitation, we implemented a preliminary real-time pipeline to improve the rendering quality by transferring the object pose (including cameras and lights) from the AMBF simulator to Blender. This allows us to utilize the two state-of-the-art rendering engines included in Blender since version 3.0: Eevee (a real-time rasterization-based renderer) and Cycles (a physically based path tracer). As shown in figure <a class="ltx_ref" href="https://arxiv.org/html/2406.07328v1#S3.F6" title="Figure 6 ‣ III-E Evaluation metrics for the pose estimates ‣ III Methodology ‣ Realistic Data Generation for 6D Pose Estimation of Surgical Instruments"><span class="ltx_text ltx_ref_tag">6</span></a>, shadows and metal shininess rendered using Blender are significantly better than AMBF. Future studies will focus on understanding the effects of different rendering algorithms on the simulation-to-real transfer of neural networks. Additional future improvements on our simulation platform will include more accurate models for the robotic instruments and advanced materials that more accurately reflect surgical tools.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgments</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This work was supported in part by NSF AccelNet awards OISE-1927354 and OISE-1927275. We thank Irene Kim, Haochen Wei, and Nicholas Greene for their invaluable insights on 6D pose estimation models.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Supplementary information</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">For more information, visit the project repository at <a class="ltx_ref ltx_href" href="https://github.com/surgical-robotics-ai/realistic-6dof-data-generation" title="">https://github.com/surgical-robotics-ai/realistic-6dof-data-generation</a></p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
A. Attanasio, B. Scaglioni, E. De Momi, P. Fiorini, and P. Valdastri,
“Autonomy in Surgical Robotics,” <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Annual Review of Control,
Robotics, and Autonomous Systems</em>, vol. 4, no. 1, pp. 651–679, May 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
K. L. Schwaner, D. Dall’Alba, P. T. Jensen, P. Fiorini, and T. R. Savarimuthu,
“Autonomous Needle Manipulation for Robotic Surgical Suturing Based
on Skills Learned from Demonstration,” in <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">2021 IEEE 17th
International Conference on Automation Science and Engineering
(CASE)</em>, Aug. 2021, pp. 235–241.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
A. Wilcox, J. Kerr, B. Thananjeyan, J. Ichnowski, M. Hwang, S. Paradis, D. Fer,
and K. Goldberg, “Learning to Localize, Grasp, and Hand Over
Unmodified Surgical Needles,” in <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">2022 International Conference
on Robotics and Automation (ICRA)</em>.   Philadelphia, PA, USA: IEEE, May 2022, pp. 9637–9643.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Y. Jiang, H. Zhou, and G. S. Fischer, “Markerless Suture Needle Tracking
From A Robotic Endoscope Based On Deep Learning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">2023
International Symposium on Medical Robotics (ISMR)</em>, Apr. 2023,
pp. 1–7.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Z.-Y. Chiu, A. Z. Liao, F. Richter, B. Johnson, and M. C. Yip, “Markerless
Suture Needle 6D Pose Tracking with Robust Uncertainty Estimation for
Autonomous Minimally Invasive Robotic Surgery,” in <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">2022
IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS)</em>, Oct. 2022, pp. 5286–5292.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
F. Richter, S. Shen, F. Liu, J. Huang, E. K. Funk, R. K. Orosco, and M. C. Yip,
“Autonomous Robotic Suction to Clear the Surgical Field for
Hemostasis Using Image-Based Blood Flow Detection,” <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">IEEE Robotics
and Automation Letters</em>, vol. 6, no. 2, pp. 1383–1390, Apr. 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
J. A. Barragan, D. Chanci, D. Yu, and J. P. Wachs, “SACHETS:
Semi-Autonomous Cognitive Hybrid Emergency Teleoperated Suction,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">2021 30th IEEE International Conference on Robot &amp; Human
Interactive Communication (RO-MAN)</em>.   Vancouver, BC, Canada: IEEE, Aug. 2021, pp. 1243–1248.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
J. Lu, A. Jayakumari, F. Richter, Y. Li, and M. C. Yip, “SuPer Deep: A
Surgical Perception Framework for Robotic Tissue Manipulation using
Deep Learning for Feature Extraction,” in <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">2021 IEEE
International Conference on Robotics and Automation (ICRA)</em>,
May 2021, pp. 4783–4789.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
M. A. Fischler and R. C. Bolles, “Random sample consensus: A paradigm for
model fitting with applications to image analysis and automated
cartography,” <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Commun. ACM</em>, vol. 24, no. 6, p. 381–395, jun 1981.
[Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/358669.358692" title="">https://doi.org/10.1145/358669.358692</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
M. Sundermeyer, T. Hodaň, Y. Labbé, G. Wang, E. Brachmann, B. Drost,
C. Rother, and J. Matas, “BOP Challenge 2022 on Detection,
Segmentation and Pose Estimation of Specific Rigid Objects,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">2023 IEEE/CVF Conference on Computer Vision and Pattern
Recognition Workshops (CVPRW)</em>, Jun. 2023, pp. 2785–2794.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, “Domain
randomization for transferring deep neural networks from simulation to the
real world,” in <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">2017 IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS)</em>, Sep. 2017, pp. 23–30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
J. Cartucho, S. Tukra, Y. Li, D. S. Elson, and S. Giannarou,
“VisionBlender: A tool to efficiently generate computer vision datasets
for robotic surgery,” <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Computer Methods in Biomechanics and Biomedical
Engineering: Imaging &amp; Visualization</em>, vol. 0, no. 0, pp. 1–8, Dec. 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
M. Denninger, M. Sundermeyer, D. Winkelbauer, D. Olefir, T. Hodan, Y. Zidan,
M. Elbadrawy, M. Knauer, H. Katam, and A. Lodhi, “BlenderProc:
Reducing the Reality Gap with Photorealistic Rendering.”

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
A. Munawar, J. Y. Wu, G. S. Fischer, R. H. Taylor, and P. Kazanzides, “Open
Simulation Environment for Learning and Practice of
Robot-Assisted Surgical Suturing,” <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">IEEE Robotics and Automation
Letters</em>, vol. 7, no. 2, pp. 3843–3850, Apr. 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
A. Munawar, Y. Wang, R. Gondokaryono, and G. S. Fischer, “A real-time dynamic
simulator and an associated front-end representation format for simulating
complex robots and environments,” <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">2019 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS)</em>. [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://par.nsf.gov/biblio/10207704" title="">https://par.nsf.gov/biblio/10207704</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
G. Wang, F. Manhardt, F. Tombari, and X. Ji, “GDR-Net: Geometry-Guided
Direct Regression Network for Monocular 6D Object Pose Estimation,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">2021 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</em>.   Nashville,
TN, USA: IEEE, Jun. 2021, pp. 16 606–16 616.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
M. Quigley, “ROS: an open-source robot operating system,” in <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">IEEE
International Conference on Robotics and Automation</em>, 2009. [Online].
Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:6324125" title="">https://api.semanticscholar.org/CorpusID:6324125</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
V. Makoviychuk, L. Wawrzyniak, Y. Guo, M. Lu, K. Storey, M. Macklin,
D. Hoeller, N. Rudin, A. Allshire, A. Handa <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">et al.</em>, “Isaac gym: High
performance gpu-based physics simulation for robot learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib18.2.2">arXiv
preprint arXiv:2108.10470</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
M. Mittal, C. Yu, Q. Yu, J. Liu, N. Rudin, D. Hoeller, J. L. Yuan, R. Singh,
Y. Guo, H. Mazhar <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">et al.</em>, “Orbit: A unified simulation framework for
interactive robot learning environments,” <em class="ltx_emph ltx_font_italic" id="bib.bib19.2.2">IEEE Robotics and Automation
Letters</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
E. Coumans, “Bullet physics simulation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">ACM SIGGRAPH 2015 Courses</em>,
ser. SIGGRAPH ’15.   New York, NY, USA:
Association for Computing Machinery, 2015. [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/2776880.2792704" title="">https://doi.org/10.1145/2776880.2792704</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
S. Musić and S. Hirche, “Haptic shared control for human-robot collaboration:
A game-theoretical approach,” <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">IFAC-PapersOnLine</em>, vol. 53, no. 2, pp.
10 216–10 222, 2020, 21st IFAC World Congress. [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S240589632033514X" title="">https://www.sciencedirect.com/science/article/pii/S240589632033514X</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
H. Ishida, J. A. Barragan, A. Munawar, Z. Li, A. Ding, P. Kazanzides,
D. Trakimas, F. X. Creighton, and R. H. Taylor, “Improving surgical
situational awareness with signed distance field: A pilot study in virtual
reality,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
H. Shu, R. Liang, Z. Li, A. Goodridge, X. Zhang, H. Ding, N. Nagururu, M. Sahu,
F. X. Creighton, R. H. Taylor, and et al., “Twin-s: A digital twin for skull
base surgery,” <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">International Journal of Computer Assisted Radiology
and Surgery</em>, vol. 18, no. 6, p. 1077–1084, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
H. Ding, J. Zhang, P. Kazanzides, J. Y. Wu, and M. Unberath, “CaRTS:
Causality-driven robot tool segmentation from vision and kinematics data,”
in <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Medical Image Computing and Computer Assisted Intervention –
MICCAI 2022: 25th International Conference, Singapore, September 18–22,
2022, Proceedings, Part VII</em>.   Berlin,
Heidelberg: Springer-Verlag, 2022, p. 387–398. [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/978-3-031-16449-1_37" title="">https://doi.org/10.1007/978-3-031-16449-1_37</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
P. Kazanzides, Z. Chen, A. Deguet, G. S. Fischer, R. H. Taylor, and S. P.
DiMaio, “An open-source research kit for the da Vinci®
Surgical System,” in <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">2014 IEEE International Conference on
Robotics and Automation (ICRA)</em>, May 2014, pp. 6434–6439.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
S. Tyree, J. Tremblay, T. To, J. Cheng, T. Mosier, J. Smith, and S. Birchfield,
“6-DoF Pose Estimation of Household Objects for Robotic
Manipulation: An Accessible Dataset and Benchmark,” in <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">2022
IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS)</em>, Oct. 2022, pp. 13 081–13 088.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Y. Xiang, T. Schmidt, V. Narayanan, and D. Fox, “PoseCNN: A
Convolutional Neural Network for 6D Object Pose Estimation in
Cluttered Scenes,” May 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
A. Fedorov, R. Beichel, J. Kalpathy-Cramer, J. Finet, J.-C. Fillion-Robin,
S. Pujol, C. Bauer, D. Jennings, F. Fennessy, M. Sonka, and et al., “3D
Slicer as an image computing platform for the quantitative imaging
network,” <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Magnetic Resonance Imaging</em>, vol. 30, no. 9, p. 1323–1341,
2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
P. Cignoni, M. Callieri, M. Corsini, M. Dellepiane, F. Ganovelli, and
G. Ranzuglia, “MeshLab: an Open-Source Mesh Processing Tool,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Eurographics Italian Chapter Conference</em>, V. Scarano, R. D. Chiara, and
U. Erra, Eds.   The Eurographics
Association, 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
M. P. Hiorns, “Imaging of the urinary tract: the role of CT and MRI,”
<em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Pediatric nephrology</em>, vol. 26, no. 1, pp. 59–68, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Z. Ge, S. Liu, F. Wang, Z. Li, and J. Sun, “YOLOX: Exceeding YOLO Series in
2021,” <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">ArXiv</em>, vol. abs/2107.08430, 2021. [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:236088010" title="">https://api.semanticscholar.org/CorpusID:236088010</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
T. Hodan, J. Matas, and S. Obdrzálek, “On evaluation of 6d object pose
estimation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">ECCV Workshops</em>, 2016. [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:14980684" title="">https://api.semanticscholar.org/CorpusID:14980684</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
B. Drost, M. Ulrich, P. Bergmann, P. Hartinger, and C. Steger, “Introducing
MVTEC Itodd — a dataset for 3D object recognition in industry,”
<em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">2017 IEEE International Conference on Computer Vision Workshops
(ICCVW)</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
L. Wright, “Ranger - a synergistic optimizer.”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer" title="">https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer</a>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
D. Schraml, “Physically based synthetic image generation for machine learning:
A review of pertinent literature,” in <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Photonics and Education in
Measurement Science 2019</em>, vol. 11144.   SPIE, Sep. 2019, pp. 108–120.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
S. A. Heredia Perez, M. Marques Marinho, K. Harada, and M. Mitsuishi, “The
effects of different levels of realism on the training of CNNs with only
synthetic images for the semantic segmentation of robotic instruments in a
head phantom,” <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">International Journal of Computer Assisted Radiology
and Surgery</em>, vol. 15, no. 8, pp. 1257–1265, Aug. 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
J. F. Blinn, “Models of light reflection for computer synthesized pictures,”
<em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">SIGGRAPH Comput. Graph.</em>, vol. 11, no. 2, p. 192–198, jul 1977.
[Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/965141.563893" title="">https://doi.org/10.1145/965141.563893</a>
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jun 11 14:48:19 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
