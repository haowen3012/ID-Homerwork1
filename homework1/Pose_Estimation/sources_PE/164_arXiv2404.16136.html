<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement</title>
<!--Generated on Tue Apr 30 19:20:23 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2404.16136v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S1" title="In Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S2" title="In Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S2.SS1" title="In 2 Related Work ‣ Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>From 2D to 3D Transition</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S2.SS2" title="In 2 Related Work ‣ Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>2D HPE</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S2.SS3" title="In 2 Related Work ‣ Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>3D HPE</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S2.SS4" title="In 2 Related Work ‣ Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Graph Convolutional Network</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S2.SS5" title="In 2 Related Work ‣ Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5 </span>HPE Datasets</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S3" title="In Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>BlendMimic3D Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S4" title="In Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Pose Refinement with GCN</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S5" title="In Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S5.SS1" title="In 5 Experimental Setup ‣ Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Datasets and Evaluation Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S5.SS2" title="In 5 Experimental Setup ‣ Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Implementation Details</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S6" title="In Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Experimental Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S6.SS1" title="In 6 Experimental Results ‣ Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Quantitative results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S6.SS2" title="In 6 Experimental Results ‣ Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Qualitative results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S7" title="In Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S8" title="In Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span> BlendMimic3D examples</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S9" title="In Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9 </span>Detailed Evaluation of GCN</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">
<span class="ltx_text" id="id1.id1" style="font-size:52%;"> <span class="ltx_text ltx_font_medium" id="id1.id1.1">
<br class="ltx_break"/>Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild 
<br class="ltx_break"/>CVPR 2024 Workshop 
<br class="ltx_break"/> </span></span>3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Filipa Lino, Carlos Santiago, Manuel Marques
<br class="ltx_break"/>Institute for Systems and Robotics, LARSyS, Instituto Superior Técnico, Portugal
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id2.1.id1" style="font-size:90%;">{filipa.lino, carlos.santiago}@tecnico.ulisboa.pt, manuel@isr.tecnico.ulisboa.pt </span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id3.id1">In the field of 3D Human Pose Estimation (HPE), accurately estimating human pose, especially in scenarios with occlusions, is a significant challenge.
This work identifies and addresses a gap in the current state of the art in 3D HPE concerning the scarcity of data and strategies for handling occlusions.
We introduce our novel BlendMimic3D dataset, designed to mimic real-world situations where occlusions occur for seamless integration in 3D HPE algorithms. Additionally, we propose a 3D pose refinement block, employing a Graph Convolutional Network (GCN) to enhance pose representation through a graph model. This GCN block acts as a plug-and-play solution, adaptable to various 3D HPE frameworks without requiring retraining them. By training the GCN with occluded data from BlendMimic3D, we demonstrate significant improvements in resolving occluded poses, with comparable results for non-occluded ones. Project web page is available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://blendmimic3d.github.io/BlendMimic3D/" title="">https://blendmimic3d.github.io/BlendMimic3D/</a>.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Human pose estimation (HPE) from visual data has become crucial in computer vision, with wide-ranging applications from sports analysis to enhancing smart retail experiences. It involves interpreting a person’s position and orientation from images or videos. Despite the emergence of various techniques <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib35" title=""><span class="ltx_text" style="font-size:90%;">35</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">30</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite> and datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib37" title=""><span class="ltx_text" style="font-size:90%;">37</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>, 3D HPE remains challenging, particularly with monocular camera views in occluded scenarios. Under occlusions, estimating 3D poses becomes even harder, due to the increased ambiguity, and the lack of datasets specifically targeting occluded scenarios makes most state-of-the-art approach struggle with this type of data.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">To fill this gap, we introduce a new synthetic dataset, called BlendMimic3D<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Available at https://github.com/FilipaLino/BlendMimic3D</span></span></span>, illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_tag">1</span></a>, that aims to serve as a novel benchmark for HPE with occlusions. Our dataset, built using Blender <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite>, comprises a variety of scenarios mirroring real-world complexities, and purposely contains several types of occlusions, including self, object-based and out-of-frame occlusions. This makes BlendMimic3D an invaluable tool for both training HPE models and benchmarking their performance in occluded scenarios.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S1.F1.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;"> BlendMimic3D, our synthetic dataset for 3D HPE occlusion benchmarking, features diverse multi-camera scenarios with up to three subjects. It includes Blender animations (top left), keypoint visibility (top right), cameras’ parameters, 3D poses (bottom left) and 2D pose representations (bottom right).</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Additionally, this work proposes a new pose refinement module<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Available at https://github.com/FilipaLino/GCN-Pose-Refinement</span></span></span>, designed to overcome the limitations of the current state of the art. Our approach is based on a graph convolutional network (GCN) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib17" title=""><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite> that takes into account spatial and temporal information and is compatible with various 2D-to-3D HPE backbones, including VideoPose3D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite>, PoseFormerV2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite>, and D3DP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite>. It works as a plugin feature that enhances occluded keypoint estimates and does not require training or fine-tuning the HPE backbone, substantially simplifying its use.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">The main contributions are the following:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">BlendMimic3D dataset: a comprehensive, realistic synthetic benchmark dataset focused on occlusions, aiding in the training and evaluation of HPE models.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">A novel GCN for 3D pose refinement, leveraging spatial-temporal keypoint relationships. It integrates with most current monocular 3D HPE methods and is designed to address occlusions without additional retraining.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Extensive evaluation with two different 2D keypoint detection algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">39</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite> and three state-of-the-art 2D-to-3D algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">30</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">34</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite>, validate the utility of our new benchmark dataset and the efficacy of our proposed refinement module in estimating poses in occluded conditions.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Advances in deep learning, especially Convolutional Neural Networks (CNNs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>, have significantly improved HPE, offering enhanced accuracy and speed. Toshev et al.’s DeepPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite> exemplifies the potential of these methods. In HPE, three primary body modeling approaches are used: kinematic (body keypoints) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">22</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">1</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">15</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">16</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib37" title=""><span class="ltx_text" style="font-size:90%;">37</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite>; planar (body contours) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">8</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib37" title=""><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite>; and volumetric (3D meshes) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib37" title=""><span class="ltx_text" style="font-size:90%;">37</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>. Our work focuses on the kinematic model due to its versatility.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>From 2D to 3D Transition </h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">When estimating 3D human pose from 2D video inputs, direct 3D estimation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">21</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib29" title=""><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite> is challenging due to the loss of depth information in 2D representations. Instead, researchers have found it more effective to first extract the 2D pose and then infer the corresponding 3D pose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib35" title=""><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Lee and Chen <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite> were early pioneers in 2D joints projection into 3D spaces, but the arrival of deep learning later shifted the focus towards neural network-based methods. Martinez et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite> emphasized the critical role of 2D pose data in predicting 3D keypoints.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">Current methods have adopted two primary paradigms: bottom-up <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">11</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite> and top-down <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">12</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">39</span></a>]</cite>. While the former starts with individual body joint estimations, the latter begins by detecting persons. Each approach comes with its set of advantages and challenges, with the trade-off between accuracy and computational speed being paramount.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>2D HPE</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Single-person estimation primarily employs regression methods, such as DeepPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite>, and heatmap-based techniques <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">6</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite>. Multi-person scenarios see the use of both bottom-up methods, like OpenPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>, and top-down strategies like AlphaPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>. Hybrid methods, highlighted by Miaopeng Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite>, merge these techniques.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Another noteworthy top-down model in this category is Mask R-CNN by Kaiming He et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite>, initially designed for object detection and semantic segmentation, but later incorporated HPE. Based on that framework, Detectron2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">39</span></a>]</cite> was introduced to handle tasks from object detection to 2D keypoint identification. It uses CNNs to generate heatmaps for keypoints, with the heatmap’s peak indicating the exact keypoint location for accurate results. Also following <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite>, Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite> developed the Cascaded Pyramid Network (CPN) to improve multi-person pose estimation, focusing on “hard" keypoints that are occluded or not visible.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">Our study employed Detectron2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">39</span></a>]</cite> and CPN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite> for 2D keypoint detection due to their precision and state-of-the-art features, with both achieving a high performance on the COCO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite> benchmark. We also integrated DeepSort <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite> for tracking individuals in multi-person scenarios, basing 3D pose predictions from specific 2D keypoints.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>3D HPE </h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Despite advances in 2D HPE, 3D HPE struggles with depth ambiguities, limited datasets, and complexities associated with occlusions. Considering monocular RGB images and videos and a two-stage approach (2D to 3D Lifting), Martinez et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite> set a benchmark in this domain by using a fully connected residual network to regress 3D joint locations from 2D ones. Another influential work by Tome et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib35" title=""><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite> proposed a multi-stage approach where 2D and 3D poses are processed concurrently.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">Additionally, temporal data from videos has been incorporated to address depth issues. Pavllo et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite> introduced a temporal dilated convolutional model, named VideoPose3D. While this approach is noted for its simplicity and efficiency, it may encounter difficulties in handling continuous occlusions.</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">Motivated by that, Cheng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite> presented a network that addressing occlusions through temporal frame analysis. This architecture is particularly effective in scenarios with occluded body parts, but only accounts for self-occlusions, since the testing was conducted using data that primarily featured such occlusions, limiting its applicability.</p>
</div>
<div class="ltx_para" id="S2.SS3.p4">
<p class="ltx_p" id="S2.SS3.p4.1">Zheng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite> introduced PoseFormer, a purely transformer-based model for 3D HPE from videos. This model process both spatial and temporal aspects of human movement. Building on this, Zhao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite> developed PoseFormerV2, which employs the frequency domain to boost efficiency and accuracy of 3D HPE. This approach reduces computational demands and increases robustness to noisy in 2D joint detections, making it effective in complex and occluded scenarios.</p>
</div>
<div class="ltx_para" id="S2.SS3.p5">
<p class="ltx_p" id="S2.SS3.p5.1">Shan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite> presented D3DP, an innovative method for probabilistic 3D HPE. D3DP generates multiple potential 3D poses from a single 2D observation, using a denoiser conditioned on 2D keypoints to refine the poses. The hypotheses for the 3D poses are reprojected onto the 2D camera plane, and the best hypothesis for each joint is selected based on reprojection errors. These selections are combined to form the final pose.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Graph Convolutional Network</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">Graph-based approaches, such as GCNs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib17" title=""><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite>, can be used to address occlusions in 3D HPE by representing the body as a graph, where each node represents a body keypoint and each edge represents the relationship between two joints. A notable application is the Dynamic Graph Convolutional Network (DGCN) introduced by Zhongwei Qiu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite>, that can model relationships between 2D joints over time. Wenbo Hu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite> proposed representing a 3D human skeleton as a directed graph, to capture hierarchical orders among the joints.</p>
</div>
<div class="ltx_para" id="S2.SS4.p2">
<p class="ltx_p" id="S2.SS4.p2.1">Following the DGCN approach, to further enrich the 3D HPE domain, Cai et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite> proposed a graph-based approach leveraging spatial-temporal relationships. They formulated 2D pose sequences as graphs and designed a network to capture multi-scale features and temporal constraints. Later, Yu Cheng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib9" title=""><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite> presented a novel framework for estimating 3D multi-person poses from monocular videos with two directed GCNs, one dedicated to joints and the other to bones, which together estimate the full pose. This framework integrates GCNs and Temporal Convolutional Networks (TCNs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite> to handle challenges like occlusions and inaccuracies in person detection. They also include directed graph-based joint and bone GCNs.</p>
</div>
<div class="ltx_para" id="S2.SS4.p3">
<p class="ltx_p" id="S2.SS4.p3.1">Our proposal employs a GCN model, which is designed to represent the 3D human pose as an enhanced, undirected graph, inspired by the method in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>. We have tailored our model to specifically refine 3D pose predictions, particularly effective in scenarios with occlusions. This is achieved by expanding joint relationships, with training conducted on a variety of cases involving occlusions.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span>HPE Datasets</h3>
<div class="ltx_para" id="S2.SS5.p1">
<p class="ltx_p" id="S2.SS5.p1.1">The evolution of HPE approaches has underscored the importance of comprehensive datasets, especially in the context of occlusions. For 2D HPE, datasets like MPII <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite>, COCO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite>, and PoseTrack <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite> offer diverse scenarios ranging from static images to dynamic videos, capturing real-world complexities. These datasets facilitate the development of models that generalize to multiple environments.</p>
</div>
<div class="ltx_para" id="S2.SS5.p2">
<p class="ltx_p" id="S2.SS5.p2.1">Well-known 3D HPE datasets, such as Human3.6M <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>, SURREAL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib37" title=""><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite>, and AMASS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>, typically require sophisticated equipment like motion capture systems for accurate pose recording. While these datasets offer high precision, they often face challenges in diversity and real-world applicability. For multi-person scenarios, datasets like CMU Panoptic <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite>, 3DPW<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite> and AGORA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> become crucial as they capture more complex interactions and dynamics, including occlusions. Table <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S2.T1" title="Table 1 ‣ 2.5 HPE Datasets ‣ 2 Related Work ‣ Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the diversity and focus of some of these datasets.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T1.10.3.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S2.T1.4.2" style="font-size:90%;">3D HPE Datasets. Data type ‘R’ and ‘S’ denote ‘Real’ and ‘Synthetic’. <sup class="ltx_sup" id="S2.T1.4.2.1">†</sup> non-self occlusions (object-based, multi-person, and out-of-frame). <sup class="ltx_sup" id="S2.T1.4.2.2">‡</sup> annotations of keypoint visibility.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T1.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.6.3.1">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S2.T1.6.3.1.1" style="padding-left:1.5pt;padding-right:1.5pt;"></th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T1.6.3.1.2" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.6.3.1.2.1" style="font-size:70%;">Data</span></th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T1.6.3.1.3" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.6.3.1.3.1" style="font-size:70%;">No. of</span></th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T1.6.3.1.4" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.6.3.1.4.1" style="font-size:70%;">Action</span></th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T1.6.3.1.5" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.6.3.1.5.1" style="font-size:70%;">Single-</span></th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T1.6.3.1.6" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.6.3.1.6.1" style="font-size:70%;">Multi-</span></th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2" id="S2.T1.6.3.1.7" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.6.3.1.7.1" style="font-size:70%;">Occlusions</span></th>
</tr>
<tr class="ltx_tr" id="S2.T1.6.2">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S2.T1.6.2.3" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.6.2.3.1" style="font-size:70%;">Dataset</span></th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" id="S2.T1.6.2.4" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.6.2.4.1" style="font-size:70%;">Type</span></th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" id="S2.T1.6.2.5" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.6.2.5.1" style="font-size:70%;">Frames</span></th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" id="S2.T1.6.2.6" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.6.2.6.1" style="font-size:70%;">Tags</span></th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" id="S2.T1.6.2.7" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.6.2.7.1" style="font-size:70%;">Person</span></th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" id="S2.T1.6.2.8" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.6.2.8.1" style="font-size:70%;">Person</span></th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" id="S2.T1.5.1.1" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.5.1.1.1" style="font-size:70%;">Complex<sup class="ltx_sup" id="S2.T1.5.1.1.1.1"><span class="ltx_text ltx_font_medium" id="S2.T1.5.1.1.1.1.1">†</span></sup></span></th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S2.T1.6.2.2" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.6.2.2.1" style="font-size:70%;">Labels<sup class="ltx_sup" id="S2.T1.6.2.2.1.1"><span class="ltx_text ltx_font_medium" id="S2.T1.6.2.2.1.1.1">‡</span></sup></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.6.4.1" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T1.6.4.1.1" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.4.1.1.1" style="font-size:70%;background-color:#E6E6E6;">Human3.6M <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite></span></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S2.T1.6.4.1.2" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.4.1.2.1" style="font-size:70%;background-color:#E6E6E6;">R</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S2.T1.6.4.1.3" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.4.1.3.1" style="font-size:70%;background-color:#E6E6E6;">3.6M</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S2.T1.6.4.1.4" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.4.1.4.1" style="font-size:70%;background-color:#E6E6E6;">✓</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S2.T1.6.4.1.5" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.4.1.5.1" style="font-size:70%;background-color:#E6E6E6;">✓</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S2.T1.6.4.1.6" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.4.1.6.1" style="font-size:70%;background-color:#E6E6E6;">✕</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S2.T1.6.4.1.7" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.4.1.7.1" style="font-size:70%;background-color:#E6E6E6;">✕</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.6.4.1.8" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.4.1.8.1" style="font-size:70%;background-color:#E6E6E6;">✕</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.6.5.2">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T1.6.5.2.1" style="padding-left:1.5pt;padding-right:1.5pt;">
<span class="ltx_text" id="S2.T1.6.5.2.1.1" style="font-size:70%;">CMU Panoptic </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.6.5.2.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">16</span></a><span class="ltx_text" id="S2.T1.6.5.2.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.T1.6.5.2.2" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.5.2.2.1" style="font-size:70%;">R</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.T1.6.5.2.3" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.5.2.3.1" style="font-size:70%;">1.5M</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.T1.6.5.2.4" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.5.2.4.1" style="font-size:70%;">✕</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.T1.6.5.2.5" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.5.2.5.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.T1.6.5.2.6" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.5.2.6.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.T1.6.5.2.7" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.5.2.7.1" style="font-size:70%;">✕</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S2.T1.6.5.2.8" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.5.2.8.1" style="font-size:70%;">✕</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.6.6.3" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T1.6.6.3.1" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.6.3.1.1" style="font-size:70%;background-color:#E6E6E6;">SURREAL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib37" title=""><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite></span></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.T1.6.6.3.2" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.6.3.2.1" style="font-size:70%;background-color:#E6E6E6;">S</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.T1.6.6.3.3" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.6.3.3.1" style="font-size:70%;background-color:#E6E6E6;">6M</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.T1.6.6.3.4" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.6.3.4.1" style="font-size:70%;background-color:#E6E6E6;">✕</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.T1.6.6.3.5" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.6.3.5.1" style="font-size:70%;background-color:#E6E6E6;">✓</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.T1.6.6.3.6" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.6.3.6.1" style="font-size:70%;background-color:#E6E6E6;">✕</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.T1.6.6.3.7" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.6.3.7.1" style="font-size:70%;background-color:#E6E6E6;">✕</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S2.T1.6.6.3.8" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.6.3.8.1" style="font-size:70%;background-color:#E6E6E6;">✕</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.6.7.4">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T1.6.7.4.1" style="padding-left:1.5pt;padding-right:1.5pt;">
<span class="ltx_text" id="S2.T1.6.7.4.1.1" style="font-size:70%;">3DPW </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.6.7.4.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2</span></a><span class="ltx_text" id="S2.T1.6.7.4.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.T1.6.7.4.2" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.7.4.2.1" style="font-size:70%;">R</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.T1.6.7.4.3" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.7.4.3.1" style="font-size:70%;">51K</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.T1.6.7.4.4" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.7.4.4.1" style="font-size:70%;">✕</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.T1.6.7.4.5" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.7.4.5.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.T1.6.7.4.6" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.7.4.6.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.T1.6.7.4.7" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.7.4.7.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S2.T1.6.7.4.8" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.7.4.8.1" style="font-size:70%;">✕</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.6.8.5" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T1.6.8.5.1" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.8.5.1.1" style="font-size:70%;background-color:#E6E6E6;">AGORA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite></span></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.T1.6.8.5.2" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.8.5.2.1" style="font-size:70%;background-color:#E6E6E6;">S</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.T1.6.8.5.3" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.8.5.3.1" style="font-size:70%;background-color:#E6E6E6;">17K</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.T1.6.8.5.4" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.8.5.4.1" style="font-size:70%;background-color:#E6E6E6;">✕</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.T1.6.8.5.5" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.8.5.5.1" style="font-size:70%;background-color:#E6E6E6;">✕</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.T1.6.8.5.6" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.8.5.6.1" style="font-size:70%;background-color:#E6E6E6;">✓</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.T1.6.8.5.7" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.8.5.7.1" style="font-size:70%;background-color:#E6E6E6;">✓</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S2.T1.6.8.5.8" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.8.5.8.1" style="font-size:70%;background-color:#E6E6E6;">✕</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.6.9.6">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T1.6.9.6.1" style="padding-left:1.5pt;padding-right:1.5pt;">
<span class="ltx_text" id="S2.T1.6.9.6.1.1" style="font-size:70%;">BEDLAM </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.6.9.6.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">3</span></a><span class="ltx_text" id="S2.T1.6.9.6.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.T1.6.9.6.2" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.9.6.2.1" style="font-size:70%;">S</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.T1.6.9.6.3" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.9.6.3.1" style="font-size:70%;">380K</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.T1.6.9.6.4" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.9.6.4.1" style="font-size:70%;">✕</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.T1.6.9.6.5" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.9.6.5.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.T1.6.9.6.6" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.9.6.6.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.T1.6.9.6.7" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.9.6.7.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S2.T1.6.9.6.8" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.9.6.8.1" style="font-size:70%;">✕</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.6.10.7" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.6.10.7.1" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.6.10.7.1.1" style="font-size:70%;background-color:#E6E6E6;">BlendMimic3D</span></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t" id="S2.T1.6.10.7.2" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.6.10.7.2.1" style="font-size:70%;background-color:#E6E6E6;">S</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t" id="S2.T1.6.10.7.3" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.6.10.7.3.1" style="font-size:70%;background-color:#E6E6E6;">136K</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t" id="S2.T1.6.10.7.4" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.10.7.4.1" style="font-size:70%;background-color:#E6E6E6;">✓</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t" id="S2.T1.6.10.7.5" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.10.7.5.1" style="font-size:70%;background-color:#E6E6E6;">✓</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t" id="S2.T1.6.10.7.6" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.10.7.6.1" style="font-size:70%;background-color:#E6E6E6;">✓</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t" id="S2.T1.6.10.7.7" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.10.7.7.1" style="font-size:70%;background-color:#E6E6E6;">✓</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.6.10.7.8" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S2.T1.6.10.7.8.1" style="font-size:70%;background-color:#E6E6E6;">✓</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S2.SS5.p3">
<p class="ltx_p" id="S2.SS5.p3.1">Following the discussion on existing datasets, the introduction of the BEDLAM dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite> represents a significant advancement. As a synthetic dataset designed for 3D human pose and shape (HPS) estimation, BEDLAM demonstrated that neural networks trained solely on synthetic data can achieve state-of-the-art accuracy in 3D HPS estimation from real images.</p>
</div>
<div class="ltx_para" id="S2.SS5.p4">
<p class="ltx_p" id="S2.SS5.p4.1">While both the COCO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite> and Human3.6M <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> datasets have been instrumental in advancing state-of-the-art algorithms, they present limitations. COCO’s human-curated nature is prone to errors, whereas Human3.6M, although providing high-precision pose data, lacks in representing occluded scenarios. Wandt et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib33" title=""><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite> showed that state-of-the-art 3D HPE models significantly underperform when faced with synthetic occlusions.</p>
</div>
<div class="ltx_para" id="S2.SS5.p5">
<p class="ltx_p" id="S2.SS5.p5.1">Addressing the challenges in 3D HPE occlusion handling highlighted by Wandt et al., we introduce BlendMimic3D. Inspired by Human3.6M and leveraging BEDLAM’s synthetic capabilities, BlendMimic3D offers advanced occlusion management across various levels. It sets a new benchmark in occlusion-aware 3D HPE with action-oriented labeled activities and occlusions, marking keypoints’ visibility per frame as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S2.T1" title="Table 1 ‣ 2.5 HPE Datasets ‣ 2 Related Work ‣ Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="92" id="S2.F2.g1" src="extracted/2404.16136v1/images/Blendmimic3DOccl3.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S2.F2.3.2" style="font-size:90%;">Visual representation of different scenes from BlendMimic3D datasets. From left to right: synthetic subjects, SS1, SS2 and SS3.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>BlendMimic3D Dataset</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">As the need for HPE grows, so does the demand for detailed datasets to train and test models. The efficacy of these datasets is judged by their accuracy, completeness, and variety. Creating 3D HPE datasets is complex and usually requires special tools such as MoCap systems and wearable devices, resulting in datasets created in controlled settings. As argued by Wandt et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib33" title=""><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite>, despite the progress achieved with Human3.6M <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> dataset, there remains a notable gap that synthetic datasets can address.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="220" id="S3.F3.g1" src="extracted/2404.16136v1/images/Blenderspecs2.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.3.2" style="font-size:90%;">Left: Camera distribution with the world coordinate system at the origin, with subject SS1 of BlendMimic3D dataset. Right: Visualization of 3D character armature, highlighting the specific keypoints used for coordinate extraction.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">Using Blender <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite>, a popular open-source 3D computer graphics software, we introduce a synthetic dataset tailored to address challenges such as self, object-based and out-of-frame occlusions. To ensure its adaptability and relevance, we designed it to comprise a diverse set of scenarios, from simple environments resembling Human3.6M <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>, to more complex ones with numerous occlusions and multi-person contexts. Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S2.F2" title="Figure 2 ‣ 2.5 HPE Datasets ‣ 2 Related Work ‣ Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates examples of frames from videos in our dataset, showcasing the range of settings and multi-person contexts of BlendMimic3D. More detailed examples are available in the supplementary material.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">In the process of crafting BlendMimic3D, four cameras were positioned within the virtual environment, as depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S3.F3" title="Figure 3 ‣ 3 BlendMimic3D Dataset ‣ Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_tag">3</span></a> (Left). A skeletal framework, shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S3.F3" title="Figure 3 ‣ 3 BlendMimic3D Dataset ‣ Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_tag">3</span></a> (Right), was attached to a 3D character model, enabling animation of our synthetic subjects. Utilizing resources from Mixamo<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mixamo.com/#/" title="">https://www.mixamo.com/#/</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib4" title=""><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite>, the characters were animated to simulate a range of actions, such as “Arguing", “Greeting", or “Picking Objects". From each camera’s perspective, videos were generated utilizing Blender’s rendering engine. The resulting dataset comprises:</p>
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">3 scenarios, from a simple environment to more complex and realistic ones;</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">3 subjects, each one performing several different actions;</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">Single and multi-person settings, with up to 3 subjects;</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1">A total of 128 videos with an average duration of 35 seconds (1050 frames).</p>
</div>
</li>
</ol>
<p class="ltx_p" id="S3.p3.2">Metadata is available for all videos, including the parameters used for camera calibration, 2D and 3D positions of keypoints, as well as a binary array depicting which keypoints were occluded in each frame. All this extracted data is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">BlendMimic3D is organized in the same manner as the Human3.6M dataset, with videos categorized by subject and action. The dataset includes synthetic subjects designated as SS1, SS2 and SS3. While SS1 focuses on self-occlusions, SS2 addresses object and out-of-frame occlusions. Each of these subjects covers 14 distinct actions. SS3, set in a smart store environment, manages both occlusions and multi-person scenarios. It offers two variations of the same action—one in a single-person context and the other in a multi-person setting. Just like with Human3.6M, each synthetic action is captured in four videos, each with a different perspective.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Pose Refinement with GCN</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Our proposed methodology is a pose refinement stage, illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S4.F4" title="Figure 4 ‣ 4 Pose Refinement with GCN ‣ Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_tag">4</span></a>, where we introduce our Graph Convolutional Network (GCN) as a plugin to enhance the estimated 3D poses. Our GCN is trained on BlendMimic3D dataset, which provides a diverse range of occlusion scenarios. This allows the network to learn and adapt to various occlusion types, refining the pose estimation for occluded joints.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="227" id="S4.F4.g1" src="extracted/2404.16136v1/images/GCNplugin_pt2.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S4.F4.3.2" style="font-size:90%;">Overview of the proposed framework. After any chosen 3D HPE algorithm, our Graph Convolutional Network (GCN) refines the estimated 3D poses by integrating spatial and temporal insights, leading to enhanced and precise 3D pose estimation, particularly effective in handling occlusions.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">The GCN not only considers spatial relationships between body joints but also temporal continuity across frames. It conceptualizes the human body as a graph structure, where nodes are body keypoints and edges represent joint connections. Unlike traditional models that primarily link a keypoint to its immediate neighbors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite>, our model, inspired by the work of Cai et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite> and Yu Cheng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib9" title=""><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite>, establishes broader connections across consecutive frames, as depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S4.F5" title="Figure 5 ‣ 4 Pose Refinement with GCN ‣ Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_tag">5</span></a>. This extended connectivity is crucial for accurately inferring occluded or ambiguous keypoints in challenging environments.</p>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="251" id="S4.F5.g1" src="extracted/2404.16136v1/images/GCN_Pose.png" width="299"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S4.F5.3.2" style="font-size:90%;">Illustration of the graph dynamics for the right elbow keypoint, with neighboring nodes categorized into six classes: (1) Center (red). (2) Physically-connected node closer to the spine (blue). (3) Physically-connected farther from the spine (green). (4) Symmetric node (pink). (5) Time-forward node (orange). (6) Time-backward (yellow). </span></figcaption>
</figure>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.2">Drawing on the formulation by Kipf and Welling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib17" title=""><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite>, their GCN approach refines spectral graph convolutions to enhance efficiency and scalability. Given a graph <math alttext="\mathcal{G}" class="ltx_Math" display="inline" id="S4.p3.1.m1.1"><semantics id="S4.p3.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml">𝒢</mi><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><ci id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1">𝒢</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">\mathcal{G}</annotation><annotation encoding="application/x-llamapun" id="S4.p3.1.m1.1d">caligraphic_G</annotation></semantics></math> with an adjacency matrix <math alttext="A" class="ltx_Math" display="inline" id="S4.p3.2.m2.1"><semantics id="S4.p3.2.m2.1a"><mi id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.1b"><ci id="S4.p3.2.m2.1.1.cmml" xref="S4.p3.2.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.1c">A</annotation><annotation encoding="application/x-llamapun" id="S4.p3.2.m2.1d">italic_A</annotation></semantics></math>, the propagation rule in their GCN model for each layer is expressed as</p>
</div>
<div class="ltx_para" id="S4.p4">
<table class="ltx_equation ltx_eqn_table" id="S4.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="H^{(l+1)}=\sigma\left(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}%
}H^{(l)}W^{(l)}\right)," class="ltx_Math" display="block" id="S4.E1.m1.4"><semantics id="S4.E1.m1.4a"><mrow id="S4.E1.m1.4.4.1" xref="S4.E1.m1.4.4.1.1.cmml"><mrow id="S4.E1.m1.4.4.1.1" xref="S4.E1.m1.4.4.1.1.cmml"><msup id="S4.E1.m1.4.4.1.1.3" xref="S4.E1.m1.4.4.1.1.3.cmml"><mi id="S4.E1.m1.4.4.1.1.3.2" xref="S4.E1.m1.4.4.1.1.3.2.cmml">H</mi><mrow id="S4.E1.m1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.cmml"><mo id="S4.E1.m1.1.1.1.1.2" stretchy="false" xref="S4.E1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E1.m1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.cmml"><mi id="S4.E1.m1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.2.cmml">l</mi><mo id="S4.E1.m1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.cmml">+</mo><mn id="S4.E1.m1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="S4.E1.m1.1.1.1.1.3" stretchy="false" xref="S4.E1.m1.1.1.1.1.1.cmml">)</mo></mrow></msup><mo id="S4.E1.m1.4.4.1.1.2" xref="S4.E1.m1.4.4.1.1.2.cmml">=</mo><mrow id="S4.E1.m1.4.4.1.1.1" xref="S4.E1.m1.4.4.1.1.1.cmml"><mi id="S4.E1.m1.4.4.1.1.1.3" xref="S4.E1.m1.4.4.1.1.1.3.cmml">σ</mi><mo id="S4.E1.m1.4.4.1.1.1.2" xref="S4.E1.m1.4.4.1.1.1.2.cmml">⁢</mo><mrow id="S4.E1.m1.4.4.1.1.1.1.1" xref="S4.E1.m1.4.4.1.1.1.1.1.1.cmml"><mo id="S4.E1.m1.4.4.1.1.1.1.1.2" xref="S4.E1.m1.4.4.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E1.m1.4.4.1.1.1.1.1.1" xref="S4.E1.m1.4.4.1.1.1.1.1.1.cmml"><msup id="S4.E1.m1.4.4.1.1.1.1.1.1.2" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.cmml"><mover accent="true" id="S4.E1.m1.4.4.1.1.1.1.1.1.2.2" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.2.cmml"><mi id="S4.E1.m1.4.4.1.1.1.1.1.1.2.2.2" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.2.2.cmml">D</mi><mo id="S4.E1.m1.4.4.1.1.1.1.1.1.2.2.1" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.2.1.cmml">~</mo></mover><mrow id="S4.E1.m1.4.4.1.1.1.1.1.1.2.3" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.3.cmml"><mo id="S4.E1.m1.4.4.1.1.1.1.1.1.2.3a" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.3.cmml">−</mo><mfrac id="S4.E1.m1.4.4.1.1.1.1.1.1.2.3.2" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.3.2.cmml"><mn id="S4.E1.m1.4.4.1.1.1.1.1.1.2.3.2.2" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.3.2.2.cmml">1</mn><mn id="S4.E1.m1.4.4.1.1.1.1.1.1.2.3.2.3" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.3.2.3.cmml">2</mn></mfrac></mrow></msup><mo id="S4.E1.m1.4.4.1.1.1.1.1.1.1" xref="S4.E1.m1.4.4.1.1.1.1.1.1.1.cmml">⁢</mo><mover accent="true" id="S4.E1.m1.4.4.1.1.1.1.1.1.3" xref="S4.E1.m1.4.4.1.1.1.1.1.1.3.cmml"><mi id="S4.E1.m1.4.4.1.1.1.1.1.1.3.2" xref="S4.E1.m1.4.4.1.1.1.1.1.1.3.2.cmml">A</mi><mo id="S4.E1.m1.4.4.1.1.1.1.1.1.3.1" xref="S4.E1.m1.4.4.1.1.1.1.1.1.3.1.cmml">~</mo></mover><mo id="S4.E1.m1.4.4.1.1.1.1.1.1.1a" xref="S4.E1.m1.4.4.1.1.1.1.1.1.1.cmml">⁢</mo><msup id="S4.E1.m1.4.4.1.1.1.1.1.1.4" xref="S4.E1.m1.4.4.1.1.1.1.1.1.4.cmml"><mover accent="true" id="S4.E1.m1.4.4.1.1.1.1.1.1.4.2" xref="S4.E1.m1.4.4.1.1.1.1.1.1.4.2.cmml"><mi id="S4.E1.m1.4.4.1.1.1.1.1.1.4.2.2" xref="S4.E1.m1.4.4.1.1.1.1.1.1.4.2.2.cmml">D</mi><mo id="S4.E1.m1.4.4.1.1.1.1.1.1.4.2.1" xref="S4.E1.m1.4.4.1.1.1.1.1.1.4.2.1.cmml">~</mo></mover><mrow id="S4.E1.m1.4.4.1.1.1.1.1.1.4.3" xref="S4.E1.m1.4.4.1.1.1.1.1.1.4.3.cmml"><mo id="S4.E1.m1.4.4.1.1.1.1.1.1.4.3a" xref="S4.E1.m1.4.4.1.1.1.1.1.1.4.3.cmml">−</mo><mfrac id="S4.E1.m1.4.4.1.1.1.1.1.1.4.3.2" xref="S4.E1.m1.4.4.1.1.1.1.1.1.4.3.2.cmml"><mn id="S4.E1.m1.4.4.1.1.1.1.1.1.4.3.2.2" xref="S4.E1.m1.4.4.1.1.1.1.1.1.4.3.2.2.cmml">1</mn><mn id="S4.E1.m1.4.4.1.1.1.1.1.1.4.3.2.3" xref="S4.E1.m1.4.4.1.1.1.1.1.1.4.3.2.3.cmml">2</mn></mfrac></mrow></msup><mo id="S4.E1.m1.4.4.1.1.1.1.1.1.1b" xref="S4.E1.m1.4.4.1.1.1.1.1.1.1.cmml">⁢</mo><msup id="S4.E1.m1.4.4.1.1.1.1.1.1.5" xref="S4.E1.m1.4.4.1.1.1.1.1.1.5.cmml"><mi id="S4.E1.m1.4.4.1.1.1.1.1.1.5.2" xref="S4.E1.m1.4.4.1.1.1.1.1.1.5.2.cmml">H</mi><mrow id="S4.E1.m1.2.2.1.3" xref="S4.E1.m1.4.4.1.1.1.1.1.1.5.cmml"><mo id="S4.E1.m1.2.2.1.3.1" stretchy="false" xref="S4.E1.m1.4.4.1.1.1.1.1.1.5.cmml">(</mo><mi id="S4.E1.m1.2.2.1.1" xref="S4.E1.m1.2.2.1.1.cmml">l</mi><mo id="S4.E1.m1.2.2.1.3.2" stretchy="false" xref="S4.E1.m1.4.4.1.1.1.1.1.1.5.cmml">)</mo></mrow></msup><mo id="S4.E1.m1.4.4.1.1.1.1.1.1.1c" xref="S4.E1.m1.4.4.1.1.1.1.1.1.1.cmml">⁢</mo><msup id="S4.E1.m1.4.4.1.1.1.1.1.1.6" xref="S4.E1.m1.4.4.1.1.1.1.1.1.6.cmml"><mi id="S4.E1.m1.4.4.1.1.1.1.1.1.6.2" xref="S4.E1.m1.4.4.1.1.1.1.1.1.6.2.cmml">W</mi><mrow id="S4.E1.m1.3.3.1.3" xref="S4.E1.m1.4.4.1.1.1.1.1.1.6.cmml"><mo id="S4.E1.m1.3.3.1.3.1" stretchy="false" xref="S4.E1.m1.4.4.1.1.1.1.1.1.6.cmml">(</mo><mi id="S4.E1.m1.3.3.1.1" xref="S4.E1.m1.3.3.1.1.cmml">l</mi><mo id="S4.E1.m1.3.3.1.3.2" stretchy="false" xref="S4.E1.m1.4.4.1.1.1.1.1.1.6.cmml">)</mo></mrow></msup></mrow><mo id="S4.E1.m1.4.4.1.1.1.1.1.3" xref="S4.E1.m1.4.4.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S4.E1.m1.4.4.1.2" xref="S4.E1.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.4b"><apply id="S4.E1.m1.4.4.1.1.cmml" xref="S4.E1.m1.4.4.1"><eq id="S4.E1.m1.4.4.1.1.2.cmml" xref="S4.E1.m1.4.4.1.1.2"></eq><apply id="S4.E1.m1.4.4.1.1.3.cmml" xref="S4.E1.m1.4.4.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.4.4.1.1.3.1.cmml" xref="S4.E1.m1.4.4.1.1.3">superscript</csymbol><ci id="S4.E1.m1.4.4.1.1.3.2.cmml" xref="S4.E1.m1.4.4.1.1.3.2">𝐻</ci><apply id="S4.E1.m1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1"><plus id="S4.E1.m1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1"></plus><ci id="S4.E1.m1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1.2">𝑙</ci><cn id="S4.E1.m1.1.1.1.1.1.3.cmml" type="integer" xref="S4.E1.m1.1.1.1.1.1.3">1</cn></apply></apply><apply id="S4.E1.m1.4.4.1.1.1.cmml" xref="S4.E1.m1.4.4.1.1.1"><times id="S4.E1.m1.4.4.1.1.1.2.cmml" xref="S4.E1.m1.4.4.1.1.1.2"></times><ci id="S4.E1.m1.4.4.1.1.1.3.cmml" xref="S4.E1.m1.4.4.1.1.1.3">𝜎</ci><apply id="S4.E1.m1.4.4.1.1.1.1.1.1.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1"><times id="S4.E1.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.1"></times><apply id="S4.E1.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.4.4.1.1.1.1.1.1.2.1.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2">superscript</csymbol><apply id="S4.E1.m1.4.4.1.1.1.1.1.1.2.2.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.2"><ci id="S4.E1.m1.4.4.1.1.1.1.1.1.2.2.1.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.2.1">~</ci><ci id="S4.E1.m1.4.4.1.1.1.1.1.1.2.2.2.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.2.2">𝐷</ci></apply><apply id="S4.E1.m1.4.4.1.1.1.1.1.1.2.3.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.3"><minus id="S4.E1.m1.4.4.1.1.1.1.1.1.2.3.1.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.3"></minus><apply id="S4.E1.m1.4.4.1.1.1.1.1.1.2.3.2.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.3.2"><divide id="S4.E1.m1.4.4.1.1.1.1.1.1.2.3.2.1.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.3.2"></divide><cn id="S4.E1.m1.4.4.1.1.1.1.1.1.2.3.2.2.cmml" type="integer" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.3.2.2">1</cn><cn id="S4.E1.m1.4.4.1.1.1.1.1.1.2.3.2.3.cmml" type="integer" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.3.2.3">2</cn></apply></apply></apply><apply id="S4.E1.m1.4.4.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.3"><ci id="S4.E1.m1.4.4.1.1.1.1.1.1.3.1.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.3.1">~</ci><ci id="S4.E1.m1.4.4.1.1.1.1.1.1.3.2.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.3.2">𝐴</ci></apply><apply id="S4.E1.m1.4.4.1.1.1.1.1.1.4.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S4.E1.m1.4.4.1.1.1.1.1.1.4.1.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.4">superscript</csymbol><apply id="S4.E1.m1.4.4.1.1.1.1.1.1.4.2.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.4.2"><ci id="S4.E1.m1.4.4.1.1.1.1.1.1.4.2.1.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.4.2.1">~</ci><ci id="S4.E1.m1.4.4.1.1.1.1.1.1.4.2.2.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.4.2.2">𝐷</ci></apply><apply id="S4.E1.m1.4.4.1.1.1.1.1.1.4.3.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.4.3"><minus id="S4.E1.m1.4.4.1.1.1.1.1.1.4.3.1.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.4.3"></minus><apply id="S4.E1.m1.4.4.1.1.1.1.1.1.4.3.2.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.4.3.2"><divide id="S4.E1.m1.4.4.1.1.1.1.1.1.4.3.2.1.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.4.3.2"></divide><cn id="S4.E1.m1.4.4.1.1.1.1.1.1.4.3.2.2.cmml" type="integer" xref="S4.E1.m1.4.4.1.1.1.1.1.1.4.3.2.2">1</cn><cn id="S4.E1.m1.4.4.1.1.1.1.1.1.4.3.2.3.cmml" type="integer" xref="S4.E1.m1.4.4.1.1.1.1.1.1.4.3.2.3">2</cn></apply></apply></apply><apply id="S4.E1.m1.4.4.1.1.1.1.1.1.5.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.5"><csymbol cd="ambiguous" id="S4.E1.m1.4.4.1.1.1.1.1.1.5.1.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.5">superscript</csymbol><ci id="S4.E1.m1.4.4.1.1.1.1.1.1.5.2.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.5.2">𝐻</ci><ci id="S4.E1.m1.2.2.1.1.cmml" xref="S4.E1.m1.2.2.1.1">𝑙</ci></apply><apply id="S4.E1.m1.4.4.1.1.1.1.1.1.6.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.6"><csymbol cd="ambiguous" id="S4.E1.m1.4.4.1.1.1.1.1.1.6.1.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.6">superscript</csymbol><ci id="S4.E1.m1.4.4.1.1.1.1.1.1.6.2.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.6.2">𝑊</ci><ci id="S4.E1.m1.3.3.1.1.cmml" xref="S4.E1.m1.3.3.1.1">𝑙</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.4c">H^{(l+1)}=\sigma\left(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}%
}H^{(l)}W^{(l)}\right),</annotation><annotation encoding="application/x-llamapun" id="S4.E1.m1.4d">italic_H start_POSTSUPERSCRIPT ( italic_l + 1 ) end_POSTSUPERSCRIPT = italic_σ ( over~ start_ARG italic_D end_ARG start_POSTSUPERSCRIPT - divide start_ARG 1 end_ARG start_ARG 2 end_ARG end_POSTSUPERSCRIPT over~ start_ARG italic_A end_ARG over~ start_ARG italic_D end_ARG start_POSTSUPERSCRIPT - divide start_ARG 1 end_ARG start_ARG 2 end_ARG end_POSTSUPERSCRIPT italic_H start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT italic_W start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.p4.8">where <math alttext="H^{(l)}" class="ltx_Math" display="inline" id="S4.p4.1.m1.1"><semantics id="S4.p4.1.m1.1a"><msup id="S4.p4.1.m1.1.2" xref="S4.p4.1.m1.1.2.cmml"><mi id="S4.p4.1.m1.1.2.2" xref="S4.p4.1.m1.1.2.2.cmml">H</mi><mrow id="S4.p4.1.m1.1.1.1.3" xref="S4.p4.1.m1.1.2.cmml"><mo id="S4.p4.1.m1.1.1.1.3.1" stretchy="false" xref="S4.p4.1.m1.1.2.cmml">(</mo><mi id="S4.p4.1.m1.1.1.1.1" xref="S4.p4.1.m1.1.1.1.1.cmml">l</mi><mo id="S4.p4.1.m1.1.1.1.3.2" stretchy="false" xref="S4.p4.1.m1.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.p4.1.m1.1b"><apply id="S4.p4.1.m1.1.2.cmml" xref="S4.p4.1.m1.1.2"><csymbol cd="ambiguous" id="S4.p4.1.m1.1.2.1.cmml" xref="S4.p4.1.m1.1.2">superscript</csymbol><ci id="S4.p4.1.m1.1.2.2.cmml" xref="S4.p4.1.m1.1.2.2">𝐻</ci><ci id="S4.p4.1.m1.1.1.1.1.cmml" xref="S4.p4.1.m1.1.1.1.1">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.1.m1.1c">H^{(l)}</annotation><annotation encoding="application/x-llamapun" id="S4.p4.1.m1.1d">italic_H start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT</annotation></semantics></math> is the activation matrix for the <math alttext="l" class="ltx_Math" display="inline" id="S4.p4.2.m2.1"><semantics id="S4.p4.2.m2.1a"><mi id="S4.p4.2.m2.1.1" xref="S4.p4.2.m2.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S4.p4.2.m2.1b"><ci id="S4.p4.2.m2.1.1.cmml" xref="S4.p4.2.m2.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.2.m2.1c">l</annotation><annotation encoding="application/x-llamapun" id="S4.p4.2.m2.1d">italic_l</annotation></semantics></math>-th layer. <math alttext="H^{(0)}" class="ltx_Math" display="inline" id="S4.p4.3.m3.1"><semantics id="S4.p4.3.m3.1a"><msup id="S4.p4.3.m3.1.2" xref="S4.p4.3.m3.1.2.cmml"><mi id="S4.p4.3.m3.1.2.2" xref="S4.p4.3.m3.1.2.2.cmml">H</mi><mrow id="S4.p4.3.m3.1.1.1.3" xref="S4.p4.3.m3.1.2.cmml"><mo id="S4.p4.3.m3.1.1.1.3.1" stretchy="false" xref="S4.p4.3.m3.1.2.cmml">(</mo><mn id="S4.p4.3.m3.1.1.1.1" xref="S4.p4.3.m3.1.1.1.1.cmml">0</mn><mo id="S4.p4.3.m3.1.1.1.3.2" stretchy="false" xref="S4.p4.3.m3.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.p4.3.m3.1b"><apply id="S4.p4.3.m3.1.2.cmml" xref="S4.p4.3.m3.1.2"><csymbol cd="ambiguous" id="S4.p4.3.m3.1.2.1.cmml" xref="S4.p4.3.m3.1.2">superscript</csymbol><ci id="S4.p4.3.m3.1.2.2.cmml" xref="S4.p4.3.m3.1.2.2">𝐻</ci><cn id="S4.p4.3.m3.1.1.1.1.cmml" type="integer" xref="S4.p4.3.m3.1.1.1.1">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.3.m3.1c">H^{(0)}</annotation><annotation encoding="application/x-llamapun" id="S4.p4.3.m3.1d">italic_H start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT</annotation></semantics></math> denotes the input feature matrix, with each row representing a feature vector for every node. <math alttext="W^{(l)}" class="ltx_Math" display="inline" id="S4.p4.4.m4.1"><semantics id="S4.p4.4.m4.1a"><msup id="S4.p4.4.m4.1.2" xref="S4.p4.4.m4.1.2.cmml"><mi id="S4.p4.4.m4.1.2.2" xref="S4.p4.4.m4.1.2.2.cmml">W</mi><mrow id="S4.p4.4.m4.1.1.1.3" xref="S4.p4.4.m4.1.2.cmml"><mo id="S4.p4.4.m4.1.1.1.3.1" stretchy="false" xref="S4.p4.4.m4.1.2.cmml">(</mo><mi id="S4.p4.4.m4.1.1.1.1" xref="S4.p4.4.m4.1.1.1.1.cmml">l</mi><mo id="S4.p4.4.m4.1.1.1.3.2" stretchy="false" xref="S4.p4.4.m4.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.p4.4.m4.1b"><apply id="S4.p4.4.m4.1.2.cmml" xref="S4.p4.4.m4.1.2"><csymbol cd="ambiguous" id="S4.p4.4.m4.1.2.1.cmml" xref="S4.p4.4.m4.1.2">superscript</csymbol><ci id="S4.p4.4.m4.1.2.2.cmml" xref="S4.p4.4.m4.1.2.2">𝑊</ci><ci id="S4.p4.4.m4.1.1.1.1.cmml" xref="S4.p4.4.m4.1.1.1.1">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.4.m4.1c">W^{(l)}</annotation><annotation encoding="application/x-llamapun" id="S4.p4.4.m4.1d">italic_W start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT</annotation></semantics></math>, often referred to as the kernel, is the weight matrix for the <math alttext="l" class="ltx_Math" display="inline" id="S4.p4.5.m5.1"><semantics id="S4.p4.5.m5.1a"><mi id="S4.p4.5.m5.1.1" xref="S4.p4.5.m5.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S4.p4.5.m5.1b"><ci id="S4.p4.5.m5.1.1.cmml" xref="S4.p4.5.m5.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.5.m5.1c">l</annotation><annotation encoding="application/x-llamapun" id="S4.p4.5.m5.1d">italic_l</annotation></semantics></math>-th layer. <math alttext="\sigma" class="ltx_Math" display="inline" id="S4.p4.6.m6.1"><semantics id="S4.p4.6.m6.1a"><mi id="S4.p4.6.m6.1.1" xref="S4.p4.6.m6.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S4.p4.6.m6.1b"><ci id="S4.p4.6.m6.1.1.cmml" xref="S4.p4.6.m6.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.6.m6.1c">\sigma</annotation><annotation encoding="application/x-llamapun" id="S4.p4.6.m6.1d">italic_σ</annotation></semantics></math> is an activation function, typically the ReLU. The augmented adjacency matrix, <math alttext="\tilde{A}=A+I" class="ltx_Math" display="inline" id="S4.p4.7.m7.1"><semantics id="S4.p4.7.m7.1a"><mrow id="S4.p4.7.m7.1.1" xref="S4.p4.7.m7.1.1.cmml"><mover accent="true" id="S4.p4.7.m7.1.1.2" xref="S4.p4.7.m7.1.1.2.cmml"><mi id="S4.p4.7.m7.1.1.2.2" xref="S4.p4.7.m7.1.1.2.2.cmml">A</mi><mo id="S4.p4.7.m7.1.1.2.1" xref="S4.p4.7.m7.1.1.2.1.cmml">~</mo></mover><mo id="S4.p4.7.m7.1.1.1" xref="S4.p4.7.m7.1.1.1.cmml">=</mo><mrow id="S4.p4.7.m7.1.1.3" xref="S4.p4.7.m7.1.1.3.cmml"><mi id="S4.p4.7.m7.1.1.3.2" xref="S4.p4.7.m7.1.1.3.2.cmml">A</mi><mo id="S4.p4.7.m7.1.1.3.1" xref="S4.p4.7.m7.1.1.3.1.cmml">+</mo><mi id="S4.p4.7.m7.1.1.3.3" xref="S4.p4.7.m7.1.1.3.3.cmml">I</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.7.m7.1b"><apply id="S4.p4.7.m7.1.1.cmml" xref="S4.p4.7.m7.1.1"><eq id="S4.p4.7.m7.1.1.1.cmml" xref="S4.p4.7.m7.1.1.1"></eq><apply id="S4.p4.7.m7.1.1.2.cmml" xref="S4.p4.7.m7.1.1.2"><ci id="S4.p4.7.m7.1.1.2.1.cmml" xref="S4.p4.7.m7.1.1.2.1">~</ci><ci id="S4.p4.7.m7.1.1.2.2.cmml" xref="S4.p4.7.m7.1.1.2.2">𝐴</ci></apply><apply id="S4.p4.7.m7.1.1.3.cmml" xref="S4.p4.7.m7.1.1.3"><plus id="S4.p4.7.m7.1.1.3.1.cmml" xref="S4.p4.7.m7.1.1.3.1"></plus><ci id="S4.p4.7.m7.1.1.3.2.cmml" xref="S4.p4.7.m7.1.1.3.2">𝐴</ci><ci id="S4.p4.7.m7.1.1.3.3.cmml" xref="S4.p4.7.m7.1.1.3.3">𝐼</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.7.m7.1c">\tilde{A}=A+I</annotation><annotation encoding="application/x-llamapun" id="S4.p4.7.m7.1d">over~ start_ARG italic_A end_ARG = italic_A + italic_I</annotation></semantics></math>, includes self-connections, and <math alttext="\tilde{D}" class="ltx_Math" display="inline" id="S4.p4.8.m8.1"><semantics id="S4.p4.8.m8.1a"><mover accent="true" id="S4.p4.8.m8.1.1" xref="S4.p4.8.m8.1.1.cmml"><mi id="S4.p4.8.m8.1.1.2" xref="S4.p4.8.m8.1.1.2.cmml">D</mi><mo id="S4.p4.8.m8.1.1.1" xref="S4.p4.8.m8.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S4.p4.8.m8.1b"><apply id="S4.p4.8.m8.1.1.cmml" xref="S4.p4.8.m8.1.1"><ci id="S4.p4.8.m8.1.1.1.cmml" xref="S4.p4.8.m8.1.1.1">~</ci><ci id="S4.p4.8.m8.1.1.2.cmml" xref="S4.p4.8.m8.1.1.2">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.8.m8.1c">\tilde{D}</annotation><annotation encoding="application/x-llamapun" id="S4.p4.8.m8.1d">over~ start_ARG italic_D end_ARG</annotation></semantics></math> is its corresponding diagonal node degree matrix.</p>
</div>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p" id="S4.p5.2">Kipf and Welling’s strategy uses the normalized adjacency matrix to spread node features across the graph. Normalization by the degree matrix <math alttext="\tilde{D}" class="ltx_Math" display="inline" id="S4.p5.1.m1.1"><semantics id="S4.p5.1.m1.1a"><mover accent="true" id="S4.p5.1.m1.1.1" xref="S4.p5.1.m1.1.1.cmml"><mi id="S4.p5.1.m1.1.1.2" xref="S4.p5.1.m1.1.1.2.cmml">D</mi><mo id="S4.p5.1.m1.1.1.1" xref="S4.p5.1.m1.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S4.p5.1.m1.1b"><apply id="S4.p5.1.m1.1.1.cmml" xref="S4.p5.1.m1.1.1"><ci id="S4.p5.1.m1.1.1.1.cmml" xref="S4.p5.1.m1.1.1.1">~</ci><ci id="S4.p5.1.m1.1.1.2.cmml" xref="S4.p5.1.m1.1.1.2">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.1.m1.1c">\tilde{D}</annotation><annotation encoding="application/x-llamapun" id="S4.p5.1.m1.1d">over~ start_ARG italic_D end_ARG</annotation></semantics></math> ensures stable gradients and effective training. In (<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S4.E1" title="Equation 1 ‣ 4 Pose Refinement with GCN ‣ Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_tag">1</span></a>), the kernel <math alttext="W^{(l)}" class="ltx_Math" display="inline" id="S4.p5.2.m2.1"><semantics id="S4.p5.2.m2.1a"><msup id="S4.p5.2.m2.1.2" xref="S4.p5.2.m2.1.2.cmml"><mi id="S4.p5.2.m2.1.2.2" xref="S4.p5.2.m2.1.2.2.cmml">W</mi><mrow id="S4.p5.2.m2.1.1.1.3" xref="S4.p5.2.m2.1.2.cmml"><mo id="S4.p5.2.m2.1.1.1.3.1" stretchy="false" xref="S4.p5.2.m2.1.2.cmml">(</mo><mi id="S4.p5.2.m2.1.1.1.1" xref="S4.p5.2.m2.1.1.1.1.cmml">l</mi><mo id="S4.p5.2.m2.1.1.1.3.2" stretchy="false" xref="S4.p5.2.m2.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.p5.2.m2.1b"><apply id="S4.p5.2.m2.1.2.cmml" xref="S4.p5.2.m2.1.2"><csymbol cd="ambiguous" id="S4.p5.2.m2.1.2.1.cmml" xref="S4.p5.2.m2.1.2">superscript</csymbol><ci id="S4.p5.2.m2.1.2.2.cmml" xref="S4.p5.2.m2.1.2.2">𝑊</ci><ci id="S4.p5.2.m2.1.1.1.1.cmml" xref="S4.p5.2.m2.1.1.1.1">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.2.m2.1c">W^{(l)}</annotation><annotation encoding="application/x-llamapun" id="S4.p5.2.m2.1d">italic_W start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT</annotation></semantics></math>, is shared by all 1-hop neighboring nodes, suggesting a consistent treatment of these immediate neighbors.</p>
</div>
<div class="ltx_para" id="S4.p6">
<p class="ltx_p" id="S4.p6.11">To enhance this approach, we expanded from merely considering 1-hop neighbors, recognizing the need for distinct kernels tailored to different neighboring nodes based on their semantics. Following (<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S4.E1" title="Equation 1 ‣ 4 Pose Refinement with GCN ‣ Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_tag">1</span></a>), we devised a spatial-temporal undirected graph <math alttext="\mathcal{G}=(\mathcal{V},\mathcal{E},A)" class="ltx_Math" display="inline" id="S4.p6.1.m1.3"><semantics id="S4.p6.1.m1.3a"><mrow id="S4.p6.1.m1.3.4" xref="S4.p6.1.m1.3.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p6.1.m1.3.4.2" xref="S4.p6.1.m1.3.4.2.cmml">𝒢</mi><mo id="S4.p6.1.m1.3.4.1" xref="S4.p6.1.m1.3.4.1.cmml">=</mo><mrow id="S4.p6.1.m1.3.4.3.2" xref="S4.p6.1.m1.3.4.3.1.cmml"><mo id="S4.p6.1.m1.3.4.3.2.1" stretchy="false" xref="S4.p6.1.m1.3.4.3.1.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S4.p6.1.m1.1.1" xref="S4.p6.1.m1.1.1.cmml">𝒱</mi><mo id="S4.p6.1.m1.3.4.3.2.2" xref="S4.p6.1.m1.3.4.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S4.p6.1.m1.2.2" xref="S4.p6.1.m1.2.2.cmml">ℰ</mi><mo id="S4.p6.1.m1.3.4.3.2.3" xref="S4.p6.1.m1.3.4.3.1.cmml">,</mo><mi id="S4.p6.1.m1.3.3" xref="S4.p6.1.m1.3.3.cmml">A</mi><mo id="S4.p6.1.m1.3.4.3.2.4" stretchy="false" xref="S4.p6.1.m1.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p6.1.m1.3b"><apply id="S4.p6.1.m1.3.4.cmml" xref="S4.p6.1.m1.3.4"><eq id="S4.p6.1.m1.3.4.1.cmml" xref="S4.p6.1.m1.3.4.1"></eq><ci id="S4.p6.1.m1.3.4.2.cmml" xref="S4.p6.1.m1.3.4.2">𝒢</ci><vector id="S4.p6.1.m1.3.4.3.1.cmml" xref="S4.p6.1.m1.3.4.3.2"><ci id="S4.p6.1.m1.1.1.cmml" xref="S4.p6.1.m1.1.1">𝒱</ci><ci id="S4.p6.1.m1.2.2.cmml" xref="S4.p6.1.m1.2.2">ℰ</ci><ci id="S4.p6.1.m1.3.3.cmml" xref="S4.p6.1.m1.3.3">𝐴</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.1.m1.3c">\mathcal{G}=(\mathcal{V},\mathcal{E},A)</annotation><annotation encoding="application/x-llamapun" id="S4.p6.1.m1.3d">caligraphic_G = ( caligraphic_V , caligraphic_E , italic_A )</annotation></semantics></math>. In this graph, <math alttext="\mathcal{V}\in\mathbb{R}^{T\times J}" class="ltx_Math" display="inline" id="S4.p6.2.m2.1"><semantics id="S4.p6.2.m2.1a"><mrow id="S4.p6.2.m2.1.1" xref="S4.p6.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p6.2.m2.1.1.2" xref="S4.p6.2.m2.1.1.2.cmml">𝒱</mi><mo id="S4.p6.2.m2.1.1.1" xref="S4.p6.2.m2.1.1.1.cmml">∈</mo><msup id="S4.p6.2.m2.1.1.3" xref="S4.p6.2.m2.1.1.3.cmml"><mi id="S4.p6.2.m2.1.1.3.2" xref="S4.p6.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S4.p6.2.m2.1.1.3.3" xref="S4.p6.2.m2.1.1.3.3.cmml"><mi id="S4.p6.2.m2.1.1.3.3.2" xref="S4.p6.2.m2.1.1.3.3.2.cmml">T</mi><mo id="S4.p6.2.m2.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S4.p6.2.m2.1.1.3.3.1.cmml">×</mo><mi id="S4.p6.2.m2.1.1.3.3.3" xref="S4.p6.2.m2.1.1.3.3.3.cmml">J</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.p6.2.m2.1b"><apply id="S4.p6.2.m2.1.1.cmml" xref="S4.p6.2.m2.1.1"><in id="S4.p6.2.m2.1.1.1.cmml" xref="S4.p6.2.m2.1.1.1"></in><ci id="S4.p6.2.m2.1.1.2.cmml" xref="S4.p6.2.m2.1.1.2">𝒱</ci><apply id="S4.p6.2.m2.1.1.3.cmml" xref="S4.p6.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.p6.2.m2.1.1.3.1.cmml" xref="S4.p6.2.m2.1.1.3">superscript</csymbol><ci id="S4.p6.2.m2.1.1.3.2.cmml" xref="S4.p6.2.m2.1.1.3.2">ℝ</ci><apply id="S4.p6.2.m2.1.1.3.3.cmml" xref="S4.p6.2.m2.1.1.3.3"><times id="S4.p6.2.m2.1.1.3.3.1.cmml" xref="S4.p6.2.m2.1.1.3.3.1"></times><ci id="S4.p6.2.m2.1.1.3.3.2.cmml" xref="S4.p6.2.m2.1.1.3.3.2">𝑇</ci><ci id="S4.p6.2.m2.1.1.3.3.3.cmml" xref="S4.p6.2.m2.1.1.3.3.3">𝐽</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.2.m2.1c">\mathcal{V}\in\mathbb{R}^{T\times J}</annotation><annotation encoding="application/x-llamapun" id="S4.p6.2.m2.1d">caligraphic_V ∈ blackboard_R start_POSTSUPERSCRIPT italic_T × italic_J end_POSTSUPERSCRIPT</annotation></semantics></math> signifies the vertices set corresponding to <math alttext="T" class="ltx_Math" display="inline" id="S4.p6.3.m3.1"><semantics id="S4.p6.3.m3.1a"><mi id="S4.p6.3.m3.1.1" xref="S4.p6.3.m3.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.p6.3.m3.1b"><ci id="S4.p6.3.m3.1.1.cmml" xref="S4.p6.3.m3.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.3.m3.1c">T</annotation><annotation encoding="application/x-llamapun" id="S4.p6.3.m3.1d">italic_T</annotation></semantics></math> consecutive frames (one for each, past, present, and future), with <math alttext="J" class="ltx_Math" display="inline" id="S4.p6.4.m4.1"><semantics id="S4.p6.4.m4.1a"><mi id="S4.p6.4.m4.1.1" xref="S4.p6.4.m4.1.1.cmml">J</mi><annotation-xml encoding="MathML-Content" id="S4.p6.4.m4.1b"><ci id="S4.p6.4.m4.1.1.cmml" xref="S4.p6.4.m4.1.1">𝐽</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.4.m4.1c">J</annotation><annotation encoding="application/x-llamapun" id="S4.p6.4.m4.1d">italic_J</annotation></semantics></math> joints in each frame. <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S4.p6.5.m5.1"><semantics id="S4.p6.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S4.p6.5.m5.1.1" xref="S4.p6.5.m5.1.1.cmml">ℰ</mi><annotation-xml encoding="MathML-Content" id="S4.p6.5.m5.1b"><ci id="S4.p6.5.m5.1.1.cmml" xref="S4.p6.5.m5.1.1">ℰ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.5.m5.1c">\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="S4.p6.5.m5.1d">caligraphic_E</annotation></semantics></math> represents the nodes’ connections. The adjacency matrix <math alttext="A\in\mathbb{R}^{P\times P}" class="ltx_Math" display="inline" id="S4.p6.6.m6.1"><semantics id="S4.p6.6.m6.1a"><mrow id="S4.p6.6.m6.1.1" xref="S4.p6.6.m6.1.1.cmml"><mi id="S4.p6.6.m6.1.1.2" xref="S4.p6.6.m6.1.1.2.cmml">A</mi><mo id="S4.p6.6.m6.1.1.1" xref="S4.p6.6.m6.1.1.1.cmml">∈</mo><msup id="S4.p6.6.m6.1.1.3" xref="S4.p6.6.m6.1.1.3.cmml"><mi id="S4.p6.6.m6.1.1.3.2" xref="S4.p6.6.m6.1.1.3.2.cmml">ℝ</mi><mrow id="S4.p6.6.m6.1.1.3.3" xref="S4.p6.6.m6.1.1.3.3.cmml"><mi id="S4.p6.6.m6.1.1.3.3.2" xref="S4.p6.6.m6.1.1.3.3.2.cmml">P</mi><mo id="S4.p6.6.m6.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S4.p6.6.m6.1.1.3.3.1.cmml">×</mo><mi id="S4.p6.6.m6.1.1.3.3.3" xref="S4.p6.6.m6.1.1.3.3.3.cmml">P</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.p6.6.m6.1b"><apply id="S4.p6.6.m6.1.1.cmml" xref="S4.p6.6.m6.1.1"><in id="S4.p6.6.m6.1.1.1.cmml" xref="S4.p6.6.m6.1.1.1"></in><ci id="S4.p6.6.m6.1.1.2.cmml" xref="S4.p6.6.m6.1.1.2">𝐴</ci><apply id="S4.p6.6.m6.1.1.3.cmml" xref="S4.p6.6.m6.1.1.3"><csymbol cd="ambiguous" id="S4.p6.6.m6.1.1.3.1.cmml" xref="S4.p6.6.m6.1.1.3">superscript</csymbol><ci id="S4.p6.6.m6.1.1.3.2.cmml" xref="S4.p6.6.m6.1.1.3.2">ℝ</ci><apply id="S4.p6.6.m6.1.1.3.3.cmml" xref="S4.p6.6.m6.1.1.3.3"><times id="S4.p6.6.m6.1.1.3.3.1.cmml" xref="S4.p6.6.m6.1.1.3.3.1"></times><ci id="S4.p6.6.m6.1.1.3.3.2.cmml" xref="S4.p6.6.m6.1.1.3.3.2">𝑃</ci><ci id="S4.p6.6.m6.1.1.3.3.3.cmml" xref="S4.p6.6.m6.1.1.3.3.3">𝑃</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.6.m6.1c">A\in\mathbb{R}^{P\times P}</annotation><annotation encoding="application/x-llamapun" id="S4.p6.6.m6.1d">italic_A ∈ blackboard_R start_POSTSUPERSCRIPT italic_P × italic_P end_POSTSUPERSCRIPT</annotation></semantics></math>, considering <math alttext="P=TJ" class="ltx_Math" display="inline" id="S4.p6.7.m7.1"><semantics id="S4.p6.7.m7.1a"><mrow id="S4.p6.7.m7.1.1" xref="S4.p6.7.m7.1.1.cmml"><mi id="S4.p6.7.m7.1.1.2" xref="S4.p6.7.m7.1.1.2.cmml">P</mi><mo id="S4.p6.7.m7.1.1.1" xref="S4.p6.7.m7.1.1.1.cmml">=</mo><mrow id="S4.p6.7.m7.1.1.3" xref="S4.p6.7.m7.1.1.3.cmml"><mi id="S4.p6.7.m7.1.1.3.2" xref="S4.p6.7.m7.1.1.3.2.cmml">T</mi><mo id="S4.p6.7.m7.1.1.3.1" xref="S4.p6.7.m7.1.1.3.1.cmml">⁢</mo><mi id="S4.p6.7.m7.1.1.3.3" xref="S4.p6.7.m7.1.1.3.3.cmml">J</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p6.7.m7.1b"><apply id="S4.p6.7.m7.1.1.cmml" xref="S4.p6.7.m7.1.1"><eq id="S4.p6.7.m7.1.1.1.cmml" xref="S4.p6.7.m7.1.1.1"></eq><ci id="S4.p6.7.m7.1.1.2.cmml" xref="S4.p6.7.m7.1.1.2">𝑃</ci><apply id="S4.p6.7.m7.1.1.3.cmml" xref="S4.p6.7.m7.1.1.3"><times id="S4.p6.7.m7.1.1.3.1.cmml" xref="S4.p6.7.m7.1.1.3.1"></times><ci id="S4.p6.7.m7.1.1.3.2.cmml" xref="S4.p6.7.m7.1.1.3.2">𝑇</ci><ci id="S4.p6.7.m7.1.1.3.3.cmml" xref="S4.p6.7.m7.1.1.3.3">𝐽</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.7.m7.1c">P=TJ</annotation><annotation encoding="application/x-llamapun" id="S4.p6.7.m7.1d">italic_P = italic_T italic_J</annotation></semantics></math>, that <math alttext="a_{ij}=0" class="ltx_Math" display="inline" id="S4.p6.8.m8.1"><semantics id="S4.p6.8.m8.1a"><mrow id="S4.p6.8.m8.1.1" xref="S4.p6.8.m8.1.1.cmml"><msub id="S4.p6.8.m8.1.1.2" xref="S4.p6.8.m8.1.1.2.cmml"><mi id="S4.p6.8.m8.1.1.2.2" xref="S4.p6.8.m8.1.1.2.2.cmml">a</mi><mrow id="S4.p6.8.m8.1.1.2.3" xref="S4.p6.8.m8.1.1.2.3.cmml"><mi id="S4.p6.8.m8.1.1.2.3.2" xref="S4.p6.8.m8.1.1.2.3.2.cmml">i</mi><mo id="S4.p6.8.m8.1.1.2.3.1" xref="S4.p6.8.m8.1.1.2.3.1.cmml">⁢</mo><mi id="S4.p6.8.m8.1.1.2.3.3" xref="S4.p6.8.m8.1.1.2.3.3.cmml">j</mi></mrow></msub><mo id="S4.p6.8.m8.1.1.1" xref="S4.p6.8.m8.1.1.1.cmml">=</mo><mn id="S4.p6.8.m8.1.1.3" xref="S4.p6.8.m8.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p6.8.m8.1b"><apply id="S4.p6.8.m8.1.1.cmml" xref="S4.p6.8.m8.1.1"><eq id="S4.p6.8.m8.1.1.1.cmml" xref="S4.p6.8.m8.1.1.1"></eq><apply id="S4.p6.8.m8.1.1.2.cmml" xref="S4.p6.8.m8.1.1.2"><csymbol cd="ambiguous" id="S4.p6.8.m8.1.1.2.1.cmml" xref="S4.p6.8.m8.1.1.2">subscript</csymbol><ci id="S4.p6.8.m8.1.1.2.2.cmml" xref="S4.p6.8.m8.1.1.2.2">𝑎</ci><apply id="S4.p6.8.m8.1.1.2.3.cmml" xref="S4.p6.8.m8.1.1.2.3"><times id="S4.p6.8.m8.1.1.2.3.1.cmml" xref="S4.p6.8.m8.1.1.2.3.1"></times><ci id="S4.p6.8.m8.1.1.2.3.2.cmml" xref="S4.p6.8.m8.1.1.2.3.2">𝑖</ci><ci id="S4.p6.8.m8.1.1.2.3.3.cmml" xref="S4.p6.8.m8.1.1.2.3.3">𝑗</ci></apply></apply><cn id="S4.p6.8.m8.1.1.3.cmml" type="integer" xref="S4.p6.8.m8.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.8.m8.1c">a_{ij}=0</annotation><annotation encoding="application/x-llamapun" id="S4.p6.8.m8.1d">italic_a start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = 0</annotation></semantics></math> if <math alttext="(i,j)\not\in\mathcal{E}" class="ltx_Math" display="inline" id="S4.p6.9.m9.2"><semantics id="S4.p6.9.m9.2a"><mrow id="S4.p6.9.m9.2.3" xref="S4.p6.9.m9.2.3.cmml"><mrow id="S4.p6.9.m9.2.3.2.2" xref="S4.p6.9.m9.2.3.2.1.cmml"><mo id="S4.p6.9.m9.2.3.2.2.1" stretchy="false" xref="S4.p6.9.m9.2.3.2.1.cmml">(</mo><mi id="S4.p6.9.m9.1.1" xref="S4.p6.9.m9.1.1.cmml">i</mi><mo id="S4.p6.9.m9.2.3.2.2.2" xref="S4.p6.9.m9.2.3.2.1.cmml">,</mo><mi id="S4.p6.9.m9.2.2" xref="S4.p6.9.m9.2.2.cmml">j</mi><mo id="S4.p6.9.m9.2.3.2.2.3" stretchy="false" xref="S4.p6.9.m9.2.3.2.1.cmml">)</mo></mrow><mo id="S4.p6.9.m9.2.3.1" xref="S4.p6.9.m9.2.3.1.cmml">∉</mo><mi class="ltx_font_mathcaligraphic" id="S4.p6.9.m9.2.3.3" xref="S4.p6.9.m9.2.3.3.cmml">ℰ</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p6.9.m9.2b"><apply id="S4.p6.9.m9.2.3.cmml" xref="S4.p6.9.m9.2.3"><notin id="S4.p6.9.m9.2.3.1.cmml" xref="S4.p6.9.m9.2.3.1"></notin><interval closure="open" id="S4.p6.9.m9.2.3.2.1.cmml" xref="S4.p6.9.m9.2.3.2.2"><ci id="S4.p6.9.m9.1.1.cmml" xref="S4.p6.9.m9.1.1">𝑖</ci><ci id="S4.p6.9.m9.2.2.cmml" xref="S4.p6.9.m9.2.2">𝑗</ci></interval><ci id="S4.p6.9.m9.2.3.3.cmml" xref="S4.p6.9.m9.2.3.3">ℰ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.9.m9.2c">(i,j)\not\in\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="S4.p6.9.m9.2d">( italic_i , italic_j ) ∉ caligraphic_E</annotation></semantics></math> and <math alttext="a_{ij}=1" class="ltx_Math" display="inline" id="S4.p6.10.m10.1"><semantics id="S4.p6.10.m10.1a"><mrow id="S4.p6.10.m10.1.1" xref="S4.p6.10.m10.1.1.cmml"><msub id="S4.p6.10.m10.1.1.2" xref="S4.p6.10.m10.1.1.2.cmml"><mi id="S4.p6.10.m10.1.1.2.2" xref="S4.p6.10.m10.1.1.2.2.cmml">a</mi><mrow id="S4.p6.10.m10.1.1.2.3" xref="S4.p6.10.m10.1.1.2.3.cmml"><mi id="S4.p6.10.m10.1.1.2.3.2" xref="S4.p6.10.m10.1.1.2.3.2.cmml">i</mi><mo id="S4.p6.10.m10.1.1.2.3.1" xref="S4.p6.10.m10.1.1.2.3.1.cmml">⁢</mo><mi id="S4.p6.10.m10.1.1.2.3.3" xref="S4.p6.10.m10.1.1.2.3.3.cmml">j</mi></mrow></msub><mo id="S4.p6.10.m10.1.1.1" xref="S4.p6.10.m10.1.1.1.cmml">=</mo><mn id="S4.p6.10.m10.1.1.3" xref="S4.p6.10.m10.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p6.10.m10.1b"><apply id="S4.p6.10.m10.1.1.cmml" xref="S4.p6.10.m10.1.1"><eq id="S4.p6.10.m10.1.1.1.cmml" xref="S4.p6.10.m10.1.1.1"></eq><apply id="S4.p6.10.m10.1.1.2.cmml" xref="S4.p6.10.m10.1.1.2"><csymbol cd="ambiguous" id="S4.p6.10.m10.1.1.2.1.cmml" xref="S4.p6.10.m10.1.1.2">subscript</csymbol><ci id="S4.p6.10.m10.1.1.2.2.cmml" xref="S4.p6.10.m10.1.1.2.2">𝑎</ci><apply id="S4.p6.10.m10.1.1.2.3.cmml" xref="S4.p6.10.m10.1.1.2.3"><times id="S4.p6.10.m10.1.1.2.3.1.cmml" xref="S4.p6.10.m10.1.1.2.3.1"></times><ci id="S4.p6.10.m10.1.1.2.3.2.cmml" xref="S4.p6.10.m10.1.1.2.3.2">𝑖</ci><ci id="S4.p6.10.m10.1.1.2.3.3.cmml" xref="S4.p6.10.m10.1.1.2.3.3">𝑗</ci></apply></apply><cn id="S4.p6.10.m10.1.1.3.cmml" type="integer" xref="S4.p6.10.m10.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.10.m10.1c">a_{ij}=1</annotation><annotation encoding="application/x-llamapun" id="S4.p6.10.m10.1d">italic_a start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = 1</annotation></semantics></math> if <math alttext="(i,j)\in\mathcal{E}" class="ltx_Math" display="inline" id="S4.p6.11.m11.2"><semantics id="S4.p6.11.m11.2a"><mrow id="S4.p6.11.m11.2.3" xref="S4.p6.11.m11.2.3.cmml"><mrow id="S4.p6.11.m11.2.3.2.2" xref="S4.p6.11.m11.2.3.2.1.cmml"><mo id="S4.p6.11.m11.2.3.2.2.1" stretchy="false" xref="S4.p6.11.m11.2.3.2.1.cmml">(</mo><mi id="S4.p6.11.m11.1.1" xref="S4.p6.11.m11.1.1.cmml">i</mi><mo id="S4.p6.11.m11.2.3.2.2.2" xref="S4.p6.11.m11.2.3.2.1.cmml">,</mo><mi id="S4.p6.11.m11.2.2" xref="S4.p6.11.m11.2.2.cmml">j</mi><mo id="S4.p6.11.m11.2.3.2.2.3" stretchy="false" xref="S4.p6.11.m11.2.3.2.1.cmml">)</mo></mrow><mo id="S4.p6.11.m11.2.3.1" xref="S4.p6.11.m11.2.3.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S4.p6.11.m11.2.3.3" xref="S4.p6.11.m11.2.3.3.cmml">ℰ</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p6.11.m11.2b"><apply id="S4.p6.11.m11.2.3.cmml" xref="S4.p6.11.m11.2.3"><in id="S4.p6.11.m11.2.3.1.cmml" xref="S4.p6.11.m11.2.3.1"></in><interval closure="open" id="S4.p6.11.m11.2.3.2.1.cmml" xref="S4.p6.11.m11.2.3.2.2"><ci id="S4.p6.11.m11.1.1.cmml" xref="S4.p6.11.m11.1.1">𝑖</ci><ci id="S4.p6.11.m11.2.2.cmml" xref="S4.p6.11.m11.2.2">𝑗</ci></interval><ci id="S4.p6.11.m11.2.3.3.cmml" xref="S4.p6.11.m11.2.3.3">ℰ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.11.m11.2c">(i,j)\in\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="S4.p6.11.m11.2d">( italic_i , italic_j ) ∈ caligraphic_E</annotation></semantics></math>. This adjacency matrix captures both spatial and temporal dynamics across frames.</p>
</div>
<div class="ltx_para" id="S4.p7">
<p class="ltx_p" id="S4.p7.4">By classifying neighboring nodes and understanding their semantic relationships (as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S4.F5" title="Figure 5 ‣ 4 Pose Refinement with GCN ‣ Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_tag">5</span></a>), we apply distinct kernels for each class of neighborhood. Drawing from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>, consider an input signal <math alttext="X\in\mathbb{R}^{P\times C}" class="ltx_Math" display="inline" id="S4.p7.1.m1.1"><semantics id="S4.p7.1.m1.1a"><mrow id="S4.p7.1.m1.1.1" xref="S4.p7.1.m1.1.1.cmml"><mi id="S4.p7.1.m1.1.1.2" xref="S4.p7.1.m1.1.1.2.cmml">X</mi><mo id="S4.p7.1.m1.1.1.1" xref="S4.p7.1.m1.1.1.1.cmml">∈</mo><msup id="S4.p7.1.m1.1.1.3" xref="S4.p7.1.m1.1.1.3.cmml"><mi id="S4.p7.1.m1.1.1.3.2" xref="S4.p7.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S4.p7.1.m1.1.1.3.3" xref="S4.p7.1.m1.1.1.3.3.cmml"><mi id="S4.p7.1.m1.1.1.3.3.2" xref="S4.p7.1.m1.1.1.3.3.2.cmml">P</mi><mo id="S4.p7.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S4.p7.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S4.p7.1.m1.1.1.3.3.3" xref="S4.p7.1.m1.1.1.3.3.3.cmml">C</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.p7.1.m1.1b"><apply id="S4.p7.1.m1.1.1.cmml" xref="S4.p7.1.m1.1.1"><in id="S4.p7.1.m1.1.1.1.cmml" xref="S4.p7.1.m1.1.1.1"></in><ci id="S4.p7.1.m1.1.1.2.cmml" xref="S4.p7.1.m1.1.1.2">𝑋</ci><apply id="S4.p7.1.m1.1.1.3.cmml" xref="S4.p7.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.p7.1.m1.1.1.3.1.cmml" xref="S4.p7.1.m1.1.1.3">superscript</csymbol><ci id="S4.p7.1.m1.1.1.3.2.cmml" xref="S4.p7.1.m1.1.1.3.2">ℝ</ci><apply id="S4.p7.1.m1.1.1.3.3.cmml" xref="S4.p7.1.m1.1.1.3.3"><times id="S4.p7.1.m1.1.1.3.3.1.cmml" xref="S4.p7.1.m1.1.1.3.3.1"></times><ci id="S4.p7.1.m1.1.1.3.3.2.cmml" xref="S4.p7.1.m1.1.1.3.3.2">𝑃</ci><ci id="S4.p7.1.m1.1.1.3.3.3.cmml" xref="S4.p7.1.m1.1.1.3.3.3">𝐶</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.1.m1.1c">X\in\mathbb{R}^{P\times C}</annotation><annotation encoding="application/x-llamapun" id="S4.p7.1.m1.1d">italic_X ∈ blackboard_R start_POSTSUPERSCRIPT italic_P × italic_C end_POSTSUPERSCRIPT</annotation></semantics></math> that represents <math alttext="C" class="ltx_Math" display="inline" id="S4.p7.2.m2.1"><semantics id="S4.p7.2.m2.1a"><mi id="S4.p7.2.m2.1.1" xref="S4.p7.2.m2.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S4.p7.2.m2.1b"><ci id="S4.p7.2.m2.1.1.cmml" xref="S4.p7.2.m2.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.2.m2.1c">C</annotation><annotation encoding="application/x-llamapun" id="S4.p7.2.m2.1d">italic_C</annotation></semantics></math>-dimensional features of <math alttext="P" class="ltx_Math" display="inline" id="S4.p7.3.m3.1"><semantics id="S4.p7.3.m3.1a"><mi id="S4.p7.3.m3.1.1" xref="S4.p7.3.m3.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S4.p7.3.m3.1b"><ci id="S4.p7.3.m3.1.1.cmml" xref="S4.p7.3.m3.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.3.m3.1c">P</annotation><annotation encoding="application/x-llamapun" id="S4.p7.3.m3.1d">italic_P</annotation></semantics></math> vertices on the graph. The convolved signal matrix <math alttext="Z\in\mathbb{R}^{P\times C}" class="ltx_Math" display="inline" id="S4.p7.4.m4.1"><semantics id="S4.p7.4.m4.1a"><mrow id="S4.p7.4.m4.1.1" xref="S4.p7.4.m4.1.1.cmml"><mi id="S4.p7.4.m4.1.1.2" xref="S4.p7.4.m4.1.1.2.cmml">Z</mi><mo id="S4.p7.4.m4.1.1.1" xref="S4.p7.4.m4.1.1.1.cmml">∈</mo><msup id="S4.p7.4.m4.1.1.3" xref="S4.p7.4.m4.1.1.3.cmml"><mi id="S4.p7.4.m4.1.1.3.2" xref="S4.p7.4.m4.1.1.3.2.cmml">ℝ</mi><mrow id="S4.p7.4.m4.1.1.3.3" xref="S4.p7.4.m4.1.1.3.3.cmml"><mi id="S4.p7.4.m4.1.1.3.3.2" xref="S4.p7.4.m4.1.1.3.3.2.cmml">P</mi><mo id="S4.p7.4.m4.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S4.p7.4.m4.1.1.3.3.1.cmml">×</mo><mi id="S4.p7.4.m4.1.1.3.3.3" xref="S4.p7.4.m4.1.1.3.3.3.cmml">C</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.p7.4.m4.1b"><apply id="S4.p7.4.m4.1.1.cmml" xref="S4.p7.4.m4.1.1"><in id="S4.p7.4.m4.1.1.1.cmml" xref="S4.p7.4.m4.1.1.1"></in><ci id="S4.p7.4.m4.1.1.2.cmml" xref="S4.p7.4.m4.1.1.2">𝑍</ci><apply id="S4.p7.4.m4.1.1.3.cmml" xref="S4.p7.4.m4.1.1.3"><csymbol cd="ambiguous" id="S4.p7.4.m4.1.1.3.1.cmml" xref="S4.p7.4.m4.1.1.3">superscript</csymbol><ci id="S4.p7.4.m4.1.1.3.2.cmml" xref="S4.p7.4.m4.1.1.3.2">ℝ</ci><apply id="S4.p7.4.m4.1.1.3.3.cmml" xref="S4.p7.4.m4.1.1.3.3"><times id="S4.p7.4.m4.1.1.3.3.1.cmml" xref="S4.p7.4.m4.1.1.3.3.1"></times><ci id="S4.p7.4.m4.1.1.3.3.2.cmml" xref="S4.p7.4.m4.1.1.3.3.2">𝑃</ci><ci id="S4.p7.4.m4.1.1.3.3.3.cmml" xref="S4.p7.4.m4.1.1.3.3.3">𝐶</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.4.m4.1c">Z\in\mathbb{R}^{P\times C}</annotation><annotation encoding="application/x-llamapun" id="S4.p7.4.m4.1d">italic_Z ∈ blackboard_R start_POSTSUPERSCRIPT italic_P × italic_C end_POSTSUPERSCRIPT</annotation></semantics></math>, is given by the graph convolution, articulated as</p>
</div>
<div class="ltx_para" id="S4.p8">
<table class="ltx_equation ltx_eqn_table" id="S4.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Z=\sum_{k}D_{k}^{-\frac{1}{2}}A_{k}D_{k}^{-\frac{1}{2}}XW_{k}," class="ltx_Math" display="block" id="S4.E2.m1.1"><semantics id="S4.E2.m1.1a"><mrow id="S4.E2.m1.1.1.1" xref="S4.E2.m1.1.1.1.1.cmml"><mrow id="S4.E2.m1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.cmml"><mi id="S4.E2.m1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.2.cmml">Z</mi><mo id="S4.E2.m1.1.1.1.1.1" rspace="0.111em" xref="S4.E2.m1.1.1.1.1.1.cmml">=</mo><mrow id="S4.E2.m1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.3.cmml"><munder id="S4.E2.m1.1.1.1.1.3.1" xref="S4.E2.m1.1.1.1.1.3.1.cmml"><mo id="S4.E2.m1.1.1.1.1.3.1.2" movablelimits="false" xref="S4.E2.m1.1.1.1.1.3.1.2.cmml">∑</mo><mi id="S4.E2.m1.1.1.1.1.3.1.3" xref="S4.E2.m1.1.1.1.1.3.1.3.cmml">k</mi></munder><mrow id="S4.E2.m1.1.1.1.1.3.2" xref="S4.E2.m1.1.1.1.1.3.2.cmml"><msubsup id="S4.E2.m1.1.1.1.1.3.2.2" xref="S4.E2.m1.1.1.1.1.3.2.2.cmml"><mi id="S4.E2.m1.1.1.1.1.3.2.2.2.2" xref="S4.E2.m1.1.1.1.1.3.2.2.2.2.cmml">D</mi><mi id="S4.E2.m1.1.1.1.1.3.2.2.2.3" xref="S4.E2.m1.1.1.1.1.3.2.2.2.3.cmml">k</mi><mrow id="S4.E2.m1.1.1.1.1.3.2.2.3" xref="S4.E2.m1.1.1.1.1.3.2.2.3.cmml"><mo id="S4.E2.m1.1.1.1.1.3.2.2.3a" xref="S4.E2.m1.1.1.1.1.3.2.2.3.cmml">−</mo><mfrac id="S4.E2.m1.1.1.1.1.3.2.2.3.2" xref="S4.E2.m1.1.1.1.1.3.2.2.3.2.cmml"><mn id="S4.E2.m1.1.1.1.1.3.2.2.3.2.2" xref="S4.E2.m1.1.1.1.1.3.2.2.3.2.2.cmml">1</mn><mn id="S4.E2.m1.1.1.1.1.3.2.2.3.2.3" xref="S4.E2.m1.1.1.1.1.3.2.2.3.2.3.cmml">2</mn></mfrac></mrow></msubsup><mo id="S4.E2.m1.1.1.1.1.3.2.1" xref="S4.E2.m1.1.1.1.1.3.2.1.cmml">⁢</mo><msub id="S4.E2.m1.1.1.1.1.3.2.3" xref="S4.E2.m1.1.1.1.1.3.2.3.cmml"><mi id="S4.E2.m1.1.1.1.1.3.2.3.2" xref="S4.E2.m1.1.1.1.1.3.2.3.2.cmml">A</mi><mi id="S4.E2.m1.1.1.1.1.3.2.3.3" xref="S4.E2.m1.1.1.1.1.3.2.3.3.cmml">k</mi></msub><mo id="S4.E2.m1.1.1.1.1.3.2.1a" xref="S4.E2.m1.1.1.1.1.3.2.1.cmml">⁢</mo><msubsup id="S4.E2.m1.1.1.1.1.3.2.4" xref="S4.E2.m1.1.1.1.1.3.2.4.cmml"><mi id="S4.E2.m1.1.1.1.1.3.2.4.2.2" xref="S4.E2.m1.1.1.1.1.3.2.4.2.2.cmml">D</mi><mi id="S4.E2.m1.1.1.1.1.3.2.4.2.3" xref="S4.E2.m1.1.1.1.1.3.2.4.2.3.cmml">k</mi><mrow id="S4.E2.m1.1.1.1.1.3.2.4.3" xref="S4.E2.m1.1.1.1.1.3.2.4.3.cmml"><mo id="S4.E2.m1.1.1.1.1.3.2.4.3a" xref="S4.E2.m1.1.1.1.1.3.2.4.3.cmml">−</mo><mfrac id="S4.E2.m1.1.1.1.1.3.2.4.3.2" xref="S4.E2.m1.1.1.1.1.3.2.4.3.2.cmml"><mn id="S4.E2.m1.1.1.1.1.3.2.4.3.2.2" xref="S4.E2.m1.1.1.1.1.3.2.4.3.2.2.cmml">1</mn><mn id="S4.E2.m1.1.1.1.1.3.2.4.3.2.3" xref="S4.E2.m1.1.1.1.1.3.2.4.3.2.3.cmml">2</mn></mfrac></mrow></msubsup><mo id="S4.E2.m1.1.1.1.1.3.2.1b" xref="S4.E2.m1.1.1.1.1.3.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.1.1.3.2.5" xref="S4.E2.m1.1.1.1.1.3.2.5.cmml">X</mi><mo id="S4.E2.m1.1.1.1.1.3.2.1c" xref="S4.E2.m1.1.1.1.1.3.2.1.cmml">⁢</mo><msub id="S4.E2.m1.1.1.1.1.3.2.6" xref="S4.E2.m1.1.1.1.1.3.2.6.cmml"><mi id="S4.E2.m1.1.1.1.1.3.2.6.2" xref="S4.E2.m1.1.1.1.1.3.2.6.2.cmml">W</mi><mi id="S4.E2.m1.1.1.1.1.3.2.6.3" xref="S4.E2.m1.1.1.1.1.3.2.6.3.cmml">k</mi></msub></mrow></mrow></mrow><mo id="S4.E2.m1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.1b"><apply id="S4.E2.m1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1"><eq id="S4.E2.m1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1"></eq><ci id="S4.E2.m1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.2">𝑍</ci><apply id="S4.E2.m1.1.1.1.1.3.cmml" xref="S4.E2.m1.1.1.1.1.3"><apply id="S4.E2.m1.1.1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.1.1.3.1"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.3.1.1.cmml" xref="S4.E2.m1.1.1.1.1.3.1">subscript</csymbol><sum id="S4.E2.m1.1.1.1.1.3.1.2.cmml" xref="S4.E2.m1.1.1.1.1.3.1.2"></sum><ci id="S4.E2.m1.1.1.1.1.3.1.3.cmml" xref="S4.E2.m1.1.1.1.1.3.1.3">𝑘</ci></apply><apply id="S4.E2.m1.1.1.1.1.3.2.cmml" xref="S4.E2.m1.1.1.1.1.3.2"><times id="S4.E2.m1.1.1.1.1.3.2.1.cmml" xref="S4.E2.m1.1.1.1.1.3.2.1"></times><apply id="S4.E2.m1.1.1.1.1.3.2.2.cmml" xref="S4.E2.m1.1.1.1.1.3.2.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.3.2.2.1.cmml" xref="S4.E2.m1.1.1.1.1.3.2.2">superscript</csymbol><apply id="S4.E2.m1.1.1.1.1.3.2.2.2.cmml" xref="S4.E2.m1.1.1.1.1.3.2.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.3.2.2.2.1.cmml" xref="S4.E2.m1.1.1.1.1.3.2.2">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.3.2.2.2.2.cmml" xref="S4.E2.m1.1.1.1.1.3.2.2.2.2">𝐷</ci><ci id="S4.E2.m1.1.1.1.1.3.2.2.2.3.cmml" xref="S4.E2.m1.1.1.1.1.3.2.2.2.3">𝑘</ci></apply><apply id="S4.E2.m1.1.1.1.1.3.2.2.3.cmml" xref="S4.E2.m1.1.1.1.1.3.2.2.3"><minus id="S4.E2.m1.1.1.1.1.3.2.2.3.1.cmml" xref="S4.E2.m1.1.1.1.1.3.2.2.3"></minus><apply id="S4.E2.m1.1.1.1.1.3.2.2.3.2.cmml" xref="S4.E2.m1.1.1.1.1.3.2.2.3.2"><divide id="S4.E2.m1.1.1.1.1.3.2.2.3.2.1.cmml" xref="S4.E2.m1.1.1.1.1.3.2.2.3.2"></divide><cn id="S4.E2.m1.1.1.1.1.3.2.2.3.2.2.cmml" type="integer" xref="S4.E2.m1.1.1.1.1.3.2.2.3.2.2">1</cn><cn id="S4.E2.m1.1.1.1.1.3.2.2.3.2.3.cmml" type="integer" xref="S4.E2.m1.1.1.1.1.3.2.2.3.2.3">2</cn></apply></apply></apply><apply id="S4.E2.m1.1.1.1.1.3.2.3.cmml" xref="S4.E2.m1.1.1.1.1.3.2.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.3.2.3.1.cmml" xref="S4.E2.m1.1.1.1.1.3.2.3">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.3.2.3.2.cmml" xref="S4.E2.m1.1.1.1.1.3.2.3.2">𝐴</ci><ci id="S4.E2.m1.1.1.1.1.3.2.3.3.cmml" xref="S4.E2.m1.1.1.1.1.3.2.3.3">𝑘</ci></apply><apply id="S4.E2.m1.1.1.1.1.3.2.4.cmml" xref="S4.E2.m1.1.1.1.1.3.2.4"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.3.2.4.1.cmml" xref="S4.E2.m1.1.1.1.1.3.2.4">superscript</csymbol><apply id="S4.E2.m1.1.1.1.1.3.2.4.2.cmml" xref="S4.E2.m1.1.1.1.1.3.2.4"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.3.2.4.2.1.cmml" xref="S4.E2.m1.1.1.1.1.3.2.4">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.3.2.4.2.2.cmml" xref="S4.E2.m1.1.1.1.1.3.2.4.2.2">𝐷</ci><ci id="S4.E2.m1.1.1.1.1.3.2.4.2.3.cmml" xref="S4.E2.m1.1.1.1.1.3.2.4.2.3">𝑘</ci></apply><apply id="S4.E2.m1.1.1.1.1.3.2.4.3.cmml" xref="S4.E2.m1.1.1.1.1.3.2.4.3"><minus id="S4.E2.m1.1.1.1.1.3.2.4.3.1.cmml" xref="S4.E2.m1.1.1.1.1.3.2.4.3"></minus><apply id="S4.E2.m1.1.1.1.1.3.2.4.3.2.cmml" xref="S4.E2.m1.1.1.1.1.3.2.4.3.2"><divide id="S4.E2.m1.1.1.1.1.3.2.4.3.2.1.cmml" xref="S4.E2.m1.1.1.1.1.3.2.4.3.2"></divide><cn id="S4.E2.m1.1.1.1.1.3.2.4.3.2.2.cmml" type="integer" xref="S4.E2.m1.1.1.1.1.3.2.4.3.2.2">1</cn><cn id="S4.E2.m1.1.1.1.1.3.2.4.3.2.3.cmml" type="integer" xref="S4.E2.m1.1.1.1.1.3.2.4.3.2.3">2</cn></apply></apply></apply><ci id="S4.E2.m1.1.1.1.1.3.2.5.cmml" xref="S4.E2.m1.1.1.1.1.3.2.5">𝑋</ci><apply id="S4.E2.m1.1.1.1.1.3.2.6.cmml" xref="S4.E2.m1.1.1.1.1.3.2.6"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.3.2.6.1.cmml" xref="S4.E2.m1.1.1.1.1.3.2.6">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.3.2.6.2.cmml" xref="S4.E2.m1.1.1.1.1.3.2.6.2">𝑊</ci><ci id="S4.E2.m1.1.1.1.1.3.2.6.3.cmml" xref="S4.E2.m1.1.1.1.1.3.2.6.3">𝑘</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.1c">Z=\sum_{k}D_{k}^{-\frac{1}{2}}A_{k}D_{k}^{-\frac{1}{2}}XW_{k},</annotation><annotation encoding="application/x-llamapun" id="S4.E2.m1.1d">italic_Z = ∑ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - divide start_ARG 1 end_ARG start_ARG 2 end_ARG end_POSTSUPERSCRIPT italic_A start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - divide start_ARG 1 end_ARG start_ARG 2 end_ARG end_POSTSUPERSCRIPT italic_X italic_W start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.p8.8">in which <math alttext="k" class="ltx_Math" display="inline" id="S4.p8.1.m1.1"><semantics id="S4.p8.1.m1.1a"><mi id="S4.p8.1.m1.1.1" xref="S4.p8.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.p8.1.m1.1b"><ci id="S4.p8.1.m1.1.1.cmml" xref="S4.p8.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p8.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.p8.1.m1.1d">italic_k</annotation></semantics></math> indexes the neighbor class, <math alttext="W_{k}" class="ltx_Math" display="inline" id="S4.p8.2.m2.1"><semantics id="S4.p8.2.m2.1a"><msub id="S4.p8.2.m2.1.1" xref="S4.p8.2.m2.1.1.cmml"><mi id="S4.p8.2.m2.1.1.2" xref="S4.p8.2.m2.1.1.2.cmml">W</mi><mi id="S4.p8.2.m2.1.1.3" xref="S4.p8.2.m2.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p8.2.m2.1b"><apply id="S4.p8.2.m2.1.1.cmml" xref="S4.p8.2.m2.1.1"><csymbol cd="ambiguous" id="S4.p8.2.m2.1.1.1.cmml" xref="S4.p8.2.m2.1.1">subscript</csymbol><ci id="S4.p8.2.m2.1.1.2.cmml" xref="S4.p8.2.m2.1.1.2">𝑊</ci><ci id="S4.p8.2.m2.1.1.3.cmml" xref="S4.p8.2.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p8.2.m2.1c">W_{k}</annotation><annotation encoding="application/x-llamapun" id="S4.p8.2.m2.1d">italic_W start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> denotes the filter matrix for the <math alttext="k" class="ltx_Math" display="inline" id="S4.p8.3.m3.1"><semantics id="S4.p8.3.m3.1a"><mi id="S4.p8.3.m3.1.1" xref="S4.p8.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.p8.3.m3.1b"><ci id="S4.p8.3.m3.1.1.cmml" xref="S4.p8.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p8.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.p8.3.m3.1d">italic_k</annotation></semantics></math>-th type of 1-hop neighboring nodes. In relation to the normalized <math alttext="\tilde{A}=A+I_{P}" class="ltx_Math" display="inline" id="S4.p8.4.m4.1"><semantics id="S4.p8.4.m4.1a"><mrow id="S4.p8.4.m4.1.1" xref="S4.p8.4.m4.1.1.cmml"><mover accent="true" id="S4.p8.4.m4.1.1.2" xref="S4.p8.4.m4.1.1.2.cmml"><mi id="S4.p8.4.m4.1.1.2.2" xref="S4.p8.4.m4.1.1.2.2.cmml">A</mi><mo id="S4.p8.4.m4.1.1.2.1" xref="S4.p8.4.m4.1.1.2.1.cmml">~</mo></mover><mo id="S4.p8.4.m4.1.1.1" xref="S4.p8.4.m4.1.1.1.cmml">=</mo><mrow id="S4.p8.4.m4.1.1.3" xref="S4.p8.4.m4.1.1.3.cmml"><mi id="S4.p8.4.m4.1.1.3.2" xref="S4.p8.4.m4.1.1.3.2.cmml">A</mi><mo id="S4.p8.4.m4.1.1.3.1" xref="S4.p8.4.m4.1.1.3.1.cmml">+</mo><msub id="S4.p8.4.m4.1.1.3.3" xref="S4.p8.4.m4.1.1.3.3.cmml"><mi id="S4.p8.4.m4.1.1.3.3.2" xref="S4.p8.4.m4.1.1.3.3.2.cmml">I</mi><mi id="S4.p8.4.m4.1.1.3.3.3" xref="S4.p8.4.m4.1.1.3.3.3.cmml">P</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p8.4.m4.1b"><apply id="S4.p8.4.m4.1.1.cmml" xref="S4.p8.4.m4.1.1"><eq id="S4.p8.4.m4.1.1.1.cmml" xref="S4.p8.4.m4.1.1.1"></eq><apply id="S4.p8.4.m4.1.1.2.cmml" xref="S4.p8.4.m4.1.1.2"><ci id="S4.p8.4.m4.1.1.2.1.cmml" xref="S4.p8.4.m4.1.1.2.1">~</ci><ci id="S4.p8.4.m4.1.1.2.2.cmml" xref="S4.p8.4.m4.1.1.2.2">𝐴</ci></apply><apply id="S4.p8.4.m4.1.1.3.cmml" xref="S4.p8.4.m4.1.1.3"><plus id="S4.p8.4.m4.1.1.3.1.cmml" xref="S4.p8.4.m4.1.1.3.1"></plus><ci id="S4.p8.4.m4.1.1.3.2.cmml" xref="S4.p8.4.m4.1.1.3.2">𝐴</ci><apply id="S4.p8.4.m4.1.1.3.3.cmml" xref="S4.p8.4.m4.1.1.3.3"><csymbol cd="ambiguous" id="S4.p8.4.m4.1.1.3.3.1.cmml" xref="S4.p8.4.m4.1.1.3.3">subscript</csymbol><ci id="S4.p8.4.m4.1.1.3.3.2.cmml" xref="S4.p8.4.m4.1.1.3.3.2">𝐼</ci><ci id="S4.p8.4.m4.1.1.3.3.3.cmml" xref="S4.p8.4.m4.1.1.3.3.3">𝑃</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p8.4.m4.1c">\tilde{A}=A+I_{P}</annotation><annotation encoding="application/x-llamapun" id="S4.p8.4.m4.1d">over~ start_ARG italic_A end_ARG = italic_A + italic_I start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT</annotation></semantics></math> from equation (<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S4.E1" title="Equation 1 ‣ 4 Pose Refinement with GCN ‣ Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_tag">1</span></a>), expression (<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S4.E2" title="Equation 2 ‣ 4 Pose Refinement with GCN ‣ Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_tag">2</span></a>) decomposes it into <math alttext="k" class="ltx_Math" display="inline" id="S4.p8.5.m5.1"><semantics id="S4.p8.5.m5.1a"><mi id="S4.p8.5.m5.1.1" xref="S4.p8.5.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.p8.5.m5.1b"><ci id="S4.p8.5.m5.1.1.cmml" xref="S4.p8.5.m5.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p8.5.m5.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.p8.5.m5.1d">italic_k</annotation></semantics></math> sub-matrices with <math alttext="\tilde{A}=\sum_{k}A_{k}" class="ltx_Math" display="inline" id="S4.p8.6.m6.1"><semantics id="S4.p8.6.m6.1a"><mrow id="S4.p8.6.m6.1.1" xref="S4.p8.6.m6.1.1.cmml"><mover accent="true" id="S4.p8.6.m6.1.1.2" xref="S4.p8.6.m6.1.1.2.cmml"><mi id="S4.p8.6.m6.1.1.2.2" xref="S4.p8.6.m6.1.1.2.2.cmml">A</mi><mo id="S4.p8.6.m6.1.1.2.1" xref="S4.p8.6.m6.1.1.2.1.cmml">~</mo></mover><mo id="S4.p8.6.m6.1.1.1" rspace="0.111em" xref="S4.p8.6.m6.1.1.1.cmml">=</mo><mrow id="S4.p8.6.m6.1.1.3" xref="S4.p8.6.m6.1.1.3.cmml"><msub id="S4.p8.6.m6.1.1.3.1" xref="S4.p8.6.m6.1.1.3.1.cmml"><mo id="S4.p8.6.m6.1.1.3.1.2" xref="S4.p8.6.m6.1.1.3.1.2.cmml">∑</mo><mi id="S4.p8.6.m6.1.1.3.1.3" xref="S4.p8.6.m6.1.1.3.1.3.cmml">k</mi></msub><msub id="S4.p8.6.m6.1.1.3.2" xref="S4.p8.6.m6.1.1.3.2.cmml"><mi id="S4.p8.6.m6.1.1.3.2.2" xref="S4.p8.6.m6.1.1.3.2.2.cmml">A</mi><mi id="S4.p8.6.m6.1.1.3.2.3" xref="S4.p8.6.m6.1.1.3.2.3.cmml">k</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p8.6.m6.1b"><apply id="S4.p8.6.m6.1.1.cmml" xref="S4.p8.6.m6.1.1"><eq id="S4.p8.6.m6.1.1.1.cmml" xref="S4.p8.6.m6.1.1.1"></eq><apply id="S4.p8.6.m6.1.1.2.cmml" xref="S4.p8.6.m6.1.1.2"><ci id="S4.p8.6.m6.1.1.2.1.cmml" xref="S4.p8.6.m6.1.1.2.1">~</ci><ci id="S4.p8.6.m6.1.1.2.2.cmml" xref="S4.p8.6.m6.1.1.2.2">𝐴</ci></apply><apply id="S4.p8.6.m6.1.1.3.cmml" xref="S4.p8.6.m6.1.1.3"><apply id="S4.p8.6.m6.1.1.3.1.cmml" xref="S4.p8.6.m6.1.1.3.1"><csymbol cd="ambiguous" id="S4.p8.6.m6.1.1.3.1.1.cmml" xref="S4.p8.6.m6.1.1.3.1">subscript</csymbol><sum id="S4.p8.6.m6.1.1.3.1.2.cmml" xref="S4.p8.6.m6.1.1.3.1.2"></sum><ci id="S4.p8.6.m6.1.1.3.1.3.cmml" xref="S4.p8.6.m6.1.1.3.1.3">𝑘</ci></apply><apply id="S4.p8.6.m6.1.1.3.2.cmml" xref="S4.p8.6.m6.1.1.3.2"><csymbol cd="ambiguous" id="S4.p8.6.m6.1.1.3.2.1.cmml" xref="S4.p8.6.m6.1.1.3.2">subscript</csymbol><ci id="S4.p8.6.m6.1.1.3.2.2.cmml" xref="S4.p8.6.m6.1.1.3.2.2">𝐴</ci><ci id="S4.p8.6.m6.1.1.3.2.3.cmml" xref="S4.p8.6.m6.1.1.3.2.3">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p8.6.m6.1c">\tilde{A}=\sum_{k}A_{k}</annotation><annotation encoding="application/x-llamapun" id="S4.p8.6.m6.1d">over~ start_ARG italic_A end_ARG = ∑ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_A start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>. Here, <math alttext="D_{k}^{ii}=\sum_{j}A_{k}^{ij}" class="ltx_Math" display="inline" id="S4.p8.7.m7.1"><semantics id="S4.p8.7.m7.1a"><mrow id="S4.p8.7.m7.1.1" xref="S4.p8.7.m7.1.1.cmml"><msubsup id="S4.p8.7.m7.1.1.2" xref="S4.p8.7.m7.1.1.2.cmml"><mi id="S4.p8.7.m7.1.1.2.2.2" xref="S4.p8.7.m7.1.1.2.2.2.cmml">D</mi><mi id="S4.p8.7.m7.1.1.2.2.3" xref="S4.p8.7.m7.1.1.2.2.3.cmml">k</mi><mrow id="S4.p8.7.m7.1.1.2.3" xref="S4.p8.7.m7.1.1.2.3.cmml"><mi id="S4.p8.7.m7.1.1.2.3.2" xref="S4.p8.7.m7.1.1.2.3.2.cmml">i</mi><mo id="S4.p8.7.m7.1.1.2.3.1" xref="S4.p8.7.m7.1.1.2.3.1.cmml">⁢</mo><mi id="S4.p8.7.m7.1.1.2.3.3" xref="S4.p8.7.m7.1.1.2.3.3.cmml">i</mi></mrow></msubsup><mo id="S4.p8.7.m7.1.1.1" rspace="0.111em" xref="S4.p8.7.m7.1.1.1.cmml">=</mo><mrow id="S4.p8.7.m7.1.1.3" xref="S4.p8.7.m7.1.1.3.cmml"><msub id="S4.p8.7.m7.1.1.3.1" xref="S4.p8.7.m7.1.1.3.1.cmml"><mo id="S4.p8.7.m7.1.1.3.1.2" xref="S4.p8.7.m7.1.1.3.1.2.cmml">∑</mo><mi id="S4.p8.7.m7.1.1.3.1.3" xref="S4.p8.7.m7.1.1.3.1.3.cmml">j</mi></msub><msubsup id="S4.p8.7.m7.1.1.3.2" xref="S4.p8.7.m7.1.1.3.2.cmml"><mi id="S4.p8.7.m7.1.1.3.2.2.2" xref="S4.p8.7.m7.1.1.3.2.2.2.cmml">A</mi><mi id="S4.p8.7.m7.1.1.3.2.2.3" xref="S4.p8.7.m7.1.1.3.2.2.3.cmml">k</mi><mrow id="S4.p8.7.m7.1.1.3.2.3" xref="S4.p8.7.m7.1.1.3.2.3.cmml"><mi id="S4.p8.7.m7.1.1.3.2.3.2" xref="S4.p8.7.m7.1.1.3.2.3.2.cmml">i</mi><mo id="S4.p8.7.m7.1.1.3.2.3.1" xref="S4.p8.7.m7.1.1.3.2.3.1.cmml">⁢</mo><mi id="S4.p8.7.m7.1.1.3.2.3.3" xref="S4.p8.7.m7.1.1.3.2.3.3.cmml">j</mi></mrow></msubsup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p8.7.m7.1b"><apply id="S4.p8.7.m7.1.1.cmml" xref="S4.p8.7.m7.1.1"><eq id="S4.p8.7.m7.1.1.1.cmml" xref="S4.p8.7.m7.1.1.1"></eq><apply id="S4.p8.7.m7.1.1.2.cmml" xref="S4.p8.7.m7.1.1.2"><csymbol cd="ambiguous" id="S4.p8.7.m7.1.1.2.1.cmml" xref="S4.p8.7.m7.1.1.2">superscript</csymbol><apply id="S4.p8.7.m7.1.1.2.2.cmml" xref="S4.p8.7.m7.1.1.2"><csymbol cd="ambiguous" id="S4.p8.7.m7.1.1.2.2.1.cmml" xref="S4.p8.7.m7.1.1.2">subscript</csymbol><ci id="S4.p8.7.m7.1.1.2.2.2.cmml" xref="S4.p8.7.m7.1.1.2.2.2">𝐷</ci><ci id="S4.p8.7.m7.1.1.2.2.3.cmml" xref="S4.p8.7.m7.1.1.2.2.3">𝑘</ci></apply><apply id="S4.p8.7.m7.1.1.2.3.cmml" xref="S4.p8.7.m7.1.1.2.3"><times id="S4.p8.7.m7.1.1.2.3.1.cmml" xref="S4.p8.7.m7.1.1.2.3.1"></times><ci id="S4.p8.7.m7.1.1.2.3.2.cmml" xref="S4.p8.7.m7.1.1.2.3.2">𝑖</ci><ci id="S4.p8.7.m7.1.1.2.3.3.cmml" xref="S4.p8.7.m7.1.1.2.3.3">𝑖</ci></apply></apply><apply id="S4.p8.7.m7.1.1.3.cmml" xref="S4.p8.7.m7.1.1.3"><apply id="S4.p8.7.m7.1.1.3.1.cmml" xref="S4.p8.7.m7.1.1.3.1"><csymbol cd="ambiguous" id="S4.p8.7.m7.1.1.3.1.1.cmml" xref="S4.p8.7.m7.1.1.3.1">subscript</csymbol><sum id="S4.p8.7.m7.1.1.3.1.2.cmml" xref="S4.p8.7.m7.1.1.3.1.2"></sum><ci id="S4.p8.7.m7.1.1.3.1.3.cmml" xref="S4.p8.7.m7.1.1.3.1.3">𝑗</ci></apply><apply id="S4.p8.7.m7.1.1.3.2.cmml" xref="S4.p8.7.m7.1.1.3.2"><csymbol cd="ambiguous" id="S4.p8.7.m7.1.1.3.2.1.cmml" xref="S4.p8.7.m7.1.1.3.2">superscript</csymbol><apply id="S4.p8.7.m7.1.1.3.2.2.cmml" xref="S4.p8.7.m7.1.1.3.2"><csymbol cd="ambiguous" id="S4.p8.7.m7.1.1.3.2.2.1.cmml" xref="S4.p8.7.m7.1.1.3.2">subscript</csymbol><ci id="S4.p8.7.m7.1.1.3.2.2.2.cmml" xref="S4.p8.7.m7.1.1.3.2.2.2">𝐴</ci><ci id="S4.p8.7.m7.1.1.3.2.2.3.cmml" xref="S4.p8.7.m7.1.1.3.2.2.3">𝑘</ci></apply><apply id="S4.p8.7.m7.1.1.3.2.3.cmml" xref="S4.p8.7.m7.1.1.3.2.3"><times id="S4.p8.7.m7.1.1.3.2.3.1.cmml" xref="S4.p8.7.m7.1.1.3.2.3.1"></times><ci id="S4.p8.7.m7.1.1.3.2.3.2.cmml" xref="S4.p8.7.m7.1.1.3.2.3.2">𝑖</ci><ci id="S4.p8.7.m7.1.1.3.2.3.3.cmml" xref="S4.p8.7.m7.1.1.3.2.3.3">𝑗</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p8.7.m7.1c">D_{k}^{ii}=\sum_{j}A_{k}^{ij}</annotation><annotation encoding="application/x-llamapun" id="S4.p8.7.m7.1d">italic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i italic_i end_POSTSUPERSCRIPT = ∑ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_A start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i italic_j end_POSTSUPERSCRIPT</annotation></semantics></math> represents the degree matrix that normalizes <math alttext="A_{k}" class="ltx_Math" display="inline" id="S4.p8.8.m8.1"><semantics id="S4.p8.8.m8.1a"><msub id="S4.p8.8.m8.1.1" xref="S4.p8.8.m8.1.1.cmml"><mi id="S4.p8.8.m8.1.1.2" xref="S4.p8.8.m8.1.1.2.cmml">A</mi><mi id="S4.p8.8.m8.1.1.3" xref="S4.p8.8.m8.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p8.8.m8.1b"><apply id="S4.p8.8.m8.1.1.cmml" xref="S4.p8.8.m8.1.1"><csymbol cd="ambiguous" id="S4.p8.8.m8.1.1.1.cmml" xref="S4.p8.8.m8.1.1">subscript</csymbol><ci id="S4.p8.8.m8.1.1.2.cmml" xref="S4.p8.8.m8.1.1.2">𝐴</ci><ci id="S4.p8.8.m8.1.1.3.cmml" xref="S4.p8.8.m8.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p8.8.m8.1c">A_{k}</annotation><annotation encoding="application/x-llamapun" id="S4.p8.8.m8.1d">italic_A start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S4.p9">
<p class="ltx_p" id="S4.p9.1">Our model combines graph convolution operation from  (<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S4.E2" title="Equation 2 ‣ 4 Pose Refinement with GCN ‣ Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_tag">2</span></a>) and 3D convolutions. The primary architecture, depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S4.F6" title="Figure 6 ‣ 4 Pose Refinement with GCN ‣ Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_tag">6</span></a>, merges both spatial and temporal graph convolutions. The input, a tensor representing 3D keypoints, undergoes normalization for stability. This input tensor has dimensions (N,C,T,V,M), with N as the batch size, C as the number of features, T as the temporal dimension (input sequence length), V as the graph nodes for each frame, and M as the number of instances in a frame.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="333" id="S4.F6.g1" src="extracted/2404.16136v1/images/NewGraph.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.2.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S4.F6.3.2" style="font-size:90%;">Graph-based 3D human pose refinement architecture with detailed architecture of the Spatial-Temporal Graph Convolutional (ST-GCN) layer. </span></figcaption>
</figure>
<div class="ltx_para" id="S4.p10">
<p class="ltx_p" id="S4.p10.1">The core of the model comprises several spatial-temporal graph convolutional (ST-GCN) layers, depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S4.F6" title="Figure 6 ‣ 4 Pose Refinement with GCN ‣ Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_tag">6</span></a> , designed for feature extraction and refinement. A non-local block is also incorporated, capturing long-range dependencies and relationships between different input parts. The resulting features are then passed through a fully connected layer producing the final refined 3D pose.</p>
</div>
<div class="ltx_para" id="S4.p11">
<p class="ltx_p" id="S4.p11.1">The details of the ST-GCN layers are shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S4.F6" title="Figure 6 ‣ 4 Pose Refinement with GCN ‣ Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_tag">6</span></a>. Each layer begins with an operation that applies a graph convolution, incorporating the spatial structure and connections defined by an adjacency matrix. Following the graph convolution, the output undergoes a temporal 3D convolution, capturing the temporal relationships across frames. A residual connection is employed to facilitate faster convergence and mitigate the vanishing gradient problem. The final output of each ST-GCN layer passes through a ReLU function for a non-linear transformation. The combination of these operations ensures that our model understands the spatial-temporal dynamics of human actions, enabling accurate 3D pose estimation even in challenging scenarios.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental Setup</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Our architecture provides an end-to-end solution for 3D pose estimation from video, handling occlusions—a common real-world challenge. Our tests on standard benchmarks show it outperforms existing top methods, especially in occluded scenarios, while also maintaining strong performance in standard situations. The experimental framework was implemented using Pytorch <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib27" title=""><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>, an Intel(R) Core(TM) i7-8700K CPU @ 3.70GHz
and two NVIDIA GeForce GTX 1080 Ti.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Datasets and Evaluation Metrics</h3>
<div class="ltx_para ltx_noindent" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.1">Datasets.</span> A primary dataset in our study is Human3.6M <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>, which serves as the training foundation for several 3D HPE algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">30</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">34</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib9" title=""><span class="ltx_text" style="font-size:90%;">9</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">8</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>. Human3.6M furnishes 3.6 million human poses and corresponding images. Captured in controlled indoor environments, it documents 15 unique actions, with two versions each, from four viewpoints. It is important to note that, due to privacy concerns, data from only 7 subjects is available: S1, S5, S6, S7, S8, S9, S11, totaling 840 videos. For performance evaluation, we sourced all available subjects from Human3.6M <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>. These were combined with our synthetic subjects from BlendMimic3D: SS1, SS2 and SS3.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">In line with established practices in 3D HPE research, as seen in previous works  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">30</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">34</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib9" title=""><span class="ltx_text" style="font-size:90%;">9</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">8</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>, we have selected a specific set of subjects for our training and testing phases. For the training of our GCN, we utilize the 3D pose predictions from 6 subjects of Human3.6M, S1, S5, S6, S7, S8 and S11, along with our synthetic subjects SS1, and SS2. We then evaluate the performance of our model on two different subjects, S9 and SS3. Both 3D pose predictions and 3D refined poses are represented in the camera’s coordinate system and a single model is used to train all camera views for all actions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p3.1.1">Evaluation metrics.</span> Our 3D human pose estimation evaluation harnesses the Mean Per-Joint Positional Error (MPJPE) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib42" title=""><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite>, a metric that calculates the average <math alttext="\ell_{2}" class="ltx_Math" display="inline" id="S5.SS1.p3.1.m1.1"><semantics id="S5.SS1.p3.1.m1.1a"><msub id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml"><mi id="S5.SS1.p3.1.m1.1.1.2" mathvariant="normal" xref="S5.SS1.p3.1.m1.1.1.2.cmml">ℓ</mi><mn id="S5.SS1.p3.1.m1.1.1.3" xref="S5.SS1.p3.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><apply id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.p3.1.m1.1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="S5.SS1.p3.1.m1.1.1.2.cmml" xref="S5.SS1.p3.1.m1.1.1.2">ℓ</ci><cn id="S5.SS1.p3.1.m1.1.1.3.cmml" type="integer" xref="S5.SS1.p3.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">\ell_{2}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.1.m1.1d">roman_ℓ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>-norm difference between estimated and true 3D poses, represented by the equation</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{MPJPE}=\frac{1}{N}\sum_{i=1}^{N}\left\|J_{i}-J_{i}^{*}\right\|_{2}," class="ltx_Math" display="block" id="S5.E3.m1.1"><semantics id="S5.E3.m1.1a"><mrow id="S5.E3.m1.1.1.1" xref="S5.E3.m1.1.1.1.1.cmml"><mrow id="S5.E3.m1.1.1.1.1" xref="S5.E3.m1.1.1.1.1.cmml"><mtext id="S5.E3.m1.1.1.1.1.3" xref="S5.E3.m1.1.1.1.1.3a.cmml">MPJPE</mtext><mo id="S5.E3.m1.1.1.1.1.2" xref="S5.E3.m1.1.1.1.1.2.cmml">=</mo><mrow id="S5.E3.m1.1.1.1.1.1" xref="S5.E3.m1.1.1.1.1.1.cmml"><mfrac id="S5.E3.m1.1.1.1.1.1.3" xref="S5.E3.m1.1.1.1.1.1.3.cmml"><mn id="S5.E3.m1.1.1.1.1.1.3.2" xref="S5.E3.m1.1.1.1.1.1.3.2.cmml">1</mn><mi id="S5.E3.m1.1.1.1.1.1.3.3" xref="S5.E3.m1.1.1.1.1.1.3.3.cmml">N</mi></mfrac><mo id="S5.E3.m1.1.1.1.1.1.2" xref="S5.E3.m1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S5.E3.m1.1.1.1.1.1.1" xref="S5.E3.m1.1.1.1.1.1.1.cmml"><munderover id="S5.E3.m1.1.1.1.1.1.1.2" xref="S5.E3.m1.1.1.1.1.1.1.2.cmml"><mo id="S5.E3.m1.1.1.1.1.1.1.2.2.2" movablelimits="false" rspace="0em" xref="S5.E3.m1.1.1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S5.E3.m1.1.1.1.1.1.1.2.2.3" xref="S5.E3.m1.1.1.1.1.1.1.2.2.3.cmml"><mi id="S5.E3.m1.1.1.1.1.1.1.2.2.3.2" xref="S5.E3.m1.1.1.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S5.E3.m1.1.1.1.1.1.1.2.2.3.1" xref="S5.E3.m1.1.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S5.E3.m1.1.1.1.1.1.1.2.2.3.3" xref="S5.E3.m1.1.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S5.E3.m1.1.1.1.1.1.1.2.3" xref="S5.E3.m1.1.1.1.1.1.1.2.3.cmml">N</mi></munderover><msub id="S5.E3.m1.1.1.1.1.1.1.1" xref="S5.E3.m1.1.1.1.1.1.1.1.cmml"><mrow id="S5.E3.m1.1.1.1.1.1.1.1.1.1" xref="S5.E3.m1.1.1.1.1.1.1.1.1.2.cmml"><mo id="S5.E3.m1.1.1.1.1.1.1.1.1.1.2" xref="S5.E3.m1.1.1.1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">J</mi><mi id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">−</mo><msubsup id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.3.2.2" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml">J</mi><mi id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.3.2.3" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.3.2.3.cmml">i</mi><mo id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">∗</mo></msubsup></mrow><mo id="S5.E3.m1.1.1.1.1.1.1.1.1.1.3" xref="S5.E3.m1.1.1.1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S5.E3.m1.1.1.1.1.1.1.1.3" xref="S5.E3.m1.1.1.1.1.1.1.1.3.cmml">2</mn></msub></mrow></mrow></mrow><mo id="S5.E3.m1.1.1.1.2" xref="S5.E3.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.E3.m1.1b"><apply id="S5.E3.m1.1.1.1.1.cmml" xref="S5.E3.m1.1.1.1"><eq id="S5.E3.m1.1.1.1.1.2.cmml" xref="S5.E3.m1.1.1.1.1.2"></eq><ci id="S5.E3.m1.1.1.1.1.3a.cmml" xref="S5.E3.m1.1.1.1.1.3"><mtext id="S5.E3.m1.1.1.1.1.3.cmml" xref="S5.E3.m1.1.1.1.1.3">MPJPE</mtext></ci><apply id="S5.E3.m1.1.1.1.1.1.cmml" xref="S5.E3.m1.1.1.1.1.1"><times id="S5.E3.m1.1.1.1.1.1.2.cmml" xref="S5.E3.m1.1.1.1.1.1.2"></times><apply id="S5.E3.m1.1.1.1.1.1.3.cmml" xref="S5.E3.m1.1.1.1.1.1.3"><divide id="S5.E3.m1.1.1.1.1.1.3.1.cmml" xref="S5.E3.m1.1.1.1.1.1.3"></divide><cn id="S5.E3.m1.1.1.1.1.1.3.2.cmml" type="integer" xref="S5.E3.m1.1.1.1.1.1.3.2">1</cn><ci id="S5.E3.m1.1.1.1.1.1.3.3.cmml" xref="S5.E3.m1.1.1.1.1.1.3.3">𝑁</ci></apply><apply id="S5.E3.m1.1.1.1.1.1.1.cmml" xref="S5.E3.m1.1.1.1.1.1.1"><apply id="S5.E3.m1.1.1.1.1.1.1.2.cmml" xref="S5.E3.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.E3.m1.1.1.1.1.1.1.2.1.cmml" xref="S5.E3.m1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S5.E3.m1.1.1.1.1.1.1.2.2.cmml" xref="S5.E3.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.E3.m1.1.1.1.1.1.1.2.2.1.cmml" xref="S5.E3.m1.1.1.1.1.1.1.2">subscript</csymbol><sum id="S5.E3.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S5.E3.m1.1.1.1.1.1.1.2.2.2"></sum><apply id="S5.E3.m1.1.1.1.1.1.1.2.2.3.cmml" xref="S5.E3.m1.1.1.1.1.1.1.2.2.3"><eq id="S5.E3.m1.1.1.1.1.1.1.2.2.3.1.cmml" xref="S5.E3.m1.1.1.1.1.1.1.2.2.3.1"></eq><ci id="S5.E3.m1.1.1.1.1.1.1.2.2.3.2.cmml" xref="S5.E3.m1.1.1.1.1.1.1.2.2.3.2">𝑖</ci><cn id="S5.E3.m1.1.1.1.1.1.1.2.2.3.3.cmml" type="integer" xref="S5.E3.m1.1.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S5.E3.m1.1.1.1.1.1.1.2.3.cmml" xref="S5.E3.m1.1.1.1.1.1.1.2.3">𝑁</ci></apply><apply id="S5.E3.m1.1.1.1.1.1.1.1.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.E3.m1.1.1.1.1.1.1.1.2.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S5.E3.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S5.E3.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1"><minus id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1"></minus><apply id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.2.2">𝐽</ci><ci id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.3.2.2">𝐽</ci><ci id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.3.2.3">𝑖</ci></apply><times id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.3.3"></times></apply></apply></apply><cn id="S5.E3.m1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S5.E3.m1.1.1.1.1.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E3.m1.1c">\text{MPJPE}=\frac{1}{N}\sum_{i=1}^{N}\left\|J_{i}-J_{i}^{*}\right\|_{2},</annotation><annotation encoding="application/x-llamapun" id="S5.E3.m1.1d">MPJPE = divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ∥ italic_J start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_J start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S5.SS1.p3.5">where <math alttext="N" class="ltx_Math" display="inline" id="S5.SS1.p3.2.m1.1"><semantics id="S5.SS1.p3.2.m1.1a"><mi id="S5.SS1.p3.2.m1.1.1" xref="S5.SS1.p3.2.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.2.m1.1b"><ci id="S5.SS1.p3.2.m1.1.1.cmml" xref="S5.SS1.p3.2.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.2.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.2.m1.1d">italic_N</annotation></semantics></math> represents the joint count, and <math alttext="J_{i}" class="ltx_Math" display="inline" id="S5.SS1.p3.3.m2.1"><semantics id="S5.SS1.p3.3.m2.1a"><msub id="S5.SS1.p3.3.m2.1.1" xref="S5.SS1.p3.3.m2.1.1.cmml"><mi id="S5.SS1.p3.3.m2.1.1.2" xref="S5.SS1.p3.3.m2.1.1.2.cmml">J</mi><mi id="S5.SS1.p3.3.m2.1.1.3" xref="S5.SS1.p3.3.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.3.m2.1b"><apply id="S5.SS1.p3.3.m2.1.1.cmml" xref="S5.SS1.p3.3.m2.1.1"><csymbol cd="ambiguous" id="S5.SS1.p3.3.m2.1.1.1.cmml" xref="S5.SS1.p3.3.m2.1.1">subscript</csymbol><ci id="S5.SS1.p3.3.m2.1.1.2.cmml" xref="S5.SS1.p3.3.m2.1.1.2">𝐽</ci><ci id="S5.SS1.p3.3.m2.1.1.3.cmml" xref="S5.SS1.p3.3.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.3.m2.1c">J_{i}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.3.m2.1d">italic_J start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="J_{i}^{*}" class="ltx_Math" display="inline" id="S5.SS1.p3.4.m3.1"><semantics id="S5.SS1.p3.4.m3.1a"><msubsup id="S5.SS1.p3.4.m3.1.1" xref="S5.SS1.p3.4.m3.1.1.cmml"><mi id="S5.SS1.p3.4.m3.1.1.2.2" xref="S5.SS1.p3.4.m3.1.1.2.2.cmml">J</mi><mi id="S5.SS1.p3.4.m3.1.1.2.3" xref="S5.SS1.p3.4.m3.1.1.2.3.cmml">i</mi><mo id="S5.SS1.p3.4.m3.1.1.3" xref="S5.SS1.p3.4.m3.1.1.3.cmml">∗</mo></msubsup><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.4.m3.1b"><apply id="S5.SS1.p3.4.m3.1.1.cmml" xref="S5.SS1.p3.4.m3.1.1"><csymbol cd="ambiguous" id="S5.SS1.p3.4.m3.1.1.1.cmml" xref="S5.SS1.p3.4.m3.1.1">superscript</csymbol><apply id="S5.SS1.p3.4.m3.1.1.2.cmml" xref="S5.SS1.p3.4.m3.1.1"><csymbol cd="ambiguous" id="S5.SS1.p3.4.m3.1.1.2.1.cmml" xref="S5.SS1.p3.4.m3.1.1">subscript</csymbol><ci id="S5.SS1.p3.4.m3.1.1.2.2.cmml" xref="S5.SS1.p3.4.m3.1.1.2.2">𝐽</ci><ci id="S5.SS1.p3.4.m3.1.1.2.3.cmml" xref="S5.SS1.p3.4.m3.1.1.2.3">𝑖</ci></apply><times id="S5.SS1.p3.4.m3.1.1.3.cmml" xref="S5.SS1.p3.4.m3.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.4.m3.1c">J_{i}^{*}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.4.m3.1d">italic_J start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math> denote the true and estimated positions of the <math alttext="i_{th}" class="ltx_Math" display="inline" id="S5.SS1.p3.5.m4.1"><semantics id="S5.SS1.p3.5.m4.1a"><msub id="S5.SS1.p3.5.m4.1.1" xref="S5.SS1.p3.5.m4.1.1.cmml"><mi id="S5.SS1.p3.5.m4.1.1.2" xref="S5.SS1.p3.5.m4.1.1.2.cmml">i</mi><mrow id="S5.SS1.p3.5.m4.1.1.3" xref="S5.SS1.p3.5.m4.1.1.3.cmml"><mi id="S5.SS1.p3.5.m4.1.1.3.2" xref="S5.SS1.p3.5.m4.1.1.3.2.cmml">t</mi><mo id="S5.SS1.p3.5.m4.1.1.3.1" xref="S5.SS1.p3.5.m4.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.p3.5.m4.1.1.3.3" xref="S5.SS1.p3.5.m4.1.1.3.3.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.5.m4.1b"><apply id="S5.SS1.p3.5.m4.1.1.cmml" xref="S5.SS1.p3.5.m4.1.1"><csymbol cd="ambiguous" id="S5.SS1.p3.5.m4.1.1.1.cmml" xref="S5.SS1.p3.5.m4.1.1">subscript</csymbol><ci id="S5.SS1.p3.5.m4.1.1.2.cmml" xref="S5.SS1.p3.5.m4.1.1.2">𝑖</ci><apply id="S5.SS1.p3.5.m4.1.1.3.cmml" xref="S5.SS1.p3.5.m4.1.1.3"><times id="S5.SS1.p3.5.m4.1.1.3.1.cmml" xref="S5.SS1.p3.5.m4.1.1.3.1"></times><ci id="S5.SS1.p3.5.m4.1.1.3.2.cmml" xref="S5.SS1.p3.5.m4.1.1.3.2">𝑡</ci><ci id="S5.SS1.p3.5.m4.1.1.3.3.cmml" xref="S5.SS1.p3.5.m4.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.5.m4.1c">i_{th}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.5.m4.1d">italic_i start_POSTSUBSCRIPT italic_t italic_h end_POSTSUBSCRIPT</annotation></semantics></math> joint, respectively.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Implementation Details</h3>
<div class="ltx_para ltx_noindent" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p1.1.1">2D HPE.</span> This stage focuses on accurately capturing the skeletal structure in two dimensions, which lays the foundation for the subsequent 3D pose estimation. To identify subjects in video frames and extract their 2D keypoints, we use two detection algorithms in our 2D HPE process: CPN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite> and Detectron2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">39</span></a>]</cite>. For keypoints detection using Detectron2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">39</span></a>]</cite>, we utilize a pretrained model that uses Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite> with ResNet-101-FPN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite> as backbone. Regarding CPN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>, which is an extension of FPN as suggested by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite>, we employ the 2D keypoint predictions provided from their fine-tuned CPN model for the Human3.6M dataset. For our synthetic 2D pose predictions, we re-implement CPN, using a ResNet-101 backbone with a 384<math alttext="\times" class="ltx_Math" display="inline" id="S5.SS2.p1.1.m1.1"><semantics id="S5.SS2.p1.1.m1.1a"><mo id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><times id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.1.m1.1d">×</annotation></semantics></math>288 resolution. This model uses externally provided bounding boxes generated by Detectron2.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">To handle dynamic scenes with multiple people, we integrate the DeepSort <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite> algorithm, modified to track a specific individual with a unique ID. Our method assumes that the individual remains in the frame throughout the monitoring in a multi-person environment. In order to maintain the tracking continuity, especially when the target ID is temporarily lost, we select the closest bounding box based on centroid distance, prioritizing those with high confidence scores. Also, to improve detection performance, our approach resizes subsequent frames according to the previous tracked bounding box and implementing an region of interest (ROI) cropping strategy focused on the target ID. This entire preprocessing approach, encompassing both detection and tracking phases, is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S5.F7" title="Figure 7 ‣ 5.2 Implementation Details ‣ 5 Experimental Setup ‣ Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<figure class="ltx_figure" id="S5.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="256" id="S5.F7.g1" src="extracted/2404.16136v1/images/GCNplugin_pt1.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F7.2.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text" id="S5.F7.3.2" style="font-size:90%;">Overview of the proposed preprocessing strategy for 2D HPE. It begins with (1) employing a detection algorithm for pinpointing subjects and capturing their 2D keypoints, followed by (2) a tracking mechanism to maintain focus on a target subject, supplying a sequence of 2D poses. </span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p3.1.1">2D-to-3D Pose Conversion</span>
We extracted 2D keypoints as inputs for our 2D-to-3D pose lifting module and assessed the performance of various algorithms in handling occlusions with our BlendMimic3D dataset. These algorithms include VideoPose3D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite>, PoseFormerV2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite>, and D3DP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite>, for which we utilized their available pretrained models. For all three algorithms, we used an input sequence length of 243 frames. Specifically, for PoseFormerV2, we inputted 27 frames into the spatial encoder along with 27 DCT coefficients. For D3DP, we configured the model to use 1 hypothesis and one iteration.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p4.1.1">GCN Pose Refinement</span> Our GCN model is trained for 40 epochs with a mini-batch size of 256, using the AMSGrad <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite> optimizer and an initial learning rate of 0.001. The learning rate is reduced by 0.1 every 5 epochs and shrinks by a factor of 0.95 after each epoch, with a more significant reduction of 0.5 every 5 epochs. Training batches are created by a generator based on pre-split subject IDs for training and testing groups, as detailed in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S5.SS1" title="5.1 Datasets and Evaluation Metrics ‣ 5 Experimental Setup ‣ Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_tag">5.1</span></a>. The model is updated through backpropagation based on these batches. We follow the training losses of  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>, including 3D pose loss (MPJPE), derivative loss (measuring the Euclidean distance of the first derivative between predicted and ground truth velocities), and symmetry loss (focusing on the accuracy of left and right bone pairs). Test data is solely used for evaluation.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Experimental Results</h2>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Quantitative results</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">Our analysis on Human3.6M and BlendMimic3D datasets, using CPN-based and Detectron2-based 2D detections, demonstrates the positive impact of the GCN pose refinement block in handling occlusions, as evidenced by MPJPE improvements particularly on the occlusion-heavy BlendMimic3D dataset. Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S6.F8" title="Figure 8 ‣ 6.1 Quantitative results ‣ 6 Experimental Results ‣ Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_tag">8</span></a> visually summarizes the enhancements and trade-offs introduced by our GCN across VideoPose3D and PoseFormerV2.</p>
</div>
<figure class="ltx_figure" id="S6.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="323" id="S6.F8.g1" src="extracted/2404.16136v1/images/boxplot1.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S6.F8.2.1.1" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text" id="S6.F8.3.2" style="font-size:90%;">Evaluation of our GCN pose refinement block against previous methods: VideoPose3D (VP3D) and PoseFormerV2 (PFV2), showcasing performance on CPN-based detections across Human3.6M and BlendMimic3D test sets.</span></figcaption>
</figure>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S6.F8" title="Figure 8 ‣ 6.1 Quantitative results ‣ 6 Experimental Results ‣ Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_tag">8</span></a> shows that the results with our GCN achieve a comparable error on the non-occluded Human3.6M dataset. However, the baseline approaches exhibit worse MPJPE in occluded scenarios (BlendMimic3D dataset), while our GCN reduces this error escalation. The proposed approach leads to a notable decrease of more than 30% on the average errors with occlusions.</p>
</div>
<div class="ltx_para" id="S6.SS1.p3">
<p class="ltx_p" id="S6.SS1.p3.1">Unlike PoseFormerV2 (PFV2) and VideoPose3D (VP3D), D3DP incorporates mechanisms that can handle occlusions, utilizing a diffusion process to add noise and a denoiser conditioned on 2D keypoints, leading to a variety of hypotheses that can capture the possible variations in pose. GCN integration addresses the occlusion management challenges in both VP3D and PFV2 models, as demonstrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S6.T2" title="Table 2 ‣ 6.1 Quantitative results ‣ 6 Experimental Results ‣ Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_tag">2</span></a>. This table also highlights the GCN’s broader impact, including its application to D3DP, within the BlendMimic3D test set.</p>
</div>
<figure class="ltx_table" id="S6.T2">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S6.T2.6.1.1" style="font-size:129%;">Table 2</span>: </span><span class="ltx_text" id="S6.T2.7.2" style="font-size:129%;">Evaluation of our GCN pose refinement block against previous methods, with CPN and Detectron2, on BlendMimic3D test set. Best results are highlighted in <span class="ltx_text" id="S6.T2.7.2.1" style="color:#0ACC66;">green</span>.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T2.8">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T2.8.1.1">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S6.T2.8.1.1.1" style="padding-left:2.5pt;padding-right:2.5pt;">
<span class="ltx_text" id="S6.T2.8.1.1.1.1" style="font-size:70%;"> </span><span class="ltx_rule" style="width:1.0pt;background:black;display:inline-block;"> </span>
</th>
<td class="ltx_td ltx_align_center" colspan="6" id="S6.T2.8.1.1.2" style="padding-left:2.5pt;padding-right:2.5pt;">
<span class="ltx_text ltx_font_bold" id="S6.T2.8.1.1.2.1" style="font-size:70%;">3D HPE Model </span><span class="ltx_text" id="S6.T2.8.1.1.2.2" style="font-size:70%;">–</span><span class="ltx_text ltx_font_bold" id="S6.T2.8.1.1.2.3" style="font-size:70%;">MPJPE (Avg [mm])</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.8.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T2.8.2.2.1" style="padding-left:2.5pt;padding-right:2.5pt;">
<span class="ltx_text ltx_font_bold" id="S6.T2.8.2.2.1.1" style="font-size:70%;">2D HPE</span><span class="ltx_text" id="S6.T2.8.2.2.1.2" style="font-size:70%;">  </span><span class="ltx_rule" style="width:1.0pt;background:black;display:inline-block;"> </span>
</th>
<td class="ltx_td ltx_align_left" id="S6.T2.8.2.2.2" style="padding-left:2.5pt;padding-right:2.5pt;">
<span class="ltx_text" id="S6.T2.8.2.2.2.1" style="font-size:70%;">VP3D </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.T2.8.2.2.2.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">30</span></a><span class="ltx_text" id="S6.T2.8.2.2.2.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S6.T2.8.2.2.3" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text" id="S6.T2.8.2.2.3.1" style="font-size:70%;">+ GCN</span></td>
<td class="ltx_td ltx_align_left" id="S6.T2.8.2.2.4" style="padding-left:2.5pt;padding-right:2.5pt;">
<span class="ltx_text" id="S6.T2.8.2.2.4.1" style="font-size:70%;">PFV2 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.T2.8.2.2.4.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a><span class="ltx_text" id="S6.T2.8.2.2.4.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S6.T2.8.2.2.5" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text" id="S6.T2.8.2.2.5.1" style="font-size:70%;">+ GCN</span></td>
<td class="ltx_td ltx_align_left" id="S6.T2.8.2.2.6" style="padding-left:2.5pt;padding-right:2.5pt;">
<span class="ltx_text" id="S6.T2.8.2.2.6.1" style="font-size:70%;">D3DP </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.T2.8.2.2.6.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">34</span></a><span class="ltx_text" id="S6.T2.8.2.2.6.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_right" id="S6.T2.8.2.2.7" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text" id="S6.T2.8.2.2.7.1" style="font-size:70%;">+ GCN</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.8.3.3" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T2.8.3.3.1" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text" id="S6.T2.8.3.3.1.1" style="font-size:70%;background-color:#E6E6E6;">CPN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>  <span class="ltx_rule" style="width:1.0pt;background:black;display:inline-block;"> </span></span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T2.8.3.3.2" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text" id="S6.T2.8.3.3.2.1" style="font-size:70%;background-color:#E6E6E6;">175.0</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S6.T2.8.3.3.3" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text" id="S6.T2.8.3.3.3.1" style="font-size:70%;color:#0ACC66;background-color:#E6E6E6;">112.7</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T2.8.3.3.4" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text" id="S6.T2.8.3.3.4.1" style="font-size:70%;background-color:#E6E6E6;">148.6</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S6.T2.8.3.3.5" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text" id="S6.T2.8.3.3.5.1" style="font-size:70%;color:#0ACC66;background-color:#E6E6E6;">107.5</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T2.8.3.3.6" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text" id="S6.T2.8.3.3.6.1" style="font-size:70%;background-color:#E6E6E6;">100.7</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.8.3.3.7" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text" id="S6.T2.8.3.3.7.1" style="font-size:70%;color:#0ACC66;background-color:#E6E6E6;">95.3</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.8.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S6.T2.8.4.4.1" style="padding-left:2.5pt;padding-right:2.5pt;">
<span class="ltx_text" id="S6.T2.8.4.4.1.1" style="font-size:70%;">Detectron2 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.T2.8.4.4.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">39</span></a><span class="ltx_text" id="S6.T2.8.4.4.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S6.T2.8.4.4.1.4" style="font-size:70%;">  </span><span class="ltx_rule" style="width:1.0pt;background:black;display:inline-block;"> </span>
</th>
<td class="ltx_td ltx_align_left ltx_border_b" id="S6.T2.8.4.4.2" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text" id="S6.T2.8.4.4.2.1" style="font-size:70%;">198.0</span></td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_r" id="S6.T2.8.4.4.3" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text" id="S6.T2.8.4.4.3.1" style="font-size:70%;color:#0ACC66;">127.7</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S6.T2.8.4.4.4" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text" id="S6.T2.8.4.4.4.1" style="font-size:70%;">155.0</span></td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_r" id="S6.T2.8.4.4.5" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text" id="S6.T2.8.4.4.5.1" style="font-size:70%;color:#0ACC66;">106.9</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S6.T2.8.4.4.6" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text" id="S6.T2.8.4.4.6.1" style="font-size:70%;">99.9</span></td>
<td class="ltx_td ltx_align_right ltx_border_b" id="S6.T2.8.4.4.7" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text" id="S6.T2.8.4.4.7.1" style="font-size:70%;color:#0ACC66;">95.3</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S6.SS1.p4">
<p class="ltx_p" id="S6.SS1.p4.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S6.T2" title="Table 2 ‣ 6.1 Quantitative results ‣ 6 Experimental Results ‣ Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_tag">2</span></a> underscores the GCN’s versatility, showing consistent performance enhancements across different 2D detection methods (CPN and Detectron2). It also shows improvements in D3DP’s performance, affirming the GCN’s value even in models already equipped for occlusion management. Detailed results, categorized by action for each model, can be found in the supplementary material.</p>
</div>
<div class="ltx_para" id="S6.SS1.p5">
<p class="ltx_p" id="S6.SS1.p5.1">This evaluation highlights BlendMimic3D’s role in overcoming the self-occlusion bias of datasets like Human3.6M, emphasizing its importance for enhancing 3D HPE robustness through diverse occlusions. It showcases synthetic data’s role in model development and affirms our GCN’s advancement in occlusion management, setting a new benchmark for occlusion handling in 3D HPE systems.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Qualitative results</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">To test our approach in a real-world scenario featuring occlusions, Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S6.F9" title="Figure 9 ‣ 6.2 Qualitative results ‣ 6 Experimental Results ‣ Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_tag">9</span></a> showcases qualitative results. It compares the 3D human pose estimation from VideoPose3D with and without our refined pose, both derived from the same input video.</p>
</div>
<figure class="ltx_figure" id="S6.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="418" id="S6.F9.g1" src="extracted/2404.16136v1/images/PaperInWildResults.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S6.F9.2.1.1" style="font-size:90%;">Figure 9</span>: </span><span class="ltx_text" id="S6.F9.3.2" style="font-size:90%;">Example showcasing three frames from a real “in the wild” video with the corresponding 3D HPE using VideoPose3D, on Detectron2 detections, with and without the proposed GCN.</span></figcaption>
</figure>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1">As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S6.F9" title="Figure 9 ‣ 6.2 Qualitative results ‣ 6 Experimental Results ‣ Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_tag">9</span></a>, the GCN approach improves the estimation results, particularly for occluded legs. This suggests that our GCN is effective in handling occlusions – a critical benefit for real-world applications where such occlusions are frequent.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">This work introduces BlendMimic3D, a new benchmark designed to train and evaluate 3D HPE with occlusions. Unlike traditional datasets such as COCO and Human3.6M, with controlled settings and limited occlusion variations, our BlendMimic3D replicates real-world complexities. A standout feature of BlendMimic3D is its expandability and ease of modification<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://github.com/FilipaLino/BlendMimic3D-DataExtractor</span></span></span>
, requiring only Blender for animation generation. Additionally, we propose a GCN pose refinement block, that can be plugged in with state-of-the-art 3D HPE algorithms to improve their performance for occluded poses, requiring no further training of the HPE backbone. This ensures that performance improvements in occluded conditions do not compromise accuracy in standard, non-occluded settings. Future efforts will aim to fully preserve performance in these scenarios upon integrating the GCN.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">Acknowledgements</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1"><span class="ltx_text" id="Sx1.p1.1.1" style="font-size:90%;">This work was supported by research grant 10.54499/2022.07849.CEECIND/CP1713/CT0001 and LARSyS funding (DOI: 10.54499/LA/P/0083/2020, 10.54499/UIDP/50009/2020, 10.54499/UIDB/50009/2020), through Fundação para a Ciência e a Tecnologia, and by the SmartRetail project [PRR - C645440011-00000062], through IAPMEI - Agência para a Competitividade e Inovação.</span><span class="ltx_text" id="Sx1.p1.1.2"></span></p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib1.5.5.1" style="font-size:90%;">Andriluka et al. [2014]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.7.1" style="font-size:90%;">
Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and Bernt Schiele.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.8.1" style="font-size:90%;">2d human pose estimation: New benchmark and state of the art analysis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib1.10.2" style="font-size:90%;">2014 IEEE Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib1.11.3" style="font-size:90%;">, pages 3686–3693, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib2.5.5.1" style="font-size:90%;">Arnab et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.7.1" style="font-size:90%;">
Anurag Arnab, Carl Doersch, and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.8.1" style="font-size:90%;">Exploiting temporal context for 3d human pose estimation in the wild.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib2.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib2.11.3" style="font-size:90%;">, pages 3395–3404, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib3.5.5.1" style="font-size:90%;">Black et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.7.1" style="font-size:90%;">
Michael J. Black, Priyanka Patel, Joachim Tesch, and Jinlong Yang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.8.1" style="font-size:90%;">Bedlam: A synthetic dataset of bodies exhibiting detailed lifelike animated motion.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib3.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span class="ltx_text" id="bib.bib3.11.3" style="font-size:90%;">, pages 8726–8737, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib4.4.4.1" style="font-size:90%;">Blackman [2014]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.6.1" style="font-size:90%;">
Sue Blackman.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.7.1" style="font-size:90%;">Rigging with mixamo.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.8.1" style="font-size:90%;">Unity for Absolute Beginners</em><span class="ltx_text" id="bib.bib4.9.2" style="font-size:90%;">, pages 565–573, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib5.5.5.1" style="font-size:90%;">Cai et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.7.1" style="font-size:90%;">
Yujun Cai, Liuhao Ge, Jun Liu, Jianfei Cai, Tat-Jen Cham, Junsong Yuan, and Nadia Magnenat Thalmann.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.8.1" style="font-size:90%;">Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib5.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on computer vision</em><span class="ltx_text" id="bib.bib5.11.3" style="font-size:90%;">, pages 2272–2281, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib6.5.5.1" style="font-size:90%;">Cao et al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.7.1" style="font-size:90%;">
Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.8.1" style="font-size:90%;">Realtime multi-person 2d pose estimation using part affinity fields.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib6.10.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span class="ltx_text" id="bib.bib6.11.3" style="font-size:90%;">, pages 7291–7299, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib7.5.5.1" style="font-size:90%;">Chen et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.7.1" style="font-size:90%;">
Yilun Chen, Zhicheng Wang, Yuxiang Peng, Zhiqiang Zhang, Gang Yu, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.8.1" style="font-size:90%;">Cascaded pyramid network for multi-person pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib7.10.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span class="ltx_text" id="bib.bib7.11.3" style="font-size:90%;">, pages 7103–7112, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib8.5.5.1" style="font-size:90%;">Cheng et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.7.1" style="font-size:90%;">
Yu Cheng, Bo Yang, Bo Wang, Yan Wending, and Robby Tan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.8.1" style="font-size:90%;">Occlusion-aware networks for 3d human pose estimation in video.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib8.10.2" style="font-size:90%;">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</em><span class="ltx_text" id="bib.bib8.11.3" style="font-size:90%;">, pages 723–732, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib9.5.5.1" style="font-size:90%;">Cheng et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.7.1" style="font-size:90%;">
Yu Cheng, Bo Wang, Bo Yang, and Robby T Tan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.8.1" style="font-size:90%;">Graph and temporal convolutional networks for 3d multi-person pose estimation in monocular videos.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib9.10.2" style="font-size:90%;">Proceedings of the AAAI Conference on Artificial Intelligence</em><span class="ltx_text" id="bib.bib9.11.3" style="font-size:90%;">, pages 1157–1165, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib10.4.4.1" style="font-size:90%;">Community [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.6.1" style="font-size:90%;">
Blender Online Community.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.7.1" style="font-size:90%;">Blender - a 3D modelling and rendering package</em><span class="ltx_text" id="bib.bib10.8.2" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.9.1" style="font-size:90%;">Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib11.5.5.1" style="font-size:90%;">Fang et al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.7.1" style="font-size:90%;">
Hao-Shu Fang, Shuqin Xie, Yu-Wing Tai, and Cewu Lu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.8.1" style="font-size:90%;">Rmpe: Regional multi-person pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib11.10.2" style="font-size:90%;">Proceedings of the IEEE international conference on computer vision</em><span class="ltx_text" id="bib.bib11.11.3" style="font-size:90%;">, pages 2334–2343, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib12.5.5.1" style="font-size:90%;">He et al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.7.1" style="font-size:90%;">
Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.8.1" style="font-size:90%;">Mask r-cnn.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib12.10.2" style="font-size:90%;">Proceedings of the IEEE international conference on computer vision</em><span class="ltx_text" id="bib.bib12.11.3" style="font-size:90%;">, pages 2961–2969, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib13.5.5.1" style="font-size:90%;">Hu et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.7.1" style="font-size:90%;">
Wenbo Hu, Changgong Zhang, Fangneng Zhan, Lei Zhang, and Tien-Tsin Wong.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.8.1" style="font-size:90%;">Conditional directed graph convolution for 3d human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib13.10.2" style="font-size:90%;">Proceedings of the 29th ACM International Conference on Multimedia</em><span class="ltx_text" id="bib.bib13.11.3" style="font-size:90%;">, pages 602–611, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib14.5.5.1" style="font-size:90%;">Ionescu et al. [2014]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.7.1" style="font-size:90%;">
Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.8.1" style="font-size:90%;">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.9.1" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</em><span class="ltx_text" id="bib.bib14.10.2" style="font-size:90%;">, 36(7):1325–1339, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib15.5.5.1" style="font-size:90%;">Iqbal et al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.7.1" style="font-size:90%;">
Umar Iqbal, Anton Milan, and Juergen Gall.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.8.1" style="font-size:90%;">Posetrack: Joint multi-person pose estimation and tracking.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib15.10.2" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib15.11.3" style="font-size:90%;">, pages 2011–2020, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib16.5.5.1" style="font-size:90%;">Joo et al. [2015]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.7.1" style="font-size:90%;">
Hanbyul Joo, Hao Liu, Lei Tan, Lin Gui, Bart Nabbe, Iain Matthews, Takeo Kanade, Shohei Nobuhara, and Yaser Sheikh.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.8.1" style="font-size:90%;">Panoptic studio: A massively multiview system for social motion capture.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib16.10.2" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer Vision</em><span class="ltx_text" id="bib.bib16.11.3" style="font-size:90%;">, pages 3334–3342, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib17.4.4.1" style="font-size:90%;">Kipf and Welling [2016]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.6.1" style="font-size:90%;">
Thomas N Kipf and Max Welling.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.7.1" style="font-size:90%;">Semi-supervised classification with graph convolutional networks.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.8.1" style="font-size:90%;">arXiv preprint arXiv:1609.02907</em><span class="ltx_text" id="bib.bib17.9.2" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib18.5.5.1" style="font-size:90%;">Lea et al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.7.1" style="font-size:90%;">
Colin Lea, Michael D. Flynn, Rene Vidal, Austin Reiter, and Gregory D. Hager.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.8.1" style="font-size:90%;">Temporal convolutional networks for action segmentation and detection.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib18.10.2" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em><span class="ltx_text" id="bib.bib18.11.3" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib19.4.4.1" style="font-size:90%;">Lee and Chen [1985]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.6.1" style="font-size:90%;">
Hsi-Jian Lee and Zen Chen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.7.1" style="font-size:90%;">Determination of 3d human body postures from a single view.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.8.1" style="font-size:90%;">Computer Vision, Graphics, and Image Processing</em><span class="ltx_text" id="bib.bib19.9.2" style="font-size:90%;">, 30(2):148–168, 1985.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib20.5.5.1" style="font-size:90%;">Li et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.7.1" style="font-size:90%;">
Miaopeng Li, Zimeng Zhou, Jie Li, and Xinguo Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.8.1" style="font-size:90%;">Bottom-up pose estimation of multiple person with bounding box constraint.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib20.10.2" style="font-size:90%;">2018 24th international conference on pattern recognition (ICPR)</em><span class="ltx_text" id="bib.bib20.11.3" style="font-size:90%;">, pages 115–120. IEEE, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib21.4.4.1" style="font-size:90%;">Li and Chan [2015]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.6.1" style="font-size:90%;">
Sijin Li and Antoni B Chan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.7.1" style="font-size:90%;">3d human pose estimation from monocular images with deep convolutional neural network.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.8.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib21.9.2" style="font-size:90%;">Computer Vision–ACCV 2014: 12th Asian Conference on Computer Vision, Singapore, Singapore, November 1-5, 2014, Revised Selected Papers, Part II 12</em><span class="ltx_text" id="bib.bib21.10.3" style="font-size:90%;">, pages 332–347. Springer, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib22.5.5.1" style="font-size:90%;">Lin et al. [2014]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.7.1" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.8.1" style="font-size:90%;">Microsoft coco: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib22.10.2" style="font-size:90%;">Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13</em><span class="ltx_text" id="bib.bib22.11.3" style="font-size:90%;">, pages 740–755. Springer, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib23.5.5.1" style="font-size:90%;">Lin et al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.7.1" style="font-size:90%;">
Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.8.1" style="font-size:90%;">Feature pyramid networks for object detection.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib23.10.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span class="ltx_text" id="bib.bib23.11.3" style="font-size:90%;">, pages 2117–2125, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib24.5.5.1" style="font-size:90%;">Mahmood et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.7.1" style="font-size:90%;">
Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Gerard Pons-Moll, and Michael J Black.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.8.1" style="font-size:90%;">Amass: Archive of motion capture as surface shapes.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib24.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on computer vision</em><span class="ltx_text" id="bib.bib24.11.3" style="font-size:90%;">, pages 5442–5451, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib25.5.5.1" style="font-size:90%;">Martinez et al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.7.1" style="font-size:90%;">
Julieta Martinez, Rayat Hossain, Javier Romero, and James J Little.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.8.1" style="font-size:90%;">A simple yet effective baseline for 3d human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib25.10.2" style="font-size:90%;">Proceedings of the IEEE international conference on computer vision</em><span class="ltx_text" id="bib.bib25.11.3" style="font-size:90%;">, pages 2640–2649, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib26.4.4.1" style="font-size:90%;">O’Shea and Nash [2015]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.6.1" style="font-size:90%;">
Keiron O’Shea and Ryan Nash.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.7.1" style="font-size:90%;">An introduction to convolutional neural networks.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.8.1" style="font-size:90%;">arXiv preprint arXiv:1511.08458</em><span class="ltx_text" id="bib.bib26.9.2" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib27.5.5.1" style="font-size:90%;">Paszke et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.7.1" style="font-size:90%;">
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.8.1" style="font-size:90%;">Pytorch: An imperative style, high-performance deep learning library.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.9.1" style="font-size:90%;">Advances in neural information processing systems</em><span class="ltx_text" id="bib.bib27.10.2" style="font-size:90%;">, 32, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib28.5.5.1" style="font-size:90%;">Patel et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.7.1" style="font-size:90%;">
Priyanka Patel, Chun-Hao P. Huang, Joachim Tesch, David T. Hoffmann, Shashank Tripathi, and Michael J. Black.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.8.1" style="font-size:90%;">AGORA: Avatars in geography optimized for regression analysis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib28.10.2" style="font-size:90%;">Proceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</em><span class="ltx_text" id="bib.bib28.11.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib29.5.5.1" style="font-size:90%;">Pavlakos et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.7.1" style="font-size:90%;">
Georgios Pavlakos, Xiaowei Zhou, and Kostas Daniilidis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.8.1" style="font-size:90%;">Ordinal depth supervision for 3d human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib29.10.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span class="ltx_text" id="bib.bib29.11.3" style="font-size:90%;">, pages 7307–7316, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib30.5.5.1" style="font-size:90%;">Pavllo et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.7.1" style="font-size:90%;">
Dario Pavllo, Christoph Feichtenhofer, David Grangier, and Michael Auli.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.8.1" style="font-size:90%;">3d human pose estimation in video with temporal convolutions and semi-supervised training.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib30.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em><span class="ltx_text" id="bib.bib30.11.3" style="font-size:90%;">, pages 7753–7762, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib31.5.5.1" style="font-size:90%;">Qiu et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.7.1" style="font-size:90%;">
Zhongwei Qiu, Kai Qiu, Jianlong Fu, and Dongmei Fu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.8.1" style="font-size:90%;">Dgcn: Dynamic graph convolutional network for efficient multi-person pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib31.10.2" style="font-size:90%;">Proceedings of the AAAI Conference on Artificial Intelligence</em><span class="ltx_text" id="bib.bib31.11.3" style="font-size:90%;">, pages 11924–11931, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib32.5.5.1" style="font-size:90%;">Reddi et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.7.1" style="font-size:90%;">
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.8.1" style="font-size:90%;">On the convergence of adam and beyond.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.9.1" style="font-size:90%;">arXiv preprint arXiv:1904.09237</em><span class="ltx_text" id="bib.bib32.10.2" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib33.5.5.1" style="font-size:90%;">Sárándi et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.7.1" style="font-size:90%;">
István Sárándi, Timm Linder, Kai O Arras, and Bastian Leibe.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.8.1" style="font-size:90%;">How robust is 3d human pose estimation to occlusion?
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.9.1" style="font-size:90%;">arXiv preprint arXiv:1808.09316</em><span class="ltx_text" id="bib.bib33.10.2" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib34.5.5.1" style="font-size:90%;">Shan et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.7.1" style="font-size:90%;">
Wenkang Shan, Zhenhua Liu, Xinfeng Zhang, Zhao Wang, Kai Han, Shanshe Wang, Siwei Ma, and Wen Gao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.8.1" style="font-size:90%;">Diffusion-based 3d human pose estimation with multi-hypothesis aggregation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib34.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em><span class="ltx_text" id="bib.bib34.11.3" style="font-size:90%;">, pages 14761–14771, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib35.5.5.1" style="font-size:90%;">Tome et al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.7.1" style="font-size:90%;">
Denis Tome, Chris Russell, and Lourdes Agapito.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.8.1" style="font-size:90%;">Lifting from the deep: Convolutional 3d pose estimation from a single image.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib35.10.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span class="ltx_text" id="bib.bib35.11.3" style="font-size:90%;">, pages 2500–2509, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib36.4.4.1" style="font-size:90%;">Toshev and Szegedy [2014]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.6.1" style="font-size:90%;">
Alexander Toshev and Christian Szegedy.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.7.1" style="font-size:90%;">Deeppose: Human pose estimation via deep neural networks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.8.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib36.9.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span class="ltx_text" id="bib.bib36.10.3" style="font-size:90%;">, pages 1653–1660, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib37.5.5.1" style="font-size:90%;">Varol et al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.7.1" style="font-size:90%;">
Gul Varol, Javier Romero, Xavier Martin, Naureen Mahmood, Michael J. Black, Ivan Laptev, and Cordelia Schmid.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.8.1" style="font-size:90%;">Learning from synthetic humans.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib37.10.2" style="font-size:90%;">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em><span class="ltx_text" id="bib.bib37.11.3" style="font-size:90%;">. IEEE, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib38.5.5.1" style="font-size:90%;">Wojke et al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.7.1" style="font-size:90%;">
Nicolai Wojke, Alex Bewley, and Dietrich Paulus.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.8.1" style="font-size:90%;">Simple online and realtime tracking with a deep association metric.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib38.10.2" style="font-size:90%;">2017 IEEE international conference on image processing (ICIP)</em><span class="ltx_text" id="bib.bib38.11.3" style="font-size:90%;">, pages 3645–3649. IEEE, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib39.5.5.1" style="font-size:90%;">Wu et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.7.1" style="font-size:90%;">
Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.8.1" style="font-size:90%;">Detectron2: A pytorch-based modular object detection library.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.9.1" style="font-size:90%;">Meta AI</em><span class="ltx_text" id="bib.bib39.10.2" style="font-size:90%;">, 10, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib40.5.5.1" style="font-size:90%;">Zhao et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.7.1" style="font-size:90%;">
Qitao Zhao, Ce Zheng, Mengyuan Liu, Pichao Wang, and Chen Chen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.8.1" style="font-size:90%;">Poseformerv2: Exploring frequency domain for efficient and robust 3d human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib40.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib40.11.3" style="font-size:90%;">, pages 8877–8886, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib41.5.5.1" style="font-size:90%;">Zheng et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.7.1" style="font-size:90%;">
Ce Zheng, Sijie Zhu, Matias Mendieta, Taojiannan Yang, Chen Chen, and Zhengming Ding.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.8.1" style="font-size:90%;">3d human pose estimation with spatial and temporal transformers.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib41.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span class="ltx_text" id="bib.bib41.11.3" style="font-size:90%;">, pages 11656–11665, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib42.5.5.1" style="font-size:90%;">Zheng et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.7.1" style="font-size:90%;">
Ce Zheng, Wenhan Wu, Chen Chen, Taojiannan Yang, Sijie Zhu, Ju Shen, Nasser Kehtarnavaz, and Mubarak Shah.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.8.1" style="font-size:90%;">Deep learning-based human pose estimation: A survey.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.9.1" style="font-size:90%;">ACM Computing Surveys</em><span class="ltx_text" id="bib.bib42.10.2" style="font-size:90%;">, 56(1):1–37, 2023.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para ltx_noindent" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\thetitle</span>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.2"><span class="ltx_text" id="p1.2.1" style="font-size:144%;">Supplementary Material 
<br class="ltx_break"/></span></p>
</div>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section" style="font-size:144%;">
<span class="ltx_tag ltx_tag_section">8 </span> BlendMimic3D examples </h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1"><span class="ltx_text" id="S8.p1.1.1" style="font-size:144%;">For illustration purposes, Figure </span><a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S8.F10" style="font-size:144%;" title="Figure 10 ‣ 8 BlendMimic3D examples ‣ Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_tag">10</span></a><span class="ltx_text" id="S8.p1.1.2" style="font-size:144%;"> presents all four camera views of the same frame from the “Focus_multi" action in our synthetic test set, featuring subject SS3. This particular action is designed to simulate a multi-person scenario within a supermarket setting, where three subjects interact amidst objects, creating occlusions.</span></p>
</div>
<figure class="ltx_figure" id="S8.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="338" id="S8.F10.g1" src="extracted/2404.16136v1/images/FM.png" width="598"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S8.F10.4.1.1" style="font-size:63%;">Figure 10</span>: </span><span class="ltx_text" id="S8.F10.5.2" style="font-size:63%;">Subject SS3 engaging in the “Focus_multi" action. This figure showcases the same frame from different perspectives: (top left) Camera 0; (top right) Camera 1; (bottom left) Camera 2; (bottom right) Camera 3.</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S9">
<h2 class="ltx_title ltx_title_section" style="font-size:144%;">
<span class="ltx_tag ltx_tag_section">9 </span>Detailed Evaluation of GCN</h2>
<div class="ltx_para" id="S9.p1">
<p class="ltx_p" id="S9.p1.1"><span class="ltx_text" id="S9.p1.1.1" style="font-size:144%;">Table </span><a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#S9.T3" style="font-size:144%;" title="Table 3 ‣ 9 Detailed Evaluation of GCN ‣ Accepted at 6th Workshop and Competition on Affective Behavior Analysis in-the-wild CVPR 2024 Workshop 3D Human Pose Estimation with Occlusions: Introducing BlendMimic3D Dataset and GCN Refinement"><span class="ltx_text ltx_ref_tag">3</span></a><span class="ltx_text" id="S9.p1.1.2" style="font-size:144%;"> presents an evaluation of the GCN pose refinement block’s performance across actions in the BlendMimic3D test set. Covering both CPN-based and Detectron2-based detections, the table demonstrates MPJPE improvements through the incorporation of the GCN into established 3D HPE models. Highlighted results demonstrate improvements in the accuracy of pose estimation in scenarios with occlusions.</span></p>
</div>
<figure class="ltx_table" id="S9.T3">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>Evaluation of our GCN pose refinement block against previous methods. This evaluation includes performance on both CPN-based and Detectron2-based detections, utilizing the BlendMimic3D test set. For each 3D HPE algorithm and corresponding 2D detector, the highest scores achieved, both independently and in combination with the GCN, are highlighted in <span class="ltx_text" id="S9.T3.6.1" style="color:#0ACC66;">green</span>.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S9.T3.7">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S9.T3.7.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" id="S9.T3.7.1.1.1" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S9.T3.7.1.1.1.1" style="font-size:90%;">Model</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" id="S9.T3.7.1.1.2" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold" id="S9.T3.7.1.1.2.1" style="font-size:90%;">2D HPE</span><span class="ltx_text" id="S9.T3.7.1.1.2.2" style="font-size:90%;">   </span><span class="ltx_rule" style="width:1.0pt;background:black;display:inline-block;"> </span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S9.T3.7.1.1.3" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold" id="S9.T3.7.1.1.3.1" style="font-size:90%;">TakesItem</span><span class="ltx_text" id="S9.T3.7.1.1.3.2" style="font-size:90%;"> [mm]</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S9.T3.7.1.1.4" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold" id="S9.T3.7.1.1.4.1" style="font-size:90%;">TakesItem_multi</span><span class="ltx_text" id="S9.T3.7.1.1.4.2" style="font-size:90%;"> [mm]</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S9.T3.7.1.1.5" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold" id="S9.T3.7.1.1.5.1" style="font-size:90%;">Focus</span><span class="ltx_text" id="S9.T3.7.1.1.5.2" style="font-size:90%;"> [mm]</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S9.T3.7.1.1.6" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold" id="S9.T3.7.1.1.6.1" style="font-size:90%;">Focus_multi</span><span class="ltx_text" id="S9.T3.7.1.1.6.2" style="font-size:90%;"> [mm]</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S9.T3.7.1.1.7" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold" id="S9.T3.7.1.1.7.1" style="font-size:90%;">Avg</span><span class="ltx_text" id="S9.T3.7.1.1.7.2" style="font-size:90%;"> [mm]</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S9.T3.7.2.1" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S9.T3.7.2.1.1" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.2.1.1.1" style="font-size:90%;background-color:#E6E6E6;">VideoPose3D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S9.T3.7.2.1.2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.2.1.2.1" style="font-size:90%;background-color:#E6E6E6;">CPN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>   <span class="ltx_rule" style="width:1.0pt;background:black;display:inline-block;"> </span></span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T3.7.2.1.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.2.1.3.1" style="font-size:90%;background-color:#E6E6E6;">167.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T3.7.2.1.4" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.2.1.4.1" style="font-size:90%;background-color:#E6E6E6;">170.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T3.7.2.1.5" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.2.1.5.1" style="font-size:90%;background-color:#E6E6E6;">178.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S9.T3.7.2.1.6" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.2.1.6.1" style="font-size:90%;background-color:#E6E6E6;">183.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T3.7.2.1.7" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.2.1.7.1" style="font-size:90%;background-color:#E6E6E6;">175.0</span></td>
</tr>
<tr class="ltx_tr" id="S9.T3.7.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S9.T3.7.3.2.1" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.3.2.1.1" style="font-size:90%;">+ GCN</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S9.T3.7.3.2.2" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text" id="S9.T3.7.3.2.2.1" style="font-size:90%;">CPN </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S9.T3.7.3.2.2.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a><span class="ltx_text" id="S9.T3.7.3.2.2.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S9.T3.7.3.2.2.4" style="font-size:90%;">   </span><span class="ltx_rule" style="width:1.0pt;background:black;display:inline-block;"> </span>
</th>
<td class="ltx_td ltx_align_center" id="S9.T3.7.3.2.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.3.2.3.1" style="font-size:90%;color:#0ACC66;">106.9</span></td>
<td class="ltx_td ltx_align_center" id="S9.T3.7.3.2.4" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.3.2.4.1" style="font-size:90%;color:#0ACC66;">109.7</span></td>
<td class="ltx_td ltx_align_center" id="S9.T3.7.3.2.5" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.3.2.5.1" style="font-size:90%;color:#0ACC66;">112.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S9.T3.7.3.2.6" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.3.2.6.1" style="font-size:90%;color:#0ACC66;">122.4</span></td>
<td class="ltx_td ltx_align_center" id="S9.T3.7.3.2.7" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.3.2.7.1" style="font-size:90%;color:#0ACC66;">112.7</span></td>
</tr>
<tr class="ltx_tr" id="S9.T3.7.4.3" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S9.T3.7.4.3.1" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.4.3.1.1" style="font-size:90%;background-color:#E6E6E6;">VideoPose3D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S9.T3.7.4.3.2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.4.3.2.1" style="font-size:90%;background-color:#E6E6E6;">Detectron2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">39</span></a>]</cite>   <span class="ltx_rule" style="width:1.0pt;background:black;display:inline-block;"> </span></span></th>
<td class="ltx_td ltx_align_center" id="S9.T3.7.4.3.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.4.3.3.1" style="font-size:90%;background-color:#E6E6E6;">188.8</span></td>
<td class="ltx_td ltx_align_center" id="S9.T3.7.4.3.4" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.4.3.4.1" style="font-size:90%;background-color:#E6E6E6;">194.6</span></td>
<td class="ltx_td ltx_align_center" id="S9.T3.7.4.3.5" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.4.3.5.1" style="font-size:90%;background-color:#E6E6E6;">201.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S9.T3.7.4.3.6" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.4.3.6.1" style="font-size:90%;background-color:#E6E6E6;">206.7</span></td>
<td class="ltx_td ltx_align_center" id="S9.T3.7.4.3.7" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.4.3.7.1" style="font-size:90%;background-color:#E6E6E6;">198.0</span></td>
</tr>
<tr class="ltx_tr" id="S9.T3.7.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S9.T3.7.5.4.1" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.5.4.1.1" style="font-size:90%;">+ GCN</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S9.T3.7.5.4.2" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text" id="S9.T3.7.5.4.2.1" style="font-size:90%;">Detectron2 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S9.T3.7.5.4.2.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">39</span></a><span class="ltx_text" id="S9.T3.7.5.4.2.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S9.T3.7.5.4.2.4" style="font-size:90%;">   </span><span class="ltx_rule" style="width:1.0pt;background:black;display:inline-block;"> </span>
</th>
<td class="ltx_td ltx_align_center" id="S9.T3.7.5.4.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.5.4.3.1" style="font-size:90%;color:#0ACC66;">117.9</span></td>
<td class="ltx_td ltx_align_center" id="S9.T3.7.5.4.4" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.5.4.4.1" style="font-size:90%;color:#0ACC66;">119.7</span></td>
<td class="ltx_td ltx_align_center" id="S9.T3.7.5.4.5" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.5.4.5.1" style="font-size:90%;color:#0ACC66;">130.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S9.T3.7.5.4.6" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.5.4.6.1" style="font-size:90%;color:#0ACC66;">142.6</span></td>
<td class="ltx_td ltx_align_center" id="S9.T3.7.5.4.7" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.5.4.7.1" style="font-size:90%;color:#0ACC66;">127.7</span></td>
</tr>
<tr class="ltx_tr" id="S9.T3.7.6.5" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S9.T3.7.6.5.1" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.6.5.1.1" style="font-size:90%;background-color:#E6E6E6;">PoseFormerV2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S9.T3.7.6.5.2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.6.5.2.1" style="font-size:90%;background-color:#E6E6E6;">CPN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>   <span class="ltx_rule" style="width:1.0pt;background:black;display:inline-block;"> </span></span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T3.7.6.5.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.6.5.3.1" style="font-size:90%;background-color:#E6E6E6;">152.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T3.7.6.5.4" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.6.5.4.1" style="font-size:90%;background-color:#E6E6E6;">157.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T3.7.6.5.5" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.6.5.5.1" style="font-size:90%;background-color:#E6E6E6;">141.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S9.T3.7.6.5.6" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.6.5.6.1" style="font-size:90%;background-color:#E6E6E6;">142.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T3.7.6.5.7" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.6.5.7.1" style="font-size:90%;background-color:#E6E6E6;">148.6</span></td>
</tr>
<tr class="ltx_tr" id="S9.T3.7.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S9.T3.7.7.6.1" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.7.6.1.1" style="font-size:90%;">+ GCN</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S9.T3.7.7.6.2" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text" id="S9.T3.7.7.6.2.1" style="font-size:90%;">CPN </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S9.T3.7.7.6.2.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a><span class="ltx_text" id="S9.T3.7.7.6.2.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S9.T3.7.7.6.2.4" style="font-size:90%;">   </span><span class="ltx_rule" style="width:1.0pt;background:black;display:inline-block;"> </span>
</th>
<td class="ltx_td ltx_align_center" id="S9.T3.7.7.6.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.7.6.3.1" style="font-size:90%;color:#0ACC66;">106.4</span></td>
<td class="ltx_td ltx_align_center" id="S9.T3.7.7.6.4" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.7.6.4.1" style="font-size:90%;color:#0ACC66;">108.7</span></td>
<td class="ltx_td ltx_align_center" id="S9.T3.7.7.6.5" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.7.6.5.1" style="font-size:90%;color:#0ACC66;">105.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S9.T3.7.7.6.6" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.7.6.6.1" style="font-size:90%;color:#0ACC66;">109.7</span></td>
<td class="ltx_td ltx_align_center" id="S9.T3.7.7.6.7" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.7.6.7.1" style="font-size:90%;color:#0ACC66;">107.5</span></td>
</tr>
<tr class="ltx_tr" id="S9.T3.7.8.7" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S9.T3.7.8.7.1" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.8.7.1.1" style="font-size:90%;background-color:#E6E6E6;">PoseFormerV2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S9.T3.7.8.7.2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.8.7.2.1" style="font-size:90%;background-color:#E6E6E6;">Detectron2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">39</span></a>]</cite>   <span class="ltx_rule" style="width:1.0pt;background:black;display:inline-block;"> </span></span></th>
<td class="ltx_td ltx_align_center" id="S9.T3.7.8.7.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.8.7.3.1" style="font-size:90%;background-color:#E6E6E6;">157.8</span></td>
<td class="ltx_td ltx_align_center" id="S9.T3.7.8.7.4" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.8.7.4.1" style="font-size:90%;background-color:#E6E6E6;">164.9</span></td>
<td class="ltx_td ltx_align_center" id="S9.T3.7.8.7.5" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.8.7.5.1" style="font-size:90%;background-color:#E6E6E6;">142.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S9.T3.7.8.7.6" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.8.7.6.1" style="font-size:90%;background-color:#E6E6E6;">154.8</span></td>
<td class="ltx_td ltx_align_center" id="S9.T3.7.8.7.7" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.8.7.7.1" style="font-size:90%;background-color:#E6E6E6;">155.0</span></td>
</tr>
<tr class="ltx_tr" id="S9.T3.7.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S9.T3.7.9.8.1" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.9.8.1.1" style="font-size:90%;">+ GCN</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S9.T3.7.9.8.2" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text" id="S9.T3.7.9.8.2.1" style="font-size:90%;">Detectron2 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S9.T3.7.9.8.2.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">39</span></a><span class="ltx_text" id="S9.T3.7.9.8.2.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S9.T3.7.9.8.2.4" style="font-size:90%;">   </span><span class="ltx_rule" style="width:1.0pt;background:black;display:inline-block;"> </span>
</th>
<td class="ltx_td ltx_align_center" id="S9.T3.7.9.8.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.9.8.3.1" style="font-size:90%;color:#0ACC66;">103.7</span></td>
<td class="ltx_td ltx_align_center" id="S9.T3.7.9.8.4" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.9.8.4.1" style="font-size:90%;color:#0ACC66;">106.2</span></td>
<td class="ltx_td ltx_align_center" id="S9.T3.7.9.8.5" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.9.8.5.1" style="font-size:90%;color:#0ACC66;">99.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S9.T3.7.9.8.6" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.9.8.6.1" style="font-size:90%;color:#0ACC66;">118.3</span></td>
<td class="ltx_td ltx_align_center" id="S9.T3.7.9.8.7" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.9.8.7.1" style="font-size:90%;color:#0ACC66;">106.9</span></td>
</tr>
<tr class="ltx_tr" id="S9.T3.7.10.9" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S9.T3.7.10.9.1" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.10.9.1.1" style="font-size:90%;background-color:#E6E6E6;">D3DP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S9.T3.7.10.9.2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.10.9.2.1" style="font-size:90%;background-color:#E6E6E6;">CPN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>   <span class="ltx_rule" style="width:1.0pt;background:black;display:inline-block;"> </span></span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T3.7.10.9.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.10.9.3.1" style="font-size:90%;background-color:#E6E6E6;">94.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T3.7.10.9.4" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.10.9.4.1" style="font-size:90%;background-color:#E6E6E6;">95.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T3.7.10.9.5" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.10.9.5.1" style="font-size:90%;background-color:#E6E6E6;">100.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S9.T3.7.10.9.6" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.10.9.6.1" style="font-size:90%;background-color:#E6E6E6;">112.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T3.7.10.9.7" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.10.9.7.1" style="font-size:90%;background-color:#E6E6E6;">100.7</span></td>
</tr>
<tr class="ltx_tr" id="S9.T3.7.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S9.T3.7.11.10.1" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.11.10.1.1" style="font-size:90%;">+ GCN</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S9.T3.7.11.10.2" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text" id="S9.T3.7.11.10.2.1" style="font-size:90%;">CPN </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S9.T3.7.11.10.2.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a><span class="ltx_text" id="S9.T3.7.11.10.2.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S9.T3.7.11.10.2.4" style="font-size:90%;">   </span><span class="ltx_rule" style="width:1.0pt;background:black;display:inline-block;"> </span>
</th>
<td class="ltx_td ltx_align_center" id="S9.T3.7.11.10.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.11.10.3.1" style="font-size:90%;color:#0ACC66;">91.8</span></td>
<td class="ltx_td ltx_align_center" id="S9.T3.7.11.10.4" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.11.10.4.1" style="font-size:90%;color:#0ACC66;">93.3</span></td>
<td class="ltx_td ltx_align_center" id="S9.T3.7.11.10.5" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.11.10.5.1" style="font-size:90%;color:#0ACC66;">96.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S9.T3.7.11.10.6" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.11.10.6.1" style="font-size:90%;color:#0ACC66;">99.9</span></td>
<td class="ltx_td ltx_align_center" id="S9.T3.7.11.10.7" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.11.10.7.1" style="font-size:90%;color:#0ACC66;">95.3</span></td>
</tr>
<tr class="ltx_tr" id="S9.T3.7.12.11" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S9.T3.7.12.11.1" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.12.11.1.1" style="font-size:90%;background-color:#E6E6E6;">D3DP<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S9.T3.7.12.11.2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.12.11.2.1" style="font-size:90%;background-color:#E6E6E6;">Detectron2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">39</span></a>]</cite>   <span class="ltx_rule" style="width:1.0pt;background:black;display:inline-block;"> </span></span></th>
<td class="ltx_td ltx_align_center" id="S9.T3.7.12.11.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.12.11.3.1" style="font-size:90%;color:#0ACC66;background-color:#E6E6E6;">88.4</span></td>
<td class="ltx_td ltx_align_center" id="S9.T3.7.12.11.4" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.12.11.4.1" style="font-size:90%;background-color:#E6E6E6;">95.0</span></td>
<td class="ltx_td ltx_align_center" id="S9.T3.7.12.11.5" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.12.11.5.1" style="font-size:90%;background-color:#E6E6E6;">101.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S9.T3.7.12.11.6" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.12.11.6.1" style="font-size:90%;background-color:#E6E6E6;">114.6</span></td>
<td class="ltx_td ltx_align_center" id="S9.T3.7.12.11.7" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.12.11.7.1" style="font-size:90%;background-color:#E6E6E6;">99.9</span></td>
</tr>
<tr class="ltx_tr" id="S9.T3.7.13.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S9.T3.7.13.12.1" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.13.12.1.1" style="font-size:90%;">+ GCN</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S9.T3.7.13.12.2" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text" id="S9.T3.7.13.12.2.1" style="font-size:90%;">Detectron2 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S9.T3.7.13.12.2.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.16136v1#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">39</span></a><span class="ltx_text" id="S9.T3.7.13.12.2.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S9.T3.7.13.12.2.4" style="font-size:90%;">   </span><span class="ltx_rule" style="width:1.0pt;background:black;display:inline-block;"> </span>
</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S9.T3.7.13.12.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.13.12.3.1" style="font-size:90%;">88.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S9.T3.7.13.12.4" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.13.12.4.1" style="font-size:90%;color:#0ACC66;">94.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S9.T3.7.13.12.5" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.13.12.5.1" style="font-size:90%;color:#0ACC66;">98.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S9.T3.7.13.12.6" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.13.12.6.1" style="font-size:90%;color:#0ACC66;">99.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S9.T3.7.13.12.7" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S9.T3.7.13.12.7.1" style="font-size:90%;color:#0ACC66;">95.3</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Apr 30 19:20:23 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
