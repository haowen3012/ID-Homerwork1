<!DOCTYPE html>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond</title>
<!--Generated on Mon Dec 25 04:46:22 2023 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Animal Pose Estimation,  Tracking,  Neural Networks,  Vision Transformer,  Transfer Learning,  Benchmark
" lang="en" name="keywords"/>
<base href="/html/2312.15612v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S1" title="I Introduction ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S2" title="II Related Work ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Work</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S2.SS1" title="II-A Pose Estimation ‣ II Related Work ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Pose Estimation</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S2.SS1.SSS1" title="II-A1 Human pose estimation datasets ‣ II-A Pose Estimation ‣ II Related Work ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span>1 </span>Human pose estimation datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S2.SS1.SSS2" title="II-A2 Animal pose estimation datasets ‣ II-A Pose Estimation ‣ II Related Work ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span>2 </span>Animal pose estimation datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S2.SS1.SSS3" title="II-A3 Pose estimation methods ‣ II-A Pose Estimation ‣ II Related Work ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span>3 </span>Pose estimation methods</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S2.SS2" title="II-B Visual Object Tracking ‣ II Related Work ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Visual Object Tracking</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S2.SS3" title="II-C Comparison to the Conference Version ‣ II Related Work ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Comparison to the Conference Version</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S3" title="III Dataset ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Dataset</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS1" title="III-A Data Collection and Organization ‣ III Dataset ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Data Collection and Organization</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS2" title="III-B Data Annotation ‣ III Dataset ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Data Annotation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS3" title="III-C Statistics of the APTv2 Dataset ‣ III Dataset ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Statistics of the APTv2 Dataset</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S4" title="IV ViTPoseTrack Baseline ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps" style="color:#000000;">ViTPoseTrack<span class="ltx_text" style="color:black;"> Baseline</span></span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S4.SS1" title="IV-A Architecture Design ‣ IV ViTPoseTrack Baseline ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Architecture Design</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S4.SS1.SSS1" title="IV-A1 Pose estimation head ‣ IV-A Architecture Design ‣ IV ViTPoseTrack Baseline ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>1 </span>Pose estimation head</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S4.SS1.SSS2" title="IV-A2 Tracking head ‣ IV-A Architecture Design ‣ IV ViTPoseTrack Baseline ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>2 </span>Tracking head</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS2" title="IV-B Training Schedule ‣ IV ViTPoseTrack Baseline ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Training Schedule</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S5" title="V Experiment ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Experiment</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S5.SS1" title="V-A Representative Methods for Benchmarking ‣ V Experiment ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">Representative Methods for Benchmarking</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S5.SS1.SSS1" title="V-A1 Pose estimation methods ‣ V-A Representative Methods for Benchmarking ‣ V Experiment ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span>1 </span>Pose estimation methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S5.SS1.SSS2" title="V-A2 Tracking methods ‣ V-A Representative Methods for Benchmarking ‣ V Experiment ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span>2 </span>Tracking methods</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S5.SS2" title="V-B Evaluation Metrics ‣ V Experiment ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span> </span><span class="ltx_text ltx_font_italic">Evaluation Metrics</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S5.SS3" title="V-C Implementation Details ‣ V Experiment ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span> </span><span class="ltx_text ltx_font_italic">Implementation Details</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S5.SS4" title="V-D Single-Frame Animal Pose Estimation (SF track) ‣ V Experiment ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-D</span> </span><span class="ltx_text ltx_font_italic">Single-Frame Animal Pose Estimation (SF track)</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S5.SS4.SSS1" title="V-D1 Setting ‣ V-D Single-Frame Animal Pose Estimation (SF track) ‣ V Experiment ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-D</span>1 </span>Setting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S5.SS4.SSS2" title="V-D2 Results and analysis ‣ V-D Single-Frame Animal Pose Estimation (SF track) ‣ V Experiment ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-D</span>2 </span>Results and analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S5.SS5" title="V-E Low-data Training and Generalization (LT track) ‣ V Experiment ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-E</span> </span><span class="ltx_text ltx_font_italic">Low-data Training and Generalization (LT track)</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S5.SS5.SSS1" title="V-E1 Setting ‣ V-E Low-data Training and Generalization (LT track) ‣ V Experiment ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-E</span>1 </span>Setting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S5.SS5.SSS2" title="V-E2 Results and analysis ‣ V-E Low-data Training and Generalization (LT track) ‣ V Experiment ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-E</span>2 </span>Results and analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S5.SS6" title="V-F Animal Pose Tracking (APT track) ‣ V Experiment ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-F</span> </span><span class="ltx_text ltx_font_italic">Animal Pose Tracking (APT track)</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S5.SS6.SSS1" title="V-F1 Setting ‣ V-F Animal Pose Tracking (APT track) ‣ V Experiment ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-F</span>1 </span>Setting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S5.SS6.SSS2" title="V-F2 Results and analysis ‣ V-F Animal Pose Tracking (APT track) ‣ V Experiment ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-F</span>2 </span>Results and analysis</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S6" title="VI Conclusion ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div aria-label="”Conversion" been="" class="package-alerts ltx_document" errors="" found”="" have="" role="“status”">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewbox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: fontawesome</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p>
</div><div class="section" id="target-section"><div id="license-tr">License: arXiv.org perpetual non-exclusive license</div><div id="watermark-tr">arXiv:2312.15612v1 [cs.CV] 25 Dec 2023</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yuxiang Yang, Yingqi Deng, Yufei Xu, Jing Zhang
</span><span class="ltx_author_notes">The project was supported by the National Natural Science Foundation of China (62376080), the Zhejiang Provincial Natural Science Foundation Key Fund of China (LZ23F030003), and the Fundamental Research Funds for the Provincial Universities of Zhejiang (GK239909299001-003).
Corresponding author: Jing Zhang (jing.zhang1@sydney.edu.au)
Y. Yang and Y. Deng are with the School of Electronics and Information, Hangzhou Dianzi University, Hangzhou 310018, China. Y. Xu and J. Zhang are with the School of Computer Science, The University of Sydney, NSW 2006, Australia.
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Animal Pose Estimation and Tracking (APT) is a critical task in detecting and monitoring the keypoints of animals across a series of video frames, which is essential for understanding animal behavior. Past works relating to animals have primarily focused on either animal tracking or single-frame animal pose estimation only, neglecting the integration of both aspects. The absence of comprehensive APT datasets inhibits the progression and evaluation of animal pose estimation and tracking methods based on videos, thereby constraining their real-world applications. To fill this gap, we introduce APTv2, the pioneering large-scale benchmark for animal pose estimation and tracking. APTv2 comprises 2,749 video clips filtered and collected from 30 distinct animal species. Each video clip includes 15 frames, culminating in a total of 41,235 frames. Following meticulous manual annotation and stringent verification, we provide high-quality keypoint and tracking annotations for a total of 84,611 animal instances, split into easy and hard subsets based on the number of instances that exists in the frame. With APTv2 as the foundation, we establish a simple baseline method named <span class="ltx_text" id="id1.id1.1" style="color:#000000;">ViTPoseTrack</span> and provide benchmarks for representative models across three tracks: (1) single-frame animal pose estimation track to evaluate both intra- and inter-domain transfer learning performance, (2) low-data transfer and generalization track to evaluate the inter-species domain generalization performance, and (3) animal pose tracking track. Our experimental results deliver key empirical insights, demonstrating that APTv2 serves as a valuable benchmark for animal pose estimation and tracking. It also presents new challenges and opportunities for future research. The code and dataset are released at <a class="ltx_ref ltx_href" href="https://github.com/ViTAE-Transformer/APTv2" title="">https://github.com/ViTAE-Transformer/APTv2</a>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Animal Pose Estimation, Tracking, Neural Networks, Vision Transformer, Transfer Learning, Benchmark

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Pose estimation is a fundamental task in computer vision that involves identifying and localizing body keypoints in an image. This task plays a crucial role in various vision applications <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib3" title="">3</a>, <a class="ltx_ref" href="#bib.bib37" title="">37</a>, <a class="ltx_ref" href="#bib.bib40" title="">40</a>, <a class="ltx_ref" href="#bib.bib51" title="">51</a>]</cite>, such as behavior understanding and action recognition. While there has been significant progress in human pose estimation, driven by the availability of large-scale human pose datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib24" title="">24</a>, <a class="ltx_ref" href="#bib.bib18" title="">18</a>, <a class="ltx_ref" href="#bib.bib2" title="">2</a>]</cite>, the focus on animal pose estimation remains limited, particularly in the context of video-based estimation. However, video-based animal pose estimation is of paramount importance in understanding animal behavior and promoting wildlife conservation.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Several endeavors have been made to establish animal pose estimation datasets and greatly foster research in this field. Unfortunately, they have primarily concentrated on pose estimation for specific animal categories on individual images. For instance, datasets such as those featuring horses <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib26" title="">26</a>]</cite>, zebras <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib11" title="">11</a>]</cite>, macaques <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib16" title="">16</a>]</cite>, flies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib32" title="">32</a>]</cite>, and tigers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib20" title="">20</a>]</cite> collect and annotate keypoints for particular animal species, contributing significantly to the progression of pose estimation research for these animals. Nonetheless, due to the extensive appearance variance, behavioral differences, and shifts in joint distribution across various animal species as a result of evolution, the models trained on these datasets demonstrate limited performance when applied to unseen animal species, thereby resulting in substandard generalization performance. In an effort to further encourage research on animal pose estimation, datasets that encompass multiple animal species with keypoint annotations have been proposed, such as Animal-Pose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib5" title="">5</a>]</cite> and AP-10K <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib49" title="">49</a>]</cite>. Despite their considerable scale and diversity in terms of animal species, these datasets do not incorporate essential temporal information that facilitates pose-tracking research. On the other hand, some efforts have been made to facilitate the development of animal instance tracking and contribute to the improvement of animal behavior understanding, like the Animal Track <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib52" title="">52</a>]</cite> or Animal Kingdom <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib29" title="">29</a>]</cite>. However, they do not have pose annotations based on the videos but only the bounding box annotations. Such an omission leaves the recognition of poses in consecutive frames unexplored, which is crucial for better animal action recognition and beyond.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To fill this gap, we present APTv2, the first large-scale dataset with high-quality animal pose annotations from successive frames, catering to both animal pose estimation and tracking. APTv2 comprises 2,749 video clips amassed and filtered from 30 diverse animal species. It contains a total of 41,235 frames, with 15 frames sampled from each video. These animals can be further categorized into 15 distinct taxonomic families, which helps in assessing the inter-species and inter-family generalization capacity of pose estimation models. To gather high-quality data for annotation, we manually select videos from the YouTube dataset following the practice in YouTube-VOS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib43" title="">43</a>]</cite>. The selected videos will have high resolution and diverse backgrounds. After that, we extract frames at specific intervals to eliminate redundancy and increase the temporal motion amplitude. Subsequently, we employed 18 proficient annotators to label the keypoints for each animal in each frame in accordance with the MS COCO labeling protocols <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib24" title="">24</a>]</cite>. These labels were then manually cross-verified for accuracy. Additionally, each animal’s trajectory across the videos is indicated with bounding boxes and unique instance IDs. Consequently, APTv2 can support research on both single-frame pose estimation and tracking of animal movements in successive frames.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Utilizing APTv2, we establish three benchmark tracks for evaluating state-of-the-art (SOTA) pose estimation methodologies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib34" title="">34</a>, <a class="ltx_ref" href="#bib.bib41" title="">41</a>, <a class="ltx_ref" href="#bib.bib50" title="">50</a>, <a class="ltx_ref" href="#bib.bib44" title="">44</a>, <a class="ltx_ref" href="#bib.bib45" title="">45</a>]</cite>: (1) Single-Frame Animal Pose Estimation (SF track), (2) Low-data Training and Generalization (LT track), and (3) Animal Pose Tracking (APT track). In the SF track, we extensively assess the performance of representative Convolutional Neural Networks (CNN) and Vision Transformer (ViT)-based methods across various settings, including both inter- and intra-domain transfer learning. Besides, we also investigate the influence of different pre-training datasets, including ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib8" title="">8</a>]</cite>, MS COCO human pose estimation dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib24" title="">24</a>]</cite>, and AP-10k <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib49" title="">49</a>]</cite>, on the transfer learning performance, respectively. In the LT track, we evaluate the inter-family domain generalization capacity of the pose estimation model, which is trained on images of all species from some families and then tested on images of seen or unseen families. In the APT track, a number of object trackers, including a customized one based on the plain vision transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib9" title="">9</a>]</cite>, are utilized to track animal instances, while representative pose estimation models detect the keypoints of the tracked instances, and their performance is evaluated accordingly. What’s more, a simple yet effective <span class="ltx_text" id="S1.p4.1.1" style="color:#000000;">ViTPoseTrack</span> baseline is provided, which utilizes a shared backbone for feature extraction and task-specific heads for pose estimation and tracking. Comprehensive experiment settings and results are provided in Section <a class="ltx_ref" href="#S5" title="V Experiment ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_tag">V</span></a>, where we highlight the immense potential of vision transformers for both animal pose estimation and tracking, the benefits of knowledge transfer between human and animal pose estimation, the advantages of incorporating diverse animal species into animal pose estimation, as well as the scalability of involving large scale models in animal pose tracking with the proposed baseline method.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">The main contribution of this paper is three-fold</p>
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We introduce APTv2, the pioneering large-scale benchmark for animal pose estimation and tracking. Its vast scale, wide range of animal species, and plentiful annotations of keypoints, bounding boxes, and instance IDs within consecutive frames make it a robust testing ground for future research.
</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We establish three demanding tasks, specifically SF, LT, and APT, rooted in APTv2. We then comprehensively benchmark representative pose estimation models utilizing both CNNs and vision transformers, yielding valuable insights.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">A new baseline method called <span class="ltx_text" id="S1.I1.i3.p1.1.1" style="color:#000000;">ViTPoseTrack</span> with shared backbone and specific task heads for animal pose tracking tasks is established. With its simple structure, we have demonstrated the scalability and simplicity of large-scale models on the APT task, expanding the application boundaries of large models.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related Work</span>
</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.5.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.6.2">Pose Estimation</span>
</h3>
<section class="ltx_subsubsection" id="S2.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS1.SSS1.5.1.1">II-A</span>1 </span>Human pose estimation datasets</h4>
<div class="ltx_para" id="S2.SS1.SSS1.p1">
<p class="ltx_p" id="S2.SS1.SSS1.p1.1">Human pose estimation has experienced rapid development both in terms of datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib24" title="">24</a>, <a class="ltx_ref" href="#bib.bib18" title="">18</a>, <a class="ltx_ref" href="#bib.bib2" title="">2</a>]</cite> and methodologies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib38" title="">38</a>, <a class="ltx_ref" href="#bib.bib42" title="">42</a>, <a class="ltx_ref" href="#bib.bib35" title="">35</a>, <a class="ltx_ref" href="#bib.bib10" title="">10</a>]</cite>. Notably, MPII <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib2" title="">2</a>]</cite> and MS COCO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib24" title="">24</a>]</cite> are two widely recognized large-scale benchmarks for human pose estimation. To evaluate their performance in more challenging cases such as occlusions and crowd scenes, OCHuman <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib53" title="">53</a>]</cite> and CrowdPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib18" title="">18</a>]</cite>, have been established. Despite these substantial contributions, temporal information, which is critical for understanding human behavior and facilitating action imitation from humans to robots, has been largely overlooked. To address this deficiency, several video-based pose estimation datasets, such as VideoPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib33" title="">33</a>]</cite>, YouTube Pose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib6" title="">6</a>]</cite>, J-HMDB <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib15" title="">15</a>]</cite>, and PoseTrack <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib1" title="">1</a>]</cite>, have been introduced. These resources have considerably aided the research into human pose estimation and tracking <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib41" title="">41</a>]</cite>. Due to the distinct variance between humans and animals, models trained on these datasets are hard to generalize well on wild animals.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS1.SSS2.5.1.1">II-A</span>2 </span>Animal pose estimation datasets</h4>
<div class="ltx_para" id="S2.SS1.SSS2.p1">
<p class="ltx_p" id="S2.SS1.SSS2.p1.1">Given the vast diversity of wild animal species, animal pose estimation presents greater challenges than its human counterpart, and its progress significantly trails behind. Recently, there has been growing interest in animal pose estimation due to the increasing demand for understanding animal behavior and enhancing wildlife conservation. Early works introduced single-category animal pose estimation datasets, such as horse <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib26" title="">26</a>]</cite>, zebra <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib11" title="">11</a>]</cite>, macaque <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib16" title="">16</a>]</cite>, fly <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib32" title="">32</a>]</cite>, and tiger <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib20" title="">20</a>]</cite>. However, models trained on these datasets exhibit limited generalization abilities due to the substantial differences in appearance and movement patterns between different animal species. To remedy this, datasets covering multiple animal species have been developed, including Animal-Pose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib5" title="">5</a>]</cite>, Animal Kingdom <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib29" title="">29</a>]</cite>, and AP-10K <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib49" title="">49</a>]</cite>, the latter of which contains 10,015 annotated images from 23 animal families and 54 species. Yet, these datasets lack temporal annotations, inhibiting the development of animal pose tracking methods. </p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS1.SSS3.5.1.1">II-A</span>3 </span>Pose estimation methods</h4>
<div class="ltx_para" id="S2.SS1.SSS3.p1">
<p class="ltx_p" id="S2.SS1.SSS3.p1.1">Pose estimation has significantly evolved, transitioning from CNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib41" title="">41</a>, <a class="ltx_ref" href="#bib.bib31" title="">31</a>]</cite> to ViTs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib50" title="">50</a>, <a class="ltx_ref" href="#bib.bib45" title="">45</a>]</cite>. The typical process involves estimating a target’s pose from given instances, no matter for humans or animals. To precisely locate keypoints, early CNN-based methods tend to rely on high-resolution features via techniques like skip feature concatenation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib28" title="">28</a>]</cite> or highway structures <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib34" title="">34</a>]</cite>. SimpleBaseline <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib41" title="">41</a>]</cite> directly recovers these high-resolution features using a decoder of several deconvolution layers. As ViTs have exhibited superior performance across various vision tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib27" title="">27</a>, <a class="ltx_ref" href="#bib.bib21" title="">21</a>]</cite>, some methods aim to use transformers as enhanced decoders following the CNN backbone <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib19" title="">19</a>, <a class="ltx_ref" href="#bib.bib22" title="">22</a>, <a class="ltx_ref" href="#bib.bib47" title="">47</a>]</cite>. For example, TransPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib47" title="">47</a>]</cite> and TokenPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib22" title="">22</a>]</cite> adapt attention structures to model the relationships between different keypoints. The other kinds of methods rely on transformers only to deal with pose estimation methods. For example, HRFormer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib50" title="">50</a>]</cite> uses transformers to extract high-resolution features directly. ViTPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib44" title="">44</a>]</cite> further adopts plain vision transformers as backbones and demonstrates its scalability on human pose estimation. Despite their superior performance, they need a separate tracking model for pose tracking tasks. In contrast, we propose a simple baseline method named <span class="ltx_text" id="S2.SS1.SSS3.p1.1.1" style="color:#000000;">ViTPoseTrack</span> that reuses the backbone model trained for pose estimation in tracking, enjoying efficient memory footprint, minimal model design, and good scalability of model size.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.5.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.6.2">Visual Object Tracking</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Object tracking <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib17" title="">17</a>, <a class="ltx_ref" href="#bib.bib14" title="">14</a>, <a class="ltx_ref" href="#bib.bib56" title="">56</a>, <a class="ltx_ref" href="#bib.bib30" title="">30</a>]</cite> is a fundamental research task in computer vision. To advance the research in animal tracking, Animal Track <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib52" title="">52</a>]</cite> and Animal Kingdom <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib29" title="">29</a>]</cite> are proposed, which have diverse animal species. However, they do not have keypoint annotations for consecutive frames and are not thus suitable for the APT task. One common approach to object tracking follows the “tracking by detection” routine. Given the current frame and subsequent frames, an object detector is first used to identify candidate objects in the subsequent frames and the detected results are then associated with the target in the current frame. These methods have delivered superior results in both multiple object tracking (MOT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib39" title="">39</a>, <a class="ltx_ref" href="#bib.bib54" title="">54</a>]</cite> and single object tracking (SOT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib4" title="">4</a>, <a class="ltx_ref" href="#bib.bib55" title="">55</a>, <a class="ltx_ref" href="#bib.bib36" title="">36</a>]</cite>. However, the generalization capabilities of these trackers are limited, <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.1">i.e.</span>, they can only track objects that belong to categories that the detectors support. This limitation restricts their use in animal pose tracking, where many animal species may not be seen during detector training. Another approach to object tracking follows the “tracking by matching” pipeline. This approach employs a Siamese network to extract features from the tracked targets in the previous frame and the search regions in the subsequent frame and then matches these features to locate the targets in the search region. In this paper, we primarily adopt the single object tracking method for animal instance tracking, as it does not make assumptions about target categories. </p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.5.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.6.2">Comparison to the Conference Version</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">A preliminary version of this paper is presented in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib48" title="">48</a>]</cite>. This paper extends the previous study with three major improvements:</p>
<ol class="ltx_enumerate" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1">We specifically crawl, clean, and annotate hard cases (<span class="ltx_text ltx_font_italic" id="S2.I1.i1.p1.1.1">e.g.</span>, videos with more than two instances in each frame) to enrich the original dataset and approach real-world scenarios better, increasing the number of instances from 53,006 to 84,611. The extended dataset makes it more suitable to evaluate the performance of existing pose estimation and tracking methods thoroughly.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1">We split our APTv2 dataset into easy and hard subsets according to the difficulty level and systematically investigate the influence of using them for training and evaluation, obtaining useful insights. Moreover, we investigate the scalability of model size, especially regarding ViTs, for the first time on the APT task.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1">We delve further into the design and training aspects of the simple baseline method named <span class="ltx_text" id="S2.I1.i3.p1.1.1" style="color:#000000;">ViTPoseTrack</span>, with particular emphasis on the size of the model. Despite its simplistic design, this method achieves remarkably high performance, benefiting from both less memory footprint and good scalability in terms of model size.</p>
</div>
</li>
</ol>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Dataset</span>
</h2>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Comparison of different animal pose datasets.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.3.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T1.3.1.1.1" style="padding-left:6.5pt;padding-right:6.5pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.3.1.1.2" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.1.1.2.1" style="font-size:70%;">#Species</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.3.1.1.3" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.1.1.3.1" style="font-size:70%;">#Family</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.3.1.1.4" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.1.1.4.1" style="font-size:70%;">#Frame</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.3.1.1.5" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.1.1.5.1" style="font-size:70%;">#Keypoint</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.3.1.1.6" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.1.1.6.1" style="font-size:70%;">#Sequence</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.3.1.1.7" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.1.1.7.1" style="font-size:70%;">#Instance</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.3.1.1.8" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.1.1.8.1" style="font-size:70%;">#Background Type</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.3.1.1.9" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.1.1.9.1" style="font-size:70%;">Difficulty Level</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.3.2.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.3.2.1.1" style="padding-left:6.5pt;padding-right:6.5pt;">
<span class="ltx_text" id="S3.T1.3.2.1.1.1" style="font-size:70%;">Horses-10 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T1.3.2.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib26" title="">26</a><span class="ltx_text" id="S3.T1.3.2.1.1.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.2.1.2" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.2.1.2.1" style="font-size:70%;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.2.1.3" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.2.1.3.1" style="font-size:70%;">N/A</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.2.1.4" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.2.1.4.1" style="font-size:70%;">8,100</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.2.1.5" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.2.1.5.1" style="font-size:70%;">22</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.2.1.6" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.2.1.6.1" style="font-size:70%;">N/A</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.2.1.7" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.2.1.7.1" style="font-size:70%;">8,110</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.2.1.8" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.2.1.8.1" style="font-size:70%;">N/A</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.2.1.9" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.2.1.9.1" style="font-size:70%;">N/A</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.3.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.3.3.2.1" style="padding-left:6.5pt;padding-right:6.5pt;">
<span class="ltx_text" id="S3.T1.3.3.2.1.1" style="font-size:70%;">Animal-Pose Dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T1.3.3.2.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib5" title="">5</a><span class="ltx_text" id="S3.T1.3.3.2.1.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.3.2.2" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.3.2.2.1" style="font-size:70%;">5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.3.2.3" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.3.2.3.1" style="font-size:70%;">N/A</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.3.2.4" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.3.2.4.1" style="font-size:70%;">4,666</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.3.2.5" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.3.2.5.1" style="font-size:70%;">20</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.3.2.6" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.3.2.6.1" style="font-size:70%;">N/A</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.3.2.7" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.3.2.7.1" style="font-size:70%;">6,117</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.3.2.8" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.3.2.8.1" style="font-size:70%;">N/A</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.3.2.9" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.3.2.9.1" style="font-size:70%;">N/A</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.4.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.3.4.3.1" style="padding-left:6.5pt;padding-right:6.5pt;">
<span class="ltx_text" id="S3.T1.3.4.3.1.1" style="font-size:70%;">Animal kingdom </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T1.3.4.3.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib29" title="">29</a><span class="ltx_text" id="S3.T1.3.4.3.1.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.4.3.2" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.4.3.2.1" style="font-size:70%;">850</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.4.3.3" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.4.3.3.1" style="font-size:70%;">6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.4.3.4" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.4.3.4.1" style="font-size:70%;">33,099</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.4.3.5" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.4.3.5.1" style="font-size:70%;">23</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.4.3.6" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.4.3.6.1" style="font-size:70%;">N/A</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.4.3.7" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.4.3.7.1" style="font-size:70%;">N/A</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.4.3.8" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.4.3.8.1" style="font-size:70%;">9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.4.3.9" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.4.3.9.1" style="font-size:70%;">N/A</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.5.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.3.5.4.1" style="padding-left:6.5pt;padding-right:6.5pt;">
<span class="ltx_text" id="S3.T1.3.5.4.1.1" style="font-size:70%;">AP-10k </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T1.3.5.4.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib49" title="">49</a><span class="ltx_text" id="S3.T1.3.5.4.1.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.5.4.2" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.5.4.2.1" style="font-size:70%;">54</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.5.4.3" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.5.4.3.1" style="font-size:70%;">23</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.5.4.4" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.5.4.4.1" style="font-size:70%;">10,015</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.5.4.5" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.5.4.5.1" style="font-size:70%;">17</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.5.4.6" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.5.4.6.1" style="font-size:70%;">N/A</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.5.4.7" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.5.4.7.1" style="font-size:70%;">13,028</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.5.4.8" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.5.4.8.1" style="font-size:70%;">N/A</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.5.4.9" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.5.4.9.1" style="font-size:70%;">N/A</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.6.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.3.6.5.1" style="padding-left:6.5pt;padding-right:6.5pt;">
<span class="ltx_text" id="S3.T1.3.6.5.1.1" style="font-size:70%;">Animal track </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T1.3.6.5.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib52" title="">52</a><span class="ltx_text" id="S3.T1.3.6.5.1.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.6.5.2" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.6.5.2.1" style="font-size:70%;">10</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.6.5.3" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.6.5.3.1" style="font-size:70%;">N/A</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.6.5.4" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.6.5.4.1" style="font-size:70%;">24,700</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.6.5.5" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.6.5.5.1" style="font-size:70%;">N/A</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.6.5.6" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.6.5.6.1" style="font-size:70%;">58</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.6.5.7" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.6.5.7.1" style="font-size:70%;">429,000</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.6.5.8" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.6.5.8.1" style="font-size:70%;">N/A</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.6.5.9" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.6.5.9.1" style="font-size:70%;">N/A</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.7.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.3.7.6.1" style="padding-left:6.5pt;padding-right:6.5pt;">
<span class="ltx_text" id="S3.T1.3.7.6.1.1" style="font-size:70%;">APT-36K </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T1.3.7.6.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib48" title="">48</a><span class="ltx_text" id="S3.T1.3.7.6.1.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.7.6.2" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.7.6.2.1" style="font-size:70%;">30</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.7.6.3" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.7.6.3.1" style="font-size:70%;">15</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.7.6.4" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.7.6.4.1" style="font-size:70%;">36,000</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.7.6.5" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.7.6.5.1" style="font-size:70%;">17</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.7.6.6" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.7.6.6.1" style="font-size:70%;">2,400</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.7.6.7" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.7.6.7.1" style="font-size:70%;">53,006</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.7.6.8" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.7.6.8.1" style="font-size:70%;">10</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.7.6.9" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.7.6.9.1" style="font-size:70%;">N/A</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.8.7">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S3.T1.3.8.7.1" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.8.7.1.1" style="font-size:70%;">APTv2</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T1.3.8.7.2" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.8.7.2.1" style="font-size:70%;">30</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T1.3.8.7.3" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.8.7.3.1" style="font-size:70%;">15</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T1.3.8.7.4" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.8.7.4.1" style="font-size:70%;">41,235</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T1.3.8.7.5" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.8.7.5.1" style="font-size:70%;">17</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T1.3.8.7.6" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.8.7.6.1" style="font-size:70%;">2,749</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T1.3.8.7.7" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.8.7.7.1" style="font-size:70%;">84,611</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T1.3.8.7.8" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.8.7.8.1" style="font-size:70%;">10</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T1.3.8.7.9" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S3.T1.3.8.7.9.1" style="font-size:70%;">Easy / Hard</span></td>
</tr>
</tbody>
</table>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Data Collection and Organization</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The aim of APTv2 is to establish a comprehensive, large-scale benchmark for animal pose estimation and tracking within real-world scenarios, an area largely untouched by prior work. In order to gather high-quality data that accurately represent natural living scenarios for animals, we turn to real-world video platforms, namely YouTube, for the careful selection and filtering of video clips. These clips, numbering in the hundreds, showcase 30 diverse animal species within a range of environments, from zoos and forests to deserts, which are typical habitats for the featured species. Additionally, to address potential imbalances due to the varied movement speeds of different animals and differing frame frequencies across the video clips, we meticulously set individual frame sampling rates for each video, leading to their tailored subsampling. This process ensures noticeable differences in movement and posture for each animal, thereby avoiding outlier cases where some animals may appear almost stationary over a given period. After the subsampling process, each video retains 15 frames. Importantly, our process ensures that challenging scenarios such as truncation and occlusion are retained in the dataset. This feature enables models to be evaluated under these challenging conditions, further enhancing the value and applicability of APTv2.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">After the video collection and cleaning stages, we further categorize the videos of 30 animal species into 15 families, following the taxonomy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib49" title="">49</a>]</cite>. Typically, animals from the same taxonomic rank exhibit similarities in their behavior patterns, anatomical keypoint distribution, and appearance. For instance, the gait of dogs and wolves is quite similar, given they both belong to the Canidae family. In contrast, the walking pattern of a zebra, which belongs to a different family, <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.1">i.e.</span>, Equidae, differs significantly from them. By adhering to the taxonomic rank, we create the potential to easily expand our dataset by collecting and annotating more videos from the same or different species or families. In addition, this structured organization of the animal pose dataset suggests a possible way to improve the generalization capabilities of animal pose estimation and tracking models for rare animal species. Specifically, this can be achieved by collecting and annotating videos from more commonly encountered animals of the same taxonomic rank.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">Data Annotation</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">To ensure high-quality annotations for each image in the APTv2 dataset, 18 thoroughly trained annotators participated in the annotation process. We manually split the collected data into easy and hard subsets based on the number of instances in each video<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>While estimating the pose of a single instance in a frame may sometimes prove difficult (<span class="ltx_text ltx_font_italic" id="footnote1.1">e.g.</span>, due to motion blur), detecting and tracking animal poses in multi-instance frames present even greater challenges due to occlusions, scale variation, and appearance ambiguity. Consequently, we adopt the number of instances per frame as a straightforward criterion for dividing the dataset into easy and hard subsets.</span></span></span>. The annotators are first instructed to make annotations based on the easy set, where there is only one instance in each video. To further enhance the annotation quality, we performed a stringent cross-check, which was repeated three times during the labeling process. After that, the annotators with better annotation skills participated in the annotation of the hard subsets with multiple instances per frame, followed by another stringent cross-check. The entire data collection, cleaning, annotation, and checking process consumed approximately 2,000 person-hours. Eventually, we labeled a total of 41,179 frames<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Note that there are only a few frames (specifically 56) without labels due to the absence of objects.</span></span></span>, adhering to the COCO labeling format, with 24,700 easy images and 16,479 hard frames. We annotated at most 17 keypoints for each animal instance, including two eyes, one nose, one neck, one tail, two shoulders, two elbows, two hips, two knees, and four paws, following the protocol established by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib49" title="">49</a>]</cite>. Besides the keypoint annotations, we labeled the background type for each frame from 10 classes, <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.1">i.e.</span>, grass, city, and forest. In addition, we assigned a unique tracking ID to each individual animal instance across the video clips. We split the dataset into three disjoint subsets for training, validation, and test, following a 7:1:2 ratio for each animal species. It is worth mentioning that we adopted a video-level partitioning approach to prevent potential information leakage, as frames within the same video clip bear a high degree of similarity.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.5.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.6.2">Statistics of the APTv2 Dataset</span>
</h3>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="742" id="S3.F1.g1" src="x1.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The taxonomic classification of animal families and their descendant species in our APTv2 dataset.</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="267" id="S3.F2.g1" src="extracted/5315651/stats_instance_number_per_species.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The number of instances per species in our APTv2 dataset.</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="385" id="S3.F3.g1" src="extracted/5315651/stats_number_of_instances_per_frame.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The total count of instances and frames with respect to the frequency of instances per frame in our APTv2 dataset.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">As demonstrated in Table <a class="ltx_ref" href="#S3.T1" title="TABLE I ‣ III Dataset ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_tag">I</span></a>, APTv2 covers 30 distinct animal species across 15 different families. It features a significant number of annotated frames with numerous annotated animal instances from a large collection of video clips. This comprehensive database surpasses prior animal pose estimation datasets, <span class="ltx_text ltx_font_italic" id="S3.SS3.p1.1.1">e.g.</span>, with much more instances than AP-10K dataset and much more species than Animal Pose and Animal Track dataset, thus establishing new challenges for tasks within the field. Besides, it is the first large-scale dataset that takes the animal pose track into consideration. Although the previous Animal Kingdom dataset covers both animal pose estimation and tracking, it treats the two tasks separately, which makes it not suitable for animal pose tracking tasks. To collect high-quality videos, the video clips used in APTv2 have been sourced from YouTube, spanning an array of topics such as documentaries, vlogs, and educational films, among others. Captured using different cameras and at varied shooting distances, these clips exhibit a diverse range of camera movement patterns. APTv2 incorporates images with ten different types of backgrounds, providing varied scenes for a holistic evaluation of animal pose estimation and tracking. Furthermore, we split our APTv2 dataset into easy and hard subsets according to the difficulty level, which is beneficial for systematically investigating the influence of using them for training and evaluation. As the first dataset suitable for both animal pose estimation and tracking, APTv2 fills an existing gap in this area and brings forth new challenges and opportunities for future research.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">We also show the statistics of APTv2 to gain more insights about this dataset. As shown in Figure <a class="ltx_ref" href="#S3.F2" title="Figure 2 ‣ III-C Statistics of the APTv2 Dataset ‣ III Dataset ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_tag">2</span></a>, APTv2 contains more hard instances per animal species compared with the easy counterpart, posing more challenges on the animal pose estimation and tracking tasks. Such observation can be further validated by observing Figure <a class="ltx_ref" href="#S3.F3" title="Figure 3 ‣ III-C Statistics of the APTv2 Dataset ‣ III Dataset ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_tag">3</span></a>, where APTv2 exhibits a long-tail distribution concerning the frequency of instances per frame, <span class="ltx_text ltx_font_italic" id="S3.SS3.p2.1.1">i.e.</span>, certain frames contain a high number of animal instances, reaching up to 53. Moreover, these multi-instance frames make up a substantial portion of the dataset, containing a wealth of challenging instances.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1" style="color:#000000;">ViTPoseTrack<span class="ltx_text" id="S4.1.1.1" style="color:black;"> Baseline</span></span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We introduce a simple baseline method <span class="ltx_text" id="S4.p1.1.1" style="color:#000000;">ViTPoseTrack</span> in this section and demonstrate its good performance and scalability of model size on the APTv2 dataset.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.5.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.6.2">Architecture Design</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">As shown in Figure <a class="ltx_ref" href="#S4.F4" title="Figure 4 ‣ IV-A1 Pose estimation head ‣ IV-A Architecture Design ‣ IV ViTPoseTrack Baseline ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_tag">4</span></a>, <span class="ltx_text" id="S4.SS1.p1.1.1" style="color:#000000;">ViTPoseTrack</span> employs a simple encoder-decoder structure for both the animal pose estimation and tracking task. Specifically, a plain vision transformer is adopted as the encoder for feature extraction given the input image. The extracted feature is then fed into the task-specific head for pose estimation and tracking.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS1.5.1.1">IV-A</span>1 </span>Pose estimation head</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">The pose estimation head takes the classic structure as in SimpleBaseline <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib41" title="">41</a>]</cite>, which contains several deconvolution layers for feature upsampling and one projection layer to get the heatmap for each keypoint. After that, the locations of the corresponding keypoints are determined by finding the maximum values from each heatmap.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="413" id="S4.F4.g1" src="x2.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Architecture of the proposed <span class="ltx_text" id="S4.F4.2.1" style="color:#000000;">ViTPoseTrack</span> baseline model. The backbone encoder is shared for both pose estimation and tracking tasks and is only trained on the pose estimation task. The fire (snow) symbol denotes the corresponding module is trainable (frozen). </figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS2.5.1.1">IV-A</span>2 </span>Tracking head</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">In <span class="ltx_text" id="S4.SS1.SSS2.p1.1.1" style="color:#000000;">ViTPoseTrack</span>, we follow the traditional tracking by matching pipeline. Specifically, the target object in the initial frame is utilized as the template for feature extraction. A larger search region, centered at the location of the tracked object in the preceding frame, is extracted from the current frame. The features of this search region are also extracted using the backbone network. Subsequently, these features of the template and the search region are concatenated and sent into an encoder for further feature processing. This encoder employs several transformer encoder layers. Based on the historical tracking trajectory, a set of additional motion tokens are generated. The processed features and these motion tokens are then input into a decoder to produce the prediction for the tracking results. This decoder is made up of several cross-attention layers, in which the tokens corresponding to the searched regions act as queries. The features from the template regions, search regions, and motion information serve as keys and values for the cross-attention layers. After being processed by the decoder, the search region tokens are used to predict the tracking results through a simple projection head.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.5.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.6.2">Training Schedule</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">In the training process, we first train the ViT encoder integrated with the pose estimation head end-to-end on the proposed APTv2 dataset. Following this, we freeze the ViT encoder and append the tracking head to it. In this setup, only the parameters of the tracking head are learnable during training on the tracking task. Both the box regression loss and the Intersection over Union (IoU) loss are utilized during the training phase of the tracking head, adhering to the design principles outlined in SwinTrack <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib23" title="">23</a>]</cite>. This simplified design approach allows the backbone encoder to be shared across the two tasks, resulting in efficient memory footprint and inference speed. Experimental results validate that the features from the pose estimation task are distinctive enough to be successfully used in the tracking task.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Experiment</span>
</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.5.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.6.2">Representative Methods for Benchmarking</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">We select several representative methods for the benchmark on the APTv2 dataset. We will briefly introduce them here.</p>
</div>
<section class="ltx_subsubsection" id="S5.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S5.SS1.SSS1.5.1.1">V-A</span>1 </span>Pose estimation methods</h4>
<div class="ltx_para" id="S5.SS1.SSS1.p1">
<p class="ltx_p" id="S5.SS1.SSS1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS1.p1.1.1">SimpleBaseline</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib41" title="">41</a>]</cite> is one of the representative CNN-based methods for pose estimation tasks. It follows the top-down pipeline and utilizes a ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib13" title="">13</a>]</cite> model as the backbone encoder for feature extraction. After that, three deconvolution layers are utilized for feature upsampling by 8 times and one projection layer is utilized to estimate the heatmap for each keypoint. <span class="ltx_text ltx_font_bold" id="S5.SS1.SSS1.p1.1.2">HRNet</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib34" title="">34</a>]</cite> argues that low-resolution features are harmful to obtain the precise location of the estimated keypoints. To overcome such limitations, it utilizes a highway structure to process high-resolution and low-resolution features simultaneously and gradually fuses them. The fused features are then directly fed into one projection layer to get the heatmap for each keypoint. <span class="ltx_text ltx_font_bold" id="S5.SS1.SSS1.p1.1.3">HRFormer</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib50" title="">50</a>]</cite> is one representative transformer-based method, whose design is similar to that of HRNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib34" title="">34</a>]</cite>. Specifically, it jointly processes high- and low-resolution features and utilizes window-based attention to reduce memory consumption. The high- and low-resolution features are gradually fused at each transformer stage. The high-resolution features are then processed by one projection layer to get the estimated heatmap. <span class="ltx_text ltx_font_bold" id="S5.SS1.SSS1.p1.1.4">ViTPose</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib44" title="">44</a>]</cite> focuses on estimating the appropriate keypoints with a plain vision transformer backbone. It utilizes the plain vision transformer for feature extraction and either a classic decoder or a simple decoder for upsampling feature maps and regressing the heatmaps. Due to its simple design, it is easy to benefit from the scalability of vision transformers, delivering superior results with efficient memory usage and fast execution.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S5.SS1.SSS2.5.1.1">V-A</span>2 </span>Tracking methods</h4>
<div class="ltx_para" id="S5.SS1.SSS2.p1">
<p class="ltx_p" id="S5.SS1.SSS2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS2.p1.1.1">SiamRPN++<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_medium" id="S5.SS1.SSS2.p1.1.1.1.1">[</span><a class="ltx_ref" href="#bib.bib17" title="">17</a><span class="ltx_text ltx_font_medium" id="S5.SS1.SSS2.p1.1.1.2.2">]</span></cite></span> utilizes ResNet<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib13" title="">13</a>]</cite> as the backbone encoder for feature extraction. A Siamese structure is utilized with one branch for the template feature extraction and one branch to extract the feature of the search regions. Hierarchical features are utilized in the framework to gradually regress the location of the target object in incoming frames. <span class="ltx_text ltx_font_bold" id="S5.SS1.SSS2.p1.1.2">STARK</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib46" title="">46</a>]</cite> utilizes a transformer encoder-decoder structure for tracking. It first utilizes ResNet for feature extraction of the template and the search regions. The extracted features are then concatenated and fed into a transformer encoder for feature processing, where the processed features are then served as the key and values of the cross attention in the following transformer decoder. An extra track query is also fed into the transformer decoder to serve as the query and utilized to decode the location of the target in the incoming frames. <span class="ltx_text ltx_font_bold" id="S5.SS1.SSS2.p1.1.3">SwinTrack</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib23" title="">23</a>]</cite> gets rid of the ResNet for feature extraction and directly utilizes pure transformer structure for tracking. Specifically, it employs Swin Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib25" title="">25</a>]</cite> for feature extraction of the template and search regions, respectively. The trajectory of the target object is also encoded into a motion token, which is then concatenated with the template and search region features to serve as the key and values of the cross-attention layer in the decoders. Different from STARK which needs extra track queries, SwinTrack directly regresses the location of the target based on the features from the search regions. <span class="ltx_text ltx_font_bold" id="S5.SS1.SSS2.p1.1.4">ViTTrack</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib48" title="">48</a>]</cite> refers to the tracking method that replaces the backbone of SwinTrack <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib23" title="">23</a>]</cite>, originally based on Swin Transformer, with ViT-Base <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib9" title="">9</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS2.5.1.1">V-B</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS2.6.2">Evaluation Metrics</span>
</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.2">We adopt the average precision (AP) as the primary evaluation metric on the proposed APTv2 dataset. The average precision is defined according to the object keypoint similarity (OKS), for example, the loose metric AP<math alttext="{}_{50}" class="ltx_Math" display="inline" id="S5.SS2.p1.1.m1.1"><semantics id="S5.SS2.p1.1.m1.1a"><msub id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml"><mi id="S5.SS2.p1.1.m1.1.1a" xref="S5.SS2.p1.1.m1.1.1.cmml"></mi><mn id="S5.SS2.p1.1.m1.1.1.1" xref="S5.SS2.p1.1.m1.1.1.1.cmml">50</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><apply id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1"><cn id="S5.SS2.p1.1.m1.1.1.1.cmml" type="integer" xref="S5.SS2.p1.1.m1.1.1.1">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">{}_{50}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.1.m1.1d">start_FLOATSUBSCRIPT 50 end_FLOATSUBSCRIPT</annotation></semantics></math> and the strict metric AP<math alttext="{}_{75}" class="ltx_Math" display="inline" id="S5.SS2.p1.2.m2.1"><semantics id="S5.SS2.p1.2.m2.1a"><msub id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml"><mi id="S5.SS2.p1.2.m2.1.1a" xref="S5.SS2.p1.2.m2.1.1.cmml"></mi><mn id="S5.SS2.p1.2.m2.1.1.1" xref="S5.SS2.p1.2.m2.1.1.1.cmml">75</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.1b"><apply id="S5.SS2.p1.2.m2.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1"><cn id="S5.SS2.p1.2.m2.1.1.1.cmml" type="integer" xref="S5.SS2.p1.2.m2.1.1.1">75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.1c">{}_{75}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.2.m2.1d">start_FLOATSUBSCRIPT 75 end_FLOATSUBSCRIPT</annotation></semantics></math> are calculated by setting the OKS threshold to 0.5 and 0.75, respectively. AP is defined as the average value of the precision with a series of thresholds from 0.5 to 0.95 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib24" title="">24</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS3.5.1.1">V-C</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS3.6.2">Implementation Details</span>
</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">To ensure a comprehensive evaluation for animal pose estimation and tracking, we benchmark various representative CNN-based and ViT-based pose estimation methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib34" title="">34</a>, <a class="ltx_ref" href="#bib.bib41" title="">41</a>, <a class="ltx_ref" href="#bib.bib44" title="">44</a>, <a class="ltx_ref" href="#bib.bib50" title="">50</a>]</cite> on the newly proposed APTv2 dataset. Representative tracking methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib46" title="">46</a>, <a class="ltx_ref" href="#bib.bib17" title="">17</a>, <a class="ltx_ref" href="#bib.bib23" title="">23</a>]</cite> and the proposed baseline method <span class="ltx_text" id="S5.SS3.p1.1.1" style="color:#000000;">ViTPoseTrack</span> are employed to obtain tracked boxes for the animal instances in the video clips. Based on APTv2, we establish three tracks: the SF track, LT track, and APT track, as outlined in Sec. <a class="ltx_ref" href="#S1" title="I Introduction ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_tag">I</span></a>. The pose estimation models are implemented using the MMPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib7" title="">7</a>]</cite> codebase and are trained for 210 epochs, aligning with common practices in human/animal pose estimation tasks. The initial learning rate is set to 5e-4 and is decreased by a factor of 10 at the 170th and 200th epochs. For the training of the CNN-based methods, the Adam optimizer is utilized, and for the transformer-based methods, the AdamW optimizer is used. In the SF and LT tracks, we utilize ground truth bounding boxes, while in the APT track, we employ tracked boxes to evaluate the performance of the pose estimation methods, respectively. During the training of the baseline method <span class="ltx_text" id="S5.SS3.p1.1.2" style="color:#000000;">ViTPoseTrack</span>, we freeze the backbone encoder, and the tracking head is fine-tuned for 300 epochs using the AdamW optimizer, with a learning rate of 5e-4 and weight decay of 1e-4.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS4.5.1.1">V-D</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS4.6.2">Single-Frame Animal Pose Estimation (SF track)</span>
</h3>
<section class="ltx_subsubsection" id="S5.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S5.SS4.SSS1.5.1.1">V-D</span>1 </span>Setting</h4>
<div class="ltx_para" id="S5.SS4.SSS1.p1">
<p class="ltx_p" id="S5.SS4.SSS1.p1.1">In the SF track, we benchmark representative CNN-based and ViT-based pose estimation methods, such as SimpleBaseline <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib41" title="">41</a>]</cite>, HRNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib34" title="">34</a>]</cite>, HRFormer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib50" title="">50</a>]</cite>, and ViTPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib44" title="">44</a>]</cite>. In addition to examining the impact of different backbone networks, we also delve into the benefits of various pre-training datasets, given the importance of pre-training in modern deep learning methods. We set up three settings to evaluate their performance, using network weights pre-trained on three different datasets: ImageNet-1K <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib8" title="">8</a>]</cite>, MS COCO human pose estimation dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib24" title="">24</a>]</cite>, and the AP-10K dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib49" title="">49</a>]</cite>, respectively.</p>
</div>
<div class="ltx_para" id="S5.SS4.SSS1.p2">
<p class="ltx_p" id="S5.SS4.SSS1.p2.1">For the ImageNet-1K pre-training, as is common practice, we initialize the backbones of pose estimation models with weights pre-trained on this dataset. We then fine-tune these models for additional 210 epochs on the APTv2 training set. It should be noted that we employ a fully supervised learning scheme on ImageNet-1K to acquire the pre-trained weights for the backbones used by SimpleBaseline, HRNet, and HRFormer. The ViT backbones in ViTPose are initialized with weights by self-supervised MAE pre-training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib12" title="">12</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS4.SSS1.p3">
<p class="ltx_p" id="S5.SS4.SSS1.p3.1">To evaluate the transferability of human pose estimation datasets to animal pose estimation tasks, we utilize the MS COCO dataset for pre-training. This decision is based on the idea that the keypoint definitions of quadrupeds are similar to those of humans. This approach allows us to leverage the existing large-scale datasets for human pose estimation to bridge the domain gap between the object-centric images in ImageNet-1K and animal images in APTv2. Accordingly, we pre-train both the CNN-based and ViT-based models on the MS COCO human pose estimation dataset for 210 epochs, and use the weights to initialize the backbone models for subsequent training on APTv2.
</p>
</div>
<div class="ltx_para" id="S5.SS4.SSS1.p4">
<p class="ltx_p" id="S5.SS4.SSS1.p4.1">Lastly, to investigate the transferability between different animal pose estimation datasets, we adopt models pre-trained with abundant animal pose images and annotations from the AP-10K dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib49" title="">49</a>]</cite>. Specifically, we train these backbone models on the animal dataset for 210 epochs.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Results (AP) of different models on the SF track of APTv2 with ImageNet-1K (IN1K) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib8" title="">8</a>]</cite>, MS COCO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib24" title="">24</a>]</cite>, and AP-10K <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib49" title="">49</a>]</cite> pre-training, respectively. All, easy, and hard denote the entire validation set, and its easy and hard subsets, respectively.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T2.12">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.12.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T2.12.1.1.1" style="padding-left:6.5pt;padding-right:6.5pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T2.12.1.1.2" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.1.1.2.1" style="font-size:70%;">Pre-training</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2" id="S5.T2.12.1.1.3" style="padding-left:6.5pt;padding-right:6.5pt;">
<span class="ltx_text" id="S5.T2.12.1.1.3.1" style="font-size:70%;">SimpleBaseline </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T2.12.1.1.3.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib41" title="">41</a><span class="ltx_text" id="S5.T2.12.1.1.3.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S5.T2.12.1.1.3.4" style="font-size:70%;"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2" id="S5.T2.12.1.1.4" style="padding-left:6.5pt;padding-right:6.5pt;">
<span class="ltx_text" id="S5.T2.12.1.1.4.1" style="font-size:70%;">HRNet </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T2.12.1.1.4.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib34" title="">34</a><span class="ltx_text" id="S5.T2.12.1.1.4.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S5.T2.12.1.1.4.4" style="font-size:70%;"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2" id="S5.T2.12.1.1.5" style="padding-left:6.5pt;padding-right:6.5pt;">
<span class="ltx_text" id="S5.T2.12.1.1.5.1" style="font-size:70%;">HRFormer </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T2.12.1.1.5.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib50" title="">50</a><span class="ltx_text" id="S5.T2.12.1.1.5.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S5.T2.12.1.1.5.4" style="font-size:70%;"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.12.1.1.6" style="padding-left:6.5pt;padding-right:6.5pt;">
<span class="ltx_text" id="S5.T2.12.1.1.6.1" style="font-size:70%;">ViTPose </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T2.12.1.1.6.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib44" title="">44</a><span class="ltx_text" id="S5.T2.12.1.1.6.3.2" style="font-size:70%;">]</span></cite>
</th>
</tr>
<tr class="ltx_tr" id="S5.T2.12.2.2">
<th class="ltx_td ltx_th ltx_th_column ltx_border_r" id="S5.T2.12.2.2.1" style="padding-left:6.5pt;padding-right:6.5pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T2.12.2.2.2" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.2.2.2.1" style="font-size:70%;">dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T2.12.2.2.3" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.2.2.3.1" style="font-size:70%;">ResNet-50</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T2.12.2.2.4" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.2.2.4.1" style="font-size:70%;">ResNet-101</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T2.12.2.2.5" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.2.2.5.1" style="font-size:70%;">HRNet-w32</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T2.12.2.2.6" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.2.2.6.1" style="font-size:70%;">HRNet-w48</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T2.12.2.2.7" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.2.2.7.1" style="font-size:70%;">HRFormer-S</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T2.12.2.2.8" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.2.2.8.1" style="font-size:70%;">HRFormer-B</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T2.12.2.2.9" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.2.2.9.1" style="font-size:70%;">VITPose-B</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.12.3.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.12.3.1.1" rowspan="3" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.3.1.1.1" style="font-size:70%;">All</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.12.3.1.2" style="padding-left:6.5pt;padding-right:6.5pt;">
<span class="ltx_text" id="S5.T2.12.3.1.2.1" style="font-size:70%;">IN1k </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T2.12.3.1.2.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib8" title="">8</a><span class="ltx_text" id="S5.T2.12.3.1.2.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.12.3.1.3" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.3.1.3.1" style="font-size:70%;">64.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.12.3.1.4" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.3.1.4.1" style="font-size:70%;">65.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.12.3.1.5" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.3.1.5.1" style="font-size:70%;">68.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.12.3.1.6" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.3.1.6.1" style="font-size:70%;">70.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.12.3.1.7" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.3.1.7.1" style="font-size:70%;">67.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.12.3.1.8" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.3.1.8.1" style="font-size:70%;">69.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.12.3.1.9" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.3.1.9.1" style="font-size:70%;">72.4</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.12.4.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.12.4.2.1" style="padding-left:6.5pt;padding-right:6.5pt;">
<span class="ltx_text" id="S5.T2.12.4.2.1.1" style="font-size:70%;">AP-10K </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T2.12.4.2.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib49" title="">49</a><span class="ltx_text" id="S5.T2.12.4.2.1.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.12.4.2.2" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.4.2.2.1" style="font-size:70%;">66.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.12.4.2.3" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.4.2.3.1" style="font-size:70%;">64.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.12.4.2.4" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.4.2.4.1" style="font-size:70%;">69.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.12.4.2.5" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.4.2.5.1" style="font-size:70%;">71.2</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.12.4.2.6" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.4.2.6.1" style="font-size:70%;">67.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.12.4.2.7" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.4.2.7.1" style="font-size:70%;">69.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.12.4.2.8" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.4.2.8.1" style="font-size:70%;">72.9</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.12.5.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.12.5.3.1" style="padding-left:6.5pt;padding-right:6.5pt;">
<span class="ltx_text" id="S5.T2.12.5.3.1.1" style="font-size:70%;">COCO </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T2.12.5.3.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib24" title="">24</a><span class="ltx_text" id="S5.T2.12.5.3.1.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.12.5.3.2" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.5.3.2.1" style="font-size:70%;">67.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.12.5.3.3" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.5.3.3.1" style="font-size:70%;">68.1</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.12.5.3.4" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.5.3.4.1" style="font-size:70%;">70.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.12.5.3.5" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.5.3.5.1" style="font-size:70%;">71.7</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.12.5.3.6" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.5.3.6.1" style="font-size:70%;">69.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.12.5.3.7" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.5.3.7.1" style="font-size:70%;">69.7</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.12.5.3.8" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.5.3.8.1" style="font-size:70%;">72.5</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.12.6.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.12.6.4.1" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.6.4.1.1" style="font-size:70%;">Easy</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.12.6.4.2" style="padding-left:6.5pt;padding-right:6.5pt;">
<span class="ltx_text" id="S5.T2.12.6.4.2.1" style="font-size:70%;">COCO </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T2.12.6.4.2.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib24" title="">24</a><span class="ltx_text" id="S5.T2.12.6.4.2.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.12.6.4.3" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.6.4.3.1" style="font-size:70%;">75.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.12.6.4.4" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.6.4.4.1" style="font-size:70%;">75.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.12.6.4.5" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.6.4.5.1" style="font-size:70%;">78.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.12.6.4.6" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.6.4.6.1" style="font-size:70%;">80.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.12.6.4.7" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.6.4.7.1" style="font-size:70%;">77.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.12.6.4.8" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.6.4.8.1" style="font-size:70%;">77.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.12.6.4.9" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.6.4.9.1" style="font-size:70%;">79.7</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.12.7.5">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T2.12.7.5.1" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.7.5.1.1" style="font-size:70%;">Hard</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T2.12.7.5.2" style="padding-left:6.5pt;padding-right:6.5pt;">
<span class="ltx_text" id="S5.T2.12.7.5.2.1" style="font-size:70%;">COCO </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T2.12.7.5.2.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib24" title="">24</a><span class="ltx_text" id="S5.T2.12.7.5.2.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T2.12.7.5.3" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.7.5.3.1" style="font-size:70%;">65.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T2.12.7.5.4" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.7.5.4.1" style="font-size:70%;">65.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T2.12.7.5.5" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.7.5.5.1" style="font-size:70%;">67.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T2.12.7.5.6" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.7.5.6.1" style="font-size:70%;">69.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T2.12.7.5.7" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.7.5.7.1" style="font-size:70%;">67.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T2.12.7.5.8" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.7.5.8.1" style="font-size:70%;">67.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T2.12.7.5.9" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="S5.T2.12.7.5.9.1" style="font-size:70%;">70.4</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Results (AP) on the APTv2 validation set with larger models. All, Easy, and Hard have the same meaning as in Table <a class="ltx_ref" href="#S5.T2" title="TABLE II ‣ V-D1 Setting ‣ V-D Single-Frame Animal Pose Estimation (SF track) ‣ V Experiment ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_tag">II</span></a>.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.1.2">All</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.1.3">Easy</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.1.4">Hard</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.1.2.1.1">ViTPose-B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib44" title="">44</a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.2.1.2">72.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.2.1.3">79.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.2.1.4">70.4</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S5.T3.1.3.2.1">ViTPose-L <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib44" title="">44</a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.1.3.2.2">75.0</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.1.3.2.3">82.1</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.1.3.2.4">72.9</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="S5.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S5.SS4.SSS2.5.1.1">V-D</span>2 </span>Results and analysis</h4>
<div class="ltx_para" id="S5.SS4.SSS2.p1">
<p class="ltx_p" id="S5.SS4.SSS2.p1.1">The results are summarized in Table <a class="ltx_ref" href="#S5.T2" title="TABLE II ‣ V-D1 Setting ‣ V-D Single-Frame Animal Pose Estimation (SF track) ‣ V Experiment ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_tag">II</span></a>, where we report the performances of representative methods on the easy subset, hard subset, and the entire validation set, respectively. A significant observation is the marked performance enhancement offered by the parallel-resolution design, which is evident in the comparison between the performances of HRNet and SimpleBaseline. Through the use of attention layers and suitable pre-training, transformer-based models can match performance levels akin to those without a parallel-resolution structure. For instance, the ViTPose base model achieves an Average Precision (AP) of 72.4, whereas the HRFormer base model, with ImageNet-1K pretraining, achieves an AP of 69.0.</p>
</div>
<div class="ltx_para" id="S5.SS4.SSS2.p2">
<p class="ltx_p" id="S5.SS4.SSS2.p2.1">The study further delves into the impact of various pre-training datasets. Notably, both CNN-based and ViT-based methods show performance improvements with human pose pre-training. The performance enhancements are demonstrated by an increase in AP from 64.1 to 67.8 for SimpleBaseline with a ResNet-50 backbone network, from 70.1 to 71.7 for HRNet-w48, and from 69.0 to 69.7 for HRFormer-B.</p>
</div>
<div class="ltx_para" id="S5.SS4.SSS2.p3">
<p class="ltx_p" id="S5.SS4.SSS2.p3.1">Interestingly, while the AP-10K pre-training results in smaller gains compared to MS COCO pre-training, it still significantly surpasses ImageNet pre-training. Despite the AP-10K dataset’s diversity being substantially less than the MS COCO dataset, it suggests that leveraging animal pose datasets enhances the transfer performance of pose estimation methods on the proposed APTv2 dataset owing to the small domain gap. Furthermore, the scale of the pre-training dataset significantly impacts the performance of transfer learning. A larger pre-training dataset aids the baseline model in acquiring more discriminative features suitable for pose estimation tasks. Larger models can extract more valuable information from limited data, as further evidenced by the minimal performance gap between HRFormer base with AP-10K pre-training and MS COCO pre-training at 0.1 AP.</p>
</div>
<div class="ltx_para" id="S5.SS4.SSS2.p4">
<p class="ltx_p" id="S5.SS4.SSS2.p4.1">This observation is particularly noticeable in the comparison of MS COCO and AP-10K pre-training using the ViTPose base model, which achieved better results with AP-10K pre-training, <span class="ltx_text ltx_font_italic" id="S5.SS4.SSS2.p4.1.1">e.g.</span>, 72.9 AP versus 72.5 AP. These results underscore the value of in-domain animal pose estimation datasets in pre-training and the extensive generalization capability of large-scale vision transformers.</p>
</div>
<div class="ltx_para" id="S5.SS4.SSS2.p5">
<p class="ltx_p" id="S5.SS4.SSS2.p5.1">Moreover, when compared to the easy validation set, the pose estimation models show lower performance on the hard subset. For example, AP results drop to 65.9 from 75.5 with ResNet-101, to 69.4 from 80.1 with HRNet-w48, to 67.4 from 77.8 with HRFormer base, and to 70.4 from 79.7 with ViTPose base. This underlines the need for improvements in current pose estimation methods, as most excel only in simple cases and struggle with complex scenarios involving occlusion, truncation, scale variation, appearance ambiguity, blur, etc. This insight emphasizes the challenge posed by the APTv2 dataset as a benchmark for current pose estimation methods.</p>
</div>
<div class="ltx_para" id="S5.SS4.SSS2.p6">
<p class="ltx_p" id="S5.SS4.SSS2.p6.1">To further examine the scalability of large vision transformers, we experimented with a larger ViTPose model with the ViT-Large backbone, pre-trained it on MS COCO, and fine-tuned it for 210 epochs on the APTv2 dataset. Despite this model having over 300M parameters, its training could be efficiently performed on low-end GPUs such as NVIDIA GTX 1080Ti and 3090, due to the efficient implementation of plain vision transformers. As seen in Table <a class="ltx_ref" href="#S5.T3" title="TABLE III ‣ V-D1 Setting ‣ V-D Single-Frame Animal Pose Estimation (SF track) ‣ V Experiment ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_tag">III</span></a>, as the model size increases, the performance of ViTPose consistently improves. For instance, ViTPose-L achieves a gain of 2.5 AP over ViTPose-B, <span class="ltx_text ltx_font_italic" id="S5.SS4.SSS2.p6.1.1">i.e.</span>, 75.0 AP versus 72.5 AP.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS5.5.1.1">V-E</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS5.6.2">Low-data Training and Generalization (LT track)</span>
</h3>
<section class="ltx_subsubsection" id="S5.SS5.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S5.SS5.SSS1.5.1.1">V-E</span>1 </span>Setting</h4>
<div class="ltx_para" id="S5.SS5.SSS1.p1">
<p class="ltx_p" id="S5.SS5.SSS1.p1.1">Collecting and annotating a substantial amount of images for each animal species, especially those that are endangered, represents a significant challenge. Consequently, evaluating the generalization capabilities of pose estimation methods becomes of paramount importance, particularly when the available training data is scarce. Considering the biological relationships among various animal species, we examine both the inter- and intra-species generalization abilities of the pose estimation models. Our evaluation encompasses two distinct settings. <span class="ltx_text ltx_font_bold" id="S5.SS5.SSS1.p1.1.1">1) Leave one out:</span> In this setting, we exclude all training samples that belong to a particular animal family from the training set. The models are then trained for 210 epochs using this refined training set and evaluated on the validation subsets split according to the selected families. Specifically, we have selected six animal families for this setting, <span class="ltx_text ltx_font_italic" id="S5.SS5.SSS1.p1.1.2">i.e.</span>, Canidae, Felidae, Hominidae, Cercopithecidae, Ursidae, and Bovidae. We choose HRNet-w32 with MS COCO pre-training as the default model in this setting. <span class="ltx_text ltx_font_bold" id="S5.SS5.SSS1.p1.1.3">2) Low-data fine-tuning:</span> To account for the potential feasibility of annotating a limited number of instances for endangered animal species, we introduce a low-data fine-tuning setting to further assess the generalization capability of these pose estimation methods. From the proposed APTv2 dataset, we randomly select a specific number of images (<span class="ltx_text ltx_font_italic" id="S5.SS5.SSS1.p1.1.4">e.g.</span>, 20, 40, 60, and 80) per animal species belonging to the above-selected families and use them collectively as the training set and adopt the same validation subsets as the above setting. The models are initialized using weights from models pre-trained on the AP-10K dataset. We choose HRNet-w32, ViTPose-B, and ViTPose-L in this setting.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS5.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S5.SS5.SSS2.5.1.1">V-E</span>2 </span>Results and analysis</h4>
<div class="ltx_para" id="S5.SS5.SSS2.p1">
<p class="ltx_p" id="S5.SS5.SSS2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS5.SSS2.p1.1.1">1) Leave one out:</span> The results on the “leave one out” setting are summarized in Table <a class="ltx_ref" href="#S5.T4" title="TABLE IV ‣ V-E2 Results and analysis ‣ V-E Low-data Training and Generalization (LT track) ‣ V Experiment ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_tag">IV</span></a>. We find that the models demonstrate respectable generalization ability for the Canidae, Felidae, and Bovidae families when trained with instances from other animal families. For instance, a notable AP score of 66.8 is achieved for the Canidae family, as shown in the top-left cell of Table <a class="ltx_ref" href="#S5.T4" title="TABLE IV ‣ V-E2 Results and analysis ‣ V-E Low-data Training and Generalization (LT track) ‣ V Experiment ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_tag">IV</span></a>, although the model has never seen the training data belonging to the Canidae family. This strong performance can be attributed to the shared features among Canidae, Ursidae, and Felidae families, as they all belong to the Carnivora order. This result underscores the potential advantage of training with similar species to mitigate the lack of specific animal species in the dataset.</p>
</div>
<div class="ltx_para" id="S5.SS5.SSS2.p2">
<p class="ltx_p" id="S5.SS5.SSS2.p2.1">However, for less common species that share fewer common features with those in the training set, the models exhibit weaker generalization ability. For example, the model trained without data from the Cercopithecidae family achieves an AP of just 45.0 for that same family. This underperformance can be mitigated by including relevant training data, leading to an average precision of 66.6 AP across all settings and a significant improvement exceeding 21 AP.</p>
</div>
<figure class="ltx_table" id="S5.T4">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Results (AP) of HRNet-w32 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib34" title="">34</a>]</cite> models on the “Leave one out” setting of APTv2. The performance on the unseen category is denoted with gray color and the others are seen categories. The Average (seen) score is calculated as the mean AP of seen categories. Fully supervised denotes that the model is trained with the whole training set as in the SF track. The performance gap between the mean AP on the seen categories and the AP of the unseen categories is denoted with <math alttext="\Delta" class="ltx_Math" display="inline" id="S5.T4.2.m1.1"><semantics id="S5.T4.2.m1.1b"><mi id="S5.T4.2.m1.1.1" mathvariant="normal" xref="S5.T4.2.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S5.T4.2.m1.1c"><ci id="S5.T4.2.m1.1.1.cmml" xref="S5.T4.2.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.m1.1d">\Delta</annotation><annotation encoding="application/x-llamapun" id="S5.T4.2.m1.1e">roman_Δ</annotation></semantics></math>.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T4.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.3.1">
<th class="ltx_td ltx_nopad ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S5.T4.3.1.1" style="padding-left:2.6pt;padding-right:2.6pt;"><svg height="14.31" overflow="visible" version="1.1" width="66.78"><g transform="translate(0,14.31) scale(1,-1)"><path d="M 0,14.31 66.78,0" stroke="black" stroke-width="0.4"></path><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,8.35) scale(1, -1)"><foreignobject height="8.35" overflow="visible" width="33.39">
<span class="ltx_inline-block" id="S5.T4.3.1.1.pic1.1.1">
<span class="ltx_inline-block ltx_align_left" id="S5.T4.3.1.1.pic1.1.1.1">
<span class="ltx_p" id="S5.T4.3.1.1.pic1.1.1.1.1"><span class="ltx_text" id="S5.T4.3.1.1.pic1.1.1.1.1.1" style="font-size:70%;">training</span></span>
</span>
</span></foreignobject></g></g><g class="ltx_svg_fog" transform="translate(51.12,8.35)"><g transform="translate(0,5.96) scale(1, -1)"><foreignobject height="5.96" overflow="visible" width="15.66">
<span class="ltx_inline-block" id="S5.T4.3.1.1.pic1.2.1">
<span class="ltx_inline-block ltx_align_right" id="S5.T4.3.1.1.pic1.2.1.1">
<span class="ltx_p" id="S5.T4.3.1.1.pic1.2.1.1.1"><span class="ltx_text" id="S5.T4.3.1.1.pic1.2.1.1.1.1" style="font-size:70%;">test</span></span>
</span>
</span></foreignobject></g></g></g></svg></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.3.1.2" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.3.1.2.1" style="font-size:70%;">Canidae</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.3.1.3" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.3.1.3.1" style="font-size:70%;">Felidae</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.3.1.4" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.3.1.4.1" style="font-size:70%;">Hominidae</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.3.1.5" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.3.1.5.1" style="font-size:70%;">Cercopithecidae</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.3.1.6" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.3.1.6.1" style="font-size:70%;">Ursidae</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.3.1.7" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.3.1.7.1" style="font-size:70%;">Bovidae</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.4.3.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T4.4.3.1.1" style="padding-left:2.6pt;padding-right:2.6pt;">
<span class="ltx_text ltx_font_italic" id="S5.T4.4.3.1.1.1" style="font-size:70%;">w/o</span><span class="ltx_text" id="S5.T4.4.3.1.1.2" style="font-size:70%;"> Canidae</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.3.1.2" style="background-color:#D8D8D8;padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.3.1.2.1" style="font-size:70%;background-color:#D8D8D8;">66.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.3.1.3" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.3.1.3.1" style="font-size:70%;">77.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.3.1.4" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.3.1.4.1" style="font-size:70%;">68.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.3.1.5" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.3.1.5.1" style="font-size:70%;">66.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.3.1.6" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.3.1.6.1" style="font-size:70%;">75.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.3.1.7" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.3.1.7.1" style="font-size:70%;">68.3</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.4.4.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T4.4.4.2.1" style="padding-left:2.6pt;padding-right:2.6pt;">
<span class="ltx_text ltx_font_italic" id="S5.T4.4.4.2.1.1" style="font-size:70%;">w/o</span><span class="ltx_text" id="S5.T4.4.4.2.1.2" style="font-size:70%;"> Felidae</span>
</th>
<td class="ltx_td ltx_align_center" id="S5.T4.4.4.2.2" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.4.2.2.1" style="font-size:70%;">74.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.4.2.3" style="background-color:#D8D8D8;padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.4.2.3.1" style="font-size:70%;background-color:#D8D8D8;">68.1</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.4.2.4" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.4.2.4.1" style="font-size:70%;">68.9</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.4.2.5" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.4.2.5.1" style="font-size:70%;">67.8</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.4.2.6" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.4.2.6.1" style="font-size:70%;">76.1</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.4.2.7" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.4.2.7.1" style="font-size:70%;">66.8</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.4.5.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T4.4.5.3.1" style="padding-left:2.6pt;padding-right:2.6pt;">
<span class="ltx_text ltx_font_italic" id="S5.T4.4.5.3.1.1" style="font-size:70%;">w/o</span><span class="ltx_text" id="S5.T4.4.5.3.1.2" style="font-size:70%;"> Hominidae</span>
</th>
<td class="ltx_td ltx_align_center" id="S5.T4.4.5.3.2" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.5.3.2.1" style="font-size:70%;">74.2</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.5.3.3" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.5.3.3.1" style="font-size:70%;">77.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.5.3.4" style="background-color:#D8D8D8;padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.5.3.4.1" style="font-size:70%;background-color:#D8D8D8;">49.7</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.5.3.5" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.5.3.5.1" style="font-size:70%;">65.5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.5.3.6" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.5.3.6.1" style="font-size:70%;">75.9</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.5.3.7" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.5.3.7.1" style="font-size:70%;">67.4</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.4.6.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T4.4.6.4.1" style="padding-left:2.6pt;padding-right:2.6pt;">
<span class="ltx_text ltx_font_italic" id="S5.T4.4.6.4.1.1" style="font-size:70%;">w/o</span><span class="ltx_text" id="S5.T4.4.6.4.1.2" style="font-size:70%;"> Cercopithecidae</span>
</th>
<td class="ltx_td ltx_align_center" id="S5.T4.4.6.4.2" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.6.4.2.1" style="font-size:70%;">75.2</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.6.4.3" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.6.4.3.1" style="font-size:70%;">79</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.6.4.4" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.6.4.4.1" style="font-size:70%;">68.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.6.4.5" style="background-color:#D8D8D8;padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.6.4.5.1" style="font-size:70%;background-color:#D8D8D8;">45.0</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.6.4.6" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.6.4.6.1" style="font-size:70%;">76.2</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.6.4.7" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.6.4.7.1" style="font-size:70%;">67.7</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.4.7.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T4.4.7.5.1" style="padding-left:2.6pt;padding-right:2.6pt;">
<span class="ltx_text ltx_font_italic" id="S5.T4.4.7.5.1.1" style="font-size:70%;">w/o</span><span class="ltx_text" id="S5.T4.4.7.5.1.2" style="font-size:70%;"> Ursidae</span>
</th>
<td class="ltx_td ltx_align_center" id="S5.T4.4.7.5.2" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.7.5.2.1" style="font-size:70%;">74.2</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.7.5.3" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.7.5.3.1" style="font-size:70%;">77.9</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.7.5.4" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.7.5.4.1" style="font-size:70%;">69.9</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.7.5.5" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.7.5.5.1" style="font-size:70%;">66.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.7.5.6" style="background-color:#D8D8D8;padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.7.5.6.1" style="font-size:70%;background-color:#D8D8D8;">51.3</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.7.5.7" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.7.5.7.1" style="font-size:70%;">67.6</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.4.8.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T4.4.8.6.1" style="padding-left:2.6pt;padding-right:2.6pt;">
<span class="ltx_text ltx_font_italic" id="S5.T4.4.8.6.1.1" style="font-size:70%;">w/o</span><span class="ltx_text" id="S5.T4.4.8.6.1.2" style="font-size:70%;"> Bovidae</span>
</th>
<td class="ltx_td ltx_align_center" id="S5.T4.4.8.6.2" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.8.6.2.1" style="font-size:70%;">74.0</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.8.6.3" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.8.6.3.1" style="font-size:70%;">77.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.8.6.4" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.8.6.4.1" style="font-size:70%;">69.3</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.8.6.5" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.8.6.5.1" style="font-size:70%;">66.8</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.8.6.6" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.8.6.6.1" style="font-size:70%;">76.9</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.8.6.7" style="background-color:#D8D8D8;padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.8.6.7.1" style="font-size:70%;background-color:#D8D8D8;">59.5</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.4.9.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T4.4.9.7.1" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.9.7.1.1" style="font-size:70%;">Average (seen)</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.9.7.2" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.9.7.2.1" style="font-size:70%;">74.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.9.7.3" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.9.7.3.1" style="font-size:70%;">77.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.9.7.4" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.9.7.4.1" style="font-size:70%;">69.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.9.7.5" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.9.7.5.1" style="font-size:70%;">66.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.9.7.6" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.9.7.6.1" style="font-size:70%;">76.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.9.7.7" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.9.7.7.1" style="font-size:70%;">67.6</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.4.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" id="S5.T4.4.2.1" style="padding-left:2.6pt;padding-right:2.6pt;">
<math alttext="\Delta" class="ltx_Math" display="inline" id="S5.T4.4.2.1.m1.1"><semantics id="S5.T4.4.2.1.m1.1a"><mi id="S5.T4.4.2.1.m1.1.1" mathsize="70%" mathvariant="normal" xref="S5.T4.4.2.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S5.T4.4.2.1.m1.1b"><ci id="S5.T4.4.2.1.m1.1.1.cmml" xref="S5.T4.4.2.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.4.2.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S5.T4.4.2.1.m1.1d">roman_Δ</annotation></semantics></math><span class="ltx_text" id="S5.T4.4.2.1.1" style="font-size:70%;"> (seen - unseen)</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T4.4.2.2" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.2.2.1" style="font-size:70%;">8.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T4.4.2.3" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.2.3.1" style="font-size:70%;">9.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T4.4.2.4" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.2.4.1" style="font-size:70%;">19.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T4.4.2.5" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.2.5.1" style="font-size:70%;">21.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T4.4.2.6" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.2.6.1" style="font-size:70%;">24.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T4.4.2.7" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T4.4.2.7.1" style="font-size:70%;">8.1</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S5.T5">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">TABLE V: </span>Results (AP) of HRNet-w32 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib34" title="">34</a>]</cite> models on the “Leave one out” setting of APTv2, where only easy subset is used for training.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T5.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T5.1.1">
<th class="ltx_td ltx_nopad ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S5.T5.1.1.1" style="padding-left:2.6pt;padding-right:2.6pt;"><svg height="14.31" overflow="visible" version="1.1" width="66.78"><g transform="translate(0,14.31) scale(1,-1)"><path d="M 0,14.31 66.78,0" stroke="black" stroke-width="0.4"></path><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,8.35) scale(1, -1)"><foreignobject height="8.35" overflow="visible" width="33.39">
<span class="ltx_inline-block" id="S5.T5.1.1.1.pic1.1.1">
<span class="ltx_inline-block ltx_align_left" id="S5.T5.1.1.1.pic1.1.1.1">
<span class="ltx_p" id="S5.T5.1.1.1.pic1.1.1.1.1"><span class="ltx_text" id="S5.T5.1.1.1.pic1.1.1.1.1.1" style="font-size:70%;">training</span></span>
</span>
</span></foreignobject></g></g><g class="ltx_svg_fog" transform="translate(51.12,8.35)"><g transform="translate(0,5.96) scale(1, -1)"><foreignobject height="5.96" overflow="visible" width="15.66">
<span class="ltx_inline-block" id="S5.T5.1.1.1.pic1.2.1">
<span class="ltx_inline-block ltx_align_right" id="S5.T5.1.1.1.pic1.2.1.1">
<span class="ltx_p" id="S5.T5.1.1.1.pic1.2.1.1.1"><span class="ltx_text" id="S5.T5.1.1.1.pic1.2.1.1.1.1" style="font-size:70%;">test</span></span>
</span>
</span></foreignobject></g></g></g></svg></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.1.1.2" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.1.1.2.1" style="font-size:70%;">Canidae</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.1.1.3" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.1.1.3.1" style="font-size:70%;">Felidae</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.1.1.4" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.1.1.4.1" style="font-size:70%;">Hominidae</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.1.1.5" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.1.1.5.1" style="font-size:70%;">Cercopithecidae</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.1.1.6" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.1.1.6.1" style="font-size:70%;">Ursidae</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.1.1.7" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.1.1.7.1" style="font-size:70%;">Bovidae</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.2.3.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T5.2.3.1.1" style="padding-left:2.6pt;padding-right:2.6pt;">
<span class="ltx_text ltx_font_italic" id="S5.T5.2.3.1.1.1" style="font-size:70%;">w/o</span><span class="ltx_text" id="S5.T5.2.3.1.1.2" style="font-size:70%;"> Canidae</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.2.3.1.2" style="background-color:#D8D8D8;padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.3.1.2.1" style="font-size:70%;background-color:#D8D8D8;">55.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.2.3.1.3" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.3.1.3.1" style="font-size:70%;">68.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.2.3.1.4" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.3.1.4.1" style="font-size:70%;">63.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.2.3.1.5" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.3.1.5.1" style="font-size:70%;">60.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.2.3.1.6" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.3.1.6.1" style="font-size:70%;">61.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.2.3.1.7" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.3.1.7.1" style="font-size:70%;">55.9</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.4.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T5.2.4.2.1" style="padding-left:2.6pt;padding-right:2.6pt;">
<span class="ltx_text ltx_font_italic" id="S5.T5.2.4.2.1.1" style="font-size:70%;">w/o</span><span class="ltx_text" id="S5.T5.2.4.2.1.2" style="font-size:70%;"> Felidae</span>
</th>
<td class="ltx_td ltx_align_center" id="S5.T5.2.4.2.2" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.4.2.2.1" style="font-size:70%;">62.1</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.4.2.3" style="background-color:#D8D8D8;padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.4.2.3.1" style="font-size:70%;background-color:#D8D8D8;">59.5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.4.2.4" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.4.2.4.1" style="font-size:70%;">64.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.4.2.5" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.4.2.5.1" style="font-size:70%;">60.9</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.4.2.6" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.4.2.6.1" style="font-size:70%;">61.7</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.4.2.7" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.4.2.7.1" style="font-size:70%;">56.0</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.5.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T5.2.5.3.1" style="padding-left:2.6pt;padding-right:2.6pt;">
<span class="ltx_text ltx_font_italic" id="S5.T5.2.5.3.1.1" style="font-size:70%;">w/o</span><span class="ltx_text" id="S5.T5.2.5.3.1.2" style="font-size:70%;"> Hominidae</span>
</th>
<td class="ltx_td ltx_align_center" id="S5.T5.2.5.3.2" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.5.3.2.1" style="font-size:70%;">62.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.5.3.3" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.5.3.3.1" style="font-size:70%;">70.1</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.5.3.4" style="background-color:#D8D8D8;padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.5.3.4.1" style="font-size:70%;background-color:#D8D8D8;">47.0</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.5.3.5" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.5.3.5.1" style="font-size:70%;">55.3</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.5.3.6" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.5.3.6.1" style="font-size:70%;">61.3</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.5.3.7" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.5.3.7.1" style="font-size:70%;">57.5</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.6.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T5.2.6.4.1" style="padding-left:2.6pt;padding-right:2.6pt;">
<span class="ltx_text ltx_font_italic" id="S5.T5.2.6.4.1.1" style="font-size:70%;">w/o</span><span class="ltx_text" id="S5.T5.2.6.4.1.2" style="font-size:70%;"> Cercopithecidae</span>
</th>
<td class="ltx_td ltx_align_center" id="S5.T5.2.6.4.2" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.6.4.2.1" style="font-size:70%;">62.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.6.4.3" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.6.4.3.1" style="font-size:70%;">69.1</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.6.4.4" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.6.4.4.1" style="font-size:70%;">62.9</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.6.4.5" style="background-color:#D8D8D8;padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.6.4.5.1" style="font-size:70%;background-color:#D8D8D8;">38.1</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.6.4.6" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.6.4.6.1" style="font-size:70%;">61.9</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.6.4.7" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.6.4.7.1" style="font-size:70%;">56.0</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.7.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T5.2.7.5.1" style="padding-left:2.6pt;padding-right:2.6pt;">
<span class="ltx_text ltx_font_italic" id="S5.T5.2.7.5.1.1" style="font-size:70%;">w/o</span><span class="ltx_text" id="S5.T5.2.7.5.1.2" style="font-size:70%;"> Ursidae</span>
</th>
<td class="ltx_td ltx_align_center" id="S5.T5.2.7.5.2" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.7.5.2.1" style="font-size:70%;">62.5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.7.5.3" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.7.5.3.1" style="font-size:70%;">69.7</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.7.5.4" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.7.5.4.1" style="font-size:70%;">64.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.7.5.5" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.7.5.5.1" style="font-size:70%;">60.5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.7.5.6" style="background-color:#D8D8D8;padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.7.5.6.1" style="font-size:70%;background-color:#D8D8D8;">45.2</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.7.5.7" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.7.5.7.1" style="font-size:70%;">55.7</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.8.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T5.2.8.6.1" style="padding-left:2.6pt;padding-right:2.6pt;">
<span class="ltx_text ltx_font_italic" id="S5.T5.2.8.6.1.1" style="font-size:70%;">w/o</span><span class="ltx_text" id="S5.T5.2.8.6.1.2" style="font-size:70%;"> Bovidae</span>
</th>
<td class="ltx_td ltx_align_center" id="S5.T5.2.8.6.2" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.8.6.2.1" style="font-size:70%;">61.8</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.8.6.3" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.8.6.3.1" style="font-size:70%;">69.5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.8.6.4" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.8.6.4.1" style="font-size:70%;">65</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.8.6.5" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.8.6.5.1" style="font-size:70%;">59.9</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.8.6.6" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.8.6.6.1" style="font-size:70%;">63.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.8.6.7" style="background-color:#D8D8D8;padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.8.6.7.1" style="font-size:70%;background-color:#D8D8D8;">50.9</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.9.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T5.2.9.7.1" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.9.7.1.1" style="font-size:70%;">Average (seen)</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.2.9.7.2" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.9.7.2.1" style="font-size:70%;">62.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.2.9.7.3" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.9.7.3.1" style="font-size:70%;">69.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.2.9.7.4" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.9.7.4.1" style="font-size:70%;">64.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.2.9.7.5" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.9.7.5.1" style="font-size:70%;">59.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.2.9.7.6" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.9.7.6.1" style="font-size:70%;">62.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.2.9.7.7" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.9.7.7.1" style="font-size:70%;">56.2</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" id="S5.T5.2.2.1" style="padding-left:2.6pt;padding-right:2.6pt;">
<math alttext="\Delta" class="ltx_Math" display="inline" id="S5.T5.2.2.1.m1.1"><semantics id="S5.T5.2.2.1.m1.1a"><mi id="S5.T5.2.2.1.m1.1.1" mathsize="70%" mathvariant="normal" xref="S5.T5.2.2.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S5.T5.2.2.1.m1.1b"><ci id="S5.T5.2.2.1.m1.1.1.cmml" xref="S5.T5.2.2.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.2.2.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S5.T5.2.2.1.m1.1d">roman_Δ</annotation></semantics></math><span class="ltx_text" id="S5.T5.2.2.1.1" style="font-size:70%;"> (seen - unseen)</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T5.2.2.2" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.2.2.1" style="font-size:70%;">6.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T5.2.2.3" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.2.3.1" style="font-size:70%;">9.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T5.2.2.4" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.2.4.1" style="font-size:70%;">17.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T5.2.2.5" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.2.5.1" style="font-size:70%;">21.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T5.2.2.6" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.2.6.1" style="font-size:70%;">16.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T5.2.2.7" style="padding-left:2.6pt;padding-right:2.6pt;"><span class="ltx_text" id="S5.T5.2.2.7.1" style="font-size:70%;">5.3</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S5.SS5.SSS2.p3">
<p class="ltx_p" id="S5.SS5.SSS2.p3.1">Further, we observe that images from the hard subset significantly contribute to enhancing the model’s generalization performance, especially on the unseen families. As shown in Table <a class="ltx_ref" href="#S5.T4" title="TABLE IV ‣ V-E2 Results and analysis ‣ V-E Low-data Training and Generalization (LT track) ‣ V Experiment ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_tag">IV</span></a> and Table <a class="ltx_ref" href="#S5.T5" title="TABLE V ‣ V-E2 Results and analysis ‣ V-E Low-data Training and Generalization (LT track) ‣ V Experiment ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_tag">V</span></a>, excluding hard images from the training set (<span class="ltx_text ltx_font_italic" id="S5.SS5.SSS2.p3.1.1">i.e.</span>, training only with the easy subset) results in a significant performance drop, <span class="ltx_text ltx_font_italic" id="S5.SS5.SSS2.p3.1.2">i.e.</span>, from 74.4 AP to 62.3 AP for the seen setting and from 66.1 AP to 55.9 AP for the unseen setting, on the Canidae family. However, the gap between the performance on seen and unseen settings narrows slightly when trained solely on the easy subset, suggesting that the hard subset contributes slightly more to improving performance for in-domain animal species than unseen ones. </p>
</div>
<div class="ltx_para" id="S5.SS5.SSS2.p4">
<p class="ltx_p" id="S5.SS5.SSS2.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS5.SSS2.p4.1.1">2) Low-data fine-tuning setting</span> Table <a class="ltx_ref" href="#S5.T6" title="TABLE VI ‣ V-E2 Results and analysis ‣ V-E Low-data Training and Generalization (LT track) ‣ V Experiment ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_tag">VI</span></a> shows the results. The performance of all the models steadily improves as the volume of training data increases, underscoring the importance of collecting and labeling more data in enhancing the performance of animal pose estimation. Additionally, the scale of the models correlates positively with their generalization abilities. For example, when trained with only 20 images per species, HRNet-w32 achieves a modest AP of 59.1 while ViTPose-B achieves a superior AP of 63.5, surpassing the performance of HRNet-w32 even when the latter is trained with 80 images per species. Remarkably, the larger model ViTPose-L achieves an AP of 63.2 without using any training data from APTv2 for fine-tuning. Furthermore, with just 80 images per species for training (<span class="ltx_text ltx_font_italic" id="S5.SS5.SSS2.p4.1.2">i.e.</span>, a total of 1,600 images), ViTPose-L obtains an AP of 71.6, outperforming the HRNet-w32 model trained with all the training data (<span class="ltx_text ltx_font_italic" id="S5.SS5.SSS2.p4.1.3">i.e.</span>, 58,029 images) in the supervised setting, <span class="ltx_text ltx_font_italic" id="S5.SS5.SSS2.p4.1.4">i.e.</span>, 71.3 AP. It validates the much better data efficiency (<span class="ltx_text ltx_font_italic" id="S5.SS5.SSS2.p4.1.5">i.e.</span>, 35<math alttext="\times" class="ltx_Math" display="inline" id="S5.SS5.SSS2.p4.1.m1.1"><semantics id="S5.SS5.SSS2.p4.1.m1.1a"><mo id="S5.SS5.SSS2.p4.1.m1.1.1" xref="S5.SS5.SSS2.p4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS5.SSS2.p4.1.m1.1b"><times id="S5.SS5.SSS2.p4.1.m1.1.1.cmml" xref="S5.SS5.SSS2.p4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.SSS2.p4.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.SS5.SSS2.p4.1.m1.1d">×</annotation></semantics></math>) of large models. These observations suggest a promising strategy for enhancing model generalization with limited data, <span class="ltx_text ltx_font_italic" id="S5.SS5.SSS2.p4.1.6">i.e.</span>, scaling up the model size. While this finding aligns with results obtained from other domains like image classification, it represents a novel observation within the context of animal pose estimation.</p>
</div>
<figure class="ltx_table" id="S5.T6">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">TABLE VI: </span>Results (AP) of HRNet-w32 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib34" title="">34</a>]</cite>, ViTPose-B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib44" title="">44</a>]</cite>, ViTPose-L <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib44" title="">44</a>]</cite> in the “low-data fine-tuning” setting.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T6.12">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T6.12.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T6.12.1.1.1" style="padding-left:4.8pt;padding-right:4.8pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T6.12.1.1.2" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.1.1.2.1" style="font-size:70%;"># Training Samples per Species</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.12.1.1.3" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.1.1.3.1" style="font-size:70%;">Canidae</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.12.1.1.4" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.1.1.4.1" style="font-size:70%;">Felidae</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.12.1.1.5" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.1.1.5.1" style="font-size:70%;">Hominidae</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.12.1.1.6" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.1.1.6.1" style="font-size:70%;">Cercopithecidae</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.12.1.1.7" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.1.1.7.1" style="font-size:70%;">Ursidae</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.12.1.1.8" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.1.1.8.1" style="font-size:70%;">Bovidae</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.12.1.1.9" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.1.1.9.1" style="font-size:70%;">Average</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.12.2.2">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T6.12.2.2.1" style="padding-left:4.8pt;padding-right:4.8pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T6.12.2.2.2" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.2.2.2.1" style="font-size:70%;">0</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.12.2.2.3" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.2.2.3.1" style="font-size:70%;">59.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.12.2.2.4" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.2.2.4.1" style="font-size:70%;">64.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.12.2.2.5" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.2.2.5.1" style="font-size:70%;">42.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.12.2.2.6" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.2.2.6.1" style="font-size:70%;">38.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.12.2.2.7" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.2.2.7.1" style="font-size:70%;">51.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.12.2.2.8" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.2.2.8.1" style="font-size:70%;">58.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.12.2.2.9" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.2.2.9.1" style="font-size:70%;">52.5</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.12.3.3">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S5.T6.12.3.3.1" style="padding-left:4.8pt;padding-right:4.8pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T6.12.3.3.2" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.3.3.2.1" style="font-size:70%;">20</span></th>
<td class="ltx_td ltx_align_center" id="S5.T6.12.3.3.3" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.3.3.3.1" style="font-size:70%;">63.3</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.3.3.4" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.3.3.4.1" style="font-size:70%;">68.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.3.3.5" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.3.3.5.1" style="font-size:70%;">53.5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.3.3.6" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.3.3.6.1" style="font-size:70%;">50.2</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.3.3.7" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.3.3.7.1" style="font-size:70%;">58.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.12.3.3.8" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.3.3.8.1" style="font-size:70%;">60.2</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.3.3.9" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.3.3.9.1" style="font-size:70%;">59.1</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.12.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T6.12.4.4.1" style="padding-left:4.8pt;padding-right:4.8pt;">
<span class="ltx_text" id="S5.T6.12.4.4.1.1" style="font-size:70%;">HRNet-w32 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T6.12.4.4.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib34" title="">34</a><span class="ltx_text" id="S5.T6.12.4.4.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T6.12.4.4.2" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.4.4.2.1" style="font-size:70%;">40</span></th>
<td class="ltx_td ltx_align_center" id="S5.T6.12.4.4.3" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.4.4.3.1" style="font-size:70%;">64.5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.4.4.4" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.4.4.4.1" style="font-size:70%;">68.8</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.4.4.5" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.4.4.5.1" style="font-size:70%;">54.3</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.4.4.6" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.4.4.6.1" style="font-size:70%;">51.9</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.4.4.7" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.4.4.7.1" style="font-size:70%;">61.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.12.4.4.8" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.4.4.8.1" style="font-size:70%;">60.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.4.4.9" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.4.4.9.1" style="font-size:70%;">60.2</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.12.5.5">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S5.T6.12.5.5.1" style="padding-left:4.8pt;padding-right:4.8pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T6.12.5.5.2" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.5.5.2.1" style="font-size:70%;">60</span></th>
<td class="ltx_td ltx_align_center" id="S5.T6.12.5.5.3" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.5.5.3.1" style="font-size:70%;">64.2</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.5.5.4" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.5.5.4.1" style="font-size:70%;">70.7</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.5.5.5" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.5.5.5.1" style="font-size:70%;">55.5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.5.5.6" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.5.5.6.1" style="font-size:70%;">52.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.5.5.7" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.5.5.7.1" style="font-size:70%;">63.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.12.5.5.8" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.5.5.8.1" style="font-size:70%;">60.3</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.5.5.9" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.5.5.9.1" style="font-size:70%;">61.2</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.12.6.6">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S5.T6.12.6.6.1" style="padding-left:4.8pt;padding-right:4.8pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T6.12.6.6.2" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.6.6.2.1" style="font-size:70%;">80</span></th>
<td class="ltx_td ltx_align_center" id="S5.T6.12.6.6.3" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.6.6.3.1" style="font-size:70%;">65.3</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.6.6.4" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.6.6.4.1" style="font-size:70%;">68.9</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.6.6.5" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.6.6.5.1" style="font-size:70%;">56.5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.6.6.6" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.6.6.6.1" style="font-size:70%;">53.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.6.6.7" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.6.6.7.1" style="font-size:70%;">63.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.12.6.6.8" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.6.6.8.1" style="font-size:70%;">61.9</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.6.6.9" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.6.6.9.1" style="font-size:70%;">61.6</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.12.7.7">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S5.T6.12.7.7.1" style="padding-left:4.8pt;padding-right:4.8pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T6.12.7.7.2" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.7.7.2.1" style="font-size:70%;">All</span></th>
<td class="ltx_td ltx_align_center" id="S5.T6.12.7.7.3" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.7.7.3.1" style="font-size:70%;">73.5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.7.7.4" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.7.7.4.1" style="font-size:70%;">78.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.7.7.5" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.7.7.5.1" style="font-size:70%;">68.8</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.7.7.6" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.7.7.6.1" style="font-size:70%;">65.0</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.7.7.7" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.7.7.7.1" style="font-size:70%;">75.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.12.7.7.8" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.7.7.8.1" style="font-size:70%;">66.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.7.7.9" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.7.7.9.1" style="font-size:70%;">71.3</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.12.8.8">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T6.12.8.8.1" style="padding-left:4.8pt;padding-right:4.8pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T6.12.8.8.2" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.8.8.2.1" style="font-size:70%;">0</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.12.8.8.3" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.8.8.3.1" style="font-size:70%;">63.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.12.8.8.4" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.8.8.4.1" style="font-size:70%;">65.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.12.8.8.5" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.8.8.5.1" style="font-size:70%;">47.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.12.8.8.6" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.8.8.6.1" style="font-size:70%;">51.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.12.8.8.7" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.8.8.7.1" style="font-size:70%;">59.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.12.8.8.8" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.8.8.8.1" style="font-size:70%;">59.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.12.8.8.9" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.8.8.9.1" style="font-size:70%;">57.7</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.12.9.9">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S5.T6.12.9.9.1" style="padding-left:4.8pt;padding-right:4.8pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T6.12.9.9.2" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.9.9.2.1" style="font-size:70%;">20</span></th>
<td class="ltx_td ltx_align_center" id="S5.T6.12.9.9.3" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.9.9.3.1" style="font-size:70%;">67.1</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.9.9.4" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.9.9.4.1" style="font-size:70%;">69.3</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.9.9.5" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.9.9.5.1" style="font-size:70%;">59.9</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.9.9.6" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.9.9.6.1" style="font-size:70%;">59.1</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.9.9.7" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.9.9.7.1" style="font-size:70%;">63.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.12.9.9.8" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.9.9.8.1" style="font-size:70%;">62.2</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.9.9.9" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.9.9.9.1" style="font-size:70%;">63.5</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.12.10.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T6.12.10.10.1" style="padding-left:4.8pt;padding-right:4.8pt;">
<span class="ltx_text" id="S5.T6.12.10.10.1.1" style="font-size:70%;">ViTPose-B </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T6.12.10.10.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib44" title="">44</a><span class="ltx_text" id="S5.T6.12.10.10.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T6.12.10.10.2" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.10.10.2.1" style="font-size:70%;">40</span></th>
<td class="ltx_td ltx_align_center" id="S5.T6.12.10.10.3" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.10.10.3.1" style="font-size:70%;">68.5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.10.10.4" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.10.10.4.1" style="font-size:70%;">69.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.10.10.5" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.10.10.5.1" style="font-size:70%;">61.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.10.10.6" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.10.10.6.1" style="font-size:70%;">59.7</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.10.10.7" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.10.10.7.1" style="font-size:70%;">65.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.12.10.10.8" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.10.10.8.1" style="font-size:70%;">61.5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.10.10.9" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.10.10.9.1" style="font-size:70%;">64.4</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.12.11.11">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S5.T6.12.11.11.1" style="padding-left:4.8pt;padding-right:4.8pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T6.12.11.11.2" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.11.11.2.1" style="font-size:70%;">60</span></th>
<td class="ltx_td ltx_align_center" id="S5.T6.12.11.11.3" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.11.11.3.1" style="font-size:70%;">68.1</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.11.11.4" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.11.11.4.1" style="font-size:70%;">71.1</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.11.11.5" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.11.11.5.1" style="font-size:70%;">61.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.11.11.6" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.11.11.6.1" style="font-size:70%;">59.2</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.11.11.7" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.11.11.7.1" style="font-size:70%;">67.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.12.11.11.8" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.11.11.8.1" style="font-size:70%;">62.5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.11.11.9" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.11.11.9.1" style="font-size:70%;">65.0</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.12.12.12">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S5.T6.12.12.12.1" style="padding-left:4.8pt;padding-right:4.8pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T6.12.12.12.2" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.12.12.2.1" style="font-size:70%;">80</span></th>
<td class="ltx_td ltx_align_center" id="S5.T6.12.12.12.3" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.12.12.3.1" style="font-size:70%;">68.9</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.12.12.4" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.12.12.4.1" style="font-size:70%;">71.2</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.12.12.5" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.12.12.5.1" style="font-size:70%;">64.3</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.12.12.6" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.12.12.6.1" style="font-size:70%;">62.2</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.12.12.7" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.12.12.7.1" style="font-size:70%;">67.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.12.12.12.8" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.12.12.8.1" style="font-size:70%;">62.7</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.12.12.9" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.12.12.9.1" style="font-size:70%;">66.2</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.12.13.13">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T6.12.13.13.1" style="padding-left:4.8pt;padding-right:4.8pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T6.12.13.13.2" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.13.13.2.1" style="font-size:70%;">0</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.12.13.13.3" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.13.13.3.1" style="font-size:70%;">66.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.12.13.13.4" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.13.13.4.1" style="font-size:70%;">69.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.12.13.13.5" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.13.13.5.1" style="font-size:70%;">59.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.12.13.13.6" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.13.13.6.1" style="font-size:70%;">61.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.12.13.13.7" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.13.13.7.1" style="font-size:70%;">62.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.12.13.13.8" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.13.13.8.1" style="font-size:70%;">61.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.12.13.13.9" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.13.13.9.1" style="font-size:70%;">63.2</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.12.14.14">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S5.T6.12.14.14.1" style="padding-left:4.8pt;padding-right:4.8pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T6.12.14.14.2" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.14.14.2.1" style="font-size:70%;">20</span></th>
<td class="ltx_td ltx_align_center" id="S5.T6.12.14.14.3" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.14.14.3.1" style="font-size:70%;">70.0</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.14.14.4" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.14.14.4.1" style="font-size:70%;">73.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.14.14.5" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.14.14.5.1" style="font-size:70%;">69.3</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.14.14.6" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.14.14.6.1" style="font-size:70%;">68.3</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.14.14.7" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.14.14.7.1" style="font-size:70%;">67.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.12.14.14.8" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.14.14.8.1" style="font-size:70%;">64.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.14.14.9" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.14.14.9.1" style="font-size:70%;">68.9</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.12.15.15">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T6.12.15.15.1" style="padding-left:4.8pt;padding-right:4.8pt;">
<span class="ltx_text" id="S5.T6.12.15.15.1.1" style="font-size:70%;">ViTPose-L </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T6.12.15.15.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib44" title="">44</a><span class="ltx_text" id="S5.T6.12.15.15.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T6.12.15.15.2" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.15.15.2.1" style="font-size:70%;">40</span></th>
<td class="ltx_td ltx_align_center" id="S5.T6.12.15.15.3" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.15.15.3.1" style="font-size:70%;">71.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.15.15.4" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.15.15.4.1" style="font-size:70%;">73.0</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.15.15.5" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.15.15.5.1" style="font-size:70%;">69.5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.15.15.6" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.15.15.6.1" style="font-size:70%;">68.2</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.15.15.7" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.15.15.7.1" style="font-size:70%;">68.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.12.15.15.8" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.15.15.8.1" style="font-size:70%;">64.2</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.15.15.9" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.15.15.9.1" style="font-size:70%;">69.2</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.12.16.16">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S5.T6.12.16.16.1" style="padding-left:4.8pt;padding-right:4.8pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T6.12.16.16.2" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.16.16.2.1" style="font-size:70%;">60</span></th>
<td class="ltx_td ltx_align_center" id="S5.T6.12.16.16.3" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.16.16.3.1" style="font-size:70%;">70.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.16.16.4" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.16.16.4.1" style="font-size:70%;">75.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.16.16.5" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.16.16.5.1" style="font-size:70%;">71.3</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.16.16.6" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.16.16.6.1" style="font-size:70%;">69.1</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.16.16.7" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.16.16.7.1" style="font-size:70%;">71.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.12.16.16.8" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.16.16.8.1" style="font-size:70%;">64.5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.12.16.16.9" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.16.16.9.1" style="font-size:70%;">70.4</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.12.17.17">
<th class="ltx_td ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S5.T6.12.17.17.1" style="padding-left:4.8pt;padding-right:4.8pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S5.T6.12.17.17.2" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.17.17.2.1" style="font-size:70%;">80</span></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T6.12.17.17.3" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.17.17.3.1" style="font-size:70%;">73.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T6.12.17.17.4" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.17.17.4.1" style="font-size:70%;">75.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T6.12.17.17.5" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.17.17.5.1" style="font-size:70%;">73.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T6.12.17.17.6" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.17.17.6.1" style="font-size:70%;">70.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T6.12.17.17.7" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.17.17.7.1" style="font-size:70%;">72.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T6.12.17.17.8" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.17.17.8.1" style="font-size:70%;">65.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T6.12.17.17.9" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S5.T6.12.17.17.9.1" style="font-size:70%;">71.6</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S5.T7">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">TABLE VII: </span>The tracking results of different models on the APTv2 validation set.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T7.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T7.3.1.1">
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T7.3.1.1.1" style="padding-left:3.5pt;padding-right:3.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S5.T7.3.1.1.2" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_text" id="S5.T7.3.1.1.2.1" style="font-size:70%;">SimpleBaseline </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T7.3.1.1.2.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib41" title="">41</a><span class="ltx_text" id="S5.T7.3.1.1.2.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S5.T7.3.1.1.2.4" style="font-size:70%;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S5.T7.3.1.1.3" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_text" id="S5.T7.3.1.1.3.1" style="font-size:70%;">HRNet </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T7.3.1.1.3.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib34" title="">34</a><span class="ltx_text" id="S5.T7.3.1.1.3.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S5.T7.3.1.1.3.4" style="font-size:70%;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S5.T7.3.1.1.4" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_text" id="S5.T7.3.1.1.4.1" style="font-size:70%;">HRFormer </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T7.3.1.1.4.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib50" title="">50</a><span class="ltx_text" id="S5.T7.3.1.1.4.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S5.T7.3.1.1.4.4" style="font-size:70%;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S5.T7.3.1.1.5" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_text" id="S5.T7.3.1.1.5.1" style="font-size:70%;">ViTPose </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T7.3.1.1.5.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib44" title="">44</a><span class="ltx_text" id="S5.T7.3.1.1.5.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S5.T7.3.1.1.5.4" style="font-size:70%;"></span>
</td>
<td class="ltx_td ltx_border_t" id="S5.T7.3.1.1.6" style="padding-left:3.5pt;padding-right:3.5pt;"></td>
</tr>
<tr class="ltx_tr" id="S5.T7.3.2.2">
<td class="ltx_td ltx_border_r" id="S5.T7.3.2.2.1" style="padding-left:3.5pt;padding-right:3.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S5.T7.3.2.2.2" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.2.2.2.1" style="font-size:70%;">ResNet-50</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.3.2.2.3" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.2.2.3.1" style="font-size:70%;">ResNet-101</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.3.2.2.4" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.2.2.4.1" style="font-size:70%;">HRNet-w32</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.3.2.2.5" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.2.2.5.1" style="font-size:70%;">HRNet-w48</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.3.2.2.6" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.2.2.6.1" style="font-size:70%;">HRFormer-S</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.3.2.2.7" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.2.2.7.1" style="font-size:70%;">HRFormer-B</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.3.2.2.8" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.2.2.8.1" style="font-size:70%;">VITPose-B</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.3.2.2.9" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.2.2.9.1" style="font-size:70%;">ViTPose-L</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.3.2.2.10" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.2.2.10.1" style="font-size:70%;">Average</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.3.3.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T7.3.3.3.1" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_text" id="S5.T7.3.3.3.1.1" style="font-size:70%;">SiamRPN++ </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T7.3.3.3.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib17" title="">17</a><span class="ltx_text" id="S5.T7.3.3.3.1.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.3.3.3.2" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.3.3.2.1" style="font-size:70%;">64.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T7.3.3.3.3" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.3.3.3.1" style="font-size:70%;">64.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.3.3.3.4" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.3.3.4.1" style="font-size:70%;">66.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T7.3.3.3.5" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.3.3.5.1" style="font-size:70%;">68.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.3.3.3.6" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.3.3.6.1" style="font-size:70%;">66.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T7.3.3.3.7" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.3.3.7.1" style="font-size:70%;">66.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.3.3.3.8" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.3.3.8.1" style="font-size:70%;">69.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T7.3.3.3.9" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.3.3.9.1" style="font-size:70%;">71.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.3.3.3.10" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.3.3.10.1" style="font-size:70%;">67.2</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.3.4.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.3.4.4.1" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_text" id="S5.T7.3.4.4.1.1" style="font-size:70%;">STARK </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T7.3.4.4.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib46" title="">46</a><span class="ltx_text" id="S5.T7.3.4.4.1.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T7.3.4.4.2" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.4.4.2.1" style="font-size:70%;">63.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.3.4.4.3" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.4.4.3.1" style="font-size:70%;">63.7</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.3.4.4.4" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.4.4.4.1" style="font-size:70%;">65.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.3.4.4.5" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.4.4.5.1" style="font-size:70%;">67.2</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.3.4.4.6" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.4.4.6.1" style="font-size:70%;">65.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.3.4.4.7" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.4.4.7.1" style="font-size:70%;">65.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.3.4.4.8" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.4.4.8.1" style="font-size:70%;">67.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.3.4.4.9" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.4.4.9.1" style="font-size:70%;">69.5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.3.4.4.10" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.4.4.10.1" style="font-size:70%;">66.0</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.3.5.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.3.5.5.1" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_text" id="S5.T7.3.5.5.1.1" style="font-size:70%;">SwinTrack </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T7.3.5.5.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib23" title="">23</a><span class="ltx_text" id="S5.T7.3.5.5.1.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T7.3.5.5.2" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.5.5.2.1" style="font-size:70%;">65.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.3.5.5.3" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.5.5.3.1" style="font-size:70%;">65.2</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.3.5.5.4" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.5.5.4.1" style="font-size:70%;">67.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.3.5.5.5" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.5.5.5.1" style="font-size:70%;">68.8</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.3.5.5.6" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.5.5.6.1" style="font-size:70%;">66.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.3.5.5.7" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.5.5.7.1" style="font-size:70%;">66.9</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.3.5.5.8" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.5.5.8.1" style="font-size:70%;">69.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.3.5.5.9" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.5.5.9.1" style="font-size:70%;">71.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.3.5.5.10" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.5.5.10.1" style="font-size:70%;">67.6</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.3.6.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.3.6.6.1" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_text" id="S5.T7.3.6.6.1.1" style="font-size:70%;">ViTTrack </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T7.3.6.6.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib48" title="">48</a><span class="ltx_text" id="S5.T7.3.6.6.1.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T7.3.6.6.2" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.6.6.2.1" style="font-size:70%;">65.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.3.6.6.3" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.6.6.3.1" style="font-size:70%;">66.0</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.3.6.6.4" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.6.6.4.1" style="font-size:70%;">68.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.3.6.6.5" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.6.6.5.1" style="font-size:70%;">69.5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.3.6.6.6" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.6.6.6.1" style="font-size:70%;">67.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.3.6.6.7" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.6.6.7.1" style="font-size:70%;">67.7</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.3.6.6.8" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.6.6.8.1" style="font-size:70%;">70.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.3.6.6.9" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.6.6.9.1" style="font-size:70%;">72.2</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.3.6.6.10" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.6.6.10.1" style="font-size:70%;">68.4</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.3.7.7">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T7.3.7.7.1" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_text" id="S5.T7.3.7.7.1.1" style="font-size:70%;color:#000000;">ViTPoseTrack</span><span class="ltx_text" id="S5.T7.3.7.7.1.2" style="font-size:70%;">-B</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.3.7.7.2" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.7.7.2.1" style="font-size:70%;">65.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T7.3.7.7.3" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.7.7.3.1" style="font-size:70%;">65.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.3.7.7.4" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.7.7.4.1" style="font-size:70%;">67.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T7.3.7.7.5" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.7.7.5.1" style="font-size:70%;">69.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.3.7.7.6" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.7.7.6.1" style="font-size:70%;">66.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T7.3.7.7.7" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.7.7.7.1" style="font-size:70%;">67.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.3.7.7.8" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.7.7.8.1" style="font-size:70%;">69.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T7.3.7.7.9" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.7.7.9.1" style="font-size:70%;">71.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.3.7.7.10" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.7.7.10.1" style="font-size:70%;">67.9</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.3.8.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.3.8.8.1" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_text" id="S5.T7.3.8.8.1.1" style="font-size:70%;color:#000000;">ViTPoseTrack</span><span class="ltx_text" id="S5.T7.3.8.8.1.2" style="font-size:70%;">-L</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T7.3.8.8.2" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.8.8.2.1" style="font-size:70%;">65.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.3.8.8.3" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.8.8.3.1" style="font-size:70%;">66.0</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.3.8.8.4" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.8.8.4.1" style="font-size:70%;">68.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.3.8.8.5" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.8.8.5.1" style="font-size:70%;">69.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.3.8.8.6" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.8.8.6.1" style="font-size:70%;">67.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.3.8.8.7" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.8.8.7.1" style="font-size:70%;">67.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.3.8.8.8" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.8.8.8.1" style="font-size:70%;">70.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.3.8.8.9" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.8.8.9.1" style="font-size:70%;">72.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.3.8.8.10" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.8.8.10.1" style="font-size:70%;">68.4</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.3.9.9">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T7.3.9.9.1" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.9.9.1.1" style="font-size:70%;">Average</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T7.3.9.9.2" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.9.9.2.1" style="font-size:70%;">65.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T7.3.9.9.3" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.9.9.3.1" style="font-size:70%;">65.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T7.3.9.9.4" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.9.9.4.1" style="font-size:70%;">67.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T7.3.9.9.5" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.9.9.5.1" style="font-size:70%;">68.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T7.3.9.9.6" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.9.9.6.1" style="font-size:70%;">66.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T7.3.9.9.7" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.9.9.7.1" style="font-size:70%;">66.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T7.3.9.9.8" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.9.9.8.1" style="font-size:70%;">69.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T7.3.9.9.9" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="S5.T7.3.9.9.9.1" style="font-size:70%;">71.4</span></td>
<td class="ltx_td ltx_border_b ltx_border_t" id="S5.T7.3.9.9.10" style="padding-left:3.5pt;padding-right:3.5pt;"></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S5.T8">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">TABLE VIII: </span>The animal pose tracking results of different models on the easy validation subset.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T8.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T8.3.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T8.3.1.1.1" style="padding-left:5.2pt;padding-right:5.2pt;"></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S5.T8.3.1.1.2" style="padding-left:5.2pt;padding-right:5.2pt;">
<span class="ltx_text" id="S5.T8.3.1.1.2.1" style="font-size:70%;">SimpleBaseline </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T8.3.1.1.2.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib41" title="">41</a><span class="ltx_text" id="S5.T8.3.1.1.2.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S5.T8.3.1.1.2.4" style="font-size:70%;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S5.T8.3.1.1.3" style="padding-left:5.2pt;padding-right:5.2pt;">
<span class="ltx_text" id="S5.T8.3.1.1.3.1" style="font-size:70%;">HRNet </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T8.3.1.1.3.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib34" title="">34</a><span class="ltx_text" id="S5.T8.3.1.1.3.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S5.T8.3.1.1.3.4" style="font-size:70%;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S5.T8.3.1.1.4" style="padding-left:5.2pt;padding-right:5.2pt;">
<span class="ltx_text" id="S5.T8.3.1.1.4.1" style="font-size:70%;">HRFormer </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T8.3.1.1.4.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib50" title="">50</a><span class="ltx_text" id="S5.T8.3.1.1.4.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S5.T8.3.1.1.4.4" style="font-size:70%;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S5.T8.3.1.1.5" style="padding-left:5.2pt;padding-right:5.2pt;">
<span class="ltx_text" id="S5.T8.3.1.1.5.1" style="font-size:70%;">ViTPose </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T8.3.1.1.5.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib44" title="">44</a><span class="ltx_text" id="S5.T8.3.1.1.5.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S5.T8.3.1.1.5.4" style="font-size:70%;"></span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T8.3.2.2">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S5.T8.3.2.2.1" style="padding-left:5.2pt;padding-right:5.2pt;"></th>
<td class="ltx_td ltx_align_center" id="S5.T8.3.2.2.2" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.2.2.2.1" style="font-size:70%;">ResNet-50</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T8.3.2.2.3" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.2.2.3.1" style="font-size:70%;">ResNet-101</span></td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.2.2.4" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.2.2.4.1" style="font-size:70%;">HRNet-w32</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T8.3.2.2.5" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.2.2.5.1" style="font-size:70%;">HRNet-w48</span></td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.2.2.6" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.2.2.6.1" style="font-size:70%;">HRFormer-S</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T8.3.2.2.7" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.2.2.7.1" style="font-size:70%;">HRFormer-B</span></td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.2.2.8" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.2.2.8.1" style="font-size:70%;">VITPose-B</span></td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.2.2.9" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.2.2.9.1" style="font-size:70%;">ViTPose-L</span></td>
</tr>
<tr class="ltx_tr" id="S5.T8.3.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T8.3.3.3.1" style="padding-left:5.2pt;padding-right:5.2pt;">
<span class="ltx_text" id="S5.T8.3.3.3.1.1" style="font-size:70%;">SiamRPN++ </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T8.3.3.3.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib17" title="">17</a><span class="ltx_text" id="S5.T8.3.3.3.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.3.3.3.2" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.3.3.2.1" style="font-size:70%;">74.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T8.3.3.3.3" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.3.3.3.1" style="font-size:70%;">73.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.3.3.3.4" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.3.3.4.1" style="font-size:70%;">77.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T8.3.3.3.5" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.3.3.5.1" style="font-size:70%;">78.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.3.3.3.6" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.3.3.6.1" style="font-size:70%;">75.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T8.3.3.3.7" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.3.3.7.1" style="font-size:70%;">76.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.3.3.3.8" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.3.3.8.1" style="font-size:70%;">78.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.3.3.3.9" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.3.3.9.1" style="font-size:70%;">80.7</span></td>
</tr>
<tr class="ltx_tr" id="S5.T8.3.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T8.3.4.4.1" style="padding-left:5.2pt;padding-right:5.2pt;">
<span class="ltx_text" id="S5.T8.3.4.4.1.1" style="font-size:70%;">STARK </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T8.3.4.4.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib46" title="">46</a><span class="ltx_text" id="S5.T8.3.4.4.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T8.3.4.4.2" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.4.4.2.1" style="font-size:70%;">75.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T8.3.4.4.3" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.4.4.3.1" style="font-size:70%;">75.0</span></td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.4.4.4" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.4.4.4.1" style="font-size:70%;">78.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T8.3.4.4.5" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.4.4.5.1" style="font-size:70%;">79.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.4.4.6" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.4.4.6.1" style="font-size:70%;">76.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T8.3.4.4.7" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.4.4.7.1" style="font-size:70%;">77.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.4.4.8" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.4.4.8.1" style="font-size:70%;">79.1</span></td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.4.4.9" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.4.4.9.1" style="font-size:70%;">81.4</span></td>
</tr>
<tr class="ltx_tr" id="S5.T8.3.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T8.3.5.5.1" style="padding-left:5.2pt;padding-right:5.2pt;">
<span class="ltx_text" id="S5.T8.3.5.5.1.1" style="font-size:70%;">SwinTrack </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T8.3.5.5.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib23" title="">23</a><span class="ltx_text" id="S5.T8.3.5.5.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T8.3.5.5.2" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.5.5.2.1" style="font-size:70%;">75.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T8.3.5.5.3" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.5.5.3.1" style="font-size:70%;">75.3</span></td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.5.5.4" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.5.5.4.1" style="font-size:70%;">78.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T8.3.5.5.5" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.5.5.5.1" style="font-size:70%;">79.8</span></td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.5.5.6" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.5.5.6.1" style="font-size:70%;">76.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T8.3.5.5.7" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.5.5.7.1" style="font-size:70%;">77.5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.5.5.8" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.5.5.8.1" style="font-size:70%;">79.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.5.5.9" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.5.5.9.1" style="font-size:70%;">81.9</span></td>
</tr>
<tr class="ltx_tr" id="S5.T8.3.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T8.3.6.6.1" style="padding-left:5.2pt;padding-right:5.2pt;">
<span class="ltx_text" id="S5.T8.3.6.6.1.1" style="font-size:70%;">ViTTrack </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T8.3.6.6.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib48" title="">48</a><span class="ltx_text" id="S5.T8.3.6.6.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T8.3.6.6.2" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.6.6.2.1" style="font-size:70%;">75.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T8.3.6.6.3" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.6.6.3.1" style="font-size:70%;">75.3</span></td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.6.6.4" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.6.6.4.1" style="font-size:70%;">78.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T8.3.6.6.5" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.6.6.5.1" style="font-size:70%;">79.8</span></td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.6.6.6" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.6.6.6.1" style="font-size:70%;">77.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T8.3.6.6.7" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.6.6.7.1" style="font-size:70%;">77.7</span></td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.6.6.8" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.6.6.8.1" style="font-size:70%;">79.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.6.6.9" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.6.6.9.1" style="font-size:70%;">81.6</span></td>
</tr>
<tr class="ltx_tr" id="S5.T8.3.7.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T8.3.7.7.1" style="padding-left:5.2pt;padding-right:5.2pt;">
<span class="ltx_text" id="S5.T8.3.7.7.1.1" style="font-size:70%;color:#000000;">ViTPoseTrack</span><span class="ltx_text" id="S5.T8.3.7.7.1.2" style="font-size:70%;">-B</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.3.7.7.2" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.7.7.2.1" style="font-size:70%;">75.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T8.3.7.7.3" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.7.7.3.1" style="font-size:70%;">75.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.3.7.7.4" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.7.7.4.1" style="font-size:70%;">78.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T8.3.7.7.5" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.7.7.5.1" style="font-size:70%;">79.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.3.7.7.6" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.7.7.6.1" style="font-size:70%;">77.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T8.3.7.7.7" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.7.7.7.1" style="font-size:70%;">77.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.3.7.7.8" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.7.7.8.1" style="font-size:70%;">79.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.3.7.7.9" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.7.7.9.1" style="font-size:70%;">82.0</span></td>
</tr>
<tr class="ltx_tr" id="S5.T8.3.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S5.T8.3.8.8.1" style="padding-left:5.2pt;padding-right:5.2pt;">
<span class="ltx_text" id="S5.T8.3.8.8.1.1" style="font-size:70%;color:#000000;">ViTPoseTrack</span><span class="ltx_text" id="S5.T8.3.8.8.1.2" style="font-size:70%;">-L</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T8.3.8.8.2" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.8.8.2.1" style="font-size:70%;">75.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T8.3.8.8.3" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.8.8.3.1" style="font-size:70%;">75.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T8.3.8.8.4" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.8.8.4.1" style="font-size:70%;">78.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T8.3.8.8.5" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.8.8.5.1" style="font-size:70%;">79.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T8.3.8.8.6" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.8.8.6.1" style="font-size:70%;">76.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T8.3.8.8.7" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.8.8.7.1" style="font-size:70%;">77.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T8.3.8.8.8" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.8.8.8.1" style="font-size:70%;">79.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T8.3.8.8.9" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T8.3.8.8.9.1" style="font-size:70%;">81.9</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S5.T9">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">TABLE IX: </span>The animal pose tracking results of different models on the hard validation subset.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T9.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T9.3.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T9.3.1.1.1" style="padding-left:5.2pt;padding-right:5.2pt;"></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S5.T9.3.1.1.2" style="padding-left:5.2pt;padding-right:5.2pt;">
<span class="ltx_text" id="S5.T9.3.1.1.2.1" style="font-size:70%;">SimpleBaseline </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T9.3.1.1.2.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib41" title="">41</a><span class="ltx_text" id="S5.T9.3.1.1.2.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S5.T9.3.1.1.2.4" style="font-size:70%;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S5.T9.3.1.1.3" style="padding-left:5.2pt;padding-right:5.2pt;">
<span class="ltx_text" id="S5.T9.3.1.1.3.1" style="font-size:70%;">HRNet </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T9.3.1.1.3.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib34" title="">34</a><span class="ltx_text" id="S5.T9.3.1.1.3.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S5.T9.3.1.1.3.4" style="font-size:70%;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S5.T9.3.1.1.4" style="padding-left:5.2pt;padding-right:5.2pt;">
<span class="ltx_text" id="S5.T9.3.1.1.4.1" style="font-size:70%;">HRFormer </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T9.3.1.1.4.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib50" title="">50</a><span class="ltx_text" id="S5.T9.3.1.1.4.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S5.T9.3.1.1.4.4" style="font-size:70%;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S5.T9.3.1.1.5" style="padding-left:5.2pt;padding-right:5.2pt;">
<span class="ltx_text" id="S5.T9.3.1.1.5.1" style="font-size:70%;">ViTPose </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T9.3.1.1.5.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib45" title="">45</a><span class="ltx_text" id="S5.T9.3.1.1.5.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S5.T9.3.1.1.5.4" style="font-size:70%;"></span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T9.3.2.2">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S5.T9.3.2.2.1" style="padding-left:5.2pt;padding-right:5.2pt;"></th>
<td class="ltx_td ltx_align_center" id="S5.T9.3.2.2.2" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.2.2.2.1" style="font-size:70%;">ResNet-50</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T9.3.2.2.3" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.2.2.3.1" style="font-size:70%;">ResNet-101</span></td>
<td class="ltx_td ltx_align_center" id="S5.T9.3.2.2.4" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.2.2.4.1" style="font-size:70%;">HRNet-w32</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T9.3.2.2.5" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.2.2.5.1" style="font-size:70%;">HRNet-w48</span></td>
<td class="ltx_td ltx_align_center" id="S5.T9.3.2.2.6" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.2.2.6.1" style="font-size:70%;">HRFormer-S</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T9.3.2.2.7" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.2.2.7.1" style="font-size:70%;">HRFormer-B</span></td>
<td class="ltx_td ltx_align_center" id="S5.T9.3.2.2.8" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.2.2.8.1" style="font-size:70%;">VITPose-B</span></td>
<td class="ltx_td ltx_align_center" id="S5.T9.3.2.2.9" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.2.2.9.1" style="font-size:70%;">ViTPose-L</span></td>
</tr>
<tr class="ltx_tr" id="S5.T9.3.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T9.3.3.3.1" style="padding-left:5.2pt;padding-right:5.2pt;">
<span class="ltx_text" id="S5.T9.3.3.3.1.1" style="font-size:70%;">SiamRPN++ </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T9.3.3.3.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib17" title="">17</a><span class="ltx_text" id="S5.T9.3.3.3.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T9.3.3.3.2" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.3.3.2.1" style="font-size:70%;">61.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T9.3.3.3.3" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.3.3.3.1" style="font-size:70%;">62.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T9.3.3.3.4" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.3.3.4.1" style="font-size:70%;">63.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T9.3.3.3.5" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.3.3.5.1" style="font-size:70%;">65.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T9.3.3.3.6" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.3.3.6.1" style="font-size:70%;">63.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T9.3.3.3.7" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.3.3.7.1" style="font-size:70%;">63.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T9.3.3.3.8" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.3.3.8.1" style="font-size:70%;">66.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T9.3.3.3.9" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.3.3.9.1" style="font-size:70%;">68.3</span></td>
</tr>
<tr class="ltx_tr" id="S5.T9.3.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T9.3.4.4.1" style="padding-left:5.2pt;padding-right:5.2pt;">
<span class="ltx_text" id="S5.T9.3.4.4.1.1" style="font-size:70%;">STARK </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T9.3.4.4.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib46" title="">46</a><span class="ltx_text" id="S5.T9.3.4.4.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T9.3.4.4.2" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.4.4.2.1" style="font-size:70%;">59.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T9.3.4.4.3" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.4.4.3.1" style="font-size:70%;">59.9</span></td>
<td class="ltx_td ltx_align_center" id="S5.T9.3.4.4.4" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.4.4.4.1" style="font-size:70%;">61.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T9.3.4.4.5" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.4.4.5.1" style="font-size:70%;">63.1</span></td>
<td class="ltx_td ltx_align_center" id="S5.T9.3.4.4.6" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.4.4.6.1" style="font-size:70%;">61.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T9.3.4.4.7" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.4.4.7.1" style="font-size:70%;">61.5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T9.3.4.4.8" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.4.4.8.1" style="font-size:70%;">63.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T9.3.4.4.9" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.4.4.9.1" style="font-size:70%;">65.5</span></td>
</tr>
<tr class="ltx_tr" id="S5.T9.3.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T9.3.5.5.1" style="padding-left:5.2pt;padding-right:5.2pt;">
<span class="ltx_text" id="S5.T9.3.5.5.1.1" style="font-size:70%;">SwinTrack </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T9.3.5.5.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib23" title="">23</a><span class="ltx_text" id="S5.T9.3.5.5.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T9.3.5.5.2" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.5.5.2.1" style="font-size:70%;">61.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T9.3.5.5.3" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.5.5.3.1" style="font-size:70%;">62.1</span></td>
<td class="ltx_td ltx_align_center" id="S5.T9.3.5.5.4" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.5.5.4.1" style="font-size:70%;">64.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T9.3.5.5.5" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.5.5.5.1" style="font-size:70%;">65.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T9.3.5.5.6" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.5.5.6.1" style="font-size:70%;">63.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T9.3.5.5.7" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.5.5.7.1" style="font-size:70%;">63.7</span></td>
<td class="ltx_td ltx_align_center" id="S5.T9.3.5.5.8" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.5.5.8.1" style="font-size:70%;">66.2</span></td>
<td class="ltx_td ltx_align_center" id="S5.T9.3.5.5.9" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.5.5.9.1" style="font-size:70%;">68.1</span></td>
</tr>
<tr class="ltx_tr" id="S5.T9.3.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T9.3.6.6.1" style="padding-left:5.2pt;padding-right:5.2pt;">
<span class="ltx_text" id="S5.T9.3.6.6.1.1" style="font-size:70%;">ViTTrack </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T9.3.6.6.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="#bib.bib48" title="">48</a><span class="ltx_text" id="S5.T9.3.6.6.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T9.3.6.6.2" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.6.6.2.1" style="font-size:70%;">62.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T9.3.6.6.3" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.6.6.3.1" style="font-size:70%;">63.2</span></td>
<td class="ltx_td ltx_align_center" id="S5.T9.3.6.6.4" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.6.6.4.1" style="font-size:70%;">65.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T9.3.6.6.5" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.6.6.5.1" style="font-size:70%;">66.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T9.3.6.6.6" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.6.6.6.1" style="font-size:70%;">64.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T9.3.6.6.7" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.6.6.7.1" style="font-size:70%;">64.8</span></td>
<td class="ltx_td ltx_align_center" id="S5.T9.3.6.6.8" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.6.6.8.1" style="font-size:70%;">67.5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T9.3.6.6.9" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.6.6.9.1" style="font-size:70%;">69.4</span></td>
</tr>
<tr class="ltx_tr" id="S5.T9.3.7.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T9.3.7.7.1" style="padding-left:5.2pt;padding-right:5.2pt;">
<span class="ltx_text" id="S5.T9.3.7.7.1.1" style="font-size:70%;color:#000000;">ViTPoseTrack</span><span class="ltx_text" id="S5.T9.3.7.7.1.2" style="font-size:70%;">-B</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T9.3.7.7.2" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.7.7.2.1" style="font-size:70%;">62.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T9.3.7.7.3" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.7.7.3.1" style="font-size:70%;">62.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T9.3.7.7.4" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.7.7.4.1" style="font-size:70%;">64.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T9.3.7.7.5" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.7.7.5.1" style="font-size:70%;">65.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T9.3.7.7.6" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.7.7.6.1" style="font-size:70%;">63.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T9.3.7.7.7" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.7.7.7.1" style="font-size:70%;">64.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T9.3.7.7.8" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.7.7.8.1" style="font-size:70%;">66.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T9.3.7.7.9" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.7.7.9.1" style="font-size:70%;">68.8</span></td>
</tr>
<tr class="ltx_tr" id="S5.T9.3.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S5.T9.3.8.8.1" style="padding-left:5.2pt;padding-right:5.2pt;">
<span class="ltx_text" id="S5.T9.3.8.8.1.1" style="font-size:70%;color:#000000;">ViTPoseTrack</span><span class="ltx_text" id="S5.T9.3.8.8.1.2" style="font-size:70%;">-L</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T9.3.8.8.2" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.8.8.2.1" style="font-size:70%;">63.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T9.3.8.8.3" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.8.8.3.1" style="font-size:70%;">63.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T9.3.8.8.4" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.8.8.4.1" style="font-size:70%;">65.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T9.3.8.8.5" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.8.8.5.1" style="font-size:70%;">66.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T9.3.8.8.6" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.8.8.6.1" style="font-size:70%;">64.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T9.3.8.8.7" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.8.8.7.1" style="font-size:70%;">64.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T9.3.8.8.8" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.8.8.8.1" style="font-size:70%;">67.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T9.3.8.8.9" style="padding-left:5.2pt;padding-right:5.2pt;"><span class="ltx_text" id="S5.T9.3.8.8.9.1" style="font-size:70%;">69.7</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S5.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS6.5.1.1">V-F</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS6.6.2">Animal Pose Tracking (APT track)</span>
</h3>
<section class="ltx_subsubsection" id="S5.SS6.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S5.SS6.SSS1.5.1.1">V-F</span>1 </span>Setting</h4>
<div class="ltx_para" id="S5.SS6.SSS1.p1">
<p class="ltx_p" id="S5.SS6.SSS1.p1.1">In this track, we employ representative object trackers with both CNN-based and ViT-based backbones to track each animal instance throughout the video clips, using each animal’s ground truth bounding box in the first frame. Upon obtaining the tracked bounding boxes, the pose estimation models trained on the APTv2 training set are then applied for animal pose tracking. We also utilize the average precision (AP) metric for evaluation purposes.
For our baseline model <span class="ltx_text" id="S5.SS6.SSS1.p1.1.1" style="color:#000000;">ViTPoseTrack</span>, we provide both the base and large models for comparison. Note that the tracking results of <span class="ltx_text" id="S5.SS6.SSS1.p1.1.2" style="color:#000000;">ViTPoseTrack</span> can also be used by other pose estimation models to compare the tracking performance of <span class="ltx_text" id="S5.SS6.SSS1.p1.1.3" style="color:#000000;">ViTPoseTrack</span> and other trackers.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS6.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S5.SS6.SSS2.5.1.1">V-F</span>2 </span>Results and analysis</h4>
<div class="ltx_para" id="S5.SS6.SSS2.p1">
<p class="ltx_p" id="S5.SS6.SSS2.p1.1">The results on the entire validation set are presented in Table <a class="ltx_ref" href="#S5.T7" title="TABLE VII ‣ V-E2 Results and analysis ‣ V-E Low-data Training and Generalization (LT track) ‣ V Experiment ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_tag">VII</span></a>. The results on the easy and hard validation subsets are displayed separately in Table <a class="ltx_ref" href="#S5.T8" title="TABLE VIII ‣ V-E2 Results and analysis ‣ V-E Low-data Training and Generalization (LT track) ‣ V Experiment ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_tag">VIII</span></a> and Table <a class="ltx_ref" href="#S5.T9" title="TABLE IX ‣ V-E2 Results and analysis ‣ V-E Low-data Training and Generalization (LT track) ‣ V Experiment ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_tag">IX</span></a>, respectively.</p>
</div>
<div class="ltx_para" id="S5.SS6.SSS2.p2">
<p class="ltx_p" id="S5.SS6.SSS2.p2.2">Moreover, the proposed <span class="ltx_text" id="S5.SS6.SSS2.p2.2.1" style="color:#000000;">ViTPoseTrack</span> baseline method exhibits the best performance among all trackers, even with the backbone frozen (see the last column in Table <a class="ltx_ref" href="#S5.T7" title="TABLE VII ‣ V-E2 Results and analysis ‣ V-E Low-data Training and Generalization (LT track) ‣ V Experiment ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_tag">VII</span></a>). This validates the effectiveness of pose estimation methods in extracting discriminative features at both instance and keypoint levels, thus highlighting the excellent generalization capability of large-scale ViT models. In addition, the ViTTrack model, with a trainable backbone, records a slightly better performance than ViTPoseTrack-B with the same backbone which is however frozen, as shown by comparing the 4<math alttext="th" class="ltx_Math" display="inline" id="S5.SS6.SSS2.p2.1.m1.1"><semantics id="S5.SS6.SSS2.p2.1.m1.1a"><mrow id="S5.SS6.SSS2.p2.1.m1.1.1" xref="S5.SS6.SSS2.p2.1.m1.1.1.cmml"><mi id="S5.SS6.SSS2.p2.1.m1.1.1.2" xref="S5.SS6.SSS2.p2.1.m1.1.1.2.cmml">t</mi><mo id="S5.SS6.SSS2.p2.1.m1.1.1.1" xref="S5.SS6.SSS2.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS6.SSS2.p2.1.m1.1.1.3" xref="S5.SS6.SSS2.p2.1.m1.1.1.3.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS6.SSS2.p2.1.m1.1b"><apply id="S5.SS6.SSS2.p2.1.m1.1.1.cmml" xref="S5.SS6.SSS2.p2.1.m1.1.1"><times id="S5.SS6.SSS2.p2.1.m1.1.1.1.cmml" xref="S5.SS6.SSS2.p2.1.m1.1.1.1"></times><ci id="S5.SS6.SSS2.p2.1.m1.1.1.2.cmml" xref="S5.SS6.SSS2.p2.1.m1.1.1.2">𝑡</ci><ci id="S5.SS6.SSS2.p2.1.m1.1.1.3.cmml" xref="S5.SS6.SSS2.p2.1.m1.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.SSS2.p2.1.m1.1c">th</annotation><annotation encoding="application/x-llamapun" id="S5.SS6.SSS2.p2.1.m1.1d">italic_t italic_h</annotation></semantics></math> and 5<math alttext="th" class="ltx_Math" display="inline" id="S5.SS6.SSS2.p2.2.m2.1"><semantics id="S5.SS6.SSS2.p2.2.m2.1a"><mrow id="S5.SS6.SSS2.p2.2.m2.1.1" xref="S5.SS6.SSS2.p2.2.m2.1.1.cmml"><mi id="S5.SS6.SSS2.p2.2.m2.1.1.2" xref="S5.SS6.SSS2.p2.2.m2.1.1.2.cmml">t</mi><mo id="S5.SS6.SSS2.p2.2.m2.1.1.1" xref="S5.SS6.SSS2.p2.2.m2.1.1.1.cmml">⁢</mo><mi id="S5.SS6.SSS2.p2.2.m2.1.1.3" xref="S5.SS6.SSS2.p2.2.m2.1.1.3.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS6.SSS2.p2.2.m2.1b"><apply id="S5.SS6.SSS2.p2.2.m2.1.1.cmml" xref="S5.SS6.SSS2.p2.2.m2.1.1"><times id="S5.SS6.SSS2.p2.2.m2.1.1.1.cmml" xref="S5.SS6.SSS2.p2.2.m2.1.1.1"></times><ci id="S5.SS6.SSS2.p2.2.m2.1.1.2.cmml" xref="S5.SS6.SSS2.p2.2.m2.1.1.2">𝑡</ci><ci id="S5.SS6.SSS2.p2.2.m2.1.1.3.cmml" xref="S5.SS6.SSS2.p2.2.m2.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.SSS2.p2.2.m2.1c">th</annotation><annotation encoding="application/x-llamapun" id="S5.SS6.SSS2.p2.2.m2.1d">italic_t italic_h</annotation></semantics></math> rows in Table <a class="ltx_ref" href="#S5.T7" title="TABLE VII ‣ V-E2 Results and analysis ‣ V-E Low-data Training and Generalization (LT track) ‣ V Experiment ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_tag">VII</span></a>.
Interestingly, the two configurations show comparable performance on the easy subset (Table <a class="ltx_ref" href="#S5.T8" title="TABLE VIII ‣ V-E2 Results and analysis ‣ V-E Low-data Training and Generalization (LT track) ‣ V Experiment ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_tag">VIII</span></a>), while ViTTrack outperforms ViTPoseTrack-B by 0.8 average AP on the hard subset (Table <a class="ltx_ref" href="#S5.T9" title="TABLE IX ‣ V-E2 Results and analysis ‣ V-E Low-data Training and Generalization (LT track) ‣ V Experiment ‣ APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond"><span class="ltx_text ltx_ref_tag">IX</span></a>). This observation suggests that the APTv2 dataset provides a challenging benchmark for tracking methods, particularly within the hard subset. To further increase the model size of <span class="ltx_text" id="S5.SS6.SSS2.p2.2.2" style="color:#000000;">ViTPoseTrack</span>, our <span class="ltx_text" id="S5.SS6.SSS2.p2.2.3" style="color:#000000;">ViTPoseTrack</span>-L model with a ViT-Large backbone achieves the best performance, even if its backbone has not been trained on the tracking task.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We introduce APTv2, covering 30 diverse animal species and 41,235 annotated frames from challenging real-world videos. This dataset serves as the first comprehensive resource for animal pose estimation and tracking. Leveraging APTv2, we evaluate state-of-the-art pose estimation methods in three distinct settings: single-frame animal pose estimation, low-data training and generalization, and animal pose tracking, with a novel baseline method <span class="ltx_text" id="S6.p1.1.1" style="color:#000000;">ViTPoseTrack</span>. Our extensive experiments highlight the advantages of inter- and intra-domain pre-training for accurate animal pose estimation. Furthermore, we emphasize the promising potential of involving large-scale pre-training models, e.g., plain vision transformers, in animal pose tracking tasks, as well as the importance of collecting and annotating diverse animal species’ keypoints. By making APTv2 accessible, we aim to foster new avenues of research in animal pose estimation and tracking.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
M. Andriluka, U. Iqbal, E. Insafutdinov, L. Pishchulin, A. Milan, J. Gall, and
B. Schiele.

</span>
<span class="ltx_bibblock">Posetrack: A benchmark for human pose estimation and tracking.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 5167–5176, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
M. Andriluka, L. Pishchulin, P. Gehler, and B. Schiele.

</span>
<span class="ltx_bibblock">2d human pose estimation: New benchmark and state of the art
analysis.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 3686–3693, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
A. Arac, P. Zhao, B. H. Dobkin, S. T. Carmichael, and P. Golshani.

</span>
<span class="ltx_bibblock">Deepbehavior: A deep learning toolbox for automated analysis of
animal and human behavior imaging data.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">Frontiers in systems neuroscience</span>, 13:20, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
L. Bertinetto, J. Valmadre, J. F. Henriques, A. Vedaldi, and P. H. Torr.

</span>
<span class="ltx_bibblock">Fully-convolutional siamese networks for object tracking.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">Proceedings of the European Conference on Computer Vision</span>,
pages 850–865. Springer, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
J. Cao, H. Tang, H.-S. Fang, X. Shen, C. Lu, and Y.-W. Tai.

</span>
<span class="ltx_bibblock">Cross-domain adaptation for animal pose estimation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span>, October 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
J. Charles, T. Pfister, D. Magee, D. Hogg, and A. Zisserman.

</span>
<span class="ltx_bibblock">Personalizing human video pose estimation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 3063–3072, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
M. Contributors.

</span>
<span class="ltx_bibblock">Openmmlab pose estimation toolbox and benchmark.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/open-mmlab/mmpose" title="">https://github.com/open-mmlab/mmpose</a>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 248–255. Ieee, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al.

</span>
<span class="ltx_bibblock">An image is worth 16x16 words: Transformers for image recognition at
scale.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">International Conference on Learning Representations</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
D. Gai, R. Feng, W. Min, X. Yang, P. Su, Q. Wang, and Q. Han.

</span>
<span class="ltx_bibblock">Spatiotemporal learning transformer for video-based human pose
estimation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">IEEE Transactions on Circuits and Systems for Video Technology</span>,
2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
J. M. Graving, D. Chae, H. Naik, L. Li, B. Koger, B. R. Costelloe, and I. D.
Couzin.

</span>
<span class="ltx_bibblock">Deepposekit, a software toolkit for fast and robust animal pose
estimation using deep learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">Elife</span>, 8:e47994, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick.

</span>
<span class="ltx_bibblock">Masked autoencoders are scalable vision learners.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 16000–16009, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren, and J. Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 770–778, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
M. Hu, X. Zhu, H. Wang, S. Cao, C. Liu, and Q. Song.

</span>
<span class="ltx_bibblock">Stdformer: Spatial-temporal motion transformer for multiple object
tracking.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">IEEE Transactions on Circuits and Systems for Video Technology</span>,
2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
H. Jhuang, J. Gall, S. Zuffi, C. Schmid, and M. J. Black.

</span>
<span class="ltx_bibblock">Towards understanding action recognition.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span>, pages 3192–3199, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
R. Labuguen, J. Matsumoto, S. Negrete, H. Nishimaru, H. Nishijo, M. Takada,
Y. Go, K.-i. Inoue, and T. Shibata.

</span>
<span class="ltx_bibblock">Macaquepose: A novel ‘in the wild’macaque monkey pose dataset for
markerless motion capture.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">bioRxiv</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
B. Li, W. Wu, Q. Wang, F. Zhang, J. Xing, and J. Yan.

</span>
<span class="ltx_bibblock">Siamrpn++: Evolution of siamese visual tracking with very deep
networks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 4282–4291, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
J. Li, C. Wang, H. Zhu, Y. Mao, H.-S. Fang, and C. Lu.

</span>
<span class="ltx_bibblock">Crowdpose: Efficient crowded scenes pose estimation and a new
benchmark.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span>, pages 10863–10872, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
K. Li, S. Wang, X. Zhang, Y. Xu, W. Xu, and Z. Tu.

</span>
<span class="ltx_bibblock">Pose recognition with cascade transformers.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span>, pages 1944–1953, June 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
S. Li, J. Li, H. Tang, R. Qian, and W. Lin.

</span>
<span class="ltx_bibblock">Atrw: A benchmark for amur tiger re-identification in the wild.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">Proceedings of the 28th ACM International Conference on
Multimedia</span>, pages 2590–2598, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Y. Li, H. Mao, R. Girshick, and K. He.

</span>
<span class="ltx_bibblock">Exploring plain vision transformer backbones for object detection.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">Proceedings of the European Conference on Computer Vision
(ECCV)</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Y. Li, S. Zhang, Z. Wang, S. Yang, W. Yang, S.-T. Xia, and E. Zhou.

</span>
<span class="ltx_bibblock">Tokenpose: Learning keypoint tokens for human pose estimation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV)</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
L. Lin, H. Fan, Y. Xu, and H. Ling.

</span>
<span class="ltx_bibblock">Swintrack: A simple and strong baseline for transformer tracking.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2112.00995</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Dollár, and C. L. Zitnick.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">Proceedings of the European Conference on Computer Vision</span>,
pages 740–755. Springer, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo.

</span>
<span class="ltx_bibblock">Swin transformer: Hierarchical vision transformer using shifted
windows.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span>, pages 10012–10022, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
A. Mathis, T. Biasi, S. Schneider, M. Yuksekgonul, B. Rogers, M. Bethge, and
M. W. Mathis.

</span>
<span class="ltx_bibblock">Pretraining boosts out-of-domain robustness for pose estimation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">Proceedings of the IEEE/CVF Winter Conference on Applications
of Computer Vision</span>, pages 1859–1868, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
A. Newell, Z. Huang, and J. Deng.

</span>
<span class="ltx_bibblock">Segvit: Semantic segmentation with plain vision transformers.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">Zhang, Bowen and Tian, Zhi and Tang, Quan and Chu, Xiangxiang
and Wei, Xiaolin and Shen, Chunhua and Liu, Yifan</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
A. Newell, K. Yang, and J. Deng.

</span>
<span class="ltx_bibblock">Stacked hourglass networks for human pose estimation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">Proceedings of the European Conference on Computer Vision</span>,
pages 483–499. Springer, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
X. L. Ng, K. E. Ong, Q. Zheng, Y. Ni, S. Y. Yeo, and J. Liu.

</span>
<span class="ltx_bibblock">Animal kingdom: A large and diverse dataset for animal behavior
understanding.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 19023–19034, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
X. Ni, L. Yuan, and K. Lv.

</span>
<span class="ltx_bibblock">Efficient single-object tracker based on local-global feature fusion.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">IEEE Transactions on Circuits and Systems for Video Technology</span>,
2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
C. Papaioannidis, I. Mademlis, and I. Pitas.

</span>
<span class="ltx_bibblock">Fast cnn-based single-person 2d human pose estimation for autonomous
systems.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">IEEE Transactions on Circuits and Systems for Video Technology</span>,
33(3):1262–1275, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
T. D. Pereira, D. E. Aldarondo, L. Willmore, M. Kislin, S. S.-H. Wang,
M. Murthy, and J. W. Shaevitz.

</span>
<span class="ltx_bibblock">Fast animal pose estimation using deep neural networks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">Nature methods</span>, 16(1):117–125, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
B. Sapp, D. Weiss, and B. Taskar.

</span>
<span class="ltx_bibblock">Parsing human motion with stretchable models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 1281–1288, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
K. Sun, B. Xiao, D. Liu, and J. Wang.

</span>
<span class="ltx_bibblock">Deep high-resolution representation learning for human pose
estimation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 5693–5703, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Z. Tang, Y. Hao, J. Li, and R. Hong.

</span>
<span class="ltx_bibblock">Ftcm: Frequency-temporal collaborative module for efficient 3d human
pose estimation in video.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">IEEE Transactions on Circuits and Systems for Video Technology</span>,
2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
N. Wang, W. Zhou, J. Wang, and H. Li.

</span>
<span class="ltx_bibblock">Transformer meets tracker: Exploiting temporal context for robust
visual tracking.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 1571–1580, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
T. Wang, W. W. Ng, J. Li, Q. Wu, S. Zhang, C. Nugent, and C. Shewell.

</span>
<span class="ltx_bibblock">A deep clustering via automatic feature embedded learning for human
activity recognition.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">IEEE Transactions on Circuits and Systems for Video Technology</span>,
32(1):210–223, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Y.-J. Wang, Y.-M. Luo, G.-H. Bai, and J.-M. Guo.

</span>
<span class="ltx_bibblock">Uformpose: A u-shaped hierarchical multi-scale keypoint-aware
framework for human pose estimation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">IEEE Transactions on Circuits and Systems for Video Technology</span>,
33(4):1697–1709, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
N. Wojke, A. Bewley, and D. Paulus.

</span>
<span class="ltx_bibblock">Simple online and realtime tracking with a deep association metric.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">2017 IEEE international conference on image processing
(ICIP)</span>, pages 3645–3649. IEEE, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
H. Wu, X. Ma, and Y. Li.

</span>
<span class="ltx_bibblock">Spatiotemporal multimodal learning with 3d cnns for video action
recognition.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib40.1.1">IEEE Transactions on Circuits and Systems for Video Technology</span>,
32(3):1250–1261, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
B. Xiao, H. Wu, and Y. Wei.

</span>
<span class="ltx_bibblock">Simple baselines for human pose estimation and tracking.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">Proceedings of the European Conference on Computer Vision</span>,
pages 466–481, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
C. Xie, D. Zhang, Z. Wu, C. Yu, Y. Hu, and Y. Chen.

</span>
<span class="ltx_bibblock">Rpm 2.0: Rf-based pose machines for multi-person 3d pose estimation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib42.1.1">IEEE Transactions on Circuits and Systems for Video Technology</span>,
2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
N. Xu, L. Yang, Y. Fan, D. Yue, Y. Liang, J. Yang, and T. Huang.

</span>
<span class="ltx_bibblock">Youtube-vos: A large-scale video object segmentation benchmark.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib43.1.1">arXiv preprint arXiv:1809.03327</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Y. Xu, J. Zhang, Q. Zhang, and D. Tao.

</span>
<span class="ltx_bibblock">Vitpose: Simple vision transformer baselines for human pose
estimation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib44.1.1">Advances in Neural Information Processing Systems</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Y. Xu, Q. Zhang, J. Zhang, and D. Tao.

</span>
<span class="ltx_bibblock">Vitae: Vision transformer advanced by exploring intrinsic inductive
bias.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib45.1.1">Advances in Neural Information Processing Systems</span>, 34, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
B. Yan, H. Peng, J. Fu, D. Wang, and H. Lu.

</span>
<span class="ltx_bibblock">Learning spatio-temporal transformer for visual tracking.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib46.1.1">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span>, pages 10448–10457, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
S. Yang, Z. Quan, M. Nie, and W. Yang.

</span>
<span class="ltx_bibblock">Transpose: Keypoint localization via transformer.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib47.1.1">Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV)</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Y. Yang, J. Yang, Y. Xu, J. Zhang, L. Lan, and D. Tao.

</span>
<span class="ltx_bibblock">Apt-36k: A large-scale benchmark for animal pose estimation and
tracking.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib48.1.1">Advances in Neural Information Processing Systems</span>,
35:17301–17313, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
H. Yu, Y. Xu, J. Zhang, W. Zhao, Z. Guan, and D. Tao.

</span>
<span class="ltx_bibblock">Ap-10k: A benchmark for animal pose estimation in the wild.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib49.1.1">Thirty-fifth Conference on Neural Information Processing
Systems Datasets and Benchmarks Track (Round 2)</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Y. Yuan, R. Fu, L. Huang, W. Lin, C. Zhang, X. Chen, and J. Wang.

</span>
<span class="ltx_bibblock">Hrformer: High-resolution transformer for dense prediction.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib50.1.1">Advances in Neural Information Processing Systems</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
J. Zhang and D. Tao.

</span>
<span class="ltx_bibblock">Empowering things with intelligence: a survey of the progress,
challenges, and opportunities in artificial intelligence of things.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib51.1.1">IEEE Internet of Things Journal</span>, 8(10):7789–7817, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
L. Zhang, J. Gao, Z. Xiao, and H. Fan.

</span>
<span class="ltx_bibblock">Animaltrack: A large-scale benchmark for multi-animal tracking in the
wild.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib52.1.1">arXiv preprint arXiv:2205.00158</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
S.-H. Zhang, R. Li, X. Dong, P. Rosin, Z. Cai, X. Han, D. Yang, H. Huang, and
S.-M. Hu.

</span>
<span class="ltx_bibblock">Pose2seg: Detection free human instance segmentation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib53.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 889–898, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Y. Zhang, P. Sun, Y. Jiang, D. Yu, F. Weng, Z. Yuan, P. Luo, W. Liu, and
X. Wang.

</span>
<span class="ltx_bibblock">Bytetrack: Multi-object tracking by associating every detection box.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib54.1.1">European Conference on Computer Vision</span>, pages 1–21.
Springer, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Z. Zhang, H. Peng, J. Fu, B. Li, and W. Hu.

</span>
<span class="ltx_bibblock">Ocean: Object-aware anchor-free tracking.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib55.1.1">Proceedings of the European Conference on Computer Vision</span>,
pages 771–787. Springer, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Z. Zhou, X. Zhou, Z. Chen, P. Guo, Q.-Y. Liu, and W. Zhang.

</span>
<span class="ltx_bibblock">Memory network with pixel-level spatio-temporal learning for visual
object tracking.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib56.1.1">IEEE Transactions on Circuits and Systems for Video Technology</span>,
2023.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Dec 25 04:46:22 2023 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
