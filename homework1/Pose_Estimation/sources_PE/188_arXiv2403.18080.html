<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation</title>
<!--Generated on Thu Aug 15 17:05:49 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2403.18080v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S1" title="In EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S2" title="In EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S3" title="In EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S3.SS1" title="In 3 Method â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Two-stage Pose Estimator</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S3.SS2" title="In 3 Method â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Pose Proposal NetworkÂ (PPN)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S3.SS3" title="In 3 Method â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Pose Refinement TransformerÂ (PRFormer)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S3.SS4" title="In 3 Method â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Loss Function</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S3.SS5" title="In 3 Method â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Feature Extractor</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S4" title="In EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S4.SS1" title="In 4 Experiments â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Experiment Settings</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S4.SS2" title="In 4 Experiments â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Main Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S4.SS3" title="In 4 Experiments â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Ablation Studies and Discussions</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S5" title="In EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#Pt0.A1" title="In EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.A </span>Extra experiments</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#Pt0.A1.SS1" title="In Appendix 0.A Extra experiments â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.A.1 </span>2D heatmap pre-training.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#Pt0.A1.SS2" title="In Appendix 0.A Extra experiments â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.A.2 </span>Qualitative comparison with the baseline</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#Pt0.A1.SS3" title="In Appendix 0.A Extra experiments â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.A.3 </span>Qualitative failure cases</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#Pt0.A1.SS4" title="In Appendix 0.A Extra experiments â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.A.4 </span>Dependency of two stages</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#Pt0.A2" title="In EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.B </span>Potential ethical concerns</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>University of Edinburgh </span></span></span><span class="ltx_note ltx_role_institutetext" id="id2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Meta Reality Labs</span></span></span>
<h1 class="ltx_title ltx_title_document">EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chenhongyi Yang
</span><span class="ltx_author_notes">Work done when working as a research scientist intern at Meta Reality Labs1122</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Anastasia Tkach
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shreyas Hampali
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Linguang Zhang
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Elliot J. Crowley
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Cem Keskin
</span><span class="ltx_author_notes">22</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">We present EgoPoseFormer, a simple yet effective transformer-based model for stereo egocentric human pose estimation. The main challenge in egocentric pose estimation is overcoming joint invisibility, which is caused by self-occlusion or a limited field of view (FOV) of head-mounted cameras. Our approach overcomes this challenge by incorporating a two-stage pose estimation paradigm: in the first stage, our model leverages the global information to estimate each jointâ€™s coarse location, then in the second stage, it employs a DETR style transformer to refine the coarse locations by exploiting fine-grained stereo visual features. In addition, we present a Deformable Stereo AttentionÂ operation to enable our transformer to effectively process multi-view features, which enables it to accurately localize each joint in the 3D world. We evaluate our method on the stereo UnrealEgo dataset and show it significantly outperforms previous approaches while being computationally efficient: it improves MPJPE by 27.4mm (45% improvement) with only 7.9% model parameters and 13.1% FLOPs compared to the state-of-the-art. Surprisingly, with proper training settings, we find that even our first-stage pose proposal network can achieve superior performance compared to previous arts. We also show that our method can be seamlessly extended to monocular settings, which achieves state-of-the-art performance on the SceneEgo dataset, improving MPJPE by 25.5mm (21% improvement) compared to the best existing method with only 60.7% model parameters and 36.4% FLOPs. Code is available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ChenhongyiYang/egoposeformer" title="">https://github.com/ChenhongyiYang/egoposeformer</a>.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In the rapid expansion of Virtual Reality (VR) and Augmented Reality (AR) technologiesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib2" title="">2</a>]</cite>, the capability to accurately interpret and emulate human actions becomes increasingly crucial. Central to this pursuit is the egocentric pose estimation taskÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib14" title="">14</a>]</cite>, which aims to estimate the 3D body pose from a vantage point inherent to the user, predominantly from head-mounted cameras. Its precision is pivotal in a wide range of applications, such as gaming and virtual meetings, making it essential for crafting an immersive user experience for the next generation of VR/AR systems.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Different from outside-in pose estimation, where the human bodies are well covered by the images, a key challenge of egocentric pose estimation is the joint invisibility problem, which usually results from two causes. First, the limited field of view (FOV) of head-mounted cameras cannot fully capture the human bodyÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib32" title="">32</a>]</cite>, especially when hands and legs are stretched out. Another cause arises from the self-occlusion of different body partsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib32" title="">32</a>]</cite>, especially the lower body, which is very prone to be occluded by the main trunk. To overcome this limitation, some recent worksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib45" title="">45</a>]</cite> directly regress the 3D joint locations using the 2D heatmaps by employing an auto-encoder style architecture, which allows the locations of invisible joints to be inferred from the global information and other visible jointsâ€™ locations.
These approaches, however, take 2D heatmaps as input and cannot leverage the rich appearance information of the input image, which limits its 3D regression capability.
This also causes a poor scaling-up ability to the developed model, i.e., even if the model is equipped with a larger backbone network, its pose estimation accuracy cannot be improvedÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib3" title="">3</a>]</cite>. In another recent work, SceneEgoÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib37" title="">37</a>]</cite>, a 3D feature voxel grid was first built using fish-eye projectionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib28" title="">28</a>]</cite> with the help of depth and semantic maps, and 3D convolution was employed to operate on the feature grid to estimate each jointâ€™s location in a monocular setting.
Despite the fact that 3D joints outside the FOV can be estimated by leveraging a sufficiently large voxel grid, 3D convolutions are computationally expensive and the cost increases with the size of the voxel grid.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="151" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.3.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.4.2" style="font-size:80%;">Illustration of different egocentric pose estimation methods. While previous approaches predict jointsâ€™ locations via 2D heatmaps or 3D feature voxels, EgoPoseFormerÂ first estimates the coarse locations of each joint using a Pose Proposal NetworkÂ (PPN) and uses a transformer to refine the estimated pose.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this work, we propose Â <em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">EgoPoseFormer</em>, a simple transformer-based model for multi-view egocentric pose estimation. As illustrated inÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S1.F1" title="In 1 Introduction â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">1</span></a>, our method uses a two-stage frameworkÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib25" title="">25</a>]</cite> to overcome the joint invisibility challenge, and the body pose is predicted in a coarse-to-fine mannerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib38" title="">38</a>]</cite>. Specifically, the first stage of our model is the Pose Proposal NetworkÂ (PPN), which is a simple 2-layer MLP that leverages the global information of the input multi-view feature maps to predict a jointâ€™s coarse location. Similar to the recent heatmap-based methodsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib32" title="">32</a>]</cite>, the usage of global features allows our method to reason about the locations of all joints, including the invisible ones. Surprisingly, we found, with proper training settings, this simple MLP can already outperform previous state-of-the-art methods in stereo inputs are available. Then, in the second stage, we employed a DETR-styleÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib4" title="">4</a>]</cite> transformer, Pose Refinement TransformerÂ (PRFormer), to predict 3D refinement offsets related to the first stage estimations by exploiting the multi-view stereo features and human kinematic information. Specifically, we embed each jointâ€™s location and identity information into a Joint Query Token (JQT). Each JQT interacts with the multi-view image features and other JQTs through attention operations in each layer of PRFormer; subsequently, the refinement offsets are predicted from updated JQTs. Furthermore, we design a new Deformable Stereo AttentionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib48" title="">48</a>]</cite> to effectively process the fine-grained multi-view stereo features, which allows us to accurately estimate a jointâ€™s 3D location. In summary, we make three contributions:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We propose EgoPoseFormer, a simple transformer-based model for stereo egocentric pose estimation. Our model composes an MLP-based pose proposal network for computing coarse joint locations, which already demonstrates a strong accuracy, and a transformer-based pose refinement network to further improve the localization accuracy.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">Our method achieves state-of-the-art on the stereo UnrealEgo datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib3" title="">3</a>]</cite> by a huge advantage over previous arts with much lower computation costs.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We demonstrate that our method can be easily extended to the monocular egocentric pose estimation problem and achieve state-of-the-art performance on the SceneEgo datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib37" title="">37</a>]</cite>.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.2"><span class="ltx_text ltx_font_bold" id="S2.p1.2.1">Egocentric Pose Estimation.</span>
Prior to our work, there have been several works on egocentric pose estimation, which cover both monocular and stereo settings. Most previous approaches are based on predicting 3D joints locations from 2D heatmaps. For example, Mo<sup class="ltx_sup" id="S2.p1.2.2">2</sup>Cap<sup class="ltx_sup" id="S2.p1.2.3">2</sup>Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib41" title="">41</a>]</cite> first predicts the 2D joint heatmaps and their corresponding depth, and the 3D coordinates are computed with fish-eye unprojection. In xR-EgoPoseÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib32" title="">32</a>]</cite>, the 3D joint coordinates are directly estimated with an auto-encoder, whose input is the predicted joint heatmap. This allows it to tackle the joint invisibility difficulty. SelfPoseÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib31" title="">31</a>]</cite> improves xR-EgoPose by introducing joint rotation loss and UNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib27" title="">27</a>]</cite> backbone. EgoSTANÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib22" title="">22</a>]</cite> improves the quality of visual features by introducing temporal modeling. EgoGlassÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib45" title="">45</a>]</cite> extends the monocular heatmap-based methods to multi-view settings, where the jointsâ€™ locations are estimated from the multi-view joint heatmaps. It also adds an auxiliary segmentation loss to improve accuracy further. EgoPWÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib36" title="">36</a>]</cite> explores extending existing pose estimation methods to estimate body poses in a global space. UnrealEgoÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib3" title="">3</a>]</cite> further improves EgoGlass by introducing cross-view information exchange in the UNet decoder. The recently proposed Ego3DPoseÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib12" title="">12</a>]</cite> improves UnrealEgo by explicitly modeling limb heatmaps and orientations. Apart from heatmap-based approaches, SceneEgoÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib37" title="">37</a>]</cite> was recently introduced to directly predict joint locations by running 3D convolution on 3D feature voxels with the help of scene depth and segmentation. There are also worksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib43" title="">43</a>]</cite> about egocentric pose hallucination, where the headset wearerâ€™s body pose is estimated with front-facing cameras, in which the body is rarely observed. Different from ours, the focus of those works is generating body poses that are harmonious with the background scene. 
<br class="ltx_break"/></p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="397" id="S2.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.3.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text ltx_font_bold" id="S2.F2.4.2" style="font-size:80%;">(a)<span class="ltx_text ltx_font_medium" id="S2.F2.4.2.1"> An overview of the proposed EgoPoseFormer. The input of EgoPoseFormerÂ is the multi-view image features. In the first stage: Pose Proposal NetworkÂ (PPN), the multi-view features are globally pooled and concatenated, from which an MLP is used to estimate the coarse location of each joint (pose proposal). Then the jointsâ€™ identity and location information are embedded into Joint Query Tokens (JQTs) to serve as the queries in the second stage Pose Refinement TransformerÂ (PRFormer). In PRFormer, a JQT iteratively interacts with the stereo features and other JQTs to update itself through the attention mechanism. The updated JQTs are used to predict refinement offsets related to the pose proposal, yielding more accurate pose estimations. </span>(b)<span class="ltx_text ltx_font_medium" id="S2.F2.4.2.2"> The architecture of PRFormerÂ layer is similar to the transformer decoder layer, which includes a cross-attention block, a self-attention block, and a feed-forward network (FFN). However, in PRFormer, the cross-attention is replaced by the proposed Deformable Stereo AttentionÂ to better exploit stereo visual features.</span></span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">Transformer for Outside-in Pose Estimation.</span>
There have been lots of successful attempts to apply transformers for the outside-in body pose estimation.
One line of workÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib8" title="">8</a>]</cite> aims to develop high-performing transformer-based backbone networks for outside-in pose estimation. For example, PoseFormerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib47" title="">47</a>]</cite> introduces spatial and temporal attention mechanisms to generate high-quality visual features for 3D pose estimation, which is improved by the followed-up PoseFormer v2Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib46" title="">46</a>]</cite> by introducing frequency modelling. Furthermore, improvements in model efficiency are also elaborated inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib15" title="">15</a>]</cite>. The other line of work, similar to ours, focuses on developing DETR-styleÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib42" title="">42</a>]</cite> sparse transformers for human body pose estimation. For example, PETRÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib30" title="">30</a>]</cite> introduces inter-instance and intra-instance attention for accurate 2D multi-person pose estimation. PSVTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib23" title="">23</a>]</cite> introduces spatial-temporal encoder and decoder for 3D pose and body shape estimation. On the other hand, transformer architecture is also used for 2D human pose estimationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib19" title="">19</a>]</cite>. For example, GroupPoseÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib19" title="">19</a>]</cite> uses keypoint and instance queries to directly estimate the 2D human poses in a multi-person setting. Despite the huge success of transformer architectures in outside-in pose estimation, applying it to egocentric settings requires non-trivial adaptation due of the intrinsic difference between the two problems. For instance, in outside-in pose estimation, the human body usually lies within the cameraâ€™s FOV, while the out-of-FOV problems usually happen in egocentric pose estimation. Another difference is the input of those two tasks: most outside-in pose estimation models take regular images captured by pin-hole cameras as inputÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib17" title="">17</a>]</cite>, while in egocentric settings the images are usually captured by fish-eye camerasÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib37" title="">37</a>]</cite> to expand FOV, causing image distortions and posing further difficulties to the task.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we introduce the proposed EgoPoseFormerÂ in detail. We first discuss the motivation of our two-stage framework inÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S3.SS1" title="3.1 Two-stage Pose Estimator â€£ 3 Method â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Sec.</span>Â <span class="ltx_text ltx_ref_tag">3.1</span></a>. We present the Pose Proposal NetworkÂ in<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S3.SS2" title="3.2 Pose Proposal Network (PPN) â€£ 3 Method â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Sec.</span>Â <span class="ltx_text ltx_ref_tag">3.2</span></a> and the Pose Refinement TransformerÂ inÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S3.SS3" title="3.3 Pose Refinement Transformer (PRFormer) â€£ 3 Method â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Sec.</span>Â <span class="ltx_text ltx_ref_tag">3.3</span></a>. Later, we introduce the loss function inÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S3.SS4" title="3.4 Loss Function â€£ 3 Method â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Sec.</span>Â <span class="ltx_text ltx_ref_tag">3.4</span></a> and the feature extractor inÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S3.SS5" title="3.5 Feature Extractor â€£ 3 Method â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Sec.</span>Â <span class="ltx_text ltx_ref_tag">3.5</span></a>. An overview of our model is illustrated inÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S2.F2" title="In 2 Related Work â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">2</span></a> (a).</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Two-stage Pose Estimator</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S2.F2" title="In 2 Related Work â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">2</span></a> (a) shows the proposed two-stage framework, which is designed to overcome the joint invisibility challenge caused by self-occlusion or the limited FOV of head-mounted cameras.
In the first stage, we estimate the coarse location of each joint, which we call <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.1">pose proposal</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib25" title="">25</a>]</cite>, by utilizing the global feature pooled from the stereo features. This design enables the network to roughly localize all joints, including the invisible ones, by jointly reasoning visual clues from visible joints and background scenes.
The global feature, however, does not preserve fine-grained local details that enable more accurate joint localization.
Motivated by this observation, we added a second stage to refine the pose proposal, where we use a transformer that exploits fine-grained stereo features and the body kinematic information through the attention mechanism. Finally, the refined poses are output as the final pose estimation results.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Pose Proposal NetworkÂ (PPN)</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.7">Given the multi-view image features <math alttext="\mathbf{F}\in\mathbb{R}^{V\times H\times W\times C}" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">ğ…</mi><mo id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.p1.1.m1.1.1.3.2" xref="S3.SS2.p1.1.m1.1.1.3.2.cmml">â„</mi><mrow id="S3.SS2.p1.1.m1.1.1.3.3" xref="S3.SS2.p1.1.m1.1.1.3.3.cmml"><mi id="S3.SS2.p1.1.m1.1.1.3.3.2" xref="S3.SS2.p1.1.m1.1.1.3.3.2.cmml">V</mi><mo id="S3.SS2.p1.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.1.m1.1.1.3.3.1.cmml">Ã—</mo><mi id="S3.SS2.p1.1.m1.1.1.3.3.3" xref="S3.SS2.p1.1.m1.1.1.3.3.3.cmml">H</mi><mo id="S3.SS2.p1.1.m1.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.1.m1.1.1.3.3.1.cmml">Ã—</mo><mi id="S3.SS2.p1.1.m1.1.1.3.3.4" xref="S3.SS2.p1.1.m1.1.1.3.3.4.cmml">W</mi><mo id="S3.SS2.p1.1.m1.1.1.3.3.1b" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.1.m1.1.1.3.3.1.cmml">Ã—</mo><mi id="S3.SS2.p1.1.m1.1.1.3.3.5" xref="S3.SS2.p1.1.m1.1.1.3.3.5.cmml">C</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><in id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1"></in><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">ğ…</ci><apply id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.p1.1.m1.1.1.3.2">â„</ci><apply id="S3.SS2.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3"><times id="S3.SS2.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3.1"></times><ci id="S3.SS2.p1.1.m1.1.1.3.3.2.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3.2">ğ‘‰</ci><ci id="S3.SS2.p1.1.m1.1.1.3.3.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3.3">ğ»</ci><ci id="S3.SS2.p1.1.m1.1.1.3.3.4.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3.4">ğ‘Š</ci><ci id="S3.SS2.p1.1.m1.1.1.3.3.5.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3.5">ğ¶</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\mathbf{F}\in\mathbb{R}^{V\times H\times W\times C}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">bold_F âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_V Ã— italic_H Ã— italic_W Ã— italic_C end_POSTSUPERSCRIPT</annotation></semantics></math>, the PPNÂ computes the pose proposal <math alttext="P^{0}\in\mathbb{R}^{N_{j}\times 3}" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><mrow id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><msup id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2.2" xref="S3.SS2.p1.2.m2.1.1.2.2.cmml">P</mi><mn id="S3.SS2.p1.2.m2.1.1.2.3" xref="S3.SS2.p1.2.m2.1.1.2.3.cmml">0</mn></msup><mo id="S3.SS2.p1.2.m2.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml"><mi id="S3.SS2.p1.2.m2.1.1.3.2" xref="S3.SS2.p1.2.m2.1.1.3.2.cmml">â„</mi><mrow id="S3.SS2.p1.2.m2.1.1.3.3" xref="S3.SS2.p1.2.m2.1.1.3.3.cmml"><msub id="S3.SS2.p1.2.m2.1.1.3.3.2" xref="S3.SS2.p1.2.m2.1.1.3.3.2.cmml"><mi id="S3.SS2.p1.2.m2.1.1.3.3.2.2" xref="S3.SS2.p1.2.m2.1.1.3.3.2.2.cmml">N</mi><mi id="S3.SS2.p1.2.m2.1.1.3.3.2.3" xref="S3.SS2.p1.2.m2.1.1.3.3.2.3.cmml">j</mi></msub><mo id="S3.SS2.p1.2.m2.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.2.m2.1.1.3.3.1.cmml">Ã—</mo><mn id="S3.SS2.p1.2.m2.1.1.3.3.3" xref="S3.SS2.p1.2.m2.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><in id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1"></in><apply id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.2.1.cmml" xref="S3.SS2.p1.2.m2.1.1.2">superscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.2.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2.2">ğ‘ƒ</ci><cn id="S3.SS2.p1.2.m2.1.1.2.3.cmml" type="integer" xref="S3.SS2.p1.2.m2.1.1.2.3">0</cn></apply><apply id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.3.1.cmml" xref="S3.SS2.p1.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.3.2.cmml" xref="S3.SS2.p1.2.m2.1.1.3.2">â„</ci><apply id="S3.SS2.p1.2.m2.1.1.3.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3"><times id="S3.SS2.p1.2.m2.1.1.3.3.1.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3.1"></times><apply id="S3.SS2.p1.2.m2.1.1.3.3.2.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.3.3.2.1.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3.2">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.3.3.2.2.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3.2.2">ğ‘</ci><ci id="S3.SS2.p1.2.m2.1.1.3.3.2.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3.2.3">ğ‘—</ci></apply><cn id="S3.SS2.p1.2.m2.1.1.3.3.3.cmml" type="integer" xref="S3.SS2.p1.2.m2.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">P^{0}\in\mathbb{R}^{N_{j}\times 3}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">italic_P start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT Ã— 3 end_POSTSUPERSCRIPT</annotation></semantics></math> using global information. Here <math alttext="V" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3.1"><semantics id="S3.SS2.p1.3.m3.1a"><mi id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><ci id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">ğ‘‰</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">V</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.3.m3.1d">italic_V</annotation></semantics></math> denotes the number of views; <math alttext="H" class="ltx_Math" display="inline" id="S3.SS2.p1.4.m4.1"><semantics id="S3.SS2.p1.4.m4.1a"><mi id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><ci id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">ğ»</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">H</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.4.m4.1d">italic_H</annotation></semantics></math>, <math alttext="W" class="ltx_Math" display="inline" id="S3.SS2.p1.5.m5.1"><semantics id="S3.SS2.p1.5.m5.1a"><mi id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><ci id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">ğ‘Š</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">W</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.5.m5.1d">italic_W</annotation></semantics></math> and <math alttext="C" class="ltx_Math" display="inline" id="S3.SS2.p1.6.m6.1"><semantics id="S3.SS2.p1.6.m6.1a"><mi id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><ci id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1">ğ¶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">C</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.6.m6.1d">italic_C</annotation></semantics></math> denotes the height, width and channel number of the image feature maps; <math alttext="N_{j}" class="ltx_Math" display="inline" id="S3.SS2.p1.7.m7.1"><semantics id="S3.SS2.p1.7.m7.1a"><msub id="S3.SS2.p1.7.m7.1.1" xref="S3.SS2.p1.7.m7.1.1.cmml"><mi id="S3.SS2.p1.7.m7.1.1.2" xref="S3.SS2.p1.7.m7.1.1.2.cmml">N</mi><mi id="S3.SS2.p1.7.m7.1.1.3" xref="S3.SS2.p1.7.m7.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m7.1b"><apply id="S3.SS2.p1.7.m7.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.7.m7.1.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1">subscript</csymbol><ci id="S3.SS2.p1.7.m7.1.1.2.cmml" xref="S3.SS2.p1.7.m7.1.1.2">ğ‘</ci><ci id="S3.SS2.p1.7.m7.1.1.3.cmml" xref="S3.SS2.p1.7.m7.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m7.1c">N_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.7.m7.1d">italic_N start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> denotes the number of body joints. Specifically, PPNÂ begins by applying a global average pooling to the feature maps of each view, then the resultant averaged features are concatenated to form a unified feature representation, which captures the salient global features and stereo information across views. We then predict each jointâ€™s initial 3D location with a 2-layer MLP with GELU activation:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="Pt0.A2.EGx1">
<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle P^{0}=\mathrm{MLP}_{\mathrm{ppn}}\Bigl{(}\mathrm{Concat}_{\{k\}}%
\bigl{(}\mathrm{AvgPool}(F_{k})\bigr{)}\Bigr{)}" class="ltx_Math" display="inline" id="S3.E1.m1.2"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml"><msup id="S3.E1.m1.2.2.3" xref="S3.E1.m1.2.2.3.cmml"><mi id="S3.E1.m1.2.2.3.2" xref="S3.E1.m1.2.2.3.2.cmml">P</mi><mn id="S3.E1.m1.2.2.3.3" xref="S3.E1.m1.2.2.3.3.cmml">0</mn></msup><mo id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml">=</mo><mrow id="S3.E1.m1.2.2.1" xref="S3.E1.m1.2.2.1.cmml"><msub id="S3.E1.m1.2.2.1.3" xref="S3.E1.m1.2.2.1.3.cmml"><mi id="S3.E1.m1.2.2.1.3.2" xref="S3.E1.m1.2.2.1.3.2.cmml">MLP</mi><mi id="S3.E1.m1.2.2.1.3.3" xref="S3.E1.m1.2.2.1.3.3.cmml">ppn</mi></msub><mo id="S3.E1.m1.2.2.1.2" xref="S3.E1.m1.2.2.1.2.cmml">â¢</mo><mrow id="S3.E1.m1.2.2.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.cmml"><mo id="S3.E1.m1.2.2.1.1.1.2" maxsize="160%" minsize="160%" xref="S3.E1.m1.2.2.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.2.2.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.cmml"><msub id="S3.E1.m1.2.2.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.3.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.3.2" xref="S3.E1.m1.2.2.1.1.1.1.3.2.cmml">Concat</mi><mrow id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.1.1.1.2.cmml"><mo id="S3.E1.m1.1.1.1.3.1" stretchy="false" xref="S3.E1.m1.1.1.1.2.cmml">{</mo><mi id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">k</mi><mo id="S3.E1.m1.1.1.1.3.2" stretchy="false" xref="S3.E1.m1.1.1.1.2.cmml">}</mo></mrow></msub><mo id="S3.E1.m1.2.2.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.2.cmml">â¢</mo><mrow id="S3.E1.m1.2.2.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.2.2.1.1.1.1.1.1.2" maxsize="120%" minsize="120%" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.2.2.1.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml">AvgPool</mi><mo id="S3.E1.m1.2.2.1.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml">F</mi><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.cmml">k</mi></msub><mo id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.2.2.1.1.1.1.1.1.3" maxsize="120%" minsize="120%" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.2.2.1.1.1.3" maxsize="160%" minsize="160%" xref="S3.E1.m1.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2"><eq id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2"></eq><apply id="S3.E1.m1.2.2.3.cmml" xref="S3.E1.m1.2.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.3.1.cmml" xref="S3.E1.m1.2.2.3">superscript</csymbol><ci id="S3.E1.m1.2.2.3.2.cmml" xref="S3.E1.m1.2.2.3.2">ğ‘ƒ</ci><cn id="S3.E1.m1.2.2.3.3.cmml" type="integer" xref="S3.E1.m1.2.2.3.3">0</cn></apply><apply id="S3.E1.m1.2.2.1.cmml" xref="S3.E1.m1.2.2.1"><times id="S3.E1.m1.2.2.1.2.cmml" xref="S3.E1.m1.2.2.1.2"></times><apply id="S3.E1.m1.2.2.1.3.cmml" xref="S3.E1.m1.2.2.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.3.1.cmml" xref="S3.E1.m1.2.2.1.3">subscript</csymbol><ci id="S3.E1.m1.2.2.1.3.2.cmml" xref="S3.E1.m1.2.2.1.3.2">MLP</ci><ci id="S3.E1.m1.2.2.1.3.3.cmml" xref="S3.E1.m1.2.2.1.3.3">ppn</ci></apply><apply id="S3.E1.m1.2.2.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1"><times id="S3.E1.m1.2.2.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.2"></times><apply id="S3.E1.m1.2.2.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.3.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.1.3.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.3.2">Concat</ci><set id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.3"><ci id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">ğ‘˜</ci></set></apply><apply id="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1"><times id="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.2"></times><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3">AvgPool</ci><apply id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.2">ğ¹</ci><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.3">ğ‘˜</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">\displaystyle P^{0}=\mathrm{MLP}_{\mathrm{ppn}}\Bigl{(}\mathrm{Concat}_{\{k\}}%
\bigl{(}\mathrm{AvgPool}(F_{k})\bigr{)}\Bigr{)}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.2d">italic_P start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT = roman_MLP start_POSTSUBSCRIPT roman_ppn end_POSTSUBSCRIPT ( roman_Concat start_POSTSUBSCRIPT { italic_k } end_POSTSUBSCRIPT ( roman_AvgPool ( italic_F start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p1.8">where <math alttext="k\in\{1,...,V\}" class="ltx_Math" display="inline" id="S3.SS2.p1.8.m1.3"><semantics id="S3.SS2.p1.8.m1.3a"><mrow id="S3.SS2.p1.8.m1.3.4" xref="S3.SS2.p1.8.m1.3.4.cmml"><mi id="S3.SS2.p1.8.m1.3.4.2" xref="S3.SS2.p1.8.m1.3.4.2.cmml">k</mi><mo id="S3.SS2.p1.8.m1.3.4.1" xref="S3.SS2.p1.8.m1.3.4.1.cmml">âˆˆ</mo><mrow id="S3.SS2.p1.8.m1.3.4.3.2" xref="S3.SS2.p1.8.m1.3.4.3.1.cmml"><mo id="S3.SS2.p1.8.m1.3.4.3.2.1" stretchy="false" xref="S3.SS2.p1.8.m1.3.4.3.1.cmml">{</mo><mn id="S3.SS2.p1.8.m1.1.1" xref="S3.SS2.p1.8.m1.1.1.cmml">1</mn><mo id="S3.SS2.p1.8.m1.3.4.3.2.2" xref="S3.SS2.p1.8.m1.3.4.3.1.cmml">,</mo><mi id="S3.SS2.p1.8.m1.2.2" mathvariant="normal" xref="S3.SS2.p1.8.m1.2.2.cmml">â€¦</mi><mo id="S3.SS2.p1.8.m1.3.4.3.2.3" xref="S3.SS2.p1.8.m1.3.4.3.1.cmml">,</mo><mi id="S3.SS2.p1.8.m1.3.3" xref="S3.SS2.p1.8.m1.3.3.cmml">V</mi><mo id="S3.SS2.p1.8.m1.3.4.3.2.4" stretchy="false" xref="S3.SS2.p1.8.m1.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.8.m1.3b"><apply id="S3.SS2.p1.8.m1.3.4.cmml" xref="S3.SS2.p1.8.m1.3.4"><in id="S3.SS2.p1.8.m1.3.4.1.cmml" xref="S3.SS2.p1.8.m1.3.4.1"></in><ci id="S3.SS2.p1.8.m1.3.4.2.cmml" xref="S3.SS2.p1.8.m1.3.4.2">ğ‘˜</ci><set id="S3.SS2.p1.8.m1.3.4.3.1.cmml" xref="S3.SS2.p1.8.m1.3.4.3.2"><cn id="S3.SS2.p1.8.m1.1.1.cmml" type="integer" xref="S3.SS2.p1.8.m1.1.1">1</cn><ci id="S3.SS2.p1.8.m1.2.2.cmml" xref="S3.SS2.p1.8.m1.2.2">â€¦</ci><ci id="S3.SS2.p1.8.m1.3.3.cmml" xref="S3.SS2.p1.8.m1.3.3">ğ‘‰</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.8.m1.3c">k\in\{1,...,V\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.8.m1.3d">italic_k âˆˆ { 1 , â€¦ , italic_V }</annotation></semantics></math>. As we will show inÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S4.SS3" title="4.3 Ablation Studies and Discussions â€£ 4 Experiments â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Sec.</span>Â <span class="ltx_text ltx_ref_tag">4.3</span></a>, this simple design can already provide reasonable location estimation for each joint.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Pose Refinement TransformerÂ (PRFormer)</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.9">The PRFormerÂ takes the multi-view image features <math alttext="\mathbf{F}\in\mathbb{R}^{V\times H\times W\times C}" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mrow id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">ğ…</mi><mo id="S3.SS3.p1.1.m1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml"><mi id="S3.SS3.p1.1.m1.1.1.3.2" xref="S3.SS3.p1.1.m1.1.1.3.2.cmml">â„</mi><mrow id="S3.SS3.p1.1.m1.1.1.3.3" xref="S3.SS3.p1.1.m1.1.1.3.3.cmml"><mi id="S3.SS3.p1.1.m1.1.1.3.3.2" xref="S3.SS3.p1.1.m1.1.1.3.3.2.cmml">V</mi><mo id="S3.SS3.p1.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p1.1.m1.1.1.3.3.1.cmml">Ã—</mo><mi id="S3.SS3.p1.1.m1.1.1.3.3.3" xref="S3.SS3.p1.1.m1.1.1.3.3.3.cmml">H</mi><mo id="S3.SS3.p1.1.m1.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p1.1.m1.1.1.3.3.1.cmml">Ã—</mo><mi id="S3.SS3.p1.1.m1.1.1.3.3.4" xref="S3.SS3.p1.1.m1.1.1.3.3.4.cmml">W</mi><mo id="S3.SS3.p1.1.m1.1.1.3.3.1b" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p1.1.m1.1.1.3.3.1.cmml">Ã—</mo><mi id="S3.SS3.p1.1.m1.1.1.3.3.5" xref="S3.SS3.p1.1.m1.1.1.3.3.5.cmml">C</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><in id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1"></in><ci id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">ğ…</ci><apply id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.3.1.cmml" xref="S3.SS3.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.3.2.cmml" xref="S3.SS3.p1.1.m1.1.1.3.2">â„</ci><apply id="S3.SS3.p1.1.m1.1.1.3.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3.3"><times id="S3.SS3.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS3.p1.1.m1.1.1.3.3.1"></times><ci id="S3.SS3.p1.1.m1.1.1.3.3.2.cmml" xref="S3.SS3.p1.1.m1.1.1.3.3.2">ğ‘‰</ci><ci id="S3.SS3.p1.1.m1.1.1.3.3.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3.3.3">ğ»</ci><ci id="S3.SS3.p1.1.m1.1.1.3.3.4.cmml" xref="S3.SS3.p1.1.m1.1.1.3.3.4">ğ‘Š</ci><ci id="S3.SS3.p1.1.m1.1.1.3.3.5.cmml" xref="S3.SS3.p1.1.m1.1.1.3.3.5">ğ¶</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">\mathbf{F}\in\mathbb{R}^{V\times H\times W\times C}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">bold_F âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_V Ã— italic_H Ã— italic_W Ã— italic_C end_POSTSUPERSCRIPT</annotation></semantics></math> and the pose proposal <math alttext="P^{0}\in\mathbb{R}^{N_{j}\times 3}" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2.1"><semantics id="S3.SS3.p1.2.m2.1a"><mrow id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml"><msup id="S3.SS3.p1.2.m2.1.1.2" xref="S3.SS3.p1.2.m2.1.1.2.cmml"><mi id="S3.SS3.p1.2.m2.1.1.2.2" xref="S3.SS3.p1.2.m2.1.1.2.2.cmml">P</mi><mn id="S3.SS3.p1.2.m2.1.1.2.3" xref="S3.SS3.p1.2.m2.1.1.2.3.cmml">0</mn></msup><mo id="S3.SS3.p1.2.m2.1.1.1" xref="S3.SS3.p1.2.m2.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS3.p1.2.m2.1.1.3" xref="S3.SS3.p1.2.m2.1.1.3.cmml"><mi id="S3.SS3.p1.2.m2.1.1.3.2" xref="S3.SS3.p1.2.m2.1.1.3.2.cmml">â„</mi><mrow id="S3.SS3.p1.2.m2.1.1.3.3" xref="S3.SS3.p1.2.m2.1.1.3.3.cmml"><msub id="S3.SS3.p1.2.m2.1.1.3.3.2" xref="S3.SS3.p1.2.m2.1.1.3.3.2.cmml"><mi id="S3.SS3.p1.2.m2.1.1.3.3.2.2" xref="S3.SS3.p1.2.m2.1.1.3.3.2.2.cmml">N</mi><mi id="S3.SS3.p1.2.m2.1.1.3.3.2.3" xref="S3.SS3.p1.2.m2.1.1.3.3.2.3.cmml">j</mi></msub><mo id="S3.SS3.p1.2.m2.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p1.2.m2.1.1.3.3.1.cmml">Ã—</mo><mn id="S3.SS3.p1.2.m2.1.1.3.3.3" xref="S3.SS3.p1.2.m2.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><in id="S3.SS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1"></in><apply id="S3.SS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.2.1.cmml" xref="S3.SS3.p1.2.m2.1.1.2">superscript</csymbol><ci id="S3.SS3.p1.2.m2.1.1.2.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2.2">ğ‘ƒ</ci><cn id="S3.SS3.p1.2.m2.1.1.2.3.cmml" type="integer" xref="S3.SS3.p1.2.m2.1.1.2.3">0</cn></apply><apply id="S3.SS3.p1.2.m2.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.3.1.cmml" xref="S3.SS3.p1.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS3.p1.2.m2.1.1.3.2.cmml" xref="S3.SS3.p1.2.m2.1.1.3.2">â„</ci><apply id="S3.SS3.p1.2.m2.1.1.3.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3.3"><times id="S3.SS3.p1.2.m2.1.1.3.3.1.cmml" xref="S3.SS3.p1.2.m2.1.1.3.3.1"></times><apply id="S3.SS3.p1.2.m2.1.1.3.3.2.cmml" xref="S3.SS3.p1.2.m2.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.3.3.2.1.cmml" xref="S3.SS3.p1.2.m2.1.1.3.3.2">subscript</csymbol><ci id="S3.SS3.p1.2.m2.1.1.3.3.2.2.cmml" xref="S3.SS3.p1.2.m2.1.1.3.3.2.2">ğ‘</ci><ci id="S3.SS3.p1.2.m2.1.1.3.3.2.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3.3.2.3">ğ‘—</ci></apply><cn id="S3.SS3.p1.2.m2.1.1.3.3.3.cmml" type="integer" xref="S3.SS3.p1.2.m2.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">P^{0}\in\mathbb{R}^{N_{j}\times 3}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m2.1d">italic_P start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT Ã— 3 end_POSTSUPERSCRIPT</annotation></semantics></math> as inputs. Structurally, the transformer comprises <math alttext="S" class="ltx_Math" display="inline" id="S3.SS3.p1.3.m3.1"><semantics id="S3.SS3.p1.3.m3.1a"><mi id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><ci id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">S</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.3.m3.1d">italic_S</annotation></semantics></math> layers. At each layer <math alttext="s" class="ltx_Math" display="inline" id="S3.SS3.p1.4.m4.1"><semantics id="S3.SS3.p1.4.m4.1a"><mi id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><ci id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1">ğ‘ </ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.4.m4.1d">italic_s</annotation></semantics></math>, a refinement offset <math alttext="\Delta P^{s}\in\mathbb{R}^{N_{j}\times 3}" class="ltx_Math" display="inline" id="S3.SS3.p1.5.m5.1"><semantics id="S3.SS3.p1.5.m5.1a"><mrow id="S3.SS3.p1.5.m5.1.1" xref="S3.SS3.p1.5.m5.1.1.cmml"><mrow id="S3.SS3.p1.5.m5.1.1.2" xref="S3.SS3.p1.5.m5.1.1.2.cmml"><mi id="S3.SS3.p1.5.m5.1.1.2.2" mathvariant="normal" xref="S3.SS3.p1.5.m5.1.1.2.2.cmml">Î”</mi><mo id="S3.SS3.p1.5.m5.1.1.2.1" xref="S3.SS3.p1.5.m5.1.1.2.1.cmml">â¢</mo><msup id="S3.SS3.p1.5.m5.1.1.2.3" xref="S3.SS3.p1.5.m5.1.1.2.3.cmml"><mi id="S3.SS3.p1.5.m5.1.1.2.3.2" xref="S3.SS3.p1.5.m5.1.1.2.3.2.cmml">P</mi><mi id="S3.SS3.p1.5.m5.1.1.2.3.3" xref="S3.SS3.p1.5.m5.1.1.2.3.3.cmml">s</mi></msup></mrow><mo id="S3.SS3.p1.5.m5.1.1.1" xref="S3.SS3.p1.5.m5.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS3.p1.5.m5.1.1.3" xref="S3.SS3.p1.5.m5.1.1.3.cmml"><mi id="S3.SS3.p1.5.m5.1.1.3.2" xref="S3.SS3.p1.5.m5.1.1.3.2.cmml">â„</mi><mrow id="S3.SS3.p1.5.m5.1.1.3.3" xref="S3.SS3.p1.5.m5.1.1.3.3.cmml"><msub id="S3.SS3.p1.5.m5.1.1.3.3.2" xref="S3.SS3.p1.5.m5.1.1.3.3.2.cmml"><mi id="S3.SS3.p1.5.m5.1.1.3.3.2.2" xref="S3.SS3.p1.5.m5.1.1.3.3.2.2.cmml">N</mi><mi id="S3.SS3.p1.5.m5.1.1.3.3.2.3" xref="S3.SS3.p1.5.m5.1.1.3.3.2.3.cmml">j</mi></msub><mo id="S3.SS3.p1.5.m5.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p1.5.m5.1.1.3.3.1.cmml">Ã—</mo><mn id="S3.SS3.p1.5.m5.1.1.3.3.3" xref="S3.SS3.p1.5.m5.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m5.1b"><apply id="S3.SS3.p1.5.m5.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1"><in id="S3.SS3.p1.5.m5.1.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1.1"></in><apply id="S3.SS3.p1.5.m5.1.1.2.cmml" xref="S3.SS3.p1.5.m5.1.1.2"><times id="S3.SS3.p1.5.m5.1.1.2.1.cmml" xref="S3.SS3.p1.5.m5.1.1.2.1"></times><ci id="S3.SS3.p1.5.m5.1.1.2.2.cmml" xref="S3.SS3.p1.5.m5.1.1.2.2">Î”</ci><apply id="S3.SS3.p1.5.m5.1.1.2.3.cmml" xref="S3.SS3.p1.5.m5.1.1.2.3"><csymbol cd="ambiguous" id="S3.SS3.p1.5.m5.1.1.2.3.1.cmml" xref="S3.SS3.p1.5.m5.1.1.2.3">superscript</csymbol><ci id="S3.SS3.p1.5.m5.1.1.2.3.2.cmml" xref="S3.SS3.p1.5.m5.1.1.2.3.2">ğ‘ƒ</ci><ci id="S3.SS3.p1.5.m5.1.1.2.3.3.cmml" xref="S3.SS3.p1.5.m5.1.1.2.3.3">ğ‘ </ci></apply></apply><apply id="S3.SS3.p1.5.m5.1.1.3.cmml" xref="S3.SS3.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p1.5.m5.1.1.3.1.cmml" xref="S3.SS3.p1.5.m5.1.1.3">superscript</csymbol><ci id="S3.SS3.p1.5.m5.1.1.3.2.cmml" xref="S3.SS3.p1.5.m5.1.1.3.2">â„</ci><apply id="S3.SS3.p1.5.m5.1.1.3.3.cmml" xref="S3.SS3.p1.5.m5.1.1.3.3"><times id="S3.SS3.p1.5.m5.1.1.3.3.1.cmml" xref="S3.SS3.p1.5.m5.1.1.3.3.1"></times><apply id="S3.SS3.p1.5.m5.1.1.3.3.2.cmml" xref="S3.SS3.p1.5.m5.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS3.p1.5.m5.1.1.3.3.2.1.cmml" xref="S3.SS3.p1.5.m5.1.1.3.3.2">subscript</csymbol><ci id="S3.SS3.p1.5.m5.1.1.3.3.2.2.cmml" xref="S3.SS3.p1.5.m5.1.1.3.3.2.2">ğ‘</ci><ci id="S3.SS3.p1.5.m5.1.1.3.3.2.3.cmml" xref="S3.SS3.p1.5.m5.1.1.3.3.2.3">ğ‘—</ci></apply><cn id="S3.SS3.p1.5.m5.1.1.3.3.3.cmml" type="integer" xref="S3.SS3.p1.5.m5.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m5.1c">\Delta P^{s}\in\mathbb{R}^{N_{j}\times 3}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.5.m5.1d">roman_Î” italic_P start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT Ã— 3 end_POSTSUPERSCRIPT</annotation></semantics></math> relative to <math alttext="P^{0}" class="ltx_Math" display="inline" id="S3.SS3.p1.6.m6.1"><semantics id="S3.SS3.p1.6.m6.1a"><msup id="S3.SS3.p1.6.m6.1.1" xref="S3.SS3.p1.6.m6.1.1.cmml"><mi id="S3.SS3.p1.6.m6.1.1.2" xref="S3.SS3.p1.6.m6.1.1.2.cmml">P</mi><mn id="S3.SS3.p1.6.m6.1.1.3" xref="S3.SS3.p1.6.m6.1.1.3.cmml">0</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.6.m6.1b"><apply id="S3.SS3.p1.6.m6.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.6.m6.1.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1">superscript</csymbol><ci id="S3.SS3.p1.6.m6.1.1.2.cmml" xref="S3.SS3.p1.6.m6.1.1.2">ğ‘ƒ</ci><cn id="S3.SS3.p1.6.m6.1.1.3.cmml" type="integer" xref="S3.SS3.p1.6.m6.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.6.m6.1c">P^{0}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.6.m6.1d">italic_P start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT</annotation></semantics></math> is predicted, and the pose estimation is computed by adding the offset to the pose proposals <math alttext="P^{s}=\Delta P^{s}+P^{0}" class="ltx_Math" display="inline" id="S3.SS3.p1.7.m7.1"><semantics id="S3.SS3.p1.7.m7.1a"><mrow id="S3.SS3.p1.7.m7.1.1" xref="S3.SS3.p1.7.m7.1.1.cmml"><msup id="S3.SS3.p1.7.m7.1.1.2" xref="S3.SS3.p1.7.m7.1.1.2.cmml"><mi id="S3.SS3.p1.7.m7.1.1.2.2" xref="S3.SS3.p1.7.m7.1.1.2.2.cmml">P</mi><mi id="S3.SS3.p1.7.m7.1.1.2.3" xref="S3.SS3.p1.7.m7.1.1.2.3.cmml">s</mi></msup><mo id="S3.SS3.p1.7.m7.1.1.1" xref="S3.SS3.p1.7.m7.1.1.1.cmml">=</mo><mrow id="S3.SS3.p1.7.m7.1.1.3" xref="S3.SS3.p1.7.m7.1.1.3.cmml"><mrow id="S3.SS3.p1.7.m7.1.1.3.2" xref="S3.SS3.p1.7.m7.1.1.3.2.cmml"><mi id="S3.SS3.p1.7.m7.1.1.3.2.2" mathvariant="normal" xref="S3.SS3.p1.7.m7.1.1.3.2.2.cmml">Î”</mi><mo id="S3.SS3.p1.7.m7.1.1.3.2.1" xref="S3.SS3.p1.7.m7.1.1.3.2.1.cmml">â¢</mo><msup id="S3.SS3.p1.7.m7.1.1.3.2.3" xref="S3.SS3.p1.7.m7.1.1.3.2.3.cmml"><mi id="S3.SS3.p1.7.m7.1.1.3.2.3.2" xref="S3.SS3.p1.7.m7.1.1.3.2.3.2.cmml">P</mi><mi id="S3.SS3.p1.7.m7.1.1.3.2.3.3" xref="S3.SS3.p1.7.m7.1.1.3.2.3.3.cmml">s</mi></msup></mrow><mo id="S3.SS3.p1.7.m7.1.1.3.1" xref="S3.SS3.p1.7.m7.1.1.3.1.cmml">+</mo><msup id="S3.SS3.p1.7.m7.1.1.3.3" xref="S3.SS3.p1.7.m7.1.1.3.3.cmml"><mi id="S3.SS3.p1.7.m7.1.1.3.3.2" xref="S3.SS3.p1.7.m7.1.1.3.3.2.cmml">P</mi><mn id="S3.SS3.p1.7.m7.1.1.3.3.3" xref="S3.SS3.p1.7.m7.1.1.3.3.3.cmml">0</mn></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.7.m7.1b"><apply id="S3.SS3.p1.7.m7.1.1.cmml" xref="S3.SS3.p1.7.m7.1.1"><eq id="S3.SS3.p1.7.m7.1.1.1.cmml" xref="S3.SS3.p1.7.m7.1.1.1"></eq><apply id="S3.SS3.p1.7.m7.1.1.2.cmml" xref="S3.SS3.p1.7.m7.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p1.7.m7.1.1.2.1.cmml" xref="S3.SS3.p1.7.m7.1.1.2">superscript</csymbol><ci id="S3.SS3.p1.7.m7.1.1.2.2.cmml" xref="S3.SS3.p1.7.m7.1.1.2.2">ğ‘ƒ</ci><ci id="S3.SS3.p1.7.m7.1.1.2.3.cmml" xref="S3.SS3.p1.7.m7.1.1.2.3">ğ‘ </ci></apply><apply id="S3.SS3.p1.7.m7.1.1.3.cmml" xref="S3.SS3.p1.7.m7.1.1.3"><plus id="S3.SS3.p1.7.m7.1.1.3.1.cmml" xref="S3.SS3.p1.7.m7.1.1.3.1"></plus><apply id="S3.SS3.p1.7.m7.1.1.3.2.cmml" xref="S3.SS3.p1.7.m7.1.1.3.2"><times id="S3.SS3.p1.7.m7.1.1.3.2.1.cmml" xref="S3.SS3.p1.7.m7.1.1.3.2.1"></times><ci id="S3.SS3.p1.7.m7.1.1.3.2.2.cmml" xref="S3.SS3.p1.7.m7.1.1.3.2.2">Î”</ci><apply id="S3.SS3.p1.7.m7.1.1.3.2.3.cmml" xref="S3.SS3.p1.7.m7.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.SS3.p1.7.m7.1.1.3.2.3.1.cmml" xref="S3.SS3.p1.7.m7.1.1.3.2.3">superscript</csymbol><ci id="S3.SS3.p1.7.m7.1.1.3.2.3.2.cmml" xref="S3.SS3.p1.7.m7.1.1.3.2.3.2">ğ‘ƒ</ci><ci id="S3.SS3.p1.7.m7.1.1.3.2.3.3.cmml" xref="S3.SS3.p1.7.m7.1.1.3.2.3.3">ğ‘ </ci></apply></apply><apply id="S3.SS3.p1.7.m7.1.1.3.3.cmml" xref="S3.SS3.p1.7.m7.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS3.p1.7.m7.1.1.3.3.1.cmml" xref="S3.SS3.p1.7.m7.1.1.3.3">superscript</csymbol><ci id="S3.SS3.p1.7.m7.1.1.3.3.2.cmml" xref="S3.SS3.p1.7.m7.1.1.3.3.2">ğ‘ƒ</ci><cn id="S3.SS3.p1.7.m7.1.1.3.3.3.cmml" type="integer" xref="S3.SS3.p1.7.m7.1.1.3.3.3">0</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.7.m7.1c">P^{s}=\Delta P^{s}+P^{0}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.7.m7.1d">italic_P start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT = roman_Î” italic_P start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT + italic_P start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT</annotation></semantics></math> where <math alttext="s=\{1,...,S\}" class="ltx_Math" display="inline" id="S3.SS3.p1.8.m8.3"><semantics id="S3.SS3.p1.8.m8.3a"><mrow id="S3.SS3.p1.8.m8.3.4" xref="S3.SS3.p1.8.m8.3.4.cmml"><mi id="S3.SS3.p1.8.m8.3.4.2" xref="S3.SS3.p1.8.m8.3.4.2.cmml">s</mi><mo id="S3.SS3.p1.8.m8.3.4.1" xref="S3.SS3.p1.8.m8.3.4.1.cmml">=</mo><mrow id="S3.SS3.p1.8.m8.3.4.3.2" xref="S3.SS3.p1.8.m8.3.4.3.1.cmml"><mo id="S3.SS3.p1.8.m8.3.4.3.2.1" stretchy="false" xref="S3.SS3.p1.8.m8.3.4.3.1.cmml">{</mo><mn id="S3.SS3.p1.8.m8.1.1" xref="S3.SS3.p1.8.m8.1.1.cmml">1</mn><mo id="S3.SS3.p1.8.m8.3.4.3.2.2" xref="S3.SS3.p1.8.m8.3.4.3.1.cmml">,</mo><mi id="S3.SS3.p1.8.m8.2.2" mathvariant="normal" xref="S3.SS3.p1.8.m8.2.2.cmml">â€¦</mi><mo id="S3.SS3.p1.8.m8.3.4.3.2.3" xref="S3.SS3.p1.8.m8.3.4.3.1.cmml">,</mo><mi id="S3.SS3.p1.8.m8.3.3" xref="S3.SS3.p1.8.m8.3.3.cmml">S</mi><mo id="S3.SS3.p1.8.m8.3.4.3.2.4" stretchy="false" xref="S3.SS3.p1.8.m8.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.8.m8.3b"><apply id="S3.SS3.p1.8.m8.3.4.cmml" xref="S3.SS3.p1.8.m8.3.4"><eq id="S3.SS3.p1.8.m8.3.4.1.cmml" xref="S3.SS3.p1.8.m8.3.4.1"></eq><ci id="S3.SS3.p1.8.m8.3.4.2.cmml" xref="S3.SS3.p1.8.m8.3.4.2">ğ‘ </ci><set id="S3.SS3.p1.8.m8.3.4.3.1.cmml" xref="S3.SS3.p1.8.m8.3.4.3.2"><cn id="S3.SS3.p1.8.m8.1.1.cmml" type="integer" xref="S3.SS3.p1.8.m8.1.1">1</cn><ci id="S3.SS3.p1.8.m8.2.2.cmml" xref="S3.SS3.p1.8.m8.2.2">â€¦</ci><ci id="S3.SS3.p1.8.m8.3.3.cmml" xref="S3.SS3.p1.8.m8.3.3">ğ‘†</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.8.m8.3c">s=\{1,...,S\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.8.m8.3d">italic_s = { 1 , â€¦ , italic_S }</annotation></semantics></math>. During the inference phase, the final layerâ€™s output, <math alttext="P^{S}" class="ltx_Math" display="inline" id="S3.SS3.p1.9.m9.1"><semantics id="S3.SS3.p1.9.m9.1a"><msup id="S3.SS3.p1.9.m9.1.1" xref="S3.SS3.p1.9.m9.1.1.cmml"><mi id="S3.SS3.p1.9.m9.1.1.2" xref="S3.SS3.p1.9.m9.1.1.2.cmml">P</mi><mi id="S3.SS3.p1.9.m9.1.1.3" xref="S3.SS3.p1.9.m9.1.1.3.cmml">S</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.9.m9.1b"><apply id="S3.SS3.p1.9.m9.1.1.cmml" xref="S3.SS3.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.9.m9.1.1.1.cmml" xref="S3.SS3.p1.9.m9.1.1">superscript</csymbol><ci id="S3.SS3.p1.9.m9.1.1.2.cmml" xref="S3.SS3.p1.9.m9.1.1.2">ğ‘ƒ</ci><ci id="S3.SS3.p1.9.m9.1.1.3.cmml" xref="S3.SS3.p1.9.m9.1.1.3">ğ‘†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.9.m9.1c">P^{S}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.9.m9.1d">italic_P start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT</annotation></semantics></math>, is used as the modelâ€™s final prediction. 
<br class="ltx_break"/></p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="248" id="S3.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.3.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text ltx_font_bold" id="S3.F3.4.2" style="font-size:80%;">Left<span class="ltx_text ltx_font_medium" id="S3.F3.4.2.1">: An illustration of ourÂ Deformable Stereo Attention. The 3D joints are first projected to each view plane using camera parameters. Within each view, we compute 2D deformable attention by querying the image features with the JQTs with the projected points serving as reference points. Finally, the attention results for each view are concatenated and fed into a linear layer to be projected into the original dimension. </span>Right<span class="ltx_text ltx_font_medium" id="S3.F3.4.2.2">: (a) The feature extractor is first pre-trained to predict 2D joint heatmaps using monocular images. (b) The multi-view feature maps are computed using the pre-trained feature extractor.</span></span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.4"><span class="ltx_text ltx_font_bold" id="S3.SS3.p2.4.1">Joint Query Tokens.</span>
Inspired by DETRÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib4" title="">4</a>]</cite>, in PRFormerÂ every joint is characterized by a unique Joint Query Token (JQT). The JQTs will serve as the queries in our transformer to interact with each other and the multi-view image features through attention mechanisms. We compute the JQTs by embedding each jointâ€™s identity and initial location information with a Query Generation MLP. Specifically, for the <math alttext="j" class="ltx_Math" display="inline" id="S3.SS3.p2.1.m1.1"><semantics id="S3.SS3.p2.1.m1.1a"><mi id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><ci id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">ğ‘—</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">j</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.1.m1.1d">italic_j</annotation></semantics></math>-th joint, its JQT <math alttext="Q_{j}\in\mathbb{R}^{C}" class="ltx_Math" display="inline" id="S3.SS3.p2.2.m2.1"><semantics id="S3.SS3.p2.2.m2.1a"><mrow id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml"><msub id="S3.SS3.p2.2.m2.1.1.2" xref="S3.SS3.p2.2.m2.1.1.2.cmml"><mi id="S3.SS3.p2.2.m2.1.1.2.2" xref="S3.SS3.p2.2.m2.1.1.2.2.cmml">Q</mi><mi id="S3.SS3.p2.2.m2.1.1.2.3" xref="S3.SS3.p2.2.m2.1.1.2.3.cmml">j</mi></msub><mo id="S3.SS3.p2.2.m2.1.1.1" xref="S3.SS3.p2.2.m2.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS3.p2.2.m2.1.1.3" xref="S3.SS3.p2.2.m2.1.1.3.cmml"><mi id="S3.SS3.p2.2.m2.1.1.3.2" xref="S3.SS3.p2.2.m2.1.1.3.2.cmml">â„</mi><mi id="S3.SS3.p2.2.m2.1.1.3.3" xref="S3.SS3.p2.2.m2.1.1.3.3.cmml">C</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><apply id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"><in id="S3.SS3.p2.2.m2.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1.1"></in><apply id="S3.SS3.p2.2.m2.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.2.1.cmml" xref="S3.SS3.p2.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS3.p2.2.m2.1.1.2.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2.2">ğ‘„</ci><ci id="S3.SS3.p2.2.m2.1.1.2.3.cmml" xref="S3.SS3.p2.2.m2.1.1.2.3">ğ‘—</ci></apply><apply id="S3.SS3.p2.2.m2.1.1.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.3.1.cmml" xref="S3.SS3.p2.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS3.p2.2.m2.1.1.3.2.cmml" xref="S3.SS3.p2.2.m2.1.1.3.2">â„</ci><ci id="S3.SS3.p2.2.m2.1.1.3.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3.3">ğ¶</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">Q_{j}\in\mathbb{R}^{C}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.2.m2.1d">italic_Q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPT</annotation></semantics></math> is computed by feeding its initial location <math alttext="P^{0}_{j}=(x^{0}_{j},y^{0}_{j},z^{0}_{j})" class="ltx_Math" display="inline" id="S3.SS3.p2.3.m3.3"><semantics id="S3.SS3.p2.3.m3.3a"><mrow id="S3.SS3.p2.3.m3.3.3" xref="S3.SS3.p2.3.m3.3.3.cmml"><msubsup id="S3.SS3.p2.3.m3.3.3.5" xref="S3.SS3.p2.3.m3.3.3.5.cmml"><mi id="S3.SS3.p2.3.m3.3.3.5.2.2" xref="S3.SS3.p2.3.m3.3.3.5.2.2.cmml">P</mi><mi id="S3.SS3.p2.3.m3.3.3.5.3" xref="S3.SS3.p2.3.m3.3.3.5.3.cmml">j</mi><mn id="S3.SS3.p2.3.m3.3.3.5.2.3" xref="S3.SS3.p2.3.m3.3.3.5.2.3.cmml">0</mn></msubsup><mo id="S3.SS3.p2.3.m3.3.3.4" xref="S3.SS3.p2.3.m3.3.3.4.cmml">=</mo><mrow id="S3.SS3.p2.3.m3.3.3.3.3" xref="S3.SS3.p2.3.m3.3.3.3.4.cmml"><mo id="S3.SS3.p2.3.m3.3.3.3.3.4" stretchy="false" xref="S3.SS3.p2.3.m3.3.3.3.4.cmml">(</mo><msubsup id="S3.SS3.p2.3.m3.1.1.1.1.1" xref="S3.SS3.p2.3.m3.1.1.1.1.1.cmml"><mi id="S3.SS3.p2.3.m3.1.1.1.1.1.2.2" xref="S3.SS3.p2.3.m3.1.1.1.1.1.2.2.cmml">x</mi><mi id="S3.SS3.p2.3.m3.1.1.1.1.1.3" xref="S3.SS3.p2.3.m3.1.1.1.1.1.3.cmml">j</mi><mn id="S3.SS3.p2.3.m3.1.1.1.1.1.2.3" xref="S3.SS3.p2.3.m3.1.1.1.1.1.2.3.cmml">0</mn></msubsup><mo id="S3.SS3.p2.3.m3.3.3.3.3.5" xref="S3.SS3.p2.3.m3.3.3.3.4.cmml">,</mo><msubsup id="S3.SS3.p2.3.m3.2.2.2.2.2" xref="S3.SS3.p2.3.m3.2.2.2.2.2.cmml"><mi id="S3.SS3.p2.3.m3.2.2.2.2.2.2.2" xref="S3.SS3.p2.3.m3.2.2.2.2.2.2.2.cmml">y</mi><mi id="S3.SS3.p2.3.m3.2.2.2.2.2.3" xref="S3.SS3.p2.3.m3.2.2.2.2.2.3.cmml">j</mi><mn id="S3.SS3.p2.3.m3.2.2.2.2.2.2.3" xref="S3.SS3.p2.3.m3.2.2.2.2.2.2.3.cmml">0</mn></msubsup><mo id="S3.SS3.p2.3.m3.3.3.3.3.6" xref="S3.SS3.p2.3.m3.3.3.3.4.cmml">,</mo><msubsup id="S3.SS3.p2.3.m3.3.3.3.3.3" xref="S3.SS3.p2.3.m3.3.3.3.3.3.cmml"><mi id="S3.SS3.p2.3.m3.3.3.3.3.3.2.2" xref="S3.SS3.p2.3.m3.3.3.3.3.3.2.2.cmml">z</mi><mi id="S3.SS3.p2.3.m3.3.3.3.3.3.3" xref="S3.SS3.p2.3.m3.3.3.3.3.3.3.cmml">j</mi><mn id="S3.SS3.p2.3.m3.3.3.3.3.3.2.3" xref="S3.SS3.p2.3.m3.3.3.3.3.3.2.3.cmml">0</mn></msubsup><mo id="S3.SS3.p2.3.m3.3.3.3.3.7" stretchy="false" xref="S3.SS3.p2.3.m3.3.3.3.4.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.3b"><apply id="S3.SS3.p2.3.m3.3.3.cmml" xref="S3.SS3.p2.3.m3.3.3"><eq id="S3.SS3.p2.3.m3.3.3.4.cmml" xref="S3.SS3.p2.3.m3.3.3.4"></eq><apply id="S3.SS3.p2.3.m3.3.3.5.cmml" xref="S3.SS3.p2.3.m3.3.3.5"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.3.3.5.1.cmml" xref="S3.SS3.p2.3.m3.3.3.5">subscript</csymbol><apply id="S3.SS3.p2.3.m3.3.3.5.2.cmml" xref="S3.SS3.p2.3.m3.3.3.5"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.3.3.5.2.1.cmml" xref="S3.SS3.p2.3.m3.3.3.5">superscript</csymbol><ci id="S3.SS3.p2.3.m3.3.3.5.2.2.cmml" xref="S3.SS3.p2.3.m3.3.3.5.2.2">ğ‘ƒ</ci><cn id="S3.SS3.p2.3.m3.3.3.5.2.3.cmml" type="integer" xref="S3.SS3.p2.3.m3.3.3.5.2.3">0</cn></apply><ci id="S3.SS3.p2.3.m3.3.3.5.3.cmml" xref="S3.SS3.p2.3.m3.3.3.5.3">ğ‘—</ci></apply><vector id="S3.SS3.p2.3.m3.3.3.3.4.cmml" xref="S3.SS3.p2.3.m3.3.3.3.3"><apply id="S3.SS3.p2.3.m3.1.1.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1.1.1.1">subscript</csymbol><apply id="S3.SS3.p2.3.m3.1.1.1.1.1.2.cmml" xref="S3.SS3.p2.3.m3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.1.1.1.1.1.2.1.cmml" xref="S3.SS3.p2.3.m3.1.1.1.1.1">superscript</csymbol><ci id="S3.SS3.p2.3.m3.1.1.1.1.1.2.2.cmml" xref="S3.SS3.p2.3.m3.1.1.1.1.1.2.2">ğ‘¥</ci><cn id="S3.SS3.p2.3.m3.1.1.1.1.1.2.3.cmml" type="integer" xref="S3.SS3.p2.3.m3.1.1.1.1.1.2.3">0</cn></apply><ci id="S3.SS3.p2.3.m3.1.1.1.1.1.3.cmml" xref="S3.SS3.p2.3.m3.1.1.1.1.1.3">ğ‘—</ci></apply><apply id="S3.SS3.p2.3.m3.2.2.2.2.2.cmml" xref="S3.SS3.p2.3.m3.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.2.2.2.2.2.1.cmml" xref="S3.SS3.p2.3.m3.2.2.2.2.2">subscript</csymbol><apply id="S3.SS3.p2.3.m3.2.2.2.2.2.2.cmml" xref="S3.SS3.p2.3.m3.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.2.2.2.2.2.2.1.cmml" xref="S3.SS3.p2.3.m3.2.2.2.2.2">superscript</csymbol><ci id="S3.SS3.p2.3.m3.2.2.2.2.2.2.2.cmml" xref="S3.SS3.p2.3.m3.2.2.2.2.2.2.2">ğ‘¦</ci><cn id="S3.SS3.p2.3.m3.2.2.2.2.2.2.3.cmml" type="integer" xref="S3.SS3.p2.3.m3.2.2.2.2.2.2.3">0</cn></apply><ci id="S3.SS3.p2.3.m3.2.2.2.2.2.3.cmml" xref="S3.SS3.p2.3.m3.2.2.2.2.2.3">ğ‘—</ci></apply><apply id="S3.SS3.p2.3.m3.3.3.3.3.3.cmml" xref="S3.SS3.p2.3.m3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.3.3.3.3.3.1.cmml" xref="S3.SS3.p2.3.m3.3.3.3.3.3">subscript</csymbol><apply id="S3.SS3.p2.3.m3.3.3.3.3.3.2.cmml" xref="S3.SS3.p2.3.m3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.3.3.3.3.3.2.1.cmml" xref="S3.SS3.p2.3.m3.3.3.3.3.3">superscript</csymbol><ci id="S3.SS3.p2.3.m3.3.3.3.3.3.2.2.cmml" xref="S3.SS3.p2.3.m3.3.3.3.3.3.2.2">ğ‘§</ci><cn id="S3.SS3.p2.3.m3.3.3.3.3.3.2.3.cmml" type="integer" xref="S3.SS3.p2.3.m3.3.3.3.3.3.2.3">0</cn></apply><ci id="S3.SS3.p2.3.m3.3.3.3.3.3.3.cmml" xref="S3.SS3.p2.3.m3.3.3.3.3.3.3">ğ‘—</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.3c">P^{0}_{j}=(x^{0}_{j},y^{0}_{j},z^{0}_{j})</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.3.m3.3d">italic_P start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = ( italic_x start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_y start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_z start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT )</annotation></semantics></math> and a scalar identifier <math alttext="\sigma_{j}" class="ltx_Math" display="inline" id="S3.SS3.p2.4.m4.1"><semantics id="S3.SS3.p2.4.m4.1a"><msub id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml"><mi id="S3.SS3.p2.4.m4.1.1.2" xref="S3.SS3.p2.4.m4.1.1.2.cmml">Ïƒ</mi><mi id="S3.SS3.p2.4.m4.1.1.3" xref="S3.SS3.p2.4.m4.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><apply id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m4.1.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS3.p2.4.m4.1.1.2.cmml" xref="S3.SS3.p2.4.m4.1.1.2">ğœ</ci><ci id="S3.SS3.p2.4.m4.1.1.3.cmml" xref="S3.SS3.p2.4.m4.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">\sigma_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.4.m4.1d">italic_Ïƒ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> into the MLP:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="Pt0.A2.EGx2">
<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle Q_{j}=\mathrm{MLP}_{\mathrm{JQT}}\bigl{(}\sigma_{j},x^{0}_{j},y^%
{0}_{j},z^{0}_{j}\bigr{)}" class="ltx_Math" display="inline" id="S3.E2.m1.4"><semantics id="S3.E2.m1.4a"><mrow id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml"><msub id="S3.E2.m1.4.4.6" xref="S3.E2.m1.4.4.6.cmml"><mi id="S3.E2.m1.4.4.6.2" xref="S3.E2.m1.4.4.6.2.cmml">Q</mi><mi id="S3.E2.m1.4.4.6.3" xref="S3.E2.m1.4.4.6.3.cmml">j</mi></msub><mo id="S3.E2.m1.4.4.5" xref="S3.E2.m1.4.4.5.cmml">=</mo><mrow id="S3.E2.m1.4.4.4" xref="S3.E2.m1.4.4.4.cmml"><msub id="S3.E2.m1.4.4.4.6" xref="S3.E2.m1.4.4.4.6.cmml"><mi id="S3.E2.m1.4.4.4.6.2" xref="S3.E2.m1.4.4.4.6.2.cmml">MLP</mi><mi id="S3.E2.m1.4.4.4.6.3" xref="S3.E2.m1.4.4.4.6.3.cmml">JQT</mi></msub><mo id="S3.E2.m1.4.4.4.5" xref="S3.E2.m1.4.4.4.5.cmml">â¢</mo><mrow id="S3.E2.m1.4.4.4.4.4" xref="S3.E2.m1.4.4.4.4.5.cmml"><mo id="S3.E2.m1.4.4.4.4.4.5" maxsize="120%" minsize="120%" xref="S3.E2.m1.4.4.4.4.5.cmml">(</mo><msub id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.2.cmml">Ïƒ</mi><mi id="S3.E2.m1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.3.cmml">j</mi></msub><mo id="S3.E2.m1.4.4.4.4.4.6" xref="S3.E2.m1.4.4.4.4.5.cmml">,</mo><msubsup id="S3.E2.m1.2.2.2.2.2.2" xref="S3.E2.m1.2.2.2.2.2.2.cmml"><mi id="S3.E2.m1.2.2.2.2.2.2.2.2" xref="S3.E2.m1.2.2.2.2.2.2.2.2.cmml">x</mi><mi id="S3.E2.m1.2.2.2.2.2.2.3" xref="S3.E2.m1.2.2.2.2.2.2.3.cmml">j</mi><mn id="S3.E2.m1.2.2.2.2.2.2.2.3" xref="S3.E2.m1.2.2.2.2.2.2.2.3.cmml">0</mn></msubsup><mo id="S3.E2.m1.4.4.4.4.4.7" xref="S3.E2.m1.4.4.4.4.5.cmml">,</mo><msubsup id="S3.E2.m1.3.3.3.3.3.3" xref="S3.E2.m1.3.3.3.3.3.3.cmml"><mi id="S3.E2.m1.3.3.3.3.3.3.2.2" xref="S3.E2.m1.3.3.3.3.3.3.2.2.cmml">y</mi><mi id="S3.E2.m1.3.3.3.3.3.3.3" xref="S3.E2.m1.3.3.3.3.3.3.3.cmml">j</mi><mn id="S3.E2.m1.3.3.3.3.3.3.2.3" xref="S3.E2.m1.3.3.3.3.3.3.2.3.cmml">0</mn></msubsup><mo id="S3.E2.m1.4.4.4.4.4.8" xref="S3.E2.m1.4.4.4.4.5.cmml">,</mo><msubsup id="S3.E2.m1.4.4.4.4.4.4" xref="S3.E2.m1.4.4.4.4.4.4.cmml"><mi id="S3.E2.m1.4.4.4.4.4.4.2.2" xref="S3.E2.m1.4.4.4.4.4.4.2.2.cmml">z</mi><mi id="S3.E2.m1.4.4.4.4.4.4.3" xref="S3.E2.m1.4.4.4.4.4.4.3.cmml">j</mi><mn id="S3.E2.m1.4.4.4.4.4.4.2.3" xref="S3.E2.m1.4.4.4.4.4.4.2.3.cmml">0</mn></msubsup><mo id="S3.E2.m1.4.4.4.4.4.9" maxsize="120%" minsize="120%" xref="S3.E2.m1.4.4.4.4.5.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.4b"><apply id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4"><eq id="S3.E2.m1.4.4.5.cmml" xref="S3.E2.m1.4.4.5"></eq><apply id="S3.E2.m1.4.4.6.cmml" xref="S3.E2.m1.4.4.6"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.6.1.cmml" xref="S3.E2.m1.4.4.6">subscript</csymbol><ci id="S3.E2.m1.4.4.6.2.cmml" xref="S3.E2.m1.4.4.6.2">ğ‘„</ci><ci id="S3.E2.m1.4.4.6.3.cmml" xref="S3.E2.m1.4.4.6.3">ğ‘—</ci></apply><apply id="S3.E2.m1.4.4.4.cmml" xref="S3.E2.m1.4.4.4"><times id="S3.E2.m1.4.4.4.5.cmml" xref="S3.E2.m1.4.4.4.5"></times><apply id="S3.E2.m1.4.4.4.6.cmml" xref="S3.E2.m1.4.4.4.6"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.4.6.1.cmml" xref="S3.E2.m1.4.4.4.6">subscript</csymbol><ci id="S3.E2.m1.4.4.4.6.2.cmml" xref="S3.E2.m1.4.4.4.6.2">MLP</ci><ci id="S3.E2.m1.4.4.4.6.3.cmml" xref="S3.E2.m1.4.4.4.6.3">JQT</ci></apply><vector id="S3.E2.m1.4.4.4.4.5.cmml" xref="S3.E2.m1.4.4.4.4.4"><apply id="S3.E2.m1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2">ğœ</ci><ci id="S3.E2.m1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3">ğ‘—</ci></apply><apply id="S3.E2.m1.2.2.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.2.2.2.1.cmml" xref="S3.E2.m1.2.2.2.2.2.2">subscript</csymbol><apply id="S3.E2.m1.2.2.2.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.2.2.2.2.1.cmml" xref="S3.E2.m1.2.2.2.2.2.2">superscript</csymbol><ci id="S3.E2.m1.2.2.2.2.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.2">ğ‘¥</ci><cn id="S3.E2.m1.2.2.2.2.2.2.2.3.cmml" type="integer" xref="S3.E2.m1.2.2.2.2.2.2.2.3">0</cn></apply><ci id="S3.E2.m1.2.2.2.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.2.2.2.3">ğ‘—</ci></apply><apply id="S3.E2.m1.3.3.3.3.3.3.cmml" xref="S3.E2.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.3.3.3.3.1.cmml" xref="S3.E2.m1.3.3.3.3.3.3">subscript</csymbol><apply id="S3.E2.m1.3.3.3.3.3.3.2.cmml" xref="S3.E2.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.3.3.3.3.2.1.cmml" xref="S3.E2.m1.3.3.3.3.3.3">superscript</csymbol><ci id="S3.E2.m1.3.3.3.3.3.3.2.2.cmml" xref="S3.E2.m1.3.3.3.3.3.3.2.2">ğ‘¦</ci><cn id="S3.E2.m1.3.3.3.3.3.3.2.3.cmml" type="integer" xref="S3.E2.m1.3.3.3.3.3.3.2.3">0</cn></apply><ci id="S3.E2.m1.3.3.3.3.3.3.3.cmml" xref="S3.E2.m1.3.3.3.3.3.3.3">ğ‘—</ci></apply><apply id="S3.E2.m1.4.4.4.4.4.4.cmml" xref="S3.E2.m1.4.4.4.4.4.4"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.4.4.4.4.1.cmml" xref="S3.E2.m1.4.4.4.4.4.4">subscript</csymbol><apply id="S3.E2.m1.4.4.4.4.4.4.2.cmml" xref="S3.E2.m1.4.4.4.4.4.4"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.4.4.4.4.2.1.cmml" xref="S3.E2.m1.4.4.4.4.4.4">superscript</csymbol><ci id="S3.E2.m1.4.4.4.4.4.4.2.2.cmml" xref="S3.E2.m1.4.4.4.4.4.4.2.2">ğ‘§</ci><cn id="S3.E2.m1.4.4.4.4.4.4.2.3.cmml" type="integer" xref="S3.E2.m1.4.4.4.4.4.4.2.3">0</cn></apply><ci id="S3.E2.m1.4.4.4.4.4.4.3.cmml" xref="S3.E2.m1.4.4.4.4.4.4.3">ğ‘—</ci></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.4c">\displaystyle Q_{j}=\mathrm{MLP}_{\mathrm{JQT}}\bigl{(}\sigma_{j},x^{0}_{j},y^%
{0}_{j},z^{0}_{j}\bigr{)}</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.4d">italic_Q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = roman_MLP start_POSTSUBSCRIPT roman_JQT end_POSTSUBSCRIPT ( italic_Ïƒ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_x start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_y start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_z start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.p2.6">In practice, we simply use the joint index <math alttext="j" class="ltx_Math" display="inline" id="S3.SS3.p2.5.m1.1"><semantics id="S3.SS3.p2.5.m1.1a"><mi id="S3.SS3.p2.5.m1.1.1" xref="S3.SS3.p2.5.m1.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m1.1b"><ci id="S3.SS3.p2.5.m1.1.1.cmml" xref="S3.SS3.p2.5.m1.1.1">ğ‘—</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m1.1c">j</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.5.m1.1d">italic_j</annotation></semantics></math> to serve as the scalar identifier <math alttext="\sigma_{j}" class="ltx_Math" display="inline" id="S3.SS3.p2.6.m2.1"><semantics id="S3.SS3.p2.6.m2.1a"><msub id="S3.SS3.p2.6.m2.1.1" xref="S3.SS3.p2.6.m2.1.1.cmml"><mi id="S3.SS3.p2.6.m2.1.1.2" xref="S3.SS3.p2.6.m2.1.1.2.cmml">Ïƒ</mi><mi id="S3.SS3.p2.6.m2.1.1.3" xref="S3.SS3.p2.6.m2.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m2.1b"><apply id="S3.SS3.p2.6.m2.1.1.cmml" xref="S3.SS3.p2.6.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.6.m2.1.1.1.cmml" xref="S3.SS3.p2.6.m2.1.1">subscript</csymbol><ci id="S3.SS3.p2.6.m2.1.1.2.cmml" xref="S3.SS3.p2.6.m2.1.1.2">ğœ</ci><ci id="S3.SS3.p2.6.m2.1.1.3.cmml" xref="S3.SS3.p2.6.m2.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m2.1c">\sigma_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.6.m2.1d">italic_Ïƒ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math>. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p3.1.1">PRFormerÂ Layer.</span>
As shown inÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S2.F2" title="In 2 Related Work â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">2</span></a> (b), each layer of the PRFormerÂ is a transformer decoder layerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib34" title="">34</a>]</cite>. Each input JQT undergoes a cross-attention operation to interact with the fine-grained stereo image features, and a subsequent self-attention operation to extract spatial and human kinematic information from other JQTs. Note that here we followÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib5" title="">5</a>]</cite> to put the cross-attention operation before the self-attention operation. Finally, we use a Feed-forward Network (FFN)Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib34" title="">34</a>]</cite> to non-linearly transform the JQTs. A distinct characteristic of the PRFormerÂ layer vis-Ã -vis traditional transformer decoder layers is our replacement of the conventional cross-attention with Deformable Stereo Attention, which enables our transformer to effectively reason about multi-view stereo and thus can accurately locate the joints in the 3D world. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.4"><span class="ltx_text ltx_font_bold" id="S3.SS3.p4.4.1">Deformable Stereo Attention.</span>
As shown inÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S3.F3" title="In 3.3 Pose Refinement Transformer (PRFormer) â€£ 3 Method â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">3</span></a> Left, the proposed Deformable Stereo AttentionÂ has three steps. Firstly, we project each body jointâ€™s initial 3D location onto each view plane by leveraging the camera parametersÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib28" title="">28</a>]</cite>. Specifically, for joint <math alttext="j" class="ltx_Math" display="inline" id="S3.SS3.p4.1.m1.1"><semantics id="S3.SS3.p4.1.m1.1a"><mi id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.1b"><ci id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1">ğ‘—</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.1c">j</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.1.m1.1d">italic_j</annotation></semantics></math> and its initial 3D location <math alttext="P^{0}_{j}" class="ltx_Math" display="inline" id="S3.SS3.p4.2.m2.1"><semantics id="S3.SS3.p4.2.m2.1a"><msubsup id="S3.SS3.p4.2.m2.1.1" xref="S3.SS3.p4.2.m2.1.1.cmml"><mi id="S3.SS3.p4.2.m2.1.1.2.2" xref="S3.SS3.p4.2.m2.1.1.2.2.cmml">P</mi><mi id="S3.SS3.p4.2.m2.1.1.3" xref="S3.SS3.p4.2.m2.1.1.3.cmml">j</mi><mn id="S3.SS3.p4.2.m2.1.1.2.3" xref="S3.SS3.p4.2.m2.1.1.2.3.cmml">0</mn></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.2.m2.1b"><apply id="S3.SS3.p4.2.m2.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.2.m2.1.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1">subscript</csymbol><apply id="S3.SS3.p4.2.m2.1.1.2.cmml" xref="S3.SS3.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.2.m2.1.1.2.1.cmml" xref="S3.SS3.p4.2.m2.1.1">superscript</csymbol><ci id="S3.SS3.p4.2.m2.1.1.2.2.cmml" xref="S3.SS3.p4.2.m2.1.1.2.2">ğ‘ƒ</ci><cn id="S3.SS3.p4.2.m2.1.1.2.3.cmml" type="integer" xref="S3.SS3.p4.2.m2.1.1.2.3">0</cn></apply><ci id="S3.SS3.p4.2.m2.1.1.3.cmml" xref="S3.SS3.p4.2.m2.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.2.m2.1c">P^{0}_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.2.m2.1d">italic_P start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math>, we compute its 2D location on each view <math alttext="\{\tilde{P}^{k}_{j}\}=\{(\tilde{x}^{k}_{j},\tilde{y}^{k}_{j})\}" class="ltx_Math" display="inline" id="S3.SS3.p4.3.m3.2"><semantics id="S3.SS3.p4.3.m3.2a"><mrow id="S3.SS3.p4.3.m3.2.2" xref="S3.SS3.p4.3.m3.2.2.cmml"><mrow id="S3.SS3.p4.3.m3.1.1.1.1" xref="S3.SS3.p4.3.m3.1.1.1.2.cmml"><mo id="S3.SS3.p4.3.m3.1.1.1.1.2" stretchy="false" xref="S3.SS3.p4.3.m3.1.1.1.2.cmml">{</mo><msubsup id="S3.SS3.p4.3.m3.1.1.1.1.1" xref="S3.SS3.p4.3.m3.1.1.1.1.1.cmml"><mover accent="true" id="S3.SS3.p4.3.m3.1.1.1.1.1.2.2" xref="S3.SS3.p4.3.m3.1.1.1.1.1.2.2.cmml"><mi id="S3.SS3.p4.3.m3.1.1.1.1.1.2.2.2" xref="S3.SS3.p4.3.m3.1.1.1.1.1.2.2.2.cmml">P</mi><mo id="S3.SS3.p4.3.m3.1.1.1.1.1.2.2.1" xref="S3.SS3.p4.3.m3.1.1.1.1.1.2.2.1.cmml">~</mo></mover><mi id="S3.SS3.p4.3.m3.1.1.1.1.1.3" xref="S3.SS3.p4.3.m3.1.1.1.1.1.3.cmml">j</mi><mi id="S3.SS3.p4.3.m3.1.1.1.1.1.2.3" xref="S3.SS3.p4.3.m3.1.1.1.1.1.2.3.cmml">k</mi></msubsup><mo id="S3.SS3.p4.3.m3.1.1.1.1.3" stretchy="false" xref="S3.SS3.p4.3.m3.1.1.1.2.cmml">}</mo></mrow><mo id="S3.SS3.p4.3.m3.2.2.3" xref="S3.SS3.p4.3.m3.2.2.3.cmml">=</mo><mrow id="S3.SS3.p4.3.m3.2.2.2.1" xref="S3.SS3.p4.3.m3.2.2.2.2.cmml"><mo id="S3.SS3.p4.3.m3.2.2.2.1.2" stretchy="false" xref="S3.SS3.p4.3.m3.2.2.2.2.cmml">{</mo><mrow id="S3.SS3.p4.3.m3.2.2.2.1.1.2" xref="S3.SS3.p4.3.m3.2.2.2.1.1.3.cmml"><mo id="S3.SS3.p4.3.m3.2.2.2.1.1.2.3" stretchy="false" xref="S3.SS3.p4.3.m3.2.2.2.1.1.3.cmml">(</mo><msubsup id="S3.SS3.p4.3.m3.2.2.2.1.1.1.1" xref="S3.SS3.p4.3.m3.2.2.2.1.1.1.1.cmml"><mover accent="true" id="S3.SS3.p4.3.m3.2.2.2.1.1.1.1.2.2" xref="S3.SS3.p4.3.m3.2.2.2.1.1.1.1.2.2.cmml"><mi id="S3.SS3.p4.3.m3.2.2.2.1.1.1.1.2.2.2" xref="S3.SS3.p4.3.m3.2.2.2.1.1.1.1.2.2.2.cmml">x</mi><mo id="S3.SS3.p4.3.m3.2.2.2.1.1.1.1.2.2.1" xref="S3.SS3.p4.3.m3.2.2.2.1.1.1.1.2.2.1.cmml">~</mo></mover><mi id="S3.SS3.p4.3.m3.2.2.2.1.1.1.1.3" xref="S3.SS3.p4.3.m3.2.2.2.1.1.1.1.3.cmml">j</mi><mi id="S3.SS3.p4.3.m3.2.2.2.1.1.1.1.2.3" xref="S3.SS3.p4.3.m3.2.2.2.1.1.1.1.2.3.cmml">k</mi></msubsup><mo id="S3.SS3.p4.3.m3.2.2.2.1.1.2.4" xref="S3.SS3.p4.3.m3.2.2.2.1.1.3.cmml">,</mo><msubsup id="S3.SS3.p4.3.m3.2.2.2.1.1.2.2" xref="S3.SS3.p4.3.m3.2.2.2.1.1.2.2.cmml"><mover accent="true" id="S3.SS3.p4.3.m3.2.2.2.1.1.2.2.2.2" xref="S3.SS3.p4.3.m3.2.2.2.1.1.2.2.2.2.cmml"><mi id="S3.SS3.p4.3.m3.2.2.2.1.1.2.2.2.2.2" xref="S3.SS3.p4.3.m3.2.2.2.1.1.2.2.2.2.2.cmml">y</mi><mo id="S3.SS3.p4.3.m3.2.2.2.1.1.2.2.2.2.1" xref="S3.SS3.p4.3.m3.2.2.2.1.1.2.2.2.2.1.cmml">~</mo></mover><mi id="S3.SS3.p4.3.m3.2.2.2.1.1.2.2.3" xref="S3.SS3.p4.3.m3.2.2.2.1.1.2.2.3.cmml">j</mi><mi id="S3.SS3.p4.3.m3.2.2.2.1.1.2.2.2.3" xref="S3.SS3.p4.3.m3.2.2.2.1.1.2.2.2.3.cmml">k</mi></msubsup><mo id="S3.SS3.p4.3.m3.2.2.2.1.1.2.5" stretchy="false" xref="S3.SS3.p4.3.m3.2.2.2.1.1.3.cmml">)</mo></mrow><mo id="S3.SS3.p4.3.m3.2.2.2.1.3" stretchy="false" xref="S3.SS3.p4.3.m3.2.2.2.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.3.m3.2b"><apply id="S3.SS3.p4.3.m3.2.2.cmml" xref="S3.SS3.p4.3.m3.2.2"><eq id="S3.SS3.p4.3.m3.2.2.3.cmml" xref="S3.SS3.p4.3.m3.2.2.3"></eq><set id="S3.SS3.p4.3.m3.1.1.1.2.cmml" xref="S3.SS3.p4.3.m3.1.1.1.1"><apply id="S3.SS3.p4.3.m3.1.1.1.1.1.cmml" xref="S3.SS3.p4.3.m3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS3.p4.3.m3.1.1.1.1.1">subscript</csymbol><apply id="S3.SS3.p4.3.m3.1.1.1.1.1.2.cmml" xref="S3.SS3.p4.3.m3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.3.m3.1.1.1.1.1.2.1.cmml" xref="S3.SS3.p4.3.m3.1.1.1.1.1">superscript</csymbol><apply id="S3.SS3.p4.3.m3.1.1.1.1.1.2.2.cmml" xref="S3.SS3.p4.3.m3.1.1.1.1.1.2.2"><ci id="S3.SS3.p4.3.m3.1.1.1.1.1.2.2.1.cmml" xref="S3.SS3.p4.3.m3.1.1.1.1.1.2.2.1">~</ci><ci id="S3.SS3.p4.3.m3.1.1.1.1.1.2.2.2.cmml" xref="S3.SS3.p4.3.m3.1.1.1.1.1.2.2.2">ğ‘ƒ</ci></apply><ci id="S3.SS3.p4.3.m3.1.1.1.1.1.2.3.cmml" xref="S3.SS3.p4.3.m3.1.1.1.1.1.2.3">ğ‘˜</ci></apply><ci id="S3.SS3.p4.3.m3.1.1.1.1.1.3.cmml" xref="S3.SS3.p4.3.m3.1.1.1.1.1.3">ğ‘—</ci></apply></set><set id="S3.SS3.p4.3.m3.2.2.2.2.cmml" xref="S3.SS3.p4.3.m3.2.2.2.1"><interval closure="open" id="S3.SS3.p4.3.m3.2.2.2.1.1.3.cmml" xref="S3.SS3.p4.3.m3.2.2.2.1.1.2"><apply id="S3.SS3.p4.3.m3.2.2.2.1.1.1.1.cmml" xref="S3.SS3.p4.3.m3.2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.3.m3.2.2.2.1.1.1.1.1.cmml" xref="S3.SS3.p4.3.m3.2.2.2.1.1.1.1">subscript</csymbol><apply id="S3.SS3.p4.3.m3.2.2.2.1.1.1.1.2.cmml" xref="S3.SS3.p4.3.m3.2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.3.m3.2.2.2.1.1.1.1.2.1.cmml" xref="S3.SS3.p4.3.m3.2.2.2.1.1.1.1">superscript</csymbol><apply id="S3.SS3.p4.3.m3.2.2.2.1.1.1.1.2.2.cmml" xref="S3.SS3.p4.3.m3.2.2.2.1.1.1.1.2.2"><ci id="S3.SS3.p4.3.m3.2.2.2.1.1.1.1.2.2.1.cmml" xref="S3.SS3.p4.3.m3.2.2.2.1.1.1.1.2.2.1">~</ci><ci id="S3.SS3.p4.3.m3.2.2.2.1.1.1.1.2.2.2.cmml" xref="S3.SS3.p4.3.m3.2.2.2.1.1.1.1.2.2.2">ğ‘¥</ci></apply><ci id="S3.SS3.p4.3.m3.2.2.2.1.1.1.1.2.3.cmml" xref="S3.SS3.p4.3.m3.2.2.2.1.1.1.1.2.3">ğ‘˜</ci></apply><ci id="S3.SS3.p4.3.m3.2.2.2.1.1.1.1.3.cmml" xref="S3.SS3.p4.3.m3.2.2.2.1.1.1.1.3">ğ‘—</ci></apply><apply id="S3.SS3.p4.3.m3.2.2.2.1.1.2.2.cmml" xref="S3.SS3.p4.3.m3.2.2.2.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS3.p4.3.m3.2.2.2.1.1.2.2.1.cmml" xref="S3.SS3.p4.3.m3.2.2.2.1.1.2.2">subscript</csymbol><apply id="S3.SS3.p4.3.m3.2.2.2.1.1.2.2.2.cmml" xref="S3.SS3.p4.3.m3.2.2.2.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS3.p4.3.m3.2.2.2.1.1.2.2.2.1.cmml" xref="S3.SS3.p4.3.m3.2.2.2.1.1.2.2">superscript</csymbol><apply id="S3.SS3.p4.3.m3.2.2.2.1.1.2.2.2.2.cmml" xref="S3.SS3.p4.3.m3.2.2.2.1.1.2.2.2.2"><ci id="S3.SS3.p4.3.m3.2.2.2.1.1.2.2.2.2.1.cmml" xref="S3.SS3.p4.3.m3.2.2.2.1.1.2.2.2.2.1">~</ci><ci id="S3.SS3.p4.3.m3.2.2.2.1.1.2.2.2.2.2.cmml" xref="S3.SS3.p4.3.m3.2.2.2.1.1.2.2.2.2.2">ğ‘¦</ci></apply><ci id="S3.SS3.p4.3.m3.2.2.2.1.1.2.2.2.3.cmml" xref="S3.SS3.p4.3.m3.2.2.2.1.1.2.2.2.3">ğ‘˜</ci></apply><ci id="S3.SS3.p4.3.m3.2.2.2.1.1.2.2.3.cmml" xref="S3.SS3.p4.3.m3.2.2.2.1.1.2.2.3">ğ‘—</ci></apply></interval></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.3.m3.2c">\{\tilde{P}^{k}_{j}\}=\{(\tilde{x}^{k}_{j},\tilde{y}^{k}_{j})\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.3.m3.2d">{ over~ start_ARG italic_P end_ARG start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } = { ( over~ start_ARG italic_x end_ARG start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , over~ start_ARG italic_y end_ARG start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) }</annotation></semantics></math> where <math alttext="k=\{1,...,V\}" class="ltx_Math" display="inline" id="S3.SS3.p4.4.m4.3"><semantics id="S3.SS3.p4.4.m4.3a"><mrow id="S3.SS3.p4.4.m4.3.4" xref="S3.SS3.p4.4.m4.3.4.cmml"><mi id="S3.SS3.p4.4.m4.3.4.2" xref="S3.SS3.p4.4.m4.3.4.2.cmml">k</mi><mo id="S3.SS3.p4.4.m4.3.4.1" xref="S3.SS3.p4.4.m4.3.4.1.cmml">=</mo><mrow id="S3.SS3.p4.4.m4.3.4.3.2" xref="S3.SS3.p4.4.m4.3.4.3.1.cmml"><mo id="S3.SS3.p4.4.m4.3.4.3.2.1" stretchy="false" xref="S3.SS3.p4.4.m4.3.4.3.1.cmml">{</mo><mn id="S3.SS3.p4.4.m4.1.1" xref="S3.SS3.p4.4.m4.1.1.cmml">1</mn><mo id="S3.SS3.p4.4.m4.3.4.3.2.2" xref="S3.SS3.p4.4.m4.3.4.3.1.cmml">,</mo><mi id="S3.SS3.p4.4.m4.2.2" mathvariant="normal" xref="S3.SS3.p4.4.m4.2.2.cmml">â€¦</mi><mo id="S3.SS3.p4.4.m4.3.4.3.2.3" xref="S3.SS3.p4.4.m4.3.4.3.1.cmml">,</mo><mi id="S3.SS3.p4.4.m4.3.3" xref="S3.SS3.p4.4.m4.3.3.cmml">V</mi><mo id="S3.SS3.p4.4.m4.3.4.3.2.4" stretchy="false" xref="S3.SS3.p4.4.m4.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.4.m4.3b"><apply id="S3.SS3.p4.4.m4.3.4.cmml" xref="S3.SS3.p4.4.m4.3.4"><eq id="S3.SS3.p4.4.m4.3.4.1.cmml" xref="S3.SS3.p4.4.m4.3.4.1"></eq><ci id="S3.SS3.p4.4.m4.3.4.2.cmml" xref="S3.SS3.p4.4.m4.3.4.2">ğ‘˜</ci><set id="S3.SS3.p4.4.m4.3.4.3.1.cmml" xref="S3.SS3.p4.4.m4.3.4.3.2"><cn id="S3.SS3.p4.4.m4.1.1.cmml" type="integer" xref="S3.SS3.p4.4.m4.1.1">1</cn><ci id="S3.SS3.p4.4.m4.2.2.cmml" xref="S3.SS3.p4.4.m4.2.2">â€¦</ci><ci id="S3.SS3.p4.4.m4.3.3.cmml" xref="S3.SS3.p4.4.m4.3.3">ğ‘‰</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.4.m4.3c">k=\{1,...,V\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.4.m4.3d">italic_k = { 1 , â€¦ , italic_V }</annotation></semantics></math>. Secondly, with the jointsâ€™ projected 2D locations as reference points, we independently apply deformable attentionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib48" title="">48</a>]</cite> in each view to let the JQTs extract useful information from image features:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="Pt0.A2.EGx3">
<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle Z^{k}_{j}=\mathrm{DeformAttn}\bigl{(}Q_{j},\tilde{P}^{k}_{j},F_{%
k}\bigr{)}" class="ltx_Math" display="inline" id="S3.E3.m1.3"><semantics id="S3.E3.m1.3a"><mrow id="S3.E3.m1.3.3" xref="S3.E3.m1.3.3.cmml"><msubsup id="S3.E3.m1.3.3.5" xref="S3.E3.m1.3.3.5.cmml"><mi id="S3.E3.m1.3.3.5.2.2" xref="S3.E3.m1.3.3.5.2.2.cmml">Z</mi><mi id="S3.E3.m1.3.3.5.3" xref="S3.E3.m1.3.3.5.3.cmml">j</mi><mi id="S3.E3.m1.3.3.5.2.3" xref="S3.E3.m1.3.3.5.2.3.cmml">k</mi></msubsup><mo id="S3.E3.m1.3.3.4" xref="S3.E3.m1.3.3.4.cmml">=</mo><mrow id="S3.E3.m1.3.3.3" xref="S3.E3.m1.3.3.3.cmml"><mi id="S3.E3.m1.3.3.3.5" xref="S3.E3.m1.3.3.3.5.cmml">DeformAttn</mi><mo id="S3.E3.m1.3.3.3.4" xref="S3.E3.m1.3.3.3.4.cmml">â¢</mo><mrow id="S3.E3.m1.3.3.3.3.3" xref="S3.E3.m1.3.3.3.3.4.cmml"><mo id="S3.E3.m1.3.3.3.3.3.4" maxsize="120%" minsize="120%" xref="S3.E3.m1.3.3.3.3.4.cmml">(</mo><msub id="S3.E3.m1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.2.cmml">Q</mi><mi id="S3.E3.m1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.3.cmml">j</mi></msub><mo id="S3.E3.m1.3.3.3.3.3.5" xref="S3.E3.m1.3.3.3.3.4.cmml">,</mo><msubsup id="S3.E3.m1.2.2.2.2.2.2" xref="S3.E3.m1.2.2.2.2.2.2.cmml"><mover accent="true" id="S3.E3.m1.2.2.2.2.2.2.2.2" xref="S3.E3.m1.2.2.2.2.2.2.2.2.cmml"><mi id="S3.E3.m1.2.2.2.2.2.2.2.2.2" xref="S3.E3.m1.2.2.2.2.2.2.2.2.2.cmml">P</mi><mo id="S3.E3.m1.2.2.2.2.2.2.2.2.1" xref="S3.E3.m1.2.2.2.2.2.2.2.2.1.cmml">~</mo></mover><mi id="S3.E3.m1.2.2.2.2.2.2.3" xref="S3.E3.m1.2.2.2.2.2.2.3.cmml">j</mi><mi id="S3.E3.m1.2.2.2.2.2.2.2.3" xref="S3.E3.m1.2.2.2.2.2.2.2.3.cmml">k</mi></msubsup><mo id="S3.E3.m1.3.3.3.3.3.6" xref="S3.E3.m1.3.3.3.3.4.cmml">,</mo><msub id="S3.E3.m1.3.3.3.3.3.3" xref="S3.E3.m1.3.3.3.3.3.3.cmml"><mi id="S3.E3.m1.3.3.3.3.3.3.2" xref="S3.E3.m1.3.3.3.3.3.3.2.cmml">F</mi><mi id="S3.E3.m1.3.3.3.3.3.3.3" xref="S3.E3.m1.3.3.3.3.3.3.3.cmml">k</mi></msub><mo id="S3.E3.m1.3.3.3.3.3.7" maxsize="120%" minsize="120%" xref="S3.E3.m1.3.3.3.3.4.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.3b"><apply id="S3.E3.m1.3.3.cmml" xref="S3.E3.m1.3.3"><eq id="S3.E3.m1.3.3.4.cmml" xref="S3.E3.m1.3.3.4"></eq><apply id="S3.E3.m1.3.3.5.cmml" xref="S3.E3.m1.3.3.5"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.5.1.cmml" xref="S3.E3.m1.3.3.5">subscript</csymbol><apply id="S3.E3.m1.3.3.5.2.cmml" xref="S3.E3.m1.3.3.5"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.5.2.1.cmml" xref="S3.E3.m1.3.3.5">superscript</csymbol><ci id="S3.E3.m1.3.3.5.2.2.cmml" xref="S3.E3.m1.3.3.5.2.2">ğ‘</ci><ci id="S3.E3.m1.3.3.5.2.3.cmml" xref="S3.E3.m1.3.3.5.2.3">ğ‘˜</ci></apply><ci id="S3.E3.m1.3.3.5.3.cmml" xref="S3.E3.m1.3.3.5.3">ğ‘—</ci></apply><apply id="S3.E3.m1.3.3.3.cmml" xref="S3.E3.m1.3.3.3"><times id="S3.E3.m1.3.3.3.4.cmml" xref="S3.E3.m1.3.3.3.4"></times><ci id="S3.E3.m1.3.3.3.5.cmml" xref="S3.E3.m1.3.3.3.5">DeformAttn</ci><vector id="S3.E3.m1.3.3.3.3.4.cmml" xref="S3.E3.m1.3.3.3.3.3"><apply id="S3.E3.m1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2">ğ‘„</ci><ci id="S3.E3.m1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3">ğ‘—</ci></apply><apply id="S3.E3.m1.2.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.2.2.2.2.1.cmml" xref="S3.E3.m1.2.2.2.2.2.2">subscript</csymbol><apply id="S3.E3.m1.2.2.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.2.2.2.2.2.1.cmml" xref="S3.E3.m1.2.2.2.2.2.2">superscript</csymbol><apply id="S3.E3.m1.2.2.2.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2.2.2.2.2"><ci id="S3.E3.m1.2.2.2.2.2.2.2.2.1.cmml" xref="S3.E3.m1.2.2.2.2.2.2.2.2.1">~</ci><ci id="S3.E3.m1.2.2.2.2.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2.2.2.2.2.2">ğ‘ƒ</ci></apply><ci id="S3.E3.m1.2.2.2.2.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.2.2.2.2.3">ğ‘˜</ci></apply><ci id="S3.E3.m1.2.2.2.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.2.2.2.3">ğ‘—</ci></apply><apply id="S3.E3.m1.3.3.3.3.3.3.cmml" xref="S3.E3.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.3.3.3.3.1.cmml" xref="S3.E3.m1.3.3.3.3.3.3">subscript</csymbol><ci id="S3.E3.m1.3.3.3.3.3.3.2.cmml" xref="S3.E3.m1.3.3.3.3.3.3.2">ğ¹</ci><ci id="S3.E3.m1.3.3.3.3.3.3.3.cmml" xref="S3.E3.m1.3.3.3.3.3.3.3">ğ‘˜</ci></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.3c">\displaystyle Z^{k}_{j}=\mathrm{DeformAttn}\bigl{(}Q_{j},\tilde{P}^{k}_{j},F_{%
k}\bigr{)}</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.3d">italic_Z start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = roman_DeformAttn ( italic_Q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , over~ start_ARG italic_P end_ARG start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_F start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.p4.10">where <math alttext="Q_{j}" class="ltx_Math" display="inline" id="S3.SS3.p4.5.m1.1"><semantics id="S3.SS3.p4.5.m1.1a"><msub id="S3.SS3.p4.5.m1.1.1" xref="S3.SS3.p4.5.m1.1.1.cmml"><mi id="S3.SS3.p4.5.m1.1.1.2" xref="S3.SS3.p4.5.m1.1.1.2.cmml">Q</mi><mi id="S3.SS3.p4.5.m1.1.1.3" xref="S3.SS3.p4.5.m1.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.5.m1.1b"><apply id="S3.SS3.p4.5.m1.1.1.cmml" xref="S3.SS3.p4.5.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.5.m1.1.1.1.cmml" xref="S3.SS3.p4.5.m1.1.1">subscript</csymbol><ci id="S3.SS3.p4.5.m1.1.1.2.cmml" xref="S3.SS3.p4.5.m1.1.1.2">ğ‘„</ci><ci id="S3.SS3.p4.5.m1.1.1.3.cmml" xref="S3.SS3.p4.5.m1.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.5.m1.1c">Q_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.5.m1.1d">italic_Q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> is the <math alttext="j" class="ltx_Math" display="inline" id="S3.SS3.p4.6.m2.1"><semantics id="S3.SS3.p4.6.m2.1a"><mi id="S3.SS3.p4.6.m2.1.1" xref="S3.SS3.p4.6.m2.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.6.m2.1b"><ci id="S3.SS3.p4.6.m2.1.1.cmml" xref="S3.SS3.p4.6.m2.1.1">ğ‘—</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.6.m2.1c">j</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.6.m2.1d">italic_j</annotation></semantics></math>-th JQT and <math alttext="F_{k}" class="ltx_Math" display="inline" id="S3.SS3.p4.7.m3.1"><semantics id="S3.SS3.p4.7.m3.1a"><msub id="S3.SS3.p4.7.m3.1.1" xref="S3.SS3.p4.7.m3.1.1.cmml"><mi id="S3.SS3.p4.7.m3.1.1.2" xref="S3.SS3.p4.7.m3.1.1.2.cmml">F</mi><mi id="S3.SS3.p4.7.m3.1.1.3" xref="S3.SS3.p4.7.m3.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.7.m3.1b"><apply id="S3.SS3.p4.7.m3.1.1.cmml" xref="S3.SS3.p4.7.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.7.m3.1.1.1.cmml" xref="S3.SS3.p4.7.m3.1.1">subscript</csymbol><ci id="S3.SS3.p4.7.m3.1.1.2.cmml" xref="S3.SS3.p4.7.m3.1.1.2">ğ¹</ci><ci id="S3.SS3.p4.7.m3.1.1.3.cmml" xref="S3.SS3.p4.7.m3.1.1.3">ğ‘˜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.7.m3.1c">F_{k}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.7.m3.1d">italic_F start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> is the image feature of the <math alttext="k" class="ltx_Math" display="inline" id="S3.SS3.p4.8.m4.1"><semantics id="S3.SS3.p4.8.m4.1a"><mi id="S3.SS3.p4.8.m4.1.1" xref="S3.SS3.p4.8.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.8.m4.1b"><ci id="S3.SS3.p4.8.m4.1.1.cmml" xref="S3.SS3.p4.8.m4.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.8.m4.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.8.m4.1d">italic_k</annotation></semantics></math>-th view. Notably, if a joint is out of the viewâ€™s FOV, we simply fill the computed result <math alttext="Z^{k}_{j}" class="ltx_Math" display="inline" id="S3.SS3.p4.9.m5.1"><semantics id="S3.SS3.p4.9.m5.1a"><msubsup id="S3.SS3.p4.9.m5.1.1" xref="S3.SS3.p4.9.m5.1.1.cmml"><mi id="S3.SS3.p4.9.m5.1.1.2.2" xref="S3.SS3.p4.9.m5.1.1.2.2.cmml">Z</mi><mi id="S3.SS3.p4.9.m5.1.1.3" xref="S3.SS3.p4.9.m5.1.1.3.cmml">j</mi><mi id="S3.SS3.p4.9.m5.1.1.2.3" xref="S3.SS3.p4.9.m5.1.1.2.3.cmml">k</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.9.m5.1b"><apply id="S3.SS3.p4.9.m5.1.1.cmml" xref="S3.SS3.p4.9.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.9.m5.1.1.1.cmml" xref="S3.SS3.p4.9.m5.1.1">subscript</csymbol><apply id="S3.SS3.p4.9.m5.1.1.2.cmml" xref="S3.SS3.p4.9.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.9.m5.1.1.2.1.cmml" xref="S3.SS3.p4.9.m5.1.1">superscript</csymbol><ci id="S3.SS3.p4.9.m5.1.1.2.2.cmml" xref="S3.SS3.p4.9.m5.1.1.2.2">ğ‘</ci><ci id="S3.SS3.p4.9.m5.1.1.2.3.cmml" xref="S3.SS3.p4.9.m5.1.1.2.3">ğ‘˜</ci></apply><ci id="S3.SS3.p4.9.m5.1.1.3.cmml" xref="S3.SS3.p4.9.m5.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.9.m5.1c">Z^{k}_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.9.m5.1d">italic_Z start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> with zeros to make this information explicit to the model. Finally, the computed results <math alttext="\{Z^{k}_{j}\}" class="ltx_Math" display="inline" id="S3.SS3.p4.10.m6.1"><semantics id="S3.SS3.p4.10.m6.1a"><mrow id="S3.SS3.p4.10.m6.1.1.1" xref="S3.SS3.p4.10.m6.1.1.2.cmml"><mo id="S3.SS3.p4.10.m6.1.1.1.2" stretchy="false" xref="S3.SS3.p4.10.m6.1.1.2.cmml">{</mo><msubsup id="S3.SS3.p4.10.m6.1.1.1.1" xref="S3.SS3.p4.10.m6.1.1.1.1.cmml"><mi id="S3.SS3.p4.10.m6.1.1.1.1.2.2" xref="S3.SS3.p4.10.m6.1.1.1.1.2.2.cmml">Z</mi><mi id="S3.SS3.p4.10.m6.1.1.1.1.3" xref="S3.SS3.p4.10.m6.1.1.1.1.3.cmml">j</mi><mi id="S3.SS3.p4.10.m6.1.1.1.1.2.3" xref="S3.SS3.p4.10.m6.1.1.1.1.2.3.cmml">k</mi></msubsup><mo id="S3.SS3.p4.10.m6.1.1.1.3" stretchy="false" xref="S3.SS3.p4.10.m6.1.1.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.10.m6.1b"><set id="S3.SS3.p4.10.m6.1.1.2.cmml" xref="S3.SS3.p4.10.m6.1.1.1"><apply id="S3.SS3.p4.10.m6.1.1.1.1.cmml" xref="S3.SS3.p4.10.m6.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.10.m6.1.1.1.1.1.cmml" xref="S3.SS3.p4.10.m6.1.1.1.1">subscript</csymbol><apply id="S3.SS3.p4.10.m6.1.1.1.1.2.cmml" xref="S3.SS3.p4.10.m6.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.10.m6.1.1.1.1.2.1.cmml" xref="S3.SS3.p4.10.m6.1.1.1.1">superscript</csymbol><ci id="S3.SS3.p4.10.m6.1.1.1.1.2.2.cmml" xref="S3.SS3.p4.10.m6.1.1.1.1.2.2">ğ‘</ci><ci id="S3.SS3.p4.10.m6.1.1.1.1.2.3.cmml" xref="S3.SS3.p4.10.m6.1.1.1.1.2.3">ğ‘˜</ci></apply><ci id="S3.SS3.p4.10.m6.1.1.1.1.3.cmml" xref="S3.SS3.p4.10.m6.1.1.1.1.3">ğ‘—</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.10.m6.1c">\{Z^{k}_{j}\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.10.m6.1d">{ italic_Z start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT }</annotation></semantics></math> from each view are concatenated and fed into a linear projection layer to fuse multi-view stereo information and transform the result to the original dimension:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="Pt0.A2.EGx4">
<tbody id="S3.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle Z_{j}=\mathrm{Linear}\bigl{(}\mathrm{Concat}_{\{k\}}(Z^{k}_{j})%
\bigr{)}" class="ltx_Math" display="inline" id="S3.E4.m1.2"><semantics id="S3.E4.m1.2a"><mrow id="S3.E4.m1.2.2" xref="S3.E4.m1.2.2.cmml"><msub id="S3.E4.m1.2.2.3" xref="S3.E4.m1.2.2.3.cmml"><mi id="S3.E4.m1.2.2.3.2" xref="S3.E4.m1.2.2.3.2.cmml">Z</mi><mi id="S3.E4.m1.2.2.3.3" xref="S3.E4.m1.2.2.3.3.cmml">j</mi></msub><mo id="S3.E4.m1.2.2.2" xref="S3.E4.m1.2.2.2.cmml">=</mo><mrow id="S3.E4.m1.2.2.1" xref="S3.E4.m1.2.2.1.cmml"><mi id="S3.E4.m1.2.2.1.3" xref="S3.E4.m1.2.2.1.3.cmml">Linear</mi><mo id="S3.E4.m1.2.2.1.2" xref="S3.E4.m1.2.2.1.2.cmml">â¢</mo><mrow id="S3.E4.m1.2.2.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.cmml"><mo id="S3.E4.m1.2.2.1.1.1.2" maxsize="120%" minsize="120%" xref="S3.E4.m1.2.2.1.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.2.2.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.cmml"><msub id="S3.E4.m1.2.2.1.1.1.1.3" xref="S3.E4.m1.2.2.1.1.1.1.3.cmml"><mi id="S3.E4.m1.2.2.1.1.1.1.3.2" xref="S3.E4.m1.2.2.1.1.1.1.3.2.cmml">Concat</mi><mrow id="S3.E4.m1.1.1.1.3" xref="S3.E4.m1.1.1.1.2.cmml"><mo id="S3.E4.m1.1.1.1.3.1" stretchy="false" xref="S3.E4.m1.1.1.1.2.cmml">{</mo><mi id="S3.E4.m1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml">k</mi><mo id="S3.E4.m1.1.1.1.3.2" stretchy="false" xref="S3.E4.m1.1.1.1.2.cmml">}</mo></mrow></msub><mo id="S3.E4.m1.2.2.1.1.1.1.2" xref="S3.E4.m1.2.2.1.1.1.1.2.cmml">â¢</mo><mrow id="S3.E4.m1.2.2.1.1.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.cmml"><mo id="S3.E4.m1.2.2.1.1.1.1.1.1.2" stretchy="false" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.cmml">(</mo><msubsup id="S3.E4.m1.2.2.1.1.1.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2.cmml">Z</mi><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.cmml">j</mi><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.3" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.3.cmml">k</mi></msubsup><mo id="S3.E4.m1.2.2.1.1.1.1.1.1.3" stretchy="false" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E4.m1.2.2.1.1.1.3" maxsize="120%" minsize="120%" xref="S3.E4.m1.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.2b"><apply id="S3.E4.m1.2.2.cmml" xref="S3.E4.m1.2.2"><eq id="S3.E4.m1.2.2.2.cmml" xref="S3.E4.m1.2.2.2"></eq><apply id="S3.E4.m1.2.2.3.cmml" xref="S3.E4.m1.2.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.3.1.cmml" xref="S3.E4.m1.2.2.3">subscript</csymbol><ci id="S3.E4.m1.2.2.3.2.cmml" xref="S3.E4.m1.2.2.3.2">ğ‘</ci><ci id="S3.E4.m1.2.2.3.3.cmml" xref="S3.E4.m1.2.2.3.3">ğ‘—</ci></apply><apply id="S3.E4.m1.2.2.1.cmml" xref="S3.E4.m1.2.2.1"><times id="S3.E4.m1.2.2.1.2.cmml" xref="S3.E4.m1.2.2.1.2"></times><ci id="S3.E4.m1.2.2.1.3.cmml" xref="S3.E4.m1.2.2.1.3">Linear</ci><apply id="S3.E4.m1.2.2.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1"><times id="S3.E4.m1.2.2.1.1.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.2"></times><apply id="S3.E4.m1.2.2.1.1.1.1.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.1.1.3.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.2.2.1.1.1.1.3.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.3.2">Concat</ci><set id="S3.E4.m1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.3"><ci id="S3.E4.m1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1">ğ‘˜</ci></set></apply><apply id="S3.E4.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2">ğ‘</ci><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.3">ğ‘˜</ci></apply><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3">ğ‘—</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.2c">\displaystyle Z_{j}=\mathrm{Linear}\bigl{(}\mathrm{Concat}_{\{k\}}(Z^{k}_{j})%
\bigr{)}</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.2d">italic_Z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = roman_Linear ( roman_Concat start_POSTSUBSCRIPT { italic_k } end_POSTSUBSCRIPT ( italic_Z start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.p4.11">In this way, the Deformable Stereo AttentionÂ becomes an <span class="ltx_text ltx_font_italic" id="S3.SS3.p4.11.1">atomic</span> attention operation and can replace the normal cross-attention operation in a plug-and-play manner. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p5">
<p class="ltx_p" id="S3.SS3.p5.2"><span class="ltx_text ltx_font_bold" id="S3.SS3.p5.2.1">Predicting Refinement Offsets.</span>
Each PRFormerÂ layer empowers a JQT to engage with the multi-view image features and other JQTs to exploit stereo and human kinematic information, imbuing it with joint-specific knowledge. Based on this enriched representation, we use a shallow MLP to predict the refinement offset relative to the pose proposal. Specifically, the refinement offset for joint <math alttext="j" class="ltx_Math" display="inline" id="S3.SS3.p5.1.m1.1"><semantics id="S3.SS3.p5.1.m1.1a"><mi id="S3.SS3.p5.1.m1.1.1" xref="S3.SS3.p5.1.m1.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.1.m1.1b"><ci id="S3.SS3.p5.1.m1.1.1.cmml" xref="S3.SS3.p5.1.m1.1.1">ğ‘—</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.1.m1.1c">j</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.1.m1.1d">italic_j</annotation></semantics></math> at the <math alttext="s" class="ltx_Math" display="inline" id="S3.SS3.p5.2.m2.1"><semantics id="S3.SS3.p5.2.m2.1a"><mi id="S3.SS3.p5.2.m2.1.1" xref="S3.SS3.p5.2.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.2.m2.1b"><ci id="S3.SS3.p5.2.m2.1.1.cmml" xref="S3.SS3.p5.2.m2.1.1">ğ‘ </ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.2.m2.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.2.m2.1d">italic_s</annotation></semantics></math>-th PRFormerÂ layer is computed as:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="Pt0.A2.EGx5">
<tbody id="S3.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\Delta P^{s}_{j}=\mathrm{MLP}_{\mathrm{offset}}^{s}(Q^{s}_{j})" class="ltx_Math" display="inline" id="S3.E5.m1.1"><semantics id="S3.E5.m1.1a"><mrow id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml"><mrow id="S3.E5.m1.1.1.3" xref="S3.E5.m1.1.1.3.cmml"><mi id="S3.E5.m1.1.1.3.2" mathvariant="normal" xref="S3.E5.m1.1.1.3.2.cmml">Î”</mi><mo id="S3.E5.m1.1.1.3.1" xref="S3.E5.m1.1.1.3.1.cmml">â¢</mo><msubsup id="S3.E5.m1.1.1.3.3" xref="S3.E5.m1.1.1.3.3.cmml"><mi id="S3.E5.m1.1.1.3.3.2.2" xref="S3.E5.m1.1.1.3.3.2.2.cmml">P</mi><mi id="S3.E5.m1.1.1.3.3.3" xref="S3.E5.m1.1.1.3.3.3.cmml">j</mi><mi id="S3.E5.m1.1.1.3.3.2.3" xref="S3.E5.m1.1.1.3.3.2.3.cmml">s</mi></msubsup></mrow><mo id="S3.E5.m1.1.1.2" xref="S3.E5.m1.1.1.2.cmml">=</mo><mrow id="S3.E5.m1.1.1.1" xref="S3.E5.m1.1.1.1.cmml"><msubsup id="S3.E5.m1.1.1.1.3" xref="S3.E5.m1.1.1.1.3.cmml"><mi id="S3.E5.m1.1.1.1.3.2.2" xref="S3.E5.m1.1.1.1.3.2.2.cmml">MLP</mi><mi id="S3.E5.m1.1.1.1.3.2.3" xref="S3.E5.m1.1.1.1.3.2.3.cmml">offset</mi><mi id="S3.E5.m1.1.1.1.3.3" xref="S3.E5.m1.1.1.1.3.3.cmml">s</mi></msubsup><mo id="S3.E5.m1.1.1.1.2" xref="S3.E5.m1.1.1.1.2.cmml">â¢</mo><mrow id="S3.E5.m1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.cmml"><mo id="S3.E5.m1.1.1.1.1.1.2" stretchy="false" xref="S3.E5.m1.1.1.1.1.1.1.cmml">(</mo><msubsup id="S3.E5.m1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.cmml"><mi id="S3.E5.m1.1.1.1.1.1.1.2.2" xref="S3.E5.m1.1.1.1.1.1.1.2.2.cmml">Q</mi><mi id="S3.E5.m1.1.1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.1.1.3.cmml">j</mi><mi id="S3.E5.m1.1.1.1.1.1.1.2.3" xref="S3.E5.m1.1.1.1.1.1.1.2.3.cmml">s</mi></msubsup><mo id="S3.E5.m1.1.1.1.1.1.3" stretchy="false" xref="S3.E5.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.1b"><apply id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1"><eq id="S3.E5.m1.1.1.2.cmml" xref="S3.E5.m1.1.1.2"></eq><apply id="S3.E5.m1.1.1.3.cmml" xref="S3.E5.m1.1.1.3"><times id="S3.E5.m1.1.1.3.1.cmml" xref="S3.E5.m1.1.1.3.1"></times><ci id="S3.E5.m1.1.1.3.2.cmml" xref="S3.E5.m1.1.1.3.2">Î”</ci><apply id="S3.E5.m1.1.1.3.3.cmml" xref="S3.E5.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.3.3.1.cmml" xref="S3.E5.m1.1.1.3.3">subscript</csymbol><apply id="S3.E5.m1.1.1.3.3.2.cmml" xref="S3.E5.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.3.3.2.1.cmml" xref="S3.E5.m1.1.1.3.3">superscript</csymbol><ci id="S3.E5.m1.1.1.3.3.2.2.cmml" xref="S3.E5.m1.1.1.3.3.2.2">ğ‘ƒ</ci><ci id="S3.E5.m1.1.1.3.3.2.3.cmml" xref="S3.E5.m1.1.1.3.3.2.3">ğ‘ </ci></apply><ci id="S3.E5.m1.1.1.3.3.3.cmml" xref="S3.E5.m1.1.1.3.3.3">ğ‘—</ci></apply></apply><apply id="S3.E5.m1.1.1.1.cmml" xref="S3.E5.m1.1.1.1"><times id="S3.E5.m1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.2"></times><apply id="S3.E5.m1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.3.1.cmml" xref="S3.E5.m1.1.1.1.3">superscript</csymbol><apply id="S3.E5.m1.1.1.1.3.2.cmml" xref="S3.E5.m1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.3.2.1.cmml" xref="S3.E5.m1.1.1.1.3">subscript</csymbol><ci id="S3.E5.m1.1.1.1.3.2.2.cmml" xref="S3.E5.m1.1.1.1.3.2.2">MLP</ci><ci id="S3.E5.m1.1.1.1.3.2.3.cmml" xref="S3.E5.m1.1.1.1.3.2.3">offset</ci></apply><ci id="S3.E5.m1.1.1.1.3.3.cmml" xref="S3.E5.m1.1.1.1.3.3">ğ‘ </ci></apply><apply id="S3.E5.m1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1">subscript</csymbol><apply id="S3.E5.m1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E5.m1.1.1.1.1.1">superscript</csymbol><ci id="S3.E5.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.2.2">ğ‘„</ci><ci id="S3.E5.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.2.3">ğ‘ </ci></apply><ci id="S3.E5.m1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.3">ğ‘—</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.1c">\displaystyle\Delta P^{s}_{j}=\mathrm{MLP}_{\mathrm{offset}}^{s}(Q^{s}_{j})</annotation><annotation encoding="application/x-llamapun" id="S3.E5.m1.1d">roman_Î” italic_P start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = roman_MLP start_POSTSUBSCRIPT roman_offset end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ( italic_Q start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Loss Function</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.8">To keep our method concise, we employ a simple per-joint error lossÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib3" title="">3</a>]</cite> to train our model. Specifically, the loss function is formulated as the following:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="Pt0.A2.EGx6">
<tbody id="S3.E6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{L}=\sum_{s=0}^{S}\sum_{j=1}^{N_{j}}{||P^{s}_{j}-\hat{P}_%
{j}||_{2}}" class="ltx_Math" display="inline" id="S3.E6.m1.1"><semantics id="S3.E6.m1.1a"><mrow id="S3.E6.m1.1.1" xref="S3.E6.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E6.m1.1.1.3" xref="S3.E6.m1.1.1.3.cmml">â„’</mi><mo id="S3.E6.m1.1.1.2" xref="S3.E6.m1.1.1.2.cmml">=</mo><mrow id="S3.E6.m1.1.1.1" xref="S3.E6.m1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E6.m1.1.1.1.2" xref="S3.E6.m1.1.1.1.2.cmml"><munderover id="S3.E6.m1.1.1.1.2a" xref="S3.E6.m1.1.1.1.2.cmml"><mo id="S3.E6.m1.1.1.1.2.2.2" movablelimits="false" xref="S3.E6.m1.1.1.1.2.2.2.cmml">âˆ‘</mo><mrow id="S3.E6.m1.1.1.1.2.2.3" xref="S3.E6.m1.1.1.1.2.2.3.cmml"><mi id="S3.E6.m1.1.1.1.2.2.3.2" xref="S3.E6.m1.1.1.1.2.2.3.2.cmml">s</mi><mo id="S3.E6.m1.1.1.1.2.2.3.1" xref="S3.E6.m1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E6.m1.1.1.1.2.2.3.3" xref="S3.E6.m1.1.1.1.2.2.3.3.cmml">0</mn></mrow><mi id="S3.E6.m1.1.1.1.2.3" xref="S3.E6.m1.1.1.1.2.3.cmml">S</mi></munderover></mstyle><mrow id="S3.E6.m1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E6.m1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.2.cmml"><munderover id="S3.E6.m1.1.1.1.1.2a" xref="S3.E6.m1.1.1.1.1.2.cmml"><mo id="S3.E6.m1.1.1.1.1.2.2.2" movablelimits="false" xref="S3.E6.m1.1.1.1.1.2.2.2.cmml">âˆ‘</mo><mrow id="S3.E6.m1.1.1.1.1.2.2.3" xref="S3.E6.m1.1.1.1.1.2.2.3.cmml"><mi id="S3.E6.m1.1.1.1.1.2.2.3.2" xref="S3.E6.m1.1.1.1.1.2.2.3.2.cmml">j</mi><mo id="S3.E6.m1.1.1.1.1.2.2.3.1" xref="S3.E6.m1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E6.m1.1.1.1.1.2.2.3.3" xref="S3.E6.m1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><msub id="S3.E6.m1.1.1.1.1.2.3" xref="S3.E6.m1.1.1.1.1.2.3.cmml"><mi id="S3.E6.m1.1.1.1.1.2.3.2" xref="S3.E6.m1.1.1.1.1.2.3.2.cmml">N</mi><mi id="S3.E6.m1.1.1.1.1.2.3.3" xref="S3.E6.m1.1.1.1.1.2.3.3.cmml">j</mi></msub></munderover></mstyle><msub id="S3.E6.m1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.cmml"><mrow id="S3.E6.m1.1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.2.cmml"><mo id="S3.E6.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E6.m1.1.1.1.1.1.1.2.1.cmml">â€–</mo><mrow id="S3.E6.m1.1.1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.1.1.cmml"><msubsup id="S3.E6.m1.1.1.1.1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E6.m1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E6.m1.1.1.1.1.1.1.1.1.2.2.2.cmml">P</mi><mi id="S3.E6.m1.1.1.1.1.1.1.1.1.2.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.2.3.cmml">j</mi><mi id="S3.E6.m1.1.1.1.1.1.1.1.1.2.2.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.2.2.3.cmml">s</mi></msubsup><mo id="S3.E6.m1.1.1.1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><msub id="S3.E6.m1.1.1.1.1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.3.cmml"><mover accent="true" id="S3.E6.m1.1.1.1.1.1.1.1.1.3.2" xref="S3.E6.m1.1.1.1.1.1.1.1.1.3.2.cmml"><mi id="S3.E6.m1.1.1.1.1.1.1.1.1.3.2.2" xref="S3.E6.m1.1.1.1.1.1.1.1.1.3.2.2.cmml">P</mi><mo id="S3.E6.m1.1.1.1.1.1.1.1.1.3.2.1" xref="S3.E6.m1.1.1.1.1.1.1.1.1.3.2.1.cmml">^</mo></mover><mi id="S3.E6.m1.1.1.1.1.1.1.1.1.3.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.3.3.cmml">j</mi></msub></mrow><mo id="S3.E6.m1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E6.m1.1.1.1.1.1.1.2.1.cmml">â€–</mo></mrow><mn id="S3.E6.m1.1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.1.3.cmml">2</mn></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.1b"><apply id="S3.E6.m1.1.1.cmml" xref="S3.E6.m1.1.1"><eq id="S3.E6.m1.1.1.2.cmml" xref="S3.E6.m1.1.1.2"></eq><ci id="S3.E6.m1.1.1.3.cmml" xref="S3.E6.m1.1.1.3">â„’</ci><apply id="S3.E6.m1.1.1.1.cmml" xref="S3.E6.m1.1.1.1"><apply id="S3.E6.m1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.2.1.cmml" xref="S3.E6.m1.1.1.1.2">superscript</csymbol><apply id="S3.E6.m1.1.1.1.2.2.cmml" xref="S3.E6.m1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.2.2.1.cmml" xref="S3.E6.m1.1.1.1.2">subscript</csymbol><sum id="S3.E6.m1.1.1.1.2.2.2.cmml" xref="S3.E6.m1.1.1.1.2.2.2"></sum><apply id="S3.E6.m1.1.1.1.2.2.3.cmml" xref="S3.E6.m1.1.1.1.2.2.3"><eq id="S3.E6.m1.1.1.1.2.2.3.1.cmml" xref="S3.E6.m1.1.1.1.2.2.3.1"></eq><ci id="S3.E6.m1.1.1.1.2.2.3.2.cmml" xref="S3.E6.m1.1.1.1.2.2.3.2">ğ‘ </ci><cn id="S3.E6.m1.1.1.1.2.2.3.3.cmml" type="integer" xref="S3.E6.m1.1.1.1.2.2.3.3">0</cn></apply></apply><ci id="S3.E6.m1.1.1.1.2.3.cmml" xref="S3.E6.m1.1.1.1.2.3">ğ‘†</ci></apply><apply id="S3.E6.m1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1"><apply id="S3.E6.m1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.2.1.cmml" xref="S3.E6.m1.1.1.1.1.2">superscript</csymbol><apply id="S3.E6.m1.1.1.1.1.2.2.cmml" xref="S3.E6.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.2.2.1.cmml" xref="S3.E6.m1.1.1.1.1.2">subscript</csymbol><sum id="S3.E6.m1.1.1.1.1.2.2.2.cmml" xref="S3.E6.m1.1.1.1.1.2.2.2"></sum><apply id="S3.E6.m1.1.1.1.1.2.2.3.cmml" xref="S3.E6.m1.1.1.1.1.2.2.3"><eq id="S3.E6.m1.1.1.1.1.2.2.3.1.cmml" xref="S3.E6.m1.1.1.1.1.2.2.3.1"></eq><ci id="S3.E6.m1.1.1.1.1.2.2.3.2.cmml" xref="S3.E6.m1.1.1.1.1.2.2.3.2">ğ‘—</ci><cn id="S3.E6.m1.1.1.1.1.2.2.3.3.cmml" type="integer" xref="S3.E6.m1.1.1.1.1.2.2.3.3">1</cn></apply></apply><apply id="S3.E6.m1.1.1.1.1.2.3.cmml" xref="S3.E6.m1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.2.3.1.cmml" xref="S3.E6.m1.1.1.1.1.2.3">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.2.3.2.cmml" xref="S3.E6.m1.1.1.1.1.2.3.2">ğ‘</ci><ci id="S3.E6.m1.1.1.1.1.2.3.3.cmml" xref="S3.E6.m1.1.1.1.1.2.3.3">ğ‘—</ci></apply></apply><apply id="S3.E6.m1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.1">subscript</csymbol><apply id="S3.E6.m1.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E6.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1"><minus id="S3.E6.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1"></minus><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.2">subscript</csymbol><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.2">superscript</csymbol><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.2.2.2">ğ‘ƒ</ci><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.2.2.3">ğ‘ </ci></apply><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.2.3">ğ‘—</ci></apply><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.3">subscript</csymbol><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.3.2"><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.3.2.1">^</ci><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.3.2.2">ğ‘ƒ</ci></apply><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.3.3">ğ‘—</ci></apply></apply></apply><cn id="S3.E6.m1.1.1.1.1.1.3.cmml" type="integer" xref="S3.E6.m1.1.1.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.1c">\displaystyle\mathcal{L}=\sum_{s=0}^{S}\sum_{j=1}^{N_{j}}{||P^{s}_{j}-\hat{P}_%
{j}||_{2}}</annotation><annotation encoding="application/x-llamapun" id="S3.E6.m1.1d">caligraphic_L = âˆ‘ start_POSTSUBSCRIPT italic_s = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT âˆ‘ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUPERSCRIPT | | italic_P start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT - over^ start_ARG italic_P end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | | start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS4.p1.7">Here, different loss stages are indexed with <math alttext="s" class="ltx_Math" display="inline" id="S3.SS4.p1.1.m1.1"><semantics id="S3.SS4.p1.1.m1.1a"><mi id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><ci id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">ğ‘ </ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.1.m1.1d">italic_s</annotation></semantics></math>. Specifically, <math alttext="s=0" class="ltx_Math" display="inline" id="S3.SS4.p1.2.m2.1"><semantics id="S3.SS4.p1.2.m2.1a"><mrow id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml"><mi id="S3.SS4.p1.2.m2.1.1.2" xref="S3.SS4.p1.2.m2.1.1.2.cmml">s</mi><mo id="S3.SS4.p1.2.m2.1.1.1" xref="S3.SS4.p1.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS4.p1.2.m2.1.1.3" xref="S3.SS4.p1.2.m2.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.1b"><apply id="S3.SS4.p1.2.m2.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1"><eq id="S3.SS4.p1.2.m2.1.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1.1"></eq><ci id="S3.SS4.p1.2.m2.1.1.2.cmml" xref="S3.SS4.p1.2.m2.1.1.2">ğ‘ </ci><cn id="S3.SS4.p1.2.m2.1.1.3.cmml" type="integer" xref="S3.SS4.p1.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.1c">s=0</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.2.m2.1d">italic_s = 0</annotation></semantics></math> corresponds to the pose proposals generated by PPN, and <math alttext="s&gt;0" class="ltx_Math" display="inline" id="S3.SS4.p1.3.m3.1"><semantics id="S3.SS4.p1.3.m3.1a"><mrow id="S3.SS4.p1.3.m3.1.1" xref="S3.SS4.p1.3.m3.1.1.cmml"><mi id="S3.SS4.p1.3.m3.1.1.2" xref="S3.SS4.p1.3.m3.1.1.2.cmml">s</mi><mo id="S3.SS4.p1.3.m3.1.1.1" xref="S3.SS4.p1.3.m3.1.1.1.cmml">&gt;</mo><mn id="S3.SS4.p1.3.m3.1.1.3" xref="S3.SS4.p1.3.m3.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.3.m3.1b"><apply id="S3.SS4.p1.3.m3.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1"><gt id="S3.SS4.p1.3.m3.1.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1.1"></gt><ci id="S3.SS4.p1.3.m3.1.1.2.cmml" xref="S3.SS4.p1.3.m3.1.1.2">ğ‘ </ci><cn id="S3.SS4.p1.3.m3.1.1.3.cmml" type="integer" xref="S3.SS4.p1.3.m3.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.3.m3.1c">s&gt;0</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.3.m3.1d">italic_s &gt; 0</annotation></semantics></math> represents the subsequent refinement predictions derived from the transformer layers. <math alttext="N_{j}" class="ltx_Math" display="inline" id="S3.SS4.p1.4.m4.1"><semantics id="S3.SS4.p1.4.m4.1a"><msub id="S3.SS4.p1.4.m4.1.1" xref="S3.SS4.p1.4.m4.1.1.cmml"><mi id="S3.SS4.p1.4.m4.1.1.2" xref="S3.SS4.p1.4.m4.1.1.2.cmml">N</mi><mi id="S3.SS4.p1.4.m4.1.1.3" xref="S3.SS4.p1.4.m4.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.4.m4.1b"><apply id="S3.SS4.p1.4.m4.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.4.m4.1.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS4.p1.4.m4.1.1.2.cmml" xref="S3.SS4.p1.4.m4.1.1.2">ğ‘</ci><ci id="S3.SS4.p1.4.m4.1.1.3.cmml" xref="S3.SS4.p1.4.m4.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.4.m4.1c">N_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.4.m4.1d">italic_N start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> is the total number of joints; <math alttext="P^{*}_{j}" class="ltx_Math" display="inline" id="S3.SS4.p1.5.m5.1"><semantics id="S3.SS4.p1.5.m5.1a"><msubsup id="S3.SS4.p1.5.m5.1.1" xref="S3.SS4.p1.5.m5.1.1.cmml"><mi id="S3.SS4.p1.5.m5.1.1.2.2" xref="S3.SS4.p1.5.m5.1.1.2.2.cmml">P</mi><mi id="S3.SS4.p1.5.m5.1.1.3" xref="S3.SS4.p1.5.m5.1.1.3.cmml">j</mi><mo id="S3.SS4.p1.5.m5.1.1.2.3" xref="S3.SS4.p1.5.m5.1.1.2.3.cmml">âˆ—</mo></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.5.m5.1b"><apply id="S3.SS4.p1.5.m5.1.1.cmml" xref="S3.SS4.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.5.m5.1.1.1.cmml" xref="S3.SS4.p1.5.m5.1.1">subscript</csymbol><apply id="S3.SS4.p1.5.m5.1.1.2.cmml" xref="S3.SS4.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.5.m5.1.1.2.1.cmml" xref="S3.SS4.p1.5.m5.1.1">superscript</csymbol><ci id="S3.SS4.p1.5.m5.1.1.2.2.cmml" xref="S3.SS4.p1.5.m5.1.1.2.2">ğ‘ƒ</ci><times id="S3.SS4.p1.5.m5.1.1.2.3.cmml" xref="S3.SS4.p1.5.m5.1.1.2.3"></times></apply><ci id="S3.SS4.p1.5.m5.1.1.3.cmml" xref="S3.SS4.p1.5.m5.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.5.m5.1c">P^{*}_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.5.m5.1d">italic_P start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\hat{P}_{j}" class="ltx_Math" display="inline" id="S3.SS4.p1.6.m6.1"><semantics id="S3.SS4.p1.6.m6.1a"><msub id="S3.SS4.p1.6.m6.1.1" xref="S3.SS4.p1.6.m6.1.1.cmml"><mover accent="true" id="S3.SS4.p1.6.m6.1.1.2" xref="S3.SS4.p1.6.m6.1.1.2.cmml"><mi id="S3.SS4.p1.6.m6.1.1.2.2" xref="S3.SS4.p1.6.m6.1.1.2.2.cmml">P</mi><mo id="S3.SS4.p1.6.m6.1.1.2.1" xref="S3.SS4.p1.6.m6.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS4.p1.6.m6.1.1.3" xref="S3.SS4.p1.6.m6.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.6.m6.1b"><apply id="S3.SS4.p1.6.m6.1.1.cmml" xref="S3.SS4.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.6.m6.1.1.1.cmml" xref="S3.SS4.p1.6.m6.1.1">subscript</csymbol><apply id="S3.SS4.p1.6.m6.1.1.2.cmml" xref="S3.SS4.p1.6.m6.1.1.2"><ci id="S3.SS4.p1.6.m6.1.1.2.1.cmml" xref="S3.SS4.p1.6.m6.1.1.2.1">^</ci><ci id="S3.SS4.p1.6.m6.1.1.2.2.cmml" xref="S3.SS4.p1.6.m6.1.1.2.2">ğ‘ƒ</ci></apply><ci id="S3.SS4.p1.6.m6.1.1.3.cmml" xref="S3.SS4.p1.6.m6.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.6.m6.1c">\hat{P}_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.6.m6.1d">over^ start_ARG italic_P end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> are the predicted and ground-truth 3D coordinates of the <math alttext="j" class="ltx_Math" display="inline" id="S3.SS4.p1.7.m7.1"><semantics id="S3.SS4.p1.7.m7.1a"><mi id="S3.SS4.p1.7.m7.1.1" xref="S3.SS4.p1.7.m7.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.7.m7.1b"><ci id="S3.SS4.p1.7.m7.1.1.cmml" xref="S3.SS4.p1.7.m7.1.1">ğ‘—</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.7.m7.1c">j</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.7.m7.1d">italic_j</annotation></semantics></math>-th joint.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Feature Extractor</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">We follow UnrealEgoÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib3" title="">3</a>]</cite> to adopt a UNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib27" title="">27</a>]</cite> architecture as our visual feature extractor to compute the multi-view image features. The key difference in our method is the exclusion of multi-view concatenation in the decoder partÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib3" title="">3</a>]</cite>. As the PRFormerÂ can effectively process multi-view stereo features, we can extract features from each view independently instead of aggregating multi-view features within the feature extractor. As we show inÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S4.SS2" title="4.2 Main Results â€£ 4 Experiments â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Sec.</span>Â <span class="ltx_text ltx_ref_tag">4.2</span></a>, this modification significantly reduces the model size and improves its computational efficiency.</p>
</div>
<div class="ltx_para" id="S3.SS5.p2">
<p class="ltx_p" id="S3.SS5.p2.1">Inspired by the heatmap-based methodsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib3" title="">3</a>]</cite>, we further enhance our modelâ€™s efficacy by pre-training its feature extractor to predict 2D joint heatmaps. As shown inÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S3.F3" title="In 3.3 Pose Refinement Transformer (PRFormer) â€£ 3 Method â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">3</span></a> right, during the pre-training phase, the model predicts 2D joint heatmaps for each view using a light-weighted fully convolutional headÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib3" title="">3</a>]</cite>, which is removed after pre-training.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experiment Settings</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.1">Dataset Settings.</span>
We use the multi-view UnrealEgoÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib3" title="">3</a>]</cite> dataset to benchmark our proposed method. The UnrealEgo dataset has 451k synthetic stereo views collected using 30 different actions, which are captured by two head-mounted cameras placed 1cm from the head. We follow the official dataset splits: the model is trained using the training set (357k views) and evaluated using the test set (48k views); The validation set (46k views) is used for tuning the hyperparameters. We also test our method using the monocular SceneEgoÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib37" title="">37</a>]</cite> dataset to test its generalization ability to monocular settings. SceneEgo is a real-human dataset recorded by two actors with different daily actions. It has a total 28k images. For both datsets, we report the Mean Per Joint Position Error (MPJPE) and Procrustes Analysis MPJPE (PA-MPJPE) in millimeters as evaluation metrics following their official papersÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib37" title="">37</a>]</cite>. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">Model and Training Settings.</span>
Following their original papersÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib37" title="">37</a>]</cite>, we use ResNet-18Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib9" title="">9</a>]</cite> as the backbone network for UnrealEgo and ResNet-50 for SceneEgo. FollowingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib3" title="">3</a>]</cite>, we set the visual featureâ€™s down-sampling stride to 4. We employ three layers of transformer decoder in our PRFormer. We use very similar training recipes for both datasets without careful tuning. Specifically, for both 2D joint heatmap pre-training and 3D pose estimation, we set the batch size to 16 and trained the model for 12 epochs. AdamWÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib20" title="">20</a>]</cite> is used as the default optimizer. The initial learning rate is set to 0.001 and weight decay is set to 0.005. We set the gradient clip to 5.0. The learning rates are decayed by a factor of 10 after the 8th and 11th epochs.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.3.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S4.T1.4.2" style="font-size:90%;">Comparison of MPJPE (PA-MPJPE) between our method and previous state-of-the-art approaches on the UnrealEgo dataset.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.5" style="width:433.6pt;height:166.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-205.8pt,78.9pt) scale(0.512973142419809,0.512973142419809) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.5.1">
<tr class="ltx_tr" id="S4.T1.5.1.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T1.5.1.1.1">Method</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.5.1.1.2">Jump</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.5.1.1.3">Fall</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.5.1.1.4">Exercise</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.5.1.1.5">Pull</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.5.1.1.6">Sing</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.5.1.1.7">Roll</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.5.1.1.8">Crawl</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.5.1.1.9">Lay</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.5.1.1.10">Sitting</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.5.1.1.11">Crouch-N</td>
<td class="ltx_td ltx_border_tt" id="S4.T1.5.1.1.12"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.2">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.5.1.2.1">xr-EgoPose<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib32" title="">32</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.2.2">106.3 ( - )</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.2.3">167.2 ( - )</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.2.4">133.2 ( - )</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.2.5">119.5 ( - )</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.2.6">99.6 ( - )</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.2.7">116.1 ( - )</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.2.8">223.5 ( - )</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.2.9">146.7 ( - )</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.2.10">274.9 ( - )</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.5.1.2.11">172.2 ( - )</td>
<td class="ltx_td ltx_border_t" id="S4.T1.5.1.2.12"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.5.1.3.1">EgoGlass<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib45" title="">45</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.3.2">81.3(63.4)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.3.3">131.6(94.7)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.3.4">100.2(74.1)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.3.5">81.9(62.0)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.3.6">70.6(52.6)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.3.7">103.3(92.7)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.3.8">182.4(113.6)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.3.9">109.6(81.7)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.3.10">207.3(114.7)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.5.1.3.11">132.7(110.8)</td>
<td class="ltx_td" id="S4.T1.5.1.3.12"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.5.1.4.1">UnrealEgo<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib3" title="">3</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.4.2">76.6(61.1)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.4.3">126.8(95.9)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.4.4">90.3(68.4)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.4.5">78.2(61.4)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.4.6">67.3(49.8)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.4.7">86.2(71.6)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.4.8">181.5(116.4)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.4.9">97.8(76.6)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.4.10">194.3(150.3)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.5.1.4.11">116.8(97.9)</td>
<td class="ltx_td" id="S4.T1.5.1.4.12"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.5.1.5.1">Ego3DPose<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib12" title="">12</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.5.2">60.8(49.4)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.5.3">95.8(79.1)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.5.4">76.1(63.3)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.5.5">58.1(45.7)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.5.6">51.7(41.0)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.5.7">81.9(71.1)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.5.8">148.7(105.1)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.5.9">83.4(69.8)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.5.10">153.8(133.3)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.5.1.5.11">93.6(79.6)</td>
<td class="ltx_td" id="S4.T1.5.1.5.12"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.6">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.5.1.6.1">Ours</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.6.2"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.6.2.1">34.9(35.2)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.6.3"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.6.3.1">71.0(63.9)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.6.4"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.6.4.1">40.0(39.6)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.6.5"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.6.5.1">24.9(24.1)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.6.6"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.6.6.1">27.2(25.8)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.6.7"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.6.7.1">42.4(45.3)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.6.8"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.6.8.1">99.5(77.4)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.6.9"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.6.9.1">61.7(59.7)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.6.10"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.6.10.1">98.4(99.3)</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.5.1.6.11"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.6.11.1">54.1(52.9)</span></td>
<td class="ltx_td ltx_border_t" id="S4.T1.5.1.6.12"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.7">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T1.5.1.7.1">Method</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.5.1.7.2">Crouch-T</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.5.1.7.3">Crouch-TS</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.5.1.7.4">Crouch-F</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.5.1.7.5">Crouch-B</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.5.1.7.6">Crouch-S</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.5.1.7.7">Stand-WB</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.5.1.7.8">Stand-UB</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.5.1.7.9">Stand-T</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.5.1.7.10">Stand-TC</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.5.1.7.11">Stand-F</td>
<td class="ltx_td ltx_border_tt" id="S4.T1.5.1.7.12"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.8">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.5.1.8.1">xr-EgoPose<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib32" title="">32</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.8.2">173.8 ( - )</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.8.3">108.9 ( - )</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.8.4">119.9 ( - )</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.8.5">136.5 ( - )</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.8.6">145.8 ( - )</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.8.7">94.3 ( - )</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.8.8">93.3 ( - )</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.8.9">103.3 ( - )</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.8.10">101.6 ( - )</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.5.1.8.11">99.7 ( - )</td>
<td class="ltx_td ltx_border_t" id="S4.T1.5.1.8.12"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.9">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.5.1.9.1">EgoGlass<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib45" title="">45</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.9.2">128.5(110.8)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.9.3">82.3(60.5)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.9.4">79.2(65.4)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.9.5">94.8(77.9)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.9.6">93.9(77.9)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.9.7">70.2(51.5)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.9.8">70.2(46.3)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.9.9">77.7(59.3)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.9.10">77.8(70.0)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.5.1.9.11">77.3(62.3)</td>
<td class="ltx_td" id="S4.T1.5.1.9.12"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.10">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.5.1.10.1">UnrealEgo<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib3" title="">3</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.10.2">128.2(104.3)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.10.3">76.8(55.2)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.10.4">73.6(61.9)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.10.5">78.2(62.0)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.10.6">84.9(71.4)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.10.7">68.5(50.1)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.10.8">66.5(45.4)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.10.9">74.3(57.9)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.10.10">74.1(61.2)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.5.1.10.11">70.8(56.6)</td>
<td class="ltx_td" id="S4.T1.5.1.10.12"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.11">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.5.1.11.1">Ego3DPose<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib12" title="">12</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.11.2">109.0(89.9)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.11.3">65.3(48.7)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.11.4">53.9(45.9)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.11.5">58.1(46.9)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.11.6">67.7(56.7)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.11.7">50.4(39.9)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.11.8">50.5(37.2)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.11.9">58.5(48.1)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.11.10">59.6(52.8)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.5.1.11.11">55.7(48.7)</td>
<td class="ltx_td" id="S4.T1.5.1.11.12"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.12">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.5.1.12.1">Ours</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.12.2"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.12.2.1">78.4(69.3)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.12.3"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.12.3.1">27.9(26.1)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.12.4"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.12.4.1">23.5(25.3)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.12.5"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.12.5.1">28.9(29.6)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.12.6"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.12.6.1">33.1(36.5)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.12.7"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.12.7.1">26.2(25.3)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.12.8"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.12.8.1">25.6(24.3)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.12.9"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.12.9.1">30.4(32.8)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.12.10"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.12.10.1">22.2(23.2)</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.5.1.12.11"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.12.11.1">33.8(37.2)</span></td>
<td class="ltx_td ltx_border_t" id="S4.T1.5.1.12.12"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.13">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T1.5.1.13.1">Method</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.5.1.13.2">Stand-B</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.5.1.13.3">Stand-S</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.5.1.13.4">Dance</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.5.1.13.5">Boxing</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.5.1.13.6">Wrestling</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.5.1.13.7">Soccer</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.5.1.13.8">Baseball</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.5.1.13.9">Basketball</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.5.1.13.10">Gridiron</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.5.1.13.11">Golf</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.5.1.13.12">All</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.14">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.5.1.14.1">xr-EgoPose<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib32" title="">32</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.14.2">105.8 ( - )</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.14.3">114.3 ( - )</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.14.4">116.7 ( - )</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.14.5">97.3 ( - )</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.14.6">116.6 ( - )</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.14.7">104.6 ( - )</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.14.8">103.7 ( - )</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.14.9">98.6 ( - )</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.14.10">149.7 ( - )</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.5.1.14.11">117.5 ( - )</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.14.12">112.6 ( - )</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.15">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.5.1.15.1">EgoGlass<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib45" title="">45</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.15.2">75.1(58.5)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.15.3">85.2(67.8)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.15.4">83.7(65.5)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.15.5">71.3(54.2)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.15.6">86.1(65.3)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.15.7">52.3(58.2)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.15.8">77.7(61.0)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.15.9">59.7(47.3)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.15.10">102.1(80.5)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.5.1.15.11">69.4(48.2)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.15.12">83.3(61.6)</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.16">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.5.1.16.1">UnrealEgo<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib3" title="">3</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.16.2">68.1(55.1)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.16.3">78.9(62.3)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.16.4">79.9(63.2)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.16.5">68.7(51.4)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.16.6">83.7(64.5)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.16.7">78.3(56.2)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.16.8">72.8(56.7)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.16.9">60.5(45.4)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.16.10">99.5(81.8)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.5.1.16.11">73.8(49.0)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.16.12">79.1(59.2)</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.17">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.5.1.17.1">Ego3DPose<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib12" title="">12</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.17.2">51.9(42.6)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.17.3">60.9(51.2)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.17.4">61.3(50.8)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.17.5">49.9(40.2)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.17.6">65.5(51.7)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.17.7">54.4(43.7)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.17.8">69.2(55.6)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.17.9">48.4(35.8)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.17.10">83.3(72.9)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.5.1.17.11">54.5(38.1)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.17.12">60.8(48.5)</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.18">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="S4.T1.5.1.18.1">Ours</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.5.1.18.2"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.18.2.1">26.8(28.5)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.5.1.18.3"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.18.3.1">32.2(34.5)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.5.1.18.4"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.18.4.1">35.0(36.2)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.5.1.18.5"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.18.5.1">22.9(22.6)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.5.1.18.6"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.18.6.1">37.0(36.6)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.5.1.18.7"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.18.7.1">27.0(27.4)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.5.1.18.8"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.18.8.1">41.6(42.9)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.5.1.18.9"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.18.9.1">16.5(16.1)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.5.1.18.10"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.18.10.1">58.4(48.7)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S4.T1.5.1.18.11"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.18.11.1">26.9(21.8)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.5.1.18.12"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.18.12.1">33.4(32.7)</span></td>
</tr>
</table>
</span></div>
</figure>
<figure class="ltx_table" id="S4.T3">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_pruned_first ltx_align_bottom" id="S4.T3.2" style="width:182.1pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_block"><span class="ltx_text" id="S4.T3.2.2.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S4.T3.2.3.2" style="font-size:80%;">Comparison of MPJPE and PA-MPJPE with other methods on the monocular SceneEgo dataset.</span></figcaption>
<div class="ltx_para ltx_noindent ltx_align_center" id="S4.T3.2.p2">
<p class="ltx_p" id="S4.T3.2.p2.2"><span class="ltx_text" id="S4.T3.2.p2.2.2">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T3.2.p2.2.2.2" style="width:280.0pt;height:108pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S4.T3.2.p2.2.2.2.2"><span class="ltx_text" id="S4.T3.2.p2.2.2.2.2.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.2.p2.2.2.2.2.2.2">
<span class="ltx_tr" id="S4.T3.2.p2.2.2.2.2.2.2.3">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T3.2.p2.2.2.2.2.2.2.3.1">Method</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" id="S4.T3.2.p2.2.2.2.2.2.2.3.2">MPJPE</span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.2.p2.2.2.2.2.2.2.3.3">PA-MPJPE</span></span>
<span class="ltx_tr" id="S4.T3.2.p2.2.2.2.2.2.2.2">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.2.p2.2.2.2.2.2.2.2.2">Mo<sup class="ltx_sup" id="S4.T3.2.p2.2.2.2.2.2.2.2.2.1"><span class="ltx_text ltx_font_italic" id="S4.T3.2.p2.2.2.2.2.2.2.2.2.1.1">2</span></sup>Cap<sup class="ltx_sup" id="S4.T3.2.p2.2.2.2.2.2.2.2.2.2"><span class="ltx_text ltx_font_italic" id="S4.T3.2.p2.2.2.2.2.2.2.2.2.2.1">2</span></sup>Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib41" title="">41</a>]</cite></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T3.2.p2.2.2.2.2.2.2.2.3">200.3</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.p2.2.2.2.2.2.2.2.4">121.2</span></span>
<span class="ltx_tr" id="S4.T3.2.p2.2.2.2.2.2.2.4">
<span class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.2.p2.2.2.2.2.2.2.4.1">xR-egoposeÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib32" title="">32</a>]</cite></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T3.2.p2.2.2.2.2.2.2.4.2">241.3</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.2.2.2.2.2.2.4.3">133.9</span></span>
<span class="ltx_tr" id="S4.T3.2.p2.2.2.2.2.2.2.5">
<span class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.2.p2.2.2.2.2.2.2.5.1">EgoPWÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib36" title="">36</a>]</cite></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T3.2.p2.2.2.2.2.2.2.5.2">189.6</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.2.2.2.2.2.2.5.3">105.3</span></span>
<span class="ltx_tr" id="S4.T3.2.p2.2.2.2.2.2.2.6">
<span class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.2.p2.2.2.2.2.2.2.6.1">SceneEgoÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib37" title="">37</a>]</cite></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T3.2.p2.2.2.2.2.2.2.6.2">118.5</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.2.2.2.2.2.2.6.3">92.7</span></span>
<span class="ltx_tr" id="S4.T3.2.p2.2.2.2.2.2.2.7">
<span class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="S4.T3.2.p2.2.2.2.2.2.2.7.1">Ours</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb ltx_border_t" id="S4.T3.2.p2.2.2.2.2.2.2.7.2"><span class="ltx_text ltx_font_bold" id="S4.T3.2.p2.2.2.2.2.2.2.7.2.1">93.0</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T3.2.p2.2.2.2.2.2.2.7.3"><span class="ltx_text ltx_font_bold" id="S4.T3.2.p2.2.2.2.2.2.2.7.3.1">74.3</span></span></span>
</span></span></span>
</span></span></span></p>
</div>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_pruned_first ltx_align_bottom" id="S4.T3.4" style="width:236.3pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_block"><span class="ltx_text" id="S4.T3.4.2.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S4.T3.4.3.2" style="font-size:80%;">Comparison of model efficiency between EgoPoseFormerÂ and other methods on UnrealEgo and SceneEgo datasets.</span></figcaption>
<div class="ltx_para ltx_noindent ltx_align_center" id="S4.T3.4.p2">
<p class="ltx_p" id="S4.T3.4.p2.1"><span class="ltx_text" id="S4.T3.4.p2.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T3.4.p2.1.1.1" style="width:326.2pt;height:126pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S4.T3.4.p2.1.1.1.1"><span class="ltx_text" id="S4.T3.4.p2.1.1.1.1.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.4.p2.1.1.1.1.1.1">
<span class="ltx_tr" id="S4.T3.4.p2.1.1.1.1.1.1.1">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T3.4.p2.1.1.1.1.1.1.1.1">Dataset</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T3.4.p2.1.1.1.1.1.1.1.2">Methods</span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.4.p2.1.1.1.1.1.1.1.3">MPJPE</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.4.p2.1.1.1.1.1.1.1.4">PA-MPJPE</span>
<span class="ltx_td ltx_align_right ltx_border_tt" id="S4.T3.4.p2.1.1.1.1.1.1.1.5">Params</span>
<span class="ltx_td ltx_align_right ltx_border_tt" id="S4.T3.4.p2.1.1.1.1.1.1.1.6">FLOPs</span></span>
<span class="ltx_tr" id="S4.T3.4.p2.1.1.1.1.1.1.2">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_4" id="S4.T3.4.p2.1.1.1.1.1.1.2.1"><span class="ltx_text" id="S4.T3.4.p2.1.1.1.1.1.1.2.1.1">UnrealEgo</span></span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.4.p2.1.1.1.1.1.1.2.2">EgoGlass</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.p2.1.1.1.1.1.1.2.3">83.3</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.4.p2.1.1.1.1.1.1.2.4">61.6</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.4.p2.1.1.1.1.1.1.2.5">107.3 M</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.4.p2.1.1.1.1.1.1.2.6">16.1 G</span></span>
<span class="ltx_tr" id="S4.T3.4.p2.1.1.1.1.1.1.3">
<span class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.4.p2.1.1.1.1.1.1.3.1">UnrealEgo</span>
<span class="ltx_td ltx_align_center" id="S4.T3.4.p2.1.1.1.1.1.1.3.2">79.1</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.p2.1.1.1.1.1.1.3.3">59.2</span>
<span class="ltx_td ltx_align_right" id="S4.T3.4.p2.1.1.1.1.1.1.3.4">106.8 M</span>
<span class="ltx_td ltx_align_right" id="S4.T3.4.p2.1.1.1.1.1.1.3.5">27.1 G</span></span>
<span class="ltx_tr" id="S4.T3.4.p2.1.1.1.1.1.1.4">
<span class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.4.p2.1.1.1.1.1.1.4.1">Ego3DPose</span>
<span class="ltx_td ltx_align_center" id="S4.T3.4.p2.1.1.1.1.1.1.4.2">60.8</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.p2.1.1.1.1.1.1.4.3">48.5</span>
<span class="ltx_td ltx_align_right" id="S4.T3.4.p2.1.1.1.1.1.1.4.4">178.4 M</span>
<span class="ltx_td ltx_align_right" id="S4.T3.4.p2.1.1.1.1.1.1.4.5">55.6 G</span></span>
<span class="ltx_tr" id="S4.T3.4.p2.1.1.1.1.1.1.5">
<span class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.4.p2.1.1.1.1.1.1.5.1">Ours</span>
<span class="ltx_td ltx_align_center" id="S4.T3.4.p2.1.1.1.1.1.1.5.2"><span class="ltx_text ltx_font_bold" id="S4.T3.4.p2.1.1.1.1.1.1.5.2.1">33.4</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.p2.1.1.1.1.1.1.5.3"><span class="ltx_text ltx_font_bold" id="S4.T3.4.p2.1.1.1.1.1.1.5.3.1">32.7</span></span>
<span class="ltx_td ltx_align_right" id="S4.T3.4.p2.1.1.1.1.1.1.5.4"><span class="ltx_text ltx_font_bold" id="S4.T3.4.p2.1.1.1.1.1.1.5.4.1">14.1</span> M</span>
<span class="ltx_td ltx_align_right" id="S4.T3.4.p2.1.1.1.1.1.1.5.5"><span class="ltx_text ltx_font_bold" id="S4.T3.4.p2.1.1.1.1.1.1.5.5.1">7.3</span> G</span></span>
<span class="ltx_tr" id="S4.T3.4.p2.1.1.1.1.1.1.6">
<span class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" id="S4.T3.4.p2.1.1.1.1.1.1.6.1"><span class="ltx_text" id="S4.T3.4.p2.1.1.1.1.1.1.6.1.1">SceneEgo</span></span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.4.p2.1.1.1.1.1.1.6.2">SceneEgo</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.p2.1.1.1.1.1.1.6.3">118.5</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.4.p2.1.1.1.1.1.1.6.4">92.7</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.4.p2.1.1.1.1.1.1.6.5">45.9 M</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.4.p2.1.1.1.1.1.1.6.6">157.3 G</span></span>
<span class="ltx_tr" id="S4.T3.4.p2.1.1.1.1.1.1.7">
<span class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T3.4.p2.1.1.1.1.1.1.7.1">Ours</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.4.p2.1.1.1.1.1.1.7.2"><span class="ltx_text ltx_font_bold" id="S4.T3.4.p2.1.1.1.1.1.1.7.2.1">93.0</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.4.p2.1.1.1.1.1.1.7.3"><span class="ltx_text ltx_font_bold" id="S4.T3.4.p2.1.1.1.1.1.1.7.3.1">74.3</span></span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S4.T3.4.p2.1.1.1.1.1.1.7.4"><span class="ltx_text ltx_font_bold" id="S4.T3.4.p2.1.1.1.1.1.1.7.4.1">27.9</span> M</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S4.T3.4.p2.1.1.1.1.1.1.7.5"><span class="ltx_text ltx_font_bold" id="S4.T3.4.p2.1.1.1.1.1.1.7.5.1">50.4</span> G</span></span>
</span></span></span>
</span></span></span></p>
</div>
</div>
</div>
</div>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Main Results</h3>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.1">Comparison with the state-of-the-art.</span>
InÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S4.T1" title="In 4.1 Experiment Settings â€£ 4 Experiments â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Tab.</span>Â <span class="ltx_text ltx_ref_tag">1</span></a>, we compare our EgoPoseFormerÂ with previous state-of-the-art methods on the UnrealEgoÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib3" title="">3</a>]</cite> dataset. We report both average and action-specific metrics. The results of competing methods are taken from their corresponding papersÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib12" title="">12</a>]</cite>. As shown by the results, our approach achieves a significant advantage over all competing methods across all actions. Specifically, our method is 27.4mm better in the averaged MPJPE than the previous state-of-the-art Ego3DPoseÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib12" title="">12</a>]</cite>, pushing the best performance on this benchmark by 45% lower error. InÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S4.T3" title="In 4.1 Experiment Settings â€£ 4 Experiments â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Tab.</span>Â <span class="ltx_text ltx_ref_tag">3</span></a>, we compare our method with previous egocentric body pose estimation approaches on the monocular SceneEgoÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib37" title="">37</a>]</cite> dataset. Our method achieved an MPJPE that is 25.5mm lower than the previous best method. Note that SceneEgoÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib37" title="">37</a>]</cite> requires ground-truth depth and semantic masks to train their model, but our method does not require them. Also, the previous best model was pre-trained on a large pose estimation dataset and fine-tuned on the SceneEgo datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib37" title="">37</a>]</cite>, but we only used the SceneEgo dataset itself to train our model. InÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S4.F4" title="In 4.3 Ablation Studies and Discussions â€£ 4 Experiments â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">4</span></a>, we show qualitative results produce by our method on both datasets, which validates that EgoPoseFormerÂ can generalize well to challenging poses. In summary, these results validate the effectiveness of our method on both multi-view and monocular egocentric pose estimation. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1">Computation efficiency.</span>
InÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S4.T3" title="In 4.1 Experiment Settings â€£ 4 Experiments â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Tab.</span>Â <span class="ltx_text ltx_ref_tag">3</span></a>, we compare the computational efficiency of the proposed EgoPoseFormerÂ with other competing models on the UnrealEgo and SceneEgo datasets, where we report the model parameters together with the number of FLOPs. The results show that our method is highly effective yet efficient. Specifically, on the UnrealEgo dataset, our method decreases the error over the previous best method by 45% with only 7.9% of the parameters and 13.1% of the FLOPs. Similarly, on the SceneEgo dataset, our method reduces the MPJPE by 25.5mm compared to the previous best method, but requires only 60.7% of the parameters and 36.4% of the FLOPs.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Ablation Studies and Discussions</h3>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="425" id="S4.F4.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.3.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S4.F4.4.2" style="font-size:90%;">Qualitative visualization on UnrealEgo and SceneEgo. Ground-truths are colored in <span class="ltx_text" id="S4.F4.4.2.1" style="color:#018000;">green</span> and predictions are colored in <span class="ltx_text" id="S4.F4.4.2.2" style="color:#FF0000;">red</span>.</span></figcaption>
</figure>
<figure class="ltx_table" id="S4.T5">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_pruned_first ltx_align_bottom" id="S4.T5.2" style="width:190.8pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_block"><span class="ltx_text" id="S4.T5.2.2.1.1" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text" id="S4.T5.2.3.2" style="font-size:80%;">Ablation Study of attention operations on the UnrealEgo dataset. (DS-Attn denotes our Deformable Stereo Attention.)</span></figcaption>
<div class="ltx_para ltx_noindent ltx_align_center" id="S4.T5.2.p2">
<p class="ltx_p" id="S4.T5.2.p2.1"><span class="ltx_text" id="S4.T5.2.p2.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T5.2.p2.1.1.1" style="width:288.9pt;height:108pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S4.T5.2.p2.1.1.1.1"><span class="ltx_text" id="S4.T5.2.p2.1.1.1.1.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T5.2.p2.1.1.1.1.1.1">
<span class="ltx_tr" id="S4.T5.2.p2.1.1.1.1.1.1.1">
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.2.p2.1.1.1.1.1.1.1.1">DS</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T5.2.p2.1.1.1.1.1.1.1.2">Self</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_2" id="S4.T5.2.p2.1.1.1.1.1.1.1.3">First Stage</span>
<span class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" id="S4.T5.2.p2.1.1.1.1.1.1.1.4">Second Stage</span></span>
<span class="ltx_tr" id="S4.T5.2.p2.1.1.1.1.1.1.2">
<span class="ltx_td ltx_align_center" id="S4.T5.2.p2.1.1.1.1.1.1.2.1">Attn</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.2.p2.1.1.1.1.1.1.2.2">Attn</span>
<span class="ltx_td ltx_align_center" id="S4.T5.2.p2.1.1.1.1.1.1.2.3">MPJPE</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.2.p2.1.1.1.1.1.1.2.4">PA-MPJPE</span>
<span class="ltx_td ltx_align_center" id="S4.T5.2.p2.1.1.1.1.1.1.2.5">MPJPE</span>
<span class="ltx_td ltx_align_center" id="S4.T5.2.p2.1.1.1.1.1.1.2.6">PA-MPJPE</span></span>
<span class="ltx_tr" id="S4.T5.2.p2.1.1.1.1.1.1.3">
<span class="ltx_td ltx_border_t" id="S4.T5.2.p2.1.1.1.1.1.1.3.1"></span>
<span class="ltx_td ltx_border_r ltx_border_t" id="S4.T5.2.p2.1.1.1.1.1.1.3.2"></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.p2.1.1.1.1.1.1.3.3">46.7</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.2.p2.1.1.1.1.1.1.3.4">39.0</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.p2.1.1.1.1.1.1.3.5">-</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.p2.1.1.1.1.1.1.3.6">-</span></span>
<span class="ltx_tr" id="S4.T5.2.p2.1.1.1.1.1.1.4">
<span class="ltx_td" id="S4.T5.2.p2.1.1.1.1.1.1.4.1"></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.2.p2.1.1.1.1.1.1.4.2">âœ“</span>
<span class="ltx_td ltx_align_center" id="S4.T5.2.p2.1.1.1.1.1.1.4.3">46.9</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.2.p2.1.1.1.1.1.1.4.4">38.6</span>
<span class="ltx_td ltx_align_center" id="S4.T5.2.p2.1.1.1.1.1.1.4.5">45.9</span>
<span class="ltx_td ltx_align_center" id="S4.T5.2.p2.1.1.1.1.1.1.4.6">38.0</span></span>
<span class="ltx_tr" id="S4.T5.2.p2.1.1.1.1.1.1.5">
<span class="ltx_td ltx_align_center" id="S4.T5.2.p2.1.1.1.1.1.1.5.1">âœ“</span>
<span class="ltx_td ltx_border_r" id="S4.T5.2.p2.1.1.1.1.1.1.5.2"></span>
<span class="ltx_td ltx_align_center" id="S4.T5.2.p2.1.1.1.1.1.1.5.3">46.5</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.2.p2.1.1.1.1.1.1.5.4">39.0</span>
<span class="ltx_td ltx_align_center" id="S4.T5.2.p2.1.1.1.1.1.1.5.5">38.1</span>
<span class="ltx_td ltx_align_center" id="S4.T5.2.p2.1.1.1.1.1.1.5.6">36.2</span></span>
<span class="ltx_tr" id="S4.T5.2.p2.1.1.1.1.1.1.6">
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.2.p2.1.1.1.1.1.1.6.1">âœ“</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T5.2.p2.1.1.1.1.1.1.6.2">âœ“</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.2.p2.1.1.1.1.1.1.6.3">45.5</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T5.2.p2.1.1.1.1.1.1.6.4">38.3</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.2.p2.1.1.1.1.1.1.6.5">33.4</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.2.p2.1.1.1.1.1.1.6.6">32.7</span></span>
</span></span></span>
</span></span></span></p>
</div>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_pruned_first ltx_align_bottom" id="S4.T5.4" style="width:234.2pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_block"><span class="ltx_text" id="S4.T5.4.2.1.1" style="font-size:90%;">Table 5</span>: </span><span class="ltx_text" id="S4.T5.4.3.2" style="font-size:80%;">Ablation study of the stereo and monocular settings on the UnrealEgo dataset. Note here the feature extractors are also pre-trained using the corresponding settings.</span></figcaption>
<div class="ltx_para ltx_noindent ltx_align_center" id="S4.T5.4.p2">
<p class="ltx_p" id="S4.T5.4.p2.1"><span class="ltx_text" id="S4.T5.4.p2.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T5.4.p2.1.1.1" style="width:340.3pt;height:108pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S4.T5.4.p2.1.1.1.1"><span class="ltx_text" id="S4.T5.4.p2.1.1.1.1.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T5.4.p2.1.1.1.1.1.1">
<span class="ltx_tr" id="S4.T5.4.p2.1.1.1.1.1.1.1">
<span class="ltx_td ltx_border_r ltx_border_tt" id="S4.T5.4.p2.1.1.1.1.1.1.1.1"></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_2" id="S4.T5.4.p2.1.1.1.1.1.1.1.2">First Stage</span>
<span class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" id="S4.T5.4.p2.1.1.1.1.1.1.1.3">Second Stage</span></span>
<span class="ltx_tr" id="S4.T5.4.p2.1.1.1.1.1.1.2">
<span class="ltx_td ltx_align_left ltx_border_r" id="S4.T5.4.p2.1.1.1.1.1.1.2.1">Model Input</span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T5.4.p2.1.1.1.1.1.1.2.2">MPJPE</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.4.p2.1.1.1.1.1.1.2.3">PA-MPJPE</span>
<span class="ltx_td ltx_align_center" id="S4.T5.4.p2.1.1.1.1.1.1.2.4">MPJPE</span>
<span class="ltx_td ltx_align_center" id="S4.T5.4.p2.1.1.1.1.1.1.2.5">PA-MPJPE</span></span>
<span class="ltx_tr" id="S4.T5.4.p2.1.1.1.1.1.1.3">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T5.4.p2.1.1.1.1.1.1.3.1">Monocular Left</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T5.4.p2.1.1.1.1.1.1.3.2">56.0</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.4.p2.1.1.1.1.1.1.3.3">49.1</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.4.p2.1.1.1.1.1.1.3.4">48.3</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.4.p2.1.1.1.1.1.1.3.5">47.2</span></span>
<span class="ltx_tr" id="S4.T5.4.p2.1.1.1.1.1.1.4">
<span class="ltx_td ltx_align_left ltx_border_r" id="S4.T5.4.p2.1.1.1.1.1.1.4.1">Monocular Right</span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T5.4.p2.1.1.1.1.1.1.4.2">56.3</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.4.p2.1.1.1.1.1.1.4.3">49.0</span>
<span class="ltx_td ltx_align_center" id="S4.T5.4.p2.1.1.1.1.1.1.4.4">48.7</span>
<span class="ltx_td ltx_align_center" id="S4.T5.4.p2.1.1.1.1.1.1.4.5">47.3</span></span>
<span class="ltx_tr" id="S4.T5.4.p2.1.1.1.1.1.1.5">
<span class="ltx_td ltx_align_left ltx_border_r" id="S4.T5.4.p2.1.1.1.1.1.1.5.1">Monocular Left + Right</span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T5.4.p2.1.1.1.1.1.1.5.2">55.6</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.4.p2.1.1.1.1.1.1.5.3">48.8</span>
<span class="ltx_td ltx_align_center" id="S4.T5.4.p2.1.1.1.1.1.1.5.4">48.2</span>
<span class="ltx_td ltx_align_center" id="S4.T5.4.p2.1.1.1.1.1.1.5.5">47.2</span></span>
<span class="ltx_tr" id="S4.T5.4.p2.1.1.1.1.1.1.6">
<span class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="S4.T5.4.p2.1.1.1.1.1.1.6.1">Stereo</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb ltx_border_t" id="S4.T5.4.p2.1.1.1.1.1.1.6.2">45.5</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S4.T5.4.p2.1.1.1.1.1.1.6.3">38.3</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T5.4.p2.1.1.1.1.1.1.6.4">33.4</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T5.4.p2.1.1.1.1.1.1.6.5">32.7</span></span>
</span></span></span>
</span></span></span></p>
</div>
</div>
</div>
</div>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.1">PRFormer.</span>
We first conduct experiments on the UnrealEgo dataset to study the two attention modules in our PRFormer. We present the results inÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S4.T5" title="In 4.3 Ablation Studies and Discussions â€£ 4 Experiments â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Tab.</span>Â <span class="ltx_text ltx_ref_tag">5</span></a>. We start with a baseline that only contains the PPNÂ with the PRFormerÂ part being removed. Surprisingly, this simple baseline achieves a 14.1mm MPJPE advantage over the previous state-of-the-art. We speculate such an advantage is achieved by the usage of visual features. We then added PRFormerÂ but with only the self-attention operation, from which the refined pose estimation is only marginally better than the pose proposal. Subsequently, we add another version of PRFormerÂ that only contains Deformable Stereo Attention, this time the refined pose estimation improves MPJPE by 8.4mm over the initial pose proposal, validating its effectiveness in exploiting multi-view stereo features. Lastly, we add the full version of PRFormerÂ with both types of attention, from which the refined pose estimation is 12.1mm more accurate than the initial pose proposal.
We therefore conclude that it is essential to combine the Deformable Stereo AttentionÂ with self-attention, in which the JQTs interact with each other to propagate human kinematic information.
To further understand how PRFormerÂ refines the pose proposal, inÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S4.F5" title="In 4.3 Ablation Studies and Discussions â€£ 4 Experiments â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">5</span></a> we report the MPJPE improvement w.r.t. different body parts. Finally, we provide qualitative examples to better understand how the attention mechanisms work: InÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S4.F6" title="In 4.3 Ablation Studies and Discussions â€£ 4 Experiments â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">6</span></a>, we visualize the averaged attention weights between different joints in the self-attention operation on both datasets, from which we can see even if we do not explicitly introduce any kinematic-related losses in our model, the attention can implicitly infer the kinematic structures of a human body. 
<br class="ltx_break"/></p>
</div>
<figure class="ltx_table" id="S4.T6">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T6.4.1.1" style="font-size:113%;">Table 6</span>: </span>Error of different body parts when they are captured by different numbers of views on UnrealEgo. We report our reproduced result for the UnrealEgo model.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T6.5" style="width:433.6pt;height:174.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(37.6pt,-15.1pt) scale(1.21009990725514,1.21009990725514) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T6.5.1">
<tr class="ltx_tr" id="S4.T6.5.1.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T6.5.1.1.1" rowspan="2"><span class="ltx_text" id="S4.T6.5.1.1.1.1" style="font-size:80%;">Method</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_tt" id="S4.T6.5.1.1.2" rowspan="2"><span class="ltx_text" id="S4.T6.5.1.1.2.1" style="font-size:80%;">Views</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" id="S4.T6.5.1.1.3" rowspan="2"><span class="ltx_text" id="S4.T6.5.1.1.3.1" style="font-size:80%;">Head</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" id="S4.T6.5.1.1.4" rowspan="2"><span class="ltx_text" id="S4.T6.5.1.1.4.1" style="font-size:80%;">Neck</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" id="S4.T6.5.1.1.5"><span class="ltx_text" id="S4.T6.5.1.1.5.1" style="font-size:80%;">Upper</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" id="S4.T6.5.1.1.6"><span class="ltx_text" id="S4.T6.5.1.1.6.1" style="font-size:80%;">Lower</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" id="S4.T6.5.1.1.7" rowspan="2"><span class="ltx_text" id="S4.T6.5.1.1.7.1" style="font-size:80%;">Hand</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" id="S4.T6.5.1.1.8" rowspan="2"><span class="ltx_text" id="S4.T6.5.1.1.8.1" style="font-size:80%;">Thigh</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" id="S4.T6.5.1.1.9" rowspan="2"><span class="ltx_text" id="S4.T6.5.1.1.9.1" style="font-size:80%;">Calf</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" id="S4.T6.5.1.1.10" rowspan="2"><span class="ltx_text" id="S4.T6.5.1.1.10.1" style="font-size:80%;">Foot</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_tt" id="S4.T6.5.1.1.11" rowspan="2"><span class="ltx_text" id="S4.T6.5.1.1.11.1" style="font-size:80%;">Toe</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" id="S4.T6.5.1.1.12" rowspan="2"><span class="ltx_text" id="S4.T6.5.1.1.12.1" style="font-size:80%;">MPJPE</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.5.1.2">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.5.1.2.1"><span class="ltx_text" id="S4.T6.5.1.2.1.1" style="font-size:80%;">Arm</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.5.1.2.2"><span class="ltx_text" id="S4.T6.5.1.2.2.1" style="font-size:80%;">Arm</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.5.1.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.5.1.3.1" rowspan="3"><span class="ltx_text" id="S4.T6.5.1.3.1.1" style="font-size:80%;">UnrealEgo</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.5.1.3.2"><span class="ltx_text" id="S4.T6.5.1.3.2.1" style="font-size:80%;">0</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T6.5.1.3.3"><span class="ltx_text" id="S4.T6.5.1.3.3.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T6.5.1.3.4"><span class="ltx_text" id="S4.T6.5.1.3.4.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T6.5.1.3.5"><span class="ltx_text" id="S4.T6.5.1.3.5.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T6.5.1.3.6"><span class="ltx_text" id="S4.T6.5.1.3.6.1" style="font-size:80%;">114.7</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T6.5.1.3.7"><span class="ltx_text" id="S4.T6.5.1.3.7.1" style="font-size:80%;">139.2</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T6.5.1.3.8"><span class="ltx_text" id="S4.T6.5.1.3.8.1" style="font-size:80%;">21.5</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T6.5.1.3.9"><span class="ltx_text" id="S4.T6.5.1.3.9.1" style="font-size:80%;">236.2</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T6.5.1.3.10"><span class="ltx_text" id="S4.T6.5.1.3.10.1" style="font-size:80%;">282.3</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.5.1.3.11"><span class="ltx_text" id="S4.T6.5.1.3.11.1" style="font-size:80%;">319.1</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T6.5.1.3.12" rowspan="3"><span class="ltx_text" id="S4.T6.5.1.3.12.1" style="font-size:80%;">80.1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.5.1.4">
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T6.5.1.4.1"><span class="ltx_text" id="S4.T6.5.1.4.1.1" style="font-size:80%;">1</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.5.1.4.2"><span class="ltx_text" id="S4.T6.5.1.4.2.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.5.1.4.3"><span class="ltx_text" id="S4.T6.5.1.4.3.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.5.1.4.4"><span class="ltx_text" id="S4.T6.5.1.4.4.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.5.1.4.5"><span class="ltx_text" id="S4.T6.5.1.4.5.1" style="font-size:80%;">104.8</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.5.1.4.6"><span class="ltx_text" id="S4.T6.5.1.4.6.1" style="font-size:80%;">119.3</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.5.1.4.7"><span class="ltx_text" id="S4.T6.5.1.4.7.1" style="font-size:80%;">39.5</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.5.1.4.8"><span class="ltx_text" id="S4.T6.5.1.4.8.1" style="font-size:80%;">241.9</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.5.1.4.9"><span class="ltx_text" id="S4.T6.5.1.4.9.1" style="font-size:80%;">234.1</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T6.5.1.4.10"><span class="ltx_text" id="S4.T6.5.1.4.10.1" style="font-size:80%;">292.7</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.5.1.5">
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T6.5.1.5.1"><span class="ltx_text" id="S4.T6.5.1.5.1.1" style="font-size:80%;">2</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.5.1.5.2"><span class="ltx_text" id="S4.T6.5.1.5.2.1" style="font-size:80%;">49.1</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.5.1.5.3"><span class="ltx_text" id="S4.T6.5.1.5.3.1" style="font-size:80%;">45.4</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.5.1.5.4"><span class="ltx_text" id="S4.T6.5.1.5.4.1" style="font-size:80%;">50.0</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.5.1.5.5"><span class="ltx_text" id="S4.T6.5.1.5.5.1" style="font-size:80%;">74.3</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.5.1.5.6"><span class="ltx_text" id="S4.T6.5.1.5.6.1" style="font-size:80%;">98.3</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.5.1.5.7"><span class="ltx_text" id="S4.T6.5.1.5.7.1" style="font-size:80%;">16.3</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.5.1.5.8"><span class="ltx_text" id="S4.T6.5.1.5.8.1" style="font-size:80%;">85.7</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.5.1.5.9"><span class="ltx_text" id="S4.T6.5.1.5.9.1" style="font-size:80%;">122.1</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T6.5.1.5.10"><span class="ltx_text" id="S4.T6.5.1.5.10.1" style="font-size:80%;">139.6</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.5.1.6">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="S4.T6.5.1.6.1" rowspan="3"><span class="ltx_text" id="S4.T6.5.1.6.1.1" style="font-size:80%;">Ours</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.5.1.6.2"><span class="ltx_text" id="S4.T6.5.1.6.2.1" style="font-size:80%;">0</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T6.5.1.6.3"><span class="ltx_text" id="S4.T6.5.1.6.3.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T6.5.1.6.4"><span class="ltx_text" id="S4.T6.5.1.6.4.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T6.5.1.6.5"><span class="ltx_text" id="S4.T6.5.1.6.5.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T6.5.1.6.6"><span class="ltx_text" id="S4.T6.5.1.6.6.1" style="font-size:80%;">55.4</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T6.5.1.6.7"><span class="ltx_text" id="S4.T6.5.1.6.7.1" style="font-size:80%;">111.8</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T6.5.1.6.8"><span class="ltx_text" id="S4.T6.5.1.6.8.1" style="font-size:80%;">19.1</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T6.5.1.6.9"><span class="ltx_text" id="S4.T6.5.1.6.9.1" style="font-size:80%;">147.2</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T6.5.1.6.10"><span class="ltx_text" id="S4.T6.5.1.6.10.1" style="font-size:80%;">178.4</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.5.1.6.11"><span class="ltx_text" id="S4.T6.5.1.6.11.1" style="font-size:80%;">179.3</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.5.1.6.12" rowspan="3"><span class="ltx_text" id="S4.T6.5.1.6.12.1" style="font-size:80%;">33.4</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.5.1.7">
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T6.5.1.7.1"><span class="ltx_text" id="S4.T6.5.1.7.1.1" style="font-size:80%;">1</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.5.1.7.2"><span class="ltx_text" id="S4.T6.5.1.7.2.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.5.1.7.3"><span class="ltx_text" id="S4.T6.5.1.7.3.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.5.1.7.4"><span class="ltx_text" id="S4.T6.5.1.7.4.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.5.1.7.5"><span class="ltx_text" id="S4.T6.5.1.7.5.1" style="font-size:80%;">47.4</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.5.1.7.6"><span class="ltx_text" id="S4.T6.5.1.7.6.1" style="font-size:80%;">83.8</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.5.1.7.7"><span class="ltx_text" id="S4.T6.5.1.7.7.1" style="font-size:80%;">32.6</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.5.1.7.8"><span class="ltx_text" id="S4.T6.5.1.7.8.1" style="font-size:80%;">74.3</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T6.5.1.7.9"><span class="ltx_text" id="S4.T6.5.1.7.9.1" style="font-size:80%;">119.9</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T6.5.1.7.10"><span class="ltx_text" id="S4.T6.5.1.7.10.1" style="font-size:80%;">170.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.5.1.8">
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb ltx_border_r" id="S4.T6.5.1.8.1"><span class="ltx_text" id="S4.T6.5.1.8.1.1" style="font-size:80%;">2</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S4.T6.5.1.8.2"><span class="ltx_text" id="S4.T6.5.1.8.2.1" style="font-size:80%;">1.73</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S4.T6.5.1.8.3"><span class="ltx_text" id="S4.T6.5.1.8.3.1" style="font-size:80%;">5.3</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S4.T6.5.1.8.4"><span class="ltx_text" id="S4.T6.5.1.8.4.1" style="font-size:80%;">11.4</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S4.T6.5.1.8.5"><span class="ltx_text" id="S4.T6.5.1.8.5.1" style="font-size:80%;">18.6</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S4.T6.5.1.8.6"><span class="ltx_text" id="S4.T6.5.1.8.6.1" style="font-size:80%;">31.7</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S4.T6.5.1.8.7"><span class="ltx_text" id="S4.T6.5.1.8.7.1" style="font-size:80%;">7.2</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S4.T6.5.1.8.8"><span class="ltx_text" id="S4.T6.5.1.8.8.1" style="font-size:80%;">31.0</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S4.T6.5.1.8.9"><span class="ltx_text" id="S4.T6.5.1.8.9.1" style="font-size:80%;">71.8</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb ltx_border_r" id="S4.T6.5.1.8.10"><span class="ltx_text" id="S4.T6.5.1.8.10.1" style="font-size:80%;">81.5</span></td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p2.1.1">Monocular v.s. Multi-view.</span>
We design Deformable Stereo AttentionÂ to effectively infer the multi-view stereo information, which is crucial to accurate 3D localization. Here, we examine if the performance improvement is brought by the transformer model itself, or is a result of the effective stereo reasoning. To this end, we test our model on the UnrealEgo dataset in both monocular and stereo settings. The results are reported inÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S4.T5" title="In 4.3 Ablation Studies and Discussions â€£ 4 Experiments â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Tab.</span>Â <span class="ltx_text ltx_ref_tag">5</span></a>. For the monocular setting, we treat every image as an independent sample. We tried three monocular settings: training and testing the model with the left view only, the right view only, and a mixing of both views. We got similar numbers for all three settings. The results show using monocular images decreased the performance of both PPNÂ and PRFormer. Specifically, the monocular PPNsÂ increases MPJPE by 10mm compared to the stereo variant. In addition, the monocular PRFormers can only improve the MPJPE of pose proposal by 7.5mm, but in the multi-view setting PRFormerÂ can improve the MPJPE of the pose proposal by 12mm. From the results, we conclude: <span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.2">1) Multi-view input is important to the performance of both PPNÂ and PRFormer. 2) While PRFormerÂ can refine the pose proposal with monocular inputs, it becomes more effective with multi-view inputs.</span> To further validate the conclusions, inÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S4.T6" title="In 4.3 Ablation Studies and Discussions â€£ 4 Experiments â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Tab.</span>Â <span class="ltx_text ltx_ref_tag">6</span></a> we presents error metrics for different body parts when they are captured by varying numbers of views. We also include results for the heatmap-based UnrealEgoÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib3" title="">3</a>]</cite> for comparasion. From the findings, a clear correlation emerges between the localization accuracy of a joint and the number of views in which it is visible. When a joint is concurrently visible in both views, our method consistently achieves highly accurate localization.
<br class="ltx_break"/></p>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="203" id="S4.F5.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.3.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S4.F5.4.2" style="font-size:80%;">Error analysis of the pose proposal and the refined body pose estimation.</span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p3.1.1">Error Analysis.</span>
We first study the error distributions of our method. We report the error distribution of different body parts inÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S4.F5" title="In 4.3 Ablation Studies and Discussions â€£ 4 Experiments â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">5</span></a> (a), encompassing both pre-refinement and post-refinement results. Our observations reveal that, for the majority of joints, our PRFormerÂ exhibits a substantial capacity to reduce the errors in the pose proposals. Notably, except for the challenging <span class="ltx_text ltx_font_italic" id="S4.SS3.p3.1.2">Foot</span> and <span class="ltx_text ltx_font_italic" id="S4.SS3.p3.1.3">Toe</span>, the median error in refined poses even falls below the quartile error of the pose proposals, validating the effectiveness of our proposed PRFormerÂ in enhancing the overall accuracy of the pose estimation. On the other hand, it is worth noting that the <span class="ltx_text ltx_font_italic" id="S4.SS3.p3.1.4">Foot</span> and <span class="ltx_text ltx_font_italic" id="S4.SS3.p3.1.5">Toe</span> joints, although improved in the post-refinement phase, still exhibit considerable inaccuracy, limiting the overall accuracy of the pose estimation system. Furthermore, followingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib12" title="">12</a>]</cite>, we visualize the error distributions in the form of Cumulative Distribution Function (CDF) for different body parts. The results are shown inÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S4.F5" title="In 4.3 Ablation Studies and Discussions â€£ 4 Experiments â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">5</span></a> (b), including both the pre-refinement and post-refinement CDFs. Similar to the earlier observations, the accuracy of pose estimation is substantially improved after refinement. For example, for the <span class="ltx_text ltx_font_italic" id="S4.SS3.p3.1.6">Hand</span> joint, 68% of the proposals have an error less than 50mm, and this number rises to 77% for refined estimations under the same threshold. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p4.1.1">PRFormerÂ layers.</span>
InÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S4.T8" title="In 4.3 Ablation Studies and Discussions â€£ 4 Experiments â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Tab.</span>Â <span class="ltx_text ltx_ref_tag">8</span></a>, we report EgoPoseFormerâ€™s performance with different numbers of layers of PRFormer, in which the JQTs iteratively interact with each other and the image features to refine the pose proposal. The results show that more layers of PRFormerÂ can lead to better accuracy of pose estimation. However, when the number of layers is greater than 3, the additional layers can only bring marginal improvement. For example, increasing layers from three to four only brings 0.4mm MPJPE improvement. Therefore, we adopt a 3-layer PRFormerÂ  in our model. 
<br class="ltx_break"/></p>
</div>
<figure class="ltx_table" id="S4.T8">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_pruned_first ltx_align_bottom" id="S4.T8.2" style="width:199.5pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_block"><span class="ltx_text" id="S4.T8.2.2.1.1" style="font-size:90%;">Table 7</span>: </span><span class="ltx_text" id="S4.T8.2.3.2" style="font-size:80%;">Ablation study about how the number of refinement layers in PRFormerÂ affects the model performance on the UnrealEgo dataset.</span></figcaption>
<div class="ltx_para ltx_noindent ltx_align_center" id="S4.T8.2.p2">
<p class="ltx_p" id="S4.T8.2.p2.1"><span class="ltx_text" id="S4.T8.2.p2.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T8.2.p2.1.1.1" style="width:193.7pt;height:36pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S4.T8.2.p2.1.1.1.1"><span class="ltx_text" id="S4.T8.2.p2.1.1.1.1.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T8.2.p2.1.1.1.1.1.1">
<span class="ltx_tr" id="S4.T8.2.p2.1.1.1.1.1.1.1">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T8.2.p2.1.1.1.1.1.1.1.1">Layers</span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.T8.2.p2.1.1.1.1.1.1.1.2">0</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" id="S4.T8.2.p2.1.1.1.1.1.1.1.3">1</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" id="S4.T8.2.p2.1.1.1.1.1.1.1.4">2</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" id="S4.T8.2.p2.1.1.1.1.1.1.1.5">3</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" id="S4.T8.2.p2.1.1.1.1.1.1.1.6">4</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" id="S4.T8.2.p2.1.1.1.1.1.1.1.7">5</span></span>
<span class="ltx_tr" id="S4.T8.2.p2.1.1.1.1.1.1.2">
<span class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="S4.T8.2.p2.1.1.1.1.1.1.2.1">MPJPE</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T8.2.p2.1.1.1.1.1.1.2.2">46.7</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb ltx_border_t" id="S4.T8.2.p2.1.1.1.1.1.1.2.3">37.9</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb ltx_border_t" id="S4.T8.2.p2.1.1.1.1.1.1.2.4">35.1</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb ltx_border_t" id="S4.T8.2.p2.1.1.1.1.1.1.2.5">33.4</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb ltx_border_t" id="S4.T8.2.p2.1.1.1.1.1.1.2.6">33.0</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb ltx_border_t" id="S4.T8.2.p2.1.1.1.1.1.1.2.7">32.8</span></span>
</span></span></span>
</span></span></span></p>
</div>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_pruned_first ltx_align_bottom" id="S4.T8.4" style="width:216.8pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_block"><span class="ltx_text" id="S4.T8.4.2.1.1" style="font-size:90%;">Table 8</span>: </span><span class="ltx_text" id="S4.T8.4.3.2" style="font-size:80%;">Comparison of MPJPE (PA-MPJPE) between EgoPoseFormerÂ and the heatmap-based UnrealEgo using different backbones on the UnrealEgo dataset.</span></figcaption>
<div class="ltx_para ltx_noindent ltx_align_center" id="S4.T8.4.p2">
<p class="ltx_p" id="S4.T8.4.p2.1"><span class="ltx_text" id="S4.T8.4.p2.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T8.4.p2.1.1.1" style="width:416.2pt;height:54pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S4.T8.4.p2.1.1.1.1"><span class="ltx_text" id="S4.T8.4.p2.1.1.1.1.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T8.4.p2.1.1.1.1.1.1">
<span class="ltx_tr" id="S4.T8.4.p2.1.1.1.1.1.1.1">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T8.4.p2.1.1.1.1.1.1.1.1">Method</span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.T8.4.p2.1.1.1.1.1.1.1.2">ResNet-18</span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.T8.4.p2.1.1.1.1.1.1.1.3">ResNet-34</span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.T8.4.p2.1.1.1.1.1.1.1.4">ResNet-50</span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.T8.4.p2.1.1.1.1.1.1.1.5">ResNet-101</span></span>
<span class="ltx_tr" id="S4.T8.4.p2.1.1.1.1.1.1.2">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.4.p2.1.1.1.1.1.1.2.1">UnrealEgoÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib3" title="">3</a>]</cite></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.4.p2.1.1.1.1.1.1.2.2">79.1 (59.2)</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.4.p2.1.1.1.1.1.1.2.3">80.5 (60.1)</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.4.p2.1.1.1.1.1.1.2.4">80.1 (60.1)</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.4.p2.1.1.1.1.1.1.2.5">80.1 (60.5)</span></span>
<span class="ltx_tr" id="S4.T8.4.p2.1.1.1.1.1.1.3">
<span class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T8.4.p2.1.1.1.1.1.1.3.1">Ours</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T8.4.p2.1.1.1.1.1.1.3.2">33.4 (32.7)</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T8.4.p2.1.1.1.1.1.1.3.3">29.5 (29.0)</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T8.4.p2.1.1.1.1.1.1.3.4">28.5 (28.1)</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T8.4.p2.1.1.1.1.1.1.3.5">27.9 (27.4)</span></span>
</span></span></span>
</span></span></span></p>
</div>
</div>
</div>
</div>
</figure>
<figure class="ltx_table" id="S4.T10">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_pruned_first ltx_align_bottom" id="S4.T10.4" style="width:190.8pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_block"><span class="ltx_text" id="S4.T10.4.3.2.1" style="font-size:90%;">Table 9</span>: </span><span class="ltx_text" id="S4.T10.2.2.1" style="font-size:80%;">Comparison between our approach and the baseline UnrealEgo method on the UnrealEgo dataset using different training strategies. <math alttext="\dagger" class="ltx_Math" display="inline" id="S4.T10.2.2.1.m1.1"><semantics id="S4.T10.2.2.1.m1.1a"><mo id="S4.T10.2.2.1.m1.1.1" xref="S4.T10.2.2.1.m1.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="S4.T10.2.2.1.m1.1b"><ci id="S4.T10.2.2.1.m1.1.1.cmml" xref="S4.T10.2.2.1.m1.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T10.2.2.1.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="S4.T10.2.2.1.m1.1d">â€ </annotation></semantics></math> denotes our reproduced result. </span></figcaption>
<div class="ltx_para ltx_noindent ltx_align_center" id="S4.T10.4.p2">
<p class="ltx_p" id="S4.T10.4.p2.1"><span class="ltx_text" id="S4.T10.4.p2.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T10.4.p2.1.1.1" style="width:232.1pt;height:108pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S4.T10.4.p2.1.1.1.1"><span class="ltx_text" id="S4.T10.4.p2.1.1.1.1.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T10.4.p2.1.1.1.1.1.1">
<span class="ltx_tr" id="S4.T10.4.p2.1.1.1.1.1.1.2">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_tt ltx_rowspan ltx_rowspan_2" id="S4.T10.4.p2.1.1.1.1.1.1.2.1"><span class="ltx_text" id="S4.T10.4.p2.1.1.1.1.1.1.2.1.1">Method</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T10.4.p2.1.1.1.1.1.1.2.2">Training</span>
<span class="ltx_td ltx_align_center ltx_border_tt ltx_rowspan ltx_rowspan_2" id="S4.T10.4.p2.1.1.1.1.1.1.2.3"><span class="ltx_text" id="S4.T10.4.p2.1.1.1.1.1.1.2.3.1">MPJPE</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt ltx_rowspan ltx_rowspan_2" id="S4.T10.4.p2.1.1.1.1.1.1.2.4"><span class="ltx_text" id="S4.T10.4.p2.1.1.1.1.1.1.2.4.1">PA-MPJPE</span></span></span>
<span class="ltx_tr" id="S4.T10.4.p2.1.1.1.1.1.1.3">
<span class="ltx_td ltx_align_center ltx_border_r" id="S4.T10.4.p2.1.1.1.1.1.1.3.1">Strategy</span></span>
<span class="ltx_tr" id="S4.T10.4.p2.1.1.1.1.1.1.4">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_3" id="S4.T10.4.p2.1.1.1.1.1.1.4.1"><span class="ltx_text" id="S4.T10.4.p2.1.1.1.1.1.1.4.1.1">UnrealEgo</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.4.p2.1.1.1.1.1.1.4.2">UnrealEgo</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.4.p2.1.1.1.1.1.1.4.3">79.1</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.4.p2.1.1.1.1.1.1.4.4">59.2</span></span>
<span class="ltx_tr" id="S4.T10.4.p2.1.1.1.1.1.1.1">
<span class="ltx_td ltx_align_center ltx_border_r" id="S4.T10.4.p2.1.1.1.1.1.1.1.1">UnrealEgo<sup class="ltx_sup" id="S4.T10.4.p2.1.1.1.1.1.1.1.1.1"><span class="ltx_text ltx_font_italic" id="S4.T10.4.p2.1.1.1.1.1.1.1.1.1.1">â€ </span></sup></span>
<span class="ltx_td ltx_align_center" id="S4.T10.4.p2.1.1.1.1.1.1.1.2">80.1</span>
<span class="ltx_td ltx_align_center" id="S4.T10.4.p2.1.1.1.1.1.1.1.3">57.9</span></span>
<span class="ltx_tr" id="S4.T10.4.p2.1.1.1.1.1.1.5">
<span class="ltx_td ltx_align_center ltx_border_r" id="S4.T10.4.p2.1.1.1.1.1.1.5.1">Ours</span>
<span class="ltx_td ltx_align_center" id="S4.T10.4.p2.1.1.1.1.1.1.5.2">70.5</span>
<span class="ltx_td ltx_align_center" id="S4.T10.4.p2.1.1.1.1.1.1.5.3">55.3</span></span>
<span class="ltx_tr" id="S4.T10.4.p2.1.1.1.1.1.1.6">
<span class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="S4.T10.4.p2.1.1.1.1.1.1.6.1">Ours</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S4.T10.4.p2.1.1.1.1.1.1.6.2">Ours</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T10.4.p2.1.1.1.1.1.1.6.3">33.4</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T10.4.p2.1.1.1.1.1.1.6.4">32.7</span></span>
</span></span></span>
</span></span></span></p>
</div>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_pruned_first ltx_align_bottom" id="S4.T10.6" style="width:229.8pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_block"><span class="ltx_text" id="S4.T10.6.2.1.1" style="font-size:90%;">Table 10</span>: </span><span class="ltx_text" id="S4.T10.6.3.2" style="font-size:80%;">Detailed ablation study on training strategies of PPN on the UnrealEgo dataset. Note the PPNÂ performance is a bit better than the result in Tab.Â <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S4.T5" title="Table 5 â€£ 4.3 Ablation Studies and Discussions â€£ 4 Experiments â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_tag">5</span></a> because we used a newly pre-trained backbone.</span></figcaption>
<div class="ltx_para ltx_noindent ltx_align_center" id="S4.T10.6.p2">
<p class="ltx_p" id="S4.T10.6.p2.1"><span class="ltx_text" id="S4.T10.6.p2.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T10.6.p2.1.1.1" style="width:360.4pt;height:144pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S4.T10.6.p2.1.1.1.1"><span class="ltx_text" id="S4.T10.6.p2.1.1.1.1.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T10.6.p2.1.1.1.1.1.1">
<span class="ltx_tr" id="S4.T10.6.p2.1.1.1.1.1.1.1">
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T10.6.p2.1.1.1.1.1.1.1.1">Feature</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_rowspan ltx_rowspan_2" id="S4.T10.6.p2.1.1.1.1.1.1.1.2"><span class="ltx_text" id="S4.T10.6.p2.1.1.1.1.1.1.1.2.1">OPT</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_rowspan ltx_rowspan_2" id="S4.T10.6.p2.1.1.1.1.1.1.1.3"><span class="ltx_text" id="S4.T10.6.p2.1.1.1.1.1.1.1.3.1">ACT</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_rowspan ltx_rowspan_2" id="S4.T10.6.p2.1.1.1.1.1.1.1.4"><span class="ltx_text" id="S4.T10.6.p2.1.1.1.1.1.1.1.4.1"># Layer</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T10.6.p2.1.1.1.1.1.1.1.5">Heatmap</span>
<span class="ltx_td ltx_align_center ltx_border_tt ltx_rowspan ltx_rowspan_2" id="S4.T10.6.p2.1.1.1.1.1.1.1.6"><span class="ltx_text" id="S4.T10.6.p2.1.1.1.1.1.1.1.6.1">MPJPE</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt ltx_rowspan ltx_rowspan_2" id="S4.T10.6.p2.1.1.1.1.1.1.1.7"><span class="ltx_text" id="S4.T10.6.p2.1.1.1.1.1.1.1.7.1">PA-MPJPE</span></span></span>
<span class="ltx_tr" id="S4.T10.6.p2.1.1.1.1.1.1.2">
<span class="ltx_td ltx_align_center ltx_border_r" id="S4.T10.6.p2.1.1.1.1.1.1.2.1">Extractor</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S4.T10.6.p2.1.1.1.1.1.1.2.2">Pretrain</span></span>
<span class="ltx_tr" id="S4.T10.6.p2.1.1.1.1.1.1.3">
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.6.p2.1.1.1.1.1.1.3.1">UnrealEgo</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.6.p2.1.1.1.1.1.1.3.2">AdamW</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.6.p2.1.1.1.1.1.1.3.3">GELU</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.6.p2.1.1.1.1.1.1.3.4">2</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.6.p2.1.1.1.1.1.1.3.5">âœ—</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.6.p2.1.1.1.1.1.1.3.6">48.3</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.6.p2.1.1.1.1.1.1.3.7">40.4</span></span>
<span class="ltx_tr" id="S4.T10.6.p2.1.1.1.1.1.1.4">
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_5" id="S4.T10.6.p2.1.1.1.1.1.1.4.1"><span class="ltx_text" id="S4.T10.6.p2.1.1.1.1.1.1.4.1.1">Ours</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.6.p2.1.1.1.1.1.1.4.2">Adam</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.6.p2.1.1.1.1.1.1.4.3">Leaky</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.6.p2.1.1.1.1.1.1.4.4">3</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.6.p2.1.1.1.1.1.1.4.5">âœ—</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.6.p2.1.1.1.1.1.1.4.6">168.9</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.6.p2.1.1.1.1.1.1.4.7">125.5</span></span>
<span class="ltx_tr" id="S4.T10.6.p2.1.1.1.1.1.1.5">
<span class="ltx_td ltx_align_center ltx_border_r" id="S4.T10.6.p2.1.1.1.1.1.1.5.1">AdamW</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S4.T10.6.p2.1.1.1.1.1.1.5.2">Leaky</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S4.T10.6.p2.1.1.1.1.1.1.5.3">3</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S4.T10.6.p2.1.1.1.1.1.1.5.4">âœ—</span>
<span class="ltx_td ltx_align_center" id="S4.T10.6.p2.1.1.1.1.1.1.5.5">59.6</span>
<span class="ltx_td ltx_align_center" id="S4.T10.6.p2.1.1.1.1.1.1.5.6">48.5</span></span>
<span class="ltx_tr" id="S4.T10.6.p2.1.1.1.1.1.1.6">
<span class="ltx_td ltx_align_center ltx_border_r" id="S4.T10.6.p2.1.1.1.1.1.1.6.1">AdamW</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S4.T10.6.p2.1.1.1.1.1.1.6.2">Leaky</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S4.T10.6.p2.1.1.1.1.1.1.6.3">3</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S4.T10.6.p2.1.1.1.1.1.1.6.4">âœ“</span>
<span class="ltx_td ltx_align_center" id="S4.T10.6.p2.1.1.1.1.1.1.6.5">43.5</span>
<span class="ltx_td ltx_align_center" id="S4.T10.6.p2.1.1.1.1.1.1.6.6">36.5</span></span>
<span class="ltx_tr" id="S4.T10.6.p2.1.1.1.1.1.1.7">
<span class="ltx_td ltx_align_center ltx_border_r" id="S4.T10.6.p2.1.1.1.1.1.1.7.1">AdamW</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S4.T10.6.p2.1.1.1.1.1.1.7.2">GELU</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S4.T10.6.p2.1.1.1.1.1.1.7.3">3</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S4.T10.6.p2.1.1.1.1.1.1.7.4">âœ“</span>
<span class="ltx_td ltx_align_center" id="S4.T10.6.p2.1.1.1.1.1.1.7.5">42.6</span>
<span class="ltx_td ltx_align_center" id="S4.T10.6.p2.1.1.1.1.1.1.7.6">36.1</span></span>
<span class="ltx_tr" id="S4.T10.6.p2.1.1.1.1.1.1.8">
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T10.6.p2.1.1.1.1.1.1.8.1">AdamW</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T10.6.p2.1.1.1.1.1.1.8.2">GELU</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T10.6.p2.1.1.1.1.1.1.8.3">2</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T10.6.p2.1.1.1.1.1.1.8.4">âœ“</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T10.6.p2.1.1.1.1.1.1.8.5">43.1</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T10.6.p2.1.1.1.1.1.1.8.6">36.3</span></span>
</span></span></span>
</span></span></span></p>
</div>
</div>
</div>
</div>
</figure>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="292" id="S4.F6.g1" src="x6.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.3.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S4.F6.4.2" style="font-size:90%;">Heatmaps of averaged self-attention weights between different joints on the UnrealEgo and SceneEgo datasets.</span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS3.p5">
<p class="ltx_p" id="S4.SS3.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p5.1.1">Scaling up experiments.</span>
InÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S4.T8" title="In 4.3 Ablation Studies and Discussions â€£ 4 Experiments â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Tab.</span>Â <span class="ltx_text ltx_ref_tag">8</span></a>, we show our approach has a good scaling-up ability when using larger backbones. Specifically, we test our methods using different ResNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib9" title="">9</a>]</cite> variants on the UnrealEgo dataset. We keep the training recipe for all backbones the same, as described inÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S4.SS1" title="4.1 Experiment Settings â€£ 4 Experiments â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Sec.</span>Â <span class="ltx_text ltx_ref_tag">4.1</span></a>. We observe that the performance of our method gets consistently improves when we use larger backbones, which shows that our method scales well. In addition, we also report the performance of the previous heatmap-based UnrealEgoÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib3" title="">3</a>]</cite> method, which exhibits poor scaling-up ability when equipped with larger backbones. As we explained inÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S1" title="1 Introduction â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Sec.</span>Â <span class="ltx_text ltx_ref_tag">1</span></a>, this may be caused by the idling of visual features. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p6">
<p class="ltx_p" id="S4.SS3.p6.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p6.1.1">Training Strategy.</span>
One surprising result of our paper is that even our Pose Proposal NetworkÂ excels the prior state-of-the-art method. Therefore, we raise a pertinent question: <span class="ltx_text ltx_font_italic" id="S4.SS3.p6.1.2">Is the superiority of EgoPoseFormerÂ a consequence of a better training strategy instead of the model itself?</span> Notably, the most distinct difference in our training strategy, as compared to previous approachesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib12" title="">12</a>]</cite>, is that we followed the common practicesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib4" title="">4</a>]</cite> of transformer training to use the advanced AdamWÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib20" title="">20</a>]</cite> optimizer. In contrast, prior works employed the Adam optimizerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib13" title="">13</a>]</cite>. To investigate this, we present the results of the UnrealEgoÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#bib.bib3" title="">3</a>]</cite> baseline method using different training strategies inÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S4.T10" title="In 4.3 Ablation Studies and Discussions â€£ 4 Experiments â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Tab.</span>Â <span class="ltx_text ltx_ref_tag">10</span></a>. The outcomes clearly demonstrate that switching to our training strategy can indeed improve the MPJPE of the baseline method by around 9mm. Nevertheless, it is noteworthy that even with this improved baseline, our EgoPoseFormerÂ still significantly outperforms it, validating the effectiveness of our method. InÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#S4.T10" title="In 4.3 Ablation Studies and Discussions â€£ 4 Experiments â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Tab.</span>Â <span class="ltx_text ltx_ref_tag">10</span></a>, we report extensive ablation studies about how the training techniques affect the performance PPNÂ when the PRFormerÂ part is removed. Here we investigate several factors: 1) the feature extractor architecture, 2) optimizer, 3) activation function, 4) number of MLP layers, 5) whether the feature extractors are pre-trained. The result shows the AdamW optimizer and the heatmap pre-training are the two most important reasons for PPNâ€™s good performance.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We introduce EgoPoseFormer, a new transformer-based egocentric pose estimation method. Our two-stage method first infers each jointâ€™s coarse from the global features, then uses a DETR-style transformer Pose Refinement TransformerÂ to refine the coarse locations by exploiting fine-grained stereo features and human kinematic information. Furthermore, we design Deformable Stereo AttentionÂ to better exploit the multi-view stereo information. Our method achieves state-of-the-art with significant advantages over previous arts on two pose estimation datasets, including stereo and monocular settings. We hope our model can serve as a strong baseline approach for future research in this field.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Apple Vision Pro. <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.apple.com/apple-vision-pro/" title="">https://www.apple.com/apple-vision-pro/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Meta Quest 3. <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.meta.com/quest/quest-3/" title="">https://www.meta.com/quest/quest-3/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Akada, H., Wang, J., Shimada, S., Takahashi, M., Theobalt, C., Golyanik, V.: Unrealego: A new dataset for robust egocentric 3d human motion capture. In: European Conference on Computer Vision (ECCV) (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-to-End Object Detection with Transformers. ECCV (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Cheng, B., Misra, I., Schwing, A.G., Kirillov, A., Girdhar, R.: Masked-attention mask transformer for universal image segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 1290â€“1299 (June 2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Choi, H., Moon, G., Park, J., Lee, K.M.: Learning to estimate robust 3d human mesh from in-the-wild crowded scenes. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1475â€“1484 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., etÂ al.: An image is worth 16x16 words: Transformers for image recognition at scale. In: ICLR (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Einfalt, M., Ludwig, K., Lienhart, R.: Uplift and upsample: Efficient 3d human pose estimation with uplifting transformers. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 2903â€“2913 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Ionescu, C., Papava, D., Olaru, V., Sminchisescu, C.: Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. IEEE transactions on pattern analysis and machine intelligence <span class="ltx_text ltx_font_bold" id="bib.bib10.1.1">36</span>(7), 1325â€“1339 (2013)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Jiang, H., Ithapu, V.K.: Egocentric pose estimation from human vision span. In: 2021 IEEE/CVF International Conference on Computer Vision (ICCV). pp. 10986â€“10994. IEEE (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Kang, T., Lee, K., Zhang, J., Lee, Y.: Ego3dpose: Capturing 3d cues from binocular egocentric views. In: SIGGRAPH Asia 2023 Conference Papers (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Li, J., Liu, K., Wu, J.: Ego-body pose estimation via ego-head pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 17142â€“17151 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Li, W., Liu, H., Ding, R., Liu, M., Wang, P., Yang, W.: Exploiting temporal contexts with strided transformer for 3d human pose estimation. IEEE Transactions on Multimedia <span class="ltx_text ltx_font_bold" id="bib.bib15.1.1">25</span>, 1282â€“1293 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Li, W., Liu, H., Tang, H., Wang, P., VanÂ Gool, L.: Mhformer: Multi-hypothesis transformer for 3d human pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 13147â€“13156 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., DollÃ¡r, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: ECCV (2014)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Liu, H., Chen, Q., Tan, Z., Liu, J.J., Wang, J., Su, X., Li, X., Yao, K., Han, J., Ding, E., etÂ al.: Group pose: A simple baseline for end-to-end multi-person pose estimation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 15029â€“15038 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Liu, H., Chen, Q., Tan, Z., Liu, J., Wang, J., Su, X., Li, X., Yao, K., Han, J., Ding, E., Zhao, Y., Wang, J.: Group pose: A simple baseline for end-to-end multi-person pose estimation. In: Proceedings of the IEEE International Conference on Computer Vision (ICCV) (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: International Conference on Learning Representations (2019), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=Bkg6RiCqY7" title="">https://openreview.net/forum?id=Bkg6RiCqY7</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Luo, Z., Hachiuma, R., Yuan, Y., Kitani, K.: Dynamics-regulated kinematic policy for egocentric pose estimation. Advances in Neural Information Processing Systems <span class="ltx_text ltx_font_bold" id="bib.bib21.1.1">34</span>, 25019â€“25032 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Park, J., Kaai, K., Hossain, S., Sumi, N., Rambhatla, S., Fieguth, P.: Building spatio-temporal transformers for egocentric 3d pose estimation. arXiv preprint arXiv:2206.04785 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Qiu, Z., Yang, Q., Wang, J., Feng, H., Han, J., Ding, E., Xu, C., Fu, D., Wang, J.: Psvt: End-to-end multi-person 3d pose and shape estimation with progressive video transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 21254â€“21263 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Ren, P., Wen, C., Zheng, X., Xue, Z., Sun, H., Qi, Q., Wang, J., Liao, J.: Decoupled iterative refinement framework for interacting hands reconstruction from a single rgb image. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 8014â€“8025 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with region proposal networks. In: NeurIPS (2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Rhodin, H., Richardt, C., Casas, D., Insafutdinov, E., Shafiei, M., Seidel, H.P., Schiele, B., Theobalt, C.: Egocap: egocentric marker-less motion capture with two fisheye cameras. ACM Transactions on Graphics (TOG) <span class="ltx_text ltx_font_bold" id="bib.bib26.1.1">35</span>(6), 1â€“11 (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. In: Medical Image Computing and Computer-Assisted Interventionâ€“MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18. pp. 234â€“241. Springer (2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Scaramuzza, D., Martinelli, A., Siegwart, R.: A flexible technique for accurate omnidirectional camera calibration and structure from motion. In: Fourth IEEE International Conference on Computer Vision Systems (ICVSâ€™06). pp. 45â€“45. IEEE (2006)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Shan, W., Liu, Z., Zhang, X., Wang, S., Ma, S., Gao, W.: P-stmo: Pre-trained spatial temporal many-to-one model for 3d human pose estimation. In: European Conference on Computer Vision. pp. 461â€“478. Springer (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Shi, D., Wei, X., Li, L., Ren, Y., Tan, W.: End-to-end multi-person pose estimation with transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 11069â€“11078 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Tome, D., Alldieck, T., Peluse, P., Pons-Moll, G., Agapito, L., Badino, H., DeÂ la Torre, F.: Selfpose: 3d egocentric pose estimation from a headset mounted camera. IEEE Transactions on Pattern Analysis and Machine Intelligence (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Tome, D., Peluse, P., Agapito, L., Badino, H.: xr-egopose: Egocentric 3d human pose from an hmd camera. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 7728â€“7738 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., JÃ©gou, H.: Training data-efficient image transformers &amp; distillation through attention. In: International conference on machine learning. pp. 10347â€“10357. PMLR (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Å., Polosukhin, I.: Attention is all you need. Advances in neural information processing systems <span class="ltx_text ltx_font_bold" id="bib.bib34.1.1">30</span> (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Wang, J., Liu, L., Xu, W., Sarkar, K., Luvizon, D., Theobalt, C.: Estimating egocentric 3d human pose in the wild with external weak supervision. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 13157â€“13166 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Wang, J., Liu, L., Xu, W., Sarkar, K., Theobalt, C.: Estimating egocentric 3d human pose in global space. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 11500â€“11509 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Wang, J., Luvizon, D., Xu, W., Liu, L., Sarkar, K., Theobalt, C.: Scene-aware egocentric 3d human pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 13031â€“13040 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Wang, L., Yan, S., Zhen, J., Liu, Y., Zhang, M., Zhang, G., Zhou, X.: Deep active contours for real-time 6-dof object tracking. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14034â€“14044 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Wang, Y., Daniilidis, K.: Refit: Recurrent fitting network for 3d human recovery. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 14644â€“14654 (October 2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Xiao, Y., Su, K., Wang, X., Yu, D., Jin, L., He, M., Yuan, Z.: Querypose: Sparse multi-person pose regression via spatial-aware part-level query. Advances in Neural Information Processing Systems <span class="ltx_text ltx_font_bold" id="bib.bib40.1.1">35</span>, 12464â€“12477 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Xu, W., Chatterjee, A., Zollhoefer, M., Rhodin, H., Fua, P., Seidel, H.P., Theobalt, C.: Mo 2 cap 2: Real-time mobile 3d motion capture with a cap-mounted fisheye camera. IEEE transactions on visualization and computer graphics <span class="ltx_text ltx_font_bold" id="bib.bib41.1.1">25</span>(5), 2093â€“2101 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Yang, J., Zeng, A., Liu, S., Li, F., Zhang, R., Zhang, L.: Explicit box detection unifies end-to-end multi-person pose estimation. arXiv preprint arXiv:2302.01593 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Yuan, Y., Kitani, K.: 3d ego-pose estimation via imitation learning. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 735â€“750 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Zhang, J., Tu, Z., Yang, J., Chen, Y., Yuan, J.: Mixste: Seq2seq mixed spatio-temporal encoder for 3d human pose estimation in video. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 13232â€“13242 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Zhao, D., Wei, Z., Mahmud, J., Frahm, J.M.: Egoglass: Egocentric-view human pose estimation from an eyeglass frame. In: 2021 International Conference on 3D Vision (3DV). pp. 32â€“41. IEEE (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Zhao, Q., Zheng, C., Liu, M., Wang, P., Chen, C.: Poseformerv2: Exploring frequency domain for efficient and robust 3d human pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8877â€“8886 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Zheng, C., Zhu, S., Mendieta, M., Yang, T., Chen, C., Ding, Z.: 3d human pose estimation with spatial and temporal transformers. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 11656â€“11665 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable {detr}: Deformable transformers for end-to-end object detection. In: International Conference on Learning Representations (2021), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=gZ9hCDWe6ke" title="">https://openreview.net/forum?id=gZ9hCDWe6ke</a>
</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="Pt0.A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.A </span>Extra experiments</h2>
<figure class="ltx_table" id="Pt0.A1.T11">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Pt0.A1.T11.3.1.1" style="font-size:90%;">Table 11</span>: </span><span class="ltx_text" id="Pt0.A1.T11.4.2" style="font-size:80%;">Ablation study on 2D heatmap pre-training on the UnrealEgo dataset.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="Pt0.A1.T11.5" style="width:216.8pt;height:70.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-56.7pt,18.5pt) scale(0.65672,0.65672) ;">
<p class="ltx_p" id="Pt0.A1.T11.5.1"><span class="ltx_text" id="Pt0.A1.T11.5.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="Pt0.A1.T11.5.1.1.1" style="width:330.1pt;height:108pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="Pt0.A1.T11.5.1.1.1.1"><span class="ltx_text" id="Pt0.A1.T11.5.1.1.1.1.1">
<span class="ltx_tabular ltx_align_middle" id="Pt0.A1.T11.5.1.1.1.1.1.1">
<span class="ltx_tr" id="Pt0.A1.T11.5.1.1.1.1.1.1.1">
<span class="ltx_td ltx_border_r ltx_border_tt" id="Pt0.A1.T11.5.1.1.1.1.1.1.1.1"></span>
<span class="ltx_td ltx_border_r ltx_border_tt" id="Pt0.A1.T11.5.1.1.1.1.1.1.1.2"></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_2" id="Pt0.A1.T11.5.1.1.1.1.1.1.1.3">First Stage</span>
<span class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" id="Pt0.A1.T11.5.1.1.1.1.1.1.1.4">Second Stage</span></span>
<span class="ltx_tr" id="Pt0.A1.T11.5.1.1.1.1.1.1.2">
<span class="ltx_td ltx_align_left ltx_border_r" id="Pt0.A1.T11.5.1.1.1.1.1.1.2.1">Dataset</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="Pt0.A1.T11.5.1.1.1.1.1.1.2.2">Pretrain</span>
<span class="ltx_td ltx_align_center" id="Pt0.A1.T11.5.1.1.1.1.1.1.2.3">MPJPE</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="Pt0.A1.T11.5.1.1.1.1.1.1.2.4">PA-MPJPE</span>
<span class="ltx_td ltx_align_center" id="Pt0.A1.T11.5.1.1.1.1.1.1.2.5">MPJPE</span>
<span class="ltx_td ltx_align_center" id="Pt0.A1.T11.5.1.1.1.1.1.1.2.6">PA-MPJPE</span></span>
<span class="ltx_tr" id="Pt0.A1.T11.5.1.1.1.1.1.1.3">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" id="Pt0.A1.T11.5.1.1.1.1.1.1.3.1"><span class="ltx_text" id="Pt0.A1.T11.5.1.1.1.1.1.1.3.1.1">UnrealEgo</span></span>
<span class="ltx_td ltx_border_r ltx_border_t" id="Pt0.A1.T11.5.1.1.1.1.1.1.3.2"></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A1.T11.5.1.1.1.1.1.1.3.3">48.8</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Pt0.A1.T11.5.1.1.1.1.1.1.3.4">40.1</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A1.T11.5.1.1.1.1.1.1.3.5">36.5</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A1.T11.5.1.1.1.1.1.1.3.6">35.1</span></span>
<span class="ltx_tr" id="Pt0.A1.T11.5.1.1.1.1.1.1.4">
<span class="ltx_td ltx_align_center ltx_border_r" id="Pt0.A1.T11.5.1.1.1.1.1.1.4.1">âœ“</span>
<span class="ltx_td ltx_align_center" id="Pt0.A1.T11.5.1.1.1.1.1.1.4.2">45.5</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="Pt0.A1.T11.5.1.1.1.1.1.1.4.3">38.3</span>
<span class="ltx_td ltx_align_center" id="Pt0.A1.T11.5.1.1.1.1.1.1.4.4">33.4</span>
<span class="ltx_td ltx_align_center" id="Pt0.A1.T11.5.1.1.1.1.1.1.4.5">32.7</span></span>
<span class="ltx_tr" id="Pt0.A1.T11.5.1.1.1.1.1.1.5">
<span class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" id="Pt0.A1.T11.5.1.1.1.1.1.1.5.1"><span class="ltx_text" id="Pt0.A1.T11.5.1.1.1.1.1.1.5.1.1">SceneEgo</span></span>
<span class="ltx_td ltx_border_r ltx_border_t" id="Pt0.A1.T11.5.1.1.1.1.1.1.5.2"></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A1.T11.5.1.1.1.1.1.1.5.3">182.5</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Pt0.A1.T11.5.1.1.1.1.1.1.5.4">119.6</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A1.T11.5.1.1.1.1.1.1.5.5">122.9</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A1.T11.5.1.1.1.1.1.1.5.6">97.2</span></span>
<span class="ltx_tr" id="Pt0.A1.T11.5.1.1.1.1.1.1.6">
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="Pt0.A1.T11.5.1.1.1.1.1.1.6.1">âœ“</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="Pt0.A1.T11.5.1.1.1.1.1.1.6.2">120.3</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="Pt0.A1.T11.5.1.1.1.1.1.1.6.3">87.9</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="Pt0.A1.T11.5.1.1.1.1.1.1.6.4">93.0</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="Pt0.A1.T11.5.1.1.1.1.1.1.6.5">74.3</span></span>
</span></span></span>
</span></span></span></p>
</span></div>
</figure>
<section class="ltx_subsection" id="Pt0.A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">0.A.1 </span>2D heatmap pre-training.</h3>
<div class="ltx_para" id="Pt0.A1.SS1.p1">
<p class="ltx_p" id="Pt0.A1.SS1.p1.1">Intuitively, 2D heatmap pre-training should tell the model what the appearance of a joint should look like, thus it can guide the deformable stereo attention to attend to relevant features, which can further help with accurately estimating the jointsâ€™ 3D locations. InÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#Pt0.A1.T11" title="In Appendix 0.A Extra experiments â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_tag">Tab.</span>Â <span class="ltx_text ltx_ref_tag">11</span></a>, we report how this pre-training influences the model performance on both the stereo UnrealEgo and the monocular SceneEgo datasets. On UnrealEgo, our pre-training improves the MPJPE of the pose proposal by 3.3mm and the final prediction by 3.1mm. However, on the SceneEgo dataset, such improvement becomes more significant, with 62.2mm for the pose proposal and 29.9mm for the final prediction. The reason is that as the SceneEgo dataset does not have stereo information, the appearance features become more important in localizing a joint. This experiment validates the effectiveness of our pre-training strategy.</p>
</div>
<figure class="ltx_figure" id="Pt0.A1.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="557" id="Pt0.A1.F7.g1" src="x7.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Pt0.A1.F7.3.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text" id="Pt0.A1.F7.4.2" style="font-size:80%;">Qualitative comparison between EgoPoseFormer and the UnrealEgo baseline model on the UnrealEgo dataset. Ground-truths are colored in <span class="ltx_text" id="Pt0.A1.F7.4.2.1" style="color:#018000;">green</span> and predictions are colored in <span class="ltx_text" id="Pt0.A1.F7.4.2.2" style="color:#FF0000;">red</span>.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="Pt0.A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">0.A.2 </span>Qualitative comparison with the baseline</h3>
<div class="ltx_para" id="Pt0.A1.SS2.p1">
<p class="ltx_p" id="Pt0.A1.SS2.p1.1">In Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#Pt0.A1.F7" title="Figure 7 â€£ 0.A.1 2D heatmap pre-training. â€£ Appendix 0.A Extra experiments â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_tag">7</span></a>, we compare the qualitative failure cases of PPN (pose proposal), PRFormer (final prediction), and the baseline â€˜UnrealEgoâ€™ model. It shows that most errors in our modelâ€™s final prediction are caused by the joint invisibility problem (mostly in lower-body). Compared with PRFormer, PPNâ€™s estimations are more inaccurate because they are computed using the coarse global features. On the other hand, the baselineâ€™s performance is far from satisfactory even when the joints are captured by both cameras.</p>
</div>
<figure class="ltx_figure" id="Pt0.A1.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="271" id="Pt0.A1.F8.g1" src="extracted/5794257/figures/images/faliure_case.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Pt0.A1.F8.3.1.1" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text" id="Pt0.A1.F8.4.2" style="font-size:80%;">Visualization of EgoPoseFormerâ€™s most inaccurate failure cases on the UnrealEgo dataset. Ground-truths are colored in <span class="ltx_text" id="Pt0.A1.F8.4.2.1" style="color:#018000;">green</span> and predictions are colored in <span class="ltx_text" id="Pt0.A1.F8.4.2.2" style="color:#FF0000;">red</span>.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="Pt0.A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">0.A.3 </span>Qualitative failure cases</h3>
<div class="ltx_para" id="Pt0.A1.SS3.p1">
<p class="ltx_p" id="Pt0.A1.SS3.p1.1">In order to gain a more intuitive insight into the failure cases of our method, we present visualizations of some of the most inaccurate results in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#Pt0.A1.F8" title="Figure 8 â€£ 0.A.2 Qualitative comparison with the baseline â€£ Appendix 0.A Extra experiments â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_tag">8</span></a>. A clear observation is that the majority of these failure cases are attributed to the problem of joint invisibility. For instance, in the first example in the first column, the lower body of the wearer is entirely occluded by his upper body, leading to a substantial discrepancy in the estimated locations of the lower body joints. In another instance, illustrated in the last example of the first column, nearly half of the wearerâ€™s body extends outside the FOV of both cameras, causing severe inaccuracy for the estimated 3D pose. These examples underscore the impact of joint invisibility in egocentric 3D pose estimation. Although our method can indeed estimate the locations of invisible joints in some cases, as we explained in the main paper, such an estimation is achieved by jointly looking at the visible joints and the background scene. However, when a large part of the wearerâ€™s body is invisible, the estimated pose is still far from accurate. Therefore, achieving accurate localization for such joints remains an important and valuable topic for future research endeavors.</p>
</div>
</section>
<section class="ltx_subsection" id="Pt0.A1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">0.A.4 </span>Dependency of two stages</h3>
<figure class="ltx_figure" id="Pt0.A1.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="342" id="Pt0.A1.F9.1.g1" src="x8.png" width="415"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Pt0.A1.F9.4.1.1" style="font-size:90%;">Figure 9</span>: </span><span class="ltx_text" id="Pt0.A1.F9.5.2" style="font-size:80%;">Dependency of the accuracy of PPN and PRFormer.</span></figcaption>
</figure>
<div class="ltx_para" id="Pt0.A1.SS4.p1">
<p class="ltx_p" id="Pt0.A1.SS4.p1.1">Here, we conduct an ablation experiment on the UnrealEgo dataset to check PRFormerâ€™s performance when the quality of the pose proposal varies. Specifically, we use perturbed ground truth, computed by adding Gaussian noises with different scales, to serve as pose proposals, based on which we use PRFormer to compute the refined pose estimation. We plot the dependency of the two stageâ€™s MPJPE in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2403.18080v2#Pt0.A1.F9" title="Figure 9 â€£ 0.A.4 Dependency of two stages â€£ Appendix 0.A Extra experiments â€£ EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation"><span class="ltx_text ltx_ref_tag">9</span></a> Left. The result suggests that although PRFormerâ€™s performance is positively related to the accuracy of the pose proposal, the refined pose estimation is always more accurate than the initial estimation, validating the effectiveness of our PRFormer.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="Pt0.A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.B </span>Potential ethical concerns</h2>
<div class="ltx_para" id="Pt0.A2.p1">
<p class="ltx_p" id="Pt0.A2.p1.1">Technically, one limitation of our PPN and PRFormer is that they assume headset wearers to have a full body, resulting in poor support for estimating body poses for individuals with disabilities, particularly those who have lost parts of their bodies. We acknowledge this limitation and plan to address it in future work. Another potential negative impact of our model is related to user privacy. For example, malicious agents could misuse the technology to analyze a userâ€™s body pose without their permission. The widespread use of such technology could also potentially lead to increased surveillance and tracking of individuals, raising ethical concerns about its use in public and private spaces. Another concern could be the risk of reinforcing biases present in the training data, which could lead to inaccuracies in pose estimation for certain demographic groups. Finally, there is the potential for dependency on this technology. For example, one may first need to buy a headset before using our model, which might reduce usersâ€™ ability to perform tasks without it, affecting their autonomy and skill development.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Aug 15 17:05:49 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
