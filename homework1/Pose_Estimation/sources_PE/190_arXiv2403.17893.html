<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>A Survey on 3D Egocentric Human Pose Estimation</title>
<!--Generated on Wed May  1 14:50:05 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2403.17893v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#S1" title="In A Survey on 3D Egocentric Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#S2" title="In A Survey on 3D Egocentric Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#S3" title="In A Survey on 3D Egocentric Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>3D Egocentric Pose Estimation Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#S3.SS1" title="In 3 3D Egocentric Pose Estimation Methods ‣ A Survey on 3D Egocentric Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Skeletal Based Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#S3.SS2" title="In 3 3D Egocentric Pose Estimation Methods ‣ A Survey on 3D Egocentric Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Body Shape Based Methods</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#S4" title="In A Survey on 3D Egocentric Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Evaluation Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#S5" title="In A Survey on 3D Egocentric Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Performance Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#S6" title="In A Survey on 3D Egocentric Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion and Future Directions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#S7" title="In A Survey on 3D Egocentric Human Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Acknowledgements</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">A Survey on 3D Egocentric Human Pose Estimation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Md Mushfiqur Azam, Kevin Desai
<br class="ltx_break"/>The University of Texas at San Antonio
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1" style="font-size:90%;">{mdmushfiqur.azam, kevin.desai}@utsa.edu</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">Egocentric human pose estimation aims to estimate human body poses and develop body representations from a first-person camera perspective. It has gained vast popularity in recent years because of its wide range of applications in sectors like XR-technologies, human-computer interaction, and fitness tracking. However, to the best of our knowledge, there is no systematic literature review based on the proposed solutions regarding egocentric 3D human pose estimation. To that end, the aim of this survey paper is to provide an extensive overview of the current state of egocentric pose estimation research. In this paper, we categorize and discuss the popular datasets and the different pose estimation models, highlighting the strengths and weaknesses of different methods by comparative analysis. This survey can be a valuable resource for both researchers and practitioners in the field, offering insights into key concepts and cutting-edge solutions in egocentric pose estimation, its wide-ranging applications, as well as the open problems with future scope.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Human pose estimation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib62" title=""><span class="ltx_text" style="font-size:90%;">62</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">10</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib55" title=""><span class="ltx_text" style="font-size:90%;">55</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite> has gained prominence due to its relevance in numerous applications, ranging from animation and gaming to surveillance, healthcare, and human-computer interaction.
The rise of wearable technology, including smart glasses, body-mounted cameras, and head-mounted displays has significantly fueled interest in egocentric pose estimation, where the focus is on estimating the pose of the person from the point of view of a wearable camera or device worn by the person (first person perspective).
Egocentric pose estimation plays a crucial role across various domains, such as in human computer interaction for gesture recognition, augmented and virtual reality experiences by tracking body movements, healthcare for precise therapy monitoring, biomechanical analysis in sports training, hand-object interaction for contextual understanding, and enhancing realism in professional simulations through accurate movement replication.
Unlike traditional pose estimation, which relies on external cameras or sensors, egocentric pose estimation offers a unique and immersive perspective on human body representation. Real-time processing, adaptability to different environments, user interaction mechanisms, including gestures, and semantic scene understanding contribute to the effectiveness of egocentric pose estimation systems.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ A Survey on 3D Egocentric Human Pose Estimation"><span class="ltx_text ltx_ref_tag">1</span></a> shows the difference between traditional and egocentric 3D human pose estimation.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.p2.1.1">Challenges for Egocentric 3D Human Pose Estimation</span> stem from the complexity of accurately capturing and interpreting human movements from the first-person perspective. Some of the key challenges include:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S1.I1.i1.p1.1.1">Viewpoint Variations:</span> The use of egocentric cameras, attached to the body, introduces challenges in pose estimation as body parts may be occluded, particularly when hidden from view. The wide range of possible viewpoints in egocentric settings, involving varying camera angles, heights, and orientations, demands robust models to ensure accurate pose estimation across diverse scenarios.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S1.I1.i2.p1.1.1">Limited Depth Information:</span> Egocentric cameras, commonly mounted on wearable devices, capture scenes in 2D, lacking explicit depth details. This absence complicates the accurate determination of the distance of body parts from the camera, as 2D images may project objects at different distances onto the same plane.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S1.I1.i3.p1.1.1">Dataset Constraints:</span> In-the-wild datasets are essential for capturing real-world complexity, including variations in lighting, backgrounds, activities, and environments. However, their scarcity hinders model generalization, especially in dynamic environments with unpredictable situations. Limited availability of diverse samples, often from motion capture systems, poses challenges for models aiming at real-world outdoor applications.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S1.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S1.F1.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="140" id="S1.F1.sf1.g1" src="extracted/2403.17893v2/figures/pose-estimation.png" width="270"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.sf1.2.1.1" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S1.F1.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="397" id="S1.F1.sf2.g1" src="x1.png" width="1136"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.sf2.2.1.1" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">Difference between (a) traditional human pose estimation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib27" title=""><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite> and (b) egocentric human pose estimation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib61" title=""><span class="ltx_text" style="font-size:90%;">61</span></a>]</cite></span></figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.p3.1.1">Scope of the Survey:</span>
Currently, there are numerous systematic surveys related to 2D and 3D human pose estimation on traditional and deep learning based approaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib78" title=""><span class="ltx_text" style="font-size:90%;">78</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib65" title=""><span class="ltx_text" style="font-size:90%;">65</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib45" title=""><span class="ltx_text" style="font-size:90%;">45</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite> as well shape recovery based approaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib59" title=""><span class="ltx_text" style="font-size:90%;">59</span></a>]</cite>.
While comprehensive reviews on hand pose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite> and action recognition <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib47" title=""><span class="ltx_text" style="font-size:90%;">47</span></a>]</cite> from egocentric vision are present, it is noteworthy that, to the best of our knowledge, no comprehensive survey on full body egocentric 3D pose estimation methods has been published to date. This absence underscores a notable gap in existing research, despite the increasing interest and advancement in this domain.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this survey, we aim to explore the multifaceted aspects of 3D egocentric human pose estimation, by first describing the widely used datasets in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#S2" title="2 Datasets ‣ A Survey on 3D Egocentric Human Pose Estimation"><span class="ltx_text ltx_ref_tag">2</span></a>.
Next, in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#S3" title="3 3D Egocentric Pose Estimation Methods ‣ A Survey on 3D Egocentric Human Pose Estimation"><span class="ltx_text ltx_ref_tag">3</span></a>, we explore the different egocentric estimation methods by dividing them into two categories on the basis of output generation: skeletal based methods and human body shape based methods. Skeletal methods explore different methods which are mostly regression based (estimation of 3D joint co-ordinates) and heatmap based (estimation of 2D heatmaps). On the other hand, body shape based methods mainly generate human models using different shape recovery methods.
Additionally, we present a comprehensive evaluation of egocentric pose estimation models, showcasing various evaluation metrics in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#S4" title="4 Evaluation Metrics ‣ A Survey on 3D Egocentric Human Pose Estimation"><span class="ltx_text ltx_ref_tag">4</span></a> and a detailed performance analysis of state-of-the-art approaches on prominent datasets in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#S5" title="5 Performance Analysis ‣ A Survey on 3D Egocentric Human Pose Estimation"><span class="ltx_text ltx_ref_tag">5</span></a>.
Lastly, we conclude the survey in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#S6" title="6 Conclusion and Future Directions ‣ A Survey on 3D Egocentric Human Pose Estimation"><span class="ltx_text ltx_ref_tag">6</span></a> with some future research scopes for egocentric 3D human pose estimation.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Datasets</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Large scale dataset is one of the key factors in visualizing and analysing a computer vision problem. While benchmark datasets like MPII <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite> and Human3.6M <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite> exist for traditional human pose estimation, there’s a notable gap for egocentric pose estimation benchmark datasets. Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#S2.F2" title="Figure 2 ‣ 2 Datasets ‣ A Survey on 3D Egocentric Human Pose Estimation"><span class="ltx_text ltx_ref_tag">2</span></a> showcases sample images from 4 different datasets. Table <a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#S2.T1" title="Table 1 ‣ 2 Datasets ‣ A Survey on 3D Egocentric Human Pose Estimation"><span class="ltx_text ltx_ref_tag">1</span></a> summarizes the key features of 9 egocentric pose estimation datasets, with more details provided in the text below.</p>
</div>
<figure class="ltx_figure" id="S2.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_bottom" id="S2.F2.1" style="width:429.3pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="118" id="S2.F2.1.g1" src="extracted/2403.17893v2/figures/unrealego.png" width="344"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.1.1.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S2.F2.1.2.2" style="font-size:90%;">Dataset setup for UnrealEgo <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>: Left image shows a glass equipped with two fisheye cameras. The middle image provides a third-person perspective of the person, offering context to the scene. The right image depicts the egocentric view of the person.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_bottom" id="S2.F2.3" style="width:216.8pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_square" height="99" id="S2.F2.2.g1" src="extracted/2403.17893v2/figures/egopw1.png" width="118"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="99" id="S2.F2.3.g2" src="extracted/2403.17893v2/figures/egopw1-exo.png" width="118"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.3.1.1.1" style="font-size:90%;">(d)</span> </span><span class="ltx_text" id="S2.F2.3.2.2" style="font-size:90%;">Sample image from EgoPW <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib66" title=""><span class="ltx_text" style="font-size:90%;">66</span></a>]</cite> dataset visualizing egocentric view on the left image and exocentric view on the right image.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_bottom" id="S2.F2.4" style="width:108.4pt;"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="99" id="S2.F2.4.g1" src="extracted/2403.17893v2/figures/img_000000.jpg" width="123"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.4.1.1.1" style="font-size:90%;">(e)</span> </span><span class="ltx_text" id="S2.F2.4.2.2" style="font-size:90%;"> Sample image from EgoGTA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib67" title=""><span class="ltx_text" style="font-size:90%;">67</span></a>]</cite> dataset.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_bottom" id="S2.F2.5" style="width:108.4pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="99" id="S2.F2.5.g1" src="extracted/2403.17893v2/figures/egoglobal.jpg" width="123"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.5.1.1.1" style="font-size:90%;">(f)</span> </span><span class="ltx_text" id="S2.F2.5.2.2" style="font-size:90%;">Sample image from Wang et al.’s <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib64" title=""><span class="ltx_text" style="font-size:90%;">64</span></a>]</cite> dataset.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.7.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S2.F2.8.2" style="font-size:90%;">Sample images from different datasets used for egocentric human pose estimation.</span></figcaption>
</figure>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.p2.1.1">EgoCap</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite> proposed a method for creating large training datasets using a marker-less motion capture system. They leveraged eight fixed cameras to estimate 3D skeleton motion. They projected it onto fisheye images from a head-mounted camera setup, enhancing the dataset with background replacement, clothing color variations, and simulated lighting changes.
The training set includes 75,000 annotated fisheye images from six subjects and 25,000 images from two additional subjects for validation.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.2.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.2.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.1.1.1.1">
<span class="ltx_p" id="S2.T1.2.1.1.1.1.1" style="width:48.4pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.2.1.1.1.1.1.1" style="font-size:70%;">Dataset</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.2.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.1.1.2.1">
<span class="ltx_p" id="S2.T1.2.1.1.2.1.1" style="width:17.1pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.2.1.1.2.1.1.1" style="font-size:70%;">Year</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.2.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.1.1.3.1">
<span class="ltx_p" id="S2.T1.2.1.1.3.1.1" style="width:64.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.2.1.1.3.1.1.1" style="font-size:70%;">No. of Images</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.2.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.1.1.4.1">
<span class="ltx_p" id="S2.T1.2.1.1.4.1.1" style="width:76.8pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.2.1.1.4.1.1.1" style="font-size:70%;">No. of Subjects / Actions</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.2.1.1.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.1.1.5.1">
<span class="ltx_p" id="S2.T1.2.1.1.5.1.1" style="width:167.9pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.2.1.1.5.1.1.1" style="font-size:70%;">Characteristics</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S2.T1.2.1.1.6">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.1.1.6.1">
<span class="ltx_p" id="S2.T1.2.1.1.6.1.1"><span class="ltx_text ltx_font_bold" id="S2.T1.2.1.1.6.1.1.1" style="font-size:70%;">Dataset Website</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.2.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.2.2.1.1">
<span class="ltx_p" id="S2.T1.2.2.2.1.1.1" style="width:48.4pt;"><span class="ltx_text" id="S2.T1.2.2.2.1.1.1.1" style="font-size:70%;">EgoCap </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.2.2.2.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">51</span></a><span class="ltx_text" id="S2.T1.2.2.2.1.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.2.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.2.2.2.1">
<span class="ltx_p" id="S2.T1.2.2.2.2.1.1" style="width:17.1pt;"><span class="ltx_text" id="S2.T1.2.2.2.2.1.1.1" style="font-size:70%;">2016</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.2.2.2.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.2.2.3.1">
<span class="ltx_p" id="S2.T1.2.2.2.3.1.1" style="width:64.0pt;"><span class="ltx_text" id="S2.T1.2.2.2.3.1.1.1" style="font-size:70%;">100,000</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.2.2.2.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.2.2.4.1">
<span class="ltx_p" id="S2.T1.2.2.2.4.1.1" style="width:76.8pt;"><span class="ltx_text" id="S2.T1.2.2.2.4.1.1.1" style="font-size:70%;">8 subjects</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.2.2.2.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.2.2.5.1">
<span class="ltx_p" id="S2.T1.2.2.2.5.1.1" style="width:167.9pt;"><span class="ltx_text" id="S2.T1.2.2.2.5.1.1.1" style="font-size:70%;">marker-less motion capture system; annotated.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S2.T1.2.2.2.6">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.2.2.6.1">
<span class="ltx_p" id="S2.T1.2.2.2.6.1.1"><a class="ltx_ref ltx_href" href="https://vcai.mpi-inf.mpg.de/projects/EgoCap/" style="font-size:70%;" title="">Link</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.3.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.2.3.3.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.3.3.1.1">
<span class="ltx_p" id="S2.T1.2.3.3.1.1.1" style="width:48.4pt;"><span class="ltx_text" id="S2.T1.2.3.3.1.1.1.1" style="font-size:70%;">Mo</span><sup class="ltx_sup ltx_centering" id="S2.T1.2.3.3.1.1.1.2"><span class="ltx_text" id="S2.T1.2.3.3.1.1.1.2.1" style="font-size:70%;">2</span></sup><span class="ltx_text" id="S2.T1.2.3.3.1.1.1.3" style="font-size:70%;">Cap</span><sup class="ltx_sup ltx_centering" id="S2.T1.2.3.3.1.1.1.4"><span class="ltx_text" id="S2.T1.2.3.3.1.1.1.4.1" style="font-size:70%;">2</span></sup><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.2.3.3.1.1.1.5.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib68" title=""><span class="ltx_text" style="font-size:90%;">68</span></a><span class="ltx_text" id="S2.T1.2.3.3.1.1.1.6.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.2.3.3.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.3.3.2.1">
<span class="ltx_p" id="S2.T1.2.3.3.2.1.1" style="width:17.1pt;"><span class="ltx_text" id="S2.T1.2.3.3.2.1.1.1" style="font-size:70%;">2019</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.2.3.3.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.3.3.3.1">
<span class="ltx_p" id="S2.T1.2.3.3.3.1.1" style="width:64.0pt;"><span class="ltx_text" id="S2.T1.2.3.3.3.1.1.1" style="font-size:70%;">530,000</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.2.3.3.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.3.3.4.1">
<span class="ltx_p" id="S2.T1.2.3.3.4.1.1" style="width:76.8pt;"><span class="ltx_text" id="S2.T1.2.3.3.4.1.1.1" style="font-size:70%;">3000 actions</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.2.3.3.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.3.3.5.1">
<span class="ltx_p" id="S2.T1.2.3.3.5.1.1" style="width:167.9pt;"><span class="ltx_text" id="S2.T1.2.3.3.5.1.1.1" style="font-size:70%;">annotated; 700 different body textures.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S2.T1.2.3.3.6">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.3.3.6.1">
<span class="ltx_p" id="S2.T1.2.3.3.6.1.1"><a class="ltx_ref ltx_href" href="https://vcai.mpi-inf.mpg.de/projects/wxu/Mo2Cap2/" style="font-size:70%;" title="">Link</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.4.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.2.4.4.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.4.4.1.1">
<span class="ltx_p" id="S2.T1.2.4.4.1.1.1" style="width:48.4pt;"><span class="ltx_text" id="S2.T1.2.4.4.1.1.1.1" style="font-size:70%;">xr-EgoPose </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.2.4.4.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">60</span></a><span class="ltx_text" id="S2.T1.2.4.4.1.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.2.4.4.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.4.4.2.1">
<span class="ltx_p" id="S2.T1.2.4.4.2.1.1" style="width:17.1pt;"><span class="ltx_text" id="S2.T1.2.4.4.2.1.1.1" style="font-size:70%;">2019</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.2.4.4.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.4.4.3.1">
<span class="ltx_p" id="S2.T1.2.4.4.3.1.1" style="width:64.0pt;"><span class="ltx_text" id="S2.T1.2.4.4.3.1.1.1" style="font-size:70%;">383,000</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.2.4.4.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.4.4.4.1">
<span class="ltx_p" id="S2.T1.2.4.4.4.1.1" style="width:76.8pt;"><span class="ltx_text" id="S2.T1.2.4.4.4.1.1.1" style="font-size:70%;">23 male and 23 female subjects; 9 actions</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.2.4.4.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.4.4.5.1">
<span class="ltx_p" id="S2.T1.2.4.4.5.1.1" style="width:167.9pt;"><span class="ltx_text" id="S2.T1.2.4.4.5.1.1.1" style="font-size:70%;">synthetic; scene is generated from randomized characters, environments, lighting rigs and animation.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S2.T1.2.4.4.6">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.4.4.6.1">
<span class="ltx_p" id="S2.T1.2.4.4.6.1.1"><a class="ltx_ref ltx_href" href="https://github.com/facebookresearch/xR-EgoPose/tree/main" style="font-size:70%;" title="">Link</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.5.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.2.5.5.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.5.5.1.1">
<span class="ltx_p" id="S2.T1.2.5.5.1.1.1" style="width:48.4pt;"><span class="ltx_text" id="S2.T1.2.5.5.1.1.1.1" style="font-size:70%;">EgoBody </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.2.5.5.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib74" title=""><span class="ltx_text" style="font-size:90%;">74</span></a><span class="ltx_text" id="S2.T1.2.5.5.1.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.2.5.5.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.5.5.2.1">
<span class="ltx_p" id="S2.T1.2.5.5.2.1.1" style="width:17.1pt;"><span class="ltx_text" id="S2.T1.2.5.5.2.1.1.1" style="font-size:70%;">2022</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.2.5.5.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.5.5.3.1">
<span class="ltx_p" id="S2.T1.2.5.5.3.1.1" style="width:64.0pt;"><span class="ltx_text" id="S2.T1.2.5.5.3.1.1.1" style="font-size:70%;">219,731</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.2.5.5.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.5.5.4.1">
<span class="ltx_p" id="S2.T1.2.5.5.4.1.1" style="width:76.8pt;"><span class="ltx_text" id="S2.T1.2.5.5.4.1.1.1" style="font-size:70%;">15 indoor scenes; 36 subjects</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.2.5.5.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.5.5.5.1">
<span class="ltx_p" id="S2.T1.2.5.5.5.1.1" style="width:167.9pt;"><span class="ltx_text" id="S2.T1.2.5.5.5.1.1.1" style="font-size:70%;">two subjects (camera wearer and interactee) involved in different interaction scenarios.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S2.T1.2.5.5.6">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.5.5.6.1">
<span class="ltx_p" id="S2.T1.2.5.5.6.1.1"><a class="ltx_ref ltx_href" href="https://sanweiliti.github.io/egobody/egobody.html" style="font-size:70%;" title="">Link</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.6.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.2.6.6.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.6.6.1.1">
<span class="ltx_p" id="S2.T1.2.6.6.1.1.1" style="width:48.4pt;"><span class="ltx_text" id="S2.T1.2.6.6.1.1.1.1" style="font-size:70%;">EgoPW </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.2.6.6.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib66" title=""><span class="ltx_text" style="font-size:90%;">66</span></a><span class="ltx_text" id="S2.T1.2.6.6.1.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.2.6.6.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.6.6.2.1">
<span class="ltx_p" id="S2.T1.2.6.6.2.1.1" style="width:17.1pt;"><span class="ltx_text" id="S2.T1.2.6.6.2.1.1.1" style="font-size:70%;">2022</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.2.6.6.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.6.6.3.1">
<span class="ltx_p" id="S2.T1.2.6.6.3.1.1" style="width:64.0pt;"><span class="ltx_text" id="S2.T1.2.6.6.3.1.1.1" style="font-size:70%;">318,000</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.2.6.6.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.6.6.4.1">
<span class="ltx_p" id="S2.T1.2.6.6.4.1.1" style="width:76.8pt;"><span class="ltx_text" id="S2.T1.2.6.6.4.1.1.1" style="font-size:70%;">10 subjects; 20 actions</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.2.6.6.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.6.6.5.1">
<span class="ltx_p" id="S2.T1.2.6.6.5.1.1" style="width:167.9pt;"><span class="ltx_text" id="S2.T1.2.6.6.5.1.1.1" style="font-size:70%;">in-the-wild real data; 20 different clothing styles.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S2.T1.2.6.6.6">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.6.6.6.1">
<span class="ltx_p" id="S2.T1.2.6.6.6.1.1"><a class="ltx_ref ltx_href" href="https://people.mpi-inf.mpg.de/~jianwang/projects/egopw/" style="font-size:70%;" title="">Link</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.7.7">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.2.7.7.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.7.7.1.1">
<span class="ltx_p" id="S2.T1.2.7.7.1.1.1" style="width:48.4pt;"><span class="ltx_text" id="S2.T1.2.7.7.1.1.1.1" style="font-size:70%;">UnrealEgo </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.2.7.7.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">3</span></a><span class="ltx_text" id="S2.T1.2.7.7.1.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.2.7.7.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.7.7.2.1">
<span class="ltx_p" id="S2.T1.2.7.7.2.1.1" style="width:17.1pt;"><span class="ltx_text" id="S2.T1.2.7.7.2.1.1.1" style="font-size:70%;">2022</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.2.7.7.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.7.7.3.1">
<span class="ltx_p" id="S2.T1.2.7.7.3.1.1" style="width:64.0pt;"><span class="ltx_text" id="S2.T1.2.7.7.3.1.1.1" style="font-size:70%;">900,000</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.2.7.7.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.7.7.4.1">
<span class="ltx_p" id="S2.T1.2.7.7.4.1.1" style="width:76.8pt;"><span class="ltx_text" id="S2.T1.2.7.7.4.1.1.1" style="font-size:70%;">17 subjects; 30 actions</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.2.7.7.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.7.7.5.1">
<span class="ltx_p" id="S2.T1.2.7.7.5.1.1" style="width:167.9pt;"><span class="ltx_text" id="S2.T1.2.7.7.5.1.1.1" style="font-size:70%;">450k in-the-wild stereo views; Motions, 3D environments, spawning human characters.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S2.T1.2.7.7.6">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.7.7.6.1">
<span class="ltx_p" id="S2.T1.2.7.7.6.1.1"><a class="ltx_ref ltx_href" href="https://4dqv.mpi-inf.mpg.de/UnrealEgo/" style="font-size:70%;" title="">Link</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.8.8">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.2.8.8.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.8.8.1.1">
<span class="ltx_p" id="S2.T1.2.8.8.1.1.1" style="width:48.4pt;"><span class="ltx_text" id="S2.T1.2.8.8.1.1.1.1" style="font-size:70%;">EgoGTA </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.2.8.8.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib67" title=""><span class="ltx_text" style="font-size:90%;">67</span></a><span class="ltx_text" id="S2.T1.2.8.8.1.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.2.8.8.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.8.8.2.1">
<span class="ltx_p" id="S2.T1.2.8.8.2.1.1" style="width:17.1pt;"><span class="ltx_text" id="S2.T1.2.8.8.2.1.1.1" style="font-size:70%;">2023</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.2.8.8.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.8.8.3.1">
<span class="ltx_p" id="S2.T1.2.8.8.3.1.1" style="width:64.0pt;"><span class="ltx_text" id="S2.T1.2.8.8.3.1.1.1" style="font-size:70%;">320,000</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.2.8.8.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.8.8.4.1">
<span class="ltx_p" id="S2.T1.2.8.8.4.1.1" style="width:76.8pt;"><span class="ltx_text" id="S2.T1.2.8.8.4.1.1.1" style="font-size:70%;">101 different actions</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.2.8.8.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.8.8.5.1">
<span class="ltx_p" id="S2.T1.2.8.8.5.1.1" style="width:167.9pt;"><span class="ltx_text" id="S2.T1.2.8.8.5.1.1.1" style="font-size:70%;">synthetic; based on GTA-IM containing different daily motions and scene geometry.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S2.T1.2.8.8.6">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.8.8.6.1">
<span class="ltx_p" id="S2.T1.2.8.8.6.1.1"><a class="ltx_ref ltx_href" href="https://people.mpi-inf.mpg.de/~jianwang/projects/sceneego/" style="font-size:70%;" title="">Link</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.9.9">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.2.9.9.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.9.9.1.1">
<span class="ltx_p" id="S2.T1.2.9.9.1.1.1" style="width:48.4pt;"><span class="ltx_text" id="S2.T1.2.9.9.1.1.1.1" style="font-size:70%;">ECHP </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.2.9.9.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">39</span></a><span class="ltx_text" id="S2.T1.2.9.9.1.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.2.9.9.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.9.9.2.1">
<span class="ltx_p" id="S2.T1.2.9.9.2.1.1" style="width:17.1pt;"><span class="ltx_text" id="S2.T1.2.9.9.2.1.1.1" style="font-size:70%;">2023</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.2.9.9.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.9.9.3.1">
<span class="ltx_p" id="S2.T1.2.9.9.3.1.1" style="width:64.0pt;"><span class="ltx_text" id="S2.T1.2.9.9.3.1.1.1" style="font-size:70%;">30 video sequences; 75,000 frames</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.2.9.9.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.9.9.4.1">
<span class="ltx_p" id="S2.T1.2.9.9.4.1.1" style="width:76.8pt;"><span class="ltx_text" id="S2.T1.2.9.9.4.1.1.1" style="font-size:70%;">9 subjects; 10 daily actions</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.2.9.9.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.9.9.5.1">
<span class="ltx_p" id="S2.T1.2.9.9.5.1.1" style="width:167.9pt;"><span class="ltx_text" id="S2.T1.2.9.9.5.1.1.1" style="font-size:70%;">indoor and outdoor; real-world data.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S2.T1.2.9.9.6">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.9.9.6.1">
<span class="ltx_p" id="S2.T1.2.9.9.6.1.1"><a class="ltx_ref ltx_href" href="https://github.com/Lrnyux/EgoCentric-Human-Pose-ECHP-Dataset" style="font-size:70%;" title="">Link</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.10.10">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.2.10.10.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.10.10.1.1">
<span class="ltx_p" id="S2.T1.2.10.10.1.1.1" style="width:48.4pt;"><span class="ltx_text" id="S2.T1.2.10.10.1.1.1.1" style="font-size:70%;">Ego-Exo4D </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.2.10.10.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">18</span></a><span class="ltx_text" id="S2.T1.2.10.10.1.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.2.10.10.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.10.10.2.1">
<span class="ltx_p" id="S2.T1.2.10.10.2.1.1" style="width:17.1pt;"><span class="ltx_text" id="S2.T1.2.10.10.2.1.1.1" style="font-size:70%;">2023</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.2.10.10.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.10.10.3.1">
<span class="ltx_p" id="S2.T1.2.10.10.3.1.1" style="width:64.0pt;"><span class="ltx_text" id="S2.T1.2.10.10.3.1.1.1" style="font-size:70%;">5625 video sequences; 1422 hours</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.2.10.10.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.10.10.4.1">
<span class="ltx_p" id="S2.T1.2.10.10.4.1.1" style="width:76.8pt;"><span class="ltx_text" id="S2.T1.2.10.10.4.1.1.1" style="font-size:70%;">839 subjects; 43 actions</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.2.10.10.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.10.10.5.1">
<span class="ltx_p" id="S2.T1.2.10.10.5.1.1" style="width:167.9pt;"><span class="ltx_text" id="S2.T1.2.10.10.5.1.1.1" style="font-size:70%;">131 different scenes in 13 different cities; comprises skilled human activities (e.g., sports, music, dance, bike repair).</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.2.10.10.6">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.10.10.6.1">
<span class="ltx_p" id="S2.T1.2.10.10.6.1.1"><a class="ltx_ref ltx_href" href="https://docs.ego-exo4d-data.org/" style="font-size:70%;" title="">Link</a></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T1.5.1.1" style="font-size:129%;">Table 1</span>: </span><span class="ltx_text" id="S2.T1.6.2" style="font-size:129%;">Popular datasets for egocentric 3D human pose estimation.</span></figcaption>
</figure>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">The <span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.p3.1.1">Mo<sup class="ltx_sup" id="S2.p3.1.1.1">2</sup>Cap<sup class="ltx_sup" id="S2.p3.1.1.2">2</sup></span> dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib68" title=""><span class="ltx_text" style="font-size:90%;">68</span></a>]</cite> tackles the challenge of obtaining annotated 3D pose data and introduces a marker-less multi-view motion capture. To address the time-consuming nature of obtaining diverse egocentric training examples, the dataset includes a synthetic training corpus generated from egocentric fisheye views. Built upon the SURREAL dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib63" title=""><span class="ltx_text" style="font-size:90%;">63</span></a>]</cite>, it offers 530,000 realistic training images with ground truth annotations of 2D and 3D joint positions.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.p4.1.1">xr-EgoPose</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">60</span></a>]</cite> provides an extensive collection of 383,000 frames featuring individuals showcasing a rich diversity of skin tones, body shapes, clothing styles, set with various backgrounds and lighting scenarios. Scenes are randomly generated from mocap data, featuring realistic body types like skinny short to full tall versions and skin tones from white to black. Prioritizing photorealism, the synthetic dataset is created through Maya animation with mocap data and V-Ray’s physically based rendering setup.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.p5.1.1">EgoBody</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib74" title=""><span class="ltx_text" style="font-size:90%;">74</span></a>]</cite> captures 2-person interactions using a Microsoft HoloLens2 headset. It provides synchronized multi-modal data, including RGB, depth, head, hand, and eye gaze tracking. With 125 sequences from 36 subjects in 15 scenes, it offers accurate 3D human shape, pose, and motion ground-truth. The dataset aims to explore the relationships between human attention, interactions, and motions, overcoming limitations of prior datasets, and advancing sociological and human-computer interaction research.</p>
</div>
<div class="ltx_para" id="S2.p6">
<p class="ltx_p" id="S2.p6.1">The <span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.p6.1.1">EgoPW</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib66" title=""><span class="ltx_text" style="font-size:90%;">66</span></a>]</cite> dataset is the first in-the-wild human performance dataset captured by synchronized egocentric and external cameras. It features 10 actors, 20 clothing styles, and 20 actions from 318,000 frames organized into 97 sequences, along with the 3D poses as pseudo labels.</p>
</div>
<div class="ltx_para" id="S2.p7">
<p class="ltx_p" id="S2.p7.1">The <span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.p7.1.1">UnrealEgo</span> dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite> introduces robust egocentric 3D human motion capture with 17 diverse 3D models and over 45,000 motions in 14 environments. It has stereo fisheye images and depth maps capturing complex activities like breakdance and backflips. With metadata including 3D joint positions and camera details, it comprises 450,000 in-the-wild stereo views, showcasing wider joint position distributions compared to xR-EgoPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">60</span></a>]</cite>. They followed up with the <span class="ltx_text ltx_font_bold" id="S2.p7.1.2">UnrealEgo2</span> and <span class="ltx_text ltx_font_bold" id="S2.p7.1.3">UnrealEgo-RW</span> datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib4" title=""><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite>, which provide more views with diverse human motions.</p>
</div>
<div class="ltx_para" id="S2.p8">
<p class="ltx_p" id="S2.p8.1">The <span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.p8.1.1">EgoGTA</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib67" title=""><span class="ltx_text" style="font-size:90%;">67</span></a>]</cite> dataset comprises of 320,000 frames across 101 sequences with distinct human body textures, by leveraging the diverse daily motions and ground truth scene geometry of GTA-IM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib9" title=""><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite>. The methodology involves fitting the SMPL-X <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib50" title=""><span class="ltx_text" style="font-size:90%;">50</span></a>]</cite> model to 3D joint trajectories from GTA-IM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib9" title=""><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite>, followed by attaching a virtual fisheye camera to the forehead for generating synthetic images, semantic labels, and depth maps with and without the human body.</p>
</div>
<div class="ltx_para" id="S2.p9">
<p class="ltx_p" id="S2.p9.1">The <span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.p9.1.1">ECHP</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">39</span></a>]</cite> dataset consists of 65,000 training images, 10,000 validation images, and a test set with egocentric images and 3D ground truth from VICON Mocap. Egocentric poses are extracted using OpenPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite> and human segmentation. Calibration and Aruco markers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib17" title=""><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite> aid in obtaining egocentric camera pose. The dataset has 30 sequences with 9 subjects, 20 textures, and 10 actions in various indoor/outdoor scenes. The test set provides generalization with 4 unseen subjects and 17,000 ground truth frames.</p>
</div>
<div class="ltx_para" id="S2.p10">
<p class="ltx_p" id="S2.p10.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.p10.1.1">Ego-Exo4D</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite> is a groundbreaking multimodal dataset and benchmark suite, offering the largest public collection of time-synchronized first and third-person videos captured by 839 individuals across 131 scenes in 13 cities. It is comprised of 1,422 hours of video, featuring both egocentric and multiple synchronized exocentric views. The <span class="ltx_text ltx_font_bold" id="S2.p10.1.2">EgoPose</span> benchmark focuses on recovering 3D body and hand movements from egocentric videos. The task is to estimate 17 3D body joint positions and 21 3D joint positions per hand, following the MS COCO convention.</p>
</div>
<figure class="ltx_table" id="S2.T2">
<div class="ltx_inline-block ltx_transformed_outer" id="S2.T2.2" style="width:496.9pt;height:1147.6pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T2.2.1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.2.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.1.1.1.1">
<span class="ltx_p" id="S2.T2.2.1.1.1.1.1.1" style="width:51.2pt;"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.1.1.1.1.1.1" style="font-size:70%;">Skeletal based Methods</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.1.1.2.1">
<span class="ltx_p" id="S2.T2.2.1.1.1.2.1.1" style="width:14.2pt;"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.1.1.2.1.1.1" style="font-size:70%;">Year</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.1.1.3.1">
<span class="ltx_p" id="S2.T2.2.1.1.1.3.1.1" style="width:179.3pt;"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.1.1.3.1.1.1" style="font-size:70%;">Highlighted Characteristics</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.1.1.4.1">
<span class="ltx_p" id="S2.T2.2.1.1.1.4.1.1" style="width:41.3pt;"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.1.1.4.1.1.1" style="font-size:70%;">Dataset</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.1.1.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.1.1.5.1">
<span class="ltx_p" id="S2.T2.2.1.1.1.5.1.1" style="width:93.9pt;"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.1.1.5.1.1.1" style="font-size:70%;">Limitations</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S2.T2.2.1.1.1.6">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.1.1.6.1">
<span class="ltx_p" id="S2.T2.2.1.1.1.6.1.1"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.1.1.6.1.1.1" style="font-size:70%;">Code/Project Website Link</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.1.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.2.1.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.2.2.1.1">
<span class="ltx_p" id="S2.T2.2.1.2.2.1.1.1" style="width:51.2pt;"><span class="ltx_text" id="S2.T2.2.1.2.2.1.1.1.1" style="font-size:70%;">Egocap </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.2.1.2.2.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">51</span></a><span class="ltx_text" id="S2.T2.2.1.2.2.1.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.2.2.2.1">
<span class="ltx_p" id="S2.T2.2.1.2.2.2.1.1" style="width:14.2pt;"><span class="ltx_text" id="S2.T2.2.1.2.2.2.1.1.1" style="font-size:70%;">2016</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.2.2.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.2.2.3.1">
<span class="ltx_p" id="S2.T2.2.1.2.2.3.1.1" style="width:179.3pt;"><span class="ltx_text" id="S2.T2.2.1.2.2.3.1.1.1" style="font-size:70%;">First marker-less motion capture system; utilized pose estimation framework for fisheye views with a ConvNet based body-part detector.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.2.2.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.2.2.4.1">
<span class="ltx_p" id="S2.T2.2.1.2.2.4.1.1" style="width:41.3pt;"><span class="ltx_text" id="S2.T2.2.1.2.2.4.1.1.1" style="font-size:70%;">EgoCap</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.2.2.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.2.2.5.1">
<span class="ltx_p" id="S2.T2.2.1.2.2.5.1.1" style="width:93.9pt;"><span class="ltx_text" id="S2.T2.2.1.2.2.5.1.1.1" style="font-size:70%;">No real-time prototype.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S2.T2.2.1.2.2.6">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.2.2.6.1">
<span class="ltx_p" id="S2.T2.2.1.2.2.6.1.1"><a class="ltx_ref ltx_href" href="https://vcai.mpi-inf.mpg.de/projects/EgoCap/" style="font-size:70%;" title="">Project</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.1.3.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.2.1.3.3.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.3.3.1.1">
<span class="ltx_p" id="S2.T2.2.1.3.3.1.1.1" style="width:51.2pt;"><span class="ltx_text" id="S2.T2.2.1.3.3.1.1.1.1" style="font-size:70%;">Jiang et al. </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.2.1.3.3.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">28</span></a><span class="ltx_text" id="S2.T2.2.1.3.3.1.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.3.3.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.3.3.2.1">
<span class="ltx_p" id="S2.T2.2.1.3.3.2.1.1" style="width:14.2pt;"><span class="ltx_text" id="S2.T2.2.1.3.3.2.1.1.1" style="font-size:70%;">2017</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.3.3.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.3.3.3.1">
<span class="ltx_p" id="S2.T2.2.1.3.3.3.1.1" style="width:179.3pt;"><span class="ltx_text" id="S2.T2.2.1.3.3.3.1.1.1" style="font-size:70%;">Leveraged dynamic motion signatures and static scene structures to infer the invisible pose efficiently.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.3.3.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.3.3.4.1">
<span class="ltx_p" id="S2.T2.2.1.3.3.4.1.1" style="width:41.3pt;"><span class="ltx_text" id="S2.T2.2.1.3.3.4.1.1.1" style="font-size:70%;">custom Kinect V2 dataset</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.3.3.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.3.3.5.1">
<span class="ltx_p" id="S2.T2.2.1.3.3.5.1.1" style="width:93.9pt;"><span class="ltx_text" id="S2.T2.2.1.3.3.5.1.1.1" style="font-size:70%;">Ambiguity in egocentric inputs due to unpredictable arm poses.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S2.T2.2.1.3.3.6">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.3.3.6.1">
<span class="ltx_p" id="S2.T2.2.1.3.3.6.1.1"><span class="ltx_text" id="S2.T2.2.1.3.3.6.1.1.1" style="font-size:70%;">–</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.1.4.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.2.1.4.4.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.4.4.1.1">
<span class="ltx_p" id="S2.T2.2.1.4.4.1.1.1" style="width:51.2pt;"><span class="ltx_text" id="S2.T2.2.1.4.4.1.1.1.1" style="font-size:70%;">Mo</span><sup class="ltx_sup ltx_centering" id="S2.T2.2.1.4.4.1.1.1.2"><span class="ltx_text" id="S2.T2.2.1.4.4.1.1.1.2.1" style="font-size:70%;">2</span></sup><span class="ltx_text" id="S2.T2.2.1.4.4.1.1.1.3" style="font-size:70%;">Cap</span><sup class="ltx_sup ltx_centering" id="S2.T2.2.1.4.4.1.1.1.4"><span class="ltx_text" id="S2.T2.2.1.4.4.1.1.1.4.1" style="font-size:70%;">2</span></sup><span class="ltx_text" id="S2.T2.2.1.4.4.1.1.1.5" style="font-size:70%;"> </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.2.1.4.4.1.1.1.6.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib68" title=""><span class="ltx_text" style="font-size:90%;">68</span></a><span class="ltx_text" id="S2.T2.2.1.4.4.1.1.1.7.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.4.4.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.4.4.2.1">
<span class="ltx_p" id="S2.T2.2.1.4.4.2.1.1" style="width:14.2pt;"><span class="ltx_text" id="S2.T2.2.1.4.4.2.1.1.1" style="font-size:70%;">2019</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.4.4.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.4.4.3.1">
<span class="ltx_p" id="S2.T2.2.1.4.4.3.1.1" style="width:179.3pt;"><span class="ltx_text" id="S2.T2.2.1.4.4.3.1.1.1" style="font-size:70%;">Real-time; disentangled 3D pose estimation, addressed 2D joint detection, camera-to-joint distances, and joint position recovery for accurate results and a precise 2D overlay.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.4.4.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.4.4.4.1">
<span class="ltx_p" id="S2.T2.2.1.4.4.4.1.1" style="width:41.3pt;"><span class="ltx_text" id="S2.T2.2.1.4.4.4.1.1.1" style="font-size:70%;">Mo</span><sup class="ltx_sup ltx_centering" id="S2.T2.2.1.4.4.4.1.1.2"><span class="ltx_text" id="S2.T2.2.1.4.4.4.1.1.2.1" style="font-size:70%;">2</span></sup><span class="ltx_text" id="S2.T2.2.1.4.4.4.1.1.3" style="font-size:70%;">Cap</span><sup class="ltx_sup ltx_centering" id="S2.T2.2.1.4.4.4.1.1.4"><span class="ltx_text" id="S2.T2.2.1.4.4.4.1.1.4.1" style="font-size:70%;">2</span></sup></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.4.4.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.4.4.5.1">
<span class="ltx_p" id="S2.T2.2.1.4.4.5.1.1" style="width:93.9pt;"><span class="ltx_text" id="S2.T2.2.1.4.4.5.1.1.1" style="font-size:70%;">Scenes with severe occlusions.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S2.T2.2.1.4.4.6">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.4.4.6.1">
<span class="ltx_p" id="S2.T2.2.1.4.4.6.1.1"><a class="ltx_ref ltx_href" href="https://vcai.mpi-inf.mpg.de/projects/wxu/Mo2Cap2/" style="font-size:70%;" title="">Project</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.1.5.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.2.1.5.5.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.5.5.1.1">
<span class="ltx_p" id="S2.T2.2.1.5.5.1.1.1" style="width:51.2pt;"><span class="ltx_text" id="S2.T2.2.1.5.5.1.1.1.1" style="font-size:70%;">xr-EgoPose </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.2.1.5.5.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">60</span></a><span class="ltx_text" id="S2.T2.2.1.5.5.1.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.5.5.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.5.5.2.1">
<span class="ltx_p" id="S2.T2.2.1.5.5.2.1.1" style="width:14.2pt;"><span class="ltx_text" id="S2.T2.2.1.5.5.2.1.1.1" style="font-size:70%;">2019</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.5.5.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.5.5.3.1">
<span class="ltx_p" id="S2.T2.2.1.5.5.3.1.1" style="width:179.3pt;"><span class="ltx_text" id="S2.T2.2.1.5.5.3.1.1.1" style="font-size:70%;">Encoder-decoder model for VR headset images, addressing resolution differences in upper and lower body poses, with a dual-branch decoder preserving uncertainty information.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.5.5.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.5.5.4.1">
<span class="ltx_p" id="S2.T2.2.1.5.5.4.1.1" style="width:41.3pt;"><span class="ltx_text" id="S2.T2.2.1.5.5.4.1.1.1" style="font-size:70%;">xR-EgoPose</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.5.5.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.5.5.5.1">
<span class="ltx_p" id="S2.T2.2.1.5.5.5.1.1" style="width:93.9pt;"><span class="ltx_text" id="S2.T2.2.1.5.5.5.1.1.1" style="font-size:70%;">Scenes with extreme occlusions and out-of-field view.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S2.T2.2.1.5.5.6">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.5.5.6.1">
<span class="ltx_p" id="S2.T2.2.1.5.5.6.1.1"><a class="ltx_ref ltx_href" href="https://github.com/facebookresearch/xR-EgoPose" style="font-size:70%;" title="">Code</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.1.6.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.2.1.6.6.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.6.6.1.1">
<span class="ltx_p" id="S2.T2.2.1.6.6.1.1.1" style="width:51.2pt;"><span class="ltx_text" id="S2.T2.2.1.6.6.1.1.1.1" style="font-size:70%;">You2Me </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.2.1.6.6.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib46" title=""><span class="ltx_text" style="font-size:90%;">46</span></a><span class="ltx_text" id="S2.T2.2.1.6.6.1.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.6.6.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.6.6.2.1">
<span class="ltx_p" id="S2.T2.2.1.6.6.2.1.1" style="width:14.2pt;"><span class="ltx_text" id="S2.T2.2.1.6.6.2.1.1.1" style="font-size:70%;">2020</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.6.6.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.6.6.3.1">
<span class="ltx_p" id="S2.T2.2.1.6.6.3.1.1" style="width:179.3pt;"><span class="ltx_text" id="S2.T2.2.1.6.6.3.1.1.1" style="font-size:70%;">Inferred robust poses by incorporating static scene features, explicit second-person body interactions and utilizing dyadic interactions and dynamic first-person motion features.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.6.6.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.6.6.4.1">
<span class="ltx_p" id="S2.T2.2.1.6.6.4.1.1" style="width:41.3pt;"><span class="ltx_text" id="S2.T2.2.1.6.6.4.1.1.1" style="font-size:70%;">You2Me</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.6.6.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.6.6.5.1">
<span class="ltx_p" id="S2.T2.2.1.6.6.5.1.1" style="width:93.9pt;"><span class="ltx_text" id="S2.T2.2.1.6.6.5.1.1.1" style="font-size:70%;">Scenerios where camera wearer is crouched and camera points towards the floor.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S2.T2.2.1.6.6.6">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.6.6.6.1">
<span class="ltx_p" id="S2.T2.2.1.6.6.6.1.1"><a class="ltx_ref ltx_href" href="https://github.com/facebookresearch/you2me" style="font-size:70%;" title="">Code</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.1.7.7">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.2.1.7.7.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.7.7.1.1">
<span class="ltx_p" id="S2.T2.2.1.7.7.1.1.1" style="width:51.2pt;"><span class="ltx_text" id="S2.T2.2.1.7.7.1.1.1.1" style="font-size:70%;">EgoGlass </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.2.1.7.7.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib77" title=""><span class="ltx_text" style="font-size:90%;">77</span></a><span class="ltx_text" id="S2.T2.2.1.7.7.1.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.7.7.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.7.7.2.1">
<span class="ltx_p" id="S2.T2.2.1.7.7.2.1.1" style="width:14.2pt;"><span class="ltx_text" id="S2.T2.2.1.7.7.2.1.1.1" style="font-size:70%;">2021</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.7.7.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.7.7.3.1">
<span class="ltx_p" id="S2.T2.2.1.7.7.3.1.1" style="width:179.3pt;"><span class="ltx_text" id="S2.T2.2.1.7.7.3.1.1.1" style="font-size:70%;">Utilized body part information for low-visible joints and tackling self-occlusion by preserving uncertainty information.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.7.7.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.7.7.4.1">
<span class="ltx_p" id="S2.T2.2.1.7.7.4.1.1" style="width:41.3pt;"><span class="ltx_text" id="S2.T2.2.1.7.7.4.1.1.1" style="font-size:70%;">EgoGlass</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.7.7.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.7.7.5.1">
<span class="ltx_p" id="S2.T2.2.1.7.7.5.1.1" style="width:93.9pt;"><span class="ltx_text" id="S2.T2.2.1.7.7.5.1.1.1" style="font-size:70%;">Lower body estimation produces larger errors.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S2.T2.2.1.7.7.6">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.7.7.6.1">
<span class="ltx_p" id="S2.T2.2.1.7.7.6.1.1"><span class="ltx_text" id="S2.T2.2.1.7.7.6.1.1.1" style="font-size:70%;">–</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.1.8.8">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.2.1.8.8.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.8.8.1.1">
<span class="ltx_p" id="S2.T2.2.1.8.8.1.1.1" style="width:51.2pt;"><span class="ltx_text" id="S2.T2.2.1.8.8.1.1.1.1" style="font-size:70%;">Zhang et al. </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.2.1.8.8.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib76" title=""><span class="ltx_text" style="font-size:90%;">76</span></a><span class="ltx_text" id="S2.T2.2.1.8.8.1.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.8.8.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.8.8.2.1">
<span class="ltx_p" id="S2.T2.2.1.8.8.2.1.1" style="width:14.2pt;"><span class="ltx_text" id="S2.T2.2.1.8.8.2.1.1.1" style="font-size:70%;">2021</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.8.8.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.8.8.3.1">
<span class="ltx_p" id="S2.T2.2.1.8.8.3.1.1" style="width:179.3pt;"><span class="ltx_text" id="S2.T2.2.1.8.8.3.1.1.1" style="font-size:70%;">Implemented auto-calibration module with self-correction for fisheye cameras to rectify image distortions, ensuring alignment between 3D predictions and distorted 2D poses.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.8.8.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.8.8.4.1">
<span class="ltx_p" id="S2.T2.2.1.8.8.4.1.1" style="width:41.3pt;"><span class="ltx_text" id="S2.T2.2.1.8.8.4.1.1.1" style="font-size:70%;">xR-EgoPose</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.8.8.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.8.8.5.1">
<span class="ltx_p" id="S2.T2.2.1.8.8.5.1.1" style="width:93.9pt;"><span class="ltx_text" id="S2.T2.2.1.8.8.5.1.1.1" style="font-size:70%;">Not evaluated in real-world setting.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S2.T2.2.1.8.8.6">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.8.8.6.1">
<span class="ltx_p" id="S2.T2.2.1.8.8.6.1.1"><span class="ltx_text" id="S2.T2.2.1.8.8.6.1.1.1" style="font-size:70%;">–</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.1.9.9">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.2.1.9.9.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.9.9.1.1">
<span class="ltx_p" id="S2.T2.2.1.9.9.1.1.1" style="width:51.2pt;"><span class="ltx_text" id="S2.T2.2.1.9.9.1.1.1.1" style="font-size:70%;">Wang et al. </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.2.1.9.9.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib64" title=""><span class="ltx_text" style="font-size:90%;">64</span></a><span class="ltx_text" id="S2.T2.2.1.9.9.1.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.9.9.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.9.9.2.1">
<span class="ltx_p" id="S2.T2.2.1.9.9.2.1.1" style="width:14.2pt;"><span class="ltx_text" id="S2.T2.2.1.9.9.2.1.1.1" style="font-size:70%;">2021</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.9.9.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.9.9.3.1">
<span class="ltx_p" id="S2.T2.2.1.9.9.3.1.1" style="width:179.3pt;"><span class="ltx_text" id="S2.T2.2.1.9.9.3.1.1.1" style="font-size:70%;">Spatio-temporal optimization framework that combines 2D and 3D keypoints, VAE-based motion priors and SLAM-based camera pose estimation for stable global body pose estimation.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.9.9.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.9.9.4.1">
<span class="ltx_p" id="S2.T2.2.1.9.9.4.1.1" style="width:41.3pt;"><span class="ltx_text" id="S2.T2.2.1.9.9.4.1.1.1" style="font-size:70%;">Mo</span><sup class="ltx_sup ltx_centering" id="S2.T2.2.1.9.9.4.1.1.2"><span class="ltx_text" id="S2.T2.2.1.9.9.4.1.1.2.1" style="font-size:70%;">2</span></sup><span class="ltx_text" id="S2.T2.2.1.9.9.4.1.1.3" style="font-size:70%;">Cap</span><sup class="ltx_sup ltx_centering" id="S2.T2.2.1.9.9.4.1.1.4"><span class="ltx_text" id="S2.T2.2.1.9.9.4.1.1.4.1" style="font-size:70%;">2</span></sup><span class="ltx_text" id="S2.T2.2.1.9.9.4.1.1.5" style="font-size:70%;">, AMASS</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.9.9.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.9.9.5.1">
<span class="ltx_p" id="S2.T2.2.1.9.9.5.1.1" style="width:93.9pt;"><span class="ltx_text" id="S2.T2.2.1.9.9.5.1.1.1" style="font-size:70%;">Not evaluated in real-world setting.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S2.T2.2.1.9.9.6">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.9.9.6.1">
<span class="ltx_p" id="S2.T2.2.1.9.9.6.1.1"><a class="ltx_ref ltx_href" href="https://people.mpi-inf.mpg.de/%C2%A0jianwang/projects/globalegomocap/" style="font-size:70%;" title="">Project</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.1.10.10">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.2.1.10.10.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.10.10.1.1">
<span class="ltx_p" id="S2.T2.2.1.10.10.1.1.1" style="width:51.2pt;"><span class="ltx_text" id="S2.T2.2.1.10.10.1.1.1.1" style="font-size:70%;">Wang et al. </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.2.1.10.10.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib66" title=""><span class="ltx_text" style="font-size:90%;">66</span></a><span class="ltx_text" id="S2.T2.2.1.10.10.1.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.10.10.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.10.10.2.1">
<span class="ltx_p" id="S2.T2.2.1.10.10.2.1.1" style="width:14.2pt;"><span class="ltx_text" id="S2.T2.2.1.10.10.2.1.1.1" style="font-size:70%;">2022</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.10.10.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.10.10.3.1">
<span class="ltx_p" id="S2.T2.2.1.10.10.3.1.1" style="width:179.3pt;"><span class="ltx_text" id="S2.T2.2.1.10.10.3.1.1.1" style="font-size:70%;">Implemented weak supervision with spatio-temporal optimization and synthetic data with domain adaptation for better egocentric pose estimation.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.10.10.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.10.10.4.1">
<span class="ltx_p" id="S2.T2.2.1.10.10.4.1.1" style="width:41.3pt;"><span class="ltx_text" id="S2.T2.2.1.10.10.4.1.1.1" style="font-size:70%;">EgoPW</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.10.10.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.10.10.5.1">
<span class="ltx_p" id="S2.T2.2.1.10.10.5.1.1" style="width:93.9pt;"><span class="ltx_text" id="S2.T2.2.1.10.10.5.1.1.1" style="font-size:70%;">Accuracy of pseudo labels constrained by in-the-wild capture system.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S2.T2.2.1.10.10.6">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.10.10.6.1">
<span class="ltx_p" id="S2.T2.2.1.10.10.6.1.1"><span class="ltx_text" id="S2.T2.2.1.10.10.6.1.1.1" style="font-size:70%;">–</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.1.11.11">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.2.1.11.11.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.11.11.1.1">
<span class="ltx_p" id="S2.T2.2.1.11.11.1.1.1" style="width:51.2pt;"><span class="ltx_text" id="S2.T2.2.1.11.11.1.1.1.1" style="font-size:70%;">Akada et al. </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.2.1.11.11.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">3</span></a><span class="ltx_text" id="S2.T2.2.1.11.11.1.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.11.11.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.11.11.2.1">
<span class="ltx_p" id="S2.T2.2.1.11.11.2.1.1" style="width:14.2pt;"><span class="ltx_text" id="S2.T2.2.1.11.11.2.1.1.1" style="font-size:70%;">2022</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.11.11.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.11.11.3.1">
<span class="ltx_p" id="S2.T2.2.1.11.11.3.1.1" style="width:179.3pt;"><span class="ltx_text" id="S2.T2.2.1.11.11.3.1.1.1" style="font-size:70%;">Enhances 3D pose estimation by integrating a stereo-based 2D joint location estimation module with weight-sharing encoders and a multi-branch autoencoder for uncertainty capture.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.11.11.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.11.11.4.1">
<span class="ltx_p" id="S2.T2.2.1.11.11.4.1.1" style="width:41.3pt;"><span class="ltx_text" id="S2.T2.2.1.11.11.4.1.1.1" style="font-size:70%;">UnrealEgo</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.11.11.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.11.11.5.1">
<span class="ltx_p" id="S2.T2.2.1.11.11.5.1.1" style="width:93.9pt;"><span class="ltx_text" id="S2.T2.2.1.11.11.5.1.1.1" style="font-size:70%;">Occlusions and complex motions scenerios.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S2.T2.2.1.11.11.6">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.11.11.6.1">
<span class="ltx_p" id="S2.T2.2.1.11.11.6.1.1"><a class="ltx_ref ltx_href" href="https://4dqv.mpi-inf.mpg.de/UnrealEgo/" style="font-size:70%;" title="">Project</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.1.12.12">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.2.1.12.12.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.12.12.1.1">
<span class="ltx_p" id="S2.T2.2.1.12.12.1.1.1" style="width:51.2pt;"><span class="ltx_text" id="S2.T2.2.1.12.12.1.1.1.1" style="font-size:70%;">Ego+X </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.2.1.12.12.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">38</span></a><span class="ltx_text" id="S2.T2.2.1.12.12.1.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.12.12.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.12.12.2.1">
<span class="ltx_p" id="S2.T2.2.1.12.12.2.1.1" style="width:14.2pt;"><span class="ltx_text" id="S2.T2.2.1.12.12.2.1.1.1" style="font-size:70%;">2022</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.12.12.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.12.12.3.1">
<span class="ltx_p" id="S2.T2.2.1.12.12.3.1.1" style="width:179.3pt;"><span class="ltx_text" id="S2.T2.2.1.12.12.3.1.1.1" style="font-size:70%;">Dual-camera framework for 3D global pose estimation and social interaction characterization, leveraging visual SLAM and a Pose Refine Module (PRM) for spatial and temporal accuracy and characterizes social interactions based on global 3D poses.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.12.12.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.12.12.4.1">
<span class="ltx_p" id="S2.T2.2.1.12.12.4.1.1" style="width:41.3pt;"><span class="ltx_text" id="S2.T2.2.1.12.12.4.1.1.1" style="font-size:70%;">ECHA</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.12.12.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.12.12.5.1">
<span class="ltx_p" id="S2.T2.2.1.12.12.5.1.1" style="width:93.9pt;"><span class="ltx_text" id="S2.T2.2.1.12.12.5.1.1.1" style="font-size:70%;">Camera localization robustness limited; temporal smoothing effectiveness not fully evaluated.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S2.T2.2.1.12.12.6">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.12.12.6.1">
<span class="ltx_p" id="S2.T2.2.1.12.12.6.1.1"><span class="ltx_text" id="S2.T2.2.1.12.12.6.1.1.1" style="font-size:70%;">–</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.1.13.13">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.2.1.13.13.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.13.13.1.1">
<span class="ltx_p" id="S2.T2.2.1.13.13.1.1.1" style="width:51.2pt;"><span class="ltx_text" id="S2.T2.2.1.13.13.1.1.1.1" style="font-size:70%;">Wang et al. </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.2.1.13.13.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib67" title=""><span class="ltx_text" style="font-size:90%;">67</span></a><span class="ltx_text" id="S2.T2.2.1.13.13.1.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.13.13.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.13.13.2.1">
<span class="ltx_p" id="S2.T2.2.1.13.13.2.1.1" style="width:14.2pt;"><span class="ltx_text" id="S2.T2.2.1.13.13.2.1.1.1" style="font-size:70%;">2023</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.13.13.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.13.13.3.1">
<span class="ltx_p" id="S2.T2.2.1.13.13.3.1.1" style="width:179.3pt;"><span class="ltx_text" id="S2.T2.2.1.13.13.3.1.1.1" style="font-size:70%;">First egocentric pose estimation framework, integrating depth estimation for occlusion handling in close interactions.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.13.13.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.13.13.4.1">
<span class="ltx_p" id="S2.T2.2.1.13.13.4.1.1" style="width:41.3pt;"><span class="ltx_text" id="S2.T2.2.1.13.13.4.1.1.1" style="font-size:70%;">EgoGTA, EgoPW-Scene</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.13.13.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.13.13.5.1">
<span class="ltx_p" id="S2.T2.2.1.13.13.5.1.1" style="width:93.9pt;"><span class="ltx_text" id="S2.T2.2.1.13.13.5.1.1.1" style="font-size:70%;">Accuracy is constrained by depth estimation where scene is occluded by body.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S2.T2.2.1.13.13.6">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.13.13.6.1">
<span class="ltx_p" id="S2.T2.2.1.13.13.6.1.1"><a class="ltx_ref ltx_href" href="https://people.mpi-inf.mpg.de/%C2%A0jianwang/projects/sceneego/" style="font-size:70%;" title="">Project</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.1.14.14">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.2.1.14.14.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.14.14.1.1">
<span class="ltx_p" id="S2.T2.2.1.14.14.1.1.1" style="width:51.2pt;"><span class="ltx_text" id="S2.T2.2.1.14.14.1.1.1.1" style="font-size:70%;">EgoFish3D </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.2.1.14.14.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">39</span></a><span class="ltx_text" id="S2.T2.2.1.14.14.1.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.14.14.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.14.14.2.1">
<span class="ltx_p" id="S2.T2.2.1.14.14.2.1.1" style="width:14.2pt;"><span class="ltx_text" id="S2.T2.2.1.14.14.2.1.1.1" style="font-size:70%;">2023</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.14.14.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.14.14.3.1">
<span class="ltx_p" id="S2.T2.2.1.14.14.3.1.1" style="width:179.3pt;"><span class="ltx_text" id="S2.T2.2.1.14.14.3.1.1.1" style="font-size:70%;">A self-supervised framework for egocentric 3D pose estimation, utilizing real-world data with three key modules: third person view, egocentric, and interactive modules, achieving accurate results without the need for ground truth annotations.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.14.14.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.14.14.4.1">
<span class="ltx_p" id="S2.T2.2.1.14.14.4.1.1" style="width:41.3pt;"><span class="ltx_text" id="S2.T2.2.1.14.14.4.1.1.1" style="font-size:70%;">ECHP</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.14.14.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.14.14.5.1">
<span class="ltx_p" id="S2.T2.2.1.14.14.5.1.1" style="width:93.9pt;"><span class="ltx_text" id="S2.T2.2.1.14.14.5.1.1.1" style="font-size:70%;">Overlooked the significance of the perspective factor, which can convey valuable information about the 3D effect intensity.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S2.T2.2.1.14.14.6">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.14.14.6.1">
<span class="ltx_p" id="S2.T2.2.1.14.14.6.1.1"><span class="ltx_text" id="S2.T2.2.1.14.14.6.1.1.1" style="font-size:70%;">–</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.1.15.15">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.2.1.15.15.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.15.15.1.1">
<span class="ltx_p" id="S2.T2.2.1.15.15.1.1.1" style="width:51.2pt;"><span class="ltx_text" id="S2.T2.2.1.15.15.1.1.1.1" style="font-size:70%;">Ego3DPose </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.2.1.15.15.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib33" title=""><span class="ltx_text" style="font-size:90%;">33</span></a><span class="ltx_text" id="S2.T2.2.1.15.15.1.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.15.15.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.15.15.2.1">
<span class="ltx_p" id="S2.T2.2.1.15.15.2.1.1" style="width:14.2pt;"><span class="ltx_text" id="S2.T2.2.1.15.15.2.1.1.1" style="font-size:70%;">2023</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.15.15.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.15.15.3.1">
<span class="ltx_p" id="S2.T2.2.1.15.15.3.1.1" style="width:179.3pt;"><span class="ltx_text" id="S2.T2.2.1.15.15.3.1.1.1" style="font-size:70%;">A stereo matcher network and perspective embedding heatmap representation, independent learning of stereo correspondences and leveraging 3D perspective information.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.15.15.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.15.15.4.1">
<span class="ltx_p" id="S2.T2.2.1.15.15.4.1.1" style="width:41.3pt;"><span class="ltx_text" id="S2.T2.2.1.15.15.4.1.1.1" style="font-size:70%;">UnrealEgo</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.15.15.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.15.15.5.1">
<span class="ltx_p" id="S2.T2.2.1.15.15.5.1.1" style="width:93.9pt;"><span class="ltx_text" id="S2.T2.2.1.15.15.5.1.1.1" style="font-size:70%;">Scenes with occlusions, distortions and real-world setting.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S2.T2.2.1.15.15.6">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.15.15.6.1">
<span class="ltx_p" id="S2.T2.2.1.15.15.6.1.1"><a class="ltx_ref ltx_href" href="https://github.com/tho-kn/Ego3DPose" style="font-size:70%;" title="">Code</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.1.16.16">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.2.1.16.16.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.16.16.1.1">
<span class="ltx_p" id="S2.T2.2.1.16.16.1.1.1" style="width:51.2pt;"><span class="ltx_text" id="S2.T2.2.1.16.16.1.1.1.1" style="font-size:70%;">Ego-STAN </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.2.1.16.16.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib48" title=""><span class="ltx_text" style="font-size:90%;">48</span></a><span class="ltx_text" id="S2.T2.2.1.16.16.1.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.16.16.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.16.16.2.1">
<span class="ltx_p" id="S2.T2.2.1.16.16.2.1.1" style="width:14.2pt;"><span class="ltx_text" id="S2.T2.2.1.16.16.2.1.1.1" style="font-size:70%;">2023</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.16.16.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.16.16.3.1">
<span class="ltx_p" id="S2.T2.2.1.16.16.3.1.1" style="width:179.3pt;"><span class="ltx_text" id="S2.T2.2.1.16.16.3.1.1.1" style="font-size:70%;">Tackles fisheye distortion and self-occlusions in egocentric human pose estimation through a domain-guided spatio-temporal transformer, using 2D image representations, feature map tokens, and 3D pose estimation for accurate joint localization and uncertainty management.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.16.16.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.16.16.4.1">
<span class="ltx_p" id="S2.T2.2.1.16.16.4.1.1" style="width:41.3pt;"><span class="ltx_text" id="S2.T2.2.1.16.16.4.1.1.1" style="font-size:70%;">xr-EgoPose</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.16.16.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.16.16.5.1">
<span class="ltx_p" id="S2.T2.2.1.16.16.5.1.1" style="width:93.9pt;"><span class="ltx_text" id="S2.T2.2.1.16.16.5.1.1.1" style="font-size:70%;">Scenes in real-world setting.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S2.T2.2.1.16.16.6">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.16.16.6.1">
<span class="ltx_p" id="S2.T2.2.1.16.16.6.1.1"><span class="ltx_text" id="S2.T2.2.1.16.16.6.1.1.1" style="font-size:70%;">–</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.1.17.17">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.2.1.17.17.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.17.17.1.1">
<span class="ltx_p" id="S2.T2.2.1.17.17.1.1.1" style="width:51.2pt;"><span class="ltx_text" id="S2.T2.2.1.17.17.1.1.1.1" style="font-size:70%;">Dhamanaskar et al. </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.2.1.17.17.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">13</span></a><span class="ltx_text" id="S2.T2.2.1.17.17.1.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.17.17.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.17.17.2.1">
<span class="ltx_p" id="S2.T2.2.1.17.17.2.1.1" style="width:14.2pt;"><span class="ltx_text" id="S2.T2.2.1.17.17.2.1.1.1" style="font-size:70%;">2023</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.17.17.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.17.17.3.1">
<span class="ltx_p" id="S2.T2.2.1.17.17.3.1.1" style="width:179.3pt;"><span class="ltx_text" id="S2.T2.2.1.17.17.3.1.1.1" style="font-size:70%;">Utilized third-person view information, creating a self-supervised neural network that establishes a shared space for consistent 3D body pose detection across diverse video settings, ensuring adaptability to real-world scenarios with unknown camera configurations.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.17.17.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.17.17.4.1">
<span class="ltx_p" id="S2.T2.2.1.17.17.4.1.1" style="width:41.3pt;"><span class="ltx_text" id="S2.T2.2.1.17.17.4.1.1.1" style="font-size:70%;">First2Third-Pose</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.2.1.17.17.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.17.17.5.1">
<span class="ltx_p" id="S2.T2.2.1.17.17.5.1.1" style="width:93.9pt;"><span class="ltx_text" id="S2.T2.2.1.17.17.5.1.1.1" style="font-size:70%;">Evaluation limited to two datasets; broader assessment needed for generalization.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S2.T2.2.1.17.17.6">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.17.17.6.1">
<span class="ltx_p" id="S2.T2.2.1.17.17.6.1.1"><a class="ltx_ref ltx_href" href="https://github.com/nudlesoup/First2Third-Pose" style="font-size:70%;" title="">Code</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.1.18.18">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.2.1.18.18.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.18.18.1.1">
<span class="ltx_p" id="S2.T2.2.1.18.18.1.1.1" style="width:51.2pt;"><span class="ltx_text" id="S2.T2.2.1.18.18.1.1.1.1" style="font-size:70%;">EgoFormer </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.2.1.18.18.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a><span class="ltx_text" id="S2.T2.2.1.18.18.1.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S2.T2.2.1.18.18.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.18.18.2.1">
<span class="ltx_p" id="S2.T2.2.1.18.18.2.1.1" style="width:14.2pt;"><span class="ltx_text" id="S2.T2.2.1.18.18.2.1.1.1" style="font-size:70%;">2023</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S2.T2.2.1.18.18.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.18.18.3.1">
<span class="ltx_p" id="S2.T2.2.1.18.18.3.1.1" style="width:179.3pt;"><span class="ltx_text" id="S2.T2.2.1.18.18.3.1.1.1" style="font-size:70%;">Leveraged video context and establishing long-term temporal relationships. It addresses ambiguity in first-person videos, surpassing dynamic features, and introduces a novel motion clue representation for enhanced accuracy.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S2.T2.2.1.18.18.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.18.18.4.1">
<span class="ltx_p" id="S2.T2.2.1.18.18.4.1.1" style="width:41.3pt;"><span class="ltx_text" id="S2.T2.2.1.18.18.4.1.1.1" style="font-size:70%;">CMU Mocap </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S2.T2.2.1.18.18.4.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">1</span></a><span class="ltx_text" id="S2.T2.2.1.18.18.4.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S2.T2.2.1.18.18.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.18.18.5.1">
<span class="ltx_p" id="S2.T2.2.1.18.18.5.1.1" style="width:93.9pt;"><span class="ltx_text" id="S2.T2.2.1.18.18.5.1.1.1" style="font-size:70%;">Lack of real-world testing and limited model comparisons.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S2.T2.2.1.18.18.6">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.1.18.18.6.1">
<span class="ltx_p" id="S2.T2.2.1.18.18.6.1.1"><span class="ltx_text" id="S2.T2.2.1.18.18.6.1.1.1" style="font-size:70%;">–</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T2.5.1.1" style="font-size:129%;">Table 2</span>: </span><span class="ltx_text" id="S2.T2.6.2" style="font-size:129%;">Popular skeletal based egocentric 3D pose estimation methods.</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>3D Egocentric Pose Estimation Methods</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">After performing an extensive literature search for egocentric pose estimation, in this section, we discuss around 35 popular techniques by classifying them into two categories, namely skeletal and body shape based approaches.
Skeletal-based 3D pose estimation methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib49" title=""><span class="ltx_text" style="font-size:90%;">49</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib56" title=""><span class="ltx_text" style="font-size:90%;">56</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib57" title=""><span class="ltx_text" style="font-size:90%;">57</span></a>]</cite> leverage the human skeleton representation to accurately track and infer 3D joint position and body movements.
Human body shape based human body pose estimation methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">8</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib73" title=""><span class="ltx_text" style="font-size:90%;">73</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite> utilize a parametric model, such as SMPL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite> and SMPL-X <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib50" title=""><span class="ltx_text" style="font-size:90%;">50</span></a>]</cite>, to accurately estimate 3D joint locations and body shapes. The retrieval of human body meshes is pivotal in supporting subsequent tasks like reconstructing clothed humans <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib70" title=""><span class="ltx_text" style="font-size:90%;">70</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib79" title=""><span class="ltx_text" style="font-size:90%;">79</span></a>]</cite>, rendering <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite>, and modeling avatars <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib80" title=""><span class="ltx_text" style="font-size:90%;">80</span></a>]</cite>.
The sub-sections below expand on the different methods in each category.
We have further sub-categorized the methods based on some significant features, as highlighted in bold.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Skeletal Based Methods</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">In this section, we have provided details on the skeletal based egocentric pose estimation methods. Table <a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#S2.T2" title="Table 2 ‣ 2 Datasets ‣ A Survey on 3D Egocentric Human Pose Estimation"><span class="ltx_text ltx_ref_tag">2</span></a> provides a brief overview of 17 such skeletal based methods.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">Rhodin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite> introduced a <span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">marker-less</span> egocentric motion capture system using fisheye cameras embedded in a helmet or VR headset. The method employs a generative pose estimation framework with a ConvNet-based body part detector, ideal for VR applications needing natural movement and interaction. However, it was not able to attain <span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.2">real-time</span> performance.
To solve which, Mo<sup class="ltx_sup" id="S3.SS1.p2.1.3">2</sup>Cap<sup class="ltx_sup" id="S3.SS1.p2.1.4">2</sup> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib68" title=""><span class="ltx_text" style="font-size:90%;">68</span></a>]</cite> uses a two-scale location invariant convolutional network to detect 2D joints, accommodating perspective and radial distortions. It uses a location-sensitive distance module for estimating absolute camera-to-joint distances, and then recovers actual joint positions by back-projecting 2D detections. However, it struggles in scenes with severe occlusions.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1"><span class="ltx_text ltx_font_italic" id="S3.SS1.p3.1.1">EgoGlass</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib77" title=""><span class="ltx_text" style="font-size:90%;">77</span></a>]</cite> solves the <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.2">occlusion</span> problem by leveraging body part information for improved pose detection. The 2D module incorporates branches for heatmap and body part prediction, while the 3D module employs a pseudo-limb mask approach to handle occlusion in real-world images. This module also functions as an autoencoder for joint heatmaps, enhancing 3D body pose estimation and capturing uncertainty in 2D predictions across multiple views.
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib67" title=""><span class="ltx_text" style="font-size:90%;">67</span></a>]</cite> introduces an egocentric depth estimation network for predicting scene depth maps behind the human body using a wide-view egocentric fisheye camera, addressing occlusion caused by the human body through a depth-inpainting network. Additionally, a scene-aware pose estimation network was presented for 3D pose regression.
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite> used a Vector Quantized-Variational AutoEncoder (VQ-VAE) to predict and optimize human pose, addressing the challenge of obscured lower body appearance.
<span class="ltx_text ltx_font_italic" id="S3.SS1.p3.1.3">xR-EgoPose</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">60</span></a>]</cite> and <span class="ltx_text ltx_font_italic" id="S3.SS1.p3.1.4">SelfPose</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib61" title=""><span class="ltx_text" style="font-size:90%;">61</span></a>]</cite> uses an encoder-decoder architecture designed to improve accuracy in capturing upper and lower body poses from monocular images obtained via VR headset cameras. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">60</span></a>]</cite> employs a dual-branch decoder to address resolution discrepancies between the upper and lower body. It handles uncertainties in 2D joint locations by initially generating 2D heatmaps and subsequently using an autoencoder for 3D pose regression.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">To solve the problem of <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.1">out-of-field-view</span>, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> developed a method aiming to infer the invisible pose of a person in egocentric videos using dynamic motion signatures and static scene structures. By combining short-term and longer-term pose dynamics, the method utilizes classifiers to estimate pose probabilities and performs joint inference for a longer sequence. They extended the idea <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib29" title=""><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite> by using both dynamic motion information from camera SLAM and occasionally visible body parts for robust ego pose estimation ensuring geometrical consistency.
<span class="ltx_text ltx_font_italic" id="S3.SS1.p4.1.2">EgoTAP</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite> addresses <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.3">out-of-view limbs</span> and self-occlusion issues in stereo egocentric 3D pose estimation by introducing a Grid ViT Heatmap Encoder and Propagation Network. The Grid ViT efficiently summarizes joint heatmaps, preserving spatial relationships. The Propagation Network utilizes skeletal information to predict 3D poses, improving accuracy for both visible and less visible joints.</p>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.1">To reduce the scarcity of <span class="ltx_text ltx_font_bold" id="S3.SS1.p5.1.1">real-world datasets</span> from egocentric view, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib66" title=""><span class="ltx_text" style="font-size:90%;">66</span></a>]</cite> proposed the use of weak supervision from an external viewpoint. The approach utilizes spatio-temporal optimization to generate accurate 3D poses for frames in the <span class="ltx_text ltx_font_italic" id="S3.SS1.p5.1.2">EgoPW</span> dataset, using them as labels for training an egocentric pose estimation network.
It also incorporates a synthetic dataset and employs domain adaptation to bridge the gap between synthetic and real data.
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite> proposed a solution for egocentric pose estimation in an <span class="ltx_text ltx_font_bold" id="S3.SS1.p5.1.3">unconstrained environment</span>. It uses a 2D joint location estimation module for stereo inputs by utilizing weight-sharing encoders and a decoder leveraging stereo information to boost performance. The 3D module comprises a multi-branch autoencoder, predicting 2D heatmaps to generate 3D pose and reconstructing heatmaps to capture uncertainty.</p>
</div>
<div class="ltx_para" id="S3.SS1.p6">
<p class="ltx_p" id="S3.SS1.p6.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p6.1.1">Perspective distortion</span> can cause issues like scale variation, depth ambiguity and limited field of view. To tackle this problem, <span class="ltx_text ltx_font_italic" id="S3.SS1.p6.1.2">Ego3DPose</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib33" title=""><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite> introduces a Stereo Matcher network that independently learns stereo correspondences and predicts explicit 3D orientation for each limb, avoiding dependence on full-body information. Additionally, a Perspective Embedding Heatmap representation is introduced, allowing the 2D module to extract and utilize 3D perspective information. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib48" title=""><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite> addressed the challenges of <span class="ltx_text ltx_font_bold" id="S3.SS1.p6.1.3">fisheye distortion</span> and <span class="ltx_text ltx_font_bold" id="S3.SS1.p6.1.4">self-occlusions</span> by leveraging a domain-guided spatio-temporal transformer model, <span class="ltx_text ltx_font_italic" id="S3.SS1.p6.1.5">Ego-STAN</span>. It utilizes 2D image representations and spatiotemporal attention to mitigate distortions and accurately estimate the location of heavily occluded joints.
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib76" title=""><span class="ltx_text" style="font-size:90%;">76</span></a>]</cite> employed an <span class="ltx_text ltx_font_bold" id="S3.SS1.p6.1.6">automatic calibration</span> module with self-correction to mitigate the impact of image distortions on 3D pose estimation. Unlike traditional post-processing steps, this module ensures consistency between 3D predictions and distorted 2D poses.</p>
</div>
<div class="ltx_para" id="S3.SS1.p7">
<p class="ltx_p" id="S3.SS1.p7.1">When the predicted poses are in the fisheye camera’s <span class="ltx_text ltx_font_bold" id="S3.SS1.p7.1.1">local coordinate system</span> instead of the global coordinate system, it can cause issues like <span class="ltx_text ltx_font_bold" id="S3.SS1.p7.1.2">temporal instability</span>. To solve this issue, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib64" title=""><span class="ltx_text" style="font-size:90%;">64</span></a>]</cite> proposed a method for precise and stable global body pose estimation in egocentric videos. It utilizes CNN-detected 2D and 3D keypoints, VAE-based motion priors, and SLAM-based camera pose estimation. This approach effectively tackles challenges like temporal jitters and tracking failures, significantly enhancing accuracy and stability in obtaining coherent body poses. <span class="ltx_text ltx_font_italic" id="S3.SS1.p7.1.3">Ego+X</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite> proposed a framework with two cameras for 3D <span class="ltx_text ltx_font_bold" id="S3.SS1.p7.1.4">global pose estimation</span> and <span class="ltx_text ltx_font_bold" id="S3.SS1.p7.1.5">social interaction characterization</span>. The <span class="ltx_text ltx_font_italic" id="S3.SS1.p7.1.6">Ego-Glo</span> module solves spatial and temporal errors using a dual-branch network and visual SLAM. Whereas, the <span class="ltx_text ltx_font_italic" id="S3.SS1.p7.1.7">Ego-Soc</span> module performs egocentric social interaction characterization, including object detection and human-human interaction, based on the global 3D human poses.</p>
</div>
<figure class="ltx_table" id="S3.T3">
<div class="ltx_inline-block ltx_transformed_outer" id="S3.T3.2" style="width:496.9pt;height:874.4pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T3.2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.2.1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.2.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.1.1.1.1">
<span class="ltx_p" id="S3.T3.2.1.1.1.1.1.1" style="width:52.6pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.1.1.1.1.1.1.1" style="font-size:70%;">Body Shape based Methods</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.1.1.2.1">
<span class="ltx_p" id="S3.T3.2.1.1.1.2.1.1" style="width:14.2pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.1.1.1.2.1.1.1" style="font-size:70%;">Year</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.1.1.3.1">
<span class="ltx_p" id="S3.T3.2.1.1.1.3.1.1" style="width:184.9pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.1.1.1.3.1.1.1" style="font-size:70%;">Highlighted Characteristics</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.1.1.4.1">
<span class="ltx_p" id="S3.T3.2.1.1.1.4.1.1" style="width:32.7pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.1.1.1.4.1.1.1" style="font-size:70%;">Dataset</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.1.1.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.1.1.5.1">
<span class="ltx_p" id="S3.T3.2.1.1.1.5.1.1" style="width:96.7pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.1.1.1.5.1.1.1" style="font-size:70%;">Limitations</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T3.2.1.1.1.6">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.1.1.6.1">
<span class="ltx_p" id="S3.T3.2.1.1.1.6.1.1"><span class="ltx_text ltx_font_bold" id="S3.T3.2.1.1.1.6.1.1.1" style="font-size:70%;">Code/Project Website Link</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.1.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.2.1.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.2.2.1.1">
<span class="ltx_p" id="S3.T3.2.1.2.2.1.1.1" style="width:52.6pt;"><span class="ltx_text" id="S3.T3.2.1.2.2.1.1.1.1" style="font-size:70%;">Yuan et al. </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S3.T3.2.1.2.2.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib71" title=""><span class="ltx_text" style="font-size:90%;">71</span></a><span class="ltx_text" id="S3.T3.2.1.2.2.1.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.2.2.2.1">
<span class="ltx_p" id="S3.T3.2.1.2.2.2.1.1" style="width:14.2pt;"><span class="ltx_text" id="S3.T3.2.1.2.2.2.1.1.1" style="font-size:70%;">2018</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.2.2.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.2.2.3.1">
<span class="ltx_p" id="S3.T3.2.1.2.2.3.1.1" style="width:184.9pt;"><span class="ltx_text" id="S3.T3.2.1.2.2.3.1.1.1" style="font-size:70%;">Integrates control-based modeling, physics simulation, and imitation learning for ego-pose estimation, enabling domain adaptation by considering underlying physics dynamics.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.2.2.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.2.2.4.1">
<span class="ltx_p" id="S3.T3.2.1.2.2.4.1.1" style="width:32.7pt;"><span class="ltx_text" id="S3.T3.2.1.2.2.4.1.1.1" style="font-size:70%;">CMU Mocap </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S3.T3.2.1.2.2.4.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">1</span></a><span class="ltx_text" id="S3.T3.2.1.2.2.4.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.2.2.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.2.2.5.1">
<span class="ltx_p" id="S3.T3.2.1.2.2.5.1.1" style="width:96.7pt;"><span class="ltx_text" id="S3.T3.2.1.2.2.5.1.1.1" style="font-size:70%;">Indirect 2D evaluation may not capture full 3D accuracy; limited behaviors may hinder complex motion generalization.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T3.2.1.2.2.6">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.2.2.6.1">
<span class="ltx_p" id="S3.T3.2.1.2.2.6.1.1"><span class="ltx_text" id="S3.T3.2.1.2.2.6.1.1.1" style="font-size:70%;">–</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.1.3.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.2.1.3.3.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.3.3.1.1">
<span class="ltx_p" id="S3.T3.2.1.3.3.1.1.1" style="width:52.6pt;"><span class="ltx_text" id="S3.T3.2.1.3.3.1.1.1.1" style="font-size:70%;">Dittadi et al. </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S3.T3.2.1.3.3.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a><span class="ltx_text" id="S3.T3.2.1.3.3.1.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.3.3.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.3.3.2.1">
<span class="ltx_p" id="S3.T3.2.1.3.3.2.1.1" style="width:14.2pt;"><span class="ltx_text" id="S3.T3.2.1.3.3.2.1.1.1" style="font-size:70%;">2021</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.3.3.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.3.3.3.1">
<span class="ltx_p" id="S3.T3.2.1.3.3.3.1.1" style="width:184.9pt;"><span class="ltx_text" id="S3.T3.2.1.3.3.3.1.1.1" style="font-size:70%;">Variational autoencoders for generating human body poses from limited head and hand pose data; addressing challenges through specialized inference models.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.3.3.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.3.3.4.1">
<span class="ltx_p" id="S3.T3.2.1.3.3.4.1.1" style="width:32.7pt;"><span class="ltx_text" id="S3.T3.2.1.3.3.4.1.1.1" style="font-size:70%;">AMASS </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S3.T3.2.1.3.3.4.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib44" title=""><span class="ltx_text" style="font-size:90%;">44</span></a><span class="ltx_text" id="S3.T3.2.1.3.3.4.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.3.3.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.3.3.5.1">
<span class="ltx_p" id="S3.T3.2.1.3.3.5.1.1" style="width:96.7pt;"><span class="ltx_text" id="S3.T3.2.1.3.3.5.1.1.1" style="font-size:70%;">Incomplete utilization of temporal history, constraints on body shape variation and reliance on assumed availability of hand signals.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T3.2.1.3.3.6">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.3.3.6.1">
<span class="ltx_p" id="S3.T3.2.1.3.3.6.1.1"><span class="ltx_text" id="S3.T3.2.1.3.3.6.1.1.1" style="font-size:70%;">–</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.1.4.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.2.1.4.4.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.4.4.1.1">
<span class="ltx_p" id="S3.T3.2.1.4.4.1.1.1" style="width:52.6pt;"><span class="ltx_text" id="S3.T3.2.1.4.4.1.1.1.1" style="font-size:70%;">CoolMoves </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S3.T3.2.1.4.4.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2</span></a><span class="ltx_text" id="S3.T3.2.1.4.4.1.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.4.4.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.4.4.2.1">
<span class="ltx_p" id="S3.T3.2.1.4.4.2.1.1" style="width:14.2pt;"><span class="ltx_text" id="S3.T3.2.1.4.4.2.1.1.1" style="font-size:70%;">2021</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.4.4.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.4.4.3.1">
<span class="ltx_p" id="S3.T3.2.1.4.4.3.1.1" style="width:184.9pt;"><span class="ltx_text" id="S3.T3.2.1.4.4.3.1.1.1" style="font-size:70%;">Achieves real-time, expressive full-body motion synthesis for avatars using limited input cues, dynamically fusing stylized examples from skilled performers, excelling in activities like dancing and fighting.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.4.4.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.4.4.4.1">
<span class="ltx_p" id="S3.T3.2.1.4.4.4.1.1" style="width:32.7pt;"><span class="ltx_text" id="S3.T3.2.1.4.4.4.1.1.1" style="font-size:70%;">CMU MoCap</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.4.4.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.4.4.5.1">
<span class="ltx_p" id="S3.T3.2.1.4.4.5.1.1" style="width:96.7pt;"><span class="ltx_text" id="S3.T3.2.1.4.4.5.1.1.1" style="font-size:70%;">Limited sensing of legs and feet, resulting in lower body reconstruction jitters and reduced accuracy in foot-driven motions.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T3.2.1.4.4.6">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.4.4.6.1">
<span class="ltx_p" id="S3.T3.2.1.4.4.6.1.1"><span class="ltx_text" id="S3.T3.2.1.4.4.6.1.1.1" style="font-size:70%;">–</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.1.5.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.2.1.5.5.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.5.5.1.1">
<span class="ltx_p" id="S3.T3.2.1.5.5.1.1.1" style="width:52.6pt;"><span class="ltx_text" id="S3.T3.2.1.5.5.1.1.1.1" style="font-size:70%;">EgoRenderer </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S3.T3.2.1.5.5.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">21</span></a><span class="ltx_text" id="S3.T3.2.1.5.5.1.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.5.5.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.5.5.2.1">
<span class="ltx_p" id="S3.T3.2.1.5.5.2.1.1" style="width:14.2pt;"><span class="ltx_text" id="S3.T3.2.1.5.5.2.1.1.1" style="font-size:70%;">2021</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.5.5.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.5.5.3.1">
<span class="ltx_p" id="S3.T3.2.1.5.5.3.1.1" style="width:184.9pt;"><span class="ltx_text" id="S3.T3.2.1.5.5.3.1.1.1" style="font-size:70%;">Renders full-body neural avatars from egocentric fisheye images - texture synthesis, pose construction, and neural image translation; addresses challenges of top-down view and distortions.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.5.5.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.5.5.4.1">
<span class="ltx_p" id="S3.T3.2.1.5.5.4.1.1" style="width:32.7pt;"><span class="ltx_text" id="S3.T3.2.1.5.5.4.1.1.1" style="font-size:70%;">EgoRenderer</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.5.5.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.5.5.5.1">
<span class="ltx_p" id="S3.T3.2.1.5.5.5.1.1" style="width:96.7pt;"><span class="ltx_text" id="S3.T3.2.1.5.5.5.1.1.1" style="font-size:70%;">Incomplete joint estimation; unnatural motions in SMPL model animations and temporal instability in frame predictions.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T3.2.1.5.5.6">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.5.5.6.1">
<span class="ltx_p" id="S3.T3.2.1.5.5.6.1.1"><a class="ltx_ref ltx_href" href="https://vcai.mpi-inf.mpg.de/projects/EgoRenderer/" style="font-size:70%;" title="">Project</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.1.6.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.2.1.6.6.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.6.6.1.1">
<span class="ltx_p" id="S3.T3.2.1.6.6.1.1.1" style="width:52.6pt;"><span class="ltx_text" id="S3.T3.2.1.6.6.1.1.1.1" style="font-size:70%;">HPS </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S3.T3.2.1.6.6.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">20</span></a><span class="ltx_text" id="S3.T3.2.1.6.6.1.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.6.6.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.6.6.2.1">
<span class="ltx_p" id="S3.T3.2.1.6.6.2.1.1" style="width:14.2pt;"><span class="ltx_text" id="S3.T3.2.1.6.6.2.1.1.1" style="font-size:70%;">2021</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.6.6.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.6.6.3.1">
<span class="ltx_p" id="S3.T3.2.1.6.6.3.1.1" style="width:184.9pt;"><span class="ltx_text" id="S3.T3.2.1.6.6.3.1.1.1" style="font-size:70%;">Integrates wearable sensors, IMUs and a head-mounted camera for precise 3D pose tracking in pre-scanned environments, eliminating drift with localization and scene constraints.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.6.6.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.6.6.4.1">
<span class="ltx_p" id="S3.T3.2.1.6.6.4.1.1" style="width:32.7pt;"><span class="ltx_text" id="S3.T3.2.1.6.6.4.1.1.1" style="font-size:70%;">HPS</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.6.6.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.6.6.5.1">
<span class="ltx_p" id="S3.T3.2.1.6.6.5.1.1" style="width:96.7pt;"><span class="ltx_text" id="S3.T3.2.1.6.6.5.1.1.1" style="font-size:70%;">Lack of features and scene changes between static 3D scans and real images.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T3.2.1.6.6.6">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.6.6.6.1">
<span class="ltx_p" id="S3.T3.2.1.6.6.6.1.1"><a class="ltx_ref ltx_href" href="https://virtualhumans.mpi-inf.mpg.de/hps/" style="font-size:70%;" title="">Project</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.1.7.7">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.2.1.7.7.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.7.7.1.1">
<span class="ltx_p" id="S3.T3.2.1.7.7.1.1.1" style="width:52.6pt;"><span class="ltx_text" id="S3.T3.2.1.7.7.1.1.1.1" style="font-size:70%;">Avatarposer </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S3.T3.2.1.7.7.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">30</span></a><span class="ltx_text" id="S3.T3.2.1.7.7.1.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.7.7.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.7.7.2.1">
<span class="ltx_p" id="S3.T3.2.1.7.7.2.1.1" style="width:14.2pt;"><span class="ltx_text" id="S3.T3.2.1.7.7.2.1.1.1" style="font-size:70%;">2022</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.7.7.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.7.7.3.1">
<span class="ltx_p" id="S3.T3.2.1.7.7.3.1.1" style="width:184.9pt;"><span class="ltx_text" id="S3.T3.2.1.7.7.3.1.1.1" style="font-size:70%;">First learning-based method predicting full-body poses in world coordinates, leveraging transformer encoder and motion input from head and hands</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.7.7.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.7.7.4.1">
<span class="ltx_p" id="S3.T3.2.1.7.7.4.1.1" style="width:32.7pt;"><span class="ltx_text" id="S3.T3.2.1.7.7.4.1.1.1" style="font-size:70%;">CMU Mocap, AMASS</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.7.7.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.7.7.5.1">
<span class="ltx_p" id="S3.T3.2.1.7.7.5.1.1" style="width:96.7pt;"><span class="ltx_text" id="S3.T3.2.1.7.7.5.1.1.1" style="font-size:70%;">Sensitivity to inaccuracies and occlusions in hand tracking data.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T3.2.1.7.7.6">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.7.7.6.1">
<span class="ltx_p" id="S3.T3.2.1.7.7.6.1.1"><a class="ltx_ref ltx_href" href="https://github.com/eth-siplab/AvatarPoser" style="font-size:70%;" title="">Code</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.1.8.8">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.2.1.8.8.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.8.8.1.1">
<span class="ltx_p" id="S3.T3.2.1.8.8.1.1.1" style="width:52.6pt;"><span class="ltx_text" id="S3.T3.2.1.8.8.1.1.1.1" style="font-size:70%;">FLAG </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S3.T3.2.1.8.8.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a><span class="ltx_text" id="S3.T3.2.1.8.8.1.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.8.8.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.8.8.2.1">
<span class="ltx_p" id="S3.T3.2.1.8.8.2.1.1" style="width:14.2pt;"><span class="ltx_text" id="S3.T3.2.1.8.8.2.1.1.1" style="font-size:70%;">2022</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.8.8.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.8.8.3.1">
<span class="ltx_p" id="S3.T3.2.1.8.8.3.1.1" style="width:184.9pt;"><span class="ltx_text" id="S3.T3.2.1.8.8.3.1.1.1" style="font-size:70%;">Flow-based model for realistic 3D human body pose prediction with uncertainty estimates, enhancing prior work through high-quality pose generation and efficient latent variable sampling for optimization.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.8.8.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.8.8.4.1">
<span class="ltx_p" id="S3.T3.2.1.8.8.4.1.1" style="width:32.7pt;"><span class="ltx_text" id="S3.T3.2.1.8.8.4.1.1.1" style="font-size:70%;">AMASS</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.8.8.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.8.8.5.1">
<span class="ltx_p" id="S3.T3.2.1.8.8.5.1.1" style="width:96.7pt;"><span class="ltx_text" id="S3.T3.2.1.8.8.5.1.1.1" style="font-size:70%;">Difficulty in generating complex lower-body poses due to sparse training data and lack of temporal data integration.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T3.2.1.8.8.6">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.8.8.6.1">
<span class="ltx_p" id="S3.T3.2.1.8.8.6.1.1"><a class="ltx_ref ltx_href" href="https://microsoft.github.io/flag/" style="font-size:70%;" title="">Project</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.1.9.9">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.2.1.9.9.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.9.9.1.1">
<span class="ltx_p" id="S3.T3.2.1.9.9.1.1.1" style="width:52.6pt;"><span class="ltx_text" id="S3.T3.2.1.9.9.1.1.1.1" style="font-size:70%;">Su et al. </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S3.T3.2.1.9.9.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib54" title=""><span class="ltx_text" style="font-size:90%;">54</span></a><span class="ltx_text" id="S3.T3.2.1.9.9.1.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.9.9.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.9.9.2.1">
<span class="ltx_p" id="S3.T3.2.1.9.9.2.1.1" style="width:14.2pt;"><span class="ltx_text" id="S3.T3.2.1.9.9.2.1.1.1" style="font-size:70%;">2022</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.9.9.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.9.9.3.1">
<span class="ltx_p" id="S3.T3.2.1.9.9.3.1.1" style="width:184.9pt;"><span class="ltx_text" id="S3.T3.2.1.9.9.3.1.1.1" style="font-size:70%;">A data framework transforms raw video into 3D pose, enriched by a lightweight Self-Perception Excitation (SPE) module for egocentric self-awareness.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.9.9.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.9.9.4.1">
<span class="ltx_p" id="S3.T3.2.1.9.9.4.1.1" style="width:32.7pt;"><span class="ltx_text" id="S3.T3.2.1.9.9.4.1.1.1" style="font-size:70%;">Mocap dataset </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S3.T3.2.1.9.9.4.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib72" title=""><span class="ltx_text" style="font-size:90%;">72</span></a><span class="ltx_text" id="S3.T3.2.1.9.9.4.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.9.9.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.9.9.5.1">
<span class="ltx_p" id="S3.T3.2.1.9.9.5.1.1" style="width:96.7pt;"><span class="ltx_text" id="S3.T3.2.1.9.9.5.1.1.1" style="font-size:70%;">Dependency on MoCap data and synchronized third-person view videos may limit the method’s real-world applicability.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T3.2.1.9.9.6">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.9.9.6.1">
<span class="ltx_p" id="S3.T3.2.1.9.9.6.1.1"><span class="ltx_text" id="S3.T3.2.1.9.9.6.1.1.1" style="font-size:70%;">–</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.1.10.10">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.2.1.10.10.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.10.10.1.1">
<span class="ltx_p" id="S3.T3.2.1.10.10.1.1.1" style="width:52.6pt;"><span class="ltx_text" id="S3.T3.2.1.10.10.1.1.1.1" style="font-size:70%;">EgoEgo </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S3.T3.2.1.10.10.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib35" title=""><span class="ltx_text" style="font-size:90%;">35</span></a><span class="ltx_text" id="S3.T3.2.1.10.10.1.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.10.10.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.10.10.2.1">
<span class="ltx_p" id="S3.T3.2.1.10.10.2.1.1" style="width:14.2pt;"><span class="ltx_text" id="S3.T3.2.1.10.10.2.1.1.1" style="font-size:70%;">2023</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.10.10.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.10.10.3.1">
<span class="ltx_p" id="S3.T3.2.1.10.10.3.1.1" style="width:184.9pt;"><span class="ltx_text" id="S3.T3.2.1.10.10.3.1.1.1" style="font-size:70%;">Ego body pose estimation using ego head pose estimation leveraging SLAM, and conditional diffusion for disentangled head and body pose estimation.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.10.10.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.10.10.4.1">
<span class="ltx_p" id="S3.T3.2.1.10.10.4.1.1" style="width:32.7pt;"><span class="ltx_text" id="S3.T3.2.1.10.10.4.1.1.1" style="font-size:70%;">ARES</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.10.10.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.10.10.5.1">
<span class="ltx_p" id="S3.T3.2.1.10.10.5.1.1" style="width:96.7pt;"><span class="ltx_text" id="S3.T3.2.1.10.10.5.1.1.1" style="font-size:70%;">Evaluation on synthetic and relatively small real-world datasets.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T3.2.1.10.10.6">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.10.10.6.1">
<span class="ltx_p" id="S3.T3.2.1.10.10.6.1.1"><a class="ltx_ref ltx_href" href="https://github.com/lijiaman/egoego_release" style="font-size:70%;" title="">Code</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.1.11.11">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.2.1.11.11.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.11.11.1.1">
<span class="ltx_p" id="S3.T3.2.1.11.11.1.1.1" style="width:52.6pt;"><span class="ltx_text" id="S3.T3.2.1.11.11.1.1.1.1" style="font-size:70%;">EgoHMR </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S3.T3.2.1.11.11.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib75" title=""><span class="ltx_text" style="font-size:90%;">75</span></a><span class="ltx_text" id="S3.T3.2.1.11.11.1.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.11.11.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.11.11.2.1">
<span class="ltx_p" id="S3.T3.2.1.11.11.2.1.1" style="width:14.2pt;"><span class="ltx_text" id="S3.T3.2.1.11.11.2.1.1.1" style="font-size:70%;">2023</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.11.11.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.11.11.3.1">
<span class="ltx_p" id="S3.T3.2.1.11.11.3.1.1" style="width:184.9pt;"><span class="ltx_text" id="S3.T3.2.1.11.11.3.1.1.1" style="font-size:70%;">Scene-conditioned diffusion approach using a physics-based collision score, realistic human-scene interaction, accurate estimation for visible body parts while enhancing diversity.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.11.11.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.11.11.4.1">
<span class="ltx_p" id="S3.T3.2.1.11.11.4.1.1" style="width:32.7pt;"><span class="ltx_text" id="S3.T3.2.1.11.11.4.1.1.1" style="font-size:70%;">EgoBody</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.11.11.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.11.11.5.1">
<span class="ltx_p" id="S3.T3.2.1.11.11.5.1.1" style="width:96.7pt;"><span class="ltx_text" id="S3.T3.2.1.11.11.5.1.1.1" style="font-size:70%;">Limited temporal context for reconstructing egocentric human motions.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T3.2.1.11.11.6">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.11.11.6.1">
<span class="ltx_p" id="S3.T3.2.1.11.11.6.1.1"><a class="ltx_ref ltx_href" href="https://github.com/sanweiliti/EgoHMR" style="font-size:70%;" title="">Code</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.1.12.12">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.2.1.12.12.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.12.12.1.1">
<span class="ltx_p" id="S3.T3.2.1.12.12.1.1.1" style="width:52.6pt;"><span class="ltx_text" id="S3.T3.2.1.12.12.1.1.1.1" style="font-size:70%;">EgoPoser </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S3.T3.2.1.12.12.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">31</span></a><span class="ltx_text" id="S3.T3.2.1.12.12.1.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.12.12.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.12.12.2.1">
<span class="ltx_p" id="S3.T3.2.1.12.12.2.1.1" style="width:14.2pt;"><span class="ltx_text" id="S3.T3.2.1.12.12.2.1.1.1" style="font-size:70%;">2023</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.12.12.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.12.12.3.1">
<span class="ltx_p" id="S3.T3.2.1.12.12.3.1.1" style="width:184.9pt;"><span class="ltx_text" id="S3.T3.2.1.12.12.3.1.1.1" style="font-size:70%;">Used sparse motion sensor; mitigates overfitting with position-invariant prediction, adaptable to diverse body sizes, robust with hands out of view and reduces motion artifacts.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.12.12.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.12.12.4.1">
<span class="ltx_p" id="S3.T3.2.1.12.12.4.1.1" style="width:32.7pt;"><span class="ltx_text" id="S3.T3.2.1.12.12.4.1.1.1" style="font-size:70%;">AMASS</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.2.1.12.12.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.12.12.5.1">
<span class="ltx_p" id="S3.T3.2.1.12.12.5.1.1" style="width:96.7pt;"><span class="ltx_text" id="S3.T3.2.1.12.12.5.1.1.1" style="font-size:70%;">Limited evaluation on diverse real-world scenarios.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T3.2.1.12.12.6">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.12.12.6.1">
<span class="ltx_p" id="S3.T3.2.1.12.12.6.1.1"><span class="ltx_text" id="S3.T3.2.1.12.12.6.1.1.1" style="font-size:70%;">–</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.1.13.13">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.2.1.13.13.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.13.13.1.1">
<span class="ltx_p" id="S3.T3.2.1.13.13.1.1.1" style="width:52.6pt;"><span class="ltx_text" id="S3.T3.2.1.13.13.1.1.1.1" style="font-size:70%;">SimpleEgo </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S3.T3.2.1.13.13.1.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">11</span></a><span class="ltx_text" id="S3.T3.2.1.13.13.1.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S3.T3.2.1.13.13.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.13.13.2.1">
<span class="ltx_p" id="S3.T3.2.1.13.13.2.1.1" style="width:14.2pt;"><span class="ltx_text" id="S3.T3.2.1.13.13.2.1.1.1" style="font-size:70%;">2024</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S3.T3.2.1.13.13.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.13.13.3.1">
<span class="ltx_p" id="S3.T3.2.1.13.13.3.1.1" style="width:184.9pt;"><span class="ltx_text" id="S3.T3.2.1.13.13.3.1.1.1" style="font-size:70%;">Directly predicts joint rotations as matrix Fisher distributions, providing robust uncertainty estimation and realistic deployment prospects.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S3.T3.2.1.13.13.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.13.13.4.1">
<span class="ltx_p" id="S3.T3.2.1.13.13.4.1.1" style="width:32.7pt;"><span class="ltx_text" id="S3.T3.2.1.13.13.4.1.1.1" style="font-size:70%;">SynthEgo</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S3.T3.2.1.13.13.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.13.13.5.1">
<span class="ltx_p" id="S3.T3.2.1.13.13.5.1.1" style="width:96.7pt;"><span class="ltx_text" id="S3.T3.2.1.13.13.5.1.1.1" style="font-size:70%;">Accuracy could be limited when large portions of the body are occluded in the image.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S3.T3.2.1.13.13.6">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.1.13.13.6.1">
<span class="ltx_p" id="S3.T3.2.1.13.13.6.1.1"><a class="ltx_ref ltx_href" href="https://microsoft.github.io/SimpleEgo/" style="font-size:70%;" title="">Project</a></span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T3.5.1.1" style="font-size:129%;">Table 3</span>: </span><span class="ltx_text" id="S3.T3.6.2" style="font-size:129%;">Popular body shape based egocentric 3D pose estimation models.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p8">
<p class="ltx_p" id="S3.SS1.p8.1">Generating <span class="ltx_text ltx_font_bold" id="S3.SS1.p8.1.1">3D ground truth</span> data using motion capture system is a cumbersome task. To alleviate this problem, <span class="ltx_text ltx_font_italic" id="S3.SS1.p8.1.2">EgoFish3D</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">39</span></a>]</cite> proposed three modules: a third-person view module generating accurate 3D poses from external camera images, an egocentric module predicting 3D poses from a single fisheye image via self-supervised learning, and an interactive module estimating rotation differences between third-person and egocentric views. This method achieves self-supervised egocentric 3D pose estimation without ground truth annotations, leveraging a real-world dataset (ECHP) with synchronized third-person and egocentric images.
<span class="ltx_text ltx_font_bold" id="S3.SS1.p8.1.3">Linking first-person and third-person view</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib37" title=""><span class="ltx_text" style="font-size:90%;">37</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib53" title=""><span class="ltx_text" style="font-size:90%;">53</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib69" title=""><span class="ltx_text" style="font-size:90%;">69</span></a>]</cite> plays a crucial role for better understanding wearer’s action and poses. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite> used visual information from paired third-person videos to create a shared space where different views of the same pose are close together. They trained a special neural network to learn this shared space in a self-supervised manner, teaching it to distinguish if two views show the same 3D skeleton.</p>
</div>
<div class="ltx_para" id="S3.SS1.p9">
<p class="ltx_p" id="S3.SS1.p9.1"><span class="ltx_text ltx_font_italic" id="S3.SS1.p9.1.1">EgoFormer</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite>, a tansformer-based model for ego-pose estimation in AR and VR applications, addresses the ambiguity in first-person videos by leveraging video context and establishing long-term temporal relationships. It extracts effective temporal features, dynamic features, and introduces a novel representation for motion clues.
<span class="ltx_text ltx_font_italic" id="S3.SS1.p9.1.2">You2Me</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib46" title=""><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite> addresses the challenge of estimating the 3D body pose of the camera wearer by leveraging interactions with a <span class="ltx_text ltx_font_bold" id="S3.SS1.p9.1.3">visible second person</span>. The key insight is that dyadic interactions between individuals help to learn temporal models for interlinked poses even when one person is largely <span class="ltx_text ltx_font_bold" id="S3.SS1.p9.1.4">out of the field view</span>. The method incorporates dynamic first-person motion features, static first-person scene features, and second-person’s body pose interaction features to explicitly account for the body pose of the camera wearer.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Body Shape Based Methods</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">In this section, we expand on the different human body shape-based egocentric pose estimation methods found in the literature. Out of them, 12 are highlighted in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#S3.T3" title="Table 3 ‣ 3.1 Skeletal Based Methods ‣ 3 3D Egocentric Pose Estimation Methods ‣ A Survey on 3D Egocentric Human Pose Estimation"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure class="ltx_table" id="S3.T4">
<table class="ltx_tabular ltx_align_middle" id="S3.T4.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T4.2.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T4.2.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.1.1.1.1">
<span class="ltx_p" id="S3.T4.2.1.1.1.1.1" style="width:78.2pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.1.1.1.1.1" style="font-size:90%;">Methods</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T4.2.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.1.1.2.1">
<span class="ltx_p" id="S3.T4.2.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.1.2.1.1.1" style="font-size:90%;">Walking</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T4.2.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.1.1.3.1">
<span class="ltx_p" id="S3.T4.2.1.1.3.1.1"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.1.3.1.1.1" style="font-size:90%;">Sitting</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T4.2.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.1.1.4.1">
<span class="ltx_p" id="S3.T4.2.1.1.4.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.1.4.1.1.1" style="font-size:90%;">Crawling</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T4.2.1.1.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.1.1.5.1">
<span class="ltx_p" id="S3.T4.2.1.1.5.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.1.5.1.1.1" style="font-size:90%;">Crouching</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T4.2.1.1.6">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.1.1.6.1">
<span class="ltx_p" id="S3.T4.2.1.1.6.1.1"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.1.6.1.1.1" style="font-size:90%;">Boxing</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T4.2.1.1.7">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.1.1.7.1">
<span class="ltx_p" id="S3.T4.2.1.1.7.1.1"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.1.7.1.1.1" style="font-size:90%;">Dancing</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T4.2.1.1.8">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.1.1.8.1">
<span class="ltx_p" id="S3.T4.2.1.1.8.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.1.8.1.1.1" style="font-size:90%;">Stretching</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T4.2.1.1.9">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.1.1.9.1">
<span class="ltx_p" id="S3.T4.2.1.1.9.1.1"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.1.9.1.1.1" style="font-size:90%;">Waving</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T4.2.1.1.10">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.1.1.10.1">
<span class="ltx_p" id="S3.T4.2.1.1.10.1.1"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.1.10.1.1.1" style="font-size:90%;">Average</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T4.2.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.2.2.1.1">
<span class="ltx_p" id="S3.T4.2.2.2.1.1.1" style="width:78.2pt;"><span class="ltx_text" id="S3.T4.2.2.2.1.1.1.1" style="font-size:90%;">EgoFish3D </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S3.T4.2.2.2.1.1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">39</span></a><span class="ltx_text" id="S3.T4.2.2.2.1.1.1.3.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T4.2.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.2.2.2.1">
<span class="ltx_p" id="S3.T4.2.2.2.2.1.1"><span class="ltx_text" id="S3.T4.2.2.2.2.1.1.1" style="font-size:90%;">60.9</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T4.2.2.2.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.2.2.3.1">
<span class="ltx_p" id="S3.T4.2.2.2.3.1.1"><span class="ltx_text" id="S3.T4.2.2.2.3.1.1.1" style="font-size:90%;">42.1</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T4.2.2.2.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.2.2.4.1">
<span class="ltx_p" id="S3.T4.2.2.2.4.1.1" style="width:42.7pt;"><span class="ltx_text" id="S3.T4.2.2.2.4.1.1.1" style="font-size:90%;">65.0</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T4.2.2.2.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.2.2.5.1">
<span class="ltx_p" id="S3.T4.2.2.2.5.1.1" style="width:42.7pt;"><span class="ltx_text" id="S3.T4.2.2.2.5.1.1.1" style="font-size:90%;">82.7</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T4.2.2.2.6">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.2.2.6.1">
<span class="ltx_p" id="S3.T4.2.2.2.6.1.1"><span class="ltx_text" id="S3.T4.2.2.2.6.1.1.1" style="font-size:90%;">79.0</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T4.2.2.2.7">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.2.2.7.1">
<span class="ltx_p" id="S3.T4.2.2.2.7.1.1"><span class="ltx_text" id="S3.T4.2.2.2.7.1.1.1" style="font-size:90%;">55.5</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T4.2.2.2.8">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.2.2.8.1">
<span class="ltx_p" id="S3.T4.2.2.2.8.1.1" style="width:42.7pt;"><span class="ltx_text" id="S3.T4.2.2.2.8.1.1.1" style="font-size:90%;">59.1</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T4.2.2.2.9">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.2.2.9.1">
<span class="ltx_p" id="S3.T4.2.2.2.9.1.1"><span class="ltx_text" id="S3.T4.2.2.2.9.1.1.1" style="font-size:90%;">94.5</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T4.2.2.2.10">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.2.2.10.1">
<span class="ltx_p" id="S3.T4.2.2.2.10.1.1"><span class="ltx_text" id="S3.T4.2.2.2.10.1.1.1" style="font-size:90%;">66.8</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.3.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T4.2.3.3.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.3.3.1.1">
<span class="ltx_p" id="S3.T4.2.3.3.1.1.1" style="width:78.2pt;"><span class="ltx_text" id="S3.T4.2.3.3.1.1.1.1" style="font-size:90%;">Zhang et al. </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S3.T4.2.3.3.1.1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib76" title=""><span class="ltx_text" style="font-size:90%;">76</span></a><span class="ltx_text" id="S3.T4.2.3.3.1.1.1.3.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T4.2.3.3.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.3.3.2.1">
<span class="ltx_p" id="S3.T4.2.3.3.2.1.1"><span class="ltx_text" id="S3.T4.2.3.3.2.1.1.1" style="font-size:90%;">41.16</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T4.2.3.3.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.3.3.3.1">
<span class="ltx_p" id="S3.T4.2.3.3.3.1.1"><span class="ltx_text" id="S3.T4.2.3.3.3.1.1.1" style="font-size:90%;">76.58</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T4.2.3.3.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.3.3.4.1">
<span class="ltx_p" id="S3.T4.2.3.3.4.1.1" style="width:42.7pt;"><span class="ltx_text" id="S3.T4.2.3.3.4.1.1.1" style="font-size:90%;">73.04</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T4.2.3.3.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.3.3.5.1">
<span class="ltx_p" id="S3.T4.2.3.3.5.1.1" style="width:42.7pt;"><span class="ltx_text" id="S3.T4.2.3.3.5.1.1.1" style="font-size:90%;">89.67</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T4.2.3.3.6">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.3.3.6.1">
<span class="ltx_p" id="S3.T4.2.3.3.6.1.1"><span class="ltx_text" id="S3.T4.2.3.3.6.1.1.1" style="font-size:90%;">52.96</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T4.2.3.3.7">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.3.3.7.1">
<span class="ltx_p" id="S3.T4.2.3.3.7.1.1"><span class="ltx_text" id="S3.T4.2.3.3.7.1.1.1" style="font-size:90%;">58.90</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T4.2.3.3.8">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.3.3.8.1">
<span class="ltx_p" id="S3.T4.2.3.3.8.1.1" style="width:42.7pt;"><span class="ltx_text" id="S3.T4.2.3.3.8.1.1.1" style="font-size:90%;">92.21</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T4.2.3.3.9">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.3.3.9.1">
<span class="ltx_p" id="S3.T4.2.3.3.9.1.1"><span class="ltx_text" id="S3.T4.2.3.3.9.1.1.1" style="font-size:90%;">71.55</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T4.2.3.3.10">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.3.3.10.1">
<span class="ltx_p" id="S3.T4.2.3.3.10.1.1"><span class="ltx_text" id="S3.T4.2.3.3.10.1.1.1" style="font-size:90%;">62.13</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.4.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T4.2.4.4.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.4.4.1.1">
<span class="ltx_p" id="S3.T4.2.4.4.1.1.1" style="width:78.2pt;"><span class="ltx_text" id="S3.T4.2.4.4.1.1.1.1" style="font-size:90%;">Mo</span><sup class="ltx_sup ltx_centering" id="S3.T4.2.4.4.1.1.1.2"><span class="ltx_text" id="S3.T4.2.4.4.1.1.1.2.1" style="font-size:90%;">2</span></sup><span class="ltx_text" id="S3.T4.2.4.4.1.1.1.3" style="font-size:90%;">Cap</span><sup class="ltx_sup ltx_centering" id="S3.T4.2.4.4.1.1.1.4"><span class="ltx_text" id="S3.T4.2.4.4.1.1.1.4.1" style="font-size:90%;">2</span></sup><span class="ltx_text" id="S3.T4.2.4.4.1.1.1.5" style="font-size:90%;"> </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S3.T4.2.4.4.1.1.1.6.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib68" title=""><span class="ltx_text" style="font-size:90%;">68</span></a><span class="ltx_text" id="S3.T4.2.4.4.1.1.1.7.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T4.2.4.4.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.4.4.2.1">
<span class="ltx_p" id="S3.T4.2.4.4.2.1.1"><span class="ltx_text" id="S3.T4.2.4.4.2.1.1.1" style="font-size:90%;">38.41</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T4.2.4.4.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.4.4.3.1">
<span class="ltx_p" id="S3.T4.2.4.4.3.1.1"><span class="ltx_text" id="S3.T4.2.4.4.3.1.1.1" style="font-size:90%;">70.94</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T4.2.4.4.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.4.4.4.1">
<span class="ltx_p" id="S3.T4.2.4.4.4.1.1" style="width:42.7pt;"><span class="ltx_text" id="S3.T4.2.4.4.4.1.1.1" style="font-size:90%;">94.31</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T4.2.4.4.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.4.4.5.1">
<span class="ltx_p" id="S3.T4.2.4.4.5.1.1" style="width:42.7pt;"><span class="ltx_text" id="S3.T4.2.4.4.5.1.1.1" style="font-size:90%;">81.90</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T4.2.4.4.6">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.4.4.6.1">
<span class="ltx_p" id="S3.T4.2.4.4.6.1.1"><span class="ltx_text" id="S3.T4.2.4.4.6.1.1.1" style="font-size:90%;">48.55</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T4.2.4.4.7">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.4.4.7.1">
<span class="ltx_p" id="S3.T4.2.4.4.7.1.1"><span class="ltx_text" id="S3.T4.2.4.4.7.1.1.1" style="font-size:90%;">55.19</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T4.2.4.4.8">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.4.4.8.1">
<span class="ltx_p" id="S3.T4.2.4.4.8.1.1" style="width:42.7pt;"><span class="ltx_text" id="S3.T4.2.4.4.8.1.1.1" style="font-size:90%;">99.34</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T4.2.4.4.9">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.4.4.9.1">
<span class="ltx_p" id="S3.T4.2.4.4.9.1.1"><span class="ltx_text" id="S3.T4.2.4.4.9.1.1.1" style="font-size:90%;">60.92</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T4.2.4.4.10">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.4.4.10.1">
<span class="ltx_p" id="S3.T4.2.4.4.10.1.1"><span class="ltx_text" id="S3.T4.2.4.4.10.1.1.1" style="font-size:90%;">61.40</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.5.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T4.2.5.5.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.5.5.1.1">
<span class="ltx_p" id="S3.T4.2.5.5.1.1.1" style="width:78.2pt;"><span class="ltx_text" id="S3.T4.2.5.5.1.1.1.1" style="font-size:90%;">xR-EgoPose </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S3.T4.2.5.5.1.1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">60</span></a><span class="ltx_text" id="S3.T4.2.5.5.1.1.1.3.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T4.2.5.5.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.5.5.2.1">
<span class="ltx_p" id="S3.T4.2.5.5.2.1.1"><span class="ltx_text" id="S3.T4.2.5.5.2.1.1.1" style="font-size:90%;">38.39</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T4.2.5.5.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.5.5.3.1">
<span class="ltx_p" id="S3.T4.2.5.5.3.1.1"><span class="ltx_text" id="S3.T4.2.5.5.3.1.1.1" style="font-size:90%;">61.59</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T4.2.5.5.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.5.5.4.1">
<span class="ltx_p" id="S3.T4.2.5.5.4.1.1" style="width:42.7pt;"><span class="ltx_text" id="S3.T4.2.5.5.4.1.1.1" style="font-size:90%;">69.53</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T4.2.5.5.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.5.5.5.1">
<span class="ltx_p" id="S3.T4.2.5.5.5.1.1" style="width:42.7pt;"><span class="ltx_text" id="S3.T4.2.5.5.5.1.1.1" style="font-size:90%;">51.14</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T4.2.5.5.6">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.5.5.6.1">
<span class="ltx_p" id="S3.T4.2.5.5.6.1.1"><span class="ltx_text" id="S3.T4.2.5.5.6.1.1.1" style="font-size:90%;">37.67</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T4.2.5.5.7">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.5.5.7.1">
<span class="ltx_p" id="S3.T4.2.5.5.7.1.1"><span class="ltx_text" id="S3.T4.2.5.5.7.1.1.1" style="font-size:90%;">42.10</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T4.2.5.5.8">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.5.5.8.1">
<span class="ltx_p" id="S3.T4.2.5.5.8.1.1" style="width:42.7pt;"><span class="ltx_text" id="S3.T4.2.5.5.8.1.1.1" style="font-size:90%;">58.32</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T4.2.5.5.9">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.5.5.9.1">
<span class="ltx_p" id="S3.T4.2.5.5.9.1.1"><span class="ltx_text" id="S3.T4.2.5.5.9.1.1.1" style="font-size:90%;">44.77</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T4.2.5.5.10">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.5.5.10.1">
<span class="ltx_p" id="S3.T4.2.5.5.10.1.1"><span class="ltx_text" id="S3.T4.2.5.5.10.1.1.1" style="font-size:90%;">48.16</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.6.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T4.2.6.6.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.6.6.1.1">
<span class="ltx_p" id="S3.T4.2.6.6.1.1.1" style="width:78.2pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.2.6.6.1.1.1.1" style="font-size:90%;">SelfPose-UNet</span><span class="ltx_text" id="S3.T4.2.6.6.1.1.1.2" style="font-size:90%;"> </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S3.T4.2.6.6.1.1.1.3.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib61" title=""><span class="ltx_text" style="font-size:90%;">61</span></a><span class="ltx_text" id="S3.T4.2.6.6.1.1.1.4.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S3.T4.2.6.6.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.6.6.2.1">
<span class="ltx_p" id="S3.T4.2.6.6.2.1.1"><span class="ltx_text" id="S3.T4.2.6.6.2.1.1.1" style="font-size:90%;">45.83</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S3.T4.2.6.6.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.6.6.3.1">
<span class="ltx_p" id="S3.T4.2.6.6.3.1.1"><span class="ltx_text" id="S3.T4.2.6.6.3.1.1.1" style="font-size:90%;">47.24</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S3.T4.2.6.6.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.6.6.4.1">
<span class="ltx_p" id="S3.T4.2.6.6.4.1.1" style="width:42.7pt;"><span class="ltx_text" id="S3.T4.2.6.6.4.1.1.1" style="font-size:90%;">47.35</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S3.T4.2.6.6.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.6.6.5.1">
<span class="ltx_p" id="S3.T4.2.6.6.5.1.1" style="width:42.7pt;"><span class="ltx_text" id="S3.T4.2.6.6.5.1.1.1" style="font-size:90%;">45.15</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S3.T4.2.6.6.6">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.6.6.6.1">
<span class="ltx_p" id="S3.T4.2.6.6.6.1.1"><span class="ltx_text" id="S3.T4.2.6.6.6.1.1.1" style="font-size:90%;">48.72</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S3.T4.2.6.6.7">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.6.6.7.1">
<span class="ltx_p" id="S3.T4.2.6.6.7.1.1"><span class="ltx_text" id="S3.T4.2.6.6.7.1.1.1" style="font-size:90%;">47.00</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S3.T4.2.6.6.8">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.6.6.8.1">
<span class="ltx_p" id="S3.T4.2.6.6.8.1.1" style="width:42.7pt;"><span class="ltx_text" id="S3.T4.2.6.6.8.1.1.1" style="font-size:90%;">46.15</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S3.T4.2.6.6.9">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.6.6.9.1">
<span class="ltx_p" id="S3.T4.2.6.6.9.1.1"><span class="ltx_text" id="S3.T4.2.6.6.9.1.1.1" style="font-size:90%;">46.45</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S3.T4.2.6.6.10">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.2.6.6.10.1">
<span class="ltx_p" id="S3.T4.2.6.6.10.1.1"><span class="ltx_text ltx_font_bold" id="S3.T4.2.6.6.10.1.1.1" style="font-size:90%;">46.61</span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 4: </span>Comparison of different skeletal based 3D egocentric pose estimation methods on Mo<sup class="ltx_sup" id="S3.T4.9.1">2</sup>Cap<sup class="ltx_sup" id="S3.T4.10.2">2</sup> dataset using MPJPE (mm).</figcaption>
</figure>
<figure class="ltx_table" id="S3.T5">
<table class="ltx_tabular ltx_align_middle" id="S3.T5.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T5.2.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T5.2.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.1.1.1.1">
<span class="ltx_p" id="S3.T5.2.1.1.1.1.1" style="width:78.2pt;"><span class="ltx_text ltx_font_bold" id="S3.T5.2.1.1.1.1.1.1" style="font-size:90%;">Methods</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.1.1.2.1">
<span class="ltx_p" id="S3.T5.2.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S3.T5.2.1.1.2.1.1.1" style="font-size:90%;">Game</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.1.1.3.1">
<span class="ltx_p" id="S3.T5.2.1.1.3.1.1"><span class="ltx_text ltx_font_bold" id="S3.T5.2.1.1.3.1.1.1" style="font-size:90%;">Gest.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T5.2.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.1.1.4.1">
<span class="ltx_p" id="S3.T5.2.1.1.4.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="S3.T5.2.1.1.4.1.1.1" style="font-size:90%;">Greeting</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.1.1.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.1.1.5.1">
<span class="ltx_p" id="S3.T5.2.1.1.5.1.1"><span class="ltx_text ltx_font_bold" id="S3.T5.2.1.1.5.1.1.1" style="font-size:90%;">Lower Stretch</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.1.1.6">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.1.1.6.1">
<span class="ltx_p" id="S3.T5.2.1.1.6.1.1"><span class="ltx_text ltx_font_bold" id="S3.T5.2.1.1.6.1.1.1" style="font-size:90%;">Pat</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.1.1.7">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.1.1.7.1">
<span class="ltx_p" id="S3.T5.2.1.1.7.1.1"><span class="ltx_text ltx_font_bold" id="S3.T5.2.1.1.7.1.1.1" style="font-size:90%;">React</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.1.1.8">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.1.1.8.1">
<span class="ltx_p" id="S3.T5.2.1.1.8.1.1"><span class="ltx_text ltx_font_bold" id="S3.T5.2.1.1.8.1.1.1" style="font-size:90%;">Talk</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.1.1.9">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.1.1.9.1">
<span class="ltx_p" id="S3.T5.2.1.1.9.1.1"><span class="ltx_text ltx_font_bold" id="S3.T5.2.1.1.9.1.1.1" style="font-size:90%;">Upper Stretch</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.1.1.10">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.1.1.10.1">
<span class="ltx_p" id="S3.T5.2.1.1.10.1.1"><span class="ltx_text ltx_font_bold" id="S3.T5.2.1.1.10.1.1.1" style="font-size:90%;">Walk</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.1.1.11">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.1.1.11.1">
<span class="ltx_p" id="S3.T5.2.1.1.11.1.1"><span class="ltx_text ltx_font_bold" id="S3.T5.2.1.1.11.1.1.1" style="font-size:90%;">All</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T5.2.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T5.2.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.2.2.1.1">
<span class="ltx_p" id="S3.T5.2.2.2.1.1.1" style="width:78.2pt;"><span class="ltx_text" id="S3.T5.2.2.2.1.1.1.1" style="font-size:90%;">xR-EgoPose </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S3.T5.2.2.2.1.1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">60</span></a><span class="ltx_text" id="S3.T5.2.2.2.1.1.1.3.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.2.2.2.1">
<span class="ltx_p" id="S3.T5.2.2.2.2.1.1"><span class="ltx_text" id="S3.T5.2.2.2.2.1.1.1" style="font-size:90%;">56.0</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.2.2.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.2.2.3.1">
<span class="ltx_p" id="S3.T5.2.2.2.3.1.1"><span class="ltx_text" id="S3.T5.2.2.2.3.1.1.1" style="font-size:90%;">50.2</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T5.2.2.2.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.2.2.4.1">
<span class="ltx_p" id="S3.T5.2.2.2.4.1.1" style="width:42.7pt;"><span class="ltx_text" id="S3.T5.2.2.2.4.1.1.1" style="font-size:90%;">44.6</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.2.2.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.2.2.5.1">
<span class="ltx_p" id="S3.T5.2.2.2.5.1.1"><span class="ltx_text" id="S3.T5.2.2.2.5.1.1.1" style="font-size:90%;">51.5</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.2.2.6">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.2.2.6.1">
<span class="ltx_p" id="S3.T5.2.2.2.6.1.1"><span class="ltx_text" id="S3.T5.2.2.2.6.1.1.1" style="font-size:90%;">59.4</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.2.2.7">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.2.2.7.1">
<span class="ltx_p" id="S3.T5.2.2.2.7.1.1"><span class="ltx_text" id="S3.T5.2.2.2.7.1.1.1" style="font-size:90%;">60.8</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.2.2.8">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.2.2.8.1">
<span class="ltx_p" id="S3.T5.2.2.2.8.1.1"><span class="ltx_text" id="S3.T5.2.2.2.8.1.1.1" style="font-size:90%;">43.9</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.2.2.9">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.2.2.9.1">
<span class="ltx_p" id="S3.T5.2.2.2.9.1.1"><span class="ltx_text" id="S3.T5.2.2.2.9.1.1.1" style="font-size:90%;">53.9</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.2.2.10">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.2.2.10.1">
<span class="ltx_p" id="S3.T5.2.2.2.10.1.1"><span class="ltx_text" id="S3.T5.2.2.2.10.1.1.1" style="font-size:90%;">57.7</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.2.2.11">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.2.2.11.1">
<span class="ltx_p" id="S3.T5.2.2.2.11.1.1"><span class="ltx_text" id="S3.T5.2.2.2.11.1.1.1" style="font-size:90%;">58.2</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T5.2.3.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T5.2.3.3.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.3.3.1.1">
<span class="ltx_p" id="S3.T5.2.3.3.1.1.1" style="width:78.2pt;"><span class="ltx_text" id="S3.T5.2.3.3.1.1.1.1" style="font-size:90%;">SelfPose </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S3.T5.2.3.3.1.1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib61" title=""><span class="ltx_text" style="font-size:90%;">61</span></a><span class="ltx_text" id="S3.T5.2.3.3.1.1.1.3.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.3.3.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.3.3.2.1">
<span class="ltx_p" id="S3.T5.2.3.3.2.1.1"><span class="ltx_text" id="S3.T5.2.3.3.2.1.1.1" style="font-size:90%;">60.4</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.3.3.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.3.3.3.1">
<span class="ltx_p" id="S3.T5.2.3.3.3.1.1"><span class="ltx_text" id="S3.T5.2.3.3.3.1.1.1" style="font-size:90%;">54.6</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T5.2.3.3.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.3.3.4.1">
<span class="ltx_p" id="S3.T5.2.3.3.4.1.1" style="width:42.7pt;"><span class="ltx_text" id="S3.T5.2.3.3.4.1.1.1" style="font-size:90%;">44.7</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.3.3.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.3.3.5.1">
<span class="ltx_p" id="S3.T5.2.3.3.5.1.1"><span class="ltx_text" id="S3.T5.2.3.3.5.1.1.1" style="font-size:90%;">56.5</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.3.3.6">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.3.3.6.1">
<span class="ltx_p" id="S3.T5.2.3.3.6.1.1"><span class="ltx_text" id="S3.T5.2.3.3.6.1.1.1" style="font-size:90%;">57.7</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.3.3.7">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.3.3.7.1">
<span class="ltx_p" id="S3.T5.2.3.3.7.1.1"><span class="ltx_text" id="S3.T5.2.3.3.7.1.1.1" style="font-size:90%;">52.7</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.3.3.8">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.3.3.8.1">
<span class="ltx_p" id="S3.T5.2.3.3.8.1.1"><span class="ltx_text" id="S3.T5.2.3.3.8.1.1.1" style="font-size:90%;">56.4</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.3.3.9">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.3.3.9.1">
<span class="ltx_p" id="S3.T5.2.3.3.9.1.1"><span class="ltx_text" id="S3.T5.2.3.3.9.1.1.1" style="font-size:90%;">53.6</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.3.3.10">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.3.3.10.1">
<span class="ltx_p" id="S3.T5.2.3.3.10.1.1"><span class="ltx_text" id="S3.T5.2.3.3.10.1.1.1" style="font-size:90%;">55.4</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.3.3.11">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.3.3.11.1">
<span class="ltx_p" id="S3.T5.2.3.3.11.1.1"><span class="ltx_text" id="S3.T5.2.3.3.11.1.1.1" style="font-size:90%;">54.7</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T5.2.4.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T5.2.4.4.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.4.4.1.1">
<span class="ltx_p" id="S3.T5.2.4.4.1.1.1" style="width:78.2pt;"><span class="ltx_text" id="S3.T5.2.4.4.1.1.1.1" style="font-size:90%;">Zhang et al. </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S3.T5.2.4.4.1.1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib76" title=""><span class="ltx_text" style="font-size:90%;">76</span></a><span class="ltx_text" id="S3.T5.2.4.4.1.1.1.3.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.4.4.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.4.4.2.1">
<span class="ltx_p" id="S3.T5.2.4.4.2.1.1"><span class="ltx_text" id="S3.T5.2.4.4.2.1.1.1" style="font-size:90%;">36.8</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.4.4.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.4.4.3.1">
<span class="ltx_p" id="S3.T5.2.4.4.3.1.1"><span class="ltx_text" id="S3.T5.2.4.4.3.1.1.1" style="font-size:90%;">34.1</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T5.2.4.4.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.4.4.4.1">
<span class="ltx_p" id="S3.T5.2.4.4.4.1.1" style="width:42.7pt;"><span class="ltx_text" id="S3.T5.2.4.4.4.1.1.1" style="font-size:90%;">36.7</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.4.4.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.4.4.5.1">
<span class="ltx_p" id="S3.T5.2.4.4.5.1.1"><span class="ltx_text" id="S3.T5.2.4.4.5.1.1.1" style="font-size:90%;">50.1</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.4.4.6">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.4.4.6.1">
<span class="ltx_p" id="S3.T5.2.4.4.6.1.1"><span class="ltx_text" id="S3.T5.2.4.4.6.1.1.1" style="font-size:90%;">57.2</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.4.4.7">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.4.4.7.1">
<span class="ltx_p" id="S3.T5.2.4.4.7.1.1"><span class="ltx_text" id="S3.T5.2.4.4.7.1.1.1" style="font-size:90%;">34.4</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.4.4.8">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.4.4.8.1">
<span class="ltx_p" id="S3.T5.2.4.4.8.1.1"><span class="ltx_text" id="S3.T5.2.4.4.8.1.1.1" style="font-size:90%;">32.8</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.4.4.9">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.4.4.9.1">
<span class="ltx_p" id="S3.T5.2.4.4.9.1.1"><span class="ltx_text" id="S3.T5.2.4.4.9.1.1.1" style="font-size:90%;">54.3</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.4.4.10">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.4.4.10.1">
<span class="ltx_p" id="S3.T5.2.4.4.10.1.1"><span class="ltx_text" id="S3.T5.2.4.4.10.1.1.1" style="font-size:90%;">52.6</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.4.4.11">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.4.4.11.1">
<span class="ltx_p" id="S3.T5.2.4.4.11.1.1"><span class="ltx_text" id="S3.T5.2.4.4.11.1.1.1" style="font-size:90%;">50.0</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T5.2.5.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T5.2.5.5.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.5.5.1.1">
<span class="ltx_p" id="S3.T5.2.5.5.1.1.1" style="width:78.2pt;"><span class="ltx_text" id="S3.T5.2.5.5.1.1.1.1" style="font-size:90%;">EgoFish3D </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S3.T5.2.5.5.1.1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">39</span></a><span class="ltx_text" id="S3.T5.2.5.5.1.1.1.3.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.5.5.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.5.5.2.1">
<span class="ltx_p" id="S3.T5.2.5.5.2.1.1"><span class="ltx_text" id="S3.T5.2.5.5.2.1.1.1" style="font-size:90%;">48.0</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.5.5.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.5.5.3.1">
<span class="ltx_p" id="S3.T5.2.5.5.3.1.1"><span class="ltx_text" id="S3.T5.2.5.5.3.1.1.1" style="font-size:90%;">48.2</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T5.2.5.5.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.5.5.4.1">
<span class="ltx_p" id="S3.T5.2.5.5.4.1.1" style="width:42.7pt;"><span class="ltx_text" id="S3.T5.2.5.5.4.1.1.1" style="font-size:90%;">42.5</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.5.5.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.5.5.5.1">
<span class="ltx_p" id="S3.T5.2.5.5.5.1.1"><span class="ltx_text" id="S3.T5.2.5.5.5.1.1.1" style="font-size:90%;">47.3</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.5.5.6">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.5.5.6.1">
<span class="ltx_p" id="S3.T5.2.5.5.6.1.1"><span class="ltx_text" id="S3.T5.2.5.5.6.1.1.1" style="font-size:90%;">48.8</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.5.5.7">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.5.5.7.1">
<span class="ltx_p" id="S3.T5.2.5.5.7.1.1"><span class="ltx_text" id="S3.T5.2.5.5.7.1.1.1" style="font-size:90%;">53.6</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.5.5.8">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.5.5.8.1">
<span class="ltx_p" id="S3.T5.2.5.5.8.1.1"><span class="ltx_text" id="S3.T5.2.5.5.8.1.1.1" style="font-size:90%;">47.2</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.5.5.9">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.5.5.9.1">
<span class="ltx_p" id="S3.T5.2.5.5.9.1.1"><span class="ltx_text" id="S3.T5.2.5.5.9.1.1.1" style="font-size:90%;">36.2</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.5.5.10">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.5.5.10.1">
<span class="ltx_p" id="S3.T5.2.5.5.10.1.1"><span class="ltx_text" id="S3.T5.2.5.5.10.1.1.1" style="font-size:90%;">48.9</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.5.5.11">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.5.5.11.1">
<span class="ltx_p" id="S3.T5.2.5.5.11.1.1"><span class="ltx_text" id="S3.T5.2.5.5.11.1.1.1" style="font-size:90%;">46.1</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T5.2.6.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T5.2.6.6.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.6.6.1.1">
<span class="ltx_p" id="S3.T5.2.6.6.1.1.1" style="width:78.2pt;"><span class="ltx_text" id="S3.T5.2.6.6.1.1.1.1" style="font-size:90%;">Ego-STAN </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S3.T5.2.6.6.1.1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib48" title=""><span class="ltx_text" style="font-size:90%;">48</span></a><span class="ltx_text" id="S3.T5.2.6.6.1.1.1.3.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.6.6.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.6.6.2.1">
<span class="ltx_p" id="S3.T5.2.6.6.2.1.1"><span class="ltx_text" id="S3.T5.2.6.6.2.1.1.1" style="font-size:90%;">33.1</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.6.6.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.6.6.3.1">
<span class="ltx_p" id="S3.T5.2.6.6.3.1.1"><span class="ltx_text" id="S3.T5.2.6.6.3.1.1.1" style="font-size:90%;">31.6</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T5.2.6.6.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.6.6.4.1">
<span class="ltx_p" id="S3.T5.2.6.6.4.1.1" style="width:42.7pt;"><span class="ltx_text" id="S3.T5.2.6.6.4.1.1.1" style="font-size:90%;">36.9</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.6.6.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.6.6.5.1">
<span class="ltx_p" id="S3.T5.2.6.6.5.1.1"><span class="ltx_text" id="S3.T5.2.6.6.5.1.1.1" style="font-size:90%;">38.9</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.6.6.6">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.6.6.6.1">
<span class="ltx_p" id="S3.T5.2.6.6.6.1.1"><span class="ltx_text" id="S3.T5.2.6.6.6.1.1.1" style="font-size:90%;">29.2</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.6.6.7">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.6.6.7.1">
<span class="ltx_p" id="S3.T5.2.6.6.7.1.1"><span class="ltx_text" id="S3.T5.2.6.6.7.1.1.1" style="font-size:90%;">29.6</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.6.6.8">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.6.6.8.1">
<span class="ltx_p" id="S3.T5.2.6.6.8.1.1"><span class="ltx_text" id="S3.T5.2.6.6.8.1.1.1" style="font-size:90%;">29.7</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.6.6.9">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.6.6.9.1">
<span class="ltx_p" id="S3.T5.2.6.6.9.1.1"><span class="ltx_text" id="S3.T5.2.6.6.9.1.1.1" style="font-size:90%;">44.3</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.6.6.10">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.6.6.10.1">
<span class="ltx_p" id="S3.T5.2.6.6.10.1.1"><span class="ltx_text" id="S3.T5.2.6.6.10.1.1.1" style="font-size:90%;">40.9</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T5.2.6.6.11">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.6.6.11.1">
<span class="ltx_p" id="S3.T5.2.6.6.11.1.1"><span class="ltx_text" id="S3.T5.2.6.6.11.1.1.1" style="font-size:90%;">40.4</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T5.2.7.7">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T5.2.7.7.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.7.7.1.1">
<span class="ltx_p" id="S3.T5.2.7.7.1.1.1" style="width:78.2pt;"><span class="ltx_text ltx_font_bold" id="S3.T5.2.7.7.1.1.1.1" style="font-size:90%;">EgoGlass</span><span class="ltx_text" id="S3.T5.2.7.7.1.1.1.2" style="font-size:90%;"> </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S3.T5.2.7.7.1.1.1.3.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib77" title=""><span class="ltx_text" style="font-size:90%;">77</span></a><span class="ltx_text" id="S3.T5.2.7.7.1.1.1.4.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S3.T5.2.7.7.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.7.7.2.1">
<span class="ltx_p" id="S3.T5.2.7.7.2.1.1"><span class="ltx_text" id="S3.T5.2.7.7.2.1.1.1" style="font-size:90%;">32.8</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S3.T5.2.7.7.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.7.7.3.1">
<span class="ltx_p" id="S3.T5.2.7.7.3.1.1"><span class="ltx_text" id="S3.T5.2.7.7.3.1.1.1" style="font-size:90%;">30.5</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S3.T5.2.7.7.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.7.7.4.1">
<span class="ltx_p" id="S3.T5.2.7.7.4.1.1" style="width:42.7pt;"><span class="ltx_text" id="S3.T5.2.7.7.4.1.1.1" style="font-size:90%;">33.7</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S3.T5.2.7.7.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.7.7.5.1">
<span class="ltx_p" id="S3.T5.2.7.7.5.1.1"><span class="ltx_text" id="S3.T5.2.7.7.5.1.1.1" style="font-size:90%;">35.5</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S3.T5.2.7.7.6">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.7.7.6.1">
<span class="ltx_p" id="S3.T5.2.7.7.6.1.1"><span class="ltx_text" id="S3.T5.2.7.7.6.1.1.1" style="font-size:90%;">45.7</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S3.T5.2.7.7.7">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.7.7.7.1">
<span class="ltx_p" id="S3.T5.2.7.7.7.1.1"><span class="ltx_text" id="S3.T5.2.7.7.7.1.1.1" style="font-size:90%;">33.2</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S3.T5.2.7.7.8">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.7.7.8.1">
<span class="ltx_p" id="S3.T5.2.7.7.8.1.1"><span class="ltx_text" id="S3.T5.2.7.7.8.1.1.1" style="font-size:90%;">27.0</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S3.T5.2.7.7.9">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.7.7.9.1">
<span class="ltx_p" id="S3.T5.2.7.7.9.1.1"><span class="ltx_text" id="S3.T5.2.7.7.9.1.1.1" style="font-size:90%;">40.1</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S3.T5.2.7.7.10">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.7.7.10.1">
<span class="ltx_p" id="S3.T5.2.7.7.10.1.1"><span class="ltx_text" id="S3.T5.2.7.7.10.1.1.1" style="font-size:90%;">37.4</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S3.T5.2.7.7.11">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.2.7.7.11.1">
<span class="ltx_p" id="S3.T5.2.7.7.11.1.1"><span class="ltx_text ltx_font_bold" id="S3.T5.2.7.7.11.1.1.1" style="font-size:90%;">37.7</span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 5: </span>Comparison of different skeletal based 3D egocentric pose estimation methods on xR-EgoPose dataset using MPJPE (mm).</figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Dittadi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> used variational autoencoders to generate human body poses from <span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">noisy head and hand pose data</span>. It addresses the challenge of predicting full body poses with limited information by training specialized inference models.
Yuan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib71" title=""><span class="ltx_text" style="font-size:90%;">71</span></a>]</cite> employed control-based modeling with physics simulation and used imitation learning to acquire a video-conditioned control policy for ego-pose estimation. Traditional computer vision methods focus solely on motion kinematics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib42" title=""><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite> neglecting the underlying physics of dynamics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib43" title=""><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite>. Taking this into account, this framework allows domain adaptation, transferring the policy from simulation to real-world data.
<span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.2">CoolMoves</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite> is a VR system that has achieved real-time, expressive full-body motion synthesis for a user’s avatar using limited input cues from VR systems. It delineates the prominent movements through dynamic fusion with stylized examples from skilled performers. The system excels in synthesizing upper and lower-body motions without explicit tracking cues, addressing challenges in activities like dancing and fighting.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">To solve the problem of <span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.1">top-down view distortions</span>, <span class="ltx_text ltx_font_italic" id="S3.SS2.p3.1.2">EgoRenderer</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite> renderes full-body avatars from egocentric images by decomposing the process into texture synthesis, pose construction, and neural image translation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib52" title=""><span class="ltx_text" style="font-size:90%;">52</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib58" title=""><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite>.
<span class="ltx_text ltx_font_italic" id="S3.SS2.p3.1.3">The Human POSEitioning System (HPS)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite> combines wearable sensors, IMUs, and a head-mounted camera to accurately track and integrate 3D human poses within pre-scanned environments. By fusing camera-based localization with IMU-based tracking and scene constraints, HPS achieves physically plausible motion estimation.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">To address challenges like <span class="ltx_text ltx_font_bold" id="S3.SS2.p4.1.1">body truncation</span> and <span class="ltx_text ltx_font_bold" id="S3.SS2.p4.1.2">pose ambiguities</span>, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib75" title=""><span class="ltx_text" style="font-size:90%;">75</span></a>]</cite> introduced a scene-conditioned diffusion model guided by a physics-based collision score, facilitating the generation of realistic human-scene interactions. It uses classifier-free training for flexibility in sampling, providing accurate estimations for visible body parts and diverse plausible results for unseen parts. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite> predicted full-body poses in world coordinates solely from motion input derived from the user’s head and hands. Leveraging a transformer encoder, the method extracts deep features, distinguishing global motion from local joint orientations to facilitate pose estimation.
<span class="ltx_text ltx_font_italic" id="S3.SS2.p4.1.3">FLAG</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite> uses sparse input signals from head mounted devices and a flow-based generative model to predict full-body poses and provide uncertainty estimates for joints.
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib54" title=""><span class="ltx_text" style="font-size:90%;">54</span></a>]</cite> estimated 3D wearer poses from egocentric video, overcoming challenges of <span class="ltx_text ltx_font_bold" id="S3.SS2.p4.1.4">body invisibility and complex motion</span>. They convert raw video to 3D pose, incorporating Self-Perception Excitation module for self understanding from egocentric view.</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1"><span class="ltx_text ltx_font_italic" id="S3.SS2.p5.1.1">EgoEgo</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib35" title=""><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite> uses monocular egocentric videos to estimate ego-head pose and generate ego-body pose, allowing <span class="ltx_text ltx_font_bold" id="S3.SS2.p5.1.2">independent learning without paired datasets</span>. It combines monocular SLAM and transformer-based models for accurate ego-head pose estimation, employing a conditional diffusion model for full-body pose generation based on the predicted head pose.
<span class="ltx_text ltx_font_italic" id="S3.SS2.p5.1.3">SimpleEgo</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite> performs regression of probabilistic full-body pose parameters from head-mounted camera images. It directly predicts joint rotations, eliminating the need for iterative fitting processes or manual tuning. By representing joint rotations as matrix Fisher distributions, the model predicts confidence scores, allowing for robust uncertainty estimation.
<span class="ltx_text ltx_font_italic" id="S3.SS2.p5.1.4">AGRoL</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite> proposes a lightweight MLP-based diffusion model for realistic full-body motion synthesis from sparse tracking signals. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite> introduces affordable motion capture using smartwatches and head-mounted camera, integrating head poses for sparsity, tracking floor levels for outdoor settings, and optimizing motion with visual cues.
<span class="ltx_text ltx_font_italic" id="S3.SS2.p5.1.5">EgoPoser</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite> generates full-body pose estimation using sparse motion sensors, focusing on HMD-based ego-body pose estimation in large scenes. It addresses overfitting issues by emphasizing position-invariant prediction with a Global-in-Local motion decomposition strategy. Notably, it adapts to diverse body sizes and remains robust when hands are out of view.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Evaluation Metrics</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we briefly describe the different metrics used to assess 3D egocentric human pose estimation methods.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.p2.1.1">MPJPE (Mean Per Joint Position Error)</span> is a widely utilized metric which measures the mean error between all the predicted 3D joint positions and the ground truth positions, by calculating the Euclidean distance between them.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.p3.1.1">PA-MPJPE</span> focuses on the individual pose accuracy by checking the alignment between the estimated pose and the ground truth pose of each frame using Procrustes analysis.</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.p4.1.1">BA-MPJPE</span> first resizes the bone lengths to a standard skeleton and then calculates the PA-MPJPE, providing a comprehensive evaluation by considering structural consistency in bone lengths and eliminating body scale influence.</p>
</div>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p" id="S4.p5.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.p5.1.1">Global MPJPE</span> evaluates global joint position accuracy by aligning all poses within a batch to the ground truth, considering translation and rotation.</p>
</div>
<div class="ltx_para" id="S4.p6">
<p class="ltx_p" id="S4.p6.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.p6.1.1">MPJRE (Mean Per Joint Rotation Error)</span> and <span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.p6.1.2">MPJVE (Mean Per Joint Velocity Error)</span> compares the predicted and ground truth joints by calculating the average rotational and velocity disparity respectively.</p>
</div>
<div class="ltx_para" id="S4.p7">
<p class="ltx_p" id="S4.p7.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.p7.1.1">Percentage of Correct Key-points (PCK)</span> is a measure of accuracy that checks if the predicted keypoint and the actual joint are close within a specific distance limit. Typically, this distance threshold is set based on the size of the subject.</p>
</div>
<div class="ltx_para" id="S4.p8">
<p class="ltx_p" id="S4.p8.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.p8.1.1">Head Translation &amp; Orientation Error</span> focuses on translation and rotational accuracy in head pose estimation respectively. The translation error is quantified using the mean Euclidean distance between predicted and ground truth head trajectories. Whereas, the orientation error is calculated using the Frobenius norm of the difference between the predicted and ground truth head rotation matrices.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Performance Analysis</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we compare the performance of different state-of-the-art methods for the 3D egocentric human pose estimation on some of the popular egocentric datasets.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.p2.1.1">Performance of Skeletal-based Methods: </span>
Table <a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#S3.T4" title="Table 4 ‣ 3.2 Body Shape Based Methods ‣ 3 3D Egocentric Pose Estimation Methods ‣ A Survey on 3D Egocentric Human Pose Estimation"><span class="ltx_text ltx_ref_tag">4</span></a> shows the performance of five different skeletal-based egocentric pose estimation methods across the different actions on the widely used Mo<sup class="ltx_sup" id="S5.p2.1.2">2</sup>Cap<sup class="ltx_sup" id="S5.p2.1.3">2</sup> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib68" title=""><span class="ltx_text" style="font-size:90%;">68</span></a>]</cite> dataset. The average MPJPE across all actions reduces from 66.8 mm in EgoFish3D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">39</span></a>]</cite> method to
46.61 mm in SelfPose-UNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib61" title=""><span class="ltx_text" style="font-size:90%;">61</span></a>]</cite> method. We see that 2D-3D lifting models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib61" title=""><span class="ltx_text" style="font-size:90%;">61</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">60</span></a>]</cite> achieved better results than direct 3D pose estimation methods, which may be due to the preserved uncertainty information of the joints.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#S5.F3" title="Figure 3 ‣ 5 Performance Analysis ‣ A Survey on 3D Egocentric Human Pose Estimation"><span class="ltx_text ltx_ref_tag">3</span></a> shows the qualitative evaluation of the 3D poses generated by three different skeletal-based methods on the xR-EgoPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">60</span></a>]</cite> dataset.
Table <a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#S3.T5" title="Table 5 ‣ 3.2 Body Shape Based Methods ‣ 3 3D Egocentric Pose Estimation Methods ‣ A Survey on 3D Egocentric Human Pose Estimation"><span class="ltx_text ltx_ref_tag">5</span></a> compares six different skeletal based methods on the xR-EgoPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">60</span></a>]</cite> dataset. Overall, the MPJPE of the methods is lower here than those tested in Mo<sup class="ltx_sup" id="S5.p2.1.4">2</sup>Cap<sup class="ltx_sup" id="S5.p2.1.5">2</sup> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib68" title=""><span class="ltx_text" style="font-size:90%;">68</span></a>]</cite> dataset, especially for actions with less visible joints. This could be because in this dataset most of the actions used for evaluation are relatively simpler.</p>
</div>
<figure class="ltx_figure" id="S5.F3"><svg class="ltx_picture ltx_centering" height="196.69" id="S5.F3.pic1" overflow="visible" version="1.1" width="595"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,196.69) matrix(1 0 0 -1 0 0) translate(-0.28,0) translate(0,19.41)"><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 0.28 0.28)"><foreignobject height="177" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="595"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="353" id="S5.F3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" src="x2.png" width="1190"/></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 14.27 -13.37)"><foreignobject height="8.5" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="52.2"><span class="ltx_text" id="S5.F3.pic1.2.2.2.1.1" style="font-size:70%;">Input Image</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 98.92 -12.84)"><foreignobject height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="135.63"><span class="ltx_text" id="S5.F3.pic1.3.3.3.1.1" style="font-size:70%;">Mo<sup class="ltx_sup" id="S5.F3.pic1.3.3.3.1.1.1">2</sup>Cap<sup class="ltx_sup" id="S5.F3.pic1.3.3.3.1.1.2">2</sup> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib68" title=""><span class="ltx_text" style="font-size:90%;">68</span></a>]</cite></span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 173.72 -12.84)"><foreignobject height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="150.43"><span class="ltx_text" id="S5.F3.pic1.4.4.4.1.1" style="font-size:70%;">xR-EgoPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">60</span></a>]</cite></span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 252.46 -12.84)"><foreignobject height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="172.3"><span class="ltx_text" id="S5.F3.pic1.5.5.5.1.1" style="font-size:70%;">Hwang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite></span></foreignobject></g></g></svg>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Qualitative comparison between three different state-of-the-art skeletal-based egocentric 3D pose estimation models on the xR-EgoPose dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">60</span></a>]</cite>. The predicted 3D poses (red) are superimposed onto the ground truth poses (blue).</figcaption>
</figure>
<figure class="ltx_table" id="S5.T6">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T6.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T6.2.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S5.T6.2.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.2.1.1.1.1">
<span class="ltx_p" id="S5.T6.2.1.1.1.1.1" style="width:85.4pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.2.1.1.1.1.1.1" style="font-size:90%;">Methods</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T6.2.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.2.1.1.2.1">
<span class="ltx_p" id="S5.T6.2.1.1.2.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.2.1.1.2.1.1.1" style="font-size:90%;">MPJPE</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T6.2.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.2.1.1.3.1">
<span class="ltx_p" id="S5.T6.2.1.1.3.1.1"><span class="ltx_text ltx_font_bold" id="S5.T6.2.1.1.3.1.1.1" style="font-size:90%;">MPJVE</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T6.2.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S5.T6.2.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.2.2.1.1.1">
<span class="ltx_p" id="S5.T6.2.2.1.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="S5.T6.2.2.1.1.1.1.1" style="font-size:90%;">CoolMoves </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S5.T6.2.2.1.1.1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2</span></a><span class="ltx_text" id="S5.T6.2.2.1.1.1.1.3.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S5.T6.2.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.2.2.1.2.1">
<span class="ltx_p" id="S5.T6.2.2.1.2.1.1" style="width:56.9pt;"><span class="ltx_text" id="S5.T6.2.2.1.2.1.1.1" style="font-size:90%;">7.83</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S5.T6.2.2.1.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.2.2.1.3.1">
<span class="ltx_p" id="S5.T6.2.2.1.3.1.1"><span class="ltx_text" id="S5.T6.2.2.1.3.1.1.1" style="font-size:90%;">100.54</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T6.2.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S5.T6.2.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.2.3.2.1.1">
<span class="ltx_p" id="S5.T6.2.3.2.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="S5.T6.2.3.2.1.1.1.1" style="font-size:90%;">Lee et al. </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S5.T6.2.3.2.1.1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">34</span></a><span class="ltx_text" id="S5.T6.2.3.2.1.1.1.3.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S5.T6.2.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.2.3.2.2.1">
<span class="ltx_p" id="S5.T6.2.3.2.2.1.1" style="width:56.9pt;"><span class="ltx_text" id="S5.T6.2.3.2.2.1.1.1" style="font-size:90%;">5.87</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S5.T6.2.3.2.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.2.3.2.3.1">
<span class="ltx_p" id="S5.T6.2.3.2.3.1.1"><span class="ltx_text" id="S5.T6.2.3.2.3.1.1.1" style="font-size:90%;">19.11</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T6.2.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S5.T6.2.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.2.4.3.1.1">
<span class="ltx_p" id="S5.T6.2.4.3.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="S5.T6.2.4.3.1.1.1.1" style="font-size:90%;">AvatarPoser </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S5.T6.2.4.3.1.1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">30</span></a><span class="ltx_text" id="S5.T6.2.4.3.1.1.1.3.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S5.T6.2.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.2.4.3.2.1">
<span class="ltx_p" id="S5.T6.2.4.3.2.1.1" style="width:56.9pt;"><span class="ltx_text" id="S5.T6.2.4.3.2.1.1.1" style="font-size:90%;">4.18</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S5.T6.2.4.3.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.2.4.3.3.1">
<span class="ltx_p" id="S5.T6.2.4.3.3.1.1"><span class="ltx_text" id="S5.T6.2.4.3.3.1.1.1" style="font-size:90%;">29.40</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T6.2.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S5.T6.2.5.4.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.2.5.4.1.1">
<span class="ltx_p" id="S5.T6.2.5.4.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="S5.T6.2.5.4.1.1.1.1" style="font-size:90%;">EgoPoser </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S5.T6.2.5.4.1.1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">31</span></a><span class="ltx_text" id="S5.T6.2.5.4.1.1.1.3.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S5.T6.2.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.2.5.4.2.1">
<span class="ltx_p" id="S5.T6.2.5.4.2.1.1" style="width:56.9pt;"><span class="ltx_text" id="S5.T6.2.5.4.2.1.1.1" style="font-size:90%;">4.14</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S5.T6.2.5.4.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.2.5.4.3.1">
<span class="ltx_p" id="S5.T6.2.5.4.3.1.1"><span class="ltx_text" id="S5.T6.2.5.4.3.1.1.1" style="font-size:90%;">25.95</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T6.2.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S5.T6.2.6.5.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.2.6.5.1.1">
<span class="ltx_p" id="S5.T6.2.6.5.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="S5.T6.2.6.5.1.1.1.1" style="font-size:90%;">AGRoL </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S5.T6.2.6.5.1.1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">15</span></a><span class="ltx_text" id="S5.T6.2.6.5.1.1.1.3.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S5.T6.2.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.2.6.5.2.1">
<span class="ltx_p" id="S5.T6.2.6.5.2.1.1" style="width:56.9pt;"><span class="ltx_text" id="S5.T6.2.6.5.2.1.1.1" style="font-size:90%;">3.86</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S5.T6.2.6.5.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.2.6.5.3.1">
<span class="ltx_p" id="S5.T6.2.6.5.3.1.1"><span class="ltx_text" id="S5.T6.2.6.5.3.1.1.1" style="font-size:90%;">50.94</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T6.2.7.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S5.T6.2.7.6.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.2.7.6.1.1">
<span class="ltx_p" id="S5.T6.2.7.6.1.1.1" style="width:85.4pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.2.7.6.1.1.1.1" style="font-size:90%;">AGRoL-Offline</span><span class="ltx_text" id="S5.T6.2.7.6.1.1.1.2" style="font-size:90%;"> </span><cite class="ltx_cite ltx_centering ltx_citemacro_cite"><span class="ltx_text" id="S5.T6.2.7.6.1.1.1.3.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">15</span></a><span class="ltx_text" id="S5.T6.2.7.6.1.1.1.4.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S5.T6.2.7.6.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.2.7.6.2.1">
<span class="ltx_p" id="S5.T6.2.7.6.2.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.2.7.6.2.1.1.1" style="font-size:90%;">3.71</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S5.T6.2.7.6.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.2.7.6.3.1">
<span class="ltx_p" id="S5.T6.2.7.6.3.1.1"><span class="ltx_text ltx_font_bold" id="S5.T6.2.7.6.3.1.1.1" style="font-size:90%;">18.59</span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 6: </span>Comparison of different body shape based 3D Egocentric Pose Estimation methods on AMASS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib44" title=""><span class="ltx_text" style="font-size:90%;">44</span></a>]</cite> dataset using MPJPE (cm) and MPJVE (cm/s).</figcaption>
</figure>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.p3.1.1">Performance of body shape based Methods: </span>
Table <a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#S5.T6" title="Table 6 ‣ 5 Performance Analysis ‣ A Survey on 3D Egocentric Human Pose Estimation"><span class="ltx_text ltx_ref_tag">6</span></a> shows the performance of six different body shape based egocentric pose estimation methods on the AMASS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib44" title=""><span class="ltx_text" style="font-size:90%;">44</span></a>]</cite> dataset. We can see that, AGRoL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite> outperforms other methods with its smooth motion generation, but it’s limited to offline use. For real-time applications, EgoPoser <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite> is more suitable as it provides more adaptability to diverse body sizes as well as robustness with hands out of view.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#S5.F4" title="Figure 4 ‣ 5 Performance Analysis ‣ A Survey on 3D Egocentric Human Pose Estimation"><span class="ltx_text ltx_ref_tag">4</span></a> qualitatively compares the 3D human pose and shape on three different body shape based methods using HPS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite> dataset.</p>
</div>
<figure class="ltx_figure" id="S5.F4"><svg class="ltx_picture ltx_centering" height="106.69" id="S5.F4.pic1" overflow="visible" version="1.1" width="595"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,106.69) matrix(1 0 0 -1 0 0) translate(-0.28,0) translate(0,19.41)"><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 0.28 0.28)"><foreignobject height="87" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="595"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="173" id="S5.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" src="x3.png" width="1190"/></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 5.21 -13.37)"><foreignobject height="8.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="83.26"><span class="ltx_text" id="S5.F4.pic1.2.2.2.1.1" style="font-size:70%;">Groundtruth Image</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 93.01 -12.84)"><foreignobject height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="159.12"><span class="ltx_text" id="S5.F4.pic1.3.3.3.1.1" style="font-size:70%;">AvatarPoser <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite></span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 175.69 -12.84)"><foreignobject height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="109.02"><span class="ltx_text" id="S5.F4.pic1.4.4.4.1.1" style="font-size:70%;">AgRoL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite></span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 256.4 -12.84)"><foreignobject height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="133.91"><span class="ltx_text" id="S5.F4.pic1.5.5.5.1.1" style="font-size:70%;">EgoPoser <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite></span></foreignobject></g></g></svg>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Qualitative comparison of three different state-of-the-art body shape based egocentric 3D pose estimation models on the HPS dataset. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite>.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion and Future Directions</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this survey paper, we provide an overview of 3D egocentric human pose estimation using RGB images or video sequences, encompassing diverse datasets and estimation methodologies.
Researchers have proposed diverse datasets with lightweight setups. However, the lack of standardized <span class="ltx_text ltx_font_bold" id="S6.p1.1.1">benchmark datasets</span>, except for the recent Ego-Exo4D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.17893v2#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite> dataset, poses a challenge for evaluating the robustness of different egocentric pose estimation models.
While discussing the individual strengths and weaknesses of different skeletal and body shape based methods for egocentric pose estimation, we realize that most of the existing methods encounter difficulties with <span class="ltx_text ltx_font_bold" id="S6.p1.1.2">in-the-wild scenarios</span> mainly due to insufficient training data.
Notably, similar to traditional pose estimation, the biggest challenges of egocentric pose estimation models are strong occlusions and limited field of view, especially for the lower body joints. <span class="ltx_text ltx_font_bold" id="S6.p1.1.3">Multi view consistency</span> may help to to solve this using additional 3D information. Moreover, <span class="ltx_text ltx_font_bold" id="S6.p1.1.4">temporal and contextual information</span> can be utilized further to improve the robustness of the models considering these issues.
Consequently, there exists ample scope for refining egocentric pose estimation approaches to better suit <span class="ltx_text ltx_font_bold" id="S6.p1.1.5">real-time</span> technologies.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">In conclusion, this survey paper serves as a comprehensive resource for researchers seeking to explore the existing egocentric pose estimation methods, understand prevalent challenges, and make further advancements.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Acknowledgements</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">This material is partially based upon work supported by the National Science Foundation under Grant No. 2153249 and 2316240. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib1.3.3.1" style="font-size:90%;">[1]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.5.1" style="font-size:90%;">
CMU Motion Capture Database.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://mocap.cs.cmu.edu" style="font-size:90%;" title="">http://mocap.cs.cmu.edu</a><span class="ltx_text" id="bib.bib1.6.1" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.7.1" style="font-size:90%;">Accessed: March 2, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib2.5.5.1" style="font-size:90%;">Ahuja et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.7.1" style="font-size:90%;">
Karan Ahuja, Eyal Ofek, Mar Gonzalez-Franco, Christian Holz, and Andrew D Wilson.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.8.1" style="font-size:90%;">Coolmoves: User motion accentuation in virtual reality.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.9.1" style="font-size:90%;">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</em><span class="ltx_text" id="bib.bib2.10.2" style="font-size:90%;">, 5(2):1–23, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib3.5.5.1" style="font-size:90%;">Akada et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.7.1" style="font-size:90%;">
Hiroyasu Akada, Jian Wang, Soshi Shimada, Masaki Takahashi, Christian Theobalt, and Vladislav Golyanik.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.8.1" style="font-size:90%;">Unrealego: A new dataset for robust egocentric 3d human motion capture.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib3.10.2" style="font-size:90%;">European Conference on Computer Vision</em><span class="ltx_text" id="bib.bib3.11.3" style="font-size:90%;">, pages 1–17. Springer, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib4.5.5.1" style="font-size:90%;">Akada et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.7.1" style="font-size:90%;">
Hiroyasu Akada, Jian Wang, Vladislav Golyanik, and Christian Theobalt.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.8.1" style="font-size:90%;">3d human pose perception from egocentric stereo videos, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib5.5.5.1" style="font-size:90%;">Aliakbarian et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.7.1" style="font-size:90%;">
Sadegh Aliakbarian, Pashmina Cameron, Federica Bogo, Andrew Fitzgibbon, and Thomas J Cashman.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.8.1" style="font-size:90%;">Flag: Flow-based 3d avatar generation from sparse observations.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib5.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib5.11.3" style="font-size:90%;">, pages 13253–13262, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib6.5.5.1" style="font-size:90%;">Andriluka et al. [2014]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.7.1" style="font-size:90%;">
Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and Bernt Schiele.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.8.1" style="font-size:90%;">2d human pose estimation: New benchmark and state of the art analysis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib6.10.2" style="font-size:90%;">Proceedings of the IEEE Conference on computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib6.11.3" style="font-size:90%;">, pages 3686–3693, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib7.4.4.1" style="font-size:90%;">Bandini and Zariffa [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.6.1" style="font-size:90%;">
Andrea Bandini and José Zariffa.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.7.1" style="font-size:90%;">Analysis of the hands in egocentric vision: A survey.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.8.1" style="font-size:90%;">IEEE transactions on pattern analysis and machine intelligence</em><span class="ltx_text" id="bib.bib7.9.2" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib8.5.5.1" style="font-size:90%;">Bogo et al. [2016]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.7.1" style="font-size:90%;">
Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, and Michael J Black.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.8.1" style="font-size:90%;">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib8.10.2" style="font-size:90%;">Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part V 14</em><span class="ltx_text" id="bib.bib8.11.3" style="font-size:90%;">, pages 561–578. Springer, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib9.5.5.1" style="font-size:90%;">Cao et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.7.1" style="font-size:90%;">
Zhe Cao, Hang Gao, Karttikeya Mangalam, Qi-Zhi Cai, Minh Vo, and Jitendra Malik.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.8.1" style="font-size:90%;">Long-term human motion prediction with scene context.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib9.10.2" style="font-size:90%;">Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part I 16</em><span class="ltx_text" id="bib.bib9.11.3" style="font-size:90%;">, pages 387–404. Springer, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib10.5.5.1" style="font-size:90%;">Cao et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.7.1" style="font-size:90%;">
Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.8.1" style="font-size:90%;">Openpose: Realtime multi-person 2d pose estimation using part affinity fields.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.9.1" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</em><span class="ltx_text" id="bib.bib10.10.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib11.5.5.1" style="font-size:90%;">Cuevas-Velasquez et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.7.1" style="font-size:90%;">
Hanz Cuevas-Velasquez, Charlie Hewitt, Sadegh Aliakbarian, and Tadas Baltrušaitis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.8.1" style="font-size:90%;">Simpleego: Predicting probabilistic body pose from egocentric cameras.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.9.1" style="font-size:90%;">arXiv preprint arXiv:2401.14785</em><span class="ltx_text" id="bib.bib11.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib12.5.5.1" style="font-size:90%;">Dhamanaskar et al. [2023a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.7.1" style="font-size:90%;">
Ameya Dhamanaskar, Mariella Dimiccoli, Enric Corona, Albert Pumarola, and Francesc Moreno-Noguer.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.8.1" style="font-size:90%;">Enhancing egocentric 3d pose estimation with third person views.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.9.1" style="font-size:90%;">Pattern Recognition</em><span class="ltx_text" id="bib.bib12.10.2" style="font-size:90%;">, 138:109358, 2023a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib13.5.5.1" style="font-size:90%;">Dhamanaskar et al. [2023b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.7.1" style="font-size:90%;">
Ameya Dhamanaskar, Mariella Dimiccoli, Enric Corona, Albert Pumarola, and Francesc Moreno-Noguer.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.8.1" style="font-size:90%;">Enhancing egocentric 3d pose estimation with third person views.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.9.1" style="font-size:90%;">Pattern Recognition</em><span class="ltx_text" id="bib.bib13.10.2" style="font-size:90%;">, 138:109358, 2023b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib14.5.5.1" style="font-size:90%;">Dittadi et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.7.1" style="font-size:90%;">
Andrea Dittadi, Sebastian Dziadzio, Darren Cosker, Ben Lundell, Thomas J Cashman, and Jamie Shotton.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.8.1" style="font-size:90%;">Full-body motion from a single head-mounted device: Generating smpl poses from partial observations.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib14.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span class="ltx_text" id="bib.bib14.11.3" style="font-size:90%;">, pages 11687–11697, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib15.5.5.1" style="font-size:90%;">Du et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.7.1" style="font-size:90%;">
Yuming Du, Robin Kips, Albert Pumarola, Sebastian Starke, Ali Thabet, and Artsiom Sanakoyeu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.8.1" style="font-size:90%;">Avatars grow legs: Generating smooth human motion from sparse tracking inputs with diffusion model.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib15.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span class="ltx_text" id="bib.bib15.11.3" style="font-size:90%;">, pages 481–490, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib16.4.4.1" style="font-size:90%;">Gamra and Akhloufi [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.6.1" style="font-size:90%;">
Miniar Ben Gamra and Moulay A Akhloufi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.7.1" style="font-size:90%;">A review of deep learning techniques for 2d and 3d human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.8.1" style="font-size:90%;">Image and Vision Computing</em><span class="ltx_text" id="bib.bib16.9.2" style="font-size:90%;">, 114:104282, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib17.5.5.1" style="font-size:90%;">Garrido-Jurado et al. [2014]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.7.1" style="font-size:90%;">
Sergio Garrido-Jurado, Rafael Muñoz-Salinas, Francisco José Madrid-Cuevas, and Manuel Jesús Marín-Jiménez.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.8.1" style="font-size:90%;">Automatic generation and detection of highly reliable fiducial markers under occlusion.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.9.1" style="font-size:90%;">Pattern Recognition</em><span class="ltx_text" id="bib.bib17.10.2" style="font-size:90%;">, 47(6):2280–2292, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib18.5.5.1" style="font-size:90%;">Grauman et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.7.1" style="font-size:90%;">
Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.8.1" style="font-size:90%;">Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.9.1" style="font-size:90%;">arXiv preprint arXiv:2311.18259</em><span class="ltx_text" id="bib.bib18.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib19.5.5.1" style="font-size:90%;">Güler et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.7.1" style="font-size:90%;">
Rıza Alp Güler, Natalia Neverova, and Iasonas Kokkinos.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.8.1" style="font-size:90%;">Densepose: Dense human pose estimation in the wild.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib19.10.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span class="ltx_text" id="bib.bib19.11.3" style="font-size:90%;">, pages 7297–7306, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib20.5.5.1" style="font-size:90%;">Guzov et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.7.1" style="font-size:90%;">
Vladimir Guzov, Aymen Mir, Torsten Sattler, and Gerard Pons-Moll.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.8.1" style="font-size:90%;">Human poseitioning system (hps): 3d human pose estimation and self-localization in large scenes from body-mounted sensors.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib20.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib20.11.3" style="font-size:90%;">, pages 4318–4329, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib21.5.5.1" style="font-size:90%;">Hu et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.7.1" style="font-size:90%;">
Tao Hu, Kripasindhu Sarkar, Lingjie Liu, Matthias Zwicker, and Christian Theobalt.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.8.1" style="font-size:90%;">Egorenderer: Rendering human avatars from egocentric camera images.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib21.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span class="ltx_text" id="bib.bib21.11.3" style="font-size:90%;">, pages 14528–14538, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib22.5.5.1" style="font-size:90%;">Hu et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.7.1" style="font-size:90%;">
Tao Hu, Tao Yu, Zerong Zheng, He Zhang, Yebin Liu, and Matthias Zwicker.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.8.1" style="font-size:90%;">Hvtr: Hybrid volumetric-textural rendering for human avatars.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib22.10.2" style="font-size:90%;">2022 International Conference on 3D Vision (3DV)</em><span class="ltx_text" id="bib.bib22.11.3" style="font-size:90%;">, pages 197–208. IEEE, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib23.5.5.1" style="font-size:90%;">Huang et al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.7.1" style="font-size:90%;">
Yinghao Huang, Federica Bogo, Christoph Lassner, Angjoo Kanazawa, Peter V Gehler, Javier Romero, Ijaz Akhter, and Michael J Black.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.8.1" style="font-size:90%;">Towards accurate marker-less human shape and pose estimation over time.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib23.10.2" style="font-size:90%;">2017 international conference on 3D vision (3DV)</em><span class="ltx_text" id="bib.bib23.11.3" style="font-size:90%;">, pages 421–430. IEEE, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib24.5.5.1" style="font-size:90%;">Huang et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.7.1" style="font-size:90%;">
Zeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, and Tony Tung.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.8.1" style="font-size:90%;">Arch: Animatable reconstruction of clothed humans.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib24.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib24.11.3" style="font-size:90%;">, pages 3093–3102, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib25.4.4.1" style="font-size:90%;">Hwang and Kang [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.6.1" style="font-size:90%;">
Juheon Hwang and Jiwoo Kang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.7.1" style="font-size:90%;">Double discrete representation for 3d human pose estimation from head-mounted camera.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.8.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib25.9.2" style="font-size:90%;">2024 IEEE International Conference on Consumer Electronics (ICCE)</em><span class="ltx_text" id="bib.bib25.10.3" style="font-size:90%;">, pages 1–4. IEEE, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib26.5.5.1" style="font-size:90%;">Ionescu et al. [2013]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.7.1" style="font-size:90%;">
Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.8.1" style="font-size:90%;">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.9.1" style="font-size:90%;">IEEE transactions on pattern analysis and machine intelligence</em><span class="ltx_text" id="bib.bib26.10.2" style="font-size:90%;">, 36(7):1325–1339, 2013.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib27.5.5.1" style="font-size:90%;">Ji et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.7.1" style="font-size:90%;">
Xiaopeng Ji, Qi Fang, Junting Dong, Qing Shuai, Wen Jiang, and Xiaowei Zhou.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.8.1" style="font-size:90%;">A survey on monocular 3d human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.9.1" style="font-size:90%;">Virtual Reality &amp; Intelligent Hardware</em><span class="ltx_text" id="bib.bib27.10.2" style="font-size:90%;">, 2:471–500, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib28.4.4.1" style="font-size:90%;">Jiang and Grauman [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.6.1" style="font-size:90%;">
Hao Jiang and Kristen Grauman.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.7.1" style="font-size:90%;">Seeing invisible poses: Estimating 3d body pose from egocentric video.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.8.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib28.9.2" style="font-size:90%;">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em><span class="ltx_text" id="bib.bib28.10.3" style="font-size:90%;">, pages 3501–3509. IEEE, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib29.4.4.1" style="font-size:90%;">Jiang and Ithapu [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.6.1" style="font-size:90%;">
Hao Jiang and Vamsi Krishna Ithapu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.7.1" style="font-size:90%;">Egocentric pose estimation from human vision span.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.8.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib29.9.2" style="font-size:90%;">2021 IEEE/CVF International Conference on Computer Vision (ICCV)</em><span class="ltx_text" id="bib.bib29.10.3" style="font-size:90%;">, pages 10986–10994. IEEE, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib30.5.5.1" style="font-size:90%;">Jiang et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.7.1" style="font-size:90%;">
Jiaxi Jiang, Paul Streli, Huajian Qiu, Andreas Fender, Larissa Laich, Patrick Snape, and Christian Holz.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.8.1" style="font-size:90%;">Avatarposer: Articulated full-body pose tracking from sparse motion sensing.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib30.10.2" style="font-size:90%;">European Conference on Computer Vision</em><span class="ltx_text" id="bib.bib30.11.3" style="font-size:90%;">, pages 443–460. Springer, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib31.5.5.1" style="font-size:90%;">Jiang et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.7.1" style="font-size:90%;">
Jiaxi Jiang, Paul Streli, Manuel Meier, and Christian Holz.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.8.1" style="font-size:90%;">Egoposer: Robust real-time ego-body pose estimation in large scenes, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib32.4.4.1" style="font-size:90%;">Kang and Lee [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.6.1" style="font-size:90%;">
Taeho Kang and Youngki Lee.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.7.1" style="font-size:90%;">Attention-propagation network for egocentric heatmap to 3d pose lifting.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.8.1" style="font-size:90%;">arXiv preprint arXiv:2402.18330</em><span class="ltx_text" id="bib.bib32.9.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib33.5.5.1" style="font-size:90%;">Kang et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.7.1" style="font-size:90%;">
Taeho Kang, Kyungjin Lee, Jinrui Zhang, and Youngki Lee.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.8.1" style="font-size:90%;">Ego3dpose: Capturing 3d cues from binocular egocentric views.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib33.10.2" style="font-size:90%;">SIGGRAPH Asia 2023 Conference Papers</em><span class="ltx_text" id="bib.bib33.11.3" style="font-size:90%;">, pages 1–10, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib34.4.4.1" style="font-size:90%;">Lee and Joo [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.6.1" style="font-size:90%;">
Jiye Lee and Hanbyul Joo.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.7.1" style="font-size:90%;">Mocap everyone everywhere: Lightweight motion capture with smartwatches and a head-mounted camera.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.8.1" style="font-size:90%;">arXiv preprint arXiv:2401.00847</em><span class="ltx_text" id="bib.bib34.9.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib35.5.5.1" style="font-size:90%;">Li et al. [2023a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.7.1" style="font-size:90%;">
Jiaman Li, Karen Liu, and Jiajun Wu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.8.1" style="font-size:90%;">Ego-body pose estimation via ego-head pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib35.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib35.11.3" style="font-size:90%;">, pages 17142–17151, 2023a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib36.5.5.1" style="font-size:90%;">Li et al. [2023b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.7.1" style="font-size:90%;">
Tianyi Li, Chi Zhang, Wei Su, and Yuehu Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.8.1" style="font-size:90%;">Egoformer: Transformer-based motion context learning for ego-pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib36.10.2" style="font-size:90%;">2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC)</em><span class="ltx_text" id="bib.bib36.11.3" style="font-size:90%;">, pages 4052–4057. IEEE, 2023b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib37.5.5.1" style="font-size:90%;">Li et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.7.1" style="font-size:90%;">
Yanghao Li, Tushar Nagarajan, Bo Xiong, and Kristen Grauman.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.8.1" style="font-size:90%;">Ego-exo: Transferring visual representations from third-person to first-person videos.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib37.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib37.11.3" style="font-size:90%;">, pages 6943–6953, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib38.5.5.1" style="font-size:90%;">Liu et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.7.1" style="font-size:90%;">
Yuxuan Liu, Jianxin Yang, Xiao Gu, Yao Guo, and Guang-Zhong Yang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.8.1" style="font-size:90%;">Ego+ x: An egocentric vision system for global 3d human pose estimation and social interaction characterization.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib38.10.2" style="font-size:90%;">2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em><span class="ltx_text" id="bib.bib38.11.3" style="font-size:90%;">, pages 5271–5277. IEEE, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib39.5.5.1" style="font-size:90%;">Liu et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.7.1" style="font-size:90%;">
Yuxuan Liu, Jianxin Yang, Xiao Gu, Yijun Chen, Yao Guo, and Guang-Zhong Yang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.8.1" style="font-size:90%;">Egofish3d: Egocentric 3d pose estimation from a fisheye camera via self-supervised learning.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.9.1" style="font-size:90%;">IEEE Transactions on Multimedia</em><span class="ltx_text" id="bib.bib39.10.2" style="font-size:90%;">, 25:8880–8891, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib40.5.5.1" style="font-size:90%;">Liu et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.7.1" style="font-size:90%;">
Yang Liu, Changzhen Qiu, and Zhiyong Zhang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.8.1" style="font-size:90%;">Deep learning for 3d human pose estimation and mesh recovery: A survey.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.9.1" style="font-size:90%;">arXiv preprint arXiv:2402.18844</em><span class="ltx_text" id="bib.bib40.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib41.5.5.1" style="font-size:90%;">Loper et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.7.1" style="font-size:90%;">
Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J Black.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.8.1" style="font-size:90%;">Smpl: A skinned multi-person linear model.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib41.10.2" style="font-size:90%;">Seminal Graphics Papers: Pushing the Boundaries, Volume 2</em><span class="ltx_text" id="bib.bib41.11.3" style="font-size:90%;">, pages 851–866. 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib42.5.5.1" style="font-size:90%;">Luo et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.7.1" style="font-size:90%;">
Zhengyi Luo, Ryo Hachiuma, Ye Yuan, Shun Iwase, and Kris M Kitani.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.8.1" style="font-size:90%;">Kinematics-guided reinforcement learning for object-aware 3d ego-pose estimation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.9.1" style="font-size:90%;">arXiv preprint arXiv:2011.04837</em><span class="ltx_text" id="bib.bib42.10.2" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib43.5.5.1" style="font-size:90%;">Luo et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.7.1" style="font-size:90%;">
Zhengyi Luo, Ryo Hachiuma, Ye Yuan, and Kris Kitani.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.8.1" style="font-size:90%;">Dynamics-regulated kinematic policy for egocentric pose estimation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.9.1" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span class="ltx_text" id="bib.bib43.10.2" style="font-size:90%;">, 34:25019–25032, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib44.5.5.1" style="font-size:90%;">Mahmood et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.7.1" style="font-size:90%;">
Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Gerard Pons-Moll, and Michael J Black.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.8.1" style="font-size:90%;">Amass: Archive of motion capture as surface shapes.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib44.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on computer vision</em><span class="ltx_text" id="bib.bib44.11.3" style="font-size:90%;">, pages 5442–5451, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib45.5.5.1" style="font-size:90%;">Munea et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.7.1" style="font-size:90%;">
Tewodros Legesse Munea, Yalew Zelalem Jembre, Halefom Tekle Weldegebriel, Longbiao Chen, Chenxi Huang, and Chenhui Yang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.8.1" style="font-size:90%;">The progress of human pose estimation: A survey and taxonomy of models applied in 2d human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.9.1" style="font-size:90%;">IEEE Access</em><span class="ltx_text" id="bib.bib45.10.2" style="font-size:90%;">, 8:133330–133348, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib46.5.5.1" style="font-size:90%;">Ng et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.7.1" style="font-size:90%;">
Evonne Ng, Donglai Xiang, Hanbyul Joo, and Kristen Grauman.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.8.1" style="font-size:90%;">You2me: Inferring body pose in egocentric video via first and second person interactions.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib46.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib46.11.3" style="font-size:90%;">, pages 9890–9900, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib47.5.5.1" style="font-size:90%;">Núñez-Marcos et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.7.1" style="font-size:90%;">
Adrián Núñez-Marcos, Gorka Azkune, and Ignacio Arganda-Carreras.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.8.1" style="font-size:90%;">Egocentric vision-based action recognition: A survey.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.9.1" style="font-size:90%;">Neurocomputing</em><span class="ltx_text" id="bib.bib47.10.2" style="font-size:90%;">, 472:175–197, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib48.5.5.1" style="font-size:90%;">Park et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.7.1" style="font-size:90%;">
Jinman Park, Kimathi Kaai, Saad Hossain, Norikatsu Sumi, Sirisha Rambhatla, and Paul Fieguth.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.8.1" style="font-size:90%;">Domain-guided spatio-temporal self-attention for egocentric 3d pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib48.10.2" style="font-size:90%;">Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em><span class="ltx_text" id="bib.bib48.11.3" style="font-size:90%;">, pages 1837–1849, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib49.5.5.1" style="font-size:90%;">Pavlakos et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.7.1" style="font-size:90%;">
Georgios Pavlakos, Xiaowei Zhou, and Kostas Daniilidis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.8.1" style="font-size:90%;">Ordinal depth supervision for 3d human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib49.10.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span class="ltx_text" id="bib.bib49.11.3" style="font-size:90%;">, pages 7307–7316, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib50.5.5.1" style="font-size:90%;">Pavlakos et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.7.1" style="font-size:90%;">
Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and Michael J Black.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.8.1" style="font-size:90%;">Expressive body capture: 3d hands, face, and body from a single image.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib50.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em><span class="ltx_text" id="bib.bib50.11.3" style="font-size:90%;">, pages 10975–10985, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib51.5.5.1" style="font-size:90%;">Rhodin et al. [2016]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib51.7.1" style="font-size:90%;">
Helge Rhodin, Christian Richardt, Dan Casas, Eldar Insafutdinov, Mohammad Shafiei, Hans-Peter Seidel, Bernt Schiele, and Christian Theobalt.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib51.8.1" style="font-size:90%;">Egocap: egocentric marker-less motion capture with two fisheye cameras.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.9.1" style="font-size:90%;">ACM Transactions on Graphics (TOG)</em><span class="ltx_text" id="bib.bib51.10.2" style="font-size:90%;">, 35(6):1–11, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib52.5.5.1" style="font-size:90%;">Sitzmann et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib52.7.1" style="font-size:90%;">
Vincent Sitzmann, Michael Zollhöfer, and Gordon Wetzstein.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib52.8.1" style="font-size:90%;">Scene representation networks: Continuous 3d-structure-aware neural scene representations.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.9.1" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span class="ltx_text" id="bib.bib52.10.2" style="font-size:90%;">, 32, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib53.5.5.1" style="font-size:90%;">Soran et al. [2015]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib53.7.1" style="font-size:90%;">
Bilge Soran, Ali Farhadi, and Linda Shapiro.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib53.8.1" style="font-size:90%;">Action recognition in the presence of one egocentric and multiple static cameras.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib53.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib53.10.2" style="font-size:90%;">Computer Vision–ACCV 2014: 12th Asian Conference on Computer Vision, Singapore, Singapore, November 1-5, 2014, Revised Selected Papers, Part V 12</em><span class="ltx_text" id="bib.bib53.11.3" style="font-size:90%;">, pages 178–193. Springer, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib54.5.5.1" style="font-size:90%;">Su et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.7.1" style="font-size:90%;">
Wei Su, Yuehu Liu, Shasha Li, and Zerun Cai.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.8.1" style="font-size:90%;">Proprioception-driven wearer pose estimation for egocentric video.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib54.10.2" style="font-size:90%;">2022 26th International Conference on Pattern Recognition (ICPR)</em><span class="ltx_text" id="bib.bib54.11.3" style="font-size:90%;">, pages 3728–3735. IEEE, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib55.5.5.1" style="font-size:90%;">Sun et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib55.7.1" style="font-size:90%;">
Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib55.8.1" style="font-size:90%;">Deep high-resolution representation learning for human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib55.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib55.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em><span class="ltx_text" id="bib.bib55.11.3" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib56.5.5.1" style="font-size:90%;">Sun et al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib56.7.1" style="font-size:90%;">
Xiao Sun, Jiaxiang Shang, Shuang Liang, and Yichen Wei.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib56.8.1" style="font-size:90%;">Compositional human pose regression.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib56.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib56.10.2" style="font-size:90%;">Proceedings of the IEEE international conference on computer vision</em><span class="ltx_text" id="bib.bib56.11.3" style="font-size:90%;">, pages 2602–2611, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib57.5.5.1" style="font-size:90%;">Tekin et al. [2016]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib57.7.1" style="font-size:90%;">
Bugra Tekin, Isinsu Katircioglu, Mathieu Salzmann, Vincent Lepetit, and Pascal Fua.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib57.8.1" style="font-size:90%;">Structured prediction of 3d human pose with deep neural networks.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.9.1" style="font-size:90%;">arXiv preprint arXiv:1605.05180</em><span class="ltx_text" id="bib.bib57.10.2" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib58.5.5.1" style="font-size:90%;">Thies et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib58.7.1" style="font-size:90%;">
Justus Thies, Michael Zollhöfer, and Matthias Nießner.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib58.8.1" style="font-size:90%;">Deferred neural rendering: Image synthesis using neural textures.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.9.1" style="font-size:90%;">Acm Transactions on Graphics (TOG)</em><span class="ltx_text" id="bib.bib58.10.2" style="font-size:90%;">, 38(4):1–12, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib59.5.5.1" style="font-size:90%;">Tian et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.7.1" style="font-size:90%;">
Yating Tian, Hongwen Zhang, Yebin Liu, and Limin Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.8.1" style="font-size:90%;">Recovering 3d human mesh from monocular images: A survey.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.9.1" style="font-size:90%;">IEEE transactions on pattern analysis and machine intelligence</em><span class="ltx_text" id="bib.bib59.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib60.5.5.1" style="font-size:90%;">Tome et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib60.7.1" style="font-size:90%;">
Denis Tome, Patrick Peluse, Lourdes Agapito, and Hernan Badino.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib60.8.1" style="font-size:90%;">xr-egopose: Egocentric 3d human pose from an hmd camera.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib60.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib60.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span class="ltx_text" id="bib.bib60.11.3" style="font-size:90%;">, pages 7728–7738, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib61.5.5.1" style="font-size:90%;">Tome et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib61.7.1" style="font-size:90%;">
Denis Tome, Thiemo Alldieck, Patrick Peluse, Gerard Pons-Moll, Lourdes Agapito, Hernan Badino, and Fernando de la Torre.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib61.8.1" style="font-size:90%;">Selfpose: 3d egocentric pose estimation from a headset mounted camera.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.9.1" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</em><span class="ltx_text" id="bib.bib61.10.2" style="font-size:90%;">, 45(6):6794–6806, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib62.4.4.1" style="font-size:90%;">Toshev and Szegedy [2014]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib62.6.1" style="font-size:90%;">
Alexander Toshev and Christian Szegedy.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib62.7.1" style="font-size:90%;">Deeppose: Human pose estimation via deep neural networks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib62.8.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib62.9.2" style="font-size:90%;">2014 IEEE Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib62.10.3" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib63.5.5.1" style="font-size:90%;">Varol et al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib63.7.1" style="font-size:90%;">
Gul Varol, Javier Romero, Xavier Martin, Naureen Mahmood, Michael J Black, Ivan Laptev, and Cordelia Schmid.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib63.8.1" style="font-size:90%;">Learning from synthetic humans.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib63.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib63.10.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span class="ltx_text" id="bib.bib63.11.3" style="font-size:90%;">, pages 109–117, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib64.5.5.1" style="font-size:90%;">Wang et al. [2021a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib64.7.1" style="font-size:90%;">
Jian Wang, Lingjie Liu, Weipeng Xu, Kripasindhu Sarkar, and Christian Theobalt.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib64.8.1" style="font-size:90%;">Estimating egocentric 3d human pose in global space.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib64.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib64.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span class="ltx_text" id="bib.bib64.11.3" style="font-size:90%;">, pages 11500–11509, 2021a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib65.5.5.1" style="font-size:90%;">Wang et al. [2021b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib65.7.1" style="font-size:90%;">
Jinbao Wang, Shujie Tan, Xiantong Zhen, Shuo Xu, Feng Zheng, Zhenyu He, and Ling Shao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib65.8.1" style="font-size:90%;">Deep 3d human pose estimation: A review.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.9.1" style="font-size:90%;">Computer Vision and Image Understanding</em><span class="ltx_text" id="bib.bib65.10.2" style="font-size:90%;">, 210:103225, 2021b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib66.5.5.1" style="font-size:90%;">Wang et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib66.7.1" style="font-size:90%;">
Jian Wang, Lingjie Liu, Weipeng Xu, Kripasindhu Sarkar, Diogo Luvizon, and Christian Theobalt.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib66.8.1" style="font-size:90%;">Estimating egocentric 3d human pose in the wild with external weak supervision.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib66.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib66.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib66.11.3" style="font-size:90%;">, pages 13157–13166, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib67.5.5.1" style="font-size:90%;">Wang et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib67.7.1" style="font-size:90%;">
Jian Wang, Diogo Luvizon, Weipeng Xu, Lingjie Liu, Kripasindhu Sarkar, and Christian Theobalt.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib67.8.1" style="font-size:90%;">Scene-aware egocentric 3d human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib67.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib67.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib67.11.3" style="font-size:90%;">, pages 13031–13040, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib68.5.5.1" style="font-size:90%;">Xu et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib68.7.1" style="font-size:90%;">
Weipeng Xu, Avishek Chatterjee, Michael Zollhoefer, Helge Rhodin, Pascal Fua, Hans-Peter Seidel, and Christian Theobalt.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib68.8.1" style="font-size:90%;">Mo 2 cap 2: Real-time mobile 3d motion capture with a cap-mounted fisheye camera.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.9.1" style="font-size:90%;">IEEE transactions on visualization and computer graphics</em><span class="ltx_text" id="bib.bib68.10.2" style="font-size:90%;">, 25(5):2093–2101, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib69.5.5.1" style="font-size:90%;">Yonetani et al. [2016]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib69.7.1" style="font-size:90%;">
Ryo Yonetani, Kris M Kitani, and Yoichi Sato.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib69.8.1" style="font-size:90%;">Recognizing micro-actions and reactions from paired egocentric videos.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib69.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib69.10.2" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib69.11.3" style="font-size:90%;">, pages 2629–2638, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib70.5.5.1" style="font-size:90%;">Yu et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib70.7.1" style="font-size:90%;">
Tao Yu, Zerong Zheng, Kaiwen Guo, Jianhui Zhao, Qionghai Dai, Hao Li, Gerard Pons-Moll, and Yebin Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib70.8.1" style="font-size:90%;">Doublefusion: Real-time capture of human performances with inner body shapes from a single depth sensor.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib70.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib70.10.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span class="ltx_text" id="bib.bib70.11.3" style="font-size:90%;">, pages 7287–7296, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib71.4.4.1" style="font-size:90%;">Yuan and Kitani [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib71.6.1" style="font-size:90%;">
Ye Yuan and Kris Kitani.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib71.7.1" style="font-size:90%;">3d ego-pose estimation via imitation learning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib71.8.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib71.9.2" style="font-size:90%;">Proceedings of the European Conference on Computer Vision (ECCV)</em><span class="ltx_text" id="bib.bib71.10.3" style="font-size:90%;">, pages 735–750, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib72.4.4.1" style="font-size:90%;">Yuan and Kitani [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib72.6.1" style="font-size:90%;">
Ye Yuan and Kris Kitani.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib72.7.1" style="font-size:90%;">Ego-pose estimation and forecasting as real-time pd control.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib72.8.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib72.9.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span class="ltx_text" id="bib.bib72.10.3" style="font-size:90%;">, pages 10082–10092, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib73.5.5.1" style="font-size:90%;">Zanfir et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib73.7.1" style="font-size:90%;">
Andrei Zanfir, Elisabeta Marinoiu, and Cristian Sminchisescu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib73.8.1" style="font-size:90%;">Monocular 3d pose and shape estimation of multiple people in natural scenes-the importance of multiple scene constraints.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib73.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib73.10.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span class="ltx_text" id="bib.bib73.11.3" style="font-size:90%;">, pages 2148–2157, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib74.5.5.1" style="font-size:90%;">Zhang et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib74.7.1" style="font-size:90%;">
Siwei Zhang, Qianli Ma, Yan Zhang, Zhiyin Qian, Taein Kwon, Marc Pollefeys, Federica Bogo, and Siyu Tang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib74.8.1" style="font-size:90%;">Egobody: Human body shape and motion of interacting people from head-mounted devices.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib74.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib74.10.2" style="font-size:90%;">European Conference on Computer Vision</em><span class="ltx_text" id="bib.bib74.11.3" style="font-size:90%;">, pages 180–200. Springer, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib75.5.5.1" style="font-size:90%;">Zhang et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib75.7.1" style="font-size:90%;">
Siwei Zhang, Qianli Ma, Yan Zhang, Sadegh Aliakbarian, Darren Cosker, and Siyu Tang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib75.8.1" style="font-size:90%;">Probabilistic human mesh recovery in 3d scenes from egocentric views.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib75.9.1" style="font-size:90%;">arXiv preprint arXiv:2304.06024</em><span class="ltx_text" id="bib.bib75.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib76.5.5.1" style="font-size:90%;">Zhang et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib76.7.1" style="font-size:90%;">
Yahui Zhang, Shaodi You, and Theo Gevers.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib76.8.1" style="font-size:90%;">Automatic calibration of the fisheye camera for egocentric 3d human pose estimation from a single image.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib76.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib76.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em><span class="ltx_text" id="bib.bib76.11.3" style="font-size:90%;">, pages 1772–1781, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib77.5.5.1" style="font-size:90%;">Zhao et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib77.7.1" style="font-size:90%;">
Dongxu Zhao, Zhen Wei, Jisan Mahmud, and Jan-Michael Frahm.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib77.8.1" style="font-size:90%;">Egoglass: Egocentric-view human pose estimation from an eyeglass frame.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib77.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib77.10.2" style="font-size:90%;">2021 International Conference on 3D Vision (3DV)</em><span class="ltx_text" id="bib.bib77.11.3" style="font-size:90%;">, pages 32–41. IEEE, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib78.5.5.1" style="font-size:90%;">Zheng et al. [2023a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib78.7.1" style="font-size:90%;">
Ce Zheng, Wenhan Wu, Chen Chen, Taojiannan Yang, Sijie Zhu, Ju Shen, Nasser Kehtarnavaz, and Mubarak Shah.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib78.8.1" style="font-size:90%;">Deep learning-based human pose estimation: A survey.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib78.9.1" style="font-size:90%;">ACM Computing Surveys</em><span class="ltx_text" id="bib.bib78.10.2" style="font-size:90%;">, 56(1):1–37, 2023a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib79.5.5.1" style="font-size:90%;">Zheng et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib79.7.1" style="font-size:90%;">
Zerong Zheng, Tao Yu, Yebin Liu, and Qionghai Dai.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib79.8.1" style="font-size:90%;">Pamir: Parametric model-conditioned implicit representation for image-based human reconstruction.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib79.9.1" style="font-size:90%;">IEEE transactions on pattern analysis and machine intelligence</em><span class="ltx_text" id="bib.bib79.10.2" style="font-size:90%;">, 44(6):3170–3184, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib80.5.5.1" style="font-size:90%;">Zheng et al. [2023b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib80.7.1" style="font-size:90%;">
Zerong Zheng, Xiaochen Zhao, Hongwen Zhang, Boning Liu, and Yebin Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib80.8.1" style="font-size:90%;">Avatarrex: Real-time expressive full-body avatars.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib80.9.1" style="font-size:90%;">ACM Transactions on Graphics (TOG)</em><span class="ltx_text" id="bib.bib80.10.2" style="font-size:90%;">, 42(4):1–19, 2023b.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed May  1 14:50:05 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
