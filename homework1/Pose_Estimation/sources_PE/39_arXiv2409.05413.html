<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>From Words to Poses: Enhancing Novel Object Pose Estimation with Vision Language Models</title>
<!--Generated on Mon Sep  9 08:05:39 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.05413v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#S1" title="In From Words to Poses: Enhancing Novel Object Pose Estimation with Vision Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#S2" title="In From Words to Poses: Enhancing Novel Object Pose Estimation with Vision Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related works</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#S3" title="In From Words to Poses: Enhancing Novel Object Pose Estimation with Vision Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Method</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#S3.SS1" title="In III Method ‣ From Words to Poses: Enhancing Novel Object Pose Estimation with Vision Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Promptable Object Localization</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#S3.SS2" title="In III Method ‣ From Words to Poses: Enhancing Novel Object Pose Estimation with Vision Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Promptable Object Pose Estimation</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#S4" title="In From Words to Poses: Enhancing Novel Object Pose Estimation with Vision Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Discussion and Future work</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">From Words to Poses: Enhancing Novel Object Pose Estimation with Vision Language Models
<br class="ltx_break"/>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Tessa Pulli1, Stefan Thalhammer2, Simon Schwaiger2, Markus Vincze1
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">1Vision for Robotics Laboratory, Automation and Control Institute, TU Wien, Austria
<br class="ltx_break"/>{pulli, vincze}@acin.tuwien.ac.at
</span>
<span class="ltx_contact ltx_role_affiliation">2Industrial Engineering Department, UAS Technikum Vienna, Austria
<br class="ltx_break"/>{stefan.thalhammer, schwaige}@technikum-wien.at
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Robots are increasingly envisioned to interact in real-world scenarios, where they must continuously adapt to new situations.
To detect and grasp novel objects, zero-shot pose estimators determine poses without prior knowledge.
Recently, vision language models (VLMs) have shown considerable advances in robotics applications by establishing an understanding between language input and image input.
In our work, we take advantage of VLMs zero-shot capabilities and translate this ability to 6D object pose estimation.
We propose a novel framework for promptable zero-shot 6D object pose estimation using language embeddings.
The idea is to derive a coarse location of an object based on the relevancy map of a language-embedded NeRF reconstruction and to compute the pose estimate with a point cloud registration method.
Additionally, we provide an analysis of LERF’s suitability for open-set object pose estimation.
We examine hyperparameters, such as activation thresholds for relevancy maps and investigate the zero-shot capabilities on an instance- and category-level.
Furthermore, we plan to conduct robotic grasping experiments in a real-world setting.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="184" id="S1.F1.g1" src="extracted/5842099/diagram_overview.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>From a set of RGB(-D) images, a NeRF scene is reconstructed. Using LERF, the target object is detected via text prompting. The object centroid is then computed through three-dimensional semantic segmentation. Finally, the pose estimate is determined using a point cloud registration method, e.g. Teaser++ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib1" title="">1</a>]</cite>.</figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">6D object pose estimation of unseen objects is a core task in robotics.
Classical methods estimate the pose of objects using trained networks either for object instances <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib4" title="">4</a>]</cite> or zero-shot methods with the idea of capturing classes of objects and adapting to novel objects <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib7" title="">7</a>]</cite>.
With the advent of vision language models (VLM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib9" title="">9</a>]</cite>, novel methods to detect objects and align visual content with natural language show noteworthy results for realistic scenes.
Traditional pose estimation methods output the required pose to manipulate objects in a real-world scene without any context.
VLMs show impressive results in terms of object recognition <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib11" title="">11</a>]</cite>, scene understanding, and even reasoning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib13" title="">13</a>]</cite>.
We propose to take advantage of the recent advances in VLMs and utilize their zero-shot capability in 6D object pose estimation methods by introducing a novel promptable zero-shot 6D object pose estimation pipeline.
We explore VLMs for open-vocabulary object pose estimation, leveraging their zero-shot scene understanding capabilities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib8" title="">8</a>]</cite>.
Using NeRF and the language embedding LERF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib14" title="">14</a>]</cite>, we query objects in an open-vocabulary manner.
The LERF-generated relevancy map provides the object’s location from which the centroid in 3D space can be derived.
The 6D pose is estimated using a point cloud registration method like TEASER++ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib1" title="">1</a>]</cite>. Finally, grasp points and affordances are derived for real-world manipulation.
In summary, the paper has the following key contributions:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We introduce a language-embedded zero-shot object pose estimation framework.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We analyze the zero-shot capabilities of LERF to identify key requirements to enhance their applicability in pose estimation.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related works</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Several works incorporate VLMs in robotics-related scenarios with considerable results <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib18" title="">18</a>]</cite>.
 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib16" title="">16</a>]</cite> prove the potential of VLMs in robotics scenarios by also considering 3D data for promptable navigation.
Other works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib17" title="">17</a>]</cite> introduce and perform robotic grasping experiments with VLM-based pipelines but avoid the estimation of object poses.
Instead, GEFF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib18" title="">18</a>]</cite> simplifies 6D object pose estimation by grasping the object’s centroid, while <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib17" title="">17</a>]</cite> relies on an general object grasping methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib19" title="">19</a>]</cite>, and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib20" title="">20</a>]</cite> replays previously collected pick-and-place primitives.
The mentioned approaches avoid 6D object pose estimation by using alternative strategies for object manipulation.
This simplification may lead to major limitations when it comes to manipulating objects with more complex shapes.
In this work, we propose a VLM-based method for estimating the 6D pose of novel objects to enable open-set manipulation in unknown settings.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Method</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#S1.F1" title="Figure 1 ‣ I Introduction ‣ From Words to Poses: Enhancing Novel Object Pose Estimation with Vision Language Models"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates our proposed pipeline.
We reconstruct a scene based on a set of RGB(-D) images input by using NeRFstudio <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib21" title="">21</a>]</cite>.
We assume the availability of multiple images of a scene without camera poses and retrieve these using multiview stereo, e.g. COLMAP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib22" title="">22</a>]</cite>.
Having individual images and corresponding poses allows for a joint geometric reconstruction of the scene.
Within this framework, we use a CLIP-based language embedding <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib8" title="">8</a>]</cite> to query objects in an open-vocabulary manner.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">Through the relevancy map generated by the LERF response (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#S3.F2" title="Figure 2 ‣ III-B Promptable Object Pose Estimation ‣ III Method ‣ From Words to Poses: Enhancing Novel Object Pose Estimation with Vision Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>), we can obtain a coarse 3D location of the target object.
Qui et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib18" title="">18</a>]</cite> estimate poses in their work by aggregating the semantic point cloud and calculating the centroid of the object.
We want to take advantage of this approach and obtain a coarse location of the object based on its centroid.
Afterwards, the 6D pose of an object is estimated using a point cloud registration method, like Teaser++ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib1" title="">1</a>]</cite>.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.4.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.5.2">Promptable Object Localization</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.2">To validate the potential of LERF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib14" title="">14</a>]</cite> for 6D object pose estimation, we perform an analysis to reveal strengths and limitations of the method.
For our validation, we consider the performance of instance-level and category-level prompts and investigate with which language input the most promising results can be achieved.
Furthermore, we plan to investigate hyperparameters, such as activation thresholds for the relevancy maps to understand how an optimal object centroid can be obtained.
As we hypothesize that our method provides versatile applicability for household robotics, we test our approach on the dataset HouseCat6D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib23" title="">23</a>]</cite>, which provides <math alttext="41" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mn id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">41</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><cn id="S3.SS1.p1.1.m1.1.1.cmml" type="integer" xref="S3.SS1.p1.1.m1.1.1">41</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">41</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">41</annotation></semantics></math> scenes with <math alttext="194" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><mn id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">194</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><cn id="S3.SS1.p1.2.m2.1.1.cmml" type="integer" xref="S3.SS1.p1.2.m2.1.1">194</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">194</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">194</annotation></semantics></math> object instances.
We reconstruct the scenes of the dataset with NeRFstudio <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib21" title="">21</a>]</cite> and analyze the capabilities with the intention of utilizing the approach for 6D object pose estimation.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#S3.F2" title="Figure 2 ‣ III-B Promptable Object Pose Estimation ‣ III Method ‣ From Words to Poses: Enhancing Novel Object Pose Estimation with Vision Language Models"><span class="ltx_text ltx_ref_tag">2</span></a> shows an exemplary HouseCat6D scene and relevancy map generated with a LERF language prompt.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.4.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.5.2">Promptable Object Pose Estimation</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">To localize the target object, we use an approach similar to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib18" title="">18</a>]</cite>.
Firstly, we filter point clouds for relevant vertex clusters with the CLIP activation.
Subsequently, the relevant pixels are separated into individual object instances with a clustering approach like DBScan <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib24" title="">24</a>]</cite>.
Based on the clustered point cloud, the object’s centroid and, therefore, its coarse location are determined.
Ultimately, the 6D object poses are estimated using RGB-D registration assuming the availability of the object’s 3D mesh.
A modified version of TEASER++ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib1" title="">1</a>]</cite> registers an object prior with the observed partial point cloud, also accounting for the object texture.
This consideration is crucial as it allows to disambiguate geometric symmetries with texture cues enabling robust pose estimation.
Based on our proposed VLM-based 6D object pose estimation pipeline, we plan to conduct grasping experiments in a real-world household setting using HOPE and YCB-Video objects.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="379" id="S3.F2.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span class="ltx_text ltx_font_bold" id="S3.F2.3.1">VLM-based scene reconstruction.</span> 3D reconstruction of scene 04 of the HouseCat6D dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib23" title="">23</a>]</cite> with overlaid relevancy map generated with LERF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib14" title="">14</a>]</cite> for open language prompt <span class="ltx_text ltx_font_italic" id="S3.F2.4.2">teapot</span>. Red shading indicates high relevancy between scene and prompt.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Discussion and Future work</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Future research will explore the applicability of zero-shot VLMs in settings beyond household environments, with a particular focus on industrial contexts.
We believe our method holds significant potential for these settings, despite the considerable differences between industrial scenes and the datasets on which CLIP is pre-trained <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.05413v1#bib.bib8" title="">8</a>]</cite>.
One limitation of this study is the assumption that object priors are available.
To address this, future work will aim to overcome this constraint by investigating derived affordances, enabling pose estimation and grasping without the need for pre-existing object models.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.1.1" style="font-size:90%;">
H. Yang, J. Shi, and L. Carlone, “Teaser: Fast and certifiable point cloud registration,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib1.2.2" style="font-size:90%;">IEEE Transactions on Robotics</em><span class="ltx_text" id="bib.bib1.3.3" style="font-size:90%;">, vol. 37, no. 2, pp. 314–333, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.1.1" style="font-size:90%;">
G. Wang, F. Manhardt, F. Tombari, and X. Ji, “Gdr-net: Geometry-guided direct regression network for monocular 6d object pose estimation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib2.2.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib2.3.3" style="font-size:90%;">, 2021, pp. 16 611–16 621.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.1.1" style="font-size:90%;">
Y. Su, M. Saleh, T. Fetzer, J. Rambach, N. Navab, B. Busam, D. Stricker, and F. Tombari, “Zebrapose: Coarse to fine surface encoding for 6dof object pose estimation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib3.2.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib3.3.3" style="font-size:90%;">, 2022, pp. 6738–6748.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.1.1" style="font-size:90%;">
K. Park, T. Patten, and M. Vincze, “Pix2pose: Pixel-wise coordinate regression of objects for 6d pose estimation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib4.2.2" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on computer vision</em><span class="ltx_text" id="bib.bib4.3.3" style="font-size:90%;">, 2019, pp. 7668–7677.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.1.1" style="font-size:90%;">
V. N. Nguyen, T. Groueix, M. Salzmann, and V. Lepetit, “Gigapose: Fast and robust novel object pose estimation via one correspondence,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib5.2.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib5.3.3" style="font-size:90%;">, 2024, pp. 9903–9913.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.1.1" style="font-size:90%;">
Y. Labbé, L. Manuelli, A. Mousavian, S. Tyree, S. Birchfield, J. Tremblay, J. Carpentier, M. Aubry, D. Fox, and J. Sivic, “Megapose: 6d pose estimation of novel objects via render &amp; compare,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib6.2.2" style="font-size:90%;">arXiv preprint arXiv:2212.06870</em><span class="ltx_text" id="bib.bib6.3.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.1.1" style="font-size:90%;">
J. Lin, L. Liu, D. Lu, and K. Jia, “Sam-6d: Segment anything model meets zero-shot 6d object pose estimation,” 2024. [Online]. Available: https://arxiv.org/abs/2311.15707
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.1.1" style="font-size:90%;">
A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, “Learning transferable visual models from natural language supervision,” 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.1.1" style="font-size:90%;">
C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. V. Le, Y. Sung, Z. Li, and T. Duerig, “Scaling up visual and vision-language representation learning with noisy text supervision,” 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.1.1" style="font-size:90%;">
S. Jin, X. Jiang, J. Huang, L. Lu, and S. Lu, “Llms meet vlms: Boost open vocabulary object detection with fine-grained descriptors,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib10.2.2" style="font-size:90%;">arXiv preprint arXiv:2402.04630</em><span class="ltx_text" id="bib.bib10.3.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.1.1" style="font-size:90%;">
Y. Zang, W. Li, J. Han, K. Zhou, and C. C. Loy, “Contextual object detection with multimodal large language models,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib11.2.2" style="font-size:90%;">arXiv preprint arXiv:2305.18279</em><span class="ltx_text" id="bib.bib11.3.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.1.1" style="font-size:90%;">
R. Fu, J. Liu, X. Chen, Y. Nie, and W. Xiong, “Scene-llm: Extending language model for 3d visual understanding and reasoning,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib12.2.2" style="font-size:90%;">arXiv preprint arXiv:2403.11401</em><span class="ltx_text" id="bib.bib12.3.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.1.1" style="font-size:90%;">
H. Ha and S. Song, “Semantic abstraction: Open-world 3d scene understanding from 2d vision-language models,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib13.2.2" style="font-size:90%;">arXiv preprint arXiv:2207.11514</em><span class="ltx_text" id="bib.bib13.3.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.1.1" style="font-size:90%;">
J. Kerr, C. M. Kim, K. Goldberg, A. Kanazawa, and M. Tancik, “Lerf: Language embedded radiance fields,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib14.2.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span class="ltx_text" id="bib.bib14.3.3" style="font-size:90%;">, 2023, pp. 19 729–19 739.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.1.1" style="font-size:90%;">
D. Song, J. Liang, A. Payandeh, X. Xiao, and D. Manocha, “Socially aware robot navigation through scoring using vision-language models,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib15.2.2" style="font-size:90%;">arXiv preprint arXiv:2404.00210</em><span class="ltx_text" id="bib.bib15.3.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.1.1" style="font-size:90%;">
D. Shah, B. Osiński, S. Levine </span><em class="ltx_emph ltx_font_italic" id="bib.bib16.2.2" style="font-size:90%;">et al.</em><span class="ltx_text" id="bib.bib16.3.3" style="font-size:90%;">, “Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib16.4.4" style="font-size:90%;">Conference on robot learning</em><span class="ltx_text" id="bib.bib16.5.5" style="font-size:90%;">.   PMLR, 2023, pp. 492–504.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.1.1" style="font-size:90%;">
Y. Deng, J. Wang, J. Zhao, J. Dou, Y. Yang, and Y. Yue, “Openobj: Open-vocabulary object-level neural radiance fields with fine-grained understanding,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib17.2.2" style="font-size:90%;">arXiv preprint arXiv:2406.08009</em><span class="ltx_text" id="bib.bib17.3.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.1.1" style="font-size:90%;">
R.-Z. Qiu, Y. Hu, G. Yang, Y. Song, Y. Fu, J. Ye, J. Mu, R. Yang, N. Atanasov, S. Scherer </span><em class="ltx_emph ltx_font_italic" id="bib.bib18.2.2" style="font-size:90%;">et al.</em><span class="ltx_text" id="bib.bib18.3.3" style="font-size:90%;">, “Learning generalizable feature fields for mobile manipulation,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib18.4.4" style="font-size:90%;">arXiv preprint arXiv:2403.07563</em><span class="ltx_text" id="bib.bib18.5.5" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.1.1" style="font-size:90%;">
H.-S. Fang, C. Wang, M. Gou, and C. Lu, “Graspnet-1billion: A large-scale benchmark for general object grasping,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib19.2.2" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em><span class="ltx_text" id="bib.bib19.3.3" style="font-size:90%;">, 2020, pp. 11 444–11 453.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.1.1" style="font-size:90%;">
J. Gao, B. Sarkar, F. Xia, T. Xiao, J. Wu, B. Ichter, A. Majumdar, and D. Sadigh, “Physically grounded vision-language models for robotic manipulation,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib20.2.2" style="font-size:90%;">arXiv preprint arXiv:2309.02561</em><span class="ltx_text" id="bib.bib20.3.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.1.1" style="font-size:90%;">
M. Tancik, E. Weber, E. Ng, R. Li, B. Yi, J. Kerr, T. Wang, A. Kristoffersen, J. Austin, K. Salahi, A. Ahuja, D. McAllister, and A. Kanazawa, “Nerfstudio: A modular framework for neural radiance field development,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib21.2.2" style="font-size:90%;">ACM SIGGRAPH 2023 Conference Proceedings</em><span class="ltx_text" id="bib.bib21.3.3" style="font-size:90%;">, ser. SIGGRAPH ’23, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.1.1" style="font-size:90%;">
J. L. Schönberger, E. Zheng, M. Pollefeys, and J.-M. Frahm, “Pixelwise view selection for unstructured multi-view stereo,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib22.2.2" style="font-size:90%;">European Conference on Computer Vision (ECCV)</em><span class="ltx_text" id="bib.bib22.3.3" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.1.1" style="font-size:90%;">
H. Jung, S.-C. Wu, P. Ruhkamp, G. Zhai, H. Schieber, G. Rizzoli, P. Wang, H. Zhao, L. Garattoni, S. Meier </span><em class="ltx_emph ltx_font_italic" id="bib.bib23.2.2" style="font-size:90%;">et al.</em><span class="ltx_text" id="bib.bib23.3.3" style="font-size:90%;">, “Housecat6d-a large-scale multi-modal category level 6d object perception dataset with household objects in realistic scenarios,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib23.4.4" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib23.5.5" style="font-size:90%;">, 2024, pp. 22 498–22 508.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.1.1" style="font-size:90%;">
M. Ester, H.-P. Kriegel, J. Sander, X. Xu </span><em class="ltx_emph ltx_font_italic" id="bib.bib24.2.2" style="font-size:90%;">et al.</em><span class="ltx_text" id="bib.bib24.3.3" style="font-size:90%;">, “A density-based algorithm for discovering clusters in large spatial databases with noise,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib24.4.4" style="font-size:90%;">kdd</em><span class="ltx_text" id="bib.bib24.5.5" style="font-size:90%;">, vol. 96, no. 34, 1996, pp. 226–231.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep  9 08:05:39 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
