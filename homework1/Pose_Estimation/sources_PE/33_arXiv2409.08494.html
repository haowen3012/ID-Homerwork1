<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users</title>
<!--Generated on Fri Sep 13 02:25:38 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Wheelchair Users,  Motion Capture,  Pose Estimation,  Inertial Measurement Units,  Real-time" lang="en" name="keywords"/>
<base href="/html/2409.08494v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S1" title="In WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S2" title="In WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S2.SS1" title="In 2. Related Work ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Activity Sensing for Wheelchair Users</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S2.SS2" title="In 2. Related Work ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Motion Capture for Wheelchair Users with Cameras</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S2.SS3" title="In 2. Related Work ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Motion Capture for Wheelchair Users with Dense IMUs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S2.SS4" title="In 2. Related Work ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Motion Capture with Sparse IMUs</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S3" title="In WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>WheelPoser System</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S3.SS1" title="In 3. WheelPoser System ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Device Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S3.SS1.SSS1" title="In 3.1. Device Setup ‣ 3. WheelPoser System ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Preliminary Study: Examining Wheelchair-Pelvis Motion Correlation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S3.SS2" title="In 3. WheelPoser System ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Learning-based Kinematics Module</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S3.SS2.SSS1" title="In 3.2. Learning-based Kinematics Module ‣ 3. WheelPoser System ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Data Synthesis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S3.SS2.SSS2" title="In 3.2. Learning-based Kinematics Module ‣ 3. WheelPoser System ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Kinematics Module Network Architecture</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S3.SS3" title="In 3. WheelPoser System ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Physics-based Optimization Module</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S4" title="In WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>WheelPoser-IMU Dataset</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S4.SS1" title="In 4. WheelPoser-IMU Dataset ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Data Collection Apparatus</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S4.SS2" title="In 4. WheelPoser-IMU Dataset ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Participants and Procedure</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S4.SS3" title="In 4. WheelPoser-IMU Dataset ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Data Processing</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S5" title="In WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S5.SS1" title="In 5. Evaluation ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Dataset and Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S5.SS2" title="In 5. Evaluation ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Evaluation of Kinematics Module</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S5.SS2.SSS1" title="In 5.2. Evaluation of Kinematics Module ‣ 5. Evaluation ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.1 </span>SOTA Models versus Our Candidate Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S5.SS2.SSS2" title="In 5.2. Evaluation of Kinematics Module ‣ 5. Evaluation ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.2 </span>Results Across Our Candidate Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S5.SS3" title="In 5. Evaluation ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Evaluation of Physics Module</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S5.SS4" title="In 5. Evaluation ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Performance across Motion Types</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S5.SS5" title="In 5. Evaluation ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Live Demo</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S6" title="In WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Example Applications</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S6.SS1" title="In 6. Example Applications ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Facilitating Wheelchair Skill Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S6.SS2" title="In 6. Example Applications ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Inclusive Personal Informatics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S6.SS3" title="In 6. Example Applications ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Novel Interaction and Gaming Experiences</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S6.SS4" title="In 6. Example Applications ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span>Enhanced Data-Driven Collaborative Care</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S7" title="In WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Discussion and Future Directions</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S7.SS1" title="In 7. Discussion and Future Directions ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Generalization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S7.SS2" title="In 7. Discussion and Future Directions ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Failure Cases</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S7.SS3" title="In 7. Discussion and Future Directions ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3 </span>Facilitating Marker-based Motion Capture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S7.SS4" title="In 7. Discussion and Future Directions ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.4 </span>Future Opportunities</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S7.SS4.SSS1" title="In 7.4. Future Opportunities ‣ 7. Discussion and Future Directions ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.4.1 </span>Towards Even More Practical and Unobtrusive Motion Capture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S7.SS4.SSS2" title="In 7.4. Future Opportunities ‣ 7. Discussion and Future Directions ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.4.2 </span>Advanced Physics-based Modeling for Pose Estimation and Wheelchair Kinematics Tracking</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S7.SS4.SSS3" title="In 7.4. Future Opportunities ‣ 7. Discussion and Future Directions ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.4.3 </span>Towards Large-Scale Inclusive Motion Capture Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S7.SS4.SSS4" title="In 7.4. Future Opportunities ‣ 7. Discussion and Future Directions ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.4.4 </span>From Wheelchairs to Other Mobility Aids</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S8" title="In WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Open Source</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S9" title="In WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yunzhi Li
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id3.1.id1">Carnegie Mellon University</span><span class="ltx_text ltx_affiliation_city" id="id4.2.id2">Pittsburgh</span><span class="ltx_text ltx_affiliation_state" id="id5.3.id3">PA</span><span class="ltx_text ltx_affiliation_country" id="id6.4.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:yunzhil@cs.cmu.edu">yunzhil@cs.cmu.edu</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Vimal Mollyn
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">Carnegie Mellon University</span><span class="ltx_text ltx_affiliation_city" id="id8.2.id2">Pittsburgh</span><span class="ltx_text ltx_affiliation_state" id="id9.3.id3">PA</span><span class="ltx_text ltx_affiliation_country" id="id10.4.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:vmollyn@cs.cmu.edu">vmollyn@cs.cmu.edu</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kuang Yuan
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id11.1.id1">Carnegie Mellon University</span><span class="ltx_text ltx_affiliation_city" id="id12.2.id2">Pittsburgh</span><span class="ltx_text ltx_affiliation_state" id="id13.3.id3">PA</span><span class="ltx_text ltx_affiliation_country" id="id14.4.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:kuangy@andrew.cmu.edu%20">kuangy@andrew.cmu.edu </a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Patrick Carrington
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id15.1.id1">Carnegie Mellon University</span><span class="ltx_text ltx_affiliation_city" id="id16.2.id2">Pittsburgh</span><span class="ltx_text ltx_affiliation_state" id="id17.3.id3">PA</span><span class="ltx_text ltx_affiliation_country" id="id18.4.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:pcarrington@cmu.edu">pcarrington@cmu.edu</a>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id2.2">Despite researchers having extensively studied various ways to track body pose on-the-go, most prior work does not take into account wheelchair users, leading to poor tracking performance. Wheelchair users could greatly benefit from this pose information to prevent injuries, monitor their health, identify environmental accessibility barriers, and interact with gaming and VR experiences. In this work, we present WheelPoser, a real-time pose estimation system specifically designed for wheelchair users. Our system uses only four strategically placed IMUs on the user’s body and wheelchair, making it far more practical than prior systems using cameras and dense IMU arrays. WheelPoser is able to track a wheelchair user’s pose with a mean joint angle error of <math alttext="14.30^{\circ}" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><msup id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mn id="id1.1.m1.1.1.2" xref="id1.1.m1.1.1.2.cmml">14.30</mn><mo id="id1.1.m1.1.1.3" xref="id1.1.m1.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><csymbol cd="ambiguous" id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1">superscript</csymbol><cn id="id1.1.m1.1.1.2.cmml" type="float" xref="id1.1.m1.1.1.2">14.30</cn><compose id="id1.1.m1.1.1.3.cmml" xref="id1.1.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">14.30^{\circ}</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">14.30 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT</annotation></semantics></math> and a mean joint position error of <math alttext="6.74" class="ltx_Math" display="inline" id="id2.2.m2.1"><semantics id="id2.2.m2.1a"><mn id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml">6.74</mn><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><cn id="id2.2.m2.1.1.cmml" type="float" xref="id2.2.m2.1.1">6.74</cn></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">6.74</annotation><annotation encoding="application/x-llamapun" id="id2.2.m2.1d">6.74</annotation></semantics></math> cm, more than three times better than similar systems using sparse IMUs. To train our system, we collect a novel WheelPoser-IMU dataset, consisting of 167 minutes of paired IMU sensor and motion capture data of people in wheelchairs, including wheelchair-specific motions such as propulsion and pressure relief.
Finally, we explore the potential application space enabled by our system and discuss future opportunities. Open-source code, models, and dataset can be found here: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/axle-lab/WheelPoser" title="">https://github.com/axle-lab/WheelPoser</a>.</p>
</div>
<div class="ltx_keywords">Wheelchair Users, Motion Capture, Pose Estimation, Inertial Measurement Units, Real-time
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>rightsretained</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>The 26th International ACM SIGACCESS Conference on Computers and Accessibility; October 27–30, 2024; St. John’s, NL, Canada</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_booktitle" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">booktitle: </span>The 26th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS ’24), October 27–30, 2024, St. John’s, NL, Canada</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>10.1145/3663548.3675638</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>979-8-4007-0677-6/24/10</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id7"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing Accessibility</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id8"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Motion capture</span></span></span>
<figure class="ltx_figure ltx_teaserfigure" id="S0.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="176" id="S0.F1.g1" src="extracted/5851880/camera-ready/pipeline_overview_black.png" width="598"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S0.F1.2.1.1" style="font-size:90%;">Figure 1</span>. </span><span class="ltx_text" id="S0.F1.3.2" style="font-size:90%;">WheelPoser uses a sparse-IMU setup for practical and effective pose estimation of wheelchair users.
It comprises a learning-based kinematics module that maps sparse IMU measurements to joint rotations and a physics-based optimizer for motion refinement.
Our models are pre-trained on the AMASS dataset and fine-tuned on our WheelPoser-IMU dataset leading to state-of-the-art pose estimation performance for wheelchair users.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="S0.F1.4">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S0.F1.5">This figure is a flow diagram comprising five major components that describe the pose estimation pipeline of WheelPoser. Starting from the left, there is an individual seated in a manual wheelchair with four IMU sensors attached to both their body and the wheelchair. Next, a vertical series of colored arrows represents the data from each IMU sensor. Following this, a yellow block labeled “Kinematics Module” takes the IMU data as input and outputs estimated joint rotations. Above this, two red blocks represent the datasets used to train and fine-tune this kinematics module, namely AMASS and WheelPoser-IMU, respectively. Further to the right, a green-colored block indicates the final stage where the pose data is refined by a physics-based optimizer. Finally, on the far right, a human mesh in a sitting posture represents the pose estimated by WheelPoser.</p>
</div>
</div>
</figure>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The upper body pose of people who use wheelchairs contains rich and valuable information about their behavior, activities, and interactions with the environment. Being able to track and understand these poses on a daily basis holds immense potential to enhance various aspects of wheelchair users’ lives and open up new applications and experiences. For instance, tracking the day-to-day upper body mechanics of a wheelchair user can be used to promote their health awareness and facilitate injury prevention <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib54" title="">2023</a>)</cite>. Long-term monitoring of wheelchair users’ daily movement patterns across various contexts can also be instrumental in wheelchair fitting and guiding novel design for maximized comfort and mobility <cite class="ltx_cite ltx_citemacro_citep">(Kon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib47" title="">2015</a>)</cite>, as well as identifying environmental barriers for accessible modifications <cite class="ltx_cite ltx_citemacro_citep">(Jarosz, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib39" title="">1996</a>)</cite>. More broadly, pose tracking allows for precise analytics and training for adaptive sports, while also affording new interaction modalities for consumer-level applications such as inclusive gaming and VR experiences <cite class="ltx_cite ltx_citemacro_citep">(Gerling et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib24" title="">2020</a>; Mott et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib69" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Despite this promising potential, there is still no system that achieves practical upper body pose estimation for wheelchair users across diverse contexts.
Most commonly, wheelchair users’ poses are captured using cameras, such as optical cameras in marker-based motion capture systems (e.g., Vicon <cite class="ltx_cite ltx_citemacro_citep">(Systems, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib89" title="">2023</a>)</cite>) and RGB or RGB-D cameras <cite class="ltx_cite ltx_citemacro_citep">(Cotton, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib17" title="">2020</a>; Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib97" title="">2016</a>; Hwang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib37" title="">2017</a>; Milgrom et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib64" title="">2016</a>; Rammer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib80" title="">2018</a>; Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib34" title="">2024</a>)</cite>. Yet, these systems are privacy-invasive, sensitive to occlusion, and most importantly do not work on the go.
In contrast, pose estimation using inertial measurement units (IMUs) is portable, able to work in any environment, insusceptible to visual occlusions, and more privacy-preserving, making it a more suitable solution for everyday consumer-level usage. However, existing IMU-based motion capture systems require instrumenting the wheelchair user with dense IMU arrays on each joint, totaling 11 or more sensors, to effectively estimate upper body poses <cite class="ltx_cite ltx_citemacro_citep">(Starrs et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib86" title="">2012</a>; Kondo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib48" title="">2024</a>)</cite>, significantly reducing their practicality and usability.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this work, we present WheelPoser, a motion capture system that achieves real-time upper body pose estimation for wheelchair users using a total of 4 strategically positioned IMUs on both the wheelchair and the user’s body. Drawing inspiration from previous studies that use sparse IMUs for pose estimation of people without motor impairments <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib35" title="">2018</a>; Yi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib100" title="">2021</a>; Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib41" title="">2022</a>; Mollyn et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib66" title="">2023</a>)</cite>, WheelPoser employs a learning-based kinematics module to map sparse inertial measurements to a broader set of upper body joint angles.
Unfortunately, there is no publicly available motion capture dataset of wheelchair users, let alone one on a large scale, that would allow us to directly train such a model.
To address this, we propose a data synthesis and pose estimation pipeline tailored for wheelchair users. Specifically, we first train a base model by leveraging the diverse upper body motions in existing datasets collected from people without motor impairments <cite class="ltx_cite ltx_citemacro_citep">(Mahmood et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib62" title="">2019</a>)</cite>.
We then fine-tune this pre-trained model on our newly collected WheelPoser-IMU dataset, comprising 167 minutes of typical on-wheelchair motions with IMU sensor data from 14 participants, in order to bridge the gap between existing motions and on-wheelchair motions.
Additionally, we propose a physics-based optimization module tailored for on-wheelchair motions, that further refines the predicted pose by increasing its smoothness and offering joint torque estimates, which could be beneficial for healthcare and sports applications.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.2">As a result, WheelPoser establishes a new state-of-the-art (SOTA) for sparse-IMU based upper body pose estimation of wheelchair users, achieving a mean joint angle error of <math alttext="14.30^{\circ}" class="ltx_Math" display="inline" id="S1.p4.1.m1.1"><semantics id="S1.p4.1.m1.1a"><msup id="S1.p4.1.m1.1.1" xref="S1.p4.1.m1.1.1.cmml"><mn id="S1.p4.1.m1.1.1.2" xref="S1.p4.1.m1.1.1.2.cmml">14.30</mn><mo id="S1.p4.1.m1.1.1.3" xref="S1.p4.1.m1.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S1.p4.1.m1.1b"><apply id="S1.p4.1.m1.1.1.cmml" xref="S1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S1.p4.1.m1.1.1.1.cmml" xref="S1.p4.1.m1.1.1">superscript</csymbol><cn id="S1.p4.1.m1.1.1.2.cmml" type="float" xref="S1.p4.1.m1.1.1.2">14.30</cn><compose id="S1.p4.1.m1.1.1.3.cmml" xref="S1.p4.1.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.1.m1.1c">14.30^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S1.p4.1.m1.1d">14.30 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT</annotation></semantics></math> and a mean joint position error of <math alttext="6.74" class="ltx_Math" display="inline" id="S1.p4.2.m2.1"><semantics id="S1.p4.2.m2.1a"><mn id="S1.p4.2.m2.1.1" xref="S1.p4.2.m2.1.1.cmml">6.74</mn><annotation-xml encoding="MathML-Content" id="S1.p4.2.m2.1b"><cn id="S1.p4.2.m2.1.1.cmml" type="float" xref="S1.p4.2.m2.1.1">6.74</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.2.m2.1c">6.74</annotation><annotation encoding="application/x-llamapun" id="S1.p4.2.m2.1d">6.74</annotation></semantics></math> cm for real-time pose estimation. WheelPoser outperforms existing models designed for people without motor impairments by more than threefold.
To summarize, we make the following contributions:</p>
</div>
<div class="ltx_para" id="S1.p5">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We design and implement a first-of-its-kind real-time, sparse-IMU-based wheelchair user tracking system for practical and effective upper body pose estimation.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We propose and verify an optimized IMU device setup, ensuring that it is minimally intrusive and practical for daily use while retaining essential information for accurate pose estimation.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We collect and release a novel dataset that captures 167 minutes of essential on-wheelchair motions performed by 2 full-time wheelchair users and 12 participants without motor impairments.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">We comprehensively evaluate our system, establishing the first benchmark of sparse-IMU based upper body pose estimation for wheelchair users. The results demonstrate that our proposed method significantly enhances pose estimation accuracy compared to existing SOTA models.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i5.p1">
<p class="ltx_p" id="S1.I1.i5.p1.1">We discuss the broad application space enabled by our system and opportunities for future work.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">In the sections below, we first briefly describe related work in motion tracking technologies for wheelchair users, followed by a description of WheelPoser’s pose estimation pipeline. Next, we describe our data collection process for the WheelPoser-IMU dataset, our evaluation results, and some exemplary applications. We end by discussing the limitations of our system and potential avenues for future work.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In this section, we review prior motion tracking systems with a focus on systems designed for wheelchair users.
We first look at general activity sensing technologies related to the movement of wheelchair users.
Then we review prior research on motion capture solutions for wheelchair users, including both camera-based and IMU-based solutions. Finally, we review pose estimation techniques that utilize sparse IMU measurements. For a more detailed review of motion capture technologies we refer readers to surveys by Desmarais et al. <cite class="ltx_cite ltx_citemacro_citep">(Desmarais et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib19" title="">2021</a>)</cite> and Nguyen et al. <cite class="ltx_cite ltx_citemacro_citep">(Nguyen and Kresovic, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib72" title="">2022</a>)</cite></p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Activity Sensing for Wheelchair Users</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Broadly in the field of motion tracking for wheelchair users, a substantial amount of research has been dedicated to tracking wheelchair kinematics and developing activity recognition techniques.
For instance, Rhodes et al. <cite class="ltx_cite ltx_citemacro_citep">(Rhodes et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib81" title="">2014</a>)</cite> developed a radio-frequency based localization system for tracking the field positions of manual wheelchair sports players.
Slikker et al. <cite class="ltx_cite ltx_citemacro_citep">(Van der Slikke et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib94" title="">2015</a>)</cite> explored the use of three wheelchair-mounted IMUs to measure motion features like speed and frame rotation during manual wheelchair sports. Similarly, SpokeSense <cite class="ltx_cite ltx_citemacro_citep">(Carrington et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib12" title="">2020</a>)</cite> built a sensing system that could track manual wheelchair motion data (speed zone, orientation change) and provide sports related contextual information (dribbling sound, game buzzer) for adaptive basketball analytics.
SMART<sup class="ltx_sup" id="S2.SS1.p1.1.1">wheel</sup> <cite class="ltx_cite ltx_citemacro_citep">(Cooper, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib16" title="">2009</a>)</cite> is a commercial solution that can monitor wheelchair speed and other propulsion metrics like push force and frequency. G-WRM <cite class="ltx_cite ltx_citemacro_citep">(Hiremath et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib30" title="">2013</a>)</cite> employs a wheel-mounted gyroscope to measure angular velocities of the wheel, thereby allowing for speed measurements with known wheel size. De Vries et al. <cite class="ltx_cite ltx_citemacro_citep">(de Vries et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib18" title="">2023</a>)</cite> also developed a three-IMU based system that could track the linear velocity, traveled distance, and magnitude of turns of a wheelchair.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Prior research on tracking the physical activities of wheelchair users has primarily focused on fitness, with a substantial body of work dedicated to quantifying the energy expenditure of manual wheelchair users <cite class="ltx_cite ltx_citemacro_citep">(Grillon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib27" title="">2017</a>; Learmonth et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib51" title="">2016</a>; Nightingale et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib73" title="">2015</a>; Hiremath et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib31" title="">2012</a>)</cite>. Additionally, researchers have investigated ways to track a set of discrete activities using cameras or IMUs, such as deskwork <cite class="ltx_cite ltx_citemacro_citep">(García-Massó et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib23" title="">2015</a>)</cite>, household activities <cite class="ltx_cite ltx_citemacro_citep">(Hiremath et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib32" title="">2015</a>; García-Massó et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib23" title="">2015</a>)</cite>, sitting posture <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib60" title="">2016</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib59" title="">2017</a>)</cite>, wheelchair propulsion <cite class="ltx_cite ltx_citemacro_citep">(French et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib22" title="">2008</a>; Herrera et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib29" title="">2018</a>; Chen and Morgan, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib13" title="">2018</a>; de Vries et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib18" title="">2023</a>; Hiremath et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib32" title="">2015</a>; García-Massó et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib23" title="">2015</a>)</cite>, and wheelchair transfers <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib96" title="">2021</a>; Barbareschi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib4" title="">2018</a>; García-Massó et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib23" title="">2015</a>)</cite>. More recently, commercially available fitness trackers - Apple Watch <cite class="ltx_cite ltx_citemacro_citep">(Apple, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib2" title="">2022</a>)</cite> and Garmin <cite class="ltx_cite ltx_citemacro_citep">(Technology, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib91" title="">2023</a>)</cite> have integrated wheelchair settings to measure wheelchair users’ calories burned, active minutes, number of pushes, etc.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">WheelPoser advances beyond prior work by providing a more continuous and comprehensive user representation, rather than focusing solely on coarse-grained activities or wheelchair kinematics. This high-fidelity user representation could further have a trickle-down effect on higher-level techniques like activity recognition <cite class="ltx_cite ltx_citemacro_citep">(Rajasegaran et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib79" title="">2023</a>; Choutas et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib14" title="">2018</a>; Shah et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib83" title="">2022</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Motion Capture for Wheelchair Users with Cameras</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Most commonly, motion capture for wheelchair users relies on commercial marker-based motion capture systems, such as Vicon <cite class="ltx_cite ltx_citemacro_citep">(Systems, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib89" title="">2023</a>)</cite> and OptiTrack <cite class="ltx_cite ltx_citemacro_citep">(Systems, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib88" title="">[n. d.]</a>)</cite>, which have been the gold standard for both industrial and research applications due to their millimeter level accuracy.
For instance, these systems have enabled extensive research into the biomechanics of wheelchair propulsion <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib55" title="">2004</a>; Kukla and Maliga, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib50" title="">2022</a>; Koontz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib49" title="">2002</a>; Collinger et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib15" title="">2008</a>)</cite> and transfer <cite class="ltx_cite ltx_citemacro_citep">(Nawoczenski et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib71" title="">2003</a>; Kankipati et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib43" title="">2015</a>; Tsai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib93" title="">2018</a>)</cite>, as well as the pathology of various upper extremity injuries faced by wheelchair users <cite class="ltx_cite ltx_citemacro_citep">(Briley et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib9" title="">2020</a>; Lewis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib52" title="">2018</a>; Jayaraman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib40" title="">2014</a>)</cite>. The cost and setup requirements of these systems however often prohibit consumer use.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">More recently, advancements in computer vision have led to the emergence of numerous markerless motion capture systems. RGB-based approaches have been developed to estimate both 2D <cite class="ltx_cite ltx_citemacro_citep">(Cao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib10" title="">2019</a>; Bazarevsky et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib5" title="">2020</a>; not listed, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib74" title="">2022</a>; MMPose Contributors, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib65" title="">2020</a>)</cite> and 3D <cite class="ltx_cite ltx_citemacro_citep">(Pavlakos et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib78" title="">2019</a>; Kanazawa et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib42" title="">2018</a>; Kocabas et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib46" title="">2020</a>; Goel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib25" title="">2023a</a>; Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib87" title="">2021</a>)</cite> body pose with varying levels of granularity. However, these systems fail for wheelchair users, often leading to unnatural, incorrect pose estimates, due to a lack of representation in the training data.
To enhance the diversity of training data, researchers have recently explored generating synthetic image data using game engines for model refinement <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib34" title="">2024</a>; Black et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib7" title="">2023</a>)</cite>, achieving varying levels of success.
Concurrently, depth-based systems have been employed by rehabilitation scientists to study the kinematics of wheelchair users.
Wei et al. <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib97" title="">2016</a>)</cite> demonstrated that skeletal positions tracked by a Microsoft Kinect could be used to differentiate between proper and improper wheelchair transfer techniques. Similarly, Rammer et al. <cite class="ltx_cite ltx_citemacro_citep">(Rammer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib80" title="">2018</a>)</cite> assessed the repeatability of the Kinect in extracting spatiotemporal parameters and joint range of motion during wheelchair propulsion and found high inter-trial repeatability. Furthermore, Milgrom et al. <cite class="ltx_cite ltx_citemacro_citep">(Milgrom et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib64" title="">2016</a>)</cite> examined the accuracy of the Kinect in tracking upper body joint angles during wheelchair propulsion compared to marker-based systems, finding discrepancies ranging from 6 to 26 degrees across different joints.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">While camera-based approaches have shown increasing promise, they raise privacy concerns, are often sensitive to occlusion, lighting conditions, and user appearances (clothing, hair), and most importantly require a fixed sensor location and pose tracking space, making them impractical for on-the-go motion capture.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Motion Capture for Wheelchair Users with Dense IMUs</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">An alternative to camera-based systems is to instrument the user with sensors such as IMUs. Motion tracking using IMUs offers numerous advantages – they are portable, robust to visual occlusion, work both indoors and outdoors, and are more privacy-preserving – making them extremely suitable for everyday motion capture.
Commercial IMU-based motion capture systems, such as Xsens <cite class="ltx_cite ltx_citemacro_citep">(Xsens, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib98" title="">[n. d.]</a>)</cite>, rely on the dense placement of IMUs (17 or more sensors) on each body joint to determine the orientations of these joints for pose estimation. Researchers have leveraged such setups to track wheelchair users’ upper body kinematics during wheelchair transfer <cite class="ltx_cite ltx_citemacro_citep">(Kondo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib48" title="">2024</a>)</cite> and various functional activities <cite class="ltx_cite ltx_citemacro_citep">(Starrs et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib86" title="">2012</a>)</cite>. Similarly, Hooke et al. <cite class="ltx_cite ltx_citemacro_citep">(Hooke et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib33" title="">2009</a>)</cite> focused on tracking single-arm kinematics during wheelchair propulsion by placing three IMUs on the user’s arm and one on their trunk.
However, the substantial quantity of sensors involved is intrusive for users and necessitates a time-consuming setup process, which significantly reduces the practicality and usability of such systems.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">In contrast, WheelPoser employs a minimally intrusive setup with just four IMUs to achieve accurate upper body pose estimation. Our approach significantly enhances the practicality while retaining all the benefits of IMU-based solutions.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4. </span>Motion Capture with Sparse IMUs</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">To improve the usability and practicality of IMU-based motion capture systems, researchers have recently started to investigate ways to leverage sparse inertial sensors for pose estimation.
Early work placed accelerometers on the wrists, trunk, and ankles <cite class="ltx_cite ltx_citemacro_citep">(Riaz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib82" title="">2015</a>; Slyper and Hodgins, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib85" title="">2008</a>; Tautges et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib90" title="">2011</a>)</cite> and estimated pose by searching for similar accelerations in a prerecorded motion database.
SIP <cite class="ltx_cite ltx_citemacro_citep">(Von Marcard et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib95" title="">2017</a>)</cite> instead employed an optimization-based method to achieve offline full-body pose estimation with only 6 IMUs.
With the release of the large AMASS dataset (over 60 hours of motion sequences) <cite class="ltx_cite ltx_citemacro_citep">(Mahmood et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib62" title="">2019</a>)</cite>, researchers started using deep learning based methods to predict body pose from IMU measurements.
Specifically, DIP <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib35" title="">2018</a>)</cite> first adopted a bidirectional recurrent neural network (biRNN) to learn the mapping between IMU measurements and joint rotations from extensive motion capture datasets and showed a real-time human pose estimation with 6 IMUs.
TransPose <cite class="ltx_cite ltx_citemacro_citep">(Yi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib100" title="">2021</a>)</cite>, PIP <cite class="ltx_cite ltx_citemacro_citep">(Yi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib99" title="">2022</a>)</cite>, TIP <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib41" title="">2022</a>)</cite> and IMUPoser <cite class="ltx_cite ltx_citemacro_citep">(Mollyn et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib66" title="">2023</a>)</cite> all built upon this by introducing new model architectures, prediction tasks and sensor locations to improve pose estimation.
However, these systems are only designed for and evaluated with people without motor impairments. They all require placing IMU sensors on users’ pelvis and legs, which is often challenging and uncomfortable for wheelchair users. Moreover, all existing systems are trained on motion capture datasets predominantly composed of standing and walking motions, which completely lack representation of the diverse range of wheelchair-related motions <cite class="ltx_cite ltx_citemacro_citep">(Cotton, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib17" title="">2020</a>)</cite>.
Consequently, these limitations make them unusable for wheelchair users and ill-equipped to capture their poses accurately.</p>
</div>
<div class="ltx_para" id="S2.SS4.p2">
<p class="ltx_p" id="S2.SS4.p2.1">In comparison, WheelPoser serves as the first-of-its-kind sparse-IMU-based motion capture system designed specifically for wheelchair users, from device setup and data synthesis to pose estimation. We also collect a novel motion capture dataset consisting of 167 minutes of on-wheelchair human motions and paired IMU data. This dataset is larger than the combined sizes of similar datasets in ambulatory settings (DIP <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib35" title="">2018</a>)</cite> and TotalCapture <cite class="ltx_cite ltx_citemacro_citep">(Trumble et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib92" title="">2017</a>)</cite>), and serves as an initial step towards creating inclusive motion capture datasets.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>WheelPoser System</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">The primary design goals of our system are twofold: 1) to employ a sparse-IMU setup for enhanced usability and practicality, and 2) to achieve comprehensive upper body pose estimation in real-time to support a wide range of applications. This combination presents inherent challenges due to the ambiguity in mapping sparse inertial measurements to a broader set of joint angles. Although prior research has demonstrated the feasibility of using deep neural networks to learn this intricate mapping from large-scale motion capture datasets, there is currently no publicly available motion capture dataset of wheelchair users, let alone a large-scale dataset sufficient for deep neural network training.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">We address these gaps and challenges from the following three perspectives: 1) we introduce a sparse-IMU configuration that captures the essential motions of both the user and the wheelchair while minimizing intrusiveness, 2) we propose a data synthesis and pose estimation pipeline that builds upon this device setup and enables us to leverage high-quantity and diverse upper body motions in existing datasets for wheelchair user pose estimation, and 3) we collect a novel motion capture dataset covering a wide range of essential wheelchair-related activities and fine-tune our pre-trained pose estimation model to bridge the gap between on-wheelchair activities and existing motion types.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Device Setup</h3>
<figure class="ltx_figure" id="S3.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="270" id="S3.F2.g1" src="extracted/5851880/camera-ready/device_setup.png" width="568"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.2.1.1" style="font-size:90%;">Figure 2</span>. </span><span class="ltx_text" id="S3.F2.3.2" style="font-size:90%;">WheelPoser uses four IMUs strategically placed on the user’s forearms and head, as well as their wheelchair.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S3.F2.4">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S3.F2.5">This figure illustrates the device setup for WheelPoser, featuring two side-by-side photographs of the same person seated in a manual wheelchair. On the left, two IMUs are attached to the user’s wrists. On the right, one IMU is attached to the back of the user’s head, and another is attached to the wheelchair’s axle.</p>
</div>
</div>
</figure>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Our system comprises four IMUs strategically positioned on the user’s body and their wheelchair to capture crucial aspects of the user’s upper body motion (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.08494v1#S3.F2" title="Figure 2 ‣ 3.1. Device Setup ‣ 3. WheelPoser System ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>). Specifically, one IMU is placed on the user’s head, primarily to capture the user’s head movements but also certain upper body motions. The user’s forearms are also instrumented with one IMU each to capture arm movements.
Traditionally, capturing the motion of the user’s torso requires an additional IMU placed on their pelvis <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib35" title="">2018</a>; Yi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib100" title="">2021</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib99" title="">2022</a>; Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib41" title="">2022</a>)</cite>. However, for wheelchair users, we recognize that wearing a sensor on the pelvis could be not only inconvenient but also uncomfortable, considering their seated posture and the presence of wheelchair backrests for trunk support. Therefore, we propose to position this sensor at the center of the wheelchair axle, which is a standard component in modern wheelchairs. Our intuition is that as modern wheelchairs are designed to provide wheelchair users with spinal stabilization, the wheelchair’s motion will exhibit strong correlations with the user’s pelvis motion.
To validate our intuition, we conducted a preliminary study.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1. </span>Preliminary Study: Examining Wheelchair-Pelvis Motion Correlation</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">To examine our intuition, we conducted a preliminary study with three participants without motor impairments (two males and one female) sitting on a provided manual wheelchair. The participants were asked to perform a diverse set of typical wheelchair motions, such as wheelchair propulsion and turning (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S4.SS2" title="4.2. Participants and Procedure ‣ 4. WheelPoser-IMU Dataset ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_tag">4.2</span></a>).
In addition to the four IMUs of our device setup, we attached the fifth IMU to the user’s pelvis to capture the actual pelvis motion for comparison.
All five IMUs were time-synchronized and streamed at 30Hz, resulting in a total of 100 minutes of motion data.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p2">
<p class="ltx_p" id="S3.SS1.SSS1.p2.1">Our analysis focused on two main aspects. First, we examined the similarity of orientations and accelerations between the user’s pelvis and wheelchair. Second, interrelated with our later discussed data synthesis and pose estimation pipeline (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S3.SS2.SSS1" title="3.2.1. Data Synthesis ‣ 3.2. Learning-based Kinematics Module ‣ 3. WheelPoser System ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>), we investigated the correlation between the relative accelerations of upper body joints (specifically the wrists and head) in relation to the wheelchair and the user’s pelvis. For ease of reference, we term this type of acceleration as <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS1.p2.1.1">”normalized acceleration”</span>.
Additionally, all IMU signals were transformed from the sensor-coordinate frame to a body-centric frame for analysis, where the x-axis points to the left, the y-axis points upward, and the z-axis points forward, ensuring consistency with the human kinematic model used in our study (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S3.SS2.SSS1" title="3.2.1. Data Synthesis ‣ 3.2. Learning-based Kinematics Module ‣ 3. WheelPoser System ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>).</p>
</div>
<figure class="ltx_figure" id="S3.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="213" id="S3.F3.g1" src="x1.png" width="623"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.2.1.1" style="font-size:90%;">Figure 3</span>. </span><span class="ltx_text" id="S3.F3.3.2" style="font-size:90%;">Sample orientation measurements (Euler angles) of two IMUs attached to the user’s pelvis and the wheelchair’s central axle. The orientations are highly correlated, affirming the close relationship between the pelvis and wheelchair axle motion.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S3.F3.4">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S3.F3.5">This figure presents three line charts of orientation signals from two IMUs. In each chart, two series of signals are displayed: one from the IMU attached to the user’s pelvis and another from the IMU attached to the wheelchair. The figure demonstrates that the orientation signals from the two IMUs are closely aligned, indicating a strong correlation.</p>
</div>
</div>
</figure>
<figure class="ltx_figure" id="S3.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="142" id="S3.F4.g1" src="extracted/5851880/camera-ready/acc_plot.png" width="479"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.2.1.1" style="font-size:90%;">Figure 4</span>. </span><span class="ltx_text" id="S3.F4.3.2" style="font-size:90%;">Correlation coefficient matrices displaying normalized accelerations of the left wrist (LW), right wrist (RW), head (H), pelvis (P), and wheelchair (C), where, for instance, LW-C represents the normalized acceleration of the left wrist with respect to the wheelchair. normalized accelerations with respect to the pelvis and wheelchair are highly correlated.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S3.F4.4">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S3.F4.5">This figure displays the correlation coefficient matrix for eight different acceleration signals. From top to bottom, they include: normalized left wrist acceleration relative to the wheelchair, normalized right wrist acceleration relative to the wheelchair, normalized head acceleration relative to the wheelchair, normalized left wrist acceleration relative to the pelvis, normalized right wrist acceleration relative to the pelvis, normalized head acceleration relative to the pelvis, acceleration of the wheelchair, and acceleration of the pelvis. There are three matrices depicting correlations along the x, y, and z axes, respectively. The figure indicates that the normalized accelerations are strongly correlated, and the accelerations of the wheelchair and the pelvis are also closely correlated.</p>
</div>
</div>
</figure>
<div class="ltx_para" id="S3.SS1.SSS1.p3">
<p class="ltx_p" id="S3.SS1.SSS1.p3.4"><span class="ltx_text ltx_font_italic" id="S3.SS1.SSS1.p3.4.1">Orientation:</span>
To measure the similarities between the orientations of the user’s pelvis and the wheelchair, we used the distance metric (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.08494v1#S3.E1" title="1 ‣ 3.1.1. Preliminary Study: Examining Wheelchair-Pelvis Motion Correlation ‣ 3.1. Device Setup ‣ 3. WheelPoser System ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_tag">Equation 1</span></a>) proposed by Huynh <cite class="ltx_cite ltx_citemacro_citep">(Huynh, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib36" title="">2009</a>)</cite> where <math alttext="q_{1}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p3.1.m1.1"><semantics id="S3.SS1.SSS1.p3.1.m1.1a"><msub id="S3.SS1.SSS1.p3.1.m1.1.1" xref="S3.SS1.SSS1.p3.1.m1.1.1.cmml"><mi id="S3.SS1.SSS1.p3.1.m1.1.1.2" xref="S3.SS1.SSS1.p3.1.m1.1.1.2.cmml">q</mi><mn id="S3.SS1.SSS1.p3.1.m1.1.1.3" xref="S3.SS1.SSS1.p3.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p3.1.m1.1b"><apply id="S3.SS1.SSS1.p3.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.SSS1.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.SSS1.p3.1.m1.1.1.2">𝑞</ci><cn id="S3.SS1.SSS1.p3.1.m1.1.1.3.cmml" type="integer" xref="S3.SS1.SSS1.p3.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p3.1.m1.1c">q_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p3.1.m1.1d">italic_q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="q_{2}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p3.2.m2.1"><semantics id="S3.SS1.SSS1.p3.2.m2.1a"><msub id="S3.SS1.SSS1.p3.2.m2.1.1" xref="S3.SS1.SSS1.p3.2.m2.1.1.cmml"><mi id="S3.SS1.SSS1.p3.2.m2.1.1.2" xref="S3.SS1.SSS1.p3.2.m2.1.1.2.cmml">q</mi><mn id="S3.SS1.SSS1.p3.2.m2.1.1.3" xref="S3.SS1.SSS1.p3.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p3.2.m2.1b"><apply id="S3.SS1.SSS1.p3.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p3.2.m2.1.1.1.cmml" xref="S3.SS1.SSS1.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p3.2.m2.1.1.2.cmml" xref="S3.SS1.SSS1.p3.2.m2.1.1.2">𝑞</ci><cn id="S3.SS1.SSS1.p3.2.m2.1.1.3.cmml" type="integer" xref="S3.SS1.SSS1.p3.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p3.2.m2.1c">q_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p3.2.m2.1d">italic_q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> are unit quaternions, and the function <math alttext="\phi" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p3.3.m3.1"><semantics id="S3.SS1.SSS1.p3.3.m3.1a"><mi id="S3.SS1.SSS1.p3.3.m3.1.1" xref="S3.SS1.SSS1.p3.3.m3.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p3.3.m3.1b"><ci id="S3.SS1.SSS1.p3.3.m3.1.1.cmml" xref="S3.SS1.SSS1.p3.3.m3.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p3.3.m3.1c">\phi</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p3.3.m3.1d">italic_ϕ</annotation></semantics></math> gives values in the range <math alttext="[0,1]" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p3.4.m4.2"><semantics id="S3.SS1.SSS1.p3.4.m4.2a"><mrow id="S3.SS1.SSS1.p3.4.m4.2.3.2" xref="S3.SS1.SSS1.p3.4.m4.2.3.1.cmml"><mo id="S3.SS1.SSS1.p3.4.m4.2.3.2.1" stretchy="false" xref="S3.SS1.SSS1.p3.4.m4.2.3.1.cmml">[</mo><mn id="S3.SS1.SSS1.p3.4.m4.1.1" xref="S3.SS1.SSS1.p3.4.m4.1.1.cmml">0</mn><mo id="S3.SS1.SSS1.p3.4.m4.2.3.2.2" xref="S3.SS1.SSS1.p3.4.m4.2.3.1.cmml">,</mo><mn id="S3.SS1.SSS1.p3.4.m4.2.2" xref="S3.SS1.SSS1.p3.4.m4.2.2.cmml">1</mn><mo id="S3.SS1.SSS1.p3.4.m4.2.3.2.3" stretchy="false" xref="S3.SS1.SSS1.p3.4.m4.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p3.4.m4.2b"><interval closure="closed" id="S3.SS1.SSS1.p3.4.m4.2.3.1.cmml" xref="S3.SS1.SSS1.p3.4.m4.2.3.2"><cn id="S3.SS1.SSS1.p3.4.m4.1.1.cmml" type="integer" xref="S3.SS1.SSS1.p3.4.m4.1.1">0</cn><cn id="S3.SS1.SSS1.p3.4.m4.2.2.cmml" type="integer" xref="S3.SS1.SSS1.p3.4.m4.2.2">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p3.4.m4.2c">[0,1]</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p3.4.m4.2d">[ 0 , 1 ]</annotation></semantics></math>, with 0 representing that the two quaternions are identical. Using this metric, the average distance between the orientation of the user’s pelvis and the orientation of the wheelchair is 0.007 (std = 0.011), indicating that the two orientations are indeed quite similar.
We also qualitatively examined this similarity by using their Euler angle representations and found consistent correlations across all samples (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.08494v1#S3.F3" title="Figure 3 ‣ 3.1.1. Preliminary Study: Examining Wheelchair-Pelvis Motion Correlation ‣ 3.1. Device Setup ‣ 3. WheelPoser System ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure 3</span></a>).</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\phi(q_{1},q_{2})=1-|q_{1}\cdot q_{2}|" class="ltx_Math" display="block" id="S3.E1.m1.3"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml"><mrow id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml"><mi id="S3.E1.m1.2.2.2.4" xref="S3.E1.m1.2.2.2.4.cmml">ϕ</mi><mo id="S3.E1.m1.2.2.2.3" xref="S3.E1.m1.2.2.2.3.cmml">⁢</mo><mrow id="S3.E1.m1.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.3.cmml"><mo id="S3.E1.m1.2.2.2.2.2.3" stretchy="false" xref="S3.E1.m1.2.2.2.2.3.cmml">(</mo><msub id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.2.cmml">q</mi><mn id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.E1.m1.2.2.2.2.2.4" xref="S3.E1.m1.2.2.2.2.3.cmml">,</mo><msub id="S3.E1.m1.2.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.2.cmml"><mi id="S3.E1.m1.2.2.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.2.2.cmml">q</mi><mn id="S3.E1.m1.2.2.2.2.2.2.3" xref="S3.E1.m1.2.2.2.2.2.2.3.cmml">2</mn></msub><mo id="S3.E1.m1.2.2.2.2.2.5" stretchy="false" xref="S3.E1.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.3.3.4" xref="S3.E1.m1.3.3.4.cmml">=</mo><mrow id="S3.E1.m1.3.3.3" xref="S3.E1.m1.3.3.3.cmml"><mn id="S3.E1.m1.3.3.3.3" xref="S3.E1.m1.3.3.3.3.cmml">1</mn><mo id="S3.E1.m1.3.3.3.2" xref="S3.E1.m1.3.3.3.2.cmml">−</mo><mrow id="S3.E1.m1.3.3.3.1.1" xref="S3.E1.m1.3.3.3.1.2.cmml"><mo id="S3.E1.m1.3.3.3.1.1.2" stretchy="false" xref="S3.E1.m1.3.3.3.1.2.1.cmml">|</mo><mrow id="S3.E1.m1.3.3.3.1.1.1" xref="S3.E1.m1.3.3.3.1.1.1.cmml"><msub id="S3.E1.m1.3.3.3.1.1.1.2" xref="S3.E1.m1.3.3.3.1.1.1.2.cmml"><mi id="S3.E1.m1.3.3.3.1.1.1.2.2" xref="S3.E1.m1.3.3.3.1.1.1.2.2.cmml">q</mi><mn id="S3.E1.m1.3.3.3.1.1.1.2.3" xref="S3.E1.m1.3.3.3.1.1.1.2.3.cmml">1</mn></msub><mo id="S3.E1.m1.3.3.3.1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.E1.m1.3.3.3.1.1.1.1.cmml">⋅</mo><msub id="S3.E1.m1.3.3.3.1.1.1.3" xref="S3.E1.m1.3.3.3.1.1.1.3.cmml"><mi id="S3.E1.m1.3.3.3.1.1.1.3.2" xref="S3.E1.m1.3.3.3.1.1.1.3.2.cmml">q</mi><mn id="S3.E1.m1.3.3.3.1.1.1.3.3" xref="S3.E1.m1.3.3.3.1.1.1.3.3.cmml">2</mn></msub></mrow><mo id="S3.E1.m1.3.3.3.1.1.3" stretchy="false" xref="S3.E1.m1.3.3.3.1.2.1.cmml">|</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3"><eq id="S3.E1.m1.3.3.4.cmml" xref="S3.E1.m1.3.3.4"></eq><apply id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2"><times id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.3"></times><ci id="S3.E1.m1.2.2.2.4.cmml" xref="S3.E1.m1.2.2.2.4">italic-ϕ</ci><interval closure="open" id="S3.E1.m1.2.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.2.2"><apply id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2">𝑞</ci><cn id="S3.E1.m1.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.E1.m1.1.1.1.1.1.1.3">1</cn></apply><apply id="S3.E1.m1.2.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.2.2.2">subscript</csymbol><ci id="S3.E1.m1.2.2.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2">𝑞</ci><cn id="S3.E1.m1.2.2.2.2.2.2.3.cmml" type="integer" xref="S3.E1.m1.2.2.2.2.2.2.3">2</cn></apply></interval></apply><apply id="S3.E1.m1.3.3.3.cmml" xref="S3.E1.m1.3.3.3"><minus id="S3.E1.m1.3.3.3.2.cmml" xref="S3.E1.m1.3.3.3.2"></minus><cn id="S3.E1.m1.3.3.3.3.cmml" type="integer" xref="S3.E1.m1.3.3.3.3">1</cn><apply id="S3.E1.m1.3.3.3.1.2.cmml" xref="S3.E1.m1.3.3.3.1.1"><abs id="S3.E1.m1.3.3.3.1.2.1.cmml" xref="S3.E1.m1.3.3.3.1.1.2"></abs><apply id="S3.E1.m1.3.3.3.1.1.1.cmml" xref="S3.E1.m1.3.3.3.1.1.1"><ci id="S3.E1.m1.3.3.3.1.1.1.1.cmml" xref="S3.E1.m1.3.3.3.1.1.1.1">⋅</ci><apply id="S3.E1.m1.3.3.3.1.1.1.2.cmml" xref="S3.E1.m1.3.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.3.1.1.1.2.1.cmml" xref="S3.E1.m1.3.3.3.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.3.3.3.1.1.1.2.2.cmml" xref="S3.E1.m1.3.3.3.1.1.1.2.2">𝑞</ci><cn id="S3.E1.m1.3.3.3.1.1.1.2.3.cmml" type="integer" xref="S3.E1.m1.3.3.3.1.1.1.2.3">1</cn></apply><apply id="S3.E1.m1.3.3.3.1.1.1.3.cmml" xref="S3.E1.m1.3.3.3.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.3.1.1.1.3.1.cmml" xref="S3.E1.m1.3.3.3.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.3.3.3.1.1.1.3.2.cmml" xref="S3.E1.m1.3.3.3.1.1.1.3.2">𝑞</ci><cn id="S3.E1.m1.3.3.3.1.1.1.3.3.cmml" type="integer" xref="S3.E1.m1.3.3.3.1.1.1.3.3">2</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">\phi(q_{1},q_{2})=1-|q_{1}\cdot q_{2}|</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.3d">italic_ϕ ( italic_q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) = 1 - | italic_q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ⋅ italic_q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT |</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p4">
<p class="ltx_p" id="S3.SS1.SSS1.p4.1"><span class="ltx_text ltx_font_italic" id="S3.SS1.SSS1.p4.1.1">Acceleration:</span>
For accelerations, we assessed similarity by calculating correlation coefficients along each axis (ranging from 0 to 1, with 1 indicating identical signals).
Here and throughout the rest of this paper, we note that ”acceleration” refers specifically to gravity-subtracted linear acceleration.
The resulting coefficient matrices for each axis are illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S3.F4" title="Figure 4 ‣ 3.1.1. Preliminary Study: Examining Wheelchair-Pelvis Motion Correlation ‣ 3.1. Device Setup ‣ 3. WheelPoser System ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_tag">4</span></a>. Specifically, the coefficients between the user’s pelvis acceleration and the wheelchair’s acceleration are 0.67, 0.38, and 0.70, respectively, indicating a fairly strong correlation. The lower coefficients along the y-axis can be attributed to noise, as there is minimal motion along the axis perpendicular to the ground. For the normalized accelerations of the user’s wrists and head with respect to the user’s pelvis and the wheelchair, the average coefficients for the left wrist, right wrist, and head are 0.989, 0.99, and 0.926, respectively, which demonstrate strong correlations for pose estimation.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p5">
<p class="ltx_p" id="S3.SS1.SSS1.p5.1"><span class="ltx_text ltx_font_italic" id="S3.SS1.SSS1.p5.1.1">Summary: </span>
Based on this preliminary study, we have confirmed the correlation between the motion of the user’s pelvis and that of the wheelchair, validating the effectiveness of our device setup for capturing the upper body pose of wheelchair users. Additionally, the correlations observed between the normalized accelerations hold promise for leveraging diverse upper body motions in existing datasets collected from individuals without motor impairments. Furthermore, we envision that this configuration could be seamlessly integrated with devices that users already carry on a daily basis, such as smartwatches and earbuds, as well as with low-cost devices in the chairable form factor <cite class="ltx_cite ltx_citemacro_citep">(Carrington et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib11" title="">2014</a>; Mollyn et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib66" title="">2023</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Learning-based Kinematics Module</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">WheelPoser uses a deep neural network to predict upper body joint angles from sparse-IMU measurements. We propose a data synthesis and pose estimation pipeline that enables us
to first train a base model with a broad understanding of upper body pose patterns using diverse upper body motions found in existing datasets and later fine-tune this model with a small amount of data to bridge the gap between existing motions and wheelchair mobility. Here we describe our data synthesis pipeline followed by the architectures of three neural networks we adapt from SOTA pose estimation models using sparse-IMUs.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1. </span>Data Synthesis</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.7">To train the neural networks in our kinematics module, we need a sufficiently large dataset that contains both IMU measurements and human pose ground truths.
Following previous work, we adopt the SMPL body model <cite class="ltx_cite ltx_citemacro_citep">(Loper et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib57" title="">2023</a>)</cite> as our kinematic model and the AMASS dataset <cite class="ltx_cite ltx_citemacro_citep">(Mahmood et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib62" title="">2019</a>)</cite> to train our base model. The AMASS dataset is a composition of existing motion capture datasets and contains SMPL pose parameters for more than 60 hours of motions performed by over 300 subjects without motor impairments. However, as the AMASS dataset does not contain IMU sensor data, we synthesize inertial data by attaching virtual IMUs to the corresponding vertices of our selected joints on the SMPL mesh, similar to DIP <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib35" title="">2018</a>)</cite>. We then calculate the sequential positions, accelerations, and rotations of these vertices in the global reference frame using the SMPL body model (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.08494v1#S3.E2" title="2 ‣ 3.2.1. Data Synthesis ‣ 3.2. Learning-based Kinematics Module ‣ 3. WheelPoser System ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_tag">Equation 2</span></a>).
Here, <math alttext="x_{i}(t)" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.1.m1.1"><semantics id="S3.SS2.SSS1.p1.1.m1.1a"><mrow id="S3.SS2.SSS1.p1.1.m1.1.2" xref="S3.SS2.SSS1.p1.1.m1.1.2.cmml"><msub id="S3.SS2.SSS1.p1.1.m1.1.2.2" xref="S3.SS2.SSS1.p1.1.m1.1.2.2.cmml"><mi id="S3.SS2.SSS1.p1.1.m1.1.2.2.2" xref="S3.SS2.SSS1.p1.1.m1.1.2.2.2.cmml">x</mi><mi id="S3.SS2.SSS1.p1.1.m1.1.2.2.3" xref="S3.SS2.SSS1.p1.1.m1.1.2.2.3.cmml">i</mi></msub><mo id="S3.SS2.SSS1.p1.1.m1.1.2.1" xref="S3.SS2.SSS1.p1.1.m1.1.2.1.cmml">⁢</mo><mrow id="S3.SS2.SSS1.p1.1.m1.1.2.3.2" xref="S3.SS2.SSS1.p1.1.m1.1.2.cmml"><mo id="S3.SS2.SSS1.p1.1.m1.1.2.3.2.1" stretchy="false" xref="S3.SS2.SSS1.p1.1.m1.1.2.cmml">(</mo><mi id="S3.SS2.SSS1.p1.1.m1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.cmml">t</mi><mo id="S3.SS2.SSS1.p1.1.m1.1.2.3.2.2" stretchy="false" xref="S3.SS2.SSS1.p1.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.1.m1.1b"><apply id="S3.SS2.SSS1.p1.1.m1.1.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.2"><times id="S3.SS2.SSS1.p1.1.m1.1.2.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.2.1"></times><apply id="S3.SS2.SSS1.p1.1.m1.1.2.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.1.m1.1.2.2.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.2.2">subscript</csymbol><ci id="S3.SS2.SSS1.p1.1.m1.1.2.2.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.2.2.2">𝑥</ci><ci id="S3.SS2.SSS1.p1.1.m1.1.2.2.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.2.2.3">𝑖</ci></apply><ci id="S3.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.1.m1.1c">x_{i}(t)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.1.m1.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_t )</annotation></semantics></math> and <math alttext="a_{i}(t)" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.2.m2.1"><semantics id="S3.SS2.SSS1.p1.2.m2.1a"><mrow id="S3.SS2.SSS1.p1.2.m2.1.2" xref="S3.SS2.SSS1.p1.2.m2.1.2.cmml"><msub id="S3.SS2.SSS1.p1.2.m2.1.2.2" xref="S3.SS2.SSS1.p1.2.m2.1.2.2.cmml"><mi id="S3.SS2.SSS1.p1.2.m2.1.2.2.2" xref="S3.SS2.SSS1.p1.2.m2.1.2.2.2.cmml">a</mi><mi id="S3.SS2.SSS1.p1.2.m2.1.2.2.3" xref="S3.SS2.SSS1.p1.2.m2.1.2.2.3.cmml">i</mi></msub><mo id="S3.SS2.SSS1.p1.2.m2.1.2.1" xref="S3.SS2.SSS1.p1.2.m2.1.2.1.cmml">⁢</mo><mrow id="S3.SS2.SSS1.p1.2.m2.1.2.3.2" xref="S3.SS2.SSS1.p1.2.m2.1.2.cmml"><mo id="S3.SS2.SSS1.p1.2.m2.1.2.3.2.1" stretchy="false" xref="S3.SS2.SSS1.p1.2.m2.1.2.cmml">(</mo><mi id="S3.SS2.SSS1.p1.2.m2.1.1" xref="S3.SS2.SSS1.p1.2.m2.1.1.cmml">t</mi><mo id="S3.SS2.SSS1.p1.2.m2.1.2.3.2.2" stretchy="false" xref="S3.SS2.SSS1.p1.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.2.m2.1b"><apply id="S3.SS2.SSS1.p1.2.m2.1.2.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.2"><times id="S3.SS2.SSS1.p1.2.m2.1.2.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.2.1"></times><apply id="S3.SS2.SSS1.p1.2.m2.1.2.2.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.2.m2.1.2.2.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.2.2">subscript</csymbol><ci id="S3.SS2.SSS1.p1.2.m2.1.2.2.2.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.2.2.2">𝑎</ci><ci id="S3.SS2.SSS1.p1.2.m2.1.2.2.3.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.2.2.3">𝑖</ci></apply><ci id="S3.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.2.m2.1c">a_{i}(t)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.2.m2.1d">italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_t )</annotation></semantics></math> represent the position and the acceleration of the <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.3.m3.1"><semantics id="S3.SS2.SSS1.p1.3.m3.1a"><mi id="S3.SS2.SSS1.p1.3.m3.1.1" xref="S3.SS2.SSS1.p1.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.3.m3.1b"><ci id="S3.SS2.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.3.m3.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.3.m3.1d">italic_i</annotation></semantics></math> th IMU at the frame <math alttext="t" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.4.m4.1"><semantics id="S3.SS2.SSS1.p1.4.m4.1a"><mi id="S3.SS2.SSS1.p1.4.m4.1.1" xref="S3.SS2.SSS1.p1.4.m4.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.4.m4.1b"><ci id="S3.SS2.SSS1.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.4.m4.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.4.m4.1d">italic_t</annotation></semantics></math> respectively, and <math alttext="\Delta t" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.5.m5.1"><semantics id="S3.SS2.SSS1.p1.5.m5.1a"><mrow id="S3.SS2.SSS1.p1.5.m5.1.1" xref="S3.SS2.SSS1.p1.5.m5.1.1.cmml"><mi id="S3.SS2.SSS1.p1.5.m5.1.1.2" mathvariant="normal" xref="S3.SS2.SSS1.p1.5.m5.1.1.2.cmml">Δ</mi><mo id="S3.SS2.SSS1.p1.5.m5.1.1.1" xref="S3.SS2.SSS1.p1.5.m5.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS1.p1.5.m5.1.1.3" xref="S3.SS2.SSS1.p1.5.m5.1.1.3.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.5.m5.1b"><apply id="S3.SS2.SSS1.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS1.p1.5.m5.1.1"><times id="S3.SS2.SSS1.p1.5.m5.1.1.1.cmml" xref="S3.SS2.SSS1.p1.5.m5.1.1.1"></times><ci id="S3.SS2.SSS1.p1.5.m5.1.1.2.cmml" xref="S3.SS2.SSS1.p1.5.m5.1.1.2">Δ</ci><ci id="S3.SS2.SSS1.p1.5.m5.1.1.3.cmml" xref="S3.SS2.SSS1.p1.5.m5.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.5.m5.1c">\Delta t</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.5.m5.1d">roman_Δ italic_t</annotation></semantics></math> represents the time interval between two adjacent frames which is <math alttext="\frac{1}{60}s" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.6.m6.1"><semantics id="S3.SS2.SSS1.p1.6.m6.1a"><mrow id="S3.SS2.SSS1.p1.6.m6.1.1" xref="S3.SS2.SSS1.p1.6.m6.1.1.cmml"><mfrac id="S3.SS2.SSS1.p1.6.m6.1.1.2" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.cmml"><mn id="S3.SS2.SSS1.p1.6.m6.1.1.2.2" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.2.cmml">1</mn><mn id="S3.SS2.SSS1.p1.6.m6.1.1.2.3" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.3.cmml">60</mn></mfrac><mo id="S3.SS2.SSS1.p1.6.m6.1.1.1" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS1.p1.6.m6.1.1.3" xref="S3.SS2.SSS1.p1.6.m6.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.6.m6.1b"><apply id="S3.SS2.SSS1.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1"><times id="S3.SS2.SSS1.p1.6.m6.1.1.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.1"></times><apply id="S3.SS2.SSS1.p1.6.m6.1.1.2.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.2"><divide id="S3.SS2.SSS1.p1.6.m6.1.1.2.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.2"></divide><cn id="S3.SS2.SSS1.p1.6.m6.1.1.2.2.cmml" type="integer" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.2">1</cn><cn id="S3.SS2.SSS1.p1.6.m6.1.1.2.3.cmml" type="integer" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.3">60</cn></apply><ci id="S3.SS2.SSS1.p1.6.m6.1.1.3.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.6.m6.1c">\frac{1}{60}s</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.6.m6.1d">divide start_ARG 1 end_ARG start_ARG 60 end_ARG italic_s</annotation></semantics></math>, as we set the frame rate to 60 Hz. Following prior work, we set <math alttext="n=4" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.7.m7.1"><semantics id="S3.SS2.SSS1.p1.7.m7.1a"><mrow id="S3.SS2.SSS1.p1.7.m7.1.1" xref="S3.SS2.SSS1.p1.7.m7.1.1.cmml"><mi id="S3.SS2.SSS1.p1.7.m7.1.1.2" xref="S3.SS2.SSS1.p1.7.m7.1.1.2.cmml">n</mi><mo id="S3.SS2.SSS1.p1.7.m7.1.1.1" xref="S3.SS2.SSS1.p1.7.m7.1.1.1.cmml">=</mo><mn id="S3.SS2.SSS1.p1.7.m7.1.1.3" xref="S3.SS2.SSS1.p1.7.m7.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.7.m7.1b"><apply id="S3.SS2.SSS1.p1.7.m7.1.1.cmml" xref="S3.SS2.SSS1.p1.7.m7.1.1"><eq id="S3.SS2.SSS1.p1.7.m7.1.1.1.cmml" xref="S3.SS2.SSS1.p1.7.m7.1.1.1"></eq><ci id="S3.SS2.SSS1.p1.7.m7.1.1.2.cmml" xref="S3.SS2.SSS1.p1.7.m7.1.1.2">𝑛</ci><cn id="S3.SS2.SSS1.p1.7.m7.1.1.3.cmml" type="integer" xref="S3.SS2.SSS1.p1.7.m7.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.7.m7.1c">n=4</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.7.m7.1d">italic_n = 4</annotation></semantics></math> to mitigate motion jitters in the data.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p2">
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="a_{i}(t)=\frac{x_{i}(t-n)+x_{i}(t+n)-2x_{i}(t)}{(n\Delta t)^{2}},i=1,2,3,4" class="ltx_Math" display="block" id="S3.E2.m1.11"><semantics id="S3.E2.m1.11a"><mrow id="S3.E2.m1.11.11.2" xref="S3.E2.m1.11.11.3.cmml"><mrow id="S3.E2.m1.10.10.1.1" xref="S3.E2.m1.10.10.1.1.cmml"><mrow id="S3.E2.m1.10.10.1.1.2" xref="S3.E2.m1.10.10.1.1.2.cmml"><msub id="S3.E2.m1.10.10.1.1.2.2" xref="S3.E2.m1.10.10.1.1.2.2.cmml"><mi id="S3.E2.m1.10.10.1.1.2.2.2" xref="S3.E2.m1.10.10.1.1.2.2.2.cmml">a</mi><mi id="S3.E2.m1.10.10.1.1.2.2.3" xref="S3.E2.m1.10.10.1.1.2.2.3.cmml">i</mi></msub><mo id="S3.E2.m1.10.10.1.1.2.1" xref="S3.E2.m1.10.10.1.1.2.1.cmml">⁢</mo><mrow id="S3.E2.m1.10.10.1.1.2.3.2" xref="S3.E2.m1.10.10.1.1.2.cmml"><mo id="S3.E2.m1.10.10.1.1.2.3.2.1" stretchy="false" xref="S3.E2.m1.10.10.1.1.2.cmml">(</mo><mi id="S3.E2.m1.5.5" xref="S3.E2.m1.5.5.cmml">t</mi><mo id="S3.E2.m1.10.10.1.1.2.3.2.2" stretchy="false" xref="S3.E2.m1.10.10.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.10.10.1.1.1" xref="S3.E2.m1.10.10.1.1.1.cmml">=</mo><mfrac id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml"><mrow id="S3.E2.m1.3.3.3" xref="S3.E2.m1.3.3.3.cmml"><mrow id="S3.E2.m1.3.3.3.3" xref="S3.E2.m1.3.3.3.3.cmml"><mrow id="S3.E2.m1.2.2.2.2.1" xref="S3.E2.m1.2.2.2.2.1.cmml"><msub id="S3.E2.m1.2.2.2.2.1.3" xref="S3.E2.m1.2.2.2.2.1.3.cmml"><mi id="S3.E2.m1.2.2.2.2.1.3.2" xref="S3.E2.m1.2.2.2.2.1.3.2.cmml">x</mi><mi id="S3.E2.m1.2.2.2.2.1.3.3" xref="S3.E2.m1.2.2.2.2.1.3.3.cmml">i</mi></msub><mo id="S3.E2.m1.2.2.2.2.1.2" xref="S3.E2.m1.2.2.2.2.1.2.cmml">⁢</mo><mrow id="S3.E2.m1.2.2.2.2.1.1.1" xref="S3.E2.m1.2.2.2.2.1.1.1.1.cmml"><mo id="S3.E2.m1.2.2.2.2.1.1.1.2" stretchy="false" xref="S3.E2.m1.2.2.2.2.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.2.2.2.2.1.1.1.1" xref="S3.E2.m1.2.2.2.2.1.1.1.1.cmml"><mi id="S3.E2.m1.2.2.2.2.1.1.1.1.2" xref="S3.E2.m1.2.2.2.2.1.1.1.1.2.cmml">t</mi><mo id="S3.E2.m1.2.2.2.2.1.1.1.1.1" xref="S3.E2.m1.2.2.2.2.1.1.1.1.1.cmml">−</mo><mi id="S3.E2.m1.2.2.2.2.1.1.1.1.3" xref="S3.E2.m1.2.2.2.2.1.1.1.1.3.cmml">n</mi></mrow><mo id="S3.E2.m1.2.2.2.2.1.1.1.3" stretchy="false" xref="S3.E2.m1.2.2.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.3.3.3.3.3" xref="S3.E2.m1.3.3.3.3.3.cmml">+</mo><mrow id="S3.E2.m1.3.3.3.3.2" xref="S3.E2.m1.3.3.3.3.2.cmml"><msub id="S3.E2.m1.3.3.3.3.2.3" xref="S3.E2.m1.3.3.3.3.2.3.cmml"><mi id="S3.E2.m1.3.3.3.3.2.3.2" xref="S3.E2.m1.3.3.3.3.2.3.2.cmml">x</mi><mi id="S3.E2.m1.3.3.3.3.2.3.3" xref="S3.E2.m1.3.3.3.3.2.3.3.cmml">i</mi></msub><mo id="S3.E2.m1.3.3.3.3.2.2" xref="S3.E2.m1.3.3.3.3.2.2.cmml">⁢</mo><mrow id="S3.E2.m1.3.3.3.3.2.1.1" xref="S3.E2.m1.3.3.3.3.2.1.1.1.cmml"><mo id="S3.E2.m1.3.3.3.3.2.1.1.2" stretchy="false" xref="S3.E2.m1.3.3.3.3.2.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.3.3.3.3.2.1.1.1" xref="S3.E2.m1.3.3.3.3.2.1.1.1.cmml"><mi id="S3.E2.m1.3.3.3.3.2.1.1.1.2" xref="S3.E2.m1.3.3.3.3.2.1.1.1.2.cmml">t</mi><mo id="S3.E2.m1.3.3.3.3.2.1.1.1.1" xref="S3.E2.m1.3.3.3.3.2.1.1.1.1.cmml">+</mo><mi id="S3.E2.m1.3.3.3.3.2.1.1.1.3" xref="S3.E2.m1.3.3.3.3.2.1.1.1.3.cmml">n</mi></mrow><mo id="S3.E2.m1.3.3.3.3.2.1.1.3" stretchy="false" xref="S3.E2.m1.3.3.3.3.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2.m1.3.3.3.4" xref="S3.E2.m1.3.3.3.4.cmml">−</mo><mrow id="S3.E2.m1.3.3.3.5" xref="S3.E2.m1.3.3.3.5.cmml"><mn id="S3.E2.m1.3.3.3.5.2" xref="S3.E2.m1.3.3.3.5.2.cmml">2</mn><mo id="S3.E2.m1.3.3.3.5.1" xref="S3.E2.m1.3.3.3.5.1.cmml">⁢</mo><msub id="S3.E2.m1.3.3.3.5.3" xref="S3.E2.m1.3.3.3.5.3.cmml"><mi id="S3.E2.m1.3.3.3.5.3.2" xref="S3.E2.m1.3.3.3.5.3.2.cmml">x</mi><mi id="S3.E2.m1.3.3.3.5.3.3" xref="S3.E2.m1.3.3.3.5.3.3.cmml">i</mi></msub><mo id="S3.E2.m1.3.3.3.5.1a" xref="S3.E2.m1.3.3.3.5.1.cmml">⁢</mo><mrow id="S3.E2.m1.3.3.3.5.4.2" xref="S3.E2.m1.3.3.3.5.cmml"><mo id="S3.E2.m1.3.3.3.5.4.2.1" stretchy="false" xref="S3.E2.m1.3.3.3.5.cmml">(</mo><mi id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml">t</mi><mo id="S3.E2.m1.3.3.3.5.4.2.2" stretchy="false" xref="S3.E2.m1.3.3.3.5.cmml">)</mo></mrow></mrow></mrow><msup id="S3.E2.m1.4.4.4" xref="S3.E2.m1.4.4.4.cmml"><mrow id="S3.E2.m1.4.4.4.1.1" xref="S3.E2.m1.4.4.4.1.1.1.cmml"><mo id="S3.E2.m1.4.4.4.1.1.2" stretchy="false" xref="S3.E2.m1.4.4.4.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.4.4.4.1.1.1" xref="S3.E2.m1.4.4.4.1.1.1.cmml"><mi id="S3.E2.m1.4.4.4.1.1.1.2" xref="S3.E2.m1.4.4.4.1.1.1.2.cmml">n</mi><mo id="S3.E2.m1.4.4.4.1.1.1.1" xref="S3.E2.m1.4.4.4.1.1.1.1.cmml">⁢</mo><mi id="S3.E2.m1.4.4.4.1.1.1.3" mathvariant="normal" xref="S3.E2.m1.4.4.4.1.1.1.3.cmml">Δ</mi><mo id="S3.E2.m1.4.4.4.1.1.1.1a" xref="S3.E2.m1.4.4.4.1.1.1.1.cmml">⁢</mo><mi id="S3.E2.m1.4.4.4.1.1.1.4" xref="S3.E2.m1.4.4.4.1.1.1.4.cmml">t</mi></mrow><mo id="S3.E2.m1.4.4.4.1.1.3" stretchy="false" xref="S3.E2.m1.4.4.4.1.1.1.cmml">)</mo></mrow><mn id="S3.E2.m1.4.4.4.3" xref="S3.E2.m1.4.4.4.3.cmml">2</mn></msup></mfrac></mrow><mo id="S3.E2.m1.11.11.2.3" xref="S3.E2.m1.11.11.3a.cmml">,</mo><mrow id="S3.E2.m1.11.11.2.2" xref="S3.E2.m1.11.11.2.2.cmml"><mi id="S3.E2.m1.11.11.2.2.2" xref="S3.E2.m1.11.11.2.2.2.cmml">i</mi><mo id="S3.E2.m1.11.11.2.2.1" xref="S3.E2.m1.11.11.2.2.1.cmml">=</mo><mrow id="S3.E2.m1.11.11.2.2.3.2" xref="S3.E2.m1.11.11.2.2.3.1.cmml"><mn id="S3.E2.m1.6.6" xref="S3.E2.m1.6.6.cmml">1</mn><mo id="S3.E2.m1.11.11.2.2.3.2.1" xref="S3.E2.m1.11.11.2.2.3.1.cmml">,</mo><mn id="S3.E2.m1.7.7" xref="S3.E2.m1.7.7.cmml">2</mn><mo id="S3.E2.m1.11.11.2.2.3.2.2" xref="S3.E2.m1.11.11.2.2.3.1.cmml">,</mo><mn id="S3.E2.m1.8.8" xref="S3.E2.m1.8.8.cmml">3</mn><mo id="S3.E2.m1.11.11.2.2.3.2.3" xref="S3.E2.m1.11.11.2.2.3.1.cmml">,</mo><mn id="S3.E2.m1.9.9" xref="S3.E2.m1.9.9.cmml">4</mn></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.11b"><apply id="S3.E2.m1.11.11.3.cmml" xref="S3.E2.m1.11.11.2"><csymbol cd="ambiguous" id="S3.E2.m1.11.11.3a.cmml" xref="S3.E2.m1.11.11.2.3">formulae-sequence</csymbol><apply id="S3.E2.m1.10.10.1.1.cmml" xref="S3.E2.m1.10.10.1.1"><eq id="S3.E2.m1.10.10.1.1.1.cmml" xref="S3.E2.m1.10.10.1.1.1"></eq><apply id="S3.E2.m1.10.10.1.1.2.cmml" xref="S3.E2.m1.10.10.1.1.2"><times id="S3.E2.m1.10.10.1.1.2.1.cmml" xref="S3.E2.m1.10.10.1.1.2.1"></times><apply id="S3.E2.m1.10.10.1.1.2.2.cmml" xref="S3.E2.m1.10.10.1.1.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.10.10.1.1.2.2.1.cmml" xref="S3.E2.m1.10.10.1.1.2.2">subscript</csymbol><ci id="S3.E2.m1.10.10.1.1.2.2.2.cmml" xref="S3.E2.m1.10.10.1.1.2.2.2">𝑎</ci><ci id="S3.E2.m1.10.10.1.1.2.2.3.cmml" xref="S3.E2.m1.10.10.1.1.2.2.3">𝑖</ci></apply><ci id="S3.E2.m1.5.5.cmml" xref="S3.E2.m1.5.5">𝑡</ci></apply><apply id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4"><divide id="S3.E2.m1.4.4.5.cmml" xref="S3.E2.m1.4.4"></divide><apply id="S3.E2.m1.3.3.3.cmml" xref="S3.E2.m1.3.3.3"><minus id="S3.E2.m1.3.3.3.4.cmml" xref="S3.E2.m1.3.3.3.4"></minus><apply id="S3.E2.m1.3.3.3.3.cmml" xref="S3.E2.m1.3.3.3.3"><plus id="S3.E2.m1.3.3.3.3.3.cmml" xref="S3.E2.m1.3.3.3.3.3"></plus><apply id="S3.E2.m1.2.2.2.2.1.cmml" xref="S3.E2.m1.2.2.2.2.1"><times id="S3.E2.m1.2.2.2.2.1.2.cmml" xref="S3.E2.m1.2.2.2.2.1.2"></times><apply id="S3.E2.m1.2.2.2.2.1.3.cmml" xref="S3.E2.m1.2.2.2.2.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.2.1.3.1.cmml" xref="S3.E2.m1.2.2.2.2.1.3">subscript</csymbol><ci id="S3.E2.m1.2.2.2.2.1.3.2.cmml" xref="S3.E2.m1.2.2.2.2.1.3.2">𝑥</ci><ci id="S3.E2.m1.2.2.2.2.1.3.3.cmml" xref="S3.E2.m1.2.2.2.2.1.3.3">𝑖</ci></apply><apply id="S3.E2.m1.2.2.2.2.1.1.1.1.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1"><minus id="S3.E2.m1.2.2.2.2.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1.1.1"></minus><ci id="S3.E2.m1.2.2.2.2.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1.1.2">𝑡</ci><ci id="S3.E2.m1.2.2.2.2.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1.1.3">𝑛</ci></apply></apply><apply id="S3.E2.m1.3.3.3.3.2.cmml" xref="S3.E2.m1.3.3.3.3.2"><times id="S3.E2.m1.3.3.3.3.2.2.cmml" xref="S3.E2.m1.3.3.3.3.2.2"></times><apply id="S3.E2.m1.3.3.3.3.2.3.cmml" xref="S3.E2.m1.3.3.3.3.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.3.3.2.3.1.cmml" xref="S3.E2.m1.3.3.3.3.2.3">subscript</csymbol><ci id="S3.E2.m1.3.3.3.3.2.3.2.cmml" xref="S3.E2.m1.3.3.3.3.2.3.2">𝑥</ci><ci id="S3.E2.m1.3.3.3.3.2.3.3.cmml" xref="S3.E2.m1.3.3.3.3.2.3.3">𝑖</ci></apply><apply id="S3.E2.m1.3.3.3.3.2.1.1.1.cmml" xref="S3.E2.m1.3.3.3.3.2.1.1"><plus id="S3.E2.m1.3.3.3.3.2.1.1.1.1.cmml" xref="S3.E2.m1.3.3.3.3.2.1.1.1.1"></plus><ci id="S3.E2.m1.3.3.3.3.2.1.1.1.2.cmml" xref="S3.E2.m1.3.3.3.3.2.1.1.1.2">𝑡</ci><ci id="S3.E2.m1.3.3.3.3.2.1.1.1.3.cmml" xref="S3.E2.m1.3.3.3.3.2.1.1.1.3">𝑛</ci></apply></apply></apply><apply id="S3.E2.m1.3.3.3.5.cmml" xref="S3.E2.m1.3.3.3.5"><times id="S3.E2.m1.3.3.3.5.1.cmml" xref="S3.E2.m1.3.3.3.5.1"></times><cn id="S3.E2.m1.3.3.3.5.2.cmml" type="integer" xref="S3.E2.m1.3.3.3.5.2">2</cn><apply id="S3.E2.m1.3.3.3.5.3.cmml" xref="S3.E2.m1.3.3.3.5.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.3.5.3.1.cmml" xref="S3.E2.m1.3.3.3.5.3">subscript</csymbol><ci id="S3.E2.m1.3.3.3.5.3.2.cmml" xref="S3.E2.m1.3.3.3.5.3.2">𝑥</ci><ci id="S3.E2.m1.3.3.3.5.3.3.cmml" xref="S3.E2.m1.3.3.3.5.3.3">𝑖</ci></apply><ci id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1">𝑡</ci></apply></apply><apply id="S3.E2.m1.4.4.4.cmml" xref="S3.E2.m1.4.4.4"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.4.2.cmml" xref="S3.E2.m1.4.4.4">superscript</csymbol><apply id="S3.E2.m1.4.4.4.1.1.1.cmml" xref="S3.E2.m1.4.4.4.1.1"><times id="S3.E2.m1.4.4.4.1.1.1.1.cmml" xref="S3.E2.m1.4.4.4.1.1.1.1"></times><ci id="S3.E2.m1.4.4.4.1.1.1.2.cmml" xref="S3.E2.m1.4.4.4.1.1.1.2">𝑛</ci><ci id="S3.E2.m1.4.4.4.1.1.1.3.cmml" xref="S3.E2.m1.4.4.4.1.1.1.3">Δ</ci><ci id="S3.E2.m1.4.4.4.1.1.1.4.cmml" xref="S3.E2.m1.4.4.4.1.1.1.4">𝑡</ci></apply><cn id="S3.E2.m1.4.4.4.3.cmml" type="integer" xref="S3.E2.m1.4.4.4.3">2</cn></apply></apply></apply><apply id="S3.E2.m1.11.11.2.2.cmml" xref="S3.E2.m1.11.11.2.2"><eq id="S3.E2.m1.11.11.2.2.1.cmml" xref="S3.E2.m1.11.11.2.2.1"></eq><ci id="S3.E2.m1.11.11.2.2.2.cmml" xref="S3.E2.m1.11.11.2.2.2">𝑖</ci><list id="S3.E2.m1.11.11.2.2.3.1.cmml" xref="S3.E2.m1.11.11.2.2.3.2"><cn id="S3.E2.m1.6.6.cmml" type="integer" xref="S3.E2.m1.6.6">1</cn><cn id="S3.E2.m1.7.7.cmml" type="integer" xref="S3.E2.m1.7.7">2</cn><cn id="S3.E2.m1.8.8.cmml" type="integer" xref="S3.E2.m1.8.8">3</cn><cn id="S3.E2.m1.9.9.cmml" type="integer" xref="S3.E2.m1.9.9">4</cn></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.11c">a_{i}(t)=\frac{x_{i}(t-n)+x_{i}(t+n)-2x_{i}(t)}{(n\Delta t)^{2}},i=1,2,3,4</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.11d">italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_t ) = divide start_ARG italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_t - italic_n ) + italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_t + italic_n ) - 2 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_t ) end_ARG start_ARG ( italic_n roman_Δ italic_t ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG , italic_i = 1 , 2 , 3 , 4</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p3">
<p class="ltx_p" id="S3.SS2.SSS1.p3.2">Informed by our preliminary study, we also normalize the synthetic upper body IMU data with respect to the pelvis joint. Specifically, we have the synthetic accelerations
<math alttext="[a_{pelvis},\ a_{larm},\ a_{rarm},\\
\ a_{head}]\in\mathbb{R}^{3\times 4}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p3.1.m1.4"><semantics id="S3.SS2.SSS1.p3.1.m1.4a"><mrow id="S3.SS2.SSS1.p3.1.m1.4.4" xref="S3.SS2.SSS1.p3.1.m1.4.4.cmml"><mrow id="S3.SS2.SSS1.p3.1.m1.4.4.4.4" xref="S3.SS2.SSS1.p3.1.m1.4.4.4.5.cmml"><mo id="S3.SS2.SSS1.p3.1.m1.4.4.4.4.5" stretchy="false" xref="S3.SS2.SSS1.p3.1.m1.4.4.4.5.cmml">[</mo><msub id="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.cmml"><mi id="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.2" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.2.cmml">a</mi><mrow id="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.cmml"><mi id="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.2" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.2.cmml">p</mi><mo id="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.1" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.3" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.3.cmml">e</mi><mo id="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.1a" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.4" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.4.cmml">l</mi><mo id="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.1b" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.5" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.5.cmml">v</mi><mo id="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.1c" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.6" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.6.cmml">i</mi><mo id="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.1d" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.7" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.7.cmml">s</mi></mrow></msub><mo id="S3.SS2.SSS1.p3.1.m1.4.4.4.4.6" rspace="0.667em" xref="S3.SS2.SSS1.p3.1.m1.4.4.4.5.cmml">,</mo><msub id="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2" xref="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.cmml"><mi id="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.2" xref="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.2.cmml">a</mi><mrow id="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.3" xref="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.3.cmml"><mi id="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.3.2" xref="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.3.2.cmml">l</mi><mo id="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.3.1" xref="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.3.3" xref="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.3.3.cmml">a</mi><mo id="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.3.1a" xref="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.3.4" xref="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.3.4.cmml">r</mi><mo id="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.3.1b" xref="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.3.5" xref="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.3.5.cmml">m</mi></mrow></msub><mo id="S3.SS2.SSS1.p3.1.m1.4.4.4.4.7" rspace="0.667em" xref="S3.SS2.SSS1.p3.1.m1.4.4.4.5.cmml">,</mo><msub id="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3" xref="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3.cmml"><mi id="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3.2" xref="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3.2.cmml">a</mi><mrow id="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3.3" xref="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3.3.cmml"><mi id="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3.3.2" xref="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3.3.2.cmml">r</mi><mo id="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3.3.1" xref="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3.3.3" xref="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3.3.3.cmml">a</mi><mo id="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3.3.1a" xref="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3.3.4" xref="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3.3.4.cmml">r</mi><mo id="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3.3.1b" xref="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3.3.5" xref="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3.3.5.cmml">m</mi></mrow></msub><mo id="S3.SS2.SSS1.p3.1.m1.4.4.4.4.8" rspace="0.667em" xref="S3.SS2.SSS1.p3.1.m1.4.4.4.5.cmml">,</mo><msub id="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4" xref="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4.cmml"><mi id="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4.2" xref="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4.2.cmml">a</mi><mrow id="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4.3" xref="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4.3.cmml"><mi id="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4.3.2" xref="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4.3.2.cmml">h</mi><mo id="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4.3.1" xref="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4.3.3" xref="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4.3.3.cmml">e</mi><mo id="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4.3.1a" xref="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4.3.4" xref="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4.3.4.cmml">a</mi><mo id="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4.3.1b" xref="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4.3.5" xref="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4.3.5.cmml">d</mi></mrow></msub><mo id="S3.SS2.SSS1.p3.1.m1.4.4.4.4.9" stretchy="false" xref="S3.SS2.SSS1.p3.1.m1.4.4.4.5.cmml">]</mo></mrow><mo id="S3.SS2.SSS1.p3.1.m1.4.4.5" xref="S3.SS2.SSS1.p3.1.m1.4.4.5.cmml">∈</mo><msup id="S3.SS2.SSS1.p3.1.m1.4.4.6" xref="S3.SS2.SSS1.p3.1.m1.4.4.6.cmml"><mi id="S3.SS2.SSS1.p3.1.m1.4.4.6.2" xref="S3.SS2.SSS1.p3.1.m1.4.4.6.2.cmml">ℝ</mi><mrow id="S3.SS2.SSS1.p3.1.m1.4.4.6.3" xref="S3.SS2.SSS1.p3.1.m1.4.4.6.3.cmml"><mn id="S3.SS2.SSS1.p3.1.m1.4.4.6.3.2" xref="S3.SS2.SSS1.p3.1.m1.4.4.6.3.2.cmml">3</mn><mo id="S3.SS2.SSS1.p3.1.m1.4.4.6.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.SSS1.p3.1.m1.4.4.6.3.1.cmml">×</mo><mn id="S3.SS2.SSS1.p3.1.m1.4.4.6.3.3" xref="S3.SS2.SSS1.p3.1.m1.4.4.6.3.3.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.1.m1.4b"><apply id="S3.SS2.SSS1.p3.1.m1.4.4.cmml" xref="S3.SS2.SSS1.p3.1.m1.4.4"><in id="S3.SS2.SSS1.p3.1.m1.4.4.5.cmml" xref="S3.SS2.SSS1.p3.1.m1.4.4.5"></in><list id="S3.SS2.SSS1.p3.1.m1.4.4.4.5.cmml" xref="S3.SS2.SSS1.p3.1.m1.4.4.4.4"><apply id="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.2">𝑎</ci><apply id="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3"><times id="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.1.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.1"></times><ci id="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.2.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.2">𝑝</ci><ci id="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.3.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.3">𝑒</ci><ci id="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.4.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.4">𝑙</ci><ci id="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.5.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.5">𝑣</ci><ci id="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.6.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.6">𝑖</ci><ci id="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.7.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.1.1.3.7">𝑠</ci></apply></apply><apply id="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.cmml" xref="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.1.cmml" xref="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2">subscript</csymbol><ci id="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.2.cmml" xref="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.2">𝑎</ci><apply id="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.3.cmml" xref="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.3"><times id="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.3.1.cmml" xref="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.3.1"></times><ci id="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.3.2.cmml" xref="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.3.2">𝑙</ci><ci id="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.3.3.cmml" xref="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.3.3">𝑎</ci><ci id="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.3.4.cmml" xref="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.3.4">𝑟</ci><ci id="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.3.5.cmml" xref="S3.SS2.SSS1.p3.1.m1.2.2.2.2.2.3.5">𝑚</ci></apply></apply><apply id="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3.cmml" xref="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3.1.cmml" xref="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3">subscript</csymbol><ci id="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3.2.cmml" xref="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3.2">𝑎</ci><apply id="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3.3.cmml" xref="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3.3"><times id="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3.3.1.cmml" xref="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3.3.1"></times><ci id="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3.3.2.cmml" xref="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3.3.2">𝑟</ci><ci id="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3.3.3.cmml" xref="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3.3.3">𝑎</ci><ci id="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3.3.4.cmml" xref="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3.3.4">𝑟</ci><ci id="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3.3.5.cmml" xref="S3.SS2.SSS1.p3.1.m1.3.3.3.3.3.3.5">𝑚</ci></apply></apply><apply id="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4.cmml" xref="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4.1.cmml" xref="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4">subscript</csymbol><ci id="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4.2.cmml" xref="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4.2">𝑎</ci><apply id="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4.3.cmml" xref="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4.3"><times id="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4.3.1.cmml" xref="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4.3.1"></times><ci id="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4.3.2.cmml" xref="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4.3.2">ℎ</ci><ci id="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4.3.3.cmml" xref="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4.3.3">𝑒</ci><ci id="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4.3.4.cmml" xref="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4.3.4">𝑎</ci><ci id="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4.3.5.cmml" xref="S3.SS2.SSS1.p3.1.m1.4.4.4.4.4.3.5">𝑑</ci></apply></apply></list><apply id="S3.SS2.SSS1.p3.1.m1.4.4.6.cmml" xref="S3.SS2.SSS1.p3.1.m1.4.4.6"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p3.1.m1.4.4.6.1.cmml" xref="S3.SS2.SSS1.p3.1.m1.4.4.6">superscript</csymbol><ci id="S3.SS2.SSS1.p3.1.m1.4.4.6.2.cmml" xref="S3.SS2.SSS1.p3.1.m1.4.4.6.2">ℝ</ci><apply id="S3.SS2.SSS1.p3.1.m1.4.4.6.3.cmml" xref="S3.SS2.SSS1.p3.1.m1.4.4.6.3"><times id="S3.SS2.SSS1.p3.1.m1.4.4.6.3.1.cmml" xref="S3.SS2.SSS1.p3.1.m1.4.4.6.3.1"></times><cn id="S3.SS2.SSS1.p3.1.m1.4.4.6.3.2.cmml" type="integer" xref="S3.SS2.SSS1.p3.1.m1.4.4.6.3.2">3</cn><cn id="S3.SS2.SSS1.p3.1.m1.4.4.6.3.3.cmml" type="integer" xref="S3.SS2.SSS1.p3.1.m1.4.4.6.3.3">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.1.m1.4c">[a_{pelvis},\ a_{larm},\ a_{rarm},\\
\ a_{head}]\in\mathbb{R}^{3\times 4}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p3.1.m1.4d">[ italic_a start_POSTSUBSCRIPT italic_p italic_e italic_l italic_v italic_i italic_s end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_l italic_a italic_r italic_m end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_r italic_a italic_r italic_m end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_h italic_e italic_a italic_d end_POSTSUBSCRIPT ] ∈ blackboard_R start_POSTSUPERSCRIPT 3 × 4 end_POSTSUPERSCRIPT</annotation></semantics></math> and rotations <math alttext="[R_{pelvis},\ R_{larm},\ R_{rarm},\ R_{head}]\in\mathbb{R}^{3\times 3\times 4}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p3.2.m2.4"><semantics id="S3.SS2.SSS1.p3.2.m2.4a"><mrow id="S3.SS2.SSS1.p3.2.m2.4.4" xref="S3.SS2.SSS1.p3.2.m2.4.4.cmml"><mrow id="S3.SS2.SSS1.p3.2.m2.4.4.4.4" xref="S3.SS2.SSS1.p3.2.m2.4.4.4.5.cmml"><mo id="S3.SS2.SSS1.p3.2.m2.4.4.4.4.5" stretchy="false" xref="S3.SS2.SSS1.p3.2.m2.4.4.4.5.cmml">[</mo><msub id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.cmml"><mi id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.2" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.2.cmml">R</mi><mrow id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.cmml"><mi id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.2" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.2.cmml">p</mi><mo id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.1" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.3" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.3.cmml">e</mi><mo id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.1a" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.4" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.4.cmml">l</mi><mo id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.1b" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.5" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.5.cmml">v</mi><mo id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.1c" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.6" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.6.cmml">i</mi><mo id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.1d" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.7" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.7.cmml">s</mi></mrow></msub><mo id="S3.SS2.SSS1.p3.2.m2.4.4.4.4.6" rspace="0.667em" xref="S3.SS2.SSS1.p3.2.m2.4.4.4.5.cmml">,</mo><msub id="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2" xref="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.cmml"><mi id="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.2" xref="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.2.cmml">R</mi><mrow id="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.3" xref="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.3.cmml"><mi id="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.3.2" xref="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.3.2.cmml">l</mi><mo id="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.3.1" xref="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.3.3" xref="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.3.3.cmml">a</mi><mo id="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.3.1a" xref="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.3.4" xref="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.3.4.cmml">r</mi><mo id="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.3.1b" xref="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.3.5" xref="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.3.5.cmml">m</mi></mrow></msub><mo id="S3.SS2.SSS1.p3.2.m2.4.4.4.4.7" rspace="0.667em" xref="S3.SS2.SSS1.p3.2.m2.4.4.4.5.cmml">,</mo><msub id="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3" xref="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3.cmml"><mi id="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3.2" xref="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3.2.cmml">R</mi><mrow id="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3.3" xref="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3.3.cmml"><mi id="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3.3.2" xref="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3.3.2.cmml">r</mi><mo id="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3.3.1" xref="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3.3.3" xref="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3.3.3.cmml">a</mi><mo id="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3.3.1a" xref="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3.3.4" xref="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3.3.4.cmml">r</mi><mo id="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3.3.1b" xref="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3.3.5" xref="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3.3.5.cmml">m</mi></mrow></msub><mo id="S3.SS2.SSS1.p3.2.m2.4.4.4.4.8" rspace="0.667em" xref="S3.SS2.SSS1.p3.2.m2.4.4.4.5.cmml">,</mo><msub id="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4" xref="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4.cmml"><mi id="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4.2" xref="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4.2.cmml">R</mi><mrow id="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4.3" xref="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4.3.cmml"><mi id="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4.3.2" xref="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4.3.2.cmml">h</mi><mo id="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4.3.1" xref="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4.3.3" xref="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4.3.3.cmml">e</mi><mo id="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4.3.1a" xref="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4.3.4" xref="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4.3.4.cmml">a</mi><mo id="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4.3.1b" xref="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4.3.5" xref="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4.3.5.cmml">d</mi></mrow></msub><mo id="S3.SS2.SSS1.p3.2.m2.4.4.4.4.9" stretchy="false" xref="S3.SS2.SSS1.p3.2.m2.4.4.4.5.cmml">]</mo></mrow><mo id="S3.SS2.SSS1.p3.2.m2.4.4.5" xref="S3.SS2.SSS1.p3.2.m2.4.4.5.cmml">∈</mo><msup id="S3.SS2.SSS1.p3.2.m2.4.4.6" xref="S3.SS2.SSS1.p3.2.m2.4.4.6.cmml"><mi id="S3.SS2.SSS1.p3.2.m2.4.4.6.2" xref="S3.SS2.SSS1.p3.2.m2.4.4.6.2.cmml">ℝ</mi><mrow id="S3.SS2.SSS1.p3.2.m2.4.4.6.3" xref="S3.SS2.SSS1.p3.2.m2.4.4.6.3.cmml"><mn id="S3.SS2.SSS1.p3.2.m2.4.4.6.3.2" xref="S3.SS2.SSS1.p3.2.m2.4.4.6.3.2.cmml">3</mn><mo id="S3.SS2.SSS1.p3.2.m2.4.4.6.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.SSS1.p3.2.m2.4.4.6.3.1.cmml">×</mo><mn id="S3.SS2.SSS1.p3.2.m2.4.4.6.3.3" xref="S3.SS2.SSS1.p3.2.m2.4.4.6.3.3.cmml">3</mn><mo id="S3.SS2.SSS1.p3.2.m2.4.4.6.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS2.SSS1.p3.2.m2.4.4.6.3.1.cmml">×</mo><mn id="S3.SS2.SSS1.p3.2.m2.4.4.6.3.4" xref="S3.SS2.SSS1.p3.2.m2.4.4.6.3.4.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.2.m2.4b"><apply id="S3.SS2.SSS1.p3.2.m2.4.4.cmml" xref="S3.SS2.SSS1.p3.2.m2.4.4"><in id="S3.SS2.SSS1.p3.2.m2.4.4.5.cmml" xref="S3.SS2.SSS1.p3.2.m2.4.4.5"></in><list id="S3.SS2.SSS1.p3.2.m2.4.4.4.5.cmml" xref="S3.SS2.SSS1.p3.2.m2.4.4.4.4"><apply id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.2">𝑅</ci><apply id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3"><times id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.1.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.1"></times><ci id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.2.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.2">𝑝</ci><ci id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.3.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.3">𝑒</ci><ci id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.4.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.4">𝑙</ci><ci id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.5.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.5">𝑣</ci><ci id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.6.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.6">𝑖</ci><ci id="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.7.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.1.1.3.7">𝑠</ci></apply></apply><apply id="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.cmml" xref="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.1.cmml" xref="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.2.cmml" xref="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.2">𝑅</ci><apply id="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.3.cmml" xref="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.3"><times id="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.3.1.cmml" xref="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.3.1"></times><ci id="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.3.2.cmml" xref="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.3.2">𝑙</ci><ci id="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.3.3.cmml" xref="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.3.3">𝑎</ci><ci id="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.3.4.cmml" xref="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.3.4">𝑟</ci><ci id="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.3.5.cmml" xref="S3.SS2.SSS1.p3.2.m2.2.2.2.2.2.3.5">𝑚</ci></apply></apply><apply id="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3.cmml" xref="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3.1.cmml" xref="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3">subscript</csymbol><ci id="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3.2.cmml" xref="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3.2">𝑅</ci><apply id="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3.3.cmml" xref="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3.3"><times id="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3.3.1.cmml" xref="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3.3.1"></times><ci id="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3.3.2.cmml" xref="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3.3.2">𝑟</ci><ci id="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3.3.3.cmml" xref="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3.3.3">𝑎</ci><ci id="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3.3.4.cmml" xref="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3.3.4">𝑟</ci><ci id="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3.3.5.cmml" xref="S3.SS2.SSS1.p3.2.m2.3.3.3.3.3.3.5">𝑚</ci></apply></apply><apply id="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4.cmml" xref="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4.1.cmml" xref="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4">subscript</csymbol><ci id="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4.2.cmml" xref="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4.2">𝑅</ci><apply id="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4.3.cmml" xref="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4.3"><times id="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4.3.1.cmml" xref="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4.3.1"></times><ci id="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4.3.2.cmml" xref="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4.3.2">ℎ</ci><ci id="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4.3.3.cmml" xref="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4.3.3">𝑒</ci><ci id="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4.3.4.cmml" xref="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4.3.4">𝑎</ci><ci id="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4.3.5.cmml" xref="S3.SS2.SSS1.p3.2.m2.4.4.4.4.4.3.5">𝑑</ci></apply></apply></list><apply id="S3.SS2.SSS1.p3.2.m2.4.4.6.cmml" xref="S3.SS2.SSS1.p3.2.m2.4.4.6"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p3.2.m2.4.4.6.1.cmml" xref="S3.SS2.SSS1.p3.2.m2.4.4.6">superscript</csymbol><ci id="S3.SS2.SSS1.p3.2.m2.4.4.6.2.cmml" xref="S3.SS2.SSS1.p3.2.m2.4.4.6.2">ℝ</ci><apply id="S3.SS2.SSS1.p3.2.m2.4.4.6.3.cmml" xref="S3.SS2.SSS1.p3.2.m2.4.4.6.3"><times id="S3.SS2.SSS1.p3.2.m2.4.4.6.3.1.cmml" xref="S3.SS2.SSS1.p3.2.m2.4.4.6.3.1"></times><cn id="S3.SS2.SSS1.p3.2.m2.4.4.6.3.2.cmml" type="integer" xref="S3.SS2.SSS1.p3.2.m2.4.4.6.3.2">3</cn><cn id="S3.SS2.SSS1.p3.2.m2.4.4.6.3.3.cmml" type="integer" xref="S3.SS2.SSS1.p3.2.m2.4.4.6.3.3">3</cn><cn id="S3.SS2.SSS1.p3.2.m2.4.4.6.3.4.cmml" type="integer" xref="S3.SS2.SSS1.p3.2.m2.4.4.6.3.4">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.2.m2.4c">[R_{pelvis},\ R_{larm},\ R_{rarm},\ R_{head}]\in\mathbb{R}^{3\times 3\times 4}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p3.2.m2.4d">[ italic_R start_POSTSUBSCRIPT italic_p italic_e italic_l italic_v italic_i italic_s end_POSTSUBSCRIPT , italic_R start_POSTSUBSCRIPT italic_l italic_a italic_r italic_m end_POSTSUBSCRIPT , italic_R start_POSTSUBSCRIPT italic_r italic_a italic_r italic_m end_POSTSUBSCRIPT , italic_R start_POSTSUBSCRIPT italic_h italic_e italic_a italic_d end_POSTSUBSCRIPT ] ∈ blackboard_R start_POSTSUPERSCRIPT 3 × 3 × 4 end_POSTSUPERSCRIPT</annotation></semantics></math> in the same global frame aligned with the SMPL reference frame. We normalize the inertial measurements of the leaf joints (left arm, right arm, and head) with respect to the pelvis joint as:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(3)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\tilde{a}_{leaf}=R^{-1}_{pelvis}(a_{leaf}-a_{pelvis})" class="ltx_Math" display="block" id="S3.E3.m1.1"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><msub id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml"><mover accent="true" id="S3.E3.m1.1.1.3.2" xref="S3.E3.m1.1.1.3.2.cmml"><mi id="S3.E3.m1.1.1.3.2.2" xref="S3.E3.m1.1.1.3.2.2.cmml">a</mi><mo id="S3.E3.m1.1.1.3.2.1" xref="S3.E3.m1.1.1.3.2.1.cmml">~</mo></mover><mrow id="S3.E3.m1.1.1.3.3" xref="S3.E3.m1.1.1.3.3.cmml"><mi id="S3.E3.m1.1.1.3.3.2" xref="S3.E3.m1.1.1.3.3.2.cmml">l</mi><mo id="S3.E3.m1.1.1.3.3.1" xref="S3.E3.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E3.m1.1.1.3.3.3" xref="S3.E3.m1.1.1.3.3.3.cmml">e</mi><mo id="S3.E3.m1.1.1.3.3.1a" xref="S3.E3.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E3.m1.1.1.3.3.4" xref="S3.E3.m1.1.1.3.3.4.cmml">a</mi><mo id="S3.E3.m1.1.1.3.3.1b" xref="S3.E3.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E3.m1.1.1.3.3.5" xref="S3.E3.m1.1.1.3.3.5.cmml">f</mi></mrow></msub><mo id="S3.E3.m1.1.1.2" xref="S3.E3.m1.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.cmml"><msubsup id="S3.E3.m1.1.1.1.3" xref="S3.E3.m1.1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.1.3.2.2" xref="S3.E3.m1.1.1.1.3.2.2.cmml">R</mi><mrow id="S3.E3.m1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.3.3.cmml"><mi id="S3.E3.m1.1.1.1.3.3.2" xref="S3.E3.m1.1.1.1.3.3.2.cmml">p</mi><mo id="S3.E3.m1.1.1.1.3.3.1" xref="S3.E3.m1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E3.m1.1.1.1.3.3.3" xref="S3.E3.m1.1.1.1.3.3.3.cmml">e</mi><mo id="S3.E3.m1.1.1.1.3.3.1a" xref="S3.E3.m1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E3.m1.1.1.1.3.3.4" xref="S3.E3.m1.1.1.1.3.3.4.cmml">l</mi><mo id="S3.E3.m1.1.1.1.3.3.1b" xref="S3.E3.m1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E3.m1.1.1.1.3.3.5" xref="S3.E3.m1.1.1.1.3.3.5.cmml">v</mi><mo id="S3.E3.m1.1.1.1.3.3.1c" xref="S3.E3.m1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E3.m1.1.1.1.3.3.6" xref="S3.E3.m1.1.1.1.3.3.6.cmml">i</mi><mo id="S3.E3.m1.1.1.1.3.3.1d" xref="S3.E3.m1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E3.m1.1.1.1.3.3.7" xref="S3.E3.m1.1.1.1.3.3.7.cmml">s</mi></mrow><mrow id="S3.E3.m1.1.1.1.3.2.3" xref="S3.E3.m1.1.1.1.3.2.3.cmml"><mo id="S3.E3.m1.1.1.1.3.2.3a" xref="S3.E3.m1.1.1.1.3.2.3.cmml">−</mo><mn id="S3.E3.m1.1.1.1.3.2.3.2" xref="S3.E3.m1.1.1.1.3.2.3.2.cmml">1</mn></mrow></msubsup><mo id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml"><mo id="S3.E3.m1.1.1.1.1.1.2" stretchy="false" xref="S3.E3.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml"><msub id="S3.E3.m1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.2.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.2.2" xref="S3.E3.m1.1.1.1.1.1.1.2.2.cmml">a</mi><mrow id="S3.E3.m1.1.1.1.1.1.1.2.3" xref="S3.E3.m1.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.2.3.2" xref="S3.E3.m1.1.1.1.1.1.1.2.3.2.cmml">l</mi><mo id="S3.E3.m1.1.1.1.1.1.1.2.3.1" xref="S3.E3.m1.1.1.1.1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.E3.m1.1.1.1.1.1.1.2.3.3" xref="S3.E3.m1.1.1.1.1.1.1.2.3.3.cmml">e</mi><mo id="S3.E3.m1.1.1.1.1.1.1.2.3.1a" xref="S3.E3.m1.1.1.1.1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.E3.m1.1.1.1.1.1.1.2.3.4" xref="S3.E3.m1.1.1.1.1.1.1.2.3.4.cmml">a</mi><mo id="S3.E3.m1.1.1.1.1.1.1.2.3.1b" xref="S3.E3.m1.1.1.1.1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.E3.m1.1.1.1.1.1.1.2.3.5" xref="S3.E3.m1.1.1.1.1.1.1.2.3.5.cmml">f</mi></mrow></msub><mo id="S3.E3.m1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S3.E3.m1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.1.1.1.3.2.cmml">a</mi><mrow id="S3.E3.m1.1.1.1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.3.3.2" xref="S3.E3.m1.1.1.1.1.1.1.3.3.2.cmml">p</mi><mo id="S3.E3.m1.1.1.1.1.1.1.3.3.1" xref="S3.E3.m1.1.1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E3.m1.1.1.1.1.1.1.3.3.3" xref="S3.E3.m1.1.1.1.1.1.1.3.3.3.cmml">e</mi><mo id="S3.E3.m1.1.1.1.1.1.1.3.3.1a" xref="S3.E3.m1.1.1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E3.m1.1.1.1.1.1.1.3.3.4" xref="S3.E3.m1.1.1.1.1.1.1.3.3.4.cmml">l</mi><mo id="S3.E3.m1.1.1.1.1.1.1.3.3.1b" xref="S3.E3.m1.1.1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E3.m1.1.1.1.1.1.1.3.3.5" xref="S3.E3.m1.1.1.1.1.1.1.3.3.5.cmml">v</mi><mo id="S3.E3.m1.1.1.1.1.1.1.3.3.1c" xref="S3.E3.m1.1.1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E3.m1.1.1.1.1.1.1.3.3.6" xref="S3.E3.m1.1.1.1.1.1.1.3.3.6.cmml">i</mi><mo id="S3.E3.m1.1.1.1.1.1.1.3.3.1d" xref="S3.E3.m1.1.1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E3.m1.1.1.1.1.1.1.3.3.7" xref="S3.E3.m1.1.1.1.1.1.1.3.3.7.cmml">s</mi></mrow></msub></mrow><mo id="S3.E3.m1.1.1.1.1.1.3" stretchy="false" xref="S3.E3.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><eq id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2"></eq><apply id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.3">subscript</csymbol><apply id="S3.E3.m1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.3.2"><ci id="S3.E3.m1.1.1.3.2.1.cmml" xref="S3.E3.m1.1.1.3.2.1">~</ci><ci id="S3.E3.m1.1.1.3.2.2.cmml" xref="S3.E3.m1.1.1.3.2.2">𝑎</ci></apply><apply id="S3.E3.m1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.3.3"><times id="S3.E3.m1.1.1.3.3.1.cmml" xref="S3.E3.m1.1.1.3.3.1"></times><ci id="S3.E3.m1.1.1.3.3.2.cmml" xref="S3.E3.m1.1.1.3.3.2">𝑙</ci><ci id="S3.E3.m1.1.1.3.3.3.cmml" xref="S3.E3.m1.1.1.3.3.3">𝑒</ci><ci id="S3.E3.m1.1.1.3.3.4.cmml" xref="S3.E3.m1.1.1.3.3.4">𝑎</ci><ci id="S3.E3.m1.1.1.3.3.5.cmml" xref="S3.E3.m1.1.1.3.3.5">𝑓</ci></apply></apply><apply id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><times id="S3.E3.m1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.2"></times><apply id="S3.E3.m1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.3">subscript</csymbol><apply id="S3.E3.m1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.3.2.1.cmml" xref="S3.E3.m1.1.1.1.3">superscript</csymbol><ci id="S3.E3.m1.1.1.1.3.2.2.cmml" xref="S3.E3.m1.1.1.1.3.2.2">𝑅</ci><apply id="S3.E3.m1.1.1.1.3.2.3.cmml" xref="S3.E3.m1.1.1.1.3.2.3"><minus id="S3.E3.m1.1.1.1.3.2.3.1.cmml" xref="S3.E3.m1.1.1.1.3.2.3"></minus><cn id="S3.E3.m1.1.1.1.3.2.3.2.cmml" type="integer" xref="S3.E3.m1.1.1.1.3.2.3.2">1</cn></apply></apply><apply id="S3.E3.m1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.3.3"><times id="S3.E3.m1.1.1.1.3.3.1.cmml" xref="S3.E3.m1.1.1.1.3.3.1"></times><ci id="S3.E3.m1.1.1.1.3.3.2.cmml" xref="S3.E3.m1.1.1.1.3.3.2">𝑝</ci><ci id="S3.E3.m1.1.1.1.3.3.3.cmml" xref="S3.E3.m1.1.1.1.3.3.3">𝑒</ci><ci id="S3.E3.m1.1.1.1.3.3.4.cmml" xref="S3.E3.m1.1.1.1.3.3.4">𝑙</ci><ci id="S3.E3.m1.1.1.1.3.3.5.cmml" xref="S3.E3.m1.1.1.1.3.3.5">𝑣</ci><ci id="S3.E3.m1.1.1.1.3.3.6.cmml" xref="S3.E3.m1.1.1.1.3.3.6">𝑖</ci><ci id="S3.E3.m1.1.1.1.3.3.7.cmml" xref="S3.E3.m1.1.1.1.3.3.7">𝑠</ci></apply></apply><apply id="S3.E3.m1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1"><minus id="S3.E3.m1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1"></minus><apply id="S3.E3.m1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.2">𝑎</ci><apply id="S3.E3.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.3"><times id="S3.E3.m1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.3.1"></times><ci id="S3.E3.m1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.3.2">𝑙</ci><ci id="S3.E3.m1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.3.3">𝑒</ci><ci id="S3.E3.m1.1.1.1.1.1.1.2.3.4.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.3.4">𝑎</ci><ci id="S3.E3.m1.1.1.1.1.1.1.2.3.5.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.3.5">𝑓</ci></apply></apply><apply id="S3.E3.m1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.2">𝑎</ci><apply id="S3.E3.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.3"><times id="S3.E3.m1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.3.1"></times><ci id="S3.E3.m1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.3.2">𝑝</ci><ci id="S3.E3.m1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.3.3">𝑒</ci><ci id="S3.E3.m1.1.1.1.1.1.1.3.3.4.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.3.4">𝑙</ci><ci id="S3.E3.m1.1.1.1.1.1.1.3.3.5.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.3.5">𝑣</ci><ci id="S3.E3.m1.1.1.1.1.1.1.3.3.6.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.3.6">𝑖</ci><ci id="S3.E3.m1.1.1.1.1.1.1.3.3.7.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.3.7">𝑠</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\tilde{a}_{leaf}=R^{-1}_{pelvis}(a_{leaf}-a_{pelvis})</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.1d">over~ start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_l italic_e italic_a italic_f end_POSTSUBSCRIPT = italic_R start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p italic_e italic_l italic_v italic_i italic_s end_POSTSUBSCRIPT ( italic_a start_POSTSUBSCRIPT italic_l italic_e italic_a italic_f end_POSTSUBSCRIPT - italic_a start_POSTSUBSCRIPT italic_p italic_e italic_l italic_v italic_i italic_s end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(4)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\tilde{R}_{leaf}=R^{-1}_{pelvis}R_{leaf}" class="ltx_Math" display="block" id="S3.E4.m1.1"><semantics id="S3.E4.m1.1a"><mrow id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml"><msub id="S3.E4.m1.1.1.2" xref="S3.E4.m1.1.1.2.cmml"><mover accent="true" id="S3.E4.m1.1.1.2.2" xref="S3.E4.m1.1.1.2.2.cmml"><mi id="S3.E4.m1.1.1.2.2.2" xref="S3.E4.m1.1.1.2.2.2.cmml">R</mi><mo id="S3.E4.m1.1.1.2.2.1" xref="S3.E4.m1.1.1.2.2.1.cmml">~</mo></mover><mrow id="S3.E4.m1.1.1.2.3" xref="S3.E4.m1.1.1.2.3.cmml"><mi id="S3.E4.m1.1.1.2.3.2" xref="S3.E4.m1.1.1.2.3.2.cmml">l</mi><mo id="S3.E4.m1.1.1.2.3.1" xref="S3.E4.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.E4.m1.1.1.2.3.3" xref="S3.E4.m1.1.1.2.3.3.cmml">e</mi><mo id="S3.E4.m1.1.1.2.3.1a" xref="S3.E4.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.E4.m1.1.1.2.3.4" xref="S3.E4.m1.1.1.2.3.4.cmml">a</mi><mo id="S3.E4.m1.1.1.2.3.1b" xref="S3.E4.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.E4.m1.1.1.2.3.5" xref="S3.E4.m1.1.1.2.3.5.cmml">f</mi></mrow></msub><mo id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.cmml">=</mo><mrow id="S3.E4.m1.1.1.3" xref="S3.E4.m1.1.1.3.cmml"><msubsup id="S3.E4.m1.1.1.3.2" xref="S3.E4.m1.1.1.3.2.cmml"><mi id="S3.E4.m1.1.1.3.2.2.2" xref="S3.E4.m1.1.1.3.2.2.2.cmml">R</mi><mrow id="S3.E4.m1.1.1.3.2.3" xref="S3.E4.m1.1.1.3.2.3.cmml"><mi id="S3.E4.m1.1.1.3.2.3.2" xref="S3.E4.m1.1.1.3.2.3.2.cmml">p</mi><mo id="S3.E4.m1.1.1.3.2.3.1" xref="S3.E4.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S3.E4.m1.1.1.3.2.3.3" xref="S3.E4.m1.1.1.3.2.3.3.cmml">e</mi><mo id="S3.E4.m1.1.1.3.2.3.1a" xref="S3.E4.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S3.E4.m1.1.1.3.2.3.4" xref="S3.E4.m1.1.1.3.2.3.4.cmml">l</mi><mo id="S3.E4.m1.1.1.3.2.3.1b" xref="S3.E4.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S3.E4.m1.1.1.3.2.3.5" xref="S3.E4.m1.1.1.3.2.3.5.cmml">v</mi><mo id="S3.E4.m1.1.1.3.2.3.1c" xref="S3.E4.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S3.E4.m1.1.1.3.2.3.6" xref="S3.E4.m1.1.1.3.2.3.6.cmml">i</mi><mo id="S3.E4.m1.1.1.3.2.3.1d" xref="S3.E4.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S3.E4.m1.1.1.3.2.3.7" xref="S3.E4.m1.1.1.3.2.3.7.cmml">s</mi></mrow><mrow id="S3.E4.m1.1.1.3.2.2.3" xref="S3.E4.m1.1.1.3.2.2.3.cmml"><mo id="S3.E4.m1.1.1.3.2.2.3a" xref="S3.E4.m1.1.1.3.2.2.3.cmml">−</mo><mn id="S3.E4.m1.1.1.3.2.2.3.2" xref="S3.E4.m1.1.1.3.2.2.3.2.cmml">1</mn></mrow></msubsup><mo id="S3.E4.m1.1.1.3.1" xref="S3.E4.m1.1.1.3.1.cmml">⁢</mo><msub id="S3.E4.m1.1.1.3.3" xref="S3.E4.m1.1.1.3.3.cmml"><mi id="S3.E4.m1.1.1.3.3.2" xref="S3.E4.m1.1.1.3.3.2.cmml">R</mi><mrow id="S3.E4.m1.1.1.3.3.3" xref="S3.E4.m1.1.1.3.3.3.cmml"><mi id="S3.E4.m1.1.1.3.3.3.2" xref="S3.E4.m1.1.1.3.3.3.2.cmml">l</mi><mo id="S3.E4.m1.1.1.3.3.3.1" xref="S3.E4.m1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.E4.m1.1.1.3.3.3.3" xref="S3.E4.m1.1.1.3.3.3.3.cmml">e</mi><mo id="S3.E4.m1.1.1.3.3.3.1a" xref="S3.E4.m1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.E4.m1.1.1.3.3.3.4" xref="S3.E4.m1.1.1.3.3.3.4.cmml">a</mi><mo id="S3.E4.m1.1.1.3.3.3.1b" xref="S3.E4.m1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.E4.m1.1.1.3.3.3.5" xref="S3.E4.m1.1.1.3.3.3.5.cmml">f</mi></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1"><eq id="S3.E4.m1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"></eq><apply id="S3.E4.m1.1.1.2.cmml" xref="S3.E4.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.2">subscript</csymbol><apply id="S3.E4.m1.1.1.2.2.cmml" xref="S3.E4.m1.1.1.2.2"><ci id="S3.E4.m1.1.1.2.2.1.cmml" xref="S3.E4.m1.1.1.2.2.1">~</ci><ci id="S3.E4.m1.1.1.2.2.2.cmml" xref="S3.E4.m1.1.1.2.2.2">𝑅</ci></apply><apply id="S3.E4.m1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.2.3"><times id="S3.E4.m1.1.1.2.3.1.cmml" xref="S3.E4.m1.1.1.2.3.1"></times><ci id="S3.E4.m1.1.1.2.3.2.cmml" xref="S3.E4.m1.1.1.2.3.2">𝑙</ci><ci id="S3.E4.m1.1.1.2.3.3.cmml" xref="S3.E4.m1.1.1.2.3.3">𝑒</ci><ci id="S3.E4.m1.1.1.2.3.4.cmml" xref="S3.E4.m1.1.1.2.3.4">𝑎</ci><ci id="S3.E4.m1.1.1.2.3.5.cmml" xref="S3.E4.m1.1.1.2.3.5">𝑓</ci></apply></apply><apply id="S3.E4.m1.1.1.3.cmml" xref="S3.E4.m1.1.1.3"><times id="S3.E4.m1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.3.1"></times><apply id="S3.E4.m1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.2.1.cmml" xref="S3.E4.m1.1.1.3.2">subscript</csymbol><apply id="S3.E4.m1.1.1.3.2.2.cmml" xref="S3.E4.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.2.2.1.cmml" xref="S3.E4.m1.1.1.3.2">superscript</csymbol><ci id="S3.E4.m1.1.1.3.2.2.2.cmml" xref="S3.E4.m1.1.1.3.2.2.2">𝑅</ci><apply id="S3.E4.m1.1.1.3.2.2.3.cmml" xref="S3.E4.m1.1.1.3.2.2.3"><minus id="S3.E4.m1.1.1.3.2.2.3.1.cmml" xref="S3.E4.m1.1.1.3.2.2.3"></minus><cn id="S3.E4.m1.1.1.3.2.2.3.2.cmml" type="integer" xref="S3.E4.m1.1.1.3.2.2.3.2">1</cn></apply></apply><apply id="S3.E4.m1.1.1.3.2.3.cmml" xref="S3.E4.m1.1.1.3.2.3"><times id="S3.E4.m1.1.1.3.2.3.1.cmml" xref="S3.E4.m1.1.1.3.2.3.1"></times><ci id="S3.E4.m1.1.1.3.2.3.2.cmml" xref="S3.E4.m1.1.1.3.2.3.2">𝑝</ci><ci id="S3.E4.m1.1.1.3.2.3.3.cmml" xref="S3.E4.m1.1.1.3.2.3.3">𝑒</ci><ci id="S3.E4.m1.1.1.3.2.3.4.cmml" xref="S3.E4.m1.1.1.3.2.3.4">𝑙</ci><ci id="S3.E4.m1.1.1.3.2.3.5.cmml" xref="S3.E4.m1.1.1.3.2.3.5">𝑣</ci><ci id="S3.E4.m1.1.1.3.2.3.6.cmml" xref="S3.E4.m1.1.1.3.2.3.6">𝑖</ci><ci id="S3.E4.m1.1.1.3.2.3.7.cmml" xref="S3.E4.m1.1.1.3.2.3.7">𝑠</ci></apply></apply><apply id="S3.E4.m1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.3.1.cmml" xref="S3.E4.m1.1.1.3.3">subscript</csymbol><ci id="S3.E4.m1.1.1.3.3.2.cmml" xref="S3.E4.m1.1.1.3.3.2">𝑅</ci><apply id="S3.E4.m1.1.1.3.3.3.cmml" xref="S3.E4.m1.1.1.3.3.3"><times id="S3.E4.m1.1.1.3.3.3.1.cmml" xref="S3.E4.m1.1.1.3.3.3.1"></times><ci id="S3.E4.m1.1.1.3.3.3.2.cmml" xref="S3.E4.m1.1.1.3.3.3.2">𝑙</ci><ci id="S3.E4.m1.1.1.3.3.3.3.cmml" xref="S3.E4.m1.1.1.3.3.3.3">𝑒</ci><ci id="S3.E4.m1.1.1.3.3.3.4.cmml" xref="S3.E4.m1.1.1.3.3.3.4">𝑎</ci><ci id="S3.E4.m1.1.1.3.3.3.5.cmml" xref="S3.E4.m1.1.1.3.3.3.5">𝑓</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">\tilde{R}_{leaf}=R^{-1}_{pelvis}R_{leaf}</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.1d">over~ start_ARG italic_R end_ARG start_POSTSUBSCRIPT italic_l italic_e italic_a italic_f end_POSTSUBSCRIPT = italic_R start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p italic_e italic_l italic_v italic_i italic_s end_POSTSUBSCRIPT italic_R start_POSTSUBSCRIPT italic_l italic_e italic_a italic_f end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS1.p3.3">and the inertial measurements of the pelvis joint as:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(5)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\tilde{a}_{pelvis}=R^{-1}_{pelvis}a_{pelvis}" class="ltx_Math" display="block" id="S3.E5.m1.1"><semantics id="S3.E5.m1.1a"><mrow id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml"><msub id="S3.E5.m1.1.1.2" xref="S3.E5.m1.1.1.2.cmml"><mover accent="true" id="S3.E5.m1.1.1.2.2" xref="S3.E5.m1.1.1.2.2.cmml"><mi id="S3.E5.m1.1.1.2.2.2" xref="S3.E5.m1.1.1.2.2.2.cmml">a</mi><mo id="S3.E5.m1.1.1.2.2.1" xref="S3.E5.m1.1.1.2.2.1.cmml">~</mo></mover><mrow id="S3.E5.m1.1.1.2.3" xref="S3.E5.m1.1.1.2.3.cmml"><mi id="S3.E5.m1.1.1.2.3.2" xref="S3.E5.m1.1.1.2.3.2.cmml">p</mi><mo id="S3.E5.m1.1.1.2.3.1" xref="S3.E5.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.E5.m1.1.1.2.3.3" xref="S3.E5.m1.1.1.2.3.3.cmml">e</mi><mo id="S3.E5.m1.1.1.2.3.1a" xref="S3.E5.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.E5.m1.1.1.2.3.4" xref="S3.E5.m1.1.1.2.3.4.cmml">l</mi><mo id="S3.E5.m1.1.1.2.3.1b" xref="S3.E5.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.E5.m1.1.1.2.3.5" xref="S3.E5.m1.1.1.2.3.5.cmml">v</mi><mo id="S3.E5.m1.1.1.2.3.1c" xref="S3.E5.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.E5.m1.1.1.2.3.6" xref="S3.E5.m1.1.1.2.3.6.cmml">i</mi><mo id="S3.E5.m1.1.1.2.3.1d" xref="S3.E5.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.E5.m1.1.1.2.3.7" xref="S3.E5.m1.1.1.2.3.7.cmml">s</mi></mrow></msub><mo id="S3.E5.m1.1.1.1" xref="S3.E5.m1.1.1.1.cmml">=</mo><mrow id="S3.E5.m1.1.1.3" xref="S3.E5.m1.1.1.3.cmml"><msubsup id="S3.E5.m1.1.1.3.2" xref="S3.E5.m1.1.1.3.2.cmml"><mi id="S3.E5.m1.1.1.3.2.2.2" xref="S3.E5.m1.1.1.3.2.2.2.cmml">R</mi><mrow id="S3.E5.m1.1.1.3.2.3" xref="S3.E5.m1.1.1.3.2.3.cmml"><mi id="S3.E5.m1.1.1.3.2.3.2" xref="S3.E5.m1.1.1.3.2.3.2.cmml">p</mi><mo id="S3.E5.m1.1.1.3.2.3.1" xref="S3.E5.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S3.E5.m1.1.1.3.2.3.3" xref="S3.E5.m1.1.1.3.2.3.3.cmml">e</mi><mo id="S3.E5.m1.1.1.3.2.3.1a" xref="S3.E5.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S3.E5.m1.1.1.3.2.3.4" xref="S3.E5.m1.1.1.3.2.3.4.cmml">l</mi><mo id="S3.E5.m1.1.1.3.2.3.1b" xref="S3.E5.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S3.E5.m1.1.1.3.2.3.5" xref="S3.E5.m1.1.1.3.2.3.5.cmml">v</mi><mo id="S3.E5.m1.1.1.3.2.3.1c" xref="S3.E5.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S3.E5.m1.1.1.3.2.3.6" xref="S3.E5.m1.1.1.3.2.3.6.cmml">i</mi><mo id="S3.E5.m1.1.1.3.2.3.1d" xref="S3.E5.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S3.E5.m1.1.1.3.2.3.7" xref="S3.E5.m1.1.1.3.2.3.7.cmml">s</mi></mrow><mrow id="S3.E5.m1.1.1.3.2.2.3" xref="S3.E5.m1.1.1.3.2.2.3.cmml"><mo id="S3.E5.m1.1.1.3.2.2.3a" xref="S3.E5.m1.1.1.3.2.2.3.cmml">−</mo><mn id="S3.E5.m1.1.1.3.2.2.3.2" xref="S3.E5.m1.1.1.3.2.2.3.2.cmml">1</mn></mrow></msubsup><mo id="S3.E5.m1.1.1.3.1" xref="S3.E5.m1.1.1.3.1.cmml">⁢</mo><msub id="S3.E5.m1.1.1.3.3" xref="S3.E5.m1.1.1.3.3.cmml"><mi id="S3.E5.m1.1.1.3.3.2" xref="S3.E5.m1.1.1.3.3.2.cmml">a</mi><mrow id="S3.E5.m1.1.1.3.3.3" xref="S3.E5.m1.1.1.3.3.3.cmml"><mi id="S3.E5.m1.1.1.3.3.3.2" xref="S3.E5.m1.1.1.3.3.3.2.cmml">p</mi><mo id="S3.E5.m1.1.1.3.3.3.1" xref="S3.E5.m1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.E5.m1.1.1.3.3.3.3" xref="S3.E5.m1.1.1.3.3.3.3.cmml">e</mi><mo id="S3.E5.m1.1.1.3.3.3.1a" xref="S3.E5.m1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.E5.m1.1.1.3.3.3.4" xref="S3.E5.m1.1.1.3.3.3.4.cmml">l</mi><mo id="S3.E5.m1.1.1.3.3.3.1b" xref="S3.E5.m1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.E5.m1.1.1.3.3.3.5" xref="S3.E5.m1.1.1.3.3.3.5.cmml">v</mi><mo id="S3.E5.m1.1.1.3.3.3.1c" xref="S3.E5.m1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.E5.m1.1.1.3.3.3.6" xref="S3.E5.m1.1.1.3.3.3.6.cmml">i</mi><mo id="S3.E5.m1.1.1.3.3.3.1d" xref="S3.E5.m1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.E5.m1.1.1.3.3.3.7" xref="S3.E5.m1.1.1.3.3.3.7.cmml">s</mi></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.1b"><apply id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1"><eq id="S3.E5.m1.1.1.1.cmml" xref="S3.E5.m1.1.1.1"></eq><apply id="S3.E5.m1.1.1.2.cmml" xref="S3.E5.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.2.1.cmml" xref="S3.E5.m1.1.1.2">subscript</csymbol><apply id="S3.E5.m1.1.1.2.2.cmml" xref="S3.E5.m1.1.1.2.2"><ci id="S3.E5.m1.1.1.2.2.1.cmml" xref="S3.E5.m1.1.1.2.2.1">~</ci><ci id="S3.E5.m1.1.1.2.2.2.cmml" xref="S3.E5.m1.1.1.2.2.2">𝑎</ci></apply><apply id="S3.E5.m1.1.1.2.3.cmml" xref="S3.E5.m1.1.1.2.3"><times id="S3.E5.m1.1.1.2.3.1.cmml" xref="S3.E5.m1.1.1.2.3.1"></times><ci id="S3.E5.m1.1.1.2.3.2.cmml" xref="S3.E5.m1.1.1.2.3.2">𝑝</ci><ci id="S3.E5.m1.1.1.2.3.3.cmml" xref="S3.E5.m1.1.1.2.3.3">𝑒</ci><ci id="S3.E5.m1.1.1.2.3.4.cmml" xref="S3.E5.m1.1.1.2.3.4">𝑙</ci><ci id="S3.E5.m1.1.1.2.3.5.cmml" xref="S3.E5.m1.1.1.2.3.5">𝑣</ci><ci id="S3.E5.m1.1.1.2.3.6.cmml" xref="S3.E5.m1.1.1.2.3.6">𝑖</ci><ci id="S3.E5.m1.1.1.2.3.7.cmml" xref="S3.E5.m1.1.1.2.3.7">𝑠</ci></apply></apply><apply id="S3.E5.m1.1.1.3.cmml" xref="S3.E5.m1.1.1.3"><times id="S3.E5.m1.1.1.3.1.cmml" xref="S3.E5.m1.1.1.3.1"></times><apply id="S3.E5.m1.1.1.3.2.cmml" xref="S3.E5.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.3.2.1.cmml" xref="S3.E5.m1.1.1.3.2">subscript</csymbol><apply id="S3.E5.m1.1.1.3.2.2.cmml" xref="S3.E5.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.3.2.2.1.cmml" xref="S3.E5.m1.1.1.3.2">superscript</csymbol><ci id="S3.E5.m1.1.1.3.2.2.2.cmml" xref="S3.E5.m1.1.1.3.2.2.2">𝑅</ci><apply id="S3.E5.m1.1.1.3.2.2.3.cmml" xref="S3.E5.m1.1.1.3.2.2.3"><minus id="S3.E5.m1.1.1.3.2.2.3.1.cmml" xref="S3.E5.m1.1.1.3.2.2.3"></minus><cn id="S3.E5.m1.1.1.3.2.2.3.2.cmml" type="integer" xref="S3.E5.m1.1.1.3.2.2.3.2">1</cn></apply></apply><apply id="S3.E5.m1.1.1.3.2.3.cmml" xref="S3.E5.m1.1.1.3.2.3"><times id="S3.E5.m1.1.1.3.2.3.1.cmml" xref="S3.E5.m1.1.1.3.2.3.1"></times><ci id="S3.E5.m1.1.1.3.2.3.2.cmml" xref="S3.E5.m1.1.1.3.2.3.2">𝑝</ci><ci id="S3.E5.m1.1.1.3.2.3.3.cmml" xref="S3.E5.m1.1.1.3.2.3.3">𝑒</ci><ci id="S3.E5.m1.1.1.3.2.3.4.cmml" xref="S3.E5.m1.1.1.3.2.3.4">𝑙</ci><ci id="S3.E5.m1.1.1.3.2.3.5.cmml" xref="S3.E5.m1.1.1.3.2.3.5">𝑣</ci><ci id="S3.E5.m1.1.1.3.2.3.6.cmml" xref="S3.E5.m1.1.1.3.2.3.6">𝑖</ci><ci id="S3.E5.m1.1.1.3.2.3.7.cmml" xref="S3.E5.m1.1.1.3.2.3.7">𝑠</ci></apply></apply><apply id="S3.E5.m1.1.1.3.3.cmml" xref="S3.E5.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.3.3.1.cmml" xref="S3.E5.m1.1.1.3.3">subscript</csymbol><ci id="S3.E5.m1.1.1.3.3.2.cmml" xref="S3.E5.m1.1.1.3.3.2">𝑎</ci><apply id="S3.E5.m1.1.1.3.3.3.cmml" xref="S3.E5.m1.1.1.3.3.3"><times id="S3.E5.m1.1.1.3.3.3.1.cmml" xref="S3.E5.m1.1.1.3.3.3.1"></times><ci id="S3.E5.m1.1.1.3.3.3.2.cmml" xref="S3.E5.m1.1.1.3.3.3.2">𝑝</ci><ci id="S3.E5.m1.1.1.3.3.3.3.cmml" xref="S3.E5.m1.1.1.3.3.3.3">𝑒</ci><ci id="S3.E5.m1.1.1.3.3.3.4.cmml" xref="S3.E5.m1.1.1.3.3.3.4">𝑙</ci><ci id="S3.E5.m1.1.1.3.3.3.5.cmml" xref="S3.E5.m1.1.1.3.3.3.5">𝑣</ci><ci id="S3.E5.m1.1.1.3.3.3.6.cmml" xref="S3.E5.m1.1.1.3.3.3.6">𝑖</ci><ci id="S3.E5.m1.1.1.3.3.3.7.cmml" xref="S3.E5.m1.1.1.3.3.3.7">𝑠</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.1c">\tilde{a}_{pelvis}=R^{-1}_{pelvis}a_{pelvis}</annotation><annotation encoding="application/x-llamapun" id="S3.E5.m1.1d">over~ start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_p italic_e italic_l italic_v italic_i italic_s end_POSTSUBSCRIPT = italic_R start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p italic_e italic_l italic_v italic_i italic_s end_POSTSUBSCRIPT italic_a start_POSTSUBSCRIPT italic_p italic_e italic_l italic_v italic_i italic_s end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="S3.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(6)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\tilde{R}_{pelvis}=R_{pelvis}" class="ltx_Math" display="block" id="S3.E6.m1.1"><semantics id="S3.E6.m1.1a"><mrow id="S3.E6.m1.1.1" xref="S3.E6.m1.1.1.cmml"><msub id="S3.E6.m1.1.1.2" xref="S3.E6.m1.1.1.2.cmml"><mover accent="true" id="S3.E6.m1.1.1.2.2" xref="S3.E6.m1.1.1.2.2.cmml"><mi id="S3.E6.m1.1.1.2.2.2" xref="S3.E6.m1.1.1.2.2.2.cmml">R</mi><mo id="S3.E6.m1.1.1.2.2.1" xref="S3.E6.m1.1.1.2.2.1.cmml">~</mo></mover><mrow id="S3.E6.m1.1.1.2.3" xref="S3.E6.m1.1.1.2.3.cmml"><mi id="S3.E6.m1.1.1.2.3.2" xref="S3.E6.m1.1.1.2.3.2.cmml">p</mi><mo id="S3.E6.m1.1.1.2.3.1" xref="S3.E6.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.E6.m1.1.1.2.3.3" xref="S3.E6.m1.1.1.2.3.3.cmml">e</mi><mo id="S3.E6.m1.1.1.2.3.1a" xref="S3.E6.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.E6.m1.1.1.2.3.4" xref="S3.E6.m1.1.1.2.3.4.cmml">l</mi><mo id="S3.E6.m1.1.1.2.3.1b" xref="S3.E6.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.E6.m1.1.1.2.3.5" xref="S3.E6.m1.1.1.2.3.5.cmml">v</mi><mo id="S3.E6.m1.1.1.2.3.1c" xref="S3.E6.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.E6.m1.1.1.2.3.6" xref="S3.E6.m1.1.1.2.3.6.cmml">i</mi><mo id="S3.E6.m1.1.1.2.3.1d" xref="S3.E6.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.E6.m1.1.1.2.3.7" xref="S3.E6.m1.1.1.2.3.7.cmml">s</mi></mrow></msub><mo id="S3.E6.m1.1.1.1" xref="S3.E6.m1.1.1.1.cmml">=</mo><msub id="S3.E6.m1.1.1.3" xref="S3.E6.m1.1.1.3.cmml"><mi id="S3.E6.m1.1.1.3.2" xref="S3.E6.m1.1.1.3.2.cmml">R</mi><mrow id="S3.E6.m1.1.1.3.3" xref="S3.E6.m1.1.1.3.3.cmml"><mi id="S3.E6.m1.1.1.3.3.2" xref="S3.E6.m1.1.1.3.3.2.cmml">p</mi><mo id="S3.E6.m1.1.1.3.3.1" xref="S3.E6.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E6.m1.1.1.3.3.3" xref="S3.E6.m1.1.1.3.3.3.cmml">e</mi><mo id="S3.E6.m1.1.1.3.3.1a" xref="S3.E6.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E6.m1.1.1.3.3.4" xref="S3.E6.m1.1.1.3.3.4.cmml">l</mi><mo id="S3.E6.m1.1.1.3.3.1b" xref="S3.E6.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E6.m1.1.1.3.3.5" xref="S3.E6.m1.1.1.3.3.5.cmml">v</mi><mo id="S3.E6.m1.1.1.3.3.1c" xref="S3.E6.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E6.m1.1.1.3.3.6" xref="S3.E6.m1.1.1.3.3.6.cmml">i</mi><mo id="S3.E6.m1.1.1.3.3.1d" xref="S3.E6.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E6.m1.1.1.3.3.7" xref="S3.E6.m1.1.1.3.3.7.cmml">s</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.1b"><apply id="S3.E6.m1.1.1.cmml" xref="S3.E6.m1.1.1"><eq id="S3.E6.m1.1.1.1.cmml" xref="S3.E6.m1.1.1.1"></eq><apply id="S3.E6.m1.1.1.2.cmml" xref="S3.E6.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.2.1.cmml" xref="S3.E6.m1.1.1.2">subscript</csymbol><apply id="S3.E6.m1.1.1.2.2.cmml" xref="S3.E6.m1.1.1.2.2"><ci id="S3.E6.m1.1.1.2.2.1.cmml" xref="S3.E6.m1.1.1.2.2.1">~</ci><ci id="S3.E6.m1.1.1.2.2.2.cmml" xref="S3.E6.m1.1.1.2.2.2">𝑅</ci></apply><apply id="S3.E6.m1.1.1.2.3.cmml" xref="S3.E6.m1.1.1.2.3"><times id="S3.E6.m1.1.1.2.3.1.cmml" xref="S3.E6.m1.1.1.2.3.1"></times><ci id="S3.E6.m1.1.1.2.3.2.cmml" xref="S3.E6.m1.1.1.2.3.2">𝑝</ci><ci id="S3.E6.m1.1.1.2.3.3.cmml" xref="S3.E6.m1.1.1.2.3.3">𝑒</ci><ci id="S3.E6.m1.1.1.2.3.4.cmml" xref="S3.E6.m1.1.1.2.3.4">𝑙</ci><ci id="S3.E6.m1.1.1.2.3.5.cmml" xref="S3.E6.m1.1.1.2.3.5">𝑣</ci><ci id="S3.E6.m1.1.1.2.3.6.cmml" xref="S3.E6.m1.1.1.2.3.6">𝑖</ci><ci id="S3.E6.m1.1.1.2.3.7.cmml" xref="S3.E6.m1.1.1.2.3.7">𝑠</ci></apply></apply><apply id="S3.E6.m1.1.1.3.cmml" xref="S3.E6.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.3.1.cmml" xref="S3.E6.m1.1.1.3">subscript</csymbol><ci id="S3.E6.m1.1.1.3.2.cmml" xref="S3.E6.m1.1.1.3.2">𝑅</ci><apply id="S3.E6.m1.1.1.3.3.cmml" xref="S3.E6.m1.1.1.3.3"><times id="S3.E6.m1.1.1.3.3.1.cmml" xref="S3.E6.m1.1.1.3.3.1"></times><ci id="S3.E6.m1.1.1.3.3.2.cmml" xref="S3.E6.m1.1.1.3.3.2">𝑝</ci><ci id="S3.E6.m1.1.1.3.3.3.cmml" xref="S3.E6.m1.1.1.3.3.3">𝑒</ci><ci id="S3.E6.m1.1.1.3.3.4.cmml" xref="S3.E6.m1.1.1.3.3.4">𝑙</ci><ci id="S3.E6.m1.1.1.3.3.5.cmml" xref="S3.E6.m1.1.1.3.3.5">𝑣</ci><ci id="S3.E6.m1.1.1.3.3.6.cmml" xref="S3.E6.m1.1.1.3.3.6">𝑖</ci><ci id="S3.E6.m1.1.1.3.3.7.cmml" xref="S3.E6.m1.1.1.3.3.7">𝑠</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.1c">\tilde{R}_{pelvis}=R_{pelvis}</annotation><annotation encoding="application/x-llamapun" id="S3.E6.m1.1d">over~ start_ARG italic_R end_ARG start_POSTSUBSCRIPT italic_p italic_e italic_l italic_v italic_i italic_s end_POSTSUBSCRIPT = italic_R start_POSTSUBSCRIPT italic_p italic_e italic_l italic_v italic_i italic_s end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2. </span>Kinematics Module Network Architecture</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.2">The input to our kinematics module is a concatenated vector of size 48, which includes the normalized accelerations <math alttext="[\tilde{a}_{pelvis},\ \tilde{a}_{larm},\ \tilde{a}_{rarm},\ \tilde{a}_{head}]%
\\
\in\mathbb{R}^{3\times 4}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p1.1.m1.4"><semantics id="S3.SS2.SSS2.p1.1.m1.4a"><mrow id="S3.SS2.SSS2.p1.1.m1.4.4" xref="S3.SS2.SSS2.p1.1.m1.4.4.cmml"><mrow id="S3.SS2.SSS2.p1.1.m1.4.4.4.4" xref="S3.SS2.SSS2.p1.1.m1.4.4.4.5.cmml"><mo id="S3.SS2.SSS2.p1.1.m1.4.4.4.4.5" stretchy="false" xref="S3.SS2.SSS2.p1.1.m1.4.4.4.5.cmml">[</mo><msub id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.cmml"><mover accent="true" id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.2" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.2.cmml"><mi id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.2.2" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.2.2.cmml">a</mi><mo id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.2.1" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.2.1.cmml">~</mo></mover><mrow id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.cmml"><mi id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.2" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.2.cmml">p</mi><mo id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.1" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.3" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.3.cmml">e</mi><mo id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.1a" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.4" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.4.cmml">l</mi><mo id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.1b" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.5" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.5.cmml">v</mi><mo id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.1c" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.6" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.6.cmml">i</mi><mo id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.1d" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.7" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.7.cmml">s</mi></mrow></msub><mo id="S3.SS2.SSS2.p1.1.m1.4.4.4.4.6" rspace="0.667em" xref="S3.SS2.SSS2.p1.1.m1.4.4.4.5.cmml">,</mo><msub id="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2" xref="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.cmml"><mover accent="true" id="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.2" xref="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.2.cmml"><mi id="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.2.2" xref="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.2.2.cmml">a</mi><mo id="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.2.1" xref="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.2.1.cmml">~</mo></mover><mrow id="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.3" xref="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.3.cmml"><mi id="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.3.2" xref="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.3.2.cmml">l</mi><mo id="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.3.1" xref="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.3.3" xref="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.3.3.cmml">a</mi><mo id="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.3.1a" xref="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.3.4" xref="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.3.4.cmml">r</mi><mo id="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.3.1b" xref="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.3.5" xref="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.3.5.cmml">m</mi></mrow></msub><mo id="S3.SS2.SSS2.p1.1.m1.4.4.4.4.7" rspace="0.667em" xref="S3.SS2.SSS2.p1.1.m1.4.4.4.5.cmml">,</mo><msub id="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3" xref="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.cmml"><mover accent="true" id="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.2" xref="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.2.cmml"><mi id="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.2.2" xref="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.2.2.cmml">a</mi><mo id="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.2.1" xref="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.2.1.cmml">~</mo></mover><mrow id="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.3" xref="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.3.cmml"><mi id="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.3.2" xref="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.3.2.cmml">r</mi><mo id="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.3.1" xref="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.3.3" xref="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.3.3.cmml">a</mi><mo id="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.3.1a" xref="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.3.4" xref="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.3.4.cmml">r</mi><mo id="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.3.1b" xref="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.3.5" xref="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.3.5.cmml">m</mi></mrow></msub><mo id="S3.SS2.SSS2.p1.1.m1.4.4.4.4.8" rspace="0.667em" xref="S3.SS2.SSS2.p1.1.m1.4.4.4.5.cmml">,</mo><msub id="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4" xref="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.cmml"><mover accent="true" id="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.2" xref="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.2.cmml"><mi id="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.2.2" xref="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.2.2.cmml">a</mi><mo id="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.2.1" xref="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.2.1.cmml">~</mo></mover><mrow id="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.3" xref="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.3.cmml"><mi id="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.3.2" xref="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.3.2.cmml">h</mi><mo id="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.3.1" xref="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.3.3" xref="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.3.3.cmml">e</mi><mo id="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.3.1a" xref="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.3.4" xref="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.3.4.cmml">a</mi><mo id="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.3.1b" xref="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.3.5" xref="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.3.5.cmml">d</mi></mrow></msub><mo id="S3.SS2.SSS2.p1.1.m1.4.4.4.4.9" stretchy="false" xref="S3.SS2.SSS2.p1.1.m1.4.4.4.5.cmml">]</mo></mrow><mo id="S3.SS2.SSS2.p1.1.m1.4.4.5" xref="S3.SS2.SSS2.p1.1.m1.4.4.5.cmml">∈</mo><msup id="S3.SS2.SSS2.p1.1.m1.4.4.6" xref="S3.SS2.SSS2.p1.1.m1.4.4.6.cmml"><mi id="S3.SS2.SSS2.p1.1.m1.4.4.6.2" xref="S3.SS2.SSS2.p1.1.m1.4.4.6.2.cmml">ℝ</mi><mrow id="S3.SS2.SSS2.p1.1.m1.4.4.6.3" xref="S3.SS2.SSS2.p1.1.m1.4.4.6.3.cmml"><mn id="S3.SS2.SSS2.p1.1.m1.4.4.6.3.2" xref="S3.SS2.SSS2.p1.1.m1.4.4.6.3.2.cmml">3</mn><mo id="S3.SS2.SSS2.p1.1.m1.4.4.6.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.SSS2.p1.1.m1.4.4.6.3.1.cmml">×</mo><mn id="S3.SS2.SSS2.p1.1.m1.4.4.6.3.3" xref="S3.SS2.SSS2.p1.1.m1.4.4.6.3.3.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.1.m1.4b"><apply id="S3.SS2.SSS2.p1.1.m1.4.4.cmml" xref="S3.SS2.SSS2.p1.1.m1.4.4"><in id="S3.SS2.SSS2.p1.1.m1.4.4.5.cmml" xref="S3.SS2.SSS2.p1.1.m1.4.4.5"></in><list id="S3.SS2.SSS2.p1.1.m1.4.4.4.5.cmml" xref="S3.SS2.SSS2.p1.1.m1.4.4.4.4"><apply id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1">subscript</csymbol><apply id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.2"><ci id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.2.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.2.1">~</ci><ci id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.2.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.2.2">𝑎</ci></apply><apply id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3"><times id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.1"></times><ci id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.2">𝑝</ci><ci id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.3.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.3">𝑒</ci><ci id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.4.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.4">𝑙</ci><ci id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.5.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.5">𝑣</ci><ci id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.6.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.6">𝑖</ci><ci id="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.7.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.1.1.3.7">𝑠</ci></apply></apply><apply id="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2">subscript</csymbol><apply id="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.2"><ci id="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.2.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.2.1">~</ci><ci id="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.2.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.2.2">𝑎</ci></apply><apply id="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.3.cmml" xref="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.3"><times id="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.3.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.3.1"></times><ci id="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.3.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.3.2">𝑙</ci><ci id="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.3.3.cmml" xref="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.3.3">𝑎</ci><ci id="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.3.4.cmml" xref="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.3.4">𝑟</ci><ci id="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.3.5.cmml" xref="S3.SS2.SSS2.p1.1.m1.2.2.2.2.2.3.5">𝑚</ci></apply></apply><apply id="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.cmml" xref="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3">subscript</csymbol><apply id="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.2"><ci id="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.2.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.2.1">~</ci><ci id="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.2.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.2.2">𝑎</ci></apply><apply id="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.3.cmml" xref="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.3"><times id="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.3.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.3.1"></times><ci id="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.3.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.3.2">𝑟</ci><ci id="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.3.3.cmml" xref="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.3.3">𝑎</ci><ci id="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.3.4.cmml" xref="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.3.4">𝑟</ci><ci id="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.3.5.cmml" xref="S3.SS2.SSS2.p1.1.m1.3.3.3.3.3.3.5">𝑚</ci></apply></apply><apply id="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.cmml" xref="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4">subscript</csymbol><apply id="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.2"><ci id="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.2.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.2.1">~</ci><ci id="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.2.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.2.2">𝑎</ci></apply><apply id="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.3.cmml" xref="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.3"><times id="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.3.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.3.1"></times><ci id="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.3.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.3.2">ℎ</ci><ci id="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.3.3.cmml" xref="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.3.3">𝑒</ci><ci id="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.3.4.cmml" xref="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.3.4">𝑎</ci><ci id="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.3.5.cmml" xref="S3.SS2.SSS2.p1.1.m1.4.4.4.4.4.3.5">𝑑</ci></apply></apply></list><apply id="S3.SS2.SSS2.p1.1.m1.4.4.6.cmml" xref="S3.SS2.SSS2.p1.1.m1.4.4.6"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.1.m1.4.4.6.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.4.4.6">superscript</csymbol><ci id="S3.SS2.SSS2.p1.1.m1.4.4.6.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.4.4.6.2">ℝ</ci><apply id="S3.SS2.SSS2.p1.1.m1.4.4.6.3.cmml" xref="S3.SS2.SSS2.p1.1.m1.4.4.6.3"><times id="S3.SS2.SSS2.p1.1.m1.4.4.6.3.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.4.4.6.3.1"></times><cn id="S3.SS2.SSS2.p1.1.m1.4.4.6.3.2.cmml" type="integer" xref="S3.SS2.SSS2.p1.1.m1.4.4.6.3.2">3</cn><cn id="S3.SS2.SSS2.p1.1.m1.4.4.6.3.3.cmml" type="integer" xref="S3.SS2.SSS2.p1.1.m1.4.4.6.3.3">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.1.m1.4c">[\tilde{a}_{pelvis},\ \tilde{a}_{larm},\ \tilde{a}_{rarm},\ \tilde{a}_{head}]%
\\
\in\mathbb{R}^{3\times 4}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p1.1.m1.4d">[ over~ start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_p italic_e italic_l italic_v italic_i italic_s end_POSTSUBSCRIPT , over~ start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_l italic_a italic_r italic_m end_POSTSUBSCRIPT , over~ start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_r italic_a italic_r italic_m end_POSTSUBSCRIPT , over~ start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_h italic_e italic_a italic_d end_POSTSUBSCRIPT ] ∈ blackboard_R start_POSTSUPERSCRIPT 3 × 4 end_POSTSUPERSCRIPT</annotation></semantics></math>, and normalized orientations expressed in rotation matrix format <math alttext="[\tilde{R}_{pelvis},\ \tilde{R}_{larm},\ \tilde{R}_{rarm},\ \tilde{R}_{head}]%
\in\mathbb{R}^{3\times 3\times 4}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p1.2.m2.4"><semantics id="S3.SS2.SSS2.p1.2.m2.4a"><mrow id="S3.SS2.SSS2.p1.2.m2.4.4" xref="S3.SS2.SSS2.p1.2.m2.4.4.cmml"><mrow id="S3.SS2.SSS2.p1.2.m2.4.4.4.4" xref="S3.SS2.SSS2.p1.2.m2.4.4.4.5.cmml"><mo id="S3.SS2.SSS2.p1.2.m2.4.4.4.4.5" stretchy="false" xref="S3.SS2.SSS2.p1.2.m2.4.4.4.5.cmml">[</mo><msub id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.cmml"><mover accent="true" id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.2" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.2.cmml"><mi id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.2.2" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.2.2.cmml">R</mi><mo id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.2.1" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.2.1.cmml">~</mo></mover><mrow id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.cmml"><mi id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.2" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.2.cmml">p</mi><mo id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.1" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.3" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.3.cmml">e</mi><mo id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.1a" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.4" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.4.cmml">l</mi><mo id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.1b" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.5" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.5.cmml">v</mi><mo id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.1c" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.6" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.6.cmml">i</mi><mo id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.1d" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.7" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.7.cmml">s</mi></mrow></msub><mo id="S3.SS2.SSS2.p1.2.m2.4.4.4.4.6" rspace="0.667em" xref="S3.SS2.SSS2.p1.2.m2.4.4.4.5.cmml">,</mo><msub id="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2" xref="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.cmml"><mover accent="true" id="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.2" xref="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.2.cmml"><mi id="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.2.2" xref="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.2.2.cmml">R</mi><mo id="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.2.1" xref="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.2.1.cmml">~</mo></mover><mrow id="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.3" xref="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.3.cmml"><mi id="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.3.2" xref="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.3.2.cmml">l</mi><mo id="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.3.1" xref="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.3.3" xref="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.3.3.cmml">a</mi><mo id="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.3.1a" xref="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.3.4" xref="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.3.4.cmml">r</mi><mo id="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.3.1b" xref="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.3.5" xref="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.3.5.cmml">m</mi></mrow></msub><mo id="S3.SS2.SSS2.p1.2.m2.4.4.4.4.7" rspace="0.667em" xref="S3.SS2.SSS2.p1.2.m2.4.4.4.5.cmml">,</mo><msub id="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3" xref="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.cmml"><mover accent="true" id="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.2" xref="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.2.cmml"><mi id="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.2.2" xref="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.2.2.cmml">R</mi><mo id="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.2.1" xref="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.2.1.cmml">~</mo></mover><mrow id="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.3" xref="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.3.cmml"><mi id="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.3.2" xref="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.3.2.cmml">r</mi><mo id="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.3.1" xref="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.3.3" xref="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.3.3.cmml">a</mi><mo id="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.3.1a" xref="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.3.4" xref="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.3.4.cmml">r</mi><mo id="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.3.1b" xref="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.3.5" xref="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.3.5.cmml">m</mi></mrow></msub><mo id="S3.SS2.SSS2.p1.2.m2.4.4.4.4.8" rspace="0.667em" xref="S3.SS2.SSS2.p1.2.m2.4.4.4.5.cmml">,</mo><msub id="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4" xref="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.cmml"><mover accent="true" id="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.2" xref="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.2.cmml"><mi id="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.2.2" xref="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.2.2.cmml">R</mi><mo id="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.2.1" xref="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.2.1.cmml">~</mo></mover><mrow id="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.3" xref="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.3.cmml"><mi id="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.3.2" xref="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.3.2.cmml">h</mi><mo id="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.3.1" xref="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.3.3" xref="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.3.3.cmml">e</mi><mo id="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.3.1a" xref="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.3.4" xref="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.3.4.cmml">a</mi><mo id="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.3.1b" xref="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.3.1.cmml">⁢</mo><mi id="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.3.5" xref="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.3.5.cmml">d</mi></mrow></msub><mo id="S3.SS2.SSS2.p1.2.m2.4.4.4.4.9" stretchy="false" xref="S3.SS2.SSS2.p1.2.m2.4.4.4.5.cmml">]</mo></mrow><mo id="S3.SS2.SSS2.p1.2.m2.4.4.5" xref="S3.SS2.SSS2.p1.2.m2.4.4.5.cmml">∈</mo><msup id="S3.SS2.SSS2.p1.2.m2.4.4.6" xref="S3.SS2.SSS2.p1.2.m2.4.4.6.cmml"><mi id="S3.SS2.SSS2.p1.2.m2.4.4.6.2" xref="S3.SS2.SSS2.p1.2.m2.4.4.6.2.cmml">ℝ</mi><mrow id="S3.SS2.SSS2.p1.2.m2.4.4.6.3" xref="S3.SS2.SSS2.p1.2.m2.4.4.6.3.cmml"><mn id="S3.SS2.SSS2.p1.2.m2.4.4.6.3.2" xref="S3.SS2.SSS2.p1.2.m2.4.4.6.3.2.cmml">3</mn><mo id="S3.SS2.SSS2.p1.2.m2.4.4.6.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.SSS2.p1.2.m2.4.4.6.3.1.cmml">×</mo><mn id="S3.SS2.SSS2.p1.2.m2.4.4.6.3.3" xref="S3.SS2.SSS2.p1.2.m2.4.4.6.3.3.cmml">3</mn><mo id="S3.SS2.SSS2.p1.2.m2.4.4.6.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS2.SSS2.p1.2.m2.4.4.6.3.1.cmml">×</mo><mn id="S3.SS2.SSS2.p1.2.m2.4.4.6.3.4" xref="S3.SS2.SSS2.p1.2.m2.4.4.6.3.4.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.2.m2.4b"><apply id="S3.SS2.SSS2.p1.2.m2.4.4.cmml" xref="S3.SS2.SSS2.p1.2.m2.4.4"><in id="S3.SS2.SSS2.p1.2.m2.4.4.5.cmml" xref="S3.SS2.SSS2.p1.2.m2.4.4.5"></in><list id="S3.SS2.SSS2.p1.2.m2.4.4.4.5.cmml" xref="S3.SS2.SSS2.p1.2.m2.4.4.4.4"><apply id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1">subscript</csymbol><apply id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.2"><ci id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.2.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.2.1">~</ci><ci id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.2.2.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.2.2">𝑅</ci></apply><apply id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3"><times id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.1"></times><ci id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.2.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.2">𝑝</ci><ci id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.3.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.3">𝑒</ci><ci id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.4.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.4">𝑙</ci><ci id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.5.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.5">𝑣</ci><ci id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.6.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.6">𝑖</ci><ci id="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.7.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.1.1.3.7">𝑠</ci></apply></apply><apply id="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.cmml" xref="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2">subscript</csymbol><apply id="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.2.cmml" xref="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.2"><ci id="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.2.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.2.1">~</ci><ci id="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.2.2.cmml" xref="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.2.2">𝑅</ci></apply><apply id="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.3.cmml" xref="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.3"><times id="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.3.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.3.1"></times><ci id="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.3.2.cmml" xref="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.3.2">𝑙</ci><ci id="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.3.3.cmml" xref="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.3.3">𝑎</ci><ci id="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.3.4.cmml" xref="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.3.4">𝑟</ci><ci id="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.3.5.cmml" xref="S3.SS2.SSS2.p1.2.m2.2.2.2.2.2.3.5">𝑚</ci></apply></apply><apply id="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.cmml" xref="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3">subscript</csymbol><apply id="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.2.cmml" xref="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.2"><ci id="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.2.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.2.1">~</ci><ci id="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.2.2.cmml" xref="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.2.2">𝑅</ci></apply><apply id="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.3.cmml" xref="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.3"><times id="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.3.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.3.1"></times><ci id="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.3.2.cmml" xref="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.3.2">𝑟</ci><ci id="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.3.3.cmml" xref="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.3.3">𝑎</ci><ci id="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.3.4.cmml" xref="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.3.4">𝑟</ci><ci id="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.3.5.cmml" xref="S3.SS2.SSS2.p1.2.m2.3.3.3.3.3.3.5">𝑚</ci></apply></apply><apply id="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.cmml" xref="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4">subscript</csymbol><apply id="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.2.cmml" xref="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.2"><ci id="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.2.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.2.1">~</ci><ci id="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.2.2.cmml" xref="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.2.2">𝑅</ci></apply><apply id="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.3.cmml" xref="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.3"><times id="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.3.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.3.1"></times><ci id="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.3.2.cmml" xref="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.3.2">ℎ</ci><ci id="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.3.3.cmml" xref="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.3.3">𝑒</ci><ci id="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.3.4.cmml" xref="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.3.4">𝑎</ci><ci id="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.3.5.cmml" xref="S3.SS2.SSS2.p1.2.m2.4.4.4.4.4.3.5">𝑑</ci></apply></apply></list><apply id="S3.SS2.SSS2.p1.2.m2.4.4.6.cmml" xref="S3.SS2.SSS2.p1.2.m2.4.4.6"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.2.m2.4.4.6.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.4.4.6">superscript</csymbol><ci id="S3.SS2.SSS2.p1.2.m2.4.4.6.2.cmml" xref="S3.SS2.SSS2.p1.2.m2.4.4.6.2">ℝ</ci><apply id="S3.SS2.SSS2.p1.2.m2.4.4.6.3.cmml" xref="S3.SS2.SSS2.p1.2.m2.4.4.6.3"><times id="S3.SS2.SSS2.p1.2.m2.4.4.6.3.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.4.4.6.3.1"></times><cn id="S3.SS2.SSS2.p1.2.m2.4.4.6.3.2.cmml" type="integer" xref="S3.SS2.SSS2.p1.2.m2.4.4.6.3.2">3</cn><cn id="S3.SS2.SSS2.p1.2.m2.4.4.6.3.3.cmml" type="integer" xref="S3.SS2.SSS2.p1.2.m2.4.4.6.3.3">3</cn><cn id="S3.SS2.SSS2.p1.2.m2.4.4.6.3.4.cmml" type="integer" xref="S3.SS2.SSS2.p1.2.m2.4.4.6.3.4">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.2.m2.4c">[\tilde{R}_{pelvis},\ \tilde{R}_{larm},\ \tilde{R}_{rarm},\ \tilde{R}_{head}]%
\in\mathbb{R}^{3\times 3\times 4}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p1.2.m2.4d">[ over~ start_ARG italic_R end_ARG start_POSTSUBSCRIPT italic_p italic_e italic_l italic_v italic_i italic_s end_POSTSUBSCRIPT , over~ start_ARG italic_R end_ARG start_POSTSUBSCRIPT italic_l italic_a italic_r italic_m end_POSTSUBSCRIPT , over~ start_ARG italic_R end_ARG start_POSTSUBSCRIPT italic_r italic_a italic_r italic_m end_POSTSUBSCRIPT , over~ start_ARG italic_R end_ARG start_POSTSUBSCRIPT italic_h italic_e italic_a italic_d end_POSTSUBSCRIPT ] ∈ blackboard_R start_POSTSUPERSCRIPT 3 × 3 × 4 end_POSTSUPERSCRIPT</annotation></semantics></math>, captured from the four IMUs.
The output of our kinematics module is a vector of 96 SMPL pose parameters - 16 upper body joints represented as 6D rotations <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib102" title="">2019</a>)</cite>.
Here, unlike previous approaches that directly measure pelvis rotation using the IMU placed on the pelvis <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib35" title="">2018</a>; Yi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib100" title="">2021</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib99" title="">2022</a>; Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib41" title="">2022</a>)</cite>, we opt to predict the pelvis pose in the global reference frame. This approach compensates for discrepancies between the orientations of the user’s pelvis and the wheelchair, offering a more accurate representation of the movement dynamics of wheelchair users.
Specifically, we propose three candidate network architectures for our kinematics module adapted from existing work on sparse-IMU-based pose estimation:</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p2">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">Single-stage biRNN:</span> Inspired by DIP <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib35" title="">2018</a>)</cite> and IMUPoser <cite class="ltx_cite ltx_citemacro_citep">(Mollyn et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib66" title="">2023</a>)</cite>, this architecture contains a two-layer bidirectional recurrent neural network (biRNN) with long short-term memory (LSTM) cells of hidden dimension 256. We first transform the input vector to this hidden dimension with a ReLU-activated linear layer. Next, we feed these embeddings sequentially into the LSTM after which they are transformed linearly into estimated SMPL pose parameters.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">Three-stage biRNN:</span> Previous research has demonstrated that utilizing joint positions as an intermediate representation greatly enhances a model’s ability to learn complex human motion priors. The second network in our study accordingly adopts a three-stage structure, inspired by TransPose <cite class="ltx_cite ltx_citemacro_citep">(Yi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib100" title="">2021</a>)</cite>. Here, each stage features a biRNN with a structure similar to the single-stage biRNN previously described. The first stage predicts the positions of the head, left wrist, and right wrist joints based on the input vector. Subsequently, the second stage utilizes both the initial input vector and the output from the first stage to predict all upper body joint positions. The final stage then uses both the input vector and the output from the second stage to predict the SMPL pose parameters. The LSTM cells in each stage have hidden dimensions of 256, 64, and 128, respectively.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.1">Transformer decoder:</span> We also compare against using a transformer-based model for pose estimation. We follow a similar structure to TIP <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib41" title="">2022</a>)</cite> and use an autoregressive transformer decoder (4 layers, 16 heads, model dimension 256) followed by a 1-layer RNN to estimate SMPL pose parameters. Different from TIP, we use a physics-based optimization module (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S3.SS3" title="3.3. Physics-based Optimization Module ‣ 3. WheelPoser System ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_tag">3.3</span></a>) and do not estimate stable body points or root velocity.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Physics-based Optimization Module</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">So far, our pose estimation pipeline has only considered kinematic information neglecting human motion dynamics, which can result in unnatural and jittery motion in real-time pose estimation as shown in previous work <cite class="ltx_cite ltx_citemacro_citep">(Yi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib100" title="">2021</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib99" title="">2022</a>; Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib41" title="">2022</a>)</cite>. Additionally, in light of the potential applications of our system, such as health monitoring and sports analytics, where access to information about the joint torque of wheelchair users’ upper body can be extremely helpful, we have decided to incorporate a physics-based optimization module to address both of these aspects without adding any additional sensors into our system.</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(7)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\ddot{\theta}_{ref}=k_{p}(\theta_{k}\ -\ \theta)\ -\ k_{d}\dot{\theta}" class="ltx_Math" display="block" id="S3.E7.m1.1"><semantics id="S3.E7.m1.1a"><mrow id="S3.E7.m1.1.1" xref="S3.E7.m1.1.1.cmml"><msub id="S3.E7.m1.1.1.3" xref="S3.E7.m1.1.1.3.cmml"><mover accent="true" id="S3.E7.m1.1.1.3.2" xref="S3.E7.m1.1.1.3.2.cmml"><mi id="S3.E7.m1.1.1.3.2.2" xref="S3.E7.m1.1.1.3.2.2.cmml">θ</mi><mo id="S3.E7.m1.1.1.3.2.1" xref="S3.E7.m1.1.1.3.2.1.cmml">¨</mo></mover><mrow id="S3.E7.m1.1.1.3.3" xref="S3.E7.m1.1.1.3.3.cmml"><mi id="S3.E7.m1.1.1.3.3.2" xref="S3.E7.m1.1.1.3.3.2.cmml">r</mi><mo id="S3.E7.m1.1.1.3.3.1" xref="S3.E7.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E7.m1.1.1.3.3.3" xref="S3.E7.m1.1.1.3.3.3.cmml">e</mi><mo id="S3.E7.m1.1.1.3.3.1a" xref="S3.E7.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E7.m1.1.1.3.3.4" xref="S3.E7.m1.1.1.3.3.4.cmml">f</mi></mrow></msub><mo id="S3.E7.m1.1.1.2" xref="S3.E7.m1.1.1.2.cmml">=</mo><mrow id="S3.E7.m1.1.1.1" xref="S3.E7.m1.1.1.1.cmml"><mrow id="S3.E7.m1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.cmml"><msub id="S3.E7.m1.1.1.1.1.3" xref="S3.E7.m1.1.1.1.1.3.cmml"><mi id="S3.E7.m1.1.1.1.1.3.2" xref="S3.E7.m1.1.1.1.1.3.2.cmml">k</mi><mi id="S3.E7.m1.1.1.1.1.3.3" xref="S3.E7.m1.1.1.1.1.3.3.cmml">p</mi></msub><mo id="S3.E7.m1.1.1.1.1.2" xref="S3.E7.m1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E7.m1.1.1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.1.1.1.cmml"><mo id="S3.E7.m1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E7.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E7.m1.1.1.1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.1.1.1.cmml"><msub id="S3.E7.m1.1.1.1.1.1.1.1.2" xref="S3.E7.m1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E7.m1.1.1.1.1.1.1.1.2.2" xref="S3.E7.m1.1.1.1.1.1.1.1.2.2.cmml">θ</mi><mi id="S3.E7.m1.1.1.1.1.1.1.1.2.3" xref="S3.E7.m1.1.1.1.1.1.1.1.2.3.cmml">k</mi></msub><mo id="S3.E7.m1.1.1.1.1.1.1.1.1" rspace="0.722em" xref="S3.E7.m1.1.1.1.1.1.1.1.1.cmml">−</mo><mi id="S3.E7.m1.1.1.1.1.1.1.1.3" xref="S3.E7.m1.1.1.1.1.1.1.1.3.cmml">θ</mi></mrow><mo id="S3.E7.m1.1.1.1.1.1.1.3" rspace="0.500em" stretchy="false" xref="S3.E7.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E7.m1.1.1.1.2" rspace="0.722em" xref="S3.E7.m1.1.1.1.2.cmml">−</mo><mrow id="S3.E7.m1.1.1.1.3" xref="S3.E7.m1.1.1.1.3.cmml"><msub id="S3.E7.m1.1.1.1.3.2" xref="S3.E7.m1.1.1.1.3.2.cmml"><mi id="S3.E7.m1.1.1.1.3.2.2" xref="S3.E7.m1.1.1.1.3.2.2.cmml">k</mi><mi id="S3.E7.m1.1.1.1.3.2.3" xref="S3.E7.m1.1.1.1.3.2.3.cmml">d</mi></msub><mo id="S3.E7.m1.1.1.1.3.1" xref="S3.E7.m1.1.1.1.3.1.cmml">⁢</mo><mover accent="true" id="S3.E7.m1.1.1.1.3.3" xref="S3.E7.m1.1.1.1.3.3.cmml"><mi id="S3.E7.m1.1.1.1.3.3.2" xref="S3.E7.m1.1.1.1.3.3.2.cmml">θ</mi><mo id="S3.E7.m1.1.1.1.3.3.1" xref="S3.E7.m1.1.1.1.3.3.1.cmml">˙</mo></mover></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m1.1b"><apply id="S3.E7.m1.1.1.cmml" xref="S3.E7.m1.1.1"><eq id="S3.E7.m1.1.1.2.cmml" xref="S3.E7.m1.1.1.2"></eq><apply id="S3.E7.m1.1.1.3.cmml" xref="S3.E7.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.3.1.cmml" xref="S3.E7.m1.1.1.3">subscript</csymbol><apply id="S3.E7.m1.1.1.3.2.cmml" xref="S3.E7.m1.1.1.3.2"><ci id="S3.E7.m1.1.1.3.2.1.cmml" xref="S3.E7.m1.1.1.3.2.1">¨</ci><ci id="S3.E7.m1.1.1.3.2.2.cmml" xref="S3.E7.m1.1.1.3.2.2">𝜃</ci></apply><apply id="S3.E7.m1.1.1.3.3.cmml" xref="S3.E7.m1.1.1.3.3"><times id="S3.E7.m1.1.1.3.3.1.cmml" xref="S3.E7.m1.1.1.3.3.1"></times><ci id="S3.E7.m1.1.1.3.3.2.cmml" xref="S3.E7.m1.1.1.3.3.2">𝑟</ci><ci id="S3.E7.m1.1.1.3.3.3.cmml" xref="S3.E7.m1.1.1.3.3.3">𝑒</ci><ci id="S3.E7.m1.1.1.3.3.4.cmml" xref="S3.E7.m1.1.1.3.3.4">𝑓</ci></apply></apply><apply id="S3.E7.m1.1.1.1.cmml" xref="S3.E7.m1.1.1.1"><minus id="S3.E7.m1.1.1.1.2.cmml" xref="S3.E7.m1.1.1.1.2"></minus><apply id="S3.E7.m1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1.1"><times id="S3.E7.m1.1.1.1.1.2.cmml" xref="S3.E7.m1.1.1.1.1.2"></times><apply id="S3.E7.m1.1.1.1.1.3.cmml" xref="S3.E7.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.1.3.1.cmml" xref="S3.E7.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E7.m1.1.1.1.1.3.2.cmml" xref="S3.E7.m1.1.1.1.1.3.2">𝑘</ci><ci id="S3.E7.m1.1.1.1.1.3.3.cmml" xref="S3.E7.m1.1.1.1.1.3.3">𝑝</ci></apply><apply id="S3.E7.m1.1.1.1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1"><minus id="S3.E7.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1"></minus><apply id="S3.E7.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E7.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.2.2">𝜃</ci><ci id="S3.E7.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.2.3">𝑘</ci></apply><ci id="S3.E7.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.3">𝜃</ci></apply></apply><apply id="S3.E7.m1.1.1.1.3.cmml" xref="S3.E7.m1.1.1.1.3"><times id="S3.E7.m1.1.1.1.3.1.cmml" xref="S3.E7.m1.1.1.1.3.1"></times><apply id="S3.E7.m1.1.1.1.3.2.cmml" xref="S3.E7.m1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.3.2.1.cmml" xref="S3.E7.m1.1.1.1.3.2">subscript</csymbol><ci id="S3.E7.m1.1.1.1.3.2.2.cmml" xref="S3.E7.m1.1.1.1.3.2.2">𝑘</ci><ci id="S3.E7.m1.1.1.1.3.2.3.cmml" xref="S3.E7.m1.1.1.1.3.2.3">𝑑</ci></apply><apply id="S3.E7.m1.1.1.1.3.3.cmml" xref="S3.E7.m1.1.1.1.3.3"><ci id="S3.E7.m1.1.1.1.3.3.1.cmml" xref="S3.E7.m1.1.1.1.3.3.1">˙</ci><ci id="S3.E7.m1.1.1.1.3.3.2.cmml" xref="S3.E7.m1.1.1.1.3.3.2">𝜃</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m1.1c">\ddot{\theta}_{ref}=k_{p}(\theta_{k}\ -\ \theta)\ -\ k_{d}\dot{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.E7.m1.1d">over¨ start_ARG italic_θ end_ARG start_POSTSUBSCRIPT italic_r italic_e italic_f end_POSTSUBSCRIPT = italic_k start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_θ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - italic_θ ) - italic_k start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT over˙ start_ARG italic_θ end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.6">Inspired by PIP <cite class="ltx_cite ltx_citemacro_citep">(Yi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib99" title="">2022</a>)</cite>, our physics-based optimization module first employs a joint rotation proportional-differential (PD) controller to compute the reference joint angular acceleration <math alttext="\ddot{\theta}_{ref}" class="ltx_Math" display="inline" id="S3.SS3.p2.1.m1.1"><semantics id="S3.SS3.p2.1.m1.1a"><msub id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mover accent="true" id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2.2" xref="S3.SS3.p2.1.m1.1.1.2.2.cmml">θ</mi><mo id="S3.SS3.p2.1.m1.1.1.2.1" xref="S3.SS3.p2.1.m1.1.1.2.1.cmml">¨</mo></mover><mrow id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml"><mi id="S3.SS3.p2.1.m1.1.1.3.2" xref="S3.SS3.p2.1.m1.1.1.3.2.cmml">r</mi><mo id="S3.SS3.p2.1.m1.1.1.3.1" xref="S3.SS3.p2.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS3.p2.1.m1.1.1.3.3" xref="S3.SS3.p2.1.m1.1.1.3.3.cmml">e</mi><mo id="S3.SS3.p2.1.m1.1.1.3.1a" xref="S3.SS3.p2.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS3.p2.1.m1.1.1.3.4" xref="S3.SS3.p2.1.m1.1.1.3.4.cmml">f</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">subscript</csymbol><apply id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2"><ci id="S3.SS3.p2.1.m1.1.1.2.1.cmml" xref="S3.SS3.p2.1.m1.1.1.2.1">¨</ci><ci id="S3.SS3.p2.1.m1.1.1.2.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2.2">𝜃</ci></apply><apply id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3"><times id="S3.SS3.p2.1.m1.1.1.3.1.cmml" xref="S3.SS3.p2.1.m1.1.1.3.1"></times><ci id="S3.SS3.p2.1.m1.1.1.3.2.cmml" xref="S3.SS3.p2.1.m1.1.1.3.2">𝑟</ci><ci id="S3.SS3.p2.1.m1.1.1.3.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3.3">𝑒</ci><ci id="S3.SS3.p2.1.m1.1.1.3.4.cmml" xref="S3.SS3.p2.1.m1.1.1.3.4">𝑓</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">\ddot{\theta}_{ref}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.1.m1.1d">over¨ start_ARG italic_θ end_ARG start_POSTSUBSCRIPT italic_r italic_e italic_f end_POSTSUBSCRIPT</annotation></semantics></math> from the joint angles <math alttext="\theta_{k}" class="ltx_Math" display="inline" id="S3.SS3.p2.2.m2.1"><semantics id="S3.SS3.p2.2.m2.1a"><msub id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml"><mi id="S3.SS3.p2.2.m2.1.1.2" xref="S3.SS3.p2.2.m2.1.1.2.cmml">θ</mi><mi id="S3.SS3.p2.2.m2.1.1.3" xref="S3.SS3.p2.2.m2.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><apply id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p2.2.m2.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2">𝜃</ci><ci id="S3.SS3.p2.2.m2.1.1.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">\theta_{k}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.2.m2.1d">italic_θ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> estimated by the learning-based kinematics module (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.08494v1#S3.E7" title="7 ‣ 3.3. Physics-based Optimization Module ‣ 3. WheelPoser System ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_tag">Equation 7</span></a>, <math alttext="\theta" class="ltx_Math" display="inline" id="S3.SS3.p2.3.m3.1"><semantics id="S3.SS3.p2.3.m3.1a"><mi id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><ci id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.3.m3.1d">italic_θ</annotation></semantics></math> and <math alttext="\dot{\theta}" class="ltx_Math" display="inline" id="S3.SS3.p2.4.m4.1"><semantics id="S3.SS3.p2.4.m4.1a"><mover accent="true" id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml"><mi id="S3.SS3.p2.4.m4.1.1.2" xref="S3.SS3.p2.4.m4.1.1.2.cmml">θ</mi><mo id="S3.SS3.p2.4.m4.1.1.1" xref="S3.SS3.p2.4.m4.1.1.1.cmml">˙</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><apply id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1"><ci id="S3.SS3.p2.4.m4.1.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1.1">˙</ci><ci id="S3.SS3.p2.4.m4.1.1.2.cmml" xref="S3.SS3.p2.4.m4.1.1.2">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">\dot{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.4.m4.1d">over˙ start_ARG italic_θ end_ARG</annotation></semantics></math> are the joint angles and angular velocities, respectively).
The gain parameters <math alttext="k_{p}" class="ltx_Math" display="inline" id="S3.SS3.p2.5.m5.1"><semantics id="S3.SS3.p2.5.m5.1a"><msub id="S3.SS3.p2.5.m5.1.1" xref="S3.SS3.p2.5.m5.1.1.cmml"><mi id="S3.SS3.p2.5.m5.1.1.2" xref="S3.SS3.p2.5.m5.1.1.2.cmml">k</mi><mi id="S3.SS3.p2.5.m5.1.1.3" xref="S3.SS3.p2.5.m5.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m5.1b"><apply id="S3.SS3.p2.5.m5.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.5.m5.1.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS3.p2.5.m5.1.1.2.cmml" xref="S3.SS3.p2.5.m5.1.1.2">𝑘</ci><ci id="S3.SS3.p2.5.m5.1.1.3.cmml" xref="S3.SS3.p2.5.m5.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m5.1c">k_{p}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.5.m5.1d">italic_k start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="k_{d}" class="ltx_Math" display="inline" id="S3.SS3.p2.6.m6.1"><semantics id="S3.SS3.p2.6.m6.1a"><msub id="S3.SS3.p2.6.m6.1.1" xref="S3.SS3.p2.6.m6.1.1.cmml"><mi id="S3.SS3.p2.6.m6.1.1.2" xref="S3.SS3.p2.6.m6.1.1.2.cmml">k</mi><mi id="S3.SS3.p2.6.m6.1.1.3" xref="S3.SS3.p2.6.m6.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m6.1b"><apply id="S3.SS3.p2.6.m6.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.6.m6.1.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS3.p2.6.m6.1.1.2.cmml" xref="S3.SS3.p2.6.m6.1.1.2">𝑘</ci><ci id="S3.SS3.p2.6.m6.1.1.3.cmml" xref="S3.SS3.p2.6.m6.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m6.1c">k_{d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.6.m6.1d">italic_k start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT</annotation></semantics></math> are set to 3600 and 60, respectively, following prior work <cite class="ltx_cite ltx_citemacro_citep">(Yi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib99" title="">2022</a>)</cite>.
These reference accelerations are then refined using quadratic programming to comply with a set of physical constraints, including the equation of motion <cite class="ltx_cite ltx_citemacro_citep">(Featherstone, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib21" title="">2014</a>)</cite> as well as no-sliding restrictions for all contact points between the user and the environment. For a more detailed explanation of the quadratic programming problem, we refer readers to PIP <cite class="ltx_cite ltx_citemacro_citep">(Yi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib99" title="">2022</a>)</cite> and PhysCap <cite class="ltx_cite ltx_citemacro_citep">(Shimada et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib84" title="">2020</a>)</cite>.
Here, unlike previous approaches that primarily model foot-ground contacts during walking, we focus on the contact between the user and the wheelchair, defining contact joints as the left hip, right hip, left foot, and right foot based on common wheelchair sitting scenarios. Moreover, considering that user-wheelchair contact points move together, we optimize the pose in the pelvis-relative frame rather than the global frame used in the existing work. Finally, we perform double integration on the refined joint angular accelerations to derive the optimized pose.</p>
</div>
<figure class="ltx_figure" id="S3.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="98" id="S3.F5.g1" src="extracted/5851880/camera-ready/physics_bold.png" width="592"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.2.1.1" style="font-size:90%;">Figure 5</span>. </span><span class="ltx_text" id="S3.F5.3.2" style="font-size:90%;">Overview of the physics-based optimization module: It calculates reference joint angular acceleration from the kinematics module’s estimates. These accelerations are then refined with physical constraints and double-integrated to produce optimized poses.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S3.F5.4">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S3.F5.5">This figure illustrates the detailed components of the physics-based optimization module. It includes a joint rotation PD controller that calculates the reference joint angular acceleration based on the joint angles estimated by the kinematics module. The reference accelerations are then refined using a set of physical constraints and finally double-integrated to obtain the optimized poses.</p>
</div>
</div>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>WheelPoser-IMU Dataset</h2>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.2.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.2.1.1.1"><span class="ltx_text" id="S4.T1.2.1.1.1.1" style="background-color:#EFEFEF;">ID</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.2.1.1.2"><span class="ltx_text" id="S4.T1.2.1.1.2.1" style="background-color:#EFEFEF;">Gender</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.2.1.1.3"><span class="ltx_text" id="S4.T1.2.1.1.3.1" style="background-color:#EFEFEF;">Age</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.2.1.1.4"><span class="ltx_text" id="S4.T1.2.1.1.4.1" style="background-color:#EFEFEF;">Height (cm)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.2.1.1.5"><span class="ltx_text" id="S4.T1.2.1.1.5.1" style="background-color:#EFEFEF;">Arm length (cm)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.2.1.1.6"><span class="ltx_text" id="S4.T1.2.1.1.6.1" style="background-color:#EFEFEF;">Time Using Wheelchair</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.2.1.1.7"><span class="ltx_text" id="S4.T1.2.1.1.7.1" style="background-color:#EFEFEF;">Wheelchair Model</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.2.2.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.2.2.1.1">W1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.1.2">Male</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.1.3">48</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.1.4">179</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.1.5">58.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.1.6">27 years</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.1.7">TiLite Aero X</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.3.2">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r" id="S4.T1.2.3.2.1">W2</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T1.2.3.2.2">Male</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T1.2.3.2.3">44</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T1.2.3.2.4">183</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T1.2.3.2.5">61.1</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T1.2.3.2.6">12 years</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T1.2.3.2.7">TiLite Aero T</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.3.1.1" style="font-size:90%;">Table 1</span>. </span><span class="ltx_text" id="S4.T1.4.2" style="font-size:90%;">Demographic information of full-time wheelchair user participants.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">As there is no existing motion capture dataset that encompasses typical movements performed by wheelchair users, we sought to collect our own. Our dataset, WheelPoser-IMU, contains 167 minutes of paired motion capture and IMU sensor data from 14 participants (including 2 full-time wheelchair users and 12 participants without motor impairments) performing various on-wheelchair daily activities. Here we describe our data collection apparatus, procedure, participant demographics, and data processing pipeline.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Data Collection Apparatus</h3>
<figure class="ltx_figure" id="S4.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="314" id="S4.F6.g1" src="extracted/5851880/camera-ready/marker_set_unblur.png" width="538"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.2.1.1" style="font-size:90%;">Figure 6</span>. </span><span class="ltx_text" id="S4.F6.3.2" style="font-size:90%;">The WheelPoser-IMU data collection setup includes four IMUs placed on both the participant and their wheelchair, along with 60 retroreflective markers attached across bony landmarks on the participant’s body.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S4.F6.4">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S4.F6.5">This figure illustrates the device setup for data collection, featuring two side-by-side photographs of the same individual seated in a manual wheelchair. On the left, two IMUs are attached to the user’s wrists. On the right, one IMU is attached to the back of the user’s head, and another is affixed to the wheelchair’s axle. Additionally, a total of 60 markers are attached to the user’s body, distributed around important bony landmarks, such as the wrists and shoulder joints.</p>
</div>
</div>
</figure>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Our data collection setup comprised four Movella DOTs <cite class="ltx_cite ltx_citemacro_citep">(Movella.com, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib70" title="">[n. d.]</a>)</cite>, each configured to sample data at a rate of 60 Hz, which is the maximum sampling rate supported for real-time streaming. The four Movella DOTs were time-synchronized and communicated over Bluetooth to a nearby laptop for recording and processing. To capture ground truth pose, we employed a Vicon Motion Capture System consisting of twelve MX40 cameras and four T160 cameras, operating at a sampling rate of 120 FPS. The participants were instrumented with 60 retroreflective markers (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S4.F6" title="Figure 6 ‣ 4.1. Data Collection Apparatus ‣ 4. WheelPoser-IMU Dataset ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_tag">6</span></a>). The Vicon motion capture data was down-sampled and synchronized with the concurrently collected IMU data streams. Following prior work <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib35" title="">2018</a>)</cite>, we further fit an SMPL mesh to this motion capture data using Mosh++ <cite class="ltx_cite ltx_citemacro_citep">(Mahmood et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib62" title="">2019</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Participants and Procedure</h3>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.2.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text" id="S4.T2.2.1.1.1.1" style="background-color:#EFEFEF;">ID</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.2.1.1.2" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text" id="S4.T2.2.1.1.2.1" style="background-color:#EFEFEF;">Gender</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.2.1.1.3" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text" id="S4.T2.2.1.1.3.1" style="background-color:#EFEFEF;">Age</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.2.1.1.4" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text" id="S4.T2.2.1.1.4.1" style="background-color:#EFEFEF;">Height (cm)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.2.1.1.5" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text" id="S4.T2.2.1.1.5.1" style="background-color:#EFEFEF;">Arm length (cm)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.2.2.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.2.2.1.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">P1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.1.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">Male</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.1.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">28</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.1.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">175</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.1.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">56.7</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.3.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T2.2.3.2.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">P2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.3.2.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">Male</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.3.2.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">28</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.3.2.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">180</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.3.2.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">58.3</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.4.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T2.2.4.3.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">P3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.4.3.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">Female</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.4.3.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">27</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.4.3.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">164</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.4.3.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">54.2</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.5.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T2.2.5.4.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">P4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.5.4.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">Male</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.5.4.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">24</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.5.4.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">174</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.5.4.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">58.5</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.6.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T2.2.6.5.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">P5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.6.5.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">Male</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.6.5.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">23</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.6.5.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">183</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.6.5.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">61.6</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.7.6">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T2.2.7.6.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">P6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.7.6.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">Female</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.7.6.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">25</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.7.6.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">168</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.7.6.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">53.9</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.8.7">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T2.2.8.7.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">P7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.8.7.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">Female</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.8.7.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">27</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.8.7.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">165</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.8.7.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">54.9</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.9.8">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T2.2.9.8.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">P8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.9.8.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">Male</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.9.8.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">32</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.9.8.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">173</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.9.8.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">56.7</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.10.9">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T2.2.10.9.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">P9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.10.9.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">Female</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.10.9.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">28</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.10.9.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">160</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.10.9.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">51.1</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.11.10">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T2.2.11.10.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">P10</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.11.10.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">Male</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.11.10.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">24</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.11.10.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">172</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.11.10.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">54.7</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.12.11">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T2.2.12.11.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">P11</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.12.11.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">Female</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.12.11.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">27</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.12.11.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">161</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.12.11.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">52.8</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.13.12">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r" id="S4.T2.2.13.12.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">P12</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T2.2.13.12.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">Female</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T2.2.13.12.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">27</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T2.2.13.12.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">160</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T2.2.13.12.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">52.3</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.3.1.1" style="font-size:90%;">Table 2</span>. </span><span class="ltx_text" id="S4.T2.4.2" style="font-size:90%;">Demographic information of participants without motor impairments.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We recruited 14 participants in total for our data collection study, including 2 full-time manual wheelchair users and 12 participants without motor impairments.
The two full-time wheelchair user participants had been using a wheelchair for 27 and 12 years, respectively. Detailed demographics information are show in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S4.T1" title="Table 1 ‣ 4. WheelPoser-IMU Dataset ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_tag">1</span></a> and  <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S4.T2" title="Table 2 ‣ 4.2. Participants and Procedure ‣ 4. WheelPoser-IMU Dataset ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_tag">2</span></a>. Participants first received instructions on various wheelchair movement techniques and were then allowed to familiarize themselves with these motions until they felt confident. Practice times ranged from 16 minutes to 50 minutes. Full-time wheelchair user participants were asked to use their own manual wheelchairs (TiLite Aero T and Aero X) for data collection to ensure comfort and external validity, while other participants were provided with a PER4MAX Thunder lightweight manual wheelchair.
Before starting our study, we performed a vendor-recommended magnetic field calibration and heading reset procedure on the four IMUs to account for magnetic offsets. Following this, the IMU sensors were attached to the participant and their wheelchair (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.08494v1#S3.F2" title="Figure 2 ‣ 3.1. Device Setup ‣ 3. WheelPoser System ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>). We then performed a calibration process to map the IMU measurements to a body-centric frame and to compensate for the offset between the sensor and the corresponding bone. For further details on calibration, we refer readers to <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib35" title="">2018</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citep">(Yi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib100" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">During our study, we collected the following typical motions that wheelchair users would perform on a daily basis, following established wheelchair skill testing procedures <cite class="ltx_cite ltx_citemacro_citep">(Lindquist et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib56" title="">2010</a>)</cite>:</p>
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1">Arm Motion:</span> Left arm raises (0, 45, 90 degrees), left arm raising overhead, left arm crossing the torso, right arm raises (0, 45, 90 degrees), right arm raising overhead, right arm crossing the torso, both arms raises (0, 45, 90 degrees), both arms raising overhead, both arms crossing the torso, both arms crossing behind the head, both arms swinging.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1">Upper Body:</span> Rotation to the left, rotation to the right, Leaning forward, leaning to the left, leaning to the right, leaning diagonally to the left, leaning diagonally to the right.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i3.p1.1.1">Wheelchair Locomotion:</span> Push forward, push backward, push forward in a circle, push backward in a circle, turn while moving forward, turn-in-place, pivot turn.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">Participants performed these motions in a continuous manner within a recording session (e.g. arm motions) in order to capture natural transitions between categories. Before the start of each recording session, participants were asked to perform five rapid arm raises (the upper body motion seen in typical jumping jacks), to help synchronize motion capture data with IMU sensor data. Data collection typically took 45 minutes for participants without motor impairments and 1 hour for wheelchair user participants.
We note that this 15-minute difference was primarily due to longer rest periods between sessions for wheelchair user participants. We also encountered more instances of mocap markers falling off for these participants, requiring extra time to reposition. The total duration of collected motions was consistent across all participants.
Compensation of $15 and $75 was provided to each participant without motor impairments and wheelchair user participant, respectively.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Data Processing</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">The collected IMU data was first interpolated for missing packets and later synchronized with the SMPL mesh data based on the acceleration markers. In addition to the calibration performed at the beginning of the data collection, the IMU data for each individual session were further calibrated based on the corresponding joint orientation of the first frame of the SMPL mesh to compensate for drift errors that occurred during the data collection process. Later, the arm-raising motions at the beginning of each session were discarded, resulting in an overall 167 minutes of synchronized IMU and SMPL data on various wheelchair-use-related motions, which is nearly twice the size of previously collected ambulatory motion dataset (DIP-IMU and TotalCapture).</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Evaluation</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we first outline the dataset and metrics used for the model training and evaluation in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S5.SS1" title="5.1. Dataset and Metrics ‣ 5. Evaluation ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_tag">5.1</span></a>. Leveraging these datasets and metrics, we then quantitatively evaluate the performance of SOTA models designed for people without motor impairments and our proposed kinematics modules in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S5.SS2" title="5.2. Evaluation of Kinematics Module ‣ 5. Evaluation ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_tag">5.2</span></a>. Using the kinematics module that achieves the best results, we evaluate the effectiveness of our proposed physics-based optimization module in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S5.SS3" title="5.3. Evaluation of Physics Module ‣ 5. Evaluation ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_tag">5.3</span></a>. Lastly, we delve into a detailed exploration of the model’s performance across various on-wheelchair motion types in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S5.SS4" title="5.4. Performance across Motion Types ‣ 5. Evaluation ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_tag">5.4</span></a>.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">All training and evaluation processes were conducted on a computer with an Intel(R) Core(TM) i7-13700KF CPU and an NVIDIA RTX 4080 graphics
card. The kinematics modules were trained with a batch size of 256 using the Adam optimizer and a learning rate of <math alttext="0.001" class="ltx_Math" display="inline" id="S5.p2.1.m1.1"><semantics id="S5.p2.1.m1.1a"><mn id="S5.p2.1.m1.1.1" xref="S5.p2.1.m1.1.1.cmml">0.001</mn><annotation-xml encoding="MathML-Content" id="S5.p2.1.m1.1b"><cn id="S5.p2.1.m1.1.1.cmml" type="float" xref="S5.p2.1.m1.1.1">0.001</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.1.m1.1c">0.001</annotation><annotation encoding="application/x-llamapun" id="S5.p2.1.m1.1d">0.001</annotation></semantics></math>.
We trained all kinematics modules using the mean squared error (MSE) loss function.
In line with previous work, a sliding window of 26 IMU measurement frames was used for real-time pose estimation. Specifically, for biRNN-based modules, the window includes 20 past frames, 1 current frame, and 5 future frames, whereas for the transformer-based module, it comprises 25 past frames and 1 current frame.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Dataset and Metrics</h3>
<figure class="ltx_figure" id="S5.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="411" id="S5.F7.g1" src="extracted/5851880/camera-ready/motions_collected_new.png" width="598"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F7.2.1.1" style="font-size:90%;">Figure 7</span>. </span><span class="ltx_text" id="S5.F7.3.2" style="font-size:90%;">Exemplary motion sequences and poses collected in the WheelPoser-IMU dataset.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S5.F7.4">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S5.F7.5">This figure showcases the motions collected in the WheelPoser-IMU dataset. Three rows of icons are displayed. The top row depicts the arm motions captured, including the left arm raised at 0, 45, and 90 degrees, left arm overhead raise, left arm across the torso, both arms raised at 0, 45, and 90 degrees, both arms across the torso, both arms crossed behind the head, and both arms swinging. The second row presents various upper body motions, such as leaning to the left, rotating to the left, leaning to the right, rotating to the right, leaning forward, and leaning diagonally to the left and right. The third row illustrates several wheelchair locomotions, including pushing forward and backward, pushing in a circle, turning while moving forward, turning in place, and executing pivot turns.</p>
</div>
</div>
</figure>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">For training the kinematics modules, we first utilize data synthesized from AMASS, followed by fine-tuning using our collected WheelPoser-IMU dataset.
To assess the effectiveness and external validity of our proposed method, we conducted a leave-one-subject-out evaluation study exclusively on the full-time wheelchair users’ data. Specifically, we upsample data from a single wheelchair user and combine it with data from 12 participants without motor impairments to create a fine-tuning dataset, of which 40% comprises the single full-time wheelchair user’s data. The fine-tuned model is then evaluated on the excluded wheelchair user’s data, and we report the averaged results from two full-time wheelchair users.
We use the following commonly used metrics to quantitatively evaluate estimated poses:</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<ol class="ltx_enumerate" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S5.I1.i1.p1.1.1">Joint angular error</span>, which measures the mean global rotation error of estimated upper body joints in degrees.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S5.I1.i2.p1.1.1">Joint position error</span>, which measures the mean Euclidean distance error of estimated upper body joints in centimeters with the pelvis joint aligned.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S5.I1.i3.p1.1.1">Mesh error</span>, which measures the mean Euclidean distance error of all vertices of the estimated upper body mesh also with the pelvis joint aligned and body model in mean shape</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(4)</span>
<div class="ltx_para" id="S5.I1.i4.p1">
<p class="ltx_p" id="S5.I1.i4.p1.1"><span class="ltx_text ltx_font_italic" id="S5.I1.i4.p1.1.1">Jitter error</span>, which measures the average jerk of estimated upper body joints, which is the third derivative of position with respect to time and reflects the naturalness of the motion.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">Moreover, we incorporate wrist joint position error and elbow joint position error as two additional metrics. This stems from the significance of wrist position in determining the propulsion pattern of wheelchair users <cite class="ltx_cite ltx_citemacro_citep">(Boninger et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib8" title="">2002</a>)</cite>, while elbow positions provide valuable insights into the upper extremity’s range of motion and the associated risk for overhead reaching <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib54" title="">2023</a>)</cite>.
To obtain the estimated joint positions and body mesh, we used the mean SMPL shape parameters, ensuring consistency with prior work and mimicking real-world scenarios where users’ specific shape parameters are typically unavailable.</p>
</div>
<figure class="ltx_figure" id="S5.F8">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F8.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="741" id="S5.F8.sf1.g1" src="extracted/5851880/camera-ready/arm.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F8.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S5.F8.sf1.3.2" style="font-size:90%;">Arm reaching motion</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F8.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="741" id="S5.F8.sf2.g1" src="extracted/5851880/camera-ready/upperbody.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F8.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S5.F8.sf2.3.2" style="font-size:90%;">Torso rotation.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F8.sf3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" height="356" id="S5.F8.sf3.g1" src="extracted/5851880/camera-ready/loco1.png" width="287"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" height="356" id="S5.F8.sf3.g2" src="extracted/5851880/camera-ready/loco2.png" width="287"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F8.sf3.2.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S5.F8.sf3.3.2" style="font-size:90%;">Wheelchair locomotion.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F8.2.1.1" style="font-size:90%;">Figure 8</span>. </span><span class="ltx_text" id="S5.F8.3.2" style="font-size:90%;">Qualitative comparison of pose estimation results between TransPose and our three-stage biRNN model.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S5.F8.4">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S5.F8.5">This figure illustrates a collection of pose estimation results from TransPose and WheelPoser, along with the ground truth. The left side displays the pose estimation results for arm motions, indicating that WheelPoser captures these motions more accurately than previous systems. The middle section demonstrates results for torso rotations, showing that WheelPoser captures the rotation angle with greater precision. The right side presents pose estimation results for wheelchair locomotions, revealing that WheelPoser accurately captures both the pelvis joint movements and the wheelchair propulsion and rotation motions.</p>
</div>
</div>
</figure>
<figure class="ltx_table" id="S5.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T3.1" style="width:433.6pt;height:82pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-166.7pt,31.3pt) scale(0.565375751774322,0.565375751774322) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T3.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T3.1.1.1.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span class="ltx_rule" style="width:0.0pt;height:9.0pt;background:black;display:inline-block;"></span>
<span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.2.1">Models</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.1.3" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.3.1">Ang Err (deg)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.1.4" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.4.1">Pos Err (cm)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.1.5" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.5.1">Mesh Err (cm)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.1.1" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.1.1">Jitter <math alttext="\mathbf{(10^{2}/m^{3})}" class="ltx_Math" display="inline" id="S5.T3.1.1.1.1.1.m1.1"><semantics id="S5.T3.1.1.1.1.1.m1.1a"><mrow id="S5.T3.1.1.1.1.1.m1.1.1.1" xref="S5.T3.1.1.1.1.1.m1.1.1.1.1.cmml"><mo id="S5.T3.1.1.1.1.1.m1.1.1.1.2" stretchy="false" xref="S5.T3.1.1.1.1.1.m1.1.1.1.1.cmml">(</mo><mrow id="S5.T3.1.1.1.1.1.m1.1.1.1.1" xref="S5.T3.1.1.1.1.1.m1.1.1.1.1.cmml"><msup id="S5.T3.1.1.1.1.1.m1.1.1.1.1.2" xref="S5.T3.1.1.1.1.1.m1.1.1.1.1.2.cmml"><mn id="S5.T3.1.1.1.1.1.m1.1.1.1.1.2.2" xref="S5.T3.1.1.1.1.1.m1.1.1.1.1.2.2.cmml">𝟏𝟎</mn><mn id="S5.T3.1.1.1.1.1.m1.1.1.1.1.2.3" xref="S5.T3.1.1.1.1.1.m1.1.1.1.1.2.3.cmml">𝟐</mn></msup><mo id="S5.T3.1.1.1.1.1.m1.1.1.1.1.1" xref="S5.T3.1.1.1.1.1.m1.1.1.1.1.1.cmml">/</mo><msup id="S5.T3.1.1.1.1.1.m1.1.1.1.1.3" xref="S5.T3.1.1.1.1.1.m1.1.1.1.1.3.cmml"><mi id="S5.T3.1.1.1.1.1.m1.1.1.1.1.3.2" xref="S5.T3.1.1.1.1.1.m1.1.1.1.1.3.2.cmml">𝐦</mi><mn id="S5.T3.1.1.1.1.1.m1.1.1.1.1.3.3" xref="S5.T3.1.1.1.1.1.m1.1.1.1.1.3.3.cmml">𝟑</mn></msup></mrow><mo id="S5.T3.1.1.1.1.1.m1.1.1.1.3" stretchy="false" xref="S5.T3.1.1.1.1.1.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.1.1.m1.1b"><apply id="S5.T3.1.1.1.1.1.m1.1.1.1.1.cmml" xref="S5.T3.1.1.1.1.1.m1.1.1.1"><divide id="S5.T3.1.1.1.1.1.m1.1.1.1.1.1.cmml" xref="S5.T3.1.1.1.1.1.m1.1.1.1.1.1"></divide><apply id="S5.T3.1.1.1.1.1.m1.1.1.1.1.2.cmml" xref="S5.T3.1.1.1.1.1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.T3.1.1.1.1.1.m1.1.1.1.1.2.1.cmml" xref="S5.T3.1.1.1.1.1.m1.1.1.1.1.2">superscript</csymbol><cn id="S5.T3.1.1.1.1.1.m1.1.1.1.1.2.2.cmml" type="integer" xref="S5.T3.1.1.1.1.1.m1.1.1.1.1.2.2">10</cn><cn id="S5.T3.1.1.1.1.1.m1.1.1.1.1.2.3.cmml" type="integer" xref="S5.T3.1.1.1.1.1.m1.1.1.1.1.2.3">2</cn></apply><apply id="S5.T3.1.1.1.1.1.m1.1.1.1.1.3.cmml" xref="S5.T3.1.1.1.1.1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.T3.1.1.1.1.1.m1.1.1.1.1.3.1.cmml" xref="S5.T3.1.1.1.1.1.m1.1.1.1.1.3">superscript</csymbol><ci id="S5.T3.1.1.1.1.1.m1.1.1.1.1.3.2.cmml" xref="S5.T3.1.1.1.1.1.m1.1.1.1.1.3.2">𝐦</ci><cn id="S5.T3.1.1.1.1.1.m1.1.1.1.1.3.3.cmml" type="integer" xref="S5.T3.1.1.1.1.1.m1.1.1.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.1.1.m1.1c">\mathbf{(10^{2}/m^{3})}</annotation><annotation encoding="application/x-llamapun" id="S5.T3.1.1.1.1.1.m1.1d">( bold_10 start_POSTSUPERSCRIPT bold_2 end_POSTSUPERSCRIPT / bold_m start_POSTSUPERSCRIPT bold_3 end_POSTSUPERSCRIPT )</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.1.6" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.6.1">Wrist Pos Err (cm)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.1.7" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.7.1">Elbow Pos Err (cm)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.1.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T3.1.1.2.1.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">IMUPoser <cite class="ltx_cite ltx_citemacro_citep">(Mollyn et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib66" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.2.1.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">45.20 (10.07)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.2.1.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">18.27 (5.29)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.2.1.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">39.13 (5.71)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.2.1.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">17.59 (26.81)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.2.1.6" style="padding-top:0.5pt;padding-bottom:0.5pt;">24.27 (10.27)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.2.1.7" style="padding-top:0.5pt;padding-bottom:0.5pt;">15.59 (6.69)</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T3.1.1.3.2.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">TransPose <cite class="ltx_cite ltx_citemacro_citep">(Yi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib100" title="">2021</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.3.2.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">44.31 (5.97)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.3.2.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">19.05 (4.77)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.3.2.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">40.16 (3.92)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.3.2.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">3.34 (5.72)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.3.2.6" style="padding-top:0.5pt;padding-bottom:0.5pt;">23.20 (9.21)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.3.2.7" style="padding-top:0.5pt;padding-bottom:0.5pt;">17.15 (6.79)</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T3.1.1.4.3.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">PIP <cite class="ltx_cite ltx_citemacro_citep">(Yi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib99" title="">2022</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.4.3.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">63.97 (20.07)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.4.3.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">23.09 (6.11)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.4.3.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">49.44 (7.80)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.4.3.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">3.13 (23.03)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.4.3.6" style="padding-top:0.5pt;padding-bottom:0.5pt;">28.42 (12.56)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.4.3.7" style="padding-top:0.5pt;padding-bottom:0.5pt;">22.76 (9.23)</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T3.1.1.5.4.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">TIP <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib41" title="">2022</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.5.4.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">44.19 (6.12)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.5.4.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">18.56 (3.87)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.5.4.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">40.34 (3.52)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.5.4.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">8.45 (8.68)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.5.4.6" style="padding-top:0.5pt;padding-bottom:0.5pt;">21.34 (7.42)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.5.4.7" style="padding-top:0.5pt;padding-bottom:0.5pt;">14.91 (5.87)</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.6.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T3.1.1.6.5.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">Single-stage biRNN (ours)</th>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.6.5.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">15.09 (5.55)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.6.5.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">7.12 (3.15)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.6.5.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">8.89 (2.92)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.6.5.5" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.6.5.5.1">3.09 (4.65)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.6.5.6" style="padding-top:0.5pt;padding-bottom:0.5pt;">13.56 (6.19)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.6.5.7" style="padding-top:0.5pt;padding-bottom:0.5pt;">11.38 (5.53)</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.7.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T3.1.1.7.6.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">Three-stage biRNN (ours)</th>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.7.6.2" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.7.6.2.1">14.23 (5.26)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.7.6.3" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.7.6.3.1">6.71 (2.95)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.7.6.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">8.09 (2.72)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.7.6.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">4.13 (6.04)</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.7.6.6" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.7.6.6.1">12.76 (5.81)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.7.6.7" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.7.6.7.1">11.15 (5.16)</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.8.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="S5.T3.1.1.8.7.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">Transformer-based (ours)</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.1.1.8.7.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">14.46 (5.48)</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.1.1.8.7.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">6.96 (3.30)</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.1.1.8.7.4" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.8.7.4.1">7.88 (2.74)</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.1.1.8.7.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">19.39 (17.72)</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.1.1.8.7.6" style="padding-top:0.5pt;padding-bottom:0.5pt;">12.81 (6.58)</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.1.1.8.7.7" style="padding-top:0.5pt;padding-bottom:0.5pt;">11.66 (6.10)</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T3.3.1.1" style="font-size:90%;">Table 3</span>. </span><span class="ltx_text" id="S5.T3.4.2" style="font-size:90%;">Results for upper body pose estimation across various models: mean (std). We compare our candidate models with prior pose estimation models that utilize sparse IMUs, including IMUPoser <cite class="ltx_cite ltx_citemacro_citep">(Mollyn et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib66" title="">2023</a>)</cite>, TransPose <cite class="ltx_cite ltx_citemacro_citep">(Yi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib100" title="">2021</a>)</cite>, PIP <cite class="ltx_cite ltx_citemacro_citep">(Yi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib99" title="">2022</a>)</cite>, and TIP <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib41" title="">2022</a>)</cite>.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Evaluation of Kinematics Module</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">The quantitative comparisons of the different kinematics modules are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S5.T3" title="Table 3 ‣ 5.1. Dataset and Metrics ‣ 5. Evaluation ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_tag">3</span></a>, where the mean and standard deviation for each metric are presented.
In addition to assessing the performance of the three proposed kinematics modules, we also evaluate four state-of-the-art (SOTA) models designed for people without motor impairments, including IMUPoser <cite class="ltx_cite ltx_citemacro_citep">(Mollyn et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib66" title="">2023</a>)</cite>, TransPose <cite class="ltx_cite ltx_citemacro_citep">(Yi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib100" title="">2021</a>)</cite>, PIP <cite class="ltx_cite ltx_citemacro_citep">(Yi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib99" title="">2022</a>)</cite>, and TIP <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib41" title="">2022</a>)</cite>. We use the officially released versions provided by the authors and test them on data from the two full-time wheelchair users. Given that most existing models necessitate a sensor setup of 6 IMUs (with two additional sensors on the legs or thighs), we compensate for missing IMU data from the legs using the IMU data of the wheelchair and solely compare the upper body pose estimation results to ensure fair comparisons.</p>
</div>
<section class="ltx_subsubsection" id="S5.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1. </span>SOTA Models versus Our Candidate Models</h4>
<div class="ltx_para" id="S5.SS2.SSS1.p1">
<p class="ltx_p" id="S5.SS2.SSS1.p1.1">First, it is evident that all existing models trained and fine-tuned solely on datasets from individuals without motor impairments are incapable of effectively predicting on-wheelchair motions. This demonstrates a substantial disparity between on-wheelchair motion patterns and those present in existing datasets collected in ambulatory-dominated scenarios, such as standing and walking.
In contrast, our proposed models consistently exhibit significantly better performance than existing models, demonstrating the effectiveness of our proposed data synthesis method, pose estimation pipeline, and the WheelPoser-IMU dataset.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p2">
<p class="ltx_p" id="S5.SS2.SSS1.p2.1">Additionally, we visually compare the pose estimation results between our proposed kinematics modules and SOTA models to provide an intuitive understanding of the sources of error and model differences. For presentation clarity, we showcase the results of our three-stage biRNN-based kinematics module alongside TransPose. Additionally, we present the reference and predicted poses in a full-body manner by matching the lower body joint angles to the ground truth, thereby providing an intuitive demonstration of the pelvis joint angle.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p3">
<p class="ltx_p" id="S5.SS2.SSS1.p3.1">A major source of difference arises from the estimation of pelvis orientation. Specifically, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S5.F8.sf3" title="In Figure 8 ‣ 5.1. Dataset and Metrics ‣ 5. Evaluation ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_tag">8(c)</span></a>, models trained and fine-tuned solely on ambulatory motion consistently exhibit bias towards pelvis orientation in standing poses. In contrast, our models can compensate for differences in pelvis orientation. Additionally, wheelchair users often need to perform overhead reaching and far-reaching actions in their everyday lives due to their sitting configuration and environmental accessibility barriers. However, as depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S5.F8.sf1" title="In Figure 8 ‣ 5.1. Dataset and Metrics ‣ 5. Evaluation ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_tag">8(a)</span></a>, models trained exclusively on ambulatory motions display biases in shoulder and arm bending angles and fail to predict straight arm motions accurately. In contrast, our models demonstrate better performance in capturing this essential aspect of wheelchair users’ motion. Similarly, wheelchair users often need to rotate or bend their torso for essential activities like pressure relief. In response, our models exhibit improved performance in predicting corresponding trunk joint angles, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S5.F8.sf2" title="In Figure 8 ‣ 5.1. Dataset and Metrics ‣ 5. Evaluation ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_tag">8(b)</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p4">
<p class="ltx_p" id="S5.SS2.SSS1.p4.1">Another significant difference between our model and existing models for individuals without motor impairments lies in pose prediction during wheelchair locomotion, such as propulsion and turning. As indicated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S5.F8.sf3" title="In Figure 8 ‣ 5.1. Dataset and Metrics ‣ 5. Evaluation ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_tag">8(c)</span></a>, our models exhibit significantly better performance in predicting the pose of upper extremities and torso during such motions. More comparisons can be found in our Video Figure.</p>
</div>
<figure class="ltx_table" id="S5.T4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T4.1" style="width:411.9pt;height:50.2pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-168.0pt,20.2pt) scale(0.550789997395061,0.550789997395061) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T4.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T4.1.1.1.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span class="ltx_rule" style="width:0.0pt;height:9.0pt;background:black;display:inline-block;"></span>
<span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.2.1">Models</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.1.1.1.3" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.3.1">Ang Err (deg)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.1.1.1.4" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.4.1">Pos Err (cm)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.1.1.1.5" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.5.1">Mesh Err (cm)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.1.1.1.1" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.1.1">Jitter <math alttext="\mathbf{(10^{2}/m^{3})}" class="ltx_Math" display="inline" id="S5.T4.1.1.1.1.1.m1.1"><semantics id="S5.T4.1.1.1.1.1.m1.1a"><mrow id="S5.T4.1.1.1.1.1.m1.1.1.1" xref="S5.T4.1.1.1.1.1.m1.1.1.1.1.cmml"><mo id="S5.T4.1.1.1.1.1.m1.1.1.1.2" stretchy="false" xref="S5.T4.1.1.1.1.1.m1.1.1.1.1.cmml">(</mo><mrow id="S5.T4.1.1.1.1.1.m1.1.1.1.1" xref="S5.T4.1.1.1.1.1.m1.1.1.1.1.cmml"><msup id="S5.T4.1.1.1.1.1.m1.1.1.1.1.2" xref="S5.T4.1.1.1.1.1.m1.1.1.1.1.2.cmml"><mn id="S5.T4.1.1.1.1.1.m1.1.1.1.1.2.2" xref="S5.T4.1.1.1.1.1.m1.1.1.1.1.2.2.cmml">𝟏𝟎</mn><mn id="S5.T4.1.1.1.1.1.m1.1.1.1.1.2.3" xref="S5.T4.1.1.1.1.1.m1.1.1.1.1.2.3.cmml">𝟐</mn></msup><mo id="S5.T4.1.1.1.1.1.m1.1.1.1.1.1" xref="S5.T4.1.1.1.1.1.m1.1.1.1.1.1.cmml">/</mo><msup id="S5.T4.1.1.1.1.1.m1.1.1.1.1.3" xref="S5.T4.1.1.1.1.1.m1.1.1.1.1.3.cmml"><mi id="S5.T4.1.1.1.1.1.m1.1.1.1.1.3.2" xref="S5.T4.1.1.1.1.1.m1.1.1.1.1.3.2.cmml">𝐦</mi><mn id="S5.T4.1.1.1.1.1.m1.1.1.1.1.3.3" xref="S5.T4.1.1.1.1.1.m1.1.1.1.1.3.3.cmml">𝟑</mn></msup></mrow><mo id="S5.T4.1.1.1.1.1.m1.1.1.1.3" stretchy="false" xref="S5.T4.1.1.1.1.1.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.1.1.m1.1b"><apply id="S5.T4.1.1.1.1.1.m1.1.1.1.1.cmml" xref="S5.T4.1.1.1.1.1.m1.1.1.1"><divide id="S5.T4.1.1.1.1.1.m1.1.1.1.1.1.cmml" xref="S5.T4.1.1.1.1.1.m1.1.1.1.1.1"></divide><apply id="S5.T4.1.1.1.1.1.m1.1.1.1.1.2.cmml" xref="S5.T4.1.1.1.1.1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.T4.1.1.1.1.1.m1.1.1.1.1.2.1.cmml" xref="S5.T4.1.1.1.1.1.m1.1.1.1.1.2">superscript</csymbol><cn id="S5.T4.1.1.1.1.1.m1.1.1.1.1.2.2.cmml" type="integer" xref="S5.T4.1.1.1.1.1.m1.1.1.1.1.2.2">10</cn><cn id="S5.T4.1.1.1.1.1.m1.1.1.1.1.2.3.cmml" type="integer" xref="S5.T4.1.1.1.1.1.m1.1.1.1.1.2.3">2</cn></apply><apply id="S5.T4.1.1.1.1.1.m1.1.1.1.1.3.cmml" xref="S5.T4.1.1.1.1.1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.T4.1.1.1.1.1.m1.1.1.1.1.3.1.cmml" xref="S5.T4.1.1.1.1.1.m1.1.1.1.1.3">superscript</csymbol><ci id="S5.T4.1.1.1.1.1.m1.1.1.1.1.3.2.cmml" xref="S5.T4.1.1.1.1.1.m1.1.1.1.1.3.2">𝐦</ci><cn id="S5.T4.1.1.1.1.1.m1.1.1.1.1.3.3.cmml" type="integer" xref="S5.T4.1.1.1.1.1.m1.1.1.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.1.1.m1.1c">\mathbf{(10^{2}/m^{3})}</annotation><annotation encoding="application/x-llamapun" id="S5.T4.1.1.1.1.1.m1.1d">( bold_10 start_POSTSUPERSCRIPT bold_2 end_POSTSUPERSCRIPT / bold_m start_POSTSUPERSCRIPT bold_3 end_POSTSUPERSCRIPT )</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.1.1.1.6" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.6.1">Wrist Pos Err (cm)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.1.1.1.7" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.7.1">Elbow Pos Err (cm)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.1.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T4.1.1.2.1.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">TransPose <cite class="ltx_cite ltx_citemacro_citep">(Yi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib100" title="">2021</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.1.2.1.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">44.31 (5.97)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.1.2.1.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">19.05 (4.77)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.1.2.1.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">40.16 (3.92)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.1.2.1.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">3.34 (5.72)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.1.2.1.6" style="padding-top:0.5pt;padding-bottom:0.5pt;">23.20 (9.21)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.1.2.1.7" style="padding-top:0.5pt;padding-bottom:0.5pt;">17.15 (6.79)</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T4.1.1.3.2.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">PIP <cite class="ltx_cite ltx_citemacro_citep">(Yi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib99" title="">2022</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.3.2.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">63.97 (20.07)</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.3.2.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">23.09 (6.11)</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.3.2.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">49.44 (7.80)</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.3.2.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">3.03 (23.03)</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.3.2.6" style="padding-top:0.5pt;padding-bottom:0.5pt;">28.42 (12.56)</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.3.2.7" style="padding-top:0.5pt;padding-bottom:0.5pt;">22.76 (9.23)</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T4.1.1.4.3.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">Kinematics Only</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.4.3.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">14.23 (5.26)</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.4.3.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">6.71 (2.95)</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.4.3.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">8.09 (2.72)</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.4.3.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">4.13 (6.04)</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.4.3.6" style="padding-top:0.5pt;padding-bottom:0.5pt;">12.76 (5.81)</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.4.3.7" style="padding-top:0.5pt;padding-bottom:0.5pt;">11.15 (5.16)</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="S5.T4.1.1.5.4.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">With physics</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.1.1.5.4.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">14.30 (5.31)</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.1.1.5.4.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">6.74 (2.98)</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.1.1.5.4.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">8.10 (2.73)</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.1.1.5.4.5" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.5.4.5.1">2.71 (4.21)</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.1.1.5.4.6" style="padding-top:0.5pt;padding-bottom:0.5pt;">12.86 (5.90)</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.1.1.5.4.7" style="padding-top:0.5pt;padding-bottom:0.5pt;">11.19 (5.21)</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T4.3.1.1" style="font-size:90%;">Table 4</span>. </span><span class="ltx_text" id="S5.T4.4.2" style="font-size:90%;">Results of the physics-based optimization module. Here we compare the effectiveness of our module against PIP which focused on foot-ground contact modeling. WheelPoser’s physics module reduces jitter error by more than 30%.</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S5.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2. </span>Results Across Our Candidate Models</h4>
<div class="ltx_para" id="S5.SS2.SSS2.p1">
<p class="ltx_p" id="S5.SS2.SSS2.p1.1">On comparing the performance of the three variations of our proposed kinematics module, the single-stage biRNN-based model shows the lowest jitter error, the transformer-based model exhibits the lowest vertex error, while the three-stage biRNN-based model outperforms the others across the remaining metrics. The enhanced performance of the three-stage biRNN in estimating joint angles and positions aligns with prior research on people without motor impairments <cite class="ltx_cite ltx_citemacro_citep">(Yi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib100" title="">2021</a>)</cite>, highlighting the efficacy of utilizing joint positions as an intermediate representation for learning complex human motion priors in on-wheelchair movements. We also observe a significantly high jitter error from the transformer-based model, similar to TIP <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib41" title="">2022</a>)</cite>. Furthermore, the three-stage biRNN-based model presents a higher jitter error compared to the single-stage model. This difference may arise from the presence of noisy joint position outputs in the early-stage networks of our model, as opposed to the end-to-end mapping from IMU measurements to pose employed in the single-stage biRNN model.
Overall, given the enhanced pose estimation performance and relatively low jitter error of the three-stage biRNN-based kinematics model, we select it as the kinematics model for WheelPoser and further evaluation of the physics-based optimization module.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Evaluation of Physics Module</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">We assess the effectiveness of our physics-based optimization module in reducing the jitter error of the predicted pose generated by the kinematics module. The detailed results are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S5.T4" title="Table 4 ‣ 5.2.1. SOTA Models versus Our Candidate Models ‣ 5.2. Evaluation of Kinematics Module ‣ 5. Evaluation ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_tag">4</span></a>.
Here, we revisit the performance metrics of TransPose and PIP for comparison, as TransPose is a kinematics-only model, whereas PIP incorporates physics-based modeling for foot-ground contact in ambulatory motion.
Notably, PIP achieves much worse pose estimation results for on-wheelchair motions compared to its kinematics-only counterpart. We attribute this to its focus on modeling foot-ground contact, which is not directly applicable to and actually hinders pose estimation performance in wheelchair-use scenarios.
Specifically, as PIP predicts foot-ground contact probability using a model trained on ambulatory motions, it is inherently biased towards predicting one or both feet in contact with the ground. These probabilities are then used to refine the pose with a physics-based optimizer, where errors in contact probability lead to errors in pose. However, during wheelchair locomotions, the feet “slide” with respect to the ground, which PIP fails to predict accurately, due to this bias. Additionally, PIP optimizes the pose in the global frame, where the ground and footrest should be considered as different objects, leading to further errors. Taken together, these factors cause their approach to fail in wheelchair usage cases.
In contrast, our physics-based optimization module is designed specifically for on-wheelchair motions and significantly reduces the jitter error by more than 30% while maintaining performance across other metrics.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">The inclusion of a physics module offers other advantages. For instance, we can now estimate the joint torques of the user, which are critical to applications like upper extremity health monitoring, which we will discuss in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S6.SS4" title="6.4. Enhanced Data-Driven Collaborative Care ‣ 6. Example Applications ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_tag">6.4</span></a>. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S5.F9" title="Figure 9 ‣ 5.3. Evaluation of Physics Module ‣ 5. Evaluation ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_tag">9</span></a> shows exemplary estimated joint torques.
We can see that in order for the user to accelerate their arm for the lifting motion, their right shoulder produces a large positive torque (1) and as the arm reaches its highest point, the right shoulder slows the arm down by producing a large negative torque (3). And the estimated joint torque is lower when the user’s arm is moving at a steady speed (2).</p>
</div>
<figure class="ltx_figure" id="S5.F9">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="609" id="S5.F9.g1" src="extracted/5851880/camera-ready/joint_torque_new.png" width="568"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F9.2.1.1" style="font-size:90%;">Figure 9</span>. </span><span class="ltx_text" id="S5.F9.3.2" style="font-size:90%;">Visualization of estimated joint torques of the right shoulder joint and its corresponding motion sequences.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S5.F9.4">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S5.F9.5">This figure displays the estimated joint torque information during an upper body motion. The left side shows a motion sequence of raising the left arm. On the right side, a line chart presents the estimated joint torque. The joint torque is high at the beginning of the motion due to acceleration, drops to around zero in the middle when acceleration decreases, and increases to negative values at the end to decelerate the arm movement.</p>
</div>
</div>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4. </span>Performance across Motion Types</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">In this section, we delve into a detailed analysis of our model’s performance across various wheelchair motion types. Specifically, we categorize our collected wheelchair motion types into five distinct groups, comprising: 1) arm motion and 2) upper body motion as detailed in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S4.SS2" title="4.2. Participants and Procedure ‣ 4. WheelPoser-IMU Dataset ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_tag">4.2</span></a>, 3) translational wheelchair locomotion (pushing forward and backward), 4) rotational wheelchair locomotion (turning in place and executing a pivot turn), and 5) combined wheelchair locomotion (pushing forward in a circle, pushing backward in a circle, and turning while moving forward). The results of these analyses are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S5.T5" title="Table 5 ‣ 5.4. Performance across Motion Types ‣ 5. Evaluation ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_tag">5</span></a>.
Specifically, we observe that our model demonstrates comparable performance on wheelchair location motions, including translational, rotational, and combined wheelchair motions, yet experiences a drop in performance on stationary motions such as arm and upper body movements.
For instance, arm motions demonstrate the largest wrist position error and joint angle error, and upper body motions demonstrate the largest error in joint position and body mesh.
We attribute these increased errors in arm motions to biases in the elbow and shoulder bending angles present in AMASS dataset.
Specifically, straight arm movements and overhead arm motions are underrepresented in AMASS. Yet, they constitute a significantly larger portion of our dataset, reflecting typical wheelchair use characteristics.
Regarding the higher errors found in upper body motions, we believe they are tied to the low accelerations of the user’s head during these movements. Specifically, as our model largely relies on the single IMU positioned on the user’s head to infer the trunk motion, the low accelerations hinder the model’s ability to accurately predict joint positions and consequently the body mesh.
For qualitative understanding, we also showcase samples of estimated pose colored based on mesh error in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S5.F10" title="Figure 10 ‣ 5.4. Performance across Motion Types ‣ 5. Evaluation ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_tag">10</span></a>.</p>
</div>
<figure class="ltx_table" id="S5.T5">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T5.1" style="width:411.9pt;height:78.8pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-78.9pt,15.0pt) scale(0.723063070321107,0.723063070321107) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T5.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T5.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T5.1.1.1.2">
<span class="ltx_rule" style="width:0.0pt;height:10.0pt;background:black;display:inline-block;"></span>
<span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.2.1">Motion Types</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.3.1">Ang Err (deg)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.4.1">Pos Err (cm)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.5.1">Mesh Err (cm)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.1.1">Jitter <math alttext="\mathbf{(10^{2}/m^{3})}" class="ltx_Math" display="inline" id="S5.T5.1.1.1.1.1.m1.1"><semantics id="S5.T5.1.1.1.1.1.m1.1a"><mrow id="S5.T5.1.1.1.1.1.m1.1.1.1" xref="S5.T5.1.1.1.1.1.m1.1.1.1.1.cmml"><mo id="S5.T5.1.1.1.1.1.m1.1.1.1.2" stretchy="false" xref="S5.T5.1.1.1.1.1.m1.1.1.1.1.cmml">(</mo><mrow id="S5.T5.1.1.1.1.1.m1.1.1.1.1" xref="S5.T5.1.1.1.1.1.m1.1.1.1.1.cmml"><msup id="S5.T5.1.1.1.1.1.m1.1.1.1.1.2" xref="S5.T5.1.1.1.1.1.m1.1.1.1.1.2.cmml"><mn id="S5.T5.1.1.1.1.1.m1.1.1.1.1.2.2" xref="S5.T5.1.1.1.1.1.m1.1.1.1.1.2.2.cmml">𝟏𝟎</mn><mn id="S5.T5.1.1.1.1.1.m1.1.1.1.1.2.3" xref="S5.T5.1.1.1.1.1.m1.1.1.1.1.2.3.cmml">𝟐</mn></msup><mo id="S5.T5.1.1.1.1.1.m1.1.1.1.1.1" xref="S5.T5.1.1.1.1.1.m1.1.1.1.1.1.cmml">/</mo><msup id="S5.T5.1.1.1.1.1.m1.1.1.1.1.3" xref="S5.T5.1.1.1.1.1.m1.1.1.1.1.3.cmml"><mi id="S5.T5.1.1.1.1.1.m1.1.1.1.1.3.2" xref="S5.T5.1.1.1.1.1.m1.1.1.1.1.3.2.cmml">𝐦</mi><mn id="S5.T5.1.1.1.1.1.m1.1.1.1.1.3.3" xref="S5.T5.1.1.1.1.1.m1.1.1.1.1.3.3.cmml">𝟑</mn></msup></mrow><mo id="S5.T5.1.1.1.1.1.m1.1.1.1.3" stretchy="false" xref="S5.T5.1.1.1.1.1.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.T5.1.1.1.1.1.m1.1b"><apply id="S5.T5.1.1.1.1.1.m1.1.1.1.1.cmml" xref="S5.T5.1.1.1.1.1.m1.1.1.1"><divide id="S5.T5.1.1.1.1.1.m1.1.1.1.1.1.cmml" xref="S5.T5.1.1.1.1.1.m1.1.1.1.1.1"></divide><apply id="S5.T5.1.1.1.1.1.m1.1.1.1.1.2.cmml" xref="S5.T5.1.1.1.1.1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.T5.1.1.1.1.1.m1.1.1.1.1.2.1.cmml" xref="S5.T5.1.1.1.1.1.m1.1.1.1.1.2">superscript</csymbol><cn id="S5.T5.1.1.1.1.1.m1.1.1.1.1.2.2.cmml" type="integer" xref="S5.T5.1.1.1.1.1.m1.1.1.1.1.2.2">10</cn><cn id="S5.T5.1.1.1.1.1.m1.1.1.1.1.2.3.cmml" type="integer" xref="S5.T5.1.1.1.1.1.m1.1.1.1.1.2.3">2</cn></apply><apply id="S5.T5.1.1.1.1.1.m1.1.1.1.1.3.cmml" xref="S5.T5.1.1.1.1.1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.T5.1.1.1.1.1.m1.1.1.1.1.3.1.cmml" xref="S5.T5.1.1.1.1.1.m1.1.1.1.1.3">superscript</csymbol><ci id="S5.T5.1.1.1.1.1.m1.1.1.1.1.3.2.cmml" xref="S5.T5.1.1.1.1.1.m1.1.1.1.1.3.2">𝐦</ci><cn id="S5.T5.1.1.1.1.1.m1.1.1.1.1.3.3.cmml" type="integer" xref="S5.T5.1.1.1.1.1.m1.1.1.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.1.1.1.1.1.m1.1c">\mathbf{(10^{2}/m^{3})}</annotation><annotation encoding="application/x-llamapun" id="S5.T5.1.1.1.1.1.m1.1d">( bold_10 start_POSTSUPERSCRIPT bold_2 end_POSTSUPERSCRIPT / bold_m start_POSTSUPERSCRIPT bold_3 end_POSTSUPERSCRIPT )</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.1.1.1.6"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.6.1">Wrist Pos Err (cm)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.1.1.1.7"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.7.1">Elbow Pos Err (cm)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.1.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T5.1.1.2.1.1">Arm motion</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2.1.2">16.60 (5.41)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2.1.3">7.61 (3.36)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2.1.4">9.70 (2.52)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2.1.5">1.65 (2.61)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2.1.6">14.81 (6.88)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2.1.7">11.71 (5.66)</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T5.1.1.3.2.1">Upper body</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.3.2.2">15.73 (5.74)</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.3.2.3">7.68 (3.12)</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.3.2.4">10.44 (3.10)</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.3.2.5">1.43 (1.76)</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.3.2.6">13.56 (5.73)</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.3.2.7">11.11 (5.30)</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T5.1.1.4.3.1">Translation</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.4.3.2">13.57 (4.21)</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.4.3.3">5.98 (2.77)</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.4.3.4">8.18 (3.16)</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.4.3.5">2.83 (3.55)</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.4.3.6">10.83 (5.23)</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.4.3.7">9.91 (4.64)</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T5.1.1.5.4.1">Rotation</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.5.4.2">13.51 (4.87)</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.5.4.3">6.61 (2.64)</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.5.4.4">7.56 (2.20)</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.5.4.5">2.81 (3.91)</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.5.4.6">12.97 (5.45)</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.5.4.7">11.74 (4.81)</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.1.6.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="S5.T5.1.1.6.5.1">Combined</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T5.1.1.6.5.2">14.30 (5.31)</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T5.1.1.6.5.3">6.74 (2.98)</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T5.1.1.6.5.4">8.10 (2.73)</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T5.1.1.6.5.5">3.21 (4.33)</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T5.1.1.6.5.6">11.91 (5.60)</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T5.1.1.6.5.7">10.56 (4.90)</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T5.3.1.1" style="font-size:90%;">Table 5</span>. </span><span class="ltx_text" id="S5.T5.4.2" style="font-size:90%;">Results for upper body pose estimation across different motion types.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S5.F10">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="161" id="S5.F10.g1" src="extracted/5851880/camera-ready/motion_types.png" width="598"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F10.2.1.1" style="font-size:90%;">Figure 10</span>. </span><span class="ltx_text" id="S5.F10.3.2" style="font-size:90%;">Sample estimated poses of our model categorized by its types. The rendered pose is colored based on the mesh error.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S5.F10.4">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S5.F10.5">This figure showcases sample pose estimation results from WheelPoser across different categories of motions. It includes five types of motions: arm motion, upper body motion, translation, rotation, and combined motions. The arm motion segment illustrates a sequence where a person raises their right arm. The upper body segment depicts a person leaning to their left. The translation segment demonstrates a sequence of pushing forward. The rotation segment shows a person turning in place on a wheelchair. Finally, the combined motion segment captures a person performing a turn while moving forward.</p>
</div>
</div>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5. </span>Live Demo</h3>
<div class="ltx_para" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1">We further implement a real-time live pose estimation system using 4 Movella DOTs and Unity. Our system runs in real-time at 60 fps on a Dell G16 Laptop with an Intel Core i7 12700H CPU and an NVIDIA 3060 mobile GPU. From the live demos, we can see that our system achieves high accuracy and real-time pose estimation for various motion types (refer to Video Figure). It is able to effectively handle diverse lighting conditions, visual occlusions, as well as different environmental contexts. It also demonstrates promising potential to generalize to different wheelchair models. We note that our frame rate is bottlenecked by the frame rate (60 fps) of Movella DOTs. Processing a window of 26 IMU frames in real-time (details in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S5" title="5. Evaluation ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_tag">5</span></a>) takes 10.57 ms on our hardware, meaning WheelPoser could potentially run live at over 90 fps with faster IMUs.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Example Applications</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We envision WheelPoser to enable a wide range of applications across various fields by making IMU-based pose estimation inclusive and practical for everyday uses. The following application concepts highlight how WheelPoser can be used by both full-time and novice wheelchair users as well as other relevant stakeholders in the future.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Facilitating Wheelchair Skill Training</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">The World Health Organization (WHO) Guidelines on the Provision of Wheelchairs <cite class="ltx_cite ltx_citemacro_citep">(Organization et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib77" title="">2023</a>)</cite> necessitates the importance of receiving wheelchair skill training for wheelchair users. However, the manner in which people receive their wheelchairs varies widely and many wheelchair users report having limited wheelchair skill training and education resources around <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib54" title="">2023</a>; Kirby, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib45" title="">2016</a>)</cite>. Applications that aim to facilitate wheelchair skill training in both clinical and community contexts can be built on top of our pose estimation system.
For instance, the propulsion patterns used by wheelchair users when self-propelling are critical parameters that can describe the quality of their wheelchair propulsion techniques and significantly influence the likelihood of experiencing an upper extremity injury <cite class="ltx_cite ltx_citemacro_citep">(of America Consortium for Spinal Cord Medicine et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib75" title="">2005</a>)</cite>. Despite the limited skill training resources, clinic-based training further suffers from the cognizant effect, and existing systems can only classify basic wheelchair propulsion patterns, such as semi-circular and half-moon patterns <cite class="ltx_cite ltx_citemacro_citep">(Chen and Morgan, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib13" title="">2018</a>; French et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib22" title="">2008</a>; Herrera et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib29" title="">2018</a>)</cite>. Our practical pose estimation system could track more fine-grained propulsion trajectories in wheelchair users’ everyday lives, allowing for more personalized training guidance to facilitate the adoption of efficient propulsion techniques. Similarly, wheelchair users’ poses during wheelchair transfers can be tracked and analyzed for safe transfer technique training. Further, alerts based on best practices for recognized pressure relief frequency and timing can be offered.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>Inclusive Personal Informatics</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">Personal Informatics (PI) systems aim to provide users with actionable, data-driven insights from personally relevant information, thereby enhancing their quality of life <cite class="ltx_cite ltx_citemacro_citep">(Kersten-van Dijk et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib44" title="">2017</a>)</cite>.
Although these systems have been extensively researched across diverse populations and contexts, individuals with motor impairments have received significantly less attention, leaving many of their crucial needs unmet <cite class="ltx_cite ltx_citemacro_citep">(Motahar and Wiese, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib68" title="">2022</a>)</cite>. Specifically, for wheelchair users, previous research indicates that they can greatly benefit from monitoring specific activities like pressure relief and wheelchair usage as well as general upper body motions <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib54" title="">2023</a>)</cite>. In this case, tracking a wheelchair user’s pose could serve as an intrinsic indicator of human activity and be particularly informative for tracking a wide range of activities, in contrast to previous work on single-purpose systems <cite class="ltx_cite ltx_citemacro_citep">(Barbareschi and Holloway, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib3" title="">2020</a>; Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib59" title="">2017</a>)</cite>. For instance, by analyzing the pose of a wheelchair user, we can detect the occurrence and type of pressure relief actions, which are critical for preventing pressure ulcers <cite class="ltx_cite ltx_citemacro_citep">(Motahar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib67" title="">2022</a>)</cite>. Additionally, monitoring the elbow’s position over time can provide valuable data for assessing a wheelchair user’s range of motion and supporting wheelchair users’ upper extremity health management <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib54" title="">2023</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3. </span>Novel Interaction and Gaming Experiences</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">Practical pose tracking unlocks exciting new possibilities for novel interaction and gaming experiences for wheelchair users. For instance, by enabling accurate pose estimation without using cameras, WheelPoser addresses privacy concerns while facilitating the implementation of diverse input methods, such as on-body, in-air, and on-wheelchair gestures as explored by Bilius et al. <cite class="ltx_cite ltx_citemacro_citep">(Bilius et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib6" title="">2023</a>)</cite>. Additionally, it broadens the design space for developing new, intuitive body-based interaction techniques, allowing for the creation of novel interaction and locomotion techniques in VR/AR environments using natural poses or physical wheelchair locomotion, reducing the need for inaccessible controllers <cite class="ltx_cite ltx_citemacro_citep">(Mott et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib69" title="">2020</a>)</cite>.
Similarly, a wheelchair user’s pose provides rich contextual information that could drive implicit interactions in applications like smart home systems or autonomous wheelchair navigation <cite class="ltx_cite ltx_citemacro_citep">(Jang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib38" title="">2022</a>)</cite>.
Furthermore, WheelPoser’s real-time and on-the-go pose estimation capabilities open up new avenues for inclusive gaming experiences, expanding the reach of both rehabilitation-focused and recreational games to more diverse settings, such as outdoor environments and mobile contexts.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4. </span>Enhanced Data-Driven Collaborative Care</h3>
<div class="ltx_para" id="S6.SS4.p1">
<p class="ltx_p" id="S6.SS4.p1.1">Expanding beyond the aforementioned application domains, we envision WheelPoser enabling a variety of applications involving other stakeholders, such as rehabilitation scientists, physical therapists, assistive technology specialists, occupational therapists, and caregivers. For instance, the pose and joint torque of wheelchair users outside of clinics hold immense potential to advance rehabilitation scientists’ understanding of the pathology of issues like upper extremity injuries <cite class="ltx_cite ltx_citemacro_citep">(Mercer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib63" title="">2006</a>)</cite>. It can also inform physical therapists about the effectiveness of dosed treatment and patient compliance <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib54" title="">2023</a>)</cite>. Furthermore, metrics like the count of overhead reaching can reflect the accessibility of certain physical environments and inform occupational therapists about environmental modifications. Additionally, the analysis of a user’s pose while in a wheelchair extends its utility to assistive technology specialists on issues like wheelchair fitting. Specifically, user pose data may pinpoint issues related to wheelchair ergonomics and therefore provide a foundation for tailoring wheelchair designs to individual user needs.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Discussion and Future Directions</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">Like any system, WheelPoser has limitations. Here, we reflect on the evaluation results of WheelPoser, discuss typical failure cases, and explore future opportunities for advancing pose estimation for wheelchair users.</p>
</div>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1. </span>Generalization</h3>
<div class="ltx_para" id="S7.SS1.p1">
<p class="ltx_p" id="S7.SS1.p1.1">As a data-driven approach, learning-based human pose estimation faces challenges with generalization. In our work, we have shown that our proposed data synthesis and pose estimation pipeline demonstrates strong generalizability as it enables significant improvements in pose estimation across different model architectures compared to SOTA methods (see Table <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S5.T3" title="Table 3 ‣ 5.1. Dataset and Metrics ‣ 5. Evaluation ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_tag">3</span></a>). Additionally, our trained model is able to generalize well to unseen data as it achieves good quantitative and qualitative results on the held-out test split containing full-time wheelchair users’ data.</p>
</div>
<figure class="ltx_table" id="S7.T6">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S7.T6.1" style="width:433.6pt;height:35.9pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-115.4pt,9.4pt) scale(0.652642575088683,0.652642575088683) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S7.T6.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S7.T6.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S7.T6.1.1.1.2" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S7.T6.1.1.1.2.1">Models</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S7.T6.1.1.1.3" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S7.T6.1.1.1.3.1">Ang Err (deg)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S7.T6.1.1.1.4" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S7.T6.1.1.1.4.1">Pos Err (cm)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S7.T6.1.1.1.5" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S7.T6.1.1.1.5.1">Mesh Err (cm)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S7.T6.1.1.1.1" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S7.T6.1.1.1.1.1">Jitter <math alttext="\mathbf{(10^{2}/m^{3})}" class="ltx_Math" display="inline" id="S7.T6.1.1.1.1.1.m1.1"><semantics id="S7.T6.1.1.1.1.1.m1.1a"><mrow id="S7.T6.1.1.1.1.1.m1.1.1.1" xref="S7.T6.1.1.1.1.1.m1.1.1.1.1.cmml"><mo id="S7.T6.1.1.1.1.1.m1.1.1.1.2" stretchy="false" xref="S7.T6.1.1.1.1.1.m1.1.1.1.1.cmml">(</mo><mrow id="S7.T6.1.1.1.1.1.m1.1.1.1.1" xref="S7.T6.1.1.1.1.1.m1.1.1.1.1.cmml"><msup id="S7.T6.1.1.1.1.1.m1.1.1.1.1.2" xref="S7.T6.1.1.1.1.1.m1.1.1.1.1.2.cmml"><mn id="S7.T6.1.1.1.1.1.m1.1.1.1.1.2.2" xref="S7.T6.1.1.1.1.1.m1.1.1.1.1.2.2.cmml">𝟏𝟎</mn><mn id="S7.T6.1.1.1.1.1.m1.1.1.1.1.2.3" xref="S7.T6.1.1.1.1.1.m1.1.1.1.1.2.3.cmml">𝟐</mn></msup><mo id="S7.T6.1.1.1.1.1.m1.1.1.1.1.1" xref="S7.T6.1.1.1.1.1.m1.1.1.1.1.1.cmml">/</mo><msup id="S7.T6.1.1.1.1.1.m1.1.1.1.1.3" xref="S7.T6.1.1.1.1.1.m1.1.1.1.1.3.cmml"><mi id="S7.T6.1.1.1.1.1.m1.1.1.1.1.3.2" xref="S7.T6.1.1.1.1.1.m1.1.1.1.1.3.2.cmml">𝐦</mi><mn id="S7.T6.1.1.1.1.1.m1.1.1.1.1.3.3" xref="S7.T6.1.1.1.1.1.m1.1.1.1.1.3.3.cmml">𝟑</mn></msup></mrow><mo id="S7.T6.1.1.1.1.1.m1.1.1.1.3" stretchy="false" xref="S7.T6.1.1.1.1.1.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.T6.1.1.1.1.1.m1.1b"><apply id="S7.T6.1.1.1.1.1.m1.1.1.1.1.cmml" xref="S7.T6.1.1.1.1.1.m1.1.1.1"><divide id="S7.T6.1.1.1.1.1.m1.1.1.1.1.1.cmml" xref="S7.T6.1.1.1.1.1.m1.1.1.1.1.1"></divide><apply id="S7.T6.1.1.1.1.1.m1.1.1.1.1.2.cmml" xref="S7.T6.1.1.1.1.1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S7.T6.1.1.1.1.1.m1.1.1.1.1.2.1.cmml" xref="S7.T6.1.1.1.1.1.m1.1.1.1.1.2">superscript</csymbol><cn id="S7.T6.1.1.1.1.1.m1.1.1.1.1.2.2.cmml" type="integer" xref="S7.T6.1.1.1.1.1.m1.1.1.1.1.2.2">10</cn><cn id="S7.T6.1.1.1.1.1.m1.1.1.1.1.2.3.cmml" type="integer" xref="S7.T6.1.1.1.1.1.m1.1.1.1.1.2.3">2</cn></apply><apply id="S7.T6.1.1.1.1.1.m1.1.1.1.1.3.cmml" xref="S7.T6.1.1.1.1.1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S7.T6.1.1.1.1.1.m1.1.1.1.1.3.1.cmml" xref="S7.T6.1.1.1.1.1.m1.1.1.1.1.3">superscript</csymbol><ci id="S7.T6.1.1.1.1.1.m1.1.1.1.1.3.2.cmml" xref="S7.T6.1.1.1.1.1.m1.1.1.1.1.3.2">𝐦</ci><cn id="S7.T6.1.1.1.1.1.m1.1.1.1.1.3.3.cmml" type="integer" xref="S7.T6.1.1.1.1.1.m1.1.1.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T6.1.1.1.1.1.m1.1c">\mathbf{(10^{2}/m^{3})}</annotation><annotation encoding="application/x-llamapun" id="S7.T6.1.1.1.1.1.m1.1d">( bold_10 start_POSTSUPERSCRIPT bold_2 end_POSTSUPERSCRIPT / bold_m start_POSTSUPERSCRIPT bold_3 end_POSTSUPERSCRIPT )</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S7.T6.1.1.1.6" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S7.T6.1.1.1.6.1">Wrist Pos Err (cm)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S7.T6.1.1.1.7" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S7.T6.1.1.1.7.1">Elbow Pos Err (cm)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S7.T6.1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T6.1.1.2.1.1" style="padding-top:1pt;padding-bottom:1pt;">Leave-one-subject-out</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T6.1.1.2.1.2" style="padding-top:1pt;padding-bottom:1pt;">14.30 (5.31)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T6.1.1.2.1.3" style="padding-top:1pt;padding-bottom:1pt;">6.74 (2.98)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T6.1.1.2.1.4" style="padding-top:1pt;padding-bottom:1pt;">8.10 (2.73)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T6.1.1.2.1.5" style="padding-top:1pt;padding-bottom:1pt;">2.71 (4.21)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T6.1.1.2.1.6" style="padding-top:1pt;padding-bottom:1pt;">12.86 (5.90)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T6.1.1.2.1.7" style="padding-top:1pt;padding-bottom:1pt;">11.19 (5.21)</td>
</tr>
<tr class="ltx_tr" id="S7.T6.1.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_b" id="S7.T6.1.1.3.2.1" style="padding-top:1pt;padding-bottom:1pt;">Without Full-time Wheelchair Users</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S7.T6.1.1.3.2.2" style="padding-top:1pt;padding-bottom:1pt;">16.15 (5.53)</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S7.T6.1.1.3.2.3" style="padding-top:1pt;padding-bottom:1pt;">7.65 (3.11)</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S7.T6.1.1.3.2.4" style="padding-top:1pt;padding-bottom:1pt;">9.57 (3.05)</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S7.T6.1.1.3.2.5" style="padding-top:1pt;padding-bottom:1pt;">2.82 (4.77)</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S7.T6.1.1.3.2.6" style="padding-top:1pt;padding-bottom:1pt;">14.89 (6.11)</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S7.T6.1.1.3.2.7" style="padding-top:1pt;padding-bottom:1pt;">12.65 (5.36)</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S7.T6.3.1.1" style="font-size:90%;">Table 6</span>. </span><span class="ltx_text" id="S7.T6.4.2" style="font-size:90%;">Comparison of model performance when fine-tuned with data from full-time wheelchair users versus exclusively with data from participants without motor impairments.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S7.F11">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="234" id="S7.F11.g1" src="extracted/5851880/camera-ready/sitting_posture_camera.png" width="479"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S7.F11.2.1.1" style="font-size:90%;">Figure 11</span>. </span><span class="ltx_text" id="S7.F11.3.2" style="font-size:90%;">Sample sitting postures of our participants.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S7.F11.4">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S7.F11.5">This figure demonstrates the sitting postures of our participants. The image on the left is of individuals without motor impairments, and the two images on the right are of full-time wheelchair users. The figure illustrates that the sitting posture of people without motor impairments slightly differs from those of full-time wheelchair users.</p>
</div>
</div>
</figure>
<div class="ltx_para" id="S7.SS1.p2">
<p class="ltx_p" id="S7.SS1.p2.1">We further assess the generalizability of our dataset by fine-tuning a three-stage biRNN based model exclusively on data from participants without motor impairments and evaluating its performance on the data from full-time wheelchair users. In Table <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S7.T6" title="Table 6 ‣ 7.1. Generalization ‣ 7. Discussion and Future Directions ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_tag">6</span></a>, we observe a slight performance drop, with error metrics increasing by about 11% on average. Still, this data significantly improves pose estimation performance compared to existing SOTA models trained and fine-tuned solely on ambulatory motion data. This result is promising given the prevalence of simulated data to overcome difficulties in recruiting disabled users to help narrow performance gaps. However, we acknowledge and emphasize the importance of collecting data from <span class="ltx_text ltx_font_italic" id="S7.SS1.p2.1.1">real</span> members of intended user groups.</p>
</div>
<div class="ltx_para" id="S7.SS1.p3">
<p class="ltx_p" id="S7.SS1.p3.1">Careful examination of the ground truth poses reveals a possible gap between our participants without motor impairments and full-time wheelchair users. Specifically, many wheelchair users lack trunk control due to their physical impairments, and their default sitting posture may exhibit different characteristics than those of people without motor impairments, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S7.F11" title="Figure 11 ‣ 7.1. Generalization ‣ 7. Discussion and Future Directions ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_tag">11</span></a>. This difference may have led to increased error in pose estimation, especially in body mesh error.
Additionally, based on our observation, full-time wheelchair users generally demonstrated smoother wheelchair locomotion (e.g. higher stroke efficiency), likely due to extensive usage experience.
More broadly, we acknowledge that each wheelchair user is characterized by their own body shape, posture, flexibility, mobility differences, and wheelchair types, all of which are likely to influence their motion patterns and subsequently, the IMU signals. To tackle this problem, we attempted to synthesize pose and motion data from relevant online videos (e.g., wheelchair skill training videos) using state-of-the-art (SOTA) models for human pose estimation from monocular videos—4D Human <cite class="ltx_cite ltx_citemacro_citep">(Goel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib26" title="">2023b</a>)</cite>. However, we obtained poor performance in pose reconstruction wheelchair users, and the generated data exhibited poor accuracy and was temporally inconsistent, ultimately proving unhelpful in improving the performance of our model.</p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2. </span>Failure Cases</h3>
<figure class="ltx_figure" id="S7.F12">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="273" id="S7.F12.g1" src="extracted/5851880/camera-ready/failure_cases.png" width="479"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S7.F12.2.1.1" style="font-size:90%;">Figure 12</span>. </span><span class="ltx_text" id="S7.F12.3.2" style="font-size:90%;">Typical failure cases of our model.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S7.F12.4">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S7.F12.5">This figure highlights certain pose estimation results from our system that are underperforming. The top row displays a leaning forward motion, while the bottom row depicts arm crossing motions. The predicted poses show less accurate torso and elbow angles compared to the ground truth.</p>
</div>
</div>
</figure>
<div class="ltx_para" id="S7.SS2.p1">
<p class="ltx_p" id="S7.SS2.p1.1">Our system relies on IMUs on the user’s head and forearms to predict the user’s pose when their wheelchair remains stationary. Additionally, our model utilizes IMU acceleration to address the motion ambiguity introduced by our sparse-IMU sensor setup. Taken together, these factors result in challenging cases when the user slowly moves their upper body while both of their hands remain stable, such as the leaning forward motion illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S7.F12" title="Figure 12 ‣ 7.2. Failure Cases ‣ 7. Discussion and Future Directions ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_tag">12</span></a> (top row). Similarly, precise positioning of the user’s arm can be difficult as it heavily relies on the acceleration data and our model does not explicitly account for different bone lengths among users, influencing the acceleration profile. Consequently, our model underperforms in motions that involve close coordination of both arms and may result in mesh penetration, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S7.F12" title="Figure 12 ‣ 7.2. Failure Cases ‣ 7. Discussion and Future Directions ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_tag">12</span></a> (bottom row).</p>
</div>
<div class="ltx_para" id="S7.SS2.p2">
<p class="ltx_p" id="S7.SS2.p2.1">More broadly, an inertial sensor relies on its magnetometer to facilitate orientation measurement, which can be affected by the magnetic field of the environment. As we are attaching one IMU to the axle of the wheelchair, the material used to manufacture the wheelchair can potentially affect the IMU measurement and, therefore, lead to errors. Additional errors may also be introduced if the user has a relative position change from their wheelchair, as we have no direct way of measuring the user’s pelvis position.</p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3. </span>Facilitating Marker-based Motion Capture</h3>
<div class="ltx_para" id="S7.SS3.p1">
<p class="ltx_p" id="S7.SS3.p1.1">In our study, we employed a commercial marker-based motion capture system to collect human motion ground truth. This approach required participants to visit our motion capture studio in person, which imposed logistical challenges to both the participants and the research team. Given the limited accuracy of existing RGB camera-based solutions for tracking wheelchair users’ poses, commercial motion capture systems are likely to remain necessary for motion ground truth collection in future research. Here, we share several lessons learned from our experience to inform similar endeavors.
First, motion capture with marker-based systems involves an extensive setup. A traditional full-body maker setup, for example, requires at least six markers on the user’s back. In this work, we attached part of the markers to the wheelchair backrest to avoid visual occlusion based on each participant’s wheelchair setup. Here, we found it beneficial to request a few photos of participants in their wheelchairs prior to data collection and plan the potential marker placements, which significantly reduced the length of the setup process. Additionally, different considerations and adjustments may be needed for power wheelchair users, as power wheelchairs tend to provide more trunk and head support, which can introduce even more visual occlusion.
Furthermore, coordinating with participants beforehand to review the motions to be collected is critical. We found this preparatory step to be essential in ensuring participant comfort and facilitating the planning of necessary adjustments. In our study, we were able to adjust the planned motions for W14 prior to data collection based on his trunk control capacity. In addition, both the marker-based motion capture systems and our system require calibration motions. In this study, we opted for a T-pose calibration, which our participants were comfortable with and able to perform. Future studies might explore more accessible calibration procedures that consider factors such as the wheelchair users’ range of motion and the type of wheelchair, among others.</p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.4. </span>Future Opportunities</h3>
<section class="ltx_subsubsection" id="S7.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.4.1. </span>Towards Even More Practical and Unobtrusive Motion Capture</h4>
<div class="ltx_para" id="S7.SS4.SSS1.p1">
<p class="ltx_p" id="S7.SS4.SSS1.p1.1">In this work, we demonstrated the feasibility of using four IMUs for accurate upper body pose estimation for wheelchair users. Compared to commercial systems that require dense-IMU setups, WheelPoser represents a crucial step toward inclusive, practical, and unobtrusive motion capture for wheelchair users. Specifically, our sensor setup effectively lowers accessibility barriers and our pose estimation exhibits significant improvement over SOTA methods. Further, because of the improved practicality and accessibility, WheelPoser enables a wide range of applications spanning diverse fields and multiple stakeholders, as discussed in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#S6" title="6. Example Applications ‣ WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users"><span class="ltx_text ltx_ref_tag">6</span></a>.
Despite being much more practical and unobtrusive than previous systems, we acknowledge that wearing and managing four IMUs can still be challenging for some users.
Future work may explore ways to further reduce the number of required IMUs or achieve flexible sensor setups that afford user customization. Additionally, our sensor setup can be implemented using IMUs from devices that people already carry daily, such as smartwatches, earbuds, and smartphones. Future work may explore developing pose estimation systems for wheelchair users utilizing these devices to achieve more unobtrusive motion capture, similar to previous work with individuals without motor impairments <cite class="ltx_cite ltx_citemacro_citep">(Mollyn et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib66" title="">2023</a>)</cite>.
Furthermore, a wheelchair offers ample opportunities for instrumentation, and previous studies have demonstrated that many wheelchair users prefer devices in the chairable form factor <cite class="ltx_cite ltx_citemacro_citep">(Carrington et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib11" title="">2014</a>)</cite>. Future research may explore ways to improve the pose estimation accuracy by incorporating sensor fusion techniques and integrating additional sensor types, such as ultra-wideband (UWB) anchors <cite class="ltx_cite ltx_citemacro_citep">(DeVrio et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib20" title="">2023</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib53" title="">2022</a>)</cite> and pressure sensors <cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib58" title="">2021</a>)</cite>, directly into the wheelchair.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S7.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.4.2. </span>Advanced Physics-based Modeling for Pose Estimation and Wheelchair Kinematics Tracking</h4>
<div class="ltx_para" id="S7.SS4.SSS2.p1">
<p class="ltx_p" id="S7.SS4.SSS2.p1.1">Another important direction for future research involves designing and integrating more sophisticated physics-based modeling into the pose estimation pipeline. In our current work, we have shown that physics-based modeling of general human motion and seating contact significantly enhances the smoothness of predicted on-wheelchair motions. Future studies could investigate tracking and modeling the complex hand-pushrim contact, which could lead to more physically plausible pose estimations. Additionally, modeling the interaction between the wheelchair and the ground may improve the accuracy of wheelchair kinematics tracking, offering advancements over current kinematics-only methods.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S7.SS4.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.4.3. </span>Towards Large-Scale Inclusive Motion Capture Datasets</h4>
<div class="ltx_para" id="S7.SS4.SSS3.p1">
<p class="ltx_p" id="S7.SS4.SSS3.p1.1">As learning-based approaches become more prevalent across problem domains, the creation of inclusive datasets becomes essential to mitigate bias in machine learning models <cite class="ltx_cite ltx_citemacro_citep">(Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib28" title="">2020</a>; Olugbade et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib76" title="">2022</a>)</cite>. Currently, most open-source motion capture datasets are collected predominantly from individuals without motor impairments, featuring typical ambulatory motions such as standing and walking. In this work, we take an initial step towards developing large-scale wheelchair motion capture datasets by contributing a dataset that includes 167 minutes of on-wheelchair motions from 14 participants.
Future work should encompass a broader spectrum of wheelchair users, including not only those who use manual wheelchairs but also power wheelchair users, as well as both novice and part-time users alongside full-time users.
We recognize that building such datasets involves significant labor and financial challenges that extend beyond the capacity of individual labs or organizations. Therefore, we advocate for sustained and large-scale collaborations and call for active participation from the wider research community to join us in these efforts.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S7.SS4.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.4.4. </span>From Wheelchairs to Other Mobility Aids</h4>
<div class="ltx_para" id="S7.SS4.SSS4.p1">
<p class="ltx_p" id="S7.SS4.SSS4.p1.1">More broadly, tracking and understanding users’ motion in relation to their mobility aids offers valuable insights and opens up a wide range of research possibilities. In this study, we demonstrated the feasibility of tracking wheelchair users’ poses using a setup specifically tailored to the characteristics of wheelchair use. Looking ahead, future research could extend to a wide range of mobility aids, such as walking canes and rollators. Each of these aids features distinct dynamics and user motion patterns, presenting unique challenges for pose estimation and data collection. Beyond the technical challenges, however, the applications of this research extend substantial benefits across both digital and physical spaces. For instance, inclusive pose estimation enables inclusive representations in digital spaces (e.g. inclusive avatars <cite class="ltx_cite ltx_citemacro_citep">(Mack et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib61" title="">2023</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08494v1#bib.bib101" title="">2023</a>)</cite>) that reflect a wider spectrum of user identities and experiences. In addition, detailed analysis of user behavior derived from pose estimation data can facilitate identifying and subsequently addressing accessibility barriers within physical environments.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8. </span>Open Source</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">To enable others to build upon our system, we make our dataset, architecture, and trained model freely available to the research community at
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/axle-lab/WheelPoser" title="">https://github.com/axle-lab/WheelPoser</a>.</p>
</div>
</section>
<section class="ltx_section" id="S9">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9. </span>Conclusion</h2>
<div class="ltx_para" id="S9.p1">
<p class="ltx_p" id="S9.p1.1">We present WheelPoser, a sparse-IMU based wheelchair user tracking system that estimates upper body pose in real-time using only four strategically placed IMUs on the user’s body and their wheelchair. We contribute a pose estimation pipeline tailored to wheelchair users while allowing us to leverage existing high-quality motion capture datasets collected from people without motor impairments. We also contribute the first-of-its-kind WheelPoser-IMU dataset, containing 167 minutes of on-wheelchair motions performed by 2 full-time wheelchair users and 12 participants without motor impairments. Our evaluations demonstrate the performance of WheelPoser in accurately predicting wheelchair user motions, significantly surpassing prior sparse-sensor based approaches. Overall, we believe WheelPoser holds immense potential to unlock a wide range of new application possibilities for wheelchair users, such as inclusive self-tracking experiences, wheelchair skill training support, and enhanced collaborative healthcare, all while using an accessible sensor setup.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
The authors would like to thank all participants for their time and effort in our study. We extend our gratitude to Justin Macey and Jessica Hodgins for their assistance in collecting the motion capture data. We also appreciate Giorgio Becherini and the Perceiving Systems group at the Max Planck Institute for Intelligent Systems for their help with processing the marker data. Finally, we would like to thank all members of the AXLE Lab at CMU, Chris Harrison, Soyong Shin, and the anonymous reviewers for their valuable suggestions and feedback.

</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Apple (2022)</span>
<span class="ltx_bibblock">
Watch Apple. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.apple.com/watch/" title="">https://www.apple.com/watch/</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barbareschi and Holloway (2020)</span>
<span class="ltx_bibblock">
Giulia Barbareschi and Catherine Holloway. 2020.

</span>
<span class="ltx_bibblock">Understanding independent wheelchair transfers. Perspectives from stakeholders.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Disability and Rehabilitation: assistive technology</em> 15, 5 (2020), 545–552.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barbareschi et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Giulia Barbareschi, Catherine Holloway, Nadia Bianchi-Berthouze, Sharon Sonenblum, Stephen Sprigle, et al<span class="ltx_text" id="bib.bib4.3.1">.</span> 2018.

</span>
<span class="ltx_bibblock">Use of a low-cost, chest-mounted accelerometer to evaluate transfer skills of wheelchair users during everyday activities: observational study.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.4.1">JMIR rehabilitation and assistive technologies</em> 5, 2 (2018), e11748.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bazarevsky et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Valentin Bazarevsky, Ivan Grishchenko, Karthik Raveendran, Tyler Zhu, Fan Zhang, and Matthias Grundmann. 2020.

</span>
<span class="ltx_bibblock">BlazePose: On-device Real-time Body Pose tracking.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2006.10204 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bilius et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Laura-Bianca Bilius, Ovidiu-Ciprian Ungurean, and Radu-Daniel Vatavu. 2023.

</span>
<span class="ltx_bibblock">Understanding wheelchair users’ preferences for on-body, in-air, and on-wheelchair gestures. In <em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>. 1–16.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Black et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Michael J Black, Priyanka Patel, Joachim Tesch, and Jinlong Yang. 2023.

</span>
<span class="ltx_bibblock">Bedlam: A synthetic dataset of bodies exhibiting detailed lifelike animated motion. In <em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 8726–8737.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Boninger et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2002)</span>
<span class="ltx_bibblock">
Michael L Boninger, Aaron L Souza, Rory A Cooper, Shirley G Fitzgerald, Alicia M Koontz, and Brian T Fay. 2002.

</span>
<span class="ltx_bibblock">Propulsion patterns and pushrim biomechanics in manual wheelchair propulsion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">Archives of physical medicine and rehabilitation</em> 83, 5 (2002), 718–723.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Briley et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Simon J Briley, Riemer JK Vegter, Vicky L Goosey-Tolfrey, and Barry S Mason. 2020.

</span>
<span class="ltx_bibblock">Scapular kinematic variability during wheelchair propulsion is associated with shoulder pain in wheelchair users.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">Journal of biomechanics</em> 113 (2020), 110099.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Z. Cao, G. Hidalgo Martinez, T. Simon, S. Wei, and Y. A. Sheikh. 2019.

</span>
<span class="ltx_bibblock">OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carrington et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Patrick Carrington, Amy Hurst, and Shaun K Kane. 2014.

</span>
<span class="ltx_bibblock">Wearables and chairables: inclusive design of mobile input and output techniques for power wheelchair users. In <em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">Proceedings of the SIGCHI Conference on human factors in computing systems</em>. 3103–3112.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carrington et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Patrick Carrington, Gierad Laput, and Jeffrey P Bigham. 2020.

</span>
<span class="ltx_bibblock">SpokeSense: developing a real-time sensing platform for wheelchair sports.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">ACM SIGACCESS Accessibility and Computing</em> 124 (2020), 1–1.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen and Morgan (2018)</span>
<span class="ltx_bibblock">
Pin-Wei B Chen and Kerri Morgan. 2018.

</span>
<span class="ltx_bibblock">Toward community-based wheelchair evaluation with machine learning methods.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Journal of rehabilitation and assistive technologies engineering</em> 5 (2018), 2055668318808409.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choutas et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Vasileios Choutas, Philippe Weinzaepfel, Jérôme Revaud, and Cordelia Schmid. 2018.

</span>
<span class="ltx_bibblock">PoTion: Pose MoTion Representation for Action Recognition. In <em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Collinger et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2008)</span>
<span class="ltx_bibblock">
Jennifer L Collinger, Michael L Boninger, Alicia M Koontz, Robert Price, Sue Ann Sisto, Michelle L Tolerico, and Rory A Cooper. 2008.

</span>
<span class="ltx_bibblock">Shoulder biomechanics during the push phase of wheelchair propulsion: a multisite study of persons with paraplegia.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">Archives of physical medicine and rehabilitation</em> 89, 4 (2008), 667–676.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cooper (2009)</span>
<span class="ltx_bibblock">
Rory A Cooper. 2009.

</span>
<span class="ltx_bibblock">SMARTWheel: From concept to clinical practice.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Prosthetics and orthotics international</em> 33, 3 (2009), 198–209.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cotton (2020)</span>
<span class="ltx_bibblock">
R James Cotton. 2020.

</span>
<span class="ltx_bibblock">Kinematic tracking of rehabilitation patients with markerless pose estimation fused with wearable inertial sensors. In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)</em>. IEEE, 508–514.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">de Vries et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Wiebe HK de Vries, Rienk MA van der Slikke, Marit P van Dijk, and Ursina Arnet. 2023.

</span>
<span class="ltx_bibblock">Real-Life Wheelchair Mobility Metrics from IMUs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">Sensors</em> 23, 16 (2023), 7174.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Desmarais et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yann Desmarais, Denis Mottet, Pierre Slangen, and Philippe Montesinos. 2021.

</span>
<span class="ltx_bibblock">A review of 3D human pose estimation algorithms for markerless motion capture.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">Computer Vision and Image Understanding</em> 212 (2021), 103275.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DeVrio et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Nathan DeVrio, Vimal Mollyn, and Chris Harrison. 2023.

</span>
<span class="ltx_bibblock">SmartPoser: Arm Pose Estimation with a Smartphone and Smartwatch Using UWB and IMU Data. In <em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology</em>. 1–11.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Featherstone (2014)</span>
<span class="ltx_bibblock">
Roy Featherstone. 2014.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Rigid body dynamics algorithms</em>.

</span>
<span class="ltx_bibblock">Springer.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">French et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2008)</span>
<span class="ltx_bibblock">
Brian French, Asim Smailagic, Dan Siewiorek, Vishnu Ambur, and Divya Tyamagundlu. 2008.

</span>
<span class="ltx_bibblock">Classifying wheelchair propulsion patterns with a wrist mounted accelerometer. In <em class="ltx_emph ltx_font_italic" id="bib.bib22.3.1">Proceedings of the ICST 3rd international conference on Body area networks</em>. 1–4.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">García-Massó et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Xavier García-Massó, P Serra-Añó, LM Gonzalez, Yiyao Ye-Lin, Gema Prats-Boluda, and J Garcia-Casado. 2015.

</span>
<span class="ltx_bibblock">Identifying physical activity type in manual wheelchair users with spinal cord injury by means of accelerometers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">Spinal Cord</em> 53, 10 (2015), 772–777.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gerling et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Kathrin Gerling, Patrick Dickinson, Kieran Hicks, Liam Mason, Adalberto L Simeone, and Katta Spiel. 2020.

</span>
<span class="ltx_bibblock">Virtual reality games for people using wheelchairs. In <em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</em>. 1–11.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goel et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran, Angjoo Kanazawa, and Jitendra Malik. 2023a.

</span>
<span class="ltx_bibblock">Humans in 4D: Reconstructing and Tracking Humans with Transformers. In <em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goel et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran, Angjoo Kanazawa, and Jitendra Malik. 2023b.

</span>
<span class="ltx_bibblock">Humans in 4D: Reconstructing and Tracking Humans with Transformers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">arXiv preprint arXiv:2305.20091</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grillon et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Alexandre Grillon, Andres Perez-Uribe, Hector Satizabal, Laurent Gantel, David Da Silva Andrade, Andres Upegui, and Francis Degache. 2017.

</span>
<span class="ltx_bibblock">A wireless sensor-based system for self-tracking activity levels among manual wheelchair users.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">eHealth 360°</em>. Springer, 229–240.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Anhong Guo, Ece Kamar, Jennifer Wortman Vaughan, Hanna Wallach, and Meredith Ringel Morris. 2020.

</span>
<span class="ltx_bibblock">Toward fairness in AI for people with disabilities SBG@ a research roadmap.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">ACM SIGACCESS accessibility and computing</em> 125 (2020), 1–1.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Herrera et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Roxana Ramirez Herrera, Behzad Momahed Heravi, Giulia Barbareschi, Tom Carlson, and Catherine Holloway. 2018.

</span>
<span class="ltx_bibblock">Towards a Wearable Wheelchair Monitor: Classification of push style based on inertial sensors at multiple upper limb locations. In <em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)</em>. IEEE, 1535–1540.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hiremath et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Shivayogi V Hiremath, Dan Ding, and Rory A Cooper. 2013.

</span>
<span class="ltx_bibblock">Development and evaluation of a gyroscope-based wheel rotation monitor for manual wheelchair users.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">The journal of spinal cord medicine</em> 36, 4 (2013), 347–356.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hiremath et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2012)</span>
<span class="ltx_bibblock">
Shivayogi V Hiremath, Dan Ding, Jonathan Farringdon, and Rory A Cooper. 2012.

</span>
<span class="ltx_bibblock">Predicting energy expenditure of manual wheelchair users with spinal cord injury using a multisensor-based activity monitor.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">Archives of physical medicine and rehabilitation</em> 93, 11 (2012), 1937–1943.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hiremath et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Shivayogi V Hiremath, Stephen S Intille, Annmarie Kelleher, Rory A Cooper, and Dan Ding. 2015.

</span>
<span class="ltx_bibblock">Detection of physical activities using a physical activity monitor system for wheelchair users.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">Medical engineering &amp; physics</em> 37, 1 (2015), 68–76.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hooke et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2009)</span>
<span class="ltx_bibblock">
Alexander W Hooke, Melissa MB Morrow, Kai-Nan An, and Kenton R Kaufman. 2009.

</span>
<span class="ltx_bibblock">Capturing wheelchair propulsion kinematics using inertial sensors. In <em class="ltx_emph ltx_font_italic" id="bib.bib33.3.1">33rd American Society of Biomechanics conference</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
William Huang, Sam Ghahremani, Siyou Pei, and Yang Zhang. 2024.

</span>
<span class="ltx_bibblock">WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users. In <em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">Proceedings of the CHI Conference on Human Factors in Computing Systems</em>. 1–25.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Yinghao Huang, Manuel Kaufmann, Emre Aksan, Michael J Black, Otmar Hilliges, and Gerard Pons-Moll. 2018.

</span>
<span class="ltx_bibblock">Deep inertial poser: Learning to reconstruct human pose from sparse inertial measurements in real time.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">ACM Transactions on Graphics (TOG)</em> 37, 6 (2018), 1–15.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huynh (2009)</span>
<span class="ltx_bibblock">
Du Q Huynh. 2009.

</span>
<span class="ltx_bibblock">Metrics for 3D rotations: Comparison and analysis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Journal of Mathematical Imaging and Vision</em> 35 (2009), 155–164.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hwang et al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Seonhong Hwang, Chung-Ying Tsai, and Alicia M Koontz. 2017.

</span>
<span class="ltx_bibblock">Feasibility study of using a Microsoft Kinect for virtual coaching of wheelchair transfer techniques.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.3.1">Biomedical Engineering/Biomedizinische Technik</em> 62, 3 (2017), 307–313.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jang et al<span class="ltx_text" id="bib.bib38.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
JiWoong Jang, Yunzhi Li, and Patrick Carrington. 2022.

</span>
<span class="ltx_bibblock">” I Should Feel Like I’m In Control”: Understanding Expectations, Concerns, and Motivations for the Use of Autonomous Navigation on Wheelchairs. In <em class="ltx_emph ltx_font_italic" id="bib.bib38.3.1">Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility</em>. 1–5.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jarosz (1996)</span>
<span class="ltx_bibblock">
Emilia Jarosz. 1996.

</span>
<span class="ltx_bibblock">Determination of the workspace of wheelchair users.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">International Journal of Industrial Ergonomics</em> 17, 2 (1996), 123–133.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jayaraman et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Chandrasekaran Jayaraman, Yaejin Moon, Ian M Rice, Elizabeth T Hsiao Wecksler, Carolyn L Beck, and Jacob J Sosnoff. 2014.

</span>
<span class="ltx_bibblock">Shoulder pain and cycle to cycle kinematic spatial variability during recovery phase in manual wheelchair users: a pilot investigation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.3.1">PLoS One</em> 9, 3 (2014), e89794.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span class="ltx_text" id="bib.bib41.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Yifeng Jiang, Yuting Ye, Deepak Gopinath, Jungdam Won, Alexander W Winkler, and C Karen Liu. 2022.

</span>
<span class="ltx_bibblock">Transformer Inertial Poser: Real-time human motion reconstruction from sparse IMUs with simultaneous terrain generation. In <em class="ltx_emph ltx_font_italic" id="bib.bib41.3.1">SIGGRAPH Asia 2022 Conference Papers</em>. 1–9.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kanazawa et al<span class="ltx_text" id="bib.bib42.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Angjoo Kanazawa, Michael J. Black, David W. Jacobs, and Jitendra Malik. 2018.

</span>
<span class="ltx_bibblock">End-to-End Recovery of Human Shape and Pose. In <em class="ltx_emph ltx_font_italic" id="bib.bib42.3.1">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kankipati et al<span class="ltx_text" id="bib.bib43.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Padmaja Kankipati, Michael L Boninger, Dany Gagnon, Rory A Cooper, and Alicia M Koontz. 2015.

</span>
<span class="ltx_bibblock">Upper limb joint kinetics of three sitting pivot wheelchair transfer techniques in individuals with spinal cord injury.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.3.1">The journal of spinal cord medicine</em> 38, 4 (2015), 485–497.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kersten-van Dijk et al<span class="ltx_text" id="bib.bib44.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Elisabeth T Kersten-van Dijk, Joyce HDM Westerink, Femke Beute, and Wijnand A IJsselsteijn. 2017.

</span>
<span class="ltx_bibblock">Personal informatics, self-insight, and behavior change: A critical review of current literature.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.3.1">Human–Computer Interaction</em> 32, 5-6 (2017), 268–296.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kirby (2016)</span>
<span class="ltx_bibblock">
R Lee Kirby. 2016.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Wheelchair skills assessment and training</em>.

</span>
<span class="ltx_bibblock">CRC Press.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kocabas et al<span class="ltx_text" id="bib.bib46.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Muhammed Kocabas, Nikos Athanasiou, and Michael J Black. 2020.

</span>
<span class="ltx_bibblock">Vibe: Video inference for human body pose and shape estimation. In <em class="ltx_emph ltx_font_italic" id="bib.bib46.3.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 5253–5263.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kon et al<span class="ltx_text" id="bib.bib47.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Keisuke Kon, Yasuyuki Hayakawa, Shingo Shimizu, Toshiya Nosaka, Takeshi Tsuruga, Hiroyuki Matsubara, Tomohiro Nomura, Shin Murahara, Hirokazu Haruna, Takumi Ino, et al<span class="ltx_text" id="bib.bib47.3.1">.</span> 2015.

</span>
<span class="ltx_bibblock">Development of an algorithm to predict comfort of wheelchair fit based on clinical measures.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.4.1">Journal of physical therapy science</em> 27, 9 (2015), 2813–2816.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kondo et al<span class="ltx_text" id="bib.bib48.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Hikaru Kondo, Soichiro Koyama, Yohei Otaka, Nobuhiro Kumazawa, Shotaro Furuzawa, Yoshikiyo Kanada, and Shigeo Tanabe. 2024.

</span>
<span class="ltx_bibblock">Kinematic analysis of preparation for transferring from wheelchair to bed.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.3.1">Assistive Technology</em> (2024), 1–10.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koontz et al<span class="ltx_text" id="bib.bib49.2.2.1">.</span> (2002)</span>
<span class="ltx_bibblock">
Alicia M Koontz, Rory A Cooper, Michael L Boninger, Aaron L Souza, and Brian T Fay. 2002.

</span>
<span class="ltx_bibblock">Shoulder kinematics and kinetics during two speeds of wheelchair propulsion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.3.1">Journal of Rehabilitation Research &amp; Development</em> 39, 6 (2002).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kukla and Maliga (2022)</span>
<span class="ltx_bibblock">
Mateusz Kukla and Wojciech Maliga. 2022.

</span>
<span class="ltx_bibblock">Symmetry analysis of manual wheelchair propulsion using motion capture techniques.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Symmetry</em> 14, 6 (2022), 1164.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Learmonth et al<span class="ltx_text" id="bib.bib51.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Yvonne C Learmonth, Dominique Kinnett-Hopkins, Ian M Rice, JL Dysterheft, and RW Motl. 2016.

</span>
<span class="ltx_bibblock">Accelerometer output and its association with energy expenditure during manual wheelchair propulsion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.3.1">Spinal cord</em> 54, 2 (2016), 110–114.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al<span class="ltx_text" id="bib.bib52.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Amy R Lewis, Elissa J Phillips, William SP Robertson, Paul N Grimshaw, and Marc Portus. 2018.

</span>
<span class="ltx_bibblock">Injury prevention of elite wheelchair racing athletes using simulation approaches. In <em class="ltx_emph ltx_font_italic" id="bib.bib52.3.1">Proceedings</em>, Vol. 2. MDPI, 255.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib53.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Yunzhi Li, Tingyu Cheng, and Ashutosh Dhekne. 2022.

</span>
<span class="ltx_bibblock">Travelogue: Representing indoor trajectories as informative art. In <em class="ltx_emph ltx_font_italic" id="bib.bib53.3.1">CHI Conference on Human Factors in Computing Systems Extended Abstracts</em>. 1–7.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib54.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yunzhi Li, Franklin Mingzhe Li, and Patrick Carrington. 2023.

</span>
<span class="ltx_bibblock">Breaking the “Inescapable” Cycle of Pain: Supporting Wheelchair Users’ Upper Extremity Health Awareness and Management with Tracking Technologies. In <em class="ltx_emph ltx_font_italic" id="bib.bib54.3.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>. 1–17.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span class="ltx_text" id="bib.bib55.2.2.1">.</span> (2004)</span>
<span class="ltx_bibblock">
Hwai-Ting Lin, Fong-Chin Su, Hong-Wen Wu, and Kai-Nan An. 2004.

</span>
<span class="ltx_bibblock">Muscle forces analysis in the shoulder mechanism during wheelchair propulsion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.3.1">Proceedings of the Institution of Mechanical Engineers, Part H: Journal of Engineering in Medicine</em> 218, 4 (2004), 213–221.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lindquist et al<span class="ltx_text" id="bib.bib56.2.2.1">.</span> (2010)</span>
<span class="ltx_bibblock">
Noelle J Lindquist, Patricia E Loudon, Trent F Magis, Jessica E Rispin, R Lee Kirby, and Patricia J Manns. 2010.

</span>
<span class="ltx_bibblock">Reliability of the performance and safety scores of the wheelchair skills test version 4.1 for manual wheelchair users.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.3.1">Archives of physical medicine and rehabilitation</em> 91, 11 (2010), 1752–1757.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loper et al<span class="ltx_text" id="bib.bib57.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. 2023.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.3.1">SMPL: A Skinned Multi-Person Linear Model</em> (1 ed.).

</span>
<span class="ltx_bibblock">Association for Computing Machinery, New York, NY, USA.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3596711.3596800" title="">https://doi.org/10.1145/3596711.3596800</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al<span class="ltx_text" id="bib.bib58.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yiyue Luo, Yunzhu Li, Michael Foshey, Wan Shou, Pratyusha Sharma, Tomás Palacios, Antonio Torralba, and Wojciech Matusik. 2021.

</span>
<span class="ltx_bibblock">Intelligent carpet: Inferring 3d human pose from tactile signals. In <em class="ltx_emph ltx_font_italic" id="bib.bib58.3.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 11255–11265.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al<span class="ltx_text" id="bib.bib59.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Congcong Ma, Raffaele Gravina, Qimeng Li, Yu Zhang, Wenfeng Li, and Giancarlo Fortino. 2017.

</span>
<span class="ltx_bibblock">Activity recognition of wheelchair users based on sequence feature in time-series. In <em class="ltx_emph ltx_font_italic" id="bib.bib59.3.1">2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)</em>. IEEE, 3659–3664.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al<span class="ltx_text" id="bib.bib60.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Congcong Ma, Wenfeng Li, Raffaele Gravina, and Giancarlo Fortino. 2016.

</span>
<span class="ltx_bibblock">Activity recognition and monitoring for smart wheelchair users. In <em class="ltx_emph ltx_font_italic" id="bib.bib60.3.1">2016 IEEE 20th international conference on computer supported cooperative work in design (CSCWD)</em>. IEEE, 664–669.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mack et al<span class="ltx_text" id="bib.bib61.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Kelly Mack, Rai Ching Ling Hsu, Andrés Monroy-Hernández, Brian A Smith, and Fannie Liu. 2023.

</span>
<span class="ltx_bibblock">Towards inclusive avatars: Disability representation in avatar platforms. In <em class="ltx_emph ltx_font_italic" id="bib.bib61.3.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>. 1–13.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mahmood et al<span class="ltx_text" id="bib.bib62.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Gerard Pons-Moll, and Michael J Black. 2019.

</span>
<span class="ltx_bibblock">AMASS: Archive of motion capture as surface shapes. In <em class="ltx_emph ltx_font_italic" id="bib.bib62.3.1">Proceedings of the IEEE/CVF international conference on computer vision</em>. 5442–5451.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mercer et al<span class="ltx_text" id="bib.bib63.2.2.1">.</span> (2006)</span>
<span class="ltx_bibblock">
Jennifer L Mercer, Michael Boninger, Alicia Koontz, Dianxu Ren, Trevor Dyson-Hudson, and Rory Cooper. 2006.

</span>
<span class="ltx_bibblock">Shoulder joint kinetics and pathology in manual wheelchair users.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.3.1">Clinical biomechanics</em> 21, 8 (2006), 781–789.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Milgrom et al<span class="ltx_text" id="bib.bib64.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Rachel Milgrom, Matthew Foreman, John Standeven, Jack R Engsberg, and Kerri A Morgan. 2016.

</span>
<span class="ltx_bibblock">Reliability and validity of the Microsoft Kinect for assessment of manual wheelchair propulsion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.3.1">Journal of Rehabilitation Research &amp; Development</em> 53, 6 (2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MMPose Contributors (2020)</span>
<span class="ltx_bibblock">
MMPose Contributors. 2020.

</span>
<span class="ltx_bibblock">MMPose: OpenMMLab Pose Estimation Toolbox and Benchmark.

</span>
<span class="ltx_bibblock">GitHub.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/open-mmlab/mmpose" title="">https://github.com/open-mmlab/mmpose</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mollyn et al<span class="ltx_text" id="bib.bib66.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Vimal Mollyn, Riku Arakawa, Mayank Goel, Chris Harrison, and Karan Ahuja. 2023.

</span>
<span class="ltx_bibblock">IMUPoser: Full-Body Pose Estimation using IMUs in Phones, Watches, and Earbuds. In <em class="ltx_emph ltx_font_italic" id="bib.bib66.3.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>. 1–12.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Motahar et al<span class="ltx_text" id="bib.bib67.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Tamanna Motahar, Isha Ghosh, and Jason Wiese. 2022.

</span>
<span class="ltx_bibblock">Identifying factors that inhibit self-care behavior among individuals with severe spinal cord injury. In <em class="ltx_emph ltx_font_italic" id="bib.bib67.3.1">CHI Conference on Human Factors in Computing Systems</em>. 1–16.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Motahar and Wiese (2022)</span>
<span class="ltx_bibblock">
Tamanna Motahar and Jason Wiese. 2022.

</span>
<span class="ltx_bibblock">A Review of Personal Informatics Research for People with Motor Disabilities.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</em> 6, 2 (2022), 1–31.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mott et al<span class="ltx_text" id="bib.bib69.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Martez Mott, John Tang, Shaun Kane, Edward Cutrell, and Meredith Ringel Morris. 2020.

</span>
<span class="ltx_bibblock">“i just went into it assuming that i wouldn’t be able to have the full experience” understanding the accessibility of virtual reality for people with limited mobility. In <em class="ltx_emph ltx_font_italic" id="bib.bib69.3.1">Proceedings of the 22nd International ACM SIGACCESS Conference on Computers and Accessibility</em>. 1–13.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Movella.com ([n. d.])</span>
<span class="ltx_bibblock">
Movella DOT — Movella.com. [n. d.].

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.movella.com/products/wearables/movella-dot" title="">https://www.movella.com/products/wearables/movella-dot</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">(Accessed on 04/13/2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nawoczenski et al<span class="ltx_text" id="bib.bib71.2.2.1">.</span> (2003)</span>
<span class="ltx_bibblock">
Deborah A Nawoczenski, Shannon M Clobes, Stephanie L Gore, Jennifer L Neu, John E Olsen, John D Borstad, and Paula M Ludewig. 2003.

</span>
<span class="ltx_bibblock">Three-dimensional shoulder kinematics during a pressure relief technique and wheelchair transfer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.3.1">Archives of Physical Medicine and Rehabilitation</em> 84, 9 (2003), 1293–1300.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen and Kresovic (2022)</span>
<span class="ltx_bibblock">
Thong Duy Nguyen and Milan Kresovic. 2022.

</span>
<span class="ltx_bibblock">A survey of top-down approaches for human pose estimation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">arXiv preprint arXiv:2202.02656</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nightingale et al<span class="ltx_text" id="bib.bib73.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Tom Edward Nightingale, Jean-Philippe Walhin, Dylan Thompson, and James Lee John Bilzon. 2015.

</span>
<span class="ltx_bibblock">Influence of accelerometer type and placement on physical activity energy expenditure prediction in manual wheelchair users.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib73.3.1">PloS one</em> 10, 5 (2015), e0126086.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">not listed (2022)</span>
<span class="ltx_bibblock">
Authors not listed. 2022.

</span>
<span class="ltx_bibblock">Enhancing YOLO for Multi Person Pose Estimation Using Object Keypoint Similarity Loss.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2204.06806" title="">https://arxiv.org/abs/2204.06806</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed: 2024-04-21.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">of America Consortium for Spinal Cord Medicine et al<span class="ltx_text" id="bib.bib75.2.2.1">.</span> (2005)</span>
<span class="ltx_bibblock">
Paralyzed Veterans of America Consortium for Spinal Cord Medicine et al<span class="ltx_text" id="bib.bib75.3.1">.</span> 2005.

</span>
<span class="ltx_bibblock">Preservation of upper limb function following spinal cord injury: a clinical practice guideline for health-care professionals.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib75.4.1">The journal of spinal cord medicine</em> 28, 5 (2005), 434.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Olugbade et al<span class="ltx_text" id="bib.bib76.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Temitayo Olugbade, Marta Bieńkiewicz, Giulia Barbareschi, Vincenzo D’amato, Luca Oneto, Antonio Camurri, Catherine Holloway, Mårten Björkman, Peter Keller, Martin Clayton, et al<span class="ltx_text" id="bib.bib76.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Human movement datasets: An interdisciplinary scoping review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib76.4.1">Comput. Surveys</em> 55, 6 (2022), 1–29.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Organization et al<span class="ltx_text" id="bib.bib77.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
World Health Organization et al<span class="ltx_text" id="bib.bib77.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib77.4.1">Wheelchair provision guidelines</em>.

</span>
<span class="ltx_bibblock">World Health Organization.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pavlakos et al<span class="ltx_text" id="bib.bib78.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. 2019.

</span>
<span class="ltx_bibblock">Expressive Body Capture: 3D Hands, Face, and Body from a Single Image. In <em class="ltx_emph ltx_font_italic" id="bib.bib78.3.1">Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajasegaran et al<span class="ltx_text" id="bib.bib79.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jathushan Rajasegaran, Georgios Pavlakos, Angjoo Kanazawa, Christoph Feichtenhofer, and Jitendra Malik. 2023.

</span>
<span class="ltx_bibblock">On the Benefits of 3D Pose and Tracking for Human Action Recognition. In <em class="ltx_emph ltx_font_italic" id="bib.bib79.3.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>. 640–649.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rammer et al<span class="ltx_text" id="bib.bib80.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Jacob Rammer, Brooke Slavens, Joseph Krzak, Jack Winters, Susan Riedel, and Gerald Harris. 2018.

</span>
<span class="ltx_bibblock">Assessment of a markerless motion analysis system for manual wheelchair application.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib80.3.1">Journal of NeuroEngineering and Rehabilitation</em> 15 (2018), 1–12.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rhodes et al<span class="ltx_text" id="bib.bib81.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
James Rhodes, Barry Mason, Bertrand Perrat, Martin Smith, and Victoria Goosey-Tolfrey. 2014.

</span>
<span class="ltx_bibblock">The validity and reliability of a novel indoor player tracking system for use within wheelchair court sports.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib81.3.1">Journal of sports sciences</em> 32, 17 (2014), 1639–1647.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Riaz et al<span class="ltx_text" id="bib.bib82.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Qaiser Riaz, Guanhong Tao, Björn Krüger, and Andreas Weber. 2015.

</span>
<span class="ltx_bibblock">Motion reconstruction using very few accelerometers and ground contacts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib82.3.1">Graphical Models</em> 79 (2015), 23–38.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shah et al<span class="ltx_text" id="bib.bib83.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Anshul Shah, Shlok Mishra, Ankan Bansal, Jun-Cheng Chen, Rama Chellappa, and Abhinav Shrivastava. 2022.

</span>
<span class="ltx_bibblock">Pose and Joint-Aware Action Recognition. In <em class="ltx_emph ltx_font_italic" id="bib.bib83.3.1">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em>. 3850–3860.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shimada et al<span class="ltx_text" id="bib.bib84.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Soshi Shimada, Vladislav Golyanik, Weipeng Xu, and Christian Theobalt. 2020.

</span>
<span class="ltx_bibblock">Physcap: Physically plausible monocular 3d motion capture in real time.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib84.3.1">ACM Transactions on Graphics (ToG)</em> 39, 6 (2020), 1–16.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Slyper and Hodgins (2008)</span>
<span class="ltx_bibblock">
Ronit Slyper and Jessica K Hodgins. 2008.

</span>
<span class="ltx_bibblock">Action capture with accelerometers. In <em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">Proceedings of the 2008 ACM SIGGRAPH/Eurographics symposium on computer animation</em>. 193–199.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Starrs et al<span class="ltx_text" id="bib.bib86.2.2.1">.</span> (2012)</span>
<span class="ltx_bibblock">
Paul Starrs, Ambreen Chohan, David Fewtrell, Jim Richards, and James Selfe. 2012.

</span>
<span class="ltx_bibblock">Biomechanical differences between experienced and inexperienced wheelchair users during sport.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib86.3.1">Prosthetics and Orthotics International</em> 36, 3 (2012), 324–331.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib87.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yu Sun, Qian Bao, Wu Liu, Yili Fu, Black Michael J., and Tao Mei. 2021.

</span>
<span class="ltx_bibblock">Monocular, One-stage, Regression of Multiple 3D People. In <em class="ltx_emph ltx_font_italic" id="bib.bib87.3.1">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Systems ([n. d.])</span>
<span class="ltx_bibblock">
OptiTrack Motion Capture Systems. [n. d.].

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://optitrack.com/" title="">https://optitrack.com/</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">(Accessed on 04/18/2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Systems (2023)</span>
<span class="ltx_bibblock">
Vicon — Award Winning Motion Capture Systems. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.vicon.com/" title="">https://www.vicon.com/</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tautges et al<span class="ltx_text" id="bib.bib90.2.2.1">.</span> (2011)</span>
<span class="ltx_bibblock">
Jochen Tautges, Arno Zinke, Björn Krüger, Jan Baumann, Andreas Weber, Thomas Helten, Meinard Müller, Hans-Peter Seidel, and Bernd Eberhardt. 2011.

</span>
<span class="ltx_bibblock">Motion reconstruction using sparse accelerometer data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib90.3.1">ACM Transactions on Graphics (ToG)</em> 30, 3 (2011), 1–12.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Technology (2023)</span>
<span class="ltx_bibblock">
Wheelchair Physiology — Garmin Technology. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.garmin.com/en-US/garmin-technology/health-science/wheelchair-physiology/" title="">https://www.garmin.com/en-US/garmin-technology/health-science/wheelchair-physiology/</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trumble et al<span class="ltx_text" id="bib.bib92.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Matthew Trumble, Andrew Gilbert, Charles Malleson, Adrian Hilton, and John Collomosse. 2017.

</span>
<span class="ltx_bibblock">Total capture: 3d human pose estimation fusing video and inertial sensors. In <em class="ltx_emph ltx_font_italic" id="bib.bib92.3.1">Proceedings of 28th British Machine Vision Conference</em>. 1–13.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tsai et al<span class="ltx_text" id="bib.bib93.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Chung-Ying Tsai, Michael L Boninger, Sarah R Bass, and Alicia M Koontz. 2018.

</span>
<span class="ltx_bibblock">Upper-limb biomechanical analysis of wheelchair transfer techniques in two toilet configurations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib93.3.1">Clinical Biomechanics</em> 55 (2018), 79–85.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van der Slikke et al<span class="ltx_text" id="bib.bib94.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
RMA Van der Slikke, MAM Berger, DJJ Bregman, AH Lagerberg, and HEJ Veeger. 2015.

</span>
<span class="ltx_bibblock">Opportunities for measuring wheelchair kinematics in match settings; reliability of a three inertial sensor configuration.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib94.3.1">Journal of Biomechanics</em> 48, 12 (2015), 3398–3405.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Von Marcard et al<span class="ltx_text" id="bib.bib95.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Timo Von Marcard, Bodo Rosenhahn, Michael J Black, and Gerard Pons-Moll. 2017.

</span>
<span class="ltx_bibblock">Sparse inertial poser: Automatic 3d human pose estimation from sparse imus. In <em class="ltx_emph ltx_font_italic" id="bib.bib95.3.1">Computer Graphics Forum</em>, Vol. 36. Wiley Online Library, 349–360.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span class="ltx_text" id="bib.bib96.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Lin Wei, Cheng-Shiu Chung, and Alicia M Koontz. 2021.

</span>
<span class="ltx_bibblock">Automating the Clinical Assessment of Independent Wheelchair Sitting Pivot Transfer Techniques.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib96.3.1">Topics in Spinal Cord Injury Rehabilitation</em> 27, 3 (2021), 1–11.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span class="ltx_text" id="bib.bib97.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Lin Wei, W Ka Hyun, Chung-Ting Tsai, and Alicia M Koontz. 2016.

</span>
<span class="ltx_bibblock">Can the Kinect detect differences between proper and improper wheelchair transfer techniques.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib97.3.1">Rehabilitation Engineering and Assitive Technology Society of North America: Washington, DC, USA</em> (2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xsens ([n. d.])</span>
<span class="ltx_bibblock">
Xsens. [n. d.].

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.xsens.com/" title="">https://www.xsens.com/</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yi et al<span class="ltx_text" id="bib.bib99.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Xinyu Yi, Yuxiao Zhou, Marc Habermann, Soshi Shimada, Vladislav Golyanik, Christian Theobalt, and Feng Xu. 2022.

</span>
<span class="ltx_bibblock">Physical inertial poser (pip): Physics-aware real-time human motion tracking from sparse inertial sensors. In <em class="ltx_emph ltx_font_italic" id="bib.bib99.3.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 13167–13178.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yi et al<span class="ltx_text" id="bib.bib100.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Xinyu Yi, Yuxiao Zhou, and Feng Xu. 2021.

</span>
<span class="ltx_bibblock">TransPose: real-time 3D human translation and pose estimation with six inertial sensors.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib100.3.1">ACM Transactions on Graphics (TOG)</em> 40, 4 (2021), 1–13.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib101.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Kexin Zhang, Elmira Deldari, Yaxing Yao, and Yuhang Zhao. 2023.

</span>
<span class="ltx_bibblock">A Diary Study in Social Virtual Reality: Impact of Avatars with Disability Signifiers on the Social Experiences of People with Disabilities. In <em class="ltx_emph ltx_font_italic" id="bib.bib101.3.1">Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility</em>. 1–17.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib102.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. 2019.

</span>
<span class="ltx_bibblock">On the continuity of rotation representations in neural networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib102.3.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 5745–5753.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Sep 13 02:25:38 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
