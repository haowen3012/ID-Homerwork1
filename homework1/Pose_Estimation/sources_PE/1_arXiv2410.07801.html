<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation</title>
<!--Generated on Thu Oct 10 10:34:00 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.07801v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S1" title="In Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S2" title="In Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Framework overview</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S2.SS1" title="In II Framework overview ‣ Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">System architecture</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S2.SS2" title="In II Framework overview ‣ Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Visual Perception Module</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S2.SS2.SSS1" title="In II-B Visual Perception Module ‣ II Framework overview ‣ Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span>1 </span>6D Pose Estimation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S2.SS2.SSS2" title="In II-B Visual Perception Module ‣ II Framework overview ‣ Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span>2 </span>Predicting the liquid shape and vessel neck</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S2.SS3" title="In II Framework overview ‣ Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Decision-Making and Execution Module</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S2.SS3.SSS1" title="In II-C Decision-Making and Execution Module ‣ II Framework overview ‣ Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span>1 </span>Simulated Environment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S2.SS3.SSS2" title="In II-C Decision-Making and Execution Module ‣ II Framework overview ‣ Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span>2 </span>Trajectory Planning Submodule</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S2.SS3.SSS3" title="In II-C Decision-Making and Execution Module ‣ II Framework overview ‣ Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span>3 </span>Trajectory Transfer Submodule</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S2.SS4" title="In II Framework overview ‣ Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-D</span> </span><span class="ltx_text ltx_font_italic">Human assistance module (optional)</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S3" title="In Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Experiments</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S3.SS1" title="In III Experiments ‣ Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Defining work area for 6D-pose estimation</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S3.SS1.SSS1" title="In III-A Defining work area for 6D-pose estimation ‣ III Experiments ‣ Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span>1 </span>Changing camera height at a fixed distance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S3.SS1.SSS2" title="In III-A Defining work area for 6D-pose estimation ‣ III Experiments ‣ Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span>2 </span>Changing camera distance with a fixed height</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S3.SS2" title="In III Experiments ‣ Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">6D-pose estimation for complex object combinations</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S3.SS3" title="In III Experiments ‣ Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Demonstration of autonomous manipulation in an occluded environment</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S3.SS4" title="In III Experiments ‣ Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span> </span><span class="ltx_text ltx_font_italic">Performing a liquid dispensing operation in a teleoperated mode</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S4" title="In Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Conclusion and Future work</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Maria Makarova, Daria Trinitatova, and Dzmitry Tsetserukou
</span><span class="ltx_author_notes">The authors are with the Intelligent Space Robotics Laboratory, Center for Digital Engineering, Skolkovo Institute of Science and Technology (Skoltech), 121205 Moscow, Russia. <span class="ltx_text ltx_font_typewriter" id="id1.1.id1">{maria.makarova2,daria.trinitatova, d.tsetserukou}@skoltech.ru </span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">Many modern robotic systems operate autonomously, however they often lack the ability to accurately analyze the environment and adapt to changing external conditions, while teleoperation systems often require special operator skills. In the field of laboratory automation, the number of automated processes is growing, however such systems are usually developed to perform specific tasks. In addition, many of the objects used in this field are transparent, making it difficult to analyze them using visual channels.
The contributions of this work include the development of a robotic framework with autonomous mode for manipulating liquid-filled objects with different degrees of transparency in complex pose combinations. The conducted experiments demonstrated the robustness of the designed visual perception system to accurately estimate object poses for autonomous manipulation, and confirmed the performance of the algorithms in dexterous operations such as liquid dispensing. The proposed robotic framework can be applied for laboratory automation, since it allows solving the problem of performing non-trivial manipulation tasks with the analysis of object poses of varying degrees of transparency and liquid levels, requiring high accuracy and repeatability.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Nowadays, the tendency to introduce modern robotic systems into various areas of human activity is undeniable. Robots have been integrated in areas such as industry to produce components, medicine to prepare drugs, perform surgery, etc. and logistics for transportation of goods and objects. In addition, the application of robots is essential in the field of research, especially in various laboratories such as chemical, medical, biological, etc.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Robotic systems can be classified into three main categories based on the type of control, namely teleoperated, shared-control and automated.
An effective teleoperation system requires an intuitive and robust control interface, as well as a stable and convenient visual feedback channel that allows the operator to quickly respond and adapt to changing conditions of the dynamic environment. Naceri et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#bib.bib1" title="">1</a>]</cite> proposed an intuitive way of control by utilizing a virtual reality (VR)-based interface with a digital twin of the robot, augmented with real-time video streaming and object point clouds from the real environment. As an alternative for remote environment reconstruction using point clouds, Wonsick et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#bib.bib2" title="">2</a>]</cite> introduced an approach to recognize objects in remote environment and render high-fidelity models into VR interface.
Recently, there is a tendency to apply teleoperation interfaces as an effective tool for collecting data on environmental states and operator control signals for subsequent training of a robotic system for autonomous actions using Imitation Learning. Thus, Gello <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#bib.bib3" title="">3</a>]</cite> and ALOHA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#bib.bib4" title="">4</a>]</cite> systems serve as examples of low-cost and intuitive solutions that exploit kinematic similarities between target robotic manipulators and control interfaces. Despite efficient application of teleoperated robotic systems in different domains, it is difficult to provide precise execution of some labor tasks that require specific operator skills. The use of Imitation Learning technologies allows automation of such processes, but requires the collection of high quality expert data using the above systems, training and rigorous validation of the resulting algorithms before implementation in the laboratory automation systems. In some cases, it is more appropriate to build a system with closed-loop robot control algorithms. However, it is required a high-precision perception of the state of the external environment and preliminary verification of actions using a digital twin in a simulated environment, as it was proposed in the current work.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">During teleoperation, an operator experiences high workload when solving complex tasks in dynamic environments. To reduce the workload and improve efficiency, shared-control systems can be built. According to Yigitbas et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#bib.bib5" title="">5</a>]</cite>, a simplified shared-control system using VR and a digital twin of the robot may be suitable for this task. Various shared-control architectures have been proposed to assist the operator during teleoperation tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#bib.bib8" title="">8</a>]</cite>. The results of these studies showed that as the complexity of the task increased, operators preferred not to interfere with the autonomous control of the robot. Considering the manipulation of such complex and often fragile objects, as in the medical or chemical industries, these studies symbolize the need to implement the autonomous system to improve the quality and speed of operations.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Autonomous robotic systems operate without human intervention, relying on sensor information to adapt in dynamic environments. This requires a detailed closed-loop sensorimotor control system, often based on visual-tactile perception.
The implementation of automation technologies in the laboratory applications is extremely useful in achieving reproducibility in scientific research. According to Kitney et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#bib.bib9" title="">9</a>]</cite>, automation of various experimental procedures will reduce human error rate, provide uniform experimental conditions, and increase the reliability of scientific results. In addition, automation allows researchers to reduce the risks associated with handling hazardous chemicals or performing complex processes. Robotic systems are able to perform tasks accurately and precisely, reducing the likelihood of accidents or exposure to chemicals. In the field of laboratory automation, several systems have been developed that are capable of performing repetitive tasks requiring high precision. For example, automated processes for solubility determination and crystallization have been proposed by Fakhruldeen et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#bib.bib10" title="">10</a>]</cite>, however the platform architecture requires clearly defined instructions from the operator. The system presented by Lunt et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#bib.bib11" title="">11</a>]</cite> focuses on automating the complex process of powder X-ray diffraction, which is a key technique in materials science and chemistry. As in the aforementioned work, the system lacks a mechanism for analyzing the environment and, as a result, lacks variability of action in dynamically changing external conditions. Nevertheless, a number of automated systems have been proposed that are able to analyze the environment using computer vision. However, these systems are mostly focused on a specific task, such as picking and placing test tubes<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#bib.bib12" title="">12</a>]</cite> or solubility screening <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#bib.bib13" title="">13</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">It should also be noted that transparent and translucent objects are often used in the field of laboratory applications, which complicates their processing using conventional computer vision algorithms. Therefore, we have developed a system for robotic manipulations capable not only of estimating the state of the environment with high accuracy, but also of planning an efficient trajectory for different types of tasks and validating it in a simulated environment with a digital twin. The main contributions of the presented work can be summarized as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">Development of an autonomous robotic system for real-time dexterous manipulation of laboratory equipment. The remote environment is analyzed by predicting 6D poses of objects with different degrees of transparency, levels of internal liquids and geometric location of the upper neck of vessels.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">The system allows performing a wide range of manipulation tasks based on the algorithmic assignment of only a few key points of the trajectory. In addition, a simulation environment with a digital twin of the robot is used to validate the calculated actions and render the recognized objects from the real environment.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Experimental verification of the accuracy of the developed visual perception system in a series of experiments and determination of the working area.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="717" id="S1.F1.g1" src="extracted/5916005/Figures/greennewlast.png" width="580"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overview of the proposed robotic framework. The outputs of the main modules are highlighted in yellow, ROS servers in orange, and the human assistance module in green.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Framework overview</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The scheme of the developed system is shown in Fig.<a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S1.F1" title="Figure 1 ‣ I Introduction ‣ Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_tag">1</span></a>. The proposed system is able to detect 6D poses of objects, including translucent and transparent ones. In addition, it can analyze the liquid level in transparent vessels and the geometric location of the neck of the vessels, as well as perform various actions with multiple objects autonomously. It should be noted that the framework allows autonomous execution of a wide variety of manipulation tasks with different configurations of object locations, from object grasping to dispensing operations. The speed and gripping force of the dispenser can also be varied. The current work focuses on the development and evaluation of an Autonomous mode of operation, however in addition an operator-assisted Teleoperation module has been developed that can be activated for an optional manual manipulation capability.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.4.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.5.2">System architecture</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The Autonomous control mode of the system includes the <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.1">Visual Perception Module</span>, the <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.2">Decision-Making and Execution Module</span>. The latter consists of the <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.3">Simulated Environment</span>, <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.4">Trajectory Generation</span> and <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.5">Trajectory Transfer Submodules</span>. The <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.6">Human Assistance Module</span> is optional for using the system in Teleoperation mode and is highlighted in green in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S1.F1" title="Figure 1 ‣ I Introduction ‣ Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_tag">1</span></a>. Both modes interact with the <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.7">Real Environment</span> in which the robotic manipulator (Universal Robots UR3) is located. The Autonomous control mode starts with the operation of the <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.8">Visual Perception Module</span>. It estimates the 6D poses of the objects, as well as the liquid level and geometric location of the vessel neck, using RGB and depth images of objects from the robot’s environment (<span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.9">Real Environment</span>) captured by the RealSense D435 camera. This information is then used to generate an object manipulation task, after which the trajectory of the robot’s digital twin is calculated using the MoveIt! <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#bib.bib14" title="">14</a>]</cite> server (<span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.10">Trajectory Planning Submodule</span>). This trajectory is transferred to the <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.11">Simulated Environment</span>, where the digital twin manipulates rendered objects whose positions and orientations have been obtained from the <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.12">Visual Perception Module</span>. The values of the robot’s joint positions, as well as commands for the two-finger gripper (Robotiq 2F-85) are translated via ROS to a real robotic manipulator (UR3) (<span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.13">Trajectory Transfer Submodule</span>). The<span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.14"> Trajectory Planning Submodule</span>, the <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.15">Trajectory Transfer Submodule</span>, and the <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.16">Simulated Environment</span> are implemented using the Unity engine and integrated into a large module called the <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.17">Decision-Making and Execution Module</span>.
An optional <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.18">Human Assistance Module</span> allows the user to manually control the robot or assist the robotic system if needed.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.4.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.5.2">Visual Perception Module</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.1">Visual Perception Module</span> receives RGB and depth images of the <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.2">Real Environment</span> using the RealSense D435 camera. The RGB image is used to predict object segmentation masks as well as liquid and vessel shapes. The depth image and segmentation masks are used to obtain the 6D poses of objects.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS2.SSS1.4.1.1">II-B</span>1 </span>6D Pose Estimation</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">Currently, a variety of different model architectures have been proposed to predict the 6D poses of objects. For example, it is possible to match 3D models to observed objects using direct regression, but this approach becomes resource-consuming as the number of instances increases <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#bib.bib15" title="">15</a>]</cite>. Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#bib.bib16" title="">16</a>]</cite> presented the DenseFusion architecture, which allows building a single model for multiple objects. However, this requires expensive re-training every time a new object instance is added to the database. Park et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#bib.bib17" title="">17</a>]</cite> proposed the LatentFusion framework, which reconstructs a latent 3D object model from a small set of reference views, and later infers the 6D pose from the input image. The proposed approach is computationally expensive since it is based on iterative optimization at inference time.</p>
</div>
<figure class="ltx_figure" id="S2.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F2.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="445" id="S2.F2.sf1.g1" src="extracted/5916005/Figures/Object_meshes.jpg" width="509"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.sf1.2.1.1" style="font-size:80%;">(a)</span> </span><span class="ltx_text" id="S2.F2.sf1.3.2" style="font-size:80%;">Object meshes.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F2.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="216" id="S2.F2.sf2.g1" src="extracted/5916005/Figures/RGB-D.jpg" width="491"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.sf2.2.1.1" style="font-size:80%;">(b)</span> </span><span class="ltx_text" id="S2.F2.sf2.3.2" style="font-size:80%;">RGB-D images of opaque and transparent target objects.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Target objects for manipulation.</figcaption>
</figure>
<figure class="ltx_figure" id="S2.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F3.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="201" id="S2.F3.sf1.g1" src="extracted/5916005/Figures/dataset.jpg" width="269"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.sf1.2.1.1" style="font-size:80%;">(a)</span> </span><span class="ltx_text" id="S2.F3.sf1.3.2" style="font-size:80%;">Dataset objects.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F3.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="203" id="S2.F3.sf2.g1" src="extracted/5916005/Figures/ris3.png" width="249"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.sf2.2.1.1" style="font-size:80%;">(b)</span> </span><span class="ltx_text" id="S2.F3.sf2.3.2" style="font-size:80%;">Object segmentation masks.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Laboratory equipment objects for manipulation.</figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.SSS1.p2">
<p class="ltx_p" id="S2.SS2.SSS1.p2.1">In the current work, 6D object pose estimation is performed from a single depth image and the object segmentation mask using the OVE6D architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#bib.bib18" title="">18</a>]</cite>, which generalizes to new objects without any re-training of model parameters. In addition, the applied model is computationally efficient and robust to occlusions in the input data.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p3">
<p class="ltx_p" id="S2.SS2.SSS1.p3.1">The target objects for manipulation are 7 different objects of laboratory equipment (test tube, pipette, glass beaker, volumetric flask, graduated cylinder, and two tube racks), many of which are transparent or translucent. 3D mesh models of each of these are also available for the mode (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S2.F2" title="Figure 2 ‣ II-B1 6D Pose Estimation ‣ II-B Visual Perception Module ‣ II Framework overview ‣ Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_tag">2</span></a> (a)). Due to the transparency of the objects, 6D pose recognition from RGB and depth images becomes a challenging task compared to the recognition of opaque objects (Fig.<a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S2.F2" title="Figure 2 ‣ II-B1 6D Pose Estimation ‣ II-B Visual Perception Module ‣ II Framework overview ‣ Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_tag">2</span></a> (b)).</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p4">
<p class="ltx_p" id="S2.SS2.SSS1.p4.1">The OVE6D architecture utilizes the viewpoints of the object obtained by its renderer. The viewpoints are computed using 3D mesh models of objects that are preloaded into the neural network. The model is able to sequentially predict the position of the object from the given viewpoint, and then add in-plane orientation regression to the desired angle. This allows the architecture to be robust and produce sufficient pose estimation results even for transparent objects. At this stage of development, no additional training of the OVE6D architecture on the custom dataset was required.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p5">
<p class="ltx_p" id="S2.SS2.SSS1.p5.1">The segmentation masks fed to OVE6D were obtained using Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#bib.bib19" title="">19</a>]</cite>, which was trained on a collected video dataset consisting of 2500 images (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S2.F3" title="Figure 3 ‣ II-B1 6D Pose Estimation ‣ II-B Visual Perception Module ‣ II Framework overview ‣ Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_tag">3</span></a> (a)). The dataset was collected using Intel RealSense D435 camera in the format of LINEMOD dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#bib.bib20" title="">20</a>]</cite> that contains RGB and depth images with segmentation masks and 3D object mesh models. It should be mentioned that ARUCO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#bib.bib21" title="">21</a>]</cite> markers are not used in the framework algorithms and were only needed during the data collection phase. An example of the output segmentation masks is shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S2.F3" title="Figure 3 ‣ II-B1 6D Pose Estimation ‣ II-B Visual Perception Module ‣ II Framework overview ‣ Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_tag">3</span></a> (b). It is planned to eliminate the need to train segmentation models in future work, for example by using Vision Language Models (VLMs).</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p6">
<p class="ltx_p" id="S2.SS2.SSS1.p6.1">To transfer the estimated poses to the <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS1.p6.1.1">Simulated Environment</span> (Unity Engine), the obtained coordinates and angles are recalculated in the camera coordinate system. The new coordinate reference point is located at the base of the robot. This data is then transferred to the <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS1.p6.1.2">Trajectory Planning Submodule</span> and the <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS1.p6.1.3">Simulated Environment</span> via TCP/IP.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS2.SSS2.4.1.1">II-B</span>2 </span>Predicting the liquid shape and vessel neck</h4>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">To operate in a fully Autonomous mode, the system requires the ability to independently create the task conditions for manipulation. The proposed system is able to recognize the shape of the liquid in transparent objects and the geometric location of the vessel neck points, which helps to calculate the robot’s trajectory for proper operation. Using the method proposed by Eppel et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#bib.bib22" title="">22</a>]</cite>, we applied a model consisting of a fully convolutional network <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#bib.bib23" title="">23</a>]</cite> with an atrous spatial pyramid pooling (ASPP) dilated convolutional decoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#bib.bib24" title="">24</a>]</cite>, a Resnet101 encoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#bib.bib25" title="">25</a>]</cite>, and three layers of skip connection and upsampling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#bib.bib26" title="">26</a>]</cite>. Prediction of the vessel, vessel content and vessel neck maps formed the final layer of the applied network. Similar to the OVE6D model, no additional training on the custom dataset was required. The examples of modeling on our target objects are shown in Fig.<a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S2.F4" title="Figure 4 ‣ II-B2 Predicting the liquid shape and vessel neck ‣ II-B Visual Perception Module ‣ II Framework overview ‣ Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure class="ltx_figure" id="S2.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F4.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="154" id="S2.F4.sf1.g1" src="extracted/5916005/Figures/Liquid_lvl_prediction1.jpg" width="580"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F4.sf1.2.1.1" style="font-size:80%;">(a)</span> </span><span class="ltx_text" id="S2.F4.sf1.3.2" style="font-size:80%;">Case 1. Transparent vessels with liquid only.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F4.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="154" id="S2.F4.sf2.g1" src="extracted/5916005/Figures/Liquid_lvl_prediction2.jpg" width="580"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F4.sf2.2.1.1" style="font-size:80%;">(b)</span> </span><span class="ltx_text" id="S2.F4.sf2.3.2" style="font-size:80%;">Case 2. Transparent vessels with liquid in occluded environment.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Examples of predicting the neck of a vessel and the shape of the liquid inside the vessel.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.4.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.5.2">Decision-Making and Execution Module</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S1.F1" title="Figure 1 ‣ I Introduction ‣ Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_tag">1</span></a>, the <span class="ltx_text ltx_font_italic" id="S2.SS3.p1.1.1">Decision-Making and Execution Module</span>, which contains all the operational logic, consists of the <span class="ltx_text ltx_font_italic" id="S2.SS3.p1.1.2">Trajectory Planning Submodule</span>, the <span class="ltx_text ltx_font_italic" id="S2.SS3.p1.1.3">Trajectory Transfer Submodule</span> and the <span class="ltx_text ltx_font_italic" id="S2.SS3.p1.1.4">Simulated Environment</span>.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS3.SSS1.4.1.1">II-C</span>1 </span>Simulated Environment</h4>
<div class="ltx_para" id="S2.SS3.SSS1.p1">
<p class="ltx_p" id="S2.SS3.SSS1.p1.1">The <span class="ltx_text ltx_font_italic" id="S2.SS3.SSS1.p1.1.1">Simulated Environment</span> is based on the Unity engine and consists of a digital twin of the robotic manipulator (UR3) and objects whose poses are updated according to information from the <span class="ltx_text ltx_font_italic" id="S2.SS3.SSS1.p1.1.2">Visual Perception Module</span>. Once the trajectory has been calculated using the <span class="ltx_text ltx_font_italic" id="S2.SS3.SSS1.p1.1.3">Trajectory Planning Submodule</span>, the digital twin of the robot executes the necessary commands in a <span class="ltx_text ltx_font_italic" id="S2.SS3.SSS1.p1.1.4">Simulated Environment</span> with rendered objects. This allows the algorithms to be validated before they are executed by the real robot.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS3.SSS2.4.1.1">II-C</span>2 </span>Trajectory Planning Submodule</h4>
<div class="ltx_para" id="S2.SS3.SSS2.p1">
<p class="ltx_p" id="S2.SS3.SSS2.p1.1">Firstly, the trajectory of the robot’s digital twin is planned based on the location of the objects and the type of task. This is implemented in the <span class="ltx_text ltx_font_italic" id="S2.SS3.SSS2.p1.1.1">Trajectory Planning Submodule</span>. After analyzing the position of the objects, only a few key points of the trajectory are required to be algorithmically calculated to plan the robot motion (<span class="ltx_text ltx_font_italic" id="S2.SS3.SSS2.p1.1.2">Task Formation</span>). For the object picking task, there are Pre-Grab, Grab, Pick, Place and PostPlace robot poses. For the more complex tasks discussed in section <a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S3.SS3" title="III-C Demonstration of autonomous manipulation in an occluded environment ‣ III Experiments ‣ Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-C</span></span></a> , robot poses are computed iteratively for each object simultaneously with the gripper commands. All these poses are translated to the MoveIt! server to plan and actuate the robot actions for the digital twin in the <span class="ltx_text ltx_font_italic" id="S2.SS3.SSS2.p1.1.3">Simulated Environment</span> (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S1.F1" title="Figure 1 ‣ I Introduction ‣ Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS3.SSS3.4.1.1">II-C</span>3 </span>Trajectory Transfer Submodule</h4>
<div class="ltx_para" id="S2.SS3.SSS3.p1">
<p class="ltx_p" id="S2.SS3.SSS3.p1.1">During the task execution by the digital twin, the robot’s joint positions as well as commands for the Robotiq gripper are translated to the real robot with the help of several ROS servers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#bib.bib27" title="">27</a>]</cite>. This is implemented in the <span class="ltx_text ltx_font_italic" id="S2.SS3.SSS3.p1.1.1">Trajectory Transfer Submodule</span>. It operates in real-time mode, which is ensured by simultaneously running ROS servers and communicating with them through separate types of ROS messages generated for each task.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS4.4.1.1">II-D</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS4.5.2">Human assistance module (optional)</span>
</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">Optionally, the robotic system can be assisted by the operator. This may be necessary, for example, in cases of inaccurate recognition of 6D poses of objects or difficulties in <span class="ltx_text ltx_font_italic" id="S2.SS4.p1.1.1">Task formation</span> in <span class="ltx_text ltx_font_italic" id="S2.SS4.p1.1.2">Trajectory Planning Submodule</span>. For this purpose, we have built an additional <span class="ltx_text ltx_font_italic" id="S2.SS4.p1.1.3">Human Assistance Module</span> in our framework, where the operator controls the robot using the Omega.7 desktop haptic device. The detailed description of the teleoperation system is presented in our previous work<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#bib.bib28" title="">28</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Experiments</span>
</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.4.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.5.2">Defining work area for 6D-pose estimation</span>
</h3>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS1.SSS1.4.1.1">III-A</span>1 </span>Changing camera height at a fixed distance</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.5">Since the pose detection errors increase with decreasing distance between the camera and the objects, the performance of the algorithm was analyzed at a close distance of 9.5 <math alttext="cm" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p1.1.m1.1"><semantics id="S3.SS1.SSS1.p1.1.m1.1a"><mrow id="S3.SS1.SSS1.p1.1.m1.1.1" xref="S3.SS1.SSS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS1.p1.1.m1.1.1.2" xref="S3.SS1.SSS1.p1.1.m1.1.1.2.cmml">c</mi><mo id="S3.SS1.SSS1.p1.1.m1.1.1.1" xref="S3.SS1.SSS1.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS1.SSS1.p1.1.m1.1.1.3" xref="S3.SS1.SSS1.p1.1.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.1.m1.1b"><apply id="S3.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1"><times id="S3.SS1.SSS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1.1"></times><ci id="S3.SS1.SSS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1.2">𝑐</ci><ci id="S3.SS1.SSS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.1.m1.1c">cm</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p1.1.m1.1d">italic_c italic_m</annotation></semantics></math> horizontally from the nearest edge of the board with objects. Three different values of camera height above the board were considered, namely 50, 45 and 40 <math alttext="cm" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p1.2.m2.1"><semantics id="S3.SS1.SSS1.p1.2.m2.1a"><mrow id="S3.SS1.SSS1.p1.2.m2.1.1" xref="S3.SS1.SSS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.SSS1.p1.2.m2.1.1.2" xref="S3.SS1.SSS1.p1.2.m2.1.1.2.cmml">c</mi><mo id="S3.SS1.SSS1.p1.2.m2.1.1.1" xref="S3.SS1.SSS1.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.SS1.SSS1.p1.2.m2.1.1.3" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.2.m2.1b"><apply id="S3.SS1.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1"><times id="S3.SS1.SSS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.1"></times><ci id="S3.SS1.SSS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.2">𝑐</ci><ci id="S3.SS1.SSS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.2.m2.1c">cm</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p1.2.m2.1d">italic_c italic_m</annotation></semantics></math>. At each height, the value of the camera angle of view along the pitch axis was varied three times between 40<sup class="ltx_sup" id="S3.SS1.SSS1.p1.5.1">∘</sup> and 65<sup class="ltx_sup" id="S3.SS1.SSS1.p1.5.2">∘</sup> in 5<sup class="ltx_sup" id="S3.SS1.SSS1.p1.5.3">∘</sup> increments. For each height, the angles were chosen so that all objects were clearly visible over the whole area.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p2">
<p class="ltx_p" id="S3.SS1.SSS1.p2.2"><span class="ltx_text ltx_font_italic" id="S3.SS1.SSS1.p2.2.1">Experimental results</span>:
For each height value, the average errors in position (<math alttext="x" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.1.m1.1"><semantics id="S3.SS1.SSS1.p2.1.m1.1a"><mi id="S3.SS1.SSS1.p2.1.m1.1.1" xref="S3.SS1.SSS1.p2.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.1.m1.1b"><ci id="S3.SS1.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.1.m1.1d">italic_x</annotation></semantics></math> and <math alttext="y" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.2.m2.1"><semantics id="S3.SS1.SSS1.p2.2.m2.1a"><mi id="S3.SS1.SSS1.p2.2.m2.1.1" xref="S3.SS1.SSS1.p2.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.2.m2.1b"><ci id="S3.SS1.SSS1.p2.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p2.2.m2.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.2.m2.1c">y</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.2.m2.1d">italic_y</annotation></semantics></math> coordinates) and rotation of each object were estimated. The results obtained in terms of recognition accuracy are presented in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S3.F5" title="Figure 5 ‣ III-A1 Changing camera height at a fixed distance ‣ III-A Defining work area for 6D-pose estimation ‣ III Experiments ‣ Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_tag">5</span></a>. The horizontal axis shows the indices of each of the seven target objects.</p>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1140" id="S3.F5.g1" src="extracted/5916005/Figures/BP_all_exp1.jpg" width="550"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Position and rotation errors of target objects during pose estimation: 1 – flask, 2 – glass beaker, 3 – graduated cylinder, 4 – pipette, 5 – test tube, 6 – 6-hole tube rack, 7 – 25-hole tube rack.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.SSS1.p3">
<p class="ltx_p" id="S3.SS1.SSS1.p3.11">We analyzed the recognition accuracy of target objects, averaged over the camera heights, using Kruskal-Wallis non-parametric test, with a chosen significance level of <math alttext="\alpha&lt;.05" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p3.1.m1.1"><semantics id="S3.SS1.SSS1.p3.1.m1.1a"><mrow id="S3.SS1.SSS1.p3.1.m1.1.1" xref="S3.SS1.SSS1.p3.1.m1.1.1.cmml"><mi id="S3.SS1.SSS1.p3.1.m1.1.1.2" xref="S3.SS1.SSS1.p3.1.m1.1.1.2.cmml">α</mi><mo id="S3.SS1.SSS1.p3.1.m1.1.1.1" xref="S3.SS1.SSS1.p3.1.m1.1.1.1.cmml">&lt;</mo><mn id="S3.SS1.SSS1.p3.1.m1.1.1.3" xref="S3.SS1.SSS1.p3.1.m1.1.1.3.cmml">.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p3.1.m1.1b"><apply id="S3.SS1.SSS1.p3.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p3.1.m1.1.1"><lt id="S3.SS1.SSS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.SSS1.p3.1.m1.1.1.1"></lt><ci id="S3.SS1.SSS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.SSS1.p3.1.m1.1.1.2">𝛼</ci><cn id="S3.SS1.SSS1.p3.1.m1.1.1.3.cmml" type="float" xref="S3.SS1.SSS1.p3.1.m1.1.1.3">.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p3.1.m1.1c">\alpha&lt;.05</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p3.1.m1.1d">italic_α &lt; .05</annotation></semantics></math>. According to the test results, there is a statistically significant difference in position recognition accuracy along the <math alttext="Y" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p3.2.m2.1"><semantics id="S3.SS1.SSS1.p3.2.m2.1a"><mi id="S3.SS1.SSS1.p3.2.m2.1.1" xref="S3.SS1.SSS1.p3.2.m2.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p3.2.m2.1b"><ci id="S3.SS1.SSS1.p3.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p3.2.m2.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p3.2.m2.1c">Y</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p3.2.m2.1d">italic_Y</annotation></semantics></math>-axis <math alttext="(H=28.9,p&lt;.001)" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p3.3.m3.1"><semantics id="S3.SS1.SSS1.p3.3.m3.1a"><mrow id="S3.SS1.SSS1.p3.3.m3.1.1.1"><mo id="S3.SS1.SSS1.p3.3.m3.1.1.1.2" stretchy="false">(</mo><mrow id="S3.SS1.SSS1.p3.3.m3.1.1.1.1.2" xref="S3.SS1.SSS1.p3.3.m3.1.1.1.1.3.cmml"><mrow id="S3.SS1.SSS1.p3.3.m3.1.1.1.1.1.1" xref="S3.SS1.SSS1.p3.3.m3.1.1.1.1.1.1.cmml"><mi id="S3.SS1.SSS1.p3.3.m3.1.1.1.1.1.1.2" xref="S3.SS1.SSS1.p3.3.m3.1.1.1.1.1.1.2.cmml">H</mi><mo id="S3.SS1.SSS1.p3.3.m3.1.1.1.1.1.1.1" xref="S3.SS1.SSS1.p3.3.m3.1.1.1.1.1.1.1.cmml">=</mo><mn id="S3.SS1.SSS1.p3.3.m3.1.1.1.1.1.1.3" xref="S3.SS1.SSS1.p3.3.m3.1.1.1.1.1.1.3.cmml">28.9</mn></mrow><mo id="S3.SS1.SSS1.p3.3.m3.1.1.1.1.2.3" xref="S3.SS1.SSS1.p3.3.m3.1.1.1.1.3a.cmml">,</mo><mrow id="S3.SS1.SSS1.p3.3.m3.1.1.1.1.2.2" xref="S3.SS1.SSS1.p3.3.m3.1.1.1.1.2.2.cmml"><mi id="S3.SS1.SSS1.p3.3.m3.1.1.1.1.2.2.2" xref="S3.SS1.SSS1.p3.3.m3.1.1.1.1.2.2.2.cmml">p</mi><mo id="S3.SS1.SSS1.p3.3.m3.1.1.1.1.2.2.1" xref="S3.SS1.SSS1.p3.3.m3.1.1.1.1.2.2.1.cmml">&lt;</mo><mn id="S3.SS1.SSS1.p3.3.m3.1.1.1.1.2.2.3" xref="S3.SS1.SSS1.p3.3.m3.1.1.1.1.2.2.3.cmml">.001</mn></mrow></mrow><mo id="S3.SS1.SSS1.p3.3.m3.1.1.1.3" stretchy="false">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p3.3.m3.1b"><apply id="S3.SS1.SSS1.p3.3.m3.1.1.1.1.3.cmml" xref="S3.SS1.SSS1.p3.3.m3.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p3.3.m3.1.1.1.1.3a.cmml" xref="S3.SS1.SSS1.p3.3.m3.1.1.1.1.2.3">formulae-sequence</csymbol><apply id="S3.SS1.SSS1.p3.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS1.p3.3.m3.1.1.1.1.1.1"><eq id="S3.SS1.SSS1.p3.3.m3.1.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS1.p3.3.m3.1.1.1.1.1.1.1"></eq><ci id="S3.SS1.SSS1.p3.3.m3.1.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS1.p3.3.m3.1.1.1.1.1.1.2">𝐻</ci><cn id="S3.SS1.SSS1.p3.3.m3.1.1.1.1.1.1.3.cmml" type="float" xref="S3.SS1.SSS1.p3.3.m3.1.1.1.1.1.1.3">28.9</cn></apply><apply id="S3.SS1.SSS1.p3.3.m3.1.1.1.1.2.2.cmml" xref="S3.SS1.SSS1.p3.3.m3.1.1.1.1.2.2"><lt id="S3.SS1.SSS1.p3.3.m3.1.1.1.1.2.2.1.cmml" xref="S3.SS1.SSS1.p3.3.m3.1.1.1.1.2.2.1"></lt><ci id="S3.SS1.SSS1.p3.3.m3.1.1.1.1.2.2.2.cmml" xref="S3.SS1.SSS1.p3.3.m3.1.1.1.1.2.2.2">𝑝</ci><cn id="S3.SS1.SSS1.p3.3.m3.1.1.1.1.2.2.3.cmml" type="float" xref="S3.SS1.SSS1.p3.3.m3.1.1.1.1.2.2.3">.001</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p3.3.m3.1c">(H=28.9,p&lt;.001)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p3.3.m3.1d">( italic_H = 28.9 , italic_p &lt; .001 )</annotation></semantics></math> and <math alttext="X" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p3.4.m4.1"><semantics id="S3.SS1.SSS1.p3.4.m4.1a"><mi id="S3.SS1.SSS1.p3.4.m4.1.1" xref="S3.SS1.SSS1.p3.4.m4.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p3.4.m4.1b"><ci id="S3.SS1.SSS1.p3.4.m4.1.1.cmml" xref="S3.SS1.SSS1.p3.4.m4.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p3.4.m4.1c">X</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p3.4.m4.1d">italic_X</annotation></semantics></math>-axis <math alttext="(H=17.3,p=.008)" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p3.5.m5.1"><semantics id="S3.SS1.SSS1.p3.5.m5.1a"><mrow id="S3.SS1.SSS1.p3.5.m5.1.1.1"><mo id="S3.SS1.SSS1.p3.5.m5.1.1.1.2" stretchy="false">(</mo><mrow id="S3.SS1.SSS1.p3.5.m5.1.1.1.1.2" xref="S3.SS1.SSS1.p3.5.m5.1.1.1.1.3.cmml"><mrow id="S3.SS1.SSS1.p3.5.m5.1.1.1.1.1.1" xref="S3.SS1.SSS1.p3.5.m5.1.1.1.1.1.1.cmml"><mi id="S3.SS1.SSS1.p3.5.m5.1.1.1.1.1.1.2" xref="S3.SS1.SSS1.p3.5.m5.1.1.1.1.1.1.2.cmml">H</mi><mo id="S3.SS1.SSS1.p3.5.m5.1.1.1.1.1.1.1" xref="S3.SS1.SSS1.p3.5.m5.1.1.1.1.1.1.1.cmml">=</mo><mn id="S3.SS1.SSS1.p3.5.m5.1.1.1.1.1.1.3" xref="S3.SS1.SSS1.p3.5.m5.1.1.1.1.1.1.3.cmml">17.3</mn></mrow><mo id="S3.SS1.SSS1.p3.5.m5.1.1.1.1.2.3" xref="S3.SS1.SSS1.p3.5.m5.1.1.1.1.3a.cmml">,</mo><mrow id="S3.SS1.SSS1.p3.5.m5.1.1.1.1.2.2" xref="S3.SS1.SSS1.p3.5.m5.1.1.1.1.2.2.cmml"><mi id="S3.SS1.SSS1.p3.5.m5.1.1.1.1.2.2.2" xref="S3.SS1.SSS1.p3.5.m5.1.1.1.1.2.2.2.cmml">p</mi><mo id="S3.SS1.SSS1.p3.5.m5.1.1.1.1.2.2.1" xref="S3.SS1.SSS1.p3.5.m5.1.1.1.1.2.2.1.cmml">=</mo><mn id="S3.SS1.SSS1.p3.5.m5.1.1.1.1.2.2.3" xref="S3.SS1.SSS1.p3.5.m5.1.1.1.1.2.2.3.cmml">.008</mn></mrow></mrow><mo id="S3.SS1.SSS1.p3.5.m5.1.1.1.3" stretchy="false">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p3.5.m5.1b"><apply id="S3.SS1.SSS1.p3.5.m5.1.1.1.1.3.cmml" xref="S3.SS1.SSS1.p3.5.m5.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p3.5.m5.1.1.1.1.3a.cmml" xref="S3.SS1.SSS1.p3.5.m5.1.1.1.1.2.3">formulae-sequence</csymbol><apply id="S3.SS1.SSS1.p3.5.m5.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS1.p3.5.m5.1.1.1.1.1.1"><eq id="S3.SS1.SSS1.p3.5.m5.1.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS1.p3.5.m5.1.1.1.1.1.1.1"></eq><ci id="S3.SS1.SSS1.p3.5.m5.1.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS1.p3.5.m5.1.1.1.1.1.1.2">𝐻</ci><cn id="S3.SS1.SSS1.p3.5.m5.1.1.1.1.1.1.3.cmml" type="float" xref="S3.SS1.SSS1.p3.5.m5.1.1.1.1.1.1.3">17.3</cn></apply><apply id="S3.SS1.SSS1.p3.5.m5.1.1.1.1.2.2.cmml" xref="S3.SS1.SSS1.p3.5.m5.1.1.1.1.2.2"><eq id="S3.SS1.SSS1.p3.5.m5.1.1.1.1.2.2.1.cmml" xref="S3.SS1.SSS1.p3.5.m5.1.1.1.1.2.2.1"></eq><ci id="S3.SS1.SSS1.p3.5.m5.1.1.1.1.2.2.2.cmml" xref="S3.SS1.SSS1.p3.5.m5.1.1.1.1.2.2.2">𝑝</ci><cn id="S3.SS1.SSS1.p3.5.m5.1.1.1.1.2.2.3.cmml" type="float" xref="S3.SS1.SSS1.p3.5.m5.1.1.1.1.2.2.3">.008</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p3.5.m5.1c">(H=17.3,p=.008)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p3.5.m5.1d">( italic_H = 17.3 , italic_p = .008 )</annotation></semantics></math> for different objects. The position recognition was most unstable for large transparent and translucent objects such as flask, glass beaker and graduated cylinder (objects 1,2, and 3 respectively). Comparing two groups of transparent and translucent objects, we found that the position recognition of smaller translucent objects such as pipette and test tube were statistically significantly better than larger ones <math alttext="(p=.001)" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p3.6.m6.1"><semantics id="S3.SS1.SSS1.p3.6.m6.1a"><mrow id="S3.SS1.SSS1.p3.6.m6.1.1.1" xref="S3.SS1.SSS1.p3.6.m6.1.1.1.1.cmml"><mo id="S3.SS1.SSS1.p3.6.m6.1.1.1.2" stretchy="false" xref="S3.SS1.SSS1.p3.6.m6.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.SSS1.p3.6.m6.1.1.1.1" xref="S3.SS1.SSS1.p3.6.m6.1.1.1.1.cmml"><mi id="S3.SS1.SSS1.p3.6.m6.1.1.1.1.2" xref="S3.SS1.SSS1.p3.6.m6.1.1.1.1.2.cmml">p</mi><mo id="S3.SS1.SSS1.p3.6.m6.1.1.1.1.1" xref="S3.SS1.SSS1.p3.6.m6.1.1.1.1.1.cmml">=</mo><mn id="S3.SS1.SSS1.p3.6.m6.1.1.1.1.3" xref="S3.SS1.SSS1.p3.6.m6.1.1.1.1.3.cmml">.001</mn></mrow><mo id="S3.SS1.SSS1.p3.6.m6.1.1.1.3" stretchy="false" xref="S3.SS1.SSS1.p3.6.m6.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p3.6.m6.1b"><apply id="S3.SS1.SSS1.p3.6.m6.1.1.1.1.cmml" xref="S3.SS1.SSS1.p3.6.m6.1.1.1"><eq id="S3.SS1.SSS1.p3.6.m6.1.1.1.1.1.cmml" xref="S3.SS1.SSS1.p3.6.m6.1.1.1.1.1"></eq><ci id="S3.SS1.SSS1.p3.6.m6.1.1.1.1.2.cmml" xref="S3.SS1.SSS1.p3.6.m6.1.1.1.1.2">𝑝</ci><cn id="S3.SS1.SSS1.p3.6.m6.1.1.1.1.3.cmml" type="float" xref="S3.SS1.SSS1.p3.6.m6.1.1.1.1.3">.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p3.6.m6.1c">(p=.001)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p3.6.m6.1d">( italic_p = .001 )</annotation></semantics></math> according to Mann-Whitney U test. This suggests that the distance to the objects should be increased for stability.
Analyzing the rotation recognition accuracy, it should be noted that error outliers for the yaw and pitch angles were observed for asymmetric objects such as tube racks (objects 6 and 7).
Overall, the minimum errors for pose estimation were obtained at the height of 40 <math alttext="cm" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p3.7.m7.1"><semantics id="S3.SS1.SSS1.p3.7.m7.1a"><mrow id="S3.SS1.SSS1.p3.7.m7.1.1" xref="S3.SS1.SSS1.p3.7.m7.1.1.cmml"><mi id="S3.SS1.SSS1.p3.7.m7.1.1.2" xref="S3.SS1.SSS1.p3.7.m7.1.1.2.cmml">c</mi><mo id="S3.SS1.SSS1.p3.7.m7.1.1.1" xref="S3.SS1.SSS1.p3.7.m7.1.1.1.cmml">⁢</mo><mi id="S3.SS1.SSS1.p3.7.m7.1.1.3" xref="S3.SS1.SSS1.p3.7.m7.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p3.7.m7.1b"><apply id="S3.SS1.SSS1.p3.7.m7.1.1.cmml" xref="S3.SS1.SSS1.p3.7.m7.1.1"><times id="S3.SS1.SSS1.p3.7.m7.1.1.1.cmml" xref="S3.SS1.SSS1.p3.7.m7.1.1.1"></times><ci id="S3.SS1.SSS1.p3.7.m7.1.1.2.cmml" xref="S3.SS1.SSS1.p3.7.m7.1.1.2">𝑐</ci><ci id="S3.SS1.SSS1.p3.7.m7.1.1.3.cmml" xref="S3.SS1.SSS1.p3.7.m7.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p3.7.m7.1c">cm</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p3.7.m7.1d">italic_c italic_m</annotation></semantics></math>. Thus, the mean position error averaged over all objects comprised 0.3 <math alttext="cm" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p3.8.m8.1"><semantics id="S3.SS1.SSS1.p3.8.m8.1a"><mrow id="S3.SS1.SSS1.p3.8.m8.1.1" xref="S3.SS1.SSS1.p3.8.m8.1.1.cmml"><mi id="S3.SS1.SSS1.p3.8.m8.1.1.2" xref="S3.SS1.SSS1.p3.8.m8.1.1.2.cmml">c</mi><mo id="S3.SS1.SSS1.p3.8.m8.1.1.1" xref="S3.SS1.SSS1.p3.8.m8.1.1.1.cmml">⁢</mo><mi id="S3.SS1.SSS1.p3.8.m8.1.1.3" xref="S3.SS1.SSS1.p3.8.m8.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p3.8.m8.1b"><apply id="S3.SS1.SSS1.p3.8.m8.1.1.cmml" xref="S3.SS1.SSS1.p3.8.m8.1.1"><times id="S3.SS1.SSS1.p3.8.m8.1.1.1.cmml" xref="S3.SS1.SSS1.p3.8.m8.1.1.1"></times><ci id="S3.SS1.SSS1.p3.8.m8.1.1.2.cmml" xref="S3.SS1.SSS1.p3.8.m8.1.1.2">𝑐</ci><ci id="S3.SS1.SSS1.p3.8.m8.1.1.3.cmml" xref="S3.SS1.SSS1.p3.8.m8.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p3.8.m8.1c">cm</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p3.8.m8.1d">italic_c italic_m</annotation></semantics></math> (SD=0.52 <math alttext="cm" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p3.9.m9.1"><semantics id="S3.SS1.SSS1.p3.9.m9.1a"><mrow id="S3.SS1.SSS1.p3.9.m9.1.1" xref="S3.SS1.SSS1.p3.9.m9.1.1.cmml"><mi id="S3.SS1.SSS1.p3.9.m9.1.1.2" xref="S3.SS1.SSS1.p3.9.m9.1.1.2.cmml">c</mi><mo id="S3.SS1.SSS1.p3.9.m9.1.1.1" xref="S3.SS1.SSS1.p3.9.m9.1.1.1.cmml">⁢</mo><mi id="S3.SS1.SSS1.p3.9.m9.1.1.3" xref="S3.SS1.SSS1.p3.9.m9.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p3.9.m9.1b"><apply id="S3.SS1.SSS1.p3.9.m9.1.1.cmml" xref="S3.SS1.SSS1.p3.9.m9.1.1"><times id="S3.SS1.SSS1.p3.9.m9.1.1.1.cmml" xref="S3.SS1.SSS1.p3.9.m9.1.1.1"></times><ci id="S3.SS1.SSS1.p3.9.m9.1.1.2.cmml" xref="S3.SS1.SSS1.p3.9.m9.1.1.2">𝑐</ci><ci id="S3.SS1.SSS1.p3.9.m9.1.1.3.cmml" xref="S3.SS1.SSS1.p3.9.m9.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p3.9.m9.1c">cm</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p3.9.m9.1d">italic_c italic_m</annotation></semantics></math>), while the mean rotation error was 0.54<sup class="ltx_sup" id="S3.SS1.SSS1.p3.11.1">∘</sup> (SD=1.6<sup class="ltx_sup" id="S3.SS1.SSS1.p3.11.2">∘</sup>).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS1.SSS2.4.1.1">III-A</span>2 </span>Changing camera distance with a fixed height</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">Having analyzed the dependence of pose estimation accuracy on the camera height and viewing angle, we additionally analyzed the algorithm performance at various camera distances from the board with objects. For this experiment, the camera mounting height was fixed. Object poses were analyzed for six distance markers, namely 9.5, 13, 24, 33, 57, 65 and 74 <math alttext="cm" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.1.m1.1"><semantics id="S3.SS1.SSS2.p1.1.m1.1a"><mrow id="S3.SS1.SSS2.p1.1.m1.1.1" xref="S3.SS1.SSS2.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS2.p1.1.m1.1.1.2" xref="S3.SS1.SSS2.p1.1.m1.1.1.2.cmml">c</mi><mo id="S3.SS1.SSS2.p1.1.m1.1.1.1" xref="S3.SS1.SSS2.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS1.SSS2.p1.1.m1.1.1.3" xref="S3.SS1.SSS2.p1.1.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.1.m1.1b"><apply id="S3.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1"><times id="S3.SS1.SSS2.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.1"></times><ci id="S3.SS1.SSS2.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.2">𝑐</ci><ci id="S3.SS1.SSS2.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.1.m1.1c">cm</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.1.m1.1d">italic_c italic_m</annotation></semantics></math>. Camera rotation angles were chosen to provide the same angle of view in each case.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p2">
<p class="ltx_p" id="S3.SS1.SSS2.p2.1"><span class="ltx_text ltx_font_italic" id="S3.SS1.SSS2.p2.1.1">Experimental results</span>:
The resulting distribution of position and rotation errors averaged over all objects is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S3.F6" title="Figure 6 ‣ III-A2 Changing camera distance with a fixed height ‣ III-A Defining work area for 6D-pose estimation ‣ III Experiments ‣ Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="443" id="S3.F6.g1" src="extracted/5916005/Figures/BP_exp1_p2.jpg" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Dependence of position and rotation errors, averaged over all objects, on the distance of the camera to the objects.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.SSS2.p3">
<p class="ltx_p" id="S3.SS1.SSS2.p3.4">The experimental results were analyzed using Kruskal-Wallis non-parametric test, with a chosen significance level of <math alttext="\alpha&lt;.05" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p3.1.m1.1"><semantics id="S3.SS1.SSS2.p3.1.m1.1a"><mrow id="S3.SS1.SSS2.p3.1.m1.1.1" xref="S3.SS1.SSS2.p3.1.m1.1.1.cmml"><mi id="S3.SS1.SSS2.p3.1.m1.1.1.2" xref="S3.SS1.SSS2.p3.1.m1.1.1.2.cmml">α</mi><mo id="S3.SS1.SSS2.p3.1.m1.1.1.1" xref="S3.SS1.SSS2.p3.1.m1.1.1.1.cmml">&lt;</mo><mn id="S3.SS1.SSS2.p3.1.m1.1.1.3" xref="S3.SS1.SSS2.p3.1.m1.1.1.3.cmml">.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p3.1.m1.1b"><apply id="S3.SS1.SSS2.p3.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.1"><lt id="S3.SS1.SSS2.p3.1.m1.1.1.1.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.1.1"></lt><ci id="S3.SS1.SSS2.p3.1.m1.1.1.2.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.1.2">𝛼</ci><cn id="S3.SS1.SSS2.p3.1.m1.1.1.3.cmml" type="float" xref="S3.SS1.SSS2.p3.1.m1.1.1.3">.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p3.1.m1.1c">\alpha&lt;.05</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p3.1.m1.1d">italic_α &lt; .05</annotation></semantics></math>, since the obtained data deviated from normal distribution. According to the test findings, there is no statistically significant difference in the pose estimation errors averaged over all objects for different camera distances. The mean absolute position and rotation errors averaged over all objects comprised 0.18 <math alttext="cm" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p3.2.m2.1"><semantics id="S3.SS1.SSS2.p3.2.m2.1a"><mrow id="S3.SS1.SSS2.p3.2.m2.1.1" xref="S3.SS1.SSS2.p3.2.m2.1.1.cmml"><mi id="S3.SS1.SSS2.p3.2.m2.1.1.2" xref="S3.SS1.SSS2.p3.2.m2.1.1.2.cmml">c</mi><mo id="S3.SS1.SSS2.p3.2.m2.1.1.1" xref="S3.SS1.SSS2.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.SS1.SSS2.p3.2.m2.1.1.3" xref="S3.SS1.SSS2.p3.2.m2.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p3.2.m2.1b"><apply id="S3.SS1.SSS2.p3.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p3.2.m2.1.1"><times id="S3.SS1.SSS2.p3.2.m2.1.1.1.cmml" xref="S3.SS1.SSS2.p3.2.m2.1.1.1"></times><ci id="S3.SS1.SSS2.p3.2.m2.1.1.2.cmml" xref="S3.SS1.SSS2.p3.2.m2.1.1.2">𝑐</ci><ci id="S3.SS1.SSS2.p3.2.m2.1.1.3.cmml" xref="S3.SS1.SSS2.p3.2.m2.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p3.2.m2.1c">cm</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p3.2.m2.1d">italic_c italic_m</annotation></semantics></math> and 0.39<sup class="ltx_sup" id="S3.SS1.SSS2.p3.4.1">∘</sup> respectively. It should be noted that at close distances, the main contribution to the error in recognition along the <math alttext="X" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p3.4.m4.1"><semantics id="S3.SS1.SSS2.p3.4.m4.1a"><mi id="S3.SS1.SSS2.p3.4.m4.1.1" xref="S3.SS1.SSS2.p3.4.m4.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p3.4.m4.1b"><ci id="S3.SS1.SSS2.p3.4.m4.1.1.cmml" xref="S3.SS1.SSS2.p3.4.m4.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p3.4.m4.1c">X</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p3.4.m4.1d">italic_X</annotation></semantics></math>-axis and recognition of the roll angle was made by a glass beaker, which is a simple cylindrical shape.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p4">
<p class="ltx_p" id="S3.SS1.SSS2.p4.2">In addition, we analyzed the average accuracy of recognition of translucent (objects 1–5) and opaque objects (objects 6–7) using the Mann-Whitney U test for pairwise comparison. According to the obtained results, there is no statistically significant difference in accuracy of recognition for position <math alttext="(p=.52)" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p4.1.m1.1"><semantics id="S3.SS1.SSS2.p4.1.m1.1a"><mrow id="S3.SS1.SSS2.p4.1.m1.1.1.1" xref="S3.SS1.SSS2.p4.1.m1.1.1.1.1.cmml"><mo id="S3.SS1.SSS2.p4.1.m1.1.1.1.2" stretchy="false" xref="S3.SS1.SSS2.p4.1.m1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.SSS2.p4.1.m1.1.1.1.1" xref="S3.SS1.SSS2.p4.1.m1.1.1.1.1.cmml"><mi id="S3.SS1.SSS2.p4.1.m1.1.1.1.1.2" xref="S3.SS1.SSS2.p4.1.m1.1.1.1.1.2.cmml">p</mi><mo id="S3.SS1.SSS2.p4.1.m1.1.1.1.1.1" xref="S3.SS1.SSS2.p4.1.m1.1.1.1.1.1.cmml">=</mo><mn id="S3.SS1.SSS2.p4.1.m1.1.1.1.1.3" xref="S3.SS1.SSS2.p4.1.m1.1.1.1.1.3.cmml">.52</mn></mrow><mo id="S3.SS1.SSS2.p4.1.m1.1.1.1.3" stretchy="false" xref="S3.SS1.SSS2.p4.1.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p4.1.m1.1b"><apply id="S3.SS1.SSS2.p4.1.m1.1.1.1.1.cmml" xref="S3.SS1.SSS2.p4.1.m1.1.1.1"><eq id="S3.SS1.SSS2.p4.1.m1.1.1.1.1.1.cmml" xref="S3.SS1.SSS2.p4.1.m1.1.1.1.1.1"></eq><ci id="S3.SS1.SSS2.p4.1.m1.1.1.1.1.2.cmml" xref="S3.SS1.SSS2.p4.1.m1.1.1.1.1.2">𝑝</ci><cn id="S3.SS1.SSS2.p4.1.m1.1.1.1.1.3.cmml" type="float" xref="S3.SS1.SSS2.p4.1.m1.1.1.1.1.3">.52</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p4.1.m1.1c">(p=.52)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p4.1.m1.1d">( italic_p = .52 )</annotation></semantics></math> and rotation <math alttext="(p=.56)" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p4.2.m2.1"><semantics id="S3.SS1.SSS2.p4.2.m2.1a"><mrow id="S3.SS1.SSS2.p4.2.m2.1.1.1" xref="S3.SS1.SSS2.p4.2.m2.1.1.1.1.cmml"><mo id="S3.SS1.SSS2.p4.2.m2.1.1.1.2" stretchy="false" xref="S3.SS1.SSS2.p4.2.m2.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.SSS2.p4.2.m2.1.1.1.1" xref="S3.SS1.SSS2.p4.2.m2.1.1.1.1.cmml"><mi id="S3.SS1.SSS2.p4.2.m2.1.1.1.1.2" xref="S3.SS1.SSS2.p4.2.m2.1.1.1.1.2.cmml">p</mi><mo id="S3.SS1.SSS2.p4.2.m2.1.1.1.1.1" xref="S3.SS1.SSS2.p4.2.m2.1.1.1.1.1.cmml">=</mo><mn id="S3.SS1.SSS2.p4.2.m2.1.1.1.1.3" xref="S3.SS1.SSS2.p4.2.m2.1.1.1.1.3.cmml">.56</mn></mrow><mo id="S3.SS1.SSS2.p4.2.m2.1.1.1.3" stretchy="false" xref="S3.SS1.SSS2.p4.2.m2.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p4.2.m2.1b"><apply id="S3.SS1.SSS2.p4.2.m2.1.1.1.1.cmml" xref="S3.SS1.SSS2.p4.2.m2.1.1.1"><eq id="S3.SS1.SSS2.p4.2.m2.1.1.1.1.1.cmml" xref="S3.SS1.SSS2.p4.2.m2.1.1.1.1.1"></eq><ci id="S3.SS1.SSS2.p4.2.m2.1.1.1.1.2.cmml" xref="S3.SS1.SSS2.p4.2.m2.1.1.1.1.2">𝑝</ci><cn id="S3.SS1.SSS2.p4.2.m2.1.1.1.1.3.cmml" type="float" xref="S3.SS1.SSS2.p4.2.m2.1.1.1.1.3">.56</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p4.2.m2.1c">(p=.56)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p4.2.m2.1d">( italic_p = .56 )</annotation></semantics></math> between these two groups of objects. Thus, we can conclude that the recognition of translucent objects was as reliable as the opaque ones.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p5">
<p class="ltx_p" id="S3.SS1.SSS2.p5.1">As a result of the experiments, the boundaries of the working area of the <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS2.p5.1.1">Visual Perception Module</span> have been clarified, and the algorithm shows the best results when the camera is located at a medium distance from the objects. When adapting the pose estimation algorithm for another system, the working area should also be determined experimentally.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.4.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.5.2">6D-pose estimation for complex object combinations</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">After defining the working range of the pose estimation algorithm, we conducted the experiment to detect poses in random complex combinations of objects.
We estimated cases such as arrangement of objects on top of each other up to four levels in height, the placement of transparent objects on a white background or vice versa on a complex background of opaque objects, as well as the placement of one transparent object inside another (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S3.F7" title="Figure 7 ‣ III-B 6D-pose estimation for complex object combinations ‣ III Experiments ‣ Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_tag">7</span></a>).</p>
</div>
<figure class="ltx_figure" id="S3.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F7.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="192" id="S3.F7.sf1.g1" src="extracted/5916005/Figures/rand.jpg" width="568"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F7.sf1.2.1.1" style="font-size:80%;">(a)</span> </span><span class="ltx_text" id="S3.F7.sf1.3.2" style="font-size:80%;">Examples of complex object poses with occlusions.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F7.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="221" id="S3.F7.sf2.g1" src="extracted/5916005/Figures/posexmp.jpg" width="568"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F7.sf2.2.1.1" style="font-size:80%;">(b)</span> </span><span class="ltx_text" id="S3.F7.sf2.3.2" style="font-size:80%;">Example of complex object poses estimation.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Complex combinations of objects used in the experiment.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.3">The algorithm successfully coped with pose detection cases when both transparent and opaque objects are partially occluded. The average accuracy results obtained are summarized in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S3.F8" title="Figure 8 ‣ III-B 6D-pose estimation for complex object combinations ‣ III Experiments ‣ Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_tag">8</span></a>. We only analyzed rotation errors, since position errors were insignificant. The main detection problems occurred when tube racks were occluded by more than 60% of the observed surface and when the bottom of the glass beaker was occluded, which, unlike a flask, has parallel walls and therefore a smaller reflective surface. The mean errors for estimation of roll, pitch, and yaw angles averaged over all objects comprised <math alttext="0.6^{\circ}(SD=1.1^{\circ}" class="ltx_math_unparsed" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1b"><msup id="S3.SS2.p2.1.m1.1.1"><mn id="S3.SS2.p2.1.m1.1.1.2">0.6</mn><mo id="S3.SS2.p2.1.m1.1.1.3">∘</mo></msup><mrow id="S3.SS2.p2.1.m1.1.2"><mo id="S3.SS2.p2.1.m1.1.2.1" stretchy="false">(</mo><mi id="S3.SS2.p2.1.m1.1.2.2">S</mi><mi id="S3.SS2.p2.1.m1.1.2.3">D</mi><mo id="S3.SS2.p2.1.m1.1.2.4">=</mo><msup id="S3.SS2.p2.1.m1.1.2.5"><mn id="S3.SS2.p2.1.m1.1.2.5.2">1.1</mn><mo id="S3.SS2.p2.1.m1.1.2.5.3">∘</mo></msup></mrow></mrow><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">0.6^{\circ}(SD=1.1^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">0.6 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT ( italic_S italic_D = 1.1 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT</annotation></semantics></math>), <math alttext="1.1^{\circ}(SD=3.6^{\circ}" class="ltx_math_unparsed" display="inline" id="S3.SS2.p2.2.m2.1"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1b"><msup id="S3.SS2.p2.2.m2.1.1"><mn id="S3.SS2.p2.2.m2.1.1.2">1.1</mn><mo id="S3.SS2.p2.2.m2.1.1.3">∘</mo></msup><mrow id="S3.SS2.p2.2.m2.1.2"><mo id="S3.SS2.p2.2.m2.1.2.1" stretchy="false">(</mo><mi id="S3.SS2.p2.2.m2.1.2.2">S</mi><mi id="S3.SS2.p2.2.m2.1.2.3">D</mi><mo id="S3.SS2.p2.2.m2.1.2.4">=</mo><msup id="S3.SS2.p2.2.m2.1.2.5"><mn id="S3.SS2.p2.2.m2.1.2.5.2">3.6</mn><mo id="S3.SS2.p2.2.m2.1.2.5.3">∘</mo></msup></mrow></mrow><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">1.1^{\circ}(SD=3.6^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m2.1d">1.1 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT ( italic_S italic_D = 3.6 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT</annotation></semantics></math>) and <math alttext="0.5^{\circ}(SD=1.5^{\circ}" class="ltx_math_unparsed" display="inline" id="S3.SS2.p2.3.m3.1"><semantics id="S3.SS2.p2.3.m3.1a"><mrow id="S3.SS2.p2.3.m3.1b"><msup id="S3.SS2.p2.3.m3.1.1"><mn id="S3.SS2.p2.3.m3.1.1.2">0.5</mn><mo id="S3.SS2.p2.3.m3.1.1.3">∘</mo></msup><mrow id="S3.SS2.p2.3.m3.1.2"><mo id="S3.SS2.p2.3.m3.1.2.1" stretchy="false">(</mo><mi id="S3.SS2.p2.3.m3.1.2.2">S</mi><mi id="S3.SS2.p2.3.m3.1.2.3">D</mi><mo id="S3.SS2.p2.3.m3.1.2.4">=</mo><msup id="S3.SS2.p2.3.m3.1.2.5"><mn id="S3.SS2.p2.3.m3.1.2.5.2">1.5</mn><mo id="S3.SS2.p2.3.m3.1.2.5.3">∘</mo></msup></mrow></mrow><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">0.5^{\circ}(SD=1.5^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m3.1d">0.5 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT ( italic_S italic_D = 1.5 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT</annotation></semantics></math>), respectively. These results confirm the stable operation of the algorithm for determining 6D-positions even in a complex joint configuration of objects. It is worth mentioning that the accuracy can be improved through fine-tuning the OVE6D model on its own dataset.</p>
</div>
<figure class="ltx_figure" id="S3.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="261" id="S3.F8.g1" src="extracted/5916005/Figures/133exp3222.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Dependence of rotation errors estimated for complex object combinations. 1 – flask, 2 – glass beaker, 3 – graduated cylinder, 4 – pipette, 5 – test tube, 6 – 6-hole tube rack, 7 – 25-hole tube rack.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.4.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.5.2">Demonstration of autonomous manipulation in an occluded environment</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">In this experiment, the Autonomous operation mode of the developed system was tested.
The task was formulated as follows: grasp a tilted pipette, draw up liquid with a pipette from the glass beaker, and pour it into a flask.
After exploring the working area of the algorithm in the previous experiments, it was defined the camera location for stable and reliable object recognition. The result of object poses recognition is shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S3.F9" title="Figure 9 ‣ III-C Demonstration of autonomous manipulation in an occluded environment ‣ III Experiments ‣ Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="236" id="S3.F9.g1" src="extracted/5916005/Figures/Pose_estim_exp.jpg" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Object poses estimation for autonomous liquid dispensing operation.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">After recognition, the liquid in the first vessel (glass beaker) was painted for better visualization. As described in the section <a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S2.SS3.SSS1" title="II-C1 Simulated Environment ‣ II-C Decision-Making and Execution Module ‣ II Framework overview ‣ Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-C</span>1</span></a>, the robot trajectory calculation is first performed for the digital twin to check all operations in Unity before connecting the real robot to the system.</p>
</div>
<figure class="ltx_figure" id="S3.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="734" id="S3.F10.g1" src="extracted/5916005/Figures/circles_sm.jpg" width="556"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Illustration of performing autonomous liquid dispensing operation. 1 – Preparing to grip the pipette, 2 – Grasping the pipette, 3 – Lowering the pipette into the vessel containing liquid (glass beaker), 4 – The process of drawing liquid into the pipette, 5 – Pouring liquid from the pipette into another vessel (flask).</figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">The stages of completing the task are shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S3.F10" title="Figure 10 ‣ III-C Demonstration of autonomous manipulation in an occluded environment ‣ III Experiments ‣ Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_tag">10</span></a>. Based on the recognized object poses, six key points of the trajectory were algorithmically calculated.</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1">The <span class="ltx_text ltx_font_italic" id="S3.SS3.p4.1.1">first</span> point is the point where the gripper is rotated parallel to the pipette to grasp it, the <span class="ltx_text ltx_font_italic" id="S3.SS3.p4.1.2">second</span> is the gripping point, the <span class="ltx_text ltx_font_italic" id="S3.SS3.p4.1.3">third</span> is the point where the gripper with the pipette is rotated perpendicularly over the vessel containing the yellow liquid (glass beaker), the <span class="ltx_text ltx_font_italic" id="S3.SS3.p4.1.4">fourth</span> is the point where the pipette is lowered into the vessel to draw the liquid, the <span class="ltx_text ltx_font_italic" id="S3.SS3.p4.1.5">fifth</span> is the point above the second vessel (flask), and the <span class="ltx_text ltx_font_italic" id="S3.SS3.p4.1.6">sixth</span> is the point where the pipette is lowered into it to pour out the liquid.
The optimal trajectory between these points was calculated using MoveIt!. All operations were performed accurately and without collisions with other objects. This effect was achieved by algorithmically determining safe positions for lifting the gripper over each object before moving on to the next, and by defining dead zones around each object that are not currently being manipulated. These parameters were calculated from 6D object pose data.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS4.4.1.1">III-D</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS4.5.2">Performing a liquid dispensing operation in a teleoperated mode</span>
</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">We conducted a comparative experiment on a liquid dosing operation using full Teleoperation mode, with the same settings and task as was explored via the Autonomous mode (section <a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S3.SS3" title="III-C Demonstration of autonomous manipulation in an occluded environment ‣ III Experiments ‣ Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-C</span></span></a>). Ten participants (3 females) volunteered to participate in the experiment. The mean age of the participants was 27.4 (SD=3.1), with a range of 23–34. Six participants have never operated robotic manipulators, and four reported regular experience with collaborative robots.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">The experimental task was to grasp a tilted pipette from a tube rack, fill it with water from the glass beaker, and then pour the liquid into the flask. Before the experiment, each participant had a short training session to demonstrate the procedure and the control principle of the robot. Participants had three attempts to complete the task in the experiment. For each trial, the time to complete the task and the trajectory of the tool center point (TCP) of the robot during movement were recorded.</p>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.2"><span class="ltx_text ltx_font_italic" id="S3.SS4.p3.2.1">Experimental results</span>: The average task execution time comprised 157 <math alttext="s" class="ltx_Math" display="inline" id="S3.SS4.p3.1.m1.1"><semantics id="S3.SS4.p3.1.m1.1a"><mi id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><ci id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.1.m1.1d">italic_s</annotation></semantics></math> (SD=39 <math alttext="s" class="ltx_Math" display="inline" id="S3.SS4.p3.2.m2.1"><semantics id="S3.SS4.p3.2.m2.1a"><mi id="S3.SS4.p3.2.m2.1.1" xref="S3.SS4.p3.2.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.2.m2.1b"><ci id="S3.SS4.p3.2.m2.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.2.m2.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.2.m2.1d">italic_s</annotation></semantics></math>) and the mean number of attempts to complete the task comprised 1.5. Six participants successfully completed the task from the first trial. It is worth mentioning all participants grasped the tilted pipette and placed it to the glass beaker to draw the liquid on the first attempt, but had difficulty performing subsequent actions. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.07801v1#S3.F11" title="Figure 11 ‣ III-D Performing a liquid dispensing operation in a teleoperated mode ‣ III Experiments ‣ Robotic framework for autonomous manipulation of laboratory equipment with different degrees of transparency via 6D pose estimation"><span class="ltx_text ltx_ref_tag">11</span></a> shows the examples of the TCP trajectories from the initial position of the robot to the stage of pouring the liquid into the flask for two participants.</p>
</div>
<figure class="ltx_figure" id="S3.F11">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F11.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="226" id="S3.F11.sf1.g1" src="extracted/5916005/Figures/user1.jpg" width="568"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F11.sf1.2.1.1" style="font-size:80%;">(a)</span> </span><span class="ltx_text" id="S3.F11.sf1.3.2" style="font-size:80%;">Trajectory of the first participant.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F11.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="222" id="S3.F11.sf2.g1" src="extracted/5916005/Figures/user2.jpg" width="568"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F11.sf2.2.1.1" style="font-size:80%;">(b)</span> </span><span class="ltx_text" id="S3.F11.sf2.3.2" style="font-size:80%;">Trajectory of the second participant.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Examples of robot TCP trajectories for 2 participants (two angles of view each).1 — Start position, 2 — Grasping a pipette, 3 — Draw a liquid with a pipette, 4 — Pour liquid to the flask.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS4.p4">
<p class="ltx_p" id="S3.SS4.p4.1">Compared to the Autonomous mode, the robot TCP trajectories were not as smooth as optimal. As noted above, on average, operators needed more than one attempt to successfully complete the task. Participants also noted that it was difficult to maintain the pressure required to grip the pipette firmly without spilling liquid while the robot was moving.</p>
</div>
<div class="ltx_para" id="S3.SS4.p5">
<p class="ltx_p" id="S3.SS4.p5.1">These results confirm the advantage of using an Autonomous robot control mode over the Teleoperation mode.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Conclusion and Future work</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this work, we have presented a framework with an autonomous mode for robotic manipulation of objects with different degrees of transparency.
The proposed system is capable of estimating 6D poses of objects arranged in a variety of location configurations, the level of internal liquid, the geometric location of the upper neck of vessels, and autonomously manipulate objects in various tasks.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">Through a series of experiments at this stage of development, the framework has demonstrated an average accuracy of 0.18 <math alttext="cm" class="ltx_Math" display="inline" id="S4.p2.1.m1.1"><semantics id="S4.p2.1.m1.1a"><mrow id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml"><mi id="S4.p2.1.m1.1.1.2" xref="S4.p2.1.m1.1.1.2.cmml">c</mi><mo id="S4.p2.1.m1.1.1.1" xref="S4.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.p2.1.m1.1.1.3" xref="S4.p2.1.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><apply id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1"><times id="S4.p2.1.m1.1.1.1.cmml" xref="S4.p2.1.m1.1.1.1"></times><ci id="S4.p2.1.m1.1.1.2.cmml" xref="S4.p2.1.m1.1.1.2">𝑐</ci><ci id="S4.p2.1.m1.1.1.3.cmml" xref="S4.p2.1.m1.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">cm</annotation><annotation encoding="application/x-llamapun" id="S4.p2.1.m1.1d">italic_c italic_m</annotation></semantics></math> for position estimation and about 0.7° for rotation estimation for complex combinations of objects in the algorithm working area. This demonstrates the robustness of the framework’s autonomous algorithms. Besides, according to the liquid dispensing experiment, the autonomous mode provides a more robust and efficient motion trajectory as well as grasping functions compared to telemanipulation mode.
As a future work, it is planned to use information from tactile sensors on the robotic gripper to control the gripping force more precisely when handling fragile objects. The framework can also be extended with functions that generate key trajectory points for more diverse tasks, as well as by adding VLMs to the system architecture.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">The proposed framework can be potentially implemented for the automation of non-trivial tasks of manipulating objects with different degrees of transparency with additional analysis of the liquid level inside, requiring high accuracy and repeatability. We believe that the capabilities of the developed system may be essential in the field of automated chemical experiments and in medical analysis.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
A. Naceri, D. Mazzanti, J. Bimbo, Y. T. Tefera, D. Prattichizzo, D. G. Caldwell, L. S. Mattos, and N. Deshpande, “The vicarios virtual reality interface for remote robotic teleoperation: Teleporting for intuitive tele-manipulation,” <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Journal of Intelligent &amp; Robotic Systems</em>, vol. 101, pp. 1–16, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
M. Wonsick, T. Keleștemur, S. Alt, and T. Padır, “Telemanipulation via virtual reality interfaces with enhanced environment models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>.   IEEE, 2021, pp. 2999–3004.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
P. Wu, Y. Shentu, Z. Yi, X. Lin, and P. Abbeel, “Gello: A general, low-cost, and intuitive teleoperation framework for robot manipulators,” <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:2309.13037</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
T. Z. Zhao, V. Kumar, S. Levine, and C. Finn, “Learning fine-grained bimanual manipulation with low-cost hardware,” <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">arXiv preprint arXiv:2304.13705</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
E. Yigitbas, K. Karakaya, I. Jovanovikj, and G. Engels, “Enhancing human-in-the-loop adaptive systems through digital twins and vr interfaces,” in <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">2021 International Symposium on Software Engineering for Adaptive and Self-Managing Systems (SEAMS)</em>.   IEEE, 2021, pp. 30–40.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
T.-C. Lin, A. Krishnan, and Z. Li, “Shared autonomous interface for reducing physical effort in robot teleoperation via human motion mapping,” in <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">2020 IEEE International Conference on Robotics and Automation (ICRA)</em>.   IEEE, 2020, pp. 9157–9163.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
A. Pettinger, C. Elliott, P. Fan, and M. Pryor, “Reducing the teleoperator’s cognitive burden for complex contact tasks using affordance primitives,” in <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>.   IEEE, 2020, pp. 11 513–11 518.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
A. Gottardi, S. Tortora, E. Tosello, and E. Menegatti, “Shared control in robot teleoperation with improved potential fields,” <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">IEEE Transactions on Human-Machine Systems</em>, vol. 52, pp. 1–13, 06 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
R. Kitney, M. Adeogun, Y. Fujishima, Á. Goñi-Moreno, R. Johnson, M. Maxon, S. Steedman, S. Ward, D. Winickoff, and J. Philp, “Enabling the advanced bioeconomy through public policy supporting biofoundries and engineering biology,” <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Trends in biotechnology</em>, vol. 37, no. 9, pp. 917–920, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
H. Fakhruldeen, G. Pizzuto, J. Glowacki, and A. I. Cooper, “Archemist: Autonomous robotic chemistry system architecture,” in <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">2022 International Conference on Robotics and Automation (ICRA)</em>.   IEEE, 2022, pp. 6013–6019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
A. Lunt, H. Fakhruldeen, G. Pizzuto, L. Longley, A. White, N. Rankin, R. Clowes, B. Alston, L. Gigli, G. Day, A. Cooper, and S. Chong, “Modular, multi-robot integration of laboratories: An autonomous workflow for solid-state chemistry,” <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Chemical Science</em>, vol. 15, 12 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
W. Wan, T. Kotaka, and K. Harada, “Arranging test tubes in racks using combined task and motion planning,” <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Robotics and Autonomous Systems</em>, vol. 147, p. 103918, 10 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
P. Shiri, V. Lai, T. Zepel, D. Griffin, J. Reifman, S. Clark, S. Grunert, L. Yunker, S. Steiner, H. Situ, F. Yang, P. Prieto, and J. Hein, “Automated solubility screening platform using computer vision,” <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">iScience</em>, vol. 24, p. 102176, 02 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
D. Coleman, I. Sucan, S. Chitta, and N. Correll, “Reducing the barrier to entry of complex robotic software: a moveit! case study,” <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:1404.3785</em>, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Y. He, H. Huang, H. Fan, Q. Chen, and J. Sun, “Ffb6d: A full flow bidirectional fusion network for 6d pose estimation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2021, pp. 3003–3013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
C. Wang, D. Xu, Y. Zhu, R. Martín-Martín, C. Lu, L. Fei-Fei, and S. Savarese, “Densefusion: 6d object pose estimation by iterative dense fusion,” in <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2019, pp. 3343–3352.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
K. Park, A. Mousavian, Y. Xiang, and D. Fox, “Latentfusion: End-to-end differentiable reconstruction and rendering for unseen object pose estimation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2020, pp. 10 710–10 719.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
D. Cai, J. Heikkilä, and E. Rahtu, “Ove6d: Object viewpoint encoding for depth-based 6d object pose estimation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2022, pp. 6803–6813.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask r-cnn,” in <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the IEEE international conference on computer vision</em>, 2017, pp. 2961–2969.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
S. Hinterstoisser, S. Holzer, C. Cagniart, S. Ilic, K. Konolige, N. Navab, and V. Lepetit, “Multimodal templates for real-time detection of texture-less objects in heavily cluttered scenes,” in <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">2011 international conference on computer vision</em>.   IEEE, 2011, pp. 858–865.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
S. Garrido-Jurado, R. Muñoz-Salinas, F. J. Madrid-Cuevas, and M. J. Marín-Jiménez, “Automatic generation and detection of highly reliable fiducial markers under occlusion,” <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Pattern Recognition</em>, vol. 47, no. 6, pp. 2280–2292, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
S. Eppel, H. Xu, Y. R. Wang, and A. Aspuru-Guzik, “Predicting 3d shapes, masks, and properties of materials, liquids, and objects inside transparent containers, using the transproteus cgi dataset,” <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2109.07577</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for semantic segmentation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2015, pp. 3431–3440.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam, “Rethinking atrous convolution for semantic image segmentation,” <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:1706.05587</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2016, pp. 770–778.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks for biomedical image segmentation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Medical image computing and computer-assisted intervention–MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18</em>.   Springer, 2015, pp. 234–241.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
M. Quigley, K. Conley, B. Gerkey, J. Faust, T. Foote, J. Leibs, R. Wheeler, A. Y. Ng <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">et al.</em>, “Ros: an open-source robot operating system,” in <em class="ltx_emph ltx_font_italic" id="bib.bib27.2.2">ICRA workshop on open source software</em>, vol. 3, no. 3.2.   Kobe, Japan, 2009, p. 5.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
D. Trinitatova, M. A. Cabrera, P. Ponomareva, A. Fedoseev, and D. Tsetserukou, “Exploring the role of electro-tactile and kinesthetic feedback in telemanipulation task,” in <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">2022 IEEE 18th International Conference on Automation Science and Engineering (CASE)</em>.   IEEE, 2022, pp. 641–646.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Oct 10 10:34:00 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
