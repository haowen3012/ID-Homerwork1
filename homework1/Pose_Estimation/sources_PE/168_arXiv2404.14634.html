<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues</title>
<!--Generated on Wed Jul 10 00:59:36 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Markerless Motion Capture Multi-view Human Pose Estimation Uncertainty Modeling Temporal Learning" lang="en" name="keywords"/>
<base href="/html/2404.14634v3/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S1" title="In UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S2" title="In UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Works</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S3" title="In UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S3.SS1" title="In 3 Method ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>2D Pose Estimation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S3.SS2" title="In 3 Method ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Cross-view Projection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S3.SS3" title="In 3 Method ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Pose Compiler</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S3.SS4" title="In 3 Method ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>3D Pose Estimation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S3.SS5" title="In 3 Method ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Training and Multi-view Data Synthesis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S4" title="In UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S4.SS1" title="In 4 Experiments ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S4.SS2" title="In 4 Experiments ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Evaluation Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S4.SS3" title="In 4 Experiments ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Implementation Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S4.SS4" title="In 4 Experiments ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Baselines</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S5" title="In UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S5.SS1" title="In 5 Results ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>In-Distribution Performance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S5.SS2" title="In 5 Results ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Out-of-Distribution Generalization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S5.SS3" title="In 5 Results ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Ablation Study</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S5.SS4" title="In 5 Results ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Computation Costs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S5.SS5" title="In 5 Results ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Viewpoint Scalability</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S5.SS6" title="In 5 Results ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6 </span>Qualitative Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S6" title="In UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#Pt0.A1" title="In UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.A </span>Additional Details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#Pt0.A1.SS1" title="In Appendix 0.A Additional Details ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.A.1 </span>Details on Training and Multi-view Data Synthesis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#Pt0.A1.SS2" title="In Appendix 0.A Additional Details ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.A.2 </span>Details on Criss-cross Attention</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#Pt0.A2" title="In UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.B </span>Additional Experiments and Results</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#Pt0.A2.SS1" title="In Appendix 0.B Additional Experiments and Results ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.B.1 </span>2D Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#Pt0.A2.SS2" title="In Appendix 0.B Additional Experiments and Results ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.B.2 </span>Additional Ablation Study</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#Pt0.A2.SS3" title="In Appendix 0.B Additional Experiments and Results ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.B.3 </span>Additional Baselines and Comparisons</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#Pt0.A3" title="In UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.C </span>Additional Visual Examples</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#Pt0.A4" title="In UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.D </span>Qualitataive Comparisons</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Queen’s University, Canada </span></span></span><span class="ltx_note ltx_role_institutetext" id="id2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Ubisoft LaForge, Canada 
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id2.1"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">email: </span>{vandad.davoodnia,ali.etemad}@queensu.ca</span></span></span>
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id2.2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">email: </span>{saeed.ghorbani,marc-andre.carbonneau2,alexandre.messier}@ubisoft.com</span></span></span>
</span></span></span>
<h1 class="ltx_title ltx_title_document">UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Vandad Davoodnia
<span class="ltx_ERROR undefined" id="id1.1.id1">\orcidlink</span>0000-0002-2167-2119
</span><span class="ltx_author_notes">1122</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Saeed Ghorbani<span class="ltx_ERROR undefined" id="id2.1.id1">\orcidlink</span>0000-0002-3227-9013
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Marc-André Carbonneau<span class="ltx_ERROR undefined" id="id3.1.id1">\orcidlink</span>0000-0002-0677-415X
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alexandre Messier
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ali Etemad<span class="ltx_ERROR undefined" id="id4.1.id1">\orcidlink</span>0000-0001-7128-0220
</span><span class="ltx_author_notes">11</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id5.id1">We introduce UPose3D, a novel approach for multi-view 3D human pose estimation, addressing challenges in accuracy and scalability. Our method advances existing pose estimation frameworks by improving robustness and flexibility without requiring direct 3D annotations. At the core of our method, a pose compiler module refines predictions from a 2D keypoints estimator that operates on a single image by leveraging temporal and cross-view information. Our novel cross-view fusion strategy is scalable to any number of cameras, while our synthetic data generation strategy ensures generalization across diverse actors, scenes, and viewpoints. Finally, UPose3D leverages the prediction uncertainty of both the 2D keypoint estimator and the pose compiler module. This provides robustness to outliers and noisy data, resulting in state-of-the-art performance in out-of-distribution settings. In addition, for in-distribution settings, UPose3D yields performance rivalling methods that rely on 3D annotated data while being the state-of-the-art among methods relying only on 2D supervision.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Markerless Motion Capture Multi-view Human Pose Estimation Uncertainty Modeling Temporal Learning
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Multi-view 3D human pose estimation is a challenging task in computer vision that involves determining the 3D position of human body landmarks given videos or images from multiple synchronized cameras <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib14" title="">14</a>]</cite>. Compared to monocular setups, multi-view pose estimation leverages information from different viewpoints, alleviating the single-camera ambiguity and improving accuracy in challenging situations. This robustness is crucial in precision-demanding applications like markerless motion capture, essential to industries such as video gaming and film-making, where sub-centimeter accuracy is often required.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The conventional 3D keypoint estimation process involves two stages. Firstly, 2D landmarks are extracted from each camera view. This is followed by triangulation using the known camera parameters to infer 3D points <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib66" title="">66</a>]</cite>. However, the accuracy of such methods heavily relies on the precision of independent 2D predictions across views, which is problematic in scenarios with complex body-part interactions or severe occlusions. Outlier mitigation techniques such as RANdom-SAmple Consensus (RANSAC) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib43" title="">43</a>]</cite> and refinement algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib60" title="">60</a>]</cite> offer some robustness but cannot fully address these inherent limitations. Recent advances in deep learning models that use cross-view fusion strategies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib14" title="">14</a>]</cite> have yielded promising 3D pose estimation results. For instance, Epipolar Transformers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib14" title="">14</a>]</cite> shows the benefits of leveraging cross-view information using epipolar geometry. However, the scalability of such approaches is often hindered with additional cameras, requiring advanced techniques to maximize the agreement between several model outputs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib47" title="">47</a>]</cite>. Another research direction aims to leverage rich temporal information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib58" title="">58</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib55" title="">55</a>]</cite> to enhance pose estimation accuracy. For instance, numerous works show the impact of using large temporal context for monocular 3D pose estimation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib70" title="">70</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib48" title="">48</a>]</cite>. Similarly, methods like MFT-Transformers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib55" title="">55</a>]</cite>, focusing on multi-view fusion and temporal modelings, can yield improvements over single-frame methods. However, such approaches require access to large annotated 3D datasets (multi-view video streams and corresponding 3D pose coordinates) during training, which is especially scarce in outdoor and in-the-wild settings. Furthermore, these models are often trained on limited pose and camera variations, hindering their generalization to novel views.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To address the challenges of viewpoint scalability and reliance on 3D annotated training data, we introduce UPose3D, a new method for 3D human pose estimation. Our method leverages 2D keypoints and their uncertainties from two sources to improve robustness to outliers and noisy data. These sources include: <span class="ltx_text ltx_font_italic" id="S1.p3.1.1">a</span>) direct 2D pose estimation from RGB images, and <span class="ltx_text ltx_font_italic" id="S1.p3.1.2">b</span>) a pose compiler module that utilizes consistency across views and over time. Additionally, we introduce a new cross-view fusion strategy to ensure scalability to different numbers of cameras before our pose compiler. Specifically, we project the keypoints from all available views onto a reference view to obtain a 2D point cloud for each joint. These are then fed to a point cloud transformer module to learn cross-view representations. These features are then passed to a spatiotemporal encoder to efficiently process temporal and skeleton information from temporal windows. To train our pose compiler without relying on 3D annotated datasets, we generate synthetic data simulating realistic multi-view human pose recordings from a large-scale motion capture dataset. This approach promotes generalization across diverse camera configurations and postures, overcoming the limitations of real-world, multi-view 3D annotated datasets.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To evaluate the performance of our proposed model, we use four widely used public datasets, Human3.6m <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib18" title="">18</a>]</cite>, RICH <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib16" title="">16</a>]</cite>, HUMBI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib64" title="">64</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib65" title="">65</a>]</cite>, and CMU Panoptic <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib22" title="">22</a>]</cite>, across two separate experiment setups.
More specifically, we assess the performance of our method in both in-distribution (InD) and out-of-distribution (OoD) settings to better evaluate generalizability to new environments and multi-view camera setups. These experiments demonstrate that our approach outperforms prior state-of-the-art solutions in OoD while achieving competitive performance in InD settings. Next, we provide detailed ablation experiments demonstrating the impact of uncertainty modeling and our pose compiler in improving multi-view pose estimation robustness to outliers. Finally, we perform several experiments to showcase the impact of the number of camera views and larger time windows. In summary, our method achieves a high level of view-point flexibility and robustness without requiring direct 3D annotations. Our contributions can be summarized as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.I1.i1.1.1.m1.1"><semantics id="S1.I1.i1.1.1.m1.1b"><mo id="S1.I1.i1.1.1.m1.1.1" xref="S1.I1.i1.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.I1.i1.1.1.m1.1c"><ci id="S1.I1.i1.1.1.m1.1.1.cmml" xref="S1.I1.i1.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i1.1.1.m1.1d">\bullet</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i1.1.1.m1.1e">∙</annotation></semantics></math></span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We present UPose3D, a 3D human pose estimation pipeline for multi-view setups that achieves state-of-the-art results in OoD settings and performs competitively in InD evaluations.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.I1.i2.1.1.m1.1"><semantics id="S1.I1.i2.1.1.m1.1b"><mo id="S1.I1.i2.1.1.m1.1.1" xref="S1.I1.i2.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.I1.i2.1.1.m1.1c"><ci id="S1.I1.i2.1.1.m1.1.1.cmml" xref="S1.I1.i2.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i2.1.1.m1.1d">\bullet</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i2.1.1.m1.1e">∙</annotation></semantics></math></span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">Our method uses a novel uncertainty-aware 3D pose estimation algorithm that uses normalizing flows to leverage 2D pose distribution modeling. Experiments demonstrate that this approach is more effective than prior works using heatmaps in terms of accuracy and computational costs. This also allows our pipeline to scale to different camera setups with <span class="ltx_text ltx_font_italic" id="S1.I1.i2.p1.1.1">constant</span> runtime.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.I1.i3.1.1.m1.1"><semantics id="S1.I1.i3.1.1.m1.1b"><mo id="S1.I1.i3.1.1.m1.1.1" xref="S1.I1.i3.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.I1.i3.1.1.m1.1c"><ci id="S1.I1.i3.1.1.m1.1.1.cmml" xref="S1.I1.i3.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i3.1.1.m1.1d">\bullet</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i3.1.1.m1.1e">∙</annotation></semantics></math></span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We propose a new training strategy that relies only on synthetic multi-view motion sequences generated online from motion capture data.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">3D Pose Estimation.</span>
Traditionally, triangulation techniques like RANSAC have been used for 3D human pose estimation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib66" title="">66</a>]</cite>. However, these methods are generally not directly differentiable and their integration into deep learning pipelines is hindered. Therefore, recent research has explored more flexible, soft-predictive models, such as volumetric 3D keypoint representations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib19" title="">19</a>]</cite> and cross-view feature fusion strategies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib47" title="">47</a>]</cite>. Another approach <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib2" title="">2</a>]</cite> introduces a stochastic framework for human pose triangulation that relies on 3D pose hypothesis generation, scoring, and selection from 2D detection of several camera views. However, the accuracy of 2D pose detectors limits most such approaches. As a result, some methods incorporate epipolar geometry for pose consistency via self-supervised <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib27" title="">27</a>]</cite> and semi-supervised <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib63" title="">63</a>]</cite> learning. More advanced techniques have explored feature fusion strategies using epipolar lines across camera pairs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib66" title="">66</a>]</cite>, showing significant improvements. However, these methods require complex re-projection in view pairs, limiting their scalability to a large number of cameras or with different placements due to lack of variety in the training set.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">To avoid relying on large parameters, a recent work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib55" title="">55</a>]</cite> proposed a multi-view temporal transformer network for end-to-end 3D pose estimation by leveraging cross-view feature fusion. Other works have conducted experiments on using human pose priors using generative models such as GANs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib25" title="">25</a>]</cite> and Diffusion probabilistic models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib15" title="">15</a>]</cite> to perform 3D pose estimations from a variety of inputs. However, most such approaches rely on 3D pose annotations from in-studio collected datasets to train their model. Another research track explores the benefits of uncertainty modeling for 3D human pose estimation for enhancing the performance in occlusion-heavy scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib29" title="">29</a>]</cite>. For example, Residual Log-likelihood Estimation (RLE) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib31" title="">31</a>]</cite>, has been proposed to model the underlying distribution of 2D keypoints in human pose estimation via regression. This approach leverages a re-parameterization technique and normalizing flows to learn keypoint uncertainty and achieves similar performance to heatmap-based <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib57" title="">57</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib4" title="">4</a>]</cite> and SimCC-based <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib33" title="">33</a>]</cite> techniques. We exploit the ability of RLE to estimate a differentiable likelihood of keypoints, which other methods lack.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p3">
<p class="ltx_p" id="S2.p3.1"><span class="ltx_text ltx_font_bold" id="S2.p3.1.1">Transformers in Pose Estimation.</span>
The Transformer architecture and its self-attention mechanism have significantly advanced Natural Language Processing and Computer Vision. Self-attention’s ability to capture long-range dependencies makes it invaluable for 3D pose estimation, which requires careful consideration of spatial, temporal, and multi-view information. Recent works effectively leverage Transformers for 3D pose estimation in both single-camera setups <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib71" title="">71</a>]</cite> and for handling spatiotemporal information within single images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib68" title="">68</a>]</cite>. Additionally, transformers show promise in aggregating multi-view clues via epipolar geometry <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib41" title="">41</a>]</cite>. Despite the impressive performance of transformers in a variety of tasks, their memory requirements cause an obstacle in processing all spatial, temporal, and multi-view information together. As a result, a group of researchers has adopted a criss-cross attention mechanism to limit each attention layer’s receptive field without sacrificing the network’s overall receptive field <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib17" title="">17</a>]</cite>. Recent work has also adopted the criss-cross attentions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib58" title="">58</a>]</cite> to process temporal and joint information for human 3D pose estimation, showing superior performance compared to parallel or concurrent models.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="277" id="S2.F1.g1" src="x1.png" width="788"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S2.F1.3.2" style="font-size:90%;">We illustrate the key stages of UPose3D. It begins with extracting 2D keypoints and uncertainties from the multi-view videos. The keypoints are then projected onto each reference view using epipolar geometry. Our pose compiler is then used to refine the predictions by leveraging cross-view and spatiotemporal information. Finally, the 3D pose is obtained using the keypoint and uncertainty predictions of each stage.</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Upose3D determines the 3D coordinates of body joints from one or more consecutive multi-view frames. <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S2.F1" title="In 2 Related Works ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a> depicts an overview of our proposed method. At first, an RLE-based 2D pose estimator (<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S3.SS1" title="3.1 2D Pose Estimation ‣ 3 Method ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.1</span></a>) extracts 2D keypoints and their corresponding uncertainties from the input images. The keypoints are then projected onto other views using epipolar geometry (<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S3.SS2" title="3.2 Cross-view Projection ‣ 3 Method ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>). From there, the pose compiler (<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S3.SS3" title="3.3 Pose Compiler ‣ 3 Method ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.3</span></a>) refines the keypoint locations and their uncertainties by leveraging spatiotemporal and cross-view information. Lastly, using outputs from the 2D pose estimator and the pose compiler, an iterative algorithm obtains the final 3D poses (<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S3.SS4" title="3.4 3D Pose Estimation ‣ 3 Method ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.4</span></a>). We end this section by describing our multi-view training data synthesis (<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S3.SS5" title="3.5 Training and Multi-view Data Synthesis ‣ 3 Method ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.5</span></a>).</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>2D Pose Estimation</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.12">The first step of our method is estimating 2D joint position in each image <math alttext="\mathcal{I}_{i,t}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.2"><semantics id="S3.SS1.p1.1.m1.2a"><msub id="S3.SS1.p1.1.m1.2.3" xref="S3.SS1.p1.1.m1.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.1.m1.2.3.2" xref="S3.SS1.p1.1.m1.2.3.2.cmml">ℐ</mi><mrow id="S3.SS1.p1.1.m1.2.2.2.4" xref="S3.SS1.p1.1.m1.2.2.2.3.cmml"><mi id="S3.SS1.p1.1.m1.1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.1.cmml">i</mi><mo id="S3.SS1.p1.1.m1.2.2.2.4.1" xref="S3.SS1.p1.1.m1.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p1.1.m1.2.2.2.2" xref="S3.SS1.p1.1.m1.2.2.2.2.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.2b"><apply id="S3.SS1.p1.1.m1.2.3.cmml" xref="S3.SS1.p1.1.m1.2.3"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.2.3.1.cmml" xref="S3.SS1.p1.1.m1.2.3">subscript</csymbol><ci id="S3.SS1.p1.1.m1.2.3.2.cmml" xref="S3.SS1.p1.1.m1.2.3.2">ℐ</ci><list id="S3.SS1.p1.1.m1.2.2.2.3.cmml" xref="S3.SS1.p1.1.m1.2.2.2.4"><ci id="S3.SS1.p1.1.m1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1">𝑖</ci><ci id="S3.SS1.p1.1.m1.2.2.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2">𝑡</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.2c">\mathcal{I}_{i,t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.2d">caligraphic_I start_POSTSUBSCRIPT italic_i , italic_t end_POSTSUBSCRIPT</annotation></semantics></math> from all camera views <math alttext="i\in\mathcal{V}=\{1,2,\dots,V\}" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.4"><semantics id="S3.SS1.p1.2.m2.4a"><mrow id="S3.SS1.p1.2.m2.4.5" xref="S3.SS1.p1.2.m2.4.5.cmml"><mi id="S3.SS1.p1.2.m2.4.5.2" xref="S3.SS1.p1.2.m2.4.5.2.cmml">i</mi><mo id="S3.SS1.p1.2.m2.4.5.3" xref="S3.SS1.p1.2.m2.4.5.3.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.2.m2.4.5.4" xref="S3.SS1.p1.2.m2.4.5.4.cmml">𝒱</mi><mo id="S3.SS1.p1.2.m2.4.5.5" xref="S3.SS1.p1.2.m2.4.5.5.cmml">=</mo><mrow id="S3.SS1.p1.2.m2.4.5.6.2" xref="S3.SS1.p1.2.m2.4.5.6.1.cmml"><mo id="S3.SS1.p1.2.m2.4.5.6.2.1" stretchy="false" xref="S3.SS1.p1.2.m2.4.5.6.1.cmml">{</mo><mn id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">1</mn><mo id="S3.SS1.p1.2.m2.4.5.6.2.2" xref="S3.SS1.p1.2.m2.4.5.6.1.cmml">,</mo><mn id="S3.SS1.p1.2.m2.2.2" xref="S3.SS1.p1.2.m2.2.2.cmml">2</mn><mo id="S3.SS1.p1.2.m2.4.5.6.2.3" xref="S3.SS1.p1.2.m2.4.5.6.1.cmml">,</mo><mi id="S3.SS1.p1.2.m2.3.3" mathvariant="normal" xref="S3.SS1.p1.2.m2.3.3.cmml">…</mi><mo id="S3.SS1.p1.2.m2.4.5.6.2.4" xref="S3.SS1.p1.2.m2.4.5.6.1.cmml">,</mo><mi id="S3.SS1.p1.2.m2.4.4" xref="S3.SS1.p1.2.m2.4.4.cmml">V</mi><mo id="S3.SS1.p1.2.m2.4.5.6.2.5" stretchy="false" xref="S3.SS1.p1.2.m2.4.5.6.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.4b"><apply id="S3.SS1.p1.2.m2.4.5.cmml" xref="S3.SS1.p1.2.m2.4.5"><and id="S3.SS1.p1.2.m2.4.5a.cmml" xref="S3.SS1.p1.2.m2.4.5"></and><apply id="S3.SS1.p1.2.m2.4.5b.cmml" xref="S3.SS1.p1.2.m2.4.5"><in id="S3.SS1.p1.2.m2.4.5.3.cmml" xref="S3.SS1.p1.2.m2.4.5.3"></in><ci id="S3.SS1.p1.2.m2.4.5.2.cmml" xref="S3.SS1.p1.2.m2.4.5.2">𝑖</ci><ci id="S3.SS1.p1.2.m2.4.5.4.cmml" xref="S3.SS1.p1.2.m2.4.5.4">𝒱</ci></apply><apply id="S3.SS1.p1.2.m2.4.5c.cmml" xref="S3.SS1.p1.2.m2.4.5"><eq id="S3.SS1.p1.2.m2.4.5.5.cmml" xref="S3.SS1.p1.2.m2.4.5.5"></eq><share href="https://arxiv.org/html/2404.14634v3#S3.SS1.p1.2.m2.4.5.4.cmml" id="S3.SS1.p1.2.m2.4.5d.cmml" xref="S3.SS1.p1.2.m2.4.5"></share><set id="S3.SS1.p1.2.m2.4.5.6.1.cmml" xref="S3.SS1.p1.2.m2.4.5.6.2"><cn id="S3.SS1.p1.2.m2.1.1.cmml" type="integer" xref="S3.SS1.p1.2.m2.1.1">1</cn><cn id="S3.SS1.p1.2.m2.2.2.cmml" type="integer" xref="S3.SS1.p1.2.m2.2.2">2</cn><ci id="S3.SS1.p1.2.m2.3.3.cmml" xref="S3.SS1.p1.2.m2.3.3">…</ci><ci id="S3.SS1.p1.2.m2.4.4.cmml" xref="S3.SS1.p1.2.m2.4.4">𝑉</ci></set></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.4c">i\in\mathcal{V}=\{1,2,\dots,V\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.4d">italic_i ∈ caligraphic_V = { 1 , 2 , … , italic_V }</annotation></semantics></math> and frames <math alttext="t\in\mathcal{T}=\{1,2,\dots,T\}" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.4"><semantics id="S3.SS1.p1.3.m3.4a"><mrow id="S3.SS1.p1.3.m3.4.5" xref="S3.SS1.p1.3.m3.4.5.cmml"><mi id="S3.SS1.p1.3.m3.4.5.2" xref="S3.SS1.p1.3.m3.4.5.2.cmml">t</mi><mo id="S3.SS1.p1.3.m3.4.5.3" xref="S3.SS1.p1.3.m3.4.5.3.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.3.m3.4.5.4" xref="S3.SS1.p1.3.m3.4.5.4.cmml">𝒯</mi><mo id="S3.SS1.p1.3.m3.4.5.5" xref="S3.SS1.p1.3.m3.4.5.5.cmml">=</mo><mrow id="S3.SS1.p1.3.m3.4.5.6.2" xref="S3.SS1.p1.3.m3.4.5.6.1.cmml"><mo id="S3.SS1.p1.3.m3.4.5.6.2.1" stretchy="false" xref="S3.SS1.p1.3.m3.4.5.6.1.cmml">{</mo><mn id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">1</mn><mo id="S3.SS1.p1.3.m3.4.5.6.2.2" xref="S3.SS1.p1.3.m3.4.5.6.1.cmml">,</mo><mn id="S3.SS1.p1.3.m3.2.2" xref="S3.SS1.p1.3.m3.2.2.cmml">2</mn><mo id="S3.SS1.p1.3.m3.4.5.6.2.3" xref="S3.SS1.p1.3.m3.4.5.6.1.cmml">,</mo><mi id="S3.SS1.p1.3.m3.3.3" mathvariant="normal" xref="S3.SS1.p1.3.m3.3.3.cmml">…</mi><mo id="S3.SS1.p1.3.m3.4.5.6.2.4" xref="S3.SS1.p1.3.m3.4.5.6.1.cmml">,</mo><mi id="S3.SS1.p1.3.m3.4.4" xref="S3.SS1.p1.3.m3.4.4.cmml">T</mi><mo id="S3.SS1.p1.3.m3.4.5.6.2.5" stretchy="false" xref="S3.SS1.p1.3.m3.4.5.6.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.4b"><apply id="S3.SS1.p1.3.m3.4.5.cmml" xref="S3.SS1.p1.3.m3.4.5"><and id="S3.SS1.p1.3.m3.4.5a.cmml" xref="S3.SS1.p1.3.m3.4.5"></and><apply id="S3.SS1.p1.3.m3.4.5b.cmml" xref="S3.SS1.p1.3.m3.4.5"><in id="S3.SS1.p1.3.m3.4.5.3.cmml" xref="S3.SS1.p1.3.m3.4.5.3"></in><ci id="S3.SS1.p1.3.m3.4.5.2.cmml" xref="S3.SS1.p1.3.m3.4.5.2">𝑡</ci><ci id="S3.SS1.p1.3.m3.4.5.4.cmml" xref="S3.SS1.p1.3.m3.4.5.4">𝒯</ci></apply><apply id="S3.SS1.p1.3.m3.4.5c.cmml" xref="S3.SS1.p1.3.m3.4.5"><eq id="S3.SS1.p1.3.m3.4.5.5.cmml" xref="S3.SS1.p1.3.m3.4.5.5"></eq><share href="https://arxiv.org/html/2404.14634v3#S3.SS1.p1.3.m3.4.5.4.cmml" id="S3.SS1.p1.3.m3.4.5d.cmml" xref="S3.SS1.p1.3.m3.4.5"></share><set id="S3.SS1.p1.3.m3.4.5.6.1.cmml" xref="S3.SS1.p1.3.m3.4.5.6.2"><cn id="S3.SS1.p1.3.m3.1.1.cmml" type="integer" xref="S3.SS1.p1.3.m3.1.1">1</cn><cn id="S3.SS1.p1.3.m3.2.2.cmml" type="integer" xref="S3.SS1.p1.3.m3.2.2">2</cn><ci id="S3.SS1.p1.3.m3.3.3.cmml" xref="S3.SS1.p1.3.m3.3.3">…</ci><ci id="S3.SS1.p1.3.m3.4.4.cmml" xref="S3.SS1.p1.3.m3.4.4">𝑇</ci></set></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.4c">t\in\mathcal{T}=\{1,2,\dots,T\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.4d">italic_t ∈ caligraphic_T = { 1 , 2 , … , italic_T }</annotation></semantics></math>. Similar to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib8" title="">8</a>]</cite>, our method implements a single layer Residual Log-likelihood Estimation (RLE) head <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib31" title="">31</a>]</cite> on top of an off-the-shelf backbone (<em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.12.1">e.g</em>.<span class="ltx_text" id="S3.SS1.p1.12.2"></span>, CPN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib5" title="">5</a>]</cite>). Aside from being computationally effective and robust to occlusion, RLE provides uncertainty <math alttext="\hat{\sigma}" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4.1"><semantics id="S3.SS1.p1.4.m4.1a"><mover accent="true" id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mi id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml">σ</mi><mo id="S3.SS1.p1.4.m4.1.1.1" xref="S3.SS1.p1.4.m4.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><ci id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1.1">^</ci><ci id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2">𝜎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">\hat{\sigma}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.1d">over^ start_ARG italic_σ end_ARG</annotation></semantics></math> for each joint position prediction <math alttext="\hat{\mu}" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m5.1"><semantics id="S3.SS1.p1.5.m5.1a"><mover accent="true" id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mi id="S3.SS1.p1.5.m5.1.1.2" xref="S3.SS1.p1.5.m5.1.1.2.cmml">μ</mi><mo id="S3.SS1.p1.5.m5.1.1.1" xref="S3.SS1.p1.5.m5.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><ci id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1.1">^</ci><ci id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2">𝜇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">\hat{\mu}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.5.m5.1d">over^ start_ARG italic_μ end_ARG</annotation></semantics></math>. Specifically, the RLE predicts a distribution <math alttext="P_{\Theta}(x|\mathcal{I})" class="ltx_Math" display="inline" id="S3.SS1.p1.6.m6.1"><semantics id="S3.SS1.p1.6.m6.1a"><mrow id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml"><msub id="S3.SS1.p1.6.m6.1.1.3" xref="S3.SS1.p1.6.m6.1.1.3.cmml"><mi id="S3.SS1.p1.6.m6.1.1.3.2" xref="S3.SS1.p1.6.m6.1.1.3.2.cmml">P</mi><mi id="S3.SS1.p1.6.m6.1.1.3.3" mathvariant="normal" xref="S3.SS1.p1.6.m6.1.1.3.3.cmml">Θ</mi></msub><mo id="S3.SS1.p1.6.m6.1.1.2" xref="S3.SS1.p1.6.m6.1.1.2.cmml">⁢</mo><mrow id="S3.SS1.p1.6.m6.1.1.1.1" xref="S3.SS1.p1.6.m6.1.1.1.1.1.cmml"><mo id="S3.SS1.p1.6.m6.1.1.1.1.2" stretchy="false" xref="S3.SS1.p1.6.m6.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p1.6.m6.1.1.1.1.1" xref="S3.SS1.p1.6.m6.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.6.m6.1.1.1.1.1.2" xref="S3.SS1.p1.6.m6.1.1.1.1.1.2.cmml">x</mi><mo fence="false" id="S3.SS1.p1.6.m6.1.1.1.1.1.1" xref="S3.SS1.p1.6.m6.1.1.1.1.1.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.6.m6.1.1.1.1.1.3" xref="S3.SS1.p1.6.m6.1.1.1.1.1.3.cmml">ℐ</mi></mrow><mo id="S3.SS1.p1.6.m6.1.1.1.1.3" stretchy="false" xref="S3.SS1.p1.6.m6.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><apply id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1"><times id="S3.SS1.p1.6.m6.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.2"></times><apply id="S3.SS1.p1.6.m6.1.1.3.cmml" xref="S3.SS1.p1.6.m6.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.1.1.3.1.cmml" xref="S3.SS1.p1.6.m6.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.6.m6.1.1.3.2.cmml" xref="S3.SS1.p1.6.m6.1.1.3.2">𝑃</ci><ci id="S3.SS1.p1.6.m6.1.1.3.3.cmml" xref="S3.SS1.p1.6.m6.1.1.3.3">Θ</ci></apply><apply id="S3.SS1.p1.6.m6.1.1.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1"><csymbol cd="latexml" id="S3.SS1.p1.6.m6.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1.1.1">conditional</csymbol><ci id="S3.SS1.p1.6.m6.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1.1.2">𝑥</ci><ci id="S3.SS1.p1.6.m6.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1.1.3">ℐ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">P_{\Theta}(x|\mathcal{I})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.6.m6.1d">italic_P start_POSTSUBSCRIPT roman_Θ end_POSTSUBSCRIPT ( italic_x | caligraphic_I )</annotation></semantics></math> that models the probability of the ground truth appearing in position <math alttext="x" class="ltx_Math" display="inline" id="S3.SS1.p1.7.m7.1"><semantics id="S3.SS1.p1.7.m7.1a"><mi id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><ci id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.7.m7.1d">italic_x</annotation></semantics></math> using a normalizing flow model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib52" title="">52</a>]</cite> with learnable parameters <math alttext="\Theta" class="ltx_Math" display="inline" id="S3.SS1.p1.8.m8.1"><semantics id="S3.SS1.p1.8.m8.1a"><mi id="S3.SS1.p1.8.m8.1.1" mathvariant="normal" xref="S3.SS1.p1.8.m8.1.1.cmml">Θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.1b"><ci id="S3.SS1.p1.8.m8.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1">Θ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">\Theta</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.8.m8.1d">roman_Θ</annotation></semantics></math>. The <math alttext="\hat{\mu}" class="ltx_Math" display="inline" id="S3.SS1.p1.9.m9.1"><semantics id="S3.SS1.p1.9.m9.1a"><mover accent="true" id="S3.SS1.p1.9.m9.1.1" xref="S3.SS1.p1.9.m9.1.1.cmml"><mi id="S3.SS1.p1.9.m9.1.1.2" xref="S3.SS1.p1.9.m9.1.1.2.cmml">μ</mi><mo id="S3.SS1.p1.9.m9.1.1.1" xref="S3.SS1.p1.9.m9.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m9.1b"><apply id="S3.SS1.p1.9.m9.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1"><ci id="S3.SS1.p1.9.m9.1.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1.1">^</ci><ci id="S3.SS1.p1.9.m9.1.1.2.cmml" xref="S3.SS1.p1.9.m9.1.1.2">𝜇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m9.1c">\hat{\mu}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.9.m9.1d">over^ start_ARG italic_μ end_ARG</annotation></semantics></math> produced by this module is refined in the next stages by leveraging cross-view and temporal information. The estimated <math alttext="\hat{\mu}" class="ltx_Math" display="inline" id="S3.SS1.p1.10.m10.1"><semantics id="S3.SS1.p1.10.m10.1a"><mover accent="true" id="S3.SS1.p1.10.m10.1.1" xref="S3.SS1.p1.10.m10.1.1.cmml"><mi id="S3.SS1.p1.10.m10.1.1.2" xref="S3.SS1.p1.10.m10.1.1.2.cmml">μ</mi><mo id="S3.SS1.p1.10.m10.1.1.1" xref="S3.SS1.p1.10.m10.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.10.m10.1b"><apply id="S3.SS1.p1.10.m10.1.1.cmml" xref="S3.SS1.p1.10.m10.1.1"><ci id="S3.SS1.p1.10.m10.1.1.1.cmml" xref="S3.SS1.p1.10.m10.1.1.1">^</ci><ci id="S3.SS1.p1.10.m10.1.1.2.cmml" xref="S3.SS1.p1.10.m10.1.1.2">𝜇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.10.m10.1c">\hat{\mu}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.10.m10.1d">over^ start_ARG italic_μ end_ARG</annotation></semantics></math>, <math alttext="\hat{\sigma}" class="ltx_Math" display="inline" id="S3.SS1.p1.11.m11.1"><semantics id="S3.SS1.p1.11.m11.1a"><mover accent="true" id="S3.SS1.p1.11.m11.1.1" xref="S3.SS1.p1.11.m11.1.1.cmml"><mi id="S3.SS1.p1.11.m11.1.1.2" xref="S3.SS1.p1.11.m11.1.1.2.cmml">σ</mi><mo id="S3.SS1.p1.11.m11.1.1.1" xref="S3.SS1.p1.11.m11.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.11.m11.1b"><apply id="S3.SS1.p1.11.m11.1.1.cmml" xref="S3.SS1.p1.11.m11.1.1"><ci id="S3.SS1.p1.11.m11.1.1.1.cmml" xref="S3.SS1.p1.11.m11.1.1.1">^</ci><ci id="S3.SS1.p1.11.m11.1.1.2.cmml" xref="S3.SS1.p1.11.m11.1.1.2">𝜎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.11.m11.1c">\hat{\sigma}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.11.m11.1d">over^ start_ARG italic_σ end_ARG</annotation></semantics></math>, along with the normalizing flow parameters <math alttext="\Theta" class="ltx_Math" display="inline" id="S3.SS1.p1.12.m12.1"><semantics id="S3.SS1.p1.12.m12.1a"><mi id="S3.SS1.p1.12.m12.1.1" mathvariant="normal" xref="S3.SS1.p1.12.m12.1.1.cmml">Θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.12.m12.1b"><ci id="S3.SS1.p1.12.m12.1.1.cmml" xref="S3.SS1.p1.12.m12.1.1">Θ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.12.m12.1c">\Theta</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.12.m12.1d">roman_Θ</annotation></semantics></math>, are used for 3D keypoint estimation in <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S3.SS4" title="3.4 3D Pose Estimation ‣ 3 Method ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.4</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Cross-view Projection</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.16">Next, we leverage the information from multiple camera views by projecting the 2D keypoints from one view to another using epipolar geometry. We derive a fundamental matrix <math alttext="F_{ij}\in\mathbb{R}^{3\times 3}" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><msub id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2.2" xref="S3.SS2.p1.1.m1.1.1.2.2.cmml">F</mi><mrow id="S3.SS2.p1.1.m1.1.1.2.3" xref="S3.SS2.p1.1.m1.1.1.2.3.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2.3.2" xref="S3.SS2.p1.1.m1.1.1.2.3.2.cmml">i</mi><mo id="S3.SS2.p1.1.m1.1.1.2.3.1" xref="S3.SS2.p1.1.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.SS2.p1.1.m1.1.1.2.3.3" xref="S3.SS2.p1.1.m1.1.1.2.3.3.cmml">j</mi></mrow></msub><mo id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.p1.1.m1.1.1.3.2" xref="S3.SS2.p1.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS2.p1.1.m1.1.1.3.3" xref="S3.SS2.p1.1.m1.1.1.3.3.cmml"><mn id="S3.SS2.p1.1.m1.1.1.3.3.2" xref="S3.SS2.p1.1.m1.1.1.3.3.2.cmml">3</mn><mo id="S3.SS2.p1.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.1.m1.1.1.3.3.1.cmml">×</mo><mn id="S3.SS2.p1.1.m1.1.1.3.3.3" xref="S3.SS2.p1.1.m1.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><in id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1"></in><apply id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.2.1.cmml" xref="S3.SS2.p1.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2.2">𝐹</ci><apply id="S3.SS2.p1.1.m1.1.1.2.3.cmml" xref="S3.SS2.p1.1.m1.1.1.2.3"><times id="S3.SS2.p1.1.m1.1.1.2.3.1.cmml" xref="S3.SS2.p1.1.m1.1.1.2.3.1"></times><ci id="S3.SS2.p1.1.m1.1.1.2.3.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2.3.2">𝑖</ci><ci id="S3.SS2.p1.1.m1.1.1.2.3.3.cmml" xref="S3.SS2.p1.1.m1.1.1.2.3.3">𝑗</ci></apply></apply><apply id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.p1.1.m1.1.1.3.2">ℝ</ci><apply id="S3.SS2.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3"><times id="S3.SS2.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3.1"></times><cn id="S3.SS2.p1.1.m1.1.1.3.3.2.cmml" type="integer" xref="S3.SS2.p1.1.m1.1.1.3.3.2">3</cn><cn id="S3.SS2.p1.1.m1.1.1.3.3.3.cmml" type="integer" xref="S3.SS2.p1.1.m1.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">F_{ij}\in\mathbb{R}^{3\times 3}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_F start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT 3 × 3 end_POSTSUPERSCRIPT</annotation></semantics></math> relating two camera views <math alttext="i,j\in\mathcal{V}" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.2"><semantics id="S3.SS2.p1.2.m2.2a"><mrow id="S3.SS2.p1.2.m2.2.3" xref="S3.SS2.p1.2.m2.2.3.cmml"><mrow id="S3.SS2.p1.2.m2.2.3.2.2" xref="S3.SS2.p1.2.m2.2.3.2.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">i</mi><mo id="S3.SS2.p1.2.m2.2.3.2.2.1" xref="S3.SS2.p1.2.m2.2.3.2.1.cmml">,</mo><mi id="S3.SS2.p1.2.m2.2.2" xref="S3.SS2.p1.2.m2.2.2.cmml">j</mi></mrow><mo id="S3.SS2.p1.2.m2.2.3.1" xref="S3.SS2.p1.2.m2.2.3.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.2.m2.2.3.3" xref="S3.SS2.p1.2.m2.2.3.3.cmml">𝒱</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.2b"><apply id="S3.SS2.p1.2.m2.2.3.cmml" xref="S3.SS2.p1.2.m2.2.3"><in id="S3.SS2.p1.2.m2.2.3.1.cmml" xref="S3.SS2.p1.2.m2.2.3.1"></in><list id="S3.SS2.p1.2.m2.2.3.2.1.cmml" xref="S3.SS2.p1.2.m2.2.3.2.2"><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">𝑖</ci><ci id="S3.SS2.p1.2.m2.2.2.cmml" xref="S3.SS2.p1.2.m2.2.2">𝑗</ci></list><ci id="S3.SS2.p1.2.m2.2.3.3.cmml" xref="S3.SS2.p1.2.m2.2.3.3">𝒱</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.2c">i,j\in\mathcal{V}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.2d">italic_i , italic_j ∈ caligraphic_V</annotation></semantics></math> from known intrinsic and extrinsic camera parameters. For each predicted keypoint <math alttext="\hat{\mu}_{j}" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3.1"><semantics id="S3.SS2.p1.3.m3.1a"><msub id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml"><mover accent="true" id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml"><mi id="S3.SS2.p1.3.m3.1.1.2.2" xref="S3.SS2.p1.3.m3.1.1.2.2.cmml">μ</mi><mo id="S3.SS2.p1.3.m3.1.1.2.1" xref="S3.SS2.p1.3.m3.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">subscript</csymbol><apply id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2"><ci id="S3.SS2.p1.3.m3.1.1.2.1.cmml" xref="S3.SS2.p1.3.m3.1.1.2.1">^</ci><ci id="S3.SS2.p1.3.m3.1.1.2.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2.2">𝜇</ci></apply><ci id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">\hat{\mu}_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.3.m3.1d">over^ start_ARG italic_μ end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> in view <math alttext="j" class="ltx_Math" display="inline" id="S3.SS2.p1.4.m4.1"><semantics id="S3.SS2.p1.4.m4.1a"><mi id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><ci id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">j</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.4.m4.1d">italic_j</annotation></semantics></math>, we obtain the epipolar line in the reference view <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.p1.5.m5.1"><semantics id="S3.SS2.p1.5.m5.1a"><mi id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><ci id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.5.m5.1d">italic_i</annotation></semantics></math> by <math alttext="I_{ij}=F_{ij}^{\intercal}\hat{\mu}_{j}" class="ltx_Math" display="inline" id="S3.SS2.p1.6.m6.1"><semantics id="S3.SS2.p1.6.m6.1a"><mrow id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml"><msub id="S3.SS2.p1.6.m6.1.1.2" xref="S3.SS2.p1.6.m6.1.1.2.cmml"><mi id="S3.SS2.p1.6.m6.1.1.2.2" xref="S3.SS2.p1.6.m6.1.1.2.2.cmml">I</mi><mrow id="S3.SS2.p1.6.m6.1.1.2.3" xref="S3.SS2.p1.6.m6.1.1.2.3.cmml"><mi id="S3.SS2.p1.6.m6.1.1.2.3.2" xref="S3.SS2.p1.6.m6.1.1.2.3.2.cmml">i</mi><mo id="S3.SS2.p1.6.m6.1.1.2.3.1" xref="S3.SS2.p1.6.m6.1.1.2.3.1.cmml">⁢</mo><mi id="S3.SS2.p1.6.m6.1.1.2.3.3" xref="S3.SS2.p1.6.m6.1.1.2.3.3.cmml">j</mi></mrow></msub><mo id="S3.SS2.p1.6.m6.1.1.1" xref="S3.SS2.p1.6.m6.1.1.1.cmml">=</mo><mrow id="S3.SS2.p1.6.m6.1.1.3" xref="S3.SS2.p1.6.m6.1.1.3.cmml"><msubsup id="S3.SS2.p1.6.m6.1.1.3.2" xref="S3.SS2.p1.6.m6.1.1.3.2.cmml"><mi id="S3.SS2.p1.6.m6.1.1.3.2.2.2" xref="S3.SS2.p1.6.m6.1.1.3.2.2.2.cmml">F</mi><mrow id="S3.SS2.p1.6.m6.1.1.3.2.2.3" xref="S3.SS2.p1.6.m6.1.1.3.2.2.3.cmml"><mi id="S3.SS2.p1.6.m6.1.1.3.2.2.3.2" xref="S3.SS2.p1.6.m6.1.1.3.2.2.3.2.cmml">i</mi><mo id="S3.SS2.p1.6.m6.1.1.3.2.2.3.1" xref="S3.SS2.p1.6.m6.1.1.3.2.2.3.1.cmml">⁢</mo><mi id="S3.SS2.p1.6.m6.1.1.3.2.2.3.3" xref="S3.SS2.p1.6.m6.1.1.3.2.2.3.3.cmml">j</mi></mrow><mo id="S3.SS2.p1.6.m6.1.1.3.2.3" xref="S3.SS2.p1.6.m6.1.1.3.2.3.cmml">⊺</mo></msubsup><mo id="S3.SS2.p1.6.m6.1.1.3.1" xref="S3.SS2.p1.6.m6.1.1.3.1.cmml">⁢</mo><msub id="S3.SS2.p1.6.m6.1.1.3.3" xref="S3.SS2.p1.6.m6.1.1.3.3.cmml"><mover accent="true" id="S3.SS2.p1.6.m6.1.1.3.3.2" xref="S3.SS2.p1.6.m6.1.1.3.3.2.cmml"><mi id="S3.SS2.p1.6.m6.1.1.3.3.2.2" xref="S3.SS2.p1.6.m6.1.1.3.3.2.2.cmml">μ</mi><mo id="S3.SS2.p1.6.m6.1.1.3.3.2.1" xref="S3.SS2.p1.6.m6.1.1.3.3.2.1.cmml">^</mo></mover><mi id="S3.SS2.p1.6.m6.1.1.3.3.3" xref="S3.SS2.p1.6.m6.1.1.3.3.3.cmml">j</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><apply id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1"><eq id="S3.SS2.p1.6.m6.1.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1.1"></eq><apply id="S3.SS2.p1.6.m6.1.1.2.cmml" xref="S3.SS2.p1.6.m6.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.1.1.2.1.cmml" xref="S3.SS2.p1.6.m6.1.1.2">subscript</csymbol><ci id="S3.SS2.p1.6.m6.1.1.2.2.cmml" xref="S3.SS2.p1.6.m6.1.1.2.2">𝐼</ci><apply id="S3.SS2.p1.6.m6.1.1.2.3.cmml" xref="S3.SS2.p1.6.m6.1.1.2.3"><times id="S3.SS2.p1.6.m6.1.1.2.3.1.cmml" xref="S3.SS2.p1.6.m6.1.1.2.3.1"></times><ci id="S3.SS2.p1.6.m6.1.1.2.3.2.cmml" xref="S3.SS2.p1.6.m6.1.1.2.3.2">𝑖</ci><ci id="S3.SS2.p1.6.m6.1.1.2.3.3.cmml" xref="S3.SS2.p1.6.m6.1.1.2.3.3">𝑗</ci></apply></apply><apply id="S3.SS2.p1.6.m6.1.1.3.cmml" xref="S3.SS2.p1.6.m6.1.1.3"><times id="S3.SS2.p1.6.m6.1.1.3.1.cmml" xref="S3.SS2.p1.6.m6.1.1.3.1"></times><apply id="S3.SS2.p1.6.m6.1.1.3.2.cmml" xref="S3.SS2.p1.6.m6.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.1.1.3.2.1.cmml" xref="S3.SS2.p1.6.m6.1.1.3.2">superscript</csymbol><apply id="S3.SS2.p1.6.m6.1.1.3.2.2.cmml" xref="S3.SS2.p1.6.m6.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.1.1.3.2.2.1.cmml" xref="S3.SS2.p1.6.m6.1.1.3.2">subscript</csymbol><ci id="S3.SS2.p1.6.m6.1.1.3.2.2.2.cmml" xref="S3.SS2.p1.6.m6.1.1.3.2.2.2">𝐹</ci><apply id="S3.SS2.p1.6.m6.1.1.3.2.2.3.cmml" xref="S3.SS2.p1.6.m6.1.1.3.2.2.3"><times id="S3.SS2.p1.6.m6.1.1.3.2.2.3.1.cmml" xref="S3.SS2.p1.6.m6.1.1.3.2.2.3.1"></times><ci id="S3.SS2.p1.6.m6.1.1.3.2.2.3.2.cmml" xref="S3.SS2.p1.6.m6.1.1.3.2.2.3.2">𝑖</ci><ci id="S3.SS2.p1.6.m6.1.1.3.2.2.3.3.cmml" xref="S3.SS2.p1.6.m6.1.1.3.2.2.3.3">𝑗</ci></apply></apply><ci id="S3.SS2.p1.6.m6.1.1.3.2.3.cmml" xref="S3.SS2.p1.6.m6.1.1.3.2.3">⊺</ci></apply><apply id="S3.SS2.p1.6.m6.1.1.3.3.cmml" xref="S3.SS2.p1.6.m6.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.1.1.3.3.1.cmml" xref="S3.SS2.p1.6.m6.1.1.3.3">subscript</csymbol><apply id="S3.SS2.p1.6.m6.1.1.3.3.2.cmml" xref="S3.SS2.p1.6.m6.1.1.3.3.2"><ci id="S3.SS2.p1.6.m6.1.1.3.3.2.1.cmml" xref="S3.SS2.p1.6.m6.1.1.3.3.2.1">^</ci><ci id="S3.SS2.p1.6.m6.1.1.3.3.2.2.cmml" xref="S3.SS2.p1.6.m6.1.1.3.3.2.2">𝜇</ci></apply><ci id="S3.SS2.p1.6.m6.1.1.3.3.3.cmml" xref="S3.SS2.p1.6.m6.1.1.3.3.3">𝑗</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">I_{ij}=F_{ij}^{\intercal}\hat{\mu}_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.6.m6.1d">italic_I start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = italic_F start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊺ end_POSTSUPERSCRIPT over^ start_ARG italic_μ end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib12" title="">12</a>]</cite>. Next, we find the closest point on <math alttext="I_{ij}" class="ltx_Math" display="inline" id="S3.SS2.p1.7.m7.1"><semantics id="S3.SS2.p1.7.m7.1a"><msub id="S3.SS2.p1.7.m7.1.1" xref="S3.SS2.p1.7.m7.1.1.cmml"><mi id="S3.SS2.p1.7.m7.1.1.2" xref="S3.SS2.p1.7.m7.1.1.2.cmml">I</mi><mrow id="S3.SS2.p1.7.m7.1.1.3" xref="S3.SS2.p1.7.m7.1.1.3.cmml"><mi id="S3.SS2.p1.7.m7.1.1.3.2" xref="S3.SS2.p1.7.m7.1.1.3.2.cmml">i</mi><mo id="S3.SS2.p1.7.m7.1.1.3.1" xref="S3.SS2.p1.7.m7.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p1.7.m7.1.1.3.3" xref="S3.SS2.p1.7.m7.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m7.1b"><apply id="S3.SS2.p1.7.m7.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.7.m7.1.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1">subscript</csymbol><ci id="S3.SS2.p1.7.m7.1.1.2.cmml" xref="S3.SS2.p1.7.m7.1.1.2">𝐼</ci><apply id="S3.SS2.p1.7.m7.1.1.3.cmml" xref="S3.SS2.p1.7.m7.1.1.3"><times id="S3.SS2.p1.7.m7.1.1.3.1.cmml" xref="S3.SS2.p1.7.m7.1.1.3.1"></times><ci id="S3.SS2.p1.7.m7.1.1.3.2.cmml" xref="S3.SS2.p1.7.m7.1.1.3.2">𝑖</ci><ci id="S3.SS2.p1.7.m7.1.1.3.3.cmml" xref="S3.SS2.p1.7.m7.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m7.1c">I_{ij}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.7.m7.1d">italic_I start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT</annotation></semantics></math> to the keypoint <math alttext="\hat{\mu}_{i}" class="ltx_Math" display="inline" id="S3.SS2.p1.8.m8.1"><semantics id="S3.SS2.p1.8.m8.1a"><msub id="S3.SS2.p1.8.m8.1.1" xref="S3.SS2.p1.8.m8.1.1.cmml"><mover accent="true" id="S3.SS2.p1.8.m8.1.1.2" xref="S3.SS2.p1.8.m8.1.1.2.cmml"><mi id="S3.SS2.p1.8.m8.1.1.2.2" xref="S3.SS2.p1.8.m8.1.1.2.2.cmml">μ</mi><mo id="S3.SS2.p1.8.m8.1.1.2.1" xref="S3.SS2.p1.8.m8.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS2.p1.8.m8.1.1.3" xref="S3.SS2.p1.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.8.m8.1b"><apply id="S3.SS2.p1.8.m8.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.8.m8.1.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1">subscript</csymbol><apply id="S3.SS2.p1.8.m8.1.1.2.cmml" xref="S3.SS2.p1.8.m8.1.1.2"><ci id="S3.SS2.p1.8.m8.1.1.2.1.cmml" xref="S3.SS2.p1.8.m8.1.1.2.1">^</ci><ci id="S3.SS2.p1.8.m8.1.1.2.2.cmml" xref="S3.SS2.p1.8.m8.1.1.2.2">𝜇</ci></apply><ci id="S3.SS2.p1.8.m8.1.1.3.cmml" xref="S3.SS2.p1.8.m8.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.8.m8.1c">\hat{\mu}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.8.m8.1d">over^ start_ARG italic_μ end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> in view <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.p1.9.m9.1"><semantics id="S3.SS2.p1.9.m9.1a"><mi id="S3.SS2.p1.9.m9.1.1" xref="S3.SS2.p1.9.m9.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.9.m9.1b"><ci id="S3.SS2.p1.9.m9.1.1.cmml" xref="S3.SS2.p1.9.m9.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.9.m9.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.9.m9.1d">italic_i</annotation></semantics></math>. This point represents the projection of the keypoint from view <math alttext="j" class="ltx_Math" display="inline" id="S3.SS2.p1.10.m10.1"><semantics id="S3.SS2.p1.10.m10.1a"><mi id="S3.SS2.p1.10.m10.1.1" xref="S3.SS2.p1.10.m10.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.10.m10.1b"><ci id="S3.SS2.p1.10.m10.1.1.cmml" xref="S3.SS2.p1.10.m10.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.10.m10.1c">j</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.10.m10.1d">italic_j</annotation></semantics></math> onto the reference view <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.p1.11.m11.1"><semantics id="S3.SS2.p1.11.m11.1a"><mi id="S3.SS2.p1.11.m11.1.1" xref="S3.SS2.p1.11.m11.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.11.m11.1b"><ci id="S3.SS2.p1.11.m11.1.1.cmml" xref="S3.SS2.p1.11.m11.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.11.m11.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.11.m11.1d">italic_i</annotation></semantics></math>. By repeating this cross-view projection for all available views <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.p1.12.m12.1"><semantics id="S3.SS2.p1.12.m12.1a"><mi id="S3.SS2.p1.12.m12.1.1" xref="S3.SS2.p1.12.m12.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.12.m12.1b"><ci id="S3.SS2.p1.12.m12.1.1.cmml" xref="S3.SS2.p1.12.m12.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.12.m12.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.12.m12.1d">italic_i</annotation></semantics></math>, joints <math alttext="k\in\mathcal{J}=\{1,2,\dots,J\}" class="ltx_Math" display="inline" id="S3.SS2.p1.13.m13.4"><semantics id="S3.SS2.p1.13.m13.4a"><mrow id="S3.SS2.p1.13.m13.4.5" xref="S3.SS2.p1.13.m13.4.5.cmml"><mi id="S3.SS2.p1.13.m13.4.5.2" xref="S3.SS2.p1.13.m13.4.5.2.cmml">k</mi><mo id="S3.SS2.p1.13.m13.4.5.3" xref="S3.SS2.p1.13.m13.4.5.3.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.13.m13.4.5.4" xref="S3.SS2.p1.13.m13.4.5.4.cmml">𝒥</mi><mo id="S3.SS2.p1.13.m13.4.5.5" xref="S3.SS2.p1.13.m13.4.5.5.cmml">=</mo><mrow id="S3.SS2.p1.13.m13.4.5.6.2" xref="S3.SS2.p1.13.m13.4.5.6.1.cmml"><mo id="S3.SS2.p1.13.m13.4.5.6.2.1" stretchy="false" xref="S3.SS2.p1.13.m13.4.5.6.1.cmml">{</mo><mn id="S3.SS2.p1.13.m13.1.1" xref="S3.SS2.p1.13.m13.1.1.cmml">1</mn><mo id="S3.SS2.p1.13.m13.4.5.6.2.2" xref="S3.SS2.p1.13.m13.4.5.6.1.cmml">,</mo><mn id="S3.SS2.p1.13.m13.2.2" xref="S3.SS2.p1.13.m13.2.2.cmml">2</mn><mo id="S3.SS2.p1.13.m13.4.5.6.2.3" xref="S3.SS2.p1.13.m13.4.5.6.1.cmml">,</mo><mi id="S3.SS2.p1.13.m13.3.3" mathvariant="normal" xref="S3.SS2.p1.13.m13.3.3.cmml">…</mi><mo id="S3.SS2.p1.13.m13.4.5.6.2.4" xref="S3.SS2.p1.13.m13.4.5.6.1.cmml">,</mo><mi id="S3.SS2.p1.13.m13.4.4" xref="S3.SS2.p1.13.m13.4.4.cmml">J</mi><mo id="S3.SS2.p1.13.m13.4.5.6.2.5" stretchy="false" xref="S3.SS2.p1.13.m13.4.5.6.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.13.m13.4b"><apply id="S3.SS2.p1.13.m13.4.5.cmml" xref="S3.SS2.p1.13.m13.4.5"><and id="S3.SS2.p1.13.m13.4.5a.cmml" xref="S3.SS2.p1.13.m13.4.5"></and><apply id="S3.SS2.p1.13.m13.4.5b.cmml" xref="S3.SS2.p1.13.m13.4.5"><in id="S3.SS2.p1.13.m13.4.5.3.cmml" xref="S3.SS2.p1.13.m13.4.5.3"></in><ci id="S3.SS2.p1.13.m13.4.5.2.cmml" xref="S3.SS2.p1.13.m13.4.5.2">𝑘</ci><ci id="S3.SS2.p1.13.m13.4.5.4.cmml" xref="S3.SS2.p1.13.m13.4.5.4">𝒥</ci></apply><apply id="S3.SS2.p1.13.m13.4.5c.cmml" xref="S3.SS2.p1.13.m13.4.5"><eq id="S3.SS2.p1.13.m13.4.5.5.cmml" xref="S3.SS2.p1.13.m13.4.5.5"></eq><share href="https://arxiv.org/html/2404.14634v3#S3.SS2.p1.13.m13.4.5.4.cmml" id="S3.SS2.p1.13.m13.4.5d.cmml" xref="S3.SS2.p1.13.m13.4.5"></share><set id="S3.SS2.p1.13.m13.4.5.6.1.cmml" xref="S3.SS2.p1.13.m13.4.5.6.2"><cn id="S3.SS2.p1.13.m13.1.1.cmml" type="integer" xref="S3.SS2.p1.13.m13.1.1">1</cn><cn id="S3.SS2.p1.13.m13.2.2.cmml" type="integer" xref="S3.SS2.p1.13.m13.2.2">2</cn><ci id="S3.SS2.p1.13.m13.3.3.cmml" xref="S3.SS2.p1.13.m13.3.3">…</ci><ci id="S3.SS2.p1.13.m13.4.4.cmml" xref="S3.SS2.p1.13.m13.4.4">𝐽</ci></set></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.13.m13.4c">k\in\mathcal{J}=\{1,2,\dots,J\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.13.m13.4d">italic_k ∈ caligraphic_J = { 1 , 2 , … , italic_J }</annotation></semantics></math>, and time frames <math alttext="t" class="ltx_Math" display="inline" id="S3.SS2.p1.14.m14.1"><semantics id="S3.SS2.p1.14.m14.1a"><mi id="S3.SS2.p1.14.m14.1.1" xref="S3.SS2.p1.14.m14.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.14.m14.1b"><ci id="S3.SS2.p1.14.m14.1.1.cmml" xref="S3.SS2.p1.14.m14.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.14.m14.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.14.m14.1d">italic_t</annotation></semantics></math>, we create 2D point clouds <math alttext="\mathcal{C}_{i,k,t}" class="ltx_Math" display="inline" id="S3.SS2.p1.15.m15.3"><semantics id="S3.SS2.p1.15.m15.3a"><msub id="S3.SS2.p1.15.m15.3.4" xref="S3.SS2.p1.15.m15.3.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.15.m15.3.4.2" xref="S3.SS2.p1.15.m15.3.4.2.cmml">𝒞</mi><mrow id="S3.SS2.p1.15.m15.3.3.3.5" xref="S3.SS2.p1.15.m15.3.3.3.4.cmml"><mi id="S3.SS2.p1.15.m15.1.1.1.1" xref="S3.SS2.p1.15.m15.1.1.1.1.cmml">i</mi><mo id="S3.SS2.p1.15.m15.3.3.3.5.1" xref="S3.SS2.p1.15.m15.3.3.3.4.cmml">,</mo><mi id="S3.SS2.p1.15.m15.2.2.2.2" xref="S3.SS2.p1.15.m15.2.2.2.2.cmml">k</mi><mo id="S3.SS2.p1.15.m15.3.3.3.5.2" xref="S3.SS2.p1.15.m15.3.3.3.4.cmml">,</mo><mi id="S3.SS2.p1.15.m15.3.3.3.3" xref="S3.SS2.p1.15.m15.3.3.3.3.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.15.m15.3b"><apply id="S3.SS2.p1.15.m15.3.4.cmml" xref="S3.SS2.p1.15.m15.3.4"><csymbol cd="ambiguous" id="S3.SS2.p1.15.m15.3.4.1.cmml" xref="S3.SS2.p1.15.m15.3.4">subscript</csymbol><ci id="S3.SS2.p1.15.m15.3.4.2.cmml" xref="S3.SS2.p1.15.m15.3.4.2">𝒞</ci><list id="S3.SS2.p1.15.m15.3.3.3.4.cmml" xref="S3.SS2.p1.15.m15.3.3.3.5"><ci id="S3.SS2.p1.15.m15.1.1.1.1.cmml" xref="S3.SS2.p1.15.m15.1.1.1.1">𝑖</ci><ci id="S3.SS2.p1.15.m15.2.2.2.2.cmml" xref="S3.SS2.p1.15.m15.2.2.2.2">𝑘</ci><ci id="S3.SS2.p1.15.m15.3.3.3.3.cmml" xref="S3.SS2.p1.15.m15.3.3.3.3">𝑡</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.15.m15.3c">\mathcal{C}_{i,k,t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.15.m15.3d">caligraphic_C start_POSTSUBSCRIPT italic_i , italic_k , italic_t end_POSTSUBSCRIPT</annotation></semantics></math> containing the projected keypoint. Finally, for consistency between various scales and views, we normalize the elements of <math alttext="\mathcal{C}" class="ltx_Math" display="inline" id="S3.SS2.p1.16.m16.1"><semantics id="S3.SS2.p1.16.m16.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.16.m16.1.1" xref="S3.SS2.p1.16.m16.1.1.cmml">𝒞</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.16.m16.1b"><ci id="S3.SS2.p1.16.m16.1.1.cmml" xref="S3.SS2.p1.16.m16.1.1">𝒞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.16.m16.1c">\mathcal{C}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.16.m16.1d">caligraphic_C</annotation></semantics></math> using the subject’s bounding box within their reference view.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="329" id="S3.F2.g1" src="x2.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.12.6.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S3.F2.10.5" style="font-size:90%;">Architecture of the proposed pose compiler module consisting of a point cloud encoder and a spatiotemporal encoder with criss-cross attention. Tensor sizes depend on the batch size <math alttext="B" class="ltx_Math" display="inline" id="S3.F2.6.1.m1.1"><semantics id="S3.F2.6.1.m1.1b"><mi id="S3.F2.6.1.m1.1.1" xref="S3.F2.6.1.m1.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.F2.6.1.m1.1c"><ci id="S3.F2.6.1.m1.1.1.cmml" xref="S3.F2.6.1.m1.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.6.1.m1.1d">B</annotation><annotation encoding="application/x-llamapun" id="S3.F2.6.1.m1.1e">italic_B</annotation></semantics></math>, temporal window length <math alttext="T" class="ltx_Math" display="inline" id="S3.F2.7.2.m2.1"><semantics id="S3.F2.7.2.m2.1b"><mi id="S3.F2.7.2.m2.1.1" xref="S3.F2.7.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.F2.7.2.m2.1c"><ci id="S3.F2.7.2.m2.1.1.cmml" xref="S3.F2.7.2.m2.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.7.2.m2.1d">T</annotation><annotation encoding="application/x-llamapun" id="S3.F2.7.2.m2.1e">italic_T</annotation></semantics></math>, number of joints <math alttext="J" class="ltx_Math" display="inline" id="S3.F2.8.3.m3.1"><semantics id="S3.F2.8.3.m3.1b"><mi id="S3.F2.8.3.m3.1.1" xref="S3.F2.8.3.m3.1.1.cmml">J</mi><annotation-xml encoding="MathML-Content" id="S3.F2.8.3.m3.1c"><ci id="S3.F2.8.3.m3.1.1.cmml" xref="S3.F2.8.3.m3.1.1">𝐽</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.8.3.m3.1d">J</annotation><annotation encoding="application/x-llamapun" id="S3.F2.8.3.m3.1e">italic_J</annotation></semantics></math>, camera views <math alttext="V" class="ltx_Math" display="inline" id="S3.F2.9.4.m4.1"><semantics id="S3.F2.9.4.m4.1b"><mi id="S3.F2.9.4.m4.1.1" xref="S3.F2.9.4.m4.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.F2.9.4.m4.1c"><ci id="S3.F2.9.4.m4.1.1.cmml" xref="S3.F2.9.4.m4.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.9.4.m4.1d">V</annotation><annotation encoding="application/x-llamapun" id="S3.F2.9.4.m4.1e">italic_V</annotation></semantics></math> and the point cloud feature dimensionality <math alttext="H" class="ltx_Math" display="inline" id="S3.F2.10.5.m5.1"><semantics id="S3.F2.10.5.m5.1b"><mi id="S3.F2.10.5.m5.1.1" xref="S3.F2.10.5.m5.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S3.F2.10.5.m5.1c"><ci id="S3.F2.10.5.m5.1.1.cmml" xref="S3.F2.10.5.m5.1.1">𝐻</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.10.5.m5.1d">H</annotation><annotation encoding="application/x-llamapun" id="S3.F2.10.5.m5.1e">italic_H</annotation></semantics></math>.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Pose Compiler</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.2">The pose compiler module aggregates multi-view information embedded in the 2D point clouds and leverages temporal information (<em class="ltx_emph ltx_font_italic" id="S3.SS3.p1.2.1">i.e</em>.<span class="ltx_text" id="S3.SS3.p1.2.2"></span>, joint positions across time). As shown in <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S2.F1" title="In 2 Related Works ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>, the pose compiler consists of 2 main parts: a point cloud encoder and a spatiotemporal encoder. The point cloud encoder first extracts a feature vector <math alttext="f_{i,k,t}" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.3"><semantics id="S3.SS3.p1.1.m1.3a"><msub id="S3.SS3.p1.1.m1.3.4" xref="S3.SS3.p1.1.m1.3.4.cmml"><mi id="S3.SS3.p1.1.m1.3.4.2" xref="S3.SS3.p1.1.m1.3.4.2.cmml">f</mi><mrow id="S3.SS3.p1.1.m1.3.3.3.5" xref="S3.SS3.p1.1.m1.3.3.3.4.cmml"><mi id="S3.SS3.p1.1.m1.1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.1.cmml">i</mi><mo id="S3.SS3.p1.1.m1.3.3.3.5.1" xref="S3.SS3.p1.1.m1.3.3.3.4.cmml">,</mo><mi id="S3.SS3.p1.1.m1.2.2.2.2" xref="S3.SS3.p1.1.m1.2.2.2.2.cmml">k</mi><mo id="S3.SS3.p1.1.m1.3.3.3.5.2" xref="S3.SS3.p1.1.m1.3.3.3.4.cmml">,</mo><mi id="S3.SS3.p1.1.m1.3.3.3.3" xref="S3.SS3.p1.1.m1.3.3.3.3.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.3b"><apply id="S3.SS3.p1.1.m1.3.4.cmml" xref="S3.SS3.p1.1.m1.3.4"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.3.4.1.cmml" xref="S3.SS3.p1.1.m1.3.4">subscript</csymbol><ci id="S3.SS3.p1.1.m1.3.4.2.cmml" xref="S3.SS3.p1.1.m1.3.4.2">𝑓</ci><list id="S3.SS3.p1.1.m1.3.3.3.4.cmml" xref="S3.SS3.p1.1.m1.3.3.3.5"><ci id="S3.SS3.p1.1.m1.1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1">𝑖</ci><ci id="S3.SS3.p1.1.m1.2.2.2.2.cmml" xref="S3.SS3.p1.1.m1.2.2.2.2">𝑘</ci><ci id="S3.SS3.p1.1.m1.3.3.3.3.cmml" xref="S3.SS3.p1.1.m1.3.3.3.3">𝑡</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.3c">f_{i,k,t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.3d">italic_f start_POSTSUBSCRIPT italic_i , italic_k , italic_t end_POSTSUBSCRIPT</annotation></semantics></math> that describes a point cloud <math alttext="\mathcal{C}_{i,k,t}" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2.3"><semantics id="S3.SS3.p1.2.m2.3a"><msub id="S3.SS3.p1.2.m2.3.4" xref="S3.SS3.p1.2.m2.3.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p1.2.m2.3.4.2" xref="S3.SS3.p1.2.m2.3.4.2.cmml">𝒞</mi><mrow id="S3.SS3.p1.2.m2.3.3.3.5" xref="S3.SS3.p1.2.m2.3.3.3.4.cmml"><mi id="S3.SS3.p1.2.m2.1.1.1.1" xref="S3.SS3.p1.2.m2.1.1.1.1.cmml">i</mi><mo id="S3.SS3.p1.2.m2.3.3.3.5.1" xref="S3.SS3.p1.2.m2.3.3.3.4.cmml">,</mo><mi id="S3.SS3.p1.2.m2.2.2.2.2" xref="S3.SS3.p1.2.m2.2.2.2.2.cmml">k</mi><mo id="S3.SS3.p1.2.m2.3.3.3.5.2" xref="S3.SS3.p1.2.m2.3.3.3.4.cmml">,</mo><mi id="S3.SS3.p1.2.m2.3.3.3.3" xref="S3.SS3.p1.2.m2.3.3.3.3.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.3b"><apply id="S3.SS3.p1.2.m2.3.4.cmml" xref="S3.SS3.p1.2.m2.3.4"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.3.4.1.cmml" xref="S3.SS3.p1.2.m2.3.4">subscript</csymbol><ci id="S3.SS3.p1.2.m2.3.4.2.cmml" xref="S3.SS3.p1.2.m2.3.4.2">𝒞</ci><list id="S3.SS3.p1.2.m2.3.3.3.4.cmml" xref="S3.SS3.p1.2.m2.3.3.3.5"><ci id="S3.SS3.p1.2.m2.1.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1">𝑖</ci><ci id="S3.SS3.p1.2.m2.2.2.2.2.cmml" xref="S3.SS3.p1.2.m2.2.2.2.2">𝑘</ci><ci id="S3.SS3.p1.2.m2.3.3.3.3.cmml" xref="S3.SS3.p1.2.m2.3.3.3.3">𝑡</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.3c">\mathcal{C}_{i,k,t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m2.3d">caligraphic_C start_POSTSUBSCRIPT italic_i , italic_k , italic_t end_POSTSUBSCRIPT</annotation></semantics></math>. Our implementation is inspired by the naïve Point Cloud Transformers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib51" title="">51</a>]</cite>, but we modify its architecture to preserve coordinate information. Specifically, we use multi-head attention and a residual connection from the features of the reference view to the output of the last max-pooling layer. For the detailed architecture of our point cloud encoder, please refer to <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S3.F2" title="In 3.2 Cross-view Projection ‣ 3 Method ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.9">Next, we concatenate the feature vectors from every frame and joint in each reference view <math alttext="\{\textbf{f}_{i}\}" class="ltx_Math" display="inline" id="S3.SS3.p2.1.m1.1"><semantics id="S3.SS3.p2.1.m1.1a"><mrow id="S3.SS3.p2.1.m1.1.1.1" xref="S3.SS3.p2.1.m1.1.1.2.cmml"><mo id="S3.SS3.p2.1.m1.1.1.1.2" stretchy="false" xref="S3.SS3.p2.1.m1.1.1.2.cmml">{</mo><msub id="S3.SS3.p2.1.m1.1.1.1.1" xref="S3.SS3.p2.1.m1.1.1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S3.SS3.p2.1.m1.1.1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.1.1.2a.cmml">f</mtext><mi id="S3.SS3.p2.1.m1.1.1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS3.p2.1.m1.1.1.1.3" stretchy="false" xref="S3.SS3.p2.1.m1.1.1.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><set id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.1"><apply id="S3.SS3.p2.1.m1.1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.1.1.2a.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1.2"><mtext class="ltx_mathvariant_bold" id="S3.SS3.p2.1.m1.1.1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1.2">f</mtext></ci><ci id="S3.SS3.p2.1.m1.1.1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1.3">𝑖</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">\{\textbf{f}_{i}\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.1.m1.1d">{ f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }</annotation></semantics></math> with temporal and spatial position embeddings, <em class="ltx_emph ltx_font_italic" id="S3.SS3.p2.9.1">i.e</em>.<span class="ltx_text" id="S3.SS3.p2.9.2"></span>, frame and skeleton joint ID. The results are then passed to our spatiotemporal encoder that implements an RLE head to provide position <math alttext="\hat{\mu}^{\prime}_{i,k,t}" class="ltx_Math" display="inline" id="S3.SS3.p2.2.m2.3"><semantics id="S3.SS3.p2.2.m2.3a"><msubsup id="S3.SS3.p2.2.m2.3.4" xref="S3.SS3.p2.2.m2.3.4.cmml"><mover accent="true" id="S3.SS3.p2.2.m2.3.4.2.2" xref="S3.SS3.p2.2.m2.3.4.2.2.cmml"><mi id="S3.SS3.p2.2.m2.3.4.2.2.2" xref="S3.SS3.p2.2.m2.3.4.2.2.2.cmml">μ</mi><mo id="S3.SS3.p2.2.m2.3.4.2.2.1" xref="S3.SS3.p2.2.m2.3.4.2.2.1.cmml">^</mo></mover><mrow id="S3.SS3.p2.2.m2.3.3.3.5" xref="S3.SS3.p2.2.m2.3.3.3.4.cmml"><mi id="S3.SS3.p2.2.m2.1.1.1.1" xref="S3.SS3.p2.2.m2.1.1.1.1.cmml">i</mi><mo id="S3.SS3.p2.2.m2.3.3.3.5.1" xref="S3.SS3.p2.2.m2.3.3.3.4.cmml">,</mo><mi id="S3.SS3.p2.2.m2.2.2.2.2" xref="S3.SS3.p2.2.m2.2.2.2.2.cmml">k</mi><mo id="S3.SS3.p2.2.m2.3.3.3.5.2" xref="S3.SS3.p2.2.m2.3.3.3.4.cmml">,</mo><mi id="S3.SS3.p2.2.m2.3.3.3.3" xref="S3.SS3.p2.2.m2.3.3.3.3.cmml">t</mi></mrow><mo id="S3.SS3.p2.2.m2.3.4.2.3" xref="S3.SS3.p2.2.m2.3.4.2.3.cmml">′</mo></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.3b"><apply id="S3.SS3.p2.2.m2.3.4.cmml" xref="S3.SS3.p2.2.m2.3.4"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.3.4.1.cmml" xref="S3.SS3.p2.2.m2.3.4">subscript</csymbol><apply id="S3.SS3.p2.2.m2.3.4.2.cmml" xref="S3.SS3.p2.2.m2.3.4"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.3.4.2.1.cmml" xref="S3.SS3.p2.2.m2.3.4">superscript</csymbol><apply id="S3.SS3.p2.2.m2.3.4.2.2.cmml" xref="S3.SS3.p2.2.m2.3.4.2.2"><ci id="S3.SS3.p2.2.m2.3.4.2.2.1.cmml" xref="S3.SS3.p2.2.m2.3.4.2.2.1">^</ci><ci id="S3.SS3.p2.2.m2.3.4.2.2.2.cmml" xref="S3.SS3.p2.2.m2.3.4.2.2.2">𝜇</ci></apply><ci id="S3.SS3.p2.2.m2.3.4.2.3.cmml" xref="S3.SS3.p2.2.m2.3.4.2.3">′</ci></apply><list id="S3.SS3.p2.2.m2.3.3.3.4.cmml" xref="S3.SS3.p2.2.m2.3.3.3.5"><ci id="S3.SS3.p2.2.m2.1.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1.1.1">𝑖</ci><ci id="S3.SS3.p2.2.m2.2.2.2.2.cmml" xref="S3.SS3.p2.2.m2.2.2.2.2">𝑘</ci><ci id="S3.SS3.p2.2.m2.3.3.3.3.cmml" xref="S3.SS3.p2.2.m2.3.3.3.3">𝑡</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.3c">\hat{\mu}^{\prime}_{i,k,t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.2.m2.3d">over^ start_ARG italic_μ end_ARG start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i , italic_k , italic_t end_POSTSUBSCRIPT</annotation></semantics></math> and uncertainty <math alttext="\hat{\sigma}^{\prime}_{i,k,t}" class="ltx_Math" display="inline" id="S3.SS3.p2.3.m3.3"><semantics id="S3.SS3.p2.3.m3.3a"><msubsup id="S3.SS3.p2.3.m3.3.4" xref="S3.SS3.p2.3.m3.3.4.cmml"><mover accent="true" id="S3.SS3.p2.3.m3.3.4.2.2" xref="S3.SS3.p2.3.m3.3.4.2.2.cmml"><mi id="S3.SS3.p2.3.m3.3.4.2.2.2" xref="S3.SS3.p2.3.m3.3.4.2.2.2.cmml">σ</mi><mo id="S3.SS3.p2.3.m3.3.4.2.2.1" xref="S3.SS3.p2.3.m3.3.4.2.2.1.cmml">^</mo></mover><mrow id="S3.SS3.p2.3.m3.3.3.3.5" xref="S3.SS3.p2.3.m3.3.3.3.4.cmml"><mi id="S3.SS3.p2.3.m3.1.1.1.1" xref="S3.SS3.p2.3.m3.1.1.1.1.cmml">i</mi><mo id="S3.SS3.p2.3.m3.3.3.3.5.1" xref="S3.SS3.p2.3.m3.3.3.3.4.cmml">,</mo><mi id="S3.SS3.p2.3.m3.2.2.2.2" xref="S3.SS3.p2.3.m3.2.2.2.2.cmml">k</mi><mo id="S3.SS3.p2.3.m3.3.3.3.5.2" xref="S3.SS3.p2.3.m3.3.3.3.4.cmml">,</mo><mi id="S3.SS3.p2.3.m3.3.3.3.3" xref="S3.SS3.p2.3.m3.3.3.3.3.cmml">t</mi></mrow><mo id="S3.SS3.p2.3.m3.3.4.2.3" xref="S3.SS3.p2.3.m3.3.4.2.3.cmml">′</mo></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.3b"><apply id="S3.SS3.p2.3.m3.3.4.cmml" xref="S3.SS3.p2.3.m3.3.4"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.3.4.1.cmml" xref="S3.SS3.p2.3.m3.3.4">subscript</csymbol><apply id="S3.SS3.p2.3.m3.3.4.2.cmml" xref="S3.SS3.p2.3.m3.3.4"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.3.4.2.1.cmml" xref="S3.SS3.p2.3.m3.3.4">superscript</csymbol><apply id="S3.SS3.p2.3.m3.3.4.2.2.cmml" xref="S3.SS3.p2.3.m3.3.4.2.2"><ci id="S3.SS3.p2.3.m3.3.4.2.2.1.cmml" xref="S3.SS3.p2.3.m3.3.4.2.2.1">^</ci><ci id="S3.SS3.p2.3.m3.3.4.2.2.2.cmml" xref="S3.SS3.p2.3.m3.3.4.2.2.2">𝜎</ci></apply><ci id="S3.SS3.p2.3.m3.3.4.2.3.cmml" xref="S3.SS3.p2.3.m3.3.4.2.3">′</ci></apply><list id="S3.SS3.p2.3.m3.3.3.3.4.cmml" xref="S3.SS3.p2.3.m3.3.3.3.5"><ci id="S3.SS3.p2.3.m3.1.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1.1.1">𝑖</ci><ci id="S3.SS3.p2.3.m3.2.2.2.2.cmml" xref="S3.SS3.p2.3.m3.2.2.2.2">𝑘</ci><ci id="S3.SS3.p2.3.m3.3.3.3.3.cmml" xref="S3.SS3.p2.3.m3.3.3.3.3">𝑡</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.3c">\hat{\sigma}^{\prime}_{i,k,t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.3.m3.3d">over^ start_ARG italic_σ end_ARG start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i , italic_k , italic_t end_POSTSUBSCRIPT</annotation></semantics></math> estimations. Our spatiotemporal encoder is a transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib61" title="">61</a>]</cite>, with specific attention block modifications that accommodate the large dimensionality of the tensors containing all temporal and skeleton information. As detailed in <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S3.F2" title="In 3.2 Cross-view Projection ‣ 3 Method ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>, we replace the standard attention module with criss-cross attentions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib58" title="">58</a>]</cite>, which approximates full temporal and spatial dependencies while being more memory efficient than full attention. We train the RLE head of the spatiotemporal encoder using the same strategy as <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib31" title="">31</a>]</cite>. Finally, similarly to <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S3.SS1" title="3.1 2D Pose Estimation ‣ 3 Method ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.1</span></a>, the RLE head simultaneously maximizes and learns a distribution <math alttext="P_{\Theta^{\prime}}(x|\mathcal{C})" class="ltx_Math" display="inline" id="S3.SS3.p2.4.m4.1"><semantics id="S3.SS3.p2.4.m4.1a"><mrow id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml"><msub id="S3.SS3.p2.4.m4.1.1.3" xref="S3.SS3.p2.4.m4.1.1.3.cmml"><mi id="S3.SS3.p2.4.m4.1.1.3.2" xref="S3.SS3.p2.4.m4.1.1.3.2.cmml">P</mi><msup id="S3.SS3.p2.4.m4.1.1.3.3" xref="S3.SS3.p2.4.m4.1.1.3.3.cmml"><mi id="S3.SS3.p2.4.m4.1.1.3.3.2" mathvariant="normal" xref="S3.SS3.p2.4.m4.1.1.3.3.2.cmml">Θ</mi><mo id="S3.SS3.p2.4.m4.1.1.3.3.3" xref="S3.SS3.p2.4.m4.1.1.3.3.3.cmml">′</mo></msup></msub><mo id="S3.SS3.p2.4.m4.1.1.2" xref="S3.SS3.p2.4.m4.1.1.2.cmml">⁢</mo><mrow id="S3.SS3.p2.4.m4.1.1.1.1" xref="S3.SS3.p2.4.m4.1.1.1.1.1.cmml"><mo id="S3.SS3.p2.4.m4.1.1.1.1.2" stretchy="false" xref="S3.SS3.p2.4.m4.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS3.p2.4.m4.1.1.1.1.1" xref="S3.SS3.p2.4.m4.1.1.1.1.1.cmml"><mi id="S3.SS3.p2.4.m4.1.1.1.1.1.2" xref="S3.SS3.p2.4.m4.1.1.1.1.1.2.cmml">x</mi><mo fence="false" id="S3.SS3.p2.4.m4.1.1.1.1.1.1" xref="S3.SS3.p2.4.m4.1.1.1.1.1.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.4.m4.1.1.1.1.1.3" xref="S3.SS3.p2.4.m4.1.1.1.1.1.3.cmml">𝒞</mi></mrow><mo id="S3.SS3.p2.4.m4.1.1.1.1.3" stretchy="false" xref="S3.SS3.p2.4.m4.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><apply id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1"><times id="S3.SS3.p2.4.m4.1.1.2.cmml" xref="S3.SS3.p2.4.m4.1.1.2"></times><apply id="S3.SS3.p2.4.m4.1.1.3.cmml" xref="S3.SS3.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m4.1.1.3.1.cmml" xref="S3.SS3.p2.4.m4.1.1.3">subscript</csymbol><ci id="S3.SS3.p2.4.m4.1.1.3.2.cmml" xref="S3.SS3.p2.4.m4.1.1.3.2">𝑃</ci><apply id="S3.SS3.p2.4.m4.1.1.3.3.cmml" xref="S3.SS3.p2.4.m4.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m4.1.1.3.3.1.cmml" xref="S3.SS3.p2.4.m4.1.1.3.3">superscript</csymbol><ci id="S3.SS3.p2.4.m4.1.1.3.3.2.cmml" xref="S3.SS3.p2.4.m4.1.1.3.3.2">Θ</ci><ci id="S3.SS3.p2.4.m4.1.1.3.3.3.cmml" xref="S3.SS3.p2.4.m4.1.1.3.3.3">′</ci></apply></apply><apply id="S3.SS3.p2.4.m4.1.1.1.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1.1.1"><csymbol cd="latexml" id="S3.SS3.p2.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1.1.1.1.1">conditional</csymbol><ci id="S3.SS3.p2.4.m4.1.1.1.1.1.2.cmml" xref="S3.SS3.p2.4.m4.1.1.1.1.1.2">𝑥</ci><ci id="S3.SS3.p2.4.m4.1.1.1.1.1.3.cmml" xref="S3.SS3.p2.4.m4.1.1.1.1.1.3">𝒞</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">P_{\Theta^{\prime}}(x|\mathcal{C})</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.4.m4.1d">italic_P start_POSTSUBSCRIPT roman_Θ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_x | caligraphic_C )</annotation></semantics></math> that represents the probability of the appearance of a keypoint in position <math alttext="x" class="ltx_Math" display="inline" id="S3.SS3.p2.5.m5.1"><semantics id="S3.SS3.p2.5.m5.1a"><mi id="S3.SS3.p2.5.m5.1.1" xref="S3.SS3.p2.5.m5.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m5.1b"><ci id="S3.SS3.p2.5.m5.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m5.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.5.m5.1d">italic_x</annotation></semantics></math> using a normalizing flow model with learnable parameters <math alttext="\Theta^{\prime}" class="ltx_Math" display="inline" id="S3.SS3.p2.6.m6.1"><semantics id="S3.SS3.p2.6.m6.1a"><msup id="S3.SS3.p2.6.m6.1.1" xref="S3.SS3.p2.6.m6.1.1.cmml"><mi id="S3.SS3.p2.6.m6.1.1.2" mathvariant="normal" xref="S3.SS3.p2.6.m6.1.1.2.cmml">Θ</mi><mo id="S3.SS3.p2.6.m6.1.1.3" xref="S3.SS3.p2.6.m6.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m6.1b"><apply id="S3.SS3.p2.6.m6.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.6.m6.1.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1">superscript</csymbol><ci id="S3.SS3.p2.6.m6.1.1.2.cmml" xref="S3.SS3.p2.6.m6.1.1.2">Θ</ci><ci id="S3.SS3.p2.6.m6.1.1.3.cmml" xref="S3.SS3.p2.6.m6.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m6.1c">\Theta^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.6.m6.1d">roman_Θ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math>. In the next section, we use the estimated <math alttext="\hat{\mu}^{\prime}" class="ltx_Math" display="inline" id="S3.SS3.p2.7.m7.1"><semantics id="S3.SS3.p2.7.m7.1a"><msup id="S3.SS3.p2.7.m7.1.1" xref="S3.SS3.p2.7.m7.1.1.cmml"><mover accent="true" id="S3.SS3.p2.7.m7.1.1.2" xref="S3.SS3.p2.7.m7.1.1.2.cmml"><mi id="S3.SS3.p2.7.m7.1.1.2.2" xref="S3.SS3.p2.7.m7.1.1.2.2.cmml">μ</mi><mo id="S3.SS3.p2.7.m7.1.1.2.1" xref="S3.SS3.p2.7.m7.1.1.2.1.cmml">^</mo></mover><mo id="S3.SS3.p2.7.m7.1.1.3" xref="S3.SS3.p2.7.m7.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.7.m7.1b"><apply id="S3.SS3.p2.7.m7.1.1.cmml" xref="S3.SS3.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.7.m7.1.1.1.cmml" xref="S3.SS3.p2.7.m7.1.1">superscript</csymbol><apply id="S3.SS3.p2.7.m7.1.1.2.cmml" xref="S3.SS3.p2.7.m7.1.1.2"><ci id="S3.SS3.p2.7.m7.1.1.2.1.cmml" xref="S3.SS3.p2.7.m7.1.1.2.1">^</ci><ci id="S3.SS3.p2.7.m7.1.1.2.2.cmml" xref="S3.SS3.p2.7.m7.1.1.2.2">𝜇</ci></apply><ci id="S3.SS3.p2.7.m7.1.1.3.cmml" xref="S3.SS3.p2.7.m7.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.7.m7.1c">\hat{\mu}^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.7.m7.1d">over^ start_ARG italic_μ end_ARG start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="\hat{\sigma}^{\prime}" class="ltx_Math" display="inline" id="S3.SS3.p2.8.m8.1"><semantics id="S3.SS3.p2.8.m8.1a"><msup id="S3.SS3.p2.8.m8.1.1" xref="S3.SS3.p2.8.m8.1.1.cmml"><mover accent="true" id="S3.SS3.p2.8.m8.1.1.2" xref="S3.SS3.p2.8.m8.1.1.2.cmml"><mi id="S3.SS3.p2.8.m8.1.1.2.2" xref="S3.SS3.p2.8.m8.1.1.2.2.cmml">σ</mi><mo id="S3.SS3.p2.8.m8.1.1.2.1" xref="S3.SS3.p2.8.m8.1.1.2.1.cmml">^</mo></mover><mo id="S3.SS3.p2.8.m8.1.1.3" xref="S3.SS3.p2.8.m8.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.8.m8.1b"><apply id="S3.SS3.p2.8.m8.1.1.cmml" xref="S3.SS3.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.8.m8.1.1.1.cmml" xref="S3.SS3.p2.8.m8.1.1">superscript</csymbol><apply id="S3.SS3.p2.8.m8.1.1.2.cmml" xref="S3.SS3.p2.8.m8.1.1.2"><ci id="S3.SS3.p2.8.m8.1.1.2.1.cmml" xref="S3.SS3.p2.8.m8.1.1.2.1">^</ci><ci id="S3.SS3.p2.8.m8.1.1.2.2.cmml" xref="S3.SS3.p2.8.m8.1.1.2.2">𝜎</ci></apply><ci id="S3.SS3.p2.8.m8.1.1.3.cmml" xref="S3.SS3.p2.8.m8.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.8.m8.1c">\hat{\sigma}^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.8.m8.1d">over^ start_ARG italic_σ end_ARG start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math>, and <math alttext="\Theta^{\prime}" class="ltx_Math" display="inline" id="S3.SS3.p2.9.m9.1"><semantics id="S3.SS3.p2.9.m9.1a"><msup id="S3.SS3.p2.9.m9.1.1" xref="S3.SS3.p2.9.m9.1.1.cmml"><mi id="S3.SS3.p2.9.m9.1.1.2" mathvariant="normal" xref="S3.SS3.p2.9.m9.1.1.2.cmml">Θ</mi><mo id="S3.SS3.p2.9.m9.1.1.3" xref="S3.SS3.p2.9.m9.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.9.m9.1b"><apply id="S3.SS3.p2.9.m9.1.1.cmml" xref="S3.SS3.p2.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.9.m9.1.1.1.cmml" xref="S3.SS3.p2.9.m9.1.1">superscript</csymbol><ci id="S3.SS3.p2.9.m9.1.1.2.cmml" xref="S3.SS3.p2.9.m9.1.1.2">Θ</ci><ci id="S3.SS3.p2.9.m9.1.1.3.cmml" xref="S3.SS3.p2.9.m9.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.9.m9.1c">\Theta^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.9.m9.1d">roman_Θ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math> for 3D pose estimation.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>3D Pose Estimation</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.6">As the final step, our method obtains the 3D keypoints using position estimates from 2D frames (<math alttext="\hat{\mu}" class="ltx_Math" display="inline" id="S3.SS4.p1.1.m1.1"><semantics id="S3.SS4.p1.1.m1.1a"><mover accent="true" id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml"><mi id="S3.SS4.p1.1.m1.1.1.2" xref="S3.SS4.p1.1.m1.1.1.2.cmml">μ</mi><mo id="S3.SS4.p1.1.m1.1.1.1" xref="S3.SS4.p1.1.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><apply id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1"><ci id="S3.SS4.p1.1.m1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1.1">^</ci><ci id="S3.SS4.p1.1.m1.1.1.2.cmml" xref="S3.SS4.p1.1.m1.1.1.2">𝜇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">\hat{\mu}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.1.m1.1d">over^ start_ARG italic_μ end_ARG</annotation></semantics></math>, <math alttext="\hat{\sigma}" class="ltx_Math" display="inline" id="S3.SS4.p1.2.m2.1"><semantics id="S3.SS4.p1.2.m2.1a"><mover accent="true" id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml"><mi id="S3.SS4.p1.2.m2.1.1.2" xref="S3.SS4.p1.2.m2.1.1.2.cmml">σ</mi><mo id="S3.SS4.p1.2.m2.1.1.1" xref="S3.SS4.p1.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.1b"><apply id="S3.SS4.p1.2.m2.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1"><ci id="S3.SS4.p1.2.m2.1.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1.1">^</ci><ci id="S3.SS4.p1.2.m2.1.1.2.cmml" xref="S3.SS4.p1.2.m2.1.1.2">𝜎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.1c">\hat{\sigma}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.2.m2.1d">over^ start_ARG italic_σ end_ARG</annotation></semantics></math>), as well as refined versions from the pose compiler (<math alttext="\hat{\mu}^{\prime}" class="ltx_Math" display="inline" id="S3.SS4.p1.3.m3.1"><semantics id="S3.SS4.p1.3.m3.1a"><msup id="S3.SS4.p1.3.m3.1.1" xref="S3.SS4.p1.3.m3.1.1.cmml"><mover accent="true" id="S3.SS4.p1.3.m3.1.1.2" xref="S3.SS4.p1.3.m3.1.1.2.cmml"><mi id="S3.SS4.p1.3.m3.1.1.2.2" xref="S3.SS4.p1.3.m3.1.1.2.2.cmml">μ</mi><mo id="S3.SS4.p1.3.m3.1.1.2.1" xref="S3.SS4.p1.3.m3.1.1.2.1.cmml">^</mo></mover><mo id="S3.SS4.p1.3.m3.1.1.3" xref="S3.SS4.p1.3.m3.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.3.m3.1b"><apply id="S3.SS4.p1.3.m3.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.3.m3.1.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1">superscript</csymbol><apply id="S3.SS4.p1.3.m3.1.1.2.cmml" xref="S3.SS4.p1.3.m3.1.1.2"><ci id="S3.SS4.p1.3.m3.1.1.2.1.cmml" xref="S3.SS4.p1.3.m3.1.1.2.1">^</ci><ci id="S3.SS4.p1.3.m3.1.1.2.2.cmml" xref="S3.SS4.p1.3.m3.1.1.2.2">𝜇</ci></apply><ci id="S3.SS4.p1.3.m3.1.1.3.cmml" xref="S3.SS4.p1.3.m3.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.3.m3.1c">\hat{\mu}^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.3.m3.1d">over^ start_ARG italic_μ end_ARG start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="\hat{\sigma}^{\prime}" class="ltx_Math" display="inline" id="S3.SS4.p1.4.m4.1"><semantics id="S3.SS4.p1.4.m4.1a"><msup id="S3.SS4.p1.4.m4.1.1" xref="S3.SS4.p1.4.m4.1.1.cmml"><mover accent="true" id="S3.SS4.p1.4.m4.1.1.2" xref="S3.SS4.p1.4.m4.1.1.2.cmml"><mi id="S3.SS4.p1.4.m4.1.1.2.2" xref="S3.SS4.p1.4.m4.1.1.2.2.cmml">σ</mi><mo id="S3.SS4.p1.4.m4.1.1.2.1" xref="S3.SS4.p1.4.m4.1.1.2.1.cmml">^</mo></mover><mo id="S3.SS4.p1.4.m4.1.1.3" xref="S3.SS4.p1.4.m4.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.4.m4.1b"><apply id="S3.SS4.p1.4.m4.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.4.m4.1.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1">superscript</csymbol><apply id="S3.SS4.p1.4.m4.1.1.2.cmml" xref="S3.SS4.p1.4.m4.1.1.2"><ci id="S3.SS4.p1.4.m4.1.1.2.1.cmml" xref="S3.SS4.p1.4.m4.1.1.2.1">^</ci><ci id="S3.SS4.p1.4.m4.1.1.2.2.cmml" xref="S3.SS4.p1.4.m4.1.1.2.2">𝜎</ci></apply><ci id="S3.SS4.p1.4.m4.1.1.3.cmml" xref="S3.SS4.p1.4.m4.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.4.m4.1c">\hat{\sigma}^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.4.m4.1d">over^ start_ARG italic_σ end_ARG start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math>).
Specifically, we use the estimated keypoints as labels when sampling from the associated keypoint density functions (<math alttext="P_{\Theta}(x|\mathcal{I})" class="ltx_Math" display="inline" id="S3.SS4.p1.5.m5.1"><semantics id="S3.SS4.p1.5.m5.1a"><mrow id="S3.SS4.p1.5.m5.1.1" xref="S3.SS4.p1.5.m5.1.1.cmml"><msub id="S3.SS4.p1.5.m5.1.1.3" xref="S3.SS4.p1.5.m5.1.1.3.cmml"><mi id="S3.SS4.p1.5.m5.1.1.3.2" xref="S3.SS4.p1.5.m5.1.1.3.2.cmml">P</mi><mi id="S3.SS4.p1.5.m5.1.1.3.3" mathvariant="normal" xref="S3.SS4.p1.5.m5.1.1.3.3.cmml">Θ</mi></msub><mo id="S3.SS4.p1.5.m5.1.1.2" xref="S3.SS4.p1.5.m5.1.1.2.cmml">⁢</mo><mrow id="S3.SS4.p1.5.m5.1.1.1.1" xref="S3.SS4.p1.5.m5.1.1.1.1.1.cmml"><mo id="S3.SS4.p1.5.m5.1.1.1.1.2" stretchy="false" xref="S3.SS4.p1.5.m5.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS4.p1.5.m5.1.1.1.1.1" xref="S3.SS4.p1.5.m5.1.1.1.1.1.cmml"><mi id="S3.SS4.p1.5.m5.1.1.1.1.1.2" xref="S3.SS4.p1.5.m5.1.1.1.1.1.2.cmml">x</mi><mo fence="false" id="S3.SS4.p1.5.m5.1.1.1.1.1.1" xref="S3.SS4.p1.5.m5.1.1.1.1.1.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p1.5.m5.1.1.1.1.1.3" xref="S3.SS4.p1.5.m5.1.1.1.1.1.3.cmml">ℐ</mi></mrow><mo id="S3.SS4.p1.5.m5.1.1.1.1.3" stretchy="false" xref="S3.SS4.p1.5.m5.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.5.m5.1b"><apply id="S3.SS4.p1.5.m5.1.1.cmml" xref="S3.SS4.p1.5.m5.1.1"><times id="S3.SS4.p1.5.m5.1.1.2.cmml" xref="S3.SS4.p1.5.m5.1.1.2"></times><apply id="S3.SS4.p1.5.m5.1.1.3.cmml" xref="S3.SS4.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p1.5.m5.1.1.3.1.cmml" xref="S3.SS4.p1.5.m5.1.1.3">subscript</csymbol><ci id="S3.SS4.p1.5.m5.1.1.3.2.cmml" xref="S3.SS4.p1.5.m5.1.1.3.2">𝑃</ci><ci id="S3.SS4.p1.5.m5.1.1.3.3.cmml" xref="S3.SS4.p1.5.m5.1.1.3.3">Θ</ci></apply><apply id="S3.SS4.p1.5.m5.1.1.1.1.1.cmml" xref="S3.SS4.p1.5.m5.1.1.1.1"><csymbol cd="latexml" id="S3.SS4.p1.5.m5.1.1.1.1.1.1.cmml" xref="S3.SS4.p1.5.m5.1.1.1.1.1.1">conditional</csymbol><ci id="S3.SS4.p1.5.m5.1.1.1.1.1.2.cmml" xref="S3.SS4.p1.5.m5.1.1.1.1.1.2">𝑥</ci><ci id="S3.SS4.p1.5.m5.1.1.1.1.1.3.cmml" xref="S3.SS4.p1.5.m5.1.1.1.1.1.3">ℐ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.5.m5.1c">P_{\Theta}(x|\mathcal{I})</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.5.m5.1d">italic_P start_POSTSUBSCRIPT roman_Θ end_POSTSUBSCRIPT ( italic_x | caligraphic_I )</annotation></semantics></math> and <math alttext="P_{\Theta^{\prime}}(x|\mathcal{C})" class="ltx_Math" display="inline" id="S3.SS4.p1.6.m6.1"><semantics id="S3.SS4.p1.6.m6.1a"><mrow id="S3.SS4.p1.6.m6.1.1" xref="S3.SS4.p1.6.m6.1.1.cmml"><msub id="S3.SS4.p1.6.m6.1.1.3" xref="S3.SS4.p1.6.m6.1.1.3.cmml"><mi id="S3.SS4.p1.6.m6.1.1.3.2" xref="S3.SS4.p1.6.m6.1.1.3.2.cmml">P</mi><msup id="S3.SS4.p1.6.m6.1.1.3.3" xref="S3.SS4.p1.6.m6.1.1.3.3.cmml"><mi id="S3.SS4.p1.6.m6.1.1.3.3.2" mathvariant="normal" xref="S3.SS4.p1.6.m6.1.1.3.3.2.cmml">Θ</mi><mo id="S3.SS4.p1.6.m6.1.1.3.3.3" xref="S3.SS4.p1.6.m6.1.1.3.3.3.cmml">′</mo></msup></msub><mo id="S3.SS4.p1.6.m6.1.1.2" xref="S3.SS4.p1.6.m6.1.1.2.cmml">⁢</mo><mrow id="S3.SS4.p1.6.m6.1.1.1.1" xref="S3.SS4.p1.6.m6.1.1.1.1.1.cmml"><mo id="S3.SS4.p1.6.m6.1.1.1.1.2" stretchy="false" xref="S3.SS4.p1.6.m6.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS4.p1.6.m6.1.1.1.1.1" xref="S3.SS4.p1.6.m6.1.1.1.1.1.cmml"><mi id="S3.SS4.p1.6.m6.1.1.1.1.1.2" xref="S3.SS4.p1.6.m6.1.1.1.1.1.2.cmml">x</mi><mo fence="false" id="S3.SS4.p1.6.m6.1.1.1.1.1.1" xref="S3.SS4.p1.6.m6.1.1.1.1.1.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p1.6.m6.1.1.1.1.1.3" xref="S3.SS4.p1.6.m6.1.1.1.1.1.3.cmml">𝒞</mi></mrow><mo id="S3.SS4.p1.6.m6.1.1.1.1.3" stretchy="false" xref="S3.SS4.p1.6.m6.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.6.m6.1b"><apply id="S3.SS4.p1.6.m6.1.1.cmml" xref="S3.SS4.p1.6.m6.1.1"><times id="S3.SS4.p1.6.m6.1.1.2.cmml" xref="S3.SS4.p1.6.m6.1.1.2"></times><apply id="S3.SS4.p1.6.m6.1.1.3.cmml" xref="S3.SS4.p1.6.m6.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p1.6.m6.1.1.3.1.cmml" xref="S3.SS4.p1.6.m6.1.1.3">subscript</csymbol><ci id="S3.SS4.p1.6.m6.1.1.3.2.cmml" xref="S3.SS4.p1.6.m6.1.1.3.2">𝑃</ci><apply id="S3.SS4.p1.6.m6.1.1.3.3.cmml" xref="S3.SS4.p1.6.m6.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS4.p1.6.m6.1.1.3.3.1.cmml" xref="S3.SS4.p1.6.m6.1.1.3.3">superscript</csymbol><ci id="S3.SS4.p1.6.m6.1.1.3.3.2.cmml" xref="S3.SS4.p1.6.m6.1.1.3.3.2">Θ</ci><ci id="S3.SS4.p1.6.m6.1.1.3.3.3.cmml" xref="S3.SS4.p1.6.m6.1.1.3.3.3">′</ci></apply></apply><apply id="S3.SS4.p1.6.m6.1.1.1.1.1.cmml" xref="S3.SS4.p1.6.m6.1.1.1.1"><csymbol cd="latexml" id="S3.SS4.p1.6.m6.1.1.1.1.1.1.cmml" xref="S3.SS4.p1.6.m6.1.1.1.1.1.1">conditional</csymbol><ci id="S3.SS4.p1.6.m6.1.1.1.1.1.2.cmml" xref="S3.SS4.p1.6.m6.1.1.1.1.1.2">𝑥</ci><ci id="S3.SS4.p1.6.m6.1.1.1.1.1.3.cmml" xref="S3.SS4.p1.6.m6.1.1.1.1.1.3">𝒞</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.6.m6.1c">P_{\Theta^{\prime}}(x|\mathcal{C})</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.6.m6.1d">italic_P start_POSTSUBSCRIPT roman_Θ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_x | caligraphic_C )</annotation></semantics></math>) during Maximum Likelihood Estimation (MLE). Therefore, the total loss function of MLE for each joint and each time frame is defined as:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\scriptstyle\mathcal{L}_{mle}=-\log\prod_{i\in\mathcal{V}}P_{\Theta}(u_{i}|%
\mathcal{I})\bigg{|}_{u_{i}=\hat{\mu}_{i}}-\log\prod_{i\in\mathcal{V}}P_{%
\Theta^{\prime}}(u_{i}|\mathcal{C})\bigg{|}_{u_{i}=\hat{\mu}_{i}^{\prime}}," class="ltx_Math" display="block" id="S3.E1.m1.3"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.3.1" xref="S3.E1.m1.3.3.1.1.cmml"><mrow id="S3.E1.m1.3.3.1.1" xref="S3.E1.m1.3.3.1.1.cmml"><msub id="S3.E1.m1.3.3.1.1.4" xref="S3.E1.m1.3.3.1.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.3.3.1.1.4.2" mathsize="70%" xref="S3.E1.m1.3.3.1.1.4.2.cmml">ℒ</mi><mrow id="S3.E1.m1.3.3.1.1.4.3" xref="S3.E1.m1.3.3.1.1.4.3.cmml"><mi id="S3.E1.m1.3.3.1.1.4.3.2" mathsize="71%" xref="S3.E1.m1.3.3.1.1.4.3.2.cmml">m</mi><mo id="S3.E1.m1.3.3.1.1.4.3.1" xref="S3.E1.m1.3.3.1.1.4.3.1.cmml">⁢</mo><mi id="S3.E1.m1.3.3.1.1.4.3.3" mathsize="71%" xref="S3.E1.m1.3.3.1.1.4.3.3.cmml">l</mi><mo id="S3.E1.m1.3.3.1.1.4.3.1a" xref="S3.E1.m1.3.3.1.1.4.3.1.cmml">⁢</mo><mi id="S3.E1.m1.3.3.1.1.4.3.4" mathsize="71%" xref="S3.E1.m1.3.3.1.1.4.3.4.cmml">e</mi></mrow></msub><mo id="S3.E1.m1.3.3.1.1.3" mathsize="70%" xref="S3.E1.m1.3.3.1.1.3.cmml">=</mo><mrow id="S3.E1.m1.3.3.1.1.2" xref="S3.E1.m1.3.3.1.1.2.cmml"><mrow id="S3.E1.m1.3.3.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.cmml"><mo id="S3.E1.m1.3.3.1.1.1.1a" mathsize="70%" rspace="0.167em" xref="S3.E1.m1.3.3.1.1.1.1.cmml">−</mo><msub id="S3.E1.m1.3.3.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.2.cmml"><mrow id="S3.E1.m1.3.3.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.2.cmml"><mrow id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3" mathsize="70%" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3.cmml">log</mi><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2" lspace="0.167em" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mstyle displaystyle="false" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml"><msub id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2a" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml"><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.2" maxsize="70%" minsize="70%" stretchy="true" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.2.cmml">∏</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.3.2" mathsize="71%" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.3.2.cmml">i</mi><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.3.1" mathsize="71%" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.3.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.3.3" mathsize="71%" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.3.3.cmml">𝒱</mi></mrow></msub></mstyle><mrow id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2" mathsize="70%" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2.cmml">P</mi><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3" mathsize="71%" mathvariant="normal" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.cmml">Θ</mi></msub><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2" maxsize="70%" minsize="70%" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2" mathsize="70%" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">u</mi><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3" mathsize="71%" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo fence="false" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1" mathsize="70%" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.3" mathsize="70%" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">ℐ</mi></mrow><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3" maxsize="70%" minsize="70%" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.1.2" maxsize="210%" minsize="210%" xref="S3.E1.m1.3.3.1.1.1.1.1.2.1.cmml">|</mo></mrow><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.2.2" mathsize="71%" xref="S3.E1.m1.1.1.1.2.2.cmml">u</mi><mi id="S3.E1.m1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.2.3.cmml">i</mi></msub><mo id="S3.E1.m1.1.1.1.1" mathsize="71%" xref="S3.E1.m1.1.1.1.1.cmml">=</mo><msub id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.1.1.1.3.cmml"><mover accent="true" id="S3.E1.m1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.3.2.cmml"><mi id="S3.E1.m1.1.1.1.3.2.2" mathsize="71%" xref="S3.E1.m1.1.1.1.3.2.2.cmml">μ</mi><mo id="S3.E1.m1.1.1.1.3.2.1" mathsize="71%" xref="S3.E1.m1.1.1.1.3.2.1.cmml">^</mo></mover><mi id="S3.E1.m1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.3.3.cmml">i</mi></msub></mrow></msub></mrow><mo id="S3.E1.m1.3.3.1.1.2.3" mathsize="70%" xref="S3.E1.m1.3.3.1.1.2.3.cmml">−</mo><msub id="S3.E1.m1.3.3.1.1.2.2.1" xref="S3.E1.m1.3.3.1.1.2.2.2.cmml"><mrow id="S3.E1.m1.3.3.1.1.2.2.1.1" xref="S3.E1.m1.3.3.1.1.2.2.2.cmml"><mrow id="S3.E1.m1.3.3.1.1.2.2.1.1.1" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.cmml"><mi id="S3.E1.m1.3.3.1.1.2.2.1.1.1.3" mathsize="70%" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.3.cmml">log</mi><mo id="S3.E1.m1.3.3.1.1.2.2.1.1.1.2" lspace="0.167em" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.2.cmml">⁢</mo><mrow id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.cmml"><mstyle displaystyle="false" id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2.cmml"><msub id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2a" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2.cmml"><mo id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2.2" maxsize="70%" minsize="70%" stretchy="true" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2.2.cmml">∏</mo><mrow id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2.3" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2.3.cmml"><mi id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2.3.2" mathsize="71%" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2.3.2.cmml">i</mi><mo id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2.3.1" mathsize="71%" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2.3.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2.3.3" mathsize="71%" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2.3.3.cmml">𝒱</mi></mrow></msub></mstyle><mrow id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.cmml"><msub id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.3.2" mathsize="70%" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.3.2.cmml">P</mi><msup id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.3.3" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.3.3.cmml"><mi id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.3.3.2" mathsize="71%" mathvariant="normal" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.3.3.2.cmml">Θ</mi><mo id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.3.3.3" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.3.3.3.cmml">′</mo></msup></msub><mo id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.2" maxsize="70%" minsize="70%" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.1.2.2" mathsize="70%" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.1.2.2.cmml">u</mi><mi id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.1.2.3" mathsize="71%" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo fence="false" id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.1.1" mathsize="70%" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.1.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.1.3" mathsize="70%" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.1.3.cmml">𝒞</mi></mrow><mo id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.3" maxsize="70%" minsize="70%" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E1.m1.3.3.1.1.2.2.1.1.2" maxsize="210%" minsize="210%" xref="S3.E1.m1.3.3.1.1.2.2.2.1.cmml">|</mo></mrow><mrow id="S3.E1.m1.2.2.1" xref="S3.E1.m1.2.2.1.cmml"><msub id="S3.E1.m1.2.2.1.2" xref="S3.E1.m1.2.2.1.2.cmml"><mi id="S3.E1.m1.2.2.1.2.2" mathsize="71%" xref="S3.E1.m1.2.2.1.2.2.cmml">u</mi><mi id="S3.E1.m1.2.2.1.2.3" xref="S3.E1.m1.2.2.1.2.3.cmml">i</mi></msub><mo id="S3.E1.m1.2.2.1.1" mathsize="71%" xref="S3.E1.m1.2.2.1.1.cmml">=</mo><msubsup id="S3.E1.m1.2.2.1.3" xref="S3.E1.m1.2.2.1.3.cmml"><mover accent="true" id="S3.E1.m1.2.2.1.3.2.2" xref="S3.E1.m1.2.2.1.3.2.2.cmml"><mi id="S3.E1.m1.2.2.1.3.2.2.2" mathsize="71%" xref="S3.E1.m1.2.2.1.3.2.2.2.cmml">μ</mi><mo id="S3.E1.m1.2.2.1.3.2.2.1" mathsize="71%" xref="S3.E1.m1.2.2.1.3.2.2.1.cmml">^</mo></mover><mi id="S3.E1.m1.2.2.1.3.2.3" xref="S3.E1.m1.2.2.1.3.2.3.cmml">i</mi><mo id="S3.E1.m1.2.2.1.3.3" xref="S3.E1.m1.2.2.1.3.3.cmml">′</mo></msubsup></mrow></msub></mrow></mrow><mo id="S3.E1.m1.3.3.1.2" mathsize="70%" xref="S3.E1.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.3.1.1.cmml" xref="S3.E1.m1.3.3.1"><eq id="S3.E1.m1.3.3.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.3"></eq><apply id="S3.E1.m1.3.3.1.1.4.cmml" xref="S3.E1.m1.3.3.1.1.4"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.4.1.cmml" xref="S3.E1.m1.3.3.1.1.4">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.4.2.cmml" xref="S3.E1.m1.3.3.1.1.4.2">ℒ</ci><apply id="S3.E1.m1.3.3.1.1.4.3.cmml" xref="S3.E1.m1.3.3.1.1.4.3"><times id="S3.E1.m1.3.3.1.1.4.3.1.cmml" xref="S3.E1.m1.3.3.1.1.4.3.1"></times><ci id="S3.E1.m1.3.3.1.1.4.3.2.cmml" xref="S3.E1.m1.3.3.1.1.4.3.2">𝑚</ci><ci id="S3.E1.m1.3.3.1.1.4.3.3.cmml" xref="S3.E1.m1.3.3.1.1.4.3.3">𝑙</ci><ci id="S3.E1.m1.3.3.1.1.4.3.4.cmml" xref="S3.E1.m1.3.3.1.1.4.3.4">𝑒</ci></apply></apply><apply id="S3.E1.m1.3.3.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2"><minus id="S3.E1.m1.3.3.1.1.2.3.cmml" xref="S3.E1.m1.3.3.1.1.2.3"></minus><apply id="S3.E1.m1.3.3.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1"><minus id="S3.E1.m1.3.3.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1"></minus><apply id="S3.E1.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.3.3.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.2">evaluated-at</csymbol><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1"><times id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2"></times><log id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3"></log><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1"><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><csymbol cd="latexml" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.2">product</csymbol><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.3"><in id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.3.1"></in><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.3.2">𝑖</ci><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.3.3">𝒱</ci></apply></apply><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1"><times id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2"></times><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2">𝑃</ci><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3">Θ</ci></apply><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1">conditional</csymbol><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2">𝑢</ci><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.3">ℐ</ci></apply></apply></apply></apply><apply id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1"></eq><apply id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.2.2">𝑢</ci><ci id="S3.E1.m1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.2.3">𝑖</ci></apply><apply id="S3.E1.m1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.3">subscript</csymbol><apply id="S3.E1.m1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.3.2"><ci id="S3.E1.m1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.1.3.2.1">^</ci><ci id="S3.E1.m1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.1.3.2.2">𝜇</ci></apply><ci id="S3.E1.m1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.3.3">𝑖</ci></apply></apply></apply></apply><apply id="S3.E1.m1.3.3.1.1.2.2.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1"><csymbol cd="latexml" id="S3.E1.m1.3.3.1.1.2.2.2.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.2">evaluated-at</csymbol><apply id="S3.E1.m1.3.3.1.1.2.2.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1"><times id="S3.E1.m1.3.3.1.1.2.2.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.2"></times><log id="S3.E1.m1.3.3.1.1.2.2.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.3"></log><apply id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1"><apply id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2">subscript</csymbol><csymbol cd="latexml" id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2.2">product</csymbol><apply id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2.3"><in id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2.3.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2.3.1"></in><ci id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2.3.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2.3.2">𝑖</ci><ci id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2.3.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.2.3.3">𝒱</ci></apply></apply><apply id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1"><times id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.2"></times><apply id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.3.2">𝑃</ci><apply id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.3.3">superscript</csymbol><ci id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.3.3.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.3.3.2">Θ</ci><ci id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.3.3.3">′</ci></apply></apply><apply id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.1.1">conditional</csymbol><apply id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.1.2.2">𝑢</ci><ci id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><ci id="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.1.3">𝒞</ci></apply></apply></apply></apply><apply id="S3.E1.m1.2.2.1.cmml" xref="S3.E1.m1.2.2.1"><eq id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1.1"></eq><apply id="S3.E1.m1.2.2.1.2.cmml" xref="S3.E1.m1.2.2.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.2.1.cmml" xref="S3.E1.m1.2.2.1.2">subscript</csymbol><ci id="S3.E1.m1.2.2.1.2.2.cmml" xref="S3.E1.m1.2.2.1.2.2">𝑢</ci><ci id="S3.E1.m1.2.2.1.2.3.cmml" xref="S3.E1.m1.2.2.1.2.3">𝑖</ci></apply><apply id="S3.E1.m1.2.2.1.3.cmml" xref="S3.E1.m1.2.2.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.3.1.cmml" xref="S3.E1.m1.2.2.1.3">superscript</csymbol><apply id="S3.E1.m1.2.2.1.3.2.cmml" xref="S3.E1.m1.2.2.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.3.2.1.cmml" xref="S3.E1.m1.2.2.1.3">subscript</csymbol><apply id="S3.E1.m1.2.2.1.3.2.2.cmml" xref="S3.E1.m1.2.2.1.3.2.2"><ci id="S3.E1.m1.2.2.1.3.2.2.1.cmml" xref="S3.E1.m1.2.2.1.3.2.2.1">^</ci><ci id="S3.E1.m1.2.2.1.3.2.2.2.cmml" xref="S3.E1.m1.2.2.1.3.2.2.2">𝜇</ci></apply><ci id="S3.E1.m1.2.2.1.3.2.3.cmml" xref="S3.E1.m1.2.2.1.3.2.3">𝑖</ci></apply><ci id="S3.E1.m1.2.2.1.3.3.cmml" xref="S3.E1.m1.2.2.1.3.3">′</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">\scriptstyle\mathcal{L}_{mle}=-\log\prod_{i\in\mathcal{V}}P_{\Theta}(u_{i}|%
\mathcal{I})\bigg{|}_{u_{i}=\hat{\mu}_{i}}-\log\prod_{i\in\mathcal{V}}P_{%
\Theta^{\prime}}(u_{i}|\mathcal{C})\bigg{|}_{u_{i}=\hat{\mu}_{i}^{\prime}},</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.3d">caligraphic_L start_POSTSUBSCRIPT italic_m italic_l italic_e end_POSTSUBSCRIPT = - roman_log ∏ start_POSTSUBSCRIPT italic_i ∈ caligraphic_V end_POSTSUBSCRIPT italic_P start_POSTSUBSCRIPT roman_Θ end_POSTSUBSCRIPT ( italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | caligraphic_I ) | start_POSTSUBSCRIPT italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = over^ start_ARG italic_μ end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT - roman_log ∏ start_POSTSUBSCRIPT italic_i ∈ caligraphic_V end_POSTSUBSCRIPT italic_P start_POSTSUBSCRIPT roman_Θ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | caligraphic_C ) | start_POSTSUBSCRIPT italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = over^ start_ARG italic_μ end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS4.p1.12">where <math alttext="u_{i}" class="ltx_Math" display="inline" id="S3.SS4.p1.7.m1.1"><semantics id="S3.SS4.p1.7.m1.1a"><msub id="S3.SS4.p1.7.m1.1.1" xref="S3.SS4.p1.7.m1.1.1.cmml"><mi id="S3.SS4.p1.7.m1.1.1.2" xref="S3.SS4.p1.7.m1.1.1.2.cmml">u</mi><mi id="S3.SS4.p1.7.m1.1.1.3" xref="S3.SS4.p1.7.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.7.m1.1b"><apply id="S3.SS4.p1.7.m1.1.1.cmml" xref="S3.SS4.p1.7.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.7.m1.1.1.1.cmml" xref="S3.SS4.p1.7.m1.1.1">subscript</csymbol><ci id="S3.SS4.p1.7.m1.1.1.2.cmml" xref="S3.SS4.p1.7.m1.1.1.2">𝑢</ci><ci id="S3.SS4.p1.7.m1.1.1.3.cmml" xref="S3.SS4.p1.7.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.7.m1.1c">u_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.7.m1.1d">italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the projection of variable 3D point <math alttext="U" class="ltx_Math" display="inline" id="S3.SS4.p1.8.m2.1"><semantics id="S3.SS4.p1.8.m2.1a"><mi id="S3.SS4.p1.8.m2.1.1" xref="S3.SS4.p1.8.m2.1.1.cmml">U</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.8.m2.1b"><ci id="S3.SS4.p1.8.m2.1.1.cmml" xref="S3.SS4.p1.8.m2.1.1">𝑈</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.8.m2.1c">U</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.8.m2.1d">italic_U</annotation></semantics></math> onto each camera view <math alttext="i" class="ltx_Math" display="inline" id="S3.SS4.p1.9.m3.1"><semantics id="S3.SS4.p1.9.m3.1a"><mi id="S3.SS4.p1.9.m3.1.1" xref="S3.SS4.p1.9.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.9.m3.1b"><ci id="S3.SS4.p1.9.m3.1.1.cmml" xref="S3.SS4.p1.9.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.9.m3.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.9.m3.1d">italic_i</annotation></semantics></math>. Minimizing this loss function increases the likelihood of <math alttext="U" class="ltx_Math" display="inline" id="S3.SS4.p1.10.m4.1"><semantics id="S3.SS4.p1.10.m4.1a"><mi id="S3.SS4.p1.10.m4.1.1" xref="S3.SS4.p1.10.m4.1.1.cmml">U</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.10.m4.1b"><ci id="S3.SS4.p1.10.m4.1.1.cmml" xref="S3.SS4.p1.10.m4.1.1">𝑈</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.10.m4.1c">U</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.10.m4.1d">italic_U</annotation></semantics></math> appearing close to the ground-truth 3D keypoints <math alttext="U_{g}" class="ltx_Math" display="inline" id="S3.SS4.p1.11.m5.1"><semantics id="S3.SS4.p1.11.m5.1a"><msub id="S3.SS4.p1.11.m5.1.1" xref="S3.SS4.p1.11.m5.1.1.cmml"><mi id="S3.SS4.p1.11.m5.1.1.2" xref="S3.SS4.p1.11.m5.1.1.2.cmml">U</mi><mi id="S3.SS4.p1.11.m5.1.1.3" xref="S3.SS4.p1.11.m5.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.11.m5.1b"><apply id="S3.SS4.p1.11.m5.1.1.cmml" xref="S3.SS4.p1.11.m5.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.11.m5.1.1.1.cmml" xref="S3.SS4.p1.11.m5.1.1">subscript</csymbol><ci id="S3.SS4.p1.11.m5.1.1.2.cmml" xref="S3.SS4.p1.11.m5.1.1.2">𝑈</ci><ci id="S3.SS4.p1.11.m5.1.1.3.cmml" xref="S3.SS4.p1.11.m5.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.11.m5.1c">U_{g}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.11.m5.1d">italic_U start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT</annotation></semantics></math> without 3D supervision. To solve this non-convex optimization problem, we initialize <math alttext="U" class="ltx_Math" display="inline" id="S3.SS4.p1.12.m6.1"><semantics id="S3.SS4.p1.12.m6.1a"><mi id="S3.SS4.p1.12.m6.1.1" xref="S3.SS4.p1.12.m6.1.1.cmml">U</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.12.m6.1b"><ci id="S3.SS4.p1.12.m6.1.1.cmml" xref="S3.SS4.p1.12.m6.1.1">𝑈</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.12.m6.1c">U</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.12.m6.1d">italic_U</annotation></semantics></math> with a Direct Linear Transformation (DLT) algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib12" title="">12</a>]</cite> and iteratively refine it using an optimizer, <em class="ltx_emph ltx_font_italic" id="S3.SS4.p1.12.1">i.e</em>.<span class="ltx_text" id="S3.SS4.p1.12.2"></span>, L-BFGS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib37" title="">37</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Training and Multi-view Data Synthesis</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">Our proposed pose compiler does not require 3D annotated data, given that it operates on point clouds. Therefore, we can use any animation data to create a synthetic training set. The synthesis of multi-view training data starts by randomly positioning several cameras in 3D space to look roughly at the human body’s center. During this process, we limit the camera height, distance, tilt, pitch, and yaw within a representative range of a normal multi-view recording setup. Next, we extract the 3D landmarks and project them onto each camera to obtain ground-truth 2D keypoints <math alttext="\mu_{g}" class="ltx_Math" display="inline" id="S3.SS5.p1.1.m1.1"><semantics id="S3.SS5.p1.1.m1.1a"><msub id="S3.SS5.p1.1.m1.1.1" xref="S3.SS5.p1.1.m1.1.1.cmml"><mi id="S3.SS5.p1.1.m1.1.1.2" xref="S3.SS5.p1.1.m1.1.1.2.cmml">μ</mi><mi id="S3.SS5.p1.1.m1.1.1.3" xref="S3.SS5.p1.1.m1.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.1.m1.1b"><apply id="S3.SS5.p1.1.m1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.1.m1.1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS5.p1.1.m1.1.1.2.cmml" xref="S3.SS5.p1.1.m1.1.1.2">𝜇</ci><ci id="S3.SS5.p1.1.m1.1.1.3.cmml" xref="S3.SS5.p1.1.m1.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.1.m1.1c">\mu_{g}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.1.m1.1d">italic_μ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT</annotation></semantics></math>. We then corrupt these keypoints through extensive augmentations. Refer to our Supplementary Materials for more details on our data synthesis. Finally, we obtain the point clouds training data from the corrupted 2D keypoint using the cross-view projection from <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S3.SS2" title="3.2 Cross-view Projection ‣ 3 Method ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We compare our proposed approach with prior works on the Human3.6m <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib18" title="">18</a>]</cite> dataset and measure how our method generalizes to different skeleton configurations and unseen outdoor environments on the RICH <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib16" title="">16</a>]</cite> dataset. We use the COCO WholeBody <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib36" title="">36</a>]</cite> and MPII <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib1" title="">1</a>]</cite> for training our 2D pose estimator in different scenarios (refer to the Supplementary Materials for more information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib69" title="">69</a>]</cite>). The pose compiler is trained using motion capture data from the AMASS dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib42" title="">42</a>]</cite> to simulate multi-view training data. Finally, we analyze the viewpoint scalability of our approach on the CMU Panoptic <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib22" title="">22</a>]</cite> and HUMBI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib64" title="">64</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib65" title="">65</a>]</cite> datasets. The details of these datasets are as follows.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">Human3.6m.</span>
The Human3.6m <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib18" title="">18</a>]</cite> dataset is a standard benchmark for evaluating the performance of 3D human pose estimation solutions in multi-view <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib55" title="">55</a>]</cite> and single-view <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib70" title="">70</a>]</cite> settings. We report our InD performance in Protocol-I using subjects 1, 5, 6, 7, and 8 for training and 9 and 11 for testing.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.1.1">RICH.</span>
Real scenes, Interaction, Contact, and Humans (RICH) dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib16" title="">16</a>]</cite> is a dataset of multi-view videos with accurate bodies and 3D-scanned scenes obtained using markerless motion capture technology. We extract 3D ground-truth keypoints from the provided SMPL-x <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib46" title="">46</a>]</cite> bodies for evaluation. We report OoD performance on the test set, which contains 7 subjects in 52 scenarios and 3 environments, including a construction site, a gym, and a lecture hall.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p4.1.1">CMU Panoptic.</span>
The CMU Panoptic <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib22" title="">22</a>]</cite> dataset is a large collection of human body poses and interactions recorded from multiple views. The dataset provides videos from 480 VGA and 30 HD cameras and ground-truth 3D poses obtained from markerless 2D pose estimation and triangulation. We use a small subset of the validation set in line with <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib19" title="">19</a>]</cite> to analyze UPose3D’s camera scalability.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p5.1.1">HUMBI.</span>
The HUman Multiview Behavioral Imaging (HUMBI) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib64" title="">64</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib65" title="">65</a>]</cite> dataset is a large-scale multi-view dataset designed for modeling and reverse-rendering of the human body in different expressions. It contains 107 HD recordings of 772 subjects with natural clothing. We use this dataset only for OoD evaluations during our scalability analysis. Specifically, we evaluate on the first 80 subjects by extracting ground-truth 3D keypoints from the provided SMPL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib39" title="">39</a>]</cite> parameters.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p6">
<p class="ltx_p" id="S4.SS1.p6.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p6.1.1">AMASS.</span>
The Archive of Motion Capture as Surface Shapes (AMASS) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib42" title="">42</a>]</cite> is a large collection of 3D human motion capture datasets. It contains over 40 hours of recording from 300 subjects spanning 11,000 actions. We only use the training set of this dataset following prior works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib50" title="">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib7" title="">7</a>]</cite>. Since the extraction of landmarks requires a specialized joint regressor for each skeleton configuration, in our InD experiments, we use the SMPL<span class="ltx_text ltx_font_typewriter" id="S4.SS1.p6.1.2">+</span>H <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib39" title="">39</a>]</cite> body model and a 17-joint regressor provided by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib44" title="">44</a>]</cite>. For our OoD experiments, we use the SMPL-x <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib46" title="">46</a>]</cite> body model and a whole-body 133-joint regressor from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib7" title="">7</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Evaluation Metrics</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We adopt the standard evaluation metrics from prior works for our InD experiments on the Human3.6m <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib18" title="">18</a>]</cite> dataset. Mean Per Joint Position Error (MPJPE) represents the distance between the ground-truth and predicted pose in millimeters, while its Procrustes Aligned version (PA-MPJPE) and the Normalized variant (N-MPJPE) show how well the predicted pose fits the ground-truth keypoints after a similarity transformation. Next, we report the Translation Aligned error (TA-MPJPE) and PA-MPJPE for our OoD experiments on the RICH <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib16" title="">16</a>]</cite> dataset following prior works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib59" title="">59</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib54" title="">54</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Implementation Details</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">We implement our pipeline using PyTorch <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib45" title="">45</a>]</cite> and MMPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib6" title="">6</a>]</cite>. Here, we provide the details of our neural networks and the computation costs of our pose compiler. We encourage readers to refer to our Supplementary Materials for information on our multi-view data synthesis and augmentation strategies.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.2">To compare our method with prior works, we choose a CPN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib5" title="">5</a>]</cite> backbone based on ResNet152 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib13" title="">13</a>]</cite> with an input size 384 <math alttext="\times" class="ltx_Math" display="inline" id="S4.SS3.p2.1.m1.1"><semantics id="S4.SS3.p2.1.m1.1a"><mo id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><times id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.1.m1.1d">×</annotation></semantics></math> 384 and an RLE head <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib31" title="">31</a>]</cite> for 2D pose estimation. We fine-tune our 2D pose estimator jointly on Human3.6m <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib18" title="">18</a>]</cite> and MPII <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib1" title="">1</a>]</cite> datasets starting from the weights of the checkpoints provided by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib19" title="">19</a>]</cite>. For our OoD experiments, we use an HRNet-W48 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib57" title="">57</a>]</cite> with 384 <math alttext="\times" class="ltx_Math" display="inline" id="S4.SS3.p2.2.m2.1"><semantics id="S4.SS3.p2.2.m2.1a"><mo id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><times id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.2.m2.1d">×</annotation></semantics></math> 288 input size and train it from scratch.
Our point cloud encoder uses 4 multi-head self-attention blocks, each with 4 attention heads. Additionally, we reduce its hidden dimension to 64 to accommodate the relatively smaller size of our point clouds. Following the point cloud encoder, the features are concatenated with two positional embeddings of size 64 to represent the time frames and different skeleton joints. Our spatiotemporal encoder employs four criss-cross transformer blocks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib58" title="">58</a>]</cite> with 4 attention heads each. We set the hidden layer dimension of our transformers to 64 and the hidden dimension of prediction heads to 1024. We then train our pose compiler module using AdamW optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib40" title="">40</a>]</cite> with a batch size of 64 on the AMASS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib42" title="">42</a>]</cite> dataset for both OoD and InD evaluations. We use a learning rate of 4e-5 with a warm-up factor of 1e-4 for the first 500 iterations and a cosine annealing scheduler over the next 20,000 iterations. The network training takes about 6 hours on an NVIDIA 2080 RTX GPU.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">Unlike prior works that rely on rendering techniques for synthetic data generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib66" title="">66</a>]</cite>, we generate samples online with varying numbers of camera views (up to 8) during training without significant computation overhead. Furthermore, we increase the diversity of the motion capture data by applying several augmentations to the motion capture data before 3D keypoint extraction, such as random 180-degree rotations and mirroring around the mid-hip point.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Baselines</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">On Human3.6m <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib18" title="">18</a>]</cite>, we compare our UPose3D with methods that infer 3D human keypoints using only 2D supervised models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib27" title="">27</a>]</cite>. Additionally, we provide a summary of methods that rely on direct 3D annotations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib66" title="">66</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib19" title="">19</a>]</cite>, except for a few <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib48" title="">48</a>]</cite> that use uncalibrated cameras. Moreover, we compare our work with methods that only rely on multi-view 2D keypoints along with the camera parameters <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib67" title="">67</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib47" title="">47</a>]</cite>. Please refer to our Supplementary Materials for a more in-depth comparison with weakly-supervised and semi-supervised approaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib62" title="">62</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib56" title="">56</a>]</cite>, multi-view methods without that do not rely on camera parameters <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib48" title="">48</a>]</cite>, and monocular 3D pose estimation methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib59" title="">59</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib34" title="">34</a>]</cite>. We also report and compare the performance of our method against triangulation approaches, such as DLT, RANSAC, and RPSM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib47" title="">47</a>]</cite>. On the RICH dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib16" title="">16</a>]</cite>, we compare our model with triangulation approaches and replicate the performance of AdaFuse <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib66" title="">66</a>]</cite>, which is one of the top-performing methods on the Human3.6m dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib18" title="">18</a>]</cite>.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.6.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S4.T1.7.2" style="font-size:90%;">The comparison of our method in InD settings against prior multi-view works on the full test set of the Human3.6m dataset. The errors are reported in <span class="ltx_text ltx_font_italic" id="S4.T1.7.2.1">mm</span>.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.3.3.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.3.3.4.1" style="font-size:70%;">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T1.3.3.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.3.3.5.1" style="font-size:70%;">Backbone</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T1.3.3.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.3.3.6.1" style="font-size:70%;">Frames</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1" style="font-size:70%;">MPJPE<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T1.1.1.1.1.m1.1"><semantics id="S4.T1.1.1.1.1.m1.1a"><mo id="S4.T1.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.1.1.1.1.m1.1d">↓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.2.1" style="font-size:70%;">PA-MPJPE<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T1.2.2.2.1.m1.1"><semantics id="S4.T1.2.2.2.1.m1.1a"><mo id="S4.T1.2.2.2.1.m1.1.1" stretchy="false" xref="S4.T1.2.2.2.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.1.m1.1b"><ci id="S4.T1.2.2.2.1.m1.1.1.cmml" xref="S4.T1.2.2.2.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.2.2.2.1.m1.1d">↓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.3.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.3.3.3.1" style="font-size:70%;">N-MPJPE<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T1.3.3.3.1.m1.1"><semantics id="S4.T1.3.3.3.1.m1.1a"><mo id="S4.T1.3.3.3.1.m1.1.1" stretchy="false" xref="S4.T1.3.3.3.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.1.m1.1b"><ci id="S4.T1.3.3.3.1.m1.1.1.cmml" xref="S4.T1.3.3.3.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.3.3.3.1.m1.1d">↓</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.4.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" colspan="6" id="S4.T1.3.4.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_italic" id="S4.T1.3.4.1.1.1" style="font-size:70%;color:#737373;">3D Supervision</span></th>
</tr>
<tr class="ltx_tr" id="S4.T1.3.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.3.5.2.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.5.2.1.1" style="font-size:70%;color:#737373;">Learnable Triangulation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib19" title="">19</a>]</cite></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T1.3.5.2.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.5.2.2.1" style="font-size:70%;color:#737373;">ResNet152</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T1.3.5.2.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.5.2.3.1" style="font-size:70%;color:#737373;">1</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.5.2.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.5.2.4.1" style="font-size:70%;color:#737373;">20.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.5.2.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.5.2.5.1" style="font-size:70%;color:#737373;">17.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.5.2.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.5.2.6.1" style="font-size:70%;color:#737373;">-</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.3.6.3.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.6.3.1.1" style="font-size:70%;color:#737373;">Canonical Fusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib49" title="">49</a>]</cite></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.6.3.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.6.3.2.1" style="font-size:70%;color:#737373;">ResNet152</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.6.3.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.6.3.3.1" style="font-size:70%;color:#737373;">1</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.3.6.3.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.6.3.4.1" style="font-size:70%;color:#737373;">21.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.6.3.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.6.3.5.1" style="font-size:70%;color:#737373;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.6.3.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.6.3.6.1" style="font-size:70%;color:#737373;">-</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.3.7.4.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.7.4.1.1" style="font-size:70%;color:#737373;">AdaFuse <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib66" title="">66</a>]</cite></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.7.4.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.7.4.2.1" style="font-size:70%;color:#737373;">ResNet152</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.7.4.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.7.4.3.1" style="font-size:70%;color:#737373;">1</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.3.7.4.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.7.4.4.1" style="font-size:70%;color:#737373;">19.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.7.4.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.7.4.5.1" style="font-size:70%;color:#737373;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.7.4.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.7.4.6.1" style="font-size:70%;color:#737373;">-</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.8.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.3.8.5.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.8.5.1.1" style="font-size:70%;color:#737373;">TesseTrack <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib48" title="">48</a>]</cite></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.8.5.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.8.5.2.1" style="font-size:70%;color:#737373;">HRNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib57" title="">57</a>]</cite></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.8.5.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.8.5.3.1" style="font-size:70%;color:#737373;">5</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.3.8.5.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.8.5.4.1" style="font-size:70%;color:#737373;">18.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.8.5.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.8.5.5.1" style="font-size:70%;color:#737373;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.8.5.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.8.5.6.1" style="font-size:70%;color:#737373;">-</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.9.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.3.9.6.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.9.6.1.1" style="font-size:70%;color:#737373;">MTF-Transformer<span class="ltx_text ltx_font_typewriter" id="S4.T1.3.9.6.1.1.1">+<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_serif" id="S4.T1.3.9.6.1.1.1.1.1">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib55" title="">55</a><span class="ltx_text ltx_font_serif" id="S4.T1.3.9.6.1.1.1.2.2">]</span></cite></span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.9.6.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.9.6.2.1" style="font-size:70%;color:#737373;">CPN</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.9.6.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.9.6.3.1" style="font-size:70%;color:#737373;">1</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.3.9.6.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.9.6.4.1" style="font-size:70%;color:#737373;">26.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.9.6.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.9.6.5.1" style="font-size:70%;color:#737373;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.9.6.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.9.6.6.1" style="font-size:70%;color:#737373;">-</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.10.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.3.10.7.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.10.7.1.1" style="font-size:70%;color:#737373;">MTF-Transformer<span class="ltx_text ltx_font_typewriter" id="S4.T1.3.10.7.1.1.1">+<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_serif" id="S4.T1.3.10.7.1.1.1.1.1">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib55" title="">55</a><span class="ltx_text ltx_font_serif" id="S4.T1.3.10.7.1.1.1.2.2">]</span></cite></span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.10.7.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.10.7.2.1" style="font-size:70%;color:#737373;">CPN</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.10.7.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.10.7.3.1" style="font-size:70%;color:#737373;">27</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.3.10.7.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.10.7.4.1" style="font-size:70%;color:#737373;">25.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.10.7.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.10.7.5.1" style="font-size:70%;color:#737373;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.10.7.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.10.7.6.1" style="font-size:70%;color:#737373;">-</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.11.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.3.11.8.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.11.8.1.1" style="font-size:70%;color:#737373;">Flex <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib10" title="">10</a>]</cite></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.11.8.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.11.8.2.1" style="font-size:70%;color:#737373;">ResNet152</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.11.8.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.11.8.3.1" style="font-size:70%;color:#737373;">All</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.3.11.8.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.11.8.4.1" style="font-size:70%;color:#737373;">30.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.11.8.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.11.8.5.1" style="font-size:70%;color:#737373;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.11.8.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.11.8.6.1" style="font-size:70%;color:#737373;">-</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.12.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.3.12.9.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.12.9.1.1" style="font-size:70%;color:#737373;">Jiang <em class="ltx_emph ltx_font_italic" id="S4.T1.3.12.9.1.1.1">et al</em>.<span class="ltx_text" id="S4.T1.3.12.9.1.1.2"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib20" title="">20</a>]</cite></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.12.9.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.12.9.2.1" style="font-size:70%;color:#737373;">ResNet152</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.12.9.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.12.9.3.1" style="font-size:70%;color:#737373;">1</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.3.12.9.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.12.9.4.1" style="font-size:70%;color:#737373;">27.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.12.9.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.12.9.5.1" style="font-size:70%;color:#737373;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.12.9.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.12.9.6.1" style="font-size:70%;color:#737373;">-</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.13.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" colspan="6" id="S4.T1.3.13.10.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_italic" id="S4.T1.3.13.10.1.1" style="font-size:70%;">2D Supervision</span></th>
</tr>
<tr class="ltx_tr" id="S4.T1.3.14.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.3.14.11.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T1.3.14.11.1.1" style="font-size:70%;">EpipolarPose </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.3.14.11.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib27" title="">27</a><span class="ltx_text" id="S4.T1.3.14.11.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T1.3.14.11.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.14.11.2.1" style="font-size:70%;">ResNet50</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T1.3.14.11.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.14.11.3.1" style="font-size:70%;">1</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.14.11.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.14.11.4.1" style="font-size:70%;">55.08</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.14.11.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.14.11.5.1" style="font-size:70%;">47.91</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.14.11.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.14.11.6.1" style="font-size:70%;">54.90</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.15.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.3.15.12.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T1.3.15.12.1.1" style="font-size:70%;">AniPose </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.3.15.12.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib23" title="">23</a><span class="ltx_text" id="S4.T1.3.15.12.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.15.12.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.15.12.2.1" style="font-size:70%;">GT</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.15.12.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.15.12.3.1" style="font-size:70%;">1</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.3.15.12.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.15.12.4.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.15.12.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.15.12.5.1" style="font-size:70%;">75.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.15.12.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.15.12.6.1" style="font-size:70%;">103.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.16.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.3.16.13.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T1.3.16.13.1.1" style="font-size:70%;">MetaPose </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.3.16.13.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib60" title="">60</a><span class="ltx_text" id="S4.T1.3.16.13.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.16.13.2" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T1.3.16.13.2.1" style="font-size:70%;">PoseNet </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.3.16.13.2.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib24" title="">24</a><span class="ltx_text" id="S4.T1.3.16.13.2.3.2" style="font-size:70%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.16.13.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.16.13.3.1" style="font-size:70%;">1</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.3.16.13.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.16.13.4.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.16.13.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.16.13.5.1" style="font-size:70%;">32.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.16.13.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.16.13.6.1" style="font-size:70%;">49.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.17.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.3.17.14.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T1.3.17.14.1.1" style="font-size:70%;">DLT </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.3.17.14.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib47" title="">47</a><span class="ltx_text" id="S4.T1.3.17.14.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.17.14.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.17.14.2.1" style="font-size:70%;">ResNet152</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.17.14.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.17.14.3.1" style="font-size:70%;">1</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.3.17.14.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.17.14.4.1" style="font-size:70%;">36.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.17.14.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.17.14.5.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.17.14.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.17.14.6.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.18.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.3.18.15.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T1.3.18.15.1.1" style="font-size:70%;">DLT </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.3.18.15.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib5" title="">5</a><span class="ltx_text" id="S4.T1.3.18.15.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.18.15.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.18.15.2.1" style="font-size:70%;">CPN</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.18.15.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.18.15.3.1" style="font-size:70%;">1</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.3.18.15.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.18.15.4.1" style="font-size:70%;">30.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.18.15.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.18.15.5.1" style="font-size:70%;">27.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.18.15.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.18.15.6.1" style="font-size:70%;">29.8</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.19.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.3.19.16.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.19.16.1.1" style="font-size:70%;">UPose3D (Ours)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.19.16.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.19.16.2.1" style="font-size:70%;">ResNet152</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.19.16.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.19.16.3.1" style="font-size:70%;">1</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.3.19.16.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.19.16.4.1" style="font-size:70%;">31.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.19.16.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.19.16.5.1" style="font-size:70%;">29.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.19.16.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.19.16.6.1" style="font-size:70%;">31.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.20.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.3.20.17.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.20.17.1.1" style="font-size:70%;">UPose3D (Ours)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.20.17.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.20.17.2.1" style="font-size:70%;">ResNet152</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.20.17.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.20.17.3.1" style="font-size:70%;">27</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.3.20.17.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.20.17.4.1" style="font-size:70%;">29.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.20.17.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.20.17.5.1" style="font-size:70%;">27.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.20.17.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.20.17.6.1" style="font-size:70%;">29.8</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.21.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.3.21.18.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.21.18.1.1" style="font-size:70%;">UPose3D (Ours)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.21.18.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.21.18.2.1" style="font-size:70%;">CPN</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.21.18.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.21.18.3.1" style="font-size:70%;">1</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.3.21.18.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.21.18.4.1" style="font-size:70%;">26.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.21.18.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.21.18.5.1" style="font-size:70%;">24.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.21.18.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.21.18.6.1" style="font-size:70%;">26.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.22.19">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.3.22.19.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.22.19.1.1" style="font-size:70%;">UPose3D (Ours)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.22.19.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.22.19.2.1" style="font-size:70%;">CPN</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.22.19.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.22.19.3.1" style="font-size:70%;">27</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.3.22.19.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.22.19.4.1" style="font-size:70%;">26.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.22.19.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.22.19.5.1" style="font-size:70%;">23.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.22.19.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.22.19.6.1" style="font-size:70%;">25.6</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.23.20">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" colspan="6" id="S4.T1.3.23.20.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_italic" id="S4.T1.3.23.20.1.1" style="font-size:70%;">2D Supervision <span class="ltx_text ltx_font_typewriter" id="S4.T1.3.23.20.1.1.1">+</span> Camera Parameters</span></th>
</tr>
<tr class="ltx_tr" id="S4.T1.3.24.21">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.3.24.21.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T1.3.24.21.1.1" style="font-size:70%;">TransFusion </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.3.24.21.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib41" title="">41</a><span class="ltx_text" id="S4.T1.3.24.21.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T1.3.24.21.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.24.21.2.1" style="font-size:70%;">ResNet50</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T1.3.24.21.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.24.21.3.1" style="font-size:70%;">1</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.24.21.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.24.21.4.1" style="font-size:70%;">25.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.24.21.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.24.21.5.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.24.21.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.24.21.6.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.25.22">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.3.25.22.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T1.3.25.22.1.1" style="font-size:70%;">DLT</span><sup class="ltx_sup" id="S4.T1.3.25.22.1.2"><span class="ltx_text" id="S4.T1.3.25.22.1.2.1" style="font-size:70%;">*</span></sup><span class="ltx_text" id="S4.T1.3.25.22.1.3" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.3.25.22.1.4.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib13" title="">13</a><span class="ltx_text" id="S4.T1.3.25.22.1.5.2" style="font-size:70%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.25.22.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.25.22.2.1" style="font-size:70%;">ResNet50</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.25.22.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.25.22.3.1" style="font-size:70%;">1</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.3.25.22.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.25.22.4.1" style="font-size:70%;">71.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.25.22.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.25.22.5.1" style="font-size:70%;">62.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.25.22.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.25.22.6.1" style="font-size:70%;">70.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.26.23">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.3.26.23.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T1.3.26.23.1.1" style="font-size:70%;">UPose3D (Ours)</span><sup class="ltx_sup" id="S4.T1.3.26.23.1.2"><span class="ltx_text" id="S4.T1.3.26.23.1.2.1" style="font-size:70%;">*</span></sup>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.26.23.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.26.23.2.1" style="font-size:70%;">ResNet50</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.26.23.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.26.23.3.1" style="font-size:70%;">27</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.3.26.23.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.26.23.4.1" style="font-size:70%;">57.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.26.23.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.26.23.5.1" style="font-size:70%;">51.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.26.23.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.26.23.6.1" style="font-size:70%;">57.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.27.24">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.3.27.24.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T1.3.27.24.1.1" style="font-size:70%;">Cross-view Fusion </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.3.27.24.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib47" title="">47</a><span class="ltx_text" id="S4.T1.3.27.24.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.27.24.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.27.24.2.1" style="font-size:70%;">ResNet152</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.27.24.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.27.24.3.1" style="font-size:70%;">1</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.3.27.24.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.27.24.4.1" style="font-size:70%;">26.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.27.24.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.27.24.5.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.27.24.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.27.24.6.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.28.25">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.3.28.25.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T1.3.28.25.1.1" style="font-size:70%;">Epipolar Transformers </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.3.28.25.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib14" title="">14</a><span class="ltx_text" id="S4.T1.3.28.25.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.28.25.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.28.25.2.1" style="font-size:70%;">ResNet152</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.28.25.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.28.25.3.1" style="font-size:70%;">1</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.3.28.25.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.28.25.4.1" style="font-size:70%;">19.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.28.25.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.28.25.5.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.28.25.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.28.25.6.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.29.26">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.3.29.26.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T1.3.29.26.1.1" style="font-size:70%;">TR Loss</span><sup class="ltx_sup" id="S4.T1.3.29.26.1.2"><span class="ltx_text" id="S4.T1.3.29.26.1.2.1" style="font-size:70%;">✝</span></sup><span class="ltx_text" id="S4.T1.3.29.26.1.3" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.3.29.26.1.4.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib67" title="">67</a><span class="ltx_text" id="S4.T1.3.29.26.1.5.2" style="font-size:70%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.29.26.2" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S4.T1.3.29.26.2.1" style="font-size:70%;">HRNet </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.3.29.26.2.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib57" title="">57</a><span class="ltx_text" id="S4.T1.3.29.26.2.3.2" style="font-size:70%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.29.26.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.29.26.3.1" style="font-size:70%;">1</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.3.29.26.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.29.26.4.1" style="font-size:70%;">25.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.29.26.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.29.26.5.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.29.26.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.29.26.6.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.30.27">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.3.30.27.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.30.27.1.1" style="font-size:70%;">UPose3D (Ours)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.30.27.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.30.27.2.1" style="font-size:70%;">CPN</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.30.27.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.30.27.3.1" style="font-size:70%;">27</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.3.30.27.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.30.27.4.1" style="font-size:70%;">26.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.30.27.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.30.27.5.1" style="font-size:70%;">23.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.30.27.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.30.27.6.1" style="font-size:70%;">25.4</span></td>
</tr>
</tbody>
<tfoot class="ltx_tfoot">
<tr class="ltx_tr" id="S4.T1.3.31.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="6" id="S4.T1.3.31.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.31.1.1.1" style="font-size:70%;">- denotes the error was not reported in the original work.</span></th>
</tr>
<tr class="ltx_tr" id="S4.T1.3.32.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" colspan="6" id="S4.T1.3.32.2.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.32.2.1.1" style="font-size:70%;">* denotes training from scratch with no additional data.</span></th>
</tr>
<tr class="ltx_tr" id="S4.T1.3.33.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" colspan="6" id="S4.T1.3.33.3.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.3.33.3.1.1" style="font-size:70%;">✝ denotes using 1% of unlabeled testing data during training.</span></th>
</tr>
</tfoot>
</table>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>In-Distribution Performance</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">In <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S4.T1" title="In 4.4 Baselines ‣ 4 Experiments ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a>, we present a comparison of our InD results with other state-of-the-art methods on the Human3.6m dataset, divided into three categories: <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.1">a</span>) methods that, unlike ours, are trained directly with 3D annotated data; <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.2">b</span>) methods that leverage only 2D supervision; and <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.3">c</span>) methods that rely on 2D supervision along with the camera parameters. We observe that UPose3D outperforms all other methods that rely on 2D supervision. In addition, it yields results that are competitive to state-of-the-art methods relying on 3D supervision. For example, our method achieves similar results to MTF-Transformers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib55" title="">55</a>]</cite>, which, like our pose compiler, takes in multi-view and temporal 2D keypoints as input but estimates the 3D poses directly using a deep network with 10.1<span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.4">M</span> parameters. Furthermore, among methods that use the camera parameters of the Human3.6m dataset, our method performs similarly to TR Loss <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib67" title="">67</a>]</cite>, which uses 1% of the unlabeled <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.5">testing</span> set during training. Although Epipolar Transformers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib14" title="">14</a>]</cite> perform better than ours due to their effective feature fusion strategy, it relies on precise camera parameters and has constraints on the viewing angle of neighbouring cameras. The consistent performance of our approach in both groups indicates the effectiveness of our proposed pose compiler and training strategies in obtaining precise keypoints regardless of the target dataset’s camera setup.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">We also show that the choice of the 2D keypoint estimator backbone is an important factor for accuracy and that CPN is preferred over ResNet152. Interestingly, a vanilla triangulation algorithm, DLT, can outperform prior works given accurate 2D predictions from strong pose estimators. Combining strong pose estimators with our method yields even better performance. For instance, UPose3D improves the MPJPE by 3.6 <span class="ltx_text ltx_font_italic" id="S5.SS1.p2.1.1">mm</span> compared to vanilla triangulation using a CPN backbone (DLT<span class="ltx_text ltx_font_typewriter" id="S5.SS1.p2.1.2">+</span>CPN). Additionally, we experiment with a ResNet-50 model trained from scratch in the third group (2D supervised with camera parameters) and observe a 19.5% improvement when UPose3D is used. Finally, we demonstrate the positive impact of the temporal frame window, which reduces the MPJPE by 0.5 <span class="ltx_text ltx_font_italic" id="S5.SS1.p2.1.3">mm</span> when using 27 frames instead of a single frame.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Out-of-Distribution Generalization</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">To evaluate the performance of UPose3D in OoD settings, we compare it to the best-performing baseline on the Human3.6m <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib18" title="">18</a>]</cite> dataset, namely AdaFuse <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib66" title="">66</a>]</cite>. Additionally, we report the results for triangulation techniques applied to 2D pose estimators in <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S5.T2" title="In 5.2 Out-of-Distribution Generalization ‣ 5 Results ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">2</span></a>. In this experiment setup, neither the models nor their components are trained on the RICH <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib16" title="">16</a>]</cite> dataset. First, we observe that our method, despite the training source dataset, obtains the best results by achieving half of the error of the next best method. Additionally, we observe that AdaFuse performs poorly on this dataset despite being considered state-of-the-art on the Human3.6m <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib18" title="">18</a>]</cite> dataset. Our analysis of AdaFuse performance on different clips of RICH <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib16" title="">16</a>]</cite> indicates that this method performs well only if all the viewpoints’ predictions are within a reasonable range, whereas occasional noisy predictions cause large triangulation errors. In contrast, our solution deduces the correct pose by effectively discarding views with low confidence. In conclusion, when training on in-studio datasets, methods such as AdaFuse fail to adapt to in-the-wild environments. At the same time, our approach uses uncertainty modeling to better generalize to unseen camera configurations. UPose3D is not limited to the source dataset skeleton configuration because synthetic data are used for training, and it performs consistently between InD and OoD evaluations.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T2.8.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S5.T2.9.2" style="font-size:90%;">Comparison of our method in OoD setting on the RICH <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib16" title="">16</a>]</cite> dataset against prior works. The training sources are denoted as (2D pose estimator, Pose compiler).</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T2.6">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.2.2.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.2.2.3.1" style="font-size:70%;">Method</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.2.2.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.2.2.4.1" style="font-size:70%;">Training sources</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1" style="font-size:70%;">MPJPE<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T2.1.1.1.1.m1.1"><semantics id="S5.T2.1.1.1.1.m1.1a"><mo id="S5.T2.1.1.1.1.m1.1.1" stretchy="false" xref="S5.T2.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.1.m1.1b"><ci id="S5.T2.1.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.1.1.1.1.m1.1d">↓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.2.2.2.1" style="font-size:70%;">PA-MPJPE<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T2.2.2.2.1.m1.1"><semantics id="S5.T2.2.2.2.1.m1.1a"><mo id="S5.T2.2.2.2.1.m1.1.1" stretchy="false" xref="S5.T2.2.2.2.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.1.m1.1b"><ci id="S5.T2.2.2.2.1.m1.1.1.cmml" xref="S5.T2.2.2.2.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.2.2.2.1.m1.1d">↓</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.7.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S5.T2.6.7.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text" id="S5.T2.6.7.1.1.1" style="font-size:70%;">AdaFuse</span><sup class="ltx_sup" id="S5.T2.6.7.1.1.2"><span class="ltx_text" id="S5.T2.6.7.1.1.2.1" style="font-size:70%;">*</span></sup><span class="ltx_text" id="S5.T2.6.7.1.1.3" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T2.6.7.1.1.4.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib66" title="">66</a><span class="ltx_text" id="S5.T2.6.7.1.1.5.2" style="font-size:70%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S5.T2.6.7.1.2" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text" id="S5.T2.6.7.1.2.1" style="font-size:70%;">(Human3.6m</span><span class="ltx_text ltx_font_typewriter" id="S5.T2.6.7.1.2.2" style="font-size:70%;">+</span><span class="ltx_text" id="S5.T2.6.7.1.2.3" style="font-size:70%;">MPII, Human3.6m)</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.6.7.1.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T2.6.7.1.3.1" style="font-size:70%;">524.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.6.7.1.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T2.6.7.1.4.1" style="font-size:70%;">85.8</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.3.3.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text" id="S5.T2.3.3.1.1" style="font-size:70%;">Ours (</span><math alttext="T=27" class="ltx_Math" display="inline" id="S5.T2.3.3.1.m1.1"><semantics id="S5.T2.3.3.1.m1.1a"><mrow id="S5.T2.3.3.1.m1.1.1" xref="S5.T2.3.3.1.m1.1.1.cmml"><mi id="S5.T2.3.3.1.m1.1.1.2" mathsize="70%" xref="S5.T2.3.3.1.m1.1.1.2.cmml">T</mi><mo id="S5.T2.3.3.1.m1.1.1.1" mathsize="70%" xref="S5.T2.3.3.1.m1.1.1.1.cmml">=</mo><mn id="S5.T2.3.3.1.m1.1.1.3" mathsize="70%" xref="S5.T2.3.3.1.m1.1.1.3.cmml">27</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.1.m1.1b"><apply id="S5.T2.3.3.1.m1.1.1.cmml" xref="S5.T2.3.3.1.m1.1.1"><eq id="S5.T2.3.3.1.m1.1.1.1.cmml" xref="S5.T2.3.3.1.m1.1.1.1"></eq><ci id="S5.T2.3.3.1.m1.1.1.2.cmml" xref="S5.T2.3.3.1.m1.1.1.2">𝑇</ci><cn id="S5.T2.3.3.1.m1.1.1.3.cmml" type="integer" xref="S5.T2.3.3.1.m1.1.1.3">27</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.1.m1.1c">T=27</annotation><annotation encoding="application/x-llamapun" id="S5.T2.3.3.1.m1.1d">italic_T = 27</annotation></semantics></math><span class="ltx_text" id="S5.T2.3.3.1.2" style="font-size:70%;">)</span>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.3.3.2" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text" id="S5.T2.3.3.2.1" style="font-size:70%;">(Human3.6m</span><span class="ltx_text ltx_font_typewriter" id="S5.T2.3.3.2.2" style="font-size:70%;">+</span><span class="ltx_text" id="S5.T2.3.3.2.3" style="font-size:70%;">MPII, Human3.6m)</span>
</th>
<td class="ltx_td ltx_align_center" id="S5.T2.3.3.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T2.3.3.3.1" style="font-size:70%;">51.8</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.3.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T2.3.3.4.1" style="font-size:70%;">43.6</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.4.4.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text" id="S5.T2.4.4.1.1" style="font-size:70%;">Ours (</span><math alttext="T=1" class="ltx_Math" display="inline" id="S5.T2.4.4.1.m1.1"><semantics id="S5.T2.4.4.1.m1.1a"><mrow id="S5.T2.4.4.1.m1.1.1" xref="S5.T2.4.4.1.m1.1.1.cmml"><mi id="S5.T2.4.4.1.m1.1.1.2" mathsize="70%" xref="S5.T2.4.4.1.m1.1.1.2.cmml">T</mi><mo id="S5.T2.4.4.1.m1.1.1.1" mathsize="70%" xref="S5.T2.4.4.1.m1.1.1.1.cmml">=</mo><mn id="S5.T2.4.4.1.m1.1.1.3" mathsize="70%" xref="S5.T2.4.4.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.4.4.1.m1.1b"><apply id="S5.T2.4.4.1.m1.1.1.cmml" xref="S5.T2.4.4.1.m1.1.1"><eq id="S5.T2.4.4.1.m1.1.1.1.cmml" xref="S5.T2.4.4.1.m1.1.1.1"></eq><ci id="S5.T2.4.4.1.m1.1.1.2.cmml" xref="S5.T2.4.4.1.m1.1.1.2">𝑇</ci><cn id="S5.T2.4.4.1.m1.1.1.3.cmml" type="integer" xref="S5.T2.4.4.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.4.1.m1.1c">T=1</annotation><annotation encoding="application/x-llamapun" id="S5.T2.4.4.1.m1.1d">italic_T = 1</annotation></semantics></math><span class="ltx_text" id="S5.T2.4.4.1.2" style="font-size:70%;">)</span>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.4.4.2" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text" id="S5.T2.4.4.2.1" style="font-size:70%;">(Human3.6m</span><span class="ltx_text ltx_font_typewriter" id="S5.T2.4.4.2.2" style="font-size:70%;">+</span><span class="ltx_text" id="S5.T2.4.4.2.3" style="font-size:70%;">MPII, AMASS)</span>
</th>
<td class="ltx_td ltx_align_center" id="S5.T2.4.4.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T2.4.4.3.1" style="font-size:70%;">49.9</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.4.4.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T2.4.4.4.1" style="font-size:70%;">42.3</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.8.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.6.8.2.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text" id="S5.T2.6.8.2.1.1" style="font-size:70%;">HRNet-W48</span><span class="ltx_text ltx_font_typewriter" id="S5.T2.6.8.2.1.2" style="font-size:70%;">+</span><span class="ltx_text" id="S5.T2.6.8.2.1.3" style="font-size:70%;">DLT</span><sup class="ltx_sup" id="S5.T2.6.8.2.1.4"><span class="ltx_text" id="S5.T2.6.8.2.1.4.1" style="font-size:70%;">*</span></sup>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.6.8.2.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T2.6.8.2.2.1" style="font-size:70%;">(COCO, N/A)</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.8.2.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T2.6.8.2.3.1" style="font-size:70%;">66.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.8.2.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T2.6.8.2.4.1" style="font-size:70%;">55.1</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.9.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.6.9.3.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text" id="S5.T2.6.9.3.1.1" style="font-size:70%;">HRNet-W48</span><span class="ltx_text ltx_font_typewriter" id="S5.T2.6.9.3.1.2" style="font-size:70%;">+</span><span class="ltx_text" id="S5.T2.6.9.3.1.3" style="font-size:70%;">Grid Search</span><sup class="ltx_sup" id="S5.T2.6.9.3.1.4"><span class="ltx_text" id="S5.T2.6.9.3.1.4.1" style="font-size:70%;">*</span></sup>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.6.9.3.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T2.6.9.3.2.1" style="font-size:70%;">(COCO, N/A)</span></th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.9.3.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T2.6.9.3.3.1" style="font-size:70%;">64.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.9.3.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T2.6.9.3.4.1" style="font-size:70%;">54.9</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.5.5.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text" id="S5.T2.5.5.1.1" style="font-size:70%;">Ours (</span><math alttext="T=1" class="ltx_Math" display="inline" id="S5.T2.5.5.1.m1.1"><semantics id="S5.T2.5.5.1.m1.1a"><mrow id="S5.T2.5.5.1.m1.1.1" xref="S5.T2.5.5.1.m1.1.1.cmml"><mi id="S5.T2.5.5.1.m1.1.1.2" mathsize="70%" xref="S5.T2.5.5.1.m1.1.1.2.cmml">T</mi><mo id="S5.T2.5.5.1.m1.1.1.1" mathsize="70%" xref="S5.T2.5.5.1.m1.1.1.1.cmml">=</mo><mn id="S5.T2.5.5.1.m1.1.1.3" mathsize="70%" xref="S5.T2.5.5.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.5.5.1.m1.1b"><apply id="S5.T2.5.5.1.m1.1.1.cmml" xref="S5.T2.5.5.1.m1.1.1"><eq id="S5.T2.5.5.1.m1.1.1.1.cmml" xref="S5.T2.5.5.1.m1.1.1.1"></eq><ci id="S5.T2.5.5.1.m1.1.1.2.cmml" xref="S5.T2.5.5.1.m1.1.1.2">𝑇</ci><cn id="S5.T2.5.5.1.m1.1.1.3.cmml" type="integer" xref="S5.T2.5.5.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.5.5.1.m1.1c">T=1</annotation><annotation encoding="application/x-llamapun" id="S5.T2.5.5.1.m1.1d">italic_T = 1</annotation></semantics></math><span class="ltx_text" id="S5.T2.5.5.1.2" style="font-size:70%;">)</span>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.5.5.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T2.5.5.2.1" style="font-size:70%;">(COCO, AMASS)</span></th>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T2.5.5.3.1" style="font-size:70%;">36.2</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T2.5.5.4.1" style="font-size:70%;">33.4</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.6.6.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text" id="S5.T2.6.6.1.1" style="font-size:70%;">Ours (</span><math alttext="T=27" class="ltx_Math" display="inline" id="S5.T2.6.6.1.m1.1"><semantics id="S5.T2.6.6.1.m1.1a"><mrow id="S5.T2.6.6.1.m1.1.1" xref="S5.T2.6.6.1.m1.1.1.cmml"><mi id="S5.T2.6.6.1.m1.1.1.2" mathsize="70%" xref="S5.T2.6.6.1.m1.1.1.2.cmml">T</mi><mo id="S5.T2.6.6.1.m1.1.1.1" mathsize="70%" xref="S5.T2.6.6.1.m1.1.1.1.cmml">=</mo><mn id="S5.T2.6.6.1.m1.1.1.3" mathsize="70%" xref="S5.T2.6.6.1.m1.1.1.3.cmml">27</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.6.6.1.m1.1b"><apply id="S5.T2.6.6.1.m1.1.1.cmml" xref="S5.T2.6.6.1.m1.1.1"><eq id="S5.T2.6.6.1.m1.1.1.1.cmml" xref="S5.T2.6.6.1.m1.1.1.1"></eq><ci id="S5.T2.6.6.1.m1.1.1.2.cmml" xref="S5.T2.6.6.1.m1.1.1.2">𝑇</ci><cn id="S5.T2.6.6.1.m1.1.1.3.cmml" type="integer" xref="S5.T2.6.6.1.m1.1.1.3">27</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.6.6.1.m1.1c">T=27</annotation><annotation encoding="application/x-llamapun" id="S5.T2.6.6.1.m1.1d">italic_T = 27</annotation></semantics></math><span class="ltx_text" id="S5.T2.6.6.1.2" style="font-size:70%;">)</span>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.6.6.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T2.6.6.2.1" style="font-size:70%;">(COCO, AMASS)</span></th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T2.6.6.3.1" style="font-size:70%;">34.7</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T2.6.6.4.1" style="font-size:70%;">32.0</span></td>
</tr>
</tbody>
<tfoot class="ltx_tfoot">
<tr class="ltx_tr" id="S5.T2.6.10.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="4" id="S5.T2.6.10.1.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T2.6.10.1.1.1" style="font-size:70%;">* denotes our implementation of prior works.</span></th>
</tr>
</tfoot>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Ablation Study</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">To investigate the impact of each component, we systematically remove them during <span class="ltx_text ltx_font_italic" id="S5.SS3.p1.1.1">inference</span> and report the results with a CPN backbone on the test set of Human3.6m <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib18" title="">18</a>]</cite>. We perform all experiments using a model with a temporal window of 27 frames. Please refer to our Supplementary Materials for additional analysis on temporal length, spatiotemporal encoder’s architecture, different point cloud formulations, and initialization strategies for 3D estimation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p2.1.1">Pose Compiler.</span>
We employ the pose compiler module to improve the keypoint and uncertainty predictions using cross-view and spatiotemporal information. By ablating this component, we use the original keypoints and uncertainties predicted by our 2D pose estimator to perform MLE. As shown in <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S5.T3" title="In 5.3 Ablation Study ‣ 5 Results ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">3</span></a>, this experiment results in an additional 9.28 <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.2">mm</span> error resulting in a higher error compared to the DLT algorithm (see <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S4.T1" title="In 4.4 Baselines ‣ 4 Experiments ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a>). This shows the effectiveness of our pose compiler for 3D pose estimation using normalizing flows.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T3.6.2.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S5.T3.2.1" style="font-size:90%;">Ablation experiments on the Human3.6m dataset with <math alttext="T=27" class="ltx_Math" display="inline" id="S5.T3.2.1.m1.1"><semantics id="S5.T3.2.1.m1.1b"><mrow id="S5.T3.2.1.m1.1.1" xref="S5.T3.2.1.m1.1.1.cmml"><mi id="S5.T3.2.1.m1.1.1.2" xref="S5.T3.2.1.m1.1.1.2.cmml">T</mi><mo id="S5.T3.2.1.m1.1.1.1" xref="S5.T3.2.1.m1.1.1.1.cmml">=</mo><mn id="S5.T3.2.1.m1.1.1.3" xref="S5.T3.2.1.m1.1.1.3.cmml">27</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.2.1.m1.1c"><apply id="S5.T3.2.1.m1.1.1.cmml" xref="S5.T3.2.1.m1.1.1"><eq id="S5.T3.2.1.m1.1.1.1.cmml" xref="S5.T3.2.1.m1.1.1.1"></eq><ci id="S5.T3.2.1.m1.1.1.2.cmml" xref="S5.T3.2.1.m1.1.1.2">𝑇</ci><cn id="S5.T3.2.1.m1.1.1.3.cmml" type="integer" xref="S5.T3.2.1.m1.1.1.3">27</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.1.m1.1d">T=27</annotation><annotation encoding="application/x-llamapun" id="S5.T3.2.1.m1.1e">italic_T = 27</annotation></semantics></math>.
</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T3.4.2.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.4.2.3.1" style="font-size:70%;">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.3.1.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.3.1.1.1" style="font-size:70%;">MPJPE<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T3.3.1.1.1.m1.1"><semantics id="S5.T3.3.1.1.1.m1.1a"><mo id="S5.T3.3.1.1.1.m1.1.1" stretchy="false" xref="S5.T3.3.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T3.3.1.1.1.m1.1b"><ci id="S5.T3.3.1.1.1.m1.1.1.cmml" xref="S5.T3.3.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.3.1.1.1.m1.1d">↓</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.2.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.4.2.2.1" style="font-size:70%;">PA-MPJPE<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T3.4.2.2.1.m1.1"><semantics id="S5.T3.4.2.2.1.m1.1a"><mo id="S5.T3.4.2.2.1.m1.1.1" stretchy="false" xref="S5.T3.4.2.2.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T3.4.2.2.1.m1.1b"><ci id="S5.T3.4.2.2.1.m1.1.1.cmml" xref="S5.T3.4.2.2.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.2.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.4.2.2.1.m1.1d">↓</annotation></semantics></math></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.4.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S5.T3.4.3.1.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.4.3.1.1.1" style="font-size:70%;">UPose3D</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.4.3.1.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T3.4.3.1.2.1" style="font-size:70%;">26.42</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.4.3.1.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T3.4.3.1.3.1" style="font-size:70%;">23.42</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.4.4.2.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T3.4.4.2.1.1" style="font-size:70%;">      w/o compiler</span></th>
<td class="ltx_td ltx_align_center" id="S5.T3.4.4.2.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T3.4.4.2.2.1" style="font-size:70%;">37.14</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.4.2.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T3.4.4.2.3.1" style="font-size:70%;">33.90</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.4.5.3.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T3.4.5.3.1.1" style="font-size:70%;">      w/o image branch</span></th>
<td class="ltx_td ltx_align_center" id="S5.T3.4.5.3.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T3.4.5.3.2.1" style="font-size:70%;">69.90</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.5.3.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T3.4.5.3.3.1" style="font-size:70%;">50.97</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.4.6.4.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T3.4.6.4.1.1" style="font-size:70%;">       w/o compiler uncertainty</span></th>
<td class="ltx_td ltx_align_center" id="S5.T3.4.6.4.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T3.4.6.4.2.1" style="font-size:70%;">26.42</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.6.4.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T3.4.6.4.3.1" style="font-size:70%;">23.58</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.4.7.5.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T3.4.7.5.1.1" style="font-size:70%;">       w/o image uncertainty</span></th>
<td class="ltx_td ltx_align_center" id="S5.T3.4.7.5.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T3.4.7.5.2.1" style="font-size:70%;">27.61</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.7.5.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T3.4.7.5.3.1" style="font-size:70%;">24.88</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.4.8.6.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T3.4.8.6.1.1" style="font-size:70%;">       w/o uncertainty</span></th>
<td class="ltx_td ltx_align_center" id="S5.T3.4.8.6.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T3.4.8.6.2.1" style="font-size:70%;">48.11</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.8.6.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T3.4.8.6.3.1" style="font-size:70%;">41.20</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.9.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.4.9.7.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T3.4.9.7.1.1" style="font-size:70%;">          w/o image branch</span></th>
<td class="ltx_td ltx_align_center" id="S5.T3.4.9.7.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T3.4.9.7.2.1" style="font-size:70%;">77.25</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.9.7.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T3.4.9.7.3.1" style="font-size:70%;">54.02</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.10.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S5.T3.4.10.8.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T3.4.10.8.1.1" style="font-size:70%;">          w/o compiler</span></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.4.10.8.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T3.4.10.8.2.1" style="font-size:70%;">30.48</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.4.10.8.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T3.4.10.8.3.1" style="font-size:70%;">27.63</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p3.1.1">Image Branch.</span>
Removing the image branch from the pipeline causes a significant rise in the error, seen in <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S4.T1" title="In 4.4 Baselines ‣ 4 Experiments ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a>, indicating the significance of keeping the original predictions for the final estimation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.p4">
<p class="ltx_p" id="S5.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p4.1.1">Uncertainty Modeling.</span>
We employ the normalizing flows of our RLE heads during the MLE loss minimization stage to incorporate the uncertainties of our predictions toward improving the model’s robustness.
We employ the RLE head’s normalizing flows module to model the uncertainties, improving robustness. To study the effect of uncertainty modeling, we remove the RLE heads from the 2D pose estimator and pose compiler branches. Accordingly, we first remove the uncertainties from our pose compiler and replace our likelihood loss function (<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S3.E1" title="In 3.4 3D Pose Estimation ‣ 3 Method ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Eq.</span> <span class="ltx_text ltx_ref_tag">1</span></a>) with a reprojection distance loss. <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S4.T1" title="In 4.4 Baselines ‣ 4 Experiments ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a> shows that the PA-MPJPE error slightly rises, but the MPJPE remains unchanged. However, it should be noted that removing the pose compiler uncertainty during <span class="ltx_text ltx_font_italic" id="S5.SS3.p4.1.2">training</span> causes more performance degradation by resulting in 26.8 <span class="ltx_text ltx_font_italic" id="S5.SS3.p4.1.3">mm</span>, which shows the significance of uncertainty modeling. Next, we remove the image pose estimator uncertainties, which results in a 1.2 <span class="ltx_text ltx_font_italic" id="S5.SS3.p4.1.4">mm</span> rise in the error. Finally, we completely remove the uncertainties from both branches, effectively reducing our problem to a classic triangulation problem without confidence that can be solved via the DLT algorithm. This variation shows over 20 <span class="ltx_text ltx_font_italic" id="S5.SS3.p4.1.5">mm</span> higher error than UPose3D. In our final two experiments, we analyze the image and pose compiler predictions without uncertainty modeling. The first experiment shows the highest error, while the second experiment results in the same performance as a simple DLT algorithm without cross-view fusion.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Computation Costs</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">In <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S5.T4" title="In 5.4 Computation Costs ‣ 5 Results ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">4</span></a>, we compare the total computation costs of our method in comparison to state-of-the-art 3D approaches in single-frame scenarios. Compared to others, UPose3D has fewer parameters and significantly less computational cost, especially with more cameras. The computational cost of our 10-frame model can be broken down to 508.5<span class="ltx_text ltx_font_italic" id="S5.SS4.p1.1.1">G</span> FLOPs for CPN, 0.385<span class="ltx_text ltx_font_italic" id="S5.SS4.p1.1.2">G</span> for pose compiler, and 8.8<math alttext="\pm" class="ltx_Math" display="inline" id="S5.SS4.p1.1.m1.1"><semantics id="S5.SS4.p1.1.m1.1a"><mo id="S5.SS4.p1.1.m1.1.1" xref="S5.SS4.p1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.1.m1.1b"><csymbol cd="latexml" id="S5.SS4.p1.1.m1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.1.m1.1d">±</annotation></semantics></math>1.25<span class="ltx_text ltx_font_italic" id="S5.SS4.p1.1.3">G</span> for the optimization process. Due to the computational cost fluctuations caused by the L-BFGS optimizer, we measure it by averaging 20 runs using randomly selected cameras from the HUMBI dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib64" title="">64</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib65" title="">65</a>]</cite>.</p>
</div>
<figure class="ltx_table" id="S5.T4">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T4.7.1.1" style="font-size:129%;">Table 4</span>: </span><span class="ltx_text" id="S5.T4.8.2" style="font-size:129%;">Comparison of computation costs.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T4.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T4.3.3.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S5.T4.3.3.4.1" style="font-size:70%;">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S5.T4.1.1.1.1" style="font-size:70%;">Params(</span><span class="ltx_text ltx_font_italic" id="S5.T4.1.1.1.2" style="font-size:70%;">M</span><span class="ltx_text" id="S5.T4.1.1.1.3" style="font-size:70%;">)</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T4.1.1.1.m1.1"><semantics id="S5.T4.1.1.1.m1.1a"><mo id="S5.T4.1.1.1.m1.1.1" mathsize="70%" stretchy="false" xref="S5.T4.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.m1.1b"><ci id="S5.T4.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.1.1.1.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.2.2.2" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S5.T4.2.2.2.1" style="font-size:70%;">FLOPs for 4 Cams(</span><span class="ltx_text ltx_font_italic" id="S5.T4.2.2.2.2" style="font-size:70%;">G</span><span class="ltx_text" id="S5.T4.2.2.2.3" style="font-size:70%;">)</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T4.2.2.2.m1.1"><semantics id="S5.T4.2.2.2.m1.1a"><mo id="S5.T4.2.2.2.m1.1.1" mathsize="70%" stretchy="false" xref="S5.T4.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T4.2.2.2.m1.1b"><ci id="S5.T4.2.2.2.m1.1.1.cmml" xref="S5.T4.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.2.2.2.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.3.3.3" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S5.T4.3.3.3.1" style="font-size:70%;">FLOPs for 10 Cams(</span><span class="ltx_text ltx_font_italic" id="S5.T4.3.3.3.2" style="font-size:70%;">G</span><span class="ltx_text" id="S5.T4.3.3.3.3" style="font-size:70%;">)</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T4.3.3.3.m1.1"><semantics id="S5.T4.3.3.3.m1.1a"><mo id="S5.T4.3.3.3.m1.1.1" mathsize="70%" stretchy="false" xref="S5.T4.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T4.3.3.3.m1.1b"><ci id="S5.T4.3.3.3.m1.1.1.cmml" xref="S5.T4.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.3.3.3.m1.1d">↓</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S5.T4.3.4.1.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S5.T4.3.4.1.1.1" style="font-size:70%;">Learnable Triangulation </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T4.3.4.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib19" title="">19</a><span class="ltx_text" id="S5.T4.3.4.1.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T4.3.4.1.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S5.T4.3.4.1.2.1" style="font-size:70%;">80.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T4.3.4.1.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S5.T4.3.4.1.3.1" style="font-size:70%;">716.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T4.3.4.1.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S5.T4.3.4.1.4.1" style="font-size:70%;">1326.9</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.3.5.2.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S5.T4.3.5.2.1.1" style="font-size:70%;">Epipolar Transformers </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T4.3.5.2.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib14" title="">14</a><span class="ltx_text" id="S5.T4.3.5.2.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T4.3.5.2.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S5.T4.3.5.2.2.1" style="font-size:70%;">68.1</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.5.2.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S5.T4.3.5.2.3.1" style="font-size:70%;">406.5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.5.2.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S5.T4.3.5.2.4.1" style="font-size:70%;">1016.2</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.3.6.3.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S5.T4.3.6.3.1.1" style="font-size:70%;">MTF-Transformers </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T4.3.6.3.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib55" title="">55</a><span class="ltx_text" id="S5.T4.3.6.3.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T4.3.6.3.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S5.T4.3.6.3.2.1" style="font-size:70%;">78.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.6.3.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S5.T4.3.6.3.3.1" style="font-size:70%;">407.0</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.6.3.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S5.T4.3.6.3.4.1" style="font-size:70%;">1017.8</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.3.7.4.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S5.T4.3.7.4.1.1" style="font-size:70%;">AdaFuse </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T4.3.7.4.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib66" title="">66</a><span class="ltx_text" id="S5.T4.3.7.4.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T4.3.7.4.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S5.T4.3.7.4.2.1" style="font-size:70%;">69.7</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.7.4.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S5.T4.3.7.4.3.1" style="font-size:70%;">595.0</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.7.4.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S5.T4.3.7.4.4.1" style="font-size:70%;">1487.6</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.8.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S5.T4.3.8.5.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S5.T4.3.8.5.1.1" style="font-size:70%;">Ours</span></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.3.8.5.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S5.T4.3.8.5.2.1" style="font-size:70%;">65.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.3.8.5.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S5.T4.3.8.5.3.1" style="font-size:70%;">208.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.3.8.5.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S5.T4.3.8.5.4.1" style="font-size:70%;">517.7</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="196" id="S5.F3.g1" src="x3.png" width="788"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S5.F3.3.2" style="font-size:90%;">We demonstrate the scalability of UPose3D to the number of cameras.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="546" id="S5.F4.g1" src="x4.png" width="706"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S5.F4.3.2" style="font-size:90%;">Illustration of UPose3D on Human3.6m <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib18" title="">18</a>]</cite> (right) and RICH <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib16" title="">16</a>]</cite> (left) datasets, showing the accurate 3D pose estimated by our UPose3D (top) compared to ground-truth (bottom).</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Viewpoint Scalability</h3>
<div class="ltx_para" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1">In the context of multi-view 3D human pose estimation, scalability may be defined as the capability of a method to handle data from an increasing number of camera views effectively. Specifically, such a solution should have an almost constant runtime with constantly decreasing error when more cameras are used. Accordingly, to test the scalability of our method, we evaluate it with different numbers of cameras on the CMU Panoptic <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib22" title="">22</a>]</cite> and the HUMBI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib64" title="">64</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib65" title="">65</a>]</cite> datasets and present the results in <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S5.F3" title="In 5.4 Computation Costs ‣ 5 Results ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>. For this experiment, we use the HRNet-W48 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib57" title="">57</a>]</cite> backbone, similar to our experiments on the RICH dataset, to analyze scalability in OoD scenarios. Additionally, we report and compare our error and runtime with two triangulation techniques, RANSAC and DLT. We tune the RANSAC implementation of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib19" title="">19</a>]</cite> and run it for 20 iterations. The error analysis shows that as the number of views increases, UPose3D’s error continuously improves while DLT and RANSAC reach a plateau. Next, we discard the common factor of the 2D pose estimators for runtime scalability analysis. The inference time of our pipeline with a single batch size remains constant despite the number of cameras. However, increasing the batch size causes runtime improvements at the cost of memory and runtime variability.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.6 </span>Qualitative Analysis</h3>
<div class="ltx_para" id="S5.SS6.p1">
<p class="ltx_p" id="S5.SS6.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S5.F4" title="In 5.4 Computation Costs ‣ 5 Results ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a> shows the outputs of our method on challenging examples, where we sample the keypoint distributions in every pixel to demonstrate the keypoint likelihoods from the 2D pose estimator and our pose compiler as heatmaps. Despite choosing challenging scenes, we observe that our predictions are still close to the ground-truth keypoints, indicating that our method does not produce any significant outliers within its output even on the unseen samples of RICH <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib16" title="">16</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">This paper presents UPose3D, a multi-view 3D human pose estimation method designed to address the challenges in generalization, scalability, and over-reliance on real-world 3D annotated data. It includes a novel cross-view fusion strategy that scales well with varying camera numbers and volume sizes. Additionally, UPose3D integrates a pose compiler that learns to predict keypoint positions and uncertainties given the cross-view and temporal information. Finally, the uncertainties and predicted key points from two sources of the image branch and pose compiler are used for uncertainty-aware 3D pose estimation. UPose3D outperforms state-of-the-art approaches in OoD setup and achieves competitive InD results without any 3D pose annotations. Therefore, UPose3D may positively impact applications where performance and robustness are crucial.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p2">
<p class="ltx_p" id="S6.p2.1"><span class="ltx_text ltx_font_bold" id="S6.p2.1.1">Future Work.</span>
Our primary focus in this paper was the fidelity of estimated output 3D poses from multi-view inputs without requiring 3D supervision. We acknowledge, however, that our method could be optimized for <span class="ltx_text ltx_font_italic" id="S6.p2.1.2">real-time</span> applications. A promising avenue to achieve faster inference involves the exploration of specialized second-order optimizers that use a deep-learning neural network to estimate the Hessian matrix during likelihood maximization. Furthermore, depth and trajectory modalities can be explored for additional supervision in the likelihood function to reduce noisy predictions.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This work was performed during Vandad Davoodnia’s internship at Ubisoft Laforge, partially funded by Mitacs through the Accelerate program.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Andriluka, M., Pishchulin, L., Gehler, P., Schiele, B.: 2D human pose estimation: New benchmark and state of the art analysis. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 3686–3693 (2014). https://doi.org/10.1109/CVPR.2014.471

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Bartol, K., Bojanić, D., Petković, T., Pribanić, T.: Generalizable human pose triangulation. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 11028–11037 (2022). https://doi.org/10.1109/CVPR52688.2022.01075

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Bramlage, L., Karg, M., Curio, C.: Plausible uncertainties for human pose regression. In: IEEE/CVF International Conference on Computer Vision (ICCV). pp. 15133–15142 (2023). https://doi.org/10.1109/ICCV51070.2023.01389

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Cao, Z., Simon, T., Wei, S.E., Sheikh, Y.: Realtime multi-person 2D pose estimation using part affinity fields. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 7291–7299 (2017). https://doi.org/10.1109/CVPR.2017.143

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Chen, Y., Wang, Z., Peng, Y., Zhang, Z., Yu, G., Sun, J.: Cascaded pyramid network for multi-person pose estimation. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 7103–7112 (2018). https://doi.org/10.1109/CVPR.2018.00742

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Contributors, M.: Openmmlab pose estimation toolbox and benchmark. <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/open-mmlab/mmpose" title="">https://github.com/open-mmlab/mmpose</a> (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Davoodnia, V., Ghorbani, S., Messier, A., Etemad, A.: Skelformer: Markerless 3D pose and shape estimation using skeletal transformers. arXiv preprint arXiv:2404.12625 (2024). https://doi.org/10.48550/arXiv.2404.12625

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Dwivedi, S.K., Schmid, C., Yi, H., Black, M.J., Tzionas, D.: Poco: 3D pose and shape estimation with confidence. In: IEEE International Conference on 3D Vision (3DV). pp. 85–95 (2024). https://doi.org/10.1109/3DV62453.2024.00115

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Gong, X., Song, L., Zheng, M., Planche, B., Chen, T., Yuan, J., Doermann, D., Wu, Z.: Progressive multi-view human mesh recovery with self-supervision. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 37, pp. 676–684 (2023). https://doi.org/10.1609/aaai.v37i1.25144

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Gordon, B., Raab, S., Azov, G., Giryes, R., Cohen-Or, D.: Flex: Extrinsic parameters-free multi-view 3D human motion reconstruction. In: European Conference on Computer Vision (ECCV). pp. 176–196. Springer (2022). https://doi.org/10.1007/978-3-031-19827-4_11

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Guo, M.H., Cai, J.X., Liu, Z.N., Mu, T.J., Martin, R.R., Hu, S.M.: Pct: Point cloud transformer. Computational Visual Media <span class="ltx_text ltx_font_bold" id="bib.bib11.1.1">7</span>, 187–199 (2021). https://doi.org/10.1007/s41095-021-0229-5

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Hartley, R., Zisserman, A.: Multiple view geometry in computer vision. Cambridge University Press, 2 edn. (2004). https://doi.org/10.1108/k.2001.30.9_10.1333.2

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 770–778 (2016). https://doi.org/10.1109/CVPR.2016.90

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
He, Y., Yan, R., Fragkiadaki, K., Yu, S.I.: Epipolar transformers. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 7779–7788 (2020). https://doi.org/10.1109/CVPR42600.2020.00780

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Holmquist, K., Wandt, B.: Diffpose: Multi-hypothesis human pose estimation using diffusion models. In: IEEE/CVF International Conference on Computer Vision (ICCV). pp. 15977–15987 (2023). https://doi.org/10.1109/ICCV51070.2023.01464

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Huang, C.H.P., Yi, H., Höschle, M., Safroshkin, M., Alexiadis, T., Polikovsky, S., Scharstein, D., Black, M.J.: Capturing and inferring dense full-body human-scene contact. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 13274–13285 (2022). https://doi.org/10.1109/CVPR52688.2022.01292

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Huang, Z., Wang, X., Huang, L., Huang, C., Wei, Y., Liu, W.: Ccnet: Criss-cross attention for semantic segmentation. In: IEEE/CVF International Conference on Computer Vision (ICCV). pp. 603–612 (2019). https://doi.org/10.1109/ICCV.2019.00069

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Ionescu, C., Papava, D., Olaru, V., Sminchisescu, C.: Human3.6m: Large scale datasets and predictive methods for 3D human sensing in natural environments. IEEE Transactions on Pattern Analysis and Machine Intelligence <span class="ltx_text ltx_font_bold" id="bib.bib18.1.1">36</span>(7), 1325–1339 (2013). https://doi.org/10.1109/TPAMI.2013.248

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Iskakov, K., Burkov, E., Lempitsky, V., Malkov, Y.: Learnable triangulation of human pose. In: IEEE/CVF International Conference on Computer Vision (ICCV). pp. 7718–7727 (2019). https://doi.org/10.1109/ICCV.2019.00781

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Jiang, B., Hu, L., Xia, S.: Probabilistic triangulation for uncalibrated multi-view 3D human pose estimation. In: IEEE/CVF International Conference on Computer Vision (ICCV). pp. 14850–14860 (2023). https://doi.org/10.1109/ICCV51070.2023.01364

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Jin, S., Xu, L., Xu, J., Wang, C., Liu, W., Qian, C., Ouyang, W., Luo, P.: Whole-body human pose estimation in the wild. In: European Conference on Computer Vision (ECCV). pp. 196–214. Springer (2020). https://doi.org/10.1007/978-3-030-58545-7_12

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Joo, H., Liu, H., Tan, L., Gui, L., Nabbe, B., Matthews, I., Kanade, T., Nobuhara, S., Sheikh, Y.: Panoptic studio: A massively multiview system for social motion capture. In: IEEE/CVF International Conference on Computer Vision (ICCV). pp. 3334–3342 (2015). https://doi.org/10.1109/ICCV.2015.381

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Karashchuk, P., Rupp, K.L., Dickinson, E.S., Walling-Bell, S., Sanders, E., Azim, E., Brunton, B.W., Tuthill, J.C.: Anipose: a toolkit for robust markerless 3D pose estimation. Cell Reports <span class="ltx_text ltx_font_bold" id="bib.bib23.1.1">36</span>(13) (2021). https://doi.org/10.1016/j.celrep.2021.109730

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Kendall, A., Grimes, M., Cipolla, R.: Posenet: A convolutional network for real-time 6-dof camera relocalization. In: IEEE/CVF International Conference on Computer Vision (ICCV). pp. 2938–2946 (2015). https://doi.org/10.1109/ICCV.2015.336

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Kocabas, M., Athanasiou, N., Black, M.J.: Vibe: Video inference for human body pose and shape estimation. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 5253–5263 (2020). https://doi.org/10.1109/CVPR42600.2020.00530

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Kocabas, M., Huang, C.H.P., Hilliges, O., Black, M.J.: Pare: Part attention regressor for 3D human body estimation. In: IEEE/CVF International Conference on Computer Vision (ICCV). pp. 11127–11137 (2021). https://doi.org/10.1109/ICCV48922.2021.01094

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Kocabas, M., Karagoz, S., Akbas, E.: Self-supervised learning of 3D human pose using multi-view geometry. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 1077–1086 (2019). https://doi.org/10.1109/CVPR.2019.00117

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Kolotouros, N., Pavlakos, G., Black, M.J., Daniilidis, K.: Learning to reconstruct 3D human pose and shape via model-fitting in the loop. In: IEEE/CVF International Conference on Computer Vision (ICCV). pp. 2252–2261 (2019). https://doi.org/10.1109/ICCV.2019.00234

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Kundu, J.N., Seth, S., YM, P., Jampani, V., Chakraborty, A., Babu, R.V.: Uncertainty-aware adaptation for self-supervised 3D human pose estimation. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 20448–20459 (2022). https://doi.org/10.1109/CVPR52688.2022.01980

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Li, J., Bian, S., Xu, C., Chen, Z., Yang, L., Lu, C.: Hybrik-x: Hybrid analytical-neural inverse kinematics for whole-body mesh recovery. arXiv preprint arXiv:2304.05690 (2023). https://doi.org/10.48550/arXiv.2304.05690

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Li, J., Bian, S., Zeng, A., Wang, C., Pang, B., Liu, W., Lu, C.: Human pose regression with residual log-likelihood estimation. In: IEEE/CVF International Conference on Computer Vision (ICCV). pp. 11025–11034 (2021). https://doi.org/10.1109/ICCV48922.2021.01084

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Li, W., Liu, H., Tang, H., Wang, P., Van Gool, L.: Mhformer: Multi-hypothesis transformer for 3D human pose estimation. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 13147–13156 (2022). https://doi.org/10.1109/CVPR52688.2022.01280

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Li, Y., Yang, S., Liu, P., Zhang, S., Wang, Y., Wang, Z., Yang, W., Xia, S.T.: Simcc: A simple coordinate classification perspective for human pose estimation. In: European Conference on Computer Vision (ECCV). pp. 89–106. Springer (2022). https://doi.org/10.1007/978-3-031-20068-7_6

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Li, Z., Liu, J., Zhang, Z., Xu, S., Yan, Y.: Cliff: Carrying location information in full frames into human pose and shape estimation. In: European Conference on Computer Vision (ECCV). pp. 590–606. Springer (2022). https://doi.org/10.1007/978-3-031-20065-6_34

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Lin, K., Wang, L., Liu, Z.: End-to-end human pose and mesh reconstruction with transformers. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 1954–1963 (2021). https://doi.org/10.1109/CVPR46437.2021.00199

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: European Conference on Computer Vision (ECCV). pp. 740–755. Springer (2014). https://doi.org/10.1007/978-3-319-10602-1_48

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Liu, D.C., Nocedal, J.: On the limited memory bfgs method for large scale optimization. Mathematical Programming <span class="ltx_text ltx_font_bold" id="bib.bib37.1.1">45</span>(1-3), 503–528 (1989). https://doi.org/10.1007/BF01589116

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Llopart, A.: Liftformer: 3D human pose estimation using attention models. arXiv preprint arXiv:2009.00348 (2020). https://doi.org/10.48550/arXiv.2009.00348

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black, M.J.: Smpl: A skinned multi-person linear model. ACM Transactions on Graphics <span class="ltx_text ltx_font_bold" id="bib.bib39.1.1">34</span>(6), 1–16 (2015). https://doi.org/10.1145/2816795.2818013

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: International Conference on Learning Representations (ICLR) (2019), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=Bkg6RiCqY7" title="">https://openreview.net/forum?id=Bkg6RiCqY7</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Ma, H., Chen, L., Kong, D., Wang, Z., Liu, X., Tang, H., Yan, X., Xie, Y., Lin, S.Y., Xie, X.: Transfusion: Cross-view fusion with transformer for 3D human pose estimation. In: British Machine Vision Conference (BMVC). British Machine Vision Association (2021). https://doi.org/10.48550/arXiv.2110.09554

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Mahmood, N., Ghorbani, N., Troje, N.F., Pons-Moll, G., Black, M.J.: Amass: Archive of motion capture as surface shapes. In: IEEE/CVF International Conference on Computer Vision (ICCV). pp. 5442–5451 (2019). https://doi.org/10.1109/ICCV.2019.00554

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Martínez-Otzeta, J.M., Rodríguez-Moreno, I., Mendialdua, I., Sierra, B.: Ransac for robotic applications: A survey. Sensors <span class="ltx_text ltx_font_bold" id="bib.bib43.1.1">23</span>(1),  327 (2022). https://doi.org/10.3390/s23010327

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Moon, G., Choi, H., Lee, K.M.: Neuralannot: Neural annotator for 3D human mesh training sets. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 2299–2307 (2022). https://doi.org/10.1109/CVPRW56347.2022.00256

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Köpf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., Chintala, S.: Pytorch: An imperative style, high-performance deep learning library. Advances in Neural Information Processing Systems (NeurIPS) <span class="ltx_text ltx_font_bold" id="bib.bib45.1.1">32</span> (2019), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pytorch.org/" title="">https://pytorch.org/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Pavlakos, G., Choutas, V., Ghorbani, N., Bolkart, T., Osman, A.A., Tzionas, D., Black, M.J.: Expressive body capture: 3D hands, face, and body from a single image. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 10975–10985 (2019). https://doi.org/10.1109/CVPR.2019.01123

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Qiu, H., Wang, C., Wang, J., Wang, N., Zeng, W.: Cross view fusion for 3D human pose estimation. In: IEEE/CVF International Conference on Computer Vision (ICCV). pp. 4342–4351 (2019). https://doi.org/10.1109/ICCV.2019.00444

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Reddy, N.D., Guigues, L., Pishchulin, L., Eledath, J., Narasimhan, S.G.: Tessetrack: End-to-end learnable multi-person articulated 3D pose tracking. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 15190–15200 (2021). https://doi.org/10.1109/CVPR46437.2021.01494

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Remelli, E., Han, S., Honari, S., Fua, P., Wang, R.: Lightweight multi-view 3D pose estimation through camera-disentangled representation. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 6040–6049 (2020). https://doi.org/10.1109/CVPR42600.2020.00608

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Rempe, D., Birdal, T., Hertzmann, A., Yang, J., Sridhar, S., Guibas, L.J.: Humor: 3D human motion model for robust pose estimation. In: IEEE/CVF International Conference on Computer Vision (ICCV). pp. 11488–11499 (2021). https://doi.org/10.1109/ICCV48922.2021.01129

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Ren, J., Pan, L., Liu, Z.: Benchmarking and analyzing point cloud classification under corruptions. In: International Conference on Machine Learning (ICML). pp. 18559–18575. PMLR (2022), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.mlr.press/v162/ren22c.html" title="">https://proceedings.mlr.press/v162/ren22c.html</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Rezende, D., Mohamed, S.: Variational inference with normalizing flows. In: International Conference on Machine Learning (ICML). pp. 1530–1538. PMLR (2015), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.mlr.press/v37/rezende15.html" title="">https://proceedings.mlr.press/v37/rezende15.html</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Rhodin, H., Spörri, J., Katircioglu, I., Constantin, V., Meyer, F., Müller, E., Salzmann, M., Fua, P.: Learning monocular 3D human pose estimation from multi-view images. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 8437–8446 (2018). https://doi.org/10.1109/CVPR.2018.00880

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Shen, Z., Cen, Z., Peng, S., Shuai, Q., Bao, H., Zhou, X.: Learning human mesh recovery in 3D scenes. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 17038–17047 (2023). https://doi.org/10.1109/CVPR52729.2023.01634

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Shuai, H., Wu, L., Liu, Q.: Adaptive multi-view and temporal fusing transformer for 3D human pose estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence <span class="ltx_text ltx_font_bold" id="bib.bib55.1.1">45</span>(4), 4122–4135 (2022). https://doi.org/10.1109/TPAMI.2022.3188716

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Sun, J.J., Karashchuk, L., Dravid, A., Ryou, S., Fereidooni, S., Tuthill, J., Katsaggelos, A., Brunton, B.W., Gkioxari, G., Kennedy, A., Yue, Y., Perona, P.: BKinD-3D: self-supervised 3D keypoint discovery from multi-view videos. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 9001–9010 (2023). https://doi.org/10.1109/CVPR52729.2023.00869

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Sun, K., Xiao, B., Liu, D., Wang, J.: Deep high-resolution representation learning for human pose estimation. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 5693–5703 (2019). https://doi.org/10.1109/CVPR.2019.00584

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Tang, Z., Qiu, Z., Hao, Y., Hong, R., Yao, T.: 3D human pose estimation with spatio-temporal criss-cross attention. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4790–4799 (2023). https://doi.org/10.1109/CVPR52729.2023.00464

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Tripathi, S., Müller, L., Huang, C.H.P., Taheri, O., Black, M.J., Tzionas, D.: 3D human pose estimation via intuitive physics. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4713–4725 (2023). https://doi.org/10.1109/CVPR52729.2023.00457

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Usman, B., Tagliasacchi, A., Saenko, K., Sud, A.: Metapose: Fast 3D pose from multiple views without 3D supervision. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 6759–6770 (2022). https://doi.org/10.1109/CVPR52688.2022.00664

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I.: Attention is all you need. Advances in Neural Information Processing Systems (NeurIPS) <span class="ltx_text ltx_font_bold" id="bib.bib61.1.1">30</span> (2017). https://doi.org/10.48550/arXiv.1706.03762

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Wandt, B., Rudolph, M., Zell, P., Rhodin, H., Rosenhahn, B.: Canonpose: Self-supervised monocular 3D human pose estimation in the wild. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 13294–13304 (2021). https://doi.org/10.1109/CVPR46437.2021.01309

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Yao, Y., Jafarian, Y., Park, H.S.: Monet: Multiview semi-supervised keypoint detection via epipolar divergence. In: IEEE/CVF International Conference on Computer Vision (ICCV). pp. 753–762 (2019). https://doi.org/10.1109/ICCV.2019.00084

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Yoon, J.S., Yu, Z., Park, J., Park, H.S.: Humbi: A large multiview dataset of human body expressions and benchmark challenge. IEEE Transactions on Pattern Analysis and Machine Intelligence <span class="ltx_text ltx_font_bold" id="bib.bib64.1.1">45</span>(1), 623–640 (2021). https://doi.org/10.1109/TPAMI.2021.3138762

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Yu, Z., Yoon, J.S., Lee, I.K., Venkatesh, P., Park, J., Yu, J., Park, H.S.: Humbi: A large multiview dataset of human body expressions. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 2990–3000 (2020). https://doi.org/10.1109/CVPR42600.2020.00306

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Zhang, Z., Wang, C., Qiu, W., Qin, W., Zeng, W.: Adafuse: Adaptive multiview fusion for accurate human pose estimation in the wild. International Journal of Computer Vision <span class="ltx_text ltx_font_bold" id="bib.bib66.1.1">129</span>, 703–718 (2021). https://doi.org/10.1007/s11263-020-01398-9

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Zhao, J., Yu, T., An, L., Huang, Y., Deng, F., Dai, Q.: Triangulation residual loss for data-efficient 3D pose estimation. Advances in Neural Information Processing Systems (NeurIPS) <span class="ltx_text ltx_font_bold" id="bib.bib67.1.1">36</span> (2024), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=gLwjBDsE3G" title="">https://openreview.net/forum?id=gLwjBDsE3G</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Zheng, C., Zhu, S., Mendieta, M., Yang, T., Chen, C., Ding, Z.: 3D human pose estimation with spatial and temporal transformers. In: IEEE/CVF International Conference on Computer Vision (ICCV). pp. 11656–11665 (2021). https://doi.org/10.1109/ICCV48922.2021.01145

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Zhou, X., Huang, Q., Sun, X., Xue, X., Wei, Y.: Towards 3D human pose estimation in the wild: a weakly-supervised approach. In: IEEE/CVF International Conference on Computer Vision (ICCV). pp. 398–407 (2017). https://doi.org/10.1109/ICCV.2017.51

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
Zhu, W., Ma, X., Liu, Z., Liu, L., Wu, W., Wang, Y.: Motionbert: A unified perspective on learning human motion representations. In: IEEE/CVF International Conference on Computer Vision (ICCV). pp. 15085–15099. IEEE (2023). https://doi.org/10.1109/ICCV51070.2023.01385

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Zhu, Y., Xu, X., Shen, F., Ji, Y., Gao, L., Shen, H.T.: Posegtac: Graph transformer encoder-decoder with atrous convolution for 3D human pose estimation. In: International Joint Conference on Artificial Intelligence (IJCAI). pp. 1359–1365 (2021). https://doi.org/10.24963/ijcai.2021/188

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="Pt0.A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.A </span>Additional Details</h2>
<section class="ltx_subsection" id="Pt0.A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">0.A.1 </span>Details on Training and Multi-view Data Synthesis</h3>
<div class="ltx_para" id="Pt0.A1.SS1.p1">
<p class="ltx_p" id="Pt0.A1.SS1.p1.5">Let <math alttext="r\in\mathbb{R}^{T\times 3}" class="ltx_Math" display="inline" id="Pt0.A1.SS1.p1.1.m1.1"><semantics id="Pt0.A1.SS1.p1.1.m1.1a"><mrow id="Pt0.A1.SS1.p1.1.m1.1.1" xref="Pt0.A1.SS1.p1.1.m1.1.1.cmml"><mi id="Pt0.A1.SS1.p1.1.m1.1.1.2" xref="Pt0.A1.SS1.p1.1.m1.1.1.2.cmml">r</mi><mo id="Pt0.A1.SS1.p1.1.m1.1.1.1" xref="Pt0.A1.SS1.p1.1.m1.1.1.1.cmml">∈</mo><msup id="Pt0.A1.SS1.p1.1.m1.1.1.3" xref="Pt0.A1.SS1.p1.1.m1.1.1.3.cmml"><mi id="Pt0.A1.SS1.p1.1.m1.1.1.3.2" xref="Pt0.A1.SS1.p1.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="Pt0.A1.SS1.p1.1.m1.1.1.3.3" xref="Pt0.A1.SS1.p1.1.m1.1.1.3.3.cmml"><mi id="Pt0.A1.SS1.p1.1.m1.1.1.3.3.2" xref="Pt0.A1.SS1.p1.1.m1.1.1.3.3.2.cmml">T</mi><mo id="Pt0.A1.SS1.p1.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="Pt0.A1.SS1.p1.1.m1.1.1.3.3.1.cmml">×</mo><mn id="Pt0.A1.SS1.p1.1.m1.1.1.3.3.3" xref="Pt0.A1.SS1.p1.1.m1.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="Pt0.A1.SS1.p1.1.m1.1b"><apply id="Pt0.A1.SS1.p1.1.m1.1.1.cmml" xref="Pt0.A1.SS1.p1.1.m1.1.1"><in id="Pt0.A1.SS1.p1.1.m1.1.1.1.cmml" xref="Pt0.A1.SS1.p1.1.m1.1.1.1"></in><ci id="Pt0.A1.SS1.p1.1.m1.1.1.2.cmml" xref="Pt0.A1.SS1.p1.1.m1.1.1.2">𝑟</ci><apply id="Pt0.A1.SS1.p1.1.m1.1.1.3.cmml" xref="Pt0.A1.SS1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="Pt0.A1.SS1.p1.1.m1.1.1.3.1.cmml" xref="Pt0.A1.SS1.p1.1.m1.1.1.3">superscript</csymbol><ci id="Pt0.A1.SS1.p1.1.m1.1.1.3.2.cmml" xref="Pt0.A1.SS1.p1.1.m1.1.1.3.2">ℝ</ci><apply id="Pt0.A1.SS1.p1.1.m1.1.1.3.3.cmml" xref="Pt0.A1.SS1.p1.1.m1.1.1.3.3"><times id="Pt0.A1.SS1.p1.1.m1.1.1.3.3.1.cmml" xref="Pt0.A1.SS1.p1.1.m1.1.1.3.3.1"></times><ci id="Pt0.A1.SS1.p1.1.m1.1.1.3.3.2.cmml" xref="Pt0.A1.SS1.p1.1.m1.1.1.3.3.2">𝑇</ci><cn id="Pt0.A1.SS1.p1.1.m1.1.1.3.3.3.cmml" type="integer" xref="Pt0.A1.SS1.p1.1.m1.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A1.SS1.p1.1.m1.1c">r\in\mathbb{R}^{T\times 3}</annotation><annotation encoding="application/x-llamapun" id="Pt0.A1.SS1.p1.1.m1.1d">italic_r ∈ blackboard_R start_POSTSUPERSCRIPT italic_T × 3 end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="\Theta\in\mathbb{R}^{T\times 55\times 3\times 3}" class="ltx_Math" display="inline" id="Pt0.A1.SS1.p1.2.m2.1"><semantics id="Pt0.A1.SS1.p1.2.m2.1a"><mrow id="Pt0.A1.SS1.p1.2.m2.1.1" xref="Pt0.A1.SS1.p1.2.m2.1.1.cmml"><mi id="Pt0.A1.SS1.p1.2.m2.1.1.2" mathvariant="normal" xref="Pt0.A1.SS1.p1.2.m2.1.1.2.cmml">Θ</mi><mo id="Pt0.A1.SS1.p1.2.m2.1.1.1" xref="Pt0.A1.SS1.p1.2.m2.1.1.1.cmml">∈</mo><msup id="Pt0.A1.SS1.p1.2.m2.1.1.3" xref="Pt0.A1.SS1.p1.2.m2.1.1.3.cmml"><mi id="Pt0.A1.SS1.p1.2.m2.1.1.3.2" xref="Pt0.A1.SS1.p1.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="Pt0.A1.SS1.p1.2.m2.1.1.3.3" xref="Pt0.A1.SS1.p1.2.m2.1.1.3.3.cmml"><mi id="Pt0.A1.SS1.p1.2.m2.1.1.3.3.2" xref="Pt0.A1.SS1.p1.2.m2.1.1.3.3.2.cmml">T</mi><mo id="Pt0.A1.SS1.p1.2.m2.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="Pt0.A1.SS1.p1.2.m2.1.1.3.3.1.cmml">×</mo><mn id="Pt0.A1.SS1.p1.2.m2.1.1.3.3.3" xref="Pt0.A1.SS1.p1.2.m2.1.1.3.3.3.cmml">55</mn><mo id="Pt0.A1.SS1.p1.2.m2.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="Pt0.A1.SS1.p1.2.m2.1.1.3.3.1.cmml">×</mo><mn id="Pt0.A1.SS1.p1.2.m2.1.1.3.3.4" xref="Pt0.A1.SS1.p1.2.m2.1.1.3.3.4.cmml">3</mn><mo id="Pt0.A1.SS1.p1.2.m2.1.1.3.3.1b" lspace="0.222em" rspace="0.222em" xref="Pt0.A1.SS1.p1.2.m2.1.1.3.3.1.cmml">×</mo><mn id="Pt0.A1.SS1.p1.2.m2.1.1.3.3.5" xref="Pt0.A1.SS1.p1.2.m2.1.1.3.3.5.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="Pt0.A1.SS1.p1.2.m2.1b"><apply id="Pt0.A1.SS1.p1.2.m2.1.1.cmml" xref="Pt0.A1.SS1.p1.2.m2.1.1"><in id="Pt0.A1.SS1.p1.2.m2.1.1.1.cmml" xref="Pt0.A1.SS1.p1.2.m2.1.1.1"></in><ci id="Pt0.A1.SS1.p1.2.m2.1.1.2.cmml" xref="Pt0.A1.SS1.p1.2.m2.1.1.2">Θ</ci><apply id="Pt0.A1.SS1.p1.2.m2.1.1.3.cmml" xref="Pt0.A1.SS1.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="Pt0.A1.SS1.p1.2.m2.1.1.3.1.cmml" xref="Pt0.A1.SS1.p1.2.m2.1.1.3">superscript</csymbol><ci id="Pt0.A1.SS1.p1.2.m2.1.1.3.2.cmml" xref="Pt0.A1.SS1.p1.2.m2.1.1.3.2">ℝ</ci><apply id="Pt0.A1.SS1.p1.2.m2.1.1.3.3.cmml" xref="Pt0.A1.SS1.p1.2.m2.1.1.3.3"><times id="Pt0.A1.SS1.p1.2.m2.1.1.3.3.1.cmml" xref="Pt0.A1.SS1.p1.2.m2.1.1.3.3.1"></times><ci id="Pt0.A1.SS1.p1.2.m2.1.1.3.3.2.cmml" xref="Pt0.A1.SS1.p1.2.m2.1.1.3.3.2">𝑇</ci><cn id="Pt0.A1.SS1.p1.2.m2.1.1.3.3.3.cmml" type="integer" xref="Pt0.A1.SS1.p1.2.m2.1.1.3.3.3">55</cn><cn id="Pt0.A1.SS1.p1.2.m2.1.1.3.3.4.cmml" type="integer" xref="Pt0.A1.SS1.p1.2.m2.1.1.3.3.4">3</cn><cn id="Pt0.A1.SS1.p1.2.m2.1.1.3.3.5.cmml" type="integer" xref="Pt0.A1.SS1.p1.2.m2.1.1.3.3.5">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A1.SS1.p1.2.m2.1c">\Theta\in\mathbb{R}^{T\times 55\times 3\times 3}</annotation><annotation encoding="application/x-llamapun" id="Pt0.A1.SS1.p1.2.m2.1d">roman_Θ ∈ blackboard_R start_POSTSUPERSCRIPT italic_T × 55 × 3 × 3 end_POSTSUPERSCRIPT</annotation></semantics></math>, and <math alttext="\beta\in\mathbb{R}^{16}" class="ltx_Math" display="inline" id="Pt0.A1.SS1.p1.3.m3.1"><semantics id="Pt0.A1.SS1.p1.3.m3.1a"><mrow id="Pt0.A1.SS1.p1.3.m3.1.1" xref="Pt0.A1.SS1.p1.3.m3.1.1.cmml"><mi id="Pt0.A1.SS1.p1.3.m3.1.1.2" xref="Pt0.A1.SS1.p1.3.m3.1.1.2.cmml">β</mi><mo id="Pt0.A1.SS1.p1.3.m3.1.1.1" xref="Pt0.A1.SS1.p1.3.m3.1.1.1.cmml">∈</mo><msup id="Pt0.A1.SS1.p1.3.m3.1.1.3" xref="Pt0.A1.SS1.p1.3.m3.1.1.3.cmml"><mi id="Pt0.A1.SS1.p1.3.m3.1.1.3.2" xref="Pt0.A1.SS1.p1.3.m3.1.1.3.2.cmml">ℝ</mi><mn id="Pt0.A1.SS1.p1.3.m3.1.1.3.3" xref="Pt0.A1.SS1.p1.3.m3.1.1.3.3.cmml">16</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="Pt0.A1.SS1.p1.3.m3.1b"><apply id="Pt0.A1.SS1.p1.3.m3.1.1.cmml" xref="Pt0.A1.SS1.p1.3.m3.1.1"><in id="Pt0.A1.SS1.p1.3.m3.1.1.1.cmml" xref="Pt0.A1.SS1.p1.3.m3.1.1.1"></in><ci id="Pt0.A1.SS1.p1.3.m3.1.1.2.cmml" xref="Pt0.A1.SS1.p1.3.m3.1.1.2">𝛽</ci><apply id="Pt0.A1.SS1.p1.3.m3.1.1.3.cmml" xref="Pt0.A1.SS1.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="Pt0.A1.SS1.p1.3.m3.1.1.3.1.cmml" xref="Pt0.A1.SS1.p1.3.m3.1.1.3">superscript</csymbol><ci id="Pt0.A1.SS1.p1.3.m3.1.1.3.2.cmml" xref="Pt0.A1.SS1.p1.3.m3.1.1.3.2">ℝ</ci><cn id="Pt0.A1.SS1.p1.3.m3.1.1.3.3.cmml" type="integer" xref="Pt0.A1.SS1.p1.3.m3.1.1.3.3">16</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A1.SS1.p1.3.m3.1c">\beta\in\mathbb{R}^{16}</annotation><annotation encoding="application/x-llamapun" id="Pt0.A1.SS1.p1.3.m3.1d">italic_β ∈ blackboard_R start_POSTSUPERSCRIPT 16 end_POSTSUPERSCRIPT</annotation></semantics></math> be root position, body joint rotation matrices in a temporal window of length <math alttext="T" class="ltx_Math" display="inline" id="Pt0.A1.SS1.p1.4.m4.1"><semantics id="Pt0.A1.SS1.p1.4.m4.1a"><mi id="Pt0.A1.SS1.p1.4.m4.1.1" xref="Pt0.A1.SS1.p1.4.m4.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="Pt0.A1.SS1.p1.4.m4.1b"><ci id="Pt0.A1.SS1.p1.4.m4.1.1.cmml" xref="Pt0.A1.SS1.p1.4.m4.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A1.SS1.p1.4.m4.1c">T</annotation><annotation encoding="application/x-llamapun" id="Pt0.A1.SS1.p1.4.m4.1d">italic_T</annotation></semantics></math>, and shape parameters from human motion capture data. We begin our multi-view data generation by augmenting the shape parameters with Gaussian noise with a standard deviation equal to the standard deviation of all shape parameters within the datasets. Next, we apply mediolateral mirroring with a 50% chance and randomly rotate the motion sequence around its center. We pass the augmented <math alttext="\{r,\Theta,\beta\}" class="ltx_Math" display="inline" id="Pt0.A1.SS1.p1.5.m5.3"><semantics id="Pt0.A1.SS1.p1.5.m5.3a"><mrow id="Pt0.A1.SS1.p1.5.m5.3.4.2" xref="Pt0.A1.SS1.p1.5.m5.3.4.1.cmml"><mo id="Pt0.A1.SS1.p1.5.m5.3.4.2.1" stretchy="false" xref="Pt0.A1.SS1.p1.5.m5.3.4.1.cmml">{</mo><mi id="Pt0.A1.SS1.p1.5.m5.1.1" xref="Pt0.A1.SS1.p1.5.m5.1.1.cmml">r</mi><mo id="Pt0.A1.SS1.p1.5.m5.3.4.2.2" xref="Pt0.A1.SS1.p1.5.m5.3.4.1.cmml">,</mo><mi id="Pt0.A1.SS1.p1.5.m5.2.2" mathvariant="normal" xref="Pt0.A1.SS1.p1.5.m5.2.2.cmml">Θ</mi><mo id="Pt0.A1.SS1.p1.5.m5.3.4.2.3" xref="Pt0.A1.SS1.p1.5.m5.3.4.1.cmml">,</mo><mi id="Pt0.A1.SS1.p1.5.m5.3.3" xref="Pt0.A1.SS1.p1.5.m5.3.3.cmml">β</mi><mo id="Pt0.A1.SS1.p1.5.m5.3.4.2.4" stretchy="false" xref="Pt0.A1.SS1.p1.5.m5.3.4.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="Pt0.A1.SS1.p1.5.m5.3b"><set id="Pt0.A1.SS1.p1.5.m5.3.4.1.cmml" xref="Pt0.A1.SS1.p1.5.m5.3.4.2"><ci id="Pt0.A1.SS1.p1.5.m5.1.1.cmml" xref="Pt0.A1.SS1.p1.5.m5.1.1">𝑟</ci><ci id="Pt0.A1.SS1.p1.5.m5.2.2.cmml" xref="Pt0.A1.SS1.p1.5.m5.2.2">Θ</ci><ci id="Pt0.A1.SS1.p1.5.m5.3.3.cmml" xref="Pt0.A1.SS1.p1.5.m5.3.3">𝛽</ci></set></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A1.SS1.p1.5.m5.3c">\{r,\Theta,\beta\}</annotation><annotation encoding="application/x-llamapun" id="Pt0.A1.SS1.p1.5.m5.3d">{ italic_r , roman_Θ , italic_β }</annotation></semantics></math> parameters to the forward-kinematic layer of the SMPL body model to obtain 3D vertices. Lastly, we use a dataset-specific joint regressor on the vertices to extract the 3D keypoints used in the next steps of our pipeline.</p>
</div>
<div class="ltx_para" id="Pt0.A1.SS1.p2">
<p class="ltx_p" id="Pt0.A1.SS1.p2.1">Next, we simulate a multi-camera recording setup by randomly positioning several cameras in cylindrical space. As depicted in <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#Pt0.A1.F5" title="In 0.A.1 Details on Training and Multi-view Data Synthesis ‣ Appendix 0.A Additional Details ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>-a, we randomly choose a recording volume size that encircles the space occupied by the human body. To ensure that the subject appears in most cameras, we select the tilt, pitch, and yaw so that they look at a random point in the center of the recording volume while maintaining the correct up direction. Additionally, we limit the camera height to mimic typical multi-view video recording setups.</p>
</div>
<figure class="ltx_figure" id="Pt0.A1.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="277" id="Pt0.A1.F5.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Pt0.A1.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="Pt0.A1.F5.3.2" style="font-size:90%;">
We illustrate our multi-view data synthesis framework, starting with (a) camera placement in a space surrounding a motion-captured human body; (b) extraction and projection of keypoints onto the synthetic cameras; (c) 2D ground-truth keypoints; (d) data corruption; and (e) cross-view projection to prepare the point cloud training data for our pose compiler.
</span></figcaption>
</figure>
<div class="ltx_para" id="Pt0.A1.SS1.p3">
<p class="ltx_p" id="Pt0.A1.SS1.p3.1">After obtaining the camera intrinsic and extrinsic parameters, we project the 3D body keypoints onto each camera view (see <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#Pt0.A1.F5" title="In 0.A.1 Details on Training and Multi-view Data Synthesis ‣ Appendix 0.A Additional Details ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>-b). We use these 2D keypoints as ground truths <math alttext="\mu_{g}" class="ltx_Math" display="inline" id="Pt0.A1.SS1.p3.1.m1.1"><semantics id="Pt0.A1.SS1.p3.1.m1.1a"><msub id="Pt0.A1.SS1.p3.1.m1.1.1" xref="Pt0.A1.SS1.p3.1.m1.1.1.cmml"><mi id="Pt0.A1.SS1.p3.1.m1.1.1.2" xref="Pt0.A1.SS1.p3.1.m1.1.1.2.cmml">μ</mi><mi id="Pt0.A1.SS1.p3.1.m1.1.1.3" xref="Pt0.A1.SS1.p3.1.m1.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="Pt0.A1.SS1.p3.1.m1.1b"><apply id="Pt0.A1.SS1.p3.1.m1.1.1.cmml" xref="Pt0.A1.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="Pt0.A1.SS1.p3.1.m1.1.1.1.cmml" xref="Pt0.A1.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="Pt0.A1.SS1.p3.1.m1.1.1.2.cmml" xref="Pt0.A1.SS1.p3.1.m1.1.1.2">𝜇</ci><ci id="Pt0.A1.SS1.p3.1.m1.1.1.3.cmml" xref="Pt0.A1.SS1.p3.1.m1.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A1.SS1.p3.1.m1.1c">\mu_{g}</annotation><annotation encoding="application/x-llamapun" id="Pt0.A1.SS1.p3.1.m1.1d">italic_μ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT</annotation></semantics></math> to train our pose compiler (see <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#Pt0.A1.F5" title="In 0.A.1 Details on Training and Multi-view Data Synthesis ‣ Appendix 0.A Additional Details ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>-c). We then add 2D point corruptions to the 2D keypoints, including Gaussian noise with varying standard deviations, simulated occlusions with varying sizes and probabilities, mediolateral flipping, and occasional truncation effects (see <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#Pt0.A1.F5" title="In 0.A.1 Details on Training and Multi-view Data Synthesis ‣ Appendix 0.A Additional Details ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>-d). Next, we obtain the cross-view projected keypoints (see <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#Pt0.A1.F5" title="In 0.A.1 Details on Training and Multi-view Data Synthesis ‣ Appendix 0.A Additional Details ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>-e) via the algorithm described in <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S3.SS2" title="3.2 Cross-view Projection ‣ 3 Method ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.2</span></a> in the main paper. Finally, we train our pose compiler using the ground-truth keypoints and point clouds containing noisy 2D data, as depicted in <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#Pt0.A1.F6" title="In 0.A.1 Details on Training and Multi-view Data Synthesis ‣ Appendix 0.A Additional Details ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure class="ltx_figure" id="Pt0.A1.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="255" id="Pt0.A1.F6.g1" src="x6.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Pt0.A1.F6.2.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="Pt0.A1.F6.3.2" style="font-size:90%;">
We illustrate the training routine of our pose compiler using synthetic data generated based on real motion capture sequences.
</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="Pt0.A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">0.A.2 </span>Details on Criss-cross Attention</h3>
<div class="ltx_para" id="Pt0.A1.SS2.p1">
<p class="ltx_p" id="Pt0.A1.SS2.p1.4">As discussed in <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S3.SS3" title="3.3 Pose Compiler ‣ 3 Method ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.3</span></a> in the main paper, we use criss-cross attention blocks in our spatiotemporal encoder to process information more efficiently. Accordingly, the cross-view input features <math alttext="\{f_{i}\}" class="ltx_Math" display="inline" id="Pt0.A1.SS2.p1.1.m1.1"><semantics id="Pt0.A1.SS2.p1.1.m1.1a"><mrow id="Pt0.A1.SS2.p1.1.m1.1.1.1" xref="Pt0.A1.SS2.p1.1.m1.1.1.2.cmml"><mo id="Pt0.A1.SS2.p1.1.m1.1.1.1.2" stretchy="false" xref="Pt0.A1.SS2.p1.1.m1.1.1.2.cmml">{</mo><msub id="Pt0.A1.SS2.p1.1.m1.1.1.1.1" xref="Pt0.A1.SS2.p1.1.m1.1.1.1.1.cmml"><mi id="Pt0.A1.SS2.p1.1.m1.1.1.1.1.2" xref="Pt0.A1.SS2.p1.1.m1.1.1.1.1.2.cmml">f</mi><mi id="Pt0.A1.SS2.p1.1.m1.1.1.1.1.3" xref="Pt0.A1.SS2.p1.1.m1.1.1.1.1.3.cmml">i</mi></msub><mo id="Pt0.A1.SS2.p1.1.m1.1.1.1.3" stretchy="false" xref="Pt0.A1.SS2.p1.1.m1.1.1.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="Pt0.A1.SS2.p1.1.m1.1b"><set id="Pt0.A1.SS2.p1.1.m1.1.1.2.cmml" xref="Pt0.A1.SS2.p1.1.m1.1.1.1"><apply id="Pt0.A1.SS2.p1.1.m1.1.1.1.1.cmml" xref="Pt0.A1.SS2.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="Pt0.A1.SS2.p1.1.m1.1.1.1.1.1.cmml" xref="Pt0.A1.SS2.p1.1.m1.1.1.1.1">subscript</csymbol><ci id="Pt0.A1.SS2.p1.1.m1.1.1.1.1.2.cmml" xref="Pt0.A1.SS2.p1.1.m1.1.1.1.1.2">𝑓</ci><ci id="Pt0.A1.SS2.p1.1.m1.1.1.1.1.3.cmml" xref="Pt0.A1.SS2.p1.1.m1.1.1.1.1.3">𝑖</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A1.SS2.p1.1.m1.1c">\{f_{i}\}</annotation><annotation encoding="application/x-llamapun" id="Pt0.A1.SS2.p1.1.m1.1d">{ italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }</annotation></semantics></math> are first projected into queries, keys, and values (<math alttext="Q,K,V\in\mathbb{R}^{T\times J\times 2H}" class="ltx_Math" display="inline" id="Pt0.A1.SS2.p1.2.m2.3"><semantics id="Pt0.A1.SS2.p1.2.m2.3a"><mrow id="Pt0.A1.SS2.p1.2.m2.3.4" xref="Pt0.A1.SS2.p1.2.m2.3.4.cmml"><mrow id="Pt0.A1.SS2.p1.2.m2.3.4.2.2" xref="Pt0.A1.SS2.p1.2.m2.3.4.2.1.cmml"><mi id="Pt0.A1.SS2.p1.2.m2.1.1" xref="Pt0.A1.SS2.p1.2.m2.1.1.cmml">Q</mi><mo id="Pt0.A1.SS2.p1.2.m2.3.4.2.2.1" xref="Pt0.A1.SS2.p1.2.m2.3.4.2.1.cmml">,</mo><mi id="Pt0.A1.SS2.p1.2.m2.2.2" xref="Pt0.A1.SS2.p1.2.m2.2.2.cmml">K</mi><mo id="Pt0.A1.SS2.p1.2.m2.3.4.2.2.2" xref="Pt0.A1.SS2.p1.2.m2.3.4.2.1.cmml">,</mo><mi id="Pt0.A1.SS2.p1.2.m2.3.3" xref="Pt0.A1.SS2.p1.2.m2.3.3.cmml">V</mi></mrow><mo id="Pt0.A1.SS2.p1.2.m2.3.4.1" xref="Pt0.A1.SS2.p1.2.m2.3.4.1.cmml">∈</mo><msup id="Pt0.A1.SS2.p1.2.m2.3.4.3" xref="Pt0.A1.SS2.p1.2.m2.3.4.3.cmml"><mi id="Pt0.A1.SS2.p1.2.m2.3.4.3.2" xref="Pt0.A1.SS2.p1.2.m2.3.4.3.2.cmml">ℝ</mi><mrow id="Pt0.A1.SS2.p1.2.m2.3.4.3.3" xref="Pt0.A1.SS2.p1.2.m2.3.4.3.3.cmml"><mrow id="Pt0.A1.SS2.p1.2.m2.3.4.3.3.2" xref="Pt0.A1.SS2.p1.2.m2.3.4.3.3.2.cmml"><mi id="Pt0.A1.SS2.p1.2.m2.3.4.3.3.2.2" xref="Pt0.A1.SS2.p1.2.m2.3.4.3.3.2.2.cmml">T</mi><mo id="Pt0.A1.SS2.p1.2.m2.3.4.3.3.2.1" lspace="0.222em" rspace="0.222em" xref="Pt0.A1.SS2.p1.2.m2.3.4.3.3.2.1.cmml">×</mo><mi id="Pt0.A1.SS2.p1.2.m2.3.4.3.3.2.3" xref="Pt0.A1.SS2.p1.2.m2.3.4.3.3.2.3.cmml">J</mi><mo id="Pt0.A1.SS2.p1.2.m2.3.4.3.3.2.1a" lspace="0.222em" rspace="0.222em" xref="Pt0.A1.SS2.p1.2.m2.3.4.3.3.2.1.cmml">×</mo><mn id="Pt0.A1.SS2.p1.2.m2.3.4.3.3.2.4" xref="Pt0.A1.SS2.p1.2.m2.3.4.3.3.2.4.cmml">2</mn></mrow><mo id="Pt0.A1.SS2.p1.2.m2.3.4.3.3.1" xref="Pt0.A1.SS2.p1.2.m2.3.4.3.3.1.cmml">⁢</mo><mi id="Pt0.A1.SS2.p1.2.m2.3.4.3.3.3" xref="Pt0.A1.SS2.p1.2.m2.3.4.3.3.3.cmml">H</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="Pt0.A1.SS2.p1.2.m2.3b"><apply id="Pt0.A1.SS2.p1.2.m2.3.4.cmml" xref="Pt0.A1.SS2.p1.2.m2.3.4"><in id="Pt0.A1.SS2.p1.2.m2.3.4.1.cmml" xref="Pt0.A1.SS2.p1.2.m2.3.4.1"></in><list id="Pt0.A1.SS2.p1.2.m2.3.4.2.1.cmml" xref="Pt0.A1.SS2.p1.2.m2.3.4.2.2"><ci id="Pt0.A1.SS2.p1.2.m2.1.1.cmml" xref="Pt0.A1.SS2.p1.2.m2.1.1">𝑄</ci><ci id="Pt0.A1.SS2.p1.2.m2.2.2.cmml" xref="Pt0.A1.SS2.p1.2.m2.2.2">𝐾</ci><ci id="Pt0.A1.SS2.p1.2.m2.3.3.cmml" xref="Pt0.A1.SS2.p1.2.m2.3.3">𝑉</ci></list><apply id="Pt0.A1.SS2.p1.2.m2.3.4.3.cmml" xref="Pt0.A1.SS2.p1.2.m2.3.4.3"><csymbol cd="ambiguous" id="Pt0.A1.SS2.p1.2.m2.3.4.3.1.cmml" xref="Pt0.A1.SS2.p1.2.m2.3.4.3">superscript</csymbol><ci id="Pt0.A1.SS2.p1.2.m2.3.4.3.2.cmml" xref="Pt0.A1.SS2.p1.2.m2.3.4.3.2">ℝ</ci><apply id="Pt0.A1.SS2.p1.2.m2.3.4.3.3.cmml" xref="Pt0.A1.SS2.p1.2.m2.3.4.3.3"><times id="Pt0.A1.SS2.p1.2.m2.3.4.3.3.1.cmml" xref="Pt0.A1.SS2.p1.2.m2.3.4.3.3.1"></times><apply id="Pt0.A1.SS2.p1.2.m2.3.4.3.3.2.cmml" xref="Pt0.A1.SS2.p1.2.m2.3.4.3.3.2"><times id="Pt0.A1.SS2.p1.2.m2.3.4.3.3.2.1.cmml" xref="Pt0.A1.SS2.p1.2.m2.3.4.3.3.2.1"></times><ci id="Pt0.A1.SS2.p1.2.m2.3.4.3.3.2.2.cmml" xref="Pt0.A1.SS2.p1.2.m2.3.4.3.3.2.2">𝑇</ci><ci id="Pt0.A1.SS2.p1.2.m2.3.4.3.3.2.3.cmml" xref="Pt0.A1.SS2.p1.2.m2.3.4.3.3.2.3">𝐽</ci><cn id="Pt0.A1.SS2.p1.2.m2.3.4.3.3.2.4.cmml" type="integer" xref="Pt0.A1.SS2.p1.2.m2.3.4.3.3.2.4">2</cn></apply><ci id="Pt0.A1.SS2.p1.2.m2.3.4.3.3.3.cmml" xref="Pt0.A1.SS2.p1.2.m2.3.4.3.3.3">𝐻</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A1.SS2.p1.2.m2.3c">Q,K,V\in\mathbb{R}^{T\times J\times 2H}</annotation><annotation encoding="application/x-llamapun" id="Pt0.A1.SS2.p1.2.m2.3d">italic_Q , italic_K , italic_V ∈ blackboard_R start_POSTSUPERSCRIPT italic_T × italic_J × 2 italic_H end_POSTSUPERSCRIPT</annotation></semantics></math>) via a linear layer. Next, we divide them into temporal <math alttext="Q_{T},K_{T},V_{T}\in\mathbb{R}^{T\times J\times H}" class="ltx_Math" display="inline" id="Pt0.A1.SS2.p1.3.m3.3"><semantics id="Pt0.A1.SS2.p1.3.m3.3a"><mrow id="Pt0.A1.SS2.p1.3.m3.3.3" xref="Pt0.A1.SS2.p1.3.m3.3.3.cmml"><mrow id="Pt0.A1.SS2.p1.3.m3.3.3.3.3" xref="Pt0.A1.SS2.p1.3.m3.3.3.3.4.cmml"><msub id="Pt0.A1.SS2.p1.3.m3.1.1.1.1.1" xref="Pt0.A1.SS2.p1.3.m3.1.1.1.1.1.cmml"><mi id="Pt0.A1.SS2.p1.3.m3.1.1.1.1.1.2" xref="Pt0.A1.SS2.p1.3.m3.1.1.1.1.1.2.cmml">Q</mi><mi id="Pt0.A1.SS2.p1.3.m3.1.1.1.1.1.3" xref="Pt0.A1.SS2.p1.3.m3.1.1.1.1.1.3.cmml">T</mi></msub><mo id="Pt0.A1.SS2.p1.3.m3.3.3.3.3.4" xref="Pt0.A1.SS2.p1.3.m3.3.3.3.4.cmml">,</mo><msub id="Pt0.A1.SS2.p1.3.m3.2.2.2.2.2" xref="Pt0.A1.SS2.p1.3.m3.2.2.2.2.2.cmml"><mi id="Pt0.A1.SS2.p1.3.m3.2.2.2.2.2.2" xref="Pt0.A1.SS2.p1.3.m3.2.2.2.2.2.2.cmml">K</mi><mi id="Pt0.A1.SS2.p1.3.m3.2.2.2.2.2.3" xref="Pt0.A1.SS2.p1.3.m3.2.2.2.2.2.3.cmml">T</mi></msub><mo id="Pt0.A1.SS2.p1.3.m3.3.3.3.3.5" xref="Pt0.A1.SS2.p1.3.m3.3.3.3.4.cmml">,</mo><msub id="Pt0.A1.SS2.p1.3.m3.3.3.3.3.3" xref="Pt0.A1.SS2.p1.3.m3.3.3.3.3.3.cmml"><mi id="Pt0.A1.SS2.p1.3.m3.3.3.3.3.3.2" xref="Pt0.A1.SS2.p1.3.m3.3.3.3.3.3.2.cmml">V</mi><mi id="Pt0.A1.SS2.p1.3.m3.3.3.3.3.3.3" xref="Pt0.A1.SS2.p1.3.m3.3.3.3.3.3.3.cmml">T</mi></msub></mrow><mo id="Pt0.A1.SS2.p1.3.m3.3.3.4" xref="Pt0.A1.SS2.p1.3.m3.3.3.4.cmml">∈</mo><msup id="Pt0.A1.SS2.p1.3.m3.3.3.5" xref="Pt0.A1.SS2.p1.3.m3.3.3.5.cmml"><mi id="Pt0.A1.SS2.p1.3.m3.3.3.5.2" xref="Pt0.A1.SS2.p1.3.m3.3.3.5.2.cmml">ℝ</mi><mrow id="Pt0.A1.SS2.p1.3.m3.3.3.5.3" xref="Pt0.A1.SS2.p1.3.m3.3.3.5.3.cmml"><mi id="Pt0.A1.SS2.p1.3.m3.3.3.5.3.2" xref="Pt0.A1.SS2.p1.3.m3.3.3.5.3.2.cmml">T</mi><mo id="Pt0.A1.SS2.p1.3.m3.3.3.5.3.1" lspace="0.222em" rspace="0.222em" xref="Pt0.A1.SS2.p1.3.m3.3.3.5.3.1.cmml">×</mo><mi id="Pt0.A1.SS2.p1.3.m3.3.3.5.3.3" xref="Pt0.A1.SS2.p1.3.m3.3.3.5.3.3.cmml">J</mi><mo id="Pt0.A1.SS2.p1.3.m3.3.3.5.3.1a" lspace="0.222em" rspace="0.222em" xref="Pt0.A1.SS2.p1.3.m3.3.3.5.3.1.cmml">×</mo><mi id="Pt0.A1.SS2.p1.3.m3.3.3.5.3.4" xref="Pt0.A1.SS2.p1.3.m3.3.3.5.3.4.cmml">H</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="Pt0.A1.SS2.p1.3.m3.3b"><apply id="Pt0.A1.SS2.p1.3.m3.3.3.cmml" xref="Pt0.A1.SS2.p1.3.m3.3.3"><in id="Pt0.A1.SS2.p1.3.m3.3.3.4.cmml" xref="Pt0.A1.SS2.p1.3.m3.3.3.4"></in><list id="Pt0.A1.SS2.p1.3.m3.3.3.3.4.cmml" xref="Pt0.A1.SS2.p1.3.m3.3.3.3.3"><apply id="Pt0.A1.SS2.p1.3.m3.1.1.1.1.1.cmml" xref="Pt0.A1.SS2.p1.3.m3.1.1.1.1.1"><csymbol cd="ambiguous" id="Pt0.A1.SS2.p1.3.m3.1.1.1.1.1.1.cmml" xref="Pt0.A1.SS2.p1.3.m3.1.1.1.1.1">subscript</csymbol><ci id="Pt0.A1.SS2.p1.3.m3.1.1.1.1.1.2.cmml" xref="Pt0.A1.SS2.p1.3.m3.1.1.1.1.1.2">𝑄</ci><ci id="Pt0.A1.SS2.p1.3.m3.1.1.1.1.1.3.cmml" xref="Pt0.A1.SS2.p1.3.m3.1.1.1.1.1.3">𝑇</ci></apply><apply id="Pt0.A1.SS2.p1.3.m3.2.2.2.2.2.cmml" xref="Pt0.A1.SS2.p1.3.m3.2.2.2.2.2"><csymbol cd="ambiguous" id="Pt0.A1.SS2.p1.3.m3.2.2.2.2.2.1.cmml" xref="Pt0.A1.SS2.p1.3.m3.2.2.2.2.2">subscript</csymbol><ci id="Pt0.A1.SS2.p1.3.m3.2.2.2.2.2.2.cmml" xref="Pt0.A1.SS2.p1.3.m3.2.2.2.2.2.2">𝐾</ci><ci id="Pt0.A1.SS2.p1.3.m3.2.2.2.2.2.3.cmml" xref="Pt0.A1.SS2.p1.3.m3.2.2.2.2.2.3">𝑇</ci></apply><apply id="Pt0.A1.SS2.p1.3.m3.3.3.3.3.3.cmml" xref="Pt0.A1.SS2.p1.3.m3.3.3.3.3.3"><csymbol cd="ambiguous" id="Pt0.A1.SS2.p1.3.m3.3.3.3.3.3.1.cmml" xref="Pt0.A1.SS2.p1.3.m3.3.3.3.3.3">subscript</csymbol><ci id="Pt0.A1.SS2.p1.3.m3.3.3.3.3.3.2.cmml" xref="Pt0.A1.SS2.p1.3.m3.3.3.3.3.3.2">𝑉</ci><ci id="Pt0.A1.SS2.p1.3.m3.3.3.3.3.3.3.cmml" xref="Pt0.A1.SS2.p1.3.m3.3.3.3.3.3.3">𝑇</ci></apply></list><apply id="Pt0.A1.SS2.p1.3.m3.3.3.5.cmml" xref="Pt0.A1.SS2.p1.3.m3.3.3.5"><csymbol cd="ambiguous" id="Pt0.A1.SS2.p1.3.m3.3.3.5.1.cmml" xref="Pt0.A1.SS2.p1.3.m3.3.3.5">superscript</csymbol><ci id="Pt0.A1.SS2.p1.3.m3.3.3.5.2.cmml" xref="Pt0.A1.SS2.p1.3.m3.3.3.5.2">ℝ</ci><apply id="Pt0.A1.SS2.p1.3.m3.3.3.5.3.cmml" xref="Pt0.A1.SS2.p1.3.m3.3.3.5.3"><times id="Pt0.A1.SS2.p1.3.m3.3.3.5.3.1.cmml" xref="Pt0.A1.SS2.p1.3.m3.3.3.5.3.1"></times><ci id="Pt0.A1.SS2.p1.3.m3.3.3.5.3.2.cmml" xref="Pt0.A1.SS2.p1.3.m3.3.3.5.3.2">𝑇</ci><ci id="Pt0.A1.SS2.p1.3.m3.3.3.5.3.3.cmml" xref="Pt0.A1.SS2.p1.3.m3.3.3.5.3.3">𝐽</ci><ci id="Pt0.A1.SS2.p1.3.m3.3.3.5.3.4.cmml" xref="Pt0.A1.SS2.p1.3.m3.3.3.5.3.4">𝐻</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A1.SS2.p1.3.m3.3c">Q_{T},K_{T},V_{T}\in\mathbb{R}^{T\times J\times H}</annotation><annotation encoding="application/x-llamapun" id="Pt0.A1.SS2.p1.3.m3.3d">italic_Q start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT , italic_K start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT , italic_V start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_T × italic_J × italic_H end_POSTSUPERSCRIPT</annotation></semantics></math> and spatial groups <math alttext="Q_{S},K_{S},V_{S}\in\mathbb{R}^{T\times J\times H}" class="ltx_Math" display="inline" id="Pt0.A1.SS2.p1.4.m4.3"><semantics id="Pt0.A1.SS2.p1.4.m4.3a"><mrow id="Pt0.A1.SS2.p1.4.m4.3.3" xref="Pt0.A1.SS2.p1.4.m4.3.3.cmml"><mrow id="Pt0.A1.SS2.p1.4.m4.3.3.3.3" xref="Pt0.A1.SS2.p1.4.m4.3.3.3.4.cmml"><msub id="Pt0.A1.SS2.p1.4.m4.1.1.1.1.1" xref="Pt0.A1.SS2.p1.4.m4.1.1.1.1.1.cmml"><mi id="Pt0.A1.SS2.p1.4.m4.1.1.1.1.1.2" xref="Pt0.A1.SS2.p1.4.m4.1.1.1.1.1.2.cmml">Q</mi><mi id="Pt0.A1.SS2.p1.4.m4.1.1.1.1.1.3" xref="Pt0.A1.SS2.p1.4.m4.1.1.1.1.1.3.cmml">S</mi></msub><mo id="Pt0.A1.SS2.p1.4.m4.3.3.3.3.4" xref="Pt0.A1.SS2.p1.4.m4.3.3.3.4.cmml">,</mo><msub id="Pt0.A1.SS2.p1.4.m4.2.2.2.2.2" xref="Pt0.A1.SS2.p1.4.m4.2.2.2.2.2.cmml"><mi id="Pt0.A1.SS2.p1.4.m4.2.2.2.2.2.2" xref="Pt0.A1.SS2.p1.4.m4.2.2.2.2.2.2.cmml">K</mi><mi id="Pt0.A1.SS2.p1.4.m4.2.2.2.2.2.3" xref="Pt0.A1.SS2.p1.4.m4.2.2.2.2.2.3.cmml">S</mi></msub><mo id="Pt0.A1.SS2.p1.4.m4.3.3.3.3.5" xref="Pt0.A1.SS2.p1.4.m4.3.3.3.4.cmml">,</mo><msub id="Pt0.A1.SS2.p1.4.m4.3.3.3.3.3" xref="Pt0.A1.SS2.p1.4.m4.3.3.3.3.3.cmml"><mi id="Pt0.A1.SS2.p1.4.m4.3.3.3.3.3.2" xref="Pt0.A1.SS2.p1.4.m4.3.3.3.3.3.2.cmml">V</mi><mi id="Pt0.A1.SS2.p1.4.m4.3.3.3.3.3.3" xref="Pt0.A1.SS2.p1.4.m4.3.3.3.3.3.3.cmml">S</mi></msub></mrow><mo id="Pt0.A1.SS2.p1.4.m4.3.3.4" xref="Pt0.A1.SS2.p1.4.m4.3.3.4.cmml">∈</mo><msup id="Pt0.A1.SS2.p1.4.m4.3.3.5" xref="Pt0.A1.SS2.p1.4.m4.3.3.5.cmml"><mi id="Pt0.A1.SS2.p1.4.m4.3.3.5.2" xref="Pt0.A1.SS2.p1.4.m4.3.3.5.2.cmml">ℝ</mi><mrow id="Pt0.A1.SS2.p1.4.m4.3.3.5.3" xref="Pt0.A1.SS2.p1.4.m4.3.3.5.3.cmml"><mi id="Pt0.A1.SS2.p1.4.m4.3.3.5.3.2" xref="Pt0.A1.SS2.p1.4.m4.3.3.5.3.2.cmml">T</mi><mo id="Pt0.A1.SS2.p1.4.m4.3.3.5.3.1" lspace="0.222em" rspace="0.222em" xref="Pt0.A1.SS2.p1.4.m4.3.3.5.3.1.cmml">×</mo><mi id="Pt0.A1.SS2.p1.4.m4.3.3.5.3.3" xref="Pt0.A1.SS2.p1.4.m4.3.3.5.3.3.cmml">J</mi><mo id="Pt0.A1.SS2.p1.4.m4.3.3.5.3.1a" lspace="0.222em" rspace="0.222em" xref="Pt0.A1.SS2.p1.4.m4.3.3.5.3.1.cmml">×</mo><mi id="Pt0.A1.SS2.p1.4.m4.3.3.5.3.4" xref="Pt0.A1.SS2.p1.4.m4.3.3.5.3.4.cmml">H</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="Pt0.A1.SS2.p1.4.m4.3b"><apply id="Pt0.A1.SS2.p1.4.m4.3.3.cmml" xref="Pt0.A1.SS2.p1.4.m4.3.3"><in id="Pt0.A1.SS2.p1.4.m4.3.3.4.cmml" xref="Pt0.A1.SS2.p1.4.m4.3.3.4"></in><list id="Pt0.A1.SS2.p1.4.m4.3.3.3.4.cmml" xref="Pt0.A1.SS2.p1.4.m4.3.3.3.3"><apply id="Pt0.A1.SS2.p1.4.m4.1.1.1.1.1.cmml" xref="Pt0.A1.SS2.p1.4.m4.1.1.1.1.1"><csymbol cd="ambiguous" id="Pt0.A1.SS2.p1.4.m4.1.1.1.1.1.1.cmml" xref="Pt0.A1.SS2.p1.4.m4.1.1.1.1.1">subscript</csymbol><ci id="Pt0.A1.SS2.p1.4.m4.1.1.1.1.1.2.cmml" xref="Pt0.A1.SS2.p1.4.m4.1.1.1.1.1.2">𝑄</ci><ci id="Pt0.A1.SS2.p1.4.m4.1.1.1.1.1.3.cmml" xref="Pt0.A1.SS2.p1.4.m4.1.1.1.1.1.3">𝑆</ci></apply><apply id="Pt0.A1.SS2.p1.4.m4.2.2.2.2.2.cmml" xref="Pt0.A1.SS2.p1.4.m4.2.2.2.2.2"><csymbol cd="ambiguous" id="Pt0.A1.SS2.p1.4.m4.2.2.2.2.2.1.cmml" xref="Pt0.A1.SS2.p1.4.m4.2.2.2.2.2">subscript</csymbol><ci id="Pt0.A1.SS2.p1.4.m4.2.2.2.2.2.2.cmml" xref="Pt0.A1.SS2.p1.4.m4.2.2.2.2.2.2">𝐾</ci><ci id="Pt0.A1.SS2.p1.4.m4.2.2.2.2.2.3.cmml" xref="Pt0.A1.SS2.p1.4.m4.2.2.2.2.2.3">𝑆</ci></apply><apply id="Pt0.A1.SS2.p1.4.m4.3.3.3.3.3.cmml" xref="Pt0.A1.SS2.p1.4.m4.3.3.3.3.3"><csymbol cd="ambiguous" id="Pt0.A1.SS2.p1.4.m4.3.3.3.3.3.1.cmml" xref="Pt0.A1.SS2.p1.4.m4.3.3.3.3.3">subscript</csymbol><ci id="Pt0.A1.SS2.p1.4.m4.3.3.3.3.3.2.cmml" xref="Pt0.A1.SS2.p1.4.m4.3.3.3.3.3.2">𝑉</ci><ci id="Pt0.A1.SS2.p1.4.m4.3.3.3.3.3.3.cmml" xref="Pt0.A1.SS2.p1.4.m4.3.3.3.3.3.3">𝑆</ci></apply></list><apply id="Pt0.A1.SS2.p1.4.m4.3.3.5.cmml" xref="Pt0.A1.SS2.p1.4.m4.3.3.5"><csymbol cd="ambiguous" id="Pt0.A1.SS2.p1.4.m4.3.3.5.1.cmml" xref="Pt0.A1.SS2.p1.4.m4.3.3.5">superscript</csymbol><ci id="Pt0.A1.SS2.p1.4.m4.3.3.5.2.cmml" xref="Pt0.A1.SS2.p1.4.m4.3.3.5.2">ℝ</ci><apply id="Pt0.A1.SS2.p1.4.m4.3.3.5.3.cmml" xref="Pt0.A1.SS2.p1.4.m4.3.3.5.3"><times id="Pt0.A1.SS2.p1.4.m4.3.3.5.3.1.cmml" xref="Pt0.A1.SS2.p1.4.m4.3.3.5.3.1"></times><ci id="Pt0.A1.SS2.p1.4.m4.3.3.5.3.2.cmml" xref="Pt0.A1.SS2.p1.4.m4.3.3.5.3.2">𝑇</ci><ci id="Pt0.A1.SS2.p1.4.m4.3.3.5.3.3.cmml" xref="Pt0.A1.SS2.p1.4.m4.3.3.5.3.3">𝐽</ci><ci id="Pt0.A1.SS2.p1.4.m4.3.3.5.3.4.cmml" xref="Pt0.A1.SS2.p1.4.m4.3.3.5.3.4">𝐻</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A1.SS2.p1.4.m4.3c">Q_{S},K_{S},V_{S}\in\mathbb{R}^{T\times J\times H}</annotation><annotation encoding="application/x-llamapun" id="Pt0.A1.SS2.p1.4.m4.3d">italic_Q start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT , italic_K start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT , italic_V start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_T × italic_J × italic_H end_POSTSUPERSCRIPT</annotation></semantics></math>. The temporal and spatial (skeleton joints) attentions are then calculated in two separate self-attention modules and concatenated before the next feed-forward layer and normalization. As a result of this operation, the receptive field of each transformer layer is the information residing on the spatial and temporal axis, and stacking multiple layers can approximate the full spatiotemporal attention without large computational overhead. In the following sections, we study the effectiveness of our design choice and compare its computation cost and performance against full attention and concurrent attention designs.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="Pt0.A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.B </span>Additional Experiments and Results</h2>
<div class="ltx_para" id="Pt0.A2.p1">
<p class="ltx_p" id="Pt0.A2.p1.1">This section describes the 2D datasets used during training and fine-tuning of our 2D pose estimator. We then study the details of our pipeline to evaluate its performance under different inputs, network architectures, and initialization strategies for 3D keypoint estimation. Next, we provide additional comparisons on the Human3.6m <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib18" title="">18</a>]</cite> dataset with weak or semi-supervised methods. We will also provide more comparisons with monocular pose estimation approaches on the RICH <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib16" title="">16</a>]</cite> dataset.</p>
</div>
<figure class="ltx_table" id="Pt0.A2.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Pt0.A2.T5.10.1.1" style="font-size:90%;">Table 5</span>: </span><span class="ltx_text" id="Pt0.A2.T5.11.2" style="font-size:90%;">Additional ablation study on Human3.6m dataset. We only report the computation cost of our pose compiler (in FLOPs) and exclude the CPN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib5" title="">5</a>]</cite> network with 5.16<span class="ltx_text ltx_font_italic" id="Pt0.A2.T5.11.2.1">T</span> FLOPs for 27 frames of 4 views. Additionally, 64.87<span class="ltx_text ltx_font_italic" id="Pt0.A2.T5.11.2.2">M</span> of parameters in all experiments belong to the CPN network. </span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="Pt0.A2.T5.6" style="width:433.6pt;height:243pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(56.2pt,-31.5pt) scale(1.34984135241585,1.34984135241585) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Pt0.A2.T5.6.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Pt0.A2.T5.4.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="Pt0.A2.T5.4.4.4.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="Pt0.A2.T5.4.4.4.5.1" style="font-size:70%;">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Pt0.A2.T5.1.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="Pt0.A2.T5.1.1.1.1.1" style="font-size:70%;">MPJPE<math alttext="\downarrow" class="ltx_Math" display="inline" id="Pt0.A2.T5.1.1.1.1.1.m1.1"><semantics id="Pt0.A2.T5.1.1.1.1.1.m1.1a"><mo id="Pt0.A2.T5.1.1.1.1.1.m1.1.1" stretchy="false" xref="Pt0.A2.T5.1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="Pt0.A2.T5.1.1.1.1.1.m1.1b"><ci id="Pt0.A2.T5.1.1.1.1.1.m1.1.1.cmml" xref="Pt0.A2.T5.1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A2.T5.1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="Pt0.A2.T5.1.1.1.1.1.m1.1d">↓</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Pt0.A2.T5.2.2.2.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="Pt0.A2.T5.2.2.2.2.1" style="font-size:70%;">PA-MPJPE<math alttext="\downarrow" class="ltx_Math" display="inline" id="Pt0.A2.T5.2.2.2.2.1.m1.1"><semantics id="Pt0.A2.T5.2.2.2.2.1.m1.1a"><mo id="Pt0.A2.T5.2.2.2.2.1.m1.1.1" stretchy="false" xref="Pt0.A2.T5.2.2.2.2.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="Pt0.A2.T5.2.2.2.2.1.m1.1b"><ci id="Pt0.A2.T5.2.2.2.2.1.m1.1.1.cmml" xref="Pt0.A2.T5.2.2.2.2.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A2.T5.2.2.2.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="Pt0.A2.T5.2.2.2.2.1.m1.1d">↓</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Pt0.A2.T5.3.3.3.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="Pt0.A2.T5.3.3.3.3.1" style="font-size:70%;">Param. (M)<math alttext="\downarrow" class="ltx_Math" display="inline" id="Pt0.A2.T5.3.3.3.3.1.m1.1"><semantics id="Pt0.A2.T5.3.3.3.3.1.m1.1a"><mo id="Pt0.A2.T5.3.3.3.3.1.m1.1.1" stretchy="false" xref="Pt0.A2.T5.3.3.3.3.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="Pt0.A2.T5.3.3.3.3.1.m1.1b"><ci id="Pt0.A2.T5.3.3.3.3.1.m1.1.1.cmml" xref="Pt0.A2.T5.3.3.3.3.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A2.T5.3.3.3.3.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="Pt0.A2.T5.3.3.3.3.1.m1.1d">↓</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Pt0.A2.T5.4.4.4.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="Pt0.A2.T5.4.4.4.4.1" style="font-size:70%;">Time (s)<math alttext="\downarrow" class="ltx_Math" display="inline" id="Pt0.A2.T5.4.4.4.4.1.m1.1"><semantics id="Pt0.A2.T5.4.4.4.4.1.m1.1a"><mo id="Pt0.A2.T5.4.4.4.4.1.m1.1.1" stretchy="false" xref="Pt0.A2.T5.4.4.4.4.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="Pt0.A2.T5.4.4.4.4.1.m1.1b"><ci id="Pt0.A2.T5.4.4.4.4.1.m1.1.1.cmml" xref="Pt0.A2.T5.4.4.4.4.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A2.T5.4.4.4.4.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="Pt0.A2.T5.4.4.4.4.1.m1.1d">↓</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Pt0.A2.T5.4.4.4.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="Pt0.A2.T5.4.4.4.6.1" style="font-size:70%;">FLOPs (G)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Pt0.A2.T5.5.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="Pt0.A2.T5.5.5.5.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text ltx_font_bold" id="Pt0.A2.T5.5.5.5.1.2" style="font-size:70%;">UPose3D</span><span class="ltx_text" id="Pt0.A2.T5.5.5.5.1.3" style="font-size:70%;"> </span><span class="ltx_text" id="Pt0.A2.T5.5.5.5.1.1" style="font-size:50%;">(T = 27, Tol<math alttext=".=10^{-3}" class="ltx_math_unparsed" display="inline" id="Pt0.A2.T5.5.5.5.1.1.m1.1"><semantics id="Pt0.A2.T5.5.5.5.1.1.m1.1a"><mrow id="Pt0.A2.T5.5.5.5.1.1.m1.1b"><mo id="Pt0.A2.T5.5.5.5.1.1.m1.1.1" rspace="0.0835em">.</mo><mo id="Pt0.A2.T5.5.5.5.1.1.m1.1.2" lspace="0.0835em">=</mo><msup id="Pt0.A2.T5.5.5.5.1.1.m1.1.3"><mn id="Pt0.A2.T5.5.5.5.1.1.m1.1.3.2">10</mn><mrow id="Pt0.A2.T5.5.5.5.1.1.m1.1.3.3"><mo id="Pt0.A2.T5.5.5.5.1.1.m1.1.3.3a">−</mo><mn id="Pt0.A2.T5.5.5.5.1.1.m1.1.3.3.2">3</mn></mrow></msup></mrow><annotation encoding="application/x-tex" id="Pt0.A2.T5.5.5.5.1.1.m1.1c">.=10^{-3}</annotation><annotation encoding="application/x-llamapun" id="Pt0.A2.T5.5.5.5.1.1.m1.1d">. = 10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT</annotation></semantics></math> <span class="ltx_text ltx_font_italic" id="Pt0.A2.T5.5.5.5.1.1.1">mm</span>)</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Pt0.A2.T5.5.5.5.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.5.5.5.2.1" style="font-size:70%;">26.42</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Pt0.A2.T5.5.5.5.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.5.5.5.3.1" style="font-size:70%;">23.42</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Pt0.A2.T5.5.5.5.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.5.5.5.4.1" style="font-size:70%;">65.407</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Pt0.A2.T5.5.5.5.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.5.5.5.5.1" style="font-size:70%;">10.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Pt0.A2.T5.5.5.5.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.5.5.5.6.1" style="font-size:70%;">2.04</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T5.6.6.7.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.T5.6.6.7.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.7.1.1.1" style="font-size:70%;">     w/ zero init</span></th>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.6.6.7.1.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.7.1.2.1" style="font-size:70%;">28.51</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.6.6.7.1.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.7.1.3.1" style="font-size:70%;">32.85</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.6.6.7.1.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.7.1.4.1" style="font-size:70%;">65.407</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.6.6.7.1.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.7.1.5.1" style="font-size:70%;">12.5</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.6.6.7.1.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.7.1.6.1" style="font-size:70%;">2.04</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T5.6.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.T5.6.6.6.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="Pt0.A2.T5.6.6.6.1.2" style="font-size:70%;">     w/ zero init </span><span class="ltx_text" id="Pt0.A2.T5.6.6.6.1.1" style="font-size:50%;">(Tol<math alttext=".=10^{-6}" class="ltx_math_unparsed" display="inline" id="Pt0.A2.T5.6.6.6.1.1.m1.1"><semantics id="Pt0.A2.T5.6.6.6.1.1.m1.1a"><mrow id="Pt0.A2.T5.6.6.6.1.1.m1.1b"><mo id="Pt0.A2.T5.6.6.6.1.1.m1.1.1" rspace="0.0835em">.</mo><mo id="Pt0.A2.T5.6.6.6.1.1.m1.1.2" lspace="0.0835em">=</mo><msup id="Pt0.A2.T5.6.6.6.1.1.m1.1.3"><mn id="Pt0.A2.T5.6.6.6.1.1.m1.1.3.2">10</mn><mrow id="Pt0.A2.T5.6.6.6.1.1.m1.1.3.3"><mo id="Pt0.A2.T5.6.6.6.1.1.m1.1.3.3a">−</mo><mn id="Pt0.A2.T5.6.6.6.1.1.m1.1.3.3.2">6</mn></mrow></msup></mrow><annotation encoding="application/x-tex" id="Pt0.A2.T5.6.6.6.1.1.m1.1c">.=10^{-6}</annotation><annotation encoding="application/x-llamapun" id="Pt0.A2.T5.6.6.6.1.1.m1.1d">. = 10 start_POSTSUPERSCRIPT - 6 end_POSTSUPERSCRIPT</annotation></semantics></math> <span class="ltx_text ltx_font_italic" id="Pt0.A2.T5.6.6.6.1.1.1">mm</span>)</span>
</th>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.6.6.6.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.6.2.1" style="font-size:70%;">26.42</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.6.6.6.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.6.3.1" style="font-size:70%;">23.42</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.6.6.6.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.6.4.1" style="font-size:70%;">65.407</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.6.6.6.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.6.5.1" style="font-size:70%;">28.9</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.6.6.6.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.6.6.1" style="font-size:70%;">2.04</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T5.6.6.8.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.T5.6.6.8.2.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.8.2.1.1" style="font-size:70%;">     w/ T = 243</span></th>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.6.6.8.2.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.8.2.2.1" style="font-size:70%;">33.17</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.6.6.8.2.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.8.2.3.1" style="font-size:70%;">25.11</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.6.6.8.2.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.8.2.4.1" style="font-size:70%;">62.660</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.6.6.8.2.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.8.2.5.1" style="font-size:70%;">10.9</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.6.6.8.2.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.8.2.6.1" style="font-size:70%;">20.18</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T5.6.6.9.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.T5.6.6.9.3.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.9.3.1.1" style="font-size:70%;">     w/ concurent attention</span></th>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.6.6.9.3.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.9.3.2.1" style="font-size:70%;">26.57</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.6.6.9.3.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.9.3.3.1" style="font-size:70%;">23.61</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.6.6.9.3.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.9.3.4.1" style="font-size:70%;">65.391</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.6.6.9.3.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.9.3.5.1" style="font-size:70%;">10.3</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.6.6.9.3.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.9.3.6.1" style="font-size:70%;">2.01</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T5.6.6.10.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.T5.6.6.10.4.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.10.4.1.1" style="font-size:70%;">     w/ full attention</span></th>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.6.6.10.4.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.10.4.2.1" style="font-size:70%;">26.50</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.6.6.10.4.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.10.4.3.1" style="font-size:70%;">23.57</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.6.6.10.4.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.10.4.4.1" style="font-size:70%;">65.322</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.6.6.10.4.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.10.4.5.1" style="font-size:70%;">10.3</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.6.6.10.4.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.10.4.6.1" style="font-size:70%;">2.28</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T5.6.6.11.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.T5.6.6.11.5.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.11.5.1.1" style="font-size:70%;">     w/ full attention (T = 243)</span></th>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.6.6.11.5.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.11.5.2.1" style="font-size:70%;">34.97</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.6.6.11.5.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.11.5.3.1" style="font-size:70%;">28.60</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.6.6.11.5.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.11.5.4.1" style="font-size:70%;">65.336</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.6.6.11.5.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.11.5.5.1" style="font-size:70%;">10.3</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.6.6.11.5.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.11.5.6.1" style="font-size:70%;">51.56</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T5.6.6.12.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.T5.6.6.12.6.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.12.6.1.1" style="font-size:70%;">     w/ epipolar line</span></th>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.6.6.12.6.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.12.6.2.1" style="font-size:70%;">26.46</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.6.6.12.6.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.12.6.3.1" style="font-size:70%;">23.45</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.6.6.12.6.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.12.6.4.1" style="font-size:70%;">65.407</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.6.6.12.6.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.12.6.5.1" style="font-size:70%;">10.1</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T5.6.6.12.6.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.12.6.6.1" style="font-size:70%;">2.04</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T5.6.6.13.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="Pt0.A2.T5.6.6.13.7.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.13.7.1.1" style="font-size:70%;">     w/ relative camera pos. emb.</span></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="Pt0.A2.T5.6.6.13.7.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.13.7.2.1" style="font-size:70%;">26.37</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Pt0.A2.T5.6.6.13.7.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.13.7.3.1" style="font-size:70%;">23.43</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Pt0.A2.T5.6.6.13.7.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.13.7.4.1" style="font-size:70%;">65.407</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Pt0.A2.T5.6.6.13.7.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.13.7.5.1" style="font-size:70%;">10.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Pt0.A2.T5.6.6.13.7.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="Pt0.A2.T5.6.6.13.7.6.1" style="font-size:70%;">2.04</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<section class="ltx_subsection" id="Pt0.A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">0.B.1 </span>2D Datasets</h3>
<div class="ltx_para ltx_noindent" id="Pt0.A2.SS1.p1">
<p class="ltx_p" id="Pt0.A2.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="Pt0.A2.SS1.p1.1.1">COCO WholeBody.</span>
The COCO WholeBody <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib21" title="">21</a>]</cite> dataset is a large-scale whole-body pose estimation dataset with over 250K samples. This dataset is an extension of Common Objects in COntext (COCO) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib36" title="">36</a>]</cite> dataset with the same training and testing splits. The dataset provides 133 2D keypoints (17 for body, 6 for feet, 68 for face, and 42 for hands) on in-the-wild images. We use this dataset to train our 2D pose estimator during OoD experiments.</p>
</div>
<div class="ltx_para ltx_noindent" id="Pt0.A2.SS1.p2">
<p class="ltx_p" id="Pt0.A2.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="Pt0.A2.SS1.p2.1.1">MPII.</span>
The MPII Human Pose dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib1" title="">1</a>]</cite> dataset is a popular 2D pose estimation benchmark. It contains over 40,000 images of people performing over 400 actions in diverse scenarios. The dataset contains 16 body joint labels and is frequently used to pre-train <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib66" title="">66</a>]</cite> or improve cross-dataset generalization<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib69" title="">69</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib47" title="">47</a>]</cite>. We use this dataset for 2D pose estimator pre-training and fine-tuning.</p>
</div>
</section>
<section class="ltx_subsection" id="Pt0.A2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">0.B.2 </span>Additional Ablation Study</h3>
<div class="ltx_para" id="Pt0.A2.SS2.p1">
<p class="ltx_p" id="Pt0.A2.SS2.p1.1">Following the ablation study originally presented in <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S5.SS3" title="5.3 Ablation Study ‣ 5 Results ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">5.3</span></a> of the main paper, we investigate the impact of temporal length, our spatiotemporal encoder’s architecture, different formulations of the point clouds, and our initialization strategy for 3D keypoint estimation in <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#Pt0.A2.T5" title="In Appendix 0.B Additional Experiments and Results ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">5</span></a>. Additionally, we report and provide the computational cost comparisons for a single input batch with T = 27 and 4 views. Our pose compiler is significantly smaller than a single 2D pose estimator, taking less than 1% of the total parameter count.</p>
</div>
<div class="ltx_para ltx_noindent" id="Pt0.A2.SS2.p2">
<p class="ltx_p" id="Pt0.A2.SS2.p2.4"><span class="ltx_text ltx_font_bold" id="Pt0.A2.SS2.p2.4.1">Random Initialization.</span>
We use the L-BFGS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib37" title="">37</a>]</cite> optimization algorithm to solve the 3D keypoint MLE iteratively. To speed up this process, we stop the optimization when the changes of our optimization variables, namely <math alttext="U" class="ltx_Math" display="inline" id="Pt0.A2.SS2.p2.1.m1.1"><semantics id="Pt0.A2.SS2.p2.1.m1.1a"><mi id="Pt0.A2.SS2.p2.1.m1.1.1" xref="Pt0.A2.SS2.p2.1.m1.1.1.cmml">U</mi><annotation-xml encoding="MathML-Content" id="Pt0.A2.SS2.p2.1.m1.1b"><ci id="Pt0.A2.SS2.p2.1.m1.1.1.cmml" xref="Pt0.A2.SS2.p2.1.m1.1.1">𝑈</ci></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A2.SS2.p2.1.m1.1c">U</annotation><annotation encoding="application/x-llamapun" id="Pt0.A2.SS2.p2.1.m1.1d">italic_U</annotation></semantics></math>, are less than a specific tolerance (Tol<math alttext=".=0.001" class="ltx_math_unparsed" display="inline" id="Pt0.A2.SS2.p2.2.m2.1"><semantics id="Pt0.A2.SS2.p2.2.m2.1a"><mrow id="Pt0.A2.SS2.p2.2.m2.1b"><mo id="Pt0.A2.SS2.p2.2.m2.1.1" rspace="0.0835em">.</mo><mo id="Pt0.A2.SS2.p2.2.m2.1.2" lspace="0.0835em">=</mo><mn id="Pt0.A2.SS2.p2.2.m2.1.3">0.001</mn></mrow><annotation encoding="application/x-tex" id="Pt0.A2.SS2.p2.2.m2.1c">.=0.001</annotation><annotation encoding="application/x-llamapun" id="Pt0.A2.SS2.p2.2.m2.1d">. = 0.001</annotation></semantics></math> <span class="ltx_text ltx_font_italic" id="Pt0.A2.SS2.p2.4.2">mm</span>). We further speed up the optimization process by using a DLT algorithm to initialize the 3D points <math alttext="U" class="ltx_Math" display="inline" id="Pt0.A2.SS2.p2.3.m3.1"><semantics id="Pt0.A2.SS2.p2.3.m3.1a"><mi id="Pt0.A2.SS2.p2.3.m3.1.1" xref="Pt0.A2.SS2.p2.3.m3.1.1.cmml">U</mi><annotation-xml encoding="MathML-Content" id="Pt0.A2.SS2.p2.3.m3.1b"><ci id="Pt0.A2.SS2.p2.3.m3.1.1.cmml" xref="Pt0.A2.SS2.p2.3.m3.1.1">𝑈</ci></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A2.SS2.p2.3.m3.1c">U</annotation><annotation encoding="application/x-llamapun" id="Pt0.A2.SS2.p2.3.m3.1d">italic_U</annotation></semantics></math>. <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#Pt0.A2.T5" title="In Appendix 0.B Additional Experiments and Results ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">5</span></a> first examines the effect of our initialization strategy when <math alttext="U" class="ltx_Math" display="inline" id="Pt0.A2.SS2.p2.4.m4.1"><semantics id="Pt0.A2.SS2.p2.4.m4.1a"><mi id="Pt0.A2.SS2.p2.4.m4.1.1" xref="Pt0.A2.SS2.p2.4.m4.1.1.cmml">U</mi><annotation-xml encoding="MathML-Content" id="Pt0.A2.SS2.p2.4.m4.1b"><ci id="Pt0.A2.SS2.p2.4.m4.1.1.cmml" xref="Pt0.A2.SS2.p2.4.m4.1.1">𝑈</ci></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A2.SS2.p2.4.m4.1c">U</annotation><annotation encoding="application/x-llamapun" id="Pt0.A2.SS2.p2.4.m4.1d">italic_U</annotation></semantics></math> is initialized to zero, and the tolerance remains unchanged, showing a significant rise in the 3D keypoint estimation error and inference time. Next, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#Pt0.A2.T5" title="In Appendix 0.B Additional Experiments and Results ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">5</span></a> shows that by lowering the tolerance, zero-initialization performs similarly to our proposed strategy, but at 3 times more inference time. Therefore, we conclude that unlike prior works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib60" title="">60</a>]</cite>, our method is not reliant on initialization, and the initialization only speeds up the estimation process. This may be due to the smooth nature of the uncertainty distributions learned by the normalizing flows <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib31" title="">31</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="Pt0.A2.SS2.p3">
<p class="ltx_p" id="Pt0.A2.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="Pt0.A2.SS2.p3.1.1">Longer Temporal Window.</span>
We study the computational cost and performance impact of very long temporal context size. Following <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib70" title="">70</a>]</cite>, we report the performance of UPose3D when 243 frames, as opposed to 27 frames, to infer the 3D keypoints of the center frame in <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#Pt0.A2.T5" title="In Appendix 0.B Additional Experiments and Results ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">5</span></a>. This new model takes 10 times more FLOPs to compute and does not perform as well as our original model.
This may be because our synthetic data augmentations and corruption strategies are tuned for smaller time windows, as longer context sizes were not in our considerations. Our observations of the training and validation losses also show signs of overfitting during training for longer time windows. As extremely long context sizes are not in the scope of this paper, we do not perform any additional tuning of these models and leave them for future research.</p>
</div>
<div class="ltx_para ltx_noindent" id="Pt0.A2.SS2.p4">
<p class="ltx_p" id="Pt0.A2.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="Pt0.A2.SS2.p4.1.1">Pose Compiler Architecture.</span>
We compare the effect of our criss-cross attention modules with vanilla (full) and concurrent attention. <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#Pt0.A2.T5" title="In Appendix 0.B Additional Experiments and Results ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">5</span></a> shows that criss-cross attention outperforms the other two designs while requiring less computation (FLOPs) than full attention. Additionally, we observe that on the extreme case of very long temporal context sizes (T = 243), criss-cross attention still outperforms full attention models by 1.8 <span class="ltx_text ltx_font_italic" id="Pt0.A2.SS2.p4.1.2">mm</span> while requiring 60% less computations.</p>
</div>
<figure class="ltx_table" id="Pt0.A2.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Pt0.A2.T6.5.1.1" style="font-size:90%;">Table 6</span>: </span><span class="ltx_text" id="Pt0.A2.T6.6.2" style="font-size:90%;">Additional comparisons with prior works on the full test set of the Human3.6m dataset in InD settings. (-) denotes that the error was not reported in the original work.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Pt0.A2.T6.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Pt0.A2.T6.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Pt0.A2.T6.3.3.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="Pt0.A2.T6.3.3.4.1" style="font-size:70%;">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Pt0.A2.T6.3.3.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="Pt0.A2.T6.3.3.5.1" style="font-size:70%;">Supervision</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Pt0.A2.T6.3.3.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="Pt0.A2.T6.3.3.6.1" style="font-size:70%;">Multi-view</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Pt0.A2.T6.3.3.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="Pt0.A2.T6.3.3.7.1" style="font-size:70%;">Frames</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Pt0.A2.T6.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="Pt0.A2.T6.1.1.1.1" style="font-size:70%;">MPJPE<math alttext="\downarrow" class="ltx_Math" display="inline" id="Pt0.A2.T6.1.1.1.1.m1.1"><semantics id="Pt0.A2.T6.1.1.1.1.m1.1a"><mo id="Pt0.A2.T6.1.1.1.1.m1.1.1" stretchy="false" xref="Pt0.A2.T6.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="Pt0.A2.T6.1.1.1.1.m1.1b"><ci id="Pt0.A2.T6.1.1.1.1.m1.1.1.cmml" xref="Pt0.A2.T6.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A2.T6.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="Pt0.A2.T6.1.1.1.1.m1.1d">↓</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Pt0.A2.T6.2.2.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="Pt0.A2.T6.2.2.2.1" style="font-size:70%;">PA-MPJPE<math alttext="\downarrow" class="ltx_Math" display="inline" id="Pt0.A2.T6.2.2.2.1.m1.1"><semantics id="Pt0.A2.T6.2.2.2.1.m1.1a"><mo id="Pt0.A2.T6.2.2.2.1.m1.1.1" stretchy="false" xref="Pt0.A2.T6.2.2.2.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="Pt0.A2.T6.2.2.2.1.m1.1b"><ci id="Pt0.A2.T6.2.2.2.1.m1.1.1.cmml" xref="Pt0.A2.T6.2.2.2.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A2.T6.2.2.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="Pt0.A2.T6.2.2.2.1.m1.1d">↓</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Pt0.A2.T6.3.3.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="Pt0.A2.T6.3.3.3.1" style="font-size:70%;">N-MPJPE<math alttext="\downarrow" class="ltx_Math" display="inline" id="Pt0.A2.T6.3.3.3.1.m1.1"><semantics id="Pt0.A2.T6.3.3.3.1.m1.1a"><mo id="Pt0.A2.T6.3.3.3.1.m1.1.1" stretchy="false" xref="Pt0.A2.T6.3.3.3.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="Pt0.A2.T6.3.3.3.1.m1.1b"><ci id="Pt0.A2.T6.3.3.3.1.m1.1.1.cmml" xref="Pt0.A2.T6.3.3.3.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A2.T6.3.3.3.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="Pt0.A2.T6.3.3.3.1.m1.1d">↓</annotation></semantics></math></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Pt0.A2.T6.3.4.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="Pt0.A2.T6.3.4.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="Pt0.A2.T6.3.4.1.1.1" style="font-size:70%;">Rhodin </span><em class="ltx_emph ltx_font_italic" id="Pt0.A2.T6.3.4.1.1.2" style="font-size:70%;">et al</em><span class="ltx_text" id="Pt0.A2.T6.3.4.1.1.3" style="font-size:70%;">.</span><span class="ltx_text" id="Pt0.A2.T6.3.4.1.1.4"></span><span class="ltx_text" id="Pt0.A2.T6.3.4.1.1.5" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="Pt0.A2.T6.3.4.1.1.6.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib53" title="">53</a><span class="ltx_text" id="Pt0.A2.T6.3.4.1.1.7.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Pt0.A2.T6.3.4.1.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.4.1.2.1" style="font-size:70%;">3D</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Pt0.A2.T6.3.4.1.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.4.1.3.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Pt0.A2.T6.3.4.1.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.4.1.4.1" style="font-size:70%;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Pt0.A2.T6.3.4.1.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.4.1.5.1" style="font-size:70%;">66.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Pt0.A2.T6.3.4.1.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.4.1.6.1" style="font-size:70%;">51.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Pt0.A2.T6.3.4.1.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.4.1.7.1" style="font-size:70%;">63.3</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T6.3.5.2">
<td class="ltx_td ltx_align_left" id="Pt0.A2.T6.3.5.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="Pt0.A2.T6.3.5.2.1.1" style="font-size:70%;">Rhodin </span><em class="ltx_emph ltx_font_italic" id="Pt0.A2.T6.3.5.2.1.2" style="font-size:70%;">et al</em><span class="ltx_text" id="Pt0.A2.T6.3.5.2.1.3" style="font-size:70%;">.</span><span class="ltx_text" id="Pt0.A2.T6.3.5.2.1.4"></span><span class="ltx_text" id="Pt0.A2.T6.3.5.2.1.5" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="Pt0.A2.T6.3.5.2.1.6.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib53" title="">53</a><span class="ltx_text" id="Pt0.A2.T6.3.5.2.1.7.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T6.3.5.2.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.5.2.2.1" style="font-size:70%;">Weakly 3D</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T6.3.5.2.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.5.2.3.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T6.3.5.2.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.5.2.4.1" style="font-size:70%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T6.3.5.2.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.5.2.5.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T6.3.5.2.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.5.2.6.1" style="font-size:70%;">65.1</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T6.3.5.2.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.5.2.7.1" style="font-size:70%;">80.1</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T6.3.6.3">
<td class="ltx_td ltx_align_left" id="Pt0.A2.T6.3.6.3.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="Pt0.A2.T6.3.6.3.1.1" style="font-size:70%;">EpipolarPose </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="Pt0.A2.T6.3.6.3.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib27" title="">27</a><span class="ltx_text" id="Pt0.A2.T6.3.6.3.1.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T6.3.6.3.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.6.3.2.1" style="font-size:70%;">Weakly 3D</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T6.3.6.3.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.6.3.3.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T6.3.6.3.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.6.3.4.1" style="font-size:70%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T6.3.6.3.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.6.3.5.1" style="font-size:70%;">55.08</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T6.3.6.3.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.6.3.6.1" style="font-size:70%;">47.91</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T6.3.6.3.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.6.3.7.1" style="font-size:70%;">54.90</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T6.3.7.4">
<td class="ltx_td ltx_align_left" id="Pt0.A2.T6.3.7.4.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="Pt0.A2.T6.3.7.4.1.1" style="font-size:70%;">CanonPose </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="Pt0.A2.T6.3.7.4.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib62" title="">62</a><span class="ltx_text" id="Pt0.A2.T6.3.7.4.1.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T6.3.7.4.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.7.4.2.1" style="font-size:70%;">Weakly 3D</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T6.3.7.4.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.7.4.3.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T6.3.7.4.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.7.4.4.1" style="font-size:70%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T6.3.7.4.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.7.4.5.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T6.3.7.4.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.7.4.6.1" style="font-size:70%;">53.0</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T6.3.7.4.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.7.4.7.1" style="font-size:70%;">82.0</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T6.3.8.5">
<td class="ltx_td ltx_align_left" id="Pt0.A2.T6.3.8.5.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="Pt0.A2.T6.3.8.5.1.1" style="font-size:70%;">Gong </span><em class="ltx_emph ltx_font_italic" id="Pt0.A2.T6.3.8.5.1.2" style="font-size:70%;">et al</em><span class="ltx_text" id="Pt0.A2.T6.3.8.5.1.3" style="font-size:70%;">.</span><span class="ltx_text" id="Pt0.A2.T6.3.8.5.1.4"></span><span class="ltx_text" id="Pt0.A2.T6.3.8.5.1.5" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="Pt0.A2.T6.3.8.5.1.6.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib9" title="">9</a><span class="ltx_text" id="Pt0.A2.T6.3.8.5.1.7.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T6.3.8.5.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.8.5.2.1" style="font-size:70%;">Synthetic 3D</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T6.3.8.5.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.8.5.3.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T6.3.8.5.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.8.5.4.1" style="font-size:70%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T6.3.8.5.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.8.5.5.1" style="font-size:70%;">53.8</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T6.3.8.5.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.8.5.6.1" style="font-size:70%;">42.4</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T6.3.8.5.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.8.5.7.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T6.3.9.6">
<td class="ltx_td ltx_align_left" id="Pt0.A2.T6.3.9.6.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="Pt0.A2.T6.3.9.6.1.1" style="font-size:70%;">BKinD-3D </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="Pt0.A2.T6.3.9.6.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib56" title="">56</a><span class="ltx_text" id="Pt0.A2.T6.3.9.6.1.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T6.3.9.6.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.9.6.2.1" style="font-size:70%;">3D Discovery</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T6.3.9.6.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.9.6.3.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T6.3.9.6.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.9.6.4.1" style="font-size:70%;">20</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T6.3.9.6.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.9.6.5.1" style="font-size:70%;">125.0</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T6.3.9.6.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.9.6.6.1" style="font-size:70%;">105.0</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T6.3.9.6.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.9.6.7.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T6.3.10.7">
<td class="ltx_td ltx_align_left" id="Pt0.A2.T6.3.10.7.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.10.7.1.1" style="font-size:70%;">UPose3D (Ours)</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T6.3.10.7.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.10.7.2.1" style="font-size:70%;">2D</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T6.3.10.7.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.10.7.3.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T6.3.10.7.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.10.7.4.1" style="font-size:70%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T6.3.10.7.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.10.7.5.1" style="font-size:70%;">26.9</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T6.3.10.7.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.10.7.6.1" style="font-size:70%;">24.1</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T6.3.10.7.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.10.7.7.1" style="font-size:70%;">26.2</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T6.3.11.8">
<td class="ltx_td ltx_align_left ltx_border_b" id="Pt0.A2.T6.3.11.8.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.11.8.1.1" style="font-size:70%;">UPose3D (Ours)</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Pt0.A2.T6.3.11.8.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.11.8.2.1" style="font-size:70%;">2D</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Pt0.A2.T6.3.11.8.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.11.8.3.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Pt0.A2.T6.3.11.8.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.11.8.4.1" style="font-size:70%;">27</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Pt0.A2.T6.3.11.8.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.11.8.5.1" style="font-size:70%;">26.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Pt0.A2.T6.3.11.8.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.11.8.6.1" style="font-size:70%;">23.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Pt0.A2.T6.3.11.8.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="Pt0.A2.T6.3.11.8.7.1" style="font-size:70%;">25.6</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="Pt0.A2.T7">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Pt0.A2.T7.6.1.1" style="font-size:90%;">Table 7</span>: </span><span class="ltx_text" id="Pt0.A2.T7.7.2" style="font-size:90%;">Comparison of our method in OoD setting on RICH dataset against prior works. * denotes our replication of prior works.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Pt0.A2.T7.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Pt0.A2.T7.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="Pt0.A2.T7.2.2.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="Pt0.A2.T7.2.2.3.1" style="font-size:70%;">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Pt0.A2.T7.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="Pt0.A2.T7.1.1.1.1" style="font-size:70%;">MPJPE<math alttext="\downarrow" class="ltx_Math" display="inline" id="Pt0.A2.T7.1.1.1.1.m1.1"><semantics id="Pt0.A2.T7.1.1.1.1.m1.1a"><mo id="Pt0.A2.T7.1.1.1.1.m1.1.1" stretchy="false" xref="Pt0.A2.T7.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="Pt0.A2.T7.1.1.1.1.m1.1b"><ci id="Pt0.A2.T7.1.1.1.1.m1.1.1.cmml" xref="Pt0.A2.T7.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A2.T7.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="Pt0.A2.T7.1.1.1.1.m1.1d">↓</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Pt0.A2.T7.2.2.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="Pt0.A2.T7.2.2.2.1" style="font-size:70%;">PA-MPJPE<math alttext="\downarrow" class="ltx_Math" display="inline" id="Pt0.A2.T7.2.2.2.1.m1.1"><semantics id="Pt0.A2.T7.2.2.2.1.m1.1a"><mo id="Pt0.A2.T7.2.2.2.1.m1.1.1" stretchy="false" xref="Pt0.A2.T7.2.2.2.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="Pt0.A2.T7.2.2.2.1.m1.1b"><ci id="Pt0.A2.T7.2.2.2.1.m1.1.1.cmml" xref="Pt0.A2.T7.2.2.2.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A2.T7.2.2.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="Pt0.A2.T7.2.2.2.1.m1.1d">↓</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Pt0.A2.T7.2.2.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="Pt0.A2.T7.2.2.4.1" style="font-size:70%;">OoD</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Pt0.A2.T7.2.2.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="Pt0.A2.T7.2.2.5.1" style="font-size:70%;">Multi-view</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Pt0.A2.T7.2.2.6" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="Pt0.A2.T7.2.2.6.1" style="font-size:70%;">Output</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Pt0.A2.T7.4.5.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="Pt0.A2.T7.4.5.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text" id="Pt0.A2.T7.4.5.1.1.1" style="font-size:70%;">SA-HMR </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="Pt0.A2.T7.4.5.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib54" title="">54</a><span class="ltx_text" id="Pt0.A2.T7.4.5.1.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Pt0.A2.T7.4.5.1.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.5.1.2.1" style="font-size:70%;">93.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Pt0.A2.T7.4.5.1.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.5.1.3.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Pt0.A2.T7.4.5.1.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.5.1.4.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Pt0.A2.T7.4.5.1.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.5.1.5.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Pt0.A2.T7.4.5.1.6" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.5.1.6.1" style="font-size:70%;">SMPL</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T7.4.6.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.T7.4.6.2.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text" id="Pt0.A2.T7.4.6.2.1.1" style="font-size:70%;">IPMAN-R </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="Pt0.A2.T7.4.6.2.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib59" title="">59</a><span class="ltx_text" id="Pt0.A2.T7.4.6.2.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.6.2.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.6.2.2.1" style="font-size:70%;">79.0</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.6.2.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.6.2.3.1" style="font-size:70%;">47.6</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.6.2.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.6.2.4.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.6.2.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.6.2.5.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.6.2.6" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.6.2.6.1" style="font-size:70%;">SMPL</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T7.4.7.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.T7.4.7.3.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text" id="Pt0.A2.T7.4.7.3.1.1" style="font-size:70%;">METRO </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="Pt0.A2.T7.4.7.3.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib35" title="">35</a><span class="ltx_text" id="Pt0.A2.T7.4.7.3.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.7.3.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.7.3.2.1" style="font-size:70%;">98.8</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.7.3.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.7.3.3.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.7.3.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.7.3.4.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.7.3.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.7.3.5.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.7.3.6" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.7.3.6.1" style="font-size:70%;">SMPL</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T7.4.8.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.T7.4.8.4.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text" id="Pt0.A2.T7.4.8.4.1.1" style="font-size:70%;">METRO </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="Pt0.A2.T7.4.8.4.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib35" title="">35</a><span class="ltx_text" id="Pt0.A2.T7.4.8.4.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.8.4.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.8.4.2.1" style="font-size:70%;">129.6</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.8.4.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.8.4.3.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.8.4.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.8.4.4.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.8.4.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.8.4.5.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.8.4.6" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.8.4.6.1" style="font-size:70%;">SMPL</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T7.4.9.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.T7.4.9.5.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text" id="Pt0.A2.T7.4.9.5.1.1" style="font-size:70%;">SPIN </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="Pt0.A2.T7.4.9.5.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib28" title="">28</a><span class="ltx_text" id="Pt0.A2.T7.4.9.5.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.9.5.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.9.5.2.1" style="font-size:70%;">112.2</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.9.5.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.9.5.3.1" style="font-size:70%;">71.5</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.9.5.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.9.5.4.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.9.5.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.9.5.5.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.9.5.6" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.9.5.6.1" style="font-size:70%;">SMPL</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T7.4.10.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.T7.4.10.6.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text" id="Pt0.A2.T7.4.10.6.1.1" style="font-size:70%;">PARE </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="Pt0.A2.T7.4.10.6.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib26" title="">26</a><span class="ltx_text" id="Pt0.A2.T7.4.10.6.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.10.6.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.10.6.2.1" style="font-size:70%;">107.0</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.10.6.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.10.6.3.1" style="font-size:70%;">73.1</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.10.6.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.10.6.4.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.10.6.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.10.6.5.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.10.6.6" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.10.6.6.1" style="font-size:70%;">SMPL</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T7.4.11.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.T7.4.11.7.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text" id="Pt0.A2.T7.4.11.7.1.1" style="font-size:70%;">CLIFF </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="Pt0.A2.T7.4.11.7.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib34" title="">34</a><span class="ltx_text" id="Pt0.A2.T7.4.11.7.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.11.7.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.11.7.2.1" style="font-size:70%;">107.0</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.11.7.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.11.7.3.1" style="font-size:70%;">67.2</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.11.7.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.11.7.4.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.11.7.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.11.7.5.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.11.7.6" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.11.7.6.1" style="font-size:70%;">SMPL</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T7.4.12.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.T7.4.12.8.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text" id="Pt0.A2.T7.4.12.8.1.1" style="font-size:70%;">AdaFuse</span><sup class="ltx_sup" id="Pt0.A2.T7.4.12.8.1.2"><span class="ltx_text" id="Pt0.A2.T7.4.12.8.1.2.1" style="font-size:70%;">*</span></sup><span class="ltx_text" id="Pt0.A2.T7.4.12.8.1.3" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="Pt0.A2.T7.4.12.8.1.4.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib66" title="">66</a><span class="ltx_text" id="Pt0.A2.T7.4.12.8.1.5.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.12.8.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.12.8.2.1" style="font-size:70%;">524.0</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.12.8.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.12.8.3.1" style="font-size:70%;">85.8</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.12.8.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.12.8.4.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.12.8.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.12.8.5.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.12.8.6" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.12.8.6.1" style="font-size:70%;">3D Keypoints</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T7.4.13.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.T7.4.13.9.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text" id="Pt0.A2.T7.4.13.9.1.1" style="font-size:70%;">HRNet-W48</span><span class="ltx_text ltx_font_typewriter" id="Pt0.A2.T7.4.13.9.1.2" style="font-size:70%;">+</span><span class="ltx_text" id="Pt0.A2.T7.4.13.9.1.3" style="font-size:70%;">Grid Search</span><sup class="ltx_sup" id="Pt0.A2.T7.4.13.9.1.4"><span class="ltx_text" id="Pt0.A2.T7.4.13.9.1.4.1" style="font-size:70%;">*</span></sup>
</th>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.13.9.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.13.9.2.1" style="font-size:70%;">64.4</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.13.9.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.13.9.3.1" style="font-size:70%;">54.9</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.13.9.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.13.9.4.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.13.9.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.13.9.5.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.13.9.6" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.13.9.6.1" style="font-size:70%;">3D Keypoints</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T7.4.14.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.T7.4.14.10.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text" id="Pt0.A2.T7.4.14.10.1.1" style="font-size:70%;">HRNet-W48</span><span class="ltx_text ltx_font_typewriter" id="Pt0.A2.T7.4.14.10.1.2" style="font-size:70%;">+</span><span class="ltx_text" id="Pt0.A2.T7.4.14.10.1.3" style="font-size:70%;">DLT</span><sup class="ltx_sup" id="Pt0.A2.T7.4.14.10.1.4"><span class="ltx_text" id="Pt0.A2.T7.4.14.10.1.4.1" style="font-size:70%;">*</span></sup>
</th>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.14.10.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.14.10.2.1" style="font-size:70%;">66.0</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.14.10.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.14.10.3.1" style="font-size:70%;">55.1</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.14.10.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.14.10.4.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.14.10.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.14.10.5.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.4.14.10.6" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.14.10.6.1" style="font-size:70%;">3D Keypoints</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T7.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.T7.3.3.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text" id="Pt0.A2.T7.3.3.1.1" style="font-size:70%;">Ours (</span><math alttext="T=1" class="ltx_Math" display="inline" id="Pt0.A2.T7.3.3.1.m1.1"><semantics id="Pt0.A2.T7.3.3.1.m1.1a"><mrow id="Pt0.A2.T7.3.3.1.m1.1.1" xref="Pt0.A2.T7.3.3.1.m1.1.1.cmml"><mi id="Pt0.A2.T7.3.3.1.m1.1.1.2" mathsize="70%" xref="Pt0.A2.T7.3.3.1.m1.1.1.2.cmml">T</mi><mo id="Pt0.A2.T7.3.3.1.m1.1.1.1" mathsize="70%" xref="Pt0.A2.T7.3.3.1.m1.1.1.1.cmml">=</mo><mn id="Pt0.A2.T7.3.3.1.m1.1.1.3" mathsize="70%" xref="Pt0.A2.T7.3.3.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="Pt0.A2.T7.3.3.1.m1.1b"><apply id="Pt0.A2.T7.3.3.1.m1.1.1.cmml" xref="Pt0.A2.T7.3.3.1.m1.1.1"><eq id="Pt0.A2.T7.3.3.1.m1.1.1.1.cmml" xref="Pt0.A2.T7.3.3.1.m1.1.1.1"></eq><ci id="Pt0.A2.T7.3.3.1.m1.1.1.2.cmml" xref="Pt0.A2.T7.3.3.1.m1.1.1.2">𝑇</ci><cn id="Pt0.A2.T7.3.3.1.m1.1.1.3.cmml" type="integer" xref="Pt0.A2.T7.3.3.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A2.T7.3.3.1.m1.1c">T=1</annotation><annotation encoding="application/x-llamapun" id="Pt0.A2.T7.3.3.1.m1.1d">italic_T = 1</annotation></semantics></math><span class="ltx_text" id="Pt0.A2.T7.3.3.1.2" style="font-size:70%;">)</span>
</th>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.3.3.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.3.3.2.1" style="font-size:70%;">36.2</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.3.3.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.3.3.3.1" style="font-size:70%;">33.4</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.3.3.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.3.3.4.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.3.3.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.3.3.5.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.T7.3.3.6" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.3.3.6.1" style="font-size:70%;">3D Keypoints</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T7.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="Pt0.A2.T7.4.4.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text" id="Pt0.A2.T7.4.4.1.1" style="font-size:70%;">Ours (</span><math alttext="T=27" class="ltx_Math" display="inline" id="Pt0.A2.T7.4.4.1.m1.1"><semantics id="Pt0.A2.T7.4.4.1.m1.1a"><mrow id="Pt0.A2.T7.4.4.1.m1.1.1" xref="Pt0.A2.T7.4.4.1.m1.1.1.cmml"><mi id="Pt0.A2.T7.4.4.1.m1.1.1.2" mathsize="70%" xref="Pt0.A2.T7.4.4.1.m1.1.1.2.cmml">T</mi><mo id="Pt0.A2.T7.4.4.1.m1.1.1.1" mathsize="70%" xref="Pt0.A2.T7.4.4.1.m1.1.1.1.cmml">=</mo><mn id="Pt0.A2.T7.4.4.1.m1.1.1.3" mathsize="70%" xref="Pt0.A2.T7.4.4.1.m1.1.1.3.cmml">27</mn></mrow><annotation-xml encoding="MathML-Content" id="Pt0.A2.T7.4.4.1.m1.1b"><apply id="Pt0.A2.T7.4.4.1.m1.1.1.cmml" xref="Pt0.A2.T7.4.4.1.m1.1.1"><eq id="Pt0.A2.T7.4.4.1.m1.1.1.1.cmml" xref="Pt0.A2.T7.4.4.1.m1.1.1.1"></eq><ci id="Pt0.A2.T7.4.4.1.m1.1.1.2.cmml" xref="Pt0.A2.T7.4.4.1.m1.1.1.2">𝑇</ci><cn id="Pt0.A2.T7.4.4.1.m1.1.1.3.cmml" type="integer" xref="Pt0.A2.T7.4.4.1.m1.1.1.3">27</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A2.T7.4.4.1.m1.1c">T=27</annotation><annotation encoding="application/x-llamapun" id="Pt0.A2.T7.4.4.1.m1.1d">italic_T = 27</annotation></semantics></math><span class="ltx_text" id="Pt0.A2.T7.4.4.1.2" style="font-size:70%;">)</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="Pt0.A2.T7.4.4.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.4.2.1" style="font-size:70%;">34.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Pt0.A2.T7.4.4.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.4.3.1" style="font-size:70%;">32.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Pt0.A2.T7.4.4.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.4.4.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Pt0.A2.T7.4.4.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.4.5.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="Pt0.A2.T7.4.4.6" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="Pt0.A2.T7.4.4.6.1" style="font-size:70%;">3D Keypoints</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="Pt0.A2.SS2.p5">
<p class="ltx_p" id="Pt0.A2.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="Pt0.A2.SS2.p5.1.1">Inputs of Pose Compiler.</span>
Finally, we investigate the effect of different point cloud formation strategies in our pipeline. Specifically, we study the impact of appending a relative camera position embedding, inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib55" title="">55</a>]</cite>, to the cross-projected 2D keypoints while creating the point clouds. Accordingly, in our first experiment, we concatenate the epipolar line parameters of other views to the point cloud of the reference view. Similarly, in our second experiment, we concatenate the relative position of the other cameras to the input point cloud as well. However, as shown in <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#Pt0.A2.T5" title="In Appendix 0.B Additional Experiments and Results ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">5</span></a>, adding extra inputs does not greatly impact the performance.</p>
</div>
</section>
<section class="ltx_subsection" id="Pt0.A2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">0.B.3 </span>Additional Baselines and Comparisons</h3>
<div class="ltx_para" id="Pt0.A2.SS3.p1">
<p class="ltx_p" id="Pt0.A2.SS3.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#Pt0.A2.T6" title="In 0.B.2 Additional Ablation Study ‣ Appendix 0.B Additional Experiments and Results ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">6</span></a> complements <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#S4.T1" title="In 4.4 Baselines ‣ 4 Experiments ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a> of the main paper by providing more comparisons with prior works on the Human3.6m <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib18" title="">18</a>]</cite> dataset. Here, we include 3D keypoint estimation approaches regardless of their input modality or supervision type in InD settings. We observe that our method outperforms all of the other approaches despite only using 2D supervision. Additionally, in <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#Pt0.A2.T7" title="In 0.B.2 Additional Ablation Study ‣ Appendix 0.B Additional Experiments and Results ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">7</span></a>, we compare our work with prior research on the RICH <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib16" title="">16</a>]</cite> dataset. Since this dataset was recently published, only monocular 3D body modeling techniques have reported their performance on this dataset. Here, we observe that our method outperforms the majority of prior works. More importantly, when comparing <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#Pt0.A2.T6" title="In 0.B.2 Additional Ablation Study ‣ Appendix 0.B Additional Experiments and Results ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">6</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#Pt0.A2.T7" title="In 0.B.2 Additional Ablation Study ‣ Appendix 0.B Additional Experiments and Results ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">7</span></a> we notice that our method achieves consistent results between InD and OoD evaluations on the Human3.6m <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib18" title="">18</a>]</cite> and the RICH <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib16" title="">16</a>]</cite> datasets, showing generalizability across in-studio and outdoor environments.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="Pt0.A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.C </span>Additional Visual Examples</h2>
<div class="ltx_para" id="Pt0.A3.p1">
<p class="ltx_p" id="Pt0.A3.p1.1">We provide a supplementary video that describes our method with visual demonstrations. Additionally, we provide several video clips of input and output data from Human3.6m <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib18" title="">18</a>]</cite>, RICH <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib16" title="">16</a>]</cite>, and CMU-Panoptic <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib22" title="">22</a>]</cite> datasets and compare the visual fidelity of our approach with the state-of-the-art method on Human3.6m, AdaFuse <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib66" title="">66</a>]</cite>.</p>
</div>
</section>
<section class="ltx_appendix" id="Pt0.A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.D </span>Qualitataive Comparisons</h2>
<div class="ltx_para" id="Pt0.A4.p1">
<p class="ltx_p" id="Pt0.A4.p1.1">In <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#Pt0.A4.F7" title="In Appendix 0.D Qualitataive Comparisons ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">7</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#Pt0.A4.F8" title="In Appendix 0.D Qualitataive Comparisons ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">8</span></a>, and <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#Pt0.A4.F9" title="In Appendix 0.D Qualitataive Comparisons ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">9</span></a>, we demonstrate some examples of our UPose3D on Human3.6m <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib18" title="">18</a>]</cite> dataset to showcase its visual fidelity in comparison to ground-truth keypoints and AdaFuse <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib66" title="">66</a>]</cite> in InD evaluation scheme. Additionally, we provide more visual examples of UPose3D results in <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#Pt0.A4.F10" title="In Appendix 0.D Qualitataive Comparisons ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">10</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#Pt0.A4.F11" title="In Appendix 0.D Qualitataive Comparisons ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">11</span></a>, and <a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#Pt0.A4.F12" title="In Appendix 0.D Qualitataive Comparisons ‣ UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">12</span></a> in comparison to our implementation of AdaFuse <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib66" title="">66</a>]</cite> in OoD settings on the RICH <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib16" title="">16</a>]</cite> dataset. To better visualize the sharp keypoint distribution output of our 2D pose estimators, we show the logarithm of heatmaps in all figures for 2D pose estimators. We refer the reader to Fig. 4 of the main paper for an illustration of the real heatmaps without any post-processing. Our method performs consistently in both settings, while AdaFuse fails to correctly predict the human keypoints in some OoD samples. In all cases, the 2D pose estimator generally results in more refined predictions and sharper uncertainty distributions, while our pose compiler outputs a coarser distribution. Moreover, our method typically depicts higher horizontal uncertainties, which may be due to more frequent horizontal movements.</p>
</div>
<figure class="ltx_figure" id="Pt0.A4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1174" id="Pt0.A4.F7.g1" src="x7.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Pt0.A4.F7.2.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text" id="Pt0.A4.F7.3.2" style="font-size:90%;">
Example output of our proposed UPose3D pipeline in comparison to AdaFuse <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib66" title="">66</a>]</cite> is presented in the InD evaluation scheme on the Human3.6m <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib18" title="">18</a>]</cite> dataset.
</span></figcaption>
</figure>
<figure class="ltx_figure" id="Pt0.A4.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1174" id="Pt0.A4.F8.g1" src="x8.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Pt0.A4.F8.2.1.1" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text" id="Pt0.A4.F8.3.2" style="font-size:90%;">
Example output of our proposed UPose3D pipeline in comparison to AdaFuse <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib66" title="">66</a>]</cite> is presented in the InD evaluation scheme on the Human3.6m <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib18" title="">18</a>]</cite> dataset.
</span></figcaption>
</figure>
<figure class="ltx_figure" id="Pt0.A4.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1174" id="Pt0.A4.F9.g1" src="x9.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Pt0.A4.F9.2.1.1" style="font-size:90%;">Figure 9</span>: </span><span class="ltx_text" id="Pt0.A4.F9.3.2" style="font-size:90%;">
Example output of our proposed UPose3D pipeline in comparison to AdaFuse <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib66" title="">66</a>]</cite> is presented in the InD evaluation scheme on the Human3.6m <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib18" title="">18</a>]</cite> dataset.
</span></figcaption>
</figure>
<figure class="ltx_figure" id="Pt0.A4.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1174" id="Pt0.A4.F10.g1" src="x10.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Pt0.A4.F10.2.1.1" style="font-size:90%;">Figure 10</span>: </span><span class="ltx_text" id="Pt0.A4.F10.3.2" style="font-size:90%;">
Example output of our proposed UPose3D pipeline in comparison to AdaFuse <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib66" title="">66</a>]</cite> is presented in the OoD evaluation scheme on the RICH <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib16" title="">16</a>]</cite> dataset. The first and second samples show the effectiveness of our approach in solving occlusions for detecting hands and feet.
</span></figcaption>
</figure>
<figure class="ltx_figure" id="Pt0.A4.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1174" id="Pt0.A4.F11.g1" src="x11.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Pt0.A4.F11.2.1.1" style="font-size:90%;">Figure 11</span>: </span><span class="ltx_text" id="Pt0.A4.F11.3.2" style="font-size:90%;">
Example output of our proposed UPose3D pipeline in comparison to AdaFuse <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib66" title="">66</a>]</cite> is presented in the OoD evaluation scheme on the RICH <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib16" title="">16</a>]</cite> dataset. The first sample illustrates a challenging input with a rare posture, where both AdaFuse and our method successfully predict the correct posture.
</span></figcaption>
</figure>
<figure class="ltx_figure" id="Pt0.A4.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1174" id="Pt0.A4.F12.g1" src="x12.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Pt0.A4.F12.2.1.1" style="font-size:90%;">Figure 12</span>: </span><span class="ltx_text" id="Pt0.A4.F12.3.2" style="font-size:90%;">
Example output of our proposed UPose3D pipeline in comparison to AdaFuse <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib66" title="">66</a>]</cite> is presented in the OoD evaluation scheme on the RICH <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14634v3#bib.bib16" title="">16</a>]</cite> dataset. We observe that our method outperforms AdaFuse in the first and third samples.
</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Jul 10 00:59:36 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
