<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Active Human Pose Estimation via an Autonomous UAV Agent</title>
<!--Generated on Mon Jul  1 21:17:03 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2407.01811v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S1" title="In Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S2" title="In Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Works</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S2.SS0.SSS1" title="In II Related Works ‣ Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-</span>1 </span>Autonomous Aerial Human Inspection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S2.SS0.SSS2" title="In II Related Works ‣ Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-</span>2 </span>2D Human Pose Estimation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S2.SS0.SSS3" title="In II Related Works ‣ Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-</span>3 </span>Neural Radiance Field</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S3" title="In Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Drone-View Data Acquisition</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S4" title="In Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Mapping 2D Observations to 3D perception guidance fields</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S5" title="In Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Perception-aware motion planning</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S6" title="In Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Experiments</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S6.SS1" title="In VI Experiments ‣ Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-A</span> </span><span class="ltx_text ltx_font_italic">Implementation Details</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S6.SS2" title="In VI Experiments ‣ Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-B</span> </span><span class="ltx_text ltx_font_italic">Evaluation of 3D Perception Guidance Field Generation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S6.SS3" title="In VI Experiments ‣ Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-C</span> </span><span class="ltx_text ltx_font_italic">Perception-aware Motion Planning Experiment</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S6.SS4" title="In VI Experiments ‣ Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-D</span> </span><span class="ltx_text ltx_font_italic">System-level Comparison</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S7" title="In Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\backgroundsetup</span>
<p class="ltx_p" id="p1.2">placement=top,
position=8cm,2cm


</p>
</div>
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">Active Human Pose Estimation <span class="ltx_text" id="id1.id1" style="color:#000000;">via an Autonomous UAV Agent</span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jingxi Chen, Botao He, Chahat Deep Singh, Cornelia Fermüller, Yiannis Aloimonos 
<br class="ltx_break"/>Perception and Robotics Group, University of Maryland - College Park
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">One of the core activities of an active observer involves moving to secure a ”better” view of the scene, where the definition of ”better” is task-dependent. This paper focuses on the task of human pose estimation from videos capturing a person’s activity. Self-occlusions within the scene can complicate or even prevent accurate human pose estimation. To address this, relocating the camera to a new vantage point is necessary to clarify the view, thereby improving 2D human pose estimation. This paper formalizes the process of achieving an improved viewpoint. Our proposed solution to this challenge comprises three main components: a NeRF-based Drone-View Data Generation Framework, an On-Drone Network for Camera View Error Estimation, and a Combined Planner for devising a feasible motion plan to reposition the camera based on the predicted errors for camera views. The Data Generation Framework utilizes NeRF-based methods to generate a comprehensive dataset of human poses and activities, enhancing the drone’s adaptability in various scenarios. The Camera View Error Estimation Network is designed to evaluate the current human pose and identify the most promising next viewing angles for the drone, ensuring a reliable and precise pose estimation from those angles. Finally, the combined planner incorporates these angles while considering the drone’s physical and environmental limitations, employing efficient algorithms to navigate safe and effective flight paths. This system represents a significant advancement in active 2D human pose estimation for an autonomous UAV agent, offering substantial potential for applications in aerial cinematography by improving the performance of autonomous human pose estimation and maintaining the operational safety and efficiency of UAVs.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Recent advances in aerial robotic technologies have significantly improved the use and abilities of aerial robots in many industries. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib3" title="">3</a>]</cite>. A key aspect of modern aerial robots is their ability to be equipped with video cameras, transforming them into dynamic platforms for aerial videography <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib4" title="">4</a>]</cite>. The mobility and agility of aerial robots make them highly effective for aerial cinematography, allowing for versatile footage capture from optimal angles with minimal equipment. The growing demand in entertainment, industrial, and military sectors has shifted aerial cinematography’s focus from static objects to dynamic human subjects, and the technical challenge is how to autonomously adjust a drone’s viewing direction to best view human subjects during navigation.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The challenges associated with autonomously adjusting the viewing direction for UAV-based human pose estimation include determining the criteria for modifying the UAV’s viewing direction during human inspection, and navigating the UAV in a way that balances perceptual guidance with navigation objectives, such as feasible motion plans and collision avoidance.</p>
</div>
<figure class="ltx_figure" id="S1.F0"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="370" id="S1.F0.g1" src="x1.png" width="706"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 0: </span>
Our proposed approach features an integrated system with three key components: 1) Drone-View Data Synthesis, which generates realistic drone perspectives of human subjects from various camera angles and human poses, alongside calculating the associated human pose estimation error for these views to serve as training data pairs. 2) PoseErrNet, a network trained on the generated drone-view data pairs, is capable of predicting a 3D perception guidance field for the selection of candidate viewing angles. 3) A comprehensive planner that integrates traditional navigation cost maps with the 3D perception guidance field derived from PoseErrNet. This integration enables effective motion planning, collision avoidance, and the execution of the next-best viewing angle selection for accurate human pose estimation. </figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To address the challenges posed by dynamic videography using UAVs, we propose a sophisticated, integrated autonomous UAV videography system, as illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S1.F0" title="Figure 0 ‣ I Introduction ‣ Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_tag"></span> ‣ <span class="ltx_text ltx_ref_tag">I</span></a>. This system is engineered to intelligently interpret human poses and proactively reposition itself to capture optimal visual content. The architecture of this system can be divided into three primary components: 1) Drone-View Human Subject Data Generation Framework. This framework is designed to capture a wide range of human poses and actions under varying environmental perspectives. By utilizing advanced vision techniques (HumanNerf) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib5" title="">5</a>]</cite>, this framework will enable the UAV to have a profound understanding of human subjects in different scenarios, enhancing its ability to adapt to real-world videography tasks. 2) Robust and Efficient On-Drone Network for Viewing Angle Estimation. This network is tailored to analyze the current human pose and compute the best subset of the next viewing angles. It aims to process complex visual inputs in real-time, ensuring the UAV can react promptly and accurately to dynamic subjects. The efficiency of this network is crucial, as it directly impacts the UAV’s ability to operate under computational and power constraints typically associated with autonomous drones. 3) Combined Planner for Feasible Motion Plan. Our proposed system is a sophisticated planning module that combines the network’s viewing angle recommendations with the UAV’s dynamic and environmental constraints. The planner employs advanced algorithms to chart a feasible motion plan that not only adheres to the suggested viewing angles but also respects the physical limitations of the UAV and the navigational challenges posed by the environment. Using this planner, the UAV can maneuver in complex environments with agility and precision, ensuring high-quality videography while maintaining safety and operational efficiency.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In Sec. <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S3" title="III Drone-View Data Acquisition ‣ Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_tag">III</span></a>, we will delve into the specifics of generating human subject data from drone views. Following that, in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S4" title="IV Mapping 2D Observations to 3D perception guidance fields ‣ Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_tag">IV</span></a>, we explain our approach using PoseErrNet to transform an imperfect detection of 2D human keypoints, into an error vector for 2D human pose estimation (HPE) across all predefined hemispherical camera viewing angles. This process creates what we refer to as the 3D perception guidance field. Our goal was to design a lightweight network capable of learning the correlation between the optimal subset of next viewing angles and the current human pose estimation, utilizing a dataset we generated for this purpose. The robust estimation of the 3D perception guidance field is crucial as it provides candidate camera viewing angles for the ensuing motion planning phase.
For the motion planning part, based on <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib6" title="">6</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib7" title="">7</a>]</cite>, we crafted a perception-aware motion planning framework. This framework not only incorporates the 3D perception guidance field but also is capable of generating a smooth flight trajectory, avoiding occlusions between the target and the UAV, and ensuring the safety of the flight.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Our contributions can be summarized as:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">A drone-view data generation framework for different human poses.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">A robust and efficient network running on the drone for estimating the best subset of the next viewing angles based on the current human pose estimation.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">A combined planner that combines the perspective-aware guidance from the network and traditional navigation constraints into a feasible motion plan for improving 2D HPE, a computer vision task.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">These three interconnected components are seamlessly integrated into a system designed for application in real-world scenarios.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related Works</span>
</h2>
<section class="ltx_subsubsection" id="S2.SS0.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS0.SSS1.5.1.1">II-</span>1 </span>Autonomous Aerial Human Inspection</h4>
<div class="ltx_para" id="S2.SS0.SSS1.p1">
<p class="ltx_p" id="S2.SS0.SSS1.p1.1">Existing research on autonomous aerial inspection of human subjects primarily aims at achieving planning autonomy but often lacks objective guidance on subsequent movements. As a result, high-level guidance for the inspection tasks is typically expected to come from human operators, as seen in various studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib14" title="">14</a>]</cite>. While the autonomous planning for UAVs to follow and robustly track mobile objects along optimized trajectories is well-documented <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib18" title="">18</a>]</cite>, these works generally do not address the capability of UAVs to autonomously execute perception-aware objectives, such as human pose estimation, without human operator inputs.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS0.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS0.SSS2.5.1.1">II-</span>2 </span>2D Human Pose Estimation</h4>
<div class="ltx_para" id="S2.SS0.SSS2.p1">
<p class="ltx_p" id="S2.SS0.SSS2.p1.1">Advancements in deep learning techniques have significantly enhanced the performance of 2D Human Pose Estimation (HPE), leading to robust and efficient solutions for both single and multiple individuals. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib24" title="">24</a>]</cite>. However, the near-perfect performance of 2D Human Pose Estimation (HPE) often relies on ideal input images of humans without any occlusion of body parts. This assumption becomes challenging in the context of autonomous UAV inspections of humans. As the UAV moves, the camera’s view of human subjects can easily be obstructed by environmental elements or self-occlusion of human body parts.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S2.T1.1" style="width:390.3pt;height:91.8pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-36.4pt,8.5pt) scale(0.842680455258383,0.842680455258383) ;">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.1.1.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.1.1.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S2.T1.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.1.2.1">Cost</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S2.T1.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.1.3.1">Viewing Angle</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S2.T1.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.1.4.1">Realistic Capture</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.2.2">
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.2.2.1">Ecnomoic</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.2.2.2">Engineering</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.2.2.3">Accuracy</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.2.2.4">Resolution</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.2.2.5">Appearance</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.2.2.6">Human Pose</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.3.3.1">Drone Capture</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.3.3.2">Medium</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.3.3.3">High</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.3.3.4">Low</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.3.3.5">Low</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.3.3.6">✓</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.3.3.7">✓</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.4.4">
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.4.4.1">Camera Array</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.4.4.2">High</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.4.4.3">High</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.4.4.4">High</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.4.4.5">High</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.4.4.6">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.4.4.7">✓</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.5.5">
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.5.5.1">Simulation Software</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.5.5.2">Low</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.5.5.3">High</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.5.5.4">High</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.5.5.5">High</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.5.5.6">–</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.5.5.7">–</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.6.6">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="S2.T1.1.1.6.6.1">Ours</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="S2.T1.1.1.6.6.2">Low</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="S2.T1.1.1.6.6.3">Low</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="S2.T1.1.1.6.6.4">High</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="S2.T1.1.1.6.6.5">High</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="S2.T1.1.1.6.6.6">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="S2.T1.1.1.6.6.7">✓</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Comparison of different drone-view data acquisition methods. Cost, related to economic and engineering cost of the capture method (<span class="ltx_text ltx_font_italic" id="S2.T1.5.1">Density</span>). Viewing Angle, consists of viewing angle accuracy and resolution of the capture method (<span class="ltx_text ltx_font_italic" id="S2.T1.6.2">Viewing Angle</span>). Realistic Capture, is related to whether the capture method can capture realistic appearance and human pose (<span class="ltx_text ltx_font_italic" id="S2.T1.7.3">Realistic Caprure).</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S2.SS0.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS0.SSS3.5.1.1">II-</span>3 </span>Neural Radiance Field</h4>
<div class="ltx_para" id="S2.SS0.SSS3.p1">
<p class="ltx_p" id="S2.SS0.SSS3.p1.1">NeRF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib25" title="">25</a>]</cite> and its extensions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib32" title="">32</a>]</cite> enable high-quality and continuous rendering of static 3D scenes. A natural progression is to expand the neural radiance field approach to encompass dynamic scene representation.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib38" title="">38</a>]</cite>. In the context of dynamic scene representations, our work is most closely related to the neural representation of dynamic human subjects <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib40" title="">40</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="589" id="S2.F1.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
The process for generating NeRF-based drone-view images of human subjects and 3D perception guidance field data involves using 2D annotations to conduct batch triangulation, resulting in a 3D skeleton for a given human pose. We then render the synthesized image for ”drone views”, reproject the ground truth 3D skeleton onto NeRF poses to obtain ground truth 2D keypoints, and employ an arbitrary HPE network to predict these keypoints for computing the per camera view HPE error. Through this method, we successfully acquire paired data comprising 2D observations and the corresponding 3D perception guidance field. </figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Drone-View Data Acquisition</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">As illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S1.F0" title="Figure 0 ‣ I Introduction ‣ Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_tag"></span> ‣ <span class="ltx_text ltx_ref_tag">I</span></a>, the drone views of a human subject can be represented in a hemispherical space. This type of data can be acquired through one of three methods: 1) Drone Capture, which involves using UAVs to obtain images of human subjects in specific poses from multiple angles. 2) Camera Array, which entails setting up an array of cameras to cover the hemispherical space, with a focus on achieving time synchronization among the cameras. 3) Utilization of simulation software like Blender <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib41" title="">41</a>]</cite> or Unity <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib42" title="">42</a>]</cite> to create projected image views from human models. Each method comes with its own set of challenges and practical considerations. The pros and cons of these methods for generating drone-view data are detailed in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S2.T1" title="TABLE I ‣ II-2 2D Human Pose Estimation ‣ II Related Works ‣ Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_tag">I</span></a>, highlighting economic and engineering costs. For instance, deploying multiple drones for data capture or creating a camera array incurs significant economic costs due to the hardware required and demands considerable engineering effort for calibration and synchronization. Conversely, simulation software offers a low economic cost option, though achieving a high-quality simulation presents substantial engineering challenges. The table also compares the accuracy of the desired viewing angles, the resolution, or the detail level at which capture angles are set, and the realism of capturing human subjects and poses. Drone capture and camera arrays provide realism in both appearance and pose since they employ real-world methods. In contrast, achieving realistic captures of both appearance and human poses proves difficult with simulation software.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">Our research leverages the innovative free-viewpoint rendering method, HumanNeRF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib5" title="">5</a>]</cite>, which is designed for rendering images with complex human poses, perfectly meeting our data acquisition needs. This technique allows for the free-view synthesis of a human image in a specific pose. We configure the camera poses and viewing angles to match the desired hemispherical drone camera pose for the captured image and then render drone-view images for various human poses.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">After rendering the drone-view images, we follow the approach depicted in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S2.F1" title="Figure 1 ‣ II-3 Neural Radiance Field ‣ II Related Works ‣ Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_tag">1</span></a> to compute the human pose estimation error for each camera view. This process enables us to gather the desired training data pairs, linking each 2D human skeleton estimation to a 3D perception guidance field.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="169" id="S3.F2.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>PoseErrNet: It consists of two major parts: 1) Input abstraction and normalization to deal with the sim-to-real gap and with scale, translation, and rotation invariance for the drone applications, and 2) The auto-encoder network to map from normalized 2D observations to 3D perception guidance fields.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Mapping 2D Observations to 3D perception guidance fields</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">After acquiring our training data pairs for drone-view human images and 3D perception guidance field. A simplified auto-encoder network is employed for visual guidance, as depicted in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S3.F2" title="Figure 2 ‣ III Drone-View Data Acquisition ‣ Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_tag">2</span></a>. This network architecture is characterized by a minimal number of weights, enhancing its efficiency. For dealing with the sim-to-real gap, we proposed a process to normalize the input drone-view data, this normalization process includes first converting the input image into the HPE resulting keypoints and then normalizing the detected keypoints to account for translation, rotation, and scale variance in the input keypoint due to different human, drone-to-human distance or instability during the flight of the drone.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">The proposed normalization process for input HPE keypoints is straightforward yet effective. We begin by identifying the human spine, typically the line between the neck and the midpoint of the hips. Once the spine is determined, we translate all keypoints to align the spine’s midpoint to a consistent coordinate, addressing the translation variance of the input keypoints. Furthermore, we rotate all keypoints to orient the spine’s direction upwards and to the right, countering the rotation variance of the input keypoints. Finally, we scale all keypoints to ensure the spine length remains constant. An example of normalized human keypoints is illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S3.F2" title="Figure 2 ‣ III Drone-View Data Acquisition ‣ Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">Our normalization process for the HPE keypoints enhances robustness against variances in translation, rotation, and scale in the inputs. By using the normalized coordinates of the keypoints, represented as a vector, as input to our network, we simplify the design of the network architecture. The proposed PoseErrNet is an autoencoder with a minimal number of layers, benefiting greatly from the simplicity and normalization of its inputs.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Perception-aware motion planning</span>
</h2>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="477" id="S5.F3.g1" src="x4.png" width="829"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Visualization for the calculation of P-ESDF.</figcaption>
</figure>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.2">We introduce our proposed perception-aware motion planning framework to bridge the gap between perception and motion planning. In the proposed framework, the perception loss is added to the motion planning cost function as one of the costs. To achieve that, we built a differentiable distance field called Pose-enhanced Euclidean Distance Field (P-ESDF), noted as <math alttext="\mathcal{P}" class="ltx_Math" display="inline" id="S5.p1.1.m1.1"><semantics id="S5.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml">𝒫</mi><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><ci id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1">𝒫</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">\mathcal{P}</annotation><annotation encoding="application/x-llamapun" id="S5.p1.1.m1.1d">caligraphic_P</annotation></semantics></math>, each of its element is then represented as <math alttext="\{p_{i}\in\mathcal{P}|i\in\mathbb{Z}^{+}\}" class="ltx_Math" display="inline" id="S5.p1.2.m2.2"><semantics id="S5.p1.2.m2.2a"><mrow id="S5.p1.2.m2.2.2.2" xref="S5.p1.2.m2.2.2.3.cmml"><mo id="S5.p1.2.m2.2.2.2.3" stretchy="false" xref="S5.p1.2.m2.2.2.3.1.cmml">{</mo><mrow id="S5.p1.2.m2.1.1.1.1" xref="S5.p1.2.m2.1.1.1.1.cmml"><msub id="S5.p1.2.m2.1.1.1.1.2" xref="S5.p1.2.m2.1.1.1.1.2.cmml"><mi id="S5.p1.2.m2.1.1.1.1.2.2" xref="S5.p1.2.m2.1.1.1.1.2.2.cmml">p</mi><mi id="S5.p1.2.m2.1.1.1.1.2.3" xref="S5.p1.2.m2.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S5.p1.2.m2.1.1.1.1.1" xref="S5.p1.2.m2.1.1.1.1.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S5.p1.2.m2.1.1.1.1.3" xref="S5.p1.2.m2.1.1.1.1.3.cmml">𝒫</mi></mrow><mo id="S5.p1.2.m2.2.2.2.4" lspace="0em" rspace="0em" xref="S5.p1.2.m2.2.2.3.1.cmml">|</mo><mrow id="S5.p1.2.m2.2.2.2.2" xref="S5.p1.2.m2.2.2.2.2.cmml"><mi id="S5.p1.2.m2.2.2.2.2.2" xref="S5.p1.2.m2.2.2.2.2.2.cmml">i</mi><mo id="S5.p1.2.m2.2.2.2.2.1" xref="S5.p1.2.m2.2.2.2.2.1.cmml">∈</mo><msup id="S5.p1.2.m2.2.2.2.2.3" xref="S5.p1.2.m2.2.2.2.2.3.cmml"><mi id="S5.p1.2.m2.2.2.2.2.3.2" xref="S5.p1.2.m2.2.2.2.2.3.2.cmml">ℤ</mi><mo id="S5.p1.2.m2.2.2.2.2.3.3" xref="S5.p1.2.m2.2.2.2.2.3.3.cmml">+</mo></msup></mrow><mo id="S5.p1.2.m2.2.2.2.5" stretchy="false" xref="S5.p1.2.m2.2.2.3.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.2.m2.2b"><apply id="S5.p1.2.m2.2.2.3.cmml" xref="S5.p1.2.m2.2.2.2"><csymbol cd="latexml" id="S5.p1.2.m2.2.2.3.1.cmml" xref="S5.p1.2.m2.2.2.2.3">conditional-set</csymbol><apply id="S5.p1.2.m2.1.1.1.1.cmml" xref="S5.p1.2.m2.1.1.1.1"><in id="S5.p1.2.m2.1.1.1.1.1.cmml" xref="S5.p1.2.m2.1.1.1.1.1"></in><apply id="S5.p1.2.m2.1.1.1.1.2.cmml" xref="S5.p1.2.m2.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.p1.2.m2.1.1.1.1.2.1.cmml" xref="S5.p1.2.m2.1.1.1.1.2">subscript</csymbol><ci id="S5.p1.2.m2.1.1.1.1.2.2.cmml" xref="S5.p1.2.m2.1.1.1.1.2.2">𝑝</ci><ci id="S5.p1.2.m2.1.1.1.1.2.3.cmml" xref="S5.p1.2.m2.1.1.1.1.2.3">𝑖</ci></apply><ci id="S5.p1.2.m2.1.1.1.1.3.cmml" xref="S5.p1.2.m2.1.1.1.1.3">𝒫</ci></apply><apply id="S5.p1.2.m2.2.2.2.2.cmml" xref="S5.p1.2.m2.2.2.2.2"><in id="S5.p1.2.m2.2.2.2.2.1.cmml" xref="S5.p1.2.m2.2.2.2.2.1"></in><ci id="S5.p1.2.m2.2.2.2.2.2.cmml" xref="S5.p1.2.m2.2.2.2.2.2">𝑖</ci><apply id="S5.p1.2.m2.2.2.2.2.3.cmml" xref="S5.p1.2.m2.2.2.2.2.3"><csymbol cd="ambiguous" id="S5.p1.2.m2.2.2.2.2.3.1.cmml" xref="S5.p1.2.m2.2.2.2.2.3">superscript</csymbol><ci id="S5.p1.2.m2.2.2.2.2.3.2.cmml" xref="S5.p1.2.m2.2.2.2.2.3.2">ℤ</ci><plus id="S5.p1.2.m2.2.2.2.2.3.3.cmml" xref="S5.p1.2.m2.2.2.2.2.3.3"></plus></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.m2.2c">\{p_{i}\in\mathcal{P}|i\in\mathbb{Z}^{+}\}</annotation><annotation encoding="application/x-llamapun" id="S5.p1.2.m2.2d">{ italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ caligraphic_P | italic_i ∈ blackboard_Z start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT }</annotation></semantics></math>. The construction of the field is described below.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.8">The output of the Pose Estimation Error Predictor is a 2-D map <math alttext="\mathcal{E}:\mathbb{R}^{m\times n}\rightarrow\mathbb{R}" class="ltx_Math" display="inline" id="S5.p2.1.m1.1"><semantics id="S5.p2.1.m1.1a"><mrow id="S5.p2.1.m1.1.1" xref="S5.p2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.p2.1.m1.1.1.2" xref="S5.p2.1.m1.1.1.2.cmml">ℰ</mi><mo id="S5.p2.1.m1.1.1.1" lspace="0.278em" rspace="0.278em" xref="S5.p2.1.m1.1.1.1.cmml">:</mo><mrow id="S5.p2.1.m1.1.1.3" xref="S5.p2.1.m1.1.1.3.cmml"><msup id="S5.p2.1.m1.1.1.3.2" xref="S5.p2.1.m1.1.1.3.2.cmml"><mi id="S5.p2.1.m1.1.1.3.2.2" xref="S5.p2.1.m1.1.1.3.2.2.cmml">ℝ</mi><mrow id="S5.p2.1.m1.1.1.3.2.3" xref="S5.p2.1.m1.1.1.3.2.3.cmml"><mi id="S5.p2.1.m1.1.1.3.2.3.2" xref="S5.p2.1.m1.1.1.3.2.3.2.cmml">m</mi><mo id="S5.p2.1.m1.1.1.3.2.3.1" lspace="0.222em" rspace="0.222em" xref="S5.p2.1.m1.1.1.3.2.3.1.cmml">×</mo><mi id="S5.p2.1.m1.1.1.3.2.3.3" xref="S5.p2.1.m1.1.1.3.2.3.3.cmml">n</mi></mrow></msup><mo id="S5.p2.1.m1.1.1.3.1" stretchy="false" xref="S5.p2.1.m1.1.1.3.1.cmml">→</mo><mi id="S5.p2.1.m1.1.1.3.3" xref="S5.p2.1.m1.1.1.3.3.cmml">ℝ</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.1.m1.1b"><apply id="S5.p2.1.m1.1.1.cmml" xref="S5.p2.1.m1.1.1"><ci id="S5.p2.1.m1.1.1.1.cmml" xref="S5.p2.1.m1.1.1.1">:</ci><ci id="S5.p2.1.m1.1.1.2.cmml" xref="S5.p2.1.m1.1.1.2">ℰ</ci><apply id="S5.p2.1.m1.1.1.3.cmml" xref="S5.p2.1.m1.1.1.3"><ci id="S5.p2.1.m1.1.1.3.1.cmml" xref="S5.p2.1.m1.1.1.3.1">→</ci><apply id="S5.p2.1.m1.1.1.3.2.cmml" xref="S5.p2.1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S5.p2.1.m1.1.1.3.2.1.cmml" xref="S5.p2.1.m1.1.1.3.2">superscript</csymbol><ci id="S5.p2.1.m1.1.1.3.2.2.cmml" xref="S5.p2.1.m1.1.1.3.2.2">ℝ</ci><apply id="S5.p2.1.m1.1.1.3.2.3.cmml" xref="S5.p2.1.m1.1.1.3.2.3"><times id="S5.p2.1.m1.1.1.3.2.3.1.cmml" xref="S5.p2.1.m1.1.1.3.2.3.1"></times><ci id="S5.p2.1.m1.1.1.3.2.3.2.cmml" xref="S5.p2.1.m1.1.1.3.2.3.2">𝑚</ci><ci id="S5.p2.1.m1.1.1.3.2.3.3.cmml" xref="S5.p2.1.m1.1.1.3.2.3.3">𝑛</ci></apply></apply><ci id="S5.p2.1.m1.1.1.3.3.cmml" xref="S5.p2.1.m1.1.1.3.3">ℝ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.1.m1.1c">\mathcal{E}:\mathbb{R}^{m\times n}\rightarrow\mathbb{R}</annotation><annotation encoding="application/x-llamapun" id="S5.p2.1.m1.1d">caligraphic_E : blackboard_R start_POSTSUPERSCRIPT italic_m × italic_n end_POSTSUPERSCRIPT → blackboard_R</annotation></semantics></math>.
The first step is to transform <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S5.p2.2.m2.1"><semantics id="S5.p2.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S5.p2.2.m2.1.1" xref="S5.p2.2.m2.1.1.cmml">ℰ</mi><annotation-xml encoding="MathML-Content" id="S5.p2.2.m2.1b"><ci id="S5.p2.2.m2.1.1.cmml" xref="S5.p2.2.m2.1.1">ℰ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.2.m2.1c">\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="S5.p2.2.m2.1d">caligraphic_E</annotation></semantics></math> to the subject frame in <math alttext="\mathbb{R}^{3}" class="ltx_Math" display="inline" id="S5.p2.3.m3.1"><semantics id="S5.p2.3.m3.1a"><msup id="S5.p2.3.m3.1.1" xref="S5.p2.3.m3.1.1.cmml"><mi id="S5.p2.3.m3.1.1.2" xref="S5.p2.3.m3.1.1.2.cmml">ℝ</mi><mn id="S5.p2.3.m3.1.1.3" xref="S5.p2.3.m3.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S5.p2.3.m3.1b"><apply id="S5.p2.3.m3.1.1.cmml" xref="S5.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S5.p2.3.m3.1.1.1.cmml" xref="S5.p2.3.m3.1.1">superscript</csymbol><ci id="S5.p2.3.m3.1.1.2.cmml" xref="S5.p2.3.m3.1.1.2">ℝ</ci><cn id="S5.p2.3.m3.1.1.3.cmml" type="integer" xref="S5.p2.3.m3.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.3.m3.1c">\mathbb{R}^{3}</annotation><annotation encoding="application/x-llamapun" id="S5.p2.3.m3.1d">blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math> with the subject in the center, as shown in the bottom sub-figure of Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S5.F3" title="Figure 3 ‣ V Perception-aware motion planning ‣ Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_tag">3</span></a>(b). Then, we project the transformed map <math alttext="\mathcal{E}^{sub}" class="ltx_Math" display="inline" id="S5.p2.4.m4.1"><semantics id="S5.p2.4.m4.1a"><msup id="S5.p2.4.m4.1.1" xref="S5.p2.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.p2.4.m4.1.1.2" xref="S5.p2.4.m4.1.1.2.cmml">ℰ</mi><mrow id="S5.p2.4.m4.1.1.3" xref="S5.p2.4.m4.1.1.3.cmml"><mi id="S5.p2.4.m4.1.1.3.2" xref="S5.p2.4.m4.1.1.3.2.cmml">s</mi><mo id="S5.p2.4.m4.1.1.3.1" xref="S5.p2.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S5.p2.4.m4.1.1.3.3" xref="S5.p2.4.m4.1.1.3.3.cmml">u</mi><mo id="S5.p2.4.m4.1.1.3.1a" xref="S5.p2.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S5.p2.4.m4.1.1.3.4" xref="S5.p2.4.m4.1.1.3.4.cmml">b</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.p2.4.m4.1b"><apply id="S5.p2.4.m4.1.1.cmml" xref="S5.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S5.p2.4.m4.1.1.1.cmml" xref="S5.p2.4.m4.1.1">superscript</csymbol><ci id="S5.p2.4.m4.1.1.2.cmml" xref="S5.p2.4.m4.1.1.2">ℰ</ci><apply id="S5.p2.4.m4.1.1.3.cmml" xref="S5.p2.4.m4.1.1.3"><times id="S5.p2.4.m4.1.1.3.1.cmml" xref="S5.p2.4.m4.1.1.3.1"></times><ci id="S5.p2.4.m4.1.1.3.2.cmml" xref="S5.p2.4.m4.1.1.3.2">𝑠</ci><ci id="S5.p2.4.m4.1.1.3.3.cmml" xref="S5.p2.4.m4.1.1.3.3">𝑢</ci><ci id="S5.p2.4.m4.1.1.3.4.cmml" xref="S5.p2.4.m4.1.1.3.4">𝑏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.4.m4.1c">\mathcal{E}^{sub}</annotation><annotation encoding="application/x-llamapun" id="S5.p2.4.m4.1d">caligraphic_E start_POSTSUPERSCRIPT italic_s italic_u italic_b end_POSTSUPERSCRIPT</annotation></semantics></math> from the subject frame to the drone frame to get the <math alttext="\mathcal{E}^{drone}" class="ltx_Math" display="inline" id="S5.p2.5.m5.1"><semantics id="S5.p2.5.m5.1a"><msup id="S5.p2.5.m5.1.1" xref="S5.p2.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.p2.5.m5.1.1.2" xref="S5.p2.5.m5.1.1.2.cmml">ℰ</mi><mrow id="S5.p2.5.m5.1.1.3" xref="S5.p2.5.m5.1.1.3.cmml"><mi id="S5.p2.5.m5.1.1.3.2" xref="S5.p2.5.m5.1.1.3.2.cmml">d</mi><mo id="S5.p2.5.m5.1.1.3.1" xref="S5.p2.5.m5.1.1.3.1.cmml">⁢</mo><mi id="S5.p2.5.m5.1.1.3.3" xref="S5.p2.5.m5.1.1.3.3.cmml">r</mi><mo id="S5.p2.5.m5.1.1.3.1a" xref="S5.p2.5.m5.1.1.3.1.cmml">⁢</mo><mi id="S5.p2.5.m5.1.1.3.4" xref="S5.p2.5.m5.1.1.3.4.cmml">o</mi><mo id="S5.p2.5.m5.1.1.3.1b" xref="S5.p2.5.m5.1.1.3.1.cmml">⁢</mo><mi id="S5.p2.5.m5.1.1.3.5" xref="S5.p2.5.m5.1.1.3.5.cmml">n</mi><mo id="S5.p2.5.m5.1.1.3.1c" xref="S5.p2.5.m5.1.1.3.1.cmml">⁢</mo><mi id="S5.p2.5.m5.1.1.3.6" xref="S5.p2.5.m5.1.1.3.6.cmml">e</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.p2.5.m5.1b"><apply id="S5.p2.5.m5.1.1.cmml" xref="S5.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S5.p2.5.m5.1.1.1.cmml" xref="S5.p2.5.m5.1.1">superscript</csymbol><ci id="S5.p2.5.m5.1.1.2.cmml" xref="S5.p2.5.m5.1.1.2">ℰ</ci><apply id="S5.p2.5.m5.1.1.3.cmml" xref="S5.p2.5.m5.1.1.3"><times id="S5.p2.5.m5.1.1.3.1.cmml" xref="S5.p2.5.m5.1.1.3.1"></times><ci id="S5.p2.5.m5.1.1.3.2.cmml" xref="S5.p2.5.m5.1.1.3.2">𝑑</ci><ci id="S5.p2.5.m5.1.1.3.3.cmml" xref="S5.p2.5.m5.1.1.3.3">𝑟</ci><ci id="S5.p2.5.m5.1.1.3.4.cmml" xref="S5.p2.5.m5.1.1.3.4">𝑜</ci><ci id="S5.p2.5.m5.1.1.3.5.cmml" xref="S5.p2.5.m5.1.1.3.5">𝑛</ci><ci id="S5.p2.5.m5.1.1.3.6.cmml" xref="S5.p2.5.m5.1.1.3.6">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.5.m5.1c">\mathcal{E}^{drone}</annotation><annotation encoding="application/x-llamapun" id="S5.p2.5.m5.1d">caligraphic_E start_POSTSUPERSCRIPT italic_d italic_r italic_o italic_n italic_e end_POSTSUPERSCRIPT</annotation></semantics></math>, as shown in the upper sub-figure of Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S5.F3" title="Figure 3 ‣ V Perception-aware motion planning ‣ Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_tag">3</span></a>(b).
To simutanrously perform obstacle avoidance and viewpoint targeting, the <math alttext="\mathcal{E}^{drone}" class="ltx_Math" display="inline" id="S5.p2.6.m6.1"><semantics id="S5.p2.6.m6.1a"><msup id="S5.p2.6.m6.1.1" xref="S5.p2.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.p2.6.m6.1.1.2" xref="S5.p2.6.m6.1.1.2.cmml">ℰ</mi><mrow id="S5.p2.6.m6.1.1.3" xref="S5.p2.6.m6.1.1.3.cmml"><mi id="S5.p2.6.m6.1.1.3.2" xref="S5.p2.6.m6.1.1.3.2.cmml">d</mi><mo id="S5.p2.6.m6.1.1.3.1" xref="S5.p2.6.m6.1.1.3.1.cmml">⁢</mo><mi id="S5.p2.6.m6.1.1.3.3" xref="S5.p2.6.m6.1.1.3.3.cmml">r</mi><mo id="S5.p2.6.m6.1.1.3.1a" xref="S5.p2.6.m6.1.1.3.1.cmml">⁢</mo><mi id="S5.p2.6.m6.1.1.3.4" xref="S5.p2.6.m6.1.1.3.4.cmml">o</mi><mo id="S5.p2.6.m6.1.1.3.1b" xref="S5.p2.6.m6.1.1.3.1.cmml">⁢</mo><mi id="S5.p2.6.m6.1.1.3.5" xref="S5.p2.6.m6.1.1.3.5.cmml">n</mi><mo id="S5.p2.6.m6.1.1.3.1c" xref="S5.p2.6.m6.1.1.3.1.cmml">⁢</mo><mi id="S5.p2.6.m6.1.1.3.6" xref="S5.p2.6.m6.1.1.3.6.cmml">e</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.p2.6.m6.1b"><apply id="S5.p2.6.m6.1.1.cmml" xref="S5.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S5.p2.6.m6.1.1.1.cmml" xref="S5.p2.6.m6.1.1">superscript</csymbol><ci id="S5.p2.6.m6.1.1.2.cmml" xref="S5.p2.6.m6.1.1.2">ℰ</ci><apply id="S5.p2.6.m6.1.1.3.cmml" xref="S5.p2.6.m6.1.1.3"><times id="S5.p2.6.m6.1.1.3.1.cmml" xref="S5.p2.6.m6.1.1.3.1"></times><ci id="S5.p2.6.m6.1.1.3.2.cmml" xref="S5.p2.6.m6.1.1.3.2">𝑑</ci><ci id="S5.p2.6.m6.1.1.3.3.cmml" xref="S5.p2.6.m6.1.1.3.3">𝑟</ci><ci id="S5.p2.6.m6.1.1.3.4.cmml" xref="S5.p2.6.m6.1.1.3.4">𝑜</ci><ci id="S5.p2.6.m6.1.1.3.5.cmml" xref="S5.p2.6.m6.1.1.3.5">𝑛</ci><ci id="S5.p2.6.m6.1.1.3.6.cmml" xref="S5.p2.6.m6.1.1.3.6">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.6.m6.1c">\mathcal{E}^{drone}</annotation><annotation encoding="application/x-llamapun" id="S5.p2.6.m6.1d">caligraphic_E start_POSTSUPERSCRIPT italic_d italic_r italic_o italic_n italic_e end_POSTSUPERSCRIPT</annotation></semantics></math> need to be merged with the standard ESDF <math alttext="\mathbf{E}" class="ltx_Math" display="inline" id="S5.p2.7.m7.1"><semantics id="S5.p2.7.m7.1a"><mi id="S5.p2.7.m7.1.1" xref="S5.p2.7.m7.1.1.cmml">𝐄</mi><annotation-xml encoding="MathML-Content" id="S5.p2.7.m7.1b"><ci id="S5.p2.7.m7.1.1.cmml" xref="S5.p2.7.m7.1.1">𝐄</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.7.m7.1c">\mathbf{E}</annotation><annotation encoding="application/x-llamapun" id="S5.p2.7.m7.1d">bold_E</annotation></semantics></math> to get the final <math alttext="\mathcal{P}" class="ltx_Math" display="inline" id="S5.p2.8.m8.1"><semantics id="S5.p2.8.m8.1a"><mi class="ltx_font_mathcaligraphic" id="S5.p2.8.m8.1.1" xref="S5.p2.8.m8.1.1.cmml">𝒫</mi><annotation-xml encoding="MathML-Content" id="S5.p2.8.m8.1b"><ci id="S5.p2.8.m8.1.1.cmml" xref="S5.p2.8.m8.1.1">𝒫</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.8.m8.1c">\mathcal{P}</annotation><annotation encoding="application/x-llamapun" id="S5.p2.8.m8.1d">caligraphic_P</annotation></semantics></math>, as illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S5.F3" title="Figure 3 ‣ V Perception-aware motion planning ‣ Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_tag">3</span></a>. The merge process can be expressed as:</p>
</div>
<div class="ltx_para" id="S5.p3">
<table class="ltx_equation ltx_eqn_table" id="S5.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{P}=\sum_{i}\lambda\mathcal{E}^{drone}_{i}+(1-\lambda)\mathbf{E}_{i}." class="ltx_Math" display="block" id="S5.E1.m1.1"><semantics id="S5.E1.m1.1a"><mrow id="S5.E1.m1.1.1.1" xref="S5.E1.m1.1.1.1.1.cmml"><mrow id="S5.E1.m1.1.1.1.1" xref="S5.E1.m1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.E1.m1.1.1.1.1.3" xref="S5.E1.m1.1.1.1.1.3.cmml">𝒫</mi><mo id="S5.E1.m1.1.1.1.1.2" rspace="0.111em" xref="S5.E1.m1.1.1.1.1.2.cmml">=</mo><mrow id="S5.E1.m1.1.1.1.1.1" xref="S5.E1.m1.1.1.1.1.1.cmml"><mrow id="S5.E1.m1.1.1.1.1.1.3" xref="S5.E1.m1.1.1.1.1.1.3.cmml"><munder id="S5.E1.m1.1.1.1.1.1.3.1" xref="S5.E1.m1.1.1.1.1.1.3.1.cmml"><mo id="S5.E1.m1.1.1.1.1.1.3.1.2" movablelimits="false" xref="S5.E1.m1.1.1.1.1.1.3.1.2.cmml">∑</mo><mi id="S5.E1.m1.1.1.1.1.1.3.1.3" xref="S5.E1.m1.1.1.1.1.1.3.1.3.cmml">i</mi></munder><mrow id="S5.E1.m1.1.1.1.1.1.3.2" xref="S5.E1.m1.1.1.1.1.1.3.2.cmml"><mi id="S5.E1.m1.1.1.1.1.1.3.2.2" xref="S5.E1.m1.1.1.1.1.1.3.2.2.cmml">λ</mi><mo id="S5.E1.m1.1.1.1.1.1.3.2.1" xref="S5.E1.m1.1.1.1.1.1.3.2.1.cmml">⁢</mo><msubsup id="S5.E1.m1.1.1.1.1.1.3.2.3" xref="S5.E1.m1.1.1.1.1.1.3.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.E1.m1.1.1.1.1.1.3.2.3.2.2" xref="S5.E1.m1.1.1.1.1.1.3.2.3.2.2.cmml">ℰ</mi><mi id="S5.E1.m1.1.1.1.1.1.3.2.3.3" xref="S5.E1.m1.1.1.1.1.1.3.2.3.3.cmml">i</mi><mrow id="S5.E1.m1.1.1.1.1.1.3.2.3.2.3" xref="S5.E1.m1.1.1.1.1.1.3.2.3.2.3.cmml"><mi id="S5.E1.m1.1.1.1.1.1.3.2.3.2.3.2" xref="S5.E1.m1.1.1.1.1.1.3.2.3.2.3.2.cmml">d</mi><mo id="S5.E1.m1.1.1.1.1.1.3.2.3.2.3.1" xref="S5.E1.m1.1.1.1.1.1.3.2.3.2.3.1.cmml">⁢</mo><mi id="S5.E1.m1.1.1.1.1.1.3.2.3.2.3.3" xref="S5.E1.m1.1.1.1.1.1.3.2.3.2.3.3.cmml">r</mi><mo id="S5.E1.m1.1.1.1.1.1.3.2.3.2.3.1a" xref="S5.E1.m1.1.1.1.1.1.3.2.3.2.3.1.cmml">⁢</mo><mi id="S5.E1.m1.1.1.1.1.1.3.2.3.2.3.4" xref="S5.E1.m1.1.1.1.1.1.3.2.3.2.3.4.cmml">o</mi><mo id="S5.E1.m1.1.1.1.1.1.3.2.3.2.3.1b" xref="S5.E1.m1.1.1.1.1.1.3.2.3.2.3.1.cmml">⁢</mo><mi id="S5.E1.m1.1.1.1.1.1.3.2.3.2.3.5" xref="S5.E1.m1.1.1.1.1.1.3.2.3.2.3.5.cmml">n</mi><mo id="S5.E1.m1.1.1.1.1.1.3.2.3.2.3.1c" xref="S5.E1.m1.1.1.1.1.1.3.2.3.2.3.1.cmml">⁢</mo><mi id="S5.E1.m1.1.1.1.1.1.3.2.3.2.3.6" xref="S5.E1.m1.1.1.1.1.1.3.2.3.2.3.6.cmml">e</mi></mrow></msubsup></mrow></mrow><mo id="S5.E1.m1.1.1.1.1.1.2" xref="S5.E1.m1.1.1.1.1.1.2.cmml">+</mo><mrow id="S5.E1.m1.1.1.1.1.1.1" xref="S5.E1.m1.1.1.1.1.1.1.cmml"><mrow id="S5.E1.m1.1.1.1.1.1.1.1.1" xref="S5.E1.m1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S5.E1.m1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S5.E1.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.E1.m1.1.1.1.1.1.1.1.1.1" xref="S5.E1.m1.1.1.1.1.1.1.1.1.1.cmml"><mn id="S5.E1.m1.1.1.1.1.1.1.1.1.1.2" xref="S5.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S5.E1.m1.1.1.1.1.1.1.1.1.1.1" xref="S5.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml">−</mo><mi id="S5.E1.m1.1.1.1.1.1.1.1.1.1.3" xref="S5.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml">λ</mi></mrow><mo id="S5.E1.m1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S5.E1.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S5.E1.m1.1.1.1.1.1.1.2" xref="S5.E1.m1.1.1.1.1.1.1.2.cmml">⁢</mo><msub id="S5.E1.m1.1.1.1.1.1.1.3" xref="S5.E1.m1.1.1.1.1.1.1.3.cmml"><mi id="S5.E1.m1.1.1.1.1.1.1.3.2" xref="S5.E1.m1.1.1.1.1.1.1.3.2.cmml">𝐄</mi><mi id="S5.E1.m1.1.1.1.1.1.1.3.3" xref="S5.E1.m1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow></mrow></mrow><mo id="S5.E1.m1.1.1.1.2" lspace="0em" xref="S5.E1.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.E1.m1.1b"><apply id="S5.E1.m1.1.1.1.1.cmml" xref="S5.E1.m1.1.1.1"><eq id="S5.E1.m1.1.1.1.1.2.cmml" xref="S5.E1.m1.1.1.1.1.2"></eq><ci id="S5.E1.m1.1.1.1.1.3.cmml" xref="S5.E1.m1.1.1.1.1.3">𝒫</ci><apply id="S5.E1.m1.1.1.1.1.1.cmml" xref="S5.E1.m1.1.1.1.1.1"><plus id="S5.E1.m1.1.1.1.1.1.2.cmml" xref="S5.E1.m1.1.1.1.1.1.2"></plus><apply id="S5.E1.m1.1.1.1.1.1.3.cmml" xref="S5.E1.m1.1.1.1.1.1.3"><apply id="S5.E1.m1.1.1.1.1.1.3.1.cmml" xref="S5.E1.m1.1.1.1.1.1.3.1"><csymbol cd="ambiguous" id="S5.E1.m1.1.1.1.1.1.3.1.1.cmml" xref="S5.E1.m1.1.1.1.1.1.3.1">subscript</csymbol><sum id="S5.E1.m1.1.1.1.1.1.3.1.2.cmml" xref="S5.E1.m1.1.1.1.1.1.3.1.2"></sum><ci id="S5.E1.m1.1.1.1.1.1.3.1.3.cmml" xref="S5.E1.m1.1.1.1.1.1.3.1.3">𝑖</ci></apply><apply id="S5.E1.m1.1.1.1.1.1.3.2.cmml" xref="S5.E1.m1.1.1.1.1.1.3.2"><times id="S5.E1.m1.1.1.1.1.1.3.2.1.cmml" xref="S5.E1.m1.1.1.1.1.1.3.2.1"></times><ci id="S5.E1.m1.1.1.1.1.1.3.2.2.cmml" xref="S5.E1.m1.1.1.1.1.1.3.2.2">𝜆</ci><apply id="S5.E1.m1.1.1.1.1.1.3.2.3.cmml" xref="S5.E1.m1.1.1.1.1.1.3.2.3"><csymbol cd="ambiguous" id="S5.E1.m1.1.1.1.1.1.3.2.3.1.cmml" xref="S5.E1.m1.1.1.1.1.1.3.2.3">subscript</csymbol><apply id="S5.E1.m1.1.1.1.1.1.3.2.3.2.cmml" xref="S5.E1.m1.1.1.1.1.1.3.2.3"><csymbol cd="ambiguous" id="S5.E1.m1.1.1.1.1.1.3.2.3.2.1.cmml" xref="S5.E1.m1.1.1.1.1.1.3.2.3">superscript</csymbol><ci id="S5.E1.m1.1.1.1.1.1.3.2.3.2.2.cmml" xref="S5.E1.m1.1.1.1.1.1.3.2.3.2.2">ℰ</ci><apply id="S5.E1.m1.1.1.1.1.1.3.2.3.2.3.cmml" xref="S5.E1.m1.1.1.1.1.1.3.2.3.2.3"><times id="S5.E1.m1.1.1.1.1.1.3.2.3.2.3.1.cmml" xref="S5.E1.m1.1.1.1.1.1.3.2.3.2.3.1"></times><ci id="S5.E1.m1.1.1.1.1.1.3.2.3.2.3.2.cmml" xref="S5.E1.m1.1.1.1.1.1.3.2.3.2.3.2">𝑑</ci><ci id="S5.E1.m1.1.1.1.1.1.3.2.3.2.3.3.cmml" xref="S5.E1.m1.1.1.1.1.1.3.2.3.2.3.3">𝑟</ci><ci id="S5.E1.m1.1.1.1.1.1.3.2.3.2.3.4.cmml" xref="S5.E1.m1.1.1.1.1.1.3.2.3.2.3.4">𝑜</ci><ci id="S5.E1.m1.1.1.1.1.1.3.2.3.2.3.5.cmml" xref="S5.E1.m1.1.1.1.1.1.3.2.3.2.3.5">𝑛</ci><ci id="S5.E1.m1.1.1.1.1.1.3.2.3.2.3.6.cmml" xref="S5.E1.m1.1.1.1.1.1.3.2.3.2.3.6">𝑒</ci></apply></apply><ci id="S5.E1.m1.1.1.1.1.1.3.2.3.3.cmml" xref="S5.E1.m1.1.1.1.1.1.3.2.3.3">𝑖</ci></apply></apply></apply><apply id="S5.E1.m1.1.1.1.1.1.1.cmml" xref="S5.E1.m1.1.1.1.1.1.1"><times id="S5.E1.m1.1.1.1.1.1.1.2.cmml" xref="S5.E1.m1.1.1.1.1.1.1.2"></times><apply id="S5.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.E1.m1.1.1.1.1.1.1.1.1"><minus id="S5.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.E1.m1.1.1.1.1.1.1.1.1.1.1"></minus><cn id="S5.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml" type="integer" xref="S5.E1.m1.1.1.1.1.1.1.1.1.1.2">1</cn><ci id="S5.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S5.E1.m1.1.1.1.1.1.1.1.1.1.3">𝜆</ci></apply><apply id="S5.E1.m1.1.1.1.1.1.1.3.cmml" xref="S5.E1.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.E1.m1.1.1.1.1.1.1.3.1.cmml" xref="S5.E1.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S5.E1.m1.1.1.1.1.1.1.3.2.cmml" xref="S5.E1.m1.1.1.1.1.1.1.3.2">𝐄</ci><ci id="S5.E1.m1.1.1.1.1.1.1.3.3.cmml" xref="S5.E1.m1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E1.m1.1c">\mathcal{P}=\sum_{i}\lambda\mathcal{E}^{drone}_{i}+(1-\lambda)\mathbf{E}_{i}.</annotation><annotation encoding="application/x-llamapun" id="S5.E1.m1.1d">caligraphic_P = ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_λ caligraphic_E start_POSTSUPERSCRIPT italic_d italic_r italic_o italic_n italic_e end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + ( 1 - italic_λ ) bold_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.7">Then, to guide the drone with the proposed P-ESDF, we design pose penalty <math alttext="J_{pose}" class="ltx_Math" display="inline" id="S5.p4.1.m1.1"><semantics id="S5.p4.1.m1.1a"><msub id="S5.p4.1.m1.1.1" xref="S5.p4.1.m1.1.1.cmml"><mi id="S5.p4.1.m1.1.1.2" xref="S5.p4.1.m1.1.1.2.cmml">J</mi><mrow id="S5.p4.1.m1.1.1.3" xref="S5.p4.1.m1.1.1.3.cmml"><mi id="S5.p4.1.m1.1.1.3.2" xref="S5.p4.1.m1.1.1.3.2.cmml">p</mi><mo id="S5.p4.1.m1.1.1.3.1" xref="S5.p4.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.p4.1.m1.1.1.3.3" xref="S5.p4.1.m1.1.1.3.3.cmml">o</mi><mo id="S5.p4.1.m1.1.1.3.1a" xref="S5.p4.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.p4.1.m1.1.1.3.4" xref="S5.p4.1.m1.1.1.3.4.cmml">s</mi><mo id="S5.p4.1.m1.1.1.3.1b" xref="S5.p4.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.p4.1.m1.1.1.3.5" xref="S5.p4.1.m1.1.1.3.5.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.p4.1.m1.1b"><apply id="S5.p4.1.m1.1.1.cmml" xref="S5.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S5.p4.1.m1.1.1.1.cmml" xref="S5.p4.1.m1.1.1">subscript</csymbol><ci id="S5.p4.1.m1.1.1.2.cmml" xref="S5.p4.1.m1.1.1.2">𝐽</ci><apply id="S5.p4.1.m1.1.1.3.cmml" xref="S5.p4.1.m1.1.1.3"><times id="S5.p4.1.m1.1.1.3.1.cmml" xref="S5.p4.1.m1.1.1.3.1"></times><ci id="S5.p4.1.m1.1.1.3.2.cmml" xref="S5.p4.1.m1.1.1.3.2">𝑝</ci><ci id="S5.p4.1.m1.1.1.3.3.cmml" xref="S5.p4.1.m1.1.1.3.3">𝑜</ci><ci id="S5.p4.1.m1.1.1.3.4.cmml" xref="S5.p4.1.m1.1.1.3.4">𝑠</ci><ci id="S5.p4.1.m1.1.1.3.5.cmml" xref="S5.p4.1.m1.1.1.3.5">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.1.m1.1c">J_{pose}</annotation><annotation encoding="application/x-llamapun" id="S5.p4.1.m1.1d">italic_J start_POSTSUBSCRIPT italic_p italic_o italic_s italic_e end_POSTSUBSCRIPT</annotation></semantics></math> as the function of <math alttext="p" class="ltx_Math" display="inline" id="S5.p4.2.m2.1"><semantics id="S5.p4.2.m2.1a"><mi id="S5.p4.2.m2.1.1" xref="S5.p4.2.m2.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S5.p4.2.m2.1b"><ci id="S5.p4.2.m2.1.1.cmml" xref="S5.p4.2.m2.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.2.m2.1c">p</annotation><annotation encoding="application/x-llamapun" id="S5.p4.2.m2.1d">italic_p</annotation></semantics></math>. Assume the path is constructed by a series of waypoints <math alttext="\{\mathbf{p_{k}}\subset\mathcal{R}^{3}|k\in\mathbb{Z}^{+}\}" class="ltx_Math" display="inline" id="S5.p4.3.m3.2"><semantics id="S5.p4.3.m3.2a"><mrow id="S5.p4.3.m3.2.2.2" xref="S5.p4.3.m3.2.2.3.cmml"><mo id="S5.p4.3.m3.2.2.2.3" stretchy="false" xref="S5.p4.3.m3.2.2.3.1.cmml">{</mo><mrow id="S5.p4.3.m3.1.1.1.1" xref="S5.p4.3.m3.1.1.1.1.cmml"><msub id="S5.p4.3.m3.1.1.1.1.2" xref="S5.p4.3.m3.1.1.1.1.2.cmml"><mi id="S5.p4.3.m3.1.1.1.1.2.2" xref="S5.p4.3.m3.1.1.1.1.2.2.cmml">𝐩</mi><mi id="S5.p4.3.m3.1.1.1.1.2.3" xref="S5.p4.3.m3.1.1.1.1.2.3.cmml">𝐤</mi></msub><mo id="S5.p4.3.m3.1.1.1.1.1" xref="S5.p4.3.m3.1.1.1.1.1.cmml">⊂</mo><msup id="S5.p4.3.m3.1.1.1.1.3" xref="S5.p4.3.m3.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.p4.3.m3.1.1.1.1.3.2" xref="S5.p4.3.m3.1.1.1.1.3.2.cmml">ℛ</mi><mn id="S5.p4.3.m3.1.1.1.1.3.3" xref="S5.p4.3.m3.1.1.1.1.3.3.cmml">3</mn></msup></mrow><mo id="S5.p4.3.m3.2.2.2.4" lspace="0em" rspace="0em" xref="S5.p4.3.m3.2.2.3.1.cmml">|</mo><mrow id="S5.p4.3.m3.2.2.2.2" xref="S5.p4.3.m3.2.2.2.2.cmml"><mi id="S5.p4.3.m3.2.2.2.2.2" xref="S5.p4.3.m3.2.2.2.2.2.cmml">k</mi><mo id="S5.p4.3.m3.2.2.2.2.1" xref="S5.p4.3.m3.2.2.2.2.1.cmml">∈</mo><msup id="S5.p4.3.m3.2.2.2.2.3" xref="S5.p4.3.m3.2.2.2.2.3.cmml"><mi id="S5.p4.3.m3.2.2.2.2.3.2" xref="S5.p4.3.m3.2.2.2.2.3.2.cmml">ℤ</mi><mo id="S5.p4.3.m3.2.2.2.2.3.3" xref="S5.p4.3.m3.2.2.2.2.3.3.cmml">+</mo></msup></mrow><mo id="S5.p4.3.m3.2.2.2.5" stretchy="false" xref="S5.p4.3.m3.2.2.3.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.p4.3.m3.2b"><apply id="S5.p4.3.m3.2.2.3.cmml" xref="S5.p4.3.m3.2.2.2"><csymbol cd="latexml" id="S5.p4.3.m3.2.2.3.1.cmml" xref="S5.p4.3.m3.2.2.2.3">conditional-set</csymbol><apply id="S5.p4.3.m3.1.1.1.1.cmml" xref="S5.p4.3.m3.1.1.1.1"><subset id="S5.p4.3.m3.1.1.1.1.1.cmml" xref="S5.p4.3.m3.1.1.1.1.1"></subset><apply id="S5.p4.3.m3.1.1.1.1.2.cmml" xref="S5.p4.3.m3.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.p4.3.m3.1.1.1.1.2.1.cmml" xref="S5.p4.3.m3.1.1.1.1.2">subscript</csymbol><ci id="S5.p4.3.m3.1.1.1.1.2.2.cmml" xref="S5.p4.3.m3.1.1.1.1.2.2">𝐩</ci><ci id="S5.p4.3.m3.1.1.1.1.2.3.cmml" xref="S5.p4.3.m3.1.1.1.1.2.3">𝐤</ci></apply><apply id="S5.p4.3.m3.1.1.1.1.3.cmml" xref="S5.p4.3.m3.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.p4.3.m3.1.1.1.1.3.1.cmml" xref="S5.p4.3.m3.1.1.1.1.3">superscript</csymbol><ci id="S5.p4.3.m3.1.1.1.1.3.2.cmml" xref="S5.p4.3.m3.1.1.1.1.3.2">ℛ</ci><cn id="S5.p4.3.m3.1.1.1.1.3.3.cmml" type="integer" xref="S5.p4.3.m3.1.1.1.1.3.3">3</cn></apply></apply><apply id="S5.p4.3.m3.2.2.2.2.cmml" xref="S5.p4.3.m3.2.2.2.2"><in id="S5.p4.3.m3.2.2.2.2.1.cmml" xref="S5.p4.3.m3.2.2.2.2.1"></in><ci id="S5.p4.3.m3.2.2.2.2.2.cmml" xref="S5.p4.3.m3.2.2.2.2.2">𝑘</ci><apply id="S5.p4.3.m3.2.2.2.2.3.cmml" xref="S5.p4.3.m3.2.2.2.2.3"><csymbol cd="ambiguous" id="S5.p4.3.m3.2.2.2.2.3.1.cmml" xref="S5.p4.3.m3.2.2.2.2.3">superscript</csymbol><ci id="S5.p4.3.m3.2.2.2.2.3.2.cmml" xref="S5.p4.3.m3.2.2.2.2.3.2">ℤ</ci><plus id="S5.p4.3.m3.2.2.2.2.3.3.cmml" xref="S5.p4.3.m3.2.2.2.2.3.3"></plus></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.3.m3.2c">\{\mathbf{p_{k}}\subset\mathcal{R}^{3}|k\in\mathbb{Z}^{+}\}</annotation><annotation encoding="application/x-llamapun" id="S5.p4.3.m3.2d">{ bold_p start_POSTSUBSCRIPT bold_k end_POSTSUBSCRIPT ⊂ caligraphic_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT | italic_k ∈ blackboard_Z start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT }</annotation></semantics></math>, and define <math alttext="\Xi(\mathbf{p_{k}})" class="ltx_Math" display="inline" id="S5.p4.4.m4.1"><semantics id="S5.p4.4.m4.1a"><mrow id="S5.p4.4.m4.1.1" xref="S5.p4.4.m4.1.1.cmml"><mi id="S5.p4.4.m4.1.1.3" mathvariant="normal" xref="S5.p4.4.m4.1.1.3.cmml">Ξ</mi><mo id="S5.p4.4.m4.1.1.2" xref="S5.p4.4.m4.1.1.2.cmml">⁢</mo><mrow id="S5.p4.4.m4.1.1.1.1" xref="S5.p4.4.m4.1.1.1.1.1.cmml"><mo id="S5.p4.4.m4.1.1.1.1.2" stretchy="false" xref="S5.p4.4.m4.1.1.1.1.1.cmml">(</mo><msub id="S5.p4.4.m4.1.1.1.1.1" xref="S5.p4.4.m4.1.1.1.1.1.cmml"><mi id="S5.p4.4.m4.1.1.1.1.1.2" xref="S5.p4.4.m4.1.1.1.1.1.2.cmml">𝐩</mi><mi id="S5.p4.4.m4.1.1.1.1.1.3" xref="S5.p4.4.m4.1.1.1.1.1.3.cmml">𝐤</mi></msub><mo id="S5.p4.4.m4.1.1.1.1.3" stretchy="false" xref="S5.p4.4.m4.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.p4.4.m4.1b"><apply id="S5.p4.4.m4.1.1.cmml" xref="S5.p4.4.m4.1.1"><times id="S5.p4.4.m4.1.1.2.cmml" xref="S5.p4.4.m4.1.1.2"></times><ci id="S5.p4.4.m4.1.1.3.cmml" xref="S5.p4.4.m4.1.1.3">Ξ</ci><apply id="S5.p4.4.m4.1.1.1.1.1.cmml" xref="S5.p4.4.m4.1.1.1.1"><csymbol cd="ambiguous" id="S5.p4.4.m4.1.1.1.1.1.1.cmml" xref="S5.p4.4.m4.1.1.1.1">subscript</csymbol><ci id="S5.p4.4.m4.1.1.1.1.1.2.cmml" xref="S5.p4.4.m4.1.1.1.1.1.2">𝐩</ci><ci id="S5.p4.4.m4.1.1.1.1.1.3.cmml" xref="S5.p4.4.m4.1.1.1.1.1.3">𝐤</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.4.m4.1c">\Xi(\mathbf{p_{k}})</annotation><annotation encoding="application/x-llamapun" id="S5.p4.4.m4.1d">roman_Ξ ( bold_p start_POSTSUBSCRIPT bold_k end_POSTSUBSCRIPT )</annotation></semantics></math> as the value of <math alttext="\mathcal{P}" class="ltx_Math" display="inline" id="S5.p4.5.m5.1"><semantics id="S5.p4.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S5.p4.5.m5.1.1" xref="S5.p4.5.m5.1.1.cmml">𝒫</mi><annotation-xml encoding="MathML-Content" id="S5.p4.5.m5.1b"><ci id="S5.p4.5.m5.1.1.cmml" xref="S5.p4.5.m5.1.1">𝒫</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.5.m5.1c">\mathcal{P}</annotation><annotation encoding="application/x-llamapun" id="S5.p4.5.m5.1d">caligraphic_P</annotation></semantics></math> at the position of <math alttext="\mathbf{p_{k}}" class="ltx_Math" display="inline" id="S5.p4.6.m6.1"><semantics id="S5.p4.6.m6.1a"><msub id="S5.p4.6.m6.1.1" xref="S5.p4.6.m6.1.1.cmml"><mi id="S5.p4.6.m6.1.1.2" xref="S5.p4.6.m6.1.1.2.cmml">𝐩</mi><mi id="S5.p4.6.m6.1.1.3" xref="S5.p4.6.m6.1.1.3.cmml">𝐤</mi></msub><annotation-xml encoding="MathML-Content" id="S5.p4.6.m6.1b"><apply id="S5.p4.6.m6.1.1.cmml" xref="S5.p4.6.m6.1.1"><csymbol cd="ambiguous" id="S5.p4.6.m6.1.1.1.cmml" xref="S5.p4.6.m6.1.1">subscript</csymbol><ci id="S5.p4.6.m6.1.1.2.cmml" xref="S5.p4.6.m6.1.1.2">𝐩</ci><ci id="S5.p4.6.m6.1.1.3.cmml" xref="S5.p4.6.m6.1.1.3">𝐤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.6.m6.1c">\mathbf{p_{k}}</annotation><annotation encoding="application/x-llamapun" id="S5.p4.6.m6.1d">bold_p start_POSTSUBSCRIPT bold_k end_POSTSUBSCRIPT</annotation></semantics></math>.
The <math alttext="J_{pose}" class="ltx_Math" display="inline" id="S5.p4.7.m7.1"><semantics id="S5.p4.7.m7.1a"><msub id="S5.p4.7.m7.1.1" xref="S5.p4.7.m7.1.1.cmml"><mi id="S5.p4.7.m7.1.1.2" xref="S5.p4.7.m7.1.1.2.cmml">J</mi><mrow id="S5.p4.7.m7.1.1.3" xref="S5.p4.7.m7.1.1.3.cmml"><mi id="S5.p4.7.m7.1.1.3.2" xref="S5.p4.7.m7.1.1.3.2.cmml">p</mi><mo id="S5.p4.7.m7.1.1.3.1" xref="S5.p4.7.m7.1.1.3.1.cmml">⁢</mo><mi id="S5.p4.7.m7.1.1.3.3" xref="S5.p4.7.m7.1.1.3.3.cmml">o</mi><mo id="S5.p4.7.m7.1.1.3.1a" xref="S5.p4.7.m7.1.1.3.1.cmml">⁢</mo><mi id="S5.p4.7.m7.1.1.3.4" xref="S5.p4.7.m7.1.1.3.4.cmml">s</mi><mo id="S5.p4.7.m7.1.1.3.1b" xref="S5.p4.7.m7.1.1.3.1.cmml">⁢</mo><mi id="S5.p4.7.m7.1.1.3.5" xref="S5.p4.7.m7.1.1.3.5.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.p4.7.m7.1b"><apply id="S5.p4.7.m7.1.1.cmml" xref="S5.p4.7.m7.1.1"><csymbol cd="ambiguous" id="S5.p4.7.m7.1.1.1.cmml" xref="S5.p4.7.m7.1.1">subscript</csymbol><ci id="S5.p4.7.m7.1.1.2.cmml" xref="S5.p4.7.m7.1.1.2">𝐽</ci><apply id="S5.p4.7.m7.1.1.3.cmml" xref="S5.p4.7.m7.1.1.3"><times id="S5.p4.7.m7.1.1.3.1.cmml" xref="S5.p4.7.m7.1.1.3.1"></times><ci id="S5.p4.7.m7.1.1.3.2.cmml" xref="S5.p4.7.m7.1.1.3.2">𝑝</ci><ci id="S5.p4.7.m7.1.1.3.3.cmml" xref="S5.p4.7.m7.1.1.3.3">𝑜</ci><ci id="S5.p4.7.m7.1.1.3.4.cmml" xref="S5.p4.7.m7.1.1.3.4">𝑠</ci><ci id="S5.p4.7.m7.1.1.3.5.cmml" xref="S5.p4.7.m7.1.1.3.5">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.7.m7.1c">J_{pose}</annotation><annotation encoding="application/x-llamapun" id="S5.p4.7.m7.1d">italic_J start_POSTSUBSCRIPT italic_p italic_o italic_s italic_e end_POSTSUBSCRIPT</annotation></semantics></math> can be expressed as</p>
</div>
<div class="ltx_para" id="S5.p5">
<table class="ltx_equation ltx_eqn_table" id="S5.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="J_{pose}=\lambda_{p}\sum_{i=0}^{M}c(p_{i})\mathbf{p_{k}}^{\prime}" class="ltx_Math" display="block" id="S5.E2.m1.1"><semantics id="S5.E2.m1.1a"><mrow id="S5.E2.m1.1.1" xref="S5.E2.m1.1.1.cmml"><msub id="S5.E2.m1.1.1.3" xref="S5.E2.m1.1.1.3.cmml"><mi id="S5.E2.m1.1.1.3.2" xref="S5.E2.m1.1.1.3.2.cmml">J</mi><mrow id="S5.E2.m1.1.1.3.3" xref="S5.E2.m1.1.1.3.3.cmml"><mi id="S5.E2.m1.1.1.3.3.2" xref="S5.E2.m1.1.1.3.3.2.cmml">p</mi><mo id="S5.E2.m1.1.1.3.3.1" xref="S5.E2.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S5.E2.m1.1.1.3.3.3" xref="S5.E2.m1.1.1.3.3.3.cmml">o</mi><mo id="S5.E2.m1.1.1.3.3.1a" xref="S5.E2.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S5.E2.m1.1.1.3.3.4" xref="S5.E2.m1.1.1.3.3.4.cmml">s</mi><mo id="S5.E2.m1.1.1.3.3.1b" xref="S5.E2.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S5.E2.m1.1.1.3.3.5" xref="S5.E2.m1.1.1.3.3.5.cmml">e</mi></mrow></msub><mo id="S5.E2.m1.1.1.2" xref="S5.E2.m1.1.1.2.cmml">=</mo><mrow id="S5.E2.m1.1.1.1" xref="S5.E2.m1.1.1.1.cmml"><msub id="S5.E2.m1.1.1.1.3" xref="S5.E2.m1.1.1.1.3.cmml"><mi id="S5.E2.m1.1.1.1.3.2" xref="S5.E2.m1.1.1.1.3.2.cmml">λ</mi><mi id="S5.E2.m1.1.1.1.3.3" xref="S5.E2.m1.1.1.1.3.3.cmml">p</mi></msub><mo id="S5.E2.m1.1.1.1.2" xref="S5.E2.m1.1.1.1.2.cmml">⁢</mo><mrow id="S5.E2.m1.1.1.1.1" xref="S5.E2.m1.1.1.1.1.cmml"><munderover id="S5.E2.m1.1.1.1.1.2" xref="S5.E2.m1.1.1.1.1.2.cmml"><mo id="S5.E2.m1.1.1.1.1.2.2.2" movablelimits="false" xref="S5.E2.m1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S5.E2.m1.1.1.1.1.2.2.3" xref="S5.E2.m1.1.1.1.1.2.2.3.cmml"><mi id="S5.E2.m1.1.1.1.1.2.2.3.2" xref="S5.E2.m1.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S5.E2.m1.1.1.1.1.2.2.3.1" xref="S5.E2.m1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S5.E2.m1.1.1.1.1.2.2.3.3" xref="S5.E2.m1.1.1.1.1.2.2.3.3.cmml">0</mn></mrow><mi id="S5.E2.m1.1.1.1.1.2.3" xref="S5.E2.m1.1.1.1.1.2.3.cmml">M</mi></munderover><mrow id="S5.E2.m1.1.1.1.1.1" xref="S5.E2.m1.1.1.1.1.1.cmml"><mi id="S5.E2.m1.1.1.1.1.1.3" xref="S5.E2.m1.1.1.1.1.1.3.cmml">c</mi><mo id="S5.E2.m1.1.1.1.1.1.2" xref="S5.E2.m1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S5.E2.m1.1.1.1.1.1.1.1" xref="S5.E2.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S5.E2.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="S5.E2.m1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S5.E2.m1.1.1.1.1.1.1.1.1" xref="S5.E2.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S5.E2.m1.1.1.1.1.1.1.1.1.2" xref="S5.E2.m1.1.1.1.1.1.1.1.1.2.cmml">p</mi><mi id="S5.E2.m1.1.1.1.1.1.1.1.1.3" xref="S5.E2.m1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S5.E2.m1.1.1.1.1.1.1.1.3" stretchy="false" xref="S5.E2.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S5.E2.m1.1.1.1.1.1.2a" xref="S5.E2.m1.1.1.1.1.1.2.cmml">⁢</mo><mmultiscripts id="S5.E2.m1.1.1.1.1.1.4" xref="S5.E2.m1.1.1.1.1.1.4.cmml"><mi id="S5.E2.m1.1.1.1.1.1.4.2.2" xref="S5.E2.m1.1.1.1.1.1.4.2.2.cmml">𝐩</mi><mi id="S5.E2.m1.1.1.1.1.1.4.2.3" xref="S5.E2.m1.1.1.1.1.1.4.2.3.cmml">𝐤</mi><mrow id="S5.E2.m1.1.1.1.1.1.4a" xref="S5.E2.m1.1.1.1.1.1.4.cmml"></mrow><mrow id="S5.E2.m1.1.1.1.1.1.4b" xref="S5.E2.m1.1.1.1.1.1.4.cmml"></mrow><mo id="S5.E2.m1.1.1.1.1.1.4.3" xref="S5.E2.m1.1.1.1.1.1.4.3.cmml">′</mo></mmultiscripts></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.E2.m1.1b"><apply id="S5.E2.m1.1.1.cmml" xref="S5.E2.m1.1.1"><eq id="S5.E2.m1.1.1.2.cmml" xref="S5.E2.m1.1.1.2"></eq><apply id="S5.E2.m1.1.1.3.cmml" xref="S5.E2.m1.1.1.3"><csymbol cd="ambiguous" id="S5.E2.m1.1.1.3.1.cmml" xref="S5.E2.m1.1.1.3">subscript</csymbol><ci id="S5.E2.m1.1.1.3.2.cmml" xref="S5.E2.m1.1.1.3.2">𝐽</ci><apply id="S5.E2.m1.1.1.3.3.cmml" xref="S5.E2.m1.1.1.3.3"><times id="S5.E2.m1.1.1.3.3.1.cmml" xref="S5.E2.m1.1.1.3.3.1"></times><ci id="S5.E2.m1.1.1.3.3.2.cmml" xref="S5.E2.m1.1.1.3.3.2">𝑝</ci><ci id="S5.E2.m1.1.1.3.3.3.cmml" xref="S5.E2.m1.1.1.3.3.3">𝑜</ci><ci id="S5.E2.m1.1.1.3.3.4.cmml" xref="S5.E2.m1.1.1.3.3.4">𝑠</ci><ci id="S5.E2.m1.1.1.3.3.5.cmml" xref="S5.E2.m1.1.1.3.3.5">𝑒</ci></apply></apply><apply id="S5.E2.m1.1.1.1.cmml" xref="S5.E2.m1.1.1.1"><times id="S5.E2.m1.1.1.1.2.cmml" xref="S5.E2.m1.1.1.1.2"></times><apply id="S5.E2.m1.1.1.1.3.cmml" xref="S5.E2.m1.1.1.1.3"><csymbol cd="ambiguous" id="S5.E2.m1.1.1.1.3.1.cmml" xref="S5.E2.m1.1.1.1.3">subscript</csymbol><ci id="S5.E2.m1.1.1.1.3.2.cmml" xref="S5.E2.m1.1.1.1.3.2">𝜆</ci><ci id="S5.E2.m1.1.1.1.3.3.cmml" xref="S5.E2.m1.1.1.1.3.3">𝑝</ci></apply><apply id="S5.E2.m1.1.1.1.1.cmml" xref="S5.E2.m1.1.1.1.1"><apply id="S5.E2.m1.1.1.1.1.2.cmml" xref="S5.E2.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.E2.m1.1.1.1.1.2.1.cmml" xref="S5.E2.m1.1.1.1.1.2">superscript</csymbol><apply id="S5.E2.m1.1.1.1.1.2.2.cmml" xref="S5.E2.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.E2.m1.1.1.1.1.2.2.1.cmml" xref="S5.E2.m1.1.1.1.1.2">subscript</csymbol><sum id="S5.E2.m1.1.1.1.1.2.2.2.cmml" xref="S5.E2.m1.1.1.1.1.2.2.2"></sum><apply id="S5.E2.m1.1.1.1.1.2.2.3.cmml" xref="S5.E2.m1.1.1.1.1.2.2.3"><eq id="S5.E2.m1.1.1.1.1.2.2.3.1.cmml" xref="S5.E2.m1.1.1.1.1.2.2.3.1"></eq><ci id="S5.E2.m1.1.1.1.1.2.2.3.2.cmml" xref="S5.E2.m1.1.1.1.1.2.2.3.2">𝑖</ci><cn id="S5.E2.m1.1.1.1.1.2.2.3.3.cmml" type="integer" xref="S5.E2.m1.1.1.1.1.2.2.3.3">0</cn></apply></apply><ci id="S5.E2.m1.1.1.1.1.2.3.cmml" xref="S5.E2.m1.1.1.1.1.2.3">𝑀</ci></apply><apply id="S5.E2.m1.1.1.1.1.1.cmml" xref="S5.E2.m1.1.1.1.1.1"><times id="S5.E2.m1.1.1.1.1.1.2.cmml" xref="S5.E2.m1.1.1.1.1.1.2"></times><ci id="S5.E2.m1.1.1.1.1.1.3.cmml" xref="S5.E2.m1.1.1.1.1.1.3">𝑐</ci><apply id="S5.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S5.E2.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.E2.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S5.E2.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S5.E2.m1.1.1.1.1.1.1.1.1.2">𝑝</ci><ci id="S5.E2.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S5.E2.m1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S5.E2.m1.1.1.1.1.1.4.cmml" xref="S5.E2.m1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S5.E2.m1.1.1.1.1.1.4.1.cmml" xref="S5.E2.m1.1.1.1.1.1.4">superscript</csymbol><apply id="S5.E2.m1.1.1.1.1.1.4.2.cmml" xref="S5.E2.m1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S5.E2.m1.1.1.1.1.1.4.2.1.cmml" xref="S5.E2.m1.1.1.1.1.1.4">subscript</csymbol><ci id="S5.E2.m1.1.1.1.1.1.4.2.2.cmml" xref="S5.E2.m1.1.1.1.1.1.4.2.2">𝐩</ci><ci id="S5.E2.m1.1.1.1.1.1.4.2.3.cmml" xref="S5.E2.m1.1.1.1.1.1.4.2.3">𝐤</ci></apply><ci id="S5.E2.m1.1.1.1.1.1.4.3.cmml" xref="S5.E2.m1.1.1.1.1.1.4.3">′</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E2.m1.1c">J_{pose}=\lambda_{p}\sum_{i=0}^{M}c(p_{i})\mathbf{p_{k}}^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S5.E2.m1.1d">italic_J start_POSTSUBSCRIPT italic_p italic_o italic_s italic_e end_POSTSUBSCRIPT = italic_λ start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ∑ start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT italic_c ( italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) bold_p start_POSTSUBSCRIPT bold_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S5.p5.2">where <math alttext="\mathbf{p}_{k}^{\prime}=\frac{\partial\Xi(\mathbf{p}_{k})}{\partial\mathbf{p}_%
{k}}" class="ltx_Math" display="inline" id="S5.p5.1.m1.1"><semantics id="S5.p5.1.m1.1a"><mrow id="S5.p5.1.m1.1.2" xref="S5.p5.1.m1.1.2.cmml"><msubsup id="S5.p5.1.m1.1.2.2" xref="S5.p5.1.m1.1.2.2.cmml"><mi id="S5.p5.1.m1.1.2.2.2.2" xref="S5.p5.1.m1.1.2.2.2.2.cmml">𝐩</mi><mi id="S5.p5.1.m1.1.2.2.2.3" xref="S5.p5.1.m1.1.2.2.2.3.cmml">k</mi><mo id="S5.p5.1.m1.1.2.2.3" xref="S5.p5.1.m1.1.2.2.3.cmml">′</mo></msubsup><mo id="S5.p5.1.m1.1.2.1" xref="S5.p5.1.m1.1.2.1.cmml">=</mo><mfrac id="S5.p5.1.m1.1.1" xref="S5.p5.1.m1.1.1.cmml"><mrow id="S5.p5.1.m1.1.1.1" xref="S5.p5.1.m1.1.1.1.cmml"><mo id="S5.p5.1.m1.1.1.1.2" rspace="0em" xref="S5.p5.1.m1.1.1.1.2.cmml">∂</mo><mrow id="S5.p5.1.m1.1.1.1.1" xref="S5.p5.1.m1.1.1.1.1.cmml"><mi id="S5.p5.1.m1.1.1.1.1.3" mathvariant="normal" xref="S5.p5.1.m1.1.1.1.1.3.cmml">Ξ</mi><mo id="S5.p5.1.m1.1.1.1.1.2" xref="S5.p5.1.m1.1.1.1.1.2.cmml">⁢</mo><mrow id="S5.p5.1.m1.1.1.1.1.1.1" xref="S5.p5.1.m1.1.1.1.1.1.1.1.cmml"><mo id="S5.p5.1.m1.1.1.1.1.1.1.2" stretchy="false" xref="S5.p5.1.m1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S5.p5.1.m1.1.1.1.1.1.1.1" xref="S5.p5.1.m1.1.1.1.1.1.1.1.cmml"><mi id="S5.p5.1.m1.1.1.1.1.1.1.1.2" xref="S5.p5.1.m1.1.1.1.1.1.1.1.2.cmml">𝐩</mi><mi id="S5.p5.1.m1.1.1.1.1.1.1.1.3" xref="S5.p5.1.m1.1.1.1.1.1.1.1.3.cmml">k</mi></msub><mo id="S5.p5.1.m1.1.1.1.1.1.1.3" stretchy="false" xref="S5.p5.1.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mrow id="S5.p5.1.m1.1.1.3" xref="S5.p5.1.m1.1.1.3.cmml"><mo id="S5.p5.1.m1.1.1.3.1" rspace="0em" xref="S5.p5.1.m1.1.1.3.1.cmml">∂</mo><msub id="S5.p5.1.m1.1.1.3.2" xref="S5.p5.1.m1.1.1.3.2.cmml"><mi id="S5.p5.1.m1.1.1.3.2.2" xref="S5.p5.1.m1.1.1.3.2.2.cmml">𝐩</mi><mi id="S5.p5.1.m1.1.1.3.2.3" xref="S5.p5.1.m1.1.1.3.2.3.cmml">k</mi></msub></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S5.p5.1.m1.1b"><apply id="S5.p5.1.m1.1.2.cmml" xref="S5.p5.1.m1.1.2"><eq id="S5.p5.1.m1.1.2.1.cmml" xref="S5.p5.1.m1.1.2.1"></eq><apply id="S5.p5.1.m1.1.2.2.cmml" xref="S5.p5.1.m1.1.2.2"><csymbol cd="ambiguous" id="S5.p5.1.m1.1.2.2.1.cmml" xref="S5.p5.1.m1.1.2.2">superscript</csymbol><apply id="S5.p5.1.m1.1.2.2.2.cmml" xref="S5.p5.1.m1.1.2.2"><csymbol cd="ambiguous" id="S5.p5.1.m1.1.2.2.2.1.cmml" xref="S5.p5.1.m1.1.2.2">subscript</csymbol><ci id="S5.p5.1.m1.1.2.2.2.2.cmml" xref="S5.p5.1.m1.1.2.2.2.2">𝐩</ci><ci id="S5.p5.1.m1.1.2.2.2.3.cmml" xref="S5.p5.1.m1.1.2.2.2.3">𝑘</ci></apply><ci id="S5.p5.1.m1.1.2.2.3.cmml" xref="S5.p5.1.m1.1.2.2.3">′</ci></apply><apply id="S5.p5.1.m1.1.1.cmml" xref="S5.p5.1.m1.1.1"><divide id="S5.p5.1.m1.1.1.2.cmml" xref="S5.p5.1.m1.1.1"></divide><apply id="S5.p5.1.m1.1.1.1.cmml" xref="S5.p5.1.m1.1.1.1"><partialdiff id="S5.p5.1.m1.1.1.1.2.cmml" xref="S5.p5.1.m1.1.1.1.2"></partialdiff><apply id="S5.p5.1.m1.1.1.1.1.cmml" xref="S5.p5.1.m1.1.1.1.1"><times id="S5.p5.1.m1.1.1.1.1.2.cmml" xref="S5.p5.1.m1.1.1.1.1.2"></times><ci id="S5.p5.1.m1.1.1.1.1.3.cmml" xref="S5.p5.1.m1.1.1.1.1.3">Ξ</ci><apply id="S5.p5.1.m1.1.1.1.1.1.1.1.cmml" xref="S5.p5.1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.p5.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S5.p5.1.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S5.p5.1.m1.1.1.1.1.1.1.1.2.cmml" xref="S5.p5.1.m1.1.1.1.1.1.1.1.2">𝐩</ci><ci id="S5.p5.1.m1.1.1.1.1.1.1.1.3.cmml" xref="S5.p5.1.m1.1.1.1.1.1.1.1.3">𝑘</ci></apply></apply></apply><apply id="S5.p5.1.m1.1.1.3.cmml" xref="S5.p5.1.m1.1.1.3"><partialdiff id="S5.p5.1.m1.1.1.3.1.cmml" xref="S5.p5.1.m1.1.1.3.1"></partialdiff><apply id="S5.p5.1.m1.1.1.3.2.cmml" xref="S5.p5.1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S5.p5.1.m1.1.1.3.2.1.cmml" xref="S5.p5.1.m1.1.1.3.2">subscript</csymbol><ci id="S5.p5.1.m1.1.1.3.2.2.cmml" xref="S5.p5.1.m1.1.1.3.2.2">𝐩</ci><ci id="S5.p5.1.m1.1.1.3.2.3.cmml" xref="S5.p5.1.m1.1.1.3.2.3">𝑘</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p5.1.m1.1c">\mathbf{p}_{k}^{\prime}=\frac{\partial\Xi(\mathbf{p}_{k})}{\partial\mathbf{p}_%
{k}}</annotation><annotation encoding="application/x-llamapun" id="S5.p5.1.m1.1d">bold_p start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = divide start_ARG ∂ roman_Ξ ( bold_p start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) end_ARG start_ARG ∂ bold_p start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG</annotation></semantics></math> can be efficiently acquired from P-ESDF, and <math alttext="c(\mathbf{p}_{k})" class="ltx_Math" display="inline" id="S5.p5.2.m2.1"><semantics id="S5.p5.2.m2.1a"><mrow id="S5.p5.2.m2.1.1" xref="S5.p5.2.m2.1.1.cmml"><mi id="S5.p5.2.m2.1.1.3" xref="S5.p5.2.m2.1.1.3.cmml">c</mi><mo id="S5.p5.2.m2.1.1.2" xref="S5.p5.2.m2.1.1.2.cmml">⁢</mo><mrow id="S5.p5.2.m2.1.1.1.1" xref="S5.p5.2.m2.1.1.1.1.1.cmml"><mo id="S5.p5.2.m2.1.1.1.1.2" stretchy="false" xref="S5.p5.2.m2.1.1.1.1.1.cmml">(</mo><msub id="S5.p5.2.m2.1.1.1.1.1" xref="S5.p5.2.m2.1.1.1.1.1.cmml"><mi id="S5.p5.2.m2.1.1.1.1.1.2" xref="S5.p5.2.m2.1.1.1.1.1.2.cmml">𝐩</mi><mi id="S5.p5.2.m2.1.1.1.1.1.3" xref="S5.p5.2.m2.1.1.1.1.1.3.cmml">k</mi></msub><mo id="S5.p5.2.m2.1.1.1.1.3" stretchy="false" xref="S5.p5.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.p5.2.m2.1b"><apply id="S5.p5.2.m2.1.1.cmml" xref="S5.p5.2.m2.1.1"><times id="S5.p5.2.m2.1.1.2.cmml" xref="S5.p5.2.m2.1.1.2"></times><ci id="S5.p5.2.m2.1.1.3.cmml" xref="S5.p5.2.m2.1.1.3">𝑐</ci><apply id="S5.p5.2.m2.1.1.1.1.1.cmml" xref="S5.p5.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S5.p5.2.m2.1.1.1.1.1.1.cmml" xref="S5.p5.2.m2.1.1.1.1">subscript</csymbol><ci id="S5.p5.2.m2.1.1.1.1.1.2.cmml" xref="S5.p5.2.m2.1.1.1.1.1.2">𝐩</ci><ci id="S5.p5.2.m2.1.1.1.1.1.3.cmml" xref="S5.p5.2.m2.1.1.1.1.1.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p5.2.m2.1c">c(\mathbf{p}_{k})</annotation><annotation encoding="application/x-llamapun" id="S5.p5.2.m2.1d">italic_c ( bold_p start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT )</annotation></semantics></math> can be expressed as:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="c(\mathbf{p}_{k})=\begin{cases}\displaystyle\frac{1}{2\rho}(\Xi(\mathbf{p}_{k}%
)-\rho)^{2},\Xi(p_{k})\leq\rho\\
0\hfill,\Xi(\mathbf{p}_{k})&gt;\rho\end{cases}" class="ltx_Math" display="block" id="S5.E3.m1.3"><semantics id="S5.E3.m1.3a"><mrow id="S5.E3.m1.3.3" xref="S5.E3.m1.3.3.cmml"><mrow id="S5.E3.m1.3.3.1" xref="S5.E3.m1.3.3.1.cmml"><mi id="S5.E3.m1.3.3.1.3" xref="S5.E3.m1.3.3.1.3.cmml">c</mi><mo id="S5.E3.m1.3.3.1.2" xref="S5.E3.m1.3.3.1.2.cmml">⁢</mo><mrow id="S5.E3.m1.3.3.1.1.1" xref="S5.E3.m1.3.3.1.1.1.1.cmml"><mo id="S5.E3.m1.3.3.1.1.1.2" stretchy="false" xref="S5.E3.m1.3.3.1.1.1.1.cmml">(</mo><msub id="S5.E3.m1.3.3.1.1.1.1" xref="S5.E3.m1.3.3.1.1.1.1.cmml"><mi id="S5.E3.m1.3.3.1.1.1.1.2" xref="S5.E3.m1.3.3.1.1.1.1.2.cmml">𝐩</mi><mi id="S5.E3.m1.3.3.1.1.1.1.3" xref="S5.E3.m1.3.3.1.1.1.1.3.cmml">k</mi></msub><mo id="S5.E3.m1.3.3.1.1.1.3" stretchy="false" xref="S5.E3.m1.3.3.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S5.E3.m1.3.3.2" xref="S5.E3.m1.3.3.2.cmml">=</mo><mrow id="S5.E3.m1.2.2" xref="S5.E3.m1.3.3.3.1.cmml"><mo id="S5.E3.m1.2.2.3" xref="S5.E3.m1.3.3.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" id="S5.E3.m1.2.2.2" rowspacing="0pt" xref="S5.E3.m1.3.3.3.1.cmml"><mtr id="S5.E3.m1.2.2.2a" xref="S5.E3.m1.3.3.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S5.E3.m1.2.2.2b" xref="S5.E3.m1.3.3.3.1.cmml"><mrow id="S5.E3.m1.1.1.1.1.1.1" xref="S5.E3.m1.1.1.1.1.1.1.cmml"><mrow id="S5.E3.m1.1.1.1.1.1.1.2.2" xref="S5.E3.m1.1.1.1.1.1.1.2.3.cmml"><mrow id="S5.E3.m1.1.1.1.1.1.1.1.1.1" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.cmml"><mfrac id="S5.E3.m1.1.1.1.1.1.1.1.1.1.3" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mn id="S5.E3.m1.1.1.1.1.1.1.1.1.1.3.2" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.3.2.cmml">1</mn><mrow id="S5.E3.m1.1.1.1.1.1.1.1.1.1.3.3" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.3.3.cmml"><mn id="S5.E3.m1.1.1.1.1.1.1.1.1.1.3.3.2" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.3.3.2.cmml">2</mn><mo id="S5.E3.m1.1.1.1.1.1.1.1.1.1.3.3.1" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S5.E3.m1.1.1.1.1.1.1.1.1.1.3.3.3" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.3.3.3.cmml">ρ</mi></mrow></mfrac><mo id="S5.E3.m1.1.1.1.1.1.1.1.1.1.2" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.2.cmml">⁢</mo><msup id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" mathvariant="normal" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">Ξ</mi><mo id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">𝐩</mi><mi id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">k</mi></msub><mo id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">−</mo><mi id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">ρ</mi></mrow><mo id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mn id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.3.cmml">2</mn></msup></mrow><mo id="S5.E3.m1.1.1.1.1.1.1.2.2.3" xref="S5.E3.m1.1.1.1.1.1.1.2.3.cmml">,</mo><mrow id="S5.E3.m1.1.1.1.1.1.1.2.2.2" xref="S5.E3.m1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S5.E3.m1.1.1.1.1.1.1.2.2.2.3" mathvariant="normal" xref="S5.E3.m1.1.1.1.1.1.1.2.2.2.3.cmml">Ξ</mi><mo id="S5.E3.m1.1.1.1.1.1.1.2.2.2.2" xref="S5.E3.m1.1.1.1.1.1.1.2.2.2.2.cmml">⁢</mo><mrow id="S5.E3.m1.1.1.1.1.1.1.2.2.2.1.1" xref="S5.E3.m1.1.1.1.1.1.1.2.2.2.1.1.1.cmml"><mo id="S5.E3.m1.1.1.1.1.1.1.2.2.2.1.1.2" stretchy="false" xref="S5.E3.m1.1.1.1.1.1.1.2.2.2.1.1.1.cmml">(</mo><msub id="S5.E3.m1.1.1.1.1.1.1.2.2.2.1.1.1" xref="S5.E3.m1.1.1.1.1.1.1.2.2.2.1.1.1.cmml"><mi id="S5.E3.m1.1.1.1.1.1.1.2.2.2.1.1.1.2" xref="S5.E3.m1.1.1.1.1.1.1.2.2.2.1.1.1.2.cmml">p</mi><mi id="S5.E3.m1.1.1.1.1.1.1.2.2.2.1.1.1.3" xref="S5.E3.m1.1.1.1.1.1.1.2.2.2.1.1.1.3.cmml">k</mi></msub><mo id="S5.E3.m1.1.1.1.1.1.1.2.2.2.1.1.3" stretchy="false" xref="S5.E3.m1.1.1.1.1.1.1.2.2.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S5.E3.m1.1.1.1.1.1.1.3" xref="S5.E3.m1.1.1.1.1.1.1.3.cmml">≤</mo><mi id="S5.E3.m1.1.1.1.1.1.1.4" xref="S5.E3.m1.1.1.1.1.1.1.4.cmml">ρ</mi></mrow></mtd><mtd id="S5.E3.m1.2.2.2c" xref="S5.E3.m1.3.3.3.1.1.cmml"></mtd></mtr><mtr id="S5.E3.m1.2.2.2d" xref="S5.E3.m1.3.3.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S5.E3.m1.2.2.2e" xref="S5.E3.m1.3.3.3.1.cmml"><mrow id="S5.E3.m1.2.2.2.2.1.1" xref="S5.E3.m1.2.2.2.2.1.1.cmml"><mrow id="S5.E3.m1.2.2.2.2.1.1.2.1" xref="S5.E3.m1.2.2.2.2.1.1.2.2.cmml"><mn id="S5.E3.m1.2.2.2.2.1.1.1" xref="S5.E3.m1.2.2.2.2.1.1.1.cmml">0</mn><mo id="S5.E3.m1.2.2.2.2.1.1.2.1.2" xref="S5.E3.m1.2.2.2.2.1.1.2.2.cmml">,</mo><mrow id="S5.E3.m1.2.2.2.2.1.1.2.1.1" xref="S5.E3.m1.2.2.2.2.1.1.2.1.1.cmml"><mi id="S5.E3.m1.2.2.2.2.1.1.2.1.1.3" mathvariant="normal" xref="S5.E3.m1.2.2.2.2.1.1.2.1.1.3.cmml">Ξ</mi><mo id="S5.E3.m1.2.2.2.2.1.1.2.1.1.2" xref="S5.E3.m1.2.2.2.2.1.1.2.1.1.2.cmml">⁢</mo><mrow id="S5.E3.m1.2.2.2.2.1.1.2.1.1.1.1" xref="S5.E3.m1.2.2.2.2.1.1.2.1.1.1.1.1.cmml"><mo id="S5.E3.m1.2.2.2.2.1.1.2.1.1.1.1.2" stretchy="false" xref="S5.E3.m1.2.2.2.2.1.1.2.1.1.1.1.1.cmml">(</mo><msub id="S5.E3.m1.2.2.2.2.1.1.2.1.1.1.1.1" xref="S5.E3.m1.2.2.2.2.1.1.2.1.1.1.1.1.cmml"><mi id="S5.E3.m1.2.2.2.2.1.1.2.1.1.1.1.1.2" xref="S5.E3.m1.2.2.2.2.1.1.2.1.1.1.1.1.2.cmml">𝐩</mi><mi id="S5.E3.m1.2.2.2.2.1.1.2.1.1.1.1.1.3" xref="S5.E3.m1.2.2.2.2.1.1.2.1.1.1.1.1.3.cmml">k</mi></msub><mo id="S5.E3.m1.2.2.2.2.1.1.2.1.1.1.1.3" stretchy="false" xref="S5.E3.m1.2.2.2.2.1.1.2.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S5.E3.m1.2.2.2.2.1.1.3" xref="S5.E3.m1.2.2.2.2.1.1.3.cmml">&gt;</mo><mi id="S5.E3.m1.2.2.2.2.1.1.4" xref="S5.E3.m1.2.2.2.2.1.1.4.cmml">ρ</mi></mrow></mtd><mtd id="S5.E3.m1.2.2.2f" xref="S5.E3.m1.3.3.3.1.1.cmml"></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.E3.m1.3b"><apply id="S5.E3.m1.3.3.cmml" xref="S5.E3.m1.3.3"><eq id="S5.E3.m1.3.3.2.cmml" xref="S5.E3.m1.3.3.2"></eq><apply id="S5.E3.m1.3.3.1.cmml" xref="S5.E3.m1.3.3.1"><times id="S5.E3.m1.3.3.1.2.cmml" xref="S5.E3.m1.3.3.1.2"></times><ci id="S5.E3.m1.3.3.1.3.cmml" xref="S5.E3.m1.3.3.1.3">𝑐</ci><apply id="S5.E3.m1.3.3.1.1.1.1.cmml" xref="S5.E3.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="S5.E3.m1.3.3.1.1.1.1.1.cmml" xref="S5.E3.m1.3.3.1.1.1">subscript</csymbol><ci id="S5.E3.m1.3.3.1.1.1.1.2.cmml" xref="S5.E3.m1.3.3.1.1.1.1.2">𝐩</ci><ci id="S5.E3.m1.3.3.1.1.1.1.3.cmml" xref="S5.E3.m1.3.3.1.1.1.1.3">𝑘</ci></apply></apply><apply id="S5.E3.m1.3.3.3.1.cmml" xref="S5.E3.m1.2.2"><csymbol cd="latexml" id="S5.E3.m1.3.3.3.1.1.cmml" xref="S5.E3.m1.2.2.3">cases</csymbol><apply id="S5.E3.m1.1.1.1.1.1.1.cmml" xref="S5.E3.m1.1.1.1.1.1.1"><leq id="S5.E3.m1.1.1.1.1.1.1.3.cmml" xref="S5.E3.m1.1.1.1.1.1.1.3"></leq><list id="S5.E3.m1.1.1.1.1.1.1.2.3.cmml" xref="S5.E3.m1.1.1.1.1.1.1.2.2"><apply id="S5.E3.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1"><times id="S5.E3.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.2"></times><apply id="S5.E3.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.3"><divide id="S5.E3.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.3"></divide><cn id="S5.E3.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" type="integer" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.3.2">1</cn><apply id="S5.E3.m1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.3.3"><times id="S5.E3.m1.1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.3.3.1"></times><cn id="S5.E3.m1.1.1.1.1.1.1.1.1.1.3.3.2.cmml" type="integer" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.3.3.2">2</cn><ci id="S5.E3.m1.1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.3.3.3">𝜌</ci></apply></apply><apply id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1"><minus id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2"></minus><apply id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1"><times id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2"></times><ci id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">Ξ</ci><apply id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">𝐩</ci><ci id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">𝑘</ci></apply></apply><ci id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">𝜌</ci></apply><cn id="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S5.E3.m1.1.1.1.1.1.1.1.1.1.1.3">2</cn></apply></apply><apply id="S5.E3.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S5.E3.m1.1.1.1.1.1.1.2.2.2"><times id="S5.E3.m1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S5.E3.m1.1.1.1.1.1.1.2.2.2.2"></times><ci id="S5.E3.m1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S5.E3.m1.1.1.1.1.1.1.2.2.2.3">Ξ</ci><apply id="S5.E3.m1.1.1.1.1.1.1.2.2.2.1.1.1.cmml" xref="S5.E3.m1.1.1.1.1.1.1.2.2.2.1.1"><csymbol cd="ambiguous" id="S5.E3.m1.1.1.1.1.1.1.2.2.2.1.1.1.1.cmml" xref="S5.E3.m1.1.1.1.1.1.1.2.2.2.1.1">subscript</csymbol><ci id="S5.E3.m1.1.1.1.1.1.1.2.2.2.1.1.1.2.cmml" xref="S5.E3.m1.1.1.1.1.1.1.2.2.2.1.1.1.2">𝑝</ci><ci id="S5.E3.m1.1.1.1.1.1.1.2.2.2.1.1.1.3.cmml" xref="S5.E3.m1.1.1.1.1.1.1.2.2.2.1.1.1.3">𝑘</ci></apply></apply></list><ci id="S5.E3.m1.1.1.1.1.1.1.4.cmml" xref="S5.E3.m1.1.1.1.1.1.1.4">𝜌</ci></apply><ci id="S5.E3.m1.3.3.3.1.3a.cmml" xref="S5.E3.m1.2.2"><mtext class="ltx_mathvariant_italic" id="S5.E3.m1.3.3.3.1.3.cmml" xref="S5.E3.m1.2.2.3">otherwise</mtext></ci><apply id="S5.E3.m1.2.2.2.2.1.1.cmml" xref="S5.E3.m1.2.2.2.2.1.1"><gt id="S5.E3.m1.2.2.2.2.1.1.3.cmml" xref="S5.E3.m1.2.2.2.2.1.1.3"></gt><list id="S5.E3.m1.2.2.2.2.1.1.2.2.cmml" xref="S5.E3.m1.2.2.2.2.1.1.2.1"><cn id="S5.E3.m1.2.2.2.2.1.1.1.cmml" type="integer" xref="S5.E3.m1.2.2.2.2.1.1.1">0</cn><apply id="S5.E3.m1.2.2.2.2.1.1.2.1.1.cmml" xref="S5.E3.m1.2.2.2.2.1.1.2.1.1"><times id="S5.E3.m1.2.2.2.2.1.1.2.1.1.2.cmml" xref="S5.E3.m1.2.2.2.2.1.1.2.1.1.2"></times><ci id="S5.E3.m1.2.2.2.2.1.1.2.1.1.3.cmml" xref="S5.E3.m1.2.2.2.2.1.1.2.1.1.3">Ξ</ci><apply id="S5.E3.m1.2.2.2.2.1.1.2.1.1.1.1.1.cmml" xref="S5.E3.m1.2.2.2.2.1.1.2.1.1.1.1"><csymbol cd="ambiguous" id="S5.E3.m1.2.2.2.2.1.1.2.1.1.1.1.1.1.cmml" xref="S5.E3.m1.2.2.2.2.1.1.2.1.1.1.1">subscript</csymbol><ci id="S5.E3.m1.2.2.2.2.1.1.2.1.1.1.1.1.2.cmml" xref="S5.E3.m1.2.2.2.2.1.1.2.1.1.1.1.1.2">𝐩</ci><ci id="S5.E3.m1.2.2.2.2.1.1.2.1.1.1.1.1.3.cmml" xref="S5.E3.m1.2.2.2.2.1.1.2.1.1.1.1.1.3">𝑘</ci></apply></apply></list><ci id="S5.E3.m1.2.2.2.2.1.1.4.cmml" xref="S5.E3.m1.2.2.2.2.1.1.4">𝜌</ci></apply><ci id="S5.E3.m1.3.3.3.1.5a.cmml" xref="S5.E3.m1.2.2"><mtext class="ltx_mathvariant_italic" id="S5.E3.m1.3.3.3.1.5.cmml" xref="S5.E3.m1.2.2.3">otherwise</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E3.m1.3c">c(\mathbf{p}_{k})=\begin{cases}\displaystyle\frac{1}{2\rho}(\Xi(\mathbf{p}_{k}%
)-\rho)^{2},\Xi(p_{k})\leq\rho\\
0\hfill,\Xi(\mathbf{p}_{k})&gt;\rho\end{cases}</annotation><annotation encoding="application/x-llamapun" id="S5.E3.m1.3d">italic_c ( bold_p start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) = { start_ROW start_CELL divide start_ARG 1 end_ARG start_ARG 2 italic_ρ end_ARG ( roman_Ξ ( bold_p start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) - italic_ρ ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , roman_Ξ ( italic_p start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ≤ italic_ρ end_CELL start_CELL end_CELL end_ROW start_ROW start_CELL 0 , roman_Ξ ( bold_p start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) &gt; italic_ρ end_CELL start_CELL end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S5.p6">
<p class="ltx_p" id="S5.p6.1">The result is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S5.F3" title="Figure 3 ‣ V Perception-aware motion planning ‣ Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Experiments</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">To demonstrate the effectiveness of our system from different dimensions, we conduct <math alttext="3" class="ltx_Math" display="inline" id="S6.p1.1.m1.1"><semantics id="S6.p1.1.m1.1a"><mn id="S6.p1.1.m1.1.1" xref="S6.p1.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S6.p1.1.m1.1b"><cn id="S6.p1.1.m1.1.1.cmml" type="integer" xref="S6.p1.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.m1.1c">3</annotation><annotation encoding="application/x-llamapun" id="S6.p1.1.m1.1d">3</annotation></semantics></math> tasks in simulated environments with varied scales and complexity, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S6.F4" title="Figure 4 ‣ VI-B Evaluation of 3D Perception Guidance Field Generation ‣ VI Experiments ‣ Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_tag">4</span></a>.
The first task is to estimate a static challenging pose, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S6.F4" title="Figure 4 ‣ VI-B Evaluation of 3D Perception Guidance Field Generation ‣ VI Experiments ‣ Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_tag">4</span></a> (a).
The later two tasks are to estimate the human pose online during walking in the forests, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S6.F4" title="Figure 4 ‣ VI-B Evaluation of 3D Perception Guidance Field Generation ‣ VI Experiments ‣ Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_tag">4</span></a> (b-c). The drone needs to simutaneously track the person, choose the best view-point, avoid occlusion and ensure the safety.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS1.5.1.1">VI-A</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS1.6.2">Implementation Details</span>
</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.2">The implementation of PoseErrNet is an autoencoder with 4 layers in the encoder layers and 4 layers in the decoder layers.
The TRT-Pose is utilized for 2-D human pose estimation. It runs on NVIDIA RTX 3070Ti GPU with 15 HZ update rate.
The simulated vehicle is equipped with an Intel D435 depth camera, which is used both as the range sensor for navigation planning and camera sensor for RGB images.
The onboard autonomy system of the UAV integrates several key navigation modules from the development environment of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib43" title="">43</a>]</cite>. These include kinodynamic path search, mapping module and GUI. These components serve as fundamental navigation modules. The proposed perception-aware planner is on the top of the navigation system.
The framework runs on a laptop with i7-12700H CPU. We configure the navigation system to update at 15Hz and perform trajectory optimization at each sensor update. The spatial resolution is set as <math alttext="0.2m" class="ltx_Math" display="inline" id="S6.SS1.p1.1.m1.1"><semantics id="S6.SS1.p1.1.m1.1a"><mrow id="S6.SS1.p1.1.m1.1.1" xref="S6.SS1.p1.1.m1.1.1.cmml"><mn id="S6.SS1.p1.1.m1.1.1.2" xref="S6.SS1.p1.1.m1.1.1.2.cmml">0.2</mn><mo id="S6.SS1.p1.1.m1.1.1.1" xref="S6.SS1.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p1.1.m1.1.1.3" xref="S6.SS1.p1.1.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.1.m1.1b"><apply id="S6.SS1.p1.1.m1.1.1.cmml" xref="S6.SS1.p1.1.m1.1.1"><times id="S6.SS1.p1.1.m1.1.1.1.cmml" xref="S6.SS1.p1.1.m1.1.1.1"></times><cn id="S6.SS1.p1.1.m1.1.1.2.cmml" type="float" xref="S6.SS1.p1.1.m1.1.1.2">0.2</cn><ci id="S6.SS1.p1.1.m1.1.1.3.cmml" xref="S6.SS1.p1.1.m1.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.1.m1.1c">0.2m</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p1.1.m1.1d">0.2 italic_m</annotation></semantics></math>. The P-ESDF is set a <math alttext="10\times 10m" class="ltx_Math" display="inline" id="S6.SS1.p1.2.m2.1"><semantics id="S6.SS1.p1.2.m2.1a"><mrow id="S6.SS1.p1.2.m2.1.1" xref="S6.SS1.p1.2.m2.1.1.cmml"><mrow id="S6.SS1.p1.2.m2.1.1.2" xref="S6.SS1.p1.2.m2.1.1.2.cmml"><mn id="S6.SS1.p1.2.m2.1.1.2.2" xref="S6.SS1.p1.2.m2.1.1.2.2.cmml">10</mn><mo id="S6.SS1.p1.2.m2.1.1.2.1" lspace="0.222em" rspace="0.222em" xref="S6.SS1.p1.2.m2.1.1.2.1.cmml">×</mo><mn id="S6.SS1.p1.2.m2.1.1.2.3" xref="S6.SS1.p1.2.m2.1.1.2.3.cmml">10</mn></mrow><mo id="S6.SS1.p1.2.m2.1.1.1" xref="S6.SS1.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p1.2.m2.1.1.3" xref="S6.SS1.p1.2.m2.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.2.m2.1b"><apply id="S6.SS1.p1.2.m2.1.1.cmml" xref="S6.SS1.p1.2.m2.1.1"><times id="S6.SS1.p1.2.m2.1.1.1.cmml" xref="S6.SS1.p1.2.m2.1.1.1"></times><apply id="S6.SS1.p1.2.m2.1.1.2.cmml" xref="S6.SS1.p1.2.m2.1.1.2"><times id="S6.SS1.p1.2.m2.1.1.2.1.cmml" xref="S6.SS1.p1.2.m2.1.1.2.1"></times><cn id="S6.SS1.p1.2.m2.1.1.2.2.cmml" type="integer" xref="S6.SS1.p1.2.m2.1.1.2.2">10</cn><cn id="S6.SS1.p1.2.m2.1.1.2.3.cmml" type="integer" xref="S6.SS1.p1.2.m2.1.1.2.3">10</cn></apply><ci id="S6.SS1.p1.2.m2.1.1.3.cmml" xref="S6.SS1.p1.2.m2.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.2.m2.1c">10\times 10m</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p1.2.m2.1d">10 × 10 italic_m</annotation></semantics></math> area with the vehicle in the center.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS2.5.1.1">VI-B</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS2.6.2">Evaluation of 3D Perception Guidance Field Generation</span>
</h3>
<figure class="ltx_table" id="S6.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T2.1" style="width:260.2pt;height:55.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-38.6pt,8.2pt) scale(0.771203917766134,0.771203917766134) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T2.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T2.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" id="S6.T2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T2.1.1.1.1.1.1">Perturbation</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T2.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T2.1.1.1.1.2.1">Translation (%)</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T2.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S6.T2.1.1.1.1.3.1">Rotation (%)</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T2.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S6.T2.1.1.1.1.4.1">Scale (%)</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T2.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S6.T2.1.1.1.1.5.1">All (%)</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S6.T2.1.1.2.2.1">T1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.1.1.2.2.2">5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.1.1.2.2.3">8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.1.1.2.2.4">12</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.1.1.2.2.5">17</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S6.T2.1.1.3.3.1">T2</th>
<td class="ltx_td ltx_align_center" id="S6.T2.1.1.3.3.2">11</td>
<td class="ltx_td ltx_align_center" id="S6.T2.1.1.3.3.3">12</td>
<td class="ltx_td ltx_align_center" id="S6.T2.1.1.3.3.4">16</td>
<td class="ltx_td ltx_align_center" id="S6.T2.1.1.3.3.5">25</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S6.T2.1.1.4.4.1">T3</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T2.1.1.4.4.2">19</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T2.1.1.4.4.3">18</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T2.1.1.4.4.4">21</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T2.1.1.4.4.5">34</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Evaluation of Robustness of Pose Normalization for 3 perturbation levels T1, T2, and T3. The translation-only results (<span class="ltx_text ltx_font_italic" id="S6.T2.6.1">Translation</span>), the rotation-only results (<span class="ltx_text ltx_font_italic" id="S6.T2.7.2">Roataion</span>), the scale-only results (<span class="ltx_text ltx_font_italic" id="S6.T2.8.3">Scale</span>) and combing all translation, rotation, scale results (<span class="ltx_text ltx_font_italic" id="S6.T2.9.4">All</span>). Here we show after input keypoint normalization how many percentages of PoseErrNet output change due to input keypoint perturbation. </figcaption>
</figure>
<figure class="ltx_figure" id="S6.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="716" id="S6.F4.g1" src="x5.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The testing environment for the proposed motion planning framework.</figcaption>
</figure>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">To demonstrate the performance of the proposed 3D perception guidance field generation, we showcase an example in challenging real-world scenarios as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S6.F5" title="Figure 5 ‣ VI-B Evaluation of 3D Perception Guidance Field Generation ‣ VI Experiments ‣ Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1">In Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S6.F5" title="Figure 5 ‣ VI-B Evaluation of 3D Perception Guidance Field Generation ‣ VI Experiments ‣ Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_tag">5</span></a>, the algorithm begins by normalizing the detected imperfect 2D human keypoints, as outlined in the methodology section (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S4" title="IV Mapping 2D Observations to 3D perception guidance fields ‣ Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_tag">IV</span></a>). The robustness of our PoseErrNet’s output against variations in scale, translation, and rotation of the input keypoint detection is enhanced by this normalization process. We captured a real-world video with a hand-held camera following a human subject and selected 146 representative frames. To these frames, we applied three levels of perturbation: T1 involves uniform random translation ranging from 0 to 5 pixels, uniform random rotation from 0 to 5 degrees, and uniform random scaling from 1.0 to 1.05; T2 includes uniform random translation from 0 to 10 pixels, uniform random rotation from 0 to 10 degrees, and uniform random scaling from 1.0 to 1.10; T3 consists of uniform random translation from 0 to 20 pixels, uniform random rotation from 0 to 20 degrees, and uniform random scaling from 1.0 to 1.15. As illustrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S6.T2" title="TABLE II ‣ VI-B Evaluation of 3D Perception Guidance Field Generation ‣ VI Experiments ‣ Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_tag">II</span></a>, we quantized the camera viewing angle error into 21 bins within its range and reported the percentage of bin changes from the results without perturbation for all camera viewing angle error predictions from PoseErrNet. Due to the input normalization, our PoseErrNet’s output demonstrates robustness under various levels of input keypoint perturbations.</p>
</div>
<div class="ltx_para" id="S6.SS2.p3">
<p class="ltx_p" id="S6.SS2.p3.1">This normalized keypoints detection data is then input into PoseErrNet to predict the 3D perception guidance field. As an example in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S6.F5" title="Figure 5 ‣ VI-B Evaluation of 3D Perception Guidance Field Generation ‣ VI Experiments ‣ Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_tag">5</span></a> from our real-world collected video frames, where a person moves forward while raising their right arm, obstructing their face. Viewpoints directly in front of the individual are assigned lower costs due to their superior visibility. In contrast, viewpoints from behind are usually associated with higher costs, as they are more prone to occluding important features like the face and arms. The error increases on the right-hand side due to occlusion caused by the raised arm. The left to the front side also with high error because the person’s left leg moving forward creates self-occlusion of the right leg. Meanwhile, the area from the left to the back side exhibits the lowest error, indicating the best candidate camera viewing angles for this scenario.</p>
</div>
<figure class="ltx_figure" id="S6.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="529" id="S6.F5.g1" src="x6.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The result for 3D perception guidance field prediction on the example real-world data. Each arrow within the 3D perception guidance field represents a camera viewing angle. Colors are utilized to denote the error of HPE at a camera viewing angle. </figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS3.5.1.1">VI-C</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS3.6.2">Perception-aware Motion Planning Experiment</span>
</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">To evaluate the efficacy of the On-drone Perception-aware Motion Planning method, we conducted an experimental implementation in complex, cluttered environments characterized by numerous obstacles, as depicted in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S6.F4" title="Figure 4 ‣ VI-B Evaluation of 3D Perception Guidance Field Generation ‣ VI Experiments ‣ Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para" id="S6.SS3.p2">
<p class="ltx_p" id="S6.SS3.p2.1">As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S6.F7" title="Figure 7 ‣ VI-D System-level Comparison ‣ VI Experiments ‣ Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_tag">7</span></a> and Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S6.F6" title="Figure 6 ‣ VI-C Perception-aware Motion Planning Experiment ‣ VI Experiments ‣ Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_tag">6</span></a>, in obstacle-free environments, the UAV consistently adheres to the optimal viewpoint. However, upon encountering obstacles, the UAV demonstrates the capability to maintain this optimal viewpoint while simultaneously navigating around the obstructions. In scenarios where obstacles are proximate or densely situated, the planning framework proactively shifts to the second-best viewpoint. This adjustment is crucial for mitigating occlusion issues between the target and the UAV, and for maintaining safety by avoiding obstacles. Upon successful navigation past obstacles, and making sure that occlusions do not obstruct the view of the target, the UAV seamlessly plans and executes a trajectory to return to the primary, most advantageous viewpoint.</p>
</div>
<figure class="ltx_figure" id="S6.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="527" id="S6.F6.g1" src="x7.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Perception-aware motion planning experiment. <span class="ltx_text ltx_font_bold" id="S6.F6.5.1">(a): </span>The UAV follows the best viewpoint (green arrow). <span class="ltx_text ltx_font_bold" id="S6.F6.6.2">(b): </span>The UAV can still follow the best viewpoint and while avoiding the obstacle. <span class="ltx_text ltx_font_bold" id="S6.F6.7.3">(c-d): </span>To avoid the occlusion between the target and the UAV, and also to avoid the obstacles for safety, the proposed planning framework automatically switches to the second best viewpoint. <span class="ltx_text ltx_font_bold" id="S6.F6.8.4">(e-f): </span>After avoiding the obstacle, and if there is no occlusion between the target and the UAV, the UAV plans a smooth trajectory to return to the best viewpoint.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS4.5.1.1">VI-D</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS4.6.2">System-level Comparison</span>
</h3>
<figure class="ltx_figure" id="S6.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="232" id="S6.F7.g1" src="x8.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Demonstration of the perception-aware motion planning. The upper row in each figure is an image captured using the onboard camera, and the bottom row illustrates the third-person view of the experiment. The 3D perception guidance field is represented as a hemispherical field around the drone with different colors indicating the quality of the view point (from green to red indicating good to poor). The green arrow means the best viewpoint evaluated altogether from the pose estimation accuracy, occlusion, and flight safety. <span class="ltx_text ltx_font_bold" id="S6.F7.3.1">(a - b): </span>The best viewpoint (green arrow) changes with the human pose, and the UAV follows the best view point in real-time. <span class="ltx_text ltx_font_bold" id="S6.F7.4.2">(c - f): </span>When both target and obstacles are observed in the environment, the UAV automatically chooses the second best viewpoint to avoid occlusions and ensure safety.</figcaption>
</figure>
<figure class="ltx_table" id="S6.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T3.1" style="width:346.9pt;height:100.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-13.6pt,3.9pt) scale(0.927118192301036,0.927118192301036) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T3.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T3.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S6.T3.1.1.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.1.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2" id="S6.T3.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.1.1.2.1">Challenging Pose</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2" id="S6.T3.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.1.1.3.1">Large-Scale</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2" id="S6.T3.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.1.1.4.1">Dense</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2" id="S6.T3.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.1.1.5.1">All</span></th>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S6.T3.1.1.2.2.1">PCK</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S6.T3.1.1.2.2.2">MSE</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S6.T3.1.1.2.2.3">PCK</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S6.T3.1.1.2.2.4">MSE</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S6.T3.1.1.2.2.5">PCK</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S6.T3.1.1.2.2.6">MSE</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S6.T3.1.1.2.2.7">PCK</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S6.T3.1.1.2.2.8">MSE</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T3.1.1.3.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T3.1.1.3.1.1">Auto-Filmer - Front</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.3.1.2">0.87</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.3.1.3">31.46</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.3.1.4"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.3.1.4.1">0.91</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.3.1.5"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.3.1.5.1">24.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.3.1.6">0.60</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.3.1.7">42.41</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.3.1.8">0.79</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.3.1.9">32.66</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.4.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T3.1.1.4.2.1">Auto-Filmer - Side</th>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.4.2.2">0.93</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.4.2.3">30.73</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.4.2.4">0.78</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.4.2.5">25.59</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.4.2.6">0.80</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.4.2.7">37.89</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.4.2.8">0.84</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.4.2.9">31.40</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.5.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T3.1.1.5.3.1">Auto-Filmer - Back</th>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.5.3.2">0.80</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.5.3.3">29.20</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.5.3.4">0.67</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.5.3.5">34.31</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.5.3.6">0.67</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.5.3.7">37.51</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.5.3.8">0.71</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.5.3.9">33.67</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.6.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" id="S6.T3.1.1.6.4.1">Ours</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S6.T3.1.1.6.4.2"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.6.4.2.1">1.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S6.T3.1.1.6.4.3"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.6.4.3.1">22.60</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S6.T3.1.1.6.4.4">0.86</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S6.T3.1.1.6.4.5">24.47</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S6.T3.1.1.6.4.6"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.6.4.6.1">0.92</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S6.T3.1.1.6.4.7"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.6.4.7.1">23.61</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S6.T3.1.1.6.4.8"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.6.4.8.1">0.92</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S6.T3.1.1.6.4.9"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.6.4.9.1">23.56</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>PCK and MSE Evaluation for System-level Experiments</figcaption>
</figure>
<div class="ltx_para" id="S6.SS4.p1">
<p class="ltx_p" id="S6.SS4.p1.1">This experiment was designed to validate the robustness and overall performance of our integrated system. The perception aspects were simulated within Unity, while the control and motion planning components were simulated in ROS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib44" title="">44</a>]</cite> using Rviz <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#bib.bib45" title="">45</a>]</cite> for visual representation.
Two standard metrics are introduced to evaluate the performance: Percentage of Correct Key-points (PCK) and Mean Squared Error.</p>
</div>
<div class="ltx_para" id="S6.SS4.p2">
<p class="ltx_p" id="S6.SS4.p2.1">As indicated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S6.F7" title="Figure 7 ‣ VI-D System-level Comparison ‣ VI Experiments ‣ Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_tag">7</span></a> and Table. <a class="ltx_ref" href="https://arxiv.org/html/2407.01811v1#S6.T3" title="TABLE III ‣ VI-D System-level Comparison ‣ VI Experiments ‣ Active Human Pose Estimation via an Autonomous UAV Agent"><span class="ltx_text ltx_ref_tag">III</span></a>, the optimal viewpoint (represented by a green arrow) dynamically adjusts in response to changes in the human subject’s pose. The UAV is programmed to track and align with this best viewpoint in real-time, showcasing its responsiveness to the target’s movements. In scenarios where both the target and obstacles are present within the UAV’s operational environment, the system intelligently opts for the second-best viewpoint. This strategic choice is critical for avoiding visual occlusion between the UAV and the target, and for ensuring safety by steering clear of obstacles. This approach effectively demonstrates the system’s capability to adapt to varying environmental conditions while maintaining high-quality perception and safe navigation.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">The innovative approach detailed in this paper signifies a considerable leap forward in the domain of active 2D human pose estimation through the use of autonomous Unmanned Aerial Vehicles (UAVs). By weaving together a NeRF-based Drone-View Data Generation Framework, an On-Drone Network for Camera View Error Estimation, and a Combined Planner for strategic camera repositioning, our methodology effectively tackles the issue of self-occlusions in videos capturing human activities. This integrated system not only enhances the accuracy and reliability of human pose estimation from optimized camera viewing angles but also guarantees the adaptability and operational safety of drones across varied environments. The proposed method highlights the critical role of dynamic viewpoint optimization in elevating the quality of pose estimation, thereby paving new pathways for applications in sectors like aerial cinematography and surveillance. Experimental results from both simulation and real-world experiments prove the efficacy of each component within our system and, more importantly, demonstrate the enhanced task-level performance of the integrated system.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Claudia Stöcker, Rohan Bennett, Francesco Nex, Markus Gerke, and Jaap Zevenbergen.

</span>
<span class="ltx_bibblock">Review of the current state of uav regulations.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">Remote sensing</span>, 9(5):459, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Francesco Nex and Fabio Remondino.

</span>
<span class="ltx_bibblock">Uav for 3d mapping applications: a review.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">Applied geomatics</span>, 6:1–15, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Valerio Baiocchi, D Dominici, Martina Mormile, et al.

</span>
<span class="ltx_bibblock">Uav application in post-seismic environment.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences</span>, 40:21–26, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Xin Zhou, Xiangyong Wen, Zhepei Wang, Yuman Gao, Haojia Li, Qianhao Wang, Tiankai Yang, Haojian Lu, Yanjun Cao, Chao Xu, et al.

</span>
<span class="ltx_bibblock">Swarm of micro flying robots in the wild.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">Science Robotics</span>, 7(66):eabm5954, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Chung-Yi Weng, Brian Curless, Pratul P. Srinivasan, Jonathan T. Barron, and Ira Kemelmacher-Shlizerman.

</span>
<span class="ltx_bibblock">HumanNeRF: Free-viewpoint rendering of moving people from monocular video.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 16210–16220, June 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Xiaoxia Zhou, Zhepei Wang, Chao Xu, and Fei Gao.

</span>
<span class="ltx_bibblock">Ego-planner: An esdf-free gradient-based local planner for quadrotors.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">IEEE Robotics and Automation Letters</span>, 6:478–485, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Qianhao Wang, Botao He, Zhiren Xun, Chao Xu, and Fei Gao.

</span>
<span class="ltx_bibblock">Gpa-teleoperation: Gaze enhanced perception-aware safe assistive aerial teleoperation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">IEEE Robotics and Automation Letters</span>, 7(2):5631–5638, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Zhiwei Zhang, Yuhang Zhong, Junlong Guo, Qianhao Wang, Chao Xu, and Fei Gao.

</span>
<span class="ltx_bibblock">Auto filmer: Autonomous aerial videography under human interaction.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">IEEE Robotics and Automation Letters</span>, 8(2):784–791, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Niels Joubert, Mike Roberts, Anh Truong, Floraine Berthouzoz, and Pat Hanrahan.

</span>
<span class="ltx_bibblock">An interactive tool for designing quadrotor camera shots.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">ACM Transactions on Graphics (TOG)</span>, 34(6):1–11, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Niels Joubert, Dan B Goldman, Floraine Berthouzoz, Mike Roberts, James A Landay, Pat Hanrahan, et al.

</span>
<span class="ltx_bibblock">Towards a drone cinematographer: Guiding quadrotor cameras using visual composition principles.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:1610.01691</span>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Christoph Gebhardt, Benjamin Hepp, Tobias Nägeli, Stefan Stevšić, and Otmar Hilliges.

</span>
<span class="ltx_bibblock">Airways: Optimization-based planning of quadrotor trajectories according to high-level user goals.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 2016 chi conference on human factors in computing systems</span>, pages 2508–2519, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Christoph Gebhardt, Stefan Stevšić, and Otmar Hilliges.

</span>
<span class="ltx_bibblock">Optimizing for aesthetically pleasing quadrotor camera motion.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">ACM Transactions on Graphics (TOG)</span>, 37(4):1–11, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Ziquan Lan, Mohit Shridhar, David Hsu, and Shengdong Zhao.

</span>
<span class="ltx_bibblock">Xpose: Reinventing user interaction with flying cameras.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">Robotics: Science and Systems</span>, pages 1–9, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Hao Kang, Haoxiang Li, Jianming Zhang, Xin Lu, and Bedrich Benes.

</span>
<span class="ltx_bibblock">Flycam: Multitouch gesture controlled drone gimbal photography.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">IEEE Robotics and Automation Letters</span>, 3(4):3717–3724, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Boseong Felipe Jeon and H Jin Kim.

</span>
<span class="ltx_bibblock">Online trajectory generation of a mav for chasing a moving target in 3d dense environments.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</span>, pages 1115–1121. IEEE, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Jialin Ji, Neng Pan, Chao Xu, and Fei Gao.

</span>
<span class="ltx_bibblock">Elastic tracker: A spatio-temporal trajectory planner for flexible aerial tracking.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">2022 International Conference on Robotics and Automation (ICRA)</span>, page 47–53. IEEE Press, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Boseong Jeon, Yunwoo Lee, and H Jin Kim.

</span>
<span class="ltx_bibblock">Integrated motion planner for real-time aerial videography with a drone in a dense environment.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">2020 IEEE International Conference on Robotics and Automation (ICRA)</span>, pages 1243–1249. IEEE, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Qianhao Wang, Yuman Gao, Jialin Ji, Chao Xu, and Fei Gao.

</span>
<span class="ltx_bibblock">Visibility-aware trajectory optimization with application to aerial tracking.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</span>, pages 5249–5256. IEEE, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Xuecheng Nie, Jiashi Feng, Yiming Zuo, and Shuicheng Yan.

</span>
<span class="ltx_bibblock">Human pose estimation with parsing induced learner.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</span>, pages 2100–2108, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Xiao Chu, Wei Yang, Wanli Ouyang, Cheng Ma, Alan L Yuille, and Xiaogang Wang.

</span>
<span class="ltx_bibblock">Multi-context attention for human pose estimation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, pages 1831–1840, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Alejandro Newell, Kaiyu Yang, and Jia Deng.

</span>
<span class="ltx_bibblock">Stacked hourglass networks for human pose estimation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14</span>, pages 483–499. Springer, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Xi Peng, Zhiqiang Tang, Fei Yang, Rogerio S Feris, and Dimitris Metaxas.

</span>
<span class="ltx_bibblock">Jointly optimize data augmentation and network training: Adversarial data augmentation in human pose estimation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, pages 2226–2234, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Alexander Toshev and Christian Szegedy.

</span>
<span class="ltx_bibblock">Deeppose: Human pose estimation via deep neural networks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, pages 1653–1660, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Z. Cao, G. Hidalgo Martinez, T. Simon, S. Wei, and Y. A. Sheikh.

</span>
<span class="ltx_bibblock">Openpose: Realtime multi-person 2d pose estimation using part affinity fields.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng.

</span>
<span class="ltx_bibblock">Nerf: Representing scenes as neural radiance fields for view synthesis.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">Communications of the ACM</span>, 65(1):99–106, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P Srinivasan.

</span>
<span class="ltx_bibblock">Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pages 5855–5864, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Peter Hedman, Pratul P Srinivasan, Ben Mildenhall, Jonathan T Barron, and Paul Debevec.

</span>
<span class="ltx_bibblock">Baking neural radiance fields for real-time view synthesis.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pages 5875–5884, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Michael Niemeyer and Andreas Geiger.

</span>
<span class="ltx_bibblock">Giraffe: Representing scenes as compositional generative neural feature fields.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 11453–11464, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Pratul P Srinivasan, Boyang Deng, Xiuming Zhang, Matthew Tancik, Ben Mildenhall, and Jonathan T Barron.

</span>
<span class="ltx_bibblock">Nerv: Neural reflectance and visibility fields for relighting and view synthesis.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 7495–7504, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng.

</span>
<span class="ltx_bibblock">Fourier features let networks learn high frequency functions in low dimensional domains.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">Advances in neural information processing systems</span>, 33:7537–7547, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun.

</span>
<span class="ltx_bibblock">Nerf++: Analyzing and improving neural radiance fields.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">arXiv preprint arXiv:2010.07492</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Xiuming Zhang, Pratul P Srinivasan, Boyang Deng, Paul Debevec, William T Freeman, and Jonathan T Barron.

</span>
<span class="ltx_bibblock">Nerfactor: Neural factorization of shape and reflectance under an unknown illumination.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">ACM Transactions on Graphics (ToG)</span>, 40(6):1–18, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Chen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang.

</span>
<span class="ltx_bibblock">Dynamic view synthesis from dynamic monocular video.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pages 5712–5721, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang.

</span>
<span class="ltx_bibblock">Neural scene flow fields for space-time view synthesis of dynamic scenes.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 6498–6508, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-Brualla, and Steven M Seitz.

</span>
<span class="ltx_bibblock">Hypernerf: A higher-dimensional representation for topologically varying neural radiance fields.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">arXiv preprint arXiv:2106.13228</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer.

</span>
<span class="ltx_bibblock">D-nerf: Neural radiance fields for dynamic scenes.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 10318–10327, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollhöfer, Christoph Lassner, and Christian Theobalt.

</span>
<span class="ltx_bibblock">Non-rigid neural radiance fields: Reconstruction and novel view synthesis of a dynamic scene from monocular video.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pages 12959–12970, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Wenqi Xian, Jia-Bin Huang, Johannes Kopf, and Changil Kim.

</span>
<span class="ltx_bibblock">Space-time neural irradiance fields for free-viewpoint video.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 9421–9431, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Shih-Yang Su, Frank Yu, Michael Zollhöfer, and Helge Rhodin.

</span>
<span class="ltx_bibblock">A-nerf: Articulated neural radiance fields for learning human shape, appearance, and pose.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">Advances in Neural Information Processing Systems</span>, 34:12278–12291, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Hongyi Xu, Thiemo Alldieck, and Cristian Sminchisescu.

</span>
<span class="ltx_bibblock">H-nerf: Neural radiance fields for rendering and temporal reconstruction of humans in motion.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib40.1.1">Advances in Neural Information Processing Systems</span>, 34:14955–14966, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Blender.

</span>
<span class="ltx_bibblock">Blender.

</span>
<span class="ltx_bibblock">https://www.blender.org/, 2024.

</span>
<span class="ltx_bibblock">Accessed: 2024-03-05.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Unity.

</span>
<span class="ltx_bibblock">Unity.

</span>
<span class="ltx_bibblock">https://unity.com/, 2024.

</span>
<span class="ltx_bibblock">Accessed: 2024-03-05.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Zhiwei Zhang, Yuhang Zhong, Junlong Guo, Qianhao Wang, Chao Xu, and Fei Gao.

</span>
<span class="ltx_bibblock">Auto filmer: Autonomous aerial videography under human interaction.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib43.1.1">IEEE Robotics and Automation Letters</span>, 8(2):784–791, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Morgan Quigley.

</span>
<span class="ltx_bibblock">Ros: an open-source robot operating system.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib44.1.1">IEEE International Conference on Robotics and Automation</span>, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Hyeong Ryeol Kam, Sung Ho Lee, Taejung Park, and Chang Hun Kim.

</span>
<span class="ltx_bibblock">Rviz: a toolkit for real domain data visualization.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib45.1.1">Telecommunication Systems</span>, 60(2):337–345, October 2015.

</span>
<span class="ltx_bibblock">Funding Information: This research was supported by Basic Science Research Program through the National Research Foundation of Korea(NRF) funded by the Ministry of Education, Science and Technology (No. 2011-0017595). This work was supported by the Technology Innovation Program (Industrial Strategic technology development program, 10035619) funded by the Ministry of Knowledge Economy(MKE, Korea). Publisher Copyright: © 2015, Springer Science+Business Media New York.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Jul  1 21:17:03 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
