<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space</title>
<!--Generated on Mon Jul 15 12:47:05 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2312.08291v4/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S1" title="In VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S2" title="In VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S2.SS1" title="In 2 Related Work ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Parametric Approaches</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S2.SS2" title="In 2 Related Work ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Non-parametric Approaches</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S2.SS3" title="In 2 Related Work ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Quantization of the Human Pose and Shape</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S3" title="In VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Background</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S4" title="In VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S4.SS1" title="In 4 Method ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Proposed HPSE method</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S4.SS2" title="In 4 Method ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Training VQ-HPS</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S5" title="In VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S5.SS1" title="In 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S5.SS2" title="In 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S5.SS3" title="In 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Training on limited data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S5.SS4" title="In 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Training on large-scale datasets</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S5.SS5" title="In 5.4 Training on large-scale datasets ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Ablation study</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S6" title="In 5.4 Training on large-scale datasets ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#A1" title="In 5.4 Training on large-scale datasets ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Implementation details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#A2" title="In Appendix A Implementation details ‚Ä£ 5.4 Training on large-scale datasets ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Additional qualitative results</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#A3" title="In Appendix B Additional qualitative results ‚Ä£ Appendix A Implementation details ‚Ä£ 5.4 Training on large-scale datasets ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Visualization of predictions during training</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#A4" title="In Appendix C Visualization of predictions during training ‚Ä£ Appendix B Additional qualitative results ‚Ä£ Appendix A Implementation details ‚Ä£ 5.4 Training on large-scale datasets ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Analysis of the error</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#A5" title="In Appendix D Analysis of the error ‚Ä£ Appendix C Visualization of predictions during training ‚Ä£ Appendix B Additional qualitative results ‚Ä£ Appendix A Implementation details ‚Ä£ 5.4 Training on large-scale datasets ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Failure cases</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#A6" title="In Appendix E Failure cases ‚Ä£ Appendix D Analysis of the error ‚Ä£ Appendix C Visualization of predictions during training ‚Ä£ Appendix B Additional qualitative results ‚Ä£ Appendix A Implementation details ‚Ä£ 5.4 Training on large-scale datasets ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F </span>Estimating body shapes</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#A7" title="In Appendix F Estimating body shapes ‚Ä£ Appendix E Failure cases ‚Ä£ Appendix D Analysis of the error ‚Ä£ Appendix C Visualization of predictions during training ‚Ä£ Appendix B Additional qualitative results ‚Ä£ Appendix A Implementation details ‚Ä£ 5.4 Training on large-scale datasets ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">G </span>Visualization of the Mesh-VQ-VAE</span></a></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\newcites</span>
<p class="ltx_p" id="p1.2">suppSupplementary References



















</p>
</div>
<h1 class="ltx_title ltx_title_document">VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gu√©nol√© Fiche
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">CentraleSup√©lec, IETR UMR CNRS 6164, France
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Simon Leglaive
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">CentraleSup√©lec, IETR UMR CNRS 6164, France
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xavier Alameda-Pineda
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Inria, Univ. Grenoble Alpes, CNRS, LJK, France
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Antonio Agudo
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Institut de Rob√≤tica i Inform√†tica Industrial (CSIC-UPC), Spain
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Francesc Moreno-Noguer
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Institut de Rob√≤tica i Inform√†tica Industrial (CSIC-UPC), Spain
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Previous works on Human Pose and Shape Estimation (HPSE) from RGB images can be broadly categorized into two main groups: parametric and non-parametric approaches. Parametric techniques leverage a low-dimensional statistical body model for realistic results, whereas recent non-parametric methods achieve higher precision by directly regressing the 3D coordinates of the human body mesh.
This work introduces a novel paradigm to address the HPSE problem, involving a low-dimensional discrete latent representation of the human mesh and framing HPSE as a classification task. Instead of predicting body model parameters or 3D vertex coordinates, we focus on predicting the proposed discrete latent representation, which can be decoded into a registered human mesh. This innovative paradigm offers two key advantages. Firstly, predicting a low-dimensional discrete representation confines our predictions to the space of anthropomorphic poses and shapes even when little training data is available. Secondly, by framing the problem as a classification task, we can harness the discriminative power inherent in neural networks.
The proposed model, VQ-HPS, predicts the discrete latent representation of the mesh. The experimental results demonstrate that VQ-HPS outperforms the current state-of-the-art non-parametric approaches while yielding results as realistic as those produced by parametric methods when trained with few data. VQ-HPS also shows promising results when training on large-scale datasets, highlighting the significant potential of the classification approach for HPSE. See the project page at <a class="ltx_ref ltx_href" href="https://g-fiche.github.io/research-pages/vqhps/" title="">https://g-fiche.github.io/research-pages/vqhps/</a>.</p>
</div>
<figure class="ltx_figure" id="S0.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="309" id="S0.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span class="ltx_text ltx_font_bold" id="S0.F1.2.1">VQ-HPS formulates the human pose and shape estimation problem as a classification task in a vector-quantized latent space</span>. We present the results of VQ-HPS on two challenging scenarios with in-the-wild conditions and poor illumination, comparing its performance to that of HMR¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib1" title="">1</a>]</cite>, CLIFF¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib2" title="">2</a>]</cite> and FastMETRO-S¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib3" title="">3</a>]</cite> when trained on little data.</figcaption>
</figure>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Capturing and understanding human motion from RGB data is a fundamental task in computer vision, with many applications such as character animation for the movie and video-game industries¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib6" title="">6</a>]</cite> or performance optimization in sports¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib8" title="">8</a>]</cite>. However, due to depth ambiguity, estimating 3D human pose and shape from monocular images is an underdetermined problem. To overcome this issue, parametric approaches (also called model-based) use statistical models of the human body, which enable the reconstruction of a 3D human mesh by predicting a small number of parameters¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib13" title="">13</a>]</cite>. Earlier methods were optimization-based, estimating the parameters of a human body model iteratively using 2D cues¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib16" title="">16</a>]</cite>. However, their need for a good initialization, slow running time, and propensity to converge towards local minima led many recent works to focus on regression-based methods, which predict the parameters of a human body model directly from RGB data¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib17" title="">17</a>]</cite>. Despite producing realistic results in most scenarios, methods regressing the parameters of a human body model face several issues well documented in the literature: 1) Parametric methods struggle in capturing detailed body shape and are biased towards the mean shape¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib18" title="">18</a>]</cite>; 2) Most human body models use rotations along the kinematic tree for expressing the pose. In addition to being difficult to predict for neural networks¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib20" title="">20</a>]</cite>, this representation induces error accumulation when all rotations are predicted simultaneously¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib22" title="">22</a>]</cite>; 3) Most regression methods extract global feature vectors from the image as an input, which do not contain fine-grained local details¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib23" title="">23</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">To alleviate these issues, several works switched to methods inspired by 3D pose estimation models that predict 3D coordinates directly. Earlier methods predicted the 6890 vertices of the full SMPL¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib9" title="">9</a>]</cite> mesh using graph convolutional neural networks (GCNNs) modeling the mesh structure and focusing on local interaction between neighboring vertices¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib19" title="">19</a>]</cite>. While¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib24" title="">24</a>]</cite> used Transformers¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib25" title="">25</a>]</cite> to model global interactions between joints and vertices, others argued that a hybrid architecture mixing Graph Convolutional Neural Networks (GCNNs) and Transformers would enable modeling both local and global interactions¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib23" title="">23</a>]</cite>. More recently, FastMETRO¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib3" title="">3</a>]</cite> proposed a Transformer-based encoder-decoder architecture to disentangle image and mesh features and to predict 3D coordinates of body joints and a coarse mesh that can be upsampled to the full SMPL body mesh. Significantly different from prior works, LVD¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib18" title="">18</a>]</cite> proposed an optimization-based approach estimating each vertex position independently by predicting vertex displacement with neural fields. Despite proposing alternatives to model-based approaches, these methods also present some drawbacks: 1) Approaches regressing all vertices of the body mesh at once lack global interaction modeling when using GCNNs¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib23" title="">23</a>]</cite> and have a very high computational cost when using Transformers¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib26" title="">26</a>]</cite>; 2) Regression-based methods sometimes output noisy meshes, some of them regress the SMPL parameters from the predicted mesh to obtain smoother predictions, but it comes with a loss of accuracy¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib20" title="">20</a>]</cite>. This problem is even more glaring when little training data is available, with non-anthropomorphic predictions as displayed in <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S0.F1" title="In VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">1</span></a> for<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib3" title="">3</a>]</cite>; 3) Methods regressing 3D vertices are very sensible to the distribution shift between training and test data¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib24" title="">24</a>]</cite> (see <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S5.SS4" title="5.4 Training on large-scale datasets ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Sec.</span>¬†<span class="ltx_text ltx_ref_tag">5.4</span></a>); 4) LVD¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib18" title="">18</a>]</cite> is real-time and obtains state-of-the-art results for shape estimation, but is not adapted to extreme poses.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">This work introduces a method significantly different from all prior human pose and shape estimation (HPSE) approaches. Instead of predicting the parameters of a human body model or 3D coordinates, we learn to predict a discrete latent representation of 3D meshes, transforming the HPSE into a classification problem in which we can exploit the originally targeted discriminative power of Transformers, which has been proven unmatched in natural language processing. For learning our discrete latent representation of meshes, we build on the vector quantized-variational autoencoder (VQ-VAE)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib27" title="">27</a>]</cite> framework and adapt it to the fully convolutional mesh autoencoder proposed in¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib28" title="">28</a>]</cite>. The encoder of the proposed model, called Mesh-VQ-VAE, provides a low-dimensional discrete latent representation preserving the spatial structure of the mesh. We then propose a Transformer-based encoder-decoder model, called VQ-HPS, for learning to solve the HPSE problem using the cross-entropy loss.
Once the mesh discrete representation is predicted, we can decode it using the pre-trained Mesh-VQ-VAE decoder and obtain a full mesh following the SMPL mesh topology¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib9" title="">9</a>]</cite>. Since the Mesh-VQ-VAE is pre-trained on a large human motion capture database¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib29" title="">29</a>]</cite>, it automatically learns to decode smooth and realistic human meshes. This is particularly interesting when training with little data: VQ-HPS learns to predict sequences of indices corresponding to realistic meshes early in the training process, as demonstrated in the supplementary materials.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In the context of few training data availability, VQ-HPS achieves state-of-the-art performance on the challenging in-the-wild 3DPW¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib30" title="">30</a>]</cite> and EMDB¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib31" title="">31</a>]</cite> benchmarks: it significantly outperforms other methods quantitatively while producing qualitative results as realistic as parametric methods (see Fig.¬†<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S0.F1" title="Figure 1 ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">1</span></a>). Moreover, it also shows SOTA results when trained on standard large-scale datasets, enhancing the significant potential of the classification-based approach for solving the HPSE problem.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Our key contributions can be summarized as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">A Mesh-VQ-VAE architecture providing a discrete latent representation of 3D meshes.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">A classification-based formulation of the HPSE problem using the introduced discrete latent representation of human meshes.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">VQ-HPS, a Transformer-based encoder-decoder model learning to solve the proposed HPSE classification problem using the cross-entropy loss.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para ltx_noindent" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">Code and trained models are available from the project page.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Parametric Approaches</h3>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Several methods are dedicated to recovering the parameters of a parametric human model, such as SMPL¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib9" title="">9</a>]</cite>. Optimization techniques iteratively estimate the parameters of a body model based on images or videos, ensuring that the projection of predictions aligns with a set of 2D cues, including 2D skeletons¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib33" title="">33</a>]</cite>, part segmentation¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib15" title="">15</a>]</cite>, or DensePose¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib35" title="">35</a>]</cite>. Pose and motion priors are commonly incorporated into optimization methods to enhance the realism of predictions¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib38" title="">38</a>]</cite>. On the contrary, regression methods employ neural networks to predict the parameters of a human body model from input images or videos. Many of these methods leverage convolutional neural networks (CNNs) for extracting image features¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib48" title="">48</a>]</cite>. Recent works have demonstrated remarkable performance by replacing CNNs with Vision Transformers¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib49" title="">49</a>]</cite> as seen in¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib50" title="">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib52" title="">52</a>]</cite>. Some methods output probabilistic results, enabling sampling among plausible solutions¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib57" title="">57</a>]</cite>. While optimization methods typically yield superior results, they come with significantly longer running times than regression methods and require precise initialization and accurate 2D cues. One limitation in training regression models is the scarcity of RGB data with 3D annotations. Prior works have addressed this challenge by employing synthetic data¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib58" title="">58</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib59" title="">59</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib61" title="">61</a>]</cite> or pseudo-labels¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib62" title="">62</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib15" title="">15</a>]</cite> for training their models.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">While parametric models can estimate reasonable human poses, the model parameter space may not be the most suitable focus for predicting human pose and shape¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib18" title="">18</a>]</cite>. Recognizing these limitations inherent in parametric approaches has spurred the development of non-parametric methods.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Non-parametric Approaches</h3>
<div class="ltx_para ltx_noindent" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Several works have explored methods for directly predicting 3D meshes without relying on the parameters of a human body model¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib63" title="">63</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib3" title="">3</a>]</cite>. In earlier approaches, regression architectures based on GCNNs were proposed, utilizing a graph structure derived from the topology of the SMPL human mesh¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib63" title="">63</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib23" title="">23</a>]</cite>. Recent advancements have leveraged Transformer architectures, capitalizing on attention mechanisms to capture relationships between joints and vertices. While approaches like¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib24" title="">24</a>]</cite> have introduced encoder-based strategies that concatenate image features and mesh tokens for predicting 3D coordinates, FastMETRO¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib3" title="">3</a>]</cite> presented an encoder-decoder architecture, effectively disentangling image and mesh modalities. Recently,¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib26" title="">26</a>]</cite> introduced a token pruning strategy to enhance the efficiency of Transformer-based HPSE, and¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib18" title="">18</a>]</cite> achieved state-of-the-art accuracy in body shape estimation through an optimization-based approach relying on per-vertex neural features.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">This work introduces a non-parametric approach to HPSE. Our objective is to estimate the vertices of a human body mesh, adhering to the SMPL topology¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib9" title="">9</a>]</cite>. In contrast to all prior works, our method involves predicting the mesh through a discrete latent representation, reframing HPSE as a classification problem. Although exploiting the discriminative power of classification networks has already been proposed for the Human Pose Estimation (HPE) problem (see <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S2.SS3" title="2.3 Quantization of the Human Pose and Shape ‚Ä£ 2 Related Work ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Sec.</span>¬†<span class="ltx_text ltx_ref_tag">2.3</span></a>), to our knowledge, this has not been done before for the HPSE problem.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Quantization of the Human Pose and Shape</h3>
<div class="ltx_para ltx_noindent" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Some works explored quantization for HPE. ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib64" title="">64</a>]</cite> proposed to discretize horizontal and vertical coordinates for 2D HPE. On the other hand,¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib65" title="">65</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib66" title="">66</a>]</cite> used anchor poses and refined them for solving the 3D HPE problem.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib67" title="">67</a>]</cite> proposed a human pose and shape classification method, but the system was trained on only 12 different postures. Some approaches proposed hand shape classification¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib68" title="">68</a>]</cite>, especially for sign-language recognition following the works of¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib69" title="">69</a>]</cite>. Some works also proposed face shape classification¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib70" title="">70</a>]</cite> and head pose estimation¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib71" title="">71</a>]</cite> using a Support Vector Machine.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">Recent works in human motion generation¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib72" title="">72</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib73" title="">73</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib74" title="">74</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib75" title="">75</a>]</cite> used a VQ-VAE¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib27" title="">27</a>]</cite> for quantizing human motion. The main difference with the proposed Mesh-VQ-VAE is that a single index encodes a sequence of poses in these works. In contrast, we use several indices to encode a single pose, allowing for higher precision. Also, none of these works encode the 3D mesh: <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib72" title="">72</a>]</cite> uses the SMPL parameters, and others only encode a 3D skeleton. Furthermore, human motion forecasting and generation tasks differ significantly from HPSE.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">Concurrently with the present work, TokenHMR¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib76" title="">76</a>]</cite> proposes to quantize human pose for HPSE. As in our case, this tokenization acts as a pose prior, using a dictionary of valid pose tokens. However, the tokenization of TokenHMR differs from ours as it happens on the SMPL¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib9" title="">9</a>]</cite> pose parameter while we quantize the 3D mesh. Another major difference between VQ-HPS and TokenHMR is how the tokenized pose is used. TokenHMR uses it as an intermediate representation while still solving the HPSE problem as a regression task: similar to prior works in HPSE, the training targets are the SMPL pose and shape parameters and the vertices‚Äô coordinates. We propose to frame HPSE as a classification task: the unique training target for the mesh is its discrete quantized representation. Experiments show that VQ-HPS achieves better results than TokenHMR using less training data and a less powerful backbone.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Background</h2>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.6"><span class="ltx_text ltx_font_bold" id="S3.p1.6.1">SMPL model.</span>
SMPL¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib9" title="">9</a>]</cite> is a skinned vertex-based human body model that maps the body shape parameter <math alttext="\beta\in\mathbb{R}^{10}" class="ltx_Math" display="inline" id="S3.p1.1.m1.1"><semantics id="S3.p1.1.m1.1a"><mrow id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml"><mi id="S3.p1.1.m1.1.1.2" xref="S3.p1.1.m1.1.1.2.cmml">Œ≤</mi><mo id="S3.p1.1.m1.1.1.1" xref="S3.p1.1.m1.1.1.1.cmml">‚àà</mo><msup id="S3.p1.1.m1.1.1.3" xref="S3.p1.1.m1.1.1.3.cmml"><mi id="S3.p1.1.m1.1.1.3.2" xref="S3.p1.1.m1.1.1.3.2.cmml">‚Ñù</mi><mn id="S3.p1.1.m1.1.1.3.3" xref="S3.p1.1.m1.1.1.3.3.cmml">10</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><apply id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1"><in id="S3.p1.1.m1.1.1.1.cmml" xref="S3.p1.1.m1.1.1.1"></in><ci id="S3.p1.1.m1.1.1.2.cmml" xref="S3.p1.1.m1.1.1.2">ùõΩ</ci><apply id="S3.p1.1.m1.1.1.3.cmml" xref="S3.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.p1.1.m1.1.1.3.1.cmml" xref="S3.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.p1.1.m1.1.1.3.2.cmml" xref="S3.p1.1.m1.1.1.3.2">‚Ñù</ci><cn id="S3.p1.1.m1.1.1.3.3.cmml" type="integer" xref="S3.p1.1.m1.1.1.3.3">10</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">\beta\in\mathbb{R}^{10}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.1.m1.1d">italic_Œ≤ ‚àà blackboard_R start_POSTSUPERSCRIPT 10 end_POSTSUPERSCRIPT</annotation></semantics></math> and the pose parameter <math alttext="\theta\in\mathbb{R}^{72}" class="ltx_Math" display="inline" id="S3.p1.2.m2.1"><semantics id="S3.p1.2.m2.1a"><mrow id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml"><mi id="S3.p1.2.m2.1.1.2" xref="S3.p1.2.m2.1.1.2.cmml">Œ∏</mi><mo id="S3.p1.2.m2.1.1.1" xref="S3.p1.2.m2.1.1.1.cmml">‚àà</mo><msup id="S3.p1.2.m2.1.1.3" xref="S3.p1.2.m2.1.1.3.cmml"><mi id="S3.p1.2.m2.1.1.3.2" xref="S3.p1.2.m2.1.1.3.2.cmml">‚Ñù</mi><mn id="S3.p1.2.m2.1.1.3.3" xref="S3.p1.2.m2.1.1.3.3.cmml">72</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><apply id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1"><in id="S3.p1.2.m2.1.1.1.cmml" xref="S3.p1.2.m2.1.1.1"></in><ci id="S3.p1.2.m2.1.1.2.cmml" xref="S3.p1.2.m2.1.1.2">ùúÉ</ci><apply id="S3.p1.2.m2.1.1.3.cmml" xref="S3.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.p1.2.m2.1.1.3.1.cmml" xref="S3.p1.2.m2.1.1.3">superscript</csymbol><ci id="S3.p1.2.m2.1.1.3.2.cmml" xref="S3.p1.2.m2.1.1.3.2">‚Ñù</ci><cn id="S3.p1.2.m2.1.1.3.3.cmml" type="integer" xref="S3.p1.2.m2.1.1.3.3">72</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">\theta\in\mathbb{R}^{72}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.2.m2.1d">italic_Œ∏ ‚àà blackboard_R start_POSTSUPERSCRIPT 72 end_POSTSUPERSCRIPT</annotation></semantics></math> to 3D vertices through the differentiable function <math alttext="\mathcal{M}(\beta,\theta)" class="ltx_Math" display="inline" id="S3.p1.3.m3.2"><semantics id="S3.p1.3.m3.2a"><mrow id="S3.p1.3.m3.2.3" xref="S3.p1.3.m3.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p1.3.m3.2.3.2" xref="S3.p1.3.m3.2.3.2.cmml">‚Ñ≥</mi><mo id="S3.p1.3.m3.2.3.1" xref="S3.p1.3.m3.2.3.1.cmml">‚Å¢</mo><mrow id="S3.p1.3.m3.2.3.3.2" xref="S3.p1.3.m3.2.3.3.1.cmml"><mo id="S3.p1.3.m3.2.3.3.2.1" stretchy="false" xref="S3.p1.3.m3.2.3.3.1.cmml">(</mo><mi id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml">Œ≤</mi><mo id="S3.p1.3.m3.2.3.3.2.2" xref="S3.p1.3.m3.2.3.3.1.cmml">,</mo><mi id="S3.p1.3.m3.2.2" xref="S3.p1.3.m3.2.2.cmml">Œ∏</mi><mo id="S3.p1.3.m3.2.3.3.2.3" stretchy="false" xref="S3.p1.3.m3.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.2b"><apply id="S3.p1.3.m3.2.3.cmml" xref="S3.p1.3.m3.2.3"><times id="S3.p1.3.m3.2.3.1.cmml" xref="S3.p1.3.m3.2.3.1"></times><ci id="S3.p1.3.m3.2.3.2.cmml" xref="S3.p1.3.m3.2.3.2">‚Ñ≥</ci><interval closure="open" id="S3.p1.3.m3.2.3.3.1.cmml" xref="S3.p1.3.m3.2.3.3.2"><ci id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">ùõΩ</ci><ci id="S3.p1.3.m3.2.2.cmml" xref="S3.p1.3.m3.2.2">ùúÉ</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.2c">\mathcal{M}(\beta,\theta)</annotation><annotation encoding="application/x-llamapun" id="S3.p1.3.m3.2d">caligraphic_M ( italic_Œ≤ , italic_Œ∏ )</annotation></semantics></math>. It outputs the 3D vertices <math alttext="V\in\mathbb{R}^{6890\times 3}" class="ltx_Math" display="inline" id="S3.p1.4.m4.1"><semantics id="S3.p1.4.m4.1a"><mrow id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml"><mi id="S3.p1.4.m4.1.1.2" xref="S3.p1.4.m4.1.1.2.cmml">V</mi><mo id="S3.p1.4.m4.1.1.1" xref="S3.p1.4.m4.1.1.1.cmml">‚àà</mo><msup id="S3.p1.4.m4.1.1.3" xref="S3.p1.4.m4.1.1.3.cmml"><mi id="S3.p1.4.m4.1.1.3.2" xref="S3.p1.4.m4.1.1.3.2.cmml">‚Ñù</mi><mrow id="S3.p1.4.m4.1.1.3.3" xref="S3.p1.4.m4.1.1.3.3.cmml"><mn id="S3.p1.4.m4.1.1.3.3.2" xref="S3.p1.4.m4.1.1.3.3.2.cmml">6890</mn><mo id="S3.p1.4.m4.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.p1.4.m4.1.1.3.3.1.cmml">√ó</mo><mn id="S3.p1.4.m4.1.1.3.3.3" xref="S3.p1.4.m4.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><apply id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1"><in id="S3.p1.4.m4.1.1.1.cmml" xref="S3.p1.4.m4.1.1.1"></in><ci id="S3.p1.4.m4.1.1.2.cmml" xref="S3.p1.4.m4.1.1.2">ùëâ</ci><apply id="S3.p1.4.m4.1.1.3.cmml" xref="S3.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.p1.4.m4.1.1.3.1.cmml" xref="S3.p1.4.m4.1.1.3">superscript</csymbol><ci id="S3.p1.4.m4.1.1.3.2.cmml" xref="S3.p1.4.m4.1.1.3.2">‚Ñù</ci><apply id="S3.p1.4.m4.1.1.3.3.cmml" xref="S3.p1.4.m4.1.1.3.3"><times id="S3.p1.4.m4.1.1.3.3.1.cmml" xref="S3.p1.4.m4.1.1.3.3.1"></times><cn id="S3.p1.4.m4.1.1.3.3.2.cmml" type="integer" xref="S3.p1.4.m4.1.1.3.3.2">6890</cn><cn id="S3.p1.4.m4.1.1.3.3.3.cmml" type="integer" xref="S3.p1.4.m4.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">V\in\mathbb{R}^{6890\times 3}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.4.m4.1d">italic_V ‚àà blackboard_R start_POSTSUPERSCRIPT 6890 √ó 3 end_POSTSUPERSCRIPT</annotation></semantics></math> of a registered mesh, and 3D joints <math alttext="J\in\mathbb{R}^{24\times 3}" class="ltx_Math" display="inline" id="S3.p1.5.m5.1"><semantics id="S3.p1.5.m5.1a"><mrow id="S3.p1.5.m5.1.1" xref="S3.p1.5.m5.1.1.cmml"><mi id="S3.p1.5.m5.1.1.2" xref="S3.p1.5.m5.1.1.2.cmml">J</mi><mo id="S3.p1.5.m5.1.1.1" xref="S3.p1.5.m5.1.1.1.cmml">‚àà</mo><msup id="S3.p1.5.m5.1.1.3" xref="S3.p1.5.m5.1.1.3.cmml"><mi id="S3.p1.5.m5.1.1.3.2" xref="S3.p1.5.m5.1.1.3.2.cmml">‚Ñù</mi><mrow id="S3.p1.5.m5.1.1.3.3" xref="S3.p1.5.m5.1.1.3.3.cmml"><mn id="S3.p1.5.m5.1.1.3.3.2" xref="S3.p1.5.m5.1.1.3.3.2.cmml">24</mn><mo id="S3.p1.5.m5.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.p1.5.m5.1.1.3.3.1.cmml">√ó</mo><mn id="S3.p1.5.m5.1.1.3.3.3" xref="S3.p1.5.m5.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.5.m5.1b"><apply id="S3.p1.5.m5.1.1.cmml" xref="S3.p1.5.m5.1.1"><in id="S3.p1.5.m5.1.1.1.cmml" xref="S3.p1.5.m5.1.1.1"></in><ci id="S3.p1.5.m5.1.1.2.cmml" xref="S3.p1.5.m5.1.1.2">ùêΩ</ci><apply id="S3.p1.5.m5.1.1.3.cmml" xref="S3.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.p1.5.m5.1.1.3.1.cmml" xref="S3.p1.5.m5.1.1.3">superscript</csymbol><ci id="S3.p1.5.m5.1.1.3.2.cmml" xref="S3.p1.5.m5.1.1.3.2">‚Ñù</ci><apply id="S3.p1.5.m5.1.1.3.3.cmml" xref="S3.p1.5.m5.1.1.3.3"><times id="S3.p1.5.m5.1.1.3.3.1.cmml" xref="S3.p1.5.m5.1.1.3.3.1"></times><cn id="S3.p1.5.m5.1.1.3.3.2.cmml" type="integer" xref="S3.p1.5.m5.1.1.3.3.2">24</cn><cn id="S3.p1.5.m5.1.1.3.3.3.cmml" type="integer" xref="S3.p1.5.m5.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.5.m5.1c">J\in\mathbb{R}^{24\times 3}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.5.m5.1d">italic_J ‚àà blackboard_R start_POSTSUPERSCRIPT 24 √ó 3 end_POSTSUPERSCRIPT</annotation></semantics></math> can be extracted from the mesh using the joint regressor matrix <math alttext="\mathcal{J}_{smpl}" class="ltx_Math" display="inline" id="S3.p1.6.m6.1"><semantics id="S3.p1.6.m6.1a"><msub id="S3.p1.6.m6.1.1" xref="S3.p1.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p1.6.m6.1.1.2" xref="S3.p1.6.m6.1.1.2.cmml">ùí•</mi><mrow id="S3.p1.6.m6.1.1.3" xref="S3.p1.6.m6.1.1.3.cmml"><mi id="S3.p1.6.m6.1.1.3.2" xref="S3.p1.6.m6.1.1.3.2.cmml">s</mi><mo id="S3.p1.6.m6.1.1.3.1" xref="S3.p1.6.m6.1.1.3.1.cmml">‚Å¢</mo><mi id="S3.p1.6.m6.1.1.3.3" xref="S3.p1.6.m6.1.1.3.3.cmml">m</mi><mo id="S3.p1.6.m6.1.1.3.1a" xref="S3.p1.6.m6.1.1.3.1.cmml">‚Å¢</mo><mi id="S3.p1.6.m6.1.1.3.4" xref="S3.p1.6.m6.1.1.3.4.cmml">p</mi><mo id="S3.p1.6.m6.1.1.3.1b" xref="S3.p1.6.m6.1.1.3.1.cmml">‚Å¢</mo><mi id="S3.p1.6.m6.1.1.3.5" xref="S3.p1.6.m6.1.1.3.5.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.p1.6.m6.1b"><apply id="S3.p1.6.m6.1.1.cmml" xref="S3.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.p1.6.m6.1.1.1.cmml" xref="S3.p1.6.m6.1.1">subscript</csymbol><ci id="S3.p1.6.m6.1.1.2.cmml" xref="S3.p1.6.m6.1.1.2">ùí•</ci><apply id="S3.p1.6.m6.1.1.3.cmml" xref="S3.p1.6.m6.1.1.3"><times id="S3.p1.6.m6.1.1.3.1.cmml" xref="S3.p1.6.m6.1.1.3.1"></times><ci id="S3.p1.6.m6.1.1.3.2.cmml" xref="S3.p1.6.m6.1.1.3.2">ùë†</ci><ci id="S3.p1.6.m6.1.1.3.3.cmml" xref="S3.p1.6.m6.1.1.3.3">ùëö</ci><ci id="S3.p1.6.m6.1.1.3.4.cmml" xref="S3.p1.6.m6.1.1.3.4">ùëù</ci><ci id="S3.p1.6.m6.1.1.3.5.cmml" xref="S3.p1.6.m6.1.1.3.5">ùëô</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.6.m6.1c">\mathcal{J}_{smpl}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.6.m6.1d">caligraphic_J start_POSTSUBSCRIPT italic_s italic_m italic_p italic_l end_POSTSUBSCRIPT</annotation></semantics></math>. In this work, we do not predict the parameters of the SMPL model, as prior works¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib18" title="">18</a>]</cite> showed that they are not a suitable target for regression models. However, the mesh predicted by the proposed VQ-HPS model follows the SMPL mesh topology. It allows us to use tools like joint regressors and provides a fair comparison with existing approaches.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p2">
<p class="ltx_p" id="S3.p2.3"><span class="ltx_text ltx_font_bold" id="S3.p2.3.1">Fully convolutional mesh autoencoder.</span>
The fully convolutional mesh autoencoder¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib28" title="">28</a>]</cite> is an autoencoder specifically tailored for handling arbitrary registered mesh data. It relies on the definition of novel convolution and pooling operators with globally shared weights and locally varying coefficients depending on the mesh structure. These variable coefficients are pivotal in capturing intricate details inherent to irregular mesh connections, contributing to the model‚Äôs performance in mesh reconstruction. One of the main advantages of fully convolutional architecture is that the latent codes are localized, which gives a latent space preserving the spatial structure of the mesh. The latent representation of the fully convolutional mesh autoencoder lies in <math alttext="\mathbb{R}^{N\times L}" class="ltx_Math" display="inline" id="S3.p2.1.m1.1"><semantics id="S3.p2.1.m1.1a"><msup id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml"><mi id="S3.p2.1.m1.1.1.2" xref="S3.p2.1.m1.1.1.2.cmml">‚Ñù</mi><mrow id="S3.p2.1.m1.1.1.3" xref="S3.p2.1.m1.1.1.3.cmml"><mi id="S3.p2.1.m1.1.1.3.2" xref="S3.p2.1.m1.1.1.3.2.cmml">N</mi><mo id="S3.p2.1.m1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S3.p2.1.m1.1.1.3.1.cmml">√ó</mo><mi id="S3.p2.1.m1.1.1.3.3" xref="S3.p2.1.m1.1.1.3.3.cmml">L</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><apply id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p2.1.m1.1.1.1.cmml" xref="S3.p2.1.m1.1.1">superscript</csymbol><ci id="S3.p2.1.m1.1.1.2.cmml" xref="S3.p2.1.m1.1.1.2">‚Ñù</ci><apply id="S3.p2.1.m1.1.1.3.cmml" xref="S3.p2.1.m1.1.1.3"><times id="S3.p2.1.m1.1.1.3.1.cmml" xref="S3.p2.1.m1.1.1.3.1"></times><ci id="S3.p2.1.m1.1.1.3.2.cmml" xref="S3.p2.1.m1.1.1.3.2">ùëÅ</ci><ci id="S3.p2.1.m1.1.1.3.3.cmml" xref="S3.p2.1.m1.1.1.3.3">ùêø</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">\mathbb{R}^{N\times L}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.1.m1.1d">blackboard_R start_POSTSUPERSCRIPT italic_N √ó italic_L end_POSTSUPERSCRIPT</annotation></semantics></math> where <math alttext="N" class="ltx_Math" display="inline" id="S3.p2.2.m2.1"><semantics id="S3.p2.2.m2.1a"><mi id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><ci id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.p2.2.m2.1d">italic_N</annotation></semantics></math> is the number of latent vectors, and <math alttext="L" class="ltx_Math" display="inline" id="S3.p2.3.m3.1"><semantics id="S3.p2.3.m3.1a"><mi id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.1b"><ci id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1">ùêø</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.1c">L</annotation><annotation encoding="application/x-llamapun" id="S3.p2.3.m3.1d">italic_L</annotation></semantics></math> is the dimension of latent vectors.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p3">
<p class="ltx_p" id="S3.p3.11"><span class="ltx_text ltx_font_bold" id="S3.p3.11.1">Vector quantized-variational autoencoder.</span>
The VQ-VAE¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib27" title="">27</a>]</cite> is an encoder-decoder model with a discretized latent space. The idea is to learn jointly an encoder, a dictionary of latent codes, and a decoder. The encoder maps the input data <math alttext="x" class="ltx_Math" display="inline" id="S3.p3.1.m1.1"><semantics id="S3.p3.1.m1.1a"><mi id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><ci id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1">ùë•</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.p3.1.m1.1d">italic_x</annotation></semantics></math> into a latent variable <math alttext="z\in\mathbb{R}^{N\times L}" class="ltx_Math" display="inline" id="S3.p3.2.m2.1"><semantics id="S3.p3.2.m2.1a"><mrow id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml"><mi id="S3.p3.2.m2.1.1.2" xref="S3.p3.2.m2.1.1.2.cmml">z</mi><mo id="S3.p3.2.m2.1.1.1" xref="S3.p3.2.m2.1.1.1.cmml">‚àà</mo><msup id="S3.p3.2.m2.1.1.3" xref="S3.p3.2.m2.1.1.3.cmml"><mi id="S3.p3.2.m2.1.1.3.2" xref="S3.p3.2.m2.1.1.3.2.cmml">‚Ñù</mi><mrow id="S3.p3.2.m2.1.1.3.3" xref="S3.p3.2.m2.1.1.3.3.cmml"><mi id="S3.p3.2.m2.1.1.3.3.2" xref="S3.p3.2.m2.1.1.3.3.2.cmml">N</mi><mo id="S3.p3.2.m2.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.p3.2.m2.1.1.3.3.1.cmml">√ó</mo><mi id="S3.p3.2.m2.1.1.3.3.3" xref="S3.p3.2.m2.1.1.3.3.3.cmml">L</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><apply id="S3.p3.2.m2.1.1.cmml" xref="S3.p3.2.m2.1.1"><in id="S3.p3.2.m2.1.1.1.cmml" xref="S3.p3.2.m2.1.1.1"></in><ci id="S3.p3.2.m2.1.1.2.cmml" xref="S3.p3.2.m2.1.1.2">ùëß</ci><apply id="S3.p3.2.m2.1.1.3.cmml" xref="S3.p3.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.p3.2.m2.1.1.3.1.cmml" xref="S3.p3.2.m2.1.1.3">superscript</csymbol><ci id="S3.p3.2.m2.1.1.3.2.cmml" xref="S3.p3.2.m2.1.1.3.2">‚Ñù</ci><apply id="S3.p3.2.m2.1.1.3.3.cmml" xref="S3.p3.2.m2.1.1.3.3"><times id="S3.p3.2.m2.1.1.3.3.1.cmml" xref="S3.p3.2.m2.1.1.3.3.1"></times><ci id="S3.p3.2.m2.1.1.3.3.2.cmml" xref="S3.p3.2.m2.1.1.3.3.2">ùëÅ</ci><ci id="S3.p3.2.m2.1.1.3.3.3.cmml" xref="S3.p3.2.m2.1.1.3.3.3">ùêø</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">z\in\mathbb{R}^{N\times L}</annotation><annotation encoding="application/x-llamapun" id="S3.p3.2.m2.1d">italic_z ‚àà blackboard_R start_POSTSUPERSCRIPT italic_N √ó italic_L end_POSTSUPERSCRIPT</annotation></semantics></math>. We then discretize <math alttext="z" class="ltx_Math" display="inline" id="S3.p3.3.m3.1"><semantics id="S3.p3.3.m3.1a"><mi id="S3.p3.3.m3.1.1" xref="S3.p3.3.m3.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.p3.3.m3.1b"><ci id="S3.p3.3.m3.1.1.cmml" xref="S3.p3.3.m3.1.1">ùëß</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.3.m3.1c">z</annotation><annotation encoding="application/x-llamapun" id="S3.p3.3.m3.1d">italic_z</annotation></semantics></math> using a learned dictionary of <math alttext="S" class="ltx_Math" display="inline" id="S3.p3.4.m4.1"><semantics id="S3.p3.4.m4.1a"><mi id="S3.p3.4.m4.1.1" xref="S3.p3.4.m4.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.p3.4.m4.1b"><ci id="S3.p3.4.m4.1.1.cmml" xref="S3.p3.4.m4.1.1">ùëÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.4.m4.1c">S</annotation><annotation encoding="application/x-llamapun" id="S3.p3.4.m4.1d">italic_S</annotation></semantics></math> latent codes of dimension <math alttext="L" class="ltx_Math" display="inline" id="S3.p3.5.m5.1"><semantics id="S3.p3.5.m5.1a"><mi id="S3.p3.5.m5.1.1" xref="S3.p3.5.m5.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.p3.5.m5.1b"><ci id="S3.p3.5.m5.1.1.cmml" xref="S3.p3.5.m5.1.1">ùêø</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.5.m5.1c">L</annotation><annotation encoding="application/x-llamapun" id="S3.p3.5.m5.1d">italic_L</annotation></semantics></math>. We can then write <math alttext="z_{d}\in\mathbb{R}^{N\times L}" class="ltx_Math" display="inline" id="S3.p3.6.m6.1"><semantics id="S3.p3.6.m6.1a"><mrow id="S3.p3.6.m6.1.1" xref="S3.p3.6.m6.1.1.cmml"><msub id="S3.p3.6.m6.1.1.2" xref="S3.p3.6.m6.1.1.2.cmml"><mi id="S3.p3.6.m6.1.1.2.2" xref="S3.p3.6.m6.1.1.2.2.cmml">z</mi><mi id="S3.p3.6.m6.1.1.2.3" xref="S3.p3.6.m6.1.1.2.3.cmml">d</mi></msub><mo id="S3.p3.6.m6.1.1.1" xref="S3.p3.6.m6.1.1.1.cmml">‚àà</mo><msup id="S3.p3.6.m6.1.1.3" xref="S3.p3.6.m6.1.1.3.cmml"><mi id="S3.p3.6.m6.1.1.3.2" xref="S3.p3.6.m6.1.1.3.2.cmml">‚Ñù</mi><mrow id="S3.p3.6.m6.1.1.3.3" xref="S3.p3.6.m6.1.1.3.3.cmml"><mi id="S3.p3.6.m6.1.1.3.3.2" xref="S3.p3.6.m6.1.1.3.3.2.cmml">N</mi><mo id="S3.p3.6.m6.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.p3.6.m6.1.1.3.3.1.cmml">√ó</mo><mi id="S3.p3.6.m6.1.1.3.3.3" xref="S3.p3.6.m6.1.1.3.3.3.cmml">L</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.6.m6.1b"><apply id="S3.p3.6.m6.1.1.cmml" xref="S3.p3.6.m6.1.1"><in id="S3.p3.6.m6.1.1.1.cmml" xref="S3.p3.6.m6.1.1.1"></in><apply id="S3.p3.6.m6.1.1.2.cmml" xref="S3.p3.6.m6.1.1.2"><csymbol cd="ambiguous" id="S3.p3.6.m6.1.1.2.1.cmml" xref="S3.p3.6.m6.1.1.2">subscript</csymbol><ci id="S3.p3.6.m6.1.1.2.2.cmml" xref="S3.p3.6.m6.1.1.2.2">ùëß</ci><ci id="S3.p3.6.m6.1.1.2.3.cmml" xref="S3.p3.6.m6.1.1.2.3">ùëë</ci></apply><apply id="S3.p3.6.m6.1.1.3.cmml" xref="S3.p3.6.m6.1.1.3"><csymbol cd="ambiguous" id="S3.p3.6.m6.1.1.3.1.cmml" xref="S3.p3.6.m6.1.1.3">superscript</csymbol><ci id="S3.p3.6.m6.1.1.3.2.cmml" xref="S3.p3.6.m6.1.1.3.2">‚Ñù</ci><apply id="S3.p3.6.m6.1.1.3.3.cmml" xref="S3.p3.6.m6.1.1.3.3"><times id="S3.p3.6.m6.1.1.3.3.1.cmml" xref="S3.p3.6.m6.1.1.3.3.1"></times><ci id="S3.p3.6.m6.1.1.3.3.2.cmml" xref="S3.p3.6.m6.1.1.3.3.2">ùëÅ</ci><ci id="S3.p3.6.m6.1.1.3.3.3.cmml" xref="S3.p3.6.m6.1.1.3.3.3">ùêø</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.6.m6.1c">z_{d}\in\mathbb{R}^{N\times L}</annotation><annotation encoding="application/x-llamapun" id="S3.p3.6.m6.1d">italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_N √ó italic_L end_POSTSUPERSCRIPT</annotation></semantics></math> where each vector of <math alttext="z" class="ltx_Math" display="inline" id="S3.p3.7.m7.1"><semantics id="S3.p3.7.m7.1a"><mi id="S3.p3.7.m7.1.1" xref="S3.p3.7.m7.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.p3.7.m7.1b"><ci id="S3.p3.7.m7.1.1.cmml" xref="S3.p3.7.m7.1.1">ùëß</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.7.m7.1c">z</annotation><annotation encoding="application/x-llamapun" id="S3.p3.7.m7.1d">italic_z</annotation></semantics></math> is replaced by the closest latent code, or <math alttext="z_{q}\in\{1,\dots,S\}^{N}" class="ltx_Math" display="inline" id="S3.p3.8.m8.3"><semantics id="S3.p3.8.m8.3a"><mrow id="S3.p3.8.m8.3.4" xref="S3.p3.8.m8.3.4.cmml"><msub id="S3.p3.8.m8.3.4.2" xref="S3.p3.8.m8.3.4.2.cmml"><mi id="S3.p3.8.m8.3.4.2.2" xref="S3.p3.8.m8.3.4.2.2.cmml">z</mi><mi id="S3.p3.8.m8.3.4.2.3" xref="S3.p3.8.m8.3.4.2.3.cmml">q</mi></msub><mo id="S3.p3.8.m8.3.4.1" xref="S3.p3.8.m8.3.4.1.cmml">‚àà</mo><msup id="S3.p3.8.m8.3.4.3" xref="S3.p3.8.m8.3.4.3.cmml"><mrow id="S3.p3.8.m8.3.4.3.2.2" xref="S3.p3.8.m8.3.4.3.2.1.cmml"><mo id="S3.p3.8.m8.3.4.3.2.2.1" stretchy="false" xref="S3.p3.8.m8.3.4.3.2.1.cmml">{</mo><mn id="S3.p3.8.m8.1.1" xref="S3.p3.8.m8.1.1.cmml">1</mn><mo id="S3.p3.8.m8.3.4.3.2.2.2" xref="S3.p3.8.m8.3.4.3.2.1.cmml">,</mo><mi id="S3.p3.8.m8.2.2" mathvariant="normal" xref="S3.p3.8.m8.2.2.cmml">‚Ä¶</mi><mo id="S3.p3.8.m8.3.4.3.2.2.3" xref="S3.p3.8.m8.3.4.3.2.1.cmml">,</mo><mi id="S3.p3.8.m8.3.3" xref="S3.p3.8.m8.3.3.cmml">S</mi><mo id="S3.p3.8.m8.3.4.3.2.2.4" stretchy="false" xref="S3.p3.8.m8.3.4.3.2.1.cmml">}</mo></mrow><mi id="S3.p3.8.m8.3.4.3.3" xref="S3.p3.8.m8.3.4.3.3.cmml">N</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.8.m8.3b"><apply id="S3.p3.8.m8.3.4.cmml" xref="S3.p3.8.m8.3.4"><in id="S3.p3.8.m8.3.4.1.cmml" xref="S3.p3.8.m8.3.4.1"></in><apply id="S3.p3.8.m8.3.4.2.cmml" xref="S3.p3.8.m8.3.4.2"><csymbol cd="ambiguous" id="S3.p3.8.m8.3.4.2.1.cmml" xref="S3.p3.8.m8.3.4.2">subscript</csymbol><ci id="S3.p3.8.m8.3.4.2.2.cmml" xref="S3.p3.8.m8.3.4.2.2">ùëß</ci><ci id="S3.p3.8.m8.3.4.2.3.cmml" xref="S3.p3.8.m8.3.4.2.3">ùëû</ci></apply><apply id="S3.p3.8.m8.3.4.3.cmml" xref="S3.p3.8.m8.3.4.3"><csymbol cd="ambiguous" id="S3.p3.8.m8.3.4.3.1.cmml" xref="S3.p3.8.m8.3.4.3">superscript</csymbol><set id="S3.p3.8.m8.3.4.3.2.1.cmml" xref="S3.p3.8.m8.3.4.3.2.2"><cn id="S3.p3.8.m8.1.1.cmml" type="integer" xref="S3.p3.8.m8.1.1">1</cn><ci id="S3.p3.8.m8.2.2.cmml" xref="S3.p3.8.m8.2.2">‚Ä¶</ci><ci id="S3.p3.8.m8.3.3.cmml" xref="S3.p3.8.m8.3.3">ùëÜ</ci></set><ci id="S3.p3.8.m8.3.4.3.3.cmml" xref="S3.p3.8.m8.3.4.3.3">ùëÅ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.8.m8.3c">z_{q}\in\{1,\dots,S\}^{N}</annotation><annotation encoding="application/x-llamapun" id="S3.p3.8.m8.3d">italic_z start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ‚àà { 1 , ‚Ä¶ , italic_S } start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT</annotation></semantics></math>, where the index of the closest latent code replaces each vector of <math alttext="z" class="ltx_Math" display="inline" id="S3.p3.9.m9.1"><semantics id="S3.p3.9.m9.1a"><mi id="S3.p3.9.m9.1.1" xref="S3.p3.9.m9.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.p3.9.m9.1b"><ci id="S3.p3.9.m9.1.1.cmml" xref="S3.p3.9.m9.1.1">ùëß</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.9.m9.1c">z</annotation><annotation encoding="application/x-llamapun" id="S3.p3.9.m9.1d">italic_z</annotation></semantics></math>. The decoder reconstructs <math alttext="x" class="ltx_Math" display="inline" id="S3.p3.10.m10.1"><semantics id="S3.p3.10.m10.1a"><mi id="S3.p3.10.m10.1.1" xref="S3.p3.10.m10.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.p3.10.m10.1b"><ci id="S3.p3.10.m10.1.1.cmml" xref="S3.p3.10.m10.1.1">ùë•</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.10.m10.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.p3.10.m10.1d">italic_x</annotation></semantics></math> from the discrete latent representation <math alttext="z_{d}" class="ltx_Math" display="inline" id="S3.p3.11.m11.1"><semantics id="S3.p3.11.m11.1a"><msub id="S3.p3.11.m11.1.1" xref="S3.p3.11.m11.1.1.cmml"><mi id="S3.p3.11.m11.1.1.2" xref="S3.p3.11.m11.1.1.2.cmml">z</mi><mi id="S3.p3.11.m11.1.1.3" xref="S3.p3.11.m11.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p3.11.m11.1b"><apply id="S3.p3.11.m11.1.1.cmml" xref="S3.p3.11.m11.1.1"><csymbol cd="ambiguous" id="S3.p3.11.m11.1.1.1.cmml" xref="S3.p3.11.m11.1.1">subscript</csymbol><ci id="S3.p3.11.m11.1.1.2.cmml" xref="S3.p3.11.m11.1.1.2">ùëß</ci><ci id="S3.p3.11.m11.1.1.3.cmml" xref="S3.p3.11.m11.1.1.3">ùëë</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.11.m11.1c">z_{d}</annotation><annotation encoding="application/x-llamapun" id="S3.p3.11.m11.1d">italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT</annotation></semantics></math> and the learned dictionary.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Method</h2>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="315" id="S4.F2.g1" src="x2.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span class="ltx_text ltx_font_bold" id="S4.F2.16.1">VQ-HPS global process for predicting the mesh given an image.</span> We first predict the camera <math alttext="\hat{\pi}" class="ltx_Math" display="inline" id="S4.F2.8.m1.1"><semantics id="S4.F2.8.m1.1b"><mover accent="true" id="S4.F2.8.m1.1.1" xref="S4.F2.8.m1.1.1.cmml"><mi id="S4.F2.8.m1.1.1.2" xref="S4.F2.8.m1.1.1.2.cmml">œÄ</mi><mo id="S4.F2.8.m1.1.1.1" xref="S4.F2.8.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.F2.8.m1.1c"><apply id="S4.F2.8.m1.1.1.cmml" xref="S4.F2.8.m1.1.1"><ci id="S4.F2.8.m1.1.1.1.cmml" xref="S4.F2.8.m1.1.1.1">^</ci><ci id="S4.F2.8.m1.1.1.2.cmml" xref="S4.F2.8.m1.1.1.2">ùúã</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.8.m1.1d">\hat{\pi}</annotation><annotation encoding="application/x-llamapun" id="S4.F2.8.m1.1e">over^ start_ARG italic_œÄ end_ARG</annotation></semantics></math> and the rotation <math alttext="\hat{R}" class="ltx_Math" display="inline" id="S4.F2.9.m2.1"><semantics id="S4.F2.9.m2.1b"><mover accent="true" id="S4.F2.9.m2.1.1" xref="S4.F2.9.m2.1.1.cmml"><mi id="S4.F2.9.m2.1.1.2" xref="S4.F2.9.m2.1.1.2.cmml">R</mi><mo id="S4.F2.9.m2.1.1.1" xref="S4.F2.9.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.F2.9.m2.1c"><apply id="S4.F2.9.m2.1.1.cmml" xref="S4.F2.9.m2.1.1"><ci id="S4.F2.9.m2.1.1.1.cmml" xref="S4.F2.9.m2.1.1.1">^</ci><ci id="S4.F2.9.m2.1.1.2.cmml" xref="S4.F2.9.m2.1.1.2">ùëÖ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.9.m2.1d">\hat{R}</annotation><annotation encoding="application/x-llamapun" id="S4.F2.9.m2.1e">over^ start_ARG italic_R end_ARG</annotation></semantics></math> from the image <math alttext="I" class="ltx_Math" display="inline" id="S4.F2.10.m3.1"><semantics id="S4.F2.10.m3.1b"><mi id="S4.F2.10.m3.1.1" xref="S4.F2.10.m3.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S4.F2.10.m3.1c"><ci id="S4.F2.10.m3.1.1.cmml" xref="S4.F2.10.m3.1.1">ùêº</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.10.m3.1d">I</annotation><annotation encoding="application/x-llamapun" id="S4.F2.10.m3.1e">italic_I</annotation></semantics></math>. Then, we use the image, the predicted rotation, and the camera to predict the vertices <math alttext="\hat{V}_{c}" class="ltx_Math" display="inline" id="S4.F2.11.m4.1"><semantics id="S4.F2.11.m4.1b"><msub id="S4.F2.11.m4.1.1" xref="S4.F2.11.m4.1.1.cmml"><mover accent="true" id="S4.F2.11.m4.1.1.2" xref="S4.F2.11.m4.1.1.2.cmml"><mi id="S4.F2.11.m4.1.1.2.2" xref="S4.F2.11.m4.1.1.2.2.cmml">V</mi><mo id="S4.F2.11.m4.1.1.2.1" xref="S4.F2.11.m4.1.1.2.1.cmml">^</mo></mover><mi id="S4.F2.11.m4.1.1.3" xref="S4.F2.11.m4.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S4.F2.11.m4.1c"><apply id="S4.F2.11.m4.1.1.cmml" xref="S4.F2.11.m4.1.1"><csymbol cd="ambiguous" id="S4.F2.11.m4.1.1.1.cmml" xref="S4.F2.11.m4.1.1">subscript</csymbol><apply id="S4.F2.11.m4.1.1.2.cmml" xref="S4.F2.11.m4.1.1.2"><ci id="S4.F2.11.m4.1.1.2.1.cmml" xref="S4.F2.11.m4.1.1.2.1">^</ci><ci id="S4.F2.11.m4.1.1.2.2.cmml" xref="S4.F2.11.m4.1.1.2.2">ùëâ</ci></apply><ci id="S4.F2.11.m4.1.1.3.cmml" xref="S4.F2.11.m4.1.1.3">ùëê</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.11.m4.1d">\hat{V}_{c}</annotation><annotation encoding="application/x-llamapun" id="S4.F2.11.m4.1e">over^ start_ARG italic_V end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math> of the canonical mesh. Finally, <math alttext="\hat{V}_{c}" class="ltx_Math" display="inline" id="S4.F2.12.m5.1"><semantics id="S4.F2.12.m5.1b"><msub id="S4.F2.12.m5.1.1" xref="S4.F2.12.m5.1.1.cmml"><mover accent="true" id="S4.F2.12.m5.1.1.2" xref="S4.F2.12.m5.1.1.2.cmml"><mi id="S4.F2.12.m5.1.1.2.2" xref="S4.F2.12.m5.1.1.2.2.cmml">V</mi><mo id="S4.F2.12.m5.1.1.2.1" xref="S4.F2.12.m5.1.1.2.1.cmml">^</mo></mover><mi id="S4.F2.12.m5.1.1.3" xref="S4.F2.12.m5.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S4.F2.12.m5.1c"><apply id="S4.F2.12.m5.1.1.cmml" xref="S4.F2.12.m5.1.1"><csymbol cd="ambiguous" id="S4.F2.12.m5.1.1.1.cmml" xref="S4.F2.12.m5.1.1">subscript</csymbol><apply id="S4.F2.12.m5.1.1.2.cmml" xref="S4.F2.12.m5.1.1.2"><ci id="S4.F2.12.m5.1.1.2.1.cmml" xref="S4.F2.12.m5.1.1.2.1">^</ci><ci id="S4.F2.12.m5.1.1.2.2.cmml" xref="S4.F2.12.m5.1.1.2.2">ùëâ</ci></apply><ci id="S4.F2.12.m5.1.1.3.cmml" xref="S4.F2.12.m5.1.1.3">ùëê</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.12.m5.1d">\hat{V}_{c}</annotation><annotation encoding="application/x-llamapun" id="S4.F2.12.m5.1e">over^ start_ARG italic_V end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math> is rotated according to <math alttext="\hat{R}" class="ltx_Math" display="inline" id="S4.F2.13.m6.1"><semantics id="S4.F2.13.m6.1b"><mover accent="true" id="S4.F2.13.m6.1.1" xref="S4.F2.13.m6.1.1.cmml"><mi id="S4.F2.13.m6.1.1.2" xref="S4.F2.13.m6.1.1.2.cmml">R</mi><mo id="S4.F2.13.m6.1.1.1" xref="S4.F2.13.m6.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.F2.13.m6.1c"><apply id="S4.F2.13.m6.1.1.cmml" xref="S4.F2.13.m6.1.1"><ci id="S4.F2.13.m6.1.1.1.cmml" xref="S4.F2.13.m6.1.1.1">^</ci><ci id="S4.F2.13.m6.1.1.2.cmml" xref="S4.F2.13.m6.1.1.2">ùëÖ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.13.m6.1d">\hat{R}</annotation><annotation encoding="application/x-llamapun" id="S4.F2.13.m6.1e">over^ start_ARG italic_R end_ARG</annotation></semantics></math> to obtain the final mesh vertices <math alttext="\hat{V}" class="ltx_Math" display="inline" id="S4.F2.14.m7.1"><semantics id="S4.F2.14.m7.1b"><mover accent="true" id="S4.F2.14.m7.1.1" xref="S4.F2.14.m7.1.1.cmml"><mi id="S4.F2.14.m7.1.1.2" xref="S4.F2.14.m7.1.1.2.cmml">V</mi><mo id="S4.F2.14.m7.1.1.1" xref="S4.F2.14.m7.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.F2.14.m7.1c"><apply id="S4.F2.14.m7.1.1.cmml" xref="S4.F2.14.m7.1.1"><ci id="S4.F2.14.m7.1.1.1.cmml" xref="S4.F2.14.m7.1.1.1">^</ci><ci id="S4.F2.14.m7.1.1.2.cmml" xref="S4.F2.14.m7.1.1.2">ùëâ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.14.m7.1d">\hat{V}</annotation><annotation encoding="application/x-llamapun" id="S4.F2.14.m7.1e">over^ start_ARG italic_V end_ARG</annotation></semantics></math>.</figcaption>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Proposed HPSE method</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.4">We propose a novel classification-based method for HPSE. Our goal is to predict an oriented 3D mesh from an image. VQ-HPS consists of an encoder-decoder architecture, predicting the human mesh discrete representation of the introduced Mesh-VQ-VAE from image features. We believe this is the most adapted architecture for predicting our discrete latent representation, with encoder tokens corresponding to image patches and decoder tokens corresponding to body parts and indices in the latent space. To ease the low-dimensional representation learning of the mesh, the predicted mesh is non-oriented and centered on the origin (see <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#A1" title="Appendix A Implementation details ‚Ä£ 5.4 Training on large-scale datasets ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Appendix</span>¬†<span class="ltx_text ltx_ref_tag">A</span></a>): we call it a <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.4.1">canonical mesh</span>. To obtain the final oriented mesh, we then need to predict the rotation <math alttext="R\in\mathbb{R}^{3\times 3}" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><mrow id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mi id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">R</mi><mo id="S4.SS1.p1.1.m1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.cmml">‚àà</mo><msup id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml"><mi id="S4.SS1.p1.1.m1.1.1.3.2" xref="S4.SS1.p1.1.m1.1.1.3.2.cmml">‚Ñù</mi><mrow id="S4.SS1.p1.1.m1.1.1.3.3" xref="S4.SS1.p1.1.m1.1.1.3.3.cmml"><mn id="S4.SS1.p1.1.m1.1.1.3.3.2" xref="S4.SS1.p1.1.m1.1.1.3.3.2.cmml">3</mn><mo id="S4.SS1.p1.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.p1.1.m1.1.1.3.3.1.cmml">√ó</mo><mn id="S4.SS1.p1.1.m1.1.1.3.3.3" xref="S4.SS1.p1.1.m1.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><in id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1"></in><ci id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">ùëÖ</ci><apply id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p1.1.m1.1.1.3.1.cmml" xref="S4.SS1.p1.1.m1.1.1.3">superscript</csymbol><ci id="S4.SS1.p1.1.m1.1.1.3.2.cmml" xref="S4.SS1.p1.1.m1.1.1.3.2">‚Ñù</ci><apply id="S4.SS1.p1.1.m1.1.1.3.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3.3"><times id="S4.SS1.p1.1.m1.1.1.3.3.1.cmml" xref="S4.SS1.p1.1.m1.1.1.3.3.1"></times><cn id="S4.SS1.p1.1.m1.1.1.3.3.2.cmml" type="integer" xref="S4.SS1.p1.1.m1.1.1.3.3.2">3</cn><cn id="S4.SS1.p1.1.m1.1.1.3.3.3.cmml" type="integer" xref="S4.SS1.p1.1.m1.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">R\in\mathbb{R}^{3\times 3}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">italic_R ‚àà blackboard_R start_POSTSUPERSCRIPT 3 √ó 3 end_POSTSUPERSCRIPT</annotation></semantics></math>, and for better alignment with the image, we also regress the perspective camera <math alttext="\pi=[s,t]\in\mathbb{R}^{3}" class="ltx_Math" display="inline" id="S4.SS1.p1.2.m2.2"><semantics id="S4.SS1.p1.2.m2.2a"><mrow id="S4.SS1.p1.2.m2.2.3" xref="S4.SS1.p1.2.m2.2.3.cmml"><mi id="S4.SS1.p1.2.m2.2.3.2" xref="S4.SS1.p1.2.m2.2.3.2.cmml">œÄ</mi><mo id="S4.SS1.p1.2.m2.2.3.3" xref="S4.SS1.p1.2.m2.2.3.3.cmml">=</mo><mrow id="S4.SS1.p1.2.m2.2.3.4.2" xref="S4.SS1.p1.2.m2.2.3.4.1.cmml"><mo id="S4.SS1.p1.2.m2.2.3.4.2.1" stretchy="false" xref="S4.SS1.p1.2.m2.2.3.4.1.cmml">[</mo><mi id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">s</mi><mo id="S4.SS1.p1.2.m2.2.3.4.2.2" xref="S4.SS1.p1.2.m2.2.3.4.1.cmml">,</mo><mi id="S4.SS1.p1.2.m2.2.2" xref="S4.SS1.p1.2.m2.2.2.cmml">t</mi><mo id="S4.SS1.p1.2.m2.2.3.4.2.3" stretchy="false" xref="S4.SS1.p1.2.m2.2.3.4.1.cmml">]</mo></mrow><mo id="S4.SS1.p1.2.m2.2.3.5" xref="S4.SS1.p1.2.m2.2.3.5.cmml">‚àà</mo><msup id="S4.SS1.p1.2.m2.2.3.6" xref="S4.SS1.p1.2.m2.2.3.6.cmml"><mi id="S4.SS1.p1.2.m2.2.3.6.2" xref="S4.SS1.p1.2.m2.2.3.6.2.cmml">‚Ñù</mi><mn id="S4.SS1.p1.2.m2.2.3.6.3" xref="S4.SS1.p1.2.m2.2.3.6.3.cmml">3</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.2b"><apply id="S4.SS1.p1.2.m2.2.3.cmml" xref="S4.SS1.p1.2.m2.2.3"><and id="S4.SS1.p1.2.m2.2.3a.cmml" xref="S4.SS1.p1.2.m2.2.3"></and><apply id="S4.SS1.p1.2.m2.2.3b.cmml" xref="S4.SS1.p1.2.m2.2.3"><eq id="S4.SS1.p1.2.m2.2.3.3.cmml" xref="S4.SS1.p1.2.m2.2.3.3"></eq><ci id="S4.SS1.p1.2.m2.2.3.2.cmml" xref="S4.SS1.p1.2.m2.2.3.2">ùúã</ci><interval closure="closed" id="S4.SS1.p1.2.m2.2.3.4.1.cmml" xref="S4.SS1.p1.2.m2.2.3.4.2"><ci id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">ùë†</ci><ci id="S4.SS1.p1.2.m2.2.2.cmml" xref="S4.SS1.p1.2.m2.2.2">ùë°</ci></interval></apply><apply id="S4.SS1.p1.2.m2.2.3c.cmml" xref="S4.SS1.p1.2.m2.2.3"><in id="S4.SS1.p1.2.m2.2.3.5.cmml" xref="S4.SS1.p1.2.m2.2.3.5"></in><share href="https://arxiv.org/html/2312.08291v4#S4.SS1.p1.2.m2.2.3.4.cmml" id="S4.SS1.p1.2.m2.2.3d.cmml" xref="S4.SS1.p1.2.m2.2.3"></share><apply id="S4.SS1.p1.2.m2.2.3.6.cmml" xref="S4.SS1.p1.2.m2.2.3.6"><csymbol cd="ambiguous" id="S4.SS1.p1.2.m2.2.3.6.1.cmml" xref="S4.SS1.p1.2.m2.2.3.6">superscript</csymbol><ci id="S4.SS1.p1.2.m2.2.3.6.2.cmml" xref="S4.SS1.p1.2.m2.2.3.6.2">‚Ñù</ci><cn id="S4.SS1.p1.2.m2.2.3.6.3.cmml" type="integer" xref="S4.SS1.p1.2.m2.2.3.6.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.2c">\pi=[s,t]\in\mathbb{R}^{3}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.2.m2.2d">italic_œÄ = [ italic_s , italic_t ] ‚àà blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math> where <math alttext="s" class="ltx_Math" display="inline" id="S4.SS1.p1.3.m3.1"><semantics id="S4.SS1.p1.3.m3.1a"><mi id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><ci id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1">ùë†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">s</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.3.m3.1d">italic_s</annotation></semantics></math> is a scale parameter and <math alttext="t" class="ltx_Math" display="inline" id="S4.SS1.p1.4.m4.1"><semantics id="S4.SS1.p1.4.m4.1a"><mi id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><ci id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1">ùë°</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">t</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.4.m4.1d">italic_t</annotation></semantics></math> is a 2D translation. The overall method is shown in <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S4.F2" title="In 4 Method ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">2</span></a>, and we will now detail each of its primary components.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.4"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.4.1">Mesh-VQ-VAE.</span>
For learning discrete representations of meshes, we build on the fully convolutional mesh autoencoder¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib28" title="">28</a>]</cite> (see <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S3" title="3 Background ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Sec.</span>¬†<span class="ltx_text ltx_ref_tag">3</span></a>) for encoding the full canonical mesh vertices <math alttext="V_{c}\in\mathbb{R}^{6890\times 3}" class="ltx_Math" display="inline" id="S4.SS1.p2.1.m1.1"><semantics id="S4.SS1.p2.1.m1.1a"><mrow id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><msub id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2.cmml"><mi id="S4.SS1.p2.1.m1.1.1.2.2" xref="S4.SS1.p2.1.m1.1.1.2.2.cmml">V</mi><mi id="S4.SS1.p2.1.m1.1.1.2.3" xref="S4.SS1.p2.1.m1.1.1.2.3.cmml">c</mi></msub><mo id="S4.SS1.p2.1.m1.1.1.1" xref="S4.SS1.p2.1.m1.1.1.1.cmml">‚àà</mo><msup id="S4.SS1.p2.1.m1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.3.cmml"><mi id="S4.SS1.p2.1.m1.1.1.3.2" xref="S4.SS1.p2.1.m1.1.1.3.2.cmml">‚Ñù</mi><mrow id="S4.SS1.p2.1.m1.1.1.3.3" xref="S4.SS1.p2.1.m1.1.1.3.3.cmml"><mn id="S4.SS1.p2.1.m1.1.1.3.3.2" xref="S4.SS1.p2.1.m1.1.1.3.3.2.cmml">6890</mn><mo id="S4.SS1.p2.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.p2.1.m1.1.1.3.3.1.cmml">√ó</mo><mn id="S4.SS1.p2.1.m1.1.1.3.3.3" xref="S4.SS1.p2.1.m1.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><in id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1.1"></in><apply id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p2.1.m1.1.1.2.1.cmml" xref="S4.SS1.p2.1.m1.1.1.2">subscript</csymbol><ci id="S4.SS1.p2.1.m1.1.1.2.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2.2">ùëâ</ci><ci id="S4.SS1.p2.1.m1.1.1.2.3.cmml" xref="S4.SS1.p2.1.m1.1.1.2.3">ùëê</ci></apply><apply id="S4.SS1.p2.1.m1.1.1.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p2.1.m1.1.1.3.1.cmml" xref="S4.SS1.p2.1.m1.1.1.3">superscript</csymbol><ci id="S4.SS1.p2.1.m1.1.1.3.2.cmml" xref="S4.SS1.p2.1.m1.1.1.3.2">‚Ñù</ci><apply id="S4.SS1.p2.1.m1.1.1.3.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3.3"><times id="S4.SS1.p2.1.m1.1.1.3.3.1.cmml" xref="S4.SS1.p2.1.m1.1.1.3.3.1"></times><cn id="S4.SS1.p2.1.m1.1.1.3.3.2.cmml" type="integer" xref="S4.SS1.p2.1.m1.1.1.3.3.2">6890</cn><cn id="S4.SS1.p2.1.m1.1.1.3.3.3.cmml" type="integer" xref="S4.SS1.p2.1.m1.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">V_{c}\in\mathbb{R}^{6890\times 3}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.1.m1.1d">italic_V start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT 6890 √ó 3 end_POSTSUPERSCRIPT</annotation></semantics></math> to a latent representation <math alttext="z\in\mathbb{R}^{N\times L}" class="ltx_Math" display="inline" id="S4.SS1.p2.2.m2.1"><semantics id="S4.SS1.p2.2.m2.1a"><mrow id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml"><mi id="S4.SS1.p2.2.m2.1.1.2" xref="S4.SS1.p2.2.m2.1.1.2.cmml">z</mi><mo id="S4.SS1.p2.2.m2.1.1.1" xref="S4.SS1.p2.2.m2.1.1.1.cmml">‚àà</mo><msup id="S4.SS1.p2.2.m2.1.1.3" xref="S4.SS1.p2.2.m2.1.1.3.cmml"><mi id="S4.SS1.p2.2.m2.1.1.3.2" xref="S4.SS1.p2.2.m2.1.1.3.2.cmml">‚Ñù</mi><mrow id="S4.SS1.p2.2.m2.1.1.3.3" xref="S4.SS1.p2.2.m2.1.1.3.3.cmml"><mi id="S4.SS1.p2.2.m2.1.1.3.3.2" xref="S4.SS1.p2.2.m2.1.1.3.3.2.cmml">N</mi><mo id="S4.SS1.p2.2.m2.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.p2.2.m2.1.1.3.3.1.cmml">√ó</mo><mi id="S4.SS1.p2.2.m2.1.1.3.3.3" xref="S4.SS1.p2.2.m2.1.1.3.3.3.cmml">L</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><apply id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1"><in id="S4.SS1.p2.2.m2.1.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1.1"></in><ci id="S4.SS1.p2.2.m2.1.1.2.cmml" xref="S4.SS1.p2.2.m2.1.1.2">ùëß</ci><apply id="S4.SS1.p2.2.m2.1.1.3.cmml" xref="S4.SS1.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p2.2.m2.1.1.3.1.cmml" xref="S4.SS1.p2.2.m2.1.1.3">superscript</csymbol><ci id="S4.SS1.p2.2.m2.1.1.3.2.cmml" xref="S4.SS1.p2.2.m2.1.1.3.2">‚Ñù</ci><apply id="S4.SS1.p2.2.m2.1.1.3.3.cmml" xref="S4.SS1.p2.2.m2.1.1.3.3"><times id="S4.SS1.p2.2.m2.1.1.3.3.1.cmml" xref="S4.SS1.p2.2.m2.1.1.3.3.1"></times><ci id="S4.SS1.p2.2.m2.1.1.3.3.2.cmml" xref="S4.SS1.p2.2.m2.1.1.3.3.2">ùëÅ</ci><ci id="S4.SS1.p2.2.m2.1.1.3.3.3.cmml" xref="S4.SS1.p2.2.m2.1.1.3.3.3">ùêø</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">z\in\mathbb{R}^{N\times L}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.2.m2.1d">italic_z ‚àà blackboard_R start_POSTSUPERSCRIPT italic_N √ó italic_L end_POSTSUPERSCRIPT</annotation></semantics></math>. We add a vector quantization step in the latent space similar to¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib27" title="">27</a>]</cite> (see <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S3" title="3 Background ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Sec.</span>¬†<span class="ltx_text ltx_ref_tag">3</span></a>), which maps <math alttext="z" class="ltx_Math" display="inline" id="S4.SS1.p2.3.m3.1"><semantics id="S4.SS1.p2.3.m3.1a"><mi id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><ci id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1">ùëß</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">z</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.3.m3.1d">italic_z</annotation></semantics></math> to the discrete latent representation <math alttext="z_{q}\in\{1,\dots,S\}^{N}" class="ltx_Math" display="inline" id="S4.SS1.p2.4.m4.3"><semantics id="S4.SS1.p2.4.m4.3a"><mrow id="S4.SS1.p2.4.m4.3.4" xref="S4.SS1.p2.4.m4.3.4.cmml"><msub id="S4.SS1.p2.4.m4.3.4.2" xref="S4.SS1.p2.4.m4.3.4.2.cmml"><mi id="S4.SS1.p2.4.m4.3.4.2.2" xref="S4.SS1.p2.4.m4.3.4.2.2.cmml">z</mi><mi id="S4.SS1.p2.4.m4.3.4.2.3" xref="S4.SS1.p2.4.m4.3.4.2.3.cmml">q</mi></msub><mo id="S4.SS1.p2.4.m4.3.4.1" xref="S4.SS1.p2.4.m4.3.4.1.cmml">‚àà</mo><msup id="S4.SS1.p2.4.m4.3.4.3" xref="S4.SS1.p2.4.m4.3.4.3.cmml"><mrow id="S4.SS1.p2.4.m4.3.4.3.2.2" xref="S4.SS1.p2.4.m4.3.4.3.2.1.cmml"><mo id="S4.SS1.p2.4.m4.3.4.3.2.2.1" stretchy="false" xref="S4.SS1.p2.4.m4.3.4.3.2.1.cmml">{</mo><mn id="S4.SS1.p2.4.m4.1.1" xref="S4.SS1.p2.4.m4.1.1.cmml">1</mn><mo id="S4.SS1.p2.4.m4.3.4.3.2.2.2" xref="S4.SS1.p2.4.m4.3.4.3.2.1.cmml">,</mo><mi id="S4.SS1.p2.4.m4.2.2" mathvariant="normal" xref="S4.SS1.p2.4.m4.2.2.cmml">‚Ä¶</mi><mo id="S4.SS1.p2.4.m4.3.4.3.2.2.3" xref="S4.SS1.p2.4.m4.3.4.3.2.1.cmml">,</mo><mi id="S4.SS1.p2.4.m4.3.3" xref="S4.SS1.p2.4.m4.3.3.cmml">S</mi><mo id="S4.SS1.p2.4.m4.3.4.3.2.2.4" stretchy="false" xref="S4.SS1.p2.4.m4.3.4.3.2.1.cmml">}</mo></mrow><mi id="S4.SS1.p2.4.m4.3.4.3.3" xref="S4.SS1.p2.4.m4.3.4.3.3.cmml">N</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.4.m4.3b"><apply id="S4.SS1.p2.4.m4.3.4.cmml" xref="S4.SS1.p2.4.m4.3.4"><in id="S4.SS1.p2.4.m4.3.4.1.cmml" xref="S4.SS1.p2.4.m4.3.4.1"></in><apply id="S4.SS1.p2.4.m4.3.4.2.cmml" xref="S4.SS1.p2.4.m4.3.4.2"><csymbol cd="ambiguous" id="S4.SS1.p2.4.m4.3.4.2.1.cmml" xref="S4.SS1.p2.4.m4.3.4.2">subscript</csymbol><ci id="S4.SS1.p2.4.m4.3.4.2.2.cmml" xref="S4.SS1.p2.4.m4.3.4.2.2">ùëß</ci><ci id="S4.SS1.p2.4.m4.3.4.2.3.cmml" xref="S4.SS1.p2.4.m4.3.4.2.3">ùëû</ci></apply><apply id="S4.SS1.p2.4.m4.3.4.3.cmml" xref="S4.SS1.p2.4.m4.3.4.3"><csymbol cd="ambiguous" id="S4.SS1.p2.4.m4.3.4.3.1.cmml" xref="S4.SS1.p2.4.m4.3.4.3">superscript</csymbol><set id="S4.SS1.p2.4.m4.3.4.3.2.1.cmml" xref="S4.SS1.p2.4.m4.3.4.3.2.2"><cn id="S4.SS1.p2.4.m4.1.1.cmml" type="integer" xref="S4.SS1.p2.4.m4.1.1">1</cn><ci id="S4.SS1.p2.4.m4.2.2.cmml" xref="S4.SS1.p2.4.m4.2.2">‚Ä¶</ci><ci id="S4.SS1.p2.4.m4.3.3.cmml" xref="S4.SS1.p2.4.m4.3.3">ùëÜ</ci></set><ci id="S4.SS1.p2.4.m4.3.4.3.3.cmml" xref="S4.SS1.p2.4.m4.3.4.3.3">ùëÅ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.4.m4.3c">z_{q}\in\{1,\dots,S\}^{N}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.4.m4.3d">italic_z start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ‚àà { 1 , ‚Ä¶ , italic_S } start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT</annotation></semantics></math>. While the fully convolutional architecture preserves the spatial structure of the mesh, the added quantization step allows us to view the HPSE as a classification task as we aim to predict the indices of the latent mesh representation given an image. Our Mesh-VQ-VAE in <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S4.F2" title="In 4 Method ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">2</span></a> can be seen as a VQ-VAE¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib27" title="">27</a>]</cite> whose architecture corresponds to the fully convolutional mesh autoencoder.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.7"><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.7.1">Feature extractors.</span>
The first step for image-based HPSE is to extract features from the image. We use CNN backbones to preserve the spatial structure of the image, and we obtain features <math alttext="X\in\mathbb{R}^{H\times W\times C}" class="ltx_Math" display="inline" id="S4.SS1.p3.1.m1.1"><semantics id="S4.SS1.p3.1.m1.1a"><mrow id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml"><mi id="S4.SS1.p3.1.m1.1.1.2" xref="S4.SS1.p3.1.m1.1.1.2.cmml">X</mi><mo id="S4.SS1.p3.1.m1.1.1.1" xref="S4.SS1.p3.1.m1.1.1.1.cmml">‚àà</mo><msup id="S4.SS1.p3.1.m1.1.1.3" xref="S4.SS1.p3.1.m1.1.1.3.cmml"><mi id="S4.SS1.p3.1.m1.1.1.3.2" xref="S4.SS1.p3.1.m1.1.1.3.2.cmml">‚Ñù</mi><mrow id="S4.SS1.p3.1.m1.1.1.3.3" xref="S4.SS1.p3.1.m1.1.1.3.3.cmml"><mi id="S4.SS1.p3.1.m1.1.1.3.3.2" xref="S4.SS1.p3.1.m1.1.1.3.3.2.cmml">H</mi><mo id="S4.SS1.p3.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.p3.1.m1.1.1.3.3.1.cmml">√ó</mo><mi id="S4.SS1.p3.1.m1.1.1.3.3.3" xref="S4.SS1.p3.1.m1.1.1.3.3.3.cmml">W</mi><mo id="S4.SS1.p3.1.m1.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S4.SS1.p3.1.m1.1.1.3.3.1.cmml">√ó</mo><mi id="S4.SS1.p3.1.m1.1.1.3.3.4" xref="S4.SS1.p3.1.m1.1.1.3.3.4.cmml">C</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><apply id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"><in id="S4.SS1.p3.1.m1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1"></in><ci id="S4.SS1.p3.1.m1.1.1.2.cmml" xref="S4.SS1.p3.1.m1.1.1.2">ùëã</ci><apply id="S4.SS1.p3.1.m1.1.1.3.cmml" xref="S4.SS1.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p3.1.m1.1.1.3.1.cmml" xref="S4.SS1.p3.1.m1.1.1.3">superscript</csymbol><ci id="S4.SS1.p3.1.m1.1.1.3.2.cmml" xref="S4.SS1.p3.1.m1.1.1.3.2">‚Ñù</ci><apply id="S4.SS1.p3.1.m1.1.1.3.3.cmml" xref="S4.SS1.p3.1.m1.1.1.3.3"><times id="S4.SS1.p3.1.m1.1.1.3.3.1.cmml" xref="S4.SS1.p3.1.m1.1.1.3.3.1"></times><ci id="S4.SS1.p3.1.m1.1.1.3.3.2.cmml" xref="S4.SS1.p3.1.m1.1.1.3.3.2">ùêª</ci><ci id="S4.SS1.p3.1.m1.1.1.3.3.3.cmml" xref="S4.SS1.p3.1.m1.1.1.3.3.3">ùëä</ci><ci id="S4.SS1.p3.1.m1.1.1.3.3.4.cmml" xref="S4.SS1.p3.1.m1.1.1.3.3.4">ùê∂</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">X\in\mathbb{R}^{H\times W\times C}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.1.m1.1d">italic_X ‚àà blackboard_R start_POSTSUPERSCRIPT italic_H √ó italic_W √ó italic_C end_POSTSUPERSCRIPT</annotation></semantics></math>, where <math alttext="C" class="ltx_Math" display="inline" id="S4.SS1.p3.2.m2.1"><semantics id="S4.SS1.p3.2.m2.1a"><mi id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><ci id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1">ùê∂</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">C</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.2.m2.1d">italic_C</annotation></semantics></math> is the number of channels of the backbone and <math alttext="H" class="ltx_Math" display="inline" id="S4.SS1.p3.3.m3.1"><semantics id="S4.SS1.p3.3.m3.1a"><mi id="S4.SS1.p3.3.m3.1.1" xref="S4.SS1.p3.3.m3.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.3.m3.1b"><ci id="S4.SS1.p3.3.m3.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1">ùêª</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.3.m3.1c">H</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.3.m3.1d">italic_H</annotation></semantics></math> and <math alttext="W" class="ltx_Math" display="inline" id="S4.SS1.p3.4.m4.1"><semantics id="S4.SS1.p3.4.m4.1a"><mi id="S4.SS1.p3.4.m4.1.1" xref="S4.SS1.p3.4.m4.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.4.m4.1b"><ci id="S4.SS1.p3.4.m4.1.1.cmml" xref="S4.SS1.p3.4.m4.1.1">ùëä</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.4.m4.1c">W</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.4.m4.1d">italic_W</annotation></semantics></math> are the spatial dimension. We use two feature extractors. The feature extractor of the camera and rotation predictors gives <math alttext="X_{r}ot" class="ltx_Math" display="inline" id="S4.SS1.p3.5.m5.1"><semantics id="S4.SS1.p3.5.m5.1a"><mrow id="S4.SS1.p3.5.m5.1.1" xref="S4.SS1.p3.5.m5.1.1.cmml"><msub id="S4.SS1.p3.5.m5.1.1.2" xref="S4.SS1.p3.5.m5.1.1.2.cmml"><mi id="S4.SS1.p3.5.m5.1.1.2.2" xref="S4.SS1.p3.5.m5.1.1.2.2.cmml">X</mi><mi id="S4.SS1.p3.5.m5.1.1.2.3" xref="S4.SS1.p3.5.m5.1.1.2.3.cmml">r</mi></msub><mo id="S4.SS1.p3.5.m5.1.1.1" xref="S4.SS1.p3.5.m5.1.1.1.cmml">‚Å¢</mo><mi id="S4.SS1.p3.5.m5.1.1.3" xref="S4.SS1.p3.5.m5.1.1.3.cmml">o</mi><mo id="S4.SS1.p3.5.m5.1.1.1a" xref="S4.SS1.p3.5.m5.1.1.1.cmml">‚Å¢</mo><mi id="S4.SS1.p3.5.m5.1.1.4" xref="S4.SS1.p3.5.m5.1.1.4.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.5.m5.1b"><apply id="S4.SS1.p3.5.m5.1.1.cmml" xref="S4.SS1.p3.5.m5.1.1"><times id="S4.SS1.p3.5.m5.1.1.1.cmml" xref="S4.SS1.p3.5.m5.1.1.1"></times><apply id="S4.SS1.p3.5.m5.1.1.2.cmml" xref="S4.SS1.p3.5.m5.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p3.5.m5.1.1.2.1.cmml" xref="S4.SS1.p3.5.m5.1.1.2">subscript</csymbol><ci id="S4.SS1.p3.5.m5.1.1.2.2.cmml" xref="S4.SS1.p3.5.m5.1.1.2.2">ùëã</ci><ci id="S4.SS1.p3.5.m5.1.1.2.3.cmml" xref="S4.SS1.p3.5.m5.1.1.2.3">ùëü</ci></apply><ci id="S4.SS1.p3.5.m5.1.1.3.cmml" xref="S4.SS1.p3.5.m5.1.1.3">ùëú</ci><ci id="S4.SS1.p3.5.m5.1.1.4.cmml" xref="S4.SS1.p3.5.m5.1.1.4">ùë°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.5.m5.1c">X_{r}ot</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.5.m5.1d">italic_X start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT italic_o italic_t</annotation></semantics></math>. The feature extractor gives <math alttext="X_{mesh}" class="ltx_Math" display="inline" id="S4.SS1.p3.6.m6.1"><semantics id="S4.SS1.p3.6.m6.1a"><msub id="S4.SS1.p3.6.m6.1.1" xref="S4.SS1.p3.6.m6.1.1.cmml"><mi id="S4.SS1.p3.6.m6.1.1.2" xref="S4.SS1.p3.6.m6.1.1.2.cmml">X</mi><mrow id="S4.SS1.p3.6.m6.1.1.3" xref="S4.SS1.p3.6.m6.1.1.3.cmml"><mi id="S4.SS1.p3.6.m6.1.1.3.2" xref="S4.SS1.p3.6.m6.1.1.3.2.cmml">m</mi><mo id="S4.SS1.p3.6.m6.1.1.3.1" xref="S4.SS1.p3.6.m6.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.SS1.p3.6.m6.1.1.3.3" xref="S4.SS1.p3.6.m6.1.1.3.3.cmml">e</mi><mo id="S4.SS1.p3.6.m6.1.1.3.1a" xref="S4.SS1.p3.6.m6.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.SS1.p3.6.m6.1.1.3.4" xref="S4.SS1.p3.6.m6.1.1.3.4.cmml">s</mi><mo id="S4.SS1.p3.6.m6.1.1.3.1b" xref="S4.SS1.p3.6.m6.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.SS1.p3.6.m6.1.1.3.5" xref="S4.SS1.p3.6.m6.1.1.3.5.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.6.m6.1b"><apply id="S4.SS1.p3.6.m6.1.1.cmml" xref="S4.SS1.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.6.m6.1.1.1.cmml" xref="S4.SS1.p3.6.m6.1.1">subscript</csymbol><ci id="S4.SS1.p3.6.m6.1.1.2.cmml" xref="S4.SS1.p3.6.m6.1.1.2">ùëã</ci><apply id="S4.SS1.p3.6.m6.1.1.3.cmml" xref="S4.SS1.p3.6.m6.1.1.3"><times id="S4.SS1.p3.6.m6.1.1.3.1.cmml" xref="S4.SS1.p3.6.m6.1.1.3.1"></times><ci id="S4.SS1.p3.6.m6.1.1.3.2.cmml" xref="S4.SS1.p3.6.m6.1.1.3.2">ùëö</ci><ci id="S4.SS1.p3.6.m6.1.1.3.3.cmml" xref="S4.SS1.p3.6.m6.1.1.3.3">ùëí</ci><ci id="S4.SS1.p3.6.m6.1.1.3.4.cmml" xref="S4.SS1.p3.6.m6.1.1.3.4">ùë†</ci><ci id="S4.SS1.p3.6.m6.1.1.3.5.cmml" xref="S4.SS1.p3.6.m6.1.1.3.5">‚Ñé</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.6.m6.1c">X_{mesh}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.6.m6.1d">italic_X start_POSTSUBSCRIPT italic_m italic_e italic_s italic_h end_POSTSUBSCRIPT</annotation></semantics></math> with <math alttext="W=1" class="ltx_Math" display="inline" id="S4.SS1.p3.7.m7.1"><semantics id="S4.SS1.p3.7.m7.1a"><mrow id="S4.SS1.p3.7.m7.1.1" xref="S4.SS1.p3.7.m7.1.1.cmml"><mi id="S4.SS1.p3.7.m7.1.1.2" xref="S4.SS1.p3.7.m7.1.1.2.cmml">W</mi><mo id="S4.SS1.p3.7.m7.1.1.1" xref="S4.SS1.p3.7.m7.1.1.1.cmml">=</mo><mn id="S4.SS1.p3.7.m7.1.1.3" xref="S4.SS1.p3.7.m7.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.7.m7.1b"><apply id="S4.SS1.p3.7.m7.1.1.cmml" xref="S4.SS1.p3.7.m7.1.1"><eq id="S4.SS1.p3.7.m7.1.1.1.cmml" xref="S4.SS1.p3.7.m7.1.1.1"></eq><ci id="S4.SS1.p3.7.m7.1.1.2.cmml" xref="S4.SS1.p3.7.m7.1.1.2">ùëä</ci><cn id="S4.SS1.p3.7.m7.1.1.3.cmml" type="integer" xref="S4.SS1.p3.7.m7.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.7.m7.1c">W=1</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.7.m7.1d">italic_W = 1</annotation></semantics></math> in the latent canonical mesh predictor.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.3"><span class="ltx_text ltx_font_bold" id="S4.SS1.p4.3.1">Rotation and camera prediction.</span>
We start by predicting the mesh rotation and the perspective camera parameters (see again <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S4.F2" title="In 4 Method ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">2</span></a>). These predictions depend on the image features and an initial body pose <math alttext="p\in\mathbb{R}^{17\times 3}" class="ltx_Math" display="inline" id="S4.SS1.p4.1.m1.1"><semantics id="S4.SS1.p4.1.m1.1a"><mrow id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml"><mi id="S4.SS1.p4.1.m1.1.1.2" xref="S4.SS1.p4.1.m1.1.1.2.cmml">p</mi><mo id="S4.SS1.p4.1.m1.1.1.1" xref="S4.SS1.p4.1.m1.1.1.1.cmml">‚àà</mo><msup id="S4.SS1.p4.1.m1.1.1.3" xref="S4.SS1.p4.1.m1.1.1.3.cmml"><mi id="S4.SS1.p4.1.m1.1.1.3.2" xref="S4.SS1.p4.1.m1.1.1.3.2.cmml">‚Ñù</mi><mrow id="S4.SS1.p4.1.m1.1.1.3.3" xref="S4.SS1.p4.1.m1.1.1.3.3.cmml"><mn id="S4.SS1.p4.1.m1.1.1.3.3.2" xref="S4.SS1.p4.1.m1.1.1.3.3.2.cmml">17</mn><mo id="S4.SS1.p4.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.p4.1.m1.1.1.3.3.1.cmml">√ó</mo><mn id="S4.SS1.p4.1.m1.1.1.3.3.3" xref="S4.SS1.p4.1.m1.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><apply id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1"><in id="S4.SS1.p4.1.m1.1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1.1"></in><ci id="S4.SS1.p4.1.m1.1.1.2.cmml" xref="S4.SS1.p4.1.m1.1.1.2">ùëù</ci><apply id="S4.SS1.p4.1.m1.1.1.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p4.1.m1.1.1.3.1.cmml" xref="S4.SS1.p4.1.m1.1.1.3">superscript</csymbol><ci id="S4.SS1.p4.1.m1.1.1.3.2.cmml" xref="S4.SS1.p4.1.m1.1.1.3.2">‚Ñù</ci><apply id="S4.SS1.p4.1.m1.1.1.3.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3.3"><times id="S4.SS1.p4.1.m1.1.1.3.3.1.cmml" xref="S4.SS1.p4.1.m1.1.1.3.3.1"></times><cn id="S4.SS1.p4.1.m1.1.1.3.3.2.cmml" type="integer" xref="S4.SS1.p4.1.m1.1.1.3.3.2">17</cn><cn id="S4.SS1.p4.1.m1.1.1.3.3.3.cmml" type="integer" xref="S4.SS1.p4.1.m1.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">p\in\mathbb{R}^{17\times 3}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p4.1.m1.1d">italic_p ‚àà blackboard_R start_POSTSUPERSCRIPT 17 √ó 3 end_POSTSUPERSCRIPT</annotation></semantics></math> following the Human3.6M¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib77" title="">77</a>]</cite> joints layout and corresponding to the SMPL T-pose. We predict the rotation <math alttext="\hat{R}" class="ltx_Math" display="inline" id="S4.SS1.p4.2.m2.1"><semantics id="S4.SS1.p4.2.m2.1a"><mover accent="true" id="S4.SS1.p4.2.m2.1.1" xref="S4.SS1.p4.2.m2.1.1.cmml"><mi id="S4.SS1.p4.2.m2.1.1.2" xref="S4.SS1.p4.2.m2.1.1.2.cmml">R</mi><mo id="S4.SS1.p4.2.m2.1.1.1" xref="S4.SS1.p4.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.2.m2.1b"><apply id="S4.SS1.p4.2.m2.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1"><ci id="S4.SS1.p4.2.m2.1.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1.1">^</ci><ci id="S4.SS1.p4.2.m2.1.1.2.cmml" xref="S4.SS1.p4.2.m2.1.1.2">ùëÖ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.2.m2.1c">\hat{R}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p4.2.m2.1d">over^ start_ARG italic_R end_ARG</annotation></semantics></math> and the weak perspective camera parameters <math alttext="\hat{\pi}" class="ltx_Math" display="inline" id="S4.SS1.p4.3.m3.1"><semantics id="S4.SS1.p4.3.m3.1a"><mover accent="true" id="S4.SS1.p4.3.m3.1.1" xref="S4.SS1.p4.3.m3.1.1.cmml"><mi id="S4.SS1.p4.3.m3.1.1.2" xref="S4.SS1.p4.3.m3.1.1.2.cmml">œÄ</mi><mo id="S4.SS1.p4.3.m3.1.1.1" xref="S4.SS1.p4.3.m3.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.3.m3.1b"><apply id="S4.SS1.p4.3.m3.1.1.cmml" xref="S4.SS1.p4.3.m3.1.1"><ci id="S4.SS1.p4.3.m3.1.1.1.cmml" xref="S4.SS1.p4.3.m3.1.1.1">^</ci><ci id="S4.SS1.p4.3.m3.1.1.2.cmml" xref="S4.SS1.p4.3.m3.1.1.2">ùúã</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.3.m3.1c">\hat{\pi}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p4.3.m3.1d">over^ start_ARG italic_œÄ end_ARG</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.5"><span class="ltx_text ltx_font_bold" id="S4.SS1.p5.5.1">Latent canonical mesh regressor encoder.</span>
We then predict the discrete latent representation of the canonical mesh. The Transformer encoder inputs are the features extracted by the CNN backbone. Before being fed to the Transformer encoder, we apply a 1x1 convolution on the image features to make them of dimension and obtain <math alttext="X^{\prime}_{mesh}\in\mathbb{R}^{H\times W\times D}" class="ltx_Math" display="inline" id="S4.SS1.p5.1.m1.1"><semantics id="S4.SS1.p5.1.m1.1a"><mrow id="S4.SS1.p5.1.m1.1.1" xref="S4.SS1.p5.1.m1.1.1.cmml"><msubsup id="S4.SS1.p5.1.m1.1.1.2" xref="S4.SS1.p5.1.m1.1.1.2.cmml"><mi id="S4.SS1.p5.1.m1.1.1.2.2.2" xref="S4.SS1.p5.1.m1.1.1.2.2.2.cmml">X</mi><mrow id="S4.SS1.p5.1.m1.1.1.2.3" xref="S4.SS1.p5.1.m1.1.1.2.3.cmml"><mi id="S4.SS1.p5.1.m1.1.1.2.3.2" xref="S4.SS1.p5.1.m1.1.1.2.3.2.cmml">m</mi><mo id="S4.SS1.p5.1.m1.1.1.2.3.1" xref="S4.SS1.p5.1.m1.1.1.2.3.1.cmml">‚Å¢</mo><mi id="S4.SS1.p5.1.m1.1.1.2.3.3" xref="S4.SS1.p5.1.m1.1.1.2.3.3.cmml">e</mi><mo id="S4.SS1.p5.1.m1.1.1.2.3.1a" xref="S4.SS1.p5.1.m1.1.1.2.3.1.cmml">‚Å¢</mo><mi id="S4.SS1.p5.1.m1.1.1.2.3.4" xref="S4.SS1.p5.1.m1.1.1.2.3.4.cmml">s</mi><mo id="S4.SS1.p5.1.m1.1.1.2.3.1b" xref="S4.SS1.p5.1.m1.1.1.2.3.1.cmml">‚Å¢</mo><mi id="S4.SS1.p5.1.m1.1.1.2.3.5" xref="S4.SS1.p5.1.m1.1.1.2.3.5.cmml">h</mi></mrow><mo id="S4.SS1.p5.1.m1.1.1.2.2.3" xref="S4.SS1.p5.1.m1.1.1.2.2.3.cmml">‚Ä≤</mo></msubsup><mo id="S4.SS1.p5.1.m1.1.1.1" xref="S4.SS1.p5.1.m1.1.1.1.cmml">‚àà</mo><msup id="S4.SS1.p5.1.m1.1.1.3" xref="S4.SS1.p5.1.m1.1.1.3.cmml"><mi id="S4.SS1.p5.1.m1.1.1.3.2" xref="S4.SS1.p5.1.m1.1.1.3.2.cmml">‚Ñù</mi><mrow id="S4.SS1.p5.1.m1.1.1.3.3" xref="S4.SS1.p5.1.m1.1.1.3.3.cmml"><mi id="S4.SS1.p5.1.m1.1.1.3.3.2" xref="S4.SS1.p5.1.m1.1.1.3.3.2.cmml">H</mi><mo id="S4.SS1.p5.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.p5.1.m1.1.1.3.3.1.cmml">√ó</mo><mi id="S4.SS1.p5.1.m1.1.1.3.3.3" xref="S4.SS1.p5.1.m1.1.1.3.3.3.cmml">W</mi><mo id="S4.SS1.p5.1.m1.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S4.SS1.p5.1.m1.1.1.3.3.1.cmml">√ó</mo><mi id="S4.SS1.p5.1.m1.1.1.3.3.4" xref="S4.SS1.p5.1.m1.1.1.3.3.4.cmml">D</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.1.m1.1b"><apply id="S4.SS1.p5.1.m1.1.1.cmml" xref="S4.SS1.p5.1.m1.1.1"><in id="S4.SS1.p5.1.m1.1.1.1.cmml" xref="S4.SS1.p5.1.m1.1.1.1"></in><apply id="S4.SS1.p5.1.m1.1.1.2.cmml" xref="S4.SS1.p5.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p5.1.m1.1.1.2.1.cmml" xref="S4.SS1.p5.1.m1.1.1.2">subscript</csymbol><apply id="S4.SS1.p5.1.m1.1.1.2.2.cmml" xref="S4.SS1.p5.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p5.1.m1.1.1.2.2.1.cmml" xref="S4.SS1.p5.1.m1.1.1.2">superscript</csymbol><ci id="S4.SS1.p5.1.m1.1.1.2.2.2.cmml" xref="S4.SS1.p5.1.m1.1.1.2.2.2">ùëã</ci><ci id="S4.SS1.p5.1.m1.1.1.2.2.3.cmml" xref="S4.SS1.p5.1.m1.1.1.2.2.3">‚Ä≤</ci></apply><apply id="S4.SS1.p5.1.m1.1.1.2.3.cmml" xref="S4.SS1.p5.1.m1.1.1.2.3"><times id="S4.SS1.p5.1.m1.1.1.2.3.1.cmml" xref="S4.SS1.p5.1.m1.1.1.2.3.1"></times><ci id="S4.SS1.p5.1.m1.1.1.2.3.2.cmml" xref="S4.SS1.p5.1.m1.1.1.2.3.2">ùëö</ci><ci id="S4.SS1.p5.1.m1.1.1.2.3.3.cmml" xref="S4.SS1.p5.1.m1.1.1.2.3.3">ùëí</ci><ci id="S4.SS1.p5.1.m1.1.1.2.3.4.cmml" xref="S4.SS1.p5.1.m1.1.1.2.3.4">ùë†</ci><ci id="S4.SS1.p5.1.m1.1.1.2.3.5.cmml" xref="S4.SS1.p5.1.m1.1.1.2.3.5">‚Ñé</ci></apply></apply><apply id="S4.SS1.p5.1.m1.1.1.3.cmml" xref="S4.SS1.p5.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p5.1.m1.1.1.3.1.cmml" xref="S4.SS1.p5.1.m1.1.1.3">superscript</csymbol><ci id="S4.SS1.p5.1.m1.1.1.3.2.cmml" xref="S4.SS1.p5.1.m1.1.1.3.2">‚Ñù</ci><apply id="S4.SS1.p5.1.m1.1.1.3.3.cmml" xref="S4.SS1.p5.1.m1.1.1.3.3"><times id="S4.SS1.p5.1.m1.1.1.3.3.1.cmml" xref="S4.SS1.p5.1.m1.1.1.3.3.1"></times><ci id="S4.SS1.p5.1.m1.1.1.3.3.2.cmml" xref="S4.SS1.p5.1.m1.1.1.3.3.2">ùêª</ci><ci id="S4.SS1.p5.1.m1.1.1.3.3.3.cmml" xref="S4.SS1.p5.1.m1.1.1.3.3.3">ùëä</ci><ci id="S4.SS1.p5.1.m1.1.1.3.3.4.cmml" xref="S4.SS1.p5.1.m1.1.1.3.3.4">ùê∑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.1.m1.1c">X^{\prime}_{mesh}\in\mathbb{R}^{H\times W\times D}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p5.1.m1.1d">italic_X start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_m italic_e italic_s italic_h end_POSTSUBSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_H √ó italic_W √ó italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> where <math alttext="D" class="ltx_Math" display="inline" id="S4.SS1.p5.2.m2.1"><semantics id="S4.SS1.p5.2.m2.1a"><mi id="S4.SS1.p5.2.m2.1.1" xref="S4.SS1.p5.2.m2.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.2.m2.1b"><ci id="S4.SS1.p5.2.m2.1.1.cmml" xref="S4.SS1.p5.2.m2.1.1">ùê∑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.2.m2.1c">D</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p5.2.m2.1d">italic_D</annotation></semantics></math> is the hidden state size of the Transformer. These features are flattened to obtain <math alttext="HW" class="ltx_Math" display="inline" id="S4.SS1.p5.3.m3.1"><semantics id="S4.SS1.p5.3.m3.1a"><mrow id="S4.SS1.p5.3.m3.1.1" xref="S4.SS1.p5.3.m3.1.1.cmml"><mi id="S4.SS1.p5.3.m3.1.1.2" xref="S4.SS1.p5.3.m3.1.1.2.cmml">H</mi><mo id="S4.SS1.p5.3.m3.1.1.1" xref="S4.SS1.p5.3.m3.1.1.1.cmml">‚Å¢</mo><mi id="S4.SS1.p5.3.m3.1.1.3" xref="S4.SS1.p5.3.m3.1.1.3.cmml">W</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.3.m3.1b"><apply id="S4.SS1.p5.3.m3.1.1.cmml" xref="S4.SS1.p5.3.m3.1.1"><times id="S4.SS1.p5.3.m3.1.1.1.cmml" xref="S4.SS1.p5.3.m3.1.1.1"></times><ci id="S4.SS1.p5.3.m3.1.1.2.cmml" xref="S4.SS1.p5.3.m3.1.1.2">ùêª</ci><ci id="S4.SS1.p5.3.m3.1.1.3.cmml" xref="S4.SS1.p5.3.m3.1.1.3">ùëä</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.3.m3.1c">HW</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p5.3.m3.1d">italic_H italic_W</annotation></semantics></math> tokens of dimension <math alttext="D" class="ltx_Math" display="inline" id="S4.SS1.p5.4.m4.1"><semantics id="S4.SS1.p5.4.m4.1a"><mi id="S4.SS1.p5.4.m4.1.1" xref="S4.SS1.p5.4.m4.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.4.m4.1b"><ci id="S4.SS1.p5.4.m4.1.1.cmml" xref="S4.SS1.p5.4.m4.1.1">ùê∑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.4.m4.1c">D</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p5.4.m4.1d">italic_D</annotation></semantics></math>, and then we add positional encoding. The obtained tokens are fed to a Transformer encoder, using self-attention between all image tokens to output encoded image features <math alttext="X_{feat}\in\mathbb{R}^{HW\times D}" class="ltx_Math" display="inline" id="S4.SS1.p5.5.m5.1"><semantics id="S4.SS1.p5.5.m5.1a"><mrow id="S4.SS1.p5.5.m5.1.1" xref="S4.SS1.p5.5.m5.1.1.cmml"><msub id="S4.SS1.p5.5.m5.1.1.2" xref="S4.SS1.p5.5.m5.1.1.2.cmml"><mi id="S4.SS1.p5.5.m5.1.1.2.2" xref="S4.SS1.p5.5.m5.1.1.2.2.cmml">X</mi><mrow id="S4.SS1.p5.5.m5.1.1.2.3" xref="S4.SS1.p5.5.m5.1.1.2.3.cmml"><mi id="S4.SS1.p5.5.m5.1.1.2.3.2" xref="S4.SS1.p5.5.m5.1.1.2.3.2.cmml">f</mi><mo id="S4.SS1.p5.5.m5.1.1.2.3.1" xref="S4.SS1.p5.5.m5.1.1.2.3.1.cmml">‚Å¢</mo><mi id="S4.SS1.p5.5.m5.1.1.2.3.3" xref="S4.SS1.p5.5.m5.1.1.2.3.3.cmml">e</mi><mo id="S4.SS1.p5.5.m5.1.1.2.3.1a" xref="S4.SS1.p5.5.m5.1.1.2.3.1.cmml">‚Å¢</mo><mi id="S4.SS1.p5.5.m5.1.1.2.3.4" xref="S4.SS1.p5.5.m5.1.1.2.3.4.cmml">a</mi><mo id="S4.SS1.p5.5.m5.1.1.2.3.1b" xref="S4.SS1.p5.5.m5.1.1.2.3.1.cmml">‚Å¢</mo><mi id="S4.SS1.p5.5.m5.1.1.2.3.5" xref="S4.SS1.p5.5.m5.1.1.2.3.5.cmml">t</mi></mrow></msub><mo id="S4.SS1.p5.5.m5.1.1.1" xref="S4.SS1.p5.5.m5.1.1.1.cmml">‚àà</mo><msup id="S4.SS1.p5.5.m5.1.1.3" xref="S4.SS1.p5.5.m5.1.1.3.cmml"><mi id="S4.SS1.p5.5.m5.1.1.3.2" xref="S4.SS1.p5.5.m5.1.1.3.2.cmml">‚Ñù</mi><mrow id="S4.SS1.p5.5.m5.1.1.3.3" xref="S4.SS1.p5.5.m5.1.1.3.3.cmml"><mrow id="S4.SS1.p5.5.m5.1.1.3.3.2" xref="S4.SS1.p5.5.m5.1.1.3.3.2.cmml"><mi id="S4.SS1.p5.5.m5.1.1.3.3.2.2" xref="S4.SS1.p5.5.m5.1.1.3.3.2.2.cmml">H</mi><mo id="S4.SS1.p5.5.m5.1.1.3.3.2.1" xref="S4.SS1.p5.5.m5.1.1.3.3.2.1.cmml">‚Å¢</mo><mi id="S4.SS1.p5.5.m5.1.1.3.3.2.3" xref="S4.SS1.p5.5.m5.1.1.3.3.2.3.cmml">W</mi></mrow><mo id="S4.SS1.p5.5.m5.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.p5.5.m5.1.1.3.3.1.cmml">√ó</mo><mi id="S4.SS1.p5.5.m5.1.1.3.3.3" xref="S4.SS1.p5.5.m5.1.1.3.3.3.cmml">D</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.5.m5.1b"><apply id="S4.SS1.p5.5.m5.1.1.cmml" xref="S4.SS1.p5.5.m5.1.1"><in id="S4.SS1.p5.5.m5.1.1.1.cmml" xref="S4.SS1.p5.5.m5.1.1.1"></in><apply id="S4.SS1.p5.5.m5.1.1.2.cmml" xref="S4.SS1.p5.5.m5.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p5.5.m5.1.1.2.1.cmml" xref="S4.SS1.p5.5.m5.1.1.2">subscript</csymbol><ci id="S4.SS1.p5.5.m5.1.1.2.2.cmml" xref="S4.SS1.p5.5.m5.1.1.2.2">ùëã</ci><apply id="S4.SS1.p5.5.m5.1.1.2.3.cmml" xref="S4.SS1.p5.5.m5.1.1.2.3"><times id="S4.SS1.p5.5.m5.1.1.2.3.1.cmml" xref="S4.SS1.p5.5.m5.1.1.2.3.1"></times><ci id="S4.SS1.p5.5.m5.1.1.2.3.2.cmml" xref="S4.SS1.p5.5.m5.1.1.2.3.2">ùëì</ci><ci id="S4.SS1.p5.5.m5.1.1.2.3.3.cmml" xref="S4.SS1.p5.5.m5.1.1.2.3.3">ùëí</ci><ci id="S4.SS1.p5.5.m5.1.1.2.3.4.cmml" xref="S4.SS1.p5.5.m5.1.1.2.3.4">ùëé</ci><ci id="S4.SS1.p5.5.m5.1.1.2.3.5.cmml" xref="S4.SS1.p5.5.m5.1.1.2.3.5">ùë°</ci></apply></apply><apply id="S4.SS1.p5.5.m5.1.1.3.cmml" xref="S4.SS1.p5.5.m5.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p5.5.m5.1.1.3.1.cmml" xref="S4.SS1.p5.5.m5.1.1.3">superscript</csymbol><ci id="S4.SS1.p5.5.m5.1.1.3.2.cmml" xref="S4.SS1.p5.5.m5.1.1.3.2">‚Ñù</ci><apply id="S4.SS1.p5.5.m5.1.1.3.3.cmml" xref="S4.SS1.p5.5.m5.1.1.3.3"><times id="S4.SS1.p5.5.m5.1.1.3.3.1.cmml" xref="S4.SS1.p5.5.m5.1.1.3.3.1"></times><apply id="S4.SS1.p5.5.m5.1.1.3.3.2.cmml" xref="S4.SS1.p5.5.m5.1.1.3.3.2"><times id="S4.SS1.p5.5.m5.1.1.3.3.2.1.cmml" xref="S4.SS1.p5.5.m5.1.1.3.3.2.1"></times><ci id="S4.SS1.p5.5.m5.1.1.3.3.2.2.cmml" xref="S4.SS1.p5.5.m5.1.1.3.3.2.2">ùêª</ci><ci id="S4.SS1.p5.5.m5.1.1.3.3.2.3.cmml" xref="S4.SS1.p5.5.m5.1.1.3.3.2.3">ùëä</ci></apply><ci id="S4.SS1.p5.5.m5.1.1.3.3.3.cmml" xref="S4.SS1.p5.5.m5.1.1.3.3.3">ùê∑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.5.m5.1c">X_{feat}\in\mathbb{R}^{HW\times D}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p5.5.m5.1d">italic_X start_POSTSUBSCRIPT italic_f italic_e italic_a italic_t end_POSTSUBSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_H italic_W √ó italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p6">
<p class="ltx_p" id="S4.SS1.p6.10"><span class="ltx_text ltx_font_bold" id="S4.SS1.p6.10.1">Latent canonical mesh regressor decoder.</span>
The Transformer decoder takes as inputs <math alttext="N" class="ltx_Math" display="inline" id="S4.SS1.p6.1.m1.1"><semantics id="S4.SS1.p6.1.m1.1a"><mi id="S4.SS1.p6.1.m1.1.1" xref="S4.SS1.p6.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.1.m1.1b"><ci id="S4.SS1.p6.1.m1.1.1.cmml" xref="S4.SS1.p6.1.m1.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p6.1.m1.1d">italic_N</annotation></semantics></math> learned mesh tokens <math alttext="M_{t}" class="ltx_Math" display="inline" id="S4.SS1.p6.2.m2.1"><semantics id="S4.SS1.p6.2.m2.1a"><msub id="S4.SS1.p6.2.m2.1.1" xref="S4.SS1.p6.2.m2.1.1.cmml"><mi id="S4.SS1.p6.2.m2.1.1.2" xref="S4.SS1.p6.2.m2.1.1.2.cmml">M</mi><mi id="S4.SS1.p6.2.m2.1.1.3" xref="S4.SS1.p6.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.2.m2.1b"><apply id="S4.SS1.p6.2.m2.1.1.cmml" xref="S4.SS1.p6.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p6.2.m2.1.1.1.cmml" xref="S4.SS1.p6.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.p6.2.m2.1.1.2.cmml" xref="S4.SS1.p6.2.m2.1.1.2">ùëÄ</ci><ci id="S4.SS1.p6.2.m2.1.1.3.cmml" xref="S4.SS1.p6.2.m2.1.1.3">ùë°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.2.m2.1c">M_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p6.2.m2.1d">italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> of size <math alttext="D" class="ltx_Math" display="inline" id="S4.SS1.p6.3.m3.1"><semantics id="S4.SS1.p6.3.m3.1a"><mi id="S4.SS1.p6.3.m3.1.1" xref="S4.SS1.p6.3.m3.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.3.m3.1b"><ci id="S4.SS1.p6.3.m3.1.1.cmml" xref="S4.SS1.p6.3.m3.1.1">ùê∑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.3.m3.1c">D</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p6.3.m3.1d">italic_D</annotation></semantics></math>, each responsible for predicting an index of the Mesh-VQ-VAE discrete latent representation. We need to solve <math alttext="N" class="ltx_Math" display="inline" id="S4.SS1.p6.4.m4.1"><semantics id="S4.SS1.p6.4.m4.1a"><mi id="S4.SS1.p6.4.m4.1.1" xref="S4.SS1.p6.4.m4.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.4.m4.1b"><ci id="S4.SS1.p6.4.m4.1.1.cmml" xref="S4.SS1.p6.4.m4.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.4.m4.1c">N</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p6.4.m4.1d">italic_N</annotation></semantics></math> classification problems, one for each index. Each problem has <math alttext="S" class="ltx_Math" display="inline" id="S4.SS1.p6.5.m5.1"><semantics id="S4.SS1.p6.5.m5.1a"><mi id="S4.SS1.p6.5.m5.1.1" xref="S4.SS1.p6.5.m5.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.5.m5.1b"><ci id="S4.SS1.p6.5.m5.1.1.cmml" xref="S4.SS1.p6.5.m5.1.1">ùëÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.5.m5.1c">S</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p6.5.m5.1d">italic_S</annotation></semantics></math> classes, <math alttext="S" class="ltx_Math" display="inline" id="S4.SS1.p6.6.m6.1"><semantics id="S4.SS1.p6.6.m6.1a"><mi id="S4.SS1.p6.6.m6.1.1" xref="S4.SS1.p6.6.m6.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.6.m6.1b"><ci id="S4.SS1.p6.6.m6.1.1.cmml" xref="S4.SS1.p6.6.m6.1.1">ùëÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.6.m6.1c">S</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p6.6.m6.1d">italic_S</annotation></semantics></math> corresponding to the size of the Mesh-VQ-VAE codebook. The Transformer decoder consists of self-attention between learned tokens and cross-attention with image features. It outputs latent mesh features <math alttext="z_{feat}\in\mathbb{R}^{N\times D}" class="ltx_Math" display="inline" id="S4.SS1.p6.7.m7.1"><semantics id="S4.SS1.p6.7.m7.1a"><mrow id="S4.SS1.p6.7.m7.1.1" xref="S4.SS1.p6.7.m7.1.1.cmml"><msub id="S4.SS1.p6.7.m7.1.1.2" xref="S4.SS1.p6.7.m7.1.1.2.cmml"><mi id="S4.SS1.p6.7.m7.1.1.2.2" xref="S4.SS1.p6.7.m7.1.1.2.2.cmml">z</mi><mrow id="S4.SS1.p6.7.m7.1.1.2.3" xref="S4.SS1.p6.7.m7.1.1.2.3.cmml"><mi id="S4.SS1.p6.7.m7.1.1.2.3.2" xref="S4.SS1.p6.7.m7.1.1.2.3.2.cmml">f</mi><mo id="S4.SS1.p6.7.m7.1.1.2.3.1" xref="S4.SS1.p6.7.m7.1.1.2.3.1.cmml">‚Å¢</mo><mi id="S4.SS1.p6.7.m7.1.1.2.3.3" xref="S4.SS1.p6.7.m7.1.1.2.3.3.cmml">e</mi><mo id="S4.SS1.p6.7.m7.1.1.2.3.1a" xref="S4.SS1.p6.7.m7.1.1.2.3.1.cmml">‚Å¢</mo><mi id="S4.SS1.p6.7.m7.1.1.2.3.4" xref="S4.SS1.p6.7.m7.1.1.2.3.4.cmml">a</mi><mo id="S4.SS1.p6.7.m7.1.1.2.3.1b" xref="S4.SS1.p6.7.m7.1.1.2.3.1.cmml">‚Å¢</mo><mi id="S4.SS1.p6.7.m7.1.1.2.3.5" xref="S4.SS1.p6.7.m7.1.1.2.3.5.cmml">t</mi></mrow></msub><mo id="S4.SS1.p6.7.m7.1.1.1" xref="S4.SS1.p6.7.m7.1.1.1.cmml">‚àà</mo><msup id="S4.SS1.p6.7.m7.1.1.3" xref="S4.SS1.p6.7.m7.1.1.3.cmml"><mi id="S4.SS1.p6.7.m7.1.1.3.2" xref="S4.SS1.p6.7.m7.1.1.3.2.cmml">‚Ñù</mi><mrow id="S4.SS1.p6.7.m7.1.1.3.3" xref="S4.SS1.p6.7.m7.1.1.3.3.cmml"><mi id="S4.SS1.p6.7.m7.1.1.3.3.2" xref="S4.SS1.p6.7.m7.1.1.3.3.2.cmml">N</mi><mo id="S4.SS1.p6.7.m7.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.p6.7.m7.1.1.3.3.1.cmml">√ó</mo><mi id="S4.SS1.p6.7.m7.1.1.3.3.3" xref="S4.SS1.p6.7.m7.1.1.3.3.3.cmml">D</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.7.m7.1b"><apply id="S4.SS1.p6.7.m7.1.1.cmml" xref="S4.SS1.p6.7.m7.1.1"><in id="S4.SS1.p6.7.m7.1.1.1.cmml" xref="S4.SS1.p6.7.m7.1.1.1"></in><apply id="S4.SS1.p6.7.m7.1.1.2.cmml" xref="S4.SS1.p6.7.m7.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p6.7.m7.1.1.2.1.cmml" xref="S4.SS1.p6.7.m7.1.1.2">subscript</csymbol><ci id="S4.SS1.p6.7.m7.1.1.2.2.cmml" xref="S4.SS1.p6.7.m7.1.1.2.2">ùëß</ci><apply id="S4.SS1.p6.7.m7.1.1.2.3.cmml" xref="S4.SS1.p6.7.m7.1.1.2.3"><times id="S4.SS1.p6.7.m7.1.1.2.3.1.cmml" xref="S4.SS1.p6.7.m7.1.1.2.3.1"></times><ci id="S4.SS1.p6.7.m7.1.1.2.3.2.cmml" xref="S4.SS1.p6.7.m7.1.1.2.3.2">ùëì</ci><ci id="S4.SS1.p6.7.m7.1.1.2.3.3.cmml" xref="S4.SS1.p6.7.m7.1.1.2.3.3">ùëí</ci><ci id="S4.SS1.p6.7.m7.1.1.2.3.4.cmml" xref="S4.SS1.p6.7.m7.1.1.2.3.4">ùëé</ci><ci id="S4.SS1.p6.7.m7.1.1.2.3.5.cmml" xref="S4.SS1.p6.7.m7.1.1.2.3.5">ùë°</ci></apply></apply><apply id="S4.SS1.p6.7.m7.1.1.3.cmml" xref="S4.SS1.p6.7.m7.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p6.7.m7.1.1.3.1.cmml" xref="S4.SS1.p6.7.m7.1.1.3">superscript</csymbol><ci id="S4.SS1.p6.7.m7.1.1.3.2.cmml" xref="S4.SS1.p6.7.m7.1.1.3.2">‚Ñù</ci><apply id="S4.SS1.p6.7.m7.1.1.3.3.cmml" xref="S4.SS1.p6.7.m7.1.1.3.3"><times id="S4.SS1.p6.7.m7.1.1.3.3.1.cmml" xref="S4.SS1.p6.7.m7.1.1.3.3.1"></times><ci id="S4.SS1.p6.7.m7.1.1.3.3.2.cmml" xref="S4.SS1.p6.7.m7.1.1.3.3.2">ùëÅ</ci><ci id="S4.SS1.p6.7.m7.1.1.3.3.3.cmml" xref="S4.SS1.p6.7.m7.1.1.3.3.3">ùê∑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.7.m7.1c">z_{feat}\in\mathbb{R}^{N\times D}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p6.7.m7.1d">italic_z start_POSTSUBSCRIPT italic_f italic_e italic_a italic_t end_POSTSUBSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_N √ó italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>. Then (see <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S4.F2" title="In 4 Method ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">2</span></a>), to obtain the logits <math alttext="\hat{z}_{log}\in\mathbb{R}^{N\times S}" class="ltx_Math" display="inline" id="S4.SS1.p6.8.m8.1"><semantics id="S4.SS1.p6.8.m8.1a"><mrow id="S4.SS1.p6.8.m8.1.1" xref="S4.SS1.p6.8.m8.1.1.cmml"><msub id="S4.SS1.p6.8.m8.1.1.2" xref="S4.SS1.p6.8.m8.1.1.2.cmml"><mover accent="true" id="S4.SS1.p6.8.m8.1.1.2.2" xref="S4.SS1.p6.8.m8.1.1.2.2.cmml"><mi id="S4.SS1.p6.8.m8.1.1.2.2.2" xref="S4.SS1.p6.8.m8.1.1.2.2.2.cmml">z</mi><mo id="S4.SS1.p6.8.m8.1.1.2.2.1" xref="S4.SS1.p6.8.m8.1.1.2.2.1.cmml">^</mo></mover><mrow id="S4.SS1.p6.8.m8.1.1.2.3" xref="S4.SS1.p6.8.m8.1.1.2.3.cmml"><mi id="S4.SS1.p6.8.m8.1.1.2.3.2" xref="S4.SS1.p6.8.m8.1.1.2.3.2.cmml">l</mi><mo id="S4.SS1.p6.8.m8.1.1.2.3.1" xref="S4.SS1.p6.8.m8.1.1.2.3.1.cmml">‚Å¢</mo><mi id="S4.SS1.p6.8.m8.1.1.2.3.3" xref="S4.SS1.p6.8.m8.1.1.2.3.3.cmml">o</mi><mo id="S4.SS1.p6.8.m8.1.1.2.3.1a" xref="S4.SS1.p6.8.m8.1.1.2.3.1.cmml">‚Å¢</mo><mi id="S4.SS1.p6.8.m8.1.1.2.3.4" xref="S4.SS1.p6.8.m8.1.1.2.3.4.cmml">g</mi></mrow></msub><mo id="S4.SS1.p6.8.m8.1.1.1" xref="S4.SS1.p6.8.m8.1.1.1.cmml">‚àà</mo><msup id="S4.SS1.p6.8.m8.1.1.3" xref="S4.SS1.p6.8.m8.1.1.3.cmml"><mi id="S4.SS1.p6.8.m8.1.1.3.2" xref="S4.SS1.p6.8.m8.1.1.3.2.cmml">‚Ñù</mi><mrow id="S4.SS1.p6.8.m8.1.1.3.3" xref="S4.SS1.p6.8.m8.1.1.3.3.cmml"><mi id="S4.SS1.p6.8.m8.1.1.3.3.2" xref="S4.SS1.p6.8.m8.1.1.3.3.2.cmml">N</mi><mo id="S4.SS1.p6.8.m8.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.p6.8.m8.1.1.3.3.1.cmml">√ó</mo><mi id="S4.SS1.p6.8.m8.1.1.3.3.3" xref="S4.SS1.p6.8.m8.1.1.3.3.3.cmml">S</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.8.m8.1b"><apply id="S4.SS1.p6.8.m8.1.1.cmml" xref="S4.SS1.p6.8.m8.1.1"><in id="S4.SS1.p6.8.m8.1.1.1.cmml" xref="S4.SS1.p6.8.m8.1.1.1"></in><apply id="S4.SS1.p6.8.m8.1.1.2.cmml" xref="S4.SS1.p6.8.m8.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p6.8.m8.1.1.2.1.cmml" xref="S4.SS1.p6.8.m8.1.1.2">subscript</csymbol><apply id="S4.SS1.p6.8.m8.1.1.2.2.cmml" xref="S4.SS1.p6.8.m8.1.1.2.2"><ci id="S4.SS1.p6.8.m8.1.1.2.2.1.cmml" xref="S4.SS1.p6.8.m8.1.1.2.2.1">^</ci><ci id="S4.SS1.p6.8.m8.1.1.2.2.2.cmml" xref="S4.SS1.p6.8.m8.1.1.2.2.2">ùëß</ci></apply><apply id="S4.SS1.p6.8.m8.1.1.2.3.cmml" xref="S4.SS1.p6.8.m8.1.1.2.3"><times id="S4.SS1.p6.8.m8.1.1.2.3.1.cmml" xref="S4.SS1.p6.8.m8.1.1.2.3.1"></times><ci id="S4.SS1.p6.8.m8.1.1.2.3.2.cmml" xref="S4.SS1.p6.8.m8.1.1.2.3.2">ùëô</ci><ci id="S4.SS1.p6.8.m8.1.1.2.3.3.cmml" xref="S4.SS1.p6.8.m8.1.1.2.3.3">ùëú</ci><ci id="S4.SS1.p6.8.m8.1.1.2.3.4.cmml" xref="S4.SS1.p6.8.m8.1.1.2.3.4">ùëî</ci></apply></apply><apply id="S4.SS1.p6.8.m8.1.1.3.cmml" xref="S4.SS1.p6.8.m8.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p6.8.m8.1.1.3.1.cmml" xref="S4.SS1.p6.8.m8.1.1.3">superscript</csymbol><ci id="S4.SS1.p6.8.m8.1.1.3.2.cmml" xref="S4.SS1.p6.8.m8.1.1.3.2">‚Ñù</ci><apply id="S4.SS1.p6.8.m8.1.1.3.3.cmml" xref="S4.SS1.p6.8.m8.1.1.3.3"><times id="S4.SS1.p6.8.m8.1.1.3.3.1.cmml" xref="S4.SS1.p6.8.m8.1.1.3.3.1"></times><ci id="S4.SS1.p6.8.m8.1.1.3.3.2.cmml" xref="S4.SS1.p6.8.m8.1.1.3.3.2">ùëÅ</ci><ci id="S4.SS1.p6.8.m8.1.1.3.3.3.cmml" xref="S4.SS1.p6.8.m8.1.1.3.3.3">ùëÜ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.8.m8.1c">\hat{z}_{log}\in\mathbb{R}^{N\times S}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p6.8.m8.1d">over^ start_ARG italic_z end_ARG start_POSTSUBSCRIPT italic_l italic_o italic_g end_POSTSUBSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_N √ó italic_S end_POSTSUPERSCRIPT</annotation></semantics></math>, we rely on the mesh features as well as on the previously predicted rotation and camera. We obtain the predicted discrete representation <math alttext="\hat{z}_{q}\in\{1,\dots,S\}^{N}" class="ltx_Math" display="inline" id="S4.SS1.p6.9.m9.3"><semantics id="S4.SS1.p6.9.m9.3a"><mrow id="S4.SS1.p6.9.m9.3.4" xref="S4.SS1.p6.9.m9.3.4.cmml"><msub id="S4.SS1.p6.9.m9.3.4.2" xref="S4.SS1.p6.9.m9.3.4.2.cmml"><mover accent="true" id="S4.SS1.p6.9.m9.3.4.2.2" xref="S4.SS1.p6.9.m9.3.4.2.2.cmml"><mi id="S4.SS1.p6.9.m9.3.4.2.2.2" xref="S4.SS1.p6.9.m9.3.4.2.2.2.cmml">z</mi><mo id="S4.SS1.p6.9.m9.3.4.2.2.1" xref="S4.SS1.p6.9.m9.3.4.2.2.1.cmml">^</mo></mover><mi id="S4.SS1.p6.9.m9.3.4.2.3" xref="S4.SS1.p6.9.m9.3.4.2.3.cmml">q</mi></msub><mo id="S4.SS1.p6.9.m9.3.4.1" xref="S4.SS1.p6.9.m9.3.4.1.cmml">‚àà</mo><msup id="S4.SS1.p6.9.m9.3.4.3" xref="S4.SS1.p6.9.m9.3.4.3.cmml"><mrow id="S4.SS1.p6.9.m9.3.4.3.2.2" xref="S4.SS1.p6.9.m9.3.4.3.2.1.cmml"><mo id="S4.SS1.p6.9.m9.3.4.3.2.2.1" stretchy="false" xref="S4.SS1.p6.9.m9.3.4.3.2.1.cmml">{</mo><mn id="S4.SS1.p6.9.m9.1.1" xref="S4.SS1.p6.9.m9.1.1.cmml">1</mn><mo id="S4.SS1.p6.9.m9.3.4.3.2.2.2" xref="S4.SS1.p6.9.m9.3.4.3.2.1.cmml">,</mo><mi id="S4.SS1.p6.9.m9.2.2" mathvariant="normal" xref="S4.SS1.p6.9.m9.2.2.cmml">‚Ä¶</mi><mo id="S4.SS1.p6.9.m9.3.4.3.2.2.3" xref="S4.SS1.p6.9.m9.3.4.3.2.1.cmml">,</mo><mi id="S4.SS1.p6.9.m9.3.3" xref="S4.SS1.p6.9.m9.3.3.cmml">S</mi><mo id="S4.SS1.p6.9.m9.3.4.3.2.2.4" stretchy="false" xref="S4.SS1.p6.9.m9.3.4.3.2.1.cmml">}</mo></mrow><mi id="S4.SS1.p6.9.m9.3.4.3.3" xref="S4.SS1.p6.9.m9.3.4.3.3.cmml">N</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.9.m9.3b"><apply id="S4.SS1.p6.9.m9.3.4.cmml" xref="S4.SS1.p6.9.m9.3.4"><in id="S4.SS1.p6.9.m9.3.4.1.cmml" xref="S4.SS1.p6.9.m9.3.4.1"></in><apply id="S4.SS1.p6.9.m9.3.4.2.cmml" xref="S4.SS1.p6.9.m9.3.4.2"><csymbol cd="ambiguous" id="S4.SS1.p6.9.m9.3.4.2.1.cmml" xref="S4.SS1.p6.9.m9.3.4.2">subscript</csymbol><apply id="S4.SS1.p6.9.m9.3.4.2.2.cmml" xref="S4.SS1.p6.9.m9.3.4.2.2"><ci id="S4.SS1.p6.9.m9.3.4.2.2.1.cmml" xref="S4.SS1.p6.9.m9.3.4.2.2.1">^</ci><ci id="S4.SS1.p6.9.m9.3.4.2.2.2.cmml" xref="S4.SS1.p6.9.m9.3.4.2.2.2">ùëß</ci></apply><ci id="S4.SS1.p6.9.m9.3.4.2.3.cmml" xref="S4.SS1.p6.9.m9.3.4.2.3">ùëû</ci></apply><apply id="S4.SS1.p6.9.m9.3.4.3.cmml" xref="S4.SS1.p6.9.m9.3.4.3"><csymbol cd="ambiguous" id="S4.SS1.p6.9.m9.3.4.3.1.cmml" xref="S4.SS1.p6.9.m9.3.4.3">superscript</csymbol><set id="S4.SS1.p6.9.m9.3.4.3.2.1.cmml" xref="S4.SS1.p6.9.m9.3.4.3.2.2"><cn id="S4.SS1.p6.9.m9.1.1.cmml" type="integer" xref="S4.SS1.p6.9.m9.1.1">1</cn><ci id="S4.SS1.p6.9.m9.2.2.cmml" xref="S4.SS1.p6.9.m9.2.2">‚Ä¶</ci><ci id="S4.SS1.p6.9.m9.3.3.cmml" xref="S4.SS1.p6.9.m9.3.3">ùëÜ</ci></set><ci id="S4.SS1.p6.9.m9.3.4.3.3.cmml" xref="S4.SS1.p6.9.m9.3.4.3.3">ùëÅ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.9.m9.3c">\hat{z}_{q}\in\{1,\dots,S\}^{N}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p6.9.m9.3d">over^ start_ARG italic_z end_ARG start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ‚àà { 1 , ‚Ä¶ , italic_S } start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> by applying an <math alttext="\arg\max(\cdot)" class="ltx_Math" display="inline" id="S4.SS1.p6.10.m10.2"><semantics id="S4.SS1.p6.10.m10.2a"><mrow id="S4.SS1.p6.10.m10.2.3" xref="S4.SS1.p6.10.m10.2.3.cmml"><mi id="S4.SS1.p6.10.m10.2.3.1" xref="S4.SS1.p6.10.m10.2.3.1.cmml">arg</mi><mo id="S4.SS1.p6.10.m10.2.3a" lspace="0.167em" xref="S4.SS1.p6.10.m10.2.3.cmml">‚Å°</mo><mrow id="S4.SS1.p6.10.m10.2.3.2.2" xref="S4.SS1.p6.10.m10.2.3.2.1.cmml"><mi id="S4.SS1.p6.10.m10.1.1" xref="S4.SS1.p6.10.m10.1.1.cmml">max</mi><mo id="S4.SS1.p6.10.m10.2.3.2.2a" xref="S4.SS1.p6.10.m10.2.3.2.1.cmml">‚Å°</mo><mrow id="S4.SS1.p6.10.m10.2.3.2.2.1" xref="S4.SS1.p6.10.m10.2.3.2.1.cmml"><mo id="S4.SS1.p6.10.m10.2.3.2.2.1.1" stretchy="false" xref="S4.SS1.p6.10.m10.2.3.2.1.cmml">(</mo><mo id="S4.SS1.p6.10.m10.2.2" lspace="0em" rspace="0em" xref="S4.SS1.p6.10.m10.2.2.cmml">‚ãÖ</mo><mo id="S4.SS1.p6.10.m10.2.3.2.2.1.2" stretchy="false" xref="S4.SS1.p6.10.m10.2.3.2.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.10.m10.2b"><apply id="S4.SS1.p6.10.m10.2.3.cmml" xref="S4.SS1.p6.10.m10.2.3"><arg id="S4.SS1.p6.10.m10.2.3.1.cmml" xref="S4.SS1.p6.10.m10.2.3.1"></arg><apply id="S4.SS1.p6.10.m10.2.3.2.1.cmml" xref="S4.SS1.p6.10.m10.2.3.2.2"><max id="S4.SS1.p6.10.m10.1.1.cmml" xref="S4.SS1.p6.10.m10.1.1"></max><ci id="S4.SS1.p6.10.m10.2.2.cmml" xref="S4.SS1.p6.10.m10.2.2">‚ãÖ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.10.m10.2c">\arg\max(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p6.10.m10.2d">roman_arg roman_max ( ‚ãÖ )</annotation></semantics></math> operation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p7">
<p class="ltx_p" id="S4.SS1.p7.4"><span class="ltx_text ltx_font_bold" id="S4.SS1.p7.4.1">Reconstructing the full mesh.</span>
From the discrete latent mesh representation <math alttext="\hat{z}_{q}\in\{1,\dots,S\}^{N}" class="ltx_Math" display="inline" id="S4.SS1.p7.1.m1.3"><semantics id="S4.SS1.p7.1.m1.3a"><mrow id="S4.SS1.p7.1.m1.3.4" xref="S4.SS1.p7.1.m1.3.4.cmml"><msub id="S4.SS1.p7.1.m1.3.4.2" xref="S4.SS1.p7.1.m1.3.4.2.cmml"><mover accent="true" id="S4.SS1.p7.1.m1.3.4.2.2" xref="S4.SS1.p7.1.m1.3.4.2.2.cmml"><mi id="S4.SS1.p7.1.m1.3.4.2.2.2" xref="S4.SS1.p7.1.m1.3.4.2.2.2.cmml">z</mi><mo id="S4.SS1.p7.1.m1.3.4.2.2.1" xref="S4.SS1.p7.1.m1.3.4.2.2.1.cmml">^</mo></mover><mi id="S4.SS1.p7.1.m1.3.4.2.3" xref="S4.SS1.p7.1.m1.3.4.2.3.cmml">q</mi></msub><mo id="S4.SS1.p7.1.m1.3.4.1" xref="S4.SS1.p7.1.m1.3.4.1.cmml">‚àà</mo><msup id="S4.SS1.p7.1.m1.3.4.3" xref="S4.SS1.p7.1.m1.3.4.3.cmml"><mrow id="S4.SS1.p7.1.m1.3.4.3.2.2" xref="S4.SS1.p7.1.m1.3.4.3.2.1.cmml"><mo id="S4.SS1.p7.1.m1.3.4.3.2.2.1" stretchy="false" xref="S4.SS1.p7.1.m1.3.4.3.2.1.cmml">{</mo><mn id="S4.SS1.p7.1.m1.1.1" xref="S4.SS1.p7.1.m1.1.1.cmml">1</mn><mo id="S4.SS1.p7.1.m1.3.4.3.2.2.2" xref="S4.SS1.p7.1.m1.3.4.3.2.1.cmml">,</mo><mi id="S4.SS1.p7.1.m1.2.2" mathvariant="normal" xref="S4.SS1.p7.1.m1.2.2.cmml">‚Ä¶</mi><mo id="S4.SS1.p7.1.m1.3.4.3.2.2.3" xref="S4.SS1.p7.1.m1.3.4.3.2.1.cmml">,</mo><mi id="S4.SS1.p7.1.m1.3.3" xref="S4.SS1.p7.1.m1.3.3.cmml">S</mi><mo id="S4.SS1.p7.1.m1.3.4.3.2.2.4" stretchy="false" xref="S4.SS1.p7.1.m1.3.4.3.2.1.cmml">}</mo></mrow><mi id="S4.SS1.p7.1.m1.3.4.3.3" xref="S4.SS1.p7.1.m1.3.4.3.3.cmml">N</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p7.1.m1.3b"><apply id="S4.SS1.p7.1.m1.3.4.cmml" xref="S4.SS1.p7.1.m1.3.4"><in id="S4.SS1.p7.1.m1.3.4.1.cmml" xref="S4.SS1.p7.1.m1.3.4.1"></in><apply id="S4.SS1.p7.1.m1.3.4.2.cmml" xref="S4.SS1.p7.1.m1.3.4.2"><csymbol cd="ambiguous" id="S4.SS1.p7.1.m1.3.4.2.1.cmml" xref="S4.SS1.p7.1.m1.3.4.2">subscript</csymbol><apply id="S4.SS1.p7.1.m1.3.4.2.2.cmml" xref="S4.SS1.p7.1.m1.3.4.2.2"><ci id="S4.SS1.p7.1.m1.3.4.2.2.1.cmml" xref="S4.SS1.p7.1.m1.3.4.2.2.1">^</ci><ci id="S4.SS1.p7.1.m1.3.4.2.2.2.cmml" xref="S4.SS1.p7.1.m1.3.4.2.2.2">ùëß</ci></apply><ci id="S4.SS1.p7.1.m1.3.4.2.3.cmml" xref="S4.SS1.p7.1.m1.3.4.2.3">ùëû</ci></apply><apply id="S4.SS1.p7.1.m1.3.4.3.cmml" xref="S4.SS1.p7.1.m1.3.4.3"><csymbol cd="ambiguous" id="S4.SS1.p7.1.m1.3.4.3.1.cmml" xref="S4.SS1.p7.1.m1.3.4.3">superscript</csymbol><set id="S4.SS1.p7.1.m1.3.4.3.2.1.cmml" xref="S4.SS1.p7.1.m1.3.4.3.2.2"><cn id="S4.SS1.p7.1.m1.1.1.cmml" type="integer" xref="S4.SS1.p7.1.m1.1.1">1</cn><ci id="S4.SS1.p7.1.m1.2.2.cmml" xref="S4.SS1.p7.1.m1.2.2">‚Ä¶</ci><ci id="S4.SS1.p7.1.m1.3.3.cmml" xref="S4.SS1.p7.1.m1.3.3">ùëÜ</ci></set><ci id="S4.SS1.p7.1.m1.3.4.3.3.cmml" xref="S4.SS1.p7.1.m1.3.4.3.3">ùëÅ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p7.1.m1.3c">\hat{z}_{q}\in\{1,\dots,S\}^{N}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p7.1.m1.3d">over^ start_ARG italic_z end_ARG start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ‚àà { 1 , ‚Ä¶ , italic_S } start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT</annotation></semantics></math>, we use the decoder of the introduced Mesh-VQ-VAE to reconstruct the vertices of a full canonical mesh <math alttext="\hat{V}_{c}\in\mathbb{R}^{6890\times 3}" class="ltx_Math" display="inline" id="S4.SS1.p7.2.m2.1"><semantics id="S4.SS1.p7.2.m2.1a"><mrow id="S4.SS1.p7.2.m2.1.1" xref="S4.SS1.p7.2.m2.1.1.cmml"><msub id="S4.SS1.p7.2.m2.1.1.2" xref="S4.SS1.p7.2.m2.1.1.2.cmml"><mover accent="true" id="S4.SS1.p7.2.m2.1.1.2.2" xref="S4.SS1.p7.2.m2.1.1.2.2.cmml"><mi id="S4.SS1.p7.2.m2.1.1.2.2.2" xref="S4.SS1.p7.2.m2.1.1.2.2.2.cmml">V</mi><mo id="S4.SS1.p7.2.m2.1.1.2.2.1" xref="S4.SS1.p7.2.m2.1.1.2.2.1.cmml">^</mo></mover><mi id="S4.SS1.p7.2.m2.1.1.2.3" xref="S4.SS1.p7.2.m2.1.1.2.3.cmml">c</mi></msub><mo id="S4.SS1.p7.2.m2.1.1.1" xref="S4.SS1.p7.2.m2.1.1.1.cmml">‚àà</mo><msup id="S4.SS1.p7.2.m2.1.1.3" xref="S4.SS1.p7.2.m2.1.1.3.cmml"><mi id="S4.SS1.p7.2.m2.1.1.3.2" xref="S4.SS1.p7.2.m2.1.1.3.2.cmml">‚Ñù</mi><mrow id="S4.SS1.p7.2.m2.1.1.3.3" xref="S4.SS1.p7.2.m2.1.1.3.3.cmml"><mn id="S4.SS1.p7.2.m2.1.1.3.3.2" xref="S4.SS1.p7.2.m2.1.1.3.3.2.cmml">6890</mn><mo id="S4.SS1.p7.2.m2.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.p7.2.m2.1.1.3.3.1.cmml">√ó</mo><mn id="S4.SS1.p7.2.m2.1.1.3.3.3" xref="S4.SS1.p7.2.m2.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p7.2.m2.1b"><apply id="S4.SS1.p7.2.m2.1.1.cmml" xref="S4.SS1.p7.2.m2.1.1"><in id="S4.SS1.p7.2.m2.1.1.1.cmml" xref="S4.SS1.p7.2.m2.1.1.1"></in><apply id="S4.SS1.p7.2.m2.1.1.2.cmml" xref="S4.SS1.p7.2.m2.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p7.2.m2.1.1.2.1.cmml" xref="S4.SS1.p7.2.m2.1.1.2">subscript</csymbol><apply id="S4.SS1.p7.2.m2.1.1.2.2.cmml" xref="S4.SS1.p7.2.m2.1.1.2.2"><ci id="S4.SS1.p7.2.m2.1.1.2.2.1.cmml" xref="S4.SS1.p7.2.m2.1.1.2.2.1">^</ci><ci id="S4.SS1.p7.2.m2.1.1.2.2.2.cmml" xref="S4.SS1.p7.2.m2.1.1.2.2.2">ùëâ</ci></apply><ci id="S4.SS1.p7.2.m2.1.1.2.3.cmml" xref="S4.SS1.p7.2.m2.1.1.2.3">ùëê</ci></apply><apply id="S4.SS1.p7.2.m2.1.1.3.cmml" xref="S4.SS1.p7.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p7.2.m2.1.1.3.1.cmml" xref="S4.SS1.p7.2.m2.1.1.3">superscript</csymbol><ci id="S4.SS1.p7.2.m2.1.1.3.2.cmml" xref="S4.SS1.p7.2.m2.1.1.3.2">‚Ñù</ci><apply id="S4.SS1.p7.2.m2.1.1.3.3.cmml" xref="S4.SS1.p7.2.m2.1.1.3.3"><times id="S4.SS1.p7.2.m2.1.1.3.3.1.cmml" xref="S4.SS1.p7.2.m2.1.1.3.3.1"></times><cn id="S4.SS1.p7.2.m2.1.1.3.3.2.cmml" type="integer" xref="S4.SS1.p7.2.m2.1.1.3.3.2">6890</cn><cn id="S4.SS1.p7.2.m2.1.1.3.3.3.cmml" type="integer" xref="S4.SS1.p7.2.m2.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p7.2.m2.1c">\hat{V}_{c}\in\mathbb{R}^{6890\times 3}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p7.2.m2.1d">over^ start_ARG italic_V end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT 6890 √ó 3 end_POSTSUPERSCRIPT</annotation></semantics></math>. We apply the predicted rotation <math alttext="\hat{R}" class="ltx_Math" display="inline" id="S4.SS1.p7.3.m3.1"><semantics id="S4.SS1.p7.3.m3.1a"><mover accent="true" id="S4.SS1.p7.3.m3.1.1" xref="S4.SS1.p7.3.m3.1.1.cmml"><mi id="S4.SS1.p7.3.m3.1.1.2" xref="S4.SS1.p7.3.m3.1.1.2.cmml">R</mi><mo id="S4.SS1.p7.3.m3.1.1.1" xref="S4.SS1.p7.3.m3.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS1.p7.3.m3.1b"><apply id="S4.SS1.p7.3.m3.1.1.cmml" xref="S4.SS1.p7.3.m3.1.1"><ci id="S4.SS1.p7.3.m3.1.1.1.cmml" xref="S4.SS1.p7.3.m3.1.1.1">^</ci><ci id="S4.SS1.p7.3.m3.1.1.2.cmml" xref="S4.SS1.p7.3.m3.1.1.2">ùëÖ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p7.3.m3.1c">\hat{R}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p7.3.m3.1d">over^ start_ARG italic_R end_ARG</annotation></semantics></math> of the oriented mesh in the frame coordinates to the vertices to obtain the vertices <math alttext="\hat{V}" class="ltx_Math" display="inline" id="S4.SS1.p7.4.m4.1"><semantics id="S4.SS1.p7.4.m4.1a"><mover accent="true" id="S4.SS1.p7.4.m4.1.1" xref="S4.SS1.p7.4.m4.1.1.cmml"><mi id="S4.SS1.p7.4.m4.1.1.2" xref="S4.SS1.p7.4.m4.1.1.2.cmml">V</mi><mo id="S4.SS1.p7.4.m4.1.1.1" xref="S4.SS1.p7.4.m4.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS1.p7.4.m4.1b"><apply id="S4.SS1.p7.4.m4.1.1.cmml" xref="S4.SS1.p7.4.m4.1.1"><ci id="S4.SS1.p7.4.m4.1.1.1.cmml" xref="S4.SS1.p7.4.m4.1.1.1">^</ci><ci id="S4.SS1.p7.4.m4.1.1.2.cmml" xref="S4.SS1.p7.4.m4.1.1.2">ùëâ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p7.4.m4.1c">\hat{V}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p7.4.m4.1d">over^ start_ARG italic_V end_ARG</annotation></semantics></math>. This process is shown in <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S4.F2" title="In 4 Method ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="322" id="S4.SS1.1.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span class="ltx_text ltx_font_bold" id="S4.F3.2.1">Mesh-VQ-VAE reconstruction error.</span> Samples of reconstruction on the 3DPW test set. The error is in cm and corresponds to the Euclidean distance between the reconstruction‚Äôs original mesh and the corresponding vertex.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Training VQ-HPS</h3>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">VQ-HPS is trained in a supervised manner, given a dataset of RGB images paired with meshes. The canonical mesh predictor is trained solely on the discrete latent representation of meshes. To obtain the latent representation of the ground truth and decode the predicted indices to a full mesh, we use the Mesh-VQ-VAE, which is pre-trained and frozen during the VQ-HPS training. The fact that the Mesh-VQ-VAE is frozen during the training of VQ-HPS is crucial for making realistic predictions in the context of scarce data. Pre-training acts as a regularization, allowing for the reduction of the amount of training data.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1">Mesh-VQ-VAE.</span>
The Mesh-VQ-VAE (see <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S4.F2" title="In 4 Method ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">2</span></a>) is trained on the AMASS¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib29" title="">29</a>]</cite> dataset and finetuned on the 3DPW¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib30" title="">30</a>]</cite> training set. To ease the learning of the mesh discrete representation with a limited number of indices, we train the Mesh-VQ-VAE with non-oriented meshes translated to the origin (canonical meshes). The final reconstruction error is 4.7¬†mm. This reconstruction error is an important parameter as it corresponds to the minimal per-vertex error (see <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S5.SS2" title="5.2 Metrics ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Sec.</span>¬†<span class="ltx_text ltx_ref_tag">5.2</span></a>) we can obtain. Qualitative reconstruction results on 3DPW are shown in <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S4.F3" title="In 4.1 Proposed HPSE method ‚Ä£ 4 Method ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.1">Latent canonical meshes.</span>
For learning to predict the pose and shape, we only use the discrete representation of the canonical mesh as the training target.
The loss <math alttext="\mathcal{L}_{mesh}" class="ltx_Math" display="inline" id="S4.SS2.p3.1.m1.1"><semantics id="S4.SS2.p3.1.m1.1a"><msub id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p3.1.m1.1.1.2" xref="S4.SS2.p3.1.m1.1.1.2.cmml">‚Ñí</mi><mrow id="S4.SS2.p3.1.m1.1.1.3" xref="S4.SS2.p3.1.m1.1.1.3.cmml"><mi id="S4.SS2.p3.1.m1.1.1.3.2" xref="S4.SS2.p3.1.m1.1.1.3.2.cmml">m</mi><mo id="S4.SS2.p3.1.m1.1.1.3.1" xref="S4.SS2.p3.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.SS2.p3.1.m1.1.1.3.3" xref="S4.SS2.p3.1.m1.1.1.3.3.cmml">e</mi><mo id="S4.SS2.p3.1.m1.1.1.3.1a" xref="S4.SS2.p3.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.SS2.p3.1.m1.1.1.3.4" xref="S4.SS2.p3.1.m1.1.1.3.4.cmml">s</mi><mo id="S4.SS2.p3.1.m1.1.1.3.1b" xref="S4.SS2.p3.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.SS2.p3.1.m1.1.1.3.5" xref="S4.SS2.p3.1.m1.1.1.3.5.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><apply id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.1.m1.1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.p3.1.m1.1.1.2.cmml" xref="S4.SS2.p3.1.m1.1.1.2">‚Ñí</ci><apply id="S4.SS2.p3.1.m1.1.1.3.cmml" xref="S4.SS2.p3.1.m1.1.1.3"><times id="S4.SS2.p3.1.m1.1.1.3.1.cmml" xref="S4.SS2.p3.1.m1.1.1.3.1"></times><ci id="S4.SS2.p3.1.m1.1.1.3.2.cmml" xref="S4.SS2.p3.1.m1.1.1.3.2">ùëö</ci><ci id="S4.SS2.p3.1.m1.1.1.3.3.cmml" xref="S4.SS2.p3.1.m1.1.1.3.3">ùëí</ci><ci id="S4.SS2.p3.1.m1.1.1.3.4.cmml" xref="S4.SS2.p3.1.m1.1.1.3.4">ùë†</ci><ci id="S4.SS2.p3.1.m1.1.1.3.5.cmml" xref="S4.SS2.p3.1.m1.1.1.3.5">‚Ñé</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">\mathcal{L}_{mesh}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_m italic_e italic_s italic_h end_POSTSUBSCRIPT</annotation></semantics></math> is the cross-entropy between the discrete latent representation of the ground truth mesh and the prediction.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p4.1.1">Mesh rotation.</span>
We learn to predict the global orientation by computing the mean squared error between the ground truth and predicted rotation matrices.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p5">
<p class="ltx_p" id="S4.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p5.1.1">Reprojection.</span>
We add a reprojection error to guide the rotation learning and for better image alignment. It is computed between the 2D projection (using the predicted weak-perspective camera) of the 3D joints extracted from the predicted mesh and the 2D ground truth joints. This loss is computed using the SMPL 24 joints, which can be extracted from the full mesh using a joint regressor <math alttext="\mathcal{J}_{smpl}" class="ltx_Math" display="inline" id="S4.SS2.p5.1.m1.1"><semantics id="S4.SS2.p5.1.m1.1a"><msub id="S4.SS2.p5.1.m1.1.1" xref="S4.SS2.p5.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p5.1.m1.1.1.2" xref="S4.SS2.p5.1.m1.1.1.2.cmml">ùí•</mi><mrow id="S4.SS2.p5.1.m1.1.1.3" xref="S4.SS2.p5.1.m1.1.1.3.cmml"><mi id="S4.SS2.p5.1.m1.1.1.3.2" xref="S4.SS2.p5.1.m1.1.1.3.2.cmml">s</mi><mo id="S4.SS2.p5.1.m1.1.1.3.1" xref="S4.SS2.p5.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.SS2.p5.1.m1.1.1.3.3" xref="S4.SS2.p5.1.m1.1.1.3.3.cmml">m</mi><mo id="S4.SS2.p5.1.m1.1.1.3.1a" xref="S4.SS2.p5.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.SS2.p5.1.m1.1.1.3.4" xref="S4.SS2.p5.1.m1.1.1.3.4.cmml">p</mi><mo id="S4.SS2.p5.1.m1.1.1.3.1b" xref="S4.SS2.p5.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.SS2.p5.1.m1.1.1.3.5" xref="S4.SS2.p5.1.m1.1.1.3.5.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.1.m1.1b"><apply id="S4.SS2.p5.1.m1.1.1.cmml" xref="S4.SS2.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p5.1.m1.1.1.1.cmml" xref="S4.SS2.p5.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.p5.1.m1.1.1.2.cmml" xref="S4.SS2.p5.1.m1.1.1.2">ùí•</ci><apply id="S4.SS2.p5.1.m1.1.1.3.cmml" xref="S4.SS2.p5.1.m1.1.1.3"><times id="S4.SS2.p5.1.m1.1.1.3.1.cmml" xref="S4.SS2.p5.1.m1.1.1.3.1"></times><ci id="S4.SS2.p5.1.m1.1.1.3.2.cmml" xref="S4.SS2.p5.1.m1.1.1.3.2">ùë†</ci><ci id="S4.SS2.p5.1.m1.1.1.3.3.cmml" xref="S4.SS2.p5.1.m1.1.1.3.3">ùëö</ci><ci id="S4.SS2.p5.1.m1.1.1.3.4.cmml" xref="S4.SS2.p5.1.m1.1.1.3.4">ùëù</ci><ci id="S4.SS2.p5.1.m1.1.1.3.5.cmml" xref="S4.SS2.p5.1.m1.1.1.3.5">ùëô</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.1.m1.1c">\mathcal{J}_{smpl}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p5.1.m1.1d">caligraphic_J start_POSTSUBSCRIPT italic_s italic_m italic_p italic_l end_POSTSUBSCRIPT</annotation></semantics></math> (see <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S3" title="3 Background ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Sec.</span>¬†<span class="ltx_text ltx_ref_tag">3</span></a>). The reprojection loss is computed as:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{2D}=||\hat{s}\Pi(\hat{J}_{3D})+\hat{t}-J_{2D}||_{1}," class="ltx_Math" display="block" id="S4.E1.m1.1"><semantics id="S4.E1.m1.1a"><mrow id="S4.E1.m1.1.1.1" xref="S4.E1.m1.1.1.1.1.cmml"><mrow id="S4.E1.m1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.cmml"><msub id="S4.E1.m1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.1.1.1.1.3.2" xref="S4.E1.m1.1.1.1.1.3.2.cmml">‚Ñí</mi><mrow id="S4.E1.m1.1.1.1.1.3.3" xref="S4.E1.m1.1.1.1.1.3.3.cmml"><mn id="S4.E1.m1.1.1.1.1.3.3.2" xref="S4.E1.m1.1.1.1.1.3.3.2.cmml">2</mn><mo id="S4.E1.m1.1.1.1.1.3.3.1" xref="S4.E1.m1.1.1.1.1.3.3.1.cmml">‚Å¢</mo><mi id="S4.E1.m1.1.1.1.1.3.3.3" xref="S4.E1.m1.1.1.1.1.3.3.3.cmml">D</mi></mrow></msub><mo id="S4.E1.m1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.2.cmml">=</mo><msub id="S4.E1.m1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.cmml"><mrow id="S4.E1.m1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.2.cmml"><mo id="S4.E1.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="S4.E1.m1.1.1.1.1.1.1.2.1.cmml">‚Äñ</mo><mrow id="S4.E1.m1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S4.E1.m1.1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">s</mi><mo id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">^</mo></mover><mo id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">‚Å¢</mo><mi id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.4" mathvariant="normal" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.4.cmml">Œ†</mi><mo id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.2a" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">‚Å¢</mo><mrow id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">J</mi><mo id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mrow id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mn id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">3</mn><mo id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">D</mi></mrow></msub><mo id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E1.m1.1.1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml">+</mo><mover accent="true" id="S4.E1.m1.1.1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E1.m1.1.1.1.1.1.1.1.1.1.3.2" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.3.2.cmml">t</mi><mo id="S4.E1.m1.1.1.1.1.1.1.1.1.1.3.1" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.3.1.cmml">^</mo></mover></mrow><mo id="S4.E1.m1.1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.1.1.1.2.cmml">‚àí</mo><msub id="S4.E1.m1.1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E1.m1.1.1.1.1.1.1.1.1.3.2" xref="S4.E1.m1.1.1.1.1.1.1.1.1.3.2.cmml">J</mi><mrow id="S4.E1.m1.1.1.1.1.1.1.1.1.3.3" xref="S4.E1.m1.1.1.1.1.1.1.1.1.3.3.cmml"><mn id="S4.E1.m1.1.1.1.1.1.1.1.1.3.3.2" xref="S4.E1.m1.1.1.1.1.1.1.1.1.3.3.2.cmml">2</mn><mo id="S4.E1.m1.1.1.1.1.1.1.1.1.3.3.1" xref="S4.E1.m1.1.1.1.1.1.1.1.1.3.3.1.cmml">‚Å¢</mo><mi id="S4.E1.m1.1.1.1.1.1.1.1.1.3.3.3" xref="S4.E1.m1.1.1.1.1.1.1.1.1.3.3.3.cmml">D</mi></mrow></msub></mrow><mo id="S4.E1.m1.1.1.1.1.1.1.1.3" stretchy="false" xref="S4.E1.m1.1.1.1.1.1.1.2.1.cmml">‚Äñ</mo></mrow><mn id="S4.E1.m1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.3.cmml">1</mn></msub></mrow><mo id="S4.E1.m1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.1b"><apply id="S4.E1.m1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1"><eq id="S4.E1.m1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.2"></eq><apply id="S4.E1.m1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.1.1.3">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.1.1.3.2">‚Ñí</ci><apply id="S4.E1.m1.1.1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.1.1.3.3"><times id="S4.E1.m1.1.1.1.1.3.3.1.cmml" xref="S4.E1.m1.1.1.1.1.3.3.1"></times><cn id="S4.E1.m1.1.1.1.1.3.3.2.cmml" type="integer" xref="S4.E1.m1.1.1.1.1.3.3.2">2</cn><ci id="S4.E1.m1.1.1.1.1.3.3.3.cmml" xref="S4.E1.m1.1.1.1.1.3.3.3">ùê∑</ci></apply></apply><apply id="S4.E1.m1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1">subscript</csymbol><apply id="S4.E1.m1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E1.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S4.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1"><minus id="S4.E1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.2"></minus><apply id="S4.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1"><plus id="S4.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.2"></plus><apply id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1"><times id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.2"></times><apply id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3"><ci id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3.1">^</ci><ci id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2">ùë†</ci></apply><ci id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.4.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.4">Œ†</ci><apply id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2"><ci id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1">^</ci><ci id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2">ùêΩ</ci></apply><apply id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3"><times id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1"></times><cn id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" type="integer" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2">3</cn><ci id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3">ùê∑</ci></apply></apply></apply><apply id="S4.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.3"><ci id="S4.E1.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.3.1">^</ci><ci id="S4.E1.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.3.2">ùë°</ci></apply></apply><apply id="S4.E1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.3.2">ùêΩ</ci><apply id="S4.E1.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.3.3"><times id="S4.E1.m1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.3.3.1"></times><cn id="S4.E1.m1.1.1.1.1.1.1.1.1.3.3.2.cmml" type="integer" xref="S4.E1.m1.1.1.1.1.1.1.1.1.3.3.2">2</cn><ci id="S4.E1.m1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.3.3.3">ùê∑</ci></apply></apply></apply></apply><cn id="S4.E1.m1.1.1.1.1.1.3.cmml" type="integer" xref="S4.E1.m1.1.1.1.1.1.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.1c">\mathcal{L}_{2D}=||\hat{s}\Pi(\hat{J}_{3D})+\hat{t}-J_{2D}||_{1},</annotation><annotation encoding="application/x-llamapun" id="S4.E1.m1.1d">caligraphic_L start_POSTSUBSCRIPT 2 italic_D end_POSTSUBSCRIPT = | | over^ start_ARG italic_s end_ARG roman_Œ† ( over^ start_ARG italic_J end_ARG start_POSTSUBSCRIPT 3 italic_D end_POSTSUBSCRIPT ) + over^ start_ARG italic_t end_ARG - italic_J start_POSTSUBSCRIPT 2 italic_D end_POSTSUBSCRIPT | | start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS2.p5.8">where <math alttext="\hat{s}" class="ltx_Math" display="inline" id="S4.SS2.p5.2.m1.1"><semantics id="S4.SS2.p5.2.m1.1a"><mover accent="true" id="S4.SS2.p5.2.m1.1.1" xref="S4.SS2.p5.2.m1.1.1.cmml"><mi id="S4.SS2.p5.2.m1.1.1.2" xref="S4.SS2.p5.2.m1.1.1.2.cmml">s</mi><mo id="S4.SS2.p5.2.m1.1.1.1" xref="S4.SS2.p5.2.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.2.m1.1b"><apply id="S4.SS2.p5.2.m1.1.1.cmml" xref="S4.SS2.p5.2.m1.1.1"><ci id="S4.SS2.p5.2.m1.1.1.1.cmml" xref="S4.SS2.p5.2.m1.1.1.1">^</ci><ci id="S4.SS2.p5.2.m1.1.1.2.cmml" xref="S4.SS2.p5.2.m1.1.1.2">ùë†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.2.m1.1c">\hat{s}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p5.2.m1.1d">over^ start_ARG italic_s end_ARG</annotation></semantics></math> is the predicted scale, <math alttext="\hat{t}" class="ltx_Math" display="inline" id="S4.SS2.p5.3.m2.1"><semantics id="S4.SS2.p5.3.m2.1a"><mover accent="true" id="S4.SS2.p5.3.m2.1.1" xref="S4.SS2.p5.3.m2.1.1.cmml"><mi id="S4.SS2.p5.3.m2.1.1.2" xref="S4.SS2.p5.3.m2.1.1.2.cmml">t</mi><mo id="S4.SS2.p5.3.m2.1.1.1" xref="S4.SS2.p5.3.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.3.m2.1b"><apply id="S4.SS2.p5.3.m2.1.1.cmml" xref="S4.SS2.p5.3.m2.1.1"><ci id="S4.SS2.p5.3.m2.1.1.1.cmml" xref="S4.SS2.p5.3.m2.1.1.1">^</ci><ci id="S4.SS2.p5.3.m2.1.1.2.cmml" xref="S4.SS2.p5.3.m2.1.1.2">ùë°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.3.m2.1c">\hat{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p5.3.m2.1d">over^ start_ARG italic_t end_ARG</annotation></semantics></math> the predicted 2D translation and <math alttext="\hat{J}_{3D}" class="ltx_Math" display="inline" id="S4.SS2.p5.4.m3.1"><semantics id="S4.SS2.p5.4.m3.1a"><msub id="S4.SS2.p5.4.m3.1.1" xref="S4.SS2.p5.4.m3.1.1.cmml"><mover accent="true" id="S4.SS2.p5.4.m3.1.1.2" xref="S4.SS2.p5.4.m3.1.1.2.cmml"><mi id="S4.SS2.p5.4.m3.1.1.2.2" xref="S4.SS2.p5.4.m3.1.1.2.2.cmml">J</mi><mo id="S4.SS2.p5.4.m3.1.1.2.1" xref="S4.SS2.p5.4.m3.1.1.2.1.cmml">^</mo></mover><mrow id="S4.SS2.p5.4.m3.1.1.3" xref="S4.SS2.p5.4.m3.1.1.3.cmml"><mn id="S4.SS2.p5.4.m3.1.1.3.2" xref="S4.SS2.p5.4.m3.1.1.3.2.cmml">3</mn><mo id="S4.SS2.p5.4.m3.1.1.3.1" xref="S4.SS2.p5.4.m3.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.SS2.p5.4.m3.1.1.3.3" xref="S4.SS2.p5.4.m3.1.1.3.3.cmml">D</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.4.m3.1b"><apply id="S4.SS2.p5.4.m3.1.1.cmml" xref="S4.SS2.p5.4.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.p5.4.m3.1.1.1.cmml" xref="S4.SS2.p5.4.m3.1.1">subscript</csymbol><apply id="S4.SS2.p5.4.m3.1.1.2.cmml" xref="S4.SS2.p5.4.m3.1.1.2"><ci id="S4.SS2.p5.4.m3.1.1.2.1.cmml" xref="S4.SS2.p5.4.m3.1.1.2.1">^</ci><ci id="S4.SS2.p5.4.m3.1.1.2.2.cmml" xref="S4.SS2.p5.4.m3.1.1.2.2">ùêΩ</ci></apply><apply id="S4.SS2.p5.4.m3.1.1.3.cmml" xref="S4.SS2.p5.4.m3.1.1.3"><times id="S4.SS2.p5.4.m3.1.1.3.1.cmml" xref="S4.SS2.p5.4.m3.1.1.3.1"></times><cn id="S4.SS2.p5.4.m3.1.1.3.2.cmml" type="integer" xref="S4.SS2.p5.4.m3.1.1.3.2">3</cn><ci id="S4.SS2.p5.4.m3.1.1.3.3.cmml" xref="S4.SS2.p5.4.m3.1.1.3.3">ùê∑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.4.m3.1c">\hat{J}_{3D}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p5.4.m3.1d">over^ start_ARG italic_J end_ARG start_POSTSUBSCRIPT 3 italic_D end_POSTSUBSCRIPT</annotation></semantics></math> are the 3D joints computed from the predicted oriented mesh vertices <math alttext="\hat{V}" class="ltx_Math" display="inline" id="S4.SS2.p5.5.m4.1"><semantics id="S4.SS2.p5.5.m4.1a"><mover accent="true" id="S4.SS2.p5.5.m4.1.1" xref="S4.SS2.p5.5.m4.1.1.cmml"><mi id="S4.SS2.p5.5.m4.1.1.2" xref="S4.SS2.p5.5.m4.1.1.2.cmml">V</mi><mo id="S4.SS2.p5.5.m4.1.1.1" xref="S4.SS2.p5.5.m4.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.5.m4.1b"><apply id="S4.SS2.p5.5.m4.1.1.cmml" xref="S4.SS2.p5.5.m4.1.1"><ci id="S4.SS2.p5.5.m4.1.1.1.cmml" xref="S4.SS2.p5.5.m4.1.1.1">^</ci><ci id="S4.SS2.p5.5.m4.1.1.2.cmml" xref="S4.SS2.p5.5.m4.1.1.2">ùëâ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.5.m4.1c">\hat{V}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p5.5.m4.1d">over^ start_ARG italic_V end_ARG</annotation></semantics></math>. <math alttext="\Pi" class="ltx_Math" display="inline" id="S4.SS2.p5.6.m5.1"><semantics id="S4.SS2.p5.6.m5.1a"><mi id="S4.SS2.p5.6.m5.1.1" mathvariant="normal" xref="S4.SS2.p5.6.m5.1.1.cmml">Œ†</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.6.m5.1b"><ci id="S4.SS2.p5.6.m5.1.1.cmml" xref="S4.SS2.p5.6.m5.1.1">Œ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.6.m5.1c">\Pi</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p5.6.m5.1d">roman_Œ†</annotation></semantics></math> is the orthographic projection using the matrix
<math alttext="\begin{bmatrix}1&amp;0&amp;0\\
0&amp;1&amp;0\end{bmatrix}^{\top}" class="ltx_Math" display="inline" id="S4.SS2.p5.7.m6.1"><semantics id="S4.SS2.p5.7.m6.1a"><msup id="S4.SS2.p5.7.m6.1.2" xref="S4.SS2.p5.7.m6.1.2.cmml"><mrow id="S4.SS2.p5.7.m6.1.1.3" xref="S4.SS2.p5.7.m6.1.1.2.cmml"><mo id="S4.SS2.p5.7.m6.1.1.3.1" xref="S4.SS2.p5.7.m6.1.1.2.1.cmml">[</mo><mtable columnspacing="5pt" id="S4.SS2.p5.7.m6.1.1.1.1" rowspacing="0pt" xref="S4.SS2.p5.7.m6.1.1.1.1.cmml"><mtr id="S4.SS2.p5.7.m6.1.1.1.1a" xref="S4.SS2.p5.7.m6.1.1.1.1.cmml"><mtd id="S4.SS2.p5.7.m6.1.1.1.1b" xref="S4.SS2.p5.7.m6.1.1.1.1.cmml"><mn id="S4.SS2.p5.7.m6.1.1.1.1.1.1.1" xref="S4.SS2.p5.7.m6.1.1.1.1.1.1.1.cmml">1</mn></mtd><mtd id="S4.SS2.p5.7.m6.1.1.1.1c" xref="S4.SS2.p5.7.m6.1.1.1.1.cmml"><mn id="S4.SS2.p5.7.m6.1.1.1.1.1.2.1" xref="S4.SS2.p5.7.m6.1.1.1.1.1.2.1.cmml">0</mn></mtd><mtd id="S4.SS2.p5.7.m6.1.1.1.1d" xref="S4.SS2.p5.7.m6.1.1.1.1.cmml"><mn id="S4.SS2.p5.7.m6.1.1.1.1.1.3.1" xref="S4.SS2.p5.7.m6.1.1.1.1.1.3.1.cmml">0</mn></mtd></mtr><mtr id="S4.SS2.p5.7.m6.1.1.1.1e" xref="S4.SS2.p5.7.m6.1.1.1.1.cmml"><mtd id="S4.SS2.p5.7.m6.1.1.1.1f" xref="S4.SS2.p5.7.m6.1.1.1.1.cmml"><mn id="S4.SS2.p5.7.m6.1.1.1.1.2.1.1" xref="S4.SS2.p5.7.m6.1.1.1.1.2.1.1.cmml">0</mn></mtd><mtd id="S4.SS2.p5.7.m6.1.1.1.1g" xref="S4.SS2.p5.7.m6.1.1.1.1.cmml"><mn id="S4.SS2.p5.7.m6.1.1.1.1.2.2.1" xref="S4.SS2.p5.7.m6.1.1.1.1.2.2.1.cmml">1</mn></mtd><mtd id="S4.SS2.p5.7.m6.1.1.1.1h" xref="S4.SS2.p5.7.m6.1.1.1.1.cmml"><mn id="S4.SS2.p5.7.m6.1.1.1.1.2.3.1" xref="S4.SS2.p5.7.m6.1.1.1.1.2.3.1.cmml">0</mn></mtd></mtr></mtable><mo id="S4.SS2.p5.7.m6.1.1.3.2" xref="S4.SS2.p5.7.m6.1.1.2.1.cmml">]</mo></mrow><mo id="S4.SS2.p5.7.m6.1.2.2" xref="S4.SS2.p5.7.m6.1.2.2.cmml">‚ä§</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.7.m6.1b"><apply id="S4.SS2.p5.7.m6.1.2.cmml" xref="S4.SS2.p5.7.m6.1.2"><csymbol cd="ambiguous" id="S4.SS2.p5.7.m6.1.2.1.cmml" xref="S4.SS2.p5.7.m6.1.2">superscript</csymbol><apply id="S4.SS2.p5.7.m6.1.1.2.cmml" xref="S4.SS2.p5.7.m6.1.1.3"><csymbol cd="latexml" id="S4.SS2.p5.7.m6.1.1.2.1.cmml" xref="S4.SS2.p5.7.m6.1.1.3.1">matrix</csymbol><matrix id="S4.SS2.p5.7.m6.1.1.1.1.cmml" xref="S4.SS2.p5.7.m6.1.1.1.1"><matrixrow id="S4.SS2.p5.7.m6.1.1.1.1a.cmml" xref="S4.SS2.p5.7.m6.1.1.1.1"><cn id="S4.SS2.p5.7.m6.1.1.1.1.1.1.1.cmml" type="integer" xref="S4.SS2.p5.7.m6.1.1.1.1.1.1.1">1</cn><cn id="S4.SS2.p5.7.m6.1.1.1.1.1.2.1.cmml" type="integer" xref="S4.SS2.p5.7.m6.1.1.1.1.1.2.1">0</cn><cn id="S4.SS2.p5.7.m6.1.1.1.1.1.3.1.cmml" type="integer" xref="S4.SS2.p5.7.m6.1.1.1.1.1.3.1">0</cn></matrixrow><matrixrow id="S4.SS2.p5.7.m6.1.1.1.1b.cmml" xref="S4.SS2.p5.7.m6.1.1.1.1"><cn id="S4.SS2.p5.7.m6.1.1.1.1.2.1.1.cmml" type="integer" xref="S4.SS2.p5.7.m6.1.1.1.1.2.1.1">0</cn><cn id="S4.SS2.p5.7.m6.1.1.1.1.2.2.1.cmml" type="integer" xref="S4.SS2.p5.7.m6.1.1.1.1.2.2.1">1</cn><cn id="S4.SS2.p5.7.m6.1.1.1.1.2.3.1.cmml" type="integer" xref="S4.SS2.p5.7.m6.1.1.1.1.2.3.1">0</cn></matrixrow></matrix></apply><csymbol cd="latexml" id="S4.SS2.p5.7.m6.1.2.2.cmml" xref="S4.SS2.p5.7.m6.1.2.2">top</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.7.m6.1c">\begin{bmatrix}1&amp;0&amp;0\\
0&amp;1&amp;0\end{bmatrix}^{\top}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p5.7.m6.1d">[ start_ARG start_ROW start_CELL 1 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL 1 end_CELL start_CELL 0 end_CELL end_ROW end_ARG ] start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="J_{2D}" class="ltx_Math" display="inline" id="S4.SS2.p5.8.m7.1"><semantics id="S4.SS2.p5.8.m7.1a"><msub id="S4.SS2.p5.8.m7.1.1" xref="S4.SS2.p5.8.m7.1.1.cmml"><mi id="S4.SS2.p5.8.m7.1.1.2" xref="S4.SS2.p5.8.m7.1.1.2.cmml">J</mi><mrow id="S4.SS2.p5.8.m7.1.1.3" xref="S4.SS2.p5.8.m7.1.1.3.cmml"><mn id="S4.SS2.p5.8.m7.1.1.3.2" xref="S4.SS2.p5.8.m7.1.1.3.2.cmml">2</mn><mo id="S4.SS2.p5.8.m7.1.1.3.1" xref="S4.SS2.p5.8.m7.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.SS2.p5.8.m7.1.1.3.3" xref="S4.SS2.p5.8.m7.1.1.3.3.cmml">D</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.8.m7.1b"><apply id="S4.SS2.p5.8.m7.1.1.cmml" xref="S4.SS2.p5.8.m7.1.1"><csymbol cd="ambiguous" id="S4.SS2.p5.8.m7.1.1.1.cmml" xref="S4.SS2.p5.8.m7.1.1">subscript</csymbol><ci id="S4.SS2.p5.8.m7.1.1.2.cmml" xref="S4.SS2.p5.8.m7.1.1.2">ùêΩ</ci><apply id="S4.SS2.p5.8.m7.1.1.3.cmml" xref="S4.SS2.p5.8.m7.1.1.3"><times id="S4.SS2.p5.8.m7.1.1.3.1.cmml" xref="S4.SS2.p5.8.m7.1.1.3.1"></times><cn id="S4.SS2.p5.8.m7.1.1.3.2.cmml" type="integer" xref="S4.SS2.p5.8.m7.1.1.3.2">2</cn><ci id="S4.SS2.p5.8.m7.1.1.3.3.cmml" xref="S4.SS2.p5.8.m7.1.1.3.3">ùê∑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.8.m7.1c">J_{2D}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p5.8.m7.1d">italic_J start_POSTSUBSCRIPT 2 italic_D end_POSTSUBSCRIPT</annotation></semantics></math> denotes the ground truth 2D joints.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p6">
<p class="ltx_p" id="S4.SS2.p6.5"><span class="ltx_text ltx_font_bold" id="S4.SS2.p6.5.1">Learning scheme.</span>
The rotation prediction is learned using <math alttext="\mathcal{L}_{rot}" class="ltx_Math" display="inline" id="S4.SS2.p6.1.m1.1"><semantics id="S4.SS2.p6.1.m1.1a"><msub id="S4.SS2.p6.1.m1.1.1" xref="S4.SS2.p6.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p6.1.m1.1.1.2" xref="S4.SS2.p6.1.m1.1.1.2.cmml">‚Ñí</mi><mrow id="S4.SS2.p6.1.m1.1.1.3" xref="S4.SS2.p6.1.m1.1.1.3.cmml"><mi id="S4.SS2.p6.1.m1.1.1.3.2" xref="S4.SS2.p6.1.m1.1.1.3.2.cmml">r</mi><mo id="S4.SS2.p6.1.m1.1.1.3.1" xref="S4.SS2.p6.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.SS2.p6.1.m1.1.1.3.3" xref="S4.SS2.p6.1.m1.1.1.3.3.cmml">o</mi><mo id="S4.SS2.p6.1.m1.1.1.3.1a" xref="S4.SS2.p6.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.SS2.p6.1.m1.1.1.3.4" xref="S4.SS2.p6.1.m1.1.1.3.4.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.1.m1.1b"><apply id="S4.SS2.p6.1.m1.1.1.cmml" xref="S4.SS2.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p6.1.m1.1.1.1.cmml" xref="S4.SS2.p6.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.p6.1.m1.1.1.2.cmml" xref="S4.SS2.p6.1.m1.1.1.2">‚Ñí</ci><apply id="S4.SS2.p6.1.m1.1.1.3.cmml" xref="S4.SS2.p6.1.m1.1.1.3"><times id="S4.SS2.p6.1.m1.1.1.3.1.cmml" xref="S4.SS2.p6.1.m1.1.1.3.1"></times><ci id="S4.SS2.p6.1.m1.1.1.3.2.cmml" xref="S4.SS2.p6.1.m1.1.1.3.2">ùëü</ci><ci id="S4.SS2.p6.1.m1.1.1.3.3.cmml" xref="S4.SS2.p6.1.m1.1.1.3.3">ùëú</ci><ci id="S4.SS2.p6.1.m1.1.1.3.4.cmml" xref="S4.SS2.p6.1.m1.1.1.3.4">ùë°</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.1.m1.1c">\mathcal{L}_{rot}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p6.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_r italic_o italic_t end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\mathcal{L}_{2D}" class="ltx_Math" display="inline" id="S4.SS2.p6.2.m2.1"><semantics id="S4.SS2.p6.2.m2.1a"><msub id="S4.SS2.p6.2.m2.1.1" xref="S4.SS2.p6.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p6.2.m2.1.1.2" xref="S4.SS2.p6.2.m2.1.1.2.cmml">‚Ñí</mi><mrow id="S4.SS2.p6.2.m2.1.1.3" xref="S4.SS2.p6.2.m2.1.1.3.cmml"><mn id="S4.SS2.p6.2.m2.1.1.3.2" xref="S4.SS2.p6.2.m2.1.1.3.2.cmml">2</mn><mo id="S4.SS2.p6.2.m2.1.1.3.1" xref="S4.SS2.p6.2.m2.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.SS2.p6.2.m2.1.1.3.3" xref="S4.SS2.p6.2.m2.1.1.3.3.cmml">D</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.2.m2.1b"><apply id="S4.SS2.p6.2.m2.1.1.cmml" xref="S4.SS2.p6.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.p6.2.m2.1.1.1.cmml" xref="S4.SS2.p6.2.m2.1.1">subscript</csymbol><ci id="S4.SS2.p6.2.m2.1.1.2.cmml" xref="S4.SS2.p6.2.m2.1.1.2">‚Ñí</ci><apply id="S4.SS2.p6.2.m2.1.1.3.cmml" xref="S4.SS2.p6.2.m2.1.1.3"><times id="S4.SS2.p6.2.m2.1.1.3.1.cmml" xref="S4.SS2.p6.2.m2.1.1.3.1"></times><cn id="S4.SS2.p6.2.m2.1.1.3.2.cmml" type="integer" xref="S4.SS2.p6.2.m2.1.1.3.2">2</cn><ci id="S4.SS2.p6.2.m2.1.1.3.3.cmml" xref="S4.SS2.p6.2.m2.1.1.3.3">ùê∑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.2.m2.1c">\mathcal{L}_{2D}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p6.2.m2.1d">caligraphic_L start_POSTSUBSCRIPT 2 italic_D end_POSTSUBSCRIPT</annotation></semantics></math>. We use <math alttext="\mathcal{L}_{2D}" class="ltx_Math" display="inline" id="S4.SS2.p6.3.m3.1"><semantics id="S4.SS2.p6.3.m3.1a"><msub id="S4.SS2.p6.3.m3.1.1" xref="S4.SS2.p6.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p6.3.m3.1.1.2" xref="S4.SS2.p6.3.m3.1.1.2.cmml">‚Ñí</mi><mrow id="S4.SS2.p6.3.m3.1.1.3" xref="S4.SS2.p6.3.m3.1.1.3.cmml"><mn id="S4.SS2.p6.3.m3.1.1.3.2" xref="S4.SS2.p6.3.m3.1.1.3.2.cmml">2</mn><mo id="S4.SS2.p6.3.m3.1.1.3.1" xref="S4.SS2.p6.3.m3.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.SS2.p6.3.m3.1.1.3.3" xref="S4.SS2.p6.3.m3.1.1.3.3.cmml">D</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.3.m3.1b"><apply id="S4.SS2.p6.3.m3.1.1.cmml" xref="S4.SS2.p6.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.p6.3.m3.1.1.1.cmml" xref="S4.SS2.p6.3.m3.1.1">subscript</csymbol><ci id="S4.SS2.p6.3.m3.1.1.2.cmml" xref="S4.SS2.p6.3.m3.1.1.2">‚Ñí</ci><apply id="S4.SS2.p6.3.m3.1.1.3.cmml" xref="S4.SS2.p6.3.m3.1.1.3"><times id="S4.SS2.p6.3.m3.1.1.3.1.cmml" xref="S4.SS2.p6.3.m3.1.1.3.1"></times><cn id="S4.SS2.p6.3.m3.1.1.3.2.cmml" type="integer" xref="S4.SS2.p6.3.m3.1.1.3.2">2</cn><ci id="S4.SS2.p6.3.m3.1.1.3.3.cmml" xref="S4.SS2.p6.3.m3.1.1.3.3">ùê∑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.3.m3.1c">\mathcal{L}_{2D}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p6.3.m3.1d">caligraphic_L start_POSTSUBSCRIPT 2 italic_D end_POSTSUBSCRIPT</annotation></semantics></math> for the camera and <math alttext="\mathcal{L}_{mesh}" class="ltx_Math" display="inline" id="S4.SS2.p6.4.m4.1"><semantics id="S4.SS2.p6.4.m4.1a"><msub id="S4.SS2.p6.4.m4.1.1" xref="S4.SS2.p6.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p6.4.m4.1.1.2" xref="S4.SS2.p6.4.m4.1.1.2.cmml">‚Ñí</mi><mrow id="S4.SS2.p6.4.m4.1.1.3" xref="S4.SS2.p6.4.m4.1.1.3.cmml"><mi id="S4.SS2.p6.4.m4.1.1.3.2" xref="S4.SS2.p6.4.m4.1.1.3.2.cmml">m</mi><mo id="S4.SS2.p6.4.m4.1.1.3.1" xref="S4.SS2.p6.4.m4.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.SS2.p6.4.m4.1.1.3.3" xref="S4.SS2.p6.4.m4.1.1.3.3.cmml">e</mi><mo id="S4.SS2.p6.4.m4.1.1.3.1a" xref="S4.SS2.p6.4.m4.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.SS2.p6.4.m4.1.1.3.4" xref="S4.SS2.p6.4.m4.1.1.3.4.cmml">s</mi><mo id="S4.SS2.p6.4.m4.1.1.3.1b" xref="S4.SS2.p6.4.m4.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.SS2.p6.4.m4.1.1.3.5" xref="S4.SS2.p6.4.m4.1.1.3.5.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.4.m4.1b"><apply id="S4.SS2.p6.4.m4.1.1.cmml" xref="S4.SS2.p6.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS2.p6.4.m4.1.1.1.cmml" xref="S4.SS2.p6.4.m4.1.1">subscript</csymbol><ci id="S4.SS2.p6.4.m4.1.1.2.cmml" xref="S4.SS2.p6.4.m4.1.1.2">‚Ñí</ci><apply id="S4.SS2.p6.4.m4.1.1.3.cmml" xref="S4.SS2.p6.4.m4.1.1.3"><times id="S4.SS2.p6.4.m4.1.1.3.1.cmml" xref="S4.SS2.p6.4.m4.1.1.3.1"></times><ci id="S4.SS2.p6.4.m4.1.1.3.2.cmml" xref="S4.SS2.p6.4.m4.1.1.3.2">ùëö</ci><ci id="S4.SS2.p6.4.m4.1.1.3.3.cmml" xref="S4.SS2.p6.4.m4.1.1.3.3">ùëí</ci><ci id="S4.SS2.p6.4.m4.1.1.3.4.cmml" xref="S4.SS2.p6.4.m4.1.1.3.4">ùë†</ci><ci id="S4.SS2.p6.4.m4.1.1.3.5.cmml" xref="S4.SS2.p6.4.m4.1.1.3.5">‚Ñé</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.4.m4.1c">\mathcal{L}_{mesh}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p6.4.m4.1d">caligraphic_L start_POSTSUBSCRIPT italic_m italic_e italic_s italic_h end_POSTSUBSCRIPT</annotation></semantics></math> for the canonical mesh. <math alttext="\mathcal{L}_{2D}" class="ltx_Math" display="inline" id="S4.SS2.p6.5.m5.1"><semantics id="S4.SS2.p6.5.m5.1a"><msub id="S4.SS2.p6.5.m5.1.1" xref="S4.SS2.p6.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p6.5.m5.1.1.2" xref="S4.SS2.p6.5.m5.1.1.2.cmml">‚Ñí</mi><mrow id="S4.SS2.p6.5.m5.1.1.3" xref="S4.SS2.p6.5.m5.1.1.3.cmml"><mn id="S4.SS2.p6.5.m5.1.1.3.2" xref="S4.SS2.p6.5.m5.1.1.3.2.cmml">2</mn><mo id="S4.SS2.p6.5.m5.1.1.3.1" xref="S4.SS2.p6.5.m5.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.SS2.p6.5.m5.1.1.3.3" xref="S4.SS2.p6.5.m5.1.1.3.3.cmml">D</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.5.m5.1b"><apply id="S4.SS2.p6.5.m5.1.1.cmml" xref="S4.SS2.p6.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS2.p6.5.m5.1.1.1.cmml" xref="S4.SS2.p6.5.m5.1.1">subscript</csymbol><ci id="S4.SS2.p6.5.m5.1.1.2.cmml" xref="S4.SS2.p6.5.m5.1.1.2">‚Ñí</ci><apply id="S4.SS2.p6.5.m5.1.1.3.cmml" xref="S4.SS2.p6.5.m5.1.1.3"><times id="S4.SS2.p6.5.m5.1.1.3.1.cmml" xref="S4.SS2.p6.5.m5.1.1.3.1"></times><cn id="S4.SS2.p6.5.m5.1.1.3.2.cmml" type="integer" xref="S4.SS2.p6.5.m5.1.1.3.2">2</cn><ci id="S4.SS2.p6.5.m5.1.1.3.3.cmml" xref="S4.SS2.p6.5.m5.1.1.3.3">ùê∑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.5.m5.1c">\mathcal{L}_{2D}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p6.5.m5.1d">caligraphic_L start_POSTSUBSCRIPT 2 italic_D end_POSTSUBSCRIPT</annotation></semantics></math> might help to learn the pose, but we chose not to use it to demonstrate that the cross-entropy is sufficient for making accurate predictions.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Datasets</h3>
<div class="ltx_para ltx_noindent" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.1">AMASS.</span>
The Mesh-VQ-VAE is trained on AMASS¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib29" title="">29</a>]</cite>, a large human motion database in the SMPL¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib9" title="">9</a>]</cite> format. It contains more than 11000 motions and 300 subjects, which makes it representative of the variety of body poses and shapes.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.1">3DPW.</span>
This dataset¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib30" title="">30</a>]</cite> consists of 60 in-the-wild RGB videos with 3D ground truth for human bodies. We use the pre-defined splits for training, validation, and testing. Note that when training on a mix of datasets (see <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S5.SS4" title="5.4 Training on large-scale datasets ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Sec.</span>¬†<span class="ltx_text ltx_ref_tag">5.4</span></a>), we do not finetune models on 3DPW to assess generalization. We also evaluate VQ-HPS on 3DPW-OCC, a benchmark proposed in¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib78" title="">78</a>]</cite> containing videos of 3DPW with occlusions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p3.1.1">EMDB.</span>
EMDB¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib31" title="">31</a>]</cite> contains 81 indoor and outdoor videos with the ground truth SMPL bodies. We use EMDB1, which consists of the 17 most challenging sequences for testing, and train on the rest of the dataset (referred to as EMDB2 in¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib31" title="">31</a>]</cite>).</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p4.1.1">COCO.</span>
COCO¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib79" title="">79</a>]</cite> is a dataset of images annotated with 2D keypoints. For training a human mesh predictor, we follow¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib2" title="">2</a>]</cite> and use pseudo-ground truth meshes. We use the same annotations as¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib2" title="">2</a>]</cite>, with 28k images.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Metrics</h3>
<div class="ltx_para ltx_noindent" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We use several metrics to evaluate the predictions of VQ-HPS. All of them will be expressed in millimeters (mm) for the whole results section.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p2.1.1">Per-vertex error</span> (PVE) measures the Euclidean distance between the predicted vertices and the ground truth.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p3.1.1">Mean-per-joint error</span>
(MPJPE) measures the Euclidean distance between the predicted joints and the ground truth. In our case, the joints are extracted from the predicted mesh using a joint regressor similar to <math alttext="\mathcal{J}_{smpl}" class="ltx_Math" display="inline" id="S5.SS2.p3.1.m1.1"><semantics id="S5.SS2.p3.1.m1.1a"><msub id="S5.SS2.p3.1.m1.1.1" xref="S5.SS2.p3.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.SS2.p3.1.m1.1.1.2" xref="S5.SS2.p3.1.m1.1.1.2.cmml">ùí•</mi><mrow id="S5.SS2.p3.1.m1.1.1.3" xref="S5.SS2.p3.1.m1.1.1.3.cmml"><mi id="S5.SS2.p3.1.m1.1.1.3.2" xref="S5.SS2.p3.1.m1.1.1.3.2.cmml">s</mi><mo id="S5.SS2.p3.1.m1.1.1.3.1" xref="S5.SS2.p3.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S5.SS2.p3.1.m1.1.1.3.3" xref="S5.SS2.p3.1.m1.1.1.3.3.cmml">m</mi><mo id="S5.SS2.p3.1.m1.1.1.3.1a" xref="S5.SS2.p3.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S5.SS2.p3.1.m1.1.1.3.4" xref="S5.SS2.p3.1.m1.1.1.3.4.cmml">p</mi><mo id="S5.SS2.p3.1.m1.1.1.3.1b" xref="S5.SS2.p3.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S5.SS2.p3.1.m1.1.1.3.5" xref="S5.SS2.p3.1.m1.1.1.3.5.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.1.m1.1b"><apply id="S5.SS2.p3.1.m1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p3.1.m1.1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1">subscript</csymbol><ci id="S5.SS2.p3.1.m1.1.1.2.cmml" xref="S5.SS2.p3.1.m1.1.1.2">ùí•</ci><apply id="S5.SS2.p3.1.m1.1.1.3.cmml" xref="S5.SS2.p3.1.m1.1.1.3"><times id="S5.SS2.p3.1.m1.1.1.3.1.cmml" xref="S5.SS2.p3.1.m1.1.1.3.1"></times><ci id="S5.SS2.p3.1.m1.1.1.3.2.cmml" xref="S5.SS2.p3.1.m1.1.1.3.2">ùë†</ci><ci id="S5.SS2.p3.1.m1.1.1.3.3.cmml" xref="S5.SS2.p3.1.m1.1.1.3.3">ùëö</ci><ci id="S5.SS2.p3.1.m1.1.1.3.4.cmml" xref="S5.SS2.p3.1.m1.1.1.3.4">ùëù</ci><ci id="S5.SS2.p3.1.m1.1.1.3.5.cmml" xref="S5.SS2.p3.1.m1.1.1.3.5">ùëô</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.1.m1.1c">\mathcal{J}_{smpl}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p3.1.m1.1d">caligraphic_J start_POSTSUBSCRIPT italic_s italic_m italic_p italic_l end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p4.1.1">Procrustes-aligned mean-per-joint error</span>
(PA-MPJPE) measures the Euclidean distance between the predicted joints and the ground truth after a Procrustes alignment.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Training on limited data</h3>
<figure class="ltx_table" id="S5.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span><span class="ltx_text ltx_font_bold" id="S5.T1.5.1">Results on in-the-wild datasets</span> We compare VQ-HPS with SOTA methods using standard metrics on 3DPW trained with 3DPW (1st col.), 3DPW trained with COCO (2nd col.), EMDB trained with EMDB (3rd col.), and EMDB trained with COCO (4th col.). All results are given in¬†mm.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T1.3" style="width:390.3pt;height:57.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-108.1pt,16.0pt) scale(0.643398314219133,0.643398314219133) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T1.3.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T1.3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T1.3.3.3.4">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4" id="S5.T1.1.1.1.1">PVE <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T1.1.1.1.1.m1.1"><semantics id="S5.T1.1.1.1.1.m1.1a"><mo id="S5.T1.1.1.1.1.m1.1.1" stretchy="false" xref="S5.T1.1.1.1.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.1.m1.1b"><ci id="S5.T1.1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.1.1.1.1.m1.1d">‚Üì</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4" id="S5.T1.2.2.2.2">MPJPE <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T1.2.2.2.2.m1.1"><semantics id="S5.T1.2.2.2.2.m1.1a"><mo id="S5.T1.2.2.2.2.m1.1.1" stretchy="false" xref="S5.T1.2.2.2.2.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S5.T1.2.2.2.2.m1.1b"><ci id="S5.T1.2.2.2.2.m1.1.1.cmml" xref="S5.T1.2.2.2.2.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.2.2.2.2.m1.1d">‚Üì</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4" id="S5.T1.3.3.3.3">PA-MPJPE <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T1.3.3.3.3.m1.1"><semantics id="S5.T1.3.3.3.3.m1.1a"><mo id="S5.T1.3.3.3.3.m1.1.1" stretchy="false" xref="S5.T1.3.3.3.3.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S5.T1.3.3.3.3.m1.1b"><ci id="S5.T1.3.3.3.3.m1.1.1.cmml" xref="S5.T1.3.3.3.3.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.3.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.3.3.3.3.m1.1d">‚Üì</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.3.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T1.3.3.4.1.1">HMR¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib1" title="">1</a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.3.4.1.2">209.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.3.4.1.3">110.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.3.4.1.4">191.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.3.4.1.5">170.0</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S5.T1.3.3.4.1.6">177.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.3.4.1.7">94.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.3.4.1.8">172.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.3.4.1.9">149.2</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S5.T1.3.3.4.1.10">89.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.3.4.1.11">57.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.3.4.1.12">96.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.3.4.1.13">82.4</td>
</tr>
<tr class="ltx_tr" id="S5.T1.3.3.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.3.3.5.2.1">CLIFF¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib2" title="">2</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.5.2.2">223.9</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.5.2.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.3.3.5.2.3.1">105.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.5.2.4">163.3</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.5.2.5">163.2</td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S5.T1.3.3.5.2.6">188.8</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.5.2.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.3.3.5.2.7.1">89.3</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.5.2.8">144.7</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.5.2.9">143.0</td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S5.T1.3.3.5.2.10"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.3.3.5.2.10.1">89.2</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.5.2.11"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.3.3.5.2.11.1">56.8</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.5.2.12"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.3.3.5.2.12.1">86.9</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.5.2.13">81.6</td>
</tr>
<tr class="ltx_tr" id="S5.T1.3.3.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.3.3.6.3.1">FastMETRO-S¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib3" title="">3</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.6.3.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.3.3.6.3.2.1">176.3</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.6.3.3">107.8</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.6.3.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.3.3.6.3.4.1">151.0</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.6.3.5">
<span class="ltx_text ltx_font_bold" id="S5.T1.3.3.6.3.5.1">143.1</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S5.T1.3.3.6.3.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.3.3.6.3.6.1">157.0</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.6.3.7">95.8</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.6.3.8"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.3.3.6.3.8.1">132.9</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.6.3.9">
<span class="ltx_text ltx_font_bold" id="S5.T1.3.3.6.3.9.1">123.9</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S5.T1.3.3.6.3.10">104.6</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.6.3.11">57.0</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.6.3.12">95.5</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.6.3.13"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.3.3.6.3.13.1">80.2</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.3.3.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S5.T1.3.3.7.4.1">VQ-HPS (ours)</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.3.3.7.4.2"><span class="ltx_text ltx_font_bold" id="S5.T1.3.3.7.4.2.1">163.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.3.3.7.4.3"><span class="ltx_text ltx_font_bold" id="S5.T1.3.3.7.4.3.1">102.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.3.3.7.4.4"><span class="ltx_text ltx_font_bold" id="S5.T1.3.3.7.4.4.1">138.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.3.3.7.4.5">
<span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.3.3.7.4.5.1">152.7</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.3.3.7.4.6"><span class="ltx_text ltx_font_bold" id="S5.T1.3.3.7.4.6.1">139.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.3.3.7.4.7"><span class="ltx_text ltx_font_bold" id="S5.T1.3.3.7.4.7.1">88.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.3.3.7.4.8"><span class="ltx_text ltx_font_bold" id="S5.T1.3.3.7.4.8.1">117.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.3.3.7.4.9">
<span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.3.3.7.4.9.1">131.1</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.3.3.7.4.10"><span class="ltx_text ltx_font_bold" id="S5.T1.3.3.7.4.10.1">84.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.3.3.7.4.11"><span class="ltx_text ltx_font_bold" id="S5.T1.3.3.7.4.11.1">53.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.3.3.7.4.12"><span class="ltx_text ltx_font_bold" id="S5.T1.3.3.7.4.12.1">77.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.3.3.7.4.13"><span class="ltx_text ltx_font_bold" id="S5.T1.3.3.7.4.13.1">74.5</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="528" id="S5.F4.g1" src="x4.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span class="ltx_text ltx_font_bold" id="S5.F4.2.1">Qualitative results</span> We compare our method with HMR¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib1" title="">1</a>]</cite>, CLIFF¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib2" title="">2</a>]</cite> and FastMETRO-S¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib3" title="">3</a>]</cite> on 3DPW trained on 3DPW (first 3 rows), and EMDB trained on EMDB.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">We train VQ-HPS separately on the 3DPW, COCO, and EMDB training sets (see <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S5.SS1" title="5.1 Datasets ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Sec.</span>¬†<span class="ltx_text ltx_ref_tag">5.1</span></a>) to see how it performs when trained on limited data. We compare our performance with HMR¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib1" title="">1</a>]</cite>, CLIFF¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib2" title="">2</a>]</cite>, and FastMETRO¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib3" title="">3</a>]</cite> trained with the same data. We chose these 3 models for comparison because HMR is the basic architecture for parametric human mesh recovery, CLIFF is the SOTA for parametric HPSE, and FastMETRO is the SOTA for non-parametric HPSE and the closest method to ours. For these experiments, the backbone for all networks is ResNet-50¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib80" title="">80</a>]</cite> pre-trained on ImageNet¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib81" title="">81</a>]</cite>. We use the public implementation of FastMETRO and adapt the HMR and CLIFF implementations provided by¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib60" title="">60</a>]</cite>. For all comparisons, we use FastMETRO-S, as this version is the closest to VQ-HPS. When training on COCO, we propose an improved version of VQ-HPS, replacing the MLP of the latent canonical mesh regressor with a Transformer implementing self-attention between the latent mesh features <math alttext="z_{feat}" class="ltx_Math" display="inline" id="S5.SS3.p1.1.m1.1"><semantics id="S5.SS3.p1.1.m1.1a"><msub id="S5.SS3.p1.1.m1.1.1" xref="S5.SS3.p1.1.m1.1.1.cmml"><mi id="S5.SS3.p1.1.m1.1.1.2" xref="S5.SS3.p1.1.m1.1.1.2.cmml">z</mi><mrow id="S5.SS3.p1.1.m1.1.1.3" xref="S5.SS3.p1.1.m1.1.1.3.cmml"><mi id="S5.SS3.p1.1.m1.1.1.3.2" xref="S5.SS3.p1.1.m1.1.1.3.2.cmml">f</mi><mo id="S5.SS3.p1.1.m1.1.1.3.1" xref="S5.SS3.p1.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S5.SS3.p1.1.m1.1.1.3.3" xref="S5.SS3.p1.1.m1.1.1.3.3.cmml">e</mi><mo id="S5.SS3.p1.1.m1.1.1.3.1a" xref="S5.SS3.p1.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S5.SS3.p1.1.m1.1.1.3.4" xref="S5.SS3.p1.1.m1.1.1.3.4.cmml">a</mi><mo id="S5.SS3.p1.1.m1.1.1.3.1b" xref="S5.SS3.p1.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S5.SS3.p1.1.m1.1.1.3.5" xref="S5.SS3.p1.1.m1.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><apply id="S5.SS3.p1.1.m1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS3.p1.1.m1.1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S5.SS3.p1.1.m1.1.1.2.cmml" xref="S5.SS3.p1.1.m1.1.1.2">ùëß</ci><apply id="S5.SS3.p1.1.m1.1.1.3.cmml" xref="S5.SS3.p1.1.m1.1.1.3"><times id="S5.SS3.p1.1.m1.1.1.3.1.cmml" xref="S5.SS3.p1.1.m1.1.1.3.1"></times><ci id="S5.SS3.p1.1.m1.1.1.3.2.cmml" xref="S5.SS3.p1.1.m1.1.1.3.2">ùëì</ci><ci id="S5.SS3.p1.1.m1.1.1.3.3.cmml" xref="S5.SS3.p1.1.m1.1.1.3.3">ùëí</ci><ci id="S5.SS3.p1.1.m1.1.1.3.4.cmml" xref="S5.SS3.p1.1.m1.1.1.3.4">ùëé</ci><ci id="S5.SS3.p1.1.m1.1.1.3.5.cmml" xref="S5.SS3.p1.1.m1.1.1.3.5">ùë°</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">z_{feat}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.1.m1.1d">italic_z start_POSTSUBSCRIPT italic_f italic_e italic_a italic_t end_POSTSUBSCRIPT</annotation></semantics></math>. Quantitative results are shown in <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S5.T1" title="In 5.3 Training on limited data ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Tab.</span>¬†<span class="ltx_text ltx_ref_tag">1</span></a>, and visualizations are available in <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S5.F4" title="In 5.3 Training on limited data ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">Overall, VQ-HPS outperforms the SOTA methods significantly when training on 3DPW and EMDB (see <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S5.T1" title="In 5.3 Training on limited data ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Tab.</span>¬†<span class="ltx_text ltx_ref_tag">1</span></a>, col. 1 and 3). Visualization of the results confirms that our method performs best (see <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S5.F4" title="In 5.3 Training on limited data ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">4</span></a>), and we propose a more detailed analysis of the error in <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#A4" title="Appendix D Analysis of the error ‚Ä£ Appendix C Visualization of predictions during training ‚Ä£ Appendix B Additional qualitative results ‚Ä£ Appendix A Implementation details ‚Ä£ 5.4 Training on large-scale datasets ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Appendix</span>¬†<span class="ltx_text ltx_ref_tag">D</span></a>. HMR and CLIFF show realistic predictions but are less accurate than VQ-HPS. Despite rather good metrics, FastMETRO produces non-smooth results that do not correspond to human body shapes. This is probably due to the limited training sets, as the results displayed in the original paper looked more realistic. This highlights a clear limitation of non-parametric approaches predicting the 3D coordinates; in comparison, VQ-HPS needs much less data to provide realistic predictions, probably because learning sequences of indices corresponding to anthropomorphic meshes is easier than understanding the structure of the 3D vertices. Furthermore, VQ-HPS benefits from the large-scale pre-training on human motion datasets, acting as a regularization to allow learning with fewer image data labeled with 3D poses.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1">The fact that the Mesh-VQ-VAE, whose decoder is an important part of VQ-HPS, is pre-trained on AMASS¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib29" title="">29</a>]</cite> is a great advantage of our approach. Indeed, we can leverage large amounts of body mesh data not paired with images for training. This is particularly interesting because many body motion data can be generated using animation software or generative models. We could finetune the Mesh-VQ-VAE depending on the target application, with uncommon body shapes¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib82" title="">82</a>]</cite> or extreme poses¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib83" title="">83</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib84" title="">84</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.p4">
<p class="ltx_p" id="S5.SS3.p4.1">FastMETRO slightly takes the lead in global metrics on the EMDB benchmark when training on COCO. The advantage VQ-HPS had when training with scarce data is less important here. Indeed, COCO has as many images as EMDB or 3DPW, but the diversity is much higher than for video datasets, where there exist important correlations between different images¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib33" title="">33</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Training on large-scale datasets</h3>
<figure class="ltx_table" id="S5.SS4.9">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span><span class="ltx_text ltx_font_bold" id="S5.SS4.9.11.1">Comparison to the SOTA methods.</span> We evaluate VQ-HPS trained on a mix of datasets without finetuning on 3DPW (see <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S5.SS1" title="5.1 Datasets ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Sec.</span>¬†<span class="ltx_text ltx_ref_tag">5.1</span></a>) with standard metrics on 3DPW and EMDB and compare them to the state of the art. On the left part, methods use a ResNet-50 backbone. On the right, models use an HRNet backbone, except TokenHMR¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib76" title="">76</a>]</cite> that uses a ViT¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib49" title="">49</a>]</cite> backbone and additional data. All results are given in¬†mm.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" id="S5.SS4.3.3" style="width:160.4pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>ResNet-50 backbone</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.SS4.3.3.3" style="width:433.6pt;height:208pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(10.4pt,-5.0pt) scale(1.05035818165676,1.05035818165676) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.SS4.3.3.3.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.SS4.3.3.3.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.SS4.3.3.3.3.3.4">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.SS4.1.1.1.1.1.1">PVE <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.SS4.1.1.1.1.1.1.m1.1"><semantics id="S5.SS4.1.1.1.1.1.1.m1.1a"><mo id="S5.SS4.1.1.1.1.1.1.m1.1.1" stretchy="false" xref="S5.SS4.1.1.1.1.1.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.1.1.1.1.1.1.m1.1b"><ci id="S5.SS4.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.SS4.1.1.1.1.1.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.1.1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.1.1.1.1.1.1.m1.1d">‚Üì</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.SS4.2.2.2.2.2.2">MPJPE <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.SS4.2.2.2.2.2.2.m1.1"><semantics id="S5.SS4.2.2.2.2.2.2.m1.1a"><mo id="S5.SS4.2.2.2.2.2.2.m1.1.1" stretchy="false" xref="S5.SS4.2.2.2.2.2.2.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.2.2.2.2.2.2.m1.1b"><ci id="S5.SS4.2.2.2.2.2.2.m1.1.1.cmml" xref="S5.SS4.2.2.2.2.2.2.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.2.2.2.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.2.2.2.2.2.2.m1.1d">‚Üì</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.SS4.3.3.3.3.3.3">PA-MPJPE <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.SS4.3.3.3.3.3.3.m1.1"><semantics id="S5.SS4.3.3.3.3.3.3.m1.1a"><mo id="S5.SS4.3.3.3.3.3.3.m1.1.1" stretchy="false" xref="S5.SS4.3.3.3.3.3.3.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.3.3.3.3.3.3.m1.1b"><ci id="S5.SS4.3.3.3.3.3.3.m1.1.1.cmml" xref="S5.SS4.3.3.3.3.3.3.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.3.3.3.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.3.3.3.3.3.3.m1.1d">‚Üì</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.SS4.3.3.3.3.4.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.SS4.3.3.3.3.4.1.1">GraphCMR¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib19" title="">19</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.SS4.3.3.3.3.4.1.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.SS4.3.3.3.3.4.1.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.SS4.3.3.3.3.4.1.4">70.2</td>
</tr>
<tr class="ltx_tr" id="S5.SS4.3.3.3.3.5.2">
<td class="ltx_td ltx_align_center" id="S5.SS4.3.3.3.3.5.2.1">I2LMeshNet¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib63" title="">63</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.SS4.3.3.3.3.5.2.2">-</td>
<td class="ltx_td ltx_align_center" id="S5.SS4.3.3.3.3.5.2.3">93.2</td>
<td class="ltx_td ltx_align_center" id="S5.SS4.3.3.3.3.5.2.4">58.6</td>
</tr>
<tr class="ltx_tr" id="S5.SS4.3.3.3.3.6.3">
<td class="ltx_td ltx_align_center" id="S5.SS4.3.3.3.3.6.3.1">FastMETRO-S¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib3" title="">3</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.SS4.3.3.3.3.6.3.2">129.4</td>
<td class="ltx_td ltx_align_center" id="S5.SS4.3.3.3.3.6.3.3">112.6</td>
<td class="ltx_td ltx_align_center" id="S5.SS4.3.3.3.3.6.3.4">68.9</td>
</tr>
<tr class="ltx_tr" id="S5.SS4.3.3.3.3.7.4">
<td class="ltx_td ltx_align_center" id="S5.SS4.3.3.3.3.7.4.1">HMR¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib1" title="">1</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.SS4.3.3.3.3.7.4.2">-</td>
<td class="ltx_td ltx_align_center" id="S5.SS4.3.3.3.3.7.4.3">130.0</td>
<td class="ltx_td ltx_align_center" id="S5.SS4.3.3.3.3.7.4.4">81.3</td>
</tr>
<tr class="ltx_tr" id="S5.SS4.3.3.3.3.8.5">
<td class="ltx_td ltx_align_center" id="S5.SS4.3.3.3.3.8.5.1">SPIN¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib39" title="">39</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.SS4.3.3.3.3.8.5.2">116.4</td>
<td class="ltx_td ltx_align_center" id="S5.SS4.3.3.3.3.8.5.3">96.9</td>
<td class="ltx_td ltx_align_center" id="S5.SS4.3.3.3.3.8.5.4">59.2</td>
</tr>
<tr class="ltx_tr" id="S5.SS4.3.3.3.3.9.6">
<td class="ltx_td ltx_align_center" id="S5.SS4.3.3.3.3.9.6.1">PyMAF¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib42" title="">42</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.SS4.3.3.3.3.9.6.2">110.1</td>
<td class="ltx_td ltx_align_center" id="S5.SS4.3.3.3.3.9.6.3">92.8</td>
<td class="ltx_td ltx_align_center" id="S5.SS4.3.3.3.3.9.6.4">58.9</td>
</tr>
<tr class="ltx_tr" id="S5.SS4.3.3.3.3.10.7">
<td class="ltx_td ltx_align_center" id="S5.SS4.3.3.3.3.10.7.1">ROMP¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib47" title="">47</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.SS4.3.3.3.3.10.7.2">105.6</td>
<td class="ltx_td ltx_align_center" id="S5.SS4.3.3.3.3.10.7.3">89.3</td>
<td class="ltx_td ltx_align_center" id="S5.SS4.3.3.3.3.10.7.4">53.5</td>
</tr>
<tr class="ltx_tr" id="S5.SS4.3.3.3.3.11.8">
<td class="ltx_td ltx_align_center" id="S5.SS4.3.3.3.3.11.8.1">DSR¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib48" title="">48</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.SS4.3.3.3.3.11.8.2">105.8</td>
<td class="ltx_td ltx_align_center" id="S5.SS4.3.3.3.3.11.8.3">91.7</td>
<td class="ltx_td ltx_align_center" id="S5.SS4.3.3.3.3.11.8.4">54.1</td>
</tr>
<tr class="ltx_tr" id="S5.SS4.3.3.3.3.12.9">
<td class="ltx_td ltx_align_center" id="S5.SS4.3.3.3.3.12.9.1">PARE¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib41" title="">41</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.SS4.3.3.3.3.12.9.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.SS4.3.3.3.3.12.9.2.1">99.7</span></td>
<td class="ltx_td ltx_align_center" id="S5.SS4.3.3.3.3.12.9.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.SS4.3.3.3.3.12.9.3.1">82.9</span></td>
<td class="ltx_td ltx_align_center" id="S5.SS4.3.3.3.3.12.9.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.SS4.3.3.3.3.12.9.4.1">52.3</span></td>
</tr>
<tr class="ltx_tr" id="S5.SS4.3.3.3.3.13.10">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.SS4.3.3.3.3.13.10.1">VQ-HPS (ours)</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.SS4.3.3.3.3.13.10.2"><span class="ltx_text ltx_font_bold" id="S5.SS4.3.3.3.3.13.10.2.1">93.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.SS4.3.3.3.3.13.10.3"><span class="ltx_text ltx_font_bold" id="S5.SS4.3.3.3.3.13.10.3.1">79.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.SS4.3.3.3.3.13.10.4"><span class="ltx_text ltx_font_bold" id="S5.SS4.3.3.3.3.13.10.4.1">50.4</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" id="S5.SS4.9.9" style="width:268.8pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_block">(b) </span>HRNet backbone</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S5.SS4.9.9.6" style="width:433.6pt;height:141.4pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-49.3pt,16.0pt) scale(0.814611180715981,0.814611180715981) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.SS4.9.9.6.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.SS4.9.9.6.6.7.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.SS4.9.9.6.6.7.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S5.SS4.9.9.6.6.7.1.2">3DPW</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S5.SS4.9.9.6.6.7.1.3">EMDB</th>
</tr>
<tr class="ltx_tr" id="S5.SS4.9.9.6.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S5.SS4.9.9.6.6.6.7">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.SS4.4.4.1.1.1.1">PVE <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.SS4.4.4.1.1.1.1.m1.1"><semantics id="S5.SS4.4.4.1.1.1.1.m1.1a"><mo id="S5.SS4.4.4.1.1.1.1.m1.1.1" stretchy="false" xref="S5.SS4.4.4.1.1.1.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.4.4.1.1.1.1.m1.1b"><ci id="S5.SS4.4.4.1.1.1.1.m1.1.1.cmml" xref="S5.SS4.4.4.1.1.1.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.4.4.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.4.4.1.1.1.1.m1.1d">‚Üì</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.SS4.5.5.2.2.2.2">MPJPE <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.SS4.5.5.2.2.2.2.m1.1"><semantics id="S5.SS4.5.5.2.2.2.2.m1.1a"><mo id="S5.SS4.5.5.2.2.2.2.m1.1.1" stretchy="false" xref="S5.SS4.5.5.2.2.2.2.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.5.5.2.2.2.2.m1.1b"><ci id="S5.SS4.5.5.2.2.2.2.m1.1.1.cmml" xref="S5.SS4.5.5.2.2.2.2.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.5.5.2.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.5.5.2.2.2.2.m1.1d">‚Üì</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.SS4.6.6.3.3.3.3">PA-MPJPE <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.SS4.6.6.3.3.3.3.m1.1"><semantics id="S5.SS4.6.6.3.3.3.3.m1.1a"><mo id="S5.SS4.6.6.3.3.3.3.m1.1.1" stretchy="false" xref="S5.SS4.6.6.3.3.3.3.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.6.6.3.3.3.3.m1.1b"><ci id="S5.SS4.6.6.3.3.3.3.m1.1.1.cmml" xref="S5.SS4.6.6.3.3.3.3.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.6.6.3.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.6.6.3.3.3.3.m1.1d">‚Üì</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.SS4.7.7.4.4.4.4">PVE <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.SS4.7.7.4.4.4.4.m1.1"><semantics id="S5.SS4.7.7.4.4.4.4.m1.1a"><mo id="S5.SS4.7.7.4.4.4.4.m1.1.1" stretchy="false" xref="S5.SS4.7.7.4.4.4.4.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.7.7.4.4.4.4.m1.1b"><ci id="S5.SS4.7.7.4.4.4.4.m1.1.1.cmml" xref="S5.SS4.7.7.4.4.4.4.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.7.7.4.4.4.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.7.7.4.4.4.4.m1.1d">‚Üì</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.SS4.8.8.5.5.5.5">MPJPE <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.SS4.8.8.5.5.5.5.m1.1"><semantics id="S5.SS4.8.8.5.5.5.5.m1.1a"><mo id="S5.SS4.8.8.5.5.5.5.m1.1.1" stretchy="false" xref="S5.SS4.8.8.5.5.5.5.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.8.8.5.5.5.5.m1.1b"><ci id="S5.SS4.8.8.5.5.5.5.m1.1.1.cmml" xref="S5.SS4.8.8.5.5.5.5.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.8.8.5.5.5.5.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.8.8.5.5.5.5.m1.1d">‚Üì</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.SS4.9.9.6.6.6.6">PA-MPJPE <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.SS4.9.9.6.6.6.6.m1.1"><semantics id="S5.SS4.9.9.6.6.6.6.m1.1a"><mo id="S5.SS4.9.9.6.6.6.6.m1.1.1" stretchy="false" xref="S5.SS4.9.9.6.6.6.6.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.9.9.6.6.6.6.m1.1b"><ci id="S5.SS4.9.9.6.6.6.6.m1.1.1.cmml" xref="S5.SS4.9.9.6.6.6.6.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.9.9.6.6.6.6.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.9.9.6.6.6.6.m1.1d">‚Üì</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.SS4.9.9.6.6.8.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.SS4.9.9.6.6.8.1.1">FastMETRO-L¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib3" title="">3</a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.SS4.9.9.6.6.8.1.2">121.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.SS4.9.9.6.6.8.1.3">109.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.SS4.9.9.6.6.8.1.4">65.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.SS4.9.9.6.6.8.1.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.SS4.9.9.6.6.8.1.5.1">119.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.SS4.9.9.6.6.8.1.6">108.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.SS4.9.9.6.6.8.1.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.SS4.9.9.6.6.8.1.7.1">66.8</span></td>
</tr>
<tr class="ltx_tr" id="S5.SS4.9.9.6.6.9.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.SS4.9.9.6.6.9.2.1">ROMP¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib47" title="">47</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.SS4.9.9.6.6.9.2.2">103.1</td>
<td class="ltx_td ltx_align_center" id="S5.SS4.9.9.6.6.9.2.3">85.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.SS4.9.9.6.6.9.2.4">54.9</td>
<td class="ltx_td ltx_align_center" id="S5.SS4.9.9.6.6.9.2.5">134.9</td>
<td class="ltx_td ltx_align_center" id="S5.SS4.9.9.6.6.9.2.6">112.7</td>
<td class="ltx_td ltx_align_center" id="S5.SS4.9.9.6.6.9.2.7">75.2</td>
</tr>
<tr class="ltx_tr" id="S5.SS4.9.9.6.6.10.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.SS4.9.9.6.6.10.3.1">PARE¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib41" title="">41</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.SS4.9.9.6.6.10.3.2">97.9</td>
<td class="ltx_td ltx_align_center" id="S5.SS4.9.9.6.6.10.3.3">82.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.SS4.9.9.6.6.10.3.4">50.9</td>
<td class="ltx_td ltx_align_center" id="S5.SS4.9.9.6.6.10.3.5">133.2</td>
<td class="ltx_td ltx_align_center" id="S5.SS4.9.9.6.6.10.3.6">113.9</td>
<td class="ltx_td ltx_align_center" id="S5.SS4.9.9.6.6.10.3.7">72.2</td>
</tr>
<tr class="ltx_tr" id="S5.SS4.9.9.6.6.11.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.SS4.9.9.6.6.11.4.1">Virtual Marker¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib85" title="">85</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.SS4.9.9.6.6.11.4.2">93.8</td>
<td class="ltx_td ltx_align_center" id="S5.SS4.9.9.6.6.11.4.3">80.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.SS4.9.9.6.6.11.4.4">48.9</td>
<td class="ltx_td ltx_align_center" id="S5.SS4.9.9.6.6.11.4.5">-</td>
<td class="ltx_td ltx_align_center" id="S5.SS4.9.9.6.6.11.4.6">-</td>
<td class="ltx_td ltx_align_center" id="S5.SS4.9.9.6.6.11.4.7">-</td>
</tr>
<tr class="ltx_tr" id="S5.SS4.9.9.6.6.12.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.SS4.9.9.6.6.12.5.1">CLIFF¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib2" title="">2</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.SS4.9.9.6.6.12.5.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.SS4.9.9.6.6.12.5.2.1">87.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.SS4.9.9.6.6.12.5.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.SS4.9.9.6.6.12.5.3.1">73.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.SS4.9.9.6.6.12.5.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.SS4.9.9.6.6.12.5.4.1">46.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.SS4.9.9.6.6.12.5.5">122.9</td>
<td class="ltx_td ltx_align_center" id="S5.SS4.9.9.6.6.12.5.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.SS4.9.9.6.6.12.5.6.1">103.1</span></td>
<td class="ltx_td ltx_align_center" id="S5.SS4.9.9.6.6.12.5.7">68.8</td>
</tr>
<tr class="ltx_tr" id="S5.SS4.9.9.6.6.13.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.SS4.9.9.6.6.13.6.1">
<span class="ltx_ERROR undefined" id="S5.SS4.9.9.6.6.13.6.1.1">\hdashline</span>
TokenHMR¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib76" title="">76</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.SS4.9.9.6.6.13.6.2"><span class="ltx_text ltx_font_italic" id="S5.SS4.9.9.6.6.13.6.2.1">88.1</span></td>
<td class="ltx_td ltx_align_center" id="S5.SS4.9.9.6.6.13.6.3"><span class="ltx_text ltx_font_italic" id="S5.SS4.9.9.6.6.13.6.3.1">76.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.SS4.9.9.6.6.13.6.4"><span class="ltx_text ltx_font_italic" id="S5.SS4.9.9.6.6.13.6.4.1">49.3</span></td>
<td class="ltx_td ltx_align_center" id="S5.SS4.9.9.6.6.13.6.5"><span class="ltx_text ltx_font_italic" id="S5.SS4.9.9.6.6.13.6.5.1">124.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.SS4.9.9.6.6.13.6.6"><span class="ltx_text ltx_font_italic" id="S5.SS4.9.9.6.6.13.6.6.1">102.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.SS4.9.9.6.6.13.6.7"><span class="ltx_text ltx_font_italic" id="S5.SS4.9.9.6.6.13.6.7.1">67.5</span></td>
</tr>
<tr class="ltx_tr" id="S5.SS4.9.9.6.6.14.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S5.SS4.9.9.6.6.14.7.1">VQ-HPS (ours)</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.SS4.9.9.6.6.14.7.2"><span class="ltx_text ltx_font_bold" id="S5.SS4.9.9.6.6.14.7.2.1">84.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.SS4.9.9.6.6.14.7.3"><span class="ltx_text ltx_font_bold" id="S5.SS4.9.9.6.6.14.7.3.1">71.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S5.SS4.9.9.6.6.14.7.4"><span class="ltx_text ltx_font_bold" id="S5.SS4.9.9.6.6.14.7.4.1">45.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.SS4.9.9.6.6.14.7.5"><span class="ltx_text ltx_font_bold" id="S5.SS4.9.9.6.6.14.7.5.1">112.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.SS4.9.9.6.6.14.7.6"><span class="ltx_text ltx_font_bold" id="S5.SS4.9.9.6.6.14.7.6.1">99.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.SS4.9.9.6.6.14.7.7"><span class="ltx_text ltx_font_bold" id="S5.SS4.9.9.6.6.14.7.7.1">65.2</span></td>
</tr>
</tbody>
</table>
</span></div>
<p class="ltx_p" id="S5.SS4.9.9.7">Following the standard practice¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib3" title="">3</a>]</cite>, we train VQ-HPS on Human3.6M¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib77" title="">77</a>]</cite>, MPI-INF-3DHP¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib86" title="">86</a>]</cite>, COCO¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib79" title="">79</a>]</cite>, and MPII¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib87" title="">87</a>]</cite>. For this experiment, we use the same version of VQ-HPS as in <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S5.SS3" title="5.3 Training on limited data ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Sec.</span>¬†<span class="ltx_text ltx_ref_tag">5.3</span></a>.
We evaluate VQ-HPS on 3DPW¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib30" title="">30</a>]</cite> and EMDB¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib31" title="">31</a>]</cite> without finetuning on the 3DPW training set. We only compare VQ-HPS with SOTA models using the same backbone and the same datasets for a fair comparison. We take results from the papers or use the provided implementations and checkpoints for other methods. Note that the results on¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib85" title="">85</a>]</cite> differ from the papers because we do not finetune the models on the 3DPW¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib30" title="">30</a>]</cite> dataset before testing. Recent methods using a different backbone such as a Vision Transformer (ViT)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib49" title="">49</a>]</cite>, as well as additional datasets¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib88" title="">88</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib61" title="">61</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib89" title="">89</a>]</cite> may obtain better results. However, the comparison would not be fair. Results are shown in <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S5.SS4" title="5.4 Training on large-scale datasets ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Sec.</span>¬†<span class="ltx_text ltx_ref_tag">5.4</span></a>.</p>
<p class="ltx_p" id="S5.SS4.9.9.8">VQ-HPS outperforms all other methods on all 3 metrics, being parametric¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib2" title="">2</a>]</cite> or non-parametric¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib85" title="">85</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib63" title="">63</a>]</cite>.
Note that there is a large gap between the performance of FastMETRO and Virtual Marker in <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S5.SS4" title="5.4 Training on large-scale datasets ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Sec.</span>¬†<span class="ltx_text ltx_ref_tag">5.4</span></a> and the reported results in the original papers. This is because we do not perform finetuning on 3DPW. The authors of FastMETRO acknowledge that methods regressing 3D vertices such as <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib85" title="">85</a>]</cite> perform poorly on data significantly different from the training set<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/postech-ami/FastMETRO/issues/13</span></span></span>. This limitation of non-parametric methods was also described in¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib24" title="">24</a>]</cite>.</p>
<p class="ltx_p" id="S5.SS4.9.9.9">Even though TokenHMR‚Äôs backbone is more powerful and the method is trained on additional 2D datasets, VQ-HPS still outperforms TokenHMR¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib76" title="">76</a>]</cite> on the 3DPW and EMDB datasets. When additionally incorporating Bedlam¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib60" title="">60</a>]</cite> in the training set, TokenHMR takes the lead on the 3DPW dataset as it obtains 84.6, 71.0, and 44.3¬†mm for the PVE, MPJPE, and PA-MPJPE, respectively, but VQ-HPS remains competitive using much less training data and a less powerful backbone. On the EMDB dataset, TokenHMR using Bedlam for training obtains 109.4, 91.7, and 55.6¬†mm for the PVE, MPJPE, and PA-MPJPE, respectively.</p>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Ablation study</h3>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span><span class="ltx_text ltx_font_bold" id="S5.T3.5.1">Ablation study.</span> We perform several ablations on the VQ-HPS architecture and training process on the 3DPW dataset. All results are given in¬†mm.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.3.3.4">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.1.1.1">PVE <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T3.1.1.1.m1.1"><semantics id="S5.T3.1.1.1.m1.1a"><mo id="S5.T3.1.1.1.m1.1.1" stretchy="false" xref="S5.T3.1.1.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.m1.1b"><ci id="S5.T3.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.1.1.1.m1.1d">‚Üì</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.2.2.2">MPJPE <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T3.2.2.2.m1.1"><semantics id="S5.T3.2.2.2.m1.1a"><mo id="S5.T3.2.2.2.m1.1.1" stretchy="false" xref="S5.T3.2.2.2.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.2.m1.1b"><ci id="S5.T3.2.2.2.m1.1.1.cmml" xref="S5.T3.2.2.2.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.2.2.2.m1.1d">‚Üì</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.3.3.3">PA-MPJPE <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T3.3.3.3.m1.1"><semantics id="S5.T3.3.3.3.m1.1a"><mo id="S5.T3.3.3.3.m1.1.1" stretchy="false" xref="S5.T3.3.3.3.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S5.T3.3.3.3.m1.1b"><ci id="S5.T3.3.3.3.m1.1.1.cmml" xref="S5.T3.3.3.3.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.3.3.3.m1.1d">‚Üì</annotation></semantics></math>
</th>
</tr>
<tr class="ltx_tr" id="S5.T3.3.4.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.3.4.1.1">VQ-HPS</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.3.4.1.2">176.6</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.3.4.1.3">152.0</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.3.4.1.4">91.8</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.3.5.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.3.5.1.1">SMPL</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.3.5.1.2">199.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.3.5.1.3">171.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.3.5.1.4">99.3</td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.6.2">
<td class="ltx_td ltx_align_center" id="S5.T3.3.6.2.1">3D loss</td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.6.2.2">220.3</td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.6.2.3">194.9</td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.6.2.4">144.1</td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.7.3">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.3.7.3.1">No reprojection</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.3.7.3.2">183.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.3.7.3.3">158.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.3.7.3.4">95.6</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="158" id="S5.SS5.1.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span class="ltx_text ltx_font_bold" id="S5.F5.2.1">Ablation study.</span> Effect of the ‚Äú3D loss‚Äù ablation. We can see that replacing the cross-entropy with a PVE loss produces unnatural poses, showing interest in the classification-based approach.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1">We ablate VQ-HPS architecture and training scheme and present the results in <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S5.T3" title="In 5.5 Ablation study ‚Ä£ 5.4 Training on large-scale datasets ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Tab.</span>¬†<span class="ltx_text ltx_ref_tag">3</span></a>. We train and test on the 3DPW¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib30" title="">30</a>]</cite> dataset for these experiments. Note that for faster experiments, we use early stopping with patience of 10 epochs, which makes the training stop much earlier. The first line of <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S5.T3" title="In 5.5 Ablation study ‚Ä£ 5.4 Training on large-scale datasets ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Tab.</span>¬†<span class="ltx_text ltx_ref_tag">3</span></a> shows the updated VQ-HPS results.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS5.p2">
<p class="ltx_p" id="S5.SS5.p2.1">‚ÄúSMPL" means predicting the SMPL parameters instead of using our proposed discrete latent representation. For obtaining the final 3D prediction, the SMPL model is used instead of the Mesh-VQ-VAE decoder. The performance gap shows that using similar architecture, predicting the discrete latent representation instead of the SMPL parameters yields improved performance.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS5.p3">
<p class="ltx_p" id="S5.SS5.p3.3">The ablation ‚Äú3D loss‚Äù replaces the cross-entropy loss with the PVE <math alttext="\mathcal{L}_{3D}=||V-\hat{V}||_{2}" class="ltx_Math" display="inline" id="S5.SS5.p3.1.m1.1"><semantics id="S5.SS5.p3.1.m1.1a"><mrow id="S5.SS5.p3.1.m1.1.1" xref="S5.SS5.p3.1.m1.1.1.cmml"><msub id="S5.SS5.p3.1.m1.1.1.3" xref="S5.SS5.p3.1.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.SS5.p3.1.m1.1.1.3.2" xref="S5.SS5.p3.1.m1.1.1.3.2.cmml">‚Ñí</mi><mrow id="S5.SS5.p3.1.m1.1.1.3.3" xref="S5.SS5.p3.1.m1.1.1.3.3.cmml"><mn id="S5.SS5.p3.1.m1.1.1.3.3.2" xref="S5.SS5.p3.1.m1.1.1.3.3.2.cmml">3</mn><mo id="S5.SS5.p3.1.m1.1.1.3.3.1" xref="S5.SS5.p3.1.m1.1.1.3.3.1.cmml">‚Å¢</mo><mi id="S5.SS5.p3.1.m1.1.1.3.3.3" xref="S5.SS5.p3.1.m1.1.1.3.3.3.cmml">D</mi></mrow></msub><mo id="S5.SS5.p3.1.m1.1.1.2" xref="S5.SS5.p3.1.m1.1.1.2.cmml">=</mo><msub id="S5.SS5.p3.1.m1.1.1.1" xref="S5.SS5.p3.1.m1.1.1.1.cmml"><mrow id="S5.SS5.p3.1.m1.1.1.1.1.1" xref="S5.SS5.p3.1.m1.1.1.1.1.2.cmml"><mo id="S5.SS5.p3.1.m1.1.1.1.1.1.2" stretchy="false" xref="S5.SS5.p3.1.m1.1.1.1.1.2.1.cmml">‚Äñ</mo><mrow id="S5.SS5.p3.1.m1.1.1.1.1.1.1" xref="S5.SS5.p3.1.m1.1.1.1.1.1.1.cmml"><mi id="S5.SS5.p3.1.m1.1.1.1.1.1.1.2" xref="S5.SS5.p3.1.m1.1.1.1.1.1.1.2.cmml">V</mi><mo id="S5.SS5.p3.1.m1.1.1.1.1.1.1.1" xref="S5.SS5.p3.1.m1.1.1.1.1.1.1.1.cmml">‚àí</mo><mover accent="true" id="S5.SS5.p3.1.m1.1.1.1.1.1.1.3" xref="S5.SS5.p3.1.m1.1.1.1.1.1.1.3.cmml"><mi id="S5.SS5.p3.1.m1.1.1.1.1.1.1.3.2" xref="S5.SS5.p3.1.m1.1.1.1.1.1.1.3.2.cmml">V</mi><mo id="S5.SS5.p3.1.m1.1.1.1.1.1.1.3.1" xref="S5.SS5.p3.1.m1.1.1.1.1.1.1.3.1.cmml">^</mo></mover></mrow><mo id="S5.SS5.p3.1.m1.1.1.1.1.1.3" stretchy="false" xref="S5.SS5.p3.1.m1.1.1.1.1.2.1.cmml">‚Äñ</mo></mrow><mn id="S5.SS5.p3.1.m1.1.1.1.3" xref="S5.SS5.p3.1.m1.1.1.1.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S5.SS5.p3.1.m1.1b"><apply id="S5.SS5.p3.1.m1.1.1.cmml" xref="S5.SS5.p3.1.m1.1.1"><eq id="S5.SS5.p3.1.m1.1.1.2.cmml" xref="S5.SS5.p3.1.m1.1.1.2"></eq><apply id="S5.SS5.p3.1.m1.1.1.3.cmml" xref="S5.SS5.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S5.SS5.p3.1.m1.1.1.3.1.cmml" xref="S5.SS5.p3.1.m1.1.1.3">subscript</csymbol><ci id="S5.SS5.p3.1.m1.1.1.3.2.cmml" xref="S5.SS5.p3.1.m1.1.1.3.2">‚Ñí</ci><apply id="S5.SS5.p3.1.m1.1.1.3.3.cmml" xref="S5.SS5.p3.1.m1.1.1.3.3"><times id="S5.SS5.p3.1.m1.1.1.3.3.1.cmml" xref="S5.SS5.p3.1.m1.1.1.3.3.1"></times><cn id="S5.SS5.p3.1.m1.1.1.3.3.2.cmml" type="integer" xref="S5.SS5.p3.1.m1.1.1.3.3.2">3</cn><ci id="S5.SS5.p3.1.m1.1.1.3.3.3.cmml" xref="S5.SS5.p3.1.m1.1.1.3.3.3">ùê∑</ci></apply></apply><apply id="S5.SS5.p3.1.m1.1.1.1.cmml" xref="S5.SS5.p3.1.m1.1.1.1"><csymbol cd="ambiguous" id="S5.SS5.p3.1.m1.1.1.1.2.cmml" xref="S5.SS5.p3.1.m1.1.1.1">subscript</csymbol><apply id="S5.SS5.p3.1.m1.1.1.1.1.2.cmml" xref="S5.SS5.p3.1.m1.1.1.1.1.1"><csymbol cd="latexml" id="S5.SS5.p3.1.m1.1.1.1.1.2.1.cmml" xref="S5.SS5.p3.1.m1.1.1.1.1.1.2">norm</csymbol><apply id="S5.SS5.p3.1.m1.1.1.1.1.1.1.cmml" xref="S5.SS5.p3.1.m1.1.1.1.1.1.1"><minus id="S5.SS5.p3.1.m1.1.1.1.1.1.1.1.cmml" xref="S5.SS5.p3.1.m1.1.1.1.1.1.1.1"></minus><ci id="S5.SS5.p3.1.m1.1.1.1.1.1.1.2.cmml" xref="S5.SS5.p3.1.m1.1.1.1.1.1.1.2">ùëâ</ci><apply id="S5.SS5.p3.1.m1.1.1.1.1.1.1.3.cmml" xref="S5.SS5.p3.1.m1.1.1.1.1.1.1.3"><ci id="S5.SS5.p3.1.m1.1.1.1.1.1.1.3.1.cmml" xref="S5.SS5.p3.1.m1.1.1.1.1.1.1.3.1">^</ci><ci id="S5.SS5.p3.1.m1.1.1.1.1.1.1.3.2.cmml" xref="S5.SS5.p3.1.m1.1.1.1.1.1.1.3.2">ùëâ</ci></apply></apply></apply><cn id="S5.SS5.p3.1.m1.1.1.1.3.cmml" type="integer" xref="S5.SS5.p3.1.m1.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p3.1.m1.1c">\mathcal{L}_{3D}=||V-\hat{V}||_{2}</annotation><annotation encoding="application/x-llamapun" id="S5.SS5.p3.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT 3 italic_D end_POSTSUBSCRIPT = | | italic_V - over^ start_ARG italic_V end_ARG | | start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> where <math alttext="V" class="ltx_Math" display="inline" id="S5.SS5.p3.2.m2.1"><semantics id="S5.SS5.p3.2.m2.1a"><mi id="S5.SS5.p3.2.m2.1.1" xref="S5.SS5.p3.2.m2.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S5.SS5.p3.2.m2.1b"><ci id="S5.SS5.p3.2.m2.1.1.cmml" xref="S5.SS5.p3.2.m2.1.1">ùëâ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p3.2.m2.1c">V</annotation><annotation encoding="application/x-llamapun" id="S5.SS5.p3.2.m2.1d">italic_V</annotation></semantics></math> is the ground truth mesh vertices and <math alttext="\hat{V}" class="ltx_Math" display="inline" id="S5.SS5.p3.3.m3.1"><semantics id="S5.SS5.p3.3.m3.1a"><mover accent="true" id="S5.SS5.p3.3.m3.1.1" xref="S5.SS5.p3.3.m3.1.1.cmml"><mi id="S5.SS5.p3.3.m3.1.1.2" xref="S5.SS5.p3.3.m3.1.1.2.cmml">V</mi><mo id="S5.SS5.p3.3.m3.1.1.1" xref="S5.SS5.p3.3.m3.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S5.SS5.p3.3.m3.1b"><apply id="S5.SS5.p3.3.m3.1.1.cmml" xref="S5.SS5.p3.3.m3.1.1"><ci id="S5.SS5.p3.3.m3.1.1.1.cmml" xref="S5.SS5.p3.3.m3.1.1.1">^</ci><ci id="S5.SS5.p3.3.m3.1.1.2.cmml" xref="S5.SS5.p3.3.m3.1.1.2">ùëâ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p3.3.m3.1c">\hat{V}</annotation><annotation encoding="application/x-llamapun" id="S5.SS5.p3.3.m3.1d">over^ start_ARG italic_V end_ARG</annotation></semantics></math> the final prediction. Given the huge decrease in performance (see <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S5.T3" title="In 5.5 Ablation study ‚Ä£ 5.4 Training on large-scale datasets ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Tab.</span>¬†<span class="ltx_text ltx_ref_tag">3</span></a>), we conclude that cross-entropy is a good alternative to 3D losses used in all prior works such as¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib3" title="">3</a>]</cite>. Training VQ-HPS with the PVE produces sequences of indices corresponding to non-anthropomorphic results, which recall results obtained with FastMETRO in <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S5.F4" title="In 5.3 Training on limited data ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">4</span></a> when training on limited data.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS5.p4">
<p class="ltx_p" id="S5.SS5.p4.1">‚ÄúNo reprojection" means that we do not compute the reprojection error. This mostly increases the error in PVE and MPJPE, which was expected since the PA-MPJPE is only related to the canonical mesh, and the reprojection loss is not used to train the canonical mesh predictor. However, it still has an impact since the canonical mesh prediction is conditioned on the predicted rotation.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this work, we proposed Mesh-VQ-VAE, an autoencoder architecture providing a discrete latent representation of registered human meshes. This discrete representation allowed us to tackle the HPSE problem from a classification perspective, avoiding the limitations of parametric and non-parametric HPSE methods described in <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S1" title="1 Introduction ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Sec.</span>¬†<span class="ltx_text ltx_ref_tag">1</span></a>. We also introduced VQ-HPS, a Transformer-based model for solving the proposed HPSE classification problem.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">While trained using the cross-entropy loss, VQ-HPS significantly outperforms state-of-the-art methods when trained on scarce data and shows promising performance on large-scale datasets. The classification-based approach exploits the discriminative power of Transformers. It addresses several known problems of non-parametric approaches, such as the plausibility of results and the lack of generalization when training on large-scale datasets without finetuning on the target domain. Comparisons to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib3" title="">3</a>]</cite> as well as our ablation study (<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S5.SS5" title="5.5 Ablation study ‚Ä£ 5.4 Training on large-scale datasets ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Sec.</span>¬†<span class="ltx_text ltx_ref_tag">5.5</span></a>) showed the superiority of the classification-based approach compared to parametric and existing non-parametric approaches when using similar architectures.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">Failure cases are discussed in <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#A5" title="Appendix E Failure cases ‚Ä£ Appendix D Analysis of the error ‚Ä£ Appendix C Visualization of predictions during training ‚Ä£ Appendix B Additional qualitative results ‚Ä£ Appendix A Implementation details ‚Ä£ 5.4 Training on large-scale datasets ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Appendix</span>¬†<span class="ltx_text ltx_ref_tag">E</span></a>. Future works may include the use of a SOTA backbone¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib90" title="">90</a>]</cite> or additional datasets¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib89" title="">89</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib60" title="">60</a>]</cite> for achieving better performance. Extensions of the classification-based approach may also be explored for other types of registered meshes, such as human hands or faces. We also believe that multimodal applications involving text and 3D humans¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib91" title="">91</a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#bib.bib92" title="">92</a>]</cite> would benefit from the Mesh-VQ-VAE representation as it can be considered a language.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>
<div class="ltx_para ltx_noindent" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This study is part of the EUR DIGISPORT project supported by the ANR within the framework of the PIA France 2030 (ANR-18-EURE-0022). This work was performed using HPC resources from the ‚ÄúM√©socentre‚Äù computing center of CentraleSup√©lec, √âcole Normale Sup√©rieure Paris-Saclay, and Universit√© Paris-Saclay supported by CNRS and R√©gion √éle-de-France. This work has been partially supported by MIAI@Grenoble Alpes, (ANR-19-P3IA-0003).</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Angjoo Kanazawa, Michael¬†J. Black, David¬†W. Jacobs, and Jitendra Malik.

</span>
<span class="ltx_bibblock">End-to-end recovery of human shape and pose.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 7122‚Äì7131, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Zhihao Li, Jianzhuang Liu, Zhensong Zhang, Songcen Xu, and Youliang Yan.

</span>
<span class="ltx_bibblock">Cliff: Carrying location information in full frames into human pose and shape estimation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">European Conference on Computer Vision (ECCV)</span>, pages 590‚Äì606. Springer, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Junhyeong Cho, Kim Youwang, and Tae-Hyun Oh.

</span>
<span class="ltx_bibblock">Cross-attention of disentangled modalities for 3d human mesh recovery with transformers.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">European Conference on Computer Vision (ECCV)</span>, pages 342‚Äì359. Springer, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Sebastian Starke, Ian Mason, and Taku Komura.

</span>
<span class="ltx_bibblock">Deepphase: Periodic autoencoders for learning motion phase manifolds.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">ACM Transactions on Graphics (TOG)</span>, 41(4), 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Andrew Feng, Samuel Shin, and Youngwoo Yoon.

</span>
<span class="ltx_bibblock">A tool for extracting 3d avatar-ready gesture animations from monocular videos.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">ACM SIGGRAPH Conference on Motion, Interaction and Games (ACM MIG)</span>, pages 1‚Äì7, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Haotian Zhang, Ye¬†Yuan, Viktor Makoviychuk, Yunrong Guo, Sanja Fidler, Xue¬†Bin Peng, and Kayvon Fatahalian.

</span>
<span class="ltx_bibblock">Learning physically simulated tennis skills from broadcast videos.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">ACM Transactions on Graphics (TOG)</span>, 42(4), 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Moritz Einfalt, Dan Zecha, and Rainer Lienhart.

</span>
<span class="ltx_bibblock">Activity-conditioned continuous human pose estimation for performance analysis of athletes using the example of swimming.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">IEEE/CVF Winter conference on Applications of Computer Vision (WACV)</span>, pages 446‚Äì455. IEEE, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Jianbo Wang, Kai Qiu, Houwen Peng, Jianlong Fu, and Jianke Zhu.

</span>
<span class="ltx_bibblock">Ai coach: Deep human pose estimation and analysis for personalized athletic training assistance.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">ACM International Conference on Multimedia (ACM MM)</span>, pages 374‚Äì382, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael¬†J Black.

</span>
<span class="ltx_bibblock">Smpl: A skinned multi-person linear model.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">ACM Transactions on Graphics (TOG)</span>, 34(6), 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Dragomir Anguelov, Praveen Srinivasan, Daphne Koller, Sebastian Thrun, Jim Rodgers, and James Davis.

</span>
<span class="ltx_bibblock">Scape: Shape completion and animation of people.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">ACM Transactions on Graphics (TOG)</span>, 24(3), 2005.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Ahmed A.¬†A. Osman, Timo Bolkart, Dimitrios Tzionas, and Michael¬†J. Black.

</span>
<span class="ltx_bibblock">SUPR: A sparse unified part-based human representation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">European Conference on Computer Vision (ECCV)</span>, pages 568‚Äì585. Springer, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A.¬†A. Osman, Dimitrios Tzionas, and Michael¬†J. Black.

</span>
<span class="ltx_bibblock">Expressive body capture: 3d hands, face, and body from a single image.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 10975‚Äì10985, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Hongyi Xu, Eduard¬†Gabriel Bazavan, Andrei Zanfir, William¬†T Freeman, Rahul Sukthankar, and Cristian Sminchisescu.

</span>
<span class="ltx_bibblock">Ghum &amp; ghuml: Generative 3d human shape and articulated pose models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 6184‚Äì6193, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, and Michael¬†J. Black.

</span>
<span class="ltx_bibblock">Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">European Conference on Computer Vision (ECCV)</span>, pages 561‚Äì578. Springer, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Christoph Lassner, Javier Romero, Martin Kiefel, Federica Bogo, Michael¬†J Black, and Peter¬†V Gehler.

</span>
<span class="ltx_bibblock">Unite the people: Closing the loop between 3d and 2d human representations.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 6050‚Äì6059, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Davis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang, Srinath Sridhar, and Leonidas¬†J. Guibas.

</span>
<span class="ltx_bibblock">Humor: 3d human motion model for robust pose estimation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">IEEE/CVF International Conference on Computer Vision (ICCV)</span>, pages 11488‚Äì11499, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran, Angjoo Kanazawa, and Jitendra Malik.

</span>
<span class="ltx_bibblock">Humans in 4d: Reconstructing and tracking humans with transformers.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">IEEE/CVF International Conference on Computer Vision (ICCV)</span>, pages 14783‚Äì14794, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Enric Corona, Gerard Pons-Moll, Guillem Aleny√†, and Francesc Moreno-Noguer.

</span>
<span class="ltx_bibblock">Learned vertex descent: a new direction for 3d human model fitting.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">European Conference on Computer Vision (ECCV)</span>, pages 146‚Äì165. Springer, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Nikos Kolotouros, Georgios Pavlakos, and Kostas Daniilidis.

</span>
<span class="ltx_bibblock">Convolutional mesh regression for single-image human shape reconstruction.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 4501‚Äì4510, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Hongsuk Choi, Gyeongsik Moon, and Kyoung¬†Mu Lee.

</span>
<span class="ltx_bibblock">Pose2mesh: Graph convolutional network for 3d human pose and mesh recovery from a 2d human pose.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">European Conference on Computer Vision (ECCV)</span>, pages 769‚Äì787. Springer, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Hongwen Zhang, Jie Cao, Guo Lu, Wanli Ouyang, and Zhenan Sun.

</span>
<span class="ltx_bibblock">Learning 3d human shape and pose from dense body parts.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</span>, 44(5):2610‚Äì2627, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Dongkai Wang and Shiliang Zhang.

</span>
<span class="ltx_bibblock">3d human mesh recovery with sequentially global rotation estimation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">IEEE/CVF International Conference on Computer Vision (ICCV)</span>, pages 14953‚Äì14962, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Kevin Lin, Lijuan Wang, and Zicheng Liu.

</span>
<span class="ltx_bibblock">Mesh graphormer.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">IEEE/CVF International Conference on Computer Vision (ICCV)</span>, pages 12939‚Äì12948, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Kevin Lin, Lijuan Wang, and Zicheng Liu.

</span>
<span class="ltx_bibblock">End-to-end human pose and mesh reconstruction with transformers.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 1954‚Äì1963, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan¬†N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">Advances in Neural Information Processing Systems (NIPS)</span>, 30:5998‚Äì6008, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Zhiyang Dou, Qingxuan Wu, Cheng Lin, Zeyu Cao, Qiangqiang Wu, Weilin Wan, Taku Komura, and Wenping Wang.

</span>
<span class="ltx_bibblock">Tore: Token reduction for efficient human mesh recovery with transformer.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">IEEE/CVF International Conference on Computer Vision (ICCV)</span>, pages 15143‚Äì15155, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Aaron Van Den¬†Oord, Oriol Vinyals, et¬†al.

</span>
<span class="ltx_bibblock">Neural discrete representation learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">Advances in neural information processing systems (NIPS)</span>, 30:6306‚Äì6315, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Yi¬†Zhou, Chenglei Wu, Zimo Li, Chen Cao, Yuting Ye, Jason Saragih, Hao Li, and Yaser Sheikh.

</span>
<span class="ltx_bibblock">Fully convolutional mesh autoencoder using efficient spatially varying kernels.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">Advances in neural information processing systems (NIPS)</span>, 33:9251‚Äì9262, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Naureen Mahmood, Nima Ghorbani, Nikolaus¬†F Troje, Gerard Pons-Moll, and Michael¬†J Black.

</span>
<span class="ltx_bibblock">Amass: Archive of motion capture as surface shapes.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">IEEE/CVF International Conference on Computer Vision (ICCV)</span>, pages 5442‚Äì5451, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Timo von Marcard, Roberto Henschel, Michael Black, Bodo Rosenhahn, and Gerard Pons-Moll.

</span>
<span class="ltx_bibblock">Recovering accurate 3d human pose in the wild using imus and a moving camera.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">European Conference on Computer Vision (ECCV)</span>, pages 601‚Äì617. Springer, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Manuel Kaufmann, Jie Song, Chen Guo, Kaiyue Shen, Tianjian Jiang, Chengcheng Tang, Juan¬†Jos√© Z√°rate, and Otmar Hilliges.

</span>
<span class="ltx_bibblock">EMDB: The Electromagnetic Database of Global 3D Human Pose and Shape in the Wild.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">IEEE/CVF International Conference on Computer Vision (ICCV)</span>, pages 14632‚Äì14643, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Taosha Fan, Kalyan¬†Vasudev Alwala, Donglai Xiang, Weipeng Xu, Todd Murphey, and Mustafa Mukadam.

</span>
<span class="ltx_bibblock">Revitalizing optimization for 3d human pose and shape estimation: A sparse constrained formulation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">IEEE/CVF International Conference on Computer Vision (ICCV)</span>, pages 11457‚Äì11466, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Hanbyul Joo, Natalia Neverova, and Andrea Vedaldi.

</span>
<span class="ltx_bibblock">Exemplar fine-tuning for 3d human model fitting towards in-the-wild 3d human pose estimation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">International Conference on 3D Vision (3DV)</span>, pages 42‚Äì52. IEEE, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Andrei Zanfir, Elisabeta Marinoiu, and Cristian Sminchisescu.

</span>
<span class="ltx_bibblock">Monocular 3d pose and shape estimation of multiple people in natural scenes-the importance of multiple scene constraints.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 2148‚Äì2157, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Riza¬†Alp Guler and Iasonas Kokkinos.

</span>
<span class="ltx_bibblock">Holopose: Holistic 3d human reconstruction in-the-wild.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 10884‚Äì10894, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Garvita Tiwari, Dimitrije Antic, Jan¬†Eric Lenssen, Nikolaos Sarafianos, Tony Tung, and Gerard Pons-Moll.

</span>
<span class="ltx_bibblock">Pose-ndf: Modeling human pose manifolds with neural distance fields.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">European Conference on Computer Vision (ECCV)</span>, pages 572‚Äì589. Springer, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Mingyi Shi, Sebastian Starke, Yuting Ye, Taku Komura, and Jungdam Won.

</span>
<span class="ltx_bibblock">Phasemp: Robust 3d pose estimation via phase-conditioned human motion prior.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">IEEE/CVF International Conference on Computer Vision (ICCV)</span>, pages 14725‚Äì14737, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Zhengyi Luo, S.¬†Alireza Golestaneh, and Kris¬†M. Kitani.

</span>
<span class="ltx_bibblock">3d human motion estimation via motion compression and refinement.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">Asian Conference on Computer Vision (ACCV)</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Nikos Kolotouros, Georgios Pavlakos, Michael¬†J Black, and Kostas Daniilidis.

</span>
<span class="ltx_bibblock">Learning to reconstruct 3d human pose and shape via model-fitting in the loop.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">IEEE/CVF International Conference on Computer Vision (ICCV)</span>, pages 2252‚Äì2261, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Muhammed Kocabas, Nikos Athanasiou, and Michael¬†J. Black.

</span>
<span class="ltx_bibblock">Vibe: Video inference for human body pose and shape estimation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib40.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 5253‚Äì5263, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Muhammed Kocabas, Chun-Hao¬†P. Huang, Otmar Hilliges, and Michael¬†J. Black.

</span>
<span class="ltx_bibblock">PARE: Part attention regressor for 3D human body estimation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">IEEE/CVF International Conference on Computer Vision (ICCV)</span>, pages 11127‚Äì11137, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Hongwen Zhang, Yating Tian, Xinchi Zhou, Wanli Ouyang, Yebin Liu, Limin Wang, and Zhenan Sun.

</span>
<span class="ltx_bibblock">Pymaf: 3d human pose and shape regression with pyramidal mesh alignment feedback loop.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib42.1.1">IEEE/CVF International Conference on Computer Vision (ICCV)</span>, pages 11446‚Äì11456, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Xiangyu Xu, Hao Chen, Francesc Moreno-Noguer, L√°szl√≥¬†A Jeni, and Fernando De¬†la Torre.

</span>
<span class="ltx_bibblock">3d human shape and pose from a single low-resolution image with self-supervised learning.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib43.1.1">European Conference on Computer Vision (ECCV)</span>, pages 284‚Äì300. Springer, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Hongsuk Choi, Gyeongsik Moon, Ju¬†Yong Chang, and Kyoung¬†Mu Lee.

</span>
<span class="ltx_bibblock">Beyond static features for temporally consistent 3d human pose and shape from a video.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib44.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 1964‚Äì1973, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Angjoo Kanazawa, Jason¬†Y. Zhang, Panna Felsen, and Jitendra Malik.

</span>
<span class="ltx_bibblock">Learning 3d human dynamics from video.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib45.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 5614‚Äì5623, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Xiangyu Xu, Hao Chen, Francesc Moreno-Noguer, Laszlo¬†A Jeni, and Fernando De¬†la Torre.

</span>
<span class="ltx_bibblock">3d human pose, shape and texture from low-resolution images and videos.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib46.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</span>, 44(9):4490‚Äì4504, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Yu¬†Sun, Qian Bao, Wu¬†Liu, Yili Fu, Michael¬†J Black, and Tao Mei.

</span>
<span class="ltx_bibblock">Monocular, one-stage, regression of multiple 3d people.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib47.1.1">IEEE/CVF International Conference on Computer Vision (ICCV)</span>, pages 11179‚Äì11188, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Sai¬†Kumar Dwivedi, Nikos Athanasiou, Muhammed Kocabas, and Michael¬†J Black.

</span>
<span class="ltx_bibblock">Learning to regress bodies from images using differentiable semantic rendering.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib48.1.1">IEEE/CVF International Conference on Computer Vision (ICCV)</span>, pages 11250‚Äì11259, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.

</span>
<span class="ltx_bibblock">An image is worth 16x16 words: Transformers for image recognition at scale.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib49.1.1">International Conference on Learning Representations (ICLR)</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Jing Lin, Ailing Zeng, Haoqian Wang, Lei Zhang, and Yu¬†Li.

</span>
<span class="ltx_bibblock">One-stage 3d whole-body mesh recovery with component aware transformer.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib50.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 21159‚Äì21168, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Zhongang Cai, Wanqi Yin, Ailing Zeng, Chen Wei, Qingping Sun, Yanjun Wang, Hui¬†En Pang, Haiyi Mei, Mingyuan Zhang, Lei Zhang, et¬†al.

</span>
<span class="ltx_bibblock">Smpler-x: Scaling up expressive human pose and shape estimation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib51.1.1">Advances in Neural Information Processing Systems (NIPS)</span>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Ce¬†Zheng, Xianpeng Liu, Guo-Jun Qi, and Chen Chen.

</span>
<span class="ltx_bibblock">Potter: Pooling attention transformer for efficient human mesh recovery.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib52.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 1611‚Äì1620, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Akash Sengupta, Ignas Budvytis, and Roberto Cipolla.

</span>
<span class="ltx_bibblock">Probabilistic 3d human shape and pose estimation from multiple unconstrained images in the wild.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib53.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 16094‚Äì16104, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Nikos Kolotouros, Georgios Pavlakos, Dinesh Jayaraman, and Kostas Daniilidis.

</span>
<span class="ltx_bibblock">Probabilistic modeling for human mesh recovery.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib54.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 11605‚Äì11614, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Benjamin Biggs, David Novotny, Sebastien Ehrhardt, Hanbyul Joo, Ben Graham, and Andrea Vedaldi.

</span>
<span class="ltx_bibblock">3d multi-bodies: Fitting sets of plausible 3d human models to ambiguous image data.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib55.1.1">Advances in Neural Information Processing Systems (NIPS)</span>, 33:20496‚Äì20507, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Qi¬†Fang, Kang Chen, Yinghui Fan, Qing Shuai, Jiefeng Li, and Weidong Zhang.

</span>
<span class="ltx_bibblock">Learning analytical posterior probability for human mesh recovery.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib56.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 8781‚Äì8791, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Akash Sengupta, Ignas Budvytis, and Roberto Cipolla.

</span>
<span class="ltx_bibblock">Humaniflow: Ancestor-conditioned normalising flows on so (3) manifolds for human pose and shape distribution estimation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib57.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 4779‚Äì4789, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
G√ºl Varol, Javier Romero, Xavier Martin, Naureen Mahmood, Michael¬†J. Black, Ivan Laptev, and Cordelia Schmid.

</span>
<span class="ltx_bibblock">Learning from synthetic humans.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib58.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 109‚Äì117, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Zhongang Cai, Mingyuan Zhang, Jiawei Ren, Chen Wei, Daxuan Ren, Zhengyu Lin, Haiyu Zhao, Lei Yang, and Ziwei Liu.

</span>
<span class="ltx_bibblock">Playing for 3d human recovery.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib59.1.1">arXiv preprint arXiv:2110.07588</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Michael¬†J. Black, Priyanka Patel, Joachim Tesch, and Jinlong Yang.

</span>
<span class="ltx_bibblock">BEDLAM: A synthetic dataset of bodies exhibiting detailed lifelike animated motion.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib60.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 8726‚Äì8737, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Priyanka Patel, Chun-Hao¬†P. Huang, Joachim Tesch, David¬†T. Hoffmann, Shashank Tripathi, and Michael¬†J. Black.

</span>
<span class="ltx_bibblock">AGORA: Avatars in geography optimized for regression analysis.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib61.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 13468‚Äì13478, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Gyeongsik Moon, Hongsuk Choi, and Kyoung¬†Mu Lee.

</span>
<span class="ltx_bibblock">Neuralannot: Neural annotator for 3d human mesh training sets.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib62.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 2299‚Äì2307, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Gyeongsik Moon and Kyoung¬†Mu Lee.

</span>
<span class="ltx_bibblock">I2l-meshnet: Image-to-lixel prediction network for accurate 3d human pose and mesh estimation from a single rgb image.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib63.1.1">European Conference on Computer Vision (ECCV)</span>, pages 752‚Äì768. Springer, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Yanjie Li, Sen Yang, Peidong Liu, Shoukui Zhang, Yunxiao Wang, Zhicheng Wang, Wankou Yang, and Shu-Tao Xia.

</span>
<span class="ltx_bibblock">Simcc: A simple coordinate classification perspective for human pose estimation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib64.1.1">European Conference on Computer Vision (ECCV)</span>, pages 89‚Äì106. Springer, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Gregory Rogez, Philippe Weinzaepfel, and Cordelia Schmid.

</span>
<span class="ltx_bibblock">Lcr-net++: Multi-person 2d and 3d pose detection in natural images.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib65.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</span>, 42(5):1146‚Äì1161, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Gr√©gory Rogez and Cordelia Schmid.

</span>
<span class="ltx_bibblock">Mocap-guided data augmentation for 3d pose estimation in the wild.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib66.1.1">Advances in neural information processing systems (NIPS)</span>, 29:3108‚Äì3116, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Isaac Cohen and Hongxia Li.

</span>
<span class="ltx_bibblock">Inference of human postures by classification of 3d human body shape.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib67.1.1">IEEE International Workshop on Analysis and Modeling of Faces and Gestures</span>, pages 74‚Äì81, 2003.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Cem Keskin, Furkan Kƒ±ra√ß, Yunus¬†Emre Kara, and Lale Akarun.

</span>
<span class="ltx_bibblock">Hand pose estimation and hand shape classification using multi-layered randomized decision forests.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib68.1.1">European Conference on Computer Vision (ECCV)</span>, pages 852‚Äì863. Springer, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Necati Cihan¬†Camgoz, Simon Hadfield, Oscar Koller, and Richard Bowden.

</span>
<span class="ltx_bibblock">Subunets: End-to-end hand shape and continuous sign language recognition.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib69.1.1">IEEE/CVF International Conference on Computer Vision (ICCV)</span>, pages 3056‚Äì3065, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
Pornthep Sarakon, Theekapun Charoenpong, and Supiya Charoensiriwath.

</span>
<span class="ltx_bibblock">Face shape classification from 3d human data by using svm.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib70.1.1">Biomedical Engineering International Conference</span>, pages 1‚Äì5. IEEE, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Guodong Guo, Yun Fu, Charles¬†R Dyer, and Thomas¬†S Huang.

</span>
<span class="ltx_bibblock">Head pose estimation: Classification or regression?

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib71.1.1">International Conference on Pattern Recognition (ICPR)</span>, pages 1‚Äì4. IEEE, 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Thomas Lucas, Fabien Baradel, Philippe Weinzaepfel, and Gr√©gory Rogez.

</span>
<span class="ltx_bibblock">Posegpt: Quantization-based 3d human motion generation and forecasting.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib72.1.1">European Conference on Computer Vision (ECCV)</span>, pages 417‚Äì435. Springer, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
Li¬†Siyao, Weijiang Yu, Tianpei Gu, Chunze Lin, Quan Wang, Chen Qian, Chen¬†Change Loy, and Ziwei Liu.

</span>
<span class="ltx_bibblock">Bailando: 3d dance generation by actor-critic gpt with choreographic memory.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib73.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 11050‚Äì11059, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Yong Zhang, Hongwei Zhao, Hongtao Lu, Xi¬†Shen, and Ying Shan.

</span>
<span class="ltx_bibblock">Generating human motion from textual descriptions with discrete representations.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib74.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 14730‚Äì14740, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Sicheng Yang, Zhiyong Wu, Minglei Li, Zhensong Zhang, Lei Hao, Weihong Bao, and Haolin Zhuang.

</span>
<span class="ltx_bibblock">Qpgesture: Quantization-based and phase-guided motion matching for natural speech-driven gesture generation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib75.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 2321‚Äì2330, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
Sai¬†Kumar Dwivedi, Yu¬†Sun, Priyanka Patel, Yao Feng, and Michael¬†J. Black.

</span>
<span class="ltx_bibblock">Tokenhmr: Advancing human mesh recovery with a tokenized pose representation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib76.1.1">arXiv preprint arXiv:2404.16752</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu.

</span>
<span class="ltx_bibblock">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib77.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</span>, 36(7):1325‚Äì1339, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Tianshu Zhang, Buzhen Huang, and Yangang Wang.

</span>
<span class="ltx_bibblock">Object-occluded human shape and pose estimation from a single color image.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib78.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 7376‚Äì7385, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r, and C¬†Lawrence Zitnick.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib79.1.1">European Conference on Computer Vision (ECCV)</span>, pages 740‚Äì755. Springer, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib80.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 770‚Äì778, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et¬†al.

</span>
<span class="ltx_bibblock">Imagenet large scale visual recognition challenge.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib81.1.1">International Journal of Computer Vision (IJCV)</span>, 115:211‚Äì252, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
Akash Sengupta, Ignas Budvytis, and Roberto Cipolla.

</span>
<span class="ltx_bibblock">Synthetic training for accurate 3d human pose and shape estimation in the wild.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib82.1.1">British Machine Vision Conference (BMVC)</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
Gu√©nol√© Fiche, Vincent Sevestre, Camila Gonzalez-Barral, and Simon Leglaive.

</span>
<span class="ltx_bibblock">Swimxyz: A large-scale dataset of synthetic swimming motions and videos.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib83.1.1">ACM SIGGRAPH Conference on Motion, Interaction and Games (MIG)</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
Wen Guo, Xiaoyu Bie, Xavier Alameda-Pineda, and Francesc Moreno-Noguer.

</span>
<span class="ltx_bibblock">Multi-person extreme motion prediction.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib84.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 13053‚Äì13064, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
Xiaoxuan Ma, Jiajun Su, Chunyu Wang, Wentao Zhu, and Yizhou Wang.

</span>
<span class="ltx_bibblock">3d human mesh estimation from virtual markers.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib85.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 534‚Äì543, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
Dushyant Mehta, Helge Rhodin, Dan Casas, Pascal Fua, Oleksandr Sotnychenko, Weipeng Xu, and Christian Theobalt.

</span>
<span class="ltx_bibblock">Monocular 3d human pose estimation in the wild using improved cnn supervision.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib86.1.1">International Conference on 3D Vision (3DV)</span>, pages 506‚Äì516. IEEE, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and Bernt Schiele.

</span>
<span class="ltx_bibblock">2d human pose estimation: New benchmark and state of the art analysis.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib87.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 3686‚Äì3693, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
Wenjia Wang, Yongtao Ge, Haiyi Mei, Zhongang Cai, Qingping Sun, Yanjun Wang, Chunhua Shen, Lei Yang, and Taku Komura.

</span>
<span class="ltx_bibblock">Zolly: Zoom focal length correctly for perspective-distorted human mesh reconstruction.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib88.1.1">IEEE/CVF International Conference on Computer Vision (ICCV)</span>, pages 3925‚Äì3935, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
Zhongang Cai, Daxuan Ren, Ailing Zeng, Zhengyu Lin, Tao Yu, Wenjia Wang, Xiangyu Fan, Yang Gao, Yifan Yu, Liang Pan, et¬†al.

</span>
<span class="ltx_bibblock">Humman: Multi-modal 4d human dataset for versatile sensing and modeling.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib89.1.1">European Conference on Computer Vision (ECCV)</span>, pages 557‚Äì577. Springer, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
Matthieu Armando, Salma Galaaoui, Fabien Baradel, Thomas Lucas, Vincent Leroy, Romain Br√©gier, Philippe Weinzaepfel, and Gr√©gory Rogez.

</span>
<span class="ltx_bibblock">Cross-view and cross-pose completion for 3d human understanding.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib90.1.1">arXiv preprint arXiv:2311.09104</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
Ginger Delmas, Philippe Weinzaepfel, Thomas Lucas, Francesc Moreno-Noguer, and Gr√©gory Rogez.

</span>
<span class="ltx_bibblock">Posescript: 3d human poses from natural language.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib91.1.1">European Conference on Computer Vision (ECCV)</span>, pages 346‚Äì362. Springer, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
Yao Feng, Jing Lin, Sai¬†Kumar Dwivedi, Yu¬†Sun, Priyanka Patel, and Michael¬†J Black.

</span>
<span class="ltx_bibblock">Posegpt: Chatting about 3d human pose.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib92.1.1">arXiv preprint arXiv:2311.18836</span>, 2023.

</span>
</li>
</ul>
</section>
<p class="ltx_p ltx_align_center" id="S5.SS4.9.9.10"><span class="ltx_text ltx_font_bold" id="S5.SS4.9.9.10.1">Supplementary material</span></p>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Implementation details</h2>
<div class="ltx_para ltx_noindent" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">In this section, we provide the details of the implementation of Mesh-VQ-VAE and VQ-HPS. More information on these models can be found in the main paper (see <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S4.SS1" title="4.1 Proposed HPSE method ‚Ä£ 4 Method ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Sec.</span>¬†<span class="ltx_text ltx_ref_tag">4.1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S4.F2" title="In 4 Method ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.p2">
<p class="ltx_p" id="A1.p2.4"><span class="ltx_text ltx_font_bold" id="A1.p2.4.1">Mesh-VQ-VAE.</span>
The Mesh-VQ-VAE architecture (see <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S4.F2" title="In 4 Method ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">2</span></a>) is adapted from the fully convolutional mesh autoencoder of¬†<span class="ltx_ERROR undefined" id="A1.p2.4.2">\citesupp</span>supp-zhou2020fully. This model encodes a mesh to a latent representation <math alttext="z\in\mathbb{R}^{N\times L}" class="ltx_Math" display="inline" id="A1.p2.1.m1.1"><semantics id="A1.p2.1.m1.1a"><mrow id="A1.p2.1.m1.1.1" xref="A1.p2.1.m1.1.1.cmml"><mi id="A1.p2.1.m1.1.1.2" xref="A1.p2.1.m1.1.1.2.cmml">z</mi><mo id="A1.p2.1.m1.1.1.1" xref="A1.p2.1.m1.1.1.1.cmml">‚àà</mo><msup id="A1.p2.1.m1.1.1.3" xref="A1.p2.1.m1.1.1.3.cmml"><mi id="A1.p2.1.m1.1.1.3.2" xref="A1.p2.1.m1.1.1.3.2.cmml">‚Ñù</mi><mrow id="A1.p2.1.m1.1.1.3.3" xref="A1.p2.1.m1.1.1.3.3.cmml"><mi id="A1.p2.1.m1.1.1.3.3.2" xref="A1.p2.1.m1.1.1.3.3.2.cmml">N</mi><mo id="A1.p2.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="A1.p2.1.m1.1.1.3.3.1.cmml">√ó</mo><mi id="A1.p2.1.m1.1.1.3.3.3" xref="A1.p2.1.m1.1.1.3.3.3.cmml">L</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A1.p2.1.m1.1b"><apply id="A1.p2.1.m1.1.1.cmml" xref="A1.p2.1.m1.1.1"><in id="A1.p2.1.m1.1.1.1.cmml" xref="A1.p2.1.m1.1.1.1"></in><ci id="A1.p2.1.m1.1.1.2.cmml" xref="A1.p2.1.m1.1.1.2">ùëß</ci><apply id="A1.p2.1.m1.1.1.3.cmml" xref="A1.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="A1.p2.1.m1.1.1.3.1.cmml" xref="A1.p2.1.m1.1.1.3">superscript</csymbol><ci id="A1.p2.1.m1.1.1.3.2.cmml" xref="A1.p2.1.m1.1.1.3.2">‚Ñù</ci><apply id="A1.p2.1.m1.1.1.3.3.cmml" xref="A1.p2.1.m1.1.1.3.3"><times id="A1.p2.1.m1.1.1.3.3.1.cmml" xref="A1.p2.1.m1.1.1.3.3.1"></times><ci id="A1.p2.1.m1.1.1.3.3.2.cmml" xref="A1.p2.1.m1.1.1.3.3.2">ùëÅ</ci><ci id="A1.p2.1.m1.1.1.3.3.3.cmml" xref="A1.p2.1.m1.1.1.3.3.3">ùêø</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.1.m1.1c">z\in\mathbb{R}^{N\times L}</annotation><annotation encoding="application/x-llamapun" id="A1.p2.1.m1.1d">italic_z ‚àà blackboard_R start_POSTSUPERSCRIPT italic_N √ó italic_L end_POSTSUPERSCRIPT</annotation></semantics></math> with <math alttext="N=54" class="ltx_Math" display="inline" id="A1.p2.2.m2.1"><semantics id="A1.p2.2.m2.1a"><mrow id="A1.p2.2.m2.1.1" xref="A1.p2.2.m2.1.1.cmml"><mi id="A1.p2.2.m2.1.1.2" xref="A1.p2.2.m2.1.1.2.cmml">N</mi><mo id="A1.p2.2.m2.1.1.1" xref="A1.p2.2.m2.1.1.1.cmml">=</mo><mn id="A1.p2.2.m2.1.1.3" xref="A1.p2.2.m2.1.1.3.cmml">54</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.p2.2.m2.1b"><apply id="A1.p2.2.m2.1.1.cmml" xref="A1.p2.2.m2.1.1"><eq id="A1.p2.2.m2.1.1.1.cmml" xref="A1.p2.2.m2.1.1.1"></eq><ci id="A1.p2.2.m2.1.1.2.cmml" xref="A1.p2.2.m2.1.1.2">ùëÅ</ci><cn id="A1.p2.2.m2.1.1.3.cmml" type="integer" xref="A1.p2.2.m2.1.1.3">54</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.2.m2.1c">N=54</annotation><annotation encoding="application/x-llamapun" id="A1.p2.2.m2.1d">italic_N = 54</annotation></semantics></math> and <math alttext="L=9" class="ltx_Math" display="inline" id="A1.p2.3.m3.1"><semantics id="A1.p2.3.m3.1a"><mrow id="A1.p2.3.m3.1.1" xref="A1.p2.3.m3.1.1.cmml"><mi id="A1.p2.3.m3.1.1.2" xref="A1.p2.3.m3.1.1.2.cmml">L</mi><mo id="A1.p2.3.m3.1.1.1" xref="A1.p2.3.m3.1.1.1.cmml">=</mo><mn id="A1.p2.3.m3.1.1.3" xref="A1.p2.3.m3.1.1.3.cmml">9</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.p2.3.m3.1b"><apply id="A1.p2.3.m3.1.1.cmml" xref="A1.p2.3.m3.1.1"><eq id="A1.p2.3.m3.1.1.1.cmml" xref="A1.p2.3.m3.1.1.1"></eq><ci id="A1.p2.3.m3.1.1.2.cmml" xref="A1.p2.3.m3.1.1.2">ùêø</ci><cn id="A1.p2.3.m3.1.1.3.cmml" type="integer" xref="A1.p2.3.m3.1.1.3">9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.3.m3.1c">L=9</annotation><annotation encoding="application/x-llamapun" id="A1.p2.3.m3.1d">italic_L = 9</annotation></semantics></math>. The quantization step is performed with a dictionary of size <math alttext="S=512" class="ltx_Math" display="inline" id="A1.p2.4.m4.1"><semantics id="A1.p2.4.m4.1a"><mrow id="A1.p2.4.m4.1.1" xref="A1.p2.4.m4.1.1.cmml"><mi id="A1.p2.4.m4.1.1.2" xref="A1.p2.4.m4.1.1.2.cmml">S</mi><mo id="A1.p2.4.m4.1.1.1" xref="A1.p2.4.m4.1.1.1.cmml">=</mo><mn id="A1.p2.4.m4.1.1.3" xref="A1.p2.4.m4.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.p2.4.m4.1b"><apply id="A1.p2.4.m4.1.1.cmml" xref="A1.p2.4.m4.1.1"><eq id="A1.p2.4.m4.1.1.1.cmml" xref="A1.p2.4.m4.1.1.1"></eq><ci id="A1.p2.4.m4.1.1.2.cmml" xref="A1.p2.4.m4.1.1.2">ùëÜ</ci><cn id="A1.p2.4.m4.1.1.3.cmml" type="integer" xref="A1.p2.4.m4.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.4.m4.1c">S=512</annotation><annotation encoding="application/x-llamapun" id="A1.p2.4.m4.1d">italic_S = 512</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.p3">
<p class="ltx_p" id="A1.p3.4"><span class="ltx_text ltx_font_bold" id="A1.p3.4.1">Feature extractors.</span>
Our CNN backbones are ResNet-50¬†<span class="ltx_ERROR undefined" id="A1.p3.4.2">\citesupp</span>supp-he2016deep pre-trained on ImageNet¬†<span class="ltx_ERROR undefined" id="A1.p3.4.3">\citesupp</span>supp-russakovsky2015imagenet to provide a fair comparison with previous methods. For the canonical mesh prediction, we remove the last fully connected layer to obtain features <math alttext="X_{mesh}\in\mathbb{R}^{H\times W\times C}" class="ltx_Math" display="inline" id="A1.p3.1.m1.1"><semantics id="A1.p3.1.m1.1a"><mrow id="A1.p3.1.m1.1.1" xref="A1.p3.1.m1.1.1.cmml"><msub id="A1.p3.1.m1.1.1.2" xref="A1.p3.1.m1.1.1.2.cmml"><mi id="A1.p3.1.m1.1.1.2.2" xref="A1.p3.1.m1.1.1.2.2.cmml">X</mi><mrow id="A1.p3.1.m1.1.1.2.3" xref="A1.p3.1.m1.1.1.2.3.cmml"><mi id="A1.p3.1.m1.1.1.2.3.2" xref="A1.p3.1.m1.1.1.2.3.2.cmml">m</mi><mo id="A1.p3.1.m1.1.1.2.3.1" xref="A1.p3.1.m1.1.1.2.3.1.cmml">‚Å¢</mo><mi id="A1.p3.1.m1.1.1.2.3.3" xref="A1.p3.1.m1.1.1.2.3.3.cmml">e</mi><mo id="A1.p3.1.m1.1.1.2.3.1a" xref="A1.p3.1.m1.1.1.2.3.1.cmml">‚Å¢</mo><mi id="A1.p3.1.m1.1.1.2.3.4" xref="A1.p3.1.m1.1.1.2.3.4.cmml">s</mi><mo id="A1.p3.1.m1.1.1.2.3.1b" xref="A1.p3.1.m1.1.1.2.3.1.cmml">‚Å¢</mo><mi id="A1.p3.1.m1.1.1.2.3.5" xref="A1.p3.1.m1.1.1.2.3.5.cmml">h</mi></mrow></msub><mo id="A1.p3.1.m1.1.1.1" xref="A1.p3.1.m1.1.1.1.cmml">‚àà</mo><msup id="A1.p3.1.m1.1.1.3" xref="A1.p3.1.m1.1.1.3.cmml"><mi id="A1.p3.1.m1.1.1.3.2" xref="A1.p3.1.m1.1.1.3.2.cmml">‚Ñù</mi><mrow id="A1.p3.1.m1.1.1.3.3" xref="A1.p3.1.m1.1.1.3.3.cmml"><mi id="A1.p3.1.m1.1.1.3.3.2" xref="A1.p3.1.m1.1.1.3.3.2.cmml">H</mi><mo id="A1.p3.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="A1.p3.1.m1.1.1.3.3.1.cmml">√ó</mo><mi id="A1.p3.1.m1.1.1.3.3.3" xref="A1.p3.1.m1.1.1.3.3.3.cmml">W</mi><mo id="A1.p3.1.m1.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="A1.p3.1.m1.1.1.3.3.1.cmml">√ó</mo><mi id="A1.p3.1.m1.1.1.3.3.4" xref="A1.p3.1.m1.1.1.3.3.4.cmml">C</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A1.p3.1.m1.1b"><apply id="A1.p3.1.m1.1.1.cmml" xref="A1.p3.1.m1.1.1"><in id="A1.p3.1.m1.1.1.1.cmml" xref="A1.p3.1.m1.1.1.1"></in><apply id="A1.p3.1.m1.1.1.2.cmml" xref="A1.p3.1.m1.1.1.2"><csymbol cd="ambiguous" id="A1.p3.1.m1.1.1.2.1.cmml" xref="A1.p3.1.m1.1.1.2">subscript</csymbol><ci id="A1.p3.1.m1.1.1.2.2.cmml" xref="A1.p3.1.m1.1.1.2.2">ùëã</ci><apply id="A1.p3.1.m1.1.1.2.3.cmml" xref="A1.p3.1.m1.1.1.2.3"><times id="A1.p3.1.m1.1.1.2.3.1.cmml" xref="A1.p3.1.m1.1.1.2.3.1"></times><ci id="A1.p3.1.m1.1.1.2.3.2.cmml" xref="A1.p3.1.m1.1.1.2.3.2">ùëö</ci><ci id="A1.p3.1.m1.1.1.2.3.3.cmml" xref="A1.p3.1.m1.1.1.2.3.3">ùëí</ci><ci id="A1.p3.1.m1.1.1.2.3.4.cmml" xref="A1.p3.1.m1.1.1.2.3.4">ùë†</ci><ci id="A1.p3.1.m1.1.1.2.3.5.cmml" xref="A1.p3.1.m1.1.1.2.3.5">‚Ñé</ci></apply></apply><apply id="A1.p3.1.m1.1.1.3.cmml" xref="A1.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="A1.p3.1.m1.1.1.3.1.cmml" xref="A1.p3.1.m1.1.1.3">superscript</csymbol><ci id="A1.p3.1.m1.1.1.3.2.cmml" xref="A1.p3.1.m1.1.1.3.2">‚Ñù</ci><apply id="A1.p3.1.m1.1.1.3.3.cmml" xref="A1.p3.1.m1.1.1.3.3"><times id="A1.p3.1.m1.1.1.3.3.1.cmml" xref="A1.p3.1.m1.1.1.3.3.1"></times><ci id="A1.p3.1.m1.1.1.3.3.2.cmml" xref="A1.p3.1.m1.1.1.3.3.2">ùêª</ci><ci id="A1.p3.1.m1.1.1.3.3.3.cmml" xref="A1.p3.1.m1.1.1.3.3.3">ùëä</ci><ci id="A1.p3.1.m1.1.1.3.3.4.cmml" xref="A1.p3.1.m1.1.1.3.3.4">ùê∂</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p3.1.m1.1c">X_{mesh}\in\mathbb{R}^{H\times W\times C}</annotation><annotation encoding="application/x-llamapun" id="A1.p3.1.m1.1d">italic_X start_POSTSUBSCRIPT italic_m italic_e italic_s italic_h end_POSTSUBSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_H √ó italic_W √ó italic_C end_POSTSUPERSCRIPT</annotation></semantics></math> with <math alttext="H=W=7" class="ltx_Math" display="inline" id="A1.p3.2.m2.1"><semantics id="A1.p3.2.m2.1a"><mrow id="A1.p3.2.m2.1.1" xref="A1.p3.2.m2.1.1.cmml"><mi id="A1.p3.2.m2.1.1.2" xref="A1.p3.2.m2.1.1.2.cmml">H</mi><mo id="A1.p3.2.m2.1.1.3" xref="A1.p3.2.m2.1.1.3.cmml">=</mo><mi id="A1.p3.2.m2.1.1.4" xref="A1.p3.2.m2.1.1.4.cmml">W</mi><mo id="A1.p3.2.m2.1.1.5" xref="A1.p3.2.m2.1.1.5.cmml">=</mo><mn id="A1.p3.2.m2.1.1.6" xref="A1.p3.2.m2.1.1.6.cmml">7</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.p3.2.m2.1b"><apply id="A1.p3.2.m2.1.1.cmml" xref="A1.p3.2.m2.1.1"><and id="A1.p3.2.m2.1.1a.cmml" xref="A1.p3.2.m2.1.1"></and><apply id="A1.p3.2.m2.1.1b.cmml" xref="A1.p3.2.m2.1.1"><eq id="A1.p3.2.m2.1.1.3.cmml" xref="A1.p3.2.m2.1.1.3"></eq><ci id="A1.p3.2.m2.1.1.2.cmml" xref="A1.p3.2.m2.1.1.2">ùêª</ci><ci id="A1.p3.2.m2.1.1.4.cmml" xref="A1.p3.2.m2.1.1.4">ùëä</ci></apply><apply id="A1.p3.2.m2.1.1c.cmml" xref="A1.p3.2.m2.1.1"><eq id="A1.p3.2.m2.1.1.5.cmml" xref="A1.p3.2.m2.1.1.5"></eq><share href="https://arxiv.org/html/2312.08291v4#A1.p3.2.m2.1.1.4.cmml" id="A1.p3.2.m2.1.1d.cmml" xref="A1.p3.2.m2.1.1"></share><cn id="A1.p3.2.m2.1.1.6.cmml" type="integer" xref="A1.p3.2.m2.1.1.6">7</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p3.2.m2.1c">H=W=7</annotation><annotation encoding="application/x-llamapun" id="A1.p3.2.m2.1d">italic_H = italic_W = 7</annotation></semantics></math>, <math alttext="C=2048" class="ltx_Math" display="inline" id="A1.p3.3.m3.1"><semantics id="A1.p3.3.m3.1a"><mrow id="A1.p3.3.m3.1.1" xref="A1.p3.3.m3.1.1.cmml"><mi id="A1.p3.3.m3.1.1.2" xref="A1.p3.3.m3.1.1.2.cmml">C</mi><mo id="A1.p3.3.m3.1.1.1" xref="A1.p3.3.m3.1.1.1.cmml">=</mo><mn id="A1.p3.3.m3.1.1.3" xref="A1.p3.3.m3.1.1.3.cmml">2048</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.p3.3.m3.1b"><apply id="A1.p3.3.m3.1.1.cmml" xref="A1.p3.3.m3.1.1"><eq id="A1.p3.3.m3.1.1.1.cmml" xref="A1.p3.3.m3.1.1.1"></eq><ci id="A1.p3.3.m3.1.1.2.cmml" xref="A1.p3.3.m3.1.1.2">ùê∂</ci><cn id="A1.p3.3.m3.1.1.3.cmml" type="integer" xref="A1.p3.3.m3.1.1.3">2048</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p3.3.m3.1c">C=2048</annotation><annotation encoding="application/x-llamapun" id="A1.p3.3.m3.1d">italic_C = 2048</annotation></semantics></math>. For the rotation and camera prediction, we keep the full Resnet-50 to obtain features <math alttext="X_{rot}\in\mathbb{R}^{C}" class="ltx_Math" display="inline" id="A1.p3.4.m4.1"><semantics id="A1.p3.4.m4.1a"><mrow id="A1.p3.4.m4.1.1" xref="A1.p3.4.m4.1.1.cmml"><msub id="A1.p3.4.m4.1.1.2" xref="A1.p3.4.m4.1.1.2.cmml"><mi id="A1.p3.4.m4.1.1.2.2" xref="A1.p3.4.m4.1.1.2.2.cmml">X</mi><mrow id="A1.p3.4.m4.1.1.2.3" xref="A1.p3.4.m4.1.1.2.3.cmml"><mi id="A1.p3.4.m4.1.1.2.3.2" xref="A1.p3.4.m4.1.1.2.3.2.cmml">r</mi><mo id="A1.p3.4.m4.1.1.2.3.1" xref="A1.p3.4.m4.1.1.2.3.1.cmml">‚Å¢</mo><mi id="A1.p3.4.m4.1.1.2.3.3" xref="A1.p3.4.m4.1.1.2.3.3.cmml">o</mi><mo id="A1.p3.4.m4.1.1.2.3.1a" xref="A1.p3.4.m4.1.1.2.3.1.cmml">‚Å¢</mo><mi id="A1.p3.4.m4.1.1.2.3.4" xref="A1.p3.4.m4.1.1.2.3.4.cmml">t</mi></mrow></msub><mo id="A1.p3.4.m4.1.1.1" xref="A1.p3.4.m4.1.1.1.cmml">‚àà</mo><msup id="A1.p3.4.m4.1.1.3" xref="A1.p3.4.m4.1.1.3.cmml"><mi id="A1.p3.4.m4.1.1.3.2" xref="A1.p3.4.m4.1.1.3.2.cmml">‚Ñù</mi><mi id="A1.p3.4.m4.1.1.3.3" xref="A1.p3.4.m4.1.1.3.3.cmml">C</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="A1.p3.4.m4.1b"><apply id="A1.p3.4.m4.1.1.cmml" xref="A1.p3.4.m4.1.1"><in id="A1.p3.4.m4.1.1.1.cmml" xref="A1.p3.4.m4.1.1.1"></in><apply id="A1.p3.4.m4.1.1.2.cmml" xref="A1.p3.4.m4.1.1.2"><csymbol cd="ambiguous" id="A1.p3.4.m4.1.1.2.1.cmml" xref="A1.p3.4.m4.1.1.2">subscript</csymbol><ci id="A1.p3.4.m4.1.1.2.2.cmml" xref="A1.p3.4.m4.1.1.2.2">ùëã</ci><apply id="A1.p3.4.m4.1.1.2.3.cmml" xref="A1.p3.4.m4.1.1.2.3"><times id="A1.p3.4.m4.1.1.2.3.1.cmml" xref="A1.p3.4.m4.1.1.2.3.1"></times><ci id="A1.p3.4.m4.1.1.2.3.2.cmml" xref="A1.p3.4.m4.1.1.2.3.2">ùëü</ci><ci id="A1.p3.4.m4.1.1.2.3.3.cmml" xref="A1.p3.4.m4.1.1.2.3.3">ùëú</ci><ci id="A1.p3.4.m4.1.1.2.3.4.cmml" xref="A1.p3.4.m4.1.1.2.3.4">ùë°</ci></apply></apply><apply id="A1.p3.4.m4.1.1.3.cmml" xref="A1.p3.4.m4.1.1.3"><csymbol cd="ambiguous" id="A1.p3.4.m4.1.1.3.1.cmml" xref="A1.p3.4.m4.1.1.3">superscript</csymbol><ci id="A1.p3.4.m4.1.1.3.2.cmml" xref="A1.p3.4.m4.1.1.3.2">‚Ñù</ci><ci id="A1.p3.4.m4.1.1.3.3.cmml" xref="A1.p3.4.m4.1.1.3.3">ùê∂</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p3.4.m4.1c">X_{rot}\in\mathbb{R}^{C}</annotation><annotation encoding="application/x-llamapun" id="A1.p3.4.m4.1d">italic_X start_POSTSUBSCRIPT italic_r italic_o italic_t end_POSTSUBSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.p4">
<p class="ltx_p" id="A1.p4.3"><span class="ltx_text ltx_font_bold" id="A1.p4.3.1">Rotation and camera predictors.</span>
Inspired by¬†<span class="ltx_ERROR undefined" id="A1.p4.3.2">\citesupp</span>supp-hmrKanazawa17, the network for predicting the rotation and the mesh consists of a multilayer perceptron (MLP) regression module composed of two fully connected layers with 1024 neurons following the CNN backbone (see <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S4.F2" title="In 4 Method ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">2</span></a>). The image feature <math alttext="X_{rot}\in\mathbb{R}^{C}" class="ltx_Math" display="inline" id="A1.p4.1.m1.1"><semantics id="A1.p4.1.m1.1a"><mrow id="A1.p4.1.m1.1.1" xref="A1.p4.1.m1.1.1.cmml"><msub id="A1.p4.1.m1.1.1.2" xref="A1.p4.1.m1.1.1.2.cmml"><mi id="A1.p4.1.m1.1.1.2.2" xref="A1.p4.1.m1.1.1.2.2.cmml">X</mi><mrow id="A1.p4.1.m1.1.1.2.3" xref="A1.p4.1.m1.1.1.2.3.cmml"><mi id="A1.p4.1.m1.1.1.2.3.2" xref="A1.p4.1.m1.1.1.2.3.2.cmml">r</mi><mo id="A1.p4.1.m1.1.1.2.3.1" xref="A1.p4.1.m1.1.1.2.3.1.cmml">‚Å¢</mo><mi id="A1.p4.1.m1.1.1.2.3.3" xref="A1.p4.1.m1.1.1.2.3.3.cmml">o</mi><mo id="A1.p4.1.m1.1.1.2.3.1a" xref="A1.p4.1.m1.1.1.2.3.1.cmml">‚Å¢</mo><mi id="A1.p4.1.m1.1.1.2.3.4" xref="A1.p4.1.m1.1.1.2.3.4.cmml">t</mi></mrow></msub><mo id="A1.p4.1.m1.1.1.1" xref="A1.p4.1.m1.1.1.1.cmml">‚àà</mo><msup id="A1.p4.1.m1.1.1.3" xref="A1.p4.1.m1.1.1.3.cmml"><mi id="A1.p4.1.m1.1.1.3.2" xref="A1.p4.1.m1.1.1.3.2.cmml">‚Ñù</mi><mi id="A1.p4.1.m1.1.1.3.3" xref="A1.p4.1.m1.1.1.3.3.cmml">C</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="A1.p4.1.m1.1b"><apply id="A1.p4.1.m1.1.1.cmml" xref="A1.p4.1.m1.1.1"><in id="A1.p4.1.m1.1.1.1.cmml" xref="A1.p4.1.m1.1.1.1"></in><apply id="A1.p4.1.m1.1.1.2.cmml" xref="A1.p4.1.m1.1.1.2"><csymbol cd="ambiguous" id="A1.p4.1.m1.1.1.2.1.cmml" xref="A1.p4.1.m1.1.1.2">subscript</csymbol><ci id="A1.p4.1.m1.1.1.2.2.cmml" xref="A1.p4.1.m1.1.1.2.2">ùëã</ci><apply id="A1.p4.1.m1.1.1.2.3.cmml" xref="A1.p4.1.m1.1.1.2.3"><times id="A1.p4.1.m1.1.1.2.3.1.cmml" xref="A1.p4.1.m1.1.1.2.3.1"></times><ci id="A1.p4.1.m1.1.1.2.3.2.cmml" xref="A1.p4.1.m1.1.1.2.3.2">ùëü</ci><ci id="A1.p4.1.m1.1.1.2.3.3.cmml" xref="A1.p4.1.m1.1.1.2.3.3">ùëú</ci><ci id="A1.p4.1.m1.1.1.2.3.4.cmml" xref="A1.p4.1.m1.1.1.2.3.4">ùë°</ci></apply></apply><apply id="A1.p4.1.m1.1.1.3.cmml" xref="A1.p4.1.m1.1.1.3"><csymbol cd="ambiguous" id="A1.p4.1.m1.1.1.3.1.cmml" xref="A1.p4.1.m1.1.1.3">superscript</csymbol><ci id="A1.p4.1.m1.1.1.3.2.cmml" xref="A1.p4.1.m1.1.1.3.2">‚Ñù</ci><ci id="A1.p4.1.m1.1.1.3.3.cmml" xref="A1.p4.1.m1.1.1.3.3">ùê∂</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p4.1.m1.1c">X_{rot}\in\mathbb{R}^{C}</annotation><annotation encoding="application/x-llamapun" id="A1.p4.1.m1.1d">italic_X start_POSTSUBSCRIPT italic_r italic_o italic_t end_POSTSUBSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPT</annotation></semantics></math> is concatenated with the flattened pose <math alttext="p\in\mathbb{R}^{17*3}" class="ltx_Math" display="inline" id="A1.p4.2.m2.1"><semantics id="A1.p4.2.m2.1a"><mrow id="A1.p4.2.m2.1.1" xref="A1.p4.2.m2.1.1.cmml"><mi id="A1.p4.2.m2.1.1.2" xref="A1.p4.2.m2.1.1.2.cmml">p</mi><mo id="A1.p4.2.m2.1.1.1" xref="A1.p4.2.m2.1.1.1.cmml">‚àà</mo><msup id="A1.p4.2.m2.1.1.3" xref="A1.p4.2.m2.1.1.3.cmml"><mi id="A1.p4.2.m2.1.1.3.2" xref="A1.p4.2.m2.1.1.3.2.cmml">‚Ñù</mi><mrow id="A1.p4.2.m2.1.1.3.3" xref="A1.p4.2.m2.1.1.3.3.cmml"><mn id="A1.p4.2.m2.1.1.3.3.2" xref="A1.p4.2.m2.1.1.3.3.2.cmml">17</mn><mo id="A1.p4.2.m2.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="A1.p4.2.m2.1.1.3.3.1.cmml">‚àó</mo><mn id="A1.p4.2.m2.1.1.3.3.3" xref="A1.p4.2.m2.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A1.p4.2.m2.1b"><apply id="A1.p4.2.m2.1.1.cmml" xref="A1.p4.2.m2.1.1"><in id="A1.p4.2.m2.1.1.1.cmml" xref="A1.p4.2.m2.1.1.1"></in><ci id="A1.p4.2.m2.1.1.2.cmml" xref="A1.p4.2.m2.1.1.2">ùëù</ci><apply id="A1.p4.2.m2.1.1.3.cmml" xref="A1.p4.2.m2.1.1.3"><csymbol cd="ambiguous" id="A1.p4.2.m2.1.1.3.1.cmml" xref="A1.p4.2.m2.1.1.3">superscript</csymbol><ci id="A1.p4.2.m2.1.1.3.2.cmml" xref="A1.p4.2.m2.1.1.3.2">‚Ñù</ci><apply id="A1.p4.2.m2.1.1.3.3.cmml" xref="A1.p4.2.m2.1.1.3.3"><times id="A1.p4.2.m2.1.1.3.3.1.cmml" xref="A1.p4.2.m2.1.1.3.3.1"></times><cn id="A1.p4.2.m2.1.1.3.3.2.cmml" type="integer" xref="A1.p4.2.m2.1.1.3.3.2">17</cn><cn id="A1.p4.2.m2.1.1.3.3.3.cmml" type="integer" xref="A1.p4.2.m2.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p4.2.m2.1c">p\in\mathbb{R}^{17*3}</annotation><annotation encoding="application/x-llamapun" id="A1.p4.2.m2.1d">italic_p ‚àà blackboard_R start_POSTSUPERSCRIPT 17 ‚àó 3 end_POSTSUPERSCRIPT</annotation></semantics></math> before being fed to the MLP. The rotation is predicted in the 6d-rotation format¬†<span class="ltx_ERROR undefined" id="A1.p4.3.3">\citesupp</span>supp-zhou2019continuity, and <math alttext="p" class="ltx_Math" display="inline" id="A1.p4.3.m3.1"><semantics id="A1.p4.3.m3.1a"><mi id="A1.p4.3.m3.1.1" xref="A1.p4.3.m3.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="A1.p4.3.m3.1b"><ci id="A1.p4.3.m3.1.1.cmml" xref="A1.p4.3.m3.1.1">ùëù</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p4.3.m3.1c">p</annotation><annotation encoding="application/x-llamapun" id="A1.p4.3.m3.1d">italic_p</annotation></semantics></math> is initialized with the SMPL T-pose.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.p5">
<p class="ltx_p" id="A1.p5.1"><span class="ltx_text ltx_font_bold" id="A1.p5.1.1">Latent canonical mesh regressor.</span>
The Transformer follows the original Transformer architecture¬†<span class="ltx_ERROR undefined" id="A1.p5.1.2">\citesupp</span>supp-vaswani2017attention. Its hidden dimension is <math alttext="D=512" class="ltx_Math" display="inline" id="A1.p5.1.m1.1"><semantics id="A1.p5.1.m1.1a"><mrow id="A1.p5.1.m1.1.1" xref="A1.p5.1.m1.1.1.cmml"><mi id="A1.p5.1.m1.1.1.2" xref="A1.p5.1.m1.1.1.2.cmml">D</mi><mo id="A1.p5.1.m1.1.1.1" xref="A1.p5.1.m1.1.1.1.cmml">=</mo><mn id="A1.p5.1.m1.1.1.3" xref="A1.p5.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.p5.1.m1.1b"><apply id="A1.p5.1.m1.1.1.cmml" xref="A1.p5.1.m1.1.1"><eq id="A1.p5.1.m1.1.1.1.cmml" xref="A1.p5.1.m1.1.1.1"></eq><ci id="A1.p5.1.m1.1.1.2.cmml" xref="A1.p5.1.m1.1.1.2">ùê∑</ci><cn id="A1.p5.1.m1.1.1.3.cmml" type="integer" xref="A1.p5.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p5.1.m1.1c">D=512</annotation><annotation encoding="application/x-llamapun" id="A1.p5.1.m1.1d">italic_D = 512</annotation></semantics></math>, and all MLPs in the encoder and decoder have a hidden size of 1024. It has 5.3 million parameters (7.6 million for the version used in the COCO¬†<span class="ltx_ERROR undefined" id="A1.p5.1.3">\citesupp</span>supp-lin2014microsoft and large-scale training). We use sinusoidal positional encoding for the input tokens.</p>
</div>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Additional qualitative results</h2>
<figure class="ltx_figure" id="A2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="442" id="A2.F1.g1" src="x6.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span class="ltx_text ltx_font_bold" id="A2.F1.2.1">Additional comparisons.</span> We compare our method with HMR, CLIFF, and FastMETRO-S on 3DPW trained on 3DPW (first row) and EMDB trained on EMDB.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">Additional comparisons with other methods trained on little data are provided in¬†<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#A2.F1" title="In Appendix B Additional qualitative results ‚Ä£ Appendix A Implementation details ‚Ä£ 5.4 Training on large-scale datasets ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">1</span></a> to complement the results of¬†<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S5.F4" title="In 5.3 Training on limited data ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure class="ltx_figure" id="A2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="761" id="A2.F2.g1" src="extracted/5731512/supp_viz.png" width="509"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span class="ltx_text ltx_font_bold" id="A2.F2.2.1">Qualitative results.</span> We visualize results obtained with VQ-HPS on the 3DPW dataset.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="A2.p2">
<p class="ltx_p" id="A2.p2.1">We also provide some qualitative results obtained with VQ-HPS using an HRNet backbone as evaluated in¬†<a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S5.SS4" title="5.4 Training on large-scale datasets ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Sec.</span>¬†<span class="ltx_text ltx_ref_tag">5.4</span></a>) are shown in <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#A2.F2" title="In Appendix B Additional qualitative results ‚Ä£ Appendix A Implementation details ‚Ä£ 5.4 Training on large-scale datasets ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Visualization of predictions during training</h2>
<figure class="ltx_figure" id="A3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="752" id="A3.F3.g1" src="x7.png" width="581"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Visualization of random validation samples during the training process on EMDB. Meshes are smooth from the first epoch and become anthropomorphic in about 5 epochs.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">We visualize validation samples when training on the EMDB dataset (see <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S5.SS3" title="5.3 Training on limited data ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Sec.</span>¬†<span class="ltx_text ltx_ref_tag">5.3</span></a>) in <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#A3.F3" title="In Appendix C Visualization of predictions during training ‚Ä£ Appendix B Additional qualitative results ‚Ä£ Appendix A Implementation details ‚Ä£ 5.4 Training on large-scale datasets ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">3</span></a>. The meshes are smooth from the first epoch and become anthropomorphic in about 5 epochs. As discussed in the paper, we believe this is due to multiple factors. First, the Mesh-VQ-VAE, whose decoder is essential to VQ-HPS, was pre-trained on large-scale human motion datasets and is frozen afterward. This pre-training is probably a regularization that reduces the labeled data needed for learning to solve the HPSE task. Second, VQ-HPS learns a distribution over discrete indices for producing anthropomorphic meshes. Learning a distribution over 54 discrete indices is easier than learning the structure from 6890 3D coordinates.</p>
</div>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Analysis of the error</h2>
<figure class="ltx_figure" id="A4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="387" id="A4.F4.g1" src="x8.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span class="ltx_text ltx_font_bold" id="A4.F4.2.1">Distribution of the PVE.</span> We study the distribution of the per-vertex error for all 4 methods on the testing set of EMDB. Black dots represent the mean. The box plots give the 1st and 3rd quartiles, as well as the median value, with a notch for the confidence interval around the median value. The whiskers extend from the box to the farthest data point, lying within 1.5x the interquartile range (IQR) from the box. We also draw the violin plot to visualize the distribution of the error.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">We analyze the distribution of the per-vertex error (PVE, see <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S5.SS2" title="5.2 Metrics ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Sec.</span>¬†<span class="ltx_text ltx_ref_tag">5.2</span></a>) on EMDB1 when models are trained on EMDB2 (see <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S5.SS3" title="5.3 Training on limited data ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Sec.</span>¬†<span class="ltx_text ltx_ref_tag">5.3</span></a>, and quantitative results in <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S5.T1" title="In 5.3 Training on limited data ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Tab.</span>¬†<span class="ltx_text ltx_ref_tag">1</span></a>) in <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#A4.F4" title="In Appendix D Analysis of the error ‚Ä£ Appendix C Visualization of predictions during training ‚Ä£ Appendix B Additional qualitative results ‚Ä£ Appendix A Implementation details ‚Ä£ 5.4 Training on large-scale datasets ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">4</span></a>. We can see that VQ-HPS outperforms other methods in many ways. First, its mean, median, 1st, and 3rd quartiles are lower than other methods. The distance between the 1st and 3rd quartiles is much smaller for VQ-HPS than for other methods. Non-parametric methods have a very high concentration of samples with low error, while parametric methods have a more uniform distribution. Another significant advantage of VQ-HPS is its many fewer outliers with high errors.</p>
</div>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Failure cases</h2>
<figure class="ltx_figure" id="A5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="477" id="A5.F5.g1" src="x9.png" width="581"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span class="ltx_text ltx_font_bold" id="A5.F5.2.1">Failure cases.</span> We study the failure cases when training on scarce data and large-scale datasets.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="A5.p1">
<p class="ltx_p" id="A5.p1.1">Failure cases are shown in <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#A5.F5" title="In Appendix E Failure cases ‚Ä£ Appendix D Analysis of the error ‚Ä£ Appendix C Visualization of predictions during training ‚Ä£ Appendix B Additional qualitative results ‚Ä£ Appendix A Implementation details ‚Ä£ 5.4 Training on large-scale datasets ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">5</span></a>. When training on scarce data (experiments of <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S5.SS3" title="5.3 Training on limited data ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Sec.</span>¬†<span class="ltx_text ltx_ref_tag">5.3</span></a>), low visibility and unusual poses lead to outliers with completely different poses and body orientations. There is a clear improvement when training on large-scale datasets (see <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S5.SS4" title="5.4 Training on large-scale datasets ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Sec.</span>¬†<span class="ltx_text ltx_ref_tag">5.4</span></a>), but there are still some failure cases. In the first image, the model estimates the pose of the wrong person. In the second image, a very unusual pose leads to a non-anthropomorphic prediction (especially for the left arm). Finally, VQ-HPS sometimes makes global orientation mistakes for unusual poses. Potential improvements for avoiding such failure cases would be improving the feature extractors¬†<span class="ltx_ERROR undefined" id="A5.p1.1.1">\citesupp</span>supp-armando2023cross or using additional data with more unusual poses¬†<span class="ltx_ERROR undefined" id="A5.p1.1.2">\citesupp</span>supp-Black_CVPR_2023, as recent works¬†<span class="ltx_ERROR undefined" id="A5.p1.1.3">\citesupp</span>supp-pang2022benchmarking highlighted the pivotal role of backbones and data for obtaining better performance.</p>
</div>
<section class="ltx_appendix" id="A6">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Estimating body shapes</h2>
<figure class="ltx_figure" id="A6.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="467" id="A6.F6.g1" src="x10.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span><span class="ltx_text ltx_font_bold" id="A6.F6.2.1">Estimating body shapes.</span> We evaluate VQ-HPS qualitatively on SSP-3D before and after finetuning on a dataset with diverse body shapes.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="A6.p1">
<p class="ltx_p" id="A6.p1.1">We propose to evaluate the potential of VQ-HPS for estimating body shapes. As demonstrated in prior works, the most important for estimating accurate body shapes is the training data¬†<span class="ltx_ERROR undefined" id="A6.p1.1.1">\citesupp</span>supp-Black_CVPR_2023. We finetune VQ-HPS (see <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S5.SS4" title="5.4 Training on large-scale datasets ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Sec.</span>¬†<span class="ltx_text ltx_ref_tag">5.4</span></a>, "ResNet-50 backbone" in <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S5.SS4" title="5.4 Training on large-scale datasets ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Sec.</span>¬†<span class="ltx_text ltx_ref_tag">5.4</span></a>) on 5% of Bedlam¬†<span class="ltx_ERROR undefined" id="A6.p1.1.2">\citesupp</span>supp-Black_CVPR_2023, a synthetic dataset with diverse body shapes. In <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#A6.F6" title="In Appendix F Estimating body shapes ‚Ä£ Appendix E Failure cases ‚Ä£ Appendix D Analysis of the error ‚Ä£ Appendix C Visualization of predictions during training ‚Ä£ Appendix B Additional qualitative results ‚Ä£ Appendix A Implementation details ‚Ä£ 5.4 Training on large-scale datasets ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">6</span></a>, we compare the results obtained on SSP-3D¬†<span class="ltx_ERROR undefined" id="A6.p1.1.3">\citesupp</span>supp-STRAPS2018BMVC, a dataset of real images with challenging body shapes, before and after finetuning VQ-HPS on Bedlam. The clear improvement suggests that VQ-HPS could be used for body shape estimation if trained on appropriate data.</p>
</div>
<section class="ltx_appendix" id="A7">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Visualization of the Mesh-VQ-VAE</h2>
<figure class="ltx_figure" id="A7.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="328" id="A7.F7.g1" src="x11.png" width="581"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span><span class="ltx_text ltx_font_bold" id="A7.F7.4.1">Exchanging body parts.</span> We exchange body parts between the two meshes in the latent space after manually identifying the indices responsible for each body part. The color map shows the distance between <math alttext="\mathcal{M}_{1}" class="ltx_Math" display="inline" id="A7.F7.2.m1.1"><semantics id="A7.F7.2.m1.1b"><msub id="A7.F7.2.m1.1.1" xref="A7.F7.2.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A7.F7.2.m1.1.1.2" xref="A7.F7.2.m1.1.1.2.cmml">‚Ñ≥</mi><mn id="A7.F7.2.m1.1.1.3" xref="A7.F7.2.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A7.F7.2.m1.1c"><apply id="A7.F7.2.m1.1.1.cmml" xref="A7.F7.2.m1.1.1"><csymbol cd="ambiguous" id="A7.F7.2.m1.1.1.1.cmml" xref="A7.F7.2.m1.1.1">subscript</csymbol><ci id="A7.F7.2.m1.1.1.2.cmml" xref="A7.F7.2.m1.1.1.2">‚Ñ≥</ci><cn id="A7.F7.2.m1.1.1.3.cmml" type="integer" xref="A7.F7.2.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.F7.2.m1.1d">\mathcal{M}_{1}</annotation><annotation encoding="application/x-llamapun" id="A7.F7.2.m1.1e">caligraphic_M start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> and the reconstruction.</figcaption>
</figure>
<figure class="ltx_figure" id="A7.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="177" id="A7.F8.g1" src="x12.png" width="498"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span><span class="ltx_text ltx_font_bold" id="A7.F8.2.1">Exchanging the torso.</span> We exchange the torsos between the two meshes in the latent space.</figcaption>
</figure>
<figure class="ltx_figure" id="A7.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="283" id="A7.F9.g1" src="x13.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span><span class="ltx_text ltx_font_bold" id="A7.F9.2.1">Interpolation in the latent space of Mesh-VQ-VAE.</span> Note that the latent space encapsulates both pose and shape.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="A7.p1">
<p class="ltx_p" id="A7.p1.4">The main paper ( see <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#S4.SS1" title="4.1 Proposed HPSE method ‚Ä£ 4 Method ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Sec.</span>¬†<span class="ltx_text ltx_ref_tag">4.1</span></a>) explains that the Mesh-VQ-VAE is fully convolutional. Hence, the latent space preserves the spatial structure of the mesh. We manually identify the body part associated with each index by visualizing the reconstruction after randomly modifying each. We propose to visualize this property by exchanging body parts between different meshes. Specifically, given two meshes <math alttext="\mathcal{M}_{1}" class="ltx_Math" display="inline" id="A7.p1.1.m1.1"><semantics id="A7.p1.1.m1.1a"><msub id="A7.p1.1.m1.1.1" xref="A7.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A7.p1.1.m1.1.1.2" xref="A7.p1.1.m1.1.1.2.cmml">‚Ñ≥</mi><mn id="A7.p1.1.m1.1.1.3" xref="A7.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A7.p1.1.m1.1b"><apply id="A7.p1.1.m1.1.1.cmml" xref="A7.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A7.p1.1.m1.1.1.1.cmml" xref="A7.p1.1.m1.1.1">subscript</csymbol><ci id="A7.p1.1.m1.1.1.2.cmml" xref="A7.p1.1.m1.1.1.2">‚Ñ≥</ci><cn id="A7.p1.1.m1.1.1.3.cmml" type="integer" xref="A7.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.p1.1.m1.1c">\mathcal{M}_{1}</annotation><annotation encoding="application/x-llamapun" id="A7.p1.1.m1.1d">caligraphic_M start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\mathcal{M}_{2}" class="ltx_Math" display="inline" id="A7.p1.2.m2.1"><semantics id="A7.p1.2.m2.1a"><msub id="A7.p1.2.m2.1.1" xref="A7.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A7.p1.2.m2.1.1.2" xref="A7.p1.2.m2.1.1.2.cmml">‚Ñ≥</mi><mn id="A7.p1.2.m2.1.1.3" xref="A7.p1.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="A7.p1.2.m2.1b"><apply id="A7.p1.2.m2.1.1.cmml" xref="A7.p1.2.m2.1.1"><csymbol cd="ambiguous" id="A7.p1.2.m2.1.1.1.cmml" xref="A7.p1.2.m2.1.1">subscript</csymbol><ci id="A7.p1.2.m2.1.1.2.cmml" xref="A7.p1.2.m2.1.1.2">‚Ñ≥</ci><cn id="A7.p1.2.m2.1.1.3.cmml" type="integer" xref="A7.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.p1.2.m2.1c">\mathcal{M}_{2}</annotation><annotation encoding="application/x-llamapun" id="A7.p1.2.m2.1d">caligraphic_M start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>, we encode both meshes and decode <math alttext="\mathcal{M}_{1}" class="ltx_Math" display="inline" id="A7.p1.3.m3.1"><semantics id="A7.p1.3.m3.1a"><msub id="A7.p1.3.m3.1.1" xref="A7.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A7.p1.3.m3.1.1.2" xref="A7.p1.3.m3.1.1.2.cmml">‚Ñ≥</mi><mn id="A7.p1.3.m3.1.1.3" xref="A7.p1.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A7.p1.3.m3.1b"><apply id="A7.p1.3.m3.1.1.cmml" xref="A7.p1.3.m3.1.1"><csymbol cd="ambiguous" id="A7.p1.3.m3.1.1.1.cmml" xref="A7.p1.3.m3.1.1">subscript</csymbol><ci id="A7.p1.3.m3.1.1.2.cmml" xref="A7.p1.3.m3.1.1.2">‚Ñ≥</ci><cn id="A7.p1.3.m3.1.1.3.cmml" type="integer" xref="A7.p1.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.p1.3.m3.1c">\mathcal{M}_{1}</annotation><annotation encoding="application/x-llamapun" id="A7.p1.3.m3.1d">caligraphic_M start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> after replacing the indices of a given body part by <math alttext="\mathcal{M}_{2}" class="ltx_Math" display="inline" id="A7.p1.4.m4.1"><semantics id="A7.p1.4.m4.1a"><msub id="A7.p1.4.m4.1.1" xref="A7.p1.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A7.p1.4.m4.1.1.2" xref="A7.p1.4.m4.1.1.2.cmml">‚Ñ≥</mi><mn id="A7.p1.4.m4.1.1.3" xref="A7.p1.4.m4.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="A7.p1.4.m4.1b"><apply id="A7.p1.4.m4.1.1.cmml" xref="A7.p1.4.m4.1.1"><csymbol cd="ambiguous" id="A7.p1.4.m4.1.1.1.cmml" xref="A7.p1.4.m4.1.1">subscript</csymbol><ci id="A7.p1.4.m4.1.1.2.cmml" xref="A7.p1.4.m4.1.1.2">‚Ñ≥</ci><cn id="A7.p1.4.m4.1.1.3.cmml" type="integer" xref="A7.p1.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.p1.4.m4.1c">\mathcal{M}_{2}</annotation><annotation encoding="application/x-llamapun" id="A7.p1.4.m4.1d">caligraphic_M start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>. Results are shown in <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#A7.F7" title="In Appendix G Visualization of the Mesh-VQ-VAE ‚Ä£ Appendix F Estimating body shapes ‚Ä£ Appendix E Failure cases ‚Ä£ Appendix D Analysis of the error ‚Ä£ Appendix C Visualization of predictions during training ‚Ä£ Appendix B Additional qualitative results ‚Ä£ Appendix A Implementation details ‚Ä£ 5.4 Training on large-scale datasets ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A7.p2">
<p class="ltx_p" id="A7.p2.1">We can also modify the torso. To visualize it easier, we provide qualitative results for individuals with different body shapes in <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#A7.F8" title="In Appendix G Visualization of the Mesh-VQ-VAE ‚Ä£ Appendix F Estimating body shapes ‚Ä£ Appendix E Failure cases ‚Ä£ Appendix D Analysis of the error ‚Ä£ Appendix C Visualization of predictions during training ‚Ä£ Appendix B Additional qualitative results ‚Ä£ Appendix A Implementation details ‚Ä£ 5.4 Training on large-scale datasets ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">8</span></a>. Note that this slightly differs from modifying the body shape, as the arms and legs are unchanged.</p>
</div>
<div class="ltx_para ltx_noindent" id="A7.p3">
<p class="ltx_p" id="A7.p3.4">Finally, in <a class="ltx_ref" href="https://arxiv.org/html/2312.08291v4#A7.F9" title="In Appendix G Visualization of the Mesh-VQ-VAE ‚Ä£ Appendix F Estimating body shapes ‚Ä£ Appendix E Failure cases ‚Ä£ Appendix D Analysis of the error ‚Ä£ Appendix C Visualization of predictions during training ‚Ä£ Appendix B Additional qualitative results ‚Ä£ Appendix A Implementation details ‚Ä£ 5.4 Training on large-scale datasets ‚Ä£ 5 Results ‚Ä£ VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">9</span></a>, we show that we can interpolate between meshes <math alttext="\mathcal{M}_{1}" class="ltx_Math" display="inline" id="A7.p3.1.m1.1"><semantics id="A7.p3.1.m1.1a"><msub id="A7.p3.1.m1.1.1" xref="A7.p3.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A7.p3.1.m1.1.1.2" xref="A7.p3.1.m1.1.1.2.cmml">‚Ñ≥</mi><mn id="A7.p3.1.m1.1.1.3" xref="A7.p3.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A7.p3.1.m1.1b"><apply id="A7.p3.1.m1.1.1.cmml" xref="A7.p3.1.m1.1.1"><csymbol cd="ambiguous" id="A7.p3.1.m1.1.1.1.cmml" xref="A7.p3.1.m1.1.1">subscript</csymbol><ci id="A7.p3.1.m1.1.1.2.cmml" xref="A7.p3.1.m1.1.1.2">‚Ñ≥</ci><cn id="A7.p3.1.m1.1.1.3.cmml" type="integer" xref="A7.p3.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.p3.1.m1.1c">\mathcal{M}_{1}</annotation><annotation encoding="application/x-llamapun" id="A7.p3.1.m1.1d">caligraphic_M start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\mathcal{M}_{2}" class="ltx_Math" display="inline" id="A7.p3.2.m2.1"><semantics id="A7.p3.2.m2.1a"><msub id="A7.p3.2.m2.1.1" xref="A7.p3.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A7.p3.2.m2.1.1.2" xref="A7.p3.2.m2.1.1.2.cmml">‚Ñ≥</mi><mn id="A7.p3.2.m2.1.1.3" xref="A7.p3.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="A7.p3.2.m2.1b"><apply id="A7.p3.2.m2.1.1.cmml" xref="A7.p3.2.m2.1.1"><csymbol cd="ambiguous" id="A7.p3.2.m2.1.1.1.cmml" xref="A7.p3.2.m2.1.1">subscript</csymbol><ci id="A7.p3.2.m2.1.1.2.cmml" xref="A7.p3.2.m2.1.1.2">‚Ñ≥</ci><cn id="A7.p3.2.m2.1.1.3.cmml" type="integer" xref="A7.p3.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.p3.2.m2.1c">\mathcal{M}_{2}</annotation><annotation encoding="application/x-llamapun" id="A7.p3.2.m2.1d">caligraphic_M start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> in the latent space. Specifically, we do a linear interpolation between the corresponding continuous latent representations <math alttext="z_{1}" class="ltx_Math" display="inline" id="A7.p3.3.m3.1"><semantics id="A7.p3.3.m3.1a"><msub id="A7.p3.3.m3.1.1" xref="A7.p3.3.m3.1.1.cmml"><mi id="A7.p3.3.m3.1.1.2" xref="A7.p3.3.m3.1.1.2.cmml">z</mi><mn id="A7.p3.3.m3.1.1.3" xref="A7.p3.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A7.p3.3.m3.1b"><apply id="A7.p3.3.m3.1.1.cmml" xref="A7.p3.3.m3.1.1"><csymbol cd="ambiguous" id="A7.p3.3.m3.1.1.1.cmml" xref="A7.p3.3.m3.1.1">subscript</csymbol><ci id="A7.p3.3.m3.1.1.2.cmml" xref="A7.p3.3.m3.1.1.2">ùëß</ci><cn id="A7.p3.3.m3.1.1.3.cmml" type="integer" xref="A7.p3.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.p3.3.m3.1c">z_{1}</annotation><annotation encoding="application/x-llamapun" id="A7.p3.3.m3.1d">italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="z_{2}" class="ltx_Math" display="inline" id="A7.p3.4.m4.1"><semantics id="A7.p3.4.m4.1a"><msub id="A7.p3.4.m4.1.1" xref="A7.p3.4.m4.1.1.cmml"><mi id="A7.p3.4.m4.1.1.2" xref="A7.p3.4.m4.1.1.2.cmml">z</mi><mn id="A7.p3.4.m4.1.1.3" xref="A7.p3.4.m4.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="A7.p3.4.m4.1b"><apply id="A7.p3.4.m4.1.1.cmml" xref="A7.p3.4.m4.1.1"><csymbol cd="ambiguous" id="A7.p3.4.m4.1.1.1.cmml" xref="A7.p3.4.m4.1.1">subscript</csymbol><ci id="A7.p3.4.m4.1.1.2.cmml" xref="A7.p3.4.m4.1.1.2">ùëß</ci><cn id="A7.p3.4.m4.1.1.3.cmml" type="integer" xref="A7.p3.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.p3.4.m4.1c">z_{2}</annotation><annotation encoding="application/x-llamapun" id="A7.p3.4.m4.1d">italic_z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>, which are then quantized and decoded to obtain intermediate meshes.</p>
</div>
<div class="ltx_para ltx_noindent" id="A7.p4">
<span class="ltx_ERROR undefined" id="A7.p4.1">\bibliographystylesupp</span>
<p class="ltx_p" id="A7.p4.2">unsrt
<span class="ltx_ERROR undefined" id="A7.p4.2.1">\bibliographysupp</span>bib-supp</p>
</div>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</div>
</div>
</div>
</figure>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Jul 15 12:47:05 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
