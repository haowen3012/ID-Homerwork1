<!DOCTYPE html>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion</title>
<!--Generated on Thu Mar 14 11:58:24 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2403.09309v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S1" title="I Introduction ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S2" title="II Related Work ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Work</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S2.SS0.SSS1" title="II-1 Monocular Pose Estimation ‣ II Related Work ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-</span>1 </span>Monocular Pose Estimation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S2.SS0.SSS2" title="II-2 Pose Estimation as Set Prediction ‣ II Related Work ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-</span>2 </span>Pose Estimation as Set Prediction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S2.SS0.SSS3" title="II-3 6D Pose Tracking ‣ II Related Work ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-</span>3 </span>6D Pose Tracking</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S2.SS0.SSS4" title="II-4 Multi-Object Tracking ‣ II Related Work ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-</span>4 </span>Multi-Object Tracking</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S2.SS0.SSS5" title="II-5 Tracking-by-Attention in DETR-Like Models ‣ II Related Work ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-</span>5 </span>Tracking-by-Attention in DETR-Like Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S3" title="III Method ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Method</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S3.SS1" title="III-A Multi-Object Pose Estimation as Set Prediction ‣ III Method ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Multi-Object Pose Estimation as Set Prediction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S3.SS2" title="III-B MOTPose Architecture ‣ III Method ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">MOTPose Architecture</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S3.SS2.SSS1" title="III-B1 Temporal Embedding Fusion Module (TEFM) ‣ III-B MOTPose Architecture ‣ III Method ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span>1 </span>Temporal Embedding Fusion Module (TEFM)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S3.SS2.SSS2" title="III-B2 Temporal Object Fusion Module (TOFM) ‣ III-B MOTPose Architecture ‣ III Method ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span>2 </span>Temporal Object Fusion Module (TOFM)</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S3.SS3" title="III-C Matching ‣ III Method ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Matching</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S3.SS4" title="III-D Loss Function ‣ III Method ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span> </span><span class="ltx_text ltx_font_italic">Loss Function</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S3.SS4.SSS1" title="III-D1 Class Probability Loss ‣ III-D Loss Function ‣ III Method ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span>1 </span>Class Probability Loss</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S3.SS4.SSS2" title="III-D2 Bounding Box Loss ‣ III-D Loss Function ‣ III Method ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span>2 </span>Bounding Box Loss</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S3.SS4.SSS3" title="III-D3 Keypoint Loss ‣ III-D Loss Function ‣ III Method ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span>3 </span>Keypoint Loss</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S3.SS4.SSS4" title="III-D4 Pose Loss ‣ III-D Loss Function ‣ III Method ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span>4 </span>Pose Loss</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S3.SS4.SSS5" title="III-D5 Temporal Consistency Loss ‣ III-D Loss Function ‣ III Method ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span>5 </span>Temporal Consistency Loss</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S4" title="IV Evaluation ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Evaluation</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S4.SS1" title="IV-A Datasets ‣ IV Evaluation ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Datasets</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S4.SS1.SSS1" title="IV-A1 YCB-Video ‣ IV-A Datasets ‣ IV Evaluation ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>1 </span>YCB-Video</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S4.SS1.SSS2" title="IV-A2 SynPick ‣ IV-A Datasets ‣ IV Evaluation ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>2 </span>SynPick</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S4.SS2" title="IV-B Metrics ‣ IV Evaluation ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Metrics</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S4.SS3" title="IV-C Implementation Details ‣ IV Evaluation ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Implementation Details</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S4.SS4" title="IV-D Results on SynPick ‣ IV Evaluation ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span> </span><span class="ltx_text ltx_font_italic">Results on SynPick</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S4.SS5" title="IV-E Results on YCB-Video ‣ IV Evaluation ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-E</span> </span><span class="ltx_text ltx_font_italic">Results on YCB-Video</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S4.SS6" title="IV-F Ablation Study ‣ IV Evaluation ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-F</span> </span><span class="ltx_text ltx_font_italic">Ablation Study</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S4.SS7" title="IV-G Limitations ‣ IV Evaluation ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-G</span> </span><span class="ltx_text ltx_font_italic">Limitations</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S5" title="V Conclusion ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S6" title="VI Acknowledgment ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Acknowledgment</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div aria-label="Conversion errors have been found" class="package-alerts ltx_document" role="status">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewbox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: fxthemecolor</li>
<li>failed: fxthemecolor</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p>
</div><div class="section" id="target-section"><div id="license-tr">License: CC BY 4.0</div><div id="watermark-tr">arXiv:2403.09309v1 [cs.RO] 14 Mar 2024</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Arul Selvam Periyasamy and Sven Behnke
</span><span class="ltx_author_notes">All authors are with the Autonomous Intelligent Systems group,
Computer Science Institute VI – Intelligent Systems and Robotics – and the Center for Robotics and the Lamarr Institute for Machine Learning and Artificial Intelligence, University of Bonn, Germany; <span class="ltx_text ltx_font_typewriter" id="id1.1.id1">periyasa@ais.uni-bonn.de</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">Cluttered bin-picking environments are challenging for pose estimation models.
Despite the impressive progress enabled by deep learning, single-view RGB pose estimation models perform poorly in cluttered dynamic environments.
Imbuing the rich temporal information contained in the video of scenes has the potential to enhance models’ ability to deal with the adverse effects of occlusion and the dynamic nature of the environments.
Moreover, joint object detection and pose estimation models are better suited to leverage the co-dependent nature of the tasks for improving the accuracy of both tasks.
To this end, we propose attention-based temporal fusion for multi-object 6D pose estimation that accumulates information across multiple frames of a video sequence.
Our MOTPose method takes a sequence of images as input and performs joint object detection and pose estimation for all objects in one forward pass.
It learns to aggregate both object embeddings and object parameters over multiple time steps using cross-attention-based fusion modules.
We evaluate our method on the physically-realistic cluttered bin-picking dataset SynPick and the YCB-Video dataset and demonstrate improved pose estimation accuracy as well as better object detection accuracy.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Object detection is the task of localizing instances of object categories in images—typically by predicting bounding box parameters. 6D pose estimation aims at predicting the position and orientation of objects in the sensor coordinate system.
Both tasks are essential for many autonomous robots and a prerequisite for object manipulation.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Although single-view pose estimation models have made significant progress in recent years, they face difficulties in cluttered environments <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx1" title="">1</a>]</cite> hampered by
occlusions, reflective surfaces, transparency, and other challenges.
One way to address these challenges is to utilize a sequence of images of the scene instead of a single image.
In a video sequence, image features and object attributes evolve smoothly over time.
Models can benefit from imbuing image features and predictions from the previous frames
while processing the current frame.
Also, enforcing temporal consistency of the image features and pose predictions from consecutive frames can
facilitate efficient learning and better accuracy.
Despite the apparent advantages of temporal processing, the popularity of single-view pose estimation methods can be attributed to the complexity, computation, and memory overhead of video pose estimation methods.
Furthermore, CNN-based models for video processing often utilize 3D convolutions, which need more parameters and are slow compared to their 2D counterparts.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Lately, the multi-head attention-based transformer architecture, which was initially proposed for natural language processing tasks, has shown tremendous capabilities in modeling long-term dependencies in many domains like audio, image, video, etc. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx6" title="">6</a>]</cite>.
Vision transformer architectures also enable single-stage models that jointly perform object detection and pose estimation for all objects in the scene in one forward pass <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx8" title="">8</a>]</cite>.
This ability is handy when dealing with highly cluttered bin-picking scenarios (see <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S1.F1" title="Figure 1 ‣ I Introduction ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>).
In this work, we propose a vision transformer model for multi-object 6D pose estimation from monocular video sequences.
The core component of the proposed MOTPose method is a cross-attention-based temporal fusion mechanism that fuses features from multiple past frames while processing the current frame.
We use the stacked object embeddings from the past time steps as key and value in the cross-attention computation while the object embeddings from the current time step serve as query.
To counter the permutation-invariant nature of the attention mechanism in the temporal fusion modules, we utilize relative frame encoding (RFE).</p>
</div>
<figure class="ltx_figure" id="S1.F1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S1.F1.1">
<tr class="ltx_tr" id="S1.F1.1.1">
<td class="ltx_td ltx_align_center" id="S1.F1.1.1.1" style="padding-bottom:-6.02773pt;padding-left:0.3pt;padding-right:0.3pt;">
<span class="ltx_ERROR undefined" id="S1.F1.1.1.1.1">\set@curr@file</span>figures/teaser/_pick_untargeted-000037_000027.jpg<span class="ltx_ERROR undefined" id="S1.F1.1.1.1.2">\Gin@getbase</span><span class="ltx_ERROR undefined" id="S1.F1.1.1.1.3">\Gin@sepdefault</span>
</td>
<td class="ltx_td ltx_align_center" id="S1.F1.1.1.2" style="padding-bottom:-6.02773pt;padding-left:0.3pt;padding-right:0.3pt;">
<span class="ltx_ERROR undefined" id="S1.F1.1.1.2.1">\set@curr@file</span>figures/teaser/_pick_untargeted-000037_000085.jpg<span class="ltx_ERROR undefined" id="S1.F1.1.1.2.2">\Gin@getbase</span><span class="ltx_ERROR undefined" id="S1.F1.1.1.2.3">\Gin@sepdefault</span>
</td>
<td class="ltx_td ltx_align_center" id="S1.F1.1.1.3" style="padding-bottom:-6.02773pt;padding-left:0.3pt;padding-right:0.3pt;">
<span class="ltx_ERROR undefined" id="S1.F1.1.1.3.1">\set@curr@file</span>figures/teaser/_pick_untargeted-000037_000279.jpg<span class="ltx_ERROR undefined" id="S1.F1.1.1.3.2">\Gin@getbase</span><span class="ltx_ERROR undefined" id="S1.F1.1.1.3.3">\Gin@sepdefault</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.F1.1.2">
<td class="ltx_td ltx_align_center" id="S1.F1.1.2.1" style="padding-left:0.3pt;padding-right:0.3pt;"><span class="ltx_text" id="S1.F1.1.2.1.1" style="font-size:70%;">(1)</span></td>
<td class="ltx_td ltx_align_center" id="S1.F1.1.2.2" style="padding-left:0.3pt;padding-right:0.3pt;"><span class="ltx_text" id="S1.F1.1.2.2.1" style="font-size:70%;">(2)</span></td>
<td class="ltx_td ltx_align_center" id="S1.F1.1.2.3" style="padding-left:0.3pt;padding-right:0.3pt;"><span class="ltx_text" id="S1.F1.1.2.3.1" style="font-size:70%;">(3)</span></td>
</tr>
<tr class="ltx_tr" id="S1.F1.1.3">
<td class="ltx_td ltx_align_center" id="S1.F1.1.3.1" style="padding-bottom:-6.02773pt;padding-left:0.3pt;padding-right:0.3pt;">
<span class="ltx_ERROR undefined" id="S1.F1.1.3.1.1">\set@curr@file</span>figures/teaser/_pick_untargeted-000037_000350.jpg<span class="ltx_ERROR undefined" id="S1.F1.1.3.1.2">\Gin@getbase</span><span class="ltx_ERROR undefined" id="S1.F1.1.3.1.3">\Gin@sepdefault</span>
</td>
<td class="ltx_td ltx_align_center" id="S1.F1.1.3.2" style="padding-bottom:-6.02773pt;padding-left:0.3pt;padding-right:0.3pt;">
<span class="ltx_ERROR undefined" id="S1.F1.1.3.2.1">\set@curr@file</span>figures/teaser/_pick_untargeted-000037_000407.jpg<span class="ltx_ERROR undefined" id="S1.F1.1.3.2.2">\Gin@getbase</span><span class="ltx_ERROR undefined" id="S1.F1.1.3.2.3">\Gin@sepdefault</span>
</td>
<td class="ltx_td ltx_align_center" id="S1.F1.1.3.3" style="padding-bottom:-6.02773pt;padding-left:0.3pt;padding-right:0.3pt;">
<span class="ltx_ERROR undefined" id="S1.F1.1.3.3.1">\set@curr@file</span>figures/teaser/_pick_untargeted-000037_000499.jpg<span class="ltx_ERROR undefined" id="S1.F1.1.3.3.2">\Gin@getbase</span><span class="ltx_ERROR undefined" id="S1.F1.1.3.3.3">\Gin@sepdefault</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.F1.1.4">
<td class="ltx_td ltx_align_center" id="S1.F1.1.4.1" style="padding-left:0.3pt;padding-right:0.3pt;"><span class="ltx_text" id="S1.F1.1.4.1.1" style="font-size:70%;">(4)</span></td>
<td class="ltx_td ltx_align_center" id="S1.F1.1.4.2" style="padding-left:0.3pt;padding-right:0.3pt;"><span class="ltx_text" id="S1.F1.1.4.2.1" style="font-size:70%;">(5)</span></td>
<td class="ltx_td ltx_align_center" id="S1.F1.1.4.3" style="padding-left:0.3pt;padding-right:0.3pt;"><span class="ltx_text" id="S1.F1.1.4.3.1" style="font-size:70%;">(6)</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Multi-object pose predictions for a cluttered bin-picking scene from the SynPick dataset (Untargeted-pick, Sequence 38).
MOTPose jointly detects and estimates 6D pose for all objects in the scene in a single step
using a vision-transformer model by fusing temporal information across multiple frames.
Predicted object poses are visualized by contours.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Our contributions include:
</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">a multi-object pose estimation model for dynamic video sequences,</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">a method for cross-attention-based temporal fusion of object embeddings and object-specific outputs over multiple frames,</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">SynPick-Ext, an extended version of the physically-realistic dataset SynPick consisting of 300 additional video sequences for each action split, and</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">quantitative evaluation of the joint object detection and pose estimation task on SynPick, and competitive results on YCB-Video while being lighter and faster than other methods.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related Work</span>
</h2>
<section class="ltx_subsubsection" id="S2.SS0.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS0.SSS1.5.1.1">II-</span>1 </span>Monocular Pose Estimation</h4>
<div class="ltx_para" id="S2.SS0.SSS1.p1">
<p class="ltx_p" id="S2.SS0.SSS1.p1.1">Object pose estimation from RGB images has been a long-standing problem in computer vision.
The traditional methods before the advent of deep learning include template-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx10" title="">10</a>]</cite> and
feature-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx13" title="">13</a>]</cite>.
Modern deep-learning-based approaches include direct methods that regress the 6D pose parameters given the input RGB image <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx17" title="">17</a>]</cite>,
keypoint-based methods that predict the pixel coordinates of 3D keypoints first and then use the <span class="ltx_text ltx_font_italic" id="S2.SS0.SSS1.p1.1.1">perspective-n-points</span> (P<span class="ltx_text ltx_font_italic" id="S2.SS0.SSS1.p1.1.2">n</span>P) algorithm to recover 6D pose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx22" title="">22</a>]</cite>, and refinement-based methods.
The latter iteratively refine an initial pose estimate using either the <span class="ltx_text ltx_font_italic" id="S2.SS0.SSS1.p1.1.3">render-and-compare</span> framework <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx27" title="">27</a>]</cite>
or optical flow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx29" title="">29</a>]</cite>.
Most monocular pose estimation methods are multi-staged. The standard pipeline involves object detection and/or semantic segmentation, target object crop extraction, and pose estimation from the extracted crop. To enable end-to-end trainable multi-staged models, specialized operations
like <span class="ltx_text ltx_font_italic" id="S2.SS0.SSS1.p1.1.4">non-maximum suppression</span> (NMS), <span class="ltx_text ltx_font_italic" id="S2.SS0.SSS1.p1.1.5">region-of-interest pooling</span> (ROI), or <span class="ltx_text ltx_font_italic" id="S2.SS0.SSS1.p1.1.6">anchor boxes</span> are employed.
Notable single-stage methods include <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx31" title="">31</a>]</cite>.
Our proposed MOTPose method also incorporates single-stage design elements in its architecture.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS0.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS0.SSS2.5.1.1">II-</span>2 </span>Pose Estimation as Set Prediction</h4>
<div class="ltx_para" id="S2.SS0.SSS2.p1">
<p class="ltx_p" id="S2.SS0.SSS2.p1.1">In recent years, vision transformer architectures, that formulate computer vision tasks like object detection, instance segmentation, and pose estimation as a set prediction problem, are achieving impressive results.
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx32" title="">32</a>,  ]</cite> introduced DETR, the pioneering work in this new class of methods.
Several methods extended DETR for multi-object pose estimation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx33" title="">33</a>]</cite>.
Following these methods, the proposed MOTPose model formulates multi-object pose estimation from video sequences as a set prediction problem.
</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS0.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS0.SSS3.5.1.1">II-</span>3 </span>6D Pose Tracking</h4>
<div class="ltx_para" id="S2.SS0.SSS3.p1">
<p class="ltx_p" id="S2.SS0.SSS3.p1.1">Many of the early works for 6D pose tracking were based on particle filtering <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx36" title="">36</a>]</cite>, but
the performance of particle filters heavily depends on the accuracy of the observation model.
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx37" title="">37</a>,  ]</cite> introduced PoseRBPF utilizing a CNN-based observation model in the particle filtering framework.
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx38" title="">38</a>,  ]</cite> introduced <span class="ltx_text ltx_font_italic" id="S2.SS0.SSS3.p1.1.1">se</span>(3)-TrackNet, which achieved state-of-the-art-results in object pose tracking from RGB-D images.
In contrast to <span class="ltx_text ltx_font_italic" id="S2.SS0.SSS3.p1.1.2">se</span>(3)-TrackNet, our MOTPose method only needs RGB input and can estimate 6D pose for all objects in the input images in one stage.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS0.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS0.SSS4.5.1.1">II-</span>4 </span>Multi-Object Tracking</h4>
<div class="ltx_para" id="S2.SS0.SSS4.p1">
<p class="ltx_p" id="S2.SS0.SSS4.p1.1">Multi-object tracking aims at tracking 2D bounding boxes of the target instances in a given video sequence. The task is often challenging, due to the presence of multiple instances of the same category.
To address the problem of matching detections and tracked objects, sophisticated matching strategies were proposed <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx41" title="">41</a>]</cite>.
In this work, we focus mainly on improving pose estimation accuracy by fusing information over multiple time steps.
Thus, instead of focusing on the tracking metrics, we emphasize the standard pose estimation metrics—ADD-S and <span class="ltx_text" id="S2.SS0.SSS4.p1.1.1">ADD(-S)</span>—discussed in <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S4.SS2" title="IV-B Metrics ‣ IV Evaluation ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-B</span></span></a>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS0.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS0.SSS5.5.1.1">II-</span>5 </span>Tracking-by-Attention in DETR-Like Models</h4>
<div class="ltx_para" id="S2.SS0.SSS5.p1">
<p class="ltx_p" id="S2.SS0.SSS5.p1.2">Recently, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx42" title="">42</a>,  ]</cite> proposed TrackFormer, by introducing the <span class="ltx_text ltx_font_italic" id="S2.SS0.SSS5.p1.2.1">tracking-by-attention</span> framework in a DETR-like architecture.
Their key idea is to use object embeddings from time step <math alttext="t" class="ltx_Math" display="inline" id="S2.SS0.SSS5.p1.1.m1.1"><semantics id="S2.SS0.SSS5.p1.1.m1.1a"><mi id="S2.SS0.SSS5.p1.1.m1.1.1" xref="S2.SS0.SSS5.p1.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS5.p1.1.m1.1b"><ci id="S2.SS0.SSS5.p1.1.m1.1.1.cmml" xref="S2.SS0.SSS5.p1.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS5.p1.1.m1.1c">t</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS5.p1.1.m1.1d">italic_t</annotation></semantics></math> as object queries in time step <math alttext="t" class="ltx_Math" display="inline" id="S2.SS0.SSS5.p1.2.m2.1"><semantics id="S2.SS0.SSS5.p1.2.m2.1a"><mi id="S2.SS0.SSS5.p1.2.m2.1.1" xref="S2.SS0.SSS5.p1.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS5.p1.2.m2.1b"><ci id="S2.SS0.SSS5.p1.2.m2.1.1.cmml" xref="S2.SS0.SSS5.p1.2.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS5.p1.2.m2.1c">t</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS5.p1.2.m2.1d">italic_t</annotation></semantics></math>+1.
Propagating object embeddings over multiple time steps enables tracking the object over a long video sequence.
State-of-the-art methods for multi-object tracking utilizing the tracking-by-attention framework include MOTR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx43" title="">43</a>]</cite> and TransTrack <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx44" title="">44</a>]</cite>.
The main downside of such methods is that the number of object queries in a time step is dynamic, which makes efficient vectorized implementation harder and results in a slow training process.
In contrast to the <span class="ltx_text ltx_font_italic" id="S2.SS0.SSS5.p1.2.2">tracking-by-attention</span> framework, in our model, we fuse a fixed set of object embeddings and object-specific outputs from multiple time steps using cross-attention-based modules.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><span class="ltx_ERROR ltx_centering undefined" id="S2.F2.7">\set@curr@file</span>
<p class="ltx_p ltx_align_center" id="S2.F2.8">figures/model/MOTPose_zoom_image.png<span class="ltx_ERROR undefined" id="S2.F2.8.1">\Gin@getbase</span><span class="ltx_ERROR undefined" id="S2.F2.8.2">\Gin@sepdefault</span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>MOTPose architecture.
Positional Encoding: pixel coordinates represented using sine and cosine functions of different frequencies.
Object Queries: learned embeddings that are trained jointly with the model and remain fixed during inference (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S3.SS1" title="III-A Multi-Object Pose Estimation as Set Prediction ‣ III Method ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a>).
FFPN: Feed Forward Prediction Network.
TEFM: Temporal Embedding Fusion Module (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S3.SS2.SSS1" title="III-B1 Temporal Embedding Fusion Module (TEFM) ‣ III-B MOTPose Architecture ‣ III Method ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span>1</span></a>, Fig <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S3.F3" title="Figure 3 ‣ III-B1 Temporal Embedding Fusion Module (TEFM) ‣ III-B MOTPose Architecture ‣ III Method ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_tag">3</span></a>).
TOFM: Temporal Object Fusion Module (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S3.SS2.SSS2" title="III-B2 Temporal Object Fusion Module (TOFM) ‣ III-B MOTPose Architecture ‣ III Method ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span>2</span></a>).
<math alttext="\oplus" class="ltx_Math" display="inline" id="S2.F2.4.m1.1"><semantics id="S2.F2.4.m1.1b"><mo id="S2.F2.4.m1.1.1" xref="S2.F2.4.m1.1.1.cmml">⊕</mo><annotation-xml encoding="MathML-Content" id="S2.F2.4.m1.1c"><csymbol cd="latexml" id="S2.F2.4.m1.1.1.cmml" xref="S2.F2.4.m1.1.1">direct-sum</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.4.m1.1d">\oplus</annotation><annotation encoding="application/x-llamapun" id="S2.F2.4.m1.1e">⊕</annotation></semantics></math>: Element-wise addition.
<math alttext="\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}%
\pgfsys@color@rgb@stroke{1}{0}{0}\pgfsys@color@rgb@fill{1}{0}{0}\oplus" class="ltx_Math" display="inline" id="S2.F2.5.m2.1"><semantics id="S2.F2.5.m2.1b"><mo id="S2.F2.5.m2.1.1" mathcolor="#FF0000" xref="S2.F2.5.m2.1.1.cmml">⊕</mo><annotation-xml encoding="MathML-Content" id="S2.F2.5.m2.1c"><csymbol cd="latexml" id="S2.F2.5.m2.1.1.cmml" xref="S2.F2.5.m2.1.1">direct-sum</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.5.m2.1d">\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}%
\pgfsys@color@rgb@stroke{1}{0}{0}\pgfsys@color@rgb@fill{1}{0}{0}\oplus</annotation><annotation encoding="application/x-llamapun" id="S2.F2.5.m2.1e">⊕</annotation></semantics></math>: Residual connection.
The dashed red lines represent temporal connections.
All modules that share a color also share weights.
At each time step, object embeddings are generated using a CNN backbone and transformer-based encoder-decoder modules.
The image features from the backbone are augmented with positional encoding.
The object embeddings are processed in parallel using FFPNs to generate class probability, bounding box, and 6D pose parameters. At time step <math alttext="t_{T}" class="ltx_Math" display="inline" id="S2.F2.6.m3.1"><semantics id="S2.F2.6.m3.1b"><msub id="S2.F2.6.m3.1.1" xref="S2.F2.6.m3.1.1.cmml"><mi id="S2.F2.6.m3.1.1.2" xref="S2.F2.6.m3.1.1.2.cmml">t</mi><mi id="S2.F2.6.m3.1.1.3" xref="S2.F2.6.m3.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F2.6.m3.1c"><apply id="S2.F2.6.m3.1.1.cmml" xref="S2.F2.6.m3.1.1"><csymbol cd="ambiguous" id="S2.F2.6.m3.1.1.1.cmml" xref="S2.F2.6.m3.1.1">subscript</csymbol><ci id="S2.F2.6.m3.1.1.2.cmml" xref="S2.F2.6.m3.1.1.2">𝑡</ci><ci id="S2.F2.6.m3.1.1.3.cmml" xref="S2.F2.6.m3.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.6.m3.1d">t_{T}</annotation><annotation encoding="application/x-llamapun" id="S2.F2.6.m3.1e">italic_t start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT</annotation></semantics></math>, the object embeddings of different time steps are fused using TEFM.
Similarly, object-specific predictions like the keypoints and the 6D pose parameters of different time steps are fused using TOFM.
While fusing object embeddings and object-specific outputs from different time steps, Relative Frame Encoding (RFE) is added element-wise to uniquely identify the respective time step.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Method</span>
</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Multi-Object Pose Estimation as Set Prediction</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.2">Following YOLOPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx7" title="">7</a>]</cite>, we formulate multi-object pose estimation as a set prediction problem.
YOLOPose exploits the permutation-invariant nature of the attention mechanism to generate a set of tuples—each consisting of class probabilities,
2D bounding box, 3D bounding box, position and orientation parameters.
The 3D bounding box parameters are represented using the interpolated bounding box (IBB) keypoints <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx45" title="">45</a>]</cite>.
YOLOPose employs a ResNet backbone for feature extraction (CNN). Positional encoding compensates for the loss of spatial information in the permutation-invariant attention computation. Combined image features and positional encodings are provided to the encoder module, which uses the multi-head self-attention mechanism to generate encoder feature embeddings. In the decoder, the cross-attention mechanism is employed between the encoder feature embeddings and a set of <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">italic_N</annotation></semantics></math> learned embeddings called object queries to generate <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">italic_N</annotation></semantics></math> object embeddings, which are then processed by feed-forward prediction networks (FFPNs) to generate class probabilities, 2D bounding box, and IBB keypoints in parallel.
The IBB keypoints are then processed by a subsequent FFPN to estimate the translation and rotation parameters.
Since the cardinality of the predicted set is fixed, the model is trained to predict <span class="ltx_text" id="S3.SS1.p1.2.1">Ø</span> classes after detecting all the target objects present in the image.
By associating predictions and ground truth objects using a bipartite matching algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx46" title="">46</a>]</cite>, YOLOPose is trained end-to-end.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">MOTPose Architecture</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The architecture of the proposed MOTPose model is shown in <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S2.F2" title="Figure 2 ‣ II-5 Tracking-by-Attention in DETR-Like Models ‣ II Related Work ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>.
We base the single-frame processing of MOTPose on the YOLOPose model.
The transformer-based encoder-decoder modules generate object embeddings of cardinality <math alttext="N" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_N</annotation></semantics></math> from CNN-computed
image features that are augmented with positional encoding.
FFPNs process the object embeddings to generate object-specific outputs.
The object embeddings and the object-specific outputs from the past time steps provide rich temporal information that
can be leveraged while processing the current frame.
To this end, we fuse object embeddings and object-specific outputs from multiple past time steps using the Temporal Embedding Fusion Module (TEFM, Sec. <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S3.SS2.SSS1" title="III-B1 Temporal Embedding Fusion Module (TEFM) ‣ III-B MOTPose Architecture ‣ III Method ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span>1</span></a>) and the Temporal Object Fusion Module (TOFM, Sec. <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S3.SS2.SSS2" title="III-B2 Temporal Object Fusion Module (TOFM) ‣ III-B MOTPose Architecture ‣ III Method ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span>2</span></a>), respectively, before generating outputs for the current time step.
To enable the fusion of embeddings and object parameters over multiple time steps using the permutation-invariant attention mechanism, we utilize <em class="ltx_emph ltx_font_italic" id="S3.SS2.p1.1.1">relative frame encoding</em> (RFE), which encodes the number of time steps relative to the current frame using 1D sinusoidal functions.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS2.SSS1.5.1.1">III-B</span>1 </span>Temporal Embedding Fusion Module (TEFM)</h4>
<figure class="ltx_figure" id="S3.F3"><span class="ltx_ERROR ltx_centering undefined" id="S3.F3.9">\set@curr@file</span>
<p class="ltx_p ltx_align_center" id="S3.F3.10">figures/model/TFFM_image.png<span class="ltx_ERROR undefined" id="S3.F3.10.1">\Gin@getbase</span><span class="ltx_ERROR undefined" id="S3.F3.10.2">\Gin@sepdefault</span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Temporal Embedding Fusion Module (TEFM).
<math alttext="\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}%
\pgfsys@color@rgb@stroke{1}{0}{0}\pgfsys@color@rgb@fill{1}{0}{0}\oplus" class="ltx_Math" display="inline" id="S3.F3.5.m1.1"><semantics id="S3.F3.5.m1.1b"><mo id="S3.F3.5.m1.1.1" mathcolor="#FF0000" xref="S3.F3.5.m1.1.1.cmml">⊕</mo><annotation-xml encoding="MathML-Content" id="S3.F3.5.m1.1c"><csymbol cd="latexml" id="S3.F3.5.m1.1.1.cmml" xref="S3.F3.5.m1.1.1">direct-sum</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.5.m1.1d">\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}%
\pgfsys@color@rgb@stroke{1}{0}{0}\pgfsys@color@rgb@fill{1}{0}{0}\oplus</annotation><annotation encoding="application/x-llamapun" id="S3.F3.5.m1.1e">⊕</annotation></semantics></math>: Concatenation operation.
The object embeddings at each time step of shape N<math alttext="{\times}" class="ltx_Math" display="inline" id="S3.F3.6.m2.1"><semantics id="S3.F3.6.m2.1b"><mo id="S3.F3.6.m2.1.1" xref="S3.F3.6.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.F3.6.m2.1c"><times id="S3.F3.6.m2.1.1.cmml" xref="S3.F3.6.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.6.m2.1d">{\times}</annotation><annotation encoding="application/x-llamapun" id="S3.F3.6.m2.1e">×</annotation></semantics></math>256 are added element-wise with relative frame encoding (RFE).
The resulting vectors for time steps <math alttext="t_{0}-t_{T{-}1}" class="ltx_Math" display="inline" id="S3.F3.7.m3.1"><semantics id="S3.F3.7.m3.1b"><mrow id="S3.F3.7.m3.1.1" xref="S3.F3.7.m3.1.1.cmml"><msub id="S3.F3.7.m3.1.1.2" xref="S3.F3.7.m3.1.1.2.cmml"><mi id="S3.F3.7.m3.1.1.2.2" xref="S3.F3.7.m3.1.1.2.2.cmml">t</mi><mn id="S3.F3.7.m3.1.1.2.3" xref="S3.F3.7.m3.1.1.2.3.cmml">0</mn></msub><mo id="S3.F3.7.m3.1.1.1" xref="S3.F3.7.m3.1.1.1.cmml">−</mo><msub id="S3.F3.7.m3.1.1.3" xref="S3.F3.7.m3.1.1.3.cmml"><mi id="S3.F3.7.m3.1.1.3.2" xref="S3.F3.7.m3.1.1.3.2.cmml">t</mi><mrow id="S3.F3.7.m3.1.1.3.3" xref="S3.F3.7.m3.1.1.3.3.cmml"><mi id="S3.F3.7.m3.1.1.3.3.2" xref="S3.F3.7.m3.1.1.3.3.2.cmml">T</mi><mo id="S3.F3.7.m3.1.1.3.3.1" xref="S3.F3.7.m3.1.1.3.3.1.cmml">−</mo><mn id="S3.F3.7.m3.1.1.3.3.3" xref="S3.F3.7.m3.1.1.3.3.3.cmml">1</mn></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.F3.7.m3.1c"><apply id="S3.F3.7.m3.1.1.cmml" xref="S3.F3.7.m3.1.1"><minus id="S3.F3.7.m3.1.1.1.cmml" xref="S3.F3.7.m3.1.1.1"></minus><apply id="S3.F3.7.m3.1.1.2.cmml" xref="S3.F3.7.m3.1.1.2"><csymbol cd="ambiguous" id="S3.F3.7.m3.1.1.2.1.cmml" xref="S3.F3.7.m3.1.1.2">subscript</csymbol><ci id="S3.F3.7.m3.1.1.2.2.cmml" xref="S3.F3.7.m3.1.1.2.2">𝑡</ci><cn id="S3.F3.7.m3.1.1.2.3.cmml" type="integer" xref="S3.F3.7.m3.1.1.2.3">0</cn></apply><apply id="S3.F3.7.m3.1.1.3.cmml" xref="S3.F3.7.m3.1.1.3"><csymbol cd="ambiguous" id="S3.F3.7.m3.1.1.3.1.cmml" xref="S3.F3.7.m3.1.1.3">subscript</csymbol><ci id="S3.F3.7.m3.1.1.3.2.cmml" xref="S3.F3.7.m3.1.1.3.2">𝑡</ci><apply id="S3.F3.7.m3.1.1.3.3.cmml" xref="S3.F3.7.m3.1.1.3.3"><minus id="S3.F3.7.m3.1.1.3.3.1.cmml" xref="S3.F3.7.m3.1.1.3.3.1"></minus><ci id="S3.F3.7.m3.1.1.3.3.2.cmml" xref="S3.F3.7.m3.1.1.3.3.2">𝑇</ci><cn id="S3.F3.7.m3.1.1.3.3.3.cmml" type="integer" xref="S3.F3.7.m3.1.1.3.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.7.m3.1d">t_{0}-t_{T{-}1}</annotation><annotation encoding="application/x-llamapun" id="S3.F3.7.m3.1e">italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT - italic_t start_POSTSUBSCRIPT italic_T - 1 end_POSTSUBSCRIPT</annotation></semantics></math> are stacked to form <span class="ltx_text ltx_font_italic" id="S3.F3.14.1">key</span> as well as <span class="ltx_text ltx_font_italic" id="S3.F3.15.2">value</span> for the cross-attention operation in TEFM,
whereas the embedding at time step <math alttext="T" class="ltx_Math" display="inline" id="S3.F3.8.m4.1"><semantics id="S3.F3.8.m4.1b"><mi id="S3.F3.8.m4.1.1" xref="S3.F3.8.m4.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.F3.8.m4.1c"><ci id="S3.F3.8.m4.1.1.cmml" xref="S3.F3.8.m4.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.8.m4.1d">T</annotation><annotation encoding="application/x-llamapun" id="S3.F3.8.m4.1e">italic_T</annotation></semantics></math> acts as <span class="ltx_text ltx_font_italic" id="S3.F3.16.3">query</span>.

</figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.6">At each time step, the decoder generates object embeddings of shape <math alttext="N\!\!\times\!256" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.1.m1.1"><semantics id="S3.SS2.SSS1.p1.1.m1.1a"><mrow id="S3.SS2.SSS1.p1.1.m1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.cmml"><mpadded width="0.804em"><mi id="S3.SS2.SSS1.p1.1.m1.1.1.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.cmml">N</mi></mpadded><mo id="S3.SS2.SSS1.p1.1.m1.1.1.1" rspace="0.052em" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS2.SSS1.p1.1.m1.1.1.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.1.m1.1b"><apply id="S3.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1"><times id="S3.SS2.SSS1.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.1"></times><ci id="S3.SS2.SSS1.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.2">𝑁</ci><cn id="S3.SS2.SSS1.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS2.SSS1.p1.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.1.m1.1c">N\!\!\times\!256</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.1.m1.1d">italic_N × 256</annotation></semantics></math>, where <math alttext="N" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.2.m2.1"><semantics id="S3.SS2.SSS1.p1.2.m2.1a"><mi id="S3.SS2.SSS1.p1.2.m2.1.1" xref="S3.SS2.SSS1.p1.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.2.m2.1b"><ci id="S3.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.2.m2.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.2.m2.1d">italic_N</annotation></semantics></math> is the cardinality of the object set to be predicted.
TEFM, shown in <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S3.F3" title="Figure 3 ‣ III-B1 Temporal Embedding Fusion Module (TEFM) ‣ III-B MOTPose Architecture ‣ III Method ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>, fuses object embeddings from multiple time steps to extract valuable temporal information.
First, relative frame encoding is concatenated with the object embeddings, and then the resulting embeddings are projected back to 256 dimensions using linear layers.
The stacked embeddings until <math alttext="T{-}1" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.3.m3.1"><semantics id="S3.SS2.SSS1.p1.3.m3.1a"><mrow id="S3.SS2.SSS1.p1.3.m3.1.1" xref="S3.SS2.SSS1.p1.3.m3.1.1.cmml"><mi id="S3.SS2.SSS1.p1.3.m3.1.1.2" xref="S3.SS2.SSS1.p1.3.m3.1.1.2.cmml">T</mi><mo id="S3.SS2.SSS1.p1.3.m3.1.1.1" xref="S3.SS2.SSS1.p1.3.m3.1.1.1.cmml">−</mo><mn id="S3.SS2.SSS1.p1.3.m3.1.1.3" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.3.m3.1b"><apply id="S3.SS2.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1"><minus id="S3.SS2.SSS1.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.1"></minus><ci id="S3.SS2.SSS1.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.2">𝑇</ci><cn id="S3.SS2.SSS1.p1.3.m3.1.1.3.cmml" type="integer" xref="S3.SS2.SSS1.p1.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.3.m3.1c">T{-}1</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.3.m3.1d">italic_T - 1</annotation></semantics></math> time steps form <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p1.6.1">key</span> and <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p1.6.2">value</span> for the cross-attention operation in TEFM, whereas the embedding from the time step <math alttext="T" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.4.m4.1"><semantics id="S3.SS2.SSS1.p1.4.m4.1a"><mi id="S3.SS2.SSS1.p1.4.m4.1.1" xref="S3.SS2.SSS1.p1.4.m4.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.4.m4.1b"><ci id="S3.SS2.SSS1.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.4.m4.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.4.m4.1d">italic_T</annotation></semantics></math>
is used as <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p1.6.3">query</span>.
This allows the object embeddings from time step <math alttext="T" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.5.m5.1"><semantics id="S3.SS2.SSS1.p1.5.m5.1a"><mi id="S3.SS2.SSS1.p1.5.m5.1.1" xref="S3.SS2.SSS1.p1.5.m5.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.5.m5.1b"><ci id="S3.SS2.SSS1.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS1.p1.5.m5.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.5.m5.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.5.m5.1d">italic_T</annotation></semantics></math> to interact with object embeddings from all previous time steps.
The key-query similarity is reflected in the resulting attention weights.
These attention weights are used to weigh the <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p1.6.4">value</span> vectors, which in our case are the object embeddings from all previous time steps.
After applying layer normalization, the output of TEFM is added element-wise to the object embeddings of time step <math alttext="T" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.6.m6.1"><semantics id="S3.SS2.SSS1.p1.6.m6.1a"><mi id="S3.SS2.SSS1.p1.6.m6.1.1" xref="S3.SS2.SSS1.p1.6.m6.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.6.m6.1b"><ci id="S3.SS2.SSS1.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.6.m6.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.6.m6.1d">italic_T</annotation></semantics></math>, representing a residual connection.
</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS2.SSS2.5.1.1">III-B</span>2 </span>Temporal Object Fusion Module (TOFM)</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.5">In addition to fusing embeddings using TEFM, we employ two TOFM modules to fuse object-specific outputs.
The design of TOFM is similar to that of TEFM, except for the usage of additional linear projection layers at the beginning and the end. The object embeddings are of shape
<math alttext="N\!\!\times\!256" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p1.1.m1.1"><semantics id="S3.SS2.SSS2.p1.1.m1.1a"><mrow id="S3.SS2.SSS2.p1.1.m1.1.1" xref="S3.SS2.SSS2.p1.1.m1.1.1.cmml"><mpadded width="0.804em"><mi id="S3.SS2.SSS2.p1.1.m1.1.1.2" xref="S3.SS2.SSS2.p1.1.m1.1.1.2.cmml">N</mi></mpadded><mo id="S3.SS2.SSS2.p1.1.m1.1.1.1" rspace="0.052em" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS2.SSS2.p1.1.m1.1.1.3" xref="S3.SS2.SSS2.p1.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.1.m1.1b"><apply id="S3.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1"><times id="S3.SS2.SSS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.1"></times><ci id="S3.SS2.SSS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.2">𝑁</ci><cn id="S3.SS2.SSS2.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS2.SSS2.p1.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.1.m1.1c">N\!\!\times\!256</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p1.1.m1.1d">italic_N × 256</annotation></semantics></math>, whereas the shape of the predictions is <math alttext="N{\times}P" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p1.2.m2.1"><semantics id="S3.SS2.SSS2.p1.2.m2.1a"><mrow id="S3.SS2.SSS2.p1.2.m2.1.1" xref="S3.SS2.SSS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.SSS2.p1.2.m2.1.1.2" xref="S3.SS2.SSS2.p1.2.m2.1.1.2.cmml">N</mi><mo id="S3.SS2.SSS2.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.SSS2.p1.2.m2.1.1.1.cmml">×</mo><mi id="S3.SS2.SSS2.p1.2.m2.1.1.3" xref="S3.SS2.SSS2.p1.2.m2.1.1.3.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.2.m2.1b"><apply id="S3.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1"><times id="S3.SS2.SSS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.1"></times><ci id="S3.SS2.SSS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.2">𝑁</ci><ci id="S3.SS2.SSS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.2.m2.1c">N{\times}P</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p1.2.m2.1d">italic_N × italic_P</annotation></semantics></math>, which depends on the prediction generated;
three in the case of translation prediction, six in the case of rotation prediction, and 32 in the case of keypoints.
We use a linear layer to project the predictions to a 256-dimensional vector and supplement them with RFEs.
After computing cross-attention, we project the resulting embeddings back to <math alttext="N{\times}P" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p1.3.m3.1"><semantics id="S3.SS2.SSS2.p1.3.m3.1a"><mrow id="S3.SS2.SSS2.p1.3.m3.1.1" xref="S3.SS2.SSS2.p1.3.m3.1.1.cmml"><mi id="S3.SS2.SSS2.p1.3.m3.1.1.2" xref="S3.SS2.SSS2.p1.3.m3.1.1.2.cmml">N</mi><mo id="S3.SS2.SSS2.p1.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.SSS2.p1.3.m3.1.1.1.cmml">×</mo><mi id="S3.SS2.SSS2.p1.3.m3.1.1.3" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.3.m3.1b"><apply id="S3.SS2.SSS2.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1"><times id="S3.SS2.SSS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1.1"></times><ci id="S3.SS2.SSS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1.2">𝑁</ci><ci id="S3.SS2.SSS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.3.m3.1c">N{\times}P</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p1.3.m3.1d">italic_N × italic_P</annotation></semantics></math>.
TOFM<math alttext="{}_{1}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p1.4.m4.1"><semantics id="S3.SS2.SSS2.p1.4.m4.1a"><msub id="S3.SS2.SSS2.p1.4.m4.1.1" xref="S3.SS2.SSS2.p1.4.m4.1.1.cmml"><mi id="S3.SS2.SSS2.p1.4.m4.1.1a" xref="S3.SS2.SSS2.p1.4.m4.1.1.cmml"></mi><mn id="S3.SS2.SSS2.p1.4.m4.1.1.1" xref="S3.SS2.SSS2.p1.4.m4.1.1.1.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.4.m4.1b"><apply id="S3.SS2.SSS2.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS2.p1.4.m4.1.1"><cn id="S3.SS2.SSS2.p1.4.m4.1.1.1.cmml" type="integer" xref="S3.SS2.SSS2.p1.4.m4.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.4.m4.1c">{}_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p1.4.m4.1d">start_FLOATSUBSCRIPT 1 end_FLOATSUBSCRIPT</annotation></semantics></math> is used for fusing keypoints and TOFM<math alttext="{}_{2}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p1.5.m5.1"><semantics id="S3.SS2.SSS2.p1.5.m5.1a"><msub id="S3.SS2.SSS2.p1.5.m5.1.1" xref="S3.SS2.SSS2.p1.5.m5.1.1.cmml"><mi id="S3.SS2.SSS2.p1.5.m5.1.1a" xref="S3.SS2.SSS2.p1.5.m5.1.1.cmml"></mi><mn id="S3.SS2.SSS2.p1.5.m5.1.1.1" xref="S3.SS2.SSS2.p1.5.m5.1.1.1.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.5.m5.1b"><apply id="S3.SS2.SSS2.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS2.p1.5.m5.1.1"><cn id="S3.SS2.SSS2.p1.5.m5.1.1.1.cmml" type="integer" xref="S3.SS2.SSS2.p1.5.m5.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.5.m5.1c">{}_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p1.5.m5.1d">start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT</annotation></semantics></math> is used for fusing pose parameters.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.5.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.6.2">Matching</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">We use the bipartite matching algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx46" title="">46</a>]</cite> to associate predicted and ground-truth objects.
Despite jointly estimating 2D bounding box, class probabilities, key points, and pose parameters, similar to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx47" title="">47</a>]</cite>,
we use only the bounding box and the class probability components in the matching cost function. This is based on the empirical observation that a combination of the bounding box and the class probability components alone is enough to ensure an optimal match between the ground-truth and the predicted sets.
</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS4.5.1.1">III-D</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS4.6.2">Loss Function</span>
</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">The <span class="ltx_text ltx_font_italic" id="S3.SS4.p1.1.1">Hungarian loss</span> used to train MOTPose is a weighted combination of five components:</p>
</div>
<section class="ltx_subsubsection" id="S3.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS4.SSS1.5.1.1">III-D</span>1 </span>Class Probability Loss</h4>
<div class="ltx_para" id="S3.SS4.SSS1.p1">
<p class="ltx_p" id="S3.SS4.SSS1.p1.1">We use the standard negative log-likelihood (NLL) loss to train the classification branch of the model.
To deal with the class imbalance due to the <span class="ltx_text" id="S3.SS4.SSS1.p1.1.1">Ø</span> class appearing disproportionately often, we weigh it down by a factor of 0.1.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS4.SSS2.5.1.1">III-D</span>2 </span>Bounding Box Loss</h4>
<div class="ltx_para" id="S3.SS4.SSS2.p1">
<p class="ltx_p" id="S3.SS4.SSS2.p1.1">To train the bound box prediction branch of our model, we employ a linear combination of the generalized <span class="ltx_text" id="S3.SS4.SSS2.p1.1.1">IOU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx48" title="">48</a>]</cite></span> and the <math alttext="\ell_{1}" class="ltx_Math" display="inline" id="S3.SS4.SSS2.p1.1.m1.1"><semantics id="S3.SS4.SSS2.p1.1.m1.1a"><msub id="S3.SS4.SSS2.p1.1.m1.1.1" xref="S3.SS4.SSS2.p1.1.m1.1.1.cmml"><mi id="S3.SS4.SSS2.p1.1.m1.1.1.2" mathvariant="normal" xref="S3.SS4.SSS2.p1.1.m1.1.1.2.cmml">ℓ</mi><mn id="S3.SS4.SSS2.p1.1.m1.1.1.3" xref="S3.SS4.SSS2.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p1.1.m1.1b"><apply id="S3.SS4.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS4.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS2.p1.1.m1.1.1.1.cmml" xref="S3.SS4.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.SSS2.p1.1.m1.1.1.2.cmml" xref="S3.SS4.SSS2.p1.1.m1.1.1.2">ℓ</ci><cn id="S3.SS4.SSS2.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS4.SSS2.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p1.1.m1.1c">\ell_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS2.p1.1.m1.1d">roman_ℓ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>-loss.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS4.SSS3.5.1.1">III-D</span>3 </span>Keypoint Loss</h4>
<div class="ltx_para" id="S3.SS4.SSS3.p1">
<p class="ltx_p" id="S3.SS4.SSS3.p1.1">We use a weighted combination of the <math alttext="\ell_{1}" class="ltx_Math" display="inline" id="S3.SS4.SSS3.p1.1.m1.1"><semantics id="S3.SS4.SSS3.p1.1.m1.1a"><msub id="S3.SS4.SSS3.p1.1.m1.1.1" xref="S3.SS4.SSS3.p1.1.m1.1.1.cmml"><mi id="S3.SS4.SSS3.p1.1.m1.1.1.2" mathvariant="normal" xref="S3.SS4.SSS3.p1.1.m1.1.1.2.cmml">ℓ</mi><mn id="S3.SS4.SSS3.p1.1.m1.1.1.3" xref="S3.SS4.SSS3.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS3.p1.1.m1.1b"><apply id="S3.SS4.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS4.SSS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS3.p1.1.m1.1.1.1.cmml" xref="S3.SS4.SSS3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.SSS3.p1.1.m1.1.1.2.cmml" xref="S3.SS4.SSS3.p1.1.m1.1.1.2">ℓ</ci><cn id="S3.SS4.SSS3.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS4.SSS3.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS3.p1.1.m1.1c">\ell_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS3.p1.1.m1.1d">roman_ℓ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>-loss and the cross-ratio consistency loss <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx7" title="">7</a>]</cite> to train the keypoint estimation branch.
</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS4.SSS4.5.1.1">III-D</span>4 </span>Pose Loss</h4>
<div class="ltx_para" id="S3.SS4.SSS4.p1">
<p class="ltx_p" id="S3.SS4.SSS4.p1.1">We decouple the pose loss into a translation and a rotation component. For translation, we employ the <math alttext="\ell_{2}" class="ltx_Math" display="inline" id="S3.SS4.SSS4.p1.1.m1.1"><semantics id="S3.SS4.SSS4.p1.1.m1.1a"><msub id="S3.SS4.SSS4.p1.1.m1.1.1" xref="S3.SS4.SSS4.p1.1.m1.1.1.cmml"><mi id="S3.SS4.SSS4.p1.1.m1.1.1.2" mathvariant="normal" xref="S3.SS4.SSS4.p1.1.m1.1.1.2.cmml">ℓ</mi><mn id="S3.SS4.SSS4.p1.1.m1.1.1.3" xref="S3.SS4.SSS4.p1.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS4.p1.1.m1.1b"><apply id="S3.SS4.SSS4.p1.1.m1.1.1.cmml" xref="S3.SS4.SSS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS4.p1.1.m1.1.1.1.cmml" xref="S3.SS4.SSS4.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.SSS4.p1.1.m1.1.1.2.cmml" xref="S3.SS4.SSS4.p1.1.m1.1.1.2">ℓ</ci><cn id="S3.SS4.SSS4.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS4.SSS4.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS4.p1.1.m1.1c">\ell_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS4.p1.1.m1.1d">roman_ℓ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>-loss.
For rotation, we use the symmetry-aware ShapeMatch-loss proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx14" title="">14</a>,  ]</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS4.SSS5.5.1.1">III-D</span>5 </span>Temporal Consistency Loss</h4>
<div class="ltx_para" id="S3.SS4.SSS5.p1">
<p class="ltx_p" id="S3.SS4.SSS5.p1.2">We enforce temporal consistency using the <math alttext="\ell_{2}" class="ltx_Math" display="inline" id="S3.SS4.SSS5.p1.1.m1.1"><semantics id="S3.SS4.SSS5.p1.1.m1.1a"><msub id="S3.SS4.SSS5.p1.1.m1.1.1" xref="S3.SS4.SSS5.p1.1.m1.1.1.cmml"><mi id="S3.SS4.SSS5.p1.1.m1.1.1.2" mathvariant="normal" xref="S3.SS4.SSS5.p1.1.m1.1.1.2.cmml">ℓ</mi><mn id="S3.SS4.SSS5.p1.1.m1.1.1.3" xref="S3.SS4.SSS5.p1.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS5.p1.1.m1.1b"><apply id="S3.SS4.SSS5.p1.1.m1.1.1.cmml" xref="S3.SS4.SSS5.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS5.p1.1.m1.1.1.1.cmml" xref="S3.SS4.SSS5.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.SSS5.p1.1.m1.1.1.2.cmml" xref="S3.SS4.SSS5.p1.1.m1.1.1.2">ℓ</ci><cn id="S3.SS4.SSS5.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS4.SSS5.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS5.p1.1.m1.1c">\ell_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS5.p1.1.m1.1d">roman_ℓ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>-loss between the object embeddings of consecutive time steps.
Embeddings evolve smoothly over frames and any big changes are undesirable.
Thus, the <math alttext="\ell_{2}" class="ltx_Math" display="inline" id="S3.SS4.SSS5.p1.2.m2.1"><semantics id="S3.SS4.SSS5.p1.2.m2.1a"><msub id="S3.SS4.SSS5.p1.2.m2.1.1" xref="S3.SS4.SSS5.p1.2.m2.1.1.cmml"><mi id="S3.SS4.SSS5.p1.2.m2.1.1.2" mathvariant="normal" xref="S3.SS4.SSS5.p1.2.m2.1.1.2.cmml">ℓ</mi><mn id="S3.SS4.SSS5.p1.2.m2.1.1.3" xref="S3.SS4.SSS5.p1.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS5.p1.2.m2.1b"><apply id="S3.SS4.SSS5.p1.2.m2.1.1.cmml" xref="S3.SS4.SSS5.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS5.p1.2.m2.1.1.1.cmml" xref="S3.SS4.SSS5.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.SSS5.p1.2.m2.1.1.2.cmml" xref="S3.SS4.SSS5.p1.2.m2.1.1.2">ℓ</ci><cn id="S3.SS4.SSS5.p1.2.m2.1.1.3.cmml" type="integer" xref="S3.SS4.SSS5.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS5.p1.2.m2.1c">\ell_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS5.p1.2.m2.1d">roman_ℓ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>-loss, which penalizes bigger differences significantly more than smaller differences, is a natural choice.
</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Evaluation</span>
</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.5.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.6.2">Datasets</span>
</h3>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS1.5.1.1">IV-A</span>1 </span>YCB-Video</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">We use the challenging YCB-Video dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx14" title="">14</a>]</cite> to benchmark the performance of our model against other state-of-the-art methods.
The dataset consists of 92 (80 training and 12 testing) moving-camera video sequences of static scenes with multiple objects.
High-resolution 3D models of all 21 objects are provided with the dataset.
Following <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx23" title="">23</a>,  ]</cite>, we use all the frames in the test split for evaluation.
Additionally, we utilize the synthetic dataset provided by Xiang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx14" title="">14</a>]</cite> to train our model.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS2.5.1.1">IV-A</span>2 </span>SynPick</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">SynPick <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx49" title="">49</a>]</cite> is a physically-realistic synthetic dataset of dynamic bin-picking scenes that contain a chaotic pile of the same 21 YCB-Video objects in a tote.
It consists of simulations of three different bin-picking actions: move, targeted pick, and untargeted pick.
For each action, SynPick provides 300 video sequences: 240 for training and 60 for testing.
Moreover, the dataset generator is publicly available<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/AIS-Bonn/synpick" title="">https://github.com/AIS-Bonn/synpick</a></span></span></span> making it easy to generate additional data, if needed.
In contrast to the commonly used object pose estimation datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx50" title="">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx1" title="">1</a>]</cite>, which consist of static tabletop scenes with a relatively low degree of occlusion, SynPick is highly cluttered and the gripper movements generate complex object interactions.
Moreover, the objects in the SynPick dataset appear in a wide range of pose configurations and multiple instances of the same object are present in the scenes.
Thus, SynPick is an ideal dataset for evaluating the proposed MOTPose model.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.5.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.6.2">Metrics</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We report the area under the curve (AUC) of the ADD and ADD-S metrics at an accuracy threshold of 0.1m for non-symmetric and symmetric objects, respectively <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx14" title="">14</a>]</cite>.
The ADD metric is the average <math alttext="\ell_{2}" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><msub id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mi id="S4.SS2.p1.1.m1.1.1.2" mathvariant="normal" xref="S4.SS2.p1.1.m1.1.1.2.cmml">ℓ</mi><mn id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">ℓ</ci><cn id="S4.SS2.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.SS2.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">\ell_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">roman_ℓ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> distance between the subsampled mesh points in the ground truth and the predicted pose,
whereas the symmetry-aware ADD-S metric is the average distance between the closest subsampled mesh points in the ground truth and the predicted pose.
The <span class="ltx_text" id="S4.SS2.p1.1.1">ADD(-S)</span> metric combines both ADD and ADD-S into one metric by utilizing ADD for objects without symmetry and ADD-S for objects exhibiting symmetry.
</p>
</div>
<figure class="ltx_figure" id="S4.F4">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.F4.1">
<tr class="ltx_tr" id="S4.F4.1.1">
<td class="ltx_td ltx_align_center" id="S4.F4.1.1.1" style="padding-left:0.3pt;padding-right:0.3pt;">
<span class="ltx_ERROR undefined" id="S4.F4.1.1.1.1">\set@curr@file</span><span class="ltx_text" id="S4.F4.1.1.1.2" style="font-size:80%;">figures/synpick_results/_0_move-000026_000175.jpg</span><span class="ltx_ERROR undefined" id="S4.F4.1.1.1.3">\Gin@getbase</span><span class="ltx_ERROR undefined" id="S4.F4.1.1.1.4">\Gin@sepdefault</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.F4.1.1.2" style="padding-left:0.3pt;padding-right:0.3pt;">
<span class="ltx_ERROR undefined" id="S4.F4.1.1.2.1">\set@curr@file</span><span class="ltx_text" id="S4.F4.1.1.2.2" style="font-size:80%;">figures/synpick_results/_0_move-000045_000081.jpg</span><span class="ltx_ERROR undefined" id="S4.F4.1.1.2.3">\Gin@getbase</span><span class="ltx_ERROR undefined" id="S4.F4.1.1.2.4">\Gin@sepdefault</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.F4.1.1.3" style="padding-left:0.3pt;padding-right:0.3pt;">
<span class="ltx_ERROR undefined" id="S4.F4.1.1.3.1">\set@curr@file</span><span class="ltx_text" id="S4.F4.1.1.3.2" style="font-size:80%;">figures/synpick_results/_0_move-000057_000074.jpg</span><span class="ltx_ERROR undefined" id="S4.F4.1.1.3.3">\Gin@getbase</span><span class="ltx_ERROR undefined" id="S4.F4.1.1.3.4">\Gin@sepdefault</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.F4.1.1.4" style="padding-left:0.3pt;padding-right:0.3pt;">
<span class="ltx_ERROR undefined" id="S4.F4.1.1.4.1">\set@curr@file</span><span class="ltx_text" id="S4.F4.1.1.4.2" style="font-size:80%;">figures/synpick_results/_0_pick_untargeted-000000_000019.jpg</span><span class="ltx_ERROR undefined" id="S4.F4.1.1.4.3">\Gin@getbase</span><span class="ltx_ERROR undefined" id="S4.F4.1.1.4.4">\Gin@sepdefault</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.F4.1.1.5" style="padding-left:0.3pt;padding-right:0.3pt;">
<span class="ltx_ERROR undefined" id="S4.F4.1.1.5.1">\set@curr@file</span><span class="ltx_text" id="S4.F4.1.1.5.2" style="font-size:80%;">figures/synpick_results/_0_pick_untargeted-000037_000239.jpg</span><span class="ltx_ERROR undefined" id="S4.F4.1.1.5.3">\Gin@getbase</span><span class="ltx_ERROR undefined" id="S4.F4.1.1.5.4">\Gin@sepdefault</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.F4.1.2">
<td class="ltx_td ltx_align_center" id="S4.F4.1.2.1" style="padding-left:0.3pt;padding-right:0.3pt;">
<span class="ltx_ERROR undefined" id="S4.F4.1.2.1.1">\set@curr@file</span><span class="ltx_text" id="S4.F4.1.2.1.2" style="font-size:80%;">figures/synpick_results/1_move-000026_000175.jpg</span><span class="ltx_ERROR undefined" id="S4.F4.1.2.1.3">\Gin@getbase</span><span class="ltx_ERROR undefined" id="S4.F4.1.2.1.4">\Gin@sepdefault</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.F4.1.2.2" style="padding-left:0.3pt;padding-right:0.3pt;">
<span class="ltx_ERROR undefined" id="S4.F4.1.2.2.1">\set@curr@file</span><span class="ltx_text" id="S4.F4.1.2.2.2" style="font-size:80%;">figures/synpick_results/1_move-000045_000081.jpg</span><span class="ltx_ERROR undefined" id="S4.F4.1.2.2.3">\Gin@getbase</span><span class="ltx_ERROR undefined" id="S4.F4.1.2.2.4">\Gin@sepdefault</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.F4.1.2.3" style="padding-left:0.3pt;padding-right:0.3pt;">
<span class="ltx_ERROR undefined" id="S4.F4.1.2.3.1">\set@curr@file</span><span class="ltx_text" id="S4.F4.1.2.3.2" style="font-size:80%;">figures/synpick_results/1_move-000057_000074.jpg</span><span class="ltx_ERROR undefined" id="S4.F4.1.2.3.3">\Gin@getbase</span><span class="ltx_ERROR undefined" id="S4.F4.1.2.3.4">\Gin@sepdefault</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.F4.1.2.4" style="padding-left:0.3pt;padding-right:0.3pt;">
<span class="ltx_ERROR undefined" id="S4.F4.1.2.4.1">\set@curr@file</span><span class="ltx_text" id="S4.F4.1.2.4.2" style="font-size:80%;">figures/synpick_results/1_pick_untargeted-000000_000019.jpg</span><span class="ltx_ERROR undefined" id="S4.F4.1.2.4.3">\Gin@getbase</span><span class="ltx_ERROR undefined" id="S4.F4.1.2.4.4">\Gin@sepdefault</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.F4.1.2.5" style="padding-left:0.3pt;padding-right:0.3pt;">
<span class="ltx_ERROR undefined" id="S4.F4.1.2.5.1">\set@curr@file</span><span class="ltx_text" id="S4.F4.1.2.5.2" style="font-size:80%;">figures/synpick_results/1_pick_untargeted-000037_000239.jpg</span><span class="ltx_ERROR undefined" id="S4.F4.1.2.5.3">\Gin@getbase</span><span class="ltx_ERROR undefined" id="S4.F4.1.2.5.4">\Gin@sepdefault</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.F4.1.3">
<td class="ltx_td ltx_align_center" id="S4.F4.1.3.1" style="padding-left:0.3pt;padding-right:0.3pt;"><span class="ltx_text" id="S4.F4.1.3.1.1" style="font-size:80%;">(a)</span></td>
<td class="ltx_td ltx_align_center" id="S4.F4.1.3.2" style="padding-left:0.3pt;padding-right:0.3pt;"><span class="ltx_text" id="S4.F4.1.3.2.1" style="font-size:80%;">(b)</span></td>
<td class="ltx_td ltx_align_center" id="S4.F4.1.3.3" style="padding-left:0.3pt;padding-right:0.3pt;"><span class="ltx_text" id="S4.F4.1.3.3.1" style="font-size:80%;">(c)</span></td>
<td class="ltx_td ltx_align_center" id="S4.F4.1.3.4" style="padding-left:0.3pt;padding-right:0.3pt;"><span class="ltx_text" id="S4.F4.1.3.4.1" style="font-size:80%;">(d)</span></td>
<td class="ltx_td ltx_align_center" id="S4.F4.1.3.5" style="padding-left:0.3pt;padding-right:0.3pt;"><span class="ltx_text" id="S4.F4.1.3.5.1" style="font-size:80%;">(e)</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Qualitative results on SynPick.
6D pose predictions are visualized by object contours.
Top: Predictions from the model without temporal fusion.
Bottom: Predictions from the model with temporal fusion.
Temporal fusion facilitates better pose prediction as well as object detection accuracies.
The blue circles highlight failed object detections and the yellow circles highlight erroneous pose predictions.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Quantitative results on the SynPick dataset.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T1.6">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.6.7.1">
<td class="ltx_td ltx_border_r ltx_border_tt" id="S4.T1.6.7.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt" colspan="4" id="S4.T1.6.7.1.2" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text" id="S4.T1.6.7.1.2.1"></span><span class="ltx_text" id="S4.T1.6.7.1.2.2" style="font-size:70%;"> </span><span class="ltx_text" id="S4.T1.6.7.1.2.3" style="font-size:70%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.6.7.1.2.3.1">
<span class="ltx_tr" id="S4.T1.6.7.1.2.3.1.1">
<span class="ltx_td ltx_align_center" id="S4.T1.6.7.1.2.3.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.7.1.2.3.1.1.1.1" style="font-size:114%;">MOTPose without</span></span></span>
<span class="ltx_tr" id="S4.T1.6.7.1.2.3.1.2">
<span class="ltx_td ltx_align_center" id="S4.T1.6.7.1.2.3.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.7.1.2.3.1.2.1.1" style="font-size:114%;">Temporal Fusion</span></span></span>
</span></span><span class="ltx_text" id="S4.T1.6.7.1.2.4" style="font-size:70%;"> </span><span class="ltx_text" id="S4.T1.6.7.1.2.5"></span><span class="ltx_text" id="S4.T1.6.7.1.2.6" style="font-size:70%;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S4.T1.6.7.1.3" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text" id="S4.T1.6.7.1.3.1"></span><span class="ltx_text" id="S4.T1.6.7.1.3.2" style="font-size:70%;"> </span><span class="ltx_text" id="S4.T1.6.7.1.3.3" style="font-size:70%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.6.7.1.3.3.1">
<span class="ltx_tr" id="S4.T1.6.7.1.3.3.1.1">
<span class="ltx_td ltx_align_center" id="S4.T1.6.7.1.3.3.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.7.1.3.3.1.1.1.1" style="font-size:114%;">MOTPose with</span></span></span>
<span class="ltx_tr" id="S4.T1.6.7.1.3.3.1.2">
<span class="ltx_td ltx_align_center" id="S4.T1.6.7.1.3.3.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.7.1.3.3.1.2.1.1" style="font-size:114%;">Temporal Fusion</span></span></span>
</span></span><span class="ltx_text" id="S4.T1.6.7.1.3.4" style="font-size:70%;"> </span><span class="ltx_text" id="S4.T1.6.7.1.3.5"></span><span class="ltx_text" id="S4.T1.6.7.1.3.6" style="font-size:70%;"></span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text" id="S4.T1.1.1.1.2"></span><span class="ltx_text" id="S4.T1.1.1.1.3" style="font-size:70%;"> </span><span class="ltx_text" id="S4.T1.1.1.1.1" style="font-size:70%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.1.1.1.1">
<span class="ltx_tr" id="S4.T1.1.1.1.1.1.1.2">
<span class="ltx_td ltx_align_center" id="S4.T1.1.1.1.1.1.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.1.1.1.1.1.1.2.1.1" style="font-size:114%;">Obj.</span></span></span>
<span class="ltx_tr" id="S4.T1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_align_center" id="S4.T1.1.1.1.1.1.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.1.1.1.1.1.1.1.1.1" style="font-size:114%;">ID</span><math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S4.T1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S4.T1.1.1.1.1.1.1.1.1.m1.1a"><msup id="S4.T1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mi id="S4.T1.1.1.1.1.1.1.1.1.m1.1.1a" xref="S4.T1.1.1.1.1.1.1.1.1.m1.1.1.cmml"></mi><mo id="S4.T1.1.1.1.1.1.1.1.1.m1.1.1.1" mathsize="114%" xref="S4.T1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S4.T1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.1.1.1.1.m1.1.1"><ci id="S4.T1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T1.1.1.1.1.1.1.1.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.1.1.1.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.1.1.1.1.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math></span></span>
</span></span><span class="ltx_text" id="S4.T1.1.1.1.4" style="font-size:70%;"> </span><span class="ltx_text" id="S4.T1.1.1.1.5"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T1.1.1.2" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text" id="S4.T1.1.1.2.1"></span><span class="ltx_text" id="S4.T1.1.1.2.2" style="font-size:70%;"> </span><span class="ltx_text" id="S4.T1.1.1.2.3" style="font-size:70%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.2.3.1">
<span class="ltx_tr" id="S4.T1.1.1.2.3.1.1">
<span class="ltx_td ltx_align_center" id="S4.T1.1.1.2.3.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.1.1.2.3.1.1.1.1">AUC of</span></span></span>
<span class="ltx_tr" id="S4.T1.1.1.2.3.1.2">
<span class="ltx_td ltx_align_center" id="S4.T1.1.1.2.3.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.1.1.2.3.1.2.1.1">ADD-S</span></span></span>
</span></span><span class="ltx_text" id="S4.T1.1.1.2.4" style="font-size:70%;"> </span><span class="ltx_text" id="S4.T1.1.1.2.5"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.1.1.3" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text" id="S4.T1.1.1.3.1"></span><span class="ltx_text" id="S4.T1.1.1.3.2" style="font-size:70%;"> </span><span class="ltx_text" id="S4.T1.1.1.3.3" style="font-size:70%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.3.3.1">
<span class="ltx_tr" id="S4.T1.1.1.3.3.1.1">
<span class="ltx_td ltx_align_center" id="S4.T1.1.1.3.3.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.1.1.3.3.1.1.1.1">AUC of</span></span></span>
<span class="ltx_tr" id="S4.T1.1.1.3.3.1.2">
<span class="ltx_td ltx_align_center" id="S4.T1.1.1.3.3.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.1.1.3.3.1.2.1.1">ADD(-S)</span></span></span>
</span></span><span class="ltx_text" id="S4.T1.1.1.3.4" style="font-size:70%;"> </span><span class="ltx_text" id="S4.T1.1.1.3.5"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T1.1.1.4" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text" id="S4.T1.1.1.4.1"></span><span class="ltx_text" id="S4.T1.1.1.4.2" style="font-size:70%;"> </span><span class="ltx_text" id="S4.T1.1.1.4.3" style="font-size:70%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.4.3.1">
<span class="ltx_tr" id="S4.T1.1.1.4.3.1.1">
<span class="ltx_td ltx_align_center" id="S4.T1.1.1.4.3.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.1.1.4.3.1.1.1.1">AUC of</span></span></span>
<span class="ltx_tr" id="S4.T1.1.1.4.3.1.2">
<span class="ltx_td ltx_align_center" id="S4.T1.1.1.4.3.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.1.1.4.3.1.2.1.1">ADD-S</span></span></span>
<span class="ltx_tr" id="S4.T1.1.1.4.3.1.3">
<span class="ltx_td ltx_align_center" id="S4.T1.1.1.4.3.1.3.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.1.1.4.3.1.3.1.1">@0.1d</span></span></span>
</span></span><span class="ltx_text" id="S4.T1.1.1.4.4" style="font-size:70%;"> </span><span class="ltx_text" id="S4.T1.1.1.4.5"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_rr ltx_border_t" id="S4.T1.1.1.5" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text" id="S4.T1.1.1.5.1"></span><span class="ltx_text" id="S4.T1.1.1.5.2" style="font-size:70%;"> </span><span class="ltx_text" id="S4.T1.1.1.5.3" style="font-size:70%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.5.3.1">
<span class="ltx_tr" id="S4.T1.1.1.5.3.1.1">
<span class="ltx_td ltx_align_center" id="S4.T1.1.1.5.3.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.1.1.5.3.1.1.1.1">AUC of</span></span></span>
<span class="ltx_tr" id="S4.T1.1.1.5.3.1.2">
<span class="ltx_td ltx_align_center" id="S4.T1.1.1.5.3.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.1.1.5.3.1.2.1.1">ADD(-S)</span></span></span>
<span class="ltx_tr" id="S4.T1.1.1.5.3.1.3">
<span class="ltx_td ltx_align_center" id="S4.T1.1.1.5.3.1.3.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.1.1.5.3.1.3.1.1">@0.1d</span></span></span>
</span></span><span class="ltx_text" id="S4.T1.1.1.5.4" style="font-size:70%;"> </span><span class="ltx_text" id="S4.T1.1.1.5.5"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T1.1.1.6" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text" id="S4.T1.1.1.6.1"></span><span class="ltx_text" id="S4.T1.1.1.6.2" style="font-size:70%;"> </span><span class="ltx_text" id="S4.T1.1.1.6.3" style="font-size:70%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.6.3.1">
<span class="ltx_tr" id="S4.T1.1.1.6.3.1.1">
<span class="ltx_td ltx_align_center" id="S4.T1.1.1.6.3.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.1.1.6.3.1.1.1.1">AUC of</span></span></span>
<span class="ltx_tr" id="S4.T1.1.1.6.3.1.2">
<span class="ltx_td ltx_align_center" id="S4.T1.1.1.6.3.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.1.1.6.3.1.2.1.1">ADD-S</span></span></span>
</span></span><span class="ltx_text" id="S4.T1.1.1.6.4" style="font-size:70%;"> </span><span class="ltx_text" id="S4.T1.1.1.6.5"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.1.1.7" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text" id="S4.T1.1.1.7.1"></span><span class="ltx_text" id="S4.T1.1.1.7.2" style="font-size:70%;"> </span><span class="ltx_text" id="S4.T1.1.1.7.3" style="font-size:70%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.7.3.1">
<span class="ltx_tr" id="S4.T1.1.1.7.3.1.1">
<span class="ltx_td ltx_align_center" id="S4.T1.1.1.7.3.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.1.1.7.3.1.1.1.1">AUC of</span></span></span>
<span class="ltx_tr" id="S4.T1.1.1.7.3.1.2">
<span class="ltx_td ltx_align_center" id="S4.T1.1.1.7.3.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.1.1.7.3.1.2.1.1">ADD(-S)</span></span></span>
</span></span><span class="ltx_text" id="S4.T1.1.1.7.4" style="font-size:70%;"> </span><span class="ltx_text" id="S4.T1.1.1.7.5"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T1.1.1.8" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text" id="S4.T1.1.1.8.1"></span><span class="ltx_text" id="S4.T1.1.1.8.2" style="font-size:70%;"> </span><span class="ltx_text" id="S4.T1.1.1.8.3" style="font-size:70%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.8.3.1">
<span class="ltx_tr" id="S4.T1.1.1.8.3.1.1">
<span class="ltx_td ltx_align_center" id="S4.T1.1.1.8.3.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.1.1.8.3.1.1.1.1">AUC of</span></span></span>
<span class="ltx_tr" id="S4.T1.1.1.8.3.1.2">
<span class="ltx_td ltx_align_center" id="S4.T1.1.1.8.3.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.1.1.8.3.1.2.1.1">ADD-S</span></span></span>
<span class="ltx_tr" id="S4.T1.1.1.8.3.1.3">
<span class="ltx_td ltx_align_center" id="S4.T1.1.1.8.3.1.3.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.1.1.8.3.1.3.1.1">@0.1d</span></span></span>
</span></span><span class="ltx_text" id="S4.T1.1.1.8.4" style="font-size:70%;"> </span><span class="ltx_text" id="S4.T1.1.1.8.5"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T1.1.1.9" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text" id="S4.T1.1.1.9.1"></span><span class="ltx_text" id="S4.T1.1.1.9.2" style="font-size:70%;"> </span><span class="ltx_text" id="S4.T1.1.1.9.3" style="font-size:70%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.9.3.1">
<span class="ltx_tr" id="S4.T1.1.1.9.3.1.1">
<span class="ltx_td ltx_align_center" id="S4.T1.1.1.9.3.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.1.1.9.3.1.1.1.1">AUC of</span></span></span>
<span class="ltx_tr" id="S4.T1.1.1.9.3.1.2">
<span class="ltx_td ltx_align_center" id="S4.T1.1.1.9.3.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.1.1.9.3.1.2.1.1">ADD(-S)</span></span></span>
<span class="ltx_tr" id="S4.T1.1.1.9.3.1.3">
<span class="ltx_td ltx_align_center" id="S4.T1.1.1.9.3.1.3.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.1.1.9.3.1.3.1.1">@0.1d</span></span></span>
</span></span><span class="ltx_text" id="S4.T1.1.1.9.4" style="font-size:70%;"> </span><span class="ltx_text" id="S4.T1.1.1.9.5"></span>
</th>
</tr>
<tr class="ltx_tr" id="S4.T1.6.8.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.6.8.2.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.8.2.1.1" style="font-size:70%;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.6.8.2.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.8.2.2.1" style="font-size:70%;">88.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.6.8.2.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.8.2.3.1" style="font-size:70%;">72.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.6.8.2.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.8.2.4.1" style="font-size:70%;">86.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="S4.T1.6.8.2.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.8.2.5.1" style="font-size:70%;">53.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.6.8.2.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.8.2.6.1" style="font-size:70%;">88.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.6.8.2.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.8.2.7.1" style="font-size:70%;">79.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.6.8.2.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.8.2.8.1" style="font-size:70%;">86.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.6.8.2.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.8.2.9.1" style="font-size:70%;">61.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.9.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.9.3.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.9.3.1.1" style="font-size:70%;">2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.9.3.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.9.3.2.1" style="font-size:70%;">90.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.9.3.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.9.3.3.1" style="font-size:70%;">82.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.9.3.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.9.3.4.1" style="font-size:70%;">89.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T1.6.9.3.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.9.3.5.1" style="font-size:70%;">76.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.9.3.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.9.3.6.1" style="font-size:70%;">91.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.9.3.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.9.3.7.1" style="font-size:70%;">84.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.9.3.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.9.3.8.1" style="font-size:70%;">90.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.9.3.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.9.3.9.1" style="font-size:70%;">78.4</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.10.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.10.4.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.10.4.1.1" style="font-size:70%;">3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.10.4.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.10.4.2.1" style="font-size:70%;">80.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.10.4.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.10.4.3.1" style="font-size:70%;">74.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.10.4.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.10.4.4.1" style="font-size:70%;">79.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T1.6.10.4.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.10.4.5.1" style="font-size:70%;">63.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.10.4.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.10.4.6.1" style="font-size:70%;">81.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.10.4.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.10.4.7.1" style="font-size:70%;">76.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.10.4.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.10.4.8.1" style="font-size:70%;">80.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.10.4.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.10.4.9.1" style="font-size:70%;">69.5</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.11.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.11.5.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.11.5.1.1" style="font-size:70%;">4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.11.5.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.11.5.2.1" style="font-size:70%;">72.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.11.5.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.11.5.3.1" style="font-size:70%;">64.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.11.5.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.11.5.4.1" style="font-size:70%;">70.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T1.6.11.5.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.11.5.5.1" style="font-size:70%;">43.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.11.5.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.11.5.6.1" style="font-size:70%;">73.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.11.5.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.11.5.7.1" style="font-size:70%;">68.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.11.5.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.11.5.8.1" style="font-size:70%;">71.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.11.5.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.11.5.9.1" style="font-size:70%;">45.1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.12.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.12.6.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.12.6.1.1" style="font-size:70%;">5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.12.6.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.12.6.2.1" style="font-size:70%;">80.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.12.6.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.12.6.3.1" style="font-size:70%;">72.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.12.6.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.12.6.4.1" style="font-size:70%;">78.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T1.6.12.6.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.12.6.5.1" style="font-size:70%;">62.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.12.6.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.12.6.6.1" style="font-size:70%;">80.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.12.6.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.12.6.7.1" style="font-size:70%;">74.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.12.6.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.12.6.8.1" style="font-size:70%;">78.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.12.6.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.12.6.9.1" style="font-size:70%;">67.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.13.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.13.7.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.13.7.1.1" style="font-size:70%;">6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.13.7.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.13.7.2.1" style="font-size:70%;">81.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.13.7.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.13.7.3.1" style="font-size:70%;">64.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.13.7.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.13.7.4.1" style="font-size:70%;">68.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T1.6.13.7.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.13.7.5.1" style="font-size:70%;">19.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.13.7.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.13.7.6.1" style="font-size:70%;">81.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.13.7.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.13.7.7.1" style="font-size:70%;">75.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.13.7.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.13.7.8.1" style="font-size:70%;">72.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.13.7.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.13.7.9.1" style="font-size:70%;">25.6</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.14.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.14.8.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.14.8.1.1" style="font-size:70%;">7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.14.8.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.14.8.2.1" style="font-size:70%;">69.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.14.8.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.14.8.3.1" style="font-size:70%;">63.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.14.8.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.14.8.4.1" style="font-size:70%;">66.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T1.6.14.8.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.14.8.5.1" style="font-size:70%;">48.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.14.8.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.14.8.6.1" style="font-size:70%;">70.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.14.8.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.14.8.7.1" style="font-size:70%;">65.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.14.8.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.14.8.8.1" style="font-size:70%;">68.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.14.8.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.14.8.9.1" style="font-size:70%;">48.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.15.9">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.15.9.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.15.9.1.1" style="font-size:70%;">8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.15.9.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.15.9.2.1" style="font-size:70%;">65.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.15.9.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.15.9.3.1" style="font-size:70%;">60.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.15.9.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.15.9.4.1" style="font-size:70%;">60.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T1.6.15.9.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.15.9.5.1" style="font-size:70%;">40.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.15.9.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.15.9.6.1" style="font-size:70%;">67.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.15.9.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.15.9.7.1" style="font-size:70%;">62.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.15.9.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.15.9.8.1" style="font-size:70%;">63.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.15.9.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.15.9.9.1" style="font-size:70%;">32.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.16.10">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.16.10.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.16.10.1.1" style="font-size:70%;">9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.16.10.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.16.10.2.1" style="font-size:70%;">84.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.16.10.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.16.10.3.1" style="font-size:70%;">76.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.16.10.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.16.10.4.1" style="font-size:70%;">80.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T1.6.16.10.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.16.10.5.1" style="font-size:70%;">56.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.16.10.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.16.10.6.1" style="font-size:70%;">85.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.16.10.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.16.10.7.1" style="font-size:70%;">78.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.16.10.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.16.10.8.1" style="font-size:70%;">82.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.16.10.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.16.10.9.1" style="font-size:70%;">56.5</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.17.11">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.17.11.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.17.11.1.1" style="font-size:70%;">10</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.17.11.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.17.11.2.1" style="font-size:70%;">78.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.17.11.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.17.11.3.1" style="font-size:70%;">70.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.17.11.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.17.11.4.1" style="font-size:70%;">73.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T1.6.17.11.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.17.11.5.1" style="font-size:70%;">56.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.17.11.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.17.11.6.1" style="font-size:70%;">80.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.17.11.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.17.11.7.1" style="font-size:70%;">73.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.17.11.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.17.11.8.1" style="font-size:70%;">77.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.17.11.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.17.11.9.1" style="font-size:70%;">64.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.18.12">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.18.12.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.18.12.1.1" style="font-size:70%;">11</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.18.12.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.18.12.2.1" style="font-size:70%;">92.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.18.12.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.18.12.3.1" style="font-size:70%;">84.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.18.12.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.18.12.4.1" style="font-size:70%;">92.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T1.6.18.12.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.18.12.5.1" style="font-size:70%;">79.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.18.12.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.18.12.6.1" style="font-size:70%;">93.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.18.12.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.18.12.7.1" style="font-size:70%;">85.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.18.12.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.18.12.8.1" style="font-size:70%;">92.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.18.12.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.18.12.9.1" style="font-size:70%;">81.8</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.19.13">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.19.13.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.19.13.1.1" style="font-size:70%;">12</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.19.13.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.19.13.2.1" style="font-size:70%;">85.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.19.13.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.19.13.3.1" style="font-size:70%;">76.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.19.13.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.19.13.4.1" style="font-size:70%;">85.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T1.6.19.13.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.19.13.5.1" style="font-size:70%;">71.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.19.13.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.19.13.6.1" style="font-size:70%;">87.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.19.13.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.19.13.7.1" style="font-size:70%;">80.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.19.13.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.19.13.8.1" style="font-size:70%;">86.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.19.13.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.19.13.9.1" style="font-size:70%;">76.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.2.2.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text" id="S4.T1.2.2.1.1" style="font-size:70%;">13</span><math alttext="{}^{*}" class="ltx_Math" display="inline" id="S4.T1.2.2.1.m1.1"><semantics id="S4.T1.2.2.1.m1.1a"><msup id="S4.T1.2.2.1.m1.1.1" xref="S4.T1.2.2.1.m1.1.1.cmml"><mi id="S4.T1.2.2.1.m1.1.1a" xref="S4.T1.2.2.1.m1.1.1.cmml"></mi><mo id="S4.T1.2.2.1.m1.1.1.1" mathsize="70%" xref="S4.T1.2.2.1.m1.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.1.m1.1b"><apply id="S4.T1.2.2.1.m1.1.1.cmml" xref="S4.T1.2.2.1.m1.1.1"><times id="S4.T1.2.2.1.m1.1.1.1.cmml" xref="S4.T1.2.2.1.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.1.m1.1c">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.2.2.1.m1.1d">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.2.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.2.2.2.1" style="font-size:70%;">89.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.2.2.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.2.2.3.1" style="font-size:70%;">89.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.2.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.2.2.4.1" style="font-size:70%;">83.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T1.2.2.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.2.2.5.1" style="font-size:70%;">83.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.2.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.2.2.6.1" style="font-size:70%;">89.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.2.2.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.2.2.7.1" style="font-size:70%;">89.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.2.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.2.2.8.1" style="font-size:70%;">85.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.2.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.2.2.9.1" style="font-size:70%;">85.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.20.14">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.20.14.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.20.14.1.1" style="font-size:70%;">14</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.20.14.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.20.14.2.1" style="font-size:70%;">84.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.20.14.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.20.14.3.1" style="font-size:70%;">74.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.20.14.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.20.14.4.1" style="font-size:70%;">80.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T1.6.20.14.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.20.14.5.1" style="font-size:70%;">49.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.20.14.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.20.14.6.1" style="font-size:70%;">85.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.20.14.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.20.14.7.1" style="font-size:70%;">78.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.20.14.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.20.14.8.1" style="font-size:70%;">82.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.20.14.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.20.14.9.1" style="font-size:70%;">45.6</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.21.15">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.21.15.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.21.15.1.1" style="font-size:70%;">15</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.21.15.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.21.15.2.1" style="font-size:70%;">90.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.21.15.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.21.15.3.1" style="font-size:70%;">83.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.21.15.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.21.15.4.1" style="font-size:70%;">89.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T1.6.21.15.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.21.15.5.1" style="font-size:70%;">75.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.21.15.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.21.15.6.1" style="font-size:70%;">92.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.21.15.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.21.15.7.1" style="font-size:70%;">87.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.21.15.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.21.15.8.1" style="font-size:70%;">92.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.21.15.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.21.15.9.1" style="font-size:70%;">83.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.3.3.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text" id="S4.T1.3.3.1.1" style="font-size:70%;">16</span><math alttext="{}^{*}" class="ltx_Math" display="inline" id="S4.T1.3.3.1.m1.1"><semantics id="S4.T1.3.3.1.m1.1a"><msup id="S4.T1.3.3.1.m1.1.1" xref="S4.T1.3.3.1.m1.1.1.cmml"><mi id="S4.T1.3.3.1.m1.1.1a" xref="S4.T1.3.3.1.m1.1.1.cmml"></mi><mo id="S4.T1.3.3.1.m1.1.1.1" mathsize="70%" xref="S4.T1.3.3.1.m1.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.1.m1.1b"><apply id="S4.T1.3.3.1.m1.1.1.cmml" xref="S4.T1.3.3.1.m1.1.1"><times id="S4.T1.3.3.1.m1.1.1.1.cmml" xref="S4.T1.3.3.1.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.1.m1.1c">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.3.3.1.m1.1d">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.3.3.2.1" style="font-size:70%;">90.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.3.3.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.3.3.3.1" style="font-size:70%;">90.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.3.3.4.1" style="font-size:70%;">88.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T1.3.3.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.3.3.5.1" style="font-size:70%;">88.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.3.3.6.1" style="font-size:70%;">90.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.3.3.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.3.3.7.1" style="font-size:70%;">90.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.3.3.8.1" style="font-size:70%;">88.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.3.3.9.1" style="font-size:70%;">88.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.22.16">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.22.16.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.22.16.1.1" style="font-size:70%;">17</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.22.16.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.22.16.2.1" style="font-size:70%;">72.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.22.16.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.22.16.3.1" style="font-size:70%;">65.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.22.16.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.22.16.4.1" style="font-size:70%;">65.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T1.6.22.16.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.22.16.5.1" style="font-size:70%;">49.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.22.16.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.22.16.6.1" style="font-size:70%;">75.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.22.16.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.22.16.7.1" style="font-size:70%;">69.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.22.16.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.22.16.8.1" style="font-size:70%;">71.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.22.16.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.22.16.9.1" style="font-size:70%;">55.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.23.17">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.23.17.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.23.17.1.1" style="font-size:70%;">18</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.23.17.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.23.17.2.1" style="font-size:70%;">68.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.23.17.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.23.17.3.1" style="font-size:70%;">62.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.23.17.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.23.17.4.1" style="font-size:70%;">61.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T1.6.23.17.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.23.17.5.1" style="font-size:70%;">36.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.23.17.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.23.17.6.1" style="font-size:70%;">66.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.23.17.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.23.17.7.1" style="font-size:70%;">61.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.23.17.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.23.17.8.1" style="font-size:70%;">60.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.23.17.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.23.17.9.1" style="font-size:70%;">36.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.4.4.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text" id="S4.T1.4.4.1.1" style="font-size:70%;">19</span><math alttext="{}^{*}" class="ltx_Math" display="inline" id="S4.T1.4.4.1.m1.1"><semantics id="S4.T1.4.4.1.m1.1a"><msup id="S4.T1.4.4.1.m1.1.1" xref="S4.T1.4.4.1.m1.1.1.cmml"><mi id="S4.T1.4.4.1.m1.1.1a" xref="S4.T1.4.4.1.m1.1.1.cmml"></mi><mo id="S4.T1.4.4.1.m1.1.1.1" mathsize="70%" xref="S4.T1.4.4.1.m1.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.1.m1.1b"><apply id="S4.T1.4.4.1.m1.1.1.cmml" xref="S4.T1.4.4.1.m1.1.1"><times id="S4.T1.4.4.1.m1.1.1.1.cmml" xref="S4.T1.4.4.1.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.1.m1.1c">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.4.4.1.m1.1d">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.4.4.2.1" style="font-size:70%;">76.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.4.4.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.4.4.3.1" style="font-size:70%;">76.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.4.4.4.1" style="font-size:70%;">73.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T1.4.4.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.4.4.5.1" style="font-size:70%;">73.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.4.4.6.1" style="font-size:70%;">79.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.4.4.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.4.4.7.1" style="font-size:70%;">79.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.4.4.8.1" style="font-size:70%;">77.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.4.4.9.1" style="font-size:70%;">77.5</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.5.5.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text" id="S4.T1.5.5.1.1" style="font-size:70%;">20</span><math alttext="{}^{*}" class="ltx_Math" display="inline" id="S4.T1.5.5.1.m1.1"><semantics id="S4.T1.5.5.1.m1.1a"><msup id="S4.T1.5.5.1.m1.1.1" xref="S4.T1.5.5.1.m1.1.1.cmml"><mi id="S4.T1.5.5.1.m1.1.1a" xref="S4.T1.5.5.1.m1.1.1.cmml"></mi><mo id="S4.T1.5.5.1.m1.1.1.1" mathsize="70%" xref="S4.T1.5.5.1.m1.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.1.m1.1b"><apply id="S4.T1.5.5.1.m1.1.1.cmml" xref="S4.T1.5.5.1.m1.1.1"><times id="S4.T1.5.5.1.m1.1.1.1.cmml" xref="S4.T1.5.5.1.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.1.m1.1c">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.5.5.1.m1.1d">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.5.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.5.5.2.1" style="font-size:70%;">80.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.5.5.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.5.5.3.1" style="font-size:70%;">80.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.5.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.5.5.4.1" style="font-size:70%;">75.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T1.5.5.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.5.5.5.1" style="font-size:70%;">75.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.5.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.5.5.6.1" style="font-size:70%;">83.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.5.5.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.5.5.7.1" style="font-size:70%;">83.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.5.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.5.5.8.1" style="font-size:70%;">81.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.5.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.5.5.9.1" style="font-size:70%;">81.8</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.6.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text" id="S4.T1.6.6.1.1" style="font-size:70%;">21</span><math alttext="{}^{*}" class="ltx_Math" display="inline" id="S4.T1.6.6.1.m1.1"><semantics id="S4.T1.6.6.1.m1.1a"><msup id="S4.T1.6.6.1.m1.1.1" xref="S4.T1.6.6.1.m1.1.1.cmml"><mi id="S4.T1.6.6.1.m1.1.1a" xref="S4.T1.6.6.1.m1.1.1.cmml"></mi><mo id="S4.T1.6.6.1.m1.1.1.1" mathsize="70%" xref="S4.T1.6.6.1.m1.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.1.m1.1b"><apply id="S4.T1.6.6.1.m1.1.1.cmml" xref="S4.T1.6.6.1.m1.1.1"><times id="S4.T1.6.6.1.m1.1.1.1.cmml" xref="S4.T1.6.6.1.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.1.m1.1c">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.6.6.1.m1.1d">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.6.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.6.2.1" style="font-size:70%;">75.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.6.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.6.3.1" style="font-size:70%;">75.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.6.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.6.4.1" style="font-size:70%;">72.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T1.6.6.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.6.5.1" style="font-size:70%;">72.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.6.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.6.6.1" style="font-size:70%;">76.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.6.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.6.7.1" style="font-size:70%;">76.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.6.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.6.8.1" style="font-size:70%;">69.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.6.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.6.9.1" style="font-size:70%;">69.7</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.24.18">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S4.T1.6.24.18.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_rule" style="width:0.0pt;height:7.4pt;background:black;display:inline-block;"></span><span class="ltx_text" id="S4.T1.6.24.18.1.1" style="font-size:80%;"> Mean</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.6.24.18.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.24.18.2.1" style="font-size:80%;">80.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S4.T1.6.24.18.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.24.18.3.1" style="font-size:80%;">74.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.6.24.18.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.24.18.4.1" style="font-size:80%;">77.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr ltx_border_t" id="S4.T1.6.24.18.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T1.6.24.18.5.1" style="font-size:80%;">60.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.6.24.18.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.6.24.18.6.1" style="font-size:80%;">82.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S4.T1.6.24.18.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.6.24.18.7.1" style="font-size:80%;">77.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.6.24.18.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.6.24.18.8.1" style="font-size:80%;">79.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.6.24.18.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.6.24.18.9.1" style="font-size:80%;">63.4</span></td>
</tr>
</tbody>
</table>
<ul class="ltx_itemize ltx_centering" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><math alttext="{}^{*}" class="ltx_Math" display="inline" id="S4.I1.i1.p1.1.m1.1"><semantics id="S4.I1.i1.p1.1.m1.1a"><msup id="S4.I1.i1.p1.1.m1.1.1" xref="S4.I1.i1.p1.1.m1.1.1.cmml"><mi id="S4.I1.i1.p1.1.m1.1.1a" xref="S4.I1.i1.p1.1.m1.1.1.cmml"></mi><mo id="S4.I1.i1.p1.1.m1.1.1.1" mathsize="70%" xref="S4.I1.i1.p1.1.m1.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S4.I1.i1.p1.1.m1.1b"><apply id="S4.I1.i1.p1.1.m1.1.1.cmml" xref="S4.I1.i1.p1.1.m1.1.1"><times id="S4.I1.i1.p1.1.m1.1.1.1.cmml" xref="S4.I1.i1.p1.1.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i1.p1.1.m1.1c">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i1.p1.1.m1.1d">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math><span class="ltx_text" id="S4.I1.i1.p1.1.1" style="font-size:70%;"> Symmetric objects.</span></p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><math alttext="\dagger" class="ltx_Math" display="inline" id="S4.I1.i2.p1.1.m1.1"><semantics id="S4.I1.i2.p1.1.m1.1a"><mo id="S4.I1.i2.p1.1.m1.1.1" mathsize="70%" xref="S4.I1.i2.p1.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.1.m1.1b"><ci id="S4.I1.i2.p1.1.m1.1.1.cmml" xref="S4.I1.i2.p1.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.1.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i2.p1.1.m1.1d">†</annotation></semantics></math><span class="ltx_text" id="S4.I1.i2.p1.1.1" style="font-size:70%;"> Object ID in the standard order of YCB-Video.
</span></p>
</div>
</li>
</ul>
</figure>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Cardinality Error on SynPick Splits [<math alttext="{\times}10^{-2}" class="ltx_Math" display="inline" id="S4.T2.2.m1.1"><semantics id="S4.T2.2.m1.1b"><mrow id="S4.T2.2.m1.1.1" xref="S4.T2.2.m1.1.1.cmml"><mi id="S4.T2.2.m1.1.1.2" xref="S4.T2.2.m1.1.1.2.cmml"></mi><mo id="S4.T2.2.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T2.2.m1.1.1.1.cmml">×</mo><msup id="S4.T2.2.m1.1.1.3" xref="S4.T2.2.m1.1.1.3.cmml"><mn id="S4.T2.2.m1.1.1.3.2" xref="S4.T2.2.m1.1.1.3.2.cmml">10</mn><mrow id="S4.T2.2.m1.1.1.3.3" xref="S4.T2.2.m1.1.1.3.3.cmml"><mo id="S4.T2.2.m1.1.1.3.3b" xref="S4.T2.2.m1.1.1.3.3.cmml">−</mo><mn id="S4.T2.2.m1.1.1.3.3.2" xref="S4.T2.2.m1.1.1.3.3.2.cmml">2</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.2.m1.1c"><apply id="S4.T2.2.m1.1.1.cmml" xref="S4.T2.2.m1.1.1"><times id="S4.T2.2.m1.1.1.1.cmml" xref="S4.T2.2.m1.1.1.1"></times><csymbol cd="latexml" id="S4.T2.2.m1.1.1.2.cmml" xref="S4.T2.2.m1.1.1.2">absent</csymbol><apply id="S4.T2.2.m1.1.1.3.cmml" xref="S4.T2.2.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T2.2.m1.1.1.3.1.cmml" xref="S4.T2.2.m1.1.1.3">superscript</csymbol><cn id="S4.T2.2.m1.1.1.3.2.cmml" type="integer" xref="S4.T2.2.m1.1.1.3.2">10</cn><apply id="S4.T2.2.m1.1.1.3.3.cmml" xref="S4.T2.2.m1.1.1.3.3"><minus id="S4.T2.2.m1.1.1.3.3.1.cmml" xref="S4.T2.2.m1.1.1.3.3"></minus><cn id="S4.T2.2.m1.1.1.3.3.2.cmml" type="integer" xref="S4.T2.2.m1.1.1.3.3.2">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.m1.1d">{\times}10^{-2}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.m1.1e">× 10 start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT</annotation></semantics></math>].</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T2.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.3.1.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T2.3.1.1.1"><span class="ltx_text" id="S4.T2.3.1.1.1.1" style="font-size:80%;">Method</span></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T2.3.1.1.2">
<span class="ltx_text" id="S4.T2.3.1.1.2.1"></span><span class="ltx_text" id="S4.T2.3.1.1.2.2" style="font-size:80%;"> </span><span class="ltx_text" id="S4.T2.3.1.1.2.3" style="font-size:80%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.3.1.1.2.3.1">
<span class="ltx_tr" id="S4.T2.3.1.1.2.3.1.1">
<span class="ltx_td ltx_align_center" id="S4.T2.3.1.1.2.3.1.1.1">Move</span></span>
</span></span><span class="ltx_text" id="S4.T2.3.1.1.2.4" style="font-size:80%;"> </span><span class="ltx_text" id="S4.T2.3.1.1.2.5"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T2.3.1.1.3">
<span class="ltx_text" id="S4.T2.3.1.1.3.1"></span><span class="ltx_text" id="S4.T2.3.1.1.3.2" style="font-size:80%;"> </span><span class="ltx_text" id="S4.T2.3.1.1.3.3" style="font-size:80%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.3.1.1.3.3.1">
<span class="ltx_tr" id="S4.T2.3.1.1.3.3.1.1">
<span class="ltx_td ltx_align_center" id="S4.T2.3.1.1.3.3.1.1.1">Targeted</span></span>
<span class="ltx_tr" id="S4.T2.3.1.1.3.3.1.2">
<span class="ltx_td ltx_align_center" id="S4.T2.3.1.1.3.3.1.2.1">pick</span></span>
</span></span><span class="ltx_text" id="S4.T2.3.1.1.3.4" style="font-size:80%;"> </span><span class="ltx_text" id="S4.T2.3.1.1.3.5"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T2.3.1.1.4">
<span class="ltx_text" id="S4.T2.3.1.1.4.1"></span><span class="ltx_text" id="S4.T2.3.1.1.4.2" style="font-size:80%;"> </span><span class="ltx_text" id="S4.T2.3.1.1.4.3" style="font-size:80%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.3.1.1.4.3.1">
<span class="ltx_tr" id="S4.T2.3.1.1.4.3.1.1">
<span class="ltx_td ltx_align_center" id="S4.T2.3.1.1.4.3.1.1.1">Untargeted</span></span>
<span class="ltx_tr" id="S4.T2.3.1.1.4.3.1.2">
<span class="ltx_td ltx_align_center" id="S4.T2.3.1.1.4.3.1.2.1">pick</span></span>
</span></span><span class="ltx_text" id="S4.T2.3.1.1.4.4" style="font-size:80%;"> </span><span class="ltx_text" id="S4.T2.3.1.1.4.5"></span>
</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.3.1.1.5"><span class="ltx_text" id="S4.T2.3.1.1.5.1" style="font-size:80%;">All</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.2.2">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T2.3.2.2.1"><span class="ltx_text" id="S4.T2.3.2.2.1.1" style="font-size:80%;">W/o temporal fusion</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.3.2.2.2"><span class="ltx_text" id="S4.T2.3.2.2.2.1" style="font-size:80%;">3.26</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.3.2.2.3"><span class="ltx_text" id="S4.T2.3.2.2.3.1" style="font-size:80%;">1.64</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.3.2.2.4"><span class="ltx_text" id="S4.T2.3.2.2.4.1" style="font-size:80%;">0.48</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.3.2.2.5"><span class="ltx_text" id="S4.T2.3.2.2.5.1" style="font-size:80%;">2.06</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.3.3">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T2.3.3.3.1"><span class="ltx_text" id="S4.T2.3.3.3.1.1" style="font-size:80%;">With temporal fusion</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.3.3.3.2"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.3.2.1" style="font-size:80%;">0.62</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.3.3.3.3"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.3.3.1" style="font-size:80%;">0.52</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.3.3.3.4"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.3.4.1" style="font-size:80%;">0.44</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.3.3.3.5"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.3.5.1" style="font-size:80%;">0.53</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>False negative detections on SynPick Splits [<math alttext="{\times}10^{-2}" class="ltx_Math" display="inline" id="S4.T3.2.m1.1"><semantics id="S4.T3.2.m1.1b"><mrow id="S4.T3.2.m1.1.1" xref="S4.T3.2.m1.1.1.cmml"><mi id="S4.T3.2.m1.1.1.2" xref="S4.T3.2.m1.1.1.2.cmml"></mi><mo id="S4.T3.2.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T3.2.m1.1.1.1.cmml">×</mo><msup id="S4.T3.2.m1.1.1.3" xref="S4.T3.2.m1.1.1.3.cmml"><mn id="S4.T3.2.m1.1.1.3.2" xref="S4.T3.2.m1.1.1.3.2.cmml">10</mn><mrow id="S4.T3.2.m1.1.1.3.3" xref="S4.T3.2.m1.1.1.3.3.cmml"><mo id="S4.T3.2.m1.1.1.3.3b" xref="S4.T3.2.m1.1.1.3.3.cmml">−</mo><mn id="S4.T3.2.m1.1.1.3.3.2" xref="S4.T3.2.m1.1.1.3.3.2.cmml">2</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.2.m1.1c"><apply id="S4.T3.2.m1.1.1.cmml" xref="S4.T3.2.m1.1.1"><times id="S4.T3.2.m1.1.1.1.cmml" xref="S4.T3.2.m1.1.1.1"></times><csymbol cd="latexml" id="S4.T3.2.m1.1.1.2.cmml" xref="S4.T3.2.m1.1.1.2">absent</csymbol><apply id="S4.T3.2.m1.1.1.3.cmml" xref="S4.T3.2.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T3.2.m1.1.1.3.1.cmml" xref="S4.T3.2.m1.1.1.3">superscript</csymbol><cn id="S4.T3.2.m1.1.1.3.2.cmml" type="integer" xref="S4.T3.2.m1.1.1.3.2">10</cn><apply id="S4.T3.2.m1.1.1.3.3.cmml" xref="S4.T3.2.m1.1.1.3.3"><minus id="S4.T3.2.m1.1.1.3.3.1.cmml" xref="S4.T3.2.m1.1.1.3.3"></minus><cn id="S4.T3.2.m1.1.1.3.3.2.cmml" type="integer" xref="S4.T3.2.m1.1.1.3.3.2">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.m1.1d">{\times}10^{-2}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.m1.1e">× 10 start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT</annotation></semantics></math>].</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T3.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.3.1.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T3.3.1.1.1"><span class="ltx_text" id="S4.T3.3.1.1.1.1" style="font-size:80%;">Method</span></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T3.3.1.1.2">
<span class="ltx_text" id="S4.T3.3.1.1.2.1"></span><span class="ltx_text" id="S4.T3.3.1.1.2.2" style="font-size:80%;"> </span><span class="ltx_text" id="S4.T3.3.1.1.2.3" style="font-size:80%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.3.1.1.2.3.1">
<span class="ltx_tr" id="S4.T3.3.1.1.2.3.1.1">
<span class="ltx_td ltx_align_center" id="S4.T3.3.1.1.2.3.1.1.1">Move</span></span>
</span></span><span class="ltx_text" id="S4.T3.3.1.1.2.4" style="font-size:80%;"> </span><span class="ltx_text" id="S4.T3.3.1.1.2.5"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T3.3.1.1.3">
<span class="ltx_text" id="S4.T3.3.1.1.3.1"></span><span class="ltx_text" id="S4.T3.3.1.1.3.2" style="font-size:80%;"> </span><span class="ltx_text" id="S4.T3.3.1.1.3.3" style="font-size:80%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.3.1.1.3.3.1">
<span class="ltx_tr" id="S4.T3.3.1.1.3.3.1.1">
<span class="ltx_td ltx_align_center" id="S4.T3.3.1.1.3.3.1.1.1">Targeted</span></span>
<span class="ltx_tr" id="S4.T3.3.1.1.3.3.1.2">
<span class="ltx_td ltx_align_center" id="S4.T3.3.1.1.3.3.1.2.1">pick</span></span>
</span></span><span class="ltx_text" id="S4.T3.3.1.1.3.4" style="font-size:80%;"> </span><span class="ltx_text" id="S4.T3.3.1.1.3.5"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T3.3.1.1.4">
<span class="ltx_text" id="S4.T3.3.1.1.4.1"></span><span class="ltx_text" id="S4.T3.3.1.1.4.2" style="font-size:80%;"> </span><span class="ltx_text" id="S4.T3.3.1.1.4.3" style="font-size:80%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.3.1.1.4.3.1">
<span class="ltx_tr" id="S4.T3.3.1.1.4.3.1.1">
<span class="ltx_td ltx_align_center" id="S4.T3.3.1.1.4.3.1.1.1">Untargeted</span></span>
<span class="ltx_tr" id="S4.T3.3.1.1.4.3.1.2">
<span class="ltx_td ltx_align_center" id="S4.T3.3.1.1.4.3.1.2.1">pick</span></span>
</span></span><span class="ltx_text" id="S4.T3.3.1.1.4.4" style="font-size:80%;"> </span><span class="ltx_text" id="S4.T3.3.1.1.4.5"></span>
</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.3.1.1.5"><span class="ltx_text" id="S4.T3.3.1.1.5.1" style="font-size:80%;">All</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.2.2">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T3.3.2.2.1"><span class="ltx_text" id="S4.T3.3.2.2.1.1" style="font-size:80%;">W/o temporal fusion</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.3.2.2.2"><span class="ltx_text" id="S4.T3.3.2.2.2.1" style="font-size:80%;">2.79</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.3.2.2.3"><span class="ltx_text" id="S4.T3.3.2.2.3.1" style="font-size:80%;">1.39</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.3.2.2.4"><span class="ltx_text" id="S4.T3.3.2.2.4.1" style="font-size:80%;">1.36</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.3.2.2.5"><span class="ltx_text" id="S4.T3.3.2.2.5.1" style="font-size:80%;">1.79</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.3.3">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T3.3.3.3.1"><span class="ltx_text" id="S4.T3.3.3.3.1.1" style="font-size:80%;">With temporal fusion</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.3.3.3.2"><span class="ltx_text ltx_font_bold" id="S4.T3.3.3.3.2.1" style="font-size:80%;">0.57</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.3.3.3.3"><span class="ltx_text ltx_font_bold" id="S4.T3.3.3.3.3.1" style="font-size:80%;">0.44</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.3.3.3.4"><span class="ltx_text ltx_font_bold" id="S4.T3.3.3.3.4.1" style="font-size:80%;">0.44</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.3.3.3.5"><span class="ltx_text ltx_font_bold" id="S4.T3.3.3.3.5.1" style="font-size:80%;">0.48</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.5.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.6.2">Implementation Details</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.3">Following <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx47" title="">47</a>]</cite>, we choose the cardinality of the predicted set <math alttext="N" class="ltx_Math" display="inline" id="S4.SS3.p1.1.m1.1"><semantics id="S4.SS3.p1.1.m1.1a"><mi id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><ci id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.1.m1.1d">italic_N</annotation></semantics></math> proportional to
the maximum number of objects in an image in the respective datasets: 30 for SynPick and 20 for YCB-Video.
In  <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S3.SS4" title="III-D Loss Function ‣ III Method ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-D</span></span></a>, the bounding box components are weighted using factors 2 and 5, and the keypoint components are weighted with factors 10 and 1.
The pose component and the temporal consistency component are weighted down using factors 0.05 and 0.1, respectively.
The encoder and decoder modules consist of six layers each.
All the embeddings used in our model are of dimension 256.
We train our model for 150 epochs using the AdamW optimizer with a learning rate of <math alttext="1{\times}10^{-4}" class="ltx_Math" display="inline" id="S4.SS3.p1.2.m2.1"><semantics id="S4.SS3.p1.2.m2.1a"><mrow id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml"><mn id="S4.SS3.p1.2.m2.1.1.2" xref="S4.SS3.p1.2.m2.1.1.2.cmml">1</mn><mo id="S4.SS3.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS3.p1.2.m2.1.1.1.cmml">×</mo><msup id="S4.SS3.p1.2.m2.1.1.3" xref="S4.SS3.p1.2.m2.1.1.3.cmml"><mn id="S4.SS3.p1.2.m2.1.1.3.2" xref="S4.SS3.p1.2.m2.1.1.3.2.cmml">10</mn><mrow id="S4.SS3.p1.2.m2.1.1.3.3" xref="S4.SS3.p1.2.m2.1.1.3.3.cmml"><mo id="S4.SS3.p1.2.m2.1.1.3.3a" xref="S4.SS3.p1.2.m2.1.1.3.3.cmml">−</mo><mn id="S4.SS3.p1.2.m2.1.1.3.3.2" xref="S4.SS3.p1.2.m2.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><apply id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1"><times id="S4.SS3.p1.2.m2.1.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1.1"></times><cn id="S4.SS3.p1.2.m2.1.1.2.cmml" type="integer" xref="S4.SS3.p1.2.m2.1.1.2">1</cn><apply id="S4.SS3.p1.2.m2.1.1.3.cmml" xref="S4.SS3.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.p1.2.m2.1.1.3.1.cmml" xref="S4.SS3.p1.2.m2.1.1.3">superscript</csymbol><cn id="S4.SS3.p1.2.m2.1.1.3.2.cmml" type="integer" xref="S4.SS3.p1.2.m2.1.1.3.2">10</cn><apply id="S4.SS3.p1.2.m2.1.1.3.3.cmml" xref="S4.SS3.p1.2.m2.1.1.3.3"><minus id="S4.SS3.p1.2.m2.1.1.3.3.1.cmml" xref="S4.SS3.p1.2.m2.1.1.3.3"></minus><cn id="S4.SS3.p1.2.m2.1.1.3.3.2.cmml" type="integer" xref="S4.SS3.p1.2.m2.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">1{\times}10^{-4}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.2.m2.1d">1 × 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math> and early stopping.
We set the number of time steps <math alttext="T" class="ltx_Math" display="inline" id="S4.SS3.p1.3.m3.1"><semantics id="S4.SS3.p1.3.m3.1a"><mi id="S4.SS3.p1.3.m3.1.1" xref="S4.SS3.p1.3.m3.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m3.1b"><ci id="S4.SS3.p1.3.m3.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m3.1c">T</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.3.m3.1d">italic_T</annotation></semantics></math> to eight in the temporal fusion modules and use a batch size of 32 (four groups of eight consecutive images).</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS4.5.1.1">IV-D</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS4.6.2">Results on SynPick</span>
</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">Formulating multi-object pose estimation as a set prediction problem enables joint object detection and pose estimation of all objects in the scene.
However, it compounds the size of the dataset required to train transformer models. Thus, to complement the existing 240 videos for training, we generate additional 300 video sequences for each action split.
We call this extended version <span class="ltx_text" id="S4.SS4.p1.1.1">SynPick-Ext</span> and make it publicly available<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.ais.uni-bonn.de/videos/tempose" title="">https://www.ais.uni-bonn.de/videos/tempose</a></span></span></span>.
We downsample the image resolution to 640<math alttext="{\times}" class="ltx_Math" display="inline" id="S4.SS4.p1.1.m1.1"><semantics id="S4.SS4.p1.1.m1.1a"><mo id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><times id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">{\times}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.1.m1.1d">×</annotation></semantics></math>480.
SynPick consists of objects piled up in a tote and in many cases, objects are completely occluded.
To exclude heavily occluded objects, we use a minimum visibility threshold of 30% in our evaluation.
In <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S4.F4" title="Figure 4 ‣ IV-B Metrics ‣ IV Evaluation ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>, we present pose estimates generated by our model with and without temporal fusion.
Both models generate predictions of admissible quality. However, the model without temporal fusion suffers from failed object detections (<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S4.F4" title="Figure 4 ‣ IV-B Metrics ‣ IV Evaluation ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>(a), (d)), and
isolated highly erroneous pose predictions (<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S4.F4" title="Figure 4 ‣ IV-B Metrics ‣ IV Evaluation ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>(b), (c), (e)). Temporal fusion helps in alleviating these shortcomings.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">In <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S4.T1" title="TABLE I ‣ IV-B Metrics ‣ IV Evaluation ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">I</span></a>, we report quantitative results of our model. MOTPose achieves impressive AUC of ADD-S and AUC of ADD(-S) scores of 82.0 and 77.1, respectively, which is
an improvement of 1.2 and 2.9 compared to the model without temporal fusion. Additionally, we also report the AUC metrics with a threshold of 10% of the object diameter (AUC@0.1d).
This metric takes the object size into account better. In terms of AUC of ADD-S and <span class="ltx_text" id="S4.SS4.p2.1.1">ADD(-S)@0.1d</span>, temporal fusion boosts the accuracy by 1.9 and 2.6 points, respectively.</p>
</div>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.3">Furthermore, to understand the impact of temporal fusion on object detection, we analyze the cardinality error and the bounding box accuracy metrics. The cardinality error is the difference between elements in the ground-truth and predicted sets.
Formally, given the ground-truth set <math alttext="\mathcal{Y}" class="ltx_Math" display="inline" id="S4.SS4.p3.1.m1.1"><semantics id="S4.SS4.p3.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p3.1.m1.1.1" xref="S4.SS4.p3.1.m1.1.1.cmml">𝒴</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.1.m1.1b"><ci id="S4.SS4.p3.1.m1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1">𝒴</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.1.m1.1c">\mathcal{Y}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.1.m1.1d">caligraphic_Y</annotation></semantics></math> and the predicted set <math alttext="\hat{\mathcal{Y}}" class="ltx_Math" display="inline" id="S4.SS4.p3.2.m2.1"><semantics id="S4.SS4.p3.2.m2.1a"><mover accent="true" id="S4.SS4.p3.2.m2.1.1" xref="S4.SS4.p3.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p3.2.m2.1.1.2" xref="S4.SS4.p3.2.m2.1.1.2.cmml">𝒴</mi><mo id="S4.SS4.p3.2.m2.1.1.1" xref="S4.SS4.p3.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.2.m2.1b"><apply id="S4.SS4.p3.2.m2.1.1.cmml" xref="S4.SS4.p3.2.m2.1.1"><ci id="S4.SS4.p3.2.m2.1.1.1.cmml" xref="S4.SS4.p3.2.m2.1.1.1">^</ci><ci id="S4.SS4.p3.2.m2.1.1.2.cmml" xref="S4.SS4.p3.2.m2.1.1.2">𝒴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.2.m2.1c">\hat{\mathcal{Y}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.2.m2.1d">over^ start_ARG caligraphic_Y end_ARG</annotation></semantics></math>, the cardinality error (<span class="ltx_text ltx_markedasmath" id="S4.SS4.p3.3.1">CE</span>) is defined as:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{CE}=\frac{|(\mathcal{Y}-\hat{\mathcal{Y}})\cup(\hat{\mathcal{Y}}-%
\mathcal{Y})|}{|\mathcal{Y}|}." class="ltx_Math" display="block" id="S4.E1.m1.3"><semantics id="S4.E1.m1.3a"><mrow id="S4.E1.m1.3.3.1" xref="S4.E1.m1.3.3.1.1.cmml"><mrow id="S4.E1.m1.3.3.1.1" xref="S4.E1.m1.3.3.1.1.cmml"><mtext id="S4.E1.m1.3.3.1.1.2" xref="S4.E1.m1.3.3.1.1.2a.cmml">CE</mtext><mo id="S4.E1.m1.3.3.1.1.1" xref="S4.E1.m1.3.3.1.1.1.cmml">=</mo><mfrac id="S4.E1.m1.2.2" xref="S4.E1.m1.2.2.cmml"><mrow id="S4.E1.m1.1.1.1.1" xref="S4.E1.m1.1.1.1.2.cmml"><mo id="S4.E1.m1.1.1.1.1.2" stretchy="false" xref="S4.E1.m1.1.1.1.2.1.cmml">|</mo><mrow id="S4.E1.m1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.cmml"><mrow id="S4.E1.m1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S4.E1.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="S4.E1.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E1.m1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.1.1.1.2.cmml">𝒴</mi><mo id="S4.E1.m1.1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.cmml">−</mo><mover accent="true" id="S4.E1.m1.1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.1.1.1.1.1.1.1.1.3.2" xref="S4.E1.m1.1.1.1.1.1.1.1.1.3.2.cmml">𝒴</mi><mo id="S4.E1.m1.1.1.1.1.1.1.1.1.3.1" xref="S4.E1.m1.1.1.1.1.1.1.1.1.3.1.cmml">^</mo></mover></mrow><mo id="S4.E1.m1.1.1.1.1.1.1.1.3" stretchy="false" xref="S4.E1.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S4.E1.m1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.3.cmml">∪</mo><mrow id="S4.E1.m1.1.1.1.1.1.2.1" xref="S4.E1.m1.1.1.1.1.1.2.1.1.cmml"><mo id="S4.E1.m1.1.1.1.1.1.2.1.2" stretchy="false" xref="S4.E1.m1.1.1.1.1.1.2.1.1.cmml">(</mo><mrow id="S4.E1.m1.1.1.1.1.1.2.1.1" xref="S4.E1.m1.1.1.1.1.1.2.1.1.cmml"><mover accent="true" id="S4.E1.m1.1.1.1.1.1.2.1.1.2" xref="S4.E1.m1.1.1.1.1.1.2.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.1.1.1.1.1.2.1.1.2.2" xref="S4.E1.m1.1.1.1.1.1.2.1.1.2.2.cmml">𝒴</mi><mo id="S4.E1.m1.1.1.1.1.1.2.1.1.2.1" xref="S4.E1.m1.1.1.1.1.1.2.1.1.2.1.cmml">^</mo></mover><mo id="S4.E1.m1.1.1.1.1.1.2.1.1.1" xref="S4.E1.m1.1.1.1.1.1.2.1.1.1.cmml">−</mo><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.1.1.1.1.1.2.1.1.3" xref="S4.E1.m1.1.1.1.1.1.2.1.1.3.cmml">𝒴</mi></mrow><mo id="S4.E1.m1.1.1.1.1.1.2.1.3" stretchy="false" xref="S4.E1.m1.1.1.1.1.1.2.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E1.m1.1.1.1.1.3" stretchy="false" xref="S4.E1.m1.1.1.1.2.1.cmml">|</mo></mrow><mrow id="S4.E1.m1.2.2.2.3" xref="S4.E1.m1.2.2.2.2.cmml"><mo id="S4.E1.m1.2.2.2.3.1" stretchy="false" xref="S4.E1.m1.2.2.2.2.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.2.2.2.1" xref="S4.E1.m1.2.2.2.1.cmml">𝒴</mi><mo id="S4.E1.m1.2.2.2.3.2" stretchy="false" xref="S4.E1.m1.2.2.2.2.1.cmml">|</mo></mrow></mfrac></mrow><mo id="S4.E1.m1.3.3.1.2" lspace="0em" xref="S4.E1.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.3b"><apply id="S4.E1.m1.3.3.1.1.cmml" xref="S4.E1.m1.3.3.1"><eq id="S4.E1.m1.3.3.1.1.1.cmml" xref="S4.E1.m1.3.3.1.1.1"></eq><ci id="S4.E1.m1.3.3.1.1.2a.cmml" xref="S4.E1.m1.3.3.1.1.2"><mtext id="S4.E1.m1.3.3.1.1.2.cmml" xref="S4.E1.m1.3.3.1.1.2">CE</mtext></ci><apply id="S4.E1.m1.2.2.cmml" xref="S4.E1.m1.2.2"><divide id="S4.E1.m1.2.2.3.cmml" xref="S4.E1.m1.2.2"></divide><apply id="S4.E1.m1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1"><abs id="S4.E1.m1.1.1.1.2.1.cmml" xref="S4.E1.m1.1.1.1.1.2"></abs><apply id="S4.E1.m1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1"><union id="S4.E1.m1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.1.3"></union><apply id="S4.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1"><minus id="S4.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1"></minus><ci id="S4.E1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.2">𝒴</ci><apply id="S4.E1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.3"><ci id="S4.E1.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.3.1">^</ci><ci id="S4.E1.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.3.2">𝒴</ci></apply></apply><apply id="S4.E1.m1.1.1.1.1.1.2.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.2.1"><minus id="S4.E1.m1.1.1.1.1.1.2.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.2.1.1.1"></minus><apply id="S4.E1.m1.1.1.1.1.1.2.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1.2.1.1.2"><ci id="S4.E1.m1.1.1.1.1.1.2.1.1.2.1.cmml" xref="S4.E1.m1.1.1.1.1.1.2.1.1.2.1">^</ci><ci id="S4.E1.m1.1.1.1.1.1.2.1.1.2.2.cmml" xref="S4.E1.m1.1.1.1.1.1.2.1.1.2.2">𝒴</ci></apply><ci id="S4.E1.m1.1.1.1.1.1.2.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.1.2.1.1.3">𝒴</ci></apply></apply></apply><apply id="S4.E1.m1.2.2.2.2.cmml" xref="S4.E1.m1.2.2.2.3"><abs id="S4.E1.m1.2.2.2.2.1.cmml" xref="S4.E1.m1.2.2.2.3.1"></abs><ci id="S4.E1.m1.2.2.2.1.cmml" xref="S4.E1.m1.2.2.2.1">𝒴</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.3c">\text{CE}=\frac{|(\mathcal{Y}-\hat{\mathcal{Y}})\cup(\hat{\mathcal{Y}}-%
\mathcal{Y})|}{|\mathcal{Y}|}.</annotation><annotation encoding="application/x-llamapun" id="S4.E1.m1.3d">CE = divide start_ARG | ( caligraphic_Y - over^ start_ARG caligraphic_Y end_ARG ) ∪ ( over^ start_ARG caligraphic_Y end_ARG - caligraphic_Y ) | end_ARG start_ARG | caligraphic_Y | end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS4.p4">
<p class="ltx_p" id="S4.SS4.p4.1">In <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S4.T2" title="TABLE II ‣ IV-B Metrics ‣ IV Evaluation ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">II</span></a>, we report the cardinality error of our model on different splits of the SynPick dataset.
Over the complete test set, the cardinality error of the model without temporal fusion is 0.021, whereas it is only 0.005 for the model with temporal fusion.
The difference is more evident in the <span class="ltx_text ltx_font_italic" id="S4.SS4.p4.1.1">Move</span> split, which is more challenging than the other two splits.</p>
</div>
<div class="ltx_para" id="S4.SS4.p5">
<p class="ltx_p" id="S4.SS4.p5.2">Although <span class="ltx_text ltx_markedasmath" id="S4.SS4.p5.2.1">CE</span> reflects the set prediction ability of a model, in real-world bin-picking systems, the identity of the objects present in the bin might be known a priori <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx52" title="">52</a>]</cite>.
Thus, in this <em class="ltx_emph ltx_font_italic" id="S4.SS4.p5.2.2">informed detection</em> scenario, false positives can be easily mitigated, whereas false negatives (FN), i.e., <math alttext="|(\mathcal{Y}-\hat{\mathcal{Y}})|/|\mathcal{Y}|" class="ltx_Math" display="inline" id="S4.SS4.p5.2.m2.2"><semantics id="S4.SS4.p5.2.m2.2a"><mrow id="S4.SS4.p5.2.m2.2.2" xref="S4.SS4.p5.2.m2.2.2.cmml"><mrow id="S4.SS4.p5.2.m2.2.2.1.1" xref="S4.SS4.p5.2.m2.2.2.1.2.cmml"><mo id="S4.SS4.p5.2.m2.2.2.1.1.2" stretchy="false" xref="S4.SS4.p5.2.m2.2.2.1.2.1.cmml">|</mo><mrow id="S4.SS4.p5.2.m2.2.2.1.1.1.1" xref="S4.SS4.p5.2.m2.2.2.1.1.1.1.1.cmml"><mo id="S4.SS4.p5.2.m2.2.2.1.1.1.1.2" stretchy="false" xref="S4.SS4.p5.2.m2.2.2.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS4.p5.2.m2.2.2.1.1.1.1.1" xref="S4.SS4.p5.2.m2.2.2.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p5.2.m2.2.2.1.1.1.1.1.2" xref="S4.SS4.p5.2.m2.2.2.1.1.1.1.1.2.cmml">𝒴</mi><mo id="S4.SS4.p5.2.m2.2.2.1.1.1.1.1.1" xref="S4.SS4.p5.2.m2.2.2.1.1.1.1.1.1.cmml">−</mo><mover accent="true" id="S4.SS4.p5.2.m2.2.2.1.1.1.1.1.3" xref="S4.SS4.p5.2.m2.2.2.1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p5.2.m2.2.2.1.1.1.1.1.3.2" xref="S4.SS4.p5.2.m2.2.2.1.1.1.1.1.3.2.cmml">𝒴</mi><mo id="S4.SS4.p5.2.m2.2.2.1.1.1.1.1.3.1" xref="S4.SS4.p5.2.m2.2.2.1.1.1.1.1.3.1.cmml">^</mo></mover></mrow><mo id="S4.SS4.p5.2.m2.2.2.1.1.1.1.3" stretchy="false" xref="S4.SS4.p5.2.m2.2.2.1.1.1.1.1.cmml">)</mo></mrow><mo id="S4.SS4.p5.2.m2.2.2.1.1.3" stretchy="false" xref="S4.SS4.p5.2.m2.2.2.1.2.1.cmml">|</mo></mrow><mo id="S4.SS4.p5.2.m2.2.2.2" xref="S4.SS4.p5.2.m2.2.2.2.cmml">/</mo><mrow id="S4.SS4.p5.2.m2.2.2.3.2" xref="S4.SS4.p5.2.m2.2.2.3.1.cmml"><mo id="S4.SS4.p5.2.m2.2.2.3.2.1" stretchy="false" xref="S4.SS4.p5.2.m2.2.2.3.1.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p5.2.m2.1.1" xref="S4.SS4.p5.2.m2.1.1.cmml">𝒴</mi><mo id="S4.SS4.p5.2.m2.2.2.3.2.2" stretchy="false" xref="S4.SS4.p5.2.m2.2.2.3.1.1.cmml">|</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p5.2.m2.2b"><apply id="S4.SS4.p5.2.m2.2.2.cmml" xref="S4.SS4.p5.2.m2.2.2"><divide id="S4.SS4.p5.2.m2.2.2.2.cmml" xref="S4.SS4.p5.2.m2.2.2.2"></divide><apply id="S4.SS4.p5.2.m2.2.2.1.2.cmml" xref="S4.SS4.p5.2.m2.2.2.1.1"><abs id="S4.SS4.p5.2.m2.2.2.1.2.1.cmml" xref="S4.SS4.p5.2.m2.2.2.1.1.2"></abs><apply id="S4.SS4.p5.2.m2.2.2.1.1.1.1.1.cmml" xref="S4.SS4.p5.2.m2.2.2.1.1.1.1"><minus id="S4.SS4.p5.2.m2.2.2.1.1.1.1.1.1.cmml" xref="S4.SS4.p5.2.m2.2.2.1.1.1.1.1.1"></minus><ci id="S4.SS4.p5.2.m2.2.2.1.1.1.1.1.2.cmml" xref="S4.SS4.p5.2.m2.2.2.1.1.1.1.1.2">𝒴</ci><apply id="S4.SS4.p5.2.m2.2.2.1.1.1.1.1.3.cmml" xref="S4.SS4.p5.2.m2.2.2.1.1.1.1.1.3"><ci id="S4.SS4.p5.2.m2.2.2.1.1.1.1.1.3.1.cmml" xref="S4.SS4.p5.2.m2.2.2.1.1.1.1.1.3.1">^</ci><ci id="S4.SS4.p5.2.m2.2.2.1.1.1.1.1.3.2.cmml" xref="S4.SS4.p5.2.m2.2.2.1.1.1.1.1.3.2">𝒴</ci></apply></apply></apply><apply id="S4.SS4.p5.2.m2.2.2.3.1.cmml" xref="S4.SS4.p5.2.m2.2.2.3.2"><abs id="S4.SS4.p5.2.m2.2.2.3.1.1.cmml" xref="S4.SS4.p5.2.m2.2.2.3.2.1"></abs><ci id="S4.SS4.p5.2.m2.1.1.cmml" xref="S4.SS4.p5.2.m2.1.1">𝒴</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p5.2.m2.2c">|(\mathcal{Y}-\hat{\mathcal{Y}})|/|\mathcal{Y}|</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p5.2.m2.2d">| ( caligraphic_Y - over^ start_ARG caligraphic_Y end_ARG ) | / | caligraphic_Y |</annotation></semantics></math> are detrimental.
In <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S4.T3" title="TABLE III ‣ IV-B Metrics ‣ IV Evaluation ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">III</span></a>, we report the false negatives of object detection. Over the entire test set, the model without temporal fusion has a FN rate of 0.018; with temporal fusion, the FN rate drops to 0.005.</p>
</div>
<div class="ltx_para" id="S4.SS4.p6">
<p class="ltx_p" id="S4.SS4.p6.1">To compare the bounding box detection accuracy, we analyze the average precision and recall metrics defined by COCO evaluation protocol<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://cocodataset.org/#detection-eval" title="">https://cocodataset.org/#detection-eval</a></span></span></span>.
In <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S4.T4" title="TABLE IV ‣ IV-D Results on SynPick ‣ IV Evaluation ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">IV</span></a>, we report the AP@[IoU=0.50:0.95], AP@[IoU=0.50], AP@[IoU=0.75], and AR@[IoU=0.50:0.95] metrics of the models with and without temporal fusion.
Across all the reported metrics, temporal fusion yields consistent improvements.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Bounding Box Prediction Accuracy.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T4.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.2.2">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T4.2.2.3"><span class="ltx_text" id="S4.T4.2.2.3.1" style="font-size:80%;">Method</span></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T4.1.1.1">
<span class="ltx_text" id="S4.T4.1.1.1.2"></span><span class="ltx_text" id="S4.T4.1.1.1.3" style="font-size:80%;"> </span><span class="ltx_text" id="S4.T4.1.1.1.1" style="font-size:80%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T4.1.1.1.1.1.1">
<span class="ltx_tr" id="S4.T4.1.1.1.1.1.1.1">
<span class="ltx_td ltx_align_center" id="S4.T4.1.1.1.1.1.1.1.1">AP<math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S4.T4.1.1.1.1.1.1.1.1.m1.1"><semantics id="S4.T4.1.1.1.1.1.1.1.1.m1.1a"><msup id="S4.T4.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.T4.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mi id="S4.T4.1.1.1.1.1.1.1.1.m1.1.1a" xref="S4.T4.1.1.1.1.1.1.1.1.m1.1.1.cmml"></mi><mo id="S4.T4.1.1.1.1.1.1.1.1.m1.1.1.1" xref="S4.T4.1.1.1.1.1.1.1.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.1.1.1.1.m1.1b"><apply id="S4.T4.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.1.1.1.1.m1.1.1"><ci id="S4.T4.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T4.1.1.1.1.1.1.1.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.1.1.1.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.1.1.1.1.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math></span></span>
</span></span><span class="ltx_text" id="S4.T4.1.1.1.4" style="font-size:80%;"> </span><span class="ltx_text" id="S4.T4.1.1.1.5"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T4.2.2.4">
<span class="ltx_text" id="S4.T4.2.2.4.1"></span><span class="ltx_text" id="S4.T4.2.2.4.2" style="font-size:80%;"> </span><span class="ltx_text" id="S4.T4.2.2.4.3" style="font-size:80%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T4.2.2.4.3.1">
<span class="ltx_tr" id="S4.T4.2.2.4.3.1.1">
<span class="ltx_td ltx_align_center" id="S4.T4.2.2.4.3.1.1.1">AP@</span></span>
<span class="ltx_tr" id="S4.T4.2.2.4.3.1.2">
<span class="ltx_td ltx_align_center" id="S4.T4.2.2.4.3.1.2.1">[IoU=0.50]</span></span>
</span></span><span class="ltx_text" id="S4.T4.2.2.4.4" style="font-size:80%;"> </span><span class="ltx_text" id="S4.T4.2.2.4.5"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T4.2.2.5">
<span class="ltx_text" id="S4.T4.2.2.5.1"></span><span class="ltx_text" id="S4.T4.2.2.5.2" style="font-size:80%;"> </span><span class="ltx_text" id="S4.T4.2.2.5.3" style="font-size:80%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T4.2.2.5.3.1">
<span class="ltx_tr" id="S4.T4.2.2.5.3.1.1">
<span class="ltx_td ltx_align_center" id="S4.T4.2.2.5.3.1.1.1">AP@</span></span>
<span class="ltx_tr" id="S4.T4.2.2.5.3.1.2">
<span class="ltx_td ltx_align_center" id="S4.T4.2.2.5.3.1.2.1">[IoU=0.75]</span></span>
</span></span><span class="ltx_text" id="S4.T4.2.2.5.4" style="font-size:80%;"> </span><span class="ltx_text" id="S4.T4.2.2.5.5"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T4.2.2.2">
<span class="ltx_text" id="S4.T4.2.2.2.2"></span><span class="ltx_text" id="S4.T4.2.2.2.3" style="font-size:80%;"> </span><span class="ltx_text" id="S4.T4.2.2.2.1" style="font-size:80%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T4.2.2.2.1.1.1">
<span class="ltx_tr" id="S4.T4.2.2.2.1.1.1.1">
<span class="ltx_td ltx_align_center" id="S4.T4.2.2.2.1.1.1.1.1">AR<math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S4.T4.2.2.2.1.1.1.1.1.m1.1"><semantics id="S4.T4.2.2.2.1.1.1.1.1.m1.1a"><msup id="S4.T4.2.2.2.1.1.1.1.1.m1.1.1" xref="S4.T4.2.2.2.1.1.1.1.1.m1.1.1.cmml"><mi id="S4.T4.2.2.2.1.1.1.1.1.m1.1.1a" xref="S4.T4.2.2.2.1.1.1.1.1.m1.1.1.cmml"></mi><mo id="S4.T4.2.2.2.1.1.1.1.1.m1.1.1.1" xref="S4.T4.2.2.2.1.1.1.1.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.1.1.1.1.1.m1.1b"><apply id="S4.T4.2.2.2.1.1.1.1.1.m1.1.1.cmml" xref="S4.T4.2.2.2.1.1.1.1.1.m1.1.1"><ci id="S4.T4.2.2.2.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T4.2.2.2.1.1.1.1.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.1.1.1.1.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.2.2.2.1.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math></span></span>
</span></span><span class="ltx_text" id="S4.T4.2.2.2.4" style="font-size:80%;"> </span><span class="ltx_text" id="S4.T4.2.2.2.5"></span>
</th>
</tr>
<tr class="ltx_tr" id="S4.T4.2.3.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.2.3.1.1"><span class="ltx_text" id="S4.T4.2.3.1.1.1" style="font-size:80%;">W/o temporal fusion</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.3.1.2"><span class="ltx_text" id="S4.T4.2.3.1.2.1" style="font-size:80%;">0.756</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.3.1.3"><span class="ltx_text" id="S4.T4.2.3.1.3.1" style="font-size:80%;">0.872</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.3.1.4"><span class="ltx_text" id="S4.T4.2.3.1.4.1" style="font-size:80%;">0.853</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.3.1.5"><span class="ltx_text" id="S4.T4.2.3.1.5.1" style="font-size:80%;">0.789</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.4.2">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T4.2.4.2.1"><span class="ltx_text" id="S4.T4.2.4.2.1.1" style="font-size:80%;">With temporal fusion</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T4.2.4.2.2"><span class="ltx_text ltx_font_bold" id="S4.T4.2.4.2.2.1" style="font-size:80%;">0.779</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T4.2.4.2.3"><span class="ltx_text ltx_font_bold" id="S4.T4.2.4.2.3.1" style="font-size:80%;">0.876</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T4.2.4.2.4"><span class="ltx_text ltx_font_bold" id="S4.T4.2.4.2.4.1" style="font-size:80%;">0.858</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.2.4.2.5"><span class="ltx_text ltx_font_bold" id="S4.T4.2.4.2.5.1" style="font-size:80%;">0.811</span></td>
</tr>
</tbody>
</table>
<ul class="ltx_itemize ltx_centering" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1"><math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S4.I2.i1.p1.1.m1.1"><semantics id="S4.I2.i1.p1.1.m1.1a"><msup id="S4.I2.i1.p1.1.m1.1.1" xref="S4.I2.i1.p1.1.m1.1.1.cmml"><mi id="S4.I2.i1.p1.1.m1.1.1a" xref="S4.I2.i1.p1.1.m1.1.1.cmml"></mi><mo id="S4.I2.i1.p1.1.m1.1.1.1" mathsize="80%" xref="S4.I2.i1.p1.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S4.I2.i1.p1.1.m1.1b"><apply id="S4.I2.i1.p1.1.m1.1.1.cmml" xref="S4.I2.i1.p1.1.m1.1.1"><ci id="S4.I2.i1.p1.1.m1.1.1.1.cmml" xref="S4.I2.i1.p1.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i1.p1.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i1.p1.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math><span class="ltx_text" id="S4.I2.i1.p1.1.1" style="font-size:80%;"> @[IoU=0.50:0.95]</span></p>
</div>
</li>
</ul>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS5.5.1.1">IV-E</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS5.6.2">Results on YCB-Video</span>
</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.2">In <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S4.T5" title="TABLE V ‣ IV-F Ablation Study ‣ IV Evaluation ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">V</span></a>, we report the quantitative comparison of our MOTPose model against state-of-the-art methods on the YCB-Video dataset.
In our experiments, we fuse seven previous frames (<math alttext="T{=}8" class="ltx_Math" display="inline" id="S4.SS5.p1.1.m1.1"><semantics id="S4.SS5.p1.1.m1.1a"><mrow id="S4.SS5.p1.1.m1.1.1" xref="S4.SS5.p1.1.m1.1.1.cmml"><mi id="S4.SS5.p1.1.m1.1.1.2" xref="S4.SS5.p1.1.m1.1.1.2.cmml">T</mi><mo id="S4.SS5.p1.1.m1.1.1.1" xref="S4.SS5.p1.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS5.p1.1.m1.1.1.3" xref="S4.SS5.p1.1.m1.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.1.m1.1b"><apply id="S4.SS5.p1.1.m1.1.1.cmml" xref="S4.SS5.p1.1.m1.1.1"><eq id="S4.SS5.p1.1.m1.1.1.1.cmml" xref="S4.SS5.p1.1.m1.1.1.1"></eq><ci id="S4.SS5.p1.1.m1.1.1.2.cmml" xref="S4.SS5.p1.1.m1.1.1.2">𝑇</ci><cn id="S4.SS5.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.SS5.p1.1.m1.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.1.m1.1c">T{=}8</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p1.1.m1.1d">italic_T = 8</annotation></semantics></math>) in MOTPose. Since our model does not produce outputs for the initial <math alttext="T{-}1" class="ltx_Math" display="inline" id="S4.SS5.p1.2.m2.1"><semantics id="S4.SS5.p1.2.m2.1a"><mrow id="S4.SS5.p1.2.m2.1.1" xref="S4.SS5.p1.2.m2.1.1.cmml"><mi id="S4.SS5.p1.2.m2.1.1.2" xref="S4.SS5.p1.2.m2.1.1.2.cmml">T</mi><mo id="S4.SS5.p1.2.m2.1.1.1" xref="S4.SS5.p1.2.m2.1.1.1.cmml">−</mo><mn id="S4.SS5.p1.2.m2.1.1.3" xref="S4.SS5.p1.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.2.m2.1b"><apply id="S4.SS5.p1.2.m2.1.1.cmml" xref="S4.SS5.p1.2.m2.1.1"><minus id="S4.SS5.p1.2.m2.1.1.1.cmml" xref="S4.SS5.p1.2.m2.1.1.1"></minus><ci id="S4.SS5.p1.2.m2.1.1.2.cmml" xref="S4.SS5.p1.2.m2.1.1.2">𝑇</ci><cn id="S4.SS5.p1.2.m2.1.1.3.cmml" type="integer" xref="S4.SS5.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.2.m2.1c">T{-}1</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p1.2.m2.1d">italic_T - 1</annotation></semantics></math> frames in a video sequence, we report the accuracy scores excluding the initial frames.
Temporal fusion enables considerable improvement in the MOTPose model: 0.9 and 1.3 accuracy points in terms of the AUC of ADD-S and AUC of ADD-(S) metric, respectively.
Compared to DeepIM-Tracking <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx23" title="">23</a>]</cite>, our method achieves a comparable AUC of ADD-S score and a slightly worse
AUC of ADD-(S) score.
DeepIM-Tracking formulates 6D pose tracking as pose refinement, i.e., pose prediction from the previous frame is used to initialize the <span class="ltx_text" id="S4.SS5.p1.2.1">render-and-compare</span> pose refinement for the current step. To initialize the first frame, the authors used the ground-truth pose.
While <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx27" title="">27</a>,  ]</cite> achieve a significantly better accuracy than MOTPose, they perform only pose refinement.
In contrast, our method performs multi-object detection and pose estimation jointly.
Moreover, MOTPose accuracy is comparable to the state-of-the-art multi-object pose estimation method of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx53" title="">53</a>,  ]</cite>
in terms of the AUC of ADD-(S) metric and only slightly worse in terms of the AUC of ADD-S metric.
Note that the frame rates reported in <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S4.T5" title="TABLE V ‣ IV-F Ablation Study ‣ IV Evaluation ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">V</span></a> are observed on GPUs of different generations and
the values are provided only for a relative comparison.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS6.5.1.1">IV-F</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS6.6.2">Ablation Study</span>
</h3>
<div class="ltx_para" id="S4.SS6.p1">
<p class="ltx_p" id="S4.SS6.p1.1">To understand the contribution of the individual components to the overall performance of MOTPose, we investigated removing different components of the model and varying the number of time steps used in the fusion modules.
In <a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#S4.T6" title="TABLE VI ‣ IV-F Ablation Study ‣ IV Evaluation ‣ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">VI</span></a>, we report the results of the ablation experiment on SynPick. Removing the TEFM module resulted in a big drop in the overall accuracy of the MOTPose model.
In terms of the AUC of <span class="ltx_text" id="S4.SS6.p1.1.1">ADD(-S)</span> metric, the MOTPose model without the TEFM module achieves a score of 74.9, compared to 77.1, while the AUC of ADD-S metric score drops by 0.6.
Similarly, removing the TOFM module results in a drop of 0.9 AUC of <span class="ltx_text" id="S4.SS6.p1.1.2">ADD(-S)</span> and 1.8 AUC of ADD-S accuracy scores.
Moreover, in terms of the number of time steps used in the fusion modules, eight time steps resulted in the best performance overall.</p>
</div>
<figure class="ltx_table" id="S4.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE V: </span>Results on the YCB-Video dataset.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T5.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T5.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T5.1.1.1.1"><span class="ltx_text" id="S4.T5.1.1.1.1.1" style="font-size:80%;">Method</span></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T5.1.1.1.2">
<span class="ltx_text" id="S4.T5.1.1.1.2.1"></span><span class="ltx_text" id="S4.T5.1.1.1.2.2" style="font-size:80%;"> </span><span class="ltx_text" id="S4.T5.1.1.1.2.3" style="font-size:80%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T5.1.1.1.2.3.1">
<span class="ltx_tr" id="S4.T5.1.1.1.2.3.1.1">
<span class="ltx_td ltx_align_center" id="S4.T5.1.1.1.2.3.1.1.1">AUC of</span></span>
<span class="ltx_tr" id="S4.T5.1.1.1.2.3.1.2">
<span class="ltx_td ltx_align_center" id="S4.T5.1.1.1.2.3.1.2.1">ADD-S</span></span>
</span></span><span class="ltx_text" id="S4.T5.1.1.1.2.4" style="font-size:80%;"> </span><span class="ltx_text" id="S4.T5.1.1.1.2.5"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T5.1.1.1.3">
<span class="ltx_text" id="S4.T5.1.1.1.3.1"></span><span class="ltx_text" id="S4.T5.1.1.1.3.2" style="font-size:80%;"> </span><span class="ltx_text" id="S4.T5.1.1.1.3.3" style="font-size:80%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T5.1.1.1.3.3.1">
<span class="ltx_tr" id="S4.T5.1.1.1.3.3.1.1">
<span class="ltx_td ltx_align_center" id="S4.T5.1.1.1.3.3.1.1.1">AUC of</span></span>
<span class="ltx_tr" id="S4.T5.1.1.1.3.3.1.2">
<span class="ltx_td ltx_align_center" id="S4.T5.1.1.1.3.3.1.2.1">ADD(-S)</span></span>
</span></span><span class="ltx_text" id="S4.T5.1.1.1.3.4" style="font-size:80%;"> </span><span class="ltx_text" id="S4.T5.1.1.1.3.5"></span>
</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.1.1.1.4"><span class="ltx_text ltx_font_italic" id="S4.T5.1.1.1.4.1" style="font-size:80%;">fps</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T5.1.2.2.1">
<span class="ltx_text" id="S4.T5.1.2.2.1.1" style="font-size:80%;">CRT-6D </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T5.1.2.2.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx27" title="">27</a><span class="ltx_text" id="S4.T5.1.2.2.1.3.2" style="font-size:80%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.1.2.2.2"><span class="ltx_text" id="S4.T5.1.2.2.2.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.1.2.2.3"><span class="ltx_text ltx_font_bold" id="S4.T5.1.2.2.3.1" style="font-size:80%;">87.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.2.2.4"><span class="ltx_text" id="S4.T5.1.2.2.4.1" style="font-size:80%;">30</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.3.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T5.1.3.3.1"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T5.1.3.3.1.1.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx53" title="">53</a><span class="ltx_text" id="S4.T5.1.3.3.1.2.2" style="font-size:80%;">,  ]</span></cite></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.1.3.3.2"><span class="ltx_text ltx_font_bold" id="S4.T5.1.3.3.2.1" style="font-size:80%;">92.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.1.3.3.3"><span class="ltx_text" id="S4.T5.1.3.3.3.1" style="font-size:80%;">84.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.3.3.4"><span class="ltx_text" id="S4.T5.1.3.3.4.1" style="font-size:80%;">26</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.4.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T5.1.4.4.1">
<span class="ltx_text" id="S4.T5.1.4.4.1.1" style="font-size:80%;">DeepIM-Tracking </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T5.1.4.4.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx23" title="">23</a><span class="ltx_text" id="S4.T5.1.4.4.1.3.2" style="font-size:80%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.1.4.4.2"><span class="ltx_text" id="S4.T5.1.4.4.2.1" style="font-size:80%;">91.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.1.4.4.3"><span class="ltx_text" id="S4.T5.1.4.4.3.1" style="font-size:80%;">85.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.4.4.4"><span class="ltx_text" id="S4.T5.1.4.4.4.1" style="font-size:80%;">13</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.5.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T5.1.5.5.1"><span class="ltx_text" id="S4.T5.1.5.5.1.1" style="font-size:80%;">MOTPose w/o temporal fusion</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.1.5.5.2"><span class="ltx_text" id="S4.T5.1.5.5.2.1" style="font-size:80%;">90.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.1.5.5.3"><span class="ltx_text" id="S4.T5.1.5.5.3.1" style="font-size:80%;">83.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.5.5.4"><span class="ltx_text" id="S4.T5.1.5.5.4.1" style="font-size:80%;">59</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.6.6">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T5.1.6.6.1"><span class="ltx_text" id="S4.T5.1.6.6.1.1" style="font-size:80%;">MOTPose with temporal fusion</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T5.1.6.6.2"><span class="ltx_text" id="S4.T5.1.6.6.2.1" style="font-size:80%;">91.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T5.1.6.6.3"><span class="ltx_text" id="S4.T5.1.6.6.3.1" style="font-size:80%;">84.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.6.6.4"><span class="ltx_text" id="S4.T5.1.6.6.4.1" style="font-size:80%;">30</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S4.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE VI: </span>Ablation study results on the SynPick dataset.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T6.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T6.3.4.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T6.3.4.1.1"><span class="ltx_text" id="S4.T6.3.4.1.1.1" style="font-size:80%;">Method</span></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T6.3.4.1.2">
<span class="ltx_text" id="S4.T6.3.4.1.2.1"></span><span class="ltx_text" id="S4.T6.3.4.1.2.2" style="font-size:80%;"> </span><span class="ltx_text" id="S4.T6.3.4.1.2.3" style="font-size:80%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T6.3.4.1.2.3.1">
<span class="ltx_tr" id="S4.T6.3.4.1.2.3.1.1">
<span class="ltx_td ltx_align_center" id="S4.T6.3.4.1.2.3.1.1.1">AUC of</span></span>
<span class="ltx_tr" id="S4.T6.3.4.1.2.3.1.2">
<span class="ltx_td ltx_align_center" id="S4.T6.3.4.1.2.3.1.2.1">ADD-S</span></span>
</span></span><span class="ltx_text" id="S4.T6.3.4.1.2.4" style="font-size:80%;"> </span><span class="ltx_text" id="S4.T6.3.4.1.2.5"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T6.3.4.1.3">
<span class="ltx_text" id="S4.T6.3.4.1.3.1"></span><span class="ltx_text" id="S4.T6.3.4.1.3.2" style="font-size:80%;"> </span><span class="ltx_text" id="S4.T6.3.4.1.3.3" style="font-size:80%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T6.3.4.1.3.3.1">
<span class="ltx_tr" id="S4.T6.3.4.1.3.3.1.1">
<span class="ltx_td ltx_align_center" id="S4.T6.3.4.1.3.3.1.1.1">AUC of</span></span>
<span class="ltx_tr" id="S4.T6.3.4.1.3.3.1.2">
<span class="ltx_td ltx_align_center" id="S4.T6.3.4.1.3.3.1.2.1">ADD(-S)</span></span>
</span></span><span class="ltx_text" id="S4.T6.3.4.1.3.4" style="font-size:80%;"> </span><span class="ltx_text" id="S4.T6.3.4.1.3.5"></span>
</th>
</tr>
<tr class="ltx_tr" id="S4.T6.3.5.2">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.3.5.2.1"><span class="ltx_text" id="S4.T6.3.5.2.1.1" style="font-size:80%;">MOTPose</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.3.5.2.2"><span class="ltx_text ltx_font_bold" id="S4.T6.3.5.2.2.1" style="font-size:80%;">82.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.3.5.2.3"><span class="ltx_text ltx_font_bold" id="S4.T6.3.5.2.3.1" style="font-size:80%;">77.1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.6.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T6.3.6.3.1"><span class="ltx_text" id="S4.T6.3.6.3.1.1" style="font-size:80%;">MOTPose without temporal fusion</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.3.6.3.2"><span class="ltx_text" id="S4.T6.3.6.3.2.1" style="font-size:80%;">80.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.6.3.3"><span class="ltx_text" id="S4.T6.3.6.3.3.1" style="font-size:80%;">74.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.7.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T6.3.7.4.1"><span class="ltx_text" id="S4.T6.3.7.4.1.1" style="font-size:80%;">MOTPose without TEFM</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.3.7.4.2"><span class="ltx_text" id="S4.T6.3.7.4.2.1" style="font-size:80%;">81.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.7.4.3"><span class="ltx_text" id="S4.T6.3.7.4.3.1" style="font-size:80%;">74.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.8.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T6.3.8.5.1"><span class="ltx_text" id="S4.T6.3.8.5.1.1" style="font-size:80%;">MOTPose without TOFM</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.3.8.5.2"><span class="ltx_text" id="S4.T6.3.8.5.2.1" style="font-size:80%;">81.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.8.5.3"><span class="ltx_text" id="S4.T6.3.8.5.3.1" style="font-size:80%;">75.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.9.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T6.3.9.6.1"><span class="ltx_text" id="S4.T6.3.9.6.1.1" style="font-size:80%;">MOTPose without SynPick-Ext</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.3.9.6.2"><span class="ltx_text" id="S4.T6.3.9.6.2.1" style="font-size:80%;">76.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.9.6.3"><span class="ltx_text" id="S4.T6.3.9.6.3.1" style="font-size:80%;">69.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T6.1.1.1">
<span class="ltx_text" id="S4.T6.1.1.1.1" style="font-size:80%;">MOTPose [</span><math alttext="T" class="ltx_Math" display="inline" id="S4.T6.1.1.1.m1.1"><semantics id="S4.T6.1.1.1.m1.1a"><mi id="S4.T6.1.1.1.m1.1.1" mathsize="80%" xref="S4.T6.1.1.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.T6.1.1.1.m1.1b"><ci id="S4.T6.1.1.1.m1.1.1.cmml" xref="S4.T6.1.1.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.1.1.1.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="S4.T6.1.1.1.m1.1d">italic_T</annotation></semantics></math><span class="ltx_text" id="S4.T6.1.1.1.2" style="font-size:80%;">=4]</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.1.1.2"><span class="ltx_text" id="S4.T6.1.1.2.1" style="font-size:80%;">80.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.1.3"><span class="ltx_text" id="S4.T6.1.1.3.1" style="font-size:80%;">76.4</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.2.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T6.2.2.1">
<span class="ltx_text" id="S4.T6.2.2.1.1" style="font-size:80%;">MOTPose [</span><math alttext="T" class="ltx_Math" display="inline" id="S4.T6.2.2.1.m1.1"><semantics id="S4.T6.2.2.1.m1.1a"><mi id="S4.T6.2.2.1.m1.1.1" mathsize="80%" xref="S4.T6.2.2.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.T6.2.2.1.m1.1b"><ci id="S4.T6.2.2.1.m1.1.1.cmml" xref="S4.T6.2.2.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.2.2.1.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="S4.T6.2.2.1.m1.1d">italic_T</annotation></semantics></math><span class="ltx_text" id="S4.T6.2.2.1.2" style="font-size:80%;">=8]</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.2.2.2"><span class="ltx_text" id="S4.T6.2.2.2.1" style="font-size:80%;">82.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.2.2.3"><span class="ltx_text ltx_font_bold" id="S4.T6.2.2.3.1" style="font-size:80%;">77.1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.3">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T6.3.3.1">
<span class="ltx_text" id="S4.T6.3.3.1.1" style="font-size:80%;">MOTPose [</span><math alttext="T" class="ltx_Math" display="inline" id="S4.T6.3.3.1.m1.1"><semantics id="S4.T6.3.3.1.m1.1a"><mi id="S4.T6.3.3.1.m1.1.1" mathsize="80%" xref="S4.T6.3.3.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.T6.3.3.1.m1.1b"><ci id="S4.T6.3.3.1.m1.1.1.cmml" xref="S4.T6.3.3.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.3.3.1.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="S4.T6.3.3.1.m1.1d">italic_T</annotation></semantics></math><span class="ltx_text" id="S4.T6.3.3.1.2" style="font-size:80%;">=12]</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T6.3.3.2"><span class="ltx_text ltx_font_bold" id="S4.T6.3.3.2.1" style="font-size:80%;">82.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.3.3.3"><span class="ltx_text" id="S4.T6.3.3.3.1" style="font-size:80%;">76.7</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS7.5.1.1">IV-G</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS7.6.2">Limitations</span>
</h3>
<div class="ltx_para" id="S4.SS7.p1">
<p class="ltx_p" id="S4.SS7.p1.1">Our formulation of multi-object pose estimation as a set prediction problem limits the datasets available for training our model.
Compared to 2D annotations, 6D pose annotations are significantly harder to obtain.
Thus, many of the standard datasets for evaluating object pose estimation like Linemod-Occluded <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx50" title="">50</a>]</cite> and Linemod <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09309v1#bib.bibx10" title="">10</a>]</cite> provide pose annotations only for a partial number of objects per scene in the training dataset.
While this is not a limitation for multi-stage methods that process the cropped version of the images for estimating the pose of target objects, our method needs 6D pose annotation for all objects in the scene, which can be prohibitively expensive to acquire in some scenarios.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We presented MOTPose, a multi-object pose estimation model for RGB video sequences.
Employing the cross-attention-based TEFM and TOFM modules, the MOTPose model fuses object embeddings and object-specific outputs over multiple time steps, respectively.
Aided by the temporal information, our model performs significantly better than the single-frame RGB model while being lighter and significantly faster than other pose tracking methods.
</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Acknowledgment</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">This work has been funded by the German Ministry of
Education and Research (BMBF), grant no. 01IS21080, project
“Learn2Grasp: Learning Human-like Interactive Grasping
based on Visual and Haptic Feedback”.
</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bibx1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">Tomáš Hodaň et al.
</span>
<span class="ltx_bibblock">“BOP challenge 2020 on 6D object localization”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx1.1.1">European Conference on Computer Vision (ECCV)</em>, 2020, pp. 577–594
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">Kai Han et al.
</span>
<span class="ltx_bibblock">“A survey on vision transformer”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx2.1.1">IEEE Transactions on Pattern Analysis and Machine
Intelligence (TPAMI)</em> <span class="ltx_text ltx_font_bold" id="bib.bibx2.2.2">45.1</span>, 2022, pp. 87–110
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">Qingsong Wen et al.
</span>
<span class="ltx_bibblock">“Transformers in time series: A survey”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx3.1.1">32nd International Joint Conference on Artificial
Intelligence (IJCAI)</em>, 2023
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">Salman Khan et al.
</span>
<span class="ltx_bibblock">“Transformers in Vision: A Survey”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx4.1.1">ACM Computing Survey</em> <span class="ltx_text ltx_font_bold" id="bib.bibx4.2.2">54.10s</span>
</span>
<span class="ltx_bibblock">New York, NY, USA: Association for Computing Machinery, 2022, pp. 200:1–200:41
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">Xinyu Liu et al.
</span>
<span class="ltx_bibblock">“EfficientViT: Memory Efficient Vision Transformer With
Cascaded Group Attention”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx5.1.1">IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, 2023, pp. 14420–14430
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">Feng Li et al.
</span>
<span class="ltx_bibblock">“Mask DINO: Towards a Unified Transformer-based Framework
for Object Detection and Segmentation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx6.1.1">IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, 2023, pp. 3041–3050
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">Arash Amini, Arul Selvam Periyasamy and Sven Behnke
</span>
<span class="ltx_bibblock">“YOLOPose: Transformer-based multi-object 6D pose
estimation using keypoint regression”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx7.1.1">17th International Conference on Intelligent Autonomous
Systems (IAS)</em>, 2022, pp. 392–406
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">Arash Amini, Arul Selvam Periyasamy and Sven Behnke
</span>
<span class="ltx_bibblock">“T6D-Direct: Transformers for Multi-Object 6D Object Pose
Estimation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx8.1.1">DAGM German Conference on Pattern Recognition (GCPR)</em>, 2021
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">Stefan Hinterstoißer et al.
</span>
<span class="ltx_bibblock">“Gradient Response Maps for Real-Time Detection of Textureless
Objects”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx9.1.1">IEEE Transactions on Pattern Analysis and Machine
Intelligence (TPAMI)</em> <span class="ltx_text ltx_font_bold" id="bib.bibx9.2.2">34</span>, 2012, pp. 876–888
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">Stefan Hinterstoisser et al.
</span>
<span class="ltx_bibblock">“Model-based training, detection and pose estimation of
texture-less 3D objects in heavily cluttered scenes”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx10.1.1">Asian Conference on Computer Vision (ACCV)</em>, 2013, pp. 548–562
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">Fred Rothganger, Svetlana Lazebnik, Cordelia Schmid and Jean Ponce
</span>
<span class="ltx_bibblock">“3D Object Modeling and Recognition Using Local
Affine-Invariant Image Descriptors and Multi-View Spatial Constraints”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx11.1.1">International Journal of Computer Vision (IJCV)</em> <span class="ltx_text ltx_font_bold" id="bib.bibx11.2.2">66</span>, 2006, pp. 231–259
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">Georgios Pavlakos et al.
</span>
<span class="ltx_bibblock">“6-DoF object pose from semantic keypoints”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx12.1.1">IEEE International Conference on Robotics and
Automation (ICRA)</em>, 2017, pp. 2011–2018
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">Shubham Tulsiani and Jitendra Malik
</span>
<span class="ltx_bibblock">“Viewpoints and keypoints”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx13.1.1">IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, 2014, pp. 1510–1519
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">Yu Xiang, Tanner Schmidt, Venkatraman Narayanan and Dieter Fox
</span>
<span class="ltx_bibblock">“PoseCNN: A Convolutional Neural Network for 6D Object
Pose Estimation in Cluttered Scenes”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx14.1.1">Robotics: Science and Systems (RSS)</em>, 2018
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">Arul Selvam Periyasamy, Max Schwarz and Sven Behnke
</span>
<span class="ltx_bibblock">“Robust 6D Object Pose Estimation in Cluttered Scenes Using
Semantic Segmentation and Pose Regression Networks”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx15.1.1">IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS)</em>, 2018
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.1109/IROS.2018.8594406" title="">10.1109/IROS.2018.8594406</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">Gu Wang, Fabian Manhardt, Federico Tombari and Xiangyang Ji
</span>
<span class="ltx_bibblock">“GDR-Net: Geometry-Guided Direct Regression Network for
Monocular 6D Object Pose Estimation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx16.1.1">IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, 2021
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">Yan Di et al.
</span>
<span class="ltx_bibblock">“SO-Pose: Exploiting self-occlusion for direct 6D pose
estimation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx17.1.1">IEEE/CVF International Conference on Computer Vision
(ICCV)</em>, 2021, pp. 12396–12405
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">Mahdi Rad and Vincent Lepetit
</span>
<span class="ltx_bibblock">“BB8: A scalable, accurate, robust to partial occlusion
method for predicting the 3D poses of challenging objects without using
depth”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx18.1.1">IEEE International Conference on Computer Vision (ICCV)</em>, 2017, pp. 3828–3836
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">Bugra Tekin, Sudipta N Sinha and Pascal Fua
</span>
<span class="ltx_bibblock">“Real-time seamless single shot 6D object pose prediction”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx19.1.1">IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, 2018
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">Yinlin Hu, Joachim Hugonot, Pascal Fua and Mathieu Salzmann
</span>
<span class="ltx_bibblock">“Segmentation-driven 6D object pose estimation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx20.1.1">IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, 2019, pp. 3385–3394
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">Sida Peng et al.
</span>
<span class="ltx_bibblock">“PVNet: Pixel-wise voting network for 6DOF pose
estimation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx21.1.1">IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, 2019, pp. 4561–4570
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">Yinlin Hu, Pascal Fua, Wei Wang and Mathieu Salzmann
</span>
<span class="ltx_bibblock">“Single-stage 6D object pose estimation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx22.1.1">IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, 2020, pp. 2930–2939
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">Yi Li et al.
</span>
<span class="ltx_bibblock">“DeepIM: Deep iterative matching for 6D pose estimation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx23.1.1">European Conference on Computer Vision (ECCV)</em>, 2018, pp. 683–698
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">Fabian Manhardt, Wadim Kehl, Nassir Navab and Federico Tombari
</span>
<span class="ltx_bibblock">“Deep model-based 6D pose refinement in RGB”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx24.1.1">European Conference on Computer Vision (ECCV)</em>, 2018, pp. 800–815
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">Y. Labbe, J. Carpentier, M. Aubry and J. Sivic
</span>
<span class="ltx_bibblock">“CosyPose: Consistent multi-view multi-object 6D pose
estimation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx25.1.1">European Conference on Computer Vision (ECCV)</em>, 2020
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">Arul Selvam Periyasamy, Max Schwarz and Sven Behnke
</span>
<span class="ltx_bibblock">“Refining 6D object pose predictions using abstract
render-and-compare”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx26.1.1">IEEE-RAS International Conference on Humanoid Robots
(Humanoids)</em>, 2019, pp. 739–746
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">Pedro Castro and Tae-Kyun Kim
</span>
<span class="ltx_bibblock">“CRT-6D: Fast 6D object pose estimation with cascaded
refinement transformers”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx27.1.1">IEEE/CVF Winter Conference on Applications of Computer
Vision (WACV)</em>, 2023, pp. 5746–5755
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">Yang Hai, Rui Song, Jiaojiao Li and Yinlin Hu
</span>
<span class="ltx_bibblock">“Shape-Constraint Recurrent Flow for 6D Object Pose
Estimation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx28.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em>, 2023, pp. 4831–4840
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">Yinlin Hu, Pascal Fua and Mathieu Salzmann
</span>
<span class="ltx_bibblock">“Perspective flow aggregation for data-limited 6D object
pose estimation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx29.1.1">European Conference on Computer Vision (ECCV)</em>, 2022, pp. 89–106
</span>
<span class="ltx_bibblock">Springer
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">Catherine Capellen, Max Schwarz and Sven Behnke
</span>
<span class="ltx_bibblock">“ConvPoseCNN: Dense Convolutional 6D Object Pose
Estimation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx30.1.1">15th International Conference on Computer Vision Theory
and Applications (VISAPP)</em>, 2020
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">Stefan Thalhammer, Markus Leitner, Timothy Patten and Markus Vincze
</span>
<span class="ltx_bibblock">“PyraPose: Feature Pyramids for Fast and Accurate Object
Pose Estimation under Domain Shift”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx31.1.1">IEEE International Conference on Robotics and Automation
(ICRA)</em>, 2021, pp. 13909–13915
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">Nicolas Carion et al.
</span>
<span class="ltx_bibblock">“End-to-end object detection with transformers”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx32.1.1">European Conference on Computer Vision (ECCV)</em>, 2020, pp. 213–229
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">Thomas Georg Jantos et al.
</span>
<span class="ltx_bibblock">“PoET: Pose Estimation Transformer for Single-View,
Multi-Object 6D Pose Estimation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx33.1.1">Conference on Robot Learning (CoRL)</em>, 2023, pp. 1060–1070
</span>
<span class="ltx_bibblock">PMLR
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">Pedram Azad, David Münch, Tamim Asfour and Rüdiger Dillmann
</span>
<span class="ltx_bibblock">“6-DoF model-based tracking of arbitrarily shaped 3D
objects”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx34.1.1">IEEE International Conference on Robotics and
Automation (ICRA)</em>, 2011, pp. 5204–5209
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">Karl Pauwels, Leonardo Rubio, Javier Díaz and Eduardo Ros
</span>
<span class="ltx_bibblock">“Real-Time Model-Based Rigid Object Pose Estimation and
Tracking Combining Dense and Sparse Visual Cues”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx35.1.1">IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, 2013, pp. 2347–2354
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">Yu Xiang, Changkyu Song, Roozbeh Mottaghi and Silvio Savarese
</span>
<span class="ltx_bibblock">“Monocular Multiview Object Tracking with 3D Aspect Parts”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx36.1.1">European Conference on Computer Vision (ECCV)</em>, 2014, pp. 220–235
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">Xinke Deng et al.
</span>
<span class="ltx_bibblock">“PoseRBPF: A Rao-Blackwellized Particle Filter for 6D
Object Pose Tracking”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx37.1.1">Robotics: Science and Systems (RSS)</em>, 2019
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">Bowen Wen, Chaitanya Mitash, Baozhang Ren and Kostas E. Bekris
</span>
<span class="ltx_bibblock">“se(3)-TrackNet: Data-driven 6D Pose Tracking by
Calibrating Image Residuals in Synthetic Domains”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx38.1.1">IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS)</em>, 2020
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">Philipp Bergmann, Tim Meinhardt and Laura Leal-Taixé
</span>
<span class="ltx_bibblock">“Tracking Without Bells and Whistles”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx39.1.1">IEEE International Conference on Computer Vision (ICCV)</em>, 2019
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">Xingyi Zhou, Vladlen Koltun and Philipp Krähenbühl
</span>
<span class="ltx_bibblock">“Tracking Objects as Points”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx40.1.1">15th European Conference on Computer Vision (ECCV)</em>, 2020
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">Yihong Xu et al.
</span>
<span class="ltx_bibblock">“TransCenter: Transformers With Dense Representations for
Multiple-Object Tracking”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx41.1.1">IEEE Transactions on Pattern Analysis and Machine
Intelligence (TPAMI)</em>, 2021
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">Tim Meinhardt, Alexander Kirillov, Laura Leal-Taixe and Christoph Feichtenhofer
</span>
<span class="ltx_bibblock">“TrackFormer: Multi-object tracking with transformers”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx42.1.1">IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, 2022
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">Fangao Zeng et al.
</span>
<span class="ltx_bibblock">“MOTR: End-to-end multiple-object tracking with
transformer”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx43.1.1">17th European Conference on Computer Vision (ECCV)</em>, 2022
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">Peize Sun et al.
</span>
<span class="ltx_bibblock">“TransTrack: Multiple-object tracking with transformer”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx44.1.1">arXiv:2012.15460</em>, 2020
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">Shichao Li, Zengqiang Yan, Hongyang Li and Kwang-Ting Cheng
</span>
<span class="ltx_bibblock">“Exploring intermediate representation for monocular vehicle
pose estimation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx45.1.1">IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, 2021, pp. 1873–1883
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">Harold W Kuhn
</span>
<span class="ltx_bibblock">“The Hungarian method for the assignment problem”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx46.1.1">Naval Research Logistics Quarterly</em> <span class="ltx_text ltx_font_bold" id="bib.bibx46.2.2">2.1-2</span>
</span>
<span class="ltx_bibblock">Wiley Online Library, 1955, pp. 83–97
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">Arul Selvam Periyasamy, Arash Amini, Vladimir Tsaturyan and Sven Behnke
</span>
<span class="ltx_bibblock">“YOLOPose V2: Understanding and improving transformer-based
6D‚ pose estimation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx47.1.1">Robotics and Autonomous Systems</em> <span class="ltx_text ltx_font_bold" id="bib.bibx47.2.2">168</span>
</span>
<span class="ltx_bibblock">Elsevier, 2023, pp. 104490
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">Hamid Rezatofighi et al.
</span>
<span class="ltx_bibblock">“Generalized intersection over union: A metric and a loss for
bounding box regression”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx48.1.1">IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, 2019, pp. 658–666
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">Arul Selvam Periyasamy, Max Schwarz and Sven Behnke
</span>
<span class="ltx_bibblock">“SynPick: A dataset for dynamic bin picking scene
understanding”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx49.1.1">IEEE International Conference on Automation Science and
Engineering (CASE)</em>, 2021, pp. 488–493
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">Eric Brachmann
</span>
<span class="ltx_bibblock">“6D Object Pose Estimation using 3D Object Coordinates
[Data]”
</span>
<span class="ltx_bibblock">heiDATA, 2020
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.11588/data/V4MUMX" title="">10.11588/data/V4MUMX</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">Max Schwarz et al.
</span>
<span class="ltx_bibblock">“Fast object learning and dual-arm coordination for cluttered
stowing, picking, and packing”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx51.1.1">IEEE International Conference on Robotics and Automation
(ICRA)</em>, 2018, pp. 3347–3354
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">Max Schwarz et al.
</span>
<span class="ltx_bibblock">“NimbRo picking: Versatile part handling for warehouse
automation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx52.1.1">IEEE International Conference on Robotics and Automation
(ICRA)</em>, 2017, pp. 3032–3039
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">Arul Selvam Periyasamy, Vladimir Tsaturyan and Sven Behnke
</span>
<span class="ltx_bibblock">“Efficient Multi-Object Pose Estimation using Multi-Resolution
Deformable Attention and Query Aggregation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx53.1.1">IEEE International Conference on Robotic Computing
(IRC)</em>, 2023
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Mar 14 11:58:24 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
