<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation</title>
<!--Generated on Fri Aug 16 22:01:34 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2408.09042v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#S1" title="In ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#S2" title="In ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#S2.SS1" title="In 2 Related work â€£ ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Structure-from-Motion (SfM)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#S2.SS2" title="In 2 Related work â€£ ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Data-driven pose estimation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#S2.SS3" title="In 2 Related work â€£ ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Generative learning and contrastive learning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#S3" title="In ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#S3.SS1" title="In 3 Method â€£ ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Multi-view feature extraction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#S3.SS2" title="In 3 Method â€£ ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Pose generator</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#S3.SS3" title="In 3 Method â€£ ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Pose discriminator</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#S3.SS4" title="In 3 Method â€£ ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Model training details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#S3.SS5" title="In 3 Method â€£ ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Implementation details</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#S4" title="In ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#S4.SS1" title="In 4 Experiments â€£ ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Experiment setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#S4.SS2" title="In 4 Experiments â€£ ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Comparing with SoTA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#S4.SS3" title="In 4 Experiments â€£ ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Ablation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#S4.SS4" title="In 4 Experiments â€£ ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Inference speed</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#S5" title="In ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>FAIR at Meta 
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id1.1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>{haotang, weiyaowang, gleize, mdf}@meta.com</span></span></span>
</span></span></span>
<h1 class="ltx_title ltx_title_document">ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hao Tang 
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Weiyao Wang
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pierre Gleize
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Matt Feiszli
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Recovering camera poses from a set of images is a foundational task in 3D computer vision, which powers key applications such as 3D scene/object reconstructions. Classic methods often depend on feature correspondence, such as keypoints, which require the input images to have large overlap and small viewpoint changes. Such requirements present considerable challenges in scenarios with sparse views. Recent data-driven approaches aim to directly output camera poses, either through regressing the 6DoF camera poses or formulating rotation as a probability distribution. However, each approach has its limitations. On one hand, directly regressing the camera poses can be ill-posed, since it assumes a single mode, which is not true under symmetry and leads to sub-optimal solutions. On the other hand, probabilistic approaches are capable of modeling the symmetry ambiguity, yet they sample the entire space of rotation uniformly by brute-force. This leads to an inevitable trade-off between high sample density, which improves model precision, and sample efficiency that determines the runtime. In this paper, we propose ADen to unify the two frameworks by employing a generator and a discriminator: the generator is trained to output multiple hypotheses of 6DoF camera pose to represent a distribution and handle multi-mode ambiguity, and the discriminator is trained to identify the hypothesis that best explains the data. This allows ADen to combine the best of both worlds, achieving substantially higher precision as well as lower runtime than previous methods in empirical evaluations.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Understanding 3D structure from 2D image observations of objects or scenes is an important task in computer vision. Recent advances in Neural Radiance Field (NeRF)Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib26" title="">26</a>]</cite> and Gaussian SplattingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib20" title="">20</a>]</cite> enable high quality 3D reconstruction and novel view synthesis, initially from densely posed images. At the core of these methods, Structure-from-Motion (SfM) plays an important role to extract camera poses from the input images.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Motivated by key real-world applications such as online marketplaces and casual captures by everyday usersÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib43" title="">43</a>]</cite>, there is also growing interest in bringing these methods to sparse-view imagesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib42" title="">42</a>]</cite>, where only a handful of images (<em class="ltx_emph ltx_font_italic" id="S1.p2.1.1">e.g</em>.<span class="ltx_text" id="S1.p2.1.2"></span> 3 to 5) are available, each covers a different viewpoint. Similar to their dense-view counterparts, these methods often assume known input camera poses; yet in practice, the default geometric-based SfM pipeline may fail due to minimal view overlap in the context of sparse-view images. This highlights the need for sparse pose estimations and inspires a new stream of research taking data-driven approach that learns to predict poses from large-scale object-centric dataset with superior performance than geometric-based approachÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib27" title="">27</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="371" id="S1.F1.g1" src="x1.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.3.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text ltx_font_bold" id="S1.F1.4.2" style="font-size:90%;">Ambiguity in wide baseline images<span class="ltx_text ltx_font_medium" id="S1.F1.4.2.1">. Implicit-PDF/RelPose models rotation as a probability distribution using an energy-based method, which requires evaluating densely sampled rotation hypotheses. To achieve high accuracy, RelPose requires assessing 500k rotations for each image pair, incurring significant computational costs. In contrast, ADen outputs 500 high accuracy hypotheses directly, avoiding the constraints imposed by grid resolution. Filled circles are samples while unfilled circles are the ground truth relative rotation.
</span></span></figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.2">The data-driven approaches for recovering relative camera poses from sparse-view images can be broadly classified into two categories. One approach is to directly regress the 6DoF camera parameters rotation <math alttext="R" class="ltx_Math" display="inline" id="S1.p3.1.m1.1"><semantics id="S1.p3.1.m1.1a"><mi id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><ci id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1">ğ‘…</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">R</annotation><annotation encoding="application/x-llamapun" id="S1.p3.1.m1.1d">italic_R</annotation></semantics></math> and translation <math alttext="T" class="ltx_Math" display="inline" id="S1.p3.2.m2.1"><semantics id="S1.p3.2.m2.1a"><mi id="S1.p3.2.m2.1.1" xref="S1.p3.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S1.p3.2.m2.1b"><ci id="S1.p3.2.m2.1.1.cmml" xref="S1.p3.2.m2.1.1">ğ‘‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.2.m2.1c">T</annotation><annotation encoding="application/x-llamapun" id="S1.p3.2.m2.1d">italic_T</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib35" title="">35</a>]</cite>. However, in the sparse view setting, ambiguity arises, parituclarly for objects or scenes with symmetry (<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#S1.F1" title="In 1 Introduction â€£ ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">1</span></a>). Pose regression assumes a single mode in the data, which can result in suboptimal solutions when trained with data exhibiting a multi-mode distribution.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">A different approach is to model the rotation as a probability distribution <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib43" title="">43</a>]</cite>. Implicit-PDF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib27" title="">27</a>]</cite> first introduces a method to predict arbitrary, non-parametric probability distributions over the rotation manifold to address the symmetry issue. It densely samples rotation hypotheses from SO(3), and predicts the probability for each given image features. It naturally accounts for the uncertainty in the symmetric case, allowing the model to output multiple modes, thus improving accuracy over pose regression.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">While powerful, the primary drawback of this brute-force energy-based approach lies in the requirement to densely sample from the entire parameter space. To achieve high accuracy, a dense grid must be sampled from the parameter space. For example, RelPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib43" title="">43</a>]</cite> needs to sample 500,000 rotation matrices at inference time to generate a dense enough grid for good accuracy, in particular at lower error thresholds. However, evaluating 500,000 rotation hypotheses for one pair of images is computationally expensive. Furthermore, this method suffers from the curse of dimensionality: it is only practical in low dimensions (<em class="ltx_emph ltx_font_italic" id="S1.p5.1.1">e.g</em>.<span class="ltx_text" id="S1.p5.1.2"></span> SO(3) is three dimensional) and becomes prohibitively expensive in higher dimensions. Moving from 3 to 6 dimensions to jointly model rotation and translationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib27" title="">27</a>]</cite> without reducing sampling granularity implies a 250-billion-sized grid, yet reducing the sampling granularity implies inferior results.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">How do we benefit from both approaches? Our key observation is that using uniform grid to represent pose distributions is inefficient. In real-world, the distribution of poses is highly skewed, with a few isolated modes dominating the distribution. In other words, the real-world distribution of poses sit in between the regression (one mode)Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib35" title="">35</a>]</cite> and the uniform distributionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib43" title="">43</a>]</cite>. Based on this observation,
we focus on a generator-discriminator framework: given 2 or more images, the generator learns to produce samples from the conditional distribution of relative poses, while the discriminator ranks them. We find that this requires only a few hundred samples to cover all possible modes of the distribution. The adaptive nature also eliminates any fundamental lower bound on accuracy imposed by a grid resolution (either a fixed spacing for fixed grids, or expected spacing for random grids).
</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.2">We name this work ADen; as shown in <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#S1.F1" title="In 1 Introduction â€£ ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">1</span></a>, ADen only needs 500 samples to outperform methods that sample 500K locations at inference time; ADen is not constrained by any grid resolution and can output samples arbitrarily close to the true mode. The generated samples clearly learn to follow the multi-modal distribution, capturing the uncertainty of poses. Importantly, by eliminating the need for dense sampling from the parameter space, ADen is not limited to model rotation alone; the generator easily outputs joint rotation and translation [<math alttext="\mathbf{R}" class="ltx_Math" display="inline" id="S1.p7.1.m1.1"><semantics id="S1.p7.1.m1.1a"><mi id="S1.p7.1.m1.1.1" xref="S1.p7.1.m1.1.1.cmml">ğ‘</mi><annotation-xml encoding="MathML-Content" id="S1.p7.1.m1.1b"><ci id="S1.p7.1.m1.1.1.cmml" xref="S1.p7.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p7.1.m1.1c">\mathbf{R}</annotation><annotation encoding="application/x-llamapun" id="S1.p7.1.m1.1d">bold_R</annotation></semantics></math>, <math alttext="\mathbf{t}" class="ltx_Math" display="inline" id="S1.p7.2.m2.1"><semantics id="S1.p7.2.m2.1a"><mi id="S1.p7.2.m2.1.1" xref="S1.p7.2.m2.1.1.cmml">ğ­</mi><annotation-xml encoding="MathML-Content" id="S1.p7.2.m2.1b"><ci id="S1.p7.2.m2.1.1.cmml" xref="S1.p7.2.m2.1.1">ğ­</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p7.2.m2.1c">\mathbf{t}</annotation><annotation encoding="application/x-llamapun" id="S1.p7.2.m2.1d">bold_t</annotation></semantics></math>] pairs simply by predicting particles in the product space, <span class="ltx_text ltx_font_italic" id="S1.p7.2.1">without</span> increasing their number.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">In summary, our contributions are as follows.</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We propose ADen, a method for learning and sampling from the conditional distribution of relative pose from images using an efficient, adaptive generator-discriminator framework.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">ADen extends naturally to high-dimensional spaces without requiring exhaustive sampling of the entire space; it adapts to the complexity of the distribution and not the ambient space.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Experiments show ADen outperforms SoTA methods by a large margin, especially at low error thresholds. Moreover, ADen runs much faster than previous methods, achieving real-time inference speed.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Structure-from-Motion (SfM)</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">SfM aims at recovering 3D geometry and camera poses from multi-view images set. This classic problems has been extensively studied in the past <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib29" title="">29</a>]</cite>, which typically includes computing image features (typically key points) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib23" title="">23</a>]</cite>, establishing feature correspondence <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib24" title="">24</a>]</cite>, computing camera poses using five-point or eight-point algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib21" title="">21</a>]</cite> with RANSAC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib11" title="">11</a>]</cite>, and verifying epipolar geometry with bundle adjustments <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib37" title="">37</a>]</cite>. Each component in this pipeline highly depends their previous steps and needs careful tuning to be robust to scale to hundreds or thousands of images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib32" title="">32</a>]</cite>. Among these algorithms, COLMAP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib34" title="">34</a>]</cite> is an open-source implementation of this entire pipeline and has been widely used by the community.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Recent progress in deep neural nets and large-scale image datasets, various methods have been proposed to improve the performance of different components of the SfM pipeline with better feature descriptors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib13" title="">13</a>]</cite>, improved feature matching <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib33" title="">33</a>]</cite>. Although SfM works well with multiple views, it often fails in the wide baseline setting , due to insufficient overlap between images, occlusion and failure to establish correspondence.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Data-driven pose estimation</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The key step in the above-mentioned SfM pipeline is to establish correspondence between images. However, given only sparse views or wide baselines, it struggles to find matches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib7" title="">7</a>]</cite>. As the result of advances in network architecture and availability of large-scale dataset with posed images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib9" title="">9</a>]</cite>, learning-based methods has been proposed recently to directly estimate camera poses between images using a top-down approach. Unlike previous bottom-up approach, PoseNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib19" title="">19</a>]</cite> uses a network to directly regress the 6DoF camera parameters from a single RGB image. Implicit-PDF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib27" title="">27</a>]</cite> is the first to propose a new framework for single image pose estimation which represents the rotation distribution implicitly, with a neural network to assign probability given input images and a candidate pose and demonstrated strong performance in handling symmetry. RelPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib43" title="">43</a>]</cite> finds the widespread presence of symmetry in real-world dataset under sparse-view setting and extends the Implicit-PDF framework to estimate camera rotations based on intial pairwise prediction. RelPose++ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib22" title="">22</a>]</cite> further extends RelPose by adding a transformer to process multi-view images jointly and also report camera translation by defining a new camera coordinate system. SparsePose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib35" title="">35</a>]</cite> proposes to learn an iterative refinement step to regress initial pose estimation. PoseDiff <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib41" title="">41</a>]</cite> proposes to formulate the SfM problem as a probabilistic diffusion framework which mainly mirrors the iterative procedure of bundle adjustment.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Similar to PoseNet and SparsePose, our approach involves the direct regression of poses to the ground truth. However, we distinguish our method by employing a generator that produces multiple pose hypotheses. Our strategy differs from the conventional approach of regressing all predictions to a single ground truth. Instead, we utilize losses designed to guide the mode towards the ground truth, leveraging the diversity provided by the generator.
This approach enables the model to explore various modes in the dataset without being constrained to learn a single mode. Unlike Implicit-PDF and RelPose which require evaluating hundreds of thousands of pose hypotheses, ADen only generates a few hundred hypotheses and uses a discriminator to pick the best one thus can achieve real-time inference.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Generative learning and contrastive learning</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Modeling pose distribution using generative models has been successful recently <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib5" title="">5</a>]</cite>. GANs<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib14" title="">14</a>]</cite> are generative models trained adversarially. Similar to GANs, we do use a generator and a discriminator, but with a shared backbone (as opposed to having two fully independent models). Unlike GANs, our objective is not adversarial. The discriminator is trained to score various samples, not competing with the generator.
The discriminator is trained using the contrastive loss <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib6" title="">6</a>]</cite> to model the pose distribution, by rewarding the positive samples and penalizing negative samples. Unlike previous methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib6" title="">6</a>]</cite>, we do not aim to learn visual representations but to score samples from a distribution.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="303" id="S3.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.5.2.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text ltx_font_bold" id="S3.F2.2.1" style="font-size:90%;">ADen overview.<span class="ltx_text ltx_font_medium" id="S3.F2.2.1.1"> ADen is a novel method for recovering camera poses from sparse-view RGB images. ADen starts by extracting per-image features using the ResNet backbone, then uses a transformer to fuse features from all images and propagate information globally. ADen predicts a non-uniform distribution over camera poses for each image by first applying a pose generator head on fused features to produce a support set of <math alttext="M" class="ltx_Math" display="inline" id="S3.F2.2.1.1.m1.1"><semantics id="S3.F2.2.1.1.m1.1b"><mi id="S3.F2.2.1.1.m1.1.1" xref="S3.F2.2.1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.F2.2.1.1.m1.1c"><ci id="S3.F2.2.1.1.m1.1.1.cmml" xref="S3.F2.2.1.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.2.1.1.m1.1d">M</annotation><annotation encoding="application/x-llamapun" id="S3.F2.2.1.1.m1.1e">italic_M</annotation></semantics></math> camera poses, then using a pose discriminator with fused features to predict probability on each generated pose.
</span></span></figcaption>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.12">Given a set of N sparse-view images of an object {<math alttext="\mathbf{I}_{1}" class="ltx_Math" display="inline" id="S3.p1.1.m1.1"><semantics id="S3.p1.1.m1.1a"><msub id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml"><mi id="S3.p1.1.m1.1.1.2" xref="S3.p1.1.m1.1.1.2.cmml">ğˆ</mi><mn id="S3.p1.1.m1.1.1.3" xref="S3.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><apply id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p1.1.m1.1.1.1.cmml" xref="S3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.p1.1.m1.1.1.2.cmml" xref="S3.p1.1.m1.1.1.2">ğˆ</ci><cn id="S3.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">\mathbf{I}_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.1.m1.1d">bold_I start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>, â€¦, <math alttext="\mathbf{I}_{N}" class="ltx_Math" display="inline" id="S3.p1.2.m2.1"><semantics id="S3.p1.2.m2.1a"><msub id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml"><mi id="S3.p1.2.m2.1.1.2" xref="S3.p1.2.m2.1.1.2.cmml">ğˆ</mi><mi id="S3.p1.2.m2.1.1.3" xref="S3.p1.2.m2.1.1.3.cmml">N</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><apply id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p1.2.m2.1.1.1.cmml" xref="S3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.p1.2.m2.1.1.2.cmml" xref="S3.p1.2.m2.1.1.2">ğˆ</ci><ci id="S3.p1.2.m2.1.1.3.cmml" xref="S3.p1.2.m2.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">\mathbf{I}_{N}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.2.m2.1d">bold_I start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT</annotation></semantics></math>}, ADen aims to recover the camera extrinsics {[<math alttext="\mathbf{R}_{1}" class="ltx_Math" display="inline" id="S3.p1.3.m3.1"><semantics id="S3.p1.3.m3.1a"><msub id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml"><mi id="S3.p1.3.m3.1.1.2" xref="S3.p1.3.m3.1.1.2.cmml">ğ‘</mi><mn id="S3.p1.3.m3.1.1.3" xref="S3.p1.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><apply id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p1.3.m3.1.1.1.cmml" xref="S3.p1.3.m3.1.1">subscript</csymbol><ci id="S3.p1.3.m3.1.1.2.cmml" xref="S3.p1.3.m3.1.1.2">ğ‘</ci><cn id="S3.p1.3.m3.1.1.3.cmml" type="integer" xref="S3.p1.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">\mathbf{R}_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.3.m3.1d">bold_R start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="\mathbf{t}_{1}" class="ltx_Math" display="inline" id="S3.p1.4.m4.1"><semantics id="S3.p1.4.m4.1a"><msub id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml"><mi id="S3.p1.4.m4.1.1.2" xref="S3.p1.4.m4.1.1.2.cmml">ğ­</mi><mn id="S3.p1.4.m4.1.1.3" xref="S3.p1.4.m4.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><apply id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.p1.4.m4.1.1.1.cmml" xref="S3.p1.4.m4.1.1">subscript</csymbol><ci id="S3.p1.4.m4.1.1.2.cmml" xref="S3.p1.4.m4.1.1.2">ğ­</ci><cn id="S3.p1.4.m4.1.1.3.cmml" type="integer" xref="S3.p1.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">\mathbf{t}_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.4.m4.1d">bold_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>], â€¦, [<math alttext="\mathbf{R}_{N}" class="ltx_Math" display="inline" id="S3.p1.5.m5.1"><semantics id="S3.p1.5.m5.1a"><msub id="S3.p1.5.m5.1.1" xref="S3.p1.5.m5.1.1.cmml"><mi id="S3.p1.5.m5.1.1.2" xref="S3.p1.5.m5.1.1.2.cmml">ğ‘</mi><mi id="S3.p1.5.m5.1.1.3" xref="S3.p1.5.m5.1.1.3.cmml">N</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.5.m5.1b"><apply id="S3.p1.5.m5.1.1.cmml" xref="S3.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.p1.5.m5.1.1.1.cmml" xref="S3.p1.5.m5.1.1">subscript</csymbol><ci id="S3.p1.5.m5.1.1.2.cmml" xref="S3.p1.5.m5.1.1.2">ğ‘</ci><ci id="S3.p1.5.m5.1.1.3.cmml" xref="S3.p1.5.m5.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.5.m5.1c">\mathbf{R}_{N}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.5.m5.1d">bold_R start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="\mathbf{t}_{N}" class="ltx_Math" display="inline" id="S3.p1.6.m6.1"><semantics id="S3.p1.6.m6.1a"><msub id="S3.p1.6.m6.1.1" xref="S3.p1.6.m6.1.1.cmml"><mi id="S3.p1.6.m6.1.1.2" xref="S3.p1.6.m6.1.1.2.cmml">ğ­</mi><mi id="S3.p1.6.m6.1.1.3" xref="S3.p1.6.m6.1.1.3.cmml">N</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.6.m6.1b"><apply id="S3.p1.6.m6.1.1.cmml" xref="S3.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.p1.6.m6.1.1.1.cmml" xref="S3.p1.6.m6.1.1">subscript</csymbol><ci id="S3.p1.6.m6.1.1.2.cmml" xref="S3.p1.6.m6.1.1.2">ğ­</ci><ci id="S3.p1.6.m6.1.1.3.cmml" xref="S3.p1.6.m6.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.6.m6.1c">\mathbf{t}_{N}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.6.m6.1d">bold_t start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT</annotation></semantics></math>]}, where <math alttext="\mathbf{R}_{i}\in\mathrm{SO}(3)" class="ltx_Math" display="inline" id="S3.p1.7.m7.1"><semantics id="S3.p1.7.m7.1a"><mrow id="S3.p1.7.m7.1.2" xref="S3.p1.7.m7.1.2.cmml"><msub id="S3.p1.7.m7.1.2.2" xref="S3.p1.7.m7.1.2.2.cmml"><mi id="S3.p1.7.m7.1.2.2.2" xref="S3.p1.7.m7.1.2.2.2.cmml">ğ‘</mi><mi id="S3.p1.7.m7.1.2.2.3" xref="S3.p1.7.m7.1.2.2.3.cmml">i</mi></msub><mo id="S3.p1.7.m7.1.2.1" xref="S3.p1.7.m7.1.2.1.cmml">âˆˆ</mo><mrow id="S3.p1.7.m7.1.2.3" xref="S3.p1.7.m7.1.2.3.cmml"><mi id="S3.p1.7.m7.1.2.3.2" xref="S3.p1.7.m7.1.2.3.2.cmml">SO</mi><mo id="S3.p1.7.m7.1.2.3.1" xref="S3.p1.7.m7.1.2.3.1.cmml">â¢</mo><mrow id="S3.p1.7.m7.1.2.3.3.2" xref="S3.p1.7.m7.1.2.3.cmml"><mo id="S3.p1.7.m7.1.2.3.3.2.1" stretchy="false" xref="S3.p1.7.m7.1.2.3.cmml">(</mo><mn id="S3.p1.7.m7.1.1" xref="S3.p1.7.m7.1.1.cmml">3</mn><mo id="S3.p1.7.m7.1.2.3.3.2.2" stretchy="false" xref="S3.p1.7.m7.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.7.m7.1b"><apply id="S3.p1.7.m7.1.2.cmml" xref="S3.p1.7.m7.1.2"><in id="S3.p1.7.m7.1.2.1.cmml" xref="S3.p1.7.m7.1.2.1"></in><apply id="S3.p1.7.m7.1.2.2.cmml" xref="S3.p1.7.m7.1.2.2"><csymbol cd="ambiguous" id="S3.p1.7.m7.1.2.2.1.cmml" xref="S3.p1.7.m7.1.2.2">subscript</csymbol><ci id="S3.p1.7.m7.1.2.2.2.cmml" xref="S3.p1.7.m7.1.2.2.2">ğ‘</ci><ci id="S3.p1.7.m7.1.2.2.3.cmml" xref="S3.p1.7.m7.1.2.2.3">ğ‘–</ci></apply><apply id="S3.p1.7.m7.1.2.3.cmml" xref="S3.p1.7.m7.1.2.3"><times id="S3.p1.7.m7.1.2.3.1.cmml" xref="S3.p1.7.m7.1.2.3.1"></times><ci id="S3.p1.7.m7.1.2.3.2.cmml" xref="S3.p1.7.m7.1.2.3.2">SO</ci><cn id="S3.p1.7.m7.1.1.cmml" type="integer" xref="S3.p1.7.m7.1.1">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.7.m7.1c">\mathbf{R}_{i}\in\mathrm{SO}(3)</annotation><annotation encoding="application/x-llamapun" id="S3.p1.7.m7.1d">bold_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âˆˆ roman_SO ( 3 )</annotation></semantics></math> is the rotation and <math alttext="\mathbf{t}_{i}\in\mathbb{R}^{3}" class="ltx_Math" display="inline" id="S3.p1.8.m8.1"><semantics id="S3.p1.8.m8.1a"><mrow id="S3.p1.8.m8.1.1" xref="S3.p1.8.m8.1.1.cmml"><msub id="S3.p1.8.m8.1.1.2" xref="S3.p1.8.m8.1.1.2.cmml"><mi id="S3.p1.8.m8.1.1.2.2" xref="S3.p1.8.m8.1.1.2.2.cmml">ğ­</mi><mi id="S3.p1.8.m8.1.1.2.3" xref="S3.p1.8.m8.1.1.2.3.cmml">i</mi></msub><mo id="S3.p1.8.m8.1.1.1" xref="S3.p1.8.m8.1.1.1.cmml">âˆˆ</mo><msup id="S3.p1.8.m8.1.1.3" xref="S3.p1.8.m8.1.1.3.cmml"><mi id="S3.p1.8.m8.1.1.3.2" xref="S3.p1.8.m8.1.1.3.2.cmml">â„</mi><mn id="S3.p1.8.m8.1.1.3.3" xref="S3.p1.8.m8.1.1.3.3.cmml">3</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.8.m8.1b"><apply id="S3.p1.8.m8.1.1.cmml" xref="S3.p1.8.m8.1.1"><in id="S3.p1.8.m8.1.1.1.cmml" xref="S3.p1.8.m8.1.1.1"></in><apply id="S3.p1.8.m8.1.1.2.cmml" xref="S3.p1.8.m8.1.1.2"><csymbol cd="ambiguous" id="S3.p1.8.m8.1.1.2.1.cmml" xref="S3.p1.8.m8.1.1.2">subscript</csymbol><ci id="S3.p1.8.m8.1.1.2.2.cmml" xref="S3.p1.8.m8.1.1.2.2">ğ­</ci><ci id="S3.p1.8.m8.1.1.2.3.cmml" xref="S3.p1.8.m8.1.1.2.3">ğ‘–</ci></apply><apply id="S3.p1.8.m8.1.1.3.cmml" xref="S3.p1.8.m8.1.1.3"><csymbol cd="ambiguous" id="S3.p1.8.m8.1.1.3.1.cmml" xref="S3.p1.8.m8.1.1.3">superscript</csymbol><ci id="S3.p1.8.m8.1.1.3.2.cmml" xref="S3.p1.8.m8.1.1.3.2">â„</ci><cn id="S3.p1.8.m8.1.1.3.3.cmml" type="integer" xref="S3.p1.8.m8.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.8.m8.1c">\mathbf{t}_{i}\in\mathbb{R}^{3}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.8.m8.1d">bold_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math> is the translation for image <math alttext="\mathbf{I}_{i}" class="ltx_Math" display="inline" id="S3.p1.9.m9.1"><semantics id="S3.p1.9.m9.1a"><msub id="S3.p1.9.m9.1.1" xref="S3.p1.9.m9.1.1.cmml"><mi id="S3.p1.9.m9.1.1.2" xref="S3.p1.9.m9.1.1.2.cmml">ğˆ</mi><mi id="S3.p1.9.m9.1.1.3" xref="S3.p1.9.m9.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.9.m9.1b"><apply id="S3.p1.9.m9.1.1.cmml" xref="S3.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S3.p1.9.m9.1.1.1.cmml" xref="S3.p1.9.m9.1.1">subscript</csymbol><ci id="S3.p1.9.m9.1.1.2.cmml" xref="S3.p1.9.m9.1.1.2">ğˆ</ci><ci id="S3.p1.9.m9.1.1.3.cmml" xref="S3.p1.9.m9.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.9.m9.1c">\mathbf{I}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.9.m9.1d">bold_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>.
The model is trained to first draw samples <math alttext="[\mathbf{R}_{n,k},\mathbf{t}_{n,k}]," class="ltx_Math" display="inline" id="S3.p1.10.m10.5"><semantics id="S3.p1.10.m10.5a"><mrow id="S3.p1.10.m10.5.5.1"><mrow id="S3.p1.10.m10.5.5.1.1.2" xref="S3.p1.10.m10.5.5.1.1.3.cmml"><mo id="S3.p1.10.m10.5.5.1.1.2.3" stretchy="false" xref="S3.p1.10.m10.5.5.1.1.3.cmml">[</mo><msub id="S3.p1.10.m10.5.5.1.1.1.1" xref="S3.p1.10.m10.5.5.1.1.1.1.cmml"><mi id="S3.p1.10.m10.5.5.1.1.1.1.2" xref="S3.p1.10.m10.5.5.1.1.1.1.2.cmml">ğ‘</mi><mrow id="S3.p1.10.m10.2.2.2.4" xref="S3.p1.10.m10.2.2.2.3.cmml"><mi id="S3.p1.10.m10.1.1.1.1" xref="S3.p1.10.m10.1.1.1.1.cmml">n</mi><mo id="S3.p1.10.m10.2.2.2.4.1" xref="S3.p1.10.m10.2.2.2.3.cmml">,</mo><mi id="S3.p1.10.m10.2.2.2.2" xref="S3.p1.10.m10.2.2.2.2.cmml">k</mi></mrow></msub><mo id="S3.p1.10.m10.5.5.1.1.2.4" xref="S3.p1.10.m10.5.5.1.1.3.cmml">,</mo><msub id="S3.p1.10.m10.5.5.1.1.2.2" xref="S3.p1.10.m10.5.5.1.1.2.2.cmml"><mi id="S3.p1.10.m10.5.5.1.1.2.2.2" xref="S3.p1.10.m10.5.5.1.1.2.2.2.cmml">ğ­</mi><mrow id="S3.p1.10.m10.4.4.2.4" xref="S3.p1.10.m10.4.4.2.3.cmml"><mi id="S3.p1.10.m10.3.3.1.1" xref="S3.p1.10.m10.3.3.1.1.cmml">n</mi><mo id="S3.p1.10.m10.4.4.2.4.1" xref="S3.p1.10.m10.4.4.2.3.cmml">,</mo><mi id="S3.p1.10.m10.4.4.2.2" xref="S3.p1.10.m10.4.4.2.2.cmml">k</mi></mrow></msub><mo id="S3.p1.10.m10.5.5.1.1.2.5" stretchy="false" xref="S3.p1.10.m10.5.5.1.1.3.cmml">]</mo></mrow><mo id="S3.p1.10.m10.5.5.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.10.m10.5b"><interval closure="closed" id="S3.p1.10.m10.5.5.1.1.3.cmml" xref="S3.p1.10.m10.5.5.1.1.2"><apply id="S3.p1.10.m10.5.5.1.1.1.1.cmml" xref="S3.p1.10.m10.5.5.1.1.1.1"><csymbol cd="ambiguous" id="S3.p1.10.m10.5.5.1.1.1.1.1.cmml" xref="S3.p1.10.m10.5.5.1.1.1.1">subscript</csymbol><ci id="S3.p1.10.m10.5.5.1.1.1.1.2.cmml" xref="S3.p1.10.m10.5.5.1.1.1.1.2">ğ‘</ci><list id="S3.p1.10.m10.2.2.2.3.cmml" xref="S3.p1.10.m10.2.2.2.4"><ci id="S3.p1.10.m10.1.1.1.1.cmml" xref="S3.p1.10.m10.1.1.1.1">ğ‘›</ci><ci id="S3.p1.10.m10.2.2.2.2.cmml" xref="S3.p1.10.m10.2.2.2.2">ğ‘˜</ci></list></apply><apply id="S3.p1.10.m10.5.5.1.1.2.2.cmml" xref="S3.p1.10.m10.5.5.1.1.2.2"><csymbol cd="ambiguous" id="S3.p1.10.m10.5.5.1.1.2.2.1.cmml" xref="S3.p1.10.m10.5.5.1.1.2.2">subscript</csymbol><ci id="S3.p1.10.m10.5.5.1.1.2.2.2.cmml" xref="S3.p1.10.m10.5.5.1.1.2.2.2">ğ­</ci><list id="S3.p1.10.m10.4.4.2.3.cmml" xref="S3.p1.10.m10.4.4.2.4"><ci id="S3.p1.10.m10.3.3.1.1.cmml" xref="S3.p1.10.m10.3.3.1.1">ğ‘›</ci><ci id="S3.p1.10.m10.4.4.2.2.cmml" xref="S3.p1.10.m10.4.4.2.2">ğ‘˜</ci></list></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.10.m10.5c">[\mathbf{R}_{n,k},\mathbf{t}_{n,k}],</annotation><annotation encoding="application/x-llamapun" id="S3.p1.10.m10.5d">[ bold_R start_POSTSUBSCRIPT italic_n , italic_k end_POSTSUBSCRIPT , bold_t start_POSTSUBSCRIPT italic_n , italic_k end_POSTSUBSCRIPT ] ,</annotation></semantics></math> from the distribution of all possible poses given the input images <math alttext="p(\mathbf{R}_{n},\mathbf{t}_{n}|\mathbf{I}_{1},...,\mathbf{I}_{N})" class="ltx_Math" display="inline" id="S3.p1.11.m11.3"><semantics id="S3.p1.11.m11.3a"><mrow id="S3.p1.11.m11.3.3" xref="S3.p1.11.m11.3.3.cmml"><mi id="S3.p1.11.m11.3.3.4" xref="S3.p1.11.m11.3.3.4.cmml">p</mi><mo id="S3.p1.11.m11.3.3.3" xref="S3.p1.11.m11.3.3.3.cmml">â¢</mo><mrow id="S3.p1.11.m11.3.3.2.2" xref="S3.p1.11.m11.3.3.2.3.cmml"><mo id="S3.p1.11.m11.3.3.2.2.3" stretchy="false" xref="S3.p1.11.m11.3.3.2.3.cmml">(</mo><msub id="S3.p1.11.m11.2.2.1.1.1" xref="S3.p1.11.m11.2.2.1.1.1.cmml"><mi id="S3.p1.11.m11.2.2.1.1.1.2" xref="S3.p1.11.m11.2.2.1.1.1.2.cmml">ğ‘</mi><mi id="S3.p1.11.m11.2.2.1.1.1.3" xref="S3.p1.11.m11.2.2.1.1.1.3.cmml">n</mi></msub><mo id="S3.p1.11.m11.3.3.2.2.4" xref="S3.p1.11.m11.3.3.2.3.cmml">,</mo><mrow id="S3.p1.11.m11.3.3.2.2.2" xref="S3.p1.11.m11.3.3.2.2.2.cmml"><msub id="S3.p1.11.m11.3.3.2.2.2.4" xref="S3.p1.11.m11.3.3.2.2.2.4.cmml"><mi id="S3.p1.11.m11.3.3.2.2.2.4.2" xref="S3.p1.11.m11.3.3.2.2.2.4.2.cmml">ğ­</mi><mi id="S3.p1.11.m11.3.3.2.2.2.4.3" xref="S3.p1.11.m11.3.3.2.2.2.4.3.cmml">n</mi></msub><mo fence="false" id="S3.p1.11.m11.3.3.2.2.2.3" xref="S3.p1.11.m11.3.3.2.2.2.3.cmml">|</mo><mrow id="S3.p1.11.m11.3.3.2.2.2.2.2" xref="S3.p1.11.m11.3.3.2.2.2.2.3.cmml"><msub id="S3.p1.11.m11.3.3.2.2.2.1.1.1" xref="S3.p1.11.m11.3.3.2.2.2.1.1.1.cmml"><mi id="S3.p1.11.m11.3.3.2.2.2.1.1.1.2" xref="S3.p1.11.m11.3.3.2.2.2.1.1.1.2.cmml">ğˆ</mi><mn id="S3.p1.11.m11.3.3.2.2.2.1.1.1.3" xref="S3.p1.11.m11.3.3.2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.p1.11.m11.3.3.2.2.2.2.2.3" xref="S3.p1.11.m11.3.3.2.2.2.2.3.cmml">,</mo><mi id="S3.p1.11.m11.1.1" mathvariant="normal" xref="S3.p1.11.m11.1.1.cmml">â€¦</mi><mo id="S3.p1.11.m11.3.3.2.2.2.2.2.4" xref="S3.p1.11.m11.3.3.2.2.2.2.3.cmml">,</mo><msub id="S3.p1.11.m11.3.3.2.2.2.2.2.2" xref="S3.p1.11.m11.3.3.2.2.2.2.2.2.cmml"><mi id="S3.p1.11.m11.3.3.2.2.2.2.2.2.2" xref="S3.p1.11.m11.3.3.2.2.2.2.2.2.2.cmml">ğˆ</mi><mi id="S3.p1.11.m11.3.3.2.2.2.2.2.2.3" xref="S3.p1.11.m11.3.3.2.2.2.2.2.2.3.cmml">N</mi></msub></mrow></mrow><mo id="S3.p1.11.m11.3.3.2.2.5" stretchy="false" xref="S3.p1.11.m11.3.3.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.11.m11.3b"><apply id="S3.p1.11.m11.3.3.cmml" xref="S3.p1.11.m11.3.3"><times id="S3.p1.11.m11.3.3.3.cmml" xref="S3.p1.11.m11.3.3.3"></times><ci id="S3.p1.11.m11.3.3.4.cmml" xref="S3.p1.11.m11.3.3.4">ğ‘</ci><interval closure="open" id="S3.p1.11.m11.3.3.2.3.cmml" xref="S3.p1.11.m11.3.3.2.2"><apply id="S3.p1.11.m11.2.2.1.1.1.cmml" xref="S3.p1.11.m11.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.p1.11.m11.2.2.1.1.1.1.cmml" xref="S3.p1.11.m11.2.2.1.1.1">subscript</csymbol><ci id="S3.p1.11.m11.2.2.1.1.1.2.cmml" xref="S3.p1.11.m11.2.2.1.1.1.2">ğ‘</ci><ci id="S3.p1.11.m11.2.2.1.1.1.3.cmml" xref="S3.p1.11.m11.2.2.1.1.1.3">ğ‘›</ci></apply><apply id="S3.p1.11.m11.3.3.2.2.2.cmml" xref="S3.p1.11.m11.3.3.2.2.2"><csymbol cd="latexml" id="S3.p1.11.m11.3.3.2.2.2.3.cmml" xref="S3.p1.11.m11.3.3.2.2.2.3">conditional</csymbol><apply id="S3.p1.11.m11.3.3.2.2.2.4.cmml" xref="S3.p1.11.m11.3.3.2.2.2.4"><csymbol cd="ambiguous" id="S3.p1.11.m11.3.3.2.2.2.4.1.cmml" xref="S3.p1.11.m11.3.3.2.2.2.4">subscript</csymbol><ci id="S3.p1.11.m11.3.3.2.2.2.4.2.cmml" xref="S3.p1.11.m11.3.3.2.2.2.4.2">ğ­</ci><ci id="S3.p1.11.m11.3.3.2.2.2.4.3.cmml" xref="S3.p1.11.m11.3.3.2.2.2.4.3">ğ‘›</ci></apply><list id="S3.p1.11.m11.3.3.2.2.2.2.3.cmml" xref="S3.p1.11.m11.3.3.2.2.2.2.2"><apply id="S3.p1.11.m11.3.3.2.2.2.1.1.1.cmml" xref="S3.p1.11.m11.3.3.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.p1.11.m11.3.3.2.2.2.1.1.1.1.cmml" xref="S3.p1.11.m11.3.3.2.2.2.1.1.1">subscript</csymbol><ci id="S3.p1.11.m11.3.3.2.2.2.1.1.1.2.cmml" xref="S3.p1.11.m11.3.3.2.2.2.1.1.1.2">ğˆ</ci><cn id="S3.p1.11.m11.3.3.2.2.2.1.1.1.3.cmml" type="integer" xref="S3.p1.11.m11.3.3.2.2.2.1.1.1.3">1</cn></apply><ci id="S3.p1.11.m11.1.1.cmml" xref="S3.p1.11.m11.1.1">â€¦</ci><apply id="S3.p1.11.m11.3.3.2.2.2.2.2.2.cmml" xref="S3.p1.11.m11.3.3.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.p1.11.m11.3.3.2.2.2.2.2.2.1.cmml" xref="S3.p1.11.m11.3.3.2.2.2.2.2.2">subscript</csymbol><ci id="S3.p1.11.m11.3.3.2.2.2.2.2.2.2.cmml" xref="S3.p1.11.m11.3.3.2.2.2.2.2.2.2">ğˆ</ci><ci id="S3.p1.11.m11.3.3.2.2.2.2.2.2.3.cmml" xref="S3.p1.11.m11.3.3.2.2.2.2.2.2.3">ğ‘</ci></apply></list></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.11.m11.3c">p(\mathbf{R}_{n},\mathbf{t}_{n}|\mathbf{I}_{1},...,\mathbf{I}_{N})</annotation><annotation encoding="application/x-llamapun" id="S3.p1.11.m11.3d">italic_p ( bold_R start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , bold_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT | bold_I start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , bold_I start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT )</annotation></semantics></math>, where n is the index for the image and k is the index for the sample. And subsequently, it scores these samples and selects the most probable one as its prediction <math alttext="[\hat{\mathbf{R}}_{n},\hat{\mathbf{t}}_{n}]" class="ltx_Math" display="inline" id="S3.p1.12.m12.2"><semantics id="S3.p1.12.m12.2a"><mrow id="S3.p1.12.m12.2.2.2" xref="S3.p1.12.m12.2.2.3.cmml"><mo id="S3.p1.12.m12.2.2.2.3" stretchy="false" xref="S3.p1.12.m12.2.2.3.cmml">[</mo><msub id="S3.p1.12.m12.1.1.1.1" xref="S3.p1.12.m12.1.1.1.1.cmml"><mover accent="true" id="S3.p1.12.m12.1.1.1.1.2" xref="S3.p1.12.m12.1.1.1.1.2.cmml"><mi id="S3.p1.12.m12.1.1.1.1.2.2" xref="S3.p1.12.m12.1.1.1.1.2.2.cmml">ğ‘</mi><mo id="S3.p1.12.m12.1.1.1.1.2.1" xref="S3.p1.12.m12.1.1.1.1.2.1.cmml">^</mo></mover><mi id="S3.p1.12.m12.1.1.1.1.3" xref="S3.p1.12.m12.1.1.1.1.3.cmml">n</mi></msub><mo id="S3.p1.12.m12.2.2.2.4" xref="S3.p1.12.m12.2.2.3.cmml">,</mo><msub id="S3.p1.12.m12.2.2.2.2" xref="S3.p1.12.m12.2.2.2.2.cmml"><mover accent="true" id="S3.p1.12.m12.2.2.2.2.2" xref="S3.p1.12.m12.2.2.2.2.2.cmml"><mi id="S3.p1.12.m12.2.2.2.2.2.2" xref="S3.p1.12.m12.2.2.2.2.2.2.cmml">ğ­</mi><mo id="S3.p1.12.m12.2.2.2.2.2.1" xref="S3.p1.12.m12.2.2.2.2.2.1.cmml">^</mo></mover><mi id="S3.p1.12.m12.2.2.2.2.3" xref="S3.p1.12.m12.2.2.2.2.3.cmml">n</mi></msub><mo id="S3.p1.12.m12.2.2.2.5" stretchy="false" xref="S3.p1.12.m12.2.2.3.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.12.m12.2b"><interval closure="closed" id="S3.p1.12.m12.2.2.3.cmml" xref="S3.p1.12.m12.2.2.2"><apply id="S3.p1.12.m12.1.1.1.1.cmml" xref="S3.p1.12.m12.1.1.1.1"><csymbol cd="ambiguous" id="S3.p1.12.m12.1.1.1.1.1.cmml" xref="S3.p1.12.m12.1.1.1.1">subscript</csymbol><apply id="S3.p1.12.m12.1.1.1.1.2.cmml" xref="S3.p1.12.m12.1.1.1.1.2"><ci id="S3.p1.12.m12.1.1.1.1.2.1.cmml" xref="S3.p1.12.m12.1.1.1.1.2.1">^</ci><ci id="S3.p1.12.m12.1.1.1.1.2.2.cmml" xref="S3.p1.12.m12.1.1.1.1.2.2">ğ‘</ci></apply><ci id="S3.p1.12.m12.1.1.1.1.3.cmml" xref="S3.p1.12.m12.1.1.1.1.3">ğ‘›</ci></apply><apply id="S3.p1.12.m12.2.2.2.2.cmml" xref="S3.p1.12.m12.2.2.2.2"><csymbol cd="ambiguous" id="S3.p1.12.m12.2.2.2.2.1.cmml" xref="S3.p1.12.m12.2.2.2.2">subscript</csymbol><apply id="S3.p1.12.m12.2.2.2.2.2.cmml" xref="S3.p1.12.m12.2.2.2.2.2"><ci id="S3.p1.12.m12.2.2.2.2.2.1.cmml" xref="S3.p1.12.m12.2.2.2.2.2.1">^</ci><ci id="S3.p1.12.m12.2.2.2.2.2.2.cmml" xref="S3.p1.12.m12.2.2.2.2.2.2">ğ­</ci></apply><ci id="S3.p1.12.m12.2.2.2.2.3.cmml" xref="S3.p1.12.m12.2.2.2.2.3">ğ‘›</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.12.m12.2c">[\hat{\mathbf{R}}_{n},\hat{\mathbf{t}}_{n}]</annotation><annotation encoding="application/x-llamapun" id="S3.p1.12.m12.2d">[ over^ start_ARG bold_R end_ARG start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , over^ start_ARG bold_t end_ARG start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ]</annotation></semantics></math> for each image.
Our proposed method consists of a shared feature backbone, and two heads: pose generator and pose discriminator (<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#S3.F2" title="In 3 Method â€£ ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>). We use the camera pose for the first frame as canonical reference to infer camera poses of other frames.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Multi-view feature extraction</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.6">ADen begins by extracting per-image features using ResNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib18" title="">18</a>]</cite>, denoted as <math alttext="\mathbf{F}_{i}=f(\mathbf{I}_{i})\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><msub id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml"><mi id="S3.SS1.p1.1.m1.1.1.3.2" xref="S3.SS1.p1.1.m1.1.1.3.2.cmml">ğ…</mi><mi id="S3.SS1.p1.1.m1.1.1.3.3" xref="S3.SS1.p1.1.m1.1.1.3.3.cmml">i</mi></msub><mo id="S3.SS1.p1.1.m1.1.1.4" xref="S3.SS1.p1.1.m1.1.1.4.cmml">=</mo><mrow id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.1.3.cmml">f</mi><mo id="S3.SS1.p1.1.m1.1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.1.2.cmml">â¢</mo><mrow id="S3.SS1.p1.1.m1.1.1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.cmml"><mo id="S3.SS1.p1.1.m1.1.1.1.1.1.2" stretchy="false" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS1.p1.1.m1.1.1.1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.1.1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.2.cmml">ğˆ</mi><mi id="S3.SS1.p1.1.m1.1.1.1.1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS1.p1.1.m1.1.1.1.1.1.3" stretchy="false" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p1.1.m1.1.1.5" xref="S3.SS1.p1.1.m1.1.1.5.cmml">âˆˆ</mo><msup id="S3.SS1.p1.1.m1.1.1.6" xref="S3.SS1.p1.1.m1.1.1.6.cmml"><mi id="S3.SS1.p1.1.m1.1.1.6.2" xref="S3.SS1.p1.1.m1.1.1.6.2.cmml">â„</mi><mi id="S3.SS1.p1.1.m1.1.1.6.3" xref="S3.SS1.p1.1.m1.1.1.6.3.cmml">d</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><and id="S3.SS1.p1.1.m1.1.1a.cmml" xref="S3.SS1.p1.1.m1.1.1"></and><apply id="S3.SS1.p1.1.m1.1.1b.cmml" xref="S3.SS1.p1.1.m1.1.1"><eq id="S3.SS1.p1.1.m1.1.1.4.cmml" xref="S3.SS1.p1.1.m1.1.1.4"></eq><apply id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS1.p1.1.m1.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS1.p1.1.m1.1.1.3.2">ğ…</ci><ci id="S3.SS1.p1.1.m1.1.1.3.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3">ğ‘–</ci></apply><apply id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"><times id="S3.SS1.p1.1.m1.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.1.2"></times><ci id="S3.SS1.p1.1.m1.1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.1.3">ğ‘“</ci><apply id="S3.SS1.p1.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.2">ğˆ</ci><ci id="S3.SS1.p1.1.m1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.3">ğ‘–</ci></apply></apply></apply><apply id="S3.SS1.p1.1.m1.1.1c.cmml" xref="S3.SS1.p1.1.m1.1.1"><in id="S3.SS1.p1.1.m1.1.1.5.cmml" xref="S3.SS1.p1.1.m1.1.1.5"></in><share href="https://arxiv.org/html/2408.09042v1#S3.SS1.p1.1.m1.1.1.1.cmml" id="S3.SS1.p1.1.m1.1.1d.cmml" xref="S3.SS1.p1.1.m1.1.1"></share><apply id="S3.SS1.p1.1.m1.1.1.6.cmml" xref="S3.SS1.p1.1.m1.1.1.6"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.6.1.cmml" xref="S3.SS1.p1.1.m1.1.1.6">superscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.6.2.cmml" xref="S3.SS1.p1.1.m1.1.1.6.2">â„</ci><ci id="S3.SS1.p1.1.m1.1.1.6.3.cmml" xref="S3.SS1.p1.1.m1.1.1.6.3">ğ‘‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\mathbf{F}_{i}=f(\mathbf{I}_{i})\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">bold_F start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_f ( bold_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, where <math alttext="d" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">d</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">italic_d</annotation></semantics></math> is the feature space dimension. To reason relative poses across multiple input views, ADen concatenates the features and uses transformersÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib40" title="">40</a>]</cite> <math alttext="t" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><mi id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">italic_t</annotation></semantics></math> to fuse them: <math alttext="(\mathbf{G_{1}},...,\mathbf{G}_{N})=t(\mathbf{F_{1}},...,\mathbf{F_{N}})" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4.6"><semantics id="S3.SS1.p1.4.m4.6a"><mrow id="S3.SS1.p1.4.m4.6.6" xref="S3.SS1.p1.4.m4.6.6.cmml"><mrow id="S3.SS1.p1.4.m4.4.4.2.2" xref="S3.SS1.p1.4.m4.4.4.2.3.cmml"><mo id="S3.SS1.p1.4.m4.4.4.2.2.3" stretchy="false" xref="S3.SS1.p1.4.m4.4.4.2.3.cmml">(</mo><msub id="S3.SS1.p1.4.m4.3.3.1.1.1" xref="S3.SS1.p1.4.m4.3.3.1.1.1.cmml"><mi id="S3.SS1.p1.4.m4.3.3.1.1.1.2" xref="S3.SS1.p1.4.m4.3.3.1.1.1.2.cmml">ğ†</mi><mn id="S3.SS1.p1.4.m4.3.3.1.1.1.3" xref="S3.SS1.p1.4.m4.3.3.1.1.1.3.cmml">ğŸ</mn></msub><mo id="S3.SS1.p1.4.m4.4.4.2.2.4" xref="S3.SS1.p1.4.m4.4.4.2.3.cmml">,</mo><mi id="S3.SS1.p1.4.m4.1.1" mathvariant="normal" xref="S3.SS1.p1.4.m4.1.1.cmml">â€¦</mi><mo id="S3.SS1.p1.4.m4.4.4.2.2.5" xref="S3.SS1.p1.4.m4.4.4.2.3.cmml">,</mo><msub id="S3.SS1.p1.4.m4.4.4.2.2.2" xref="S3.SS1.p1.4.m4.4.4.2.2.2.cmml"><mi id="S3.SS1.p1.4.m4.4.4.2.2.2.2" xref="S3.SS1.p1.4.m4.4.4.2.2.2.2.cmml">ğ†</mi><mi id="S3.SS1.p1.4.m4.4.4.2.2.2.3" xref="S3.SS1.p1.4.m4.4.4.2.2.2.3.cmml">N</mi></msub><mo id="S3.SS1.p1.4.m4.4.4.2.2.6" stretchy="false" xref="S3.SS1.p1.4.m4.4.4.2.3.cmml">)</mo></mrow><mo id="S3.SS1.p1.4.m4.6.6.5" xref="S3.SS1.p1.4.m4.6.6.5.cmml">=</mo><mrow id="S3.SS1.p1.4.m4.6.6.4" xref="S3.SS1.p1.4.m4.6.6.4.cmml"><mi id="S3.SS1.p1.4.m4.6.6.4.4" xref="S3.SS1.p1.4.m4.6.6.4.4.cmml">t</mi><mo id="S3.SS1.p1.4.m4.6.6.4.3" xref="S3.SS1.p1.4.m4.6.6.4.3.cmml">â¢</mo><mrow id="S3.SS1.p1.4.m4.6.6.4.2.2" xref="S3.SS1.p1.4.m4.6.6.4.2.3.cmml"><mo id="S3.SS1.p1.4.m4.6.6.4.2.2.3" stretchy="false" xref="S3.SS1.p1.4.m4.6.6.4.2.3.cmml">(</mo><msub id="S3.SS1.p1.4.m4.5.5.3.1.1.1" xref="S3.SS1.p1.4.m4.5.5.3.1.1.1.cmml"><mi id="S3.SS1.p1.4.m4.5.5.3.1.1.1.2" xref="S3.SS1.p1.4.m4.5.5.3.1.1.1.2.cmml">ğ…</mi><mn id="S3.SS1.p1.4.m4.5.5.3.1.1.1.3" xref="S3.SS1.p1.4.m4.5.5.3.1.1.1.3.cmml">ğŸ</mn></msub><mo id="S3.SS1.p1.4.m4.6.6.4.2.2.4" xref="S3.SS1.p1.4.m4.6.6.4.2.3.cmml">,</mo><mi id="S3.SS1.p1.4.m4.2.2" mathvariant="normal" xref="S3.SS1.p1.4.m4.2.2.cmml">â€¦</mi><mo id="S3.SS1.p1.4.m4.6.6.4.2.2.5" xref="S3.SS1.p1.4.m4.6.6.4.2.3.cmml">,</mo><msub id="S3.SS1.p1.4.m4.6.6.4.2.2.2" xref="S3.SS1.p1.4.m4.6.6.4.2.2.2.cmml"><mi id="S3.SS1.p1.4.m4.6.6.4.2.2.2.2" xref="S3.SS1.p1.4.m4.6.6.4.2.2.2.2.cmml">ğ…</mi><mi id="S3.SS1.p1.4.m4.6.6.4.2.2.2.3" xref="S3.SS1.p1.4.m4.6.6.4.2.2.2.3.cmml">ğ</mi></msub><mo id="S3.SS1.p1.4.m4.6.6.4.2.2.6" stretchy="false" xref="S3.SS1.p1.4.m4.6.6.4.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.6b"><apply id="S3.SS1.p1.4.m4.6.6.cmml" xref="S3.SS1.p1.4.m4.6.6"><eq id="S3.SS1.p1.4.m4.6.6.5.cmml" xref="S3.SS1.p1.4.m4.6.6.5"></eq><vector id="S3.SS1.p1.4.m4.4.4.2.3.cmml" xref="S3.SS1.p1.4.m4.4.4.2.2"><apply id="S3.SS1.p1.4.m4.3.3.1.1.1.cmml" xref="S3.SS1.p1.4.m4.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.3.3.1.1.1.1.cmml" xref="S3.SS1.p1.4.m4.3.3.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.4.m4.3.3.1.1.1.2.cmml" xref="S3.SS1.p1.4.m4.3.3.1.1.1.2">ğ†</ci><cn id="S3.SS1.p1.4.m4.3.3.1.1.1.3.cmml" type="integer" xref="S3.SS1.p1.4.m4.3.3.1.1.1.3">1</cn></apply><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">â€¦</ci><apply id="S3.SS1.p1.4.m4.4.4.2.2.2.cmml" xref="S3.SS1.p1.4.m4.4.4.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.4.4.2.2.2.1.cmml" xref="S3.SS1.p1.4.m4.4.4.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.4.m4.4.4.2.2.2.2.cmml" xref="S3.SS1.p1.4.m4.4.4.2.2.2.2">ğ†</ci><ci id="S3.SS1.p1.4.m4.4.4.2.2.2.3.cmml" xref="S3.SS1.p1.4.m4.4.4.2.2.2.3">ğ‘</ci></apply></vector><apply id="S3.SS1.p1.4.m4.6.6.4.cmml" xref="S3.SS1.p1.4.m4.6.6.4"><times id="S3.SS1.p1.4.m4.6.6.4.3.cmml" xref="S3.SS1.p1.4.m4.6.6.4.3"></times><ci id="S3.SS1.p1.4.m4.6.6.4.4.cmml" xref="S3.SS1.p1.4.m4.6.6.4.4">ğ‘¡</ci><vector id="S3.SS1.p1.4.m4.6.6.4.2.3.cmml" xref="S3.SS1.p1.4.m4.6.6.4.2.2"><apply id="S3.SS1.p1.4.m4.5.5.3.1.1.1.cmml" xref="S3.SS1.p1.4.m4.5.5.3.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.5.5.3.1.1.1.1.cmml" xref="S3.SS1.p1.4.m4.5.5.3.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.4.m4.5.5.3.1.1.1.2.cmml" xref="S3.SS1.p1.4.m4.5.5.3.1.1.1.2">ğ…</ci><cn id="S3.SS1.p1.4.m4.5.5.3.1.1.1.3.cmml" type="integer" xref="S3.SS1.p1.4.m4.5.5.3.1.1.1.3">1</cn></apply><ci id="S3.SS1.p1.4.m4.2.2.cmml" xref="S3.SS1.p1.4.m4.2.2">â€¦</ci><apply id="S3.SS1.p1.4.m4.6.6.4.2.2.2.cmml" xref="S3.SS1.p1.4.m4.6.6.4.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.6.6.4.2.2.2.1.cmml" xref="S3.SS1.p1.4.m4.6.6.4.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.4.m4.6.6.4.2.2.2.2.cmml" xref="S3.SS1.p1.4.m4.6.6.4.2.2.2.2">ğ…</ci><ci id="S3.SS1.p1.4.m4.6.6.4.2.2.2.3.cmml" xref="S3.SS1.p1.4.m4.6.6.4.2.2.2.3">ğ</ci></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.6c">(\mathbf{G_{1}},...,\mathbf{G}_{N})=t(\mathbf{F_{1}},...,\mathbf{F_{N}})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.6d">( bold_G start_POSTSUBSCRIPT bold_1 end_POSTSUBSCRIPT , â€¦ , bold_G start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ) = italic_t ( bold_F start_POSTSUBSCRIPT bold_1 end_POSTSUBSCRIPT , â€¦ , bold_F start_POSTSUBSCRIPT bold_N end_POSTSUBSCRIPT )</annotation></semantics></math>. An additional reference image encoding is added to <math alttext="\mathbf{F_{1}}" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m5.1"><semantics id="S3.SS1.p1.5.m5.1a"><msub id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mi id="S3.SS1.p1.5.m5.1.1.2" xref="S3.SS1.p1.5.m5.1.1.2.cmml">ğ…</mi><mn id="S3.SS1.p1.5.m5.1.1.3" xref="S3.SS1.p1.5.m5.1.1.3.cmml">ğŸ</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2">ğ…</ci><cn id="S3.SS1.p1.5.m5.1.1.3.cmml" type="integer" xref="S3.SS1.p1.5.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">\mathbf{F_{1}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.5.m5.1d">bold_F start_POSTSUBSCRIPT bold_1 end_POSTSUBSCRIPT</annotation></semantics></math> to designate it as representing the canonical pose. The fused features <math alttext="\mathbf{G}" class="ltx_Math" display="inline" id="S3.SS1.p1.6.m6.1"><semantics id="S3.SS1.p1.6.m6.1a"><mi id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml">ğ†</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><ci id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">ğ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">\mathbf{G}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.6.m6.1d">bold_G</annotation></semantics></math> are then fed into a pose generator and a pose discriminator to predict relative poses.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Pose generator</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The pose generator takes inputs from the fused features and predicts multiple pose hypotheses for each input image. These hypotheses aim at capturing a distribution over poses under natural ambiguities such as rotation symmetry. However, the training data only contains a single groundtruth pose. Training all pose proposals to match the ground truth will result in a mode collapse:
the generator will output only one pose and loses its ability to output a pose distribution to model these natural ambiguities. This has also been verified through empirical experiments (<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#S4.T5" title="In 4.3 Ablation â€£ 4 Experiments â€£ ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">5</span></a>). To avoid such mode collapse, we allow the model to explore multiple modes by only regressing one pose candidate closest to the ground truth; no loss is computed to penalize other poses predictions.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.8">We use an MLP to generate <math alttext="M" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">M</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">italic_M</annotation></semantics></math> pose hypotheses <math alttext="Q_{i,m}\in\mathbb{R}^{7}" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m2.2"><semantics id="S3.SS2.p2.2.m2.2a"><mrow id="S3.SS2.p2.2.m2.2.3" xref="S3.SS2.p2.2.m2.2.3.cmml"><msub id="S3.SS2.p2.2.m2.2.3.2" xref="S3.SS2.p2.2.m2.2.3.2.cmml"><mi id="S3.SS2.p2.2.m2.2.3.2.2" xref="S3.SS2.p2.2.m2.2.3.2.2.cmml">Q</mi><mrow id="S3.SS2.p2.2.m2.2.2.2.4" xref="S3.SS2.p2.2.m2.2.2.2.3.cmml"><mi id="S3.SS2.p2.2.m2.1.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.1.cmml">i</mi><mo id="S3.SS2.p2.2.m2.2.2.2.4.1" xref="S3.SS2.p2.2.m2.2.2.2.3.cmml">,</mo><mi id="S3.SS2.p2.2.m2.2.2.2.2" xref="S3.SS2.p2.2.m2.2.2.2.2.cmml">m</mi></mrow></msub><mo id="S3.SS2.p2.2.m2.2.3.1" xref="S3.SS2.p2.2.m2.2.3.1.cmml">âˆˆ</mo><msup id="S3.SS2.p2.2.m2.2.3.3" xref="S3.SS2.p2.2.m2.2.3.3.cmml"><mi id="S3.SS2.p2.2.m2.2.3.3.2" xref="S3.SS2.p2.2.m2.2.3.3.2.cmml">â„</mi><mn id="S3.SS2.p2.2.m2.2.3.3.3" xref="S3.SS2.p2.2.m2.2.3.3.3.cmml">7</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.2b"><apply id="S3.SS2.p2.2.m2.2.3.cmml" xref="S3.SS2.p2.2.m2.2.3"><in id="S3.SS2.p2.2.m2.2.3.1.cmml" xref="S3.SS2.p2.2.m2.2.3.1"></in><apply id="S3.SS2.p2.2.m2.2.3.2.cmml" xref="S3.SS2.p2.2.m2.2.3.2"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.2.3.2.1.cmml" xref="S3.SS2.p2.2.m2.2.3.2">subscript</csymbol><ci id="S3.SS2.p2.2.m2.2.3.2.2.cmml" xref="S3.SS2.p2.2.m2.2.3.2.2">ğ‘„</ci><list id="S3.SS2.p2.2.m2.2.2.2.3.cmml" xref="S3.SS2.p2.2.m2.2.2.2.4"><ci id="S3.SS2.p2.2.m2.1.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1">ğ‘–</ci><ci id="S3.SS2.p2.2.m2.2.2.2.2.cmml" xref="S3.SS2.p2.2.m2.2.2.2.2">ğ‘š</ci></list></apply><apply id="S3.SS2.p2.2.m2.2.3.3.cmml" xref="S3.SS2.p2.2.m2.2.3.3"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.2.3.3.1.cmml" xref="S3.SS2.p2.2.m2.2.3.3">superscript</csymbol><ci id="S3.SS2.p2.2.m2.2.3.3.2.cmml" xref="S3.SS2.p2.2.m2.2.3.3.2">â„</ci><cn id="S3.SS2.p2.2.m2.2.3.3.3.cmml" type="integer" xref="S3.SS2.p2.2.m2.2.3.3.3">7</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.2c">Q_{i,m}\in\mathbb{R}^{7}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m2.2d">italic_Q start_POSTSUBSCRIPT italic_i , italic_m end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT 7 end_POSTSUPERSCRIPT</annotation></semantics></math> which contains 4 elements for the quaternion and 3 for the translation. The choice to represent the rotation matrix with a quaternion is due to the ease of normalizing the modelâ€™s output in quaternion space. We then transform the quaternion representation of the rotation component of <math alttext="Q_{i,m}" class="ltx_Math" display="inline" id="S3.SS2.p2.3.m3.2"><semantics id="S3.SS2.p2.3.m3.2a"><msub id="S3.SS2.p2.3.m3.2.3" xref="S3.SS2.p2.3.m3.2.3.cmml"><mi id="S3.SS2.p2.3.m3.2.3.2" xref="S3.SS2.p2.3.m3.2.3.2.cmml">Q</mi><mrow id="S3.SS2.p2.3.m3.2.2.2.4" xref="S3.SS2.p2.3.m3.2.2.2.3.cmml"><mi id="S3.SS2.p2.3.m3.1.1.1.1" xref="S3.SS2.p2.3.m3.1.1.1.1.cmml">i</mi><mo id="S3.SS2.p2.3.m3.2.2.2.4.1" xref="S3.SS2.p2.3.m3.2.2.2.3.cmml">,</mo><mi id="S3.SS2.p2.3.m3.2.2.2.2" xref="S3.SS2.p2.3.m3.2.2.2.2.cmml">m</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.2b"><apply id="S3.SS2.p2.3.m3.2.3.cmml" xref="S3.SS2.p2.3.m3.2.3"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.2.3.1.cmml" xref="S3.SS2.p2.3.m3.2.3">subscript</csymbol><ci id="S3.SS2.p2.3.m3.2.3.2.cmml" xref="S3.SS2.p2.3.m3.2.3.2">ğ‘„</ci><list id="S3.SS2.p2.3.m3.2.2.2.3.cmml" xref="S3.SS2.p2.3.m3.2.2.2.4"><ci id="S3.SS2.p2.3.m3.1.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1.1.1">ğ‘–</ci><ci id="S3.SS2.p2.3.m3.2.2.2.2.cmml" xref="S3.SS2.p2.3.m3.2.2.2.2">ğ‘š</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.2c">Q_{i,m}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m3.2d">italic_Q start_POSTSUBSCRIPT italic_i , italic_m end_POSTSUBSCRIPT</annotation></semantics></math> back to <math alttext="3\times 3" class="ltx_Math" display="inline" id="S3.SS2.p2.4.m4.1"><semantics id="S3.SS2.p2.4.m4.1a"><mrow id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><mn id="S3.SS2.p2.4.m4.1.1.2" xref="S3.SS2.p2.4.m4.1.1.2.cmml">3</mn><mo id="S3.SS2.p2.4.m4.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p2.4.m4.1.1.1.cmml">Ã—</mo><mn id="S3.SS2.p2.4.m4.1.1.3" xref="S3.SS2.p2.4.m4.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><times id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1"></times><cn id="S3.SS2.p2.4.m4.1.1.2.cmml" type="integer" xref="S3.SS2.p2.4.m4.1.1.2">3</cn><cn id="S3.SS2.p2.4.m4.1.1.3.cmml" type="integer" xref="S3.SS2.p2.4.m4.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">3\times 3</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.4.m4.1d">3 Ã— 3</annotation></semantics></math> rotation matrix. This yields <math alttext="P_{i,m}\in\mathbb{R}^{12}" class="ltx_Math" display="inline" id="S3.SS2.p2.5.m5.2"><semantics id="S3.SS2.p2.5.m5.2a"><mrow id="S3.SS2.p2.5.m5.2.3" xref="S3.SS2.p2.5.m5.2.3.cmml"><msub id="S3.SS2.p2.5.m5.2.3.2" xref="S3.SS2.p2.5.m5.2.3.2.cmml"><mi id="S3.SS2.p2.5.m5.2.3.2.2" xref="S3.SS2.p2.5.m5.2.3.2.2.cmml">P</mi><mrow id="S3.SS2.p2.5.m5.2.2.2.4" xref="S3.SS2.p2.5.m5.2.2.2.3.cmml"><mi id="S3.SS2.p2.5.m5.1.1.1.1" xref="S3.SS2.p2.5.m5.1.1.1.1.cmml">i</mi><mo id="S3.SS2.p2.5.m5.2.2.2.4.1" xref="S3.SS2.p2.5.m5.2.2.2.3.cmml">,</mo><mi id="S3.SS2.p2.5.m5.2.2.2.2" xref="S3.SS2.p2.5.m5.2.2.2.2.cmml">m</mi></mrow></msub><mo id="S3.SS2.p2.5.m5.2.3.1" xref="S3.SS2.p2.5.m5.2.3.1.cmml">âˆˆ</mo><msup id="S3.SS2.p2.5.m5.2.3.3" xref="S3.SS2.p2.5.m5.2.3.3.cmml"><mi id="S3.SS2.p2.5.m5.2.3.3.2" xref="S3.SS2.p2.5.m5.2.3.3.2.cmml">â„</mi><mn id="S3.SS2.p2.5.m5.2.3.3.3" xref="S3.SS2.p2.5.m5.2.3.3.3.cmml">12</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.2b"><apply id="S3.SS2.p2.5.m5.2.3.cmml" xref="S3.SS2.p2.5.m5.2.3"><in id="S3.SS2.p2.5.m5.2.3.1.cmml" xref="S3.SS2.p2.5.m5.2.3.1"></in><apply id="S3.SS2.p2.5.m5.2.3.2.cmml" xref="S3.SS2.p2.5.m5.2.3.2"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m5.2.3.2.1.cmml" xref="S3.SS2.p2.5.m5.2.3.2">subscript</csymbol><ci id="S3.SS2.p2.5.m5.2.3.2.2.cmml" xref="S3.SS2.p2.5.m5.2.3.2.2">ğ‘ƒ</ci><list id="S3.SS2.p2.5.m5.2.2.2.3.cmml" xref="S3.SS2.p2.5.m5.2.2.2.4"><ci id="S3.SS2.p2.5.m5.1.1.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1.1.1">ğ‘–</ci><ci id="S3.SS2.p2.5.m5.2.2.2.2.cmml" xref="S3.SS2.p2.5.m5.2.2.2.2">ğ‘š</ci></list></apply><apply id="S3.SS2.p2.5.m5.2.3.3.cmml" xref="S3.SS2.p2.5.m5.2.3.3"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m5.2.3.3.1.cmml" xref="S3.SS2.p2.5.m5.2.3.3">superscript</csymbol><ci id="S3.SS2.p2.5.m5.2.3.3.2.cmml" xref="S3.SS2.p2.5.m5.2.3.3.2">â„</ci><cn id="S3.SS2.p2.5.m5.2.3.3.3.cmml" type="integer" xref="S3.SS2.p2.5.m5.2.3.3.3">12</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.2c">P_{i,m}\in\mathbb{R}^{12}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.5.m5.2d">italic_P start_POSTSUBSCRIPT italic_i , italic_m end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT 12 end_POSTSUPERSCRIPT</annotation></semantics></math>.
The pose generator takes inputs from the fused features <math alttext="G" class="ltx_Math" display="inline" id="S3.SS2.p2.6.m6.1"><semantics id="S3.SS2.p2.6.m6.1a"><mi id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><ci id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1">ğº</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">G</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.6.m6.1d">italic_G</annotation></semantics></math> and <math alttext="M" class="ltx_Math" display="inline" id="S3.SS2.p2.7.m7.1"><semantics id="S3.SS2.p2.7.m7.1a"><mi id="S3.SS2.p2.7.m7.1.1" xref="S3.SS2.p2.7.m7.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m7.1b"><ci id="S3.SS2.p2.7.m7.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m7.1c">M</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.7.m7.1d">italic_M</annotation></semantics></math> randomly initialized learnable queries <math alttext="\left\{e_{i,m}\right\}_{m=1}^{M}\in\mathbb{R}^{256}" class="ltx_Math" display="inline" id="S3.SS2.p2.8.m8.3"><semantics id="S3.SS2.p2.8.m8.3a"><mrow id="S3.SS2.p2.8.m8.3.3" xref="S3.SS2.p2.8.m8.3.3.cmml"><msubsup id="S3.SS2.p2.8.m8.3.3.1" xref="S3.SS2.p2.8.m8.3.3.1.cmml"><mrow id="S3.SS2.p2.8.m8.3.3.1.1.1.1" xref="S3.SS2.p2.8.m8.3.3.1.1.1.2.cmml"><mo id="S3.SS2.p2.8.m8.3.3.1.1.1.1.2" xref="S3.SS2.p2.8.m8.3.3.1.1.1.2.cmml">{</mo><msub id="S3.SS2.p2.8.m8.3.3.1.1.1.1.1" xref="S3.SS2.p2.8.m8.3.3.1.1.1.1.1.cmml"><mi id="S3.SS2.p2.8.m8.3.3.1.1.1.1.1.2" xref="S3.SS2.p2.8.m8.3.3.1.1.1.1.1.2.cmml">e</mi><mrow id="S3.SS2.p2.8.m8.2.2.2.4" xref="S3.SS2.p2.8.m8.2.2.2.3.cmml"><mi id="S3.SS2.p2.8.m8.1.1.1.1" xref="S3.SS2.p2.8.m8.1.1.1.1.cmml">i</mi><mo id="S3.SS2.p2.8.m8.2.2.2.4.1" xref="S3.SS2.p2.8.m8.2.2.2.3.cmml">,</mo><mi id="S3.SS2.p2.8.m8.2.2.2.2" xref="S3.SS2.p2.8.m8.2.2.2.2.cmml">m</mi></mrow></msub><mo id="S3.SS2.p2.8.m8.3.3.1.1.1.1.3" xref="S3.SS2.p2.8.m8.3.3.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS2.p2.8.m8.3.3.1.1.3" xref="S3.SS2.p2.8.m8.3.3.1.1.3.cmml"><mi id="S3.SS2.p2.8.m8.3.3.1.1.3.2" xref="S3.SS2.p2.8.m8.3.3.1.1.3.2.cmml">m</mi><mo id="S3.SS2.p2.8.m8.3.3.1.1.3.1" xref="S3.SS2.p2.8.m8.3.3.1.1.3.1.cmml">=</mo><mn id="S3.SS2.p2.8.m8.3.3.1.1.3.3" xref="S3.SS2.p2.8.m8.3.3.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS2.p2.8.m8.3.3.1.3" xref="S3.SS2.p2.8.m8.3.3.1.3.cmml">M</mi></msubsup><mo id="S3.SS2.p2.8.m8.3.3.2" xref="S3.SS2.p2.8.m8.3.3.2.cmml">âˆˆ</mo><msup id="S3.SS2.p2.8.m8.3.3.3" xref="S3.SS2.p2.8.m8.3.3.3.cmml"><mi id="S3.SS2.p2.8.m8.3.3.3.2" xref="S3.SS2.p2.8.m8.3.3.3.2.cmml">â„</mi><mn id="S3.SS2.p2.8.m8.3.3.3.3" xref="S3.SS2.p2.8.m8.3.3.3.3.cmml">256</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.8.m8.3b"><apply id="S3.SS2.p2.8.m8.3.3.cmml" xref="S3.SS2.p2.8.m8.3.3"><in id="S3.SS2.p2.8.m8.3.3.2.cmml" xref="S3.SS2.p2.8.m8.3.3.2"></in><apply id="S3.SS2.p2.8.m8.3.3.1.cmml" xref="S3.SS2.p2.8.m8.3.3.1"><csymbol cd="ambiguous" id="S3.SS2.p2.8.m8.3.3.1.2.cmml" xref="S3.SS2.p2.8.m8.3.3.1">superscript</csymbol><apply id="S3.SS2.p2.8.m8.3.3.1.1.cmml" xref="S3.SS2.p2.8.m8.3.3.1"><csymbol cd="ambiguous" id="S3.SS2.p2.8.m8.3.3.1.1.2.cmml" xref="S3.SS2.p2.8.m8.3.3.1">subscript</csymbol><set id="S3.SS2.p2.8.m8.3.3.1.1.1.2.cmml" xref="S3.SS2.p2.8.m8.3.3.1.1.1.1"><apply id="S3.SS2.p2.8.m8.3.3.1.1.1.1.1.cmml" xref="S3.SS2.p2.8.m8.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.8.m8.3.3.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.8.m8.3.3.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p2.8.m8.3.3.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.8.m8.3.3.1.1.1.1.1.2">ğ‘’</ci><list id="S3.SS2.p2.8.m8.2.2.2.3.cmml" xref="S3.SS2.p2.8.m8.2.2.2.4"><ci id="S3.SS2.p2.8.m8.1.1.1.1.cmml" xref="S3.SS2.p2.8.m8.1.1.1.1">ğ‘–</ci><ci id="S3.SS2.p2.8.m8.2.2.2.2.cmml" xref="S3.SS2.p2.8.m8.2.2.2.2">ğ‘š</ci></list></apply></set><apply id="S3.SS2.p2.8.m8.3.3.1.1.3.cmml" xref="S3.SS2.p2.8.m8.3.3.1.1.3"><eq id="S3.SS2.p2.8.m8.3.3.1.1.3.1.cmml" xref="S3.SS2.p2.8.m8.3.3.1.1.3.1"></eq><ci id="S3.SS2.p2.8.m8.3.3.1.1.3.2.cmml" xref="S3.SS2.p2.8.m8.3.3.1.1.3.2">ğ‘š</ci><cn id="S3.SS2.p2.8.m8.3.3.1.1.3.3.cmml" type="integer" xref="S3.SS2.p2.8.m8.3.3.1.1.3.3">1</cn></apply></apply><ci id="S3.SS2.p2.8.m8.3.3.1.3.cmml" xref="S3.SS2.p2.8.m8.3.3.1.3">ğ‘€</ci></apply><apply id="S3.SS2.p2.8.m8.3.3.3.cmml" xref="S3.SS2.p2.8.m8.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.p2.8.m8.3.3.3.1.cmml" xref="S3.SS2.p2.8.m8.3.3.3">superscript</csymbol><ci id="S3.SS2.p2.8.m8.3.3.3.2.cmml" xref="S3.SS2.p2.8.m8.3.3.3.2">â„</ci><cn id="S3.SS2.p2.8.m8.3.3.3.3.cmml" type="integer" xref="S3.SS2.p2.8.m8.3.3.3.3">256</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.8.m8.3c">\left\{e_{i,m}\right\}_{m=1}^{M}\in\mathbb{R}^{256}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.8.m8.3d">{ italic_e start_POSTSUBSCRIPT italic_i , italic_m end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_m = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT 256 end_POSTSUPERSCRIPT</annotation></semantics></math>. An MLP is used to map the queries to the feature space. Note that these randomly initialized queries specialize in distinct partition of the pose space, similar to learnable queries in object detectionsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib4" title="">4</a>]</cite>, enabling the model to fully cover the space.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<table class="ltx_equation ltx_eqn_table" id="S3.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="P_{i,m}=\mathbf{MLP}(\mathbf{G}_{i}+\mathbf{MLP}(e_{i,m}))" class="ltx_Math" display="block" id="S3.Ex1.m1.5"><semantics id="S3.Ex1.m1.5a"><mrow id="S3.Ex1.m1.5.5" xref="S3.Ex1.m1.5.5.cmml"><msub id="S3.Ex1.m1.5.5.3" xref="S3.Ex1.m1.5.5.3.cmml"><mi id="S3.Ex1.m1.5.5.3.2" xref="S3.Ex1.m1.5.5.3.2.cmml">P</mi><mrow id="S3.Ex1.m1.2.2.2.4" xref="S3.Ex1.m1.2.2.2.3.cmml"><mi id="S3.Ex1.m1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.cmml">i</mi><mo id="S3.Ex1.m1.2.2.2.4.1" xref="S3.Ex1.m1.2.2.2.3.cmml">,</mo><mi id="S3.Ex1.m1.2.2.2.2" xref="S3.Ex1.m1.2.2.2.2.cmml">m</mi></mrow></msub><mo id="S3.Ex1.m1.5.5.2" xref="S3.Ex1.m1.5.5.2.cmml">=</mo><mrow id="S3.Ex1.m1.5.5.1" xref="S3.Ex1.m1.5.5.1.cmml"><mi id="S3.Ex1.m1.5.5.1.3" xref="S3.Ex1.m1.5.5.1.3.cmml">ğŒğ‹ğ</mi><mo id="S3.Ex1.m1.5.5.1.2" xref="S3.Ex1.m1.5.5.1.2.cmml">â¢</mo><mrow id="S3.Ex1.m1.5.5.1.1.1" xref="S3.Ex1.m1.5.5.1.1.1.1.cmml"><mo id="S3.Ex1.m1.5.5.1.1.1.2" stretchy="false" xref="S3.Ex1.m1.5.5.1.1.1.1.cmml">(</mo><mrow id="S3.Ex1.m1.5.5.1.1.1.1" xref="S3.Ex1.m1.5.5.1.1.1.1.cmml"><msub id="S3.Ex1.m1.5.5.1.1.1.1.3" xref="S3.Ex1.m1.5.5.1.1.1.1.3.cmml"><mi id="S3.Ex1.m1.5.5.1.1.1.1.3.2" xref="S3.Ex1.m1.5.5.1.1.1.1.3.2.cmml">ğ†</mi><mi id="S3.Ex1.m1.5.5.1.1.1.1.3.3" xref="S3.Ex1.m1.5.5.1.1.1.1.3.3.cmml">i</mi></msub><mo id="S3.Ex1.m1.5.5.1.1.1.1.2" xref="S3.Ex1.m1.5.5.1.1.1.1.2.cmml">+</mo><mrow id="S3.Ex1.m1.5.5.1.1.1.1.1" xref="S3.Ex1.m1.5.5.1.1.1.1.1.cmml"><mi id="S3.Ex1.m1.5.5.1.1.1.1.1.3" xref="S3.Ex1.m1.5.5.1.1.1.1.1.3.cmml">ğŒğ‹ğ</mi><mo id="S3.Ex1.m1.5.5.1.1.1.1.1.2" xref="S3.Ex1.m1.5.5.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S3.Ex1.m1.5.5.1.1.1.1.1.1.1" xref="S3.Ex1.m1.5.5.1.1.1.1.1.1.1.1.cmml"><mo id="S3.Ex1.m1.5.5.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.Ex1.m1.5.5.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.Ex1.m1.5.5.1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.5.5.1.1.1.1.1.1.1.1.cmml"><mi id="S3.Ex1.m1.5.5.1.1.1.1.1.1.1.1.2" xref="S3.Ex1.m1.5.5.1.1.1.1.1.1.1.1.2.cmml">e</mi><mrow id="S3.Ex1.m1.4.4.2.4" xref="S3.Ex1.m1.4.4.2.3.cmml"><mi id="S3.Ex1.m1.3.3.1.1" xref="S3.Ex1.m1.3.3.1.1.cmml">i</mi><mo id="S3.Ex1.m1.4.4.2.4.1" xref="S3.Ex1.m1.4.4.2.3.cmml">,</mo><mi id="S3.Ex1.m1.4.4.2.2" xref="S3.Ex1.m1.4.4.2.2.cmml">m</mi></mrow></msub><mo id="S3.Ex1.m1.5.5.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.Ex1.m1.5.5.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.Ex1.m1.5.5.1.1.1.3" stretchy="false" xref="S3.Ex1.m1.5.5.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.5b"><apply id="S3.Ex1.m1.5.5.cmml" xref="S3.Ex1.m1.5.5"><eq id="S3.Ex1.m1.5.5.2.cmml" xref="S3.Ex1.m1.5.5.2"></eq><apply id="S3.Ex1.m1.5.5.3.cmml" xref="S3.Ex1.m1.5.5.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.5.5.3.1.cmml" xref="S3.Ex1.m1.5.5.3">subscript</csymbol><ci id="S3.Ex1.m1.5.5.3.2.cmml" xref="S3.Ex1.m1.5.5.3.2">ğ‘ƒ</ci><list id="S3.Ex1.m1.2.2.2.3.cmml" xref="S3.Ex1.m1.2.2.2.4"><ci id="S3.Ex1.m1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1">ğ‘–</ci><ci id="S3.Ex1.m1.2.2.2.2.cmml" xref="S3.Ex1.m1.2.2.2.2">ğ‘š</ci></list></apply><apply id="S3.Ex1.m1.5.5.1.cmml" xref="S3.Ex1.m1.5.5.1"><times id="S3.Ex1.m1.5.5.1.2.cmml" xref="S3.Ex1.m1.5.5.1.2"></times><ci id="S3.Ex1.m1.5.5.1.3.cmml" xref="S3.Ex1.m1.5.5.1.3">ğŒğ‹ğ</ci><apply id="S3.Ex1.m1.5.5.1.1.1.1.cmml" xref="S3.Ex1.m1.5.5.1.1.1"><plus id="S3.Ex1.m1.5.5.1.1.1.1.2.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.2"></plus><apply id="S3.Ex1.m1.5.5.1.1.1.1.3.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.5.5.1.1.1.1.3.1.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.3">subscript</csymbol><ci id="S3.Ex1.m1.5.5.1.1.1.1.3.2.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.3.2">ğ†</ci><ci id="S3.Ex1.m1.5.5.1.1.1.1.3.3.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.3.3">ğ‘–</ci></apply><apply id="S3.Ex1.m1.5.5.1.1.1.1.1.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.1"><times id="S3.Ex1.m1.5.5.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.1.2"></times><ci id="S3.Ex1.m1.5.5.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.1.3">ğŒğ‹ğ</ci><apply id="S3.Ex1.m1.5.5.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.5.5.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex1.m1.5.5.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.1.1.1.1.2">ğ‘’</ci><list id="S3.Ex1.m1.4.4.2.3.cmml" xref="S3.Ex1.m1.4.4.2.4"><ci id="S3.Ex1.m1.3.3.1.1.cmml" xref="S3.Ex1.m1.3.3.1.1">ğ‘–</ci><ci id="S3.Ex1.m1.4.4.2.2.cmml" xref="S3.Ex1.m1.4.4.2.2">ğ‘š</ci></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.5c">P_{i,m}=\mathbf{MLP}(\mathbf{G}_{i}+\mathbf{MLP}(e_{i,m}))</annotation><annotation encoding="application/x-llamapun" id="S3.Ex1.m1.5d">italic_P start_POSTSUBSCRIPT italic_i , italic_m end_POSTSUBSCRIPT = bold_MLP ( bold_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + bold_MLP ( italic_e start_POSTSUBSCRIPT italic_i , italic_m end_POSTSUBSCRIPT ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Pose discriminator</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Given the generated pose hypotheses from the pose generator head, the pose discriminator evaluates the probability of each hypothesis being correct. At inference, the hypothesis with the highest probability is used as the output.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">The pose discriminator first embeds the generated hypotheses to the same dimension as feature dimension and then uses an MLP to predict logits <math alttext="x_{i,m}" class="ltx_Math" display="inline" id="S3.SS3.p2.1.m1.2"><semantics id="S3.SS3.p2.1.m1.2a"><msub id="S3.SS3.p2.1.m1.2.3" xref="S3.SS3.p2.1.m1.2.3.cmml"><mi id="S3.SS3.p2.1.m1.2.3.2" xref="S3.SS3.p2.1.m1.2.3.2.cmml">x</mi><mrow id="S3.SS3.p2.1.m1.2.2.2.4" xref="S3.SS3.p2.1.m1.2.2.2.3.cmml"><mi id="S3.SS3.p2.1.m1.1.1.1.1" xref="S3.SS3.p2.1.m1.1.1.1.1.cmml">i</mi><mo id="S3.SS3.p2.1.m1.2.2.2.4.1" xref="S3.SS3.p2.1.m1.2.2.2.3.cmml">,</mo><mi id="S3.SS3.p2.1.m1.2.2.2.2" xref="S3.SS3.p2.1.m1.2.2.2.2.cmml">m</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.2b"><apply id="S3.SS3.p2.1.m1.2.3.cmml" xref="S3.SS3.p2.1.m1.2.3"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.2.3.1.cmml" xref="S3.SS3.p2.1.m1.2.3">subscript</csymbol><ci id="S3.SS3.p2.1.m1.2.3.2.cmml" xref="S3.SS3.p2.1.m1.2.3.2">ğ‘¥</ci><list id="S3.SS3.p2.1.m1.2.2.2.3.cmml" xref="S3.SS3.p2.1.m1.2.2.2.4"><ci id="S3.SS3.p2.1.m1.1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1">ğ‘–</ci><ci id="S3.SS3.p2.1.m1.2.2.2.2.cmml" xref="S3.SS3.p2.1.m1.2.2.2.2">ğ‘š</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.2c">x_{i,m}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.1.m1.2d">italic_x start_POSTSUBSCRIPT italic_i , italic_m end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<table class="ltx_equation ltx_eqn_table" id="S3.Ex2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="x_{i,m}=\mathbf{MLP}(\mathbf{G}_{i}+\mathbf{MLP}(P_{i,m}))" class="ltx_Math" display="block" id="S3.Ex2.m1.5"><semantics id="S3.Ex2.m1.5a"><mrow id="S3.Ex2.m1.5.5" xref="S3.Ex2.m1.5.5.cmml"><msub id="S3.Ex2.m1.5.5.3" xref="S3.Ex2.m1.5.5.3.cmml"><mi id="S3.Ex2.m1.5.5.3.2" xref="S3.Ex2.m1.5.5.3.2.cmml">x</mi><mrow id="S3.Ex2.m1.2.2.2.4" xref="S3.Ex2.m1.2.2.2.3.cmml"><mi id="S3.Ex2.m1.1.1.1.1" xref="S3.Ex2.m1.1.1.1.1.cmml">i</mi><mo id="S3.Ex2.m1.2.2.2.4.1" xref="S3.Ex2.m1.2.2.2.3.cmml">,</mo><mi id="S3.Ex2.m1.2.2.2.2" xref="S3.Ex2.m1.2.2.2.2.cmml">m</mi></mrow></msub><mo id="S3.Ex2.m1.5.5.2" xref="S3.Ex2.m1.5.5.2.cmml">=</mo><mrow id="S3.Ex2.m1.5.5.1" xref="S3.Ex2.m1.5.5.1.cmml"><mi id="S3.Ex2.m1.5.5.1.3" xref="S3.Ex2.m1.5.5.1.3.cmml">ğŒğ‹ğ</mi><mo id="S3.Ex2.m1.5.5.1.2" xref="S3.Ex2.m1.5.5.1.2.cmml">â¢</mo><mrow id="S3.Ex2.m1.5.5.1.1.1" xref="S3.Ex2.m1.5.5.1.1.1.1.cmml"><mo id="S3.Ex2.m1.5.5.1.1.1.2" stretchy="false" xref="S3.Ex2.m1.5.5.1.1.1.1.cmml">(</mo><mrow id="S3.Ex2.m1.5.5.1.1.1.1" xref="S3.Ex2.m1.5.5.1.1.1.1.cmml"><msub id="S3.Ex2.m1.5.5.1.1.1.1.3" xref="S3.Ex2.m1.5.5.1.1.1.1.3.cmml"><mi id="S3.Ex2.m1.5.5.1.1.1.1.3.2" xref="S3.Ex2.m1.5.5.1.1.1.1.3.2.cmml">ğ†</mi><mi id="S3.Ex2.m1.5.5.1.1.1.1.3.3" xref="S3.Ex2.m1.5.5.1.1.1.1.3.3.cmml">i</mi></msub><mo id="S3.Ex2.m1.5.5.1.1.1.1.2" xref="S3.Ex2.m1.5.5.1.1.1.1.2.cmml">+</mo><mrow id="S3.Ex2.m1.5.5.1.1.1.1.1" xref="S3.Ex2.m1.5.5.1.1.1.1.1.cmml"><mi id="S3.Ex2.m1.5.5.1.1.1.1.1.3" xref="S3.Ex2.m1.5.5.1.1.1.1.1.3.cmml">ğŒğ‹ğ</mi><mo id="S3.Ex2.m1.5.5.1.1.1.1.1.2" xref="S3.Ex2.m1.5.5.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S3.Ex2.m1.5.5.1.1.1.1.1.1.1" xref="S3.Ex2.m1.5.5.1.1.1.1.1.1.1.1.cmml"><mo id="S3.Ex2.m1.5.5.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.Ex2.m1.5.5.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.Ex2.m1.5.5.1.1.1.1.1.1.1.1" xref="S3.Ex2.m1.5.5.1.1.1.1.1.1.1.1.cmml"><mi id="S3.Ex2.m1.5.5.1.1.1.1.1.1.1.1.2" xref="S3.Ex2.m1.5.5.1.1.1.1.1.1.1.1.2.cmml">P</mi><mrow id="S3.Ex2.m1.4.4.2.4" xref="S3.Ex2.m1.4.4.2.3.cmml"><mi id="S3.Ex2.m1.3.3.1.1" xref="S3.Ex2.m1.3.3.1.1.cmml">i</mi><mo id="S3.Ex2.m1.4.4.2.4.1" xref="S3.Ex2.m1.4.4.2.3.cmml">,</mo><mi id="S3.Ex2.m1.4.4.2.2" xref="S3.Ex2.m1.4.4.2.2.cmml">m</mi></mrow></msub><mo id="S3.Ex2.m1.5.5.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.Ex2.m1.5.5.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.Ex2.m1.5.5.1.1.1.3" stretchy="false" xref="S3.Ex2.m1.5.5.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex2.m1.5b"><apply id="S3.Ex2.m1.5.5.cmml" xref="S3.Ex2.m1.5.5"><eq id="S3.Ex2.m1.5.5.2.cmml" xref="S3.Ex2.m1.5.5.2"></eq><apply id="S3.Ex2.m1.5.5.3.cmml" xref="S3.Ex2.m1.5.5.3"><csymbol cd="ambiguous" id="S3.Ex2.m1.5.5.3.1.cmml" xref="S3.Ex2.m1.5.5.3">subscript</csymbol><ci id="S3.Ex2.m1.5.5.3.2.cmml" xref="S3.Ex2.m1.5.5.3.2">ğ‘¥</ci><list id="S3.Ex2.m1.2.2.2.3.cmml" xref="S3.Ex2.m1.2.2.2.4"><ci id="S3.Ex2.m1.1.1.1.1.cmml" xref="S3.Ex2.m1.1.1.1.1">ğ‘–</ci><ci id="S3.Ex2.m1.2.2.2.2.cmml" xref="S3.Ex2.m1.2.2.2.2">ğ‘š</ci></list></apply><apply id="S3.Ex2.m1.5.5.1.cmml" xref="S3.Ex2.m1.5.5.1"><times id="S3.Ex2.m1.5.5.1.2.cmml" xref="S3.Ex2.m1.5.5.1.2"></times><ci id="S3.Ex2.m1.5.5.1.3.cmml" xref="S3.Ex2.m1.5.5.1.3">ğŒğ‹ğ</ci><apply id="S3.Ex2.m1.5.5.1.1.1.1.cmml" xref="S3.Ex2.m1.5.5.1.1.1"><plus id="S3.Ex2.m1.5.5.1.1.1.1.2.cmml" xref="S3.Ex2.m1.5.5.1.1.1.1.2"></plus><apply id="S3.Ex2.m1.5.5.1.1.1.1.3.cmml" xref="S3.Ex2.m1.5.5.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex2.m1.5.5.1.1.1.1.3.1.cmml" xref="S3.Ex2.m1.5.5.1.1.1.1.3">subscript</csymbol><ci id="S3.Ex2.m1.5.5.1.1.1.1.3.2.cmml" xref="S3.Ex2.m1.5.5.1.1.1.1.3.2">ğ†</ci><ci id="S3.Ex2.m1.5.5.1.1.1.1.3.3.cmml" xref="S3.Ex2.m1.5.5.1.1.1.1.3.3">ğ‘–</ci></apply><apply id="S3.Ex2.m1.5.5.1.1.1.1.1.cmml" xref="S3.Ex2.m1.5.5.1.1.1.1.1"><times id="S3.Ex2.m1.5.5.1.1.1.1.1.2.cmml" xref="S3.Ex2.m1.5.5.1.1.1.1.1.2"></times><ci id="S3.Ex2.m1.5.5.1.1.1.1.1.3.cmml" xref="S3.Ex2.m1.5.5.1.1.1.1.1.3">ğŒğ‹ğ</ci><apply id="S3.Ex2.m1.5.5.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.5.5.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.5.5.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.5.5.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex2.m1.5.5.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex2.m1.5.5.1.1.1.1.1.1.1.1.2">ğ‘ƒ</ci><list id="S3.Ex2.m1.4.4.2.3.cmml" xref="S3.Ex2.m1.4.4.2.4"><ci id="S3.Ex2.m1.3.3.1.1.cmml" xref="S3.Ex2.m1.3.3.1.1">ğ‘–</ci><ci id="S3.Ex2.m1.4.4.2.2.cmml" xref="S3.Ex2.m1.4.4.2.2">ğ‘š</ci></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2.m1.5c">x_{i,m}=\mathbf{MLP}(\mathbf{G}_{i}+\mathbf{MLP}(P_{i,m}))</annotation><annotation encoding="application/x-llamapun" id="S3.Ex2.m1.5d">italic_x start_POSTSUBSCRIPT italic_i , italic_m end_POSTSUBSCRIPT = bold_MLP ( bold_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + bold_MLP ( italic_P start_POSTSUBSCRIPT italic_i , italic_m end_POSTSUBSCRIPT ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.p3.1">For any pose and logit pair <math alttext="(\mathbf{P},x)" class="ltx_Math" display="inline" id="S3.SS3.p3.1.m1.2"><semantics id="S3.SS3.p3.1.m1.2a"><mrow id="S3.SS3.p3.1.m1.2.3.2" xref="S3.SS3.p3.1.m1.2.3.1.cmml"><mo id="S3.SS3.p3.1.m1.2.3.2.1" stretchy="false" xref="S3.SS3.p3.1.m1.2.3.1.cmml">(</mo><mi id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">ğ</mi><mo id="S3.SS3.p3.1.m1.2.3.2.2" xref="S3.SS3.p3.1.m1.2.3.1.cmml">,</mo><mi id="S3.SS3.p3.1.m1.2.2" xref="S3.SS3.p3.1.m1.2.2.cmml">x</mi><mo id="S3.SS3.p3.1.m1.2.3.2.3" stretchy="false" xref="S3.SS3.p3.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.2b"><interval closure="open" id="S3.SS3.p3.1.m1.2.3.1.cmml" xref="S3.SS3.p3.1.m1.2.3.2"><ci id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">ğ</ci><ci id="S3.SS3.p3.1.m1.2.2.cmml" xref="S3.SS3.p3.1.m1.2.2">ğ‘¥</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.2c">(\mathbf{P},x)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.1.m1.2d">( bold_P , italic_x )</annotation></semantics></math> we then have probability:</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S5.EGx1">
<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\hat{p}(\mathbf{P}|\mathbf{I}_{1},...,\mathbf{I}_{N};i)=\frac{e^{%
x}}{e^{x_{i}^{*}}+\sum_{m}e^{x_{i,m}}}" class="ltx_Math" display="inline" id="S3.E1.m1.5"><semantics id="S3.E1.m1.5a"><mrow id="S3.E1.m1.5.5" xref="S3.E1.m1.5.5.cmml"><mrow id="S3.E1.m1.5.5.1" xref="S3.E1.m1.5.5.1.cmml"><mover accent="true" id="S3.E1.m1.5.5.1.3" xref="S3.E1.m1.5.5.1.3.cmml"><mi id="S3.E1.m1.5.5.1.3.2" xref="S3.E1.m1.5.5.1.3.2.cmml">p</mi><mo id="S3.E1.m1.5.5.1.3.1" xref="S3.E1.m1.5.5.1.3.1.cmml">^</mo></mover><mo id="S3.E1.m1.5.5.1.2" xref="S3.E1.m1.5.5.1.2.cmml">â¢</mo><mrow id="S3.E1.m1.5.5.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.cmml"><mo id="S3.E1.m1.5.5.1.1.1.2" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.5.5.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.cmml"><mi id="S3.E1.m1.5.5.1.1.1.1.4" xref="S3.E1.m1.5.5.1.1.1.1.4.cmml">ğ</mi><mo fence="false" id="S3.E1.m1.5.5.1.1.1.1.3" xref="S3.E1.m1.5.5.1.1.1.1.3.cmml">|</mo><mrow id="S3.E1.m1.5.5.1.1.1.1.2.2" xref="S3.E1.m1.5.5.1.1.1.1.2.3.cmml"><msub id="S3.E1.m1.5.5.1.1.1.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.5.5.1.1.1.1.1.1.1.2" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.2.cmml">ğˆ</mi><mn id="S3.E1.m1.5.5.1.1.1.1.1.1.1.3" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.E1.m1.5.5.1.1.1.1.2.2.3" xref="S3.E1.m1.5.5.1.1.1.1.2.3.cmml">,</mo><mi id="S3.E1.m1.3.3" mathvariant="normal" xref="S3.E1.m1.3.3.cmml">â€¦</mi><mo id="S3.E1.m1.5.5.1.1.1.1.2.2.4" xref="S3.E1.m1.5.5.1.1.1.1.2.3.cmml">,</mo><msub id="S3.E1.m1.5.5.1.1.1.1.2.2.2" xref="S3.E1.m1.5.5.1.1.1.1.2.2.2.cmml"><mi id="S3.E1.m1.5.5.1.1.1.1.2.2.2.2" xref="S3.E1.m1.5.5.1.1.1.1.2.2.2.2.cmml">ğˆ</mi><mi id="S3.E1.m1.5.5.1.1.1.1.2.2.2.3" xref="S3.E1.m1.5.5.1.1.1.1.2.2.2.3.cmml">N</mi></msub><mo id="S3.E1.m1.5.5.1.1.1.1.2.2.5" xref="S3.E1.m1.5.5.1.1.1.1.2.3.cmml">;</mo><mi id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml">i</mi></mrow></mrow><mo id="S3.E1.m1.5.5.1.1.1.3" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.5.5.2" xref="S3.E1.m1.5.5.2.cmml">=</mo><mstyle displaystyle="true" id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml"><mfrac id="S3.E1.m1.2.2a" xref="S3.E1.m1.2.2.cmml"><msup id="S3.E1.m1.2.2.4" xref="S3.E1.m1.2.2.4.cmml"><mi id="S3.E1.m1.2.2.4.2" xref="S3.E1.m1.2.2.4.2.cmml">e</mi><mi id="S3.E1.m1.2.2.4.3" xref="S3.E1.m1.2.2.4.3.cmml">x</mi></msup><mrow id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml"><msup id="S3.E1.m1.2.2.2.4" xref="S3.E1.m1.2.2.2.4.cmml"><mi id="S3.E1.m1.2.2.2.4.2" xref="S3.E1.m1.2.2.2.4.2.cmml">e</mi><msubsup id="S3.E1.m1.2.2.2.4.3" xref="S3.E1.m1.2.2.2.4.3.cmml"><mi id="S3.E1.m1.2.2.2.4.3.2.2" xref="S3.E1.m1.2.2.2.4.3.2.2.cmml">x</mi><mi id="S3.E1.m1.2.2.2.4.3.2.3" xref="S3.E1.m1.2.2.2.4.3.2.3.cmml">i</mi><mo id="S3.E1.m1.2.2.2.4.3.3" xref="S3.E1.m1.2.2.2.4.3.3.cmml">âˆ—</mo></msubsup></msup><mo id="S3.E1.m1.2.2.2.3" rspace="0.055em" xref="S3.E1.m1.2.2.2.3.cmml">+</mo><mrow id="S3.E1.m1.2.2.2.5" xref="S3.E1.m1.2.2.2.5.cmml"><msub id="S3.E1.m1.2.2.2.5.1" xref="S3.E1.m1.2.2.2.5.1.cmml"><mo id="S3.E1.m1.2.2.2.5.1.2" xref="S3.E1.m1.2.2.2.5.1.2.cmml">âˆ‘</mo><mi id="S3.E1.m1.2.2.2.5.1.3" xref="S3.E1.m1.2.2.2.5.1.3.cmml">m</mi></msub><msup id="S3.E1.m1.2.2.2.5.2" xref="S3.E1.m1.2.2.2.5.2.cmml"><mi id="S3.E1.m1.2.2.2.5.2.2" xref="S3.E1.m1.2.2.2.5.2.2.cmml">e</mi><msub id="S3.E1.m1.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.cmml"><mi id="S3.E1.m1.2.2.2.2.2.4" xref="S3.E1.m1.2.2.2.2.2.4.cmml">x</mi><mrow id="S3.E1.m1.2.2.2.2.2.2.2.4" xref="S3.E1.m1.2.2.2.2.2.2.2.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">i</mi><mo id="S3.E1.m1.2.2.2.2.2.2.2.4.1" xref="S3.E1.m1.2.2.2.2.2.2.2.3.cmml">,</mo><mi id="S3.E1.m1.2.2.2.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.2.2.2.cmml">m</mi></mrow></msub></msup></mrow></mrow></mfrac></mstyle></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.5b"><apply id="S3.E1.m1.5.5.cmml" xref="S3.E1.m1.5.5"><eq id="S3.E1.m1.5.5.2.cmml" xref="S3.E1.m1.5.5.2"></eq><apply id="S3.E1.m1.5.5.1.cmml" xref="S3.E1.m1.5.5.1"><times id="S3.E1.m1.5.5.1.2.cmml" xref="S3.E1.m1.5.5.1.2"></times><apply id="S3.E1.m1.5.5.1.3.cmml" xref="S3.E1.m1.5.5.1.3"><ci id="S3.E1.m1.5.5.1.3.1.cmml" xref="S3.E1.m1.5.5.1.3.1">^</ci><ci id="S3.E1.m1.5.5.1.3.2.cmml" xref="S3.E1.m1.5.5.1.3.2">ğ‘</ci></apply><apply id="S3.E1.m1.5.5.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.5.5.1.1.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.1.1.3">conditional</csymbol><ci id="S3.E1.m1.5.5.1.1.1.1.4.cmml" xref="S3.E1.m1.5.5.1.1.1.1.4">ğ</ci><list id="S3.E1.m1.5.5.1.1.1.1.2.3.cmml" xref="S3.E1.m1.5.5.1.1.1.1.2.2"><apply id="S3.E1.m1.5.5.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.5.5.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.2">ğˆ</ci><cn id="S3.E1.m1.5.5.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.3">1</cn></apply><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">â€¦</ci><apply id="S3.E1.m1.5.5.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.1.1.2.2.2.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E1.m1.5.5.1.1.1.1.2.2.2.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.2.2.2.2">ğˆ</ci><ci id="S3.E1.m1.5.5.1.1.1.1.2.2.2.3.cmml" xref="S3.E1.m1.5.5.1.1.1.1.2.2.2.3">ğ‘</ci></apply><ci id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4">ğ‘–</ci></list></apply></apply><apply id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2"><divide id="S3.E1.m1.2.2.3.cmml" xref="S3.E1.m1.2.2"></divide><apply id="S3.E1.m1.2.2.4.cmml" xref="S3.E1.m1.2.2.4"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.4.1.cmml" xref="S3.E1.m1.2.2.4">superscript</csymbol><ci id="S3.E1.m1.2.2.4.2.cmml" xref="S3.E1.m1.2.2.4.2">ğ‘’</ci><ci id="S3.E1.m1.2.2.4.3.cmml" xref="S3.E1.m1.2.2.4.3">ğ‘¥</ci></apply><apply id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2"><plus id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.3"></plus><apply id="S3.E1.m1.2.2.2.4.cmml" xref="S3.E1.m1.2.2.2.4"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.4.1.cmml" xref="S3.E1.m1.2.2.2.4">superscript</csymbol><ci id="S3.E1.m1.2.2.2.4.2.cmml" xref="S3.E1.m1.2.2.2.4.2">ğ‘’</ci><apply id="S3.E1.m1.2.2.2.4.3.cmml" xref="S3.E1.m1.2.2.2.4.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.4.3.1.cmml" xref="S3.E1.m1.2.2.2.4.3">superscript</csymbol><apply id="S3.E1.m1.2.2.2.4.3.2.cmml" xref="S3.E1.m1.2.2.2.4.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.4.3.2.1.cmml" xref="S3.E1.m1.2.2.2.4.3">subscript</csymbol><ci id="S3.E1.m1.2.2.2.4.3.2.2.cmml" xref="S3.E1.m1.2.2.2.4.3.2.2">ğ‘¥</ci><ci id="S3.E1.m1.2.2.2.4.3.2.3.cmml" xref="S3.E1.m1.2.2.2.4.3.2.3">ğ‘–</ci></apply><times id="S3.E1.m1.2.2.2.4.3.3.cmml" xref="S3.E1.m1.2.2.2.4.3.3"></times></apply></apply><apply id="S3.E1.m1.2.2.2.5.cmml" xref="S3.E1.m1.2.2.2.5"><apply id="S3.E1.m1.2.2.2.5.1.cmml" xref="S3.E1.m1.2.2.2.5.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.5.1.1.cmml" xref="S3.E1.m1.2.2.2.5.1">subscript</csymbol><sum id="S3.E1.m1.2.2.2.5.1.2.cmml" xref="S3.E1.m1.2.2.2.5.1.2"></sum><ci id="S3.E1.m1.2.2.2.5.1.3.cmml" xref="S3.E1.m1.2.2.2.5.1.3">ğ‘š</ci></apply><apply id="S3.E1.m1.2.2.2.5.2.cmml" xref="S3.E1.m1.2.2.2.5.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.5.2.1.cmml" xref="S3.E1.m1.2.2.2.5.2">superscript</csymbol><ci id="S3.E1.m1.2.2.2.5.2.2.cmml" xref="S3.E1.m1.2.2.2.5.2.2">ğ‘’</ci><apply id="S3.E1.m1.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.2.2">subscript</csymbol><ci id="S3.E1.m1.2.2.2.2.2.4.cmml" xref="S3.E1.m1.2.2.2.2.2.4">ğ‘¥</ci><list id="S3.E1.m1.2.2.2.2.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2.4"><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1">ğ‘–</ci><ci id="S3.E1.m1.2.2.2.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2.2">ğ‘š</ci></list></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.5c">\displaystyle\hat{p}(\mathbf{P}|\mathbf{I}_{1},...,\mathbf{I}_{N};i)=\frac{e^{%
x}}{e^{x_{i}^{*}}+\sum_{m}e^{x_{i,m}}}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.5d">over^ start_ARG italic_p end_ARG ( bold_P | bold_I start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , bold_I start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ; italic_i ) = divide start_ARG italic_e start_POSTSUPERSCRIPT italic_x end_POSTSUPERSCRIPT end_ARG start_ARG italic_e start_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT + âˆ‘ start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT italic_e start_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_i , italic_m end_POSTSUBSCRIPT end_POSTSUPERSCRIPT end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS3.p5">
<p class="ltx_p" id="S3.SS3.p5.1">where <math alttext="(\mathbf{P}^{*}_{i},x_{i}^{*})" class="ltx_Math" display="inline" id="S3.SS3.p5.1.m1.2"><semantics id="S3.SS3.p5.1.m1.2a"><mrow id="S3.SS3.p5.1.m1.2.2.2" xref="S3.SS3.p5.1.m1.2.2.3.cmml"><mo id="S3.SS3.p5.1.m1.2.2.2.3" stretchy="false" xref="S3.SS3.p5.1.m1.2.2.3.cmml">(</mo><msubsup id="S3.SS3.p5.1.m1.1.1.1.1" xref="S3.SS3.p5.1.m1.1.1.1.1.cmml"><mi id="S3.SS3.p5.1.m1.1.1.1.1.2.2" xref="S3.SS3.p5.1.m1.1.1.1.1.2.2.cmml">ğ</mi><mi id="S3.SS3.p5.1.m1.1.1.1.1.3" xref="S3.SS3.p5.1.m1.1.1.1.1.3.cmml">i</mi><mo id="S3.SS3.p5.1.m1.1.1.1.1.2.3" xref="S3.SS3.p5.1.m1.1.1.1.1.2.3.cmml">âˆ—</mo></msubsup><mo id="S3.SS3.p5.1.m1.2.2.2.4" xref="S3.SS3.p5.1.m1.2.2.3.cmml">,</mo><msubsup id="S3.SS3.p5.1.m1.2.2.2.2" xref="S3.SS3.p5.1.m1.2.2.2.2.cmml"><mi id="S3.SS3.p5.1.m1.2.2.2.2.2.2" xref="S3.SS3.p5.1.m1.2.2.2.2.2.2.cmml">x</mi><mi id="S3.SS3.p5.1.m1.2.2.2.2.2.3" xref="S3.SS3.p5.1.m1.2.2.2.2.2.3.cmml">i</mi><mo id="S3.SS3.p5.1.m1.2.2.2.2.3" xref="S3.SS3.p5.1.m1.2.2.2.2.3.cmml">âˆ—</mo></msubsup><mo id="S3.SS3.p5.1.m1.2.2.2.5" stretchy="false" xref="S3.SS3.p5.1.m1.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.1.m1.2b"><interval closure="open" id="S3.SS3.p5.1.m1.2.2.3.cmml" xref="S3.SS3.p5.1.m1.2.2.2"><apply id="S3.SS3.p5.1.m1.1.1.1.1.cmml" xref="S3.SS3.p5.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p5.1.m1.1.1.1.1.1.cmml" xref="S3.SS3.p5.1.m1.1.1.1.1">subscript</csymbol><apply id="S3.SS3.p5.1.m1.1.1.1.1.2.cmml" xref="S3.SS3.p5.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p5.1.m1.1.1.1.1.2.1.cmml" xref="S3.SS3.p5.1.m1.1.1.1.1">superscript</csymbol><ci id="S3.SS3.p5.1.m1.1.1.1.1.2.2.cmml" xref="S3.SS3.p5.1.m1.1.1.1.1.2.2">ğ</ci><times id="S3.SS3.p5.1.m1.1.1.1.1.2.3.cmml" xref="S3.SS3.p5.1.m1.1.1.1.1.2.3"></times></apply><ci id="S3.SS3.p5.1.m1.1.1.1.1.3.cmml" xref="S3.SS3.p5.1.m1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.SS3.p5.1.m1.2.2.2.2.cmml" xref="S3.SS3.p5.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p5.1.m1.2.2.2.2.1.cmml" xref="S3.SS3.p5.1.m1.2.2.2.2">superscript</csymbol><apply id="S3.SS3.p5.1.m1.2.2.2.2.2.cmml" xref="S3.SS3.p5.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p5.1.m1.2.2.2.2.2.1.cmml" xref="S3.SS3.p5.1.m1.2.2.2.2">subscript</csymbol><ci id="S3.SS3.p5.1.m1.2.2.2.2.2.2.cmml" xref="S3.SS3.p5.1.m1.2.2.2.2.2.2">ğ‘¥</ci><ci id="S3.SS3.p5.1.m1.2.2.2.2.2.3.cmml" xref="S3.SS3.p5.1.m1.2.2.2.2.2.3">ğ‘–</ci></apply><times id="S3.SS3.p5.1.m1.2.2.2.2.3.cmml" xref="S3.SS3.p5.1.m1.2.2.2.2.3"></times></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.1.m1.2c">(\mathbf{P}^{*}_{i},x_{i}^{*})</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.1.m1.2d">( bold_P start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT )</annotation></semantics></math> is the ground truth pose during training. At inference, we omit this term since ground truth is not available.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Model training details</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p1.1.1">Pose generator</span>
To prevent mode collapse, which occurs when all proposals are trained to match the ground truth as discussed in <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#S3.SS2" title="3.2 Pose generator â€£ 3 Method â€£ ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">3.2</span></a>, we only regress the one pose proposal that is closest to the ground truth pose: <math alttext="P^{*}_{i}=[\mathbf{R}_{i}^{*},\mathbf{t}_{i}^{*}]" class="ltx_Math" display="inline" id="S3.SS4.p1.1.m1.2"><semantics id="S3.SS4.p1.1.m1.2a"><mrow id="S3.SS4.p1.1.m1.2.2" xref="S3.SS4.p1.1.m1.2.2.cmml"><msubsup id="S3.SS4.p1.1.m1.2.2.4" xref="S3.SS4.p1.1.m1.2.2.4.cmml"><mi id="S3.SS4.p1.1.m1.2.2.4.2.2" xref="S3.SS4.p1.1.m1.2.2.4.2.2.cmml">P</mi><mi id="S3.SS4.p1.1.m1.2.2.4.3" xref="S3.SS4.p1.1.m1.2.2.4.3.cmml">i</mi><mo id="S3.SS4.p1.1.m1.2.2.4.2.3" xref="S3.SS4.p1.1.m1.2.2.4.2.3.cmml">âˆ—</mo></msubsup><mo id="S3.SS4.p1.1.m1.2.2.3" xref="S3.SS4.p1.1.m1.2.2.3.cmml">=</mo><mrow id="S3.SS4.p1.1.m1.2.2.2.2" xref="S3.SS4.p1.1.m1.2.2.2.3.cmml"><mo id="S3.SS4.p1.1.m1.2.2.2.2.3" stretchy="false" xref="S3.SS4.p1.1.m1.2.2.2.3.cmml">[</mo><msubsup id="S3.SS4.p1.1.m1.1.1.1.1.1" xref="S3.SS4.p1.1.m1.1.1.1.1.1.cmml"><mi id="S3.SS4.p1.1.m1.1.1.1.1.1.2.2" xref="S3.SS4.p1.1.m1.1.1.1.1.1.2.2.cmml">ğ‘</mi><mi id="S3.SS4.p1.1.m1.1.1.1.1.1.2.3" xref="S3.SS4.p1.1.m1.1.1.1.1.1.2.3.cmml">i</mi><mo id="S3.SS4.p1.1.m1.1.1.1.1.1.3" xref="S3.SS4.p1.1.m1.1.1.1.1.1.3.cmml">âˆ—</mo></msubsup><mo id="S3.SS4.p1.1.m1.2.2.2.2.4" xref="S3.SS4.p1.1.m1.2.2.2.3.cmml">,</mo><msubsup id="S3.SS4.p1.1.m1.2.2.2.2.2" xref="S3.SS4.p1.1.m1.2.2.2.2.2.cmml"><mi id="S3.SS4.p1.1.m1.2.2.2.2.2.2.2" xref="S3.SS4.p1.1.m1.2.2.2.2.2.2.2.cmml">ğ­</mi><mi id="S3.SS4.p1.1.m1.2.2.2.2.2.2.3" xref="S3.SS4.p1.1.m1.2.2.2.2.2.2.3.cmml">i</mi><mo id="S3.SS4.p1.1.m1.2.2.2.2.2.3" xref="S3.SS4.p1.1.m1.2.2.2.2.2.3.cmml">âˆ—</mo></msubsup><mo id="S3.SS4.p1.1.m1.2.2.2.2.5" stretchy="false" xref="S3.SS4.p1.1.m1.2.2.2.3.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.2b"><apply id="S3.SS4.p1.1.m1.2.2.cmml" xref="S3.SS4.p1.1.m1.2.2"><eq id="S3.SS4.p1.1.m1.2.2.3.cmml" xref="S3.SS4.p1.1.m1.2.2.3"></eq><apply id="S3.SS4.p1.1.m1.2.2.4.cmml" xref="S3.SS4.p1.1.m1.2.2.4"><csymbol cd="ambiguous" id="S3.SS4.p1.1.m1.2.2.4.1.cmml" xref="S3.SS4.p1.1.m1.2.2.4">subscript</csymbol><apply id="S3.SS4.p1.1.m1.2.2.4.2.cmml" xref="S3.SS4.p1.1.m1.2.2.4"><csymbol cd="ambiguous" id="S3.SS4.p1.1.m1.2.2.4.2.1.cmml" xref="S3.SS4.p1.1.m1.2.2.4">superscript</csymbol><ci id="S3.SS4.p1.1.m1.2.2.4.2.2.cmml" xref="S3.SS4.p1.1.m1.2.2.4.2.2">ğ‘ƒ</ci><times id="S3.SS4.p1.1.m1.2.2.4.2.3.cmml" xref="S3.SS4.p1.1.m1.2.2.4.2.3"></times></apply><ci id="S3.SS4.p1.1.m1.2.2.4.3.cmml" xref="S3.SS4.p1.1.m1.2.2.4.3">ğ‘–</ci></apply><interval closure="closed" id="S3.SS4.p1.1.m1.2.2.2.3.cmml" xref="S3.SS4.p1.1.m1.2.2.2.2"><apply id="S3.SS4.p1.1.m1.1.1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1.1.1.1">superscript</csymbol><apply id="S3.SS4.p1.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS4.p1.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.1.m1.1.1.1.1.1.2.1.cmml" xref="S3.SS4.p1.1.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS4.p1.1.m1.1.1.1.1.1.2.2.cmml" xref="S3.SS4.p1.1.m1.1.1.1.1.1.2.2">ğ‘</ci><ci id="S3.SS4.p1.1.m1.1.1.1.1.1.2.3.cmml" xref="S3.SS4.p1.1.m1.1.1.1.1.1.2.3">ğ‘–</ci></apply><times id="S3.SS4.p1.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS4.p1.1.m1.1.1.1.1.1.3"></times></apply><apply id="S3.SS4.p1.1.m1.2.2.2.2.2.cmml" xref="S3.SS4.p1.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS4.p1.1.m1.2.2.2.2.2.1.cmml" xref="S3.SS4.p1.1.m1.2.2.2.2.2">superscript</csymbol><apply id="S3.SS4.p1.1.m1.2.2.2.2.2.2.cmml" xref="S3.SS4.p1.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS4.p1.1.m1.2.2.2.2.2.2.1.cmml" xref="S3.SS4.p1.1.m1.2.2.2.2.2">subscript</csymbol><ci id="S3.SS4.p1.1.m1.2.2.2.2.2.2.2.cmml" xref="S3.SS4.p1.1.m1.2.2.2.2.2.2.2">ğ­</ci><ci id="S3.SS4.p1.1.m1.2.2.2.2.2.2.3.cmml" xref="S3.SS4.p1.1.m1.2.2.2.2.2.2.3">ğ‘–</ci></apply><times id="S3.SS4.p1.1.m1.2.2.2.2.2.3.cmml" xref="S3.SS4.p1.1.m1.2.2.2.2.2.3"></times></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.2c">P^{*}_{i}=[\mathbf{R}_{i}^{*},\mathbf{t}_{i}^{*}]</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.1.m1.2d">italic_P start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = [ bold_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT , bold_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ]</annotation></semantics></math> and no loss is computed to penalize other poses proposals. We define the "closest" camera pose as the one with the smallest geodesic distance:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{P}_{i}=[\mathbf{\hat{R}}_{i},\mathbf{\hat{t}}_{i}]=\arg\min_{P_{i,m}}d(P_%
{i,m},P^{*}_{i}),m\in[1,\mathrm{M}]" class="ltx_Math" display="block" id="S3.Ex3.m1.8"><semantics id="S3.Ex3.m1.8a"><mrow id="S3.Ex3.m1.8.8.2" xref="S3.Ex3.m1.8.8.3.cmml"><mrow id="S3.Ex3.m1.7.7.1.1" xref="S3.Ex3.m1.7.7.1.1.cmml"><msub id="S3.Ex3.m1.7.7.1.1.6" xref="S3.Ex3.m1.7.7.1.1.6.cmml"><mover accent="true" id="S3.Ex3.m1.7.7.1.1.6.2" xref="S3.Ex3.m1.7.7.1.1.6.2.cmml"><mi id="S3.Ex3.m1.7.7.1.1.6.2.2" xref="S3.Ex3.m1.7.7.1.1.6.2.2.cmml">P</mi><mo id="S3.Ex3.m1.7.7.1.1.6.2.1" xref="S3.Ex3.m1.7.7.1.1.6.2.1.cmml">^</mo></mover><mi id="S3.Ex3.m1.7.7.1.1.6.3" xref="S3.Ex3.m1.7.7.1.1.6.3.cmml">i</mi></msub><mo id="S3.Ex3.m1.7.7.1.1.7" xref="S3.Ex3.m1.7.7.1.1.7.cmml">=</mo><mrow id="S3.Ex3.m1.7.7.1.1.2.2" xref="S3.Ex3.m1.7.7.1.1.2.3.cmml"><mo id="S3.Ex3.m1.7.7.1.1.2.2.3" stretchy="false" xref="S3.Ex3.m1.7.7.1.1.2.3.cmml">[</mo><msub id="S3.Ex3.m1.7.7.1.1.1.1.1" xref="S3.Ex3.m1.7.7.1.1.1.1.1.cmml"><mover accent="true" id="S3.Ex3.m1.7.7.1.1.1.1.1.2" xref="S3.Ex3.m1.7.7.1.1.1.1.1.2.cmml"><mi id="S3.Ex3.m1.7.7.1.1.1.1.1.2.2" xref="S3.Ex3.m1.7.7.1.1.1.1.1.2.2.cmml">ğ‘</mi><mo id="S3.Ex3.m1.7.7.1.1.1.1.1.2.1" xref="S3.Ex3.m1.7.7.1.1.1.1.1.2.1.cmml">^</mo></mover><mi id="S3.Ex3.m1.7.7.1.1.1.1.1.3" xref="S3.Ex3.m1.7.7.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.Ex3.m1.7.7.1.1.2.2.4" xref="S3.Ex3.m1.7.7.1.1.2.3.cmml">,</mo><msub id="S3.Ex3.m1.7.7.1.1.2.2.2" xref="S3.Ex3.m1.7.7.1.1.2.2.2.cmml"><mover accent="true" id="S3.Ex3.m1.7.7.1.1.2.2.2.2" xref="S3.Ex3.m1.7.7.1.1.2.2.2.2.cmml"><mi id="S3.Ex3.m1.7.7.1.1.2.2.2.2.2" xref="S3.Ex3.m1.7.7.1.1.2.2.2.2.2.cmml">ğ­</mi><mo id="S3.Ex3.m1.7.7.1.1.2.2.2.2.1" xref="S3.Ex3.m1.7.7.1.1.2.2.2.2.1.cmml">^</mo></mover><mi id="S3.Ex3.m1.7.7.1.1.2.2.2.3" xref="S3.Ex3.m1.7.7.1.1.2.2.2.3.cmml">i</mi></msub><mo id="S3.Ex3.m1.7.7.1.1.2.2.5" stretchy="false" xref="S3.Ex3.m1.7.7.1.1.2.3.cmml">]</mo></mrow><mo id="S3.Ex3.m1.7.7.1.1.8" xref="S3.Ex3.m1.7.7.1.1.8.cmml">=</mo><mrow id="S3.Ex3.m1.7.7.1.1.4" xref="S3.Ex3.m1.7.7.1.1.4.cmml"><mrow id="S3.Ex3.m1.7.7.1.1.4.4" xref="S3.Ex3.m1.7.7.1.1.4.4.cmml"><mi id="S3.Ex3.m1.7.7.1.1.4.4.1" xref="S3.Ex3.m1.7.7.1.1.4.4.1.cmml">arg</mi><mo id="S3.Ex3.m1.7.7.1.1.4.4a" lspace="0.167em" xref="S3.Ex3.m1.7.7.1.1.4.4.cmml">â¡</mo><mrow id="S3.Ex3.m1.7.7.1.1.4.4.2" xref="S3.Ex3.m1.7.7.1.1.4.4.2.cmml"><munder id="S3.Ex3.m1.7.7.1.1.4.4.2.1" xref="S3.Ex3.m1.7.7.1.1.4.4.2.1.cmml"><mi id="S3.Ex3.m1.7.7.1.1.4.4.2.1.2" xref="S3.Ex3.m1.7.7.1.1.4.4.2.1.2.cmml">min</mi><msub id="S3.Ex3.m1.2.2.2" xref="S3.Ex3.m1.2.2.2.cmml"><mi id="S3.Ex3.m1.2.2.2.4" xref="S3.Ex3.m1.2.2.2.4.cmml">P</mi><mrow id="S3.Ex3.m1.2.2.2.2.2.4" xref="S3.Ex3.m1.2.2.2.2.2.3.cmml"><mi id="S3.Ex3.m1.1.1.1.1.1.1" xref="S3.Ex3.m1.1.1.1.1.1.1.cmml">i</mi><mo id="S3.Ex3.m1.2.2.2.2.2.4.1" xref="S3.Ex3.m1.2.2.2.2.2.3.cmml">,</mo><mi id="S3.Ex3.m1.2.2.2.2.2.2" xref="S3.Ex3.m1.2.2.2.2.2.2.cmml">m</mi></mrow></msub></munder><mo id="S3.Ex3.m1.7.7.1.1.4.4.2a" lspace="0.167em" xref="S3.Ex3.m1.7.7.1.1.4.4.2.cmml">â¡</mo><mi id="S3.Ex3.m1.7.7.1.1.4.4.2.2" xref="S3.Ex3.m1.7.7.1.1.4.4.2.2.cmml">d</mi></mrow></mrow><mo id="S3.Ex3.m1.7.7.1.1.4.3" xref="S3.Ex3.m1.7.7.1.1.4.3.cmml">â¢</mo><mrow id="S3.Ex3.m1.7.7.1.1.4.2.2" xref="S3.Ex3.m1.7.7.1.1.4.2.3.cmml"><mo id="S3.Ex3.m1.7.7.1.1.4.2.2.3" stretchy="false" xref="S3.Ex3.m1.7.7.1.1.4.2.3.cmml">(</mo><msub id="S3.Ex3.m1.7.7.1.1.3.1.1.1" xref="S3.Ex3.m1.7.7.1.1.3.1.1.1.cmml"><mi id="S3.Ex3.m1.7.7.1.1.3.1.1.1.2" xref="S3.Ex3.m1.7.7.1.1.3.1.1.1.2.cmml">P</mi><mrow id="S3.Ex3.m1.4.4.2.4" xref="S3.Ex3.m1.4.4.2.3.cmml"><mi id="S3.Ex3.m1.3.3.1.1" xref="S3.Ex3.m1.3.3.1.1.cmml">i</mi><mo id="S3.Ex3.m1.4.4.2.4.1" xref="S3.Ex3.m1.4.4.2.3.cmml">,</mo><mi id="S3.Ex3.m1.4.4.2.2" xref="S3.Ex3.m1.4.4.2.2.cmml">m</mi></mrow></msub><mo id="S3.Ex3.m1.7.7.1.1.4.2.2.4" xref="S3.Ex3.m1.7.7.1.1.4.2.3.cmml">,</mo><msubsup id="S3.Ex3.m1.7.7.1.1.4.2.2.2" xref="S3.Ex3.m1.7.7.1.1.4.2.2.2.cmml"><mi id="S3.Ex3.m1.7.7.1.1.4.2.2.2.2.2" xref="S3.Ex3.m1.7.7.1.1.4.2.2.2.2.2.cmml">P</mi><mi id="S3.Ex3.m1.7.7.1.1.4.2.2.2.3" xref="S3.Ex3.m1.7.7.1.1.4.2.2.2.3.cmml">i</mi><mo id="S3.Ex3.m1.7.7.1.1.4.2.2.2.2.3" xref="S3.Ex3.m1.7.7.1.1.4.2.2.2.2.3.cmml">âˆ—</mo></msubsup><mo id="S3.Ex3.m1.7.7.1.1.4.2.2.5" stretchy="false" xref="S3.Ex3.m1.7.7.1.1.4.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.Ex3.m1.8.8.2.3" xref="S3.Ex3.m1.8.8.3a.cmml">,</mo><mrow id="S3.Ex3.m1.8.8.2.2" xref="S3.Ex3.m1.8.8.2.2.cmml"><mi id="S3.Ex3.m1.8.8.2.2.2" xref="S3.Ex3.m1.8.8.2.2.2.cmml">m</mi><mo id="S3.Ex3.m1.8.8.2.2.1" xref="S3.Ex3.m1.8.8.2.2.1.cmml">âˆˆ</mo><mrow id="S3.Ex3.m1.8.8.2.2.3.2" xref="S3.Ex3.m1.8.8.2.2.3.1.cmml"><mo id="S3.Ex3.m1.8.8.2.2.3.2.1" stretchy="false" xref="S3.Ex3.m1.8.8.2.2.3.1.cmml">[</mo><mn id="S3.Ex3.m1.5.5" xref="S3.Ex3.m1.5.5.cmml">1</mn><mo id="S3.Ex3.m1.8.8.2.2.3.2.2" xref="S3.Ex3.m1.8.8.2.2.3.1.cmml">,</mo><mi id="S3.Ex3.m1.6.6" mathvariant="normal" xref="S3.Ex3.m1.6.6.cmml">M</mi><mo id="S3.Ex3.m1.8.8.2.2.3.2.3" stretchy="false" xref="S3.Ex3.m1.8.8.2.2.3.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex3.m1.8b"><apply id="S3.Ex3.m1.8.8.3.cmml" xref="S3.Ex3.m1.8.8.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.8.8.3a.cmml" xref="S3.Ex3.m1.8.8.2.3">formulae-sequence</csymbol><apply id="S3.Ex3.m1.7.7.1.1.cmml" xref="S3.Ex3.m1.7.7.1.1"><and id="S3.Ex3.m1.7.7.1.1a.cmml" xref="S3.Ex3.m1.7.7.1.1"></and><apply id="S3.Ex3.m1.7.7.1.1b.cmml" xref="S3.Ex3.m1.7.7.1.1"><eq id="S3.Ex3.m1.7.7.1.1.7.cmml" xref="S3.Ex3.m1.7.7.1.1.7"></eq><apply id="S3.Ex3.m1.7.7.1.1.6.cmml" xref="S3.Ex3.m1.7.7.1.1.6"><csymbol cd="ambiguous" id="S3.Ex3.m1.7.7.1.1.6.1.cmml" xref="S3.Ex3.m1.7.7.1.1.6">subscript</csymbol><apply id="S3.Ex3.m1.7.7.1.1.6.2.cmml" xref="S3.Ex3.m1.7.7.1.1.6.2"><ci id="S3.Ex3.m1.7.7.1.1.6.2.1.cmml" xref="S3.Ex3.m1.7.7.1.1.6.2.1">^</ci><ci id="S3.Ex3.m1.7.7.1.1.6.2.2.cmml" xref="S3.Ex3.m1.7.7.1.1.6.2.2">ğ‘ƒ</ci></apply><ci id="S3.Ex3.m1.7.7.1.1.6.3.cmml" xref="S3.Ex3.m1.7.7.1.1.6.3">ğ‘–</ci></apply><interval closure="closed" id="S3.Ex3.m1.7.7.1.1.2.3.cmml" xref="S3.Ex3.m1.7.7.1.1.2.2"><apply id="S3.Ex3.m1.7.7.1.1.1.1.1.cmml" xref="S3.Ex3.m1.7.7.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex3.m1.7.7.1.1.1.1.1.1.cmml" xref="S3.Ex3.m1.7.7.1.1.1.1.1">subscript</csymbol><apply id="S3.Ex3.m1.7.7.1.1.1.1.1.2.cmml" xref="S3.Ex3.m1.7.7.1.1.1.1.1.2"><ci id="S3.Ex3.m1.7.7.1.1.1.1.1.2.1.cmml" xref="S3.Ex3.m1.7.7.1.1.1.1.1.2.1">^</ci><ci id="S3.Ex3.m1.7.7.1.1.1.1.1.2.2.cmml" xref="S3.Ex3.m1.7.7.1.1.1.1.1.2.2">ğ‘</ci></apply><ci id="S3.Ex3.m1.7.7.1.1.1.1.1.3.cmml" xref="S3.Ex3.m1.7.7.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.Ex3.m1.7.7.1.1.2.2.2.cmml" xref="S3.Ex3.m1.7.7.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.7.7.1.1.2.2.2.1.cmml" xref="S3.Ex3.m1.7.7.1.1.2.2.2">subscript</csymbol><apply id="S3.Ex3.m1.7.7.1.1.2.2.2.2.cmml" xref="S3.Ex3.m1.7.7.1.1.2.2.2.2"><ci id="S3.Ex3.m1.7.7.1.1.2.2.2.2.1.cmml" xref="S3.Ex3.m1.7.7.1.1.2.2.2.2.1">^</ci><ci id="S3.Ex3.m1.7.7.1.1.2.2.2.2.2.cmml" xref="S3.Ex3.m1.7.7.1.1.2.2.2.2.2">ğ­</ci></apply><ci id="S3.Ex3.m1.7.7.1.1.2.2.2.3.cmml" xref="S3.Ex3.m1.7.7.1.1.2.2.2.3">ğ‘–</ci></apply></interval></apply><apply id="S3.Ex3.m1.7.7.1.1c.cmml" xref="S3.Ex3.m1.7.7.1.1"><eq id="S3.Ex3.m1.7.7.1.1.8.cmml" xref="S3.Ex3.m1.7.7.1.1.8"></eq><share href="https://arxiv.org/html/2408.09042v1#S3.Ex3.m1.7.7.1.1.2.cmml" id="S3.Ex3.m1.7.7.1.1d.cmml" xref="S3.Ex3.m1.7.7.1.1"></share><apply id="S3.Ex3.m1.7.7.1.1.4.cmml" xref="S3.Ex3.m1.7.7.1.1.4"><times id="S3.Ex3.m1.7.7.1.1.4.3.cmml" xref="S3.Ex3.m1.7.7.1.1.4.3"></times><apply id="S3.Ex3.m1.7.7.1.1.4.4.cmml" xref="S3.Ex3.m1.7.7.1.1.4.4"><arg id="S3.Ex3.m1.7.7.1.1.4.4.1.cmml" xref="S3.Ex3.m1.7.7.1.1.4.4.1"></arg><apply id="S3.Ex3.m1.7.7.1.1.4.4.2.cmml" xref="S3.Ex3.m1.7.7.1.1.4.4.2"><apply id="S3.Ex3.m1.7.7.1.1.4.4.2.1.cmml" xref="S3.Ex3.m1.7.7.1.1.4.4.2.1"><csymbol cd="ambiguous" id="S3.Ex3.m1.7.7.1.1.4.4.2.1.1.cmml" xref="S3.Ex3.m1.7.7.1.1.4.4.2.1">subscript</csymbol><min id="S3.Ex3.m1.7.7.1.1.4.4.2.1.2.cmml" xref="S3.Ex3.m1.7.7.1.1.4.4.2.1.2"></min><apply id="S3.Ex3.m1.2.2.2.cmml" xref="S3.Ex3.m1.2.2.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.2.2.2.3.cmml" xref="S3.Ex3.m1.2.2.2">subscript</csymbol><ci id="S3.Ex3.m1.2.2.2.4.cmml" xref="S3.Ex3.m1.2.2.2.4">ğ‘ƒ</ci><list id="S3.Ex3.m1.2.2.2.2.2.3.cmml" xref="S3.Ex3.m1.2.2.2.2.2.4"><ci id="S3.Ex3.m1.1.1.1.1.1.1.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1">ğ‘–</ci><ci id="S3.Ex3.m1.2.2.2.2.2.2.cmml" xref="S3.Ex3.m1.2.2.2.2.2.2">ğ‘š</ci></list></apply></apply><ci id="S3.Ex3.m1.7.7.1.1.4.4.2.2.cmml" xref="S3.Ex3.m1.7.7.1.1.4.4.2.2">ğ‘‘</ci></apply></apply><interval closure="open" id="S3.Ex3.m1.7.7.1.1.4.2.3.cmml" xref="S3.Ex3.m1.7.7.1.1.4.2.2"><apply id="S3.Ex3.m1.7.7.1.1.3.1.1.1.cmml" xref="S3.Ex3.m1.7.7.1.1.3.1.1.1"><csymbol cd="ambiguous" id="S3.Ex3.m1.7.7.1.1.3.1.1.1.1.cmml" xref="S3.Ex3.m1.7.7.1.1.3.1.1.1">subscript</csymbol><ci id="S3.Ex3.m1.7.7.1.1.3.1.1.1.2.cmml" xref="S3.Ex3.m1.7.7.1.1.3.1.1.1.2">ğ‘ƒ</ci><list id="S3.Ex3.m1.4.4.2.3.cmml" xref="S3.Ex3.m1.4.4.2.4"><ci id="S3.Ex3.m1.3.3.1.1.cmml" xref="S3.Ex3.m1.3.3.1.1">ğ‘–</ci><ci id="S3.Ex3.m1.4.4.2.2.cmml" xref="S3.Ex3.m1.4.4.2.2">ğ‘š</ci></list></apply><apply id="S3.Ex3.m1.7.7.1.1.4.2.2.2.cmml" xref="S3.Ex3.m1.7.7.1.1.4.2.2.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.7.7.1.1.4.2.2.2.1.cmml" xref="S3.Ex3.m1.7.7.1.1.4.2.2.2">subscript</csymbol><apply id="S3.Ex3.m1.7.7.1.1.4.2.2.2.2.cmml" xref="S3.Ex3.m1.7.7.1.1.4.2.2.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.7.7.1.1.4.2.2.2.2.1.cmml" xref="S3.Ex3.m1.7.7.1.1.4.2.2.2">superscript</csymbol><ci id="S3.Ex3.m1.7.7.1.1.4.2.2.2.2.2.cmml" xref="S3.Ex3.m1.7.7.1.1.4.2.2.2.2.2">ğ‘ƒ</ci><times id="S3.Ex3.m1.7.7.1.1.4.2.2.2.2.3.cmml" xref="S3.Ex3.m1.7.7.1.1.4.2.2.2.2.3"></times></apply><ci id="S3.Ex3.m1.7.7.1.1.4.2.2.2.3.cmml" xref="S3.Ex3.m1.7.7.1.1.4.2.2.2.3">ğ‘–</ci></apply></interval></apply></apply></apply><apply id="S3.Ex3.m1.8.8.2.2.cmml" xref="S3.Ex3.m1.8.8.2.2"><in id="S3.Ex3.m1.8.8.2.2.1.cmml" xref="S3.Ex3.m1.8.8.2.2.1"></in><ci id="S3.Ex3.m1.8.8.2.2.2.cmml" xref="S3.Ex3.m1.8.8.2.2.2">ğ‘š</ci><interval closure="closed" id="S3.Ex3.m1.8.8.2.2.3.1.cmml" xref="S3.Ex3.m1.8.8.2.2.3.2"><cn id="S3.Ex3.m1.5.5.cmml" type="integer" xref="S3.Ex3.m1.5.5">1</cn><ci id="S3.Ex3.m1.6.6.cmml" xref="S3.Ex3.m1.6.6">M</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex3.m1.8c">\hat{P}_{i}=[\mathbf{\hat{R}}_{i},\mathbf{\hat{t}}_{i}]=\arg\min_{P_{i,m}}d(P_%
{i,m},P^{*}_{i}),m\in[1,\mathrm{M}]</annotation><annotation encoding="application/x-llamapun" id="S3.Ex3.m1.8d">over^ start_ARG italic_P end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = [ over^ start_ARG bold_R end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , over^ start_ARG bold_t end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ] = roman_arg roman_min start_POSTSUBSCRIPT italic_P start_POSTSUBSCRIPT italic_i , italic_m end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_d ( italic_P start_POSTSUBSCRIPT italic_i , italic_m end_POSTSUBSCRIPT , italic_P start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) , italic_m âˆˆ [ 1 , roman_M ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">The loss function then is the geodesic distance of predicted rotation to ground truth and the L2 loss of the predicted translation.</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\vspace{-1mm}\mathcal{L}_{g}=\sum_{i}^{N}\arccos(\mathrm{Tr}(\mathbf{R}_{i}^{*%
}\mathbf{\hat{R}}_{i}))+\sum_{i}^{N}||\mathbf{t}_{i}^{*}-\mathbf{\hat{t}}_{i}||" class="ltx_Math" display="block" id="S3.E2.m1.3"><semantics id="S3.E2.m1.3a"><mrow id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml"><msub id="S3.E2.m1.3.3.4" xref="S3.E2.m1.3.3.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.3.3.4.2" xref="S3.E2.m1.3.3.4.2.cmml">â„’</mi><mi id="S3.E2.m1.3.3.4.3" xref="S3.E2.m1.3.3.4.3.cmml">g</mi></msub><mo id="S3.E2.m1.3.3.3" rspace="0.111em" xref="S3.E2.m1.3.3.3.cmml">=</mo><mrow id="S3.E2.m1.3.3.2" xref="S3.E2.m1.3.3.2.cmml"><mrow id="S3.E2.m1.2.2.1.1" xref="S3.E2.m1.2.2.1.1.cmml"><munderover id="S3.E2.m1.2.2.1.1.2" xref="S3.E2.m1.2.2.1.1.2.cmml"><mo id="S3.E2.m1.2.2.1.1.2.2.2" movablelimits="false" xref="S3.E2.m1.2.2.1.1.2.2.2.cmml">âˆ‘</mo><mi id="S3.E2.m1.2.2.1.1.2.2.3" xref="S3.E2.m1.2.2.1.1.2.2.3.cmml">i</mi><mi id="S3.E2.m1.2.2.1.1.2.3" xref="S3.E2.m1.2.2.1.1.2.3.cmml">N</mi></munderover><mrow id="S3.E2.m1.2.2.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.2.cmml"><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">arccos</mi><mo id="S3.E2.m1.2.2.1.1.1.1a" xref="S3.E2.m1.2.2.1.1.1.2.cmml">â¡</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.2.cmml"><mo id="S3.E2.m1.2.2.1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.2.2.1.1.1.2.cmml">(</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3.cmml">Tr</mi><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><msubsup id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2.cmml">ğ‘</mi><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.3.cmml">i</mi><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.3.cmml">âˆ—</mo></msubsup><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml">â¢</mo><msub id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml"><mover accent="true" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.2.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.2.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.2.2.cmml">ğ‘</mi><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.2.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.2.1.cmml">^</mo></mover><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.2.2.1.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.2.2.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2.m1.3.3.2.3" rspace="0.055em" xref="S3.E2.m1.3.3.2.3.cmml">+</mo><mrow id="S3.E2.m1.3.3.2.2" xref="S3.E2.m1.3.3.2.2.cmml"><munderover id="S3.E2.m1.3.3.2.2.2" xref="S3.E2.m1.3.3.2.2.2.cmml"><mo id="S3.E2.m1.3.3.2.2.2.2.2" movablelimits="false" rspace="0em" xref="S3.E2.m1.3.3.2.2.2.2.2.cmml">âˆ‘</mo><mi id="S3.E2.m1.3.3.2.2.2.2.3" xref="S3.E2.m1.3.3.2.2.2.2.3.cmml">i</mi><mi id="S3.E2.m1.3.3.2.2.2.3" xref="S3.E2.m1.3.3.2.2.2.3.cmml">N</mi></munderover><mrow id="S3.E2.m1.3.3.2.2.1.1" xref="S3.E2.m1.3.3.2.2.1.2.cmml"><mo id="S3.E2.m1.3.3.2.2.1.1.2" stretchy="false" xref="S3.E2.m1.3.3.2.2.1.2.1.cmml">â€–</mo><mrow id="S3.E2.m1.3.3.2.2.1.1.1" xref="S3.E2.m1.3.3.2.2.1.1.1.cmml"><msubsup id="S3.E2.m1.3.3.2.2.1.1.1.2" xref="S3.E2.m1.3.3.2.2.1.1.1.2.cmml"><mi id="S3.E2.m1.3.3.2.2.1.1.1.2.2.2" xref="S3.E2.m1.3.3.2.2.1.1.1.2.2.2.cmml">ğ­</mi><mi id="S3.E2.m1.3.3.2.2.1.1.1.2.2.3" xref="S3.E2.m1.3.3.2.2.1.1.1.2.2.3.cmml">i</mi><mo id="S3.E2.m1.3.3.2.2.1.1.1.2.3" xref="S3.E2.m1.3.3.2.2.1.1.1.2.3.cmml">âˆ—</mo></msubsup><mo id="S3.E2.m1.3.3.2.2.1.1.1.1" xref="S3.E2.m1.3.3.2.2.1.1.1.1.cmml">âˆ’</mo><msub id="S3.E2.m1.3.3.2.2.1.1.1.3" xref="S3.E2.m1.3.3.2.2.1.1.1.3.cmml"><mover accent="true" id="S3.E2.m1.3.3.2.2.1.1.1.3.2" xref="S3.E2.m1.3.3.2.2.1.1.1.3.2.cmml"><mi id="S3.E2.m1.3.3.2.2.1.1.1.3.2.2" xref="S3.E2.m1.3.3.2.2.1.1.1.3.2.2.cmml">ğ­</mi><mo id="S3.E2.m1.3.3.2.2.1.1.1.3.2.1" xref="S3.E2.m1.3.3.2.2.1.1.1.3.2.1.cmml">^</mo></mover><mi id="S3.E2.m1.3.3.2.2.1.1.1.3.3" xref="S3.E2.m1.3.3.2.2.1.1.1.3.3.cmml">i</mi></msub></mrow><mo id="S3.E2.m1.3.3.2.2.1.1.3" stretchy="false" xref="S3.E2.m1.3.3.2.2.1.2.1.cmml">â€–</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.3b"><apply id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3"><eq id="S3.E2.m1.3.3.3.cmml" xref="S3.E2.m1.3.3.3"></eq><apply id="S3.E2.m1.3.3.4.cmml" xref="S3.E2.m1.3.3.4"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.4.1.cmml" xref="S3.E2.m1.3.3.4">subscript</csymbol><ci id="S3.E2.m1.3.3.4.2.cmml" xref="S3.E2.m1.3.3.4.2">â„’</ci><ci id="S3.E2.m1.3.3.4.3.cmml" xref="S3.E2.m1.3.3.4.3">ğ‘”</ci></apply><apply id="S3.E2.m1.3.3.2.cmml" xref="S3.E2.m1.3.3.2"><plus id="S3.E2.m1.3.3.2.3.cmml" xref="S3.E2.m1.3.3.2.3"></plus><apply id="S3.E2.m1.2.2.1.1.cmml" xref="S3.E2.m1.2.2.1.1"><apply id="S3.E2.m1.2.2.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.2.1.cmml" xref="S3.E2.m1.2.2.1.1.2">superscript</csymbol><apply id="S3.E2.m1.2.2.1.1.2.2.cmml" xref="S3.E2.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.2.2.1.cmml" xref="S3.E2.m1.2.2.1.1.2">subscript</csymbol><sum id="S3.E2.m1.2.2.1.1.2.2.2.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2"></sum><ci id="S3.E2.m1.2.2.1.1.2.2.3.cmml" xref="S3.E2.m1.2.2.1.1.2.2.3">ğ‘–</ci></apply><ci id="S3.E2.m1.2.2.1.1.2.3.cmml" xref="S3.E2.m1.2.2.1.1.2.3">ğ‘</ci></apply><apply id="S3.E2.m1.2.2.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1"><arccos id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"></arccos><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1"><times id="S3.E2.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2"></times><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3">Tr</ci><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1"><times id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1"></times><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2">ğ‘</ci><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.3">ğ‘–</ci></apply><times id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.3"></times></apply><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.2"><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.2.1">^</ci><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.2.2">ğ‘</ci></apply><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.3">ğ‘–</ci></apply></apply></apply></apply></apply><apply id="S3.E2.m1.3.3.2.2.cmml" xref="S3.E2.m1.3.3.2.2"><apply id="S3.E2.m1.3.3.2.2.2.cmml" xref="S3.E2.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.2.2.2.1.cmml" xref="S3.E2.m1.3.3.2.2.2">superscript</csymbol><apply id="S3.E2.m1.3.3.2.2.2.2.cmml" xref="S3.E2.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.2.2.2.2.1.cmml" xref="S3.E2.m1.3.3.2.2.2">subscript</csymbol><sum id="S3.E2.m1.3.3.2.2.2.2.2.cmml" xref="S3.E2.m1.3.3.2.2.2.2.2"></sum><ci id="S3.E2.m1.3.3.2.2.2.2.3.cmml" xref="S3.E2.m1.3.3.2.2.2.2.3">ğ‘–</ci></apply><ci id="S3.E2.m1.3.3.2.2.2.3.cmml" xref="S3.E2.m1.3.3.2.2.2.3">ğ‘</ci></apply><apply id="S3.E2.m1.3.3.2.2.1.2.cmml" xref="S3.E2.m1.3.3.2.2.1.1"><csymbol cd="latexml" id="S3.E2.m1.3.3.2.2.1.2.1.cmml" xref="S3.E2.m1.3.3.2.2.1.1.2">norm</csymbol><apply id="S3.E2.m1.3.3.2.2.1.1.1.cmml" xref="S3.E2.m1.3.3.2.2.1.1.1"><minus id="S3.E2.m1.3.3.2.2.1.1.1.1.cmml" xref="S3.E2.m1.3.3.2.2.1.1.1.1"></minus><apply id="S3.E2.m1.3.3.2.2.1.1.1.2.cmml" xref="S3.E2.m1.3.3.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.2.2.1.1.1.2.1.cmml" xref="S3.E2.m1.3.3.2.2.1.1.1.2">superscript</csymbol><apply id="S3.E2.m1.3.3.2.2.1.1.1.2.2.cmml" xref="S3.E2.m1.3.3.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.2.2.1.1.1.2.2.1.cmml" xref="S3.E2.m1.3.3.2.2.1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.3.3.2.2.1.1.1.2.2.2.cmml" xref="S3.E2.m1.3.3.2.2.1.1.1.2.2.2">ğ­</ci><ci id="S3.E2.m1.3.3.2.2.1.1.1.2.2.3.cmml" xref="S3.E2.m1.3.3.2.2.1.1.1.2.2.3">ğ‘–</ci></apply><times id="S3.E2.m1.3.3.2.2.1.1.1.2.3.cmml" xref="S3.E2.m1.3.3.2.2.1.1.1.2.3"></times></apply><apply id="S3.E2.m1.3.3.2.2.1.1.1.3.cmml" xref="S3.E2.m1.3.3.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.2.2.1.1.1.3.1.cmml" xref="S3.E2.m1.3.3.2.2.1.1.1.3">subscript</csymbol><apply id="S3.E2.m1.3.3.2.2.1.1.1.3.2.cmml" xref="S3.E2.m1.3.3.2.2.1.1.1.3.2"><ci id="S3.E2.m1.3.3.2.2.1.1.1.3.2.1.cmml" xref="S3.E2.m1.3.3.2.2.1.1.1.3.2.1">^</ci><ci id="S3.E2.m1.3.3.2.2.1.1.1.3.2.2.cmml" xref="S3.E2.m1.3.3.2.2.1.1.1.3.2.2">ğ­</ci></apply><ci id="S3.E2.m1.3.3.2.2.1.1.1.3.3.cmml" xref="S3.E2.m1.3.3.2.2.1.1.1.3.3">ğ‘–</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.3c">\vspace{-1mm}\mathcal{L}_{g}=\sum_{i}^{N}\arccos(\mathrm{Tr}(\mathbf{R}_{i}^{*%
}\mathbf{\hat{R}}_{i}))+\sum_{i}^{N}||\mathbf{t}_{i}^{*}-\mathbf{\hat{t}}_{i}||</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.3d">caligraphic_L start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT = âˆ‘ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_arccos ( roman_Tr ( bold_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT over^ start_ARG bold_R end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) + âˆ‘ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT | | bold_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT - over^ start_ARG bold_t end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | |</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p3.1.1">Pose discriminator</span>
During training, we also add the ground truth to the network so <math alttext="x_{i}^{*}=\mathbf{MLP}(\mathbf{g}_{i},\mathbf{MLP}(p_{i}^{*}))" class="ltx_Math" display="inline" id="S3.SS4.p3.1.m1.2"><semantics id="S3.SS4.p3.1.m1.2a"><mrow id="S3.SS4.p3.1.m1.2.2" xref="S3.SS4.p3.1.m1.2.2.cmml"><msubsup id="S3.SS4.p3.1.m1.2.2.4" xref="S3.SS4.p3.1.m1.2.2.4.cmml"><mi id="S3.SS4.p3.1.m1.2.2.4.2.2" xref="S3.SS4.p3.1.m1.2.2.4.2.2.cmml">x</mi><mi id="S3.SS4.p3.1.m1.2.2.4.2.3" xref="S3.SS4.p3.1.m1.2.2.4.2.3.cmml">i</mi><mo id="S3.SS4.p3.1.m1.2.2.4.3" xref="S3.SS4.p3.1.m1.2.2.4.3.cmml">âˆ—</mo></msubsup><mo id="S3.SS4.p3.1.m1.2.2.3" xref="S3.SS4.p3.1.m1.2.2.3.cmml">=</mo><mrow id="S3.SS4.p3.1.m1.2.2.2" xref="S3.SS4.p3.1.m1.2.2.2.cmml"><mi id="S3.SS4.p3.1.m1.2.2.2.4" xref="S3.SS4.p3.1.m1.2.2.2.4.cmml">ğŒğ‹ğ</mi><mo id="S3.SS4.p3.1.m1.2.2.2.3" xref="S3.SS4.p3.1.m1.2.2.2.3.cmml">â¢</mo><mrow id="S3.SS4.p3.1.m1.2.2.2.2.2" xref="S3.SS4.p3.1.m1.2.2.2.2.3.cmml"><mo id="S3.SS4.p3.1.m1.2.2.2.2.2.3" stretchy="false" xref="S3.SS4.p3.1.m1.2.2.2.2.3.cmml">(</mo><msub id="S3.SS4.p3.1.m1.1.1.1.1.1.1" xref="S3.SS4.p3.1.m1.1.1.1.1.1.1.cmml"><mi id="S3.SS4.p3.1.m1.1.1.1.1.1.1.2" xref="S3.SS4.p3.1.m1.1.1.1.1.1.1.2.cmml">ğ </mi><mi id="S3.SS4.p3.1.m1.1.1.1.1.1.1.3" xref="S3.SS4.p3.1.m1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS4.p3.1.m1.2.2.2.2.2.4" xref="S3.SS4.p3.1.m1.2.2.2.2.3.cmml">,</mo><mrow id="S3.SS4.p3.1.m1.2.2.2.2.2.2" xref="S3.SS4.p3.1.m1.2.2.2.2.2.2.cmml"><mi id="S3.SS4.p3.1.m1.2.2.2.2.2.2.3" xref="S3.SS4.p3.1.m1.2.2.2.2.2.2.3.cmml">ğŒğ‹ğ</mi><mo id="S3.SS4.p3.1.m1.2.2.2.2.2.2.2" xref="S3.SS4.p3.1.m1.2.2.2.2.2.2.2.cmml">â¢</mo><mrow id="S3.SS4.p3.1.m1.2.2.2.2.2.2.1.1" xref="S3.SS4.p3.1.m1.2.2.2.2.2.2.1.1.1.cmml"><mo id="S3.SS4.p3.1.m1.2.2.2.2.2.2.1.1.2" stretchy="false" xref="S3.SS4.p3.1.m1.2.2.2.2.2.2.1.1.1.cmml">(</mo><msubsup id="S3.SS4.p3.1.m1.2.2.2.2.2.2.1.1.1" xref="S3.SS4.p3.1.m1.2.2.2.2.2.2.1.1.1.cmml"><mi id="S3.SS4.p3.1.m1.2.2.2.2.2.2.1.1.1.2.2" xref="S3.SS4.p3.1.m1.2.2.2.2.2.2.1.1.1.2.2.cmml">p</mi><mi id="S3.SS4.p3.1.m1.2.2.2.2.2.2.1.1.1.2.3" xref="S3.SS4.p3.1.m1.2.2.2.2.2.2.1.1.1.2.3.cmml">i</mi><mo id="S3.SS4.p3.1.m1.2.2.2.2.2.2.1.1.1.3" xref="S3.SS4.p3.1.m1.2.2.2.2.2.2.1.1.1.3.cmml">âˆ—</mo></msubsup><mo id="S3.SS4.p3.1.m1.2.2.2.2.2.2.1.1.3" stretchy="false" xref="S3.SS4.p3.1.m1.2.2.2.2.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS4.p3.1.m1.2.2.2.2.2.5" stretchy="false" xref="S3.SS4.p3.1.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.2b"><apply id="S3.SS4.p3.1.m1.2.2.cmml" xref="S3.SS4.p3.1.m1.2.2"><eq id="S3.SS4.p3.1.m1.2.2.3.cmml" xref="S3.SS4.p3.1.m1.2.2.3"></eq><apply id="S3.SS4.p3.1.m1.2.2.4.cmml" xref="S3.SS4.p3.1.m1.2.2.4"><csymbol cd="ambiguous" id="S3.SS4.p3.1.m1.2.2.4.1.cmml" xref="S3.SS4.p3.1.m1.2.2.4">superscript</csymbol><apply id="S3.SS4.p3.1.m1.2.2.4.2.cmml" xref="S3.SS4.p3.1.m1.2.2.4"><csymbol cd="ambiguous" id="S3.SS4.p3.1.m1.2.2.4.2.1.cmml" xref="S3.SS4.p3.1.m1.2.2.4">subscript</csymbol><ci id="S3.SS4.p3.1.m1.2.2.4.2.2.cmml" xref="S3.SS4.p3.1.m1.2.2.4.2.2">ğ‘¥</ci><ci id="S3.SS4.p3.1.m1.2.2.4.2.3.cmml" xref="S3.SS4.p3.1.m1.2.2.4.2.3">ğ‘–</ci></apply><times id="S3.SS4.p3.1.m1.2.2.4.3.cmml" xref="S3.SS4.p3.1.m1.2.2.4.3"></times></apply><apply id="S3.SS4.p3.1.m1.2.2.2.cmml" xref="S3.SS4.p3.1.m1.2.2.2"><times id="S3.SS4.p3.1.m1.2.2.2.3.cmml" xref="S3.SS4.p3.1.m1.2.2.2.3"></times><ci id="S3.SS4.p3.1.m1.2.2.2.4.cmml" xref="S3.SS4.p3.1.m1.2.2.2.4">ğŒğ‹ğ</ci><interval closure="open" id="S3.SS4.p3.1.m1.2.2.2.2.3.cmml" xref="S3.SS4.p3.1.m1.2.2.2.2.2"><apply id="S3.SS4.p3.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS4.p3.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.SS4.p3.1.m1.1.1.1.1.1.1.2">ğ </ci><ci id="S3.SS4.p3.1.m1.1.1.1.1.1.1.3.cmml" xref="S3.SS4.p3.1.m1.1.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.SS4.p3.1.m1.2.2.2.2.2.2.cmml" xref="S3.SS4.p3.1.m1.2.2.2.2.2.2"><times id="S3.SS4.p3.1.m1.2.2.2.2.2.2.2.cmml" xref="S3.SS4.p3.1.m1.2.2.2.2.2.2.2"></times><ci id="S3.SS4.p3.1.m1.2.2.2.2.2.2.3.cmml" xref="S3.SS4.p3.1.m1.2.2.2.2.2.2.3">ğŒğ‹ğ</ci><apply id="S3.SS4.p3.1.m1.2.2.2.2.2.2.1.1.1.cmml" xref="S3.SS4.p3.1.m1.2.2.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.1.m1.2.2.2.2.2.2.1.1.1.1.cmml" xref="S3.SS4.p3.1.m1.2.2.2.2.2.2.1.1">superscript</csymbol><apply id="S3.SS4.p3.1.m1.2.2.2.2.2.2.1.1.1.2.cmml" xref="S3.SS4.p3.1.m1.2.2.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.1.m1.2.2.2.2.2.2.1.1.1.2.1.cmml" xref="S3.SS4.p3.1.m1.2.2.2.2.2.2.1.1">subscript</csymbol><ci id="S3.SS4.p3.1.m1.2.2.2.2.2.2.1.1.1.2.2.cmml" xref="S3.SS4.p3.1.m1.2.2.2.2.2.2.1.1.1.2.2">ğ‘</ci><ci id="S3.SS4.p3.1.m1.2.2.2.2.2.2.1.1.1.2.3.cmml" xref="S3.SS4.p3.1.m1.2.2.2.2.2.2.1.1.1.2.3">ğ‘–</ci></apply><times id="S3.SS4.p3.1.m1.2.2.2.2.2.2.1.1.1.3.cmml" xref="S3.SS4.p3.1.m1.2.2.2.2.2.2.1.1.1.3"></times></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.2c">x_{i}^{*}=\mathbf{MLP}(\mathbf{g}_{i},\mathbf{MLP}(p_{i}^{*}))</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.1.m1.2d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT = bold_MLP ( bold_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_MLP ( italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ) )</annotation></semantics></math>. The loss function for the discriminator then is the contrastive negative log-likelihood loss to differentiate ground truth from the rest.</p>
</div>
<div class="ltx_para" id="S3.SS4.p4">
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S5.EGx2">
<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\vspace{-1mm}\mathcal{L}_{d}" class="ltx_Math" display="inline" id="S3.E3.m1.1"><semantics id="S3.E3.m1.1a"><msub id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.1.1.2" xref="S3.E3.m1.1.1.2.cmml">â„’</mi><mi id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1">subscript</csymbol><ci id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2">â„’</ci><ci id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\displaystyle\vspace{-1mm}\mathcal{L}_{d}</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=-\sum_{i}^{N}\log\hat{p}(P_{i}^{*}|\mathbf{I}_{1},...,\mathbf{I}%
_{N},i)" class="ltx_Math" display="inline" id="S3.E3.m2.3"><semantics id="S3.E3.m2.3a"><mrow id="S3.E3.m2.3.3" xref="S3.E3.m2.3.3.cmml"><mi id="S3.E3.m2.3.3.3" xref="S3.E3.m2.3.3.3.cmml"></mi><mo id="S3.E3.m2.3.3.2" xref="S3.E3.m2.3.3.2.cmml">=</mo><mrow id="S3.E3.m2.3.3.1" xref="S3.E3.m2.3.3.1.cmml"><mo id="S3.E3.m2.3.3.1a" xref="S3.E3.m2.3.3.1.cmml">âˆ’</mo><mrow id="S3.E3.m2.3.3.1.1" xref="S3.E3.m2.3.3.1.1.cmml"><mstyle displaystyle="true" id="S3.E3.m2.3.3.1.1.2" xref="S3.E3.m2.3.3.1.1.2.cmml"><munderover id="S3.E3.m2.3.3.1.1.2a" xref="S3.E3.m2.3.3.1.1.2.cmml"><mo id="S3.E3.m2.3.3.1.1.2.2.2" movablelimits="false" xref="S3.E3.m2.3.3.1.1.2.2.2.cmml">âˆ‘</mo><mi id="S3.E3.m2.3.3.1.1.2.2.3" xref="S3.E3.m2.3.3.1.1.2.2.3.cmml">i</mi><mi id="S3.E3.m2.3.3.1.1.2.3" xref="S3.E3.m2.3.3.1.1.2.3.cmml">N</mi></munderover></mstyle><mrow id="S3.E3.m2.3.3.1.1.1" xref="S3.E3.m2.3.3.1.1.1.cmml"><mrow id="S3.E3.m2.3.3.1.1.1.3" xref="S3.E3.m2.3.3.1.1.1.3.cmml"><mi id="S3.E3.m2.3.3.1.1.1.3.1" xref="S3.E3.m2.3.3.1.1.1.3.1.cmml">log</mi><mo id="S3.E3.m2.3.3.1.1.1.3a" lspace="0.167em" xref="S3.E3.m2.3.3.1.1.1.3.cmml">â¡</mo><mover accent="true" id="S3.E3.m2.3.3.1.1.1.3.2" xref="S3.E3.m2.3.3.1.1.1.3.2.cmml"><mi id="S3.E3.m2.3.3.1.1.1.3.2.2" xref="S3.E3.m2.3.3.1.1.1.3.2.2.cmml">p</mi><mo id="S3.E3.m2.3.3.1.1.1.3.2.1" xref="S3.E3.m2.3.3.1.1.1.3.2.1.cmml">^</mo></mover></mrow><mo id="S3.E3.m2.3.3.1.1.1.2" xref="S3.E3.m2.3.3.1.1.1.2.cmml">â¢</mo><mrow id="S3.E3.m2.3.3.1.1.1.1.1" xref="S3.E3.m2.3.3.1.1.1.1.1.1.cmml"><mo id="S3.E3.m2.3.3.1.1.1.1.1.2" stretchy="false" xref="S3.E3.m2.3.3.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m2.3.3.1.1.1.1.1.1" xref="S3.E3.m2.3.3.1.1.1.1.1.1.cmml"><msubsup id="S3.E3.m2.3.3.1.1.1.1.1.1.4" xref="S3.E3.m2.3.3.1.1.1.1.1.1.4.cmml"><mi id="S3.E3.m2.3.3.1.1.1.1.1.1.4.2.2" xref="S3.E3.m2.3.3.1.1.1.1.1.1.4.2.2.cmml">P</mi><mi id="S3.E3.m2.3.3.1.1.1.1.1.1.4.2.3" xref="S3.E3.m2.3.3.1.1.1.1.1.1.4.2.3.cmml">i</mi><mo id="S3.E3.m2.3.3.1.1.1.1.1.1.4.3" xref="S3.E3.m2.3.3.1.1.1.1.1.1.4.3.cmml">âˆ—</mo></msubsup><mo fence="false" id="S3.E3.m2.3.3.1.1.1.1.1.1.3" xref="S3.E3.m2.3.3.1.1.1.1.1.1.3.cmml">|</mo><mrow id="S3.E3.m2.3.3.1.1.1.1.1.1.2.2" xref="S3.E3.m2.3.3.1.1.1.1.1.1.2.3.cmml"><msub id="S3.E3.m2.3.3.1.1.1.1.1.1.1.1.1" xref="S3.E3.m2.3.3.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m2.3.3.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m2.3.3.1.1.1.1.1.1.1.1.1.2.cmml">ğˆ</mi><mn id="S3.E3.m2.3.3.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m2.3.3.1.1.1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.E3.m2.3.3.1.1.1.1.1.1.2.2.3" xref="S3.E3.m2.3.3.1.1.1.1.1.1.2.3.cmml">,</mo><mi id="S3.E3.m2.1.1" mathvariant="normal" xref="S3.E3.m2.1.1.cmml">â€¦</mi><mo id="S3.E3.m2.3.3.1.1.1.1.1.1.2.2.4" xref="S3.E3.m2.3.3.1.1.1.1.1.1.2.3.cmml">,</mo><msub id="S3.E3.m2.3.3.1.1.1.1.1.1.2.2.2" xref="S3.E3.m2.3.3.1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.E3.m2.3.3.1.1.1.1.1.1.2.2.2.2" xref="S3.E3.m2.3.3.1.1.1.1.1.1.2.2.2.2.cmml">ğˆ</mi><mi id="S3.E3.m2.3.3.1.1.1.1.1.1.2.2.2.3" xref="S3.E3.m2.3.3.1.1.1.1.1.1.2.2.2.3.cmml">N</mi></msub><mo id="S3.E3.m2.3.3.1.1.1.1.1.1.2.2.5" xref="S3.E3.m2.3.3.1.1.1.1.1.1.2.3.cmml">,</mo><mi id="S3.E3.m2.2.2" xref="S3.E3.m2.2.2.cmml">i</mi></mrow></mrow><mo id="S3.E3.m2.3.3.1.1.1.1.1.3" stretchy="false" xref="S3.E3.m2.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m2.3b"><apply id="S3.E3.m2.3.3.cmml" xref="S3.E3.m2.3.3"><eq id="S3.E3.m2.3.3.2.cmml" xref="S3.E3.m2.3.3.2"></eq><csymbol cd="latexml" id="S3.E3.m2.3.3.3.cmml" xref="S3.E3.m2.3.3.3">absent</csymbol><apply id="S3.E3.m2.3.3.1.cmml" xref="S3.E3.m2.3.3.1"><minus id="S3.E3.m2.3.3.1.2.cmml" xref="S3.E3.m2.3.3.1"></minus><apply id="S3.E3.m2.3.3.1.1.cmml" xref="S3.E3.m2.3.3.1.1"><apply id="S3.E3.m2.3.3.1.1.2.cmml" xref="S3.E3.m2.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m2.3.3.1.1.2.1.cmml" xref="S3.E3.m2.3.3.1.1.2">superscript</csymbol><apply id="S3.E3.m2.3.3.1.1.2.2.cmml" xref="S3.E3.m2.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m2.3.3.1.1.2.2.1.cmml" xref="S3.E3.m2.3.3.1.1.2">subscript</csymbol><sum id="S3.E3.m2.3.3.1.1.2.2.2.cmml" xref="S3.E3.m2.3.3.1.1.2.2.2"></sum><ci id="S3.E3.m2.3.3.1.1.2.2.3.cmml" xref="S3.E3.m2.3.3.1.1.2.2.3">ğ‘–</ci></apply><ci id="S3.E3.m2.3.3.1.1.2.3.cmml" xref="S3.E3.m2.3.3.1.1.2.3">ğ‘</ci></apply><apply id="S3.E3.m2.3.3.1.1.1.cmml" xref="S3.E3.m2.3.3.1.1.1"><times id="S3.E3.m2.3.3.1.1.1.2.cmml" xref="S3.E3.m2.3.3.1.1.1.2"></times><apply id="S3.E3.m2.3.3.1.1.1.3.cmml" xref="S3.E3.m2.3.3.1.1.1.3"><log id="S3.E3.m2.3.3.1.1.1.3.1.cmml" xref="S3.E3.m2.3.3.1.1.1.3.1"></log><apply id="S3.E3.m2.3.3.1.1.1.3.2.cmml" xref="S3.E3.m2.3.3.1.1.1.3.2"><ci id="S3.E3.m2.3.3.1.1.1.3.2.1.cmml" xref="S3.E3.m2.3.3.1.1.1.3.2.1">^</ci><ci id="S3.E3.m2.3.3.1.1.1.3.2.2.cmml" xref="S3.E3.m2.3.3.1.1.1.3.2.2">ğ‘</ci></apply></apply><apply id="S3.E3.m2.3.3.1.1.1.1.1.1.cmml" xref="S3.E3.m2.3.3.1.1.1.1.1"><csymbol cd="latexml" id="S3.E3.m2.3.3.1.1.1.1.1.1.3.cmml" xref="S3.E3.m2.3.3.1.1.1.1.1.1.3">conditional</csymbol><apply id="S3.E3.m2.3.3.1.1.1.1.1.1.4.cmml" xref="S3.E3.m2.3.3.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E3.m2.3.3.1.1.1.1.1.1.4.1.cmml" xref="S3.E3.m2.3.3.1.1.1.1.1.1.4">superscript</csymbol><apply id="S3.E3.m2.3.3.1.1.1.1.1.1.4.2.cmml" xref="S3.E3.m2.3.3.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E3.m2.3.3.1.1.1.1.1.1.4.2.1.cmml" xref="S3.E3.m2.3.3.1.1.1.1.1.1.4">subscript</csymbol><ci id="S3.E3.m2.3.3.1.1.1.1.1.1.4.2.2.cmml" xref="S3.E3.m2.3.3.1.1.1.1.1.1.4.2.2">ğ‘ƒ</ci><ci id="S3.E3.m2.3.3.1.1.1.1.1.1.4.2.3.cmml" xref="S3.E3.m2.3.3.1.1.1.1.1.1.4.2.3">ğ‘–</ci></apply><times id="S3.E3.m2.3.3.1.1.1.1.1.1.4.3.cmml" xref="S3.E3.m2.3.3.1.1.1.1.1.1.4.3"></times></apply><list id="S3.E3.m2.3.3.1.1.1.1.1.1.2.3.cmml" xref="S3.E3.m2.3.3.1.1.1.1.1.1.2.2"><apply id="S3.E3.m2.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m2.3.3.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m2.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m2.3.3.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m2.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m2.3.3.1.1.1.1.1.1.1.1.1.2">ğˆ</ci><cn id="S3.E3.m2.3.3.1.1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.E3.m2.3.3.1.1.1.1.1.1.1.1.1.3">1</cn></apply><ci id="S3.E3.m2.1.1.cmml" xref="S3.E3.m2.1.1">â€¦</ci><apply id="S3.E3.m2.3.3.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E3.m2.3.3.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m2.3.3.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E3.m2.3.3.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E3.m2.3.3.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E3.m2.3.3.1.1.1.1.1.1.2.2.2.2">ğˆ</ci><ci id="S3.E3.m2.3.3.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E3.m2.3.3.1.1.1.1.1.1.2.2.2.3">ğ‘</ci></apply><ci id="S3.E3.m2.2.2.cmml" xref="S3.E3.m2.2.2">ğ‘–</ci></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m2.3c">\displaystyle=-\sum_{i}^{N}\log\hat{p}(P_{i}^{*}|\mathbf{I}_{1},...,\mathbf{I}%
_{N},i)</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m2.3d">= - âˆ‘ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_log over^ start_ARG italic_p end_ARG ( italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT | bold_I start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , bold_I start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT , italic_i )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS4.p4.2">with <math alttext="\hat{p}" class="ltx_Math" display="inline" id="S3.SS4.p4.1.m1.1"><semantics id="S3.SS4.p4.1.m1.1a"><mover accent="true" id="S3.SS4.p4.1.m1.1.1" xref="S3.SS4.p4.1.m1.1.1.cmml"><mi id="S3.SS4.p4.1.m1.1.1.2" xref="S3.SS4.p4.1.m1.1.1.2.cmml">p</mi><mo id="S3.SS4.p4.1.m1.1.1.1" xref="S3.SS4.p4.1.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.1.m1.1b"><apply id="S3.SS4.p4.1.m1.1.1.cmml" xref="S3.SS4.p4.1.m1.1.1"><ci id="S3.SS4.p4.1.m1.1.1.1.cmml" xref="S3.SS4.p4.1.m1.1.1.1">^</ci><ci id="S3.SS4.p4.1.m1.1.1.2.cmml" xref="S3.SS4.p4.1.m1.1.1.2">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.1.m1.1c">\hat{p}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p4.1.m1.1d">over^ start_ARG italic_p end_ARG</annotation></semantics></math> given by (<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#S3.E1" title="Equation 1 â€£ 3.3 Pose discriminator â€£ 3 Method â€£ ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation"><span class="ltx_text ltx_ref_tag">1</span></a>).
The final loss is the sum of pose generator loss and pose discriminator loss <math alttext="\mathcal{L}=\mathcal{L}_{g}+\mathcal{L}_{d}" class="ltx_Math" display="inline" id="S3.SS4.p4.2.m2.1"><semantics id="S3.SS4.p4.2.m2.1a"><mrow id="S3.SS4.p4.2.m2.1.1" xref="S3.SS4.p4.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p4.2.m2.1.1.2" xref="S3.SS4.p4.2.m2.1.1.2.cmml">â„’</mi><mo id="S3.SS4.p4.2.m2.1.1.1" xref="S3.SS4.p4.2.m2.1.1.1.cmml">=</mo><mrow id="S3.SS4.p4.2.m2.1.1.3" xref="S3.SS4.p4.2.m2.1.1.3.cmml"><msub id="S3.SS4.p4.2.m2.1.1.3.2" xref="S3.SS4.p4.2.m2.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p4.2.m2.1.1.3.2.2" xref="S3.SS4.p4.2.m2.1.1.3.2.2.cmml">â„’</mi><mi id="S3.SS4.p4.2.m2.1.1.3.2.3" xref="S3.SS4.p4.2.m2.1.1.3.2.3.cmml">g</mi></msub><mo id="S3.SS4.p4.2.m2.1.1.3.1" xref="S3.SS4.p4.2.m2.1.1.3.1.cmml">+</mo><msub id="S3.SS4.p4.2.m2.1.1.3.3" xref="S3.SS4.p4.2.m2.1.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p4.2.m2.1.1.3.3.2" xref="S3.SS4.p4.2.m2.1.1.3.3.2.cmml">â„’</mi><mi id="S3.SS4.p4.2.m2.1.1.3.3.3" xref="S3.SS4.p4.2.m2.1.1.3.3.3.cmml">d</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.2.m2.1b"><apply id="S3.SS4.p4.2.m2.1.1.cmml" xref="S3.SS4.p4.2.m2.1.1"><eq id="S3.SS4.p4.2.m2.1.1.1.cmml" xref="S3.SS4.p4.2.m2.1.1.1"></eq><ci id="S3.SS4.p4.2.m2.1.1.2.cmml" xref="S3.SS4.p4.2.m2.1.1.2">â„’</ci><apply id="S3.SS4.p4.2.m2.1.1.3.cmml" xref="S3.SS4.p4.2.m2.1.1.3"><plus id="S3.SS4.p4.2.m2.1.1.3.1.cmml" xref="S3.SS4.p4.2.m2.1.1.3.1"></plus><apply id="S3.SS4.p4.2.m2.1.1.3.2.cmml" xref="S3.SS4.p4.2.m2.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS4.p4.2.m2.1.1.3.2.1.cmml" xref="S3.SS4.p4.2.m2.1.1.3.2">subscript</csymbol><ci id="S3.SS4.p4.2.m2.1.1.3.2.2.cmml" xref="S3.SS4.p4.2.m2.1.1.3.2.2">â„’</ci><ci id="S3.SS4.p4.2.m2.1.1.3.2.3.cmml" xref="S3.SS4.p4.2.m2.1.1.3.2.3">ğ‘”</ci></apply><apply id="S3.SS4.p4.2.m2.1.1.3.3.cmml" xref="S3.SS4.p4.2.m2.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS4.p4.2.m2.1.1.3.3.1.cmml" xref="S3.SS4.p4.2.m2.1.1.3.3">subscript</csymbol><ci id="S3.SS4.p4.2.m2.1.1.3.3.2.cmml" xref="S3.SS4.p4.2.m2.1.1.3.3.2">â„’</ci><ci id="S3.SS4.p4.2.m2.1.1.3.3.3.cmml" xref="S3.SS4.p4.2.m2.1.1.3.3.3">ğ‘‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.2.m2.1c">\mathcal{L}=\mathcal{L}_{g}+\mathcal{L}_{d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p4.2.m2.1d">caligraphic_L = caligraphic_L start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT + caligraphic_L start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS4.p5">
<p class="ltx_p" id="S3.SS4.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p5.1.1">Joint training</span> Training instability is well-studied in generative adversarial models (GAN) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib14" title="">14</a>]</cite>. In particular, as the generator improves its ability to generate accurate camera poses, the discriminatorâ€™s task becomes more difficult, and its gradients may effectively become noise; this can negatively impact model performance. We observed similar behavior during our training. There are many techniques to address this for GANs, but in our setting a very simple approach works well. Specifically, we add random Gaussian noise into the query embedding <math alttext="\mathbf{e_{i}}" class="ltx_Math" display="inline" id="S3.SS4.p5.1.m1.1"><semantics id="S3.SS4.p5.1.m1.1a"><msub id="S3.SS4.p5.1.m1.1.1" xref="S3.SS4.p5.1.m1.1.1.cmml"><mi id="S3.SS4.p5.1.m1.1.1.2" xref="S3.SS4.p5.1.m1.1.1.2.cmml">ğ</mi><mi id="S3.SS4.p5.1.m1.1.1.3" xref="S3.SS4.p5.1.m1.1.1.3.cmml">ğ¢</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p5.1.m1.1b"><apply id="S3.SS4.p5.1.m1.1.1.cmml" xref="S3.SS4.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p5.1.m1.1.1.1.cmml" xref="S3.SS4.p5.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p5.1.m1.1.1.2.cmml" xref="S3.SS4.p5.1.m1.1.1.2">ğ</ci><ci id="S3.SS4.p5.1.m1.1.1.3.cmml" xref="S3.SS4.p5.1.m1.1.1.3">ğ¢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p5.1.m1.1c">\mathbf{e_{i}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p5.1.m1.1d">bold_e start_POSTSUBSCRIPT bold_i end_POSTSUBSCRIPT</annotation></semantics></math> to generate negative examples for the discriminator. In our ablation study, we explore the impact of this noise on the final performance of the model <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#S4.SS3" title="4.3 Ablation â€£ 4 Experiments â€£ ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">4.3</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS4.p6">
<p class="ltx_p" id="S3.SS4.p6.1">In our setup, the pose generator produces samples from the current estimate for the pose distribution. While we design our system as a generator-discriminator pair with a contrastive loss, we observe that if the set of samples is sufficient to approximate the distribution, then this is in fact simply a maximum-likelihood objective.
This suggests classical models like kernel density estimates and mixture models may also be effective here, and indeed this is the case (see Supplemental), although our contrastive method provides stronger results.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Implementation details</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">Since the pose coordinate is ill-defined in an absolute coordinate system, we set the first frame as the canonical frame and set its rotation to identity. A reference frame encoding is added to the first frame to inform the model. For translation, we use a similar object-centric coordinate system as <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib22" title="">22</a>]</cite>. We normalize the translation of the canonical frame to be [0, 0, 1] and normalize all translations according to the distance from the canonical camera to the object center. This can be achieved by computing the center of mass of the object point cloud.</p>
</div>
<div class="ltx_para" id="S3.SS5.p2">
<p class="ltx_p" id="S3.SS5.p2.1">We use the same Anti-alias Res50 backbone as <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib43" title="">43</a>]</cite> and a transformer with six layers of multi-head attention. The ResNet backbone is pretrained on ImageNet while the transformer is trained from scratch. We use Adam as the optimizer with learning rate set to 1<math alttext="e^{-4}" class="ltx_Math" display="inline" id="S3.SS5.p2.1.m1.1"><semantics id="S3.SS5.p2.1.m1.1a"><msup id="S3.SS5.p2.1.m1.1.1" xref="S3.SS5.p2.1.m1.1.1.cmml"><mi id="S3.SS5.p2.1.m1.1.1.2" xref="S3.SS5.p2.1.m1.1.1.2.cmml">e</mi><mrow id="S3.SS5.p2.1.m1.1.1.3" xref="S3.SS5.p2.1.m1.1.1.3.cmml"><mo id="S3.SS5.p2.1.m1.1.1.3a" xref="S3.SS5.p2.1.m1.1.1.3.cmml">âˆ’</mo><mn id="S3.SS5.p2.1.m1.1.1.3.2" xref="S3.SS5.p2.1.m1.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.1.m1.1b"><apply id="S3.SS5.p2.1.m1.1.1.cmml" xref="S3.SS5.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.1.m1.1.1.1.cmml" xref="S3.SS5.p2.1.m1.1.1">superscript</csymbol><ci id="S3.SS5.p2.1.m1.1.1.2.cmml" xref="S3.SS5.p2.1.m1.1.1.2">ğ‘’</ci><apply id="S3.SS5.p2.1.m1.1.1.3.cmml" xref="S3.SS5.p2.1.m1.1.1.3"><minus id="S3.SS5.p2.1.m1.1.1.3.1.cmml" xref="S3.SS5.p2.1.m1.1.1.3"></minus><cn id="S3.SS5.p2.1.m1.1.1.3.2.cmml" type="integer" xref="S3.SS5.p2.1.m1.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.1.m1.1c">e^{-4}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p2.1.m1.1d">italic_e start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math> and the model is trained for 2000 epochs. During training, we randomly sampled different numbers of images ranging from 2-10 for each batch.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experiment setup</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.1">Dataset</span> We use CO3D V2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib30" title="">30</a>]</cite> for training and testing ADen, which encompasses 51 object-centric sequences.The ground truth pose for each sequence is determined by employing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib34" title="">34</a>]</cite>. In line with <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib41" title="">41</a>]</cite>, ADen is trained on 41 object categories and subsequently evaluated on both the test set of these 41 seen categories and a separate test set of 10 unseen categories. We follow the same test set frame sampling method of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib22" title="">22</a>]</cite> that resamples N images from a sequence 5 times and reports the average as the final accuracy.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">To further evaluate the generalization ability of method, we test ADen on two additional datasets in zero-shot setting (without any finetuning): Objectron <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib1" title="">1</a>]</cite> and Niantic Map Free Relocalization (NMFR) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib2" title="">2</a>]</cite>. Objectron consists of short, object-centric video clips captured for AR use cases; NMFR captures diverse outdoor scenes and structures including sculptures, murals and foundations. FollowingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib22" title="">22</a>]</cite>, all methods were trained using CO3DÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib30" title="">30</a>]</cite> and tested zero-shot (no finetuning). For the Objectron dataset, we used the test set of four classes (Camera, Chair, Cup, Shoe), following the same protocol as Relpose++ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib22" title="">22</a>]</cite>. For the Niantic Map Free dataset, we used the validation set, which contains 65 scans.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.1.1">Evaluation metrics</span>
For evaluating the rotation accuracy, we measure the predicted pairwise relative rotation against ground truth rotations and report the proportion of rotation errors that are less than 15 degrees. To show accuracy at a tighter error threshold, we also report rotation errors that are less than 5 and 10 degrees. For evaluating the translation, we follow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib22" title="">22</a>]</cite> to first apply an optimal similarity transform to align the predicted centers with ground truth <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib39" title="">39</a>]</cite>. We then report the accuracy as the proportion of translation errors that are less than 10% of the scene scale. The scene scale is determined by the distance from the centroid of all ground truth cameras to the furthest camera.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Comparing with SoTA</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.1">Baselines</span> We compare the proposed method with the following state-of-the-art (SoTA) classic correspondence-based and data-driven approaches:</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1"><span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.1">COLMAP (SP + SG) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib34" title="">34</a>]</cite></span>. This represents the SoTA SfM pipeline using the open-source COLMAP implementation, with SuperPoint as key-point features <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib8" title="">8</a>]</cite> and SuperGlue for key-point matching <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib33" title="">33</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1"><span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.1">SparsePose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib35" title="">35</a>]</cite></span> SparesePose directly regresses poses and uses an iterative refinement strategy to further refine the initial poses.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1"><span class="ltx_text ltx_font_italic" id="S4.SS2.p4.1.1">PoseDiff <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib41" title="">41</a>]</cite></span>. PoseDiff formulates pose estimation inside a probabilistic diffusion framework, which mirrors the iterative procedure of bundle adjustment.</p>
</div>
<div class="ltx_para" id="S4.SS2.p5">
<p class="ltx_p" id="S4.SS2.p5.1"><span class="ltx_text ltx_font_italic" id="S4.SS2.p5.1.1">RelPose and RelPose++ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib22" title="">22</a>]</cite></span>. RelPose is the first work that models rotation as a probability distribution and RelPose++ is an extension that adds a transformer module and also reports translation error. RelPose++ represents the SoTA learning-based method on the Co3D dataset.</p>
</div>
<div class="ltx_para" id="S4.SS2.p6">
<p class="ltx_p" id="S4.SS2.p6.1"><span class="ltx_text ltx_font_italic" id="S4.SS2.p6.1.1">Pose Regression <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib22" title="">22</a>]</cite></span>. Pose Regression directly regresses poses given sparse view images. We use the one reported in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib22" title="">22</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p7">
<p class="ltx_p" id="S4.SS2.p7.1">As shown in <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#S4.T1" title="In 4.2 Comparing with SoTA â€£ 4 Experiments â€£ ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#S4.T3" title="In 4.2 Comparing with SoTA â€£ 4 Experiments â€£ ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">3</span></a>, ADen achieves SoTA performance on both rotation and translation accuracy compared to previous methods. ADen consistently outperforms other baselines with different numbers of images as input. This performance gain is more prominent at tighter rotation accuracy threshold <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#S4.T2" title="In 4.2 Comparing with SoTA â€£ 4 Experiments â€£ ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>. Compared with Pose Regression and SparsePose, our method allows the model to explore different modes during training to better learn the underlying ambiguous multi-modal distribution, so it performs significantly better. In contrast to RelPose/RelPose++, which also predicts a probabilistic distribution for poses, our approach is not constrained by the resolution of samples from the SO(3) space and can generate samples that closely match the ground truth camera pose. This improvement is more apparent at tighter rotation error thresholds (<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#S4.T2" title="In 4.2 Comparing with SoTA â€£ 4 Experiments â€£ ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>), underscoring the precision of the samples generated by our pose generator. Additionally, our method achieves SoTA results in camera translation errors, primarily due to enhanced rotation accuracy.
On the two additional datasets, ADen demonstrates strong generalization, significantly surpassing prior methods and achieving SOTA performance <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#S4.T4.st1" title="In Table 4 â€£ 4.2 Comparing with SoTA â€£ 4 Experiments â€£ ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">4(a)</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#S4.T4.st2" title="In Table 4 â€£ 4.2 Comparing with SoTA â€£ 4 Experiments â€£ ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">4(b)</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.2.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T1.2.1.1.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.2.1.1.2"># of Images</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.1.3">2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.1.4">3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.1.5">4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.1.6">5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.1.7">6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.1.8">7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.1.9">8</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.2.2.2.1" rowspan="6"><span class="ltx_text" id="S4.T1.2.2.2.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T1.2.2.2.1.1.1" style="width:6.8pt;height:20pt;vertical-align:-6.6pt;"><span class="ltx_transformed_inner" style="width:20.0pt;transform:translate(-6.58pt,0pt) rotate(-90deg) ;">
<span class="ltx_p" id="S4.T1.2.2.2.1.1.1.1">Seen</span>
</span></span></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.2.2.2.2">COLMAP(SP+SG)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.2.3">30.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.2.4">28.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.2.5">26.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.2.6">26.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.2.7">27.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.2.8">28.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.2.9">30.6</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.2.3.3.1">RelPose</th>
<td class="ltx_td ltx_align_center" id="S4.T1.2.3.3.2">56.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.3.3.3">56.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.3.3.4">57.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.3.3.5">57.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.3.3.6">57.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.3.3.7">57.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.3.3.8">57.2</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.2.4.4.1">Pose Regression</th>
<td class="ltx_td ltx_align_center" id="S4.T1.2.4.4.2">49.1</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.4.4.3">50.7</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.4.4.4">53.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.4.4.5">54.6</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.4.4.6">55.7</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.4.4.7">56.1</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.4.4.8">56.5</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.2.5.5.1">RelPose++</th>
<td class="ltx_td ltx_align_center" id="S4.T1.2.5.5.2">81.8</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.5.5.3">82.8</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.5.5.4">84.1</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.5.5.5">84.7</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.5.5.6">84.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.5.5.7">85.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.5.5.8">85.5</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.2.6.6.1">PoseDiff</th>
<td class="ltx_td ltx_align_center" id="S4.T1.2.6.6.2">76.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.6.6.3">76.7</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.6.6.4">77.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.6.6.5">77.7</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.6.6.6">78.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.6.6.7">78.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.6.6.8">78.5</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.2.7.7.1"><span class="ltx_text ltx_font_bold" id="S4.T1.2.7.7.1.1">Ours</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.2.7.7.2"><span class="ltx_text ltx_font_bold" id="S4.T1.2.7.7.2.1">84.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.7.7.3"><span class="ltx_text ltx_font_bold" id="S4.T1.2.7.7.3.1">85.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.7.7.4"><span class="ltx_text ltx_font_bold" id="S4.T1.2.7.7.4.1">86.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.7.7.5"><span class="ltx_text ltx_font_bold" id="S4.T1.2.7.7.5.1">86.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.7.7.6"><span class="ltx_text ltx_font_bold" id="S4.T1.2.7.7.6.1">87.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.7.7.7"><span class="ltx_text ltx_font_bold" id="S4.T1.2.7.7.7.1">87.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.7.7.8"><span class="ltx_text ltx_font_bold" id="S4.T1.2.7.7.8.1">87.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="S4.T1.2.8.8.1" rowspan="7"><span class="ltx_text" id="S4.T1.2.8.8.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T1.2.8.8.1.1.1" style="width:6.8pt;height:31.4pt;vertical-align:-12.3pt;"><span class="ltx_transformed_inner" style="width:31.4pt;transform:translate(-12.31pt,0pt) rotate(-90deg) ;">
<span class="ltx_p" id="S4.T1.2.8.8.1.1.1.1">Unseen</span>
</span></span></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.2.8.8.2">COLMAP(SP+SG)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.8.8.3">34.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.8.8.4">31.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.8.8.5">31.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.8.8.6">31.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.8.8.7">32.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.8.8.8">35.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.8.8.9">38.5</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.2.9.9.1">RelPose</th>
<td class="ltx_td ltx_align_center" id="S4.T1.2.9.9.2">48.6</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.9.9.3">47.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.9.9.4">48.1</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.9.9.5">48.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.9.9.6">48.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.9.9.7">48.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.9.9.8">48.3</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.2.10.10.1">Pose Regression</th>
<td class="ltx_td ltx_align_center" id="S4.T1.2.10.10.2">42.7</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.10.10.3">43.8</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.10.10.4">46.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.10.10.5">47.7</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.10.10.6">48.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.10.10.7">48.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.10.10.8">48.9</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.2.11.11.1">SparsePose</th>
<td class="ltx_td ltx_align_center" id="S4.T1.2.11.11.2">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.11.11.3">65.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.11.11.4">68.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.11.11.5">70.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.11.11.6">67.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.11.11.7">72.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.11.11.8">72.0</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.2.12.12.1">RelPose++</th>
<td class="ltx_td ltx_align_center" id="S4.T1.2.12.12.2">69.8</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.12.12.3">71.1</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.12.12.4">71.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.12.12.5">72.8</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.12.12.6">73.8</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.12.12.7">74.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.12.12.8">74.9</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.13.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.2.13.13.1">PoseDiff</th>
<td class="ltx_td ltx_align_center" id="S4.T1.2.13.13.2">60.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.13.13.3">64.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.13.13.4">64.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.13.13.5">65.6</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.13.13.6">66.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.13.13.7">67.1</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.13.13.8">67.5</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.14.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T1.2.14.14.1"><span class="ltx_text ltx_font_bold" id="S4.T1.2.14.14.1.1">Ours</span></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.2.14.14.2"><span class="ltx_text ltx_font_bold" id="S4.T1.2.14.14.2.1">78.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.2.14.14.3"><span class="ltx_text ltx_font_bold" id="S4.T1.2.14.14.3.1">79.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.2.14.14.4"><span class="ltx_text ltx_font_bold" id="S4.T1.2.14.14.4.1">80.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.2.14.14.5"><span class="ltx_text ltx_font_bold" id="S4.T1.2.14.14.5.1">80.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.2.14.14.6"><span class="ltx_text ltx_font_bold" id="S4.T1.2.14.14.6.1">81.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.2.14.14.7"><span class="ltx_text ltx_font_bold" id="S4.T1.2.14.14.7.1">81.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.2.14.14.8"><span class="ltx_text ltx_font_bold" id="S4.T1.2.14.14.8.1">82.1</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.4.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text ltx_font_bold" id="S4.T1.5.2" style="font-size:90%;">Pairwise relative rotation accuracy @ 15.<span class="ltx_text ltx_font_medium" id="S4.T1.5.2.1"> We measure the relative angular errors between relative predicted and ground truth rotation, and report the accuracy as the proportion of errors less than 15 degrees. ADen consistently outperforms SoTA methods using different number of images as input.</span></span></figcaption>
</figure>
<figure class="ltx_table" id="S4.T2">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_figure_panel ltx_guessed_headers ltx_align_middle" id="S4.T2.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.6.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T2.6.1.1.1">Acc@5</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T2.6.1.1.2"># of Images</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.6.1.1.3">2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.6.1.1.4">3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.6.1.1.5">4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.6.1.1.6">5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.6.1.1.7">6</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.6.1.1.8">7</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.6.1.1.9">8</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.6.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.6.2.1.1" rowspan="3"><span class="ltx_text" id="S4.T2.6.2.1.1.1">Seen</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.6.2.1.2">RelPose++</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.2.1.3">42.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.2.1.4">43.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.2.1.5">44.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.2.1.6">44.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.2.1.7">45.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.2.1.8">45.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.2.1.9">45.4</td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.6.3.2.1">PoseDiff</th>
<td class="ltx_td ltx_align_center" id="S4.T2.6.3.2.2">49.8</td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.3.2.3">51.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.3.2.4">50.6</td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.3.2.5">51.7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.3.2.6">52.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.3.2.7">53.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.3.2.8">54.4</td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.6.4.3.1"><span class="ltx_text ltx_font_bold" id="S4.T2.6.4.3.1.1">Ours</span></th>
<td class="ltx_td ltx_align_center" id="S4.T2.6.4.3.2"><span class="ltx_text ltx_font_bold" id="S4.T2.6.4.3.2.1">53.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.4.3.3"><span class="ltx_text ltx_font_bold" id="S4.T2.6.4.3.3.1">55.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.4.3.4"><span class="ltx_text ltx_font_bold" id="S4.T2.6.4.3.4.1">55.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.4.3.5"><span class="ltx_text ltx_font_bold" id="S4.T2.6.4.3.5.1">56.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.4.3.6"><span class="ltx_text ltx_font_bold" id="S4.T2.6.4.3.6.1">56.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.4.3.7"><span class="ltx_text ltx_font_bold" id="S4.T2.6.4.3.7.1">56.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.4.3.8"><span class="ltx_text ltx_font_bold" id="S4.T2.6.4.3.8.1">56.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="S4.T2.6.5.4.1" rowspan="3"><span class="ltx_text" id="S4.T2.6.5.4.1.1">Unseen</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.6.5.4.2">RelPose++</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.5.4.3">30.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.5.4.4">31.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.5.4.5">32.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.5.4.6">33.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.5.4.7">33.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.5.4.8">34.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.5.4.9">33.9</td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.6.6.5.1">PoseDiff</th>
<td class="ltx_td ltx_align_center" id="S4.T2.6.6.5.2">38.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.6.5.3">34.9</td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.6.5.4">37.2</td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.6.5.5">39.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.6.5.6">41.2</td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.6.5.7">42.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.6.5.8">42.6</td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T2.6.7.6.1"><span class="ltx_text ltx_font_bold" id="S4.T2.6.7.6.1.1">Ours</span></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.6.7.6.2"><span class="ltx_text ltx_font_bold" id="S4.T2.6.7.6.2.1">45.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.6.7.6.3"><span class="ltx_text ltx_font_bold" id="S4.T2.6.7.6.3.1">46.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.6.7.6.4"><span class="ltx_text ltx_font_bold" id="S4.T2.6.7.6.4.1">47.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.6.7.6.5"><span class="ltx_text ltx_font_bold" id="S4.T2.6.7.6.5.1">48.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.6.7.6.6"><span class="ltx_text ltx_font_bold" id="S4.T2.6.7.6.6.1">48.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.6.7.6.7"><span class="ltx_text ltx_font_bold" id="S4.T2.6.7.6.7.1">48.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.6.7.6.8"><span class="ltx_text ltx_font_bold" id="S4.T2.6.7.6.8.1">48.7</span></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle" id="S4.T2.7">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.7.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T2.7.1.1.1">Acc@10</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T2.7.1.1.2"># of Images</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.7.1.1.3">2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.7.1.1.4">3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.7.1.1.5">4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.7.1.1.6">5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.7.1.1.7">6</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.7.1.1.8">7</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.7.1.1.9">8</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.7.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.7.2.1.1" rowspan="3"><span class="ltx_text" id="S4.T2.7.2.1.1.1">Seen</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.7.2.1.2">RelPose++</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.2.1.3">70.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.2.1.4">72.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.2.1.5">73.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.2.1.6">74.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.2.1.7">74.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.2.1.8">75.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.2.1.9">75.2</td>
</tr>
<tr class="ltx_tr" id="S4.T2.7.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.7.3.2.1">PoseDiff</th>
<td class="ltx_td ltx_align_center" id="S4.T2.7.3.2.2">68.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.3.2.3">70.1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.3.2.4">70.2</td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.3.2.5">71.1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.3.2.6">72.2</td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.3.2.7">72.3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.3.2.8">72.4</td>
</tr>
<tr class="ltx_tr" id="S4.T2.7.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.7.4.3.1"><span class="ltx_text ltx_font_bold" id="S4.T2.7.4.3.1.1">Ours</span></th>
<td class="ltx_td ltx_align_center" id="S4.T2.7.4.3.2"><span class="ltx_text ltx_font_bold" id="S4.T2.7.4.3.2.1">77.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.4.3.3"><span class="ltx_text ltx_font_bold" id="S4.T2.7.4.3.3.1">79.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.4.3.4"><span class="ltx_text ltx_font_bold" id="S4.T2.7.4.3.4.1">79.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.4.3.5"><span class="ltx_text ltx_font_bold" id="S4.T2.7.4.3.5.1">79.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.4.3.6"><span class="ltx_text ltx_font_bold" id="S4.T2.7.4.3.6.1">80.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.4.3.7"><span class="ltx_text ltx_font_bold" id="S4.T2.7.4.3.7.1">80.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.4.3.8"><span class="ltx_text ltx_font_bold" id="S4.T2.7.4.3.8.1">80.7</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.7.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="S4.T2.7.5.4.1" rowspan="3"><span class="ltx_text" id="S4.T2.7.5.4.1.1">Unseen</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.7.5.4.2">RelPose++</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.5.4.3">57.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.5.4.4">58.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.5.4.5">60.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.5.4.6">61.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.5.4.7">61.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.5.4.8">62.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.5.4.9">62.5</td>
</tr>
<tr class="ltx_tr" id="S4.T2.7.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.7.6.5.1">PoseDiff</th>
<td class="ltx_td ltx_align_center" id="S4.T2.7.6.5.2">50.6</td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.6.5.3">55.7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.6.5.4">57.2</td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.6.5.5">57.9</td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.6.5.6">58.3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.6.5.7">60.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.6.5.8">60.8</td>
</tr>
<tr class="ltx_tr" id="S4.T2.7.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T2.7.7.6.1"><span class="ltx_text ltx_font_bold" id="S4.T2.7.7.6.1.1">Ours</span></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.7.7.6.2"><span class="ltx_text ltx_font_bold" id="S4.T2.7.7.6.2.1">69.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.7.7.6.3"><span class="ltx_text ltx_font_bold" id="S4.T2.7.7.6.3.1">70.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.7.7.6.4"><span class="ltx_text ltx_font_bold" id="S4.T2.7.7.6.4.1">72.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.7.7.6.5"><span class="ltx_text ltx_font_bold" id="S4.T2.7.7.6.5.1">72.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.7.7.6.6"><span class="ltx_text ltx_font_bold" id="S4.T2.7.7.6.6.1">73.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.7.7.6.7"><span class="ltx_text ltx_font_bold" id="S4.T2.7.7.6.7.1">73.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.7.7.6.8"><span class="ltx_text ltx_font_bold" id="S4.T2.7.7.6.8.1">74.1</span></td>
</tr>
</tbody>
</table>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.8.3.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text ltx_font_bold" id="S4.T2.4.2" style="font-size:90%;">Pairwise relative rotation accuray @ 5<sup class="ltx_sup" id="S4.T2.4.2.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T2.4.2.1.1">âˆ˜</span></sup> and 10<sup class="ltx_sup" id="S4.T2.4.2.2"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T2.4.2.2.1">âˆ˜</span></sup><span class="ltx_text ltx_font_medium" id="S4.T2.4.2.3">. We report Acc@5 and Acc@10 for ADen, PoseDiff and RelPose++. ADen is not constrained by the accuracy of the sampled grid on SO3, thereby achieving greater gains at tighter accuracy thresholds.</span></span></figcaption>
</figure>
<figure class="ltx_table" id="S4.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.2.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T3.2.1.1.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T3.2.1.1.2"># of Images</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.2.1.1.3">3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.2.1.1.4">4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.2.1.1.5">5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.2.1.1.6">6</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.2.1.1.7">7</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.2.1.1.8">8</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.2.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.2.2.1.1" rowspan="5"><span class="ltx_text" id="S4.T3.2.2.1.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T3.2.2.1.1.1.1" style="width:6.8pt;height:20pt;vertical-align:-6.6pt;"><span class="ltx_transformed_inner" style="width:20.0pt;transform:translate(-6.58pt,0pt) rotate(-90deg) ;">
<span class="ltx_p" id="S4.T3.2.2.1.1.1.1.1">Seen</span>
</span></span></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.2.2.1.2">COLMAP(SP+SG)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.1.3">35.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.1.4">26.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.1.5">21.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.1.6">18.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.1.7">18.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.1.8">19.2</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.3.2.1">Pose Regression</th>
<td class="ltx_td ltx_align_center" id="S4.T3.2.3.2.2">87.6</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.3.2.3">81.2</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.3.2.4">77.6</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.3.2.5">75.8</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.3.2.6">74.5</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.3.2.7">73.6</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.4.3.1">RelPose++</th>
<td class="ltx_td ltx_align_center" id="S4.T3.2.4.3.2">92.3</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.4.3.3">89.1</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.4.3.4">87.5</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.4.3.5">86.4</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.4.3.6">85.9</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.4.3.7">85.5</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.5.4.1">PoseDiff</th>
<td class="ltx_td ltx_align_center" id="S4.T3.2.5.4.2">78.3</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.5.4.3">78.1</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.5.4.4">79.1</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.5.4.5">79.4</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.5.4.6">79.4</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.5.4.7">79.4</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.6.5.1"><span class="ltx_text ltx_font_bold" id="S4.T3.2.6.5.1.1">Ours</span></th>
<td class="ltx_td ltx_align_center" id="S4.T3.2.6.5.2"><span class="ltx_text ltx_font_bold" id="S4.T3.2.6.5.2.1">92.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.6.5.3"><span class="ltx_text ltx_font_bold" id="S4.T3.2.6.5.3.1">89.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.6.5.4"><span class="ltx_text ltx_font_bold" id="S4.T3.2.6.5.4.1">88.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.6.5.5"><span class="ltx_text ltx_font_bold" id="S4.T3.2.6.5.5.1">86.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.6.5.6"><span class="ltx_text ltx_font_bold" id="S4.T3.2.6.5.6.1">86.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.6.5.7"><span class="ltx_text ltx_font_bold" id="S4.T3.2.6.5.7.1">85.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="S4.T3.2.7.6.1" rowspan="5"><span class="ltx_text" id="S4.T3.2.7.6.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T3.2.7.6.1.1.1" style="width:6.8pt;height:31.4pt;vertical-align:-12.3pt;"><span class="ltx_transformed_inner" style="width:31.4pt;transform:translate(-12.31pt,0pt) rotate(-90deg) ;">
<span class="ltx_p" id="S4.T3.2.7.6.1.1.1.1">Unseen</span>
</span></span></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.2.7.6.2">COLMAP(SP+SG)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.7.6.3">37.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.7.6.4">29.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.7.6.5">24.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.7.6.6">23.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.7.6.7">23.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.7.6.8">25.3</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.8.7.1">Pose Regression</th>
<td class="ltx_td ltx_align_center" id="S4.T3.2.8.7.2">82.8</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.8.7.3">74.0</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.8.7.4">70.0</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.8.7.5">67.8</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.8.7.6">65.8</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.8.7.7">65.3</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.9.8.1">RelPose++</th>
<td class="ltx_td ltx_align_center" id="S4.T3.2.9.8.2">82.5</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.9.8.3">75.6</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.9.8.4">71.9</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.9.8.5">69.9</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.9.8.6">68.5</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.9.8.7">67.5</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.10.9.1">PoseDiff</th>
<td class="ltx_td ltx_align_center" id="S4.T3.2.10.9.2">61.8</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.10.9.3">61.9</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.10.9.4">63.1</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.10.9.5">63.1</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.10.9.6">62.7</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.10.9.7">63.3</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T3.2.11.10.1"><span class="ltx_text ltx_font_bold" id="S4.T3.2.11.10.1.1">Ours</span></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.2.11.10.2"><span class="ltx_text ltx_font_bold" id="S4.T3.2.11.10.2.1">85.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.2.11.10.3"><span class="ltx_text ltx_font_bold" id="S4.T3.2.11.10.3.1">78.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.2.11.10.4"><span class="ltx_text ltx_font_bold" id="S4.T3.2.11.10.4.1">75.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.2.11.10.5"><span class="ltx_text ltx_font_bold" id="S4.T3.2.11.10.5.1">73.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.2.11.10.6"><span class="ltx_text ltx_font_bold" id="S4.T3.2.11.10.6.1">71.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.2.11.10.7"><span class="ltx_text ltx_font_bold" id="S4.T3.2.11.10.7.1">70.7</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T3.4.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text ltx_font_bold" id="S4.T3.5.2" style="font-size:90%;">Translation accuracy @ 0.2<span class="ltx_text ltx_font_medium" id="S4.T3.5.2.1"> We measure the accuracy as the proportion of predicted camera translations that are within 20% of the scene scale of each sequence.</span></span></figcaption>
</figure>
<figure class="ltx_table" id="S4.T4">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_table ltx_figure_panel ltx_align_center" id="S4.T4.st1">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" id="S4.T4.st1.2" style="width:433.6pt;height:122.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(25.1pt,-7.1pt) scale(1.13101670958534,1.13101670958534) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_top" id="S4.T4.st1.2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.st1.2.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T4.st1.2.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="3" id="S4.T4.st1.2.1.1.1.2">Rotation</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="3" id="S4.T4.st1.2.1.1.1.3">Cam. Cen.</th>
</tr>
<tr class="ltx_tr" id="S4.T4.st1.2.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" id="S4.T4.st1.2.1.2.2.1"># of Images</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.st1.2.1.2.2.2">3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.st1.2.1.2.2.3">5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.st1.2.1.2.2.4">8</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.st1.2.1.2.2.5">3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.st1.2.1.2.2.6">5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.st1.2.1.2.2.7">8</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.st1.2.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.st1.2.1.3.1.1">MediaPipe <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib25" title="">25</a>]</cite>
</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.st1.2.1.3.1.2">52.3</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.st1.2.1.3.1.3">52.8</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.st1.2.1.3.1.4">52.7</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.st1.2.1.3.1.5">74.5</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.st1.2.1.3.1.6">59.1</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.st1.2.1.3.1.7">49.9</td>
</tr>
<tr class="ltx_tr" id="S4.T4.st1.2.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.st1.2.1.4.2.1">PoseDiff <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib41" title="">41</a>]</cite>
</th>
<td class="ltx_td ltx_align_left" id="S4.T4.st1.2.1.4.2.2">69.2</td>
<td class="ltx_td ltx_align_left" id="S4.T4.st1.2.1.4.2.3">68.0</td>
<td class="ltx_td ltx_align_left" id="S4.T4.st1.2.1.4.2.4">70.0</td>
<td class="ltx_td ltx_align_left" id="S4.T4.st1.2.1.4.2.5">87.2</td>
<td class="ltx_td ltx_align_left" id="S4.T4.st1.2.1.4.2.6">73.8</td>
<td class="ltx_td ltx_align_left" id="S4.T4.st1.2.1.4.2.7">67.2</td>
</tr>
<tr class="ltx_tr" id="S4.T4.st1.2.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.st1.2.1.5.3.1">RelPose++ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib22" title="">22</a>]</cite>
</th>
<td class="ltx_td ltx_align_left" id="S4.T4.st1.2.1.5.3.2">75.8</td>
<td class="ltx_td ltx_align_left" id="S4.T4.st1.2.1.5.3.3">76.6</td>
<td class="ltx_td ltx_align_left" id="S4.T4.st1.2.1.5.3.4">77.0</td>
<td class="ltx_td ltx_align_left" id="S4.T4.st1.2.1.5.3.5">91.6</td>
<td class="ltx_td ltx_align_left" id="S4.T4.st1.2.1.5.3.6">83.9</td>
<td class="ltx_td ltx_align_left" id="S4.T4.st1.2.1.5.3.7">77.6</td>
</tr>
<tr class="ltx_tr" id="S4.T4.st1.2.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T4.st1.2.1.6.4.1"><span class="ltx_text ltx_font_bold" id="S4.T4.st1.2.1.6.4.1.1">Ours</span></th>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T4.st1.2.1.6.4.2"><span class="ltx_text ltx_font_bold" id="S4.T4.st1.2.1.6.4.2.1">86.0</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T4.st1.2.1.6.4.3"><span class="ltx_text ltx_font_bold" id="S4.T4.st1.2.1.6.4.3.1">85.8</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T4.st1.2.1.6.4.4"><span class="ltx_text ltx_font_bold" id="S4.T4.st1.2.1.6.4.4.1">85.7</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T4.st1.2.1.6.4.5"><span class="ltx_text ltx_font_bold" id="S4.T4.st1.2.1.6.4.5.1">93.1</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T4.st1.2.1.6.4.6"><span class="ltx_text ltx_font_bold" id="S4.T4.st1.2.1.6.4.6.1">85.0</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T4.st1.2.1.6.4.7"><span class="ltx_text ltx_font_bold" id="S4.T4.st1.2.1.6.4.7.1">80.0</span></td>
</tr>
</tbody>
</table>
</span></div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T4.st1.3.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S4.T4.st1.4.2" style="font-size:90%;">Objectron <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib1" title="">1</a>]</cite>
</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S4.T4.st1.5">.</p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_table ltx_figure_panel ltx_align_center" id="S4.T4.st2">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" id="S4.T4.st2.2" style="width:433.6pt;height:104.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(29.9pt,-7.2pt) scale(1.1597542772431,1.1597542772431) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_top" id="S4.T4.st2.2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.st2.2.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T4.st2.2.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="3" id="S4.T4.st2.2.1.1.1.2">Rotation</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="3" id="S4.T4.st2.2.1.1.1.3">Cam. Cen.</th>
</tr>
<tr class="ltx_tr" id="S4.T4.st2.2.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" id="S4.T4.st2.2.1.2.2.1"># of Images</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.st2.2.1.2.2.2">3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.st2.2.1.2.2.3">5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.st2.2.1.2.2.4">8</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.st2.2.1.2.2.5">3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.st2.2.1.2.2.6">5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.st2.2.1.2.2.7">8</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.st2.2.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.st2.2.1.3.1.1">PoseDiff <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib41" title="">41</a>]</cite>
</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.st2.2.1.3.1.2">39.5</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.st2.2.1.3.1.3">43.2</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.st2.2.1.3.1.4">42.3</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.st2.2.1.3.1.5">70.4</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.st2.2.1.3.1.6">48.9</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.st2.2.1.3.1.7">42.8</td>
</tr>
<tr class="ltx_tr" id="S4.T4.st2.2.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.st2.2.1.4.2.1">RelPose++ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib22" title="">22</a>]</cite>
</th>
<td class="ltx_td ltx_align_left" id="S4.T4.st2.2.1.4.2.2">45.2</td>
<td class="ltx_td ltx_align_left" id="S4.T4.st2.2.1.4.2.3">44.8</td>
<td class="ltx_td ltx_align_left" id="S4.T4.st2.2.1.4.2.4">46.2</td>
<td class="ltx_td ltx_align_left" id="S4.T4.st2.2.1.4.2.5">73.9</td>
<td class="ltx_td ltx_align_left" id="S4.T4.st2.2.1.4.2.6">50.4</td>
<td class="ltx_td ltx_align_left" id="S4.T4.st2.2.1.4.2.7">43.2</td>
</tr>
<tr class="ltx_tr" id="S4.T4.st2.2.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T4.st2.2.1.5.3.1"><span class="ltx_text ltx_font_bold" id="S4.T4.st2.2.1.5.3.1.1">Ours</span></th>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T4.st2.2.1.5.3.2"><span class="ltx_text ltx_font_bold" id="S4.T4.st2.2.1.5.3.2.1">55.3</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T4.st2.2.1.5.3.3"><span class="ltx_text ltx_font_bold" id="S4.T4.st2.2.1.5.3.3.1">52.6</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T4.st2.2.1.5.3.4"><span class="ltx_text ltx_font_bold" id="S4.T4.st2.2.1.5.3.4.1">52.8</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T4.st2.2.1.5.3.5"><span class="ltx_text ltx_font_bold" id="S4.T4.st2.2.1.5.3.5.1">74.5</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T4.st2.2.1.5.3.6"><span class="ltx_text ltx_font_bold" id="S4.T4.st2.2.1.5.3.6.1">51.5</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T4.st2.2.1.5.3.7"><span class="ltx_text ltx_font_bold" id="S4.T4.st2.2.1.5.3.7.1">44.4</span></td>
</tr>
</tbody>
</table>
</span></div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T4.st2.3.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S4.T4.st2.4.2" style="font-size:90%;">Niantic Map-Free Relocalization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib2" title="">2</a>]</cite></span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S4.T4.st2.5">.</p>
</div>
</div>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T4.4.2.1" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text ltx_font_bold" id="S4.T4.2.1" style="font-size:90%;">Zero-shot evaluation on two additional datasets on rotation (@ 15<sup class="ltx_sup" id="S4.T4.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T4.2.1.1.1">âˆ˜</span></sup>) and camera center (@ 0.2) Accuracy.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Ablation</h3>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="212" id="S4.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.3.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text ltx_font_bold" id="S4.F3.4.2" style="font-size:90%;">Camera pose prediction<span class="ltx_text ltx_font_medium" id="S4.F3.4.2.1"> of ADen on CO3D examples.</span></span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="381" id="S4.F4.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.3.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text ltx_font_bold" id="S4.F4.4.2" style="font-size:90%;">Relative rotation prediction.<span class="ltx_text ltx_font_medium" id="S4.F4.4.2.1"> We visualize the relative rotations predicted by ADen on ambiguous cases. The circle size of each filled circle represents the probability assigned by the discriminator. The unfilled larger circle is the ground truth. </span></span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">For ablation study, we by default use 5 input images to train and test ADen on the seen categories.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p2.1.1">How to train the generator.</span></p>
</div>
<figure class="ltx_table" id="S4.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T5.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T5.2.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T5.2.1.1.1">Setting</th>
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T5.2.1.1.2"></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.1.3">Acc@5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.1.4">Acc@10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.1.5">Acc@15</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="5" id="S4.T5.2.2.2.1"><span class="ltx_text ltx_font_bold" id="S4.T5.2.2.2.1.1">How to train generator</span></th>
</tr>
<tr class="ltx_tr" id="S4.T5.2.3.3">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T5.2.3.3.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T5.2.3.3.2">Top 1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.3.3.3">55.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.3.3.4">79.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.3.3.5">86.4</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.4.4">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S4.T5.2.4.4.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.2.4.4.2">Top 10</th>
<td class="ltx_td ltx_align_center" id="S4.T5.2.4.4.3">54.5</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.4.4.4">79.8</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.4.4.5">86.8</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.5.5">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S4.T5.2.5.5.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.2.5.5.2">Top 50</th>
<td class="ltx_td ltx_align_center" id="S4.T5.2.5.5.3">56.0</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.5.5.4">80.0</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.5.5.5">87.0</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.6.6">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S4.T5.2.6.6.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.2.6.6.2">All</th>
<td class="ltx_td ltx_align_center" id="S4.T5.2.6.6.3">48.2</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.6.6.4">70.5</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.6.6.5">77.3</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="5" id="S4.T5.2.7.7.1"><span class="ltx_text ltx_font_bold" id="S4.T5.2.7.7.1.1">How to train discriminator</span></th>
</tr>
<tr class="ltx_tr" id="S4.T5.2.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T5.2.8.8.1" rowspan="3"><span class="ltx_text" id="S4.T5.2.8.8.1.1">(1)</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T5.2.8.8.2">500 randomly sampled</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.8.8.3">38.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.8.8.4">72.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.8.8.5">83.4</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.2.9.9.1">5k randomly sampled</th>
<td class="ltx_td ltx_align_center" id="S4.T5.2.9.9.2">50.0</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.9.9.3">78.9</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.9.9.4">87.6</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.2.10.10.1">50k randomly sampled</th>
<td class="ltx_td ltx_align_center" id="S4.T5.2.10.10.2">54.2</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.10.10.3">80.2</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.10.10.4">87.4</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.11.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T5.2.11.11.1" rowspan="2"><span class="ltx_text" id="S4.T5.2.11.11.1.1">(2)</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T5.2.11.11.2">500 generated w/o noise</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.11.11.3">42.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.11.11.4">67.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.11.11.5">77.0</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.2.12.12.1">2x training</th>
<td class="ltx_td ltx_align_center" id="S4.T5.2.12.12.2">48.0</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.12.12.3">71.7</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.12.12.4">79.6</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.13.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" id="S4.T5.2.13.13.1">(3)</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="S4.T5.2.13.13.2">500 generated w/ noise</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T5.2.13.13.3">55.7</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T5.2.13.13.4">79.9</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T5.2.13.13.5">86.4</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T5.4.1.1" style="font-size:90%;">Table 5</span>: </span><span class="ltx_text ltx_font_bold" id="S4.T5.5.2" style="font-size:90%;">Ablation on training the generator and discriminator.<span class="ltx_text ltx_font_medium" id="S4.T5.5.2.1"> Training the generator with regression loss applied only to the nearest few samples allows the model to explore alternative modes, leading to improved outcomes. Additionally, training the discriminator with generated noisy camera poses strikes an effective balance between accuracy and training efficiency.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">Training generator directly controls the quality of the generated sample distribution and thus is critical to the performance of the model. By default we only apply <math alttext="\mathcal{L}_{g}" class="ltx_Math" display="inline" id="S4.SS3.p3.1.m1.1"><semantics id="S4.SS3.p3.1.m1.1a"><msub id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS3.p3.1.m1.1.1.2" xref="S4.SS3.p3.1.m1.1.1.2.cmml">â„’</mi><mi id="S4.SS3.p3.1.m1.1.1.3" xref="S4.SS3.p3.1.m1.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><apply id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p3.1.m1.1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.p3.1.m1.1.1.2.cmml" xref="S4.SS3.p3.1.m1.1.1.2">â„’</ci><ci id="S4.SS3.p3.1.m1.1.1.3.cmml" xref="S4.SS3.p3.1.m1.1.1.3">ğ‘”</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">\mathcal{L}_{g}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p3.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT</annotation></semantics></math> to the camera pose that is closest to the ground truth. This allows the model to move samples closer to the ground truth, while simultaneously grants the flexibility for other samples to explore different modes without penalizing them. We compare training the generator by pulling 2% (10), 10% (50), and all samples towards the ground truth. As shown in <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#S4.T5" title="In 4.3 Ablation â€£ 4 Experiments â€£ ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">5</span></a>, it indicates that applying regression loss to all samples may hinder the modelâ€™s ability to explore diverse modes during training, leading to
poorer performance. Furthermore, the generatorâ€™s sensitivity to the exact number of samples used for training is low, provided there is sufficient leeway for other samples to explore.</p>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p4.1.1">How to train the discriminator.</span></p>
</div>
<div class="ltx_para" id="S4.SS3.p5">
<p class="ltx_p" id="S4.SS3.p5.1">Training the discriminator is critical to learn good features that are informative in differentiating different camera poses.
Our observations indicate that the training dynamics of the discriminator, when utilizing samples generated by the pose generator, resemble those observed in generative adversarial networks (GANs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#bib.bib14" title="">14</a>]</cite>, which may lead to instability in training. To mitigate this, we introduce random noise to the generated queries, enhancing the discriminatorâ€™s training stability. We evaluate different strategies for training the discriminator to ascertain the impact of this added noise: (1) use randomly sampled camera poses; (2) use the generated samples from the pose generator; (3) use a noisy version of the generated samples from the pose generator. For 1) we explore the effects of using 500, 5,000, and 50,000 randomly sampled camera poses for training. For (3), we add Gaussian noise to the learnt queries embedding to produce noisy camera poses for the discriminator <math alttext="\mathbf{\mathcal{N}(0,3)+e_{i}}" class="ltx_Math" display="inline" id="S4.SS3.p5.1.m1.2"><semantics id="S4.SS3.p5.1.m1.2a"><mrow id="S4.SS3.p5.1.m1.2.3" xref="S4.SS3.p5.1.m1.2.3.cmml"><mrow id="S4.SS3.p5.1.m1.2.3.2" xref="S4.SS3.p5.1.m1.2.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS3.p5.1.m1.2.3.2.2" xref="S4.SS3.p5.1.m1.2.3.2.2.cmml">ğ’©</mi><mo id="S4.SS3.p5.1.m1.2.3.2.1" xref="S4.SS3.p5.1.m1.2.3.2.1.cmml">â¢</mo><mrow id="S4.SS3.p5.1.m1.2.3.2.3.2" xref="S4.SS3.p5.1.m1.2.3.2.3.1.cmml"><mo id="S4.SS3.p5.1.m1.2.3.2.3.2.1" stretchy="false" xref="S4.SS3.p5.1.m1.2.3.2.3.1.cmml">(</mo><mn id="S4.SS3.p5.1.m1.1.1" xref="S4.SS3.p5.1.m1.1.1.cmml">ğŸ</mn><mo id="S4.SS3.p5.1.m1.2.3.2.3.2.2" xref="S4.SS3.p5.1.m1.2.3.2.3.1.cmml">,</mo><mn id="S4.SS3.p5.1.m1.2.2" xref="S4.SS3.p5.1.m1.2.2.cmml">ğŸ‘</mn><mo id="S4.SS3.p5.1.m1.2.3.2.3.2.3" stretchy="false" xref="S4.SS3.p5.1.m1.2.3.2.3.1.cmml">)</mo></mrow></mrow><mo id="S4.SS3.p5.1.m1.2.3.1" xref="S4.SS3.p5.1.m1.2.3.1.cmml">+</mo><msub id="S4.SS3.p5.1.m1.2.3.3" xref="S4.SS3.p5.1.m1.2.3.3.cmml"><mi id="S4.SS3.p5.1.m1.2.3.3.2" xref="S4.SS3.p5.1.m1.2.3.3.2.cmml">ğ</mi><mi id="S4.SS3.p5.1.m1.2.3.3.3" xref="S4.SS3.p5.1.m1.2.3.3.3.cmml">ğ¢</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p5.1.m1.2b"><apply id="S4.SS3.p5.1.m1.2.3.cmml" xref="S4.SS3.p5.1.m1.2.3"><plus id="S4.SS3.p5.1.m1.2.3.1.cmml" xref="S4.SS3.p5.1.m1.2.3.1"></plus><apply id="S4.SS3.p5.1.m1.2.3.2.cmml" xref="S4.SS3.p5.1.m1.2.3.2"><times id="S4.SS3.p5.1.m1.2.3.2.1.cmml" xref="S4.SS3.p5.1.m1.2.3.2.1"></times><ci id="S4.SS3.p5.1.m1.2.3.2.2.cmml" xref="S4.SS3.p5.1.m1.2.3.2.2">ğ’©</ci><interval closure="open" id="S4.SS3.p5.1.m1.2.3.2.3.1.cmml" xref="S4.SS3.p5.1.m1.2.3.2.3.2"><cn id="S4.SS3.p5.1.m1.1.1.cmml" type="integer" xref="S4.SS3.p5.1.m1.1.1">0</cn><cn id="S4.SS3.p5.1.m1.2.2.cmml" type="integer" xref="S4.SS3.p5.1.m1.2.2">3</cn></interval></apply><apply id="S4.SS3.p5.1.m1.2.3.3.cmml" xref="S4.SS3.p5.1.m1.2.3.3"><csymbol cd="ambiguous" id="S4.SS3.p5.1.m1.2.3.3.1.cmml" xref="S4.SS3.p5.1.m1.2.3.3">subscript</csymbol><ci id="S4.SS3.p5.1.m1.2.3.3.2.cmml" xref="S4.SS3.p5.1.m1.2.3.3.2">ğ</ci><ci id="S4.SS3.p5.1.m1.2.3.3.3.cmml" xref="S4.SS3.p5.1.m1.2.3.3.3">ğ¢</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p5.1.m1.2c">\mathbf{\mathcal{N}(0,3)+e_{i}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p5.1.m1.2d">caligraphic_N ( bold_0 , bold_3 ) + bold_e start_POSTSUBSCRIPT bold_i end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S4.SS3.p6">
<p class="ltx_p" id="S4.SS3.p6.1">The results are shown in <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#S4.T5" title="In 4.3 Ablation â€£ 4 Experiments â€£ ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">5</span></a>. First, using randomly generated camera poses shows that the denser the sampling, the more improved the outcomes. This improvement can be attributed to denser sampling increasing the probability of the samples being closer to the true mode, which in turn provides hard negatives that facilitate the learning process of the discriminator. Second, training directly with the camera poses generated by the model poses a challenge; as the generator becomes more adept at producing accurate poses, the task of distinguishing between them becomes increasingly difficult for the discriminator, potentially causing training instability. This could potentially lead to instability in training. An attempt to mitigate this by doubling the training duration resulted in a 5% increase in performance on the Acc@5 metric. However, this still falls short when compared to other training strategies, underscoring the inherent difficulties in training the discriminator. Lastly, introducing a noisy version of the generated samples to the discriminatorâ€™s training regimen led to an approximate 2-3% improvement across most accuracy thresholds, achieving results comparable to those obtained by training with a large pool of randomly sampled poses (50k in the first setting). This suggests that adding noise to the generated samples can effectively enhance discriminator performance, paralleling the benefits of extensive random sampling.</p>
</div>
<figure class="ltx_figure" id="S4.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="S4.F5.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="465" id="S4.F5.sf1.g1" src="extracted/5796703/figs/diff_n_samples_perf_2.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.sf1.3.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text ltx_font_bold" id="S4.F5.sf1.4.2" style="font-size:90%;">Rotation accuracy with different number of sampled camera poses (N=5).<span class="ltx_text ltx_font_medium" id="S4.F5.sf1.4.2.1">
The proposed method, which directly generates samples, achieves high accuracy with just a few hundred samples. In contrast, RelPose++ relies on random rotations sampled uniformly from SO(3), necessitating a larger sample size to adequately cover the parameter space. This difference becomes more pronounced when measuring accuracy at lower error thresholds.</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="S4.F5.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="488" id="S4.F5.sf2.g1" src="extracted/5796703/figs/generator_num_samples.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.sf2.3.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text ltx_font_bold" id="S4.F5.sf2.4.2" style="font-size:90%;">Generator performance measured as the error of the rotation hypothesis that is closest to the GT.<span class="ltx_text ltx_font_medium" id="S4.F5.sf2.4.2.1">
We compare our generated samples with those randomly sampled from SO(3). We assess our generatorâ€™s effectiveness using the error from the closest rotation hypothesis to the ground truth, as the true pose distribution is unknown. While dense sampling from SO(3) might require over 500,000 samples to cover all rotations, our generator achieves similar accuracy with just 500 samples.
</span></span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.3.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text ltx_font_bold" id="S4.F5.4.2" style="font-size:90%;">Performance of the generator and discriminator as number of sampled poses.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p7">
<p class="ltx_p" id="S4.SS3.p7.1">We demonstrate the performance variations with different numbers of samples, primarily focusing on a comparison with RelPose++ because it also draws samples during inference. The key distinction lies in our methodâ€™s utilization of the generator to produce samples, as opposed to RelPose++ which randomly selects samples from the SO(3) sphere. Our method is designed to circumvent generating samples in regions unlikely to contain the true mode, thereby significantly improving data efficiency. As illustrated in <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#S4.F5.sf1" title="In Figure 5 â€£ 4.3 Ablation â€£ 4 Experiments â€£ ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">5(a)</span></a>, ADen requires merely a few hundred samples to surpass the accuracy that RelPose++ achieves with 500,000 rotations.
This ablation demonstrates ADen is sample efficient and its accuracy is not constraint by the resolution of the sampled grid.</p>
</div>
<div class="ltx_para" id="S4.SS3.p8">
<p class="ltx_p" id="S4.SS3.p8.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p8.1.1">Performance of the generator.</span></p>
</div>
<figure class="ltx_figure" id="S4.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="S4.F6.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="664" id="S4.F6.sf1.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S4.F6.sf1.3.2" style="font-size:90%;">Visualization of generatorâ€™s output with different number of samples.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="S4.F6.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="673" id="S4.F6.sf2.g1" src="x6.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S4.F6.sf2.3.2" style="font-size:90%;">Generatorâ€™s output in challenging (symmetric) cases.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.3.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text ltx_font_bold" id="S4.F6.4.2" style="font-size:90%;">Visualization of the output from generator.<span class="ltx_text ltx_font_medium" id="S4.F6.4.2.1"> Unfilled circles represent ground truth rotation. Dots represent generated rotation samples.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p9">
<p class="ltx_p" id="S4.SS3.p9.1">We present the following analysis to examine how the performance of the generator varies both qualitatively and quantitatively, given its critical role in overall performance. Specifically, <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#S4.F5.sf2" title="In Figure 5 â€£ 4.3 Ablation â€£ 4 Experiments â€£ ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">5(b)</span></a> illustrates the change in error for the optimal rotation hypothesis as the sample size is adjusted. Furthermore, <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#S4.F6.sf1" title="In Figure 6 â€£ 4.3 Ablation â€£ 4 Experiments â€£ ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">6(a)</span></a> provides a visual representation of the rotation hypothesis distribution for varying numbers of samples (M). <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#S4.F6" title="In 4.3 Ablation â€£ 4 Experiments â€£ ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">6</span></a> shows several examples of the generatorâ€™s outputs for images that pose greater challenges, such as those with symmetry. Notably, there is an observed increase in the variance of hypotheses for these more difficult examples. However, the generator accurately models symmetry as a distribution, such as the donut and wine glass.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Inference speed</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">We also compared inference speeds across various methods, each evaluated using nine images as input, tested 50 times on an A6000 GPU. Since SparsePose is not open sourced, we directly quote their runtime from their paper. As can be seen from <a class="ltx_ref" href="https://arxiv.org/html/2408.09042v1#S4.T6" title="In 4.4 Inference speed â€£ 4 Experiments â€£ ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">6</span></a>, ADen reaches an average speed of 0.05s per 9 images and that is above 20 FPS, significantly outperforming all previous methods.
In summary, ADen not only surpasses state-of-the-art methods in accuracy but also demonstrates remarkable real-time speed, averaging 20 FPS with 9 input images, underscoring its efficiency and effectiveness in application.
</p>
</div>
<figure class="ltx_table" id="S4.T6">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T6.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T6.2.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T6.2.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T6.2.1.1.2">Time (secs)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T6.2.1.1.3">FPS</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T6.2.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T6.2.2.1.1">COLMAP (SP + SG)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.2.2.1.2">18</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.2.2.1.3">0.056</td>
</tr>
<tr class="ltx_tr" id="S4.T6.2.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T6.2.3.2.1">SparsePose</th>
<td class="ltx_td ltx_align_center" id="S4.T6.2.3.2.2">3.6</td>
<td class="ltx_td ltx_align_center" id="S4.T6.2.3.2.3">0.278</td>
</tr>
<tr class="ltx_tr" id="S4.T6.2.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T6.2.4.3.1">RelPose/RelPose++</th>
<td class="ltx_td ltx_align_center" id="S4.T6.2.4.3.2">48</td>
<td class="ltx_td ltx_align_center" id="S4.T6.2.4.3.3">0.020</td>
</tr>
<tr class="ltx_tr" id="S4.T6.2.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T6.2.5.4.1">PoseDiff</th>
<td class="ltx_td ltx_align_center" id="S4.T6.2.5.4.2">63.8</td>
<td class="ltx_td ltx_align_center" id="S4.T6.2.5.4.3">0.015</td>
</tr>
<tr class="ltx_tr" id="S4.T6.2.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T6.2.6.5.1"><span class="ltx_text ltx_font_bold" id="S4.T6.2.6.5.1.1">Ours</span></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T6.2.6.5.2"><span class="ltx_text ltx_font_bold" id="S4.T6.2.6.5.2.1">0.05</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T6.2.6.5.3"><span class="ltx_text ltx_font_bold" id="S4.T6.2.6.5.3.1">20</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T6.4.1.1" style="font-size:90%;">Table 6</span>: </span><span class="ltx_text ltx_font_bold" id="S4.T6.5.2" style="font-size:90%;">Inference speed comparison<span class="ltx_text ltx_font_medium" id="S4.T6.5.2.1">. ADen achieves significantly faster inference speed than other methods, averaging 20 FPS with nine images as input.
</span></span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this paper, we present a new learning-based method for recovering camera poses from sparse-view RGB only images. The design of a pose generator and a pose discriminator in ADen empowers the network to navigate the ambiguity inherent in wide baseline images and generate multiple modes. Experiments demonstrates ADen achieves the SoTA performance on CO3D dataset, surpassing previous methods, particularly in accuracy at lower rotation error thresholds. Additionally, owing to its efficient generator, ADen can infer poses for nine images in real-time (20 FPS), demonstrating a significant speed improvement over all prior methods.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Ahmadyan, A., Zhang, L., Ablavatski, A., Wei, J., Grundmann, M.: Objectron: A large scale dataset of object-centric videos in the wild with pose annotations. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 7822â€“7831 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Arnold, E., Wynn, J., Vicente, S., Garcia-Hernando, G., Monszpart, A., Prisacariu, V., Turmukhambetov, D., Brachmann, E.: Map-free visual relocalization: Metric pose relative to a single image. In: European Conference on Computer Vision. pp. 690â€“708. Springer (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Bay, H., Tuytelaars, T., VanÂ Gool, L.: Surf: Speeded up robust features. In: Computer Visionâ€“ECCV 2006: 9th European Conference on Computer Vision, Graz, Austria, May 7-13, 2006. Proceedings, Part I 9. pp. 404â€“417. Springer (2006)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-to-end object detection with transformers. In: European Conference on Computer Vision. pp. 213â€“229. Springer (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Chen, H., Wang, P., Wang, F., Tian, W., Xiong, L., Li, H.: Epro-pnp: Generalized end-to-end probabilistic perspective-n-points for monocular object pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2781â€“2790 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for contrastive learning of visual representations. In: International conference on machine learning. pp. 1597â€“1607. PMLR (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Choi, S., Zhou, Q.Y., Koltun, V.: Robust reconstruction of indoor scenes. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 5556â€“5565 (2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
DeTone, D., Malisiewicz, T., Rabinovich, A.: Superpoint: Self-supervised interest point detection and description. In: Proceedings of the IEEE conference on computer vision and pattern recognition workshops. pp. 224â€“236 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Downs, L., Francis, A., Koenig, N., Kinman, B., Hickman, R., Reymann, K., McHugh, T.B., Vanhoucke, V.: Google scanned objects: A high-quality dataset of 3d scanned household items. In: 2022 International Conference on Robotics and Automation (ICRA). pp. 2553â€“2560. IEEE (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Dusmanu, M., Rocco, I., Pajdla, T., Pollefeys, M., Sivic, J., Torii, A., Sattler, T.: D2-net: A trainable cnn for joint detection and description of local features. arXiv preprint arXiv:1905.03561 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Fischler, M.A., Bolles, R.C.: Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Communications of the ACM <span class="ltx_text ltx_font_bold" id="bib.bib11.1.1">24</span>(6), 381â€“395 (1981)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Furukawa, Y., Curless, B., Seitz, S.M., Szeliski, R.: Towards internet-scale multi-view stereo. In: 2010 IEEE computer society conference on computer vision and pattern recognition. pp. 1434â€“1441. IEEE (2010)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Gleize, P., Wang, W., Feiszli, M.: Silkâ€“simple learned keypoints. arXiv preprint arXiv:2304.06194 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y.: Generative adversarial networks. Communications of the ACM <span class="ltx_text ltx_font_bold" id="bib.bib14.1.1">63</span>(11), 139â€“144 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Hartley, R., Zisserman, A.: Multiple view geometry in computer vision. Cambridge university press (2003)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Hartley, R.I.: In defense of the eight-point algorithm. IEEE Transactions on pattern analysis and machine intelligence <span class="ltx_text ltx_font_bold" id="bib.bib16.1.1">19</span>(6), 580â€“593 (1997)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
He, K., Chen, X., Xie, S., Li, Y., DollÃ¡r, P., Girshick, R.: Masked autoencoders are scalable vision learners. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 16000â€“16009 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770â€“778 (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Kendall, A., Grimes, M., Cipolla, R.: Posenet: A convolutional network for real-time 6-dof camera relocalization. In: Proceedings of the IEEE international conference on computer vision. pp. 2938â€“2946 (2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Kerbl, B., Kopanas, G., LeimkÃ¼hler, T., Drettakis, G.: 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics <span class="ltx_text ltx_font_bold" id="bib.bib20.1.1">42</span>(4) (July 2023), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/" title="">https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Li, H., Hartley, R.: Five-point motion estimation made easy. In: 18th International Conference on Pattern Recognition (ICPRâ€™06). vol.Â 1, pp. 630â€“633. IEEE (2006)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Lin, A., Zhang, J.Y., Ramanan, D., Tulsiani, S.: Relpose++: Recovering 6d poses from sparse-view observations. arXiv preprint arXiv:2305.04926 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Lowe, D.G.: Object recognition from local scale-invariant features. In: Proceedings of the seventh IEEE international conference on computer vision. vol.Â 2, pp. 1150â€“1157. Ieee (1999)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Lucas, B.D., Kanade, T.: An iterative image registration technique with an application to stereo vision. In: IJCAIâ€™81: 7th international joint conference on Artificial intelligence. vol.Â 2, pp. 674â€“679 (1981)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Lugaresi, C., Tang, J., Nash, H., McClanahan, C., Uboweja, E., Hays, M., Zhang, F., Chang, C.L., Yong, M.G., Lee, J., etÂ al.: Mediapipe: A framework for building perception pipelines. arXiv preprint arXiv:1906.08172 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM <span class="ltx_text ltx_font_bold" id="bib.bib26.1.1">65</span>(1), 99â€“106 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Murphy, K., Esteves, C., Jampani, V., Ramalingam, S., Makadia, A.: Implicit-pdf: Non-parametric representation of probability distributions on the rotation manifold. arXiv preprint arXiv:2106.05965 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Niemeyer, M., Barron, J.T., Mildenhall, B., Sajjadi, M.S., Geiger, A., Radwan, N.: Regnerf: Regularizing neural radiance fields for view synthesis from sparse inputs. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5480â€“5490 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Ã–zyeÅŸil, O., Voroninski, V., Basri, R., Singer, A.: A survey of structure from motion*. Acta Numerica <span class="ltx_text ltx_font_bold" id="bib.bib29.1.1">26</span>, 305â€“364 (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Reizenstein, J., Shapovalov, R., Henzler, P., Sbordone, L., Labatut, P., Novotny, D.: Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 10901â€“10911 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Revaud, J., DeÂ Souza, C., Humenberger, M., Weinzaepfel, P.: R2d2: Reliable and repeatable detector and descriptor. Advances in neural information processing systems <span class="ltx_text ltx_font_bold" id="bib.bib31.1.1">32</span> (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Sarlin, P.E., Cadena, C., Siegwart, R., Dymczyk, M.: From coarse to fine: Robust hierarchical localization at large scale. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12716â€“12725 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Sarlin, P.E., DeTone, D., Malisiewicz, T., Rabinovich, A.: Superglue: Learning feature matching with graph neural networks. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 4938â€“4947 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Schonberger, J.L., Frahm, J.M.: Structure-from-motion revisited. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 4104â€“4113 (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Sinha, S., Zhang, J.Y., Tagliasacchi, A., Gilitschenski, I., Lindell, D.B.: Sparsepose: Sparse-view camera pose regression and refinement. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 21349â€“21359 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 8922â€“8931 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Triggs, B., McLauchlan, P.F., Hartley, R.I., Fitzgibbon, A.W.: Bundle adjustmentâ€”a modern synthesis. In: Vision Algorithms: Theory and Practice: International Workshop on Vision Algorithms Corfu, Greece, September 21â€“22, 1999 Proceedings. pp. 298â€“372. Springer (2000)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Truong, P., Danelljan, M., Timofte, R.: Glu-net: Global-local universal network for dense flow and correspondences. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 6258â€“6268 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Umeyama, S.: Least-squares estimation of transformation parameters between two point patterns. IEEE Transactions on Pattern Analysis &amp; Machine Intelligence <span class="ltx_text ltx_font_bold" id="bib.bib39.1.1">13</span>(04), 376â€“380 (1991)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. In: Advances in neural information processing systems. pp. 5998â€“6008 (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Wang, J., Rupprecht, C., Novotny, D.: Posediffusion: Solving pose estimation via diffusion-aided bundle adjustment. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 9773â€“9783 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Yu, A., Ye, V., Tancik, M., Kanazawa, A.: pixelnerf: Neural radiance fields from one or few images. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4578â€“4587 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Zhang, J.Y., Ramanan, D., Tulsiani, S.: Relpose: Predicting probabilistic relative rotation for single objects in the wild. In: European Conference on Computer Vision. pp. 592â€“611. Springer (2022)

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Aug 16 22:01:34 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
