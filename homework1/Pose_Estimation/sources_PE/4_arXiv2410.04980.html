<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Comparison of marker-less 2D image-based methods for infant pose estimation</title>
<!--Generated on Mon Oct  7 12:15:44 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.04980v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S1" title="In Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S2" title="In Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Materials and Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S2.SS1" title="In 2 Materials and Methods ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S2.SS2" title="In 2 Materials and Methods ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Pose estimation frameworks</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S2.SS2.SSS1" title="In 2.2 Pose estimation frameworks ‚Ä£ 2 Materials and Methods ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>Generic pose estimation</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S2.SS2.SSS1.Px1" title="In 2.2.1 Generic pose estimation ‚Ä£ 2.2 Pose estimation frameworks ‚Ä£ 2 Materials and Methods ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_title">OpenPose <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup"><span class="ltx_ref">35</span>, <span class="ltx_ref">36</span>, <span class="ltx_ref">23</span></sup></cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S2.SS2.SSS1.Px2" title="In 2.2.1 Generic pose estimation ‚Ä£ 2.2 Pose estimation frameworks ‚Ä£ 2 Materials and Methods ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_title">MediaPipe pose <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup"><span class="ltx_ref">37</span>, <span class="ltx_ref">38</span></sup></cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S2.SS2.SSS1.Px3" title="In 2.2.1 Generic pose estimation ‚Ä£ 2.2 Pose estimation frameworks ‚Ä£ 2 Materials and Methods ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_title">HRNet <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup"><span class="ltx_ref">39</span>, <span class="ltx_ref">40</span></sup></cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S2.SS2.SSS1.Px4" title="In 2.2.1 Generic pose estimation ‚Ä£ 2.2 Pose estimation frameworks ‚Ä£ 2 Materials and Methods ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_title">ViTPose <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup"><span class="ltx_ref">25</span></sup></cite></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S2.SS2.SSS2" title="In 2.2 Pose estimation frameworks ‚Ä£ 2 Materials and Methods ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>Infant pose estimation</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S2.SS2.SSS2.Px1" title="In 2.2.2 Infant pose estimation ‚Ä£ 2.2 Pose estimation frameworks ‚Ä£ 2 Materials and Methods ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_title">AggPose <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup"><span class="ltx_ref">33</span></sup></cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S2.SS2.SSS2.Px2" title="In 2.2.2 Infant pose estimation ‚Ä£ 2.2 Pose estimation frameworks ‚Ä£ 2 Materials and Methods ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_title">AGMA-HRNet48 <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup"><span class="ltx_ref">34</span></sup></cite></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S2.SS2.SSS3" title="In 2.2 Pose estimation frameworks ‚Ä£ 2 Materials and Methods ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.3 </span>Retraining</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S2.SS3" title="In 2 Materials and Methods ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Quantification measures</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S2.SS3.SSS1" title="In 2.3 Quantification measures ‚Ä£ 2 Materials and Methods ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.1 </span>Pose estimation error</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S2.SS3.SSS2" title="In 2.3 Quantification measures ‚Ä£ 2 Materials and Methods ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.2 </span>Statistical tests</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S3" title="In Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S3.SS1" title="In 3 Results ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Variability in human labeling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S3.SS2" title="In 3 Results ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Generic pose estimators</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S3.SS3" title="In 3 Results ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Influence of the view angle</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S3.SS4" title="In 3 Results ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Infant pose estimators</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S3.SS5" title="In 3 Results ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Retraining for individual view angles</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S4" title="In Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S4.SS1" title="In 4 Discussion ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Generic pose estimation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S4.SS2" title="In 4 Discussion ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Influence of the view angle</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S4.SS3" title="In 4 Discussion ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Retraining</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S5" title="In Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#A1" title="In Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Dataset</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#A1.SS1" title="In Appendix A Dataset ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Participants</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#A1.SS2" title="In Appendix A Dataset ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Infant movements recordings</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#A1.SS3" title="In Appendix A Dataset ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Recording Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#A1.SS4" title="In Appendix A Dataset ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4 </span>Image selection and annotation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#A2" title="In Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Pose estimation</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#A2.SS1" title="In Appendix B Pose estimation ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Retraining</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#A2.SS2" title="In Appendix B Pose estimation ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span>Quantification measures</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#A3" title="In Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Keypoint detection rates</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#A4" title="In Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>SyRIP</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document">Comparison of marker-less 2D image-based methods for infant pose estimation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lennart Jahn<sup class="ltx_sup" id="id9.9.id1"><span class="ltx_text ltx_font_italic" id="id9.9.id1.1">1,2</span></sup>, Sarah Fl√ºgge<sup class="ltx_sup" id="id10.10.id2"><span class="ltx_text ltx_font_italic" id="id10.10.id2.1">1</span></sup>, Dajie Zhang<sup class="ltx_sup" id="id11.11.id3"><span class="ltx_text ltx_font_italic" id="id11.11.id3.1">3,4</span></sup>, Luise Poustka<sup class="ltx_sup" id="id12.12.id4"><span class="ltx_text ltx_font_italic" id="id12.12.id4.1">3</span></sup>,
<br class="ltx_break"/>Sven B√∂lte<sup class="ltx_sup" id="id13.13.id5"><span class="ltx_text ltx_font_italic" id="id13.13.id5.1">5,6,7</span></sup>, Florentin W√∂rg√∂tter<sup class="ltx_sup" id="id14.14.id6"><span class="ltx_text ltx_font_italic" id="id14.14.id6.1">2</span></sup>, Peter B Marschik<sup class="ltx_sup" id="id15.15.id7"><span class="ltx_text ltx_font_italic" id="id15.15.id7.1">1,3,4,5,‚àó</span></sup>,
<br class="ltx_break"/>Tomas Kulvicius<sup class="ltx_sup" id="id16.16.id8"><span class="ltx_text ltx_font_italic" id="id16.16.id8.1">1,2,3</span></sup>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id17.id1">There are increasing efforts to automate clinical methods for early diagnosis of developmental disorders, among them the General Movement Assessment (GMA), a video-based tool to classify infant motor functioning. Optimal pose estimation is a crucial part of the automated GMA. In this study we compare the performance of available generic- and infant-pose estimators, and the choice of viewing angle for optimal recordings, i.e., conventional diagonal view used in GMA vs. top-down view. For this study, we used 4500 annotated video-frames from 75 recordings of infant spontaneous motor functions from 4 to 26 weeks. To determine which available pose estimation method and camera angle yield the best pose estimation accuracy on infants in a GMA related setting, the distance to human annotations as well as the percentage of correct key-points (PCK) were computed and compared. The results show that the best performing generic model trained on adults, ViTPose, also performs best on infants. We see no improvement from using specialized infant-pose estimators over the generic pose estimators on our own infant dataset. However, when retraining a generic model on our data, there is a significant improvement in pose estimation accuracy. The pose estimation accuracy obtained from the top-down view is significantly better than that obtained from the diagonal view, especially for the detection of the hip key-points. The results also indicate only limited generalization capabilities of infant-pose estimators to other infant datasets, which hints that one should be careful when choosing infant pose estimators and using them on infant datasets which they were not trained on. While the standard GMA method uses a diagonal view for assessment, pose estimation accuracy significantly improves using a top-down view. This suggests that a top-down view should be included in recording setups for automated GMA research.</p>
</div>
<div class="ltx_para ltx_align_center" id="p1">
<p class="ltx_p" id="p1.8"><sup class="ltx_sup" id="p1.8.8"><span class="ltx_text" id="p1.8.8.1" style="font-size:90%;">1</span></sup><span class="ltx_text" id="p1.8.7" style="font-size:90%;">Child and Adolescent Psychiatry and Psychotherapy, University Medical Center G√∂ttingen; German Center for Child and Adolescent Health (DZKJ), Leibniz ScienceCampus G√∂ttingen, G√∂ttingen, Germany
<br class="ltx_break"/><sup class="ltx_sup" id="p1.8.7.1">2</sup>University of G√∂ttingen, III Institute of Physics - Biophysics, G√∂ttingen, Germany
<br class="ltx_break"/><sup class="ltx_sup" id="p1.8.7.2"><span class="ltx_text ltx_font_italic" id="p1.8.7.2.1">3</span></sup>Department of Child and Adolescent Psychiatry, University Hospital Heidelberg, Ruprecht-Karls
University of Heidelberg, Heidelberg, Germany
<br class="ltx_break"/><sup class="ltx_sup" id="p1.8.7.3"><span class="ltx_text ltx_font_italic" id="p1.8.7.3.1">4</span></sup>iDN ‚Äì interdisciplinary Developmental Neuroscience, Division of Phoniatrics, Medical University
of Graz, Graz, Austria 
<br class="ltx_break"/><sup class="ltx_sup" id="p1.8.7.4"><span class="ltx_text ltx_font_italic" id="p1.8.7.4.1">5</span></sup>Center of Neurodevelopmental Disorders (KIND), Department of Women‚Äôs and Children‚Äôs Health,
Center for Psychiatry Research, Karolinska Institutet &amp; Region Stockholm, Stockholm, Sweden
<br class="ltx_break"/><sup class="ltx_sup" id="p1.8.7.5"><span class="ltx_text ltx_font_italic" id="p1.8.7.5.1">6</span></sup>Child and Adolescent Psychiatry, Stockholm Health Care Services, Region Stockholm, Stockholm Sweden
<br class="ltx_break"/><sup class="ltx_sup" id="p1.8.7.6"><span class="ltx_text ltx_font_italic" id="p1.8.7.6.1">7</span></sup>Curtin Autism Research Group, Curtin School of Allied Health, Curtin University, Perth, Australia
<br class="ltx_break"/><sup class="ltx_sup" id="p1.8.7.7">‚àó</sup>corresponding author: peter.marschik@med.uni-heidelberg.de
<br class="ltx_break"/>
</span></p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">When classic biomarker approaches fail to detect developmental conditions early or predict neurodevelopmental outcomes following pre- or perinatal brain lesions or complications during pregnancy, assessments of overt neurofunctions come into play<cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S1.p1.1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib3" title="">3</a></sup></cite>. An early detection of biological markers of adverse outcomes or early diagnosis of developmental conditions facilitates early intervention, when brain plasticity is at its peak, aiming to achieve best possible long-term outcomes <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S1.p1.1.2.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib4" title="">4</a></sup></cite>. One of the available tools to functionally assess the developing nervous system is the Prechtl general movements assessment (GMA) <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S1.p1.1.3.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib5" title="">5</a></sup></cite>. Over the last decades, GMA widened its scope and applicability to serve as a general estimate of the integrity of the developing nervous system. It became a tool to look beyond the mere distinction of typical development vs. high risk for cerebral palsy, e.g., to determine the impact of viral infections on postnatal neurodevelopment or describe the early phases of neurodevelopmental conditions such as autism <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S1.p1.1.4.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib7" title="">7</a></sup></cite>. This broad applicability was the reason we used the multi-camera recording technique which included the standard GMA perspective as a use case for this study <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S1.p1.1.5.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib9" title="">9</a></sup></cite>. GMA is a gestalt-based observational tool to classify spontaneous infant motor functions in the first months of life <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S1.p1.1.6.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib5" title="">5</a></sup></cite>. Infants up to 20 weeks of post-term age are positioned in prone in a cot and their spontaneous movements, i.e. unstimulated, are video-recorded. These movements can be analyzed and classified into physiological or atypical movement patterns <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S1.p1.1.7.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib3" title="">3</a></sup></cite>.
Given its high sensitivity and specificity it has become one tool of choice for the early detection of cerebral palsy in the postnatal period <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S1.p1.1.8.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib3" title="">3</a></sup></cite>. Recently, efforts have been intensified to automate this clinical method using computers (for recent reviews see <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S1.p1.1.9.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib16" title="">16</a></sup></cite>).</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In early studies, spontaneous movements in the first months of life were measured with sensors directly attached to the infant <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S1.p2.1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib18" title="">18</a></sup></cite>, or using optical flow methods <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S1.p2.1.2.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib21" title="">21</a></sup></cite>.
With the advent of neural networks for pose estimation <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S1.p2.1.3.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib25" title="">25</a></sup></cite> current studies have switched to using skeleton keypoints for visual-based movement analysis and classification <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S1.p2.1.4.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib31" title="">31</a></sup></cite>.
At first, those pose estimators were only trained on adults and used for infants without modification.
Later work identified problems with this approach and developed infant specific models <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S1.p2.1.5.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib34" title="">34</a></sup></cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Overall, it is difficult to compare existing models, evaluate their true performance and generalize their results. The main reason is, that models were trained and tested on silo-data-sets (patient data) as we are facing a complex data sharing issue which presents a true obstacle for all video-based clinical methods <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S1.p3.1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib30" title="">30</a></sup></cite>.
There are comparisons between related model architectures when selecting the best model <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S1.p3.1.2.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib34" title="">34</a></sup></cite>, but there has not yet been any comparison of completely different models that were trained on completely different training sets.
Thus, it is unknown if the generalization capabilities of special infant pose estimators are sufficient for the use on datasets the particular models were not trained on.
To test the generalization capabilities of generic pose estimation models, and to compare those to the infant specific models, we first analyzed four different pose estimation models, OpenPose <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S1.p3.1.3.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib23" title="">23</a></sup></cite>, MediaPipe <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S1.p3.1.4.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib38" title="">38</a></sup></cite>, HRNet <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S1.p3.1.5.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib40" title="">40</a></sup></cite> and ViTPose <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S1.p3.1.6.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib25" title="">25</a></sup></cite>. In a second step, we retrained the best performing model ViTPose <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S1.p3.1.7.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib41" title="">41</a></sup></cite> on our dataset.
We then compare the retrained model to the two infant pose estimators, AggPose <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S1.p3.1.8.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib33" title="">33</a></sup></cite> and AGMA-HRNet48 <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S1.p3.1.9.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib34" title="">34</a></sup></cite>.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Furthermore, most analysis so far has been carried out in 2D space and has only recently been transferred to the 3D space <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S1.p4.1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib44" title="">44</a></sup></cite>.
We have also collected a new multi-view infant video dataset, taken from four different camera angles (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S2.F1" title="Figure 1 ‚Ä£ 2.1 Dataset ‚Ä£ 2 Materials and Methods ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_tag">1</span></a>a) which allows 3D reconstruction.
We chose these four angles to provide occlusion robustness for various infant poses.
The viewing angle has great influence on the visibility of different body parts and should therefore be carefully considered when designing an experimental setup.
Since we captured four angles at once, this offers the opportunity to directly compare the different viewing angles that were commonly used in other works, a diagonal view or a top view (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S2.F1" title="Figure 1 ‚Ä£ 2.1 Dataset ‚Ä£ 2 Materials and Methods ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_tag">1</span></a>b,c), and quantify their influence on pose estimation accuracy. GMA, in its clinical application, is usually done with single 2D-RGB cameras in a diagonal view <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S1.p4.1.2.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib45" title="">45</a></sup></cite>.
For this study, we split the analyses done on generic and specific infant pose estimators by viewing angle and compare them.
We also specifically retrained on single angles trying to achieve the best performing models.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">The results can be used as an indication for the optimal selection of camera angles in future projects.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">The main contributions of this work are the comparison of state-of-the-art infant pose estimators and the influence of the viewing angle on pose estimation accuracy.
Specifically, we address four main research questions:</p>
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">Which generic pose estimator is best suited for infant pose estimation?</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">Do other infant pose estimators generalize well enough such that they achieve better accuracy on our dataset, or is specific retraining necessary?</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Is there a difference in pose estimation accuracy between viewing angles (classic diagonal GMA-perspective vs. top-down)?</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">Is having a dedicated estimator per view angle better than one trained on multiple views?</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Materials and Methods</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Dataset</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.2">We built a dataset of 4500 frames with COCO <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S2.SS1.p1.2.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib46" title="">46</a></sup></cite> style labeling (keypoints: nose, eyes, ears, shoulders, elbows, wrists, hips, knees and ankles).
The recordings for this analysis were performed utilising a multi-angle marker-less motion tracking setup, specifically designed to record young infants and include 75 recordings of 31 participants from 28 <math alttext="\pm" class="ltx_Math" display="inline" id="S2.SS1.p1.1.m1.1"><semantics id="S2.SS1.p1.1.m1.1a"><mo id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">¬±</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><csymbol cd="latexml" id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.1.m1.1d">¬±</annotation></semantics></math> 2 days to 112 <math alttext="\pm" class="ltx_Math" display="inline" id="S2.SS1.p1.2.m2.1"><semantics id="S2.SS1.p1.2.m2.1a"><mo id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml">¬±</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><csymbol cd="latexml" id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.2.m2.1d">¬±</annotation></semantics></math> 2 days of gestational age.
For this study, we chose the two most common camera angles and relevant view points in the clinical use of GMA: straight down from right above the bed (top-down view) and a view as for a human standing at the foot end of the bed (diagonal view; standard clinical GMA view; <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S2.SS1.p1.2.2.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib8" title="">8</a></sup></cite>).
Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S2.F1" title="Figure 1 ‚Ä£ 2.1 Dataset ‚Ä£ 2 Materials and Methods ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_tag">1</span></a> shows an overview of the camera setup and its output.
Further description is given in Supplementary Material <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#A1" title="Appendix A Dataset ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="225" id="S2.F1.g1" src="extracted/5907085/setup.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S2.F1.3.2" style="font-size:90%;">Overview of the recording setup a) and its output b) and c). The cameras recording the infants are circled in red. For this study, only the two cameras labeled diagonal / top view were used. Panels b) and c) show example frames for infants of different age and pose complexity from the two different views. The extracted pose keypoints are displayed as skeletons over the image. Note that neither the human annotators nor any of the pose estimators could reliably determine the position of the fully covered ear in the rightmost example.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Pose estimation frameworks</h3>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Generic pose estimation</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">For comparison, we selected four different generic human pose estimation frameworks.</p>
</div>
<section class="ltx_paragraph" id="S2.SS2.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">OpenPose <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S2.SS2.SSS1.Px1.1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib23" title="">23</a></sup></cite>
</h5>
<div class="ltx_para" id="S2.SS2.SSS1.Px1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.Px1.p1.1">was one of the first pose estimation frameworks used for extraction of movements for analysis and classification of infant motor functions. <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S2.SS2.SSS1.Px1.p1.1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib27" title="">27</a></sup></cite>
We included this framework as a baseline method to evaluate the performance gain of state-of-the-art models.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">MediaPipe pose <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S2.SS2.SSS1.Px2.1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib38" title="">38</a></sup></cite>
</h5>
<div class="ltx_para" id="S2.SS2.SSS1.Px2.p1">
<p class="ltx_p" id="S2.SS2.SSS1.Px2.p1.1">This framework is not primarily intended for accurate pose estimation, but rather for fast inference on mobile devices.
It was interesting for our comparison as Google advertises it as suitable for Yoga and fitness applications and included 25000 images of fitness exercises, so we expected it to deal well with different viewing angles and poses.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS1.Px3">
<h5 class="ltx_title ltx_title_paragraph">HRNet <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S2.SS2.SSS1.Px3.1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib40" title="">40</a></sup></cite>
</h5>
<div class="ltx_para" id="S2.SS2.SSS1.Px3.p1">
<p class="ltx_p" id="S2.SS2.SSS1.Px3.p1.1">The HR in HRNet stands for high resolution. The particular version used has an input size of <math alttext="512\times 512" class="ltx_Math" display="inline" id="S2.SS2.SSS1.Px3.p1.1.m1.1"><semantics id="S2.SS2.SSS1.Px3.p1.1.m1.1a"><mrow id="S2.SS2.SSS1.Px3.p1.1.m1.1.1" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.cmml"><mn id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.2" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.2.cmml">512</mn><mo id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.1.cmml">√ó</mo><mn id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.Px3.p1.1.m1.1b"><apply id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1"><times id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.1.cmml" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.1"></times><cn id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.2.cmml" type="integer" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.2">512</cn><cn id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.cmml" type="integer" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.Px3.p1.1.m1.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.Px3.p1.1.m1.1d">512 √ó 512</annotation></semantics></math> pixels.
Because our videos are of sufficient resolution and adequate quality with low compression artefacts, this could benefit the pose estimation accuracy.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS1.Px4">
<h5 class="ltx_title ltx_title_paragraph">ViTPose <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S2.SS2.SSS1.Px4.1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib25" title="">25</a></sup></cite>
</h5>
<div class="ltx_para" id="S2.SS2.SSS1.Px4.p1">
<p class="ltx_p" id="S2.SS2.SSS1.Px4.p1.1">This model is the current best performing framework for human pose estimation on the COCO test-dev dataset and uses a Vision Transformer architecture instead of the conventional CNN.</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Infant pose estimation</h4>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">We also selected two pose estimation frameworks which were trained on infant images and tested them against generic frameworks. In addition, we also included a ViTPose model which was retrained on our infant dataset (see Section <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S2.SS2.SSS3" title="2.2.3 Retraining ‚Ä£ 2.2 Pose estimation frameworks ‚Ä£ 2 Materials and Methods ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_tag">2.2.3</span></a>).</p>
</div>
<section class="ltx_paragraph" id="S2.SS2.SSS2.Px1">
<h5 class="ltx_title ltx_title_paragraph">AggPose <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S2.SS2.SSS2.Px1.1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib33" title="">33</a></sup></cite>
</h5>
<div class="ltx_para" id="S2.SS2.SSS2.Px1.p1">
<p class="ltx_p" id="S2.SS2.SSS2.Px1.p1.1">The AggPose model is based on the Transformer architecture, like ViTPose, and is
specifically trained for infant pose detection on a proprietary dataset.
In general, the images in the dataset also stem from recordings used for GMA, with infants in supine position, like in our dataset.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS2.Px2">
<h5 class="ltx_title ltx_title_paragraph">AGMA-HRNet48 <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S2.SS2.SSS2.Px2.1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib34" title="">34</a></sup></cite>
</h5>
<div class="ltx_para" id="S2.SS2.SSS2.Px2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.Px2.p1.1">Soualmi et al. also retrained infant pose estimation networks on a proprietary dataset of GMA recordings.
Compared to our dataset, the images contain clinical equipment (e.g., tubes, electrodes, cables), leading to more visual clutter in the scenes.
The authors retrained multiple networks, of which we chose the HRNet48, because it is the same architecture as the one we used for the comparison of the generic pose estimation frameworks.</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3 </span>Retraining</h4>
<div class="ltx_para" id="S2.SS2.SSS3.p1">
<p class="ltx_p" id="S2.SS2.SSS3.p1.1">We retrained the best performing framework ViTPose (see Section <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S3.SS2" title="3.2 Generic pose estimators ‚Ä£ 3 Results ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_tag">3.2</span></a>) on our labeled dataset using 5-fold cross-validation. For information about the split and retraining procedure see Supplementary Material <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#A2.SS1" title="B.1 Retraining ‚Ä£ Appendix B Pose estimation ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_tag">B.1</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Quantification measures</h3>
<section class="ltx_subsubsection" id="S2.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.1 </span>Pose estimation error</h4>
<div class="ltx_para" id="S2.SS3.SSS1.p1">
<p class="ltx_p" id="S2.SS3.SSS1.p1.1">We used two main evaluation metrics to assess the performance of the pose estimation methods: the difference between predicted keypoint position and the human annotation, <math alttext="d_{a}" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p1.1.m1.1"><semantics id="S2.SS3.SSS1.p1.1.m1.1a"><msub id="S2.SS3.SSS1.p1.1.m1.1.1" xref="S2.SS3.SSS1.p1.1.m1.1.1.cmml"><mi id="S2.SS3.SSS1.p1.1.m1.1.1.2" xref="S2.SS3.SSS1.p1.1.m1.1.1.2.cmml">d</mi><mi id="S2.SS3.SSS1.p1.1.m1.1.1.3" xref="S2.SS3.SSS1.p1.1.m1.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p1.1.m1.1b"><apply id="S2.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S2.SS3.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS1.p1.1.m1.1.1.1.cmml" xref="S2.SS3.SSS1.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS3.SSS1.p1.1.m1.1.1.2.cmml" xref="S2.SS3.SSS1.p1.1.m1.1.1.2">ùëë</ci><ci id="S2.SS3.SSS1.p1.1.m1.1.1.3.cmml" xref="S2.SS3.SSS1.p1.1.m1.1.1.3">ùëé</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p1.1.m1.1c">d_{a}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.p1.1.m1.1d">italic_d start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math>, in pixels, and the percentage of correct keypoints (PCK). In this work we considered the PCK at 5%, 7.5% and 10% of the torso size, denoted by PCK@0.05, PCK@0.75 and PCK@0.1, respectively.
See Supplementary material <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#A2.SS2" title="B.2 Quantification measures ‚Ä£ Appendix B Pose estimation ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_tag">B.2</span></a> for the exact definitions of the metrics and their relation to real-world-distance.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS1.p2">
<p class="ltx_p" id="S2.SS3.SSS1.p2.1">Our dataset is not biased to any specific side (left or right) of the infants.
Moreover, labeling left and right sides of hands, knees, etc. is not inherently different.
Therefore, we merged the errors for keypoints of the left and right side into one.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.2 </span>Statistical tests</h4>
<div class="ltx_para" id="S2.SS3.SSS2.p1">
<p class="ltx_p" id="S2.SS3.SSS2.p1.1">When comparing the results with respect to the difference <math alttext="d_{a}" class="ltx_Math" display="inline" id="S2.SS3.SSS2.p1.1.m1.1"><semantics id="S2.SS3.SSS2.p1.1.m1.1a"><msub id="S2.SS3.SSS2.p1.1.m1.1.1" xref="S2.SS3.SSS2.p1.1.m1.1.1.cmml"><mi id="S2.SS3.SSS2.p1.1.m1.1.1.2" xref="S2.SS3.SSS2.p1.1.m1.1.1.2.cmml">d</mi><mi id="S2.SS3.SSS2.p1.1.m1.1.1.3" xref="S2.SS3.SSS2.p1.1.m1.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p1.1.m1.1b"><apply id="S2.SS3.SSS2.p1.1.m1.1.1.cmml" xref="S2.SS3.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS2.p1.1.m1.1.1.1.cmml" xref="S2.SS3.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS3.SSS2.p1.1.m1.1.1.2.cmml" xref="S2.SS3.SSS2.p1.1.m1.1.1.2">ùëë</ci><ci id="S2.SS3.SSS2.p1.1.m1.1.1.3.cmml" xref="S2.SS3.SSS2.p1.1.m1.1.1.3">ùëé</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p1.1.m1.1c">d_{a}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS2.p1.1.m1.1d">italic_d start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math>, we tested the statistical significance of model differences with a paired sample t-test on all individual keypoints in the dataset (or sub-sets of keypoints if indicated).
For PCK, where each point is either correct or incorrect, we used a Pearson‚Äôs chi-squared test on the frequencies of the outcomes (correct detection or not) to test if the samples for each pose estimation model could come from the same distribution.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Variability in human labeling</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S3.F2.sf1" title="In Figure 2 ‚Ä£ 3.1 Variability in human labeling ‚Ä£ 3 Results ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_tag">2(a)</span></a> shows the mean pixel difference between the two annotators for each keypoint.
The results show that the keypoints that were hardest to label are the hips and shoulders.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">When comparing the two viewing angles, there is a difference between the diagonal and top views.
The error for the diagonal view is consistently higher than for the top view.
Except for the nose keypoint, the confidence intervals of the means never overlap.
For the hip keypoint, which has the highest error overall, the relative difference between top and diagonal view is also highest.</p>
</div>
<figure class="ltx_figure" id="S3.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S3.F2.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F2.sf1.g1" src="extracted/5907085/interhuman_all.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S3.F2.sf1.3.2" style="font-size:90%;">Difference between two annotators, additionally split by viewing angle.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="S3.F2.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F2.sf2.g1" src="extracted/5907085/modeltrackingerror.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.sf2.4.2.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S3.F2.sf2.2.1" style="font-size:90%;">Mean <math alttext="d_{a}" class="ltx_Math" display="inline" id="S3.F2.sf2.2.1.m1.1"><semantics id="S3.F2.sf2.2.1.m1.1b"><msub id="S3.F2.sf2.2.1.m1.1.1" xref="S3.F2.sf2.2.1.m1.1.1.cmml"><mi id="S3.F2.sf2.2.1.m1.1.1.2" xref="S3.F2.sf2.2.1.m1.1.1.2.cmml">d</mi><mi id="S3.F2.sf2.2.1.m1.1.1.3" xref="S3.F2.sf2.2.1.m1.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F2.sf2.2.1.m1.1c"><apply id="S3.F2.sf2.2.1.m1.1.1.cmml" xref="S3.F2.sf2.2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.F2.sf2.2.1.m1.1.1.1.cmml" xref="S3.F2.sf2.2.1.m1.1.1">subscript</csymbol><ci id="S3.F2.sf2.2.1.m1.1.1.2.cmml" xref="S3.F2.sf2.2.1.m1.1.1.2">ùëë</ci><ci id="S3.F2.sf2.2.1.m1.1.1.3.cmml" xref="S3.F2.sf2.2.1.m1.1.1.3">ùëé</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.sf2.2.1.m1.1d">d_{a}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.sf2.2.1.m1.1e">italic_d start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math> of the different generic frameworks, human for comparison.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="S3.F2.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F2.sf3.g1" src="extracted/5907085/retrained_comparison.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.sf3.4.2.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S3.F2.sf3.2.1" style="font-size:90%;">Mean <math alttext="d_{a}" class="ltx_Math" display="inline" id="S3.F2.sf3.2.1.m1.1"><semantics id="S3.F2.sf3.2.1.m1.1b"><msub id="S3.F2.sf3.2.1.m1.1.1" xref="S3.F2.sf3.2.1.m1.1.1.cmml"><mi id="S3.F2.sf3.2.1.m1.1.1.2" xref="S3.F2.sf3.2.1.m1.1.1.2.cmml">d</mi><mi id="S3.F2.sf3.2.1.m1.1.1.3" xref="S3.F2.sf3.2.1.m1.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F2.sf3.2.1.m1.1c"><apply id="S3.F2.sf3.2.1.m1.1.1.cmml" xref="S3.F2.sf3.2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.F2.sf3.2.1.m1.1.1.1.cmml" xref="S3.F2.sf3.2.1.m1.1.1">subscript</csymbol><ci id="S3.F2.sf3.2.1.m1.1.1.2.cmml" xref="S3.F2.sf3.2.1.m1.1.1.2">ùëë</ci><ci id="S3.F2.sf3.2.1.m1.1.1.3.cmml" xref="S3.F2.sf3.2.1.m1.1.1.3">ùëé</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.sf3.2.1.m1.1d">d_{a}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.sf3.2.1.m1.1e">italic_d start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math> for the infant pose estimators. Generic ViTPose is also shown for comparison to non-retrained results. Note that AggPose does not estimate the positions of nose and ears. </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.4.2.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S3.F2.2.1" style="font-size:90%;">Difference to annotation <math alttext="d_{a}" class="ltx_Math" display="inline" id="S3.F2.2.1.m1.1"><semantics id="S3.F2.2.1.m1.1b"><msub id="S3.F2.2.1.m1.1.1" xref="S3.F2.2.1.m1.1.1.cmml"><mi id="S3.F2.2.1.m1.1.1.2" xref="S3.F2.2.1.m1.1.1.2.cmml">d</mi><mi id="S3.F2.2.1.m1.1.1.3" xref="S3.F2.2.1.m1.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F2.2.1.m1.1c"><apply id="S3.F2.2.1.m1.1.1.cmml" xref="S3.F2.2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.F2.2.1.m1.1.1.1.cmml" xref="S3.F2.2.1.m1.1.1">subscript</csymbol><ci id="S3.F2.2.1.m1.1.1.2.cmml" xref="S3.F2.2.1.m1.1.1.2">ùëë</ci><ci id="S3.F2.2.1.m1.1.1.3.cmml" xref="S3.F2.2.1.m1.1.1.3">ùëé</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.2.1.m1.1d">d_{a}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.2.1.m1.1e">italic_d start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math> in pixels for different subjects, evaluated on our dataset and grouped by key point</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Generic pose estimators</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">All four generic pose estimation models were used to estimate keypoints in the whole dataset.
The difference between the predictions and the ground truth is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S3.F2.sf2" title="In Figure 2 ‚Ä£ 3.1 Variability in human labeling ‚Ä£ 3 Results ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_tag">2(b)</span></a>.
The human labeling difference as presented in section <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S3.SS1" title="3.1 Variability in human labeling ‚Ä£ 3 Results ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_tag">3.1</span></a> is added for comparison.
Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S3.T1" title="Table 1 ‚Ä£ 3.4 Infant pose estimators ‚Ä£ 3 Results ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_tag">1</span></a> shows the corresponding PCK values.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">The qualitative result is very close to the difference between human annotators, with the hips having the highest error.
But in contrast to the human annotations, the shoulder error is relatively lower, compared to the hip error.
Because of the size of the dataset, the differences between the pose estimation models are all statistically significant with <math alttext="p&lt;0.001" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">p</mi><mo id="S3.SS2.p2.1.m1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.cmml">&lt;</mo><mn id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><lt id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1"></lt><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">ùëù</ci><cn id="S3.SS2.p2.1.m1.1.1.3.cmml" type="float" xref="S3.SS2.p2.1.m1.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">p&lt;0.001</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">italic_p &lt; 0.001</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">The current state of the art model on the COCO dataset, ViTPose-huge, is also the best performing model on our dataset.
However, the relative distance to the second best, HRNet-w48, is only 2.4% for PCK@0.1.
For high accuracy requirements, e.g., PCK@0.05, the relative distance is only 0.6%, but the difference is still statistically significant with <math alttext="p=0.0312" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.1"><semantics id="S3.SS2.p3.1.m1.1a"><mrow id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml">p</mi><mo id="S3.SS2.p3.1.m1.1.1.1" xref="S3.SS2.p3.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3.cmml">0.0312</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><eq id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1"></eq><ci id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2">ùëù</ci><cn id="S3.SS2.p3.1.m1.1.1.3.cmml" type="float" xref="S3.SS2.p3.1.m1.1.1.3">0.0312</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">p=0.0312</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.1d">italic_p = 0.0312</annotation></semantics></math>.
OpenPose, although being an older model, still achieved better results than MediaPipe, which did never achieve a mean error of less than 10 pixels, not even for the clearly defined keypoints like eye and nose.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Influence of the view angle</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S3.F3" title="Figure 3 ‚Ä£ 3.3 Influence of the view angle ‚Ä£ 3 Results ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_tag">3</span></a> shows the pose estimation error as Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S3.F2.sf2" title="In Figure 2 ‚Ä£ 3.1 Variability in human labeling ‚Ä£ 3 Results ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_tag">2(b)</span></a>, but split into top and diagonal view.</p>
</div>
<figure class="ltx_figure" id="S3.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F3.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F3.1.g1" src="extracted/5907085/modeltrackingerror_c1.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F3.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F3.2.g1" src="extracted/5907085/modeltrackingerror_c2_same_ylim.png" width="598"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.4.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.5.2" style="font-size:90%;">Pose estimation errors for the generic pose estimation models, split by viewing angle.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">The errors in case of the diagonal view are higher than in the case of the top view. All top/diagonal differences are statistically significant with <math alttext="p&lt;0.001" class="ltx_Math" display="inline" id="S3.SS3.p2.1.m1.1"><semantics id="S3.SS3.p2.1.m1.1a"><mrow id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">p</mi><mo id="S3.SS3.p2.1.m1.1.1.1" xref="S3.SS3.p2.1.m1.1.1.1.cmml">&lt;</mo><mn id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><lt id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1"></lt><ci id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">ùëù</ci><cn id="S3.SS3.p2.1.m1.1.1.3.cmml" type="float" xref="S3.SS3.p2.1.m1.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">p&lt;0.001</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.1.m1.1d">italic_p &lt; 0.001</annotation></semantics></math>.
The most pronounced difference is between the hip position estimation errors.
In the top view, the difference is not only lower than in the diagonal view, but also lower as compared to the errors of the other keypoints.
This is consistent with the results for the difference between human annotators shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S3.F2.sf1" title="In Figure 2 ‚Ä£ 3.1 Variability in human labeling ‚Ä£ 3 Results ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_tag">2(a)</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">MediaPipe does not show any benefit from being designed for fitness applications.
In contrary, its performance decreases the most for the diagonal view.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Infant pose estimators</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">We evaluated the retrained pose estimation models in the same way we evaluated the generic ones.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S3.F2.sf3" title="In Figure 2 ‚Ä£ 3.1 Variability in human labeling ‚Ä£ 3 Results ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_tag">2(c)</span></a> shows the position estimation errors for the different models with the non-retrained ViTPose network for comparison.
The corresponding overall PCK results are displayed in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S3.T1" title="Table 1 ‚Ä£ 3.4 Infant pose estimators ‚Ä£ 3 Results ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.2.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S3.T1.3.2" style="font-size:90%;">PCK values for pose estimation with the different models for the complete dataset. Upper section: generic models, lower section: retrained models</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T1.4">
<tr class="ltx_tr" id="S3.T1.4.1">
<td class="ltx_td ltx_align_left" id="S3.T1.4.1.1" style="padding-top:1pt;padding-bottom:1pt;">Model</td>
<td class="ltx_td ltx_align_left" id="S3.T1.4.1.2" style="padding-top:1pt;padding-bottom:1pt;">PCK@0.1</td>
<td class="ltx_td ltx_align_left" id="S3.T1.4.1.3" style="padding-top:1pt;padding-bottom:1pt;">PCK@0.075</td>
<td class="ltx_td ltx_align_left" id="S3.T1.4.1.4" style="padding-top:1pt;padding-bottom:1pt;">PCK@0.05</td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.4.2.1" style="padding-top:1pt;padding-bottom:1pt;">ViTPose-huge</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.4.2.2" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.4.2.2.1">84.6</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.4.2.3" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.4.2.3.1">75.49</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.4.2.4" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.4.2.4.1">59.50</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.3">
<td class="ltx_td ltx_align_left" id="S3.T1.4.3.1" style="padding-top:1pt;padding-bottom:1pt;">HRNet-w48</td>
<td class="ltx_td ltx_align_left" id="S3.T1.4.3.2" style="padding-top:1pt;padding-bottom:1pt;">82.6</td>
<td class="ltx_td ltx_align_left" id="S3.T1.4.3.3" style="padding-top:1pt;padding-bottom:1pt;">73.92</td>
<td class="ltx_td ltx_align_left" id="S3.T1.4.3.4" style="padding-top:1pt;padding-bottom:1pt;">59.12</td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.4">
<td class="ltx_td ltx_align_left" id="S3.T1.4.4.1" style="padding-top:1pt;padding-bottom:1pt;">MediaPipe</td>
<td class="ltx_td ltx_align_left" id="S3.T1.4.4.2" style="padding-top:1pt;padding-bottom:1pt;">70.97</td>
<td class="ltx_td ltx_align_left" id="S3.T1.4.4.3" style="padding-top:1pt;padding-bottom:1pt;">58.91</td>
<td class="ltx_td ltx_align_left" id="S3.T1.4.4.4" style="padding-top:1pt;padding-bottom:1pt;">39.99</td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.5">
<td class="ltx_td ltx_align_left" id="S3.T1.4.5.1" style="padding-top:1pt;padding-bottom:1pt;">OpenPose</td>
<td class="ltx_td ltx_align_left" id="S3.T1.4.5.2" style="padding-top:1pt;padding-bottom:1pt;">79.48</td>
<td class="ltx_td ltx_align_left" id="S3.T1.4.5.3" style="padding-top:1pt;padding-bottom:1pt;">70.15</td>
<td class="ltx_td ltx_align_left" id="S3.T1.4.5.4" style="padding-top:1pt;padding-bottom:1pt;">53.74</td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.6">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T1.4.6.1" style="padding-top:1pt;padding-bottom:1pt;">Retrained ViTPose</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T1.4.6.2" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.4.6.2.1">93.89</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T1.4.6.3" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.4.6.3.1">89.82</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T1.4.6.4" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.4.6.4.1">79.64</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.7">
<td class="ltx_td ltx_align_left" id="S3.T1.4.7.1" style="padding-top:1pt;padding-bottom:1pt;">AggPose</td>
<td class="ltx_td ltx_align_left" id="S3.T1.4.7.2" style="padding-top:1pt;padding-bottom:1pt;">75.60</td>
<td class="ltx_td ltx_align_left" id="S3.T1.4.7.3" style="padding-top:1pt;padding-bottom:1pt;">67.02</td>
<td class="ltx_td ltx_align_left" id="S3.T1.4.7.4" style="padding-top:1pt;padding-bottom:1pt;">52.43</td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.8">
<td class="ltx_td ltx_align_left" id="S3.T1.4.8.1" style="padding-top:1pt;padding-bottom:1pt;">AGMA-HRNet48</td>
<td class="ltx_td ltx_align_left" id="S3.T1.4.8.2" style="padding-top:1pt;padding-bottom:1pt;">84.32</td>
<td class="ltx_td ltx_align_left" id="S3.T1.4.8.3" style="padding-top:1pt;padding-bottom:1pt;">75.12</td>
<td class="ltx_td ltx_align_left" id="S3.T1.4.8.4" style="padding-top:1pt;padding-bottom:1pt;">59.41</td>
</tr>
</table>
</figure>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.2">Retraining significantly improved the ViTPose model, with the PCK increasing by 20 percentage points in PCK@0.05 (<math alttext="p&lt;0.001" class="ltx_Math" display="inline" id="S3.SS4.p2.1.m1.1"><semantics id="S3.SS4.p2.1.m1.1a"><mrow id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml"><mi id="S3.SS4.p2.1.m1.1.1.2" xref="S3.SS4.p2.1.m1.1.1.2.cmml">p</mi><mo id="S3.SS4.p2.1.m1.1.1.1" xref="S3.SS4.p2.1.m1.1.1.1.cmml">&lt;</mo><mn id="S3.SS4.p2.1.m1.1.1.3" xref="S3.SS4.p2.1.m1.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><apply id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1"><lt id="S3.SS4.p2.1.m1.1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1.1"></lt><ci id="S3.SS4.p2.1.m1.1.1.2.cmml" xref="S3.SS4.p2.1.m1.1.1.2">ùëù</ci><cn id="S3.SS4.p2.1.m1.1.1.3.cmml" type="float" xref="S3.SS4.p2.1.m1.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">p&lt;0.001</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.1.m1.1d">italic_p &lt; 0.001</annotation></semantics></math>).
The mean difference to annotation for the hips is decreased by 61%.
All other keypoints also see significant improvements (all <math alttext="p&lt;0.001" class="ltx_Math" display="inline" id="S3.SS4.p2.2.m2.1"><semantics id="S3.SS4.p2.2.m2.1a"><mrow id="S3.SS4.p2.2.m2.1.1" xref="S3.SS4.p2.2.m2.1.1.cmml"><mi id="S3.SS4.p2.2.m2.1.1.2" xref="S3.SS4.p2.2.m2.1.1.2.cmml">p</mi><mo id="S3.SS4.p2.2.m2.1.1.1" xref="S3.SS4.p2.2.m2.1.1.1.cmml">&lt;</mo><mn id="S3.SS4.p2.2.m2.1.1.3" xref="S3.SS4.p2.2.m2.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.2.m2.1b"><apply id="S3.SS4.p2.2.m2.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1"><lt id="S3.SS4.p2.2.m2.1.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1.1"></lt><ci id="S3.SS4.p2.2.m2.1.1.2.cmml" xref="S3.SS4.p2.2.m2.1.1.2">ùëù</ci><cn id="S3.SS4.p2.2.m2.1.1.3.cmml" type="float" xref="S3.SS4.p2.2.m2.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.2.m2.1c">p&lt;0.001</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.2.m2.1d">italic_p &lt; 0.001</annotation></semantics></math>).</p>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.6">The other two infant pose estimators Aggpose <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S3.SS4.p3.6.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib33" title="">33</a></sup></cite> and AGMA-HRNet48 <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S3.SS4.p3.6.2.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib34" title="">34</a></sup></cite> exhibit different behaviour.
Both perform significantly worse than our retrained network (<math alttext="p&lt;0.001" class="ltx_Math" display="inline" id="S3.SS4.p3.1.m1.1"><semantics id="S3.SS4.p3.1.m1.1a"><mrow id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml"><mi id="S3.SS4.p3.1.m1.1.1.2" xref="S3.SS4.p3.1.m1.1.1.2.cmml">p</mi><mo id="S3.SS4.p3.1.m1.1.1.1" xref="S3.SS4.p3.1.m1.1.1.1.cmml">&lt;</mo><mn id="S3.SS4.p3.1.m1.1.1.3" xref="S3.SS4.p3.1.m1.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><apply id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1"><lt id="S3.SS4.p3.1.m1.1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1.1"></lt><ci id="S3.SS4.p3.1.m1.1.1.2.cmml" xref="S3.SS4.p3.1.m1.1.1.2">ùëù</ci><cn id="S3.SS4.p3.1.m1.1.1.3.cmml" type="float" xref="S3.SS4.p3.1.m1.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">p&lt;0.001</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.1.m1.1d">italic_p &lt; 0.001</annotation></semantics></math>), with AggPose even performing worse than the non-retrained ViTPose (<math alttext="p&lt;0.001" class="ltx_Math" display="inline" id="S3.SS4.p3.2.m2.1"><semantics id="S3.SS4.p3.2.m2.1a"><mrow id="S3.SS4.p3.2.m2.1.1" xref="S3.SS4.p3.2.m2.1.1.cmml"><mi id="S3.SS4.p3.2.m2.1.1.2" xref="S3.SS4.p3.2.m2.1.1.2.cmml">p</mi><mo id="S3.SS4.p3.2.m2.1.1.1" xref="S3.SS4.p3.2.m2.1.1.1.cmml">&lt;</mo><mn id="S3.SS4.p3.2.m2.1.1.3" xref="S3.SS4.p3.2.m2.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.2.m2.1b"><apply id="S3.SS4.p3.2.m2.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1"><lt id="S3.SS4.p3.2.m2.1.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1.1"></lt><ci id="S3.SS4.p3.2.m2.1.1.2.cmml" xref="S3.SS4.p3.2.m2.1.1.2">ùëù</ci><cn id="S3.SS4.p3.2.m2.1.1.3.cmml" type="float" xref="S3.SS4.p3.2.m2.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.2.m2.1c">p&lt;0.001</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.2.m2.1d">italic_p &lt; 0.001</annotation></semantics></math>).
The difference between ViTPose and AGMA-HRNet48 is not statistically significant (<math alttext="p=0.66" class="ltx_Math" display="inline" id="S3.SS4.p3.3.m3.1"><semantics id="S3.SS4.p3.3.m3.1a"><mrow id="S3.SS4.p3.3.m3.1.1" xref="S3.SS4.p3.3.m3.1.1.cmml"><mi id="S3.SS4.p3.3.m3.1.1.2" xref="S3.SS4.p3.3.m3.1.1.2.cmml">p</mi><mo id="S3.SS4.p3.3.m3.1.1.1" xref="S3.SS4.p3.3.m3.1.1.1.cmml">=</mo><mn id="S3.SS4.p3.3.m3.1.1.3" xref="S3.SS4.p3.3.m3.1.1.3.cmml">0.66</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.3.m3.1b"><apply id="S3.SS4.p3.3.m3.1.1.cmml" xref="S3.SS4.p3.3.m3.1.1"><eq id="S3.SS4.p3.3.m3.1.1.1.cmml" xref="S3.SS4.p3.3.m3.1.1.1"></eq><ci id="S3.SS4.p3.3.m3.1.1.2.cmml" xref="S3.SS4.p3.3.m3.1.1.2">ùëù</ci><cn id="S3.SS4.p3.3.m3.1.1.3.cmml" type="float" xref="S3.SS4.p3.3.m3.1.1.3">0.66</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.3.m3.1c">p=0.66</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.3.m3.1d">italic_p = 0.66</annotation></semantics></math> for <math alttext="d_{a}" class="ltx_Math" display="inline" id="S3.SS4.p3.4.m4.1"><semantics id="S3.SS4.p3.4.m4.1a"><msub id="S3.SS4.p3.4.m4.1.1" xref="S3.SS4.p3.4.m4.1.1.cmml"><mi id="S3.SS4.p3.4.m4.1.1.2" xref="S3.SS4.p3.4.m4.1.1.2.cmml">d</mi><mi id="S3.SS4.p3.4.m4.1.1.3" xref="S3.SS4.p3.4.m4.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.4.m4.1b"><apply id="S3.SS4.p3.4.m4.1.1.cmml" xref="S3.SS4.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.4.m4.1.1.1.cmml" xref="S3.SS4.p3.4.m4.1.1">subscript</csymbol><ci id="S3.SS4.p3.4.m4.1.1.2.cmml" xref="S3.SS4.p3.4.m4.1.1.2">ùëë</ci><ci id="S3.SS4.p3.4.m4.1.1.3.cmml" xref="S3.SS4.p3.4.m4.1.1.3">ùëé</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.4.m4.1c">d_{a}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.4.m4.1d">italic_d start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="p=0.59" class="ltx_Math" display="inline" id="S3.SS4.p3.5.m5.1"><semantics id="S3.SS4.p3.5.m5.1a"><mrow id="S3.SS4.p3.5.m5.1.1" xref="S3.SS4.p3.5.m5.1.1.cmml"><mi id="S3.SS4.p3.5.m5.1.1.2" xref="S3.SS4.p3.5.m5.1.1.2.cmml">p</mi><mo id="S3.SS4.p3.5.m5.1.1.1" xref="S3.SS4.p3.5.m5.1.1.1.cmml">=</mo><mn id="S3.SS4.p3.5.m5.1.1.3" xref="S3.SS4.p3.5.m5.1.1.3.cmml">0.59</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.5.m5.1b"><apply id="S3.SS4.p3.5.m5.1.1.cmml" xref="S3.SS4.p3.5.m5.1.1"><eq id="S3.SS4.p3.5.m5.1.1.1.cmml" xref="S3.SS4.p3.5.m5.1.1.1"></eq><ci id="S3.SS4.p3.5.m5.1.1.2.cmml" xref="S3.SS4.p3.5.m5.1.1.2">ùëù</ci><cn id="S3.SS4.p3.5.m5.1.1.3.cmml" type="float" xref="S3.SS4.p3.5.m5.1.1.3">0.59</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.5.m5.1c">p=0.59</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.5.m5.1d">italic_p = 0.59</annotation></semantics></math> for PCK@0.05).
Regarding the hips, AGMA-HRNet48 improves the accuracy over generic pose estimation, while AggPose is significantly worse (both <math alttext="p&lt;0.001" class="ltx_Math" display="inline" id="S3.SS4.p3.6.m6.1"><semantics id="S3.SS4.p3.6.m6.1a"><mrow id="S3.SS4.p3.6.m6.1.1" xref="S3.SS4.p3.6.m6.1.1.cmml"><mi id="S3.SS4.p3.6.m6.1.1.2" xref="S3.SS4.p3.6.m6.1.1.2.cmml">p</mi><mo id="S3.SS4.p3.6.m6.1.1.1" xref="S3.SS4.p3.6.m6.1.1.1.cmml">&lt;</mo><mn id="S3.SS4.p3.6.m6.1.1.3" xref="S3.SS4.p3.6.m6.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.6.m6.1b"><apply id="S3.SS4.p3.6.m6.1.1.cmml" xref="S3.SS4.p3.6.m6.1.1"><lt id="S3.SS4.p3.6.m6.1.1.1.cmml" xref="S3.SS4.p3.6.m6.1.1.1"></lt><ci id="S3.SS4.p3.6.m6.1.1.2.cmml" xref="S3.SS4.p3.6.m6.1.1.2">ùëù</ci><cn id="S3.SS4.p3.6.m6.1.1.3.cmml" type="float" xref="S3.SS4.p3.6.m6.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.6.m6.1c">p&lt;0.001</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.6.m6.1d">italic_p &lt; 0.001</annotation></semantics></math>).</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Retraining for individual view angles</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">As was shown in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S3.SS3" title="3.3 Influence of the view angle ‚Ä£ 3 Results ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_tag">3.3</span></a>, there is a significant difference between the diagonal and top views.
We retrained ViTPose models only on the individual viewing angles, to see if specializing on them helps to improve the performance.
The results are presented in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S3.F4" title="Figure 4 ‚Ä£ 3.5 Retraining for individual view angles ‚Ä£ 3 Results ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F4.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F4.1.g1" src="extracted/5907085/retrain_error_split_c1.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F4.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F4.2.g1" src="extracted/5907085/retrain_error_split_c2.png" width="598"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.6.2.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S3.F4.4.1" style="font-size:90%;">Mean difference from annotation (<math alttext="d_{a}" class="ltx_Math" display="inline" id="S3.F4.4.1.m1.1"><semantics id="S3.F4.4.1.m1.1b"><msub id="S3.F4.4.1.m1.1.1" xref="S3.F4.4.1.m1.1.1.cmml"><mi id="S3.F4.4.1.m1.1.1.2" xref="S3.F4.4.1.m1.1.1.2.cmml">d</mi><mi id="S3.F4.4.1.m1.1.1.3" xref="S3.F4.4.1.m1.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F4.4.1.m1.1c"><apply id="S3.F4.4.1.m1.1.1.cmml" xref="S3.F4.4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.F4.4.1.m1.1.1.1.cmml" xref="S3.F4.4.1.m1.1.1">subscript</csymbol><ci id="S3.F4.4.1.m1.1.1.2.cmml" xref="S3.F4.4.1.m1.1.1.2">ùëë</ci><ci id="S3.F4.4.1.m1.1.1.3.cmml" xref="S3.F4.4.1.m1.1.1.3">ùëé</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.4.1.m1.1d">d_{a}</annotation><annotation encoding="application/x-llamapun" id="S3.F4.4.1.m1.1e">italic_d start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math>) for retrained ViTPose models separately evaluated on diagonal or top view images. Different models have been trained on all, only diagonal view or only top view images, respectively.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS5.p2">
<p class="ltx_p" id="S3.SS5.p2.3">There is no significant difference between the model that was trained on both views versus the ones trained on individual views (<math alttext="p=0.25" class="ltx_Math" display="inline" id="S3.SS5.p2.1.m1.1"><semantics id="S3.SS5.p2.1.m1.1a"><mrow id="S3.SS5.p2.1.m1.1.1" xref="S3.SS5.p2.1.m1.1.1.cmml"><mi id="S3.SS5.p2.1.m1.1.1.2" xref="S3.SS5.p2.1.m1.1.1.2.cmml">p</mi><mo id="S3.SS5.p2.1.m1.1.1.1" xref="S3.SS5.p2.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS5.p2.1.m1.1.1.3" xref="S3.SS5.p2.1.m1.1.1.3.cmml">0.25</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.1.m1.1b"><apply id="S3.SS5.p2.1.m1.1.1.cmml" xref="S3.SS5.p2.1.m1.1.1"><eq id="S3.SS5.p2.1.m1.1.1.1.cmml" xref="S3.SS5.p2.1.m1.1.1.1"></eq><ci id="S3.SS5.p2.1.m1.1.1.2.cmml" xref="S3.SS5.p2.1.m1.1.1.2">ùëù</ci><cn id="S3.SS5.p2.1.m1.1.1.3.cmml" type="float" xref="S3.SS5.p2.1.m1.1.1.3">0.25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.1.m1.1c">p=0.25</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p2.1.m1.1d">italic_p = 0.25</annotation></semantics></math> for diagonal, <math alttext="p=0.54" class="ltx_Math" display="inline" id="S3.SS5.p2.2.m2.1"><semantics id="S3.SS5.p2.2.m2.1a"><mrow id="S3.SS5.p2.2.m2.1.1" xref="S3.SS5.p2.2.m2.1.1.cmml"><mi id="S3.SS5.p2.2.m2.1.1.2" xref="S3.SS5.p2.2.m2.1.1.2.cmml">p</mi><mo id="S3.SS5.p2.2.m2.1.1.1" xref="S3.SS5.p2.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS5.p2.2.m2.1.1.3" xref="S3.SS5.p2.2.m2.1.1.3.cmml">0.54</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.2.m2.1b"><apply id="S3.SS5.p2.2.m2.1.1.cmml" xref="S3.SS5.p2.2.m2.1.1"><eq id="S3.SS5.p2.2.m2.1.1.1.cmml" xref="S3.SS5.p2.2.m2.1.1.1"></eq><ci id="S3.SS5.p2.2.m2.1.1.2.cmml" xref="S3.SS5.p2.2.m2.1.1.2">ùëù</ci><cn id="S3.SS5.p2.2.m2.1.1.3.cmml" type="float" xref="S3.SS5.p2.2.m2.1.1.3">0.54</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.2.m2.1c">p=0.54</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p2.2.m2.1d">italic_p = 0.54</annotation></semantics></math> for top view), if evaluated against the respective views that were used in training.
The results for the respective views that were not used in training, however, are significantly worse (<math alttext="p&lt;0.001" class="ltx_Math" display="inline" id="S3.SS5.p2.3.m3.1"><semantics id="S3.SS5.p2.3.m3.1a"><mrow id="S3.SS5.p2.3.m3.1.1" xref="S3.SS5.p2.3.m3.1.1.cmml"><mi id="S3.SS5.p2.3.m3.1.1.2" xref="S3.SS5.p2.3.m3.1.1.2.cmml">p</mi><mo id="S3.SS5.p2.3.m3.1.1.1" xref="S3.SS5.p2.3.m3.1.1.1.cmml">&lt;</mo><mn id="S3.SS5.p2.3.m3.1.1.3" xref="S3.SS5.p2.3.m3.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.3.m3.1b"><apply id="S3.SS5.p2.3.m3.1.1.cmml" xref="S3.SS5.p2.3.m3.1.1"><lt id="S3.SS5.p2.3.m3.1.1.1.cmml" xref="S3.SS5.p2.3.m3.1.1.1"></lt><ci id="S3.SS5.p2.3.m3.1.1.2.cmml" xref="S3.SS5.p2.3.m3.1.1.2">ùëù</ci><cn id="S3.SS5.p2.3.m3.1.1.3.cmml" type="float" xref="S3.SS5.p2.3.m3.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.3.m3.1c">p&lt;0.001</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p2.3.m3.1d">italic_p &lt; 0.001</annotation></semantics></math>).
This means there is no gain in specializing models to certain viewing angles in our case.
However, training on a view that is different from the one used in inference leads to worse performance.</p>
</div>
<div class="ltx_para" id="S3.SS5.p3">
<p class="ltx_p" id="S3.SS5.p3.1">Regarding the general difference in performance between the viewing angles (without specialized models), we again observe worse results in the diagonal view.
Even with retraining, it is not possible to achieve the same accuracy of the diagonal view as of the top view.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Discussion</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">There is a wealth of tools for infant movement classification available that are all trained and tested on silo-datasets mostly using one camera view, and thus not directly comparable <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S4.p1.1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib30" title="">30</a></sup></cite>. We therefore used a newly designed multi-view dataset that allowed for a direct comparison of the available models.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Generic pose estimation</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The state of the art models ViTPose and HRNet show clear improvements compared to the older model OpenPose.
The gain in PCK@0.05 of 5.76 percentage points is small compared to the gain later achieved by retraining, but still statistically significant (<math alttext="p&lt;0.001" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><mrow id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mi id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">p</mi><mo id="S4.SS1.p1.1.m1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><lt id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1"></lt><ci id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">ùëù</ci><cn id="S4.SS1.p1.1.m1.1.1.3.cmml" type="float" xref="S4.SS1.p1.1.m1.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">p&lt;0.001</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">italic_p &lt; 0.001</annotation></semantics></math>).</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">In addition to the position accuracy, the reliability of keypoint detection also increases with newer models. A discussion is given in the Supplementary Material <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#A3" title="Appendix C Keypoint detection rates ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_tag">C</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Influence of the view angle</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">There is a significant influence of the viewing angle on pose estimation accuracy.
Every performance, human, generic and retrained models, is worse on the diagonal view than on the top view.
We see two main reasons for this.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">One reason for the reduced performance is a lack of training data from the corresponding view angle.
For everyday situations like those in the COCO dataset, the viewing angle of the diagonal camera corresponds to placing the camera very close to the floor.
This is uncommon.
We included the MediaPipe framework in our evaluation, because it is marketed towards fitness applications and for the use on mobile devices, a common application in GMA <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S4.SS2.p2.1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib48" title="">48</a></sup></cite>.
This suggested to us that it might be trained on images with uncommon view angles (e.g., mobile phone put on the ground to observe the fitness-workout).
The training dataset even contained 25000 frames from fitness exercises <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S4.SS2.p2.1.2.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib38" title="">38</a></sup></cite>.
However, the model performed worse compared to all others, with no beneficial effect on the diagonal angle.
It has to be said though, that MediaPipe is a small model (with respect to the number of parameters) designed for inference on mobile devices and as such not as potent as the other models.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">Another reason for the better results of the top view angle is occlusion of features.
The anatomy of the baby makes it easy to align the arms and legs with the sight line axis of the camera when lifting them, blocking the view on body keypoints like knee, wrist, shoulder or hip (see the second example from the left in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S2.F1" title="Figure 1 ‚Ä£ 2.1 Dataset ‚Ä£ 2 Materials and Methods ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_tag">1</span></a>b,c).
It might be preferable for computer vision to use a diagonal angle from the head end of the bed, instead of the foot end, to make the extremities lift into an orthogonal direction to the camera sight line.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">Even when retraining the model to the specific view angle, we could not achieve the same performance on the diagonal view as on the top view (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S3.F4" title="Figure 4 ‚Ä£ 3.5 Retraining for individual view angles ‚Ä£ 3 Results ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_tag">4</span></a>).
We suppose this is because the human variation in the training data was still higher in the diagonal view (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S3.F2.sf1" title="In Figure 2 ‚Ä£ 3.1 Variability in human labeling ‚Ä£ 3 Results ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_tag">2(a)</span></a>), because of occlusions.
An interesting follow-up would be to only indirectly label both top and diagonal view via reprojection from the side views (which might be hard to do, as they are only the supporting views for occlusion resistance) and see if the significant difference between the view angles persists.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Retraining</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">The comparison with the other infant pose estimators showed that the generalization capabilities of neural networks are overestimated in this field of study.
Our results show that the specialized infant pose estimators do not perform substantially better on our infant dataset than the generic ViTPose model.
AGMA-HRNet48 showed no significant overall difference to generic ViTPose, but improved on the hip keypoint. AggPose however, performed worse for all keypoints.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">While the AGMA-HRNet48 does improve pose estimation accuracy of the hip, other keypoints get worse, so there is no statistically significant difference between AGMA-HRNet48 and generic ViTPose (<math alttext="p=0.66" class="ltx_Math" display="inline" id="S4.SS3.p2.1.m1.1"><semantics id="S4.SS3.p2.1.m1.1a"><mrow id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mi id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml">p</mi><mo id="S4.SS3.p2.1.m1.1.1.1" xref="S4.SS3.p2.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS3.p2.1.m1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.3.cmml">0.66</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><eq id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1"></eq><ci id="S4.SS3.p2.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2">ùëù</ci><cn id="S4.SS3.p2.1.m1.1.1.3.cmml" type="float" xref="S4.SS3.p2.1.m1.1.1.3">0.66</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">p=0.66</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.1.m1.1d">italic_p = 0.66</annotation></semantics></math>).
It has to be noted that the training data of this model and our training data were both derived from a GMA setting.
They differ in the amount of visual clutter in the image (compare with Figure 3 in <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S4.SS3.p2.1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib34" title="">34</a></sup></cite>), but otherwise closely resemble each other.
The infant specific retraining improved the HRNet48 by 0.29 and 1.72 percentage points in PCK@0.05 and PCK@0.1 on our dataset, respectively, putting AGMA-HRNet48 on the same performance level as the generic ViTPose.
Note, that the AGMA-HRNet48 model is not the best performing one from its original paper <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S4.SS3.p2.1.2.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib34" title="">34</a></sup></cite>.
Their DarkPose32 model scored 2.23 and 0.55 percentage points higher than HRNet48 at PCK@0.05 and PCK@0.1, respectively.
Since the non-retrained DarkPose32 model performed worse than the non-retrained HRNet48 in <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S4.SS3.p2.1.3.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib34" title="">34</a></sup></cite>, we do not expect it to yield a significant improvement over the generic pose estimation on our data either.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">AggPose also uses Vision Transformers, like ViTPose, but is trained on a substantially larger dataset than our retrained model (20748 images, <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S4.SS3.p3.1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib33" title="">33</a></sup></cite>), still it performs even worse than generic ViTPose, which suggests it is also overfit and cannot sufficiently generalize to our (and potentially to other) infant dataset although the underlying dataset is also from a GMA related setting.
AggPose has been compared to other generic pose estimators by Yin et al. (see <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S4.SS3.p3.1.2.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib44" title="">44</a></sup></cite> Tables 1, 2 and 3), where it also performs worse than the generic pose estimators on different infant pose datasets.</p>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1">To date, there are no publicly available infant datasets suitable for GMA (mostly due to patient data privacy issues; <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S4.SS3.p4.1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib30" title="">30</a></sup></cite>), so we could not evaluate our model on data from a different setup.
This is probably further contributing to the general overfitting situation we identified, because there is no training set available that covers multiple GMA related setups. See Supplementary Material <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#A4" title="Appendix D SyRIP ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_tag">D</span></a> for a comment on a preliminary analysis with the SyRIP <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="S4.SS3.p4.1.2.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib32" title="">32</a></sup></cite> dataset.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In summary, our study suggests that pose estimation accuracy would benefit from the utility of the top view and the pose estimator being trained on the specific dataset. However, a generic pose estimator should be preferred over a specialized pose estimator trained on a different dataset, if retraining is not feasible. Whether potentially also movement classification accuracy would be improved with more accurate pose estimation remains a question, which needs to be addressed in future work. While the standard GMA method uses a diagonal view for assessment, clinical set-ups aiming to generate data for AI and computer vision approaches should in future consider top-down viewing angles for recording new data.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">We would like to thank the participating families in the SEE-study (Systemic Ethology and Developmental Science) and the additional annotator Felicia Seita; this study was conducted within the SFB 1528, project C03, DFG; clinical aspects are related to VW-IDENTIFIED and the DFG P456967546. We were supported by the FWF KLI811. We would like to thank all SEE (Systemic Ethology and Developmental Science, Marschik Lab) members who have contributed to this study.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">1</span>
<span class="ltx_bibblock">
Marschik Peter¬†B., Einspieler Christa, Sigafoos Jeff, Enzinger Christian,
B√∂lte Sven. The Interdisciplinary Quest for Behavioral Biomarkers
Pinpointing Developmental Disorders <span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">Developmental Neurorehabilitation.
</span> 2016;19:73‚Äì74.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">2</span>
<span class="ltx_bibblock">
Einspieler Christa, Bos Arend¬†F., Spittle Alicia¬†J., et al. The General
Movement Optimality Score-Revised (GMOS-R) with Socioeconomically
Stratified Percentile Ranks <span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">Journal of Clinical Medicine. </span>
2024;13:2260.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">3</span>
<span class="ltx_bibblock">
Novak Iona, Morgan Cathy, Adde Lars, et al. Early, Accurate Diagnosis and
Early Intervention in Cerebral Palsy: Advances in Diagnosis
and Treatment <span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">JAMA Pediatrics. </span> 2017;171:897.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">4</span>
<span class="ltx_bibblock">
Kirton Adam, Metzler Megan¬†J., Craig Brandon¬†T., et al. Perinatal Stroke:
Mapping and Modulating Developmental Plasticity <span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">Nature Reviews
Neurology. </span> 2021;17:415‚Äì432.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">5</span>
<span class="ltx_bibblock">
Prechtl H.¬†F.¬†R., Einspieler C., Cioni G., Bos A.¬†F., Ferrari F., Sontheimer
D.. An early marker for neurological deficits after perinatal brain lesions
<span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">The Lancet. </span> 1997;349:1361‚Äì1363.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">6</span>
<span class="ltx_bibblock">
Einspieler Christa, Sigafoos Jeff, Bartl-Pokorny Katrin¬†D, Landa Rebecca,
Marschik Peter¬†B, B√∂lte Sven. Highlighting the First 5 Months of Life:
General Movements in Infants Later Diagnosed with Autism Spectrum Disorder or
Rett Syndrome <span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">Research in Autism Spectrum Disorders. </span>
2014;8:286‚Äì291.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">7</span>
<span class="ltx_bibblock">
Einspieler C., Utsch F., Brasil P., et al. Association of infants exposed to
prenatal Zika virus infection with their clinical, neurologic, and
developmental status evaluated via the general movement assessment tool <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">JAMA Network Ppen. </span> 2019;2:e187235.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">8</span>
<span class="ltx_bibblock">
Einspieler C., Prechtl H.F.R., Bos Arend, Ferrari Fabrizio, Cioni Giovanni.
Prechtl‚Äôs method on the qualitative assessment of general movements in
preterm, term and young infants <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">Clin. Dev. Med. </span> 2004;167:1-91.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">9</span>
<span class="ltx_bibblock">
Marschik-Zhang Dajie, Wang Jun, Shen Xiushu, et al. Building Blocks for
Deep Phenotyping in Infancy: A Use Case Comparing Spontaneous
Neuromotor Functions in Prader-Willi Syndrome and Cerebral Palsy
<span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">Journal of Clinical Medicine. </span> 2023;12:784.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">10</span>
<span class="ltx_bibblock">
Einspieler C., Marschik P.¬†B., Prechtl H.¬†F.¬†R.. Human motor behavior:
Prenatal origin and early postnatal development <span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">Zeitschrift f√ºr
Psychologie/Journal of Psychology. </span> 2008;216:147‚Äì153.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">11</span>
<span class="ltx_bibblock">
Irshad M.¬†T., Nisar M.¬†A., Gouverneur P., Rapp M., Grzegorzek M.. AI
approaches towards Prechtl‚Äôs assessment of general movements: A
systematic literature review <span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">Sensors. </span> 2020;20:5321.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">12</span>
<span class="ltx_bibblock">
Silva N., Zhang D., Kulvicius T., et al. The future of general movement
assessment: The role of computer vision and machine learning ‚Äì A scoping
review <span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">Research in Developmental Disabilities. </span> 2021;110:103854.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">13</span>
<span class="ltx_bibblock">
Raghuram K., Orlandi S., Church P., et al. Automated movement recognition to
predict motor impairment in high-risk infants: a systematic review of
diagnostic test accuracy and meta-analysis <span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">Developmental Medicine &amp;
Child Neurology. </span> 2021;63:637‚Äì648.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">14</span>
<span class="ltx_bibblock">
Leo M., Bernava G.¬†M., Carcagn√¨ P., Distante C.. Video-based automatic
baby motion analysis for early neurological disorder diagnosis: State of
the art and future directions <span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">Sensors. </span> 2022;22:866.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">15</span>
<span class="ltx_bibblock">
Peng Z., Kommers D., Liang R.-H., et al. Continuous sensing and quantification
of body motion in infants: A systematic review <span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">Heliyon. </span> 2023;9.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">16</span>
<span class="ltx_bibblock">
Reinhart L., Bischops A.¬†C., Kerth J.-L., et al. Artificial intelligence in
child development monitoring: A systematic review on usage, outcomes and
acceptance <span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">Intelligence-based medicine. </span> 2024:100134.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">17</span>
<span class="ltx_bibblock">
Karch Dominik, Kang Keun-Sun, Wochner Katarzyna, et al. Kinematic Assessment of
Stereotypy in Spontaneous Movements in Infants <span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">Gait &amp; Posture. </span>
2012;36:307‚Äì311.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">18</span>
<span class="ltx_bibblock">
Philippi Heike, Karch Dominik, Kang Keun-Sun, et al. Computer-Based Analysis of
General Movements Reveals Stereotypies Predicting Cerebral Palsy <span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">Developmental Medicine &amp; Child Neurology. </span> 2014;56:960‚Äì967.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">19</span>
<span class="ltx_bibblock">
Adde L., Helbostad J.¬†L., Jensenius A.¬†R., Taraldsen G., St√∏en R.. Using
computer-based video analysis in the study of fidgety movements <span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">Early
Human Development. </span> 2009;85:541‚Äì547.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">20</span>
<span class="ltx_bibblock">
Adde L., Helbostad J.¬†L., Jensenius A.¬†R., Taraldsen G., Grunewaldt K.¬†H.,
St√∏en R.. Early prediction of cerebral palsy by computer-based video
analysis of general movements: A feasibility study <span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">Developmental
Medicine &amp; Child Neurology. </span> 2010;52:773‚Äì778.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">21</span>
<span class="ltx_bibblock">
Orlandi Silvia, Raghuram Kamini, Smith Corinna¬†R., et al. Detection of
Atypical and Typical Infant Movements Using Computer-based Video
Analysis in <span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">2018 40th Annual International Conference of the
IEEE Engineering in Medicine and Biology Society
(EMBC)</span>:3598‚Äì3601 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">22</span>
<span class="ltx_bibblock">
Mathis Alexander, Mamidanna Pranav, Cury Kevin¬†M., et al. DeepLabCut:
Markerless Pose Estimation of User-Defined Body Parts with Deep Learning
<span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">Nature Neuroscience. </span> 2018;21:1281‚Äì1289.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">23</span>
<span class="ltx_bibblock">
Cao Zhe, Hidalgo Gines, Simon Tomas, Wei Shih-En, Sheikh Yaser. OpenPose:
Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields <span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence. </span>
2021;43:172‚Äì186.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">24</span>
<span class="ltx_bibblock">
Groos Daniel, Ramampiaro Heri, Ihlen Espen¬†Af. EfficientPose: Scalable
Single-Person Pose Estimation <span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">Applied Intelligence. </span>
2021;51:2518‚Äì2533.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">25</span>
<span class="ltx_bibblock">
Xu¬†Yufei, Zhang Jing, Zhang Qiming, Tao Dacheng. ViTPose: Simple Vision
Transformer Baselines for Human Pose Estimation in <span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">Advances
in Neural Information Processing Systems</span> 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">26</span>
<span class="ltx_bibblock">
McCay Kevin¬†D., Ho¬†Edmond S.¬†L., Shum Hubert P.¬†H., Fehringer Gerhard, Marcroft
Claire, Embleton Nicholas¬†D.. Abnormal Infant Movements Classification With
Deep Learning on Pose-Based Features <span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">IEEE Access. </span>
2020;8:51582‚Äì51592.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">27</span>
<span class="ltx_bibblock">
Reich Simon, Zhang Dajie, Kulvicius Tomas, et al. Novel AI Driven Approach
to Classify Infant Motor Functions <span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">Scientific Reports. </span> 2021;11:9888.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">28</span>
<span class="ltx_bibblock">
Nguyen-Thai B., Le¬†V., Morgan C., Badawi N., Tran T., Venkatesh S.. A
spatio-temporal attention-based model for infant movement assessment from
videos <span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">IEEE Journal of Biomedical and Health Informatics. </span>
2021;25:3911‚Äì3920.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">29</span>
<span class="ltx_bibblock">
Groos Daniel, Adde Lars, Aubert Sindre, et al. Development and Validation
of a Deep Learning Method to Predict Cerebral Palsy From Spontaneous
Movements in Infants at High Risk <span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">JAMA Network Open. </span>
2022;5:e2221325.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">30</span>
<span class="ltx_bibblock">
Marschik Peter¬†B., Kulvicius Tomas, Fl√ºgge Sarah, et al. Open Video Data
Sharing in Developmental Science and Clinical Practice <span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">iScience. </span>
2023;26:106348.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">31</span>
<span class="ltx_bibblock">
Gao Q., Yao S., Tian Y., et al. Automating General Movements Assessment with
quantitative deep learning to facilitate early screening of cerebral palsy
<span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">Nature Communications. </span> 2023;14:8294.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">32</span>
<span class="ltx_bibblock">
Huang Xiaofei, Fu¬†Nihang, Liu Shuangjun, Ostadabbas Sarah. Invariant
Representation Learning for Infant Pose Estimation with Small
Data in <span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">2021 16th IEEE International Conference on Automatic
Face and Gesture Recognition (FG 2021)</span>(Jodhpur, India):1‚Äì8IEEE
2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">33</span>
<span class="ltx_bibblock">
Cao Xu, Li¬†Xiaoye, Ma¬†Liya, et al. AggPose: Deep Aggregation Vision
Transformer for Infant Pose Estimation in <span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">Proceedings of the
Thirty-First International Joint Conference on Artificial
Intelligence</span>:5045‚Äì5051 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">34</span>
<span class="ltx_bibblock">
Soualmi Ameur, Ducottet Christophe, Patural Hugues, Giraud Antoine, Alata
Olivier. A 3D Pose Estimation Framework for Preterm Infants Hospitalized
in the Neonatal Unit <span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">Multimedia Tools and Applications. </span> 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">35</span>
<span class="ltx_bibblock">
Wei Shih-En, Ramakrishna Varun, Kanade Takeo, Sheikh Yaser. Convolutional pose
machines in <span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">CVPR</span> 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">36</span>
<span class="ltx_bibblock">
Cao Zhe, Simon Tomas, Wei Shih-En, Sheikh Yaser. Realtime Multi-Person 2D
Pose Estimation using Part Affinity Fields in <span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">CVPR</span> 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">37</span>
<span class="ltx_bibblock">
Google . MediaPipe - BlazePose

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">38</span>
<span class="ltx_bibblock">
Bazarevsky Valentin, Grishchenko Ivan, Raveendran Karthik, Zhu Tyler, Zhang
Fan, Grundmann Matthias. BlazePose: On-device Real-time Body Pose
Tracking 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">39</span>
<span class="ltx_bibblock">
Wang Jingdong, Sun Ke, Cheng Tianheng, et al. Deep High-Resolution
Representation Learning for Visual Recognition <span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">TPAMI. </span> 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">40</span>
<span class="ltx_bibblock">
Sun Ke, Xiao Bin, Liu Dong, Wang Jingdong. Deep High-Resolution
Representation Learning for Human Pose Estimation in <span class="ltx_text ltx_font_italic" id="bib.bib40.1.1">CVPR</span>
2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">41</span>
<span class="ltx_bibblock">
Xu¬†Y., Zhang J., Zhang Q., Tao D.. ViTPose: Simple vision transformer
baselines for human pose estimation <span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">Advances in Neural Information
Processing Systems. </span> 2022;35:38571‚Äì38584.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">42</span>
<span class="ltx_bibblock">
Blaschek Astrid, Hesse Nikolas, Warken Birgit, et al. Quantitative Motion
Measurements Based on Markerless 3D Full-Body Tracking in Children
with SMA Highly Correlate with Standardized Motor Assessments <span class="ltx_text ltx_font_italic" id="bib.bib42.1.1">Journal of Neuromuscular Diseases. </span> 2022;9:121‚Äì128.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">43</span>
<span class="ltx_bibblock">
Zhou Zhuoran, Jiang Zhongyu, Chai Wenhao, Yang Cheng-Yen, Li¬†Lei, Hwang
Jenq-Neng. Efficient Domain Adaptation via Generative Prior for 3D
Infant Pose Estimation in <span class="ltx_text ltx_font_italic" id="bib.bib43.1.1">2024 IEEE/CVF Winter Conference on
Applications of Computer Vision Workshops (WACVW)</span>(Waikoloa, HI,
USA):51‚Äì59IEEE 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">44</span>
<span class="ltx_bibblock">
Yin Wang, Chen Linxi, Huang Xinrui, et al. A Self-Supervised Spatio-Temporal
Attention Network for Video-Based 3D Infant Pose Estimation <span class="ltx_text ltx_font_italic" id="bib.bib44.1.1">Medical
Image Analysis. </span> 2024;96:103208.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">45</span>
<span class="ltx_bibblock">
Marschik P.¬†B., Pokorny F.¬†B., Peharz R., et al. A novel way to measure and
predict development: A heuristic approach to facilitate the early detection
of neurodevelopmental disorders <span class="ltx_text ltx_font_italic" id="bib.bib45.1.1">Current Neurology and Neuroscience
Reports. </span> 2017;17:1‚Äì15.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">46</span>
<span class="ltx_bibblock">
Lin Tsung-Yi, Maire Michael, Belongie Serge, et al. Microsoft COCO:
Common Objects in Context in <span class="ltx_text ltx_font_italic" id="bib.bib46.1.1">Computer Vision ‚Äì ECCV
2014</span> (Fleet David, Pajdla Tomas, Schiele Bernt, Tuytelaars Tinne. ,
eds.);8693:740‚Äì755Cham: Springer International Publishing 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">47</span>
<span class="ltx_bibblock">
Chambers Claire, Seethapathi Nidhi, Saluja Rachit, et al. Computer Vision
to Automatically Assess Infant Neuromotor Risk <span class="ltx_text ltx_font_italic" id="bib.bib47.1.1">IEEE Transactions on
Neural Systems and Rehabilitation Engineering. </span> 2020;28:2431‚Äì2442.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">48</span>
<span class="ltx_bibblock">
Marschik Peter¬†B., Kwong Amanda K.¬†L., Silva Nelson, et al. Mobile
Solutions for Clinical Surveillance and Evaluation in
Infancy‚ÄîGeneral Movement Apps <span class="ltx_text ltx_font_italic" id="bib.bib48.1.1">Journal of Clinical Medicine. </span>
2023;12:3576.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">49</span>
<span class="ltx_bibblock">
Einspieler C., Peharz R., Marschik P.¬†B.. Fidgety movements ‚Äì tiny in
appearance, but huge in impact <span class="ltx_text ltx_font_italic" id="bib.bib49.1.1">Jornal de Pediatria. </span> 2016;92:64‚Äì70.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">50</span>
<span class="ltx_bibblock">
Einspieler C., Prayer D., Marschik P.¬†B.. Fetal movements: The origin of
human behaviour <span class="ltx_text ltx_font_italic" id="bib.bib50.1.1">Developmental Medicine &amp; Child Neurology. </span>
2021;63:1142‚Äì1148.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">51</span>
<span class="ltx_bibblock">
H√ºser Timo. JARVIS AnnotationTool 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">52</span>
<span class="ltx_bibblock">
Yang Yi, Ramanan Deva. Articulated Human Detection with Flexible
Mixtures of Parts <span class="ltx_text ltx_font_italic" id="bib.bib52.1.1">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND
MACHINE INTELLIGENCE. </span> 2013;35.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="Ax1">
<h2 class="ltx_title ltx_title_appendix">Supplementary Material</h2>
<div class="ltx_para" id="Ax1.p1">
<span class="ltx_ERROR undefined" id="Ax1.p1.1">\startcontents</span>
<p class="ltx_p" id="Ax1.p1.2">[sections]
<span class="ltx_ERROR undefined" id="Ax1.p1.2.1">\printcontents</span>[sections]l1</p>
</div>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Dataset</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">Here we give additional information about the creation of the dataset.</p>
</div>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Participants</h3>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">Participants included in this study were prospectively recruited from 2021 to 2023 in G√∂ttingen, Germany, and its close surroundings. The umbrella project aims to investigate cross-domain ontogenetic development in early infancy. To embrace the variability of the targeted dimension, i.e., spontaneous motor functions, 31 participants (17 female) were included. The gestational age at birth of the sample ranged from 34 to 42 weeks. At the time of data analysis, no participant was diagnosed with a neurological or neuromotor deficit, nor any neurodevelopmental impairment. All parents of the participating infants provided written informed consent to study participation and publication of depersonalized data and results. The study was approved by the Institutional Review Board (Ethics Commission) of the University Medical Center G√∂ttingen (19/2019).</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Infant movements recordings</h3>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">Standard laboratory recordings of infant movements at three timepoints (T1 ‚Äì T3) were included in the current study. These were extracted from data of the umbrella project, assessments, among others, infants‚Äô spontaneous movements in a standard laboratory-setting from the 4th to the 18th week of post-term age (PTA; corrected age; from here on, ages refer to the post-term age if not otherwise specified). For the current study, available laboratory recordings (n = 75) of the participants for the following timepoints are analyzed: T1: 28 ¬± 2 days, T2: 84 ¬± 2 days, and T3: 112 ¬± 2 days of PTA. As known, infants at 4 weeks (corresponding to T1) present a different spontaneous movement pattern than at 12 and 16 weeks (corresponding to T2 and T3; <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="A1.SS2.p1.1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib50" title="">50</a></sup></cite>). With standard recordings from T1 to T3, we intended to cover the distinct age-specific spontaneous movement repertoires of the young infants <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="A1.SS2.p1.1.2.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib10" title="">10</a></sup></cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Recording Setup</h3>
<div class="ltx_para" id="A1.SS3.p1">
<p class="ltx_p" id="A1.SS3.p1.1">In total, four cameras were mounted to record infants lying in a cot: one from the top down, one diagonally from the foot end and two from the sides.
This provides robustness against occlusions and 3D-reconstruction capabilities.
The angles were fixed and did not change across the entirety of the dataset.
The recordings were done at <math alttext="1600\times 1200" class="ltx_Math" display="inline" id="A1.SS3.p1.1.m1.1"><semantics id="A1.SS3.p1.1.m1.1a"><mrow id="A1.SS3.p1.1.m1.1.1" xref="A1.SS3.p1.1.m1.1.1.cmml"><mn id="A1.SS3.p1.1.m1.1.1.2" xref="A1.SS3.p1.1.m1.1.1.2.cmml">1600</mn><mo id="A1.SS3.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.SS3.p1.1.m1.1.1.1.cmml">√ó</mo><mn id="A1.SS3.p1.1.m1.1.1.3" xref="A1.SS3.p1.1.m1.1.1.3.cmml">1200</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.p1.1.m1.1b"><apply id="A1.SS3.p1.1.m1.1.1.cmml" xref="A1.SS3.p1.1.m1.1.1"><times id="A1.SS3.p1.1.m1.1.1.1.cmml" xref="A1.SS3.p1.1.m1.1.1.1"></times><cn id="A1.SS3.p1.1.m1.1.1.2.cmml" type="integer" xref="A1.SS3.p1.1.m1.1.1.2">1600</cn><cn id="A1.SS3.p1.1.m1.1.1.3.cmml" type="integer" xref="A1.SS3.p1.1.m1.1.1.3">1200</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.p1.1.m1.1c">1600\times 1200</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.p1.1.m1.1d">1600 √ó 1200</annotation></semantics></math> pixels with a frame rate of 60‚ÄâHz.
All cameras had global shutter and were triggered synchronously for optimal 3D-triangulation capabilities.</p>
</div>
<div class="ltx_para" id="A1.SS3.p2">
<p class="ltx_p" id="A1.SS3.p2.1">For this study, we did not yet do 3D-triangulation, but first determined the optimal pose estimation method on the 2D images alone.
The setup also records two side views that are not used in standard GMA settings and were not used in this study as well.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>Image selection and annotation</h3>
<div class="ltx_para" id="A1.SS4.p1">
<p class="ltx_p" id="A1.SS4.p1.1">The 75 recordings were split into two parts of 50 and 25 which were annotated by one research assistant each, using <span class="ltx_text ltx_font_bold" id="A1.SS4.p1.1.1">JARVIS Annotation Tool</span> <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="A1.SS4.p1.1.2.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib51" title="">51</a></sup></cite>.
Additionally, 10% of the dataset, regardless of the split, were double-annotated for human error and individual joint labeling difficulty estimation.</p>
</div>
<div class="ltx_para" id="A1.SS4.p2">
<p class="ltx_p" id="A1.SS4.p2.3">To select the video frames for the dataset from the available recordings, we first selected time ranges where the infants were in a behavioural state suitable for GMA.
From those, we extracted 30 frame-sets each, by <math alttext="k" class="ltx_Math" display="inline" id="A1.SS4.p2.1.m1.1"><semantics id="A1.SS4.p2.1.m1.1a"><mi id="A1.SS4.p2.1.m1.1.1" xref="A1.SS4.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A1.SS4.p2.1.m1.1b"><ci id="A1.SS4.p2.1.m1.1.1.cmml" xref="A1.SS4.p2.1.m1.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS4.p2.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="A1.SS4.p2.1.m1.1d">italic_k</annotation></semantics></math>-means-clustering (with <math alttext="k=30" class="ltx_Math" display="inline" id="A1.SS4.p2.2.m2.1"><semantics id="A1.SS4.p2.2.m2.1a"><mrow id="A1.SS4.p2.2.m2.1.1" xref="A1.SS4.p2.2.m2.1.1.cmml"><mi id="A1.SS4.p2.2.m2.1.1.2" xref="A1.SS4.p2.2.m2.1.1.2.cmml">k</mi><mo id="A1.SS4.p2.2.m2.1.1.1" xref="A1.SS4.p2.2.m2.1.1.1.cmml">=</mo><mn id="A1.SS4.p2.2.m2.1.1.3" xref="A1.SS4.p2.2.m2.1.1.3.cmml">30</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS4.p2.2.m2.1b"><apply id="A1.SS4.p2.2.m2.1.1.cmml" xref="A1.SS4.p2.2.m2.1.1"><eq id="A1.SS4.p2.2.m2.1.1.1.cmml" xref="A1.SS4.p2.2.m2.1.1.1"></eq><ci id="A1.SS4.p2.2.m2.1.1.2.cmml" xref="A1.SS4.p2.2.m2.1.1.2">ùëò</ci><cn id="A1.SS4.p2.2.m2.1.1.3.cmml" type="integer" xref="A1.SS4.p2.2.m2.1.1.3">30</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS4.p2.2.m2.1c">k=30</annotation><annotation encoding="application/x-llamapun" id="A1.SS4.p2.2.m2.1d">italic_k = 30</annotation></semantics></math>) on sub-sampled versions of the videos.
This yielded good coverage of the different body poses displayed by the infants (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S2.F1" title="Figure 1 ‚Ä£ 2.1 Dataset ‚Ä£ 2 Materials and Methods ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_tag">1</span></a> panels b) and c)).
In summary, we therefore had 30 frame-sets each obtained from 75 recordings, from which we took two different perspectives.
In total, this corresponds to <math alttext="30\cdot 75\cdot 2=4500" class="ltx_Math" display="inline" id="A1.SS4.p2.3.m3.1"><semantics id="A1.SS4.p2.3.m3.1a"><mrow id="A1.SS4.p2.3.m3.1.1" xref="A1.SS4.p2.3.m3.1.1.cmml"><mrow id="A1.SS4.p2.3.m3.1.1.2" xref="A1.SS4.p2.3.m3.1.1.2.cmml"><mn id="A1.SS4.p2.3.m3.1.1.2.2" xref="A1.SS4.p2.3.m3.1.1.2.2.cmml">30</mn><mo id="A1.SS4.p2.3.m3.1.1.2.1" lspace="0.222em" rspace="0.222em" xref="A1.SS4.p2.3.m3.1.1.2.1.cmml">‚ãÖ</mo><mn id="A1.SS4.p2.3.m3.1.1.2.3" xref="A1.SS4.p2.3.m3.1.1.2.3.cmml">75</mn><mo id="A1.SS4.p2.3.m3.1.1.2.1a" lspace="0.222em" rspace="0.222em" xref="A1.SS4.p2.3.m3.1.1.2.1.cmml">‚ãÖ</mo><mn id="A1.SS4.p2.3.m3.1.1.2.4" xref="A1.SS4.p2.3.m3.1.1.2.4.cmml">2</mn></mrow><mo id="A1.SS4.p2.3.m3.1.1.1" xref="A1.SS4.p2.3.m3.1.1.1.cmml">=</mo><mn id="A1.SS4.p2.3.m3.1.1.3" xref="A1.SS4.p2.3.m3.1.1.3.cmml">4500</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS4.p2.3.m3.1b"><apply id="A1.SS4.p2.3.m3.1.1.cmml" xref="A1.SS4.p2.3.m3.1.1"><eq id="A1.SS4.p2.3.m3.1.1.1.cmml" xref="A1.SS4.p2.3.m3.1.1.1"></eq><apply id="A1.SS4.p2.3.m3.1.1.2.cmml" xref="A1.SS4.p2.3.m3.1.1.2"><ci id="A1.SS4.p2.3.m3.1.1.2.1.cmml" xref="A1.SS4.p2.3.m3.1.1.2.1">‚ãÖ</ci><cn id="A1.SS4.p2.3.m3.1.1.2.2.cmml" type="integer" xref="A1.SS4.p2.3.m3.1.1.2.2">30</cn><cn id="A1.SS4.p2.3.m3.1.1.2.3.cmml" type="integer" xref="A1.SS4.p2.3.m3.1.1.2.3">75</cn><cn id="A1.SS4.p2.3.m3.1.1.2.4.cmml" type="integer" xref="A1.SS4.p2.3.m3.1.1.2.4">2</cn></apply><cn id="A1.SS4.p2.3.m3.1.1.3.cmml" type="integer" xref="A1.SS4.p2.3.m3.1.1.3">4500</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS4.p2.3.m3.1c">30\cdot 75\cdot 2=4500</annotation><annotation encoding="application/x-llamapun" id="A1.SS4.p2.3.m3.1d">30 ‚ãÖ 75 ‚ãÖ 2 = 4500</annotation></semantics></math> frames.</p>
</div>
<div class="ltx_para" id="A1.SS4.p3">
<p class="ltx_p" id="A1.SS4.p3.1">It is not always possible to see all of the keypoints in an image.
While there are some instances of eyes, shoulders or ankles being completely occluded, most of the occluded keypoints result from a physiological age-specific ATNR pattern; asymmetric tonic neck reflex.
Across the whole dataset, 11.2% of ears could not be seen, because the head was turned on its side.
Only counting infants of age smaller than 42 days, the percentage of missing ears was 17.2%, compared to 7.5% for older infants.</p>
</div>
<div class="ltx_para" id="A1.SS4.p4">
<p class="ltx_p" id="A1.SS4.p4.1">Some of the pose estimation frameworks we compared support more or less keypoints than COCO, but here we only evaluate the COCO-style keypoints.
The annotation software is able to use the 3D calibration information and reproject annotated points within the frame sets. So, although we only labeled the top and diagonal views, we supplied the side views to the annotators to deal better with occlusions through re-projection of annotations from the side views (if needed).</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Pose estimation</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">This section contains additional information on the analysis method and the exact procedures used in retraining.</p>
</div>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Retraining</h3>
<div class="ltx_para" id="A2.SS1.p1">
<p class="ltx_p" id="A2.SS1.p1.1">We manually split the dataset such that all images of each infant were contained in one fold only.
This ensures the networks do not get better test results by overfitting on specific infants, which are correlated across different recording dates.
Each of the five splits therefore contained 900 frames, made up from 450 different situations recorded from two different angles.
Because of the different numbers of recordings done for each infant, four splits contained images from six different infants and one split contained images from seven different infants.
For training, we randomly split 10% of the training set off as validation set, yielding a final training set size of 3240 frames in each fold.
For each fold, We then retrained the model for a maximum of 50 epochs, keeping the model with the highest PCK on the validation set for evaluation on the test set.</p>
</div>
</section>
<section class="ltx_subsection" id="A2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Quantification measures</h3>
<div class="ltx_para" id="A2.SS2.p1">
<p class="ltx_p" id="A2.SS2.p1.1">The difference between model prediction and the human annotation (<math alttext="d_{a}" class="ltx_Math" display="inline" id="A2.SS2.p1.1.m1.1"><semantics id="A2.SS2.p1.1.m1.1a"><msub id="A2.SS2.p1.1.m1.1.1" xref="A2.SS2.p1.1.m1.1.1.cmml"><mi id="A2.SS2.p1.1.m1.1.1.2" xref="A2.SS2.p1.1.m1.1.1.2.cmml">d</mi><mi id="A2.SS2.p1.1.m1.1.1.3" xref="A2.SS2.p1.1.m1.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="A2.SS2.p1.1.m1.1b"><apply id="A2.SS2.p1.1.m1.1.1.cmml" xref="A2.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A2.SS2.p1.1.m1.1.1.1.cmml" xref="A2.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="A2.SS2.p1.1.m1.1.1.2.cmml" xref="A2.SS2.p1.1.m1.1.1.2">ùëë</ci><ci id="A2.SS2.p1.1.m1.1.1.3.cmml" xref="A2.SS2.p1.1.m1.1.1.3">ùëé</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p1.1.m1.1c">d_{a}</annotation><annotation encoding="application/x-llamapun" id="A2.SS2.p1.1.m1.1d">italic_d start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math>) is the Euclidean distance between the predicted and human label in pixels:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A4.EGx1">
<tbody id="A2.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle d_{a}=\sqrt{(p_{x}-h_{x})^{2}+(p_{y}-h_{y})^{2}}," class="ltx_Math" display="inline" id="A2.E1.m1.3"><semantics id="A2.E1.m1.3a"><mrow id="A2.E1.m1.3.3.1" xref="A2.E1.m1.3.3.1.1.cmml"><mrow id="A2.E1.m1.3.3.1.1" xref="A2.E1.m1.3.3.1.1.cmml"><msub id="A2.E1.m1.3.3.1.1.2" xref="A2.E1.m1.3.3.1.1.2.cmml"><mi id="A2.E1.m1.3.3.1.1.2.2" xref="A2.E1.m1.3.3.1.1.2.2.cmml">d</mi><mi id="A2.E1.m1.3.3.1.1.2.3" xref="A2.E1.m1.3.3.1.1.2.3.cmml">a</mi></msub><mo id="A2.E1.m1.3.3.1.1.1" xref="A2.E1.m1.3.3.1.1.1.cmml">=</mo><msqrt id="A2.E1.m1.2.2" xref="A2.E1.m1.2.2.cmml"><mrow id="A2.E1.m1.2.2.2" xref="A2.E1.m1.2.2.2.cmml"><msup id="A2.E1.m1.1.1.1.1" xref="A2.E1.m1.1.1.1.1.cmml"><mrow id="A2.E1.m1.1.1.1.1.1.1" xref="A2.E1.m1.1.1.1.1.1.1.1.cmml"><mo id="A2.E1.m1.1.1.1.1.1.1.2" stretchy="false" xref="A2.E1.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="A2.E1.m1.1.1.1.1.1.1.1" xref="A2.E1.m1.1.1.1.1.1.1.1.cmml"><msub id="A2.E1.m1.1.1.1.1.1.1.1.2" xref="A2.E1.m1.1.1.1.1.1.1.1.2.cmml"><mi id="A2.E1.m1.1.1.1.1.1.1.1.2.2" xref="A2.E1.m1.1.1.1.1.1.1.1.2.2.cmml">p</mi><mi id="A2.E1.m1.1.1.1.1.1.1.1.2.3" xref="A2.E1.m1.1.1.1.1.1.1.1.2.3.cmml">x</mi></msub><mo id="A2.E1.m1.1.1.1.1.1.1.1.1" xref="A2.E1.m1.1.1.1.1.1.1.1.1.cmml">‚àí</mo><msub id="A2.E1.m1.1.1.1.1.1.1.1.3" xref="A2.E1.m1.1.1.1.1.1.1.1.3.cmml"><mi id="A2.E1.m1.1.1.1.1.1.1.1.3.2" xref="A2.E1.m1.1.1.1.1.1.1.1.3.2.cmml">h</mi><mi id="A2.E1.m1.1.1.1.1.1.1.1.3.3" xref="A2.E1.m1.1.1.1.1.1.1.1.3.3.cmml">x</mi></msub></mrow><mo id="A2.E1.m1.1.1.1.1.1.1.3" stretchy="false" xref="A2.E1.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mn id="A2.E1.m1.1.1.1.1.3" xref="A2.E1.m1.1.1.1.1.3.cmml">2</mn></msup><mo id="A2.E1.m1.2.2.2.3" xref="A2.E1.m1.2.2.2.3.cmml">+</mo><msup id="A2.E1.m1.2.2.2.2" xref="A2.E1.m1.2.2.2.2.cmml"><mrow id="A2.E1.m1.2.2.2.2.1.1" xref="A2.E1.m1.2.2.2.2.1.1.1.cmml"><mo id="A2.E1.m1.2.2.2.2.1.1.2" stretchy="false" xref="A2.E1.m1.2.2.2.2.1.1.1.cmml">(</mo><mrow id="A2.E1.m1.2.2.2.2.1.1.1" xref="A2.E1.m1.2.2.2.2.1.1.1.cmml"><msub id="A2.E1.m1.2.2.2.2.1.1.1.2" xref="A2.E1.m1.2.2.2.2.1.1.1.2.cmml"><mi id="A2.E1.m1.2.2.2.2.1.1.1.2.2" xref="A2.E1.m1.2.2.2.2.1.1.1.2.2.cmml">p</mi><mi id="A2.E1.m1.2.2.2.2.1.1.1.2.3" xref="A2.E1.m1.2.2.2.2.1.1.1.2.3.cmml">y</mi></msub><mo id="A2.E1.m1.2.2.2.2.1.1.1.1" xref="A2.E1.m1.2.2.2.2.1.1.1.1.cmml">‚àí</mo><msub id="A2.E1.m1.2.2.2.2.1.1.1.3" xref="A2.E1.m1.2.2.2.2.1.1.1.3.cmml"><mi id="A2.E1.m1.2.2.2.2.1.1.1.3.2" xref="A2.E1.m1.2.2.2.2.1.1.1.3.2.cmml">h</mi><mi id="A2.E1.m1.2.2.2.2.1.1.1.3.3" xref="A2.E1.m1.2.2.2.2.1.1.1.3.3.cmml">y</mi></msub></mrow><mo id="A2.E1.m1.2.2.2.2.1.1.3" stretchy="false" xref="A2.E1.m1.2.2.2.2.1.1.1.cmml">)</mo></mrow><mn id="A2.E1.m1.2.2.2.2.3" xref="A2.E1.m1.2.2.2.2.3.cmml">2</mn></msup></mrow></msqrt></mrow><mo id="A2.E1.m1.3.3.1.2" xref="A2.E1.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.E1.m1.3b"><apply id="A2.E1.m1.3.3.1.1.cmml" xref="A2.E1.m1.3.3.1"><eq id="A2.E1.m1.3.3.1.1.1.cmml" xref="A2.E1.m1.3.3.1.1.1"></eq><apply id="A2.E1.m1.3.3.1.1.2.cmml" xref="A2.E1.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="A2.E1.m1.3.3.1.1.2.1.cmml" xref="A2.E1.m1.3.3.1.1.2">subscript</csymbol><ci id="A2.E1.m1.3.3.1.1.2.2.cmml" xref="A2.E1.m1.3.3.1.1.2.2">ùëë</ci><ci id="A2.E1.m1.3.3.1.1.2.3.cmml" xref="A2.E1.m1.3.3.1.1.2.3">ùëé</ci></apply><apply id="A2.E1.m1.2.2.cmml" xref="A2.E1.m1.2.2"><root id="A2.E1.m1.2.2a.cmml" xref="A2.E1.m1.2.2"></root><apply id="A2.E1.m1.2.2.2.cmml" xref="A2.E1.m1.2.2.2"><plus id="A2.E1.m1.2.2.2.3.cmml" xref="A2.E1.m1.2.2.2.3"></plus><apply id="A2.E1.m1.1.1.1.1.cmml" xref="A2.E1.m1.1.1.1.1"><csymbol cd="ambiguous" id="A2.E1.m1.1.1.1.1.2.cmml" xref="A2.E1.m1.1.1.1.1">superscript</csymbol><apply id="A2.E1.m1.1.1.1.1.1.1.1.cmml" xref="A2.E1.m1.1.1.1.1.1.1"><minus id="A2.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="A2.E1.m1.1.1.1.1.1.1.1.1"></minus><apply id="A2.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="A2.E1.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="A2.E1.m1.1.1.1.1.1.1.1.2.1.cmml" xref="A2.E1.m1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="A2.E1.m1.1.1.1.1.1.1.1.2.2.cmml" xref="A2.E1.m1.1.1.1.1.1.1.1.2.2">ùëù</ci><ci id="A2.E1.m1.1.1.1.1.1.1.1.2.3.cmml" xref="A2.E1.m1.1.1.1.1.1.1.1.2.3">ùë•</ci></apply><apply id="A2.E1.m1.1.1.1.1.1.1.1.3.cmml" xref="A2.E1.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="A2.E1.m1.1.1.1.1.1.1.1.3.1.cmml" xref="A2.E1.m1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="A2.E1.m1.1.1.1.1.1.1.1.3.2.cmml" xref="A2.E1.m1.1.1.1.1.1.1.1.3.2">‚Ñé</ci><ci id="A2.E1.m1.1.1.1.1.1.1.1.3.3.cmml" xref="A2.E1.m1.1.1.1.1.1.1.1.3.3">ùë•</ci></apply></apply><cn id="A2.E1.m1.1.1.1.1.3.cmml" type="integer" xref="A2.E1.m1.1.1.1.1.3">2</cn></apply><apply id="A2.E1.m1.2.2.2.2.cmml" xref="A2.E1.m1.2.2.2.2"><csymbol cd="ambiguous" id="A2.E1.m1.2.2.2.2.2.cmml" xref="A2.E1.m1.2.2.2.2">superscript</csymbol><apply id="A2.E1.m1.2.2.2.2.1.1.1.cmml" xref="A2.E1.m1.2.2.2.2.1.1"><minus id="A2.E1.m1.2.2.2.2.1.1.1.1.cmml" xref="A2.E1.m1.2.2.2.2.1.1.1.1"></minus><apply id="A2.E1.m1.2.2.2.2.1.1.1.2.cmml" xref="A2.E1.m1.2.2.2.2.1.1.1.2"><csymbol cd="ambiguous" id="A2.E1.m1.2.2.2.2.1.1.1.2.1.cmml" xref="A2.E1.m1.2.2.2.2.1.1.1.2">subscript</csymbol><ci id="A2.E1.m1.2.2.2.2.1.1.1.2.2.cmml" xref="A2.E1.m1.2.2.2.2.1.1.1.2.2">ùëù</ci><ci id="A2.E1.m1.2.2.2.2.1.1.1.2.3.cmml" xref="A2.E1.m1.2.2.2.2.1.1.1.2.3">ùë¶</ci></apply><apply id="A2.E1.m1.2.2.2.2.1.1.1.3.cmml" xref="A2.E1.m1.2.2.2.2.1.1.1.3"><csymbol cd="ambiguous" id="A2.E1.m1.2.2.2.2.1.1.1.3.1.cmml" xref="A2.E1.m1.2.2.2.2.1.1.1.3">subscript</csymbol><ci id="A2.E1.m1.2.2.2.2.1.1.1.3.2.cmml" xref="A2.E1.m1.2.2.2.2.1.1.1.3.2">‚Ñé</ci><ci id="A2.E1.m1.2.2.2.2.1.1.1.3.3.cmml" xref="A2.E1.m1.2.2.2.2.1.1.1.3.3">ùë¶</ci></apply></apply><cn id="A2.E1.m1.2.2.2.2.3.cmml" type="integer" xref="A2.E1.m1.2.2.2.2.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.E1.m1.3c">\displaystyle d_{a}=\sqrt{(p_{x}-h_{x})^{2}+(p_{y}-h_{y})^{2}},</annotation><annotation encoding="application/x-llamapun" id="A2.E1.m1.3d">italic_d start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT = square-root start_ARG ( italic_p start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT - italic_h start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + ( italic_p start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT - italic_h start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="A2.SS2.p1.3">where <math alttext="p_{x/y}" class="ltx_Math" display="inline" id="A2.SS2.p1.2.m1.1"><semantics id="A2.SS2.p1.2.m1.1a"><msub id="A2.SS2.p1.2.m1.1.1" xref="A2.SS2.p1.2.m1.1.1.cmml"><mi id="A2.SS2.p1.2.m1.1.1.2" xref="A2.SS2.p1.2.m1.1.1.2.cmml">p</mi><mrow id="A2.SS2.p1.2.m1.1.1.3" xref="A2.SS2.p1.2.m1.1.1.3.cmml"><mi id="A2.SS2.p1.2.m1.1.1.3.2" xref="A2.SS2.p1.2.m1.1.1.3.2.cmml">x</mi><mo id="A2.SS2.p1.2.m1.1.1.3.1" xref="A2.SS2.p1.2.m1.1.1.3.1.cmml">/</mo><mi id="A2.SS2.p1.2.m1.1.1.3.3" xref="A2.SS2.p1.2.m1.1.1.3.3.cmml">y</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A2.SS2.p1.2.m1.1b"><apply id="A2.SS2.p1.2.m1.1.1.cmml" xref="A2.SS2.p1.2.m1.1.1"><csymbol cd="ambiguous" id="A2.SS2.p1.2.m1.1.1.1.cmml" xref="A2.SS2.p1.2.m1.1.1">subscript</csymbol><ci id="A2.SS2.p1.2.m1.1.1.2.cmml" xref="A2.SS2.p1.2.m1.1.1.2">ùëù</ci><apply id="A2.SS2.p1.2.m1.1.1.3.cmml" xref="A2.SS2.p1.2.m1.1.1.3"><divide id="A2.SS2.p1.2.m1.1.1.3.1.cmml" xref="A2.SS2.p1.2.m1.1.1.3.1"></divide><ci id="A2.SS2.p1.2.m1.1.1.3.2.cmml" xref="A2.SS2.p1.2.m1.1.1.3.2">ùë•</ci><ci id="A2.SS2.p1.2.m1.1.1.3.3.cmml" xref="A2.SS2.p1.2.m1.1.1.3.3">ùë¶</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p1.2.m1.1c">p_{x/y}</annotation><annotation encoding="application/x-llamapun" id="A2.SS2.p1.2.m1.1d">italic_p start_POSTSUBSCRIPT italic_x / italic_y end_POSTSUBSCRIPT</annotation></semantics></math> are the network predictions and <math alttext="h_{x/y}" class="ltx_Math" display="inline" id="A2.SS2.p1.3.m2.1"><semantics id="A2.SS2.p1.3.m2.1a"><msub id="A2.SS2.p1.3.m2.1.1" xref="A2.SS2.p1.3.m2.1.1.cmml"><mi id="A2.SS2.p1.3.m2.1.1.2" xref="A2.SS2.p1.3.m2.1.1.2.cmml">h</mi><mrow id="A2.SS2.p1.3.m2.1.1.3" xref="A2.SS2.p1.3.m2.1.1.3.cmml"><mi id="A2.SS2.p1.3.m2.1.1.3.2" xref="A2.SS2.p1.3.m2.1.1.3.2.cmml">x</mi><mo id="A2.SS2.p1.3.m2.1.1.3.1" xref="A2.SS2.p1.3.m2.1.1.3.1.cmml">/</mo><mi id="A2.SS2.p1.3.m2.1.1.3.3" xref="A2.SS2.p1.3.m2.1.1.3.3.cmml">y</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A2.SS2.p1.3.m2.1b"><apply id="A2.SS2.p1.3.m2.1.1.cmml" xref="A2.SS2.p1.3.m2.1.1"><csymbol cd="ambiguous" id="A2.SS2.p1.3.m2.1.1.1.cmml" xref="A2.SS2.p1.3.m2.1.1">subscript</csymbol><ci id="A2.SS2.p1.3.m2.1.1.2.cmml" xref="A2.SS2.p1.3.m2.1.1.2">‚Ñé</ci><apply id="A2.SS2.p1.3.m2.1.1.3.cmml" xref="A2.SS2.p1.3.m2.1.1.3"><divide id="A2.SS2.p1.3.m2.1.1.3.1.cmml" xref="A2.SS2.p1.3.m2.1.1.3.1"></divide><ci id="A2.SS2.p1.3.m2.1.1.3.2.cmml" xref="A2.SS2.p1.3.m2.1.1.3.2">ùë•</ci><ci id="A2.SS2.p1.3.m2.1.1.3.3.cmml" xref="A2.SS2.p1.3.m2.1.1.3.3">ùë¶</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p1.3.m2.1c">h_{x/y}</annotation><annotation encoding="application/x-llamapun" id="A2.SS2.p1.3.m2.1d">italic_h start_POSTSUBSCRIPT italic_x / italic_y end_POSTSUBSCRIPT</annotation></semantics></math> the human labels for a keypoint.</p>
</div>
<div class="ltx_para" id="A2.SS2.p2">
<p class="ltx_p" id="A2.SS2.p2.1">For PCK <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="A2.SS2.p2.1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib52" title="">52</a></sup></cite>, all keypoint predictions are classified as
either correct or incorrect, based on their distance to the label (as given above).
The PCK score is then defined as the fraction of correct keypoints out of all keypoints.
The distance threshold to be considered correct is defined as a percentage of the apparent torso length of the infant in the frame, measured as the distance between the left shoulder and hip.
In this work we considered the PCK at 5%, 7.5% and 10% of the torso size, denoted by PCK@0.05, PCK@0.75 and PCK@0.1, respectively.</p>
</div>
<div class="ltx_para" id="A2.SS2.p3">
<p class="ltx_p" id="A2.SS2.p3.1">It is not possible to transform any of the quantification measures directly into real-world distances without using 3D triangulation, which was not done in this work, or assuming the points are on the same plane, which is untrue because the infants frequently lift their extremities.
However, one pixel on the plane of the bed, <span class="ltx_text ltx_font_italic" id="A2.SS2.p3.1.1">always</span> corresponds to a distance <math alttext="\leq 0.8" class="ltx_Math" display="inline" id="A2.SS2.p3.1.m1.1"><semantics id="A2.SS2.p3.1.m1.1a"><mrow id="A2.SS2.p3.1.m1.1.1" xref="A2.SS2.p3.1.m1.1.1.cmml"><mi id="A2.SS2.p3.1.m1.1.1.2" xref="A2.SS2.p3.1.m1.1.1.2.cmml"></mi><mo id="A2.SS2.p3.1.m1.1.1.1" xref="A2.SS2.p3.1.m1.1.1.1.cmml">‚â§</mo><mn id="A2.SS2.p3.1.m1.1.1.3" xref="A2.SS2.p3.1.m1.1.1.3.cmml">0.8</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.SS2.p3.1.m1.1b"><apply id="A2.SS2.p3.1.m1.1.1.cmml" xref="A2.SS2.p3.1.m1.1.1"><leq id="A2.SS2.p3.1.m1.1.1.1.cmml" xref="A2.SS2.p3.1.m1.1.1.1"></leq><csymbol cd="latexml" id="A2.SS2.p3.1.m1.1.1.2.cmml" xref="A2.SS2.p3.1.m1.1.1.2">absent</csymbol><cn id="A2.SS2.p3.1.m1.1.1.3.cmml" type="float" xref="A2.SS2.p3.1.m1.1.1.3">0.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p3.1.m1.1c">\leq 0.8</annotation><annotation encoding="application/x-llamapun" id="A2.SS2.p3.1.m1.1d">‚â§ 0.8</annotation></semantics></math>‚Äâmm.
Therefore, 80% of the difference to annotation is an upper bound to the real-world distance in mm.
Considering the mean torso length of 306 pixels, PCK@0.1 yields the percentage of points that were detected within approximately 2.5cm of the ground truth (given 0.8‚Äâmm as the upper bound).</p>
</div>
<div class="ltx_para" id="A2.SS2.p4">
<p class="ltx_p" id="A2.SS2.p4.1">Although both metrics cannot be directly used for real-world-distances, they are fully appropriate for relative comparisons between the pose estimation frameworks.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Keypoint detection rates</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.3">This section contains an additional analysis of the models own estimation of reliability.
All models also score the predictions with a certainty value <math alttext="c" class="ltx_Math" display="inline" id="A3.p1.1.m1.1"><semantics id="A3.p1.1.m1.1a"><mi id="A3.p1.1.m1.1.1" xref="A3.p1.1.m1.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="A3.p1.1.m1.1b"><ci id="A3.p1.1.m1.1.1.cmml" xref="A3.p1.1.m1.1.1">ùëê</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.1.m1.1c">c</annotation><annotation encoding="application/x-llamapun" id="A3.p1.1.m1.1d">italic_c</annotation></semantics></math> between 0 and 1, e.g., how confident the model is in its prediction.
This value in itself is not normalized between the models.
To enable comparison, we do not look at these certainty values, but at the level of missing keypoints.
To obtain it, we thresholded the certainty values, for thresholds <math alttext="t" class="ltx_Math" display="inline" id="A3.p1.2.m2.1"><semantics id="A3.p1.2.m2.1a"><mi id="A3.p1.2.m2.1.1" xref="A3.p1.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="A3.p1.2.m2.1b"><ci id="A3.p1.2.m2.1.1.cmml" xref="A3.p1.2.m2.1.1">ùë°</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.2.m2.1c">t</annotation><annotation encoding="application/x-llamapun" id="A3.p1.2.m2.1d">italic_t</annotation></semantics></math> between 0 and 1, and then calculated the respective ratio of missing predictions <math alttext="m" class="ltx_Math" display="inline" id="A3.p1.3.m3.1"><semantics id="A3.p1.3.m3.1a"><mi id="A3.p1.3.m3.1.1" xref="A3.p1.3.m3.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="A3.p1.3.m3.1b"><ci id="A3.p1.3.m3.1.1.cmml" xref="A3.p1.3.m3.1.1">ùëö</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.3.m3.1c">m</annotation><annotation encoding="application/x-llamapun" id="A3.p1.3.m3.1d">italic_m</annotation></semantics></math> as</p>
<table class="ltx_equation ltx_eqn_table" id="A3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="m=\frac{1}{N}\sum\limits_{i=1}^{N}H(c-t)." class="ltx_Math" display="block" id="A3.E2.m1.1"><semantics id="A3.E2.m1.1a"><mrow id="A3.E2.m1.1.1.1" xref="A3.E2.m1.1.1.1.1.cmml"><mrow id="A3.E2.m1.1.1.1.1" xref="A3.E2.m1.1.1.1.1.cmml"><mi id="A3.E2.m1.1.1.1.1.3" xref="A3.E2.m1.1.1.1.1.3.cmml">m</mi><mo id="A3.E2.m1.1.1.1.1.2" xref="A3.E2.m1.1.1.1.1.2.cmml">=</mo><mrow id="A3.E2.m1.1.1.1.1.1" xref="A3.E2.m1.1.1.1.1.1.cmml"><mfrac id="A3.E2.m1.1.1.1.1.1.3" xref="A3.E2.m1.1.1.1.1.1.3.cmml"><mn id="A3.E2.m1.1.1.1.1.1.3.2" xref="A3.E2.m1.1.1.1.1.1.3.2.cmml">1</mn><mi id="A3.E2.m1.1.1.1.1.1.3.3" xref="A3.E2.m1.1.1.1.1.1.3.3.cmml">N</mi></mfrac><mo id="A3.E2.m1.1.1.1.1.1.2" xref="A3.E2.m1.1.1.1.1.1.2.cmml">‚Å¢</mo><mrow id="A3.E2.m1.1.1.1.1.1.1" xref="A3.E2.m1.1.1.1.1.1.1.cmml"><munderover id="A3.E2.m1.1.1.1.1.1.1.2" xref="A3.E2.m1.1.1.1.1.1.1.2.cmml"><mo id="A3.E2.m1.1.1.1.1.1.1.2.2.2" movablelimits="false" xref="A3.E2.m1.1.1.1.1.1.1.2.2.2.cmml">‚àë</mo><mrow id="A3.E2.m1.1.1.1.1.1.1.2.2.3" xref="A3.E2.m1.1.1.1.1.1.1.2.2.3.cmml"><mi id="A3.E2.m1.1.1.1.1.1.1.2.2.3.2" xref="A3.E2.m1.1.1.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="A3.E2.m1.1.1.1.1.1.1.2.2.3.1" xref="A3.E2.m1.1.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="A3.E2.m1.1.1.1.1.1.1.2.2.3.3" xref="A3.E2.m1.1.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="A3.E2.m1.1.1.1.1.1.1.2.3" xref="A3.E2.m1.1.1.1.1.1.1.2.3.cmml">N</mi></munderover><mrow id="A3.E2.m1.1.1.1.1.1.1.1" xref="A3.E2.m1.1.1.1.1.1.1.1.cmml"><mi id="A3.E2.m1.1.1.1.1.1.1.1.3" xref="A3.E2.m1.1.1.1.1.1.1.1.3.cmml">H</mi><mo id="A3.E2.m1.1.1.1.1.1.1.1.2" xref="A3.E2.m1.1.1.1.1.1.1.1.2.cmml">‚Å¢</mo><mrow id="A3.E2.m1.1.1.1.1.1.1.1.1.1" xref="A3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="A3.E2.m1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="A3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="A3.E2.m1.1.1.1.1.1.1.1.1.1.1" xref="A3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="A3.E2.m1.1.1.1.1.1.1.1.1.1.1.2" xref="A3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">c</mi><mo id="A3.E2.m1.1.1.1.1.1.1.1.1.1.1.1" xref="A3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">‚àí</mo><mi id="A3.E2.m1.1.1.1.1.1.1.1.1.1.1.3" xref="A3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.cmml">t</mi></mrow><mo id="A3.E2.m1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="A3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="A3.E2.m1.1.1.1.2" lspace="0em" xref="A3.E2.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="A3.E2.m1.1b"><apply id="A3.E2.m1.1.1.1.1.cmml" xref="A3.E2.m1.1.1.1"><eq id="A3.E2.m1.1.1.1.1.2.cmml" xref="A3.E2.m1.1.1.1.1.2"></eq><ci id="A3.E2.m1.1.1.1.1.3.cmml" xref="A3.E2.m1.1.1.1.1.3">ùëö</ci><apply id="A3.E2.m1.1.1.1.1.1.cmml" xref="A3.E2.m1.1.1.1.1.1"><times id="A3.E2.m1.1.1.1.1.1.2.cmml" xref="A3.E2.m1.1.1.1.1.1.2"></times><apply id="A3.E2.m1.1.1.1.1.1.3.cmml" xref="A3.E2.m1.1.1.1.1.1.3"><divide id="A3.E2.m1.1.1.1.1.1.3.1.cmml" xref="A3.E2.m1.1.1.1.1.1.3"></divide><cn id="A3.E2.m1.1.1.1.1.1.3.2.cmml" type="integer" xref="A3.E2.m1.1.1.1.1.1.3.2">1</cn><ci id="A3.E2.m1.1.1.1.1.1.3.3.cmml" xref="A3.E2.m1.1.1.1.1.1.3.3">ùëÅ</ci></apply><apply id="A3.E2.m1.1.1.1.1.1.1.cmml" xref="A3.E2.m1.1.1.1.1.1.1"><apply id="A3.E2.m1.1.1.1.1.1.1.2.cmml" xref="A3.E2.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="A3.E2.m1.1.1.1.1.1.1.2.1.cmml" xref="A3.E2.m1.1.1.1.1.1.1.2">superscript</csymbol><apply id="A3.E2.m1.1.1.1.1.1.1.2.2.cmml" xref="A3.E2.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="A3.E2.m1.1.1.1.1.1.1.2.2.1.cmml" xref="A3.E2.m1.1.1.1.1.1.1.2">subscript</csymbol><sum id="A3.E2.m1.1.1.1.1.1.1.2.2.2.cmml" xref="A3.E2.m1.1.1.1.1.1.1.2.2.2"></sum><apply id="A3.E2.m1.1.1.1.1.1.1.2.2.3.cmml" xref="A3.E2.m1.1.1.1.1.1.1.2.2.3"><eq id="A3.E2.m1.1.1.1.1.1.1.2.2.3.1.cmml" xref="A3.E2.m1.1.1.1.1.1.1.2.2.3.1"></eq><ci id="A3.E2.m1.1.1.1.1.1.1.2.2.3.2.cmml" xref="A3.E2.m1.1.1.1.1.1.1.2.2.3.2">ùëñ</ci><cn id="A3.E2.m1.1.1.1.1.1.1.2.2.3.3.cmml" type="integer" xref="A3.E2.m1.1.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="A3.E2.m1.1.1.1.1.1.1.2.3.cmml" xref="A3.E2.m1.1.1.1.1.1.1.2.3">ùëÅ</ci></apply><apply id="A3.E2.m1.1.1.1.1.1.1.1.cmml" xref="A3.E2.m1.1.1.1.1.1.1.1"><times id="A3.E2.m1.1.1.1.1.1.1.1.2.cmml" xref="A3.E2.m1.1.1.1.1.1.1.1.2"></times><ci id="A3.E2.m1.1.1.1.1.1.1.1.3.cmml" xref="A3.E2.m1.1.1.1.1.1.1.1.3">ùêª</ci><apply id="A3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="A3.E2.m1.1.1.1.1.1.1.1.1.1"><minus id="A3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="A3.E2.m1.1.1.1.1.1.1.1.1.1.1.1"></minus><ci id="A3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="A3.E2.m1.1.1.1.1.1.1.1.1.1.1.2">ùëê</ci><ci id="A3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="A3.E2.m1.1.1.1.1.1.1.1.1.1.1.3">ùë°</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.E2.m1.1c">m=\frac{1}{N}\sum\limits_{i=1}^{N}H(c-t).</annotation><annotation encoding="application/x-llamapun" id="A3.E2.m1.1d">italic_m = divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_H ( italic_c - italic_t ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="A3.p1.8">There, <math alttext="H" class="ltx_Math" display="inline" id="A3.p1.4.m1.1"><semantics id="A3.p1.4.m1.1a"><mi id="A3.p1.4.m1.1.1" xref="A3.p1.4.m1.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="A3.p1.4.m1.1b"><ci id="A3.p1.4.m1.1.1.cmml" xref="A3.p1.4.m1.1.1">ùêª</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.4.m1.1c">H</annotation><annotation encoding="application/x-llamapun" id="A3.p1.4.m1.1d">italic_H</annotation></semantics></math> is the Heaviside function and <math alttext="N" class="ltx_Math" display="inline" id="A3.p1.5.m2.1"><semantics id="A3.p1.5.m2.1a"><mi id="A3.p1.5.m2.1.1" xref="A3.p1.5.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="A3.p1.5.m2.1b"><ci id="A3.p1.5.m2.1.1.cmml" xref="A3.p1.5.m2.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.5.m2.1c">N</annotation><annotation encoding="application/x-llamapun" id="A3.p1.5.m2.1d">italic_N</annotation></semantics></math> the total number of annotated keypoints in the dataset.
This effectively normalizes the score to the percentage of points deemed correct by the model itself.
We then plotted <math alttext="d_{a}" class="ltx_Math" display="inline" id="A3.p1.6.m3.1"><semantics id="A3.p1.6.m3.1a"><msub id="A3.p1.6.m3.1.1" xref="A3.p1.6.m3.1.1.cmml"><mi id="A3.p1.6.m3.1.1.2" xref="A3.p1.6.m3.1.1.2.cmml">d</mi><mi id="A3.p1.6.m3.1.1.3" xref="A3.p1.6.m3.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="A3.p1.6.m3.1b"><apply id="A3.p1.6.m3.1.1.cmml" xref="A3.p1.6.m3.1.1"><csymbol cd="ambiguous" id="A3.p1.6.m3.1.1.1.cmml" xref="A3.p1.6.m3.1.1">subscript</csymbol><ci id="A3.p1.6.m3.1.1.2.cmml" xref="A3.p1.6.m3.1.1.2">ùëë</ci><ci id="A3.p1.6.m3.1.1.3.cmml" xref="A3.p1.6.m3.1.1.3">ùëé</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.6.m3.1c">d_{a}</annotation><annotation encoding="application/x-llamapun" id="A3.p1.6.m3.1d">italic_d start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math> against this dependent value <math alttext="m" class="ltx_Math" display="inline" id="A3.p1.7.m4.1"><semantics id="A3.p1.7.m4.1a"><mi id="A3.p1.7.m4.1.1" xref="A3.p1.7.m4.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="A3.p1.7.m4.1b"><ci id="A3.p1.7.m4.1.1.cmml" xref="A3.p1.7.m4.1.1">ùëö</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.7.m4.1c">m</annotation><annotation encoding="application/x-llamapun" id="A3.p1.7.m4.1d">italic_m</annotation></semantics></math> instead of the model output <math alttext="c" class="ltx_Math" display="inline" id="A3.p1.8.m5.1"><semantics id="A3.p1.8.m5.1a"><mi id="A3.p1.8.m5.1.1" xref="A3.p1.8.m5.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="A3.p1.8.m5.1b"><ci id="A3.p1.8.m5.1.1.cmml" xref="A3.p1.8.m5.1.1">ùëê</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.8.m5.1c">c</annotation><annotation encoding="application/x-llamapun" id="A3.p1.8.m5.1d">italic_c</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="A3.p2">
<p class="ltx_p" id="A3.p2.1">The keypoint detection rates for all models are displayed in eFigure <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#A3.F1.sf1" title="In Figure 1 ‚Ä£ Appendix C Keypoint detection rates ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_tag">1(a)</span></a>.
The result for the human annotation variance (see Section <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#S3.SS1" title="3.1 Variability in human labeling ‚Ä£ 3 Results ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_tag">3.1</span></a>) was also added for comparison.</p>
</div>
<figure class="ltx_figure" id="A3.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A3.F1.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A3.F1.sf1.g1" src="extracted/5907085/scorecurve_permissing.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A3.F1.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="A3.F1.sf1.3.2" style="font-size:90%;">Overall error</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A3.F1.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A3.F1.sf2.g1" src="extracted/5907085/scorecurve_permissing_split.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A3.F1.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="A3.F1.sf2.3.2" style="font-size:90%;">Split by viewing angle</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A3.F1.4.2.1" style="font-size:90%;">eFigure 1</span>: </span><span class="ltx_text" id="A3.F1.2.1" style="font-size:90%;">Mean difference to annotation <math alttext="d_{a}" class="ltx_Math" display="inline" id="A3.F1.2.1.m1.1"><semantics id="A3.F1.2.1.m1.1b"><msub id="A3.F1.2.1.m1.1.1" xref="A3.F1.2.1.m1.1.1.cmml"><mi id="A3.F1.2.1.m1.1.1.2" xref="A3.F1.2.1.m1.1.1.2.cmml">d</mi><mi id="A3.F1.2.1.m1.1.1.3" xref="A3.F1.2.1.m1.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="A3.F1.2.1.m1.1c"><apply id="A3.F1.2.1.m1.1.1.cmml" xref="A3.F1.2.1.m1.1.1"><csymbol cd="ambiguous" id="A3.F1.2.1.m1.1.1.1.cmml" xref="A3.F1.2.1.m1.1.1">subscript</csymbol><ci id="A3.F1.2.1.m1.1.1.2.cmml" xref="A3.F1.2.1.m1.1.1.2">ùëë</ci><ci id="A3.F1.2.1.m1.1.1.3.cmml" xref="A3.F1.2.1.m1.1.1.3">ùëé</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.F1.2.1.m1.1d">d_{a}</annotation><annotation encoding="application/x-llamapun" id="A3.F1.2.1.m1.1e">italic_d start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math> vs. the ratio of missing detections, based on the keypoint prediction confidence scores. Panel a): overall, panel b): split by viewing angle, solid lines correspond to the top view, dashed lines to the diagonal view.</span></figcaption>
</figure>
<div class="ltx_para" id="A3.p3">
<p class="ltx_p" id="A3.p3.1">Except for MediaPipe, the error decreases when filtering out uncertain points.
The increase for MediaPipe comes from the model assigning relatively high scores to the ears, which are among its worst detected points.
ViTPose-huge has the lowest error across all missing ratios, MediaPipe the highest.
OpenPose achieves a lower error than HRNet for missing ratios over 15%, but this is of limited use, as ViTPose is still better and 15% of points not being detected is insufficient for the applications of automated GMA.
Moreover, OpenPose does not even detect all points when thresholding with a score of 0, resulting in the missing ratio never dropping below 2.93%.
ViTPose and HRNet, however, produce predictions for every possible point when thresholding at 0.
Below the missing ratio for humans (1.41%, because of the ears, see Section <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#A1.SS4" title="A.4 Image selection and annotation ‚Ä£ Appendix A Dataset ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_tag">A.4</span></a>), the error of ViTPose saturates, while the error of HRNet increases.
This is because ViTPose is assigning low scores to the ear keypoints that are not visible (e.g., due to head turned to the side) and therefore don‚Äôt affect the error calculation.</p>
</div>
<div class="ltx_para" id="A3.p4">
<p class="ltx_p" id="A3.p4.1">eFigure <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#A3.F1.sf2" title="In Figure 1 ‚Ä£ Appendix C Keypoint detection rates ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_tag">1(b)</span></a> shows the mean difference to annotation in dependence of the missing ratio, like eFigure <a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#A3.F1.sf1" title="In Figure 1 ‚Ä£ Appendix C Keypoint detection rates ‚Ä£ Comparison of marker-less 2D image-based methods for infant pose estimation"><span class="ltx_text ltx_ref_tag">1(a)</span></a>, but split by perspective.
The performance for the diagonal view is always worse than for the top view.
Moreover, while the error for the diagonal view drops faster than for the top view when filtering out uncertain points, they don‚Äôt converge to the same level even when filtering out more than 20% of all detected points.
We again observe OpenPose not being able to provide estimates for every point, with the diagonal angle having more non-detected keypoints as compared to the top view.</p>
</div>
<div class="ltx_para" id="A3.p5">
<p class="ltx_p" id="A3.p5.2">In summary, filtering out points with low certainty scores yields better results in terms of pose estimation error (<math alttext="d_{a}" class="ltx_Math" display="inline" id="A3.p5.1.m1.1"><semantics id="A3.p5.1.m1.1a"><msub id="A3.p5.1.m1.1.1" xref="A3.p5.1.m1.1.1.cmml"><mi id="A3.p5.1.m1.1.1.2" xref="A3.p5.1.m1.1.1.2.cmml">d</mi><mi id="A3.p5.1.m1.1.1.3" xref="A3.p5.1.m1.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="A3.p5.1.m1.1b"><apply id="A3.p5.1.m1.1.1.cmml" xref="A3.p5.1.m1.1.1"><csymbol cd="ambiguous" id="A3.p5.1.m1.1.1.1.cmml" xref="A3.p5.1.m1.1.1">subscript</csymbol><ci id="A3.p5.1.m1.1.1.2.cmml" xref="A3.p5.1.m1.1.1.2">ùëë</ci><ci id="A3.p5.1.m1.1.1.3.cmml" xref="A3.p5.1.m1.1.1.3">ùëé</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p5.1.m1.1c">d_{a}</annotation><annotation encoding="application/x-llamapun" id="A3.p5.1.m1.1d">italic_d start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math>) for all models but MediaPipe.
However, this is of limited value for practical use, since, for most applications, missing values would have to be filled in by interpolation, median filtering, or other techniques (e.g., Kalman filter), to be used for motion analysis.
Still, the fact that the least certain points for ViTPose are the ones humans could not annotate, manifesting in stagnating <math alttext="d_{a}" class="ltx_Math" display="inline" id="A3.p5.2.m2.1"><semantics id="A3.p5.2.m2.1a"><msub id="A3.p5.2.m2.1.1" xref="A3.p5.2.m2.1.1.cmml"><mi id="A3.p5.2.m2.1.1.2" xref="A3.p5.2.m2.1.1.2.cmml">d</mi><mi id="A3.p5.2.m2.1.1.3" xref="A3.p5.2.m2.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="A3.p5.2.m2.1b"><apply id="A3.p5.2.m2.1.1.cmml" xref="A3.p5.2.m2.1.1"><csymbol cd="ambiguous" id="A3.p5.2.m2.1.1.1.cmml" xref="A3.p5.2.m2.1.1">subscript</csymbol><ci id="A3.p5.2.m2.1.1.2.cmml" xref="A3.p5.2.m2.1.1.2">ùëë</ci><ci id="A3.p5.2.m2.1.1.3.cmml" xref="A3.p5.2.m2.1.1.3">ùëé</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p5.2.m2.1c">d_{a}</annotation><annotation encoding="application/x-llamapun" id="A3.p5.2.m2.1d">italic_d start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math> for low missing ratios, shows the certainty score aligns with actual visibility constraints for the state-of-the-art model.</p>
</div>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>SyRIP</h2>
<div class="ltx_para" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">The SyRIP dataset <cite class="ltx_cite ltx_citemacro_cite"><sup class="ltx_sup" id="A4.p1.1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04980v1#bib.bib32" title="">32</a></sup></cite> was considered, and some preliminary analyses were performed, however, it became obvious that the setting of this dataset is too different from GMA setting, with many more different body positions and much older infants than in our dataset.
As in case of the specialized infant pose estimators evaluated on our dataset, our model could not compete with the generic ViTPose on SyRIP.
Retraining decreased the PCK@0.05 from 51.53% to 27.65% (73.66% to 46.97% for PCK@0.1), further strengthening our point that specialized infant pose estimators (in this case our own) are overfit and do not generalize well to other datasets.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Oct  7 12:15:44 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
