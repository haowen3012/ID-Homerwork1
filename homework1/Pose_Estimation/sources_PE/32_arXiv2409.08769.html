<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry</title>
<!--Generated on Fri Sep 13 12:00:25 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="visual inertial odometry multi modal transformers deep neural networks rotation learning" lang="en" name="keywords"/>
<base href="/html/2409.08769v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#S1" title="In Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#S2" title="In Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#S2.SS1" title="In 2 Related Work â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Visual-Inertial Odometry</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#S3" title="In Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#S3.SS1" title="In 3 Method â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Feature Encoder</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#S3.SS2" title="In 3 Method â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Transformers For Fusion and Pose Estimation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#S3.SS3" title="In 3 Method â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Deep Rotation Regression</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#S4" title="In Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#S4.SS1" title="In 4 Experiments â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Experiment Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#S4.SS2" title="In 4 Experiments â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Main Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#S4.SS3" title="In 4 Experiments â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Ablation Study</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#S4.SS3.SSS1" title="In 4.3 Ablation Study â€£ 4 Experiments â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.1 </span>Model Type</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#S4.SS3.SSS2" title="In 4.3 Ablation Study â€£ 4 Experiments â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.2 </span>Norm Type in Training Criterion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#S4.SS3.SSS3" title="In 4.3 Ablation Study â€£ 4 Experiments â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.3 </span>Data Balancing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#S4.SS3.SSS4" title="In 4.3 Ablation Study â€£ 4 Experiments â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.4 </span>RPMG</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#S4.SS3.SSS5" title="In 4.3 Ablation Study â€£ 4 Experiments â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.5 </span>Sequence Length</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#S5" title="In Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Dept. Electrical and Electronics Eng., METU, Ankara, TÃ¼rkiye </span></span></span><span class="ltx_note ltx_role_institutetext" id="id2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Center for Image Analysis (OGAM), METU, Ankara, TÃ¼rkiye </span></span></span><span class="ltx_note ltx_role_institutetext" id="id3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span>Codeway AI Research</span></span></span>
<h1 class="ltx_title ltx_title_document">Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yunus Bilge Kurt<span class="ltx_ERROR undefined" id="id2.1.id1">\orcidlink</span>0000-0002-1564-3450
</span><span class="ltx_author_notes">112233</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ahmet Akman <span class="ltx_ERROR undefined" id="id3.1.id1">\orcidlink</span>0000-0001-5112-6963
</span><span class="ltx_author_notes">1122</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">A. AydÄ±n Alatan<span class="ltx_ERROR undefined" id="id4.1.id1">\orcidlink</span>0000-0001-5556-7301
</span><span class="ltx_author_notes">1122</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.1">In recent years, transformer-based architectures become the de facto standard for sequence modeling in deep learning frameworks.
Inspired by the successful examples, we propose a causal visual-inertial fusion transformer (VIFT) for pose estimation in deep visual-inertial odometry.
This study aims to improve pose estimation accuracy by leveraging the attention mechanisms in transformers, which better utilize historical data compared to the recurrent neural network (RNN) based methods seen in recent methods.
Transformers typically require large-scale data for training. To address this issue, we utilize inductive biases for deep VIO networks.
Since latent visual-inertial feature vectors encompass essential information for pose estimation, we employ transformers to refine pose estimates by updating latent vectors temporally.
Our study also examines the impact of data imbalance and rotation learning methods in supervised end-to-end learning of visual inertial odometry by utilizing specialized gradients in backpropagation for the elements of SE<math alttext="(3)" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><mrow id="id1.1.m1.1.2.2"><mo id="id1.1.m1.1.2.2.1" stretchy="false">(</mo><mn id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">3</mn><mo id="id1.1.m1.1.2.2.2" stretchy="false">)</mo></mrow><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><cn id="id1.1.m1.1.1.cmml" type="integer" xref="id1.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">(3)</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">( 3 )</annotation></semantics></math> group.
The proposed method is end-to-end trainable and requires only a monocular camera and IMU during inference.
Experimental results demonstrate that VIFT increases the accuracy of monocular VIO networks, achieving state-of-the-art results when compared to previous methods on the KITTI dataset. The code will be made available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ybkurt/VIFT" title="">https://github.com/ybkurt/VIFT</a>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>visual inertial odometry multi modal transformers deep neural networks rotation learning
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Visual Inertial Odometry (VIO) is a fundamental approach for the estimation of the pose of a moving body by visual-inertial sensor fusion. These methods can leverage the complementary nature of visual and inertial data, where the visual data provides 3D information about the scene, and the inertial data offers robust motion cues. Geometry-based VIO methods show promising results <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib11" title="">11</a>]</cite>; however, they often require careful initialization and calibration to perform accurately during operation. In contrast, end-to-end learning approaches in VIO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib27" title="">27</a>]</cite> offer the potential to bypass these challenges by directly learning fuse sensory information. Integrating deep learning into VIO systems introduces the possibility of not only improving accuracy but also simplifying deployment, as the models can generalize across different environments and conditions.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">While deep learning has shown promise in VIO, there are still significant areas for improvement, particularly in temporal modeling and rotation estimation. Current deep VIO methods often employ recurrent neural network (RNN) based methodologies to model temporal dependencies.
Nevertheless, the literature has decent alternatives to RNN-based methods for modeling complex temporal dynamics in VIO tasks. Additionally, rotation regression remains a challenge, as traditional representations such as quaternions or Euler angles are not optimal for deep learning models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib29" title="">29</a>]</cite>. Transformer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib23" title="">23</a>]</cite> offers a promising alternative for better temporal modeling. By leveraging transformers, we hypothesize that we can achieve more accurate and robust pose estimation, particularly by focusing on refining the latent representations of sensor data and improving the rotation regression.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry"><span class="ltx_text ltx_ref_tag">1</span></a>, our method uses frozen image and inertial encoders to obtain latent vectors for VIO. Then, transformer layers perform fusion and pose estimation using latent vectors. Our proposed VIFT method is a ViT-like <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib7" title="">7</a>]</cite> architecture without class token, and instead of image patches, VIFT uses latent visual and inertial representations for learning temporal relations. Instead of creating new vectors from scratch, we modify the latents with the transformer using temporal relations and use output directly to estimate poses. In the last stage, we use RPMG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib5" title="">5</a>]</cite> for manifold-aware gradient updates for rotation regression. We show that the VIFT improves the performance of deep VIO networks compared to RNN-based pose modules.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="585" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.4.2.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.2.1" style="font-size:90%;">VIFT architecture. The network consists of two fundamental sides. The first side consists of two encoders with frozen weights that map visual and inertial information to a latent space. The second side consists of sequential transformer layers followed by a fully connected layer. For backpropagation enhancement of rotation, the output is projected to <math alttext="3\times 3" class="ltx_Math" display="inline" id="S1.F1.2.1.m1.1"><semantics id="S1.F1.2.1.m1.1b"><mrow id="S1.F1.2.1.m1.1.1" xref="S1.F1.2.1.m1.1.1.cmml"><mn id="S1.F1.2.1.m1.1.1.2" xref="S1.F1.2.1.m1.1.1.2.cmml">3</mn><mo id="S1.F1.2.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S1.F1.2.1.m1.1.1.1.cmml">Ã—</mo><mn id="S1.F1.2.1.m1.1.1.3" xref="S1.F1.2.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.2.1.m1.1c"><apply id="S1.F1.2.1.m1.1.1.cmml" xref="S1.F1.2.1.m1.1.1"><times id="S1.F1.2.1.m1.1.1.1.cmml" xref="S1.F1.2.1.m1.1.1.1"></times><cn id="S1.F1.2.1.m1.1.1.2.cmml" type="integer" xref="S1.F1.2.1.m1.1.1.2">3</cn><cn id="S1.F1.2.1.m1.1.1.3.cmml" type="integer" xref="S1.F1.2.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.2.1.m1.1d">3\times 3</annotation><annotation encoding="application/x-llamapun" id="S1.F1.2.1.m1.1e">3 Ã— 3</annotation></semantics></math> rotation matrix representation, and RPMG (Regularized Projective Manifold Gradient) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib5" title="">5</a>]</cite> is used.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">We propose visual-inertial fusion transformer (VIFT), a novel fusion and pose estimation module for VIO based on causal transformer encoder architecture. Our contributions can be summarized as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">VIFT uses transformer layers for visual-inertial fusion and pose estimation. We find inductive biases to make transformers perform better, eliminating the possible problems arising from the small data scale.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">VIFT exploits Riemannian manifold optimization techniques for rotations, enabling the network to learn the rotations better than Euler angles and quaternions used in previous works.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">VIFT achieves state-of-the-art results with the proposed transformer module and improves performance further with manifold-aware gradients.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Visual-Inertial Odometry</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Visual-inertial odometry (VIO) leverages the complementary strengths of visual and inertial sensors to provide robust and accurate motion estimation. While visual odometry relies on camera images to estimate motion, it can fail in textureless environments. In contrast, inertial odometry offers high update rates using data from Inertial Measurement Units (IMUs) but requires proper initialization and is prone to drift due to biases in accelerometer and gyroscope readings. With fusion visual and inertial estimates, VIO can mitigate the weaknesses of each individual sensor. Visual measurements help correct the drift in inertial measurements by providing pose updates, while inertial measurements enhance the robustness and temporal resolution of visual estimates. This has led to the development of several VIO methods.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Geometry-based VIO methods require addressing several challenges, including the excitation of IMU biases along all axes to track biases, proper camera-IMU extrinsic calibration, and robust initialization procedures. Despite these challenges, the fusion of visual and inertial data through sophisticated filtering based <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib11" title="">11</a>]</cite> and factor graph based <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib21" title="">21</a>]</cite> methods has significantly advanced the field, enabling more accurate and reliable motion estimation in various applications.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">Geometry-based visual inertial odometry methods have several drawbacks. They require good initialization with excitation in all axes to determine IMU biases, which might not be feasible in many scenarios. Additionally, tuning various parameters is necessary for robust and accurate operation.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">VINet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib6" title="">6</a>]</cite>, being a seminal work in the field, approached visual-inertial odometry as a sequence-to-sequence learning problem. This led to development of supervised deep VIO methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib25" title="">25</a>]</cite> which use ground truth transformation to learn correct
transformation via regression, and self-supervised deep VIO methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib1" title="">1</a>]</cite> which perform view synthesis with estimated relative pose between images and train on pixel-wise intensity errors.</p>
</div>
<div class="ltx_para" id="S2.SS1.p5">
<p class="ltx_p" id="S2.SS1.p5.1">In supervised deep VIO, Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib4" title="">4</a>]</cite> proposed Soft Fusion and Hard Fusion for adaptively weighting visual and inertial embeddings during inference. Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib27" title="">27</a>]</cite> proposed training an adaptive modality selection module inside a deep VIO network to optionally disable the visual encoder. Until recent years, fusion and pose estimation modules consisted of RNN-based networks. ATVIO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib14" title="">14</a>]</cite> suggested using the attention mechanism for fusion in VIO. External memory-aided attention-based module is proposed in EMA-VIO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib22" title="">22</a>]</cite>. Recent methods showed improvements in VIO performance with attention-based modules.</p>
</div>
<div class="ltx_para" id="S2.SS1.p6">
<p class="ltx_p" id="S2.SS1.p6.1">The recent advancements induced interest in exploring transformer-based architectures for fusion and pose estimation in VIO. Transformers, known for their ability to model long-range dependencies and capture complex temporal dynamics, present a compelling alternative to traditional RNN-based approaches. By leveraging self-attention mechanisms, transformers can selectively focus on the most relevant parts of the latent visual-inertial vectors, potentially leading to a more accurate and robust fusion of visual and inertial data.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">End-to-end VIO methods consist of a visual encoder, an inertial encoder, a fusion module, and a pose estimation module. The visual encoder extracts visual features from consecutive frames to provide 3D understanding of their architecture. The inertial encoder takes input from the IMU measurements between frames. The rate of IMU data is usually higher than that of cameras.
The fusion module takes the visual and inertial representation presented by encoders. The pose estimation module uses fused representations of visual and inertial information to estimate the pose.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Feature Encoder</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.5">VIFT architecture can be seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry"><span class="ltx_text ltx_ref_tag">1</span></a>. At each timestep <math alttext="t" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">italic_t</annotation></semantics></math>, VIFT takes two consecutive frames <math alttext="\mathbf{I}_{t},\mathbf{I}_{t-1}\in\mathcal{R}^{C\times W\times H}" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.2"><semantics id="S3.SS1.p1.2.m2.2a"><mrow id="S3.SS1.p1.2.m2.2.2" xref="S3.SS1.p1.2.m2.2.2.cmml"><mrow id="S3.SS1.p1.2.m2.2.2.2.2" xref="S3.SS1.p1.2.m2.2.2.2.3.cmml"><msub id="S3.SS1.p1.2.m2.1.1.1.1.1" xref="S3.SS1.p1.2.m2.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.1.1.1.2" xref="S3.SS1.p1.2.m2.1.1.1.1.1.2.cmml">ğˆ</mi><mi id="S3.SS1.p1.2.m2.1.1.1.1.1.3" xref="S3.SS1.p1.2.m2.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S3.SS1.p1.2.m2.2.2.2.2.3" xref="S3.SS1.p1.2.m2.2.2.2.3.cmml">,</mo><msub id="S3.SS1.p1.2.m2.2.2.2.2.2" xref="S3.SS1.p1.2.m2.2.2.2.2.2.cmml"><mi id="S3.SS1.p1.2.m2.2.2.2.2.2.2" xref="S3.SS1.p1.2.m2.2.2.2.2.2.2.cmml">ğˆ</mi><mrow id="S3.SS1.p1.2.m2.2.2.2.2.2.3" xref="S3.SS1.p1.2.m2.2.2.2.2.2.3.cmml"><mi id="S3.SS1.p1.2.m2.2.2.2.2.2.3.2" xref="S3.SS1.p1.2.m2.2.2.2.2.2.3.2.cmml">t</mi><mo id="S3.SS1.p1.2.m2.2.2.2.2.2.3.1" xref="S3.SS1.p1.2.m2.2.2.2.2.2.3.1.cmml">âˆ’</mo><mn id="S3.SS1.p1.2.m2.2.2.2.2.2.3.3" xref="S3.SS1.p1.2.m2.2.2.2.2.2.3.3.cmml">1</mn></mrow></msub></mrow><mo id="S3.SS1.p1.2.m2.2.2.3" xref="S3.SS1.p1.2.m2.2.2.3.cmml">âˆˆ</mo><msup id="S3.SS1.p1.2.m2.2.2.4" xref="S3.SS1.p1.2.m2.2.2.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.2.m2.2.2.4.2" xref="S3.SS1.p1.2.m2.2.2.4.2.cmml">â„›</mi><mrow id="S3.SS1.p1.2.m2.2.2.4.3" xref="S3.SS1.p1.2.m2.2.2.4.3.cmml"><mi id="S3.SS1.p1.2.m2.2.2.4.3.2" xref="S3.SS1.p1.2.m2.2.2.4.3.2.cmml">C</mi><mo id="S3.SS1.p1.2.m2.2.2.4.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p1.2.m2.2.2.4.3.1.cmml">Ã—</mo><mi id="S3.SS1.p1.2.m2.2.2.4.3.3" xref="S3.SS1.p1.2.m2.2.2.4.3.3.cmml">W</mi><mo id="S3.SS1.p1.2.m2.2.2.4.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p1.2.m2.2.2.4.3.1.cmml">Ã—</mo><mi id="S3.SS1.p1.2.m2.2.2.4.3.4" xref="S3.SS1.p1.2.m2.2.2.4.3.4.cmml">H</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.2b"><apply id="S3.SS1.p1.2.m2.2.2.cmml" xref="S3.SS1.p1.2.m2.2.2"><in id="S3.SS1.p1.2.m2.2.2.3.cmml" xref="S3.SS1.p1.2.m2.2.2.3"></in><list id="S3.SS1.p1.2.m2.2.2.2.3.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2"><apply id="S3.SS1.p1.2.m2.1.1.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.1.2">ğˆ</ci><ci id="S3.SS1.p1.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.1.3">ğ‘¡</ci></apply><apply id="S3.SS1.p1.2.m2.2.2.2.2.2.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.2.2.2.2.2.1.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.2.m2.2.2.2.2.2.2.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.2.2">ğˆ</ci><apply id="S3.SS1.p1.2.m2.2.2.2.2.2.3.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.2.3"><minus id="S3.SS1.p1.2.m2.2.2.2.2.2.3.1.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.2.3.1"></minus><ci id="S3.SS1.p1.2.m2.2.2.2.2.2.3.2.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.2.3.2">ğ‘¡</ci><cn id="S3.SS1.p1.2.m2.2.2.2.2.2.3.3.cmml" type="integer" xref="S3.SS1.p1.2.m2.2.2.2.2.2.3.3">1</cn></apply></apply></list><apply id="S3.SS1.p1.2.m2.2.2.4.cmml" xref="S3.SS1.p1.2.m2.2.2.4"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.2.2.4.1.cmml" xref="S3.SS1.p1.2.m2.2.2.4">superscript</csymbol><ci id="S3.SS1.p1.2.m2.2.2.4.2.cmml" xref="S3.SS1.p1.2.m2.2.2.4.2">â„›</ci><apply id="S3.SS1.p1.2.m2.2.2.4.3.cmml" xref="S3.SS1.p1.2.m2.2.2.4.3"><times id="S3.SS1.p1.2.m2.2.2.4.3.1.cmml" xref="S3.SS1.p1.2.m2.2.2.4.3.1"></times><ci id="S3.SS1.p1.2.m2.2.2.4.3.2.cmml" xref="S3.SS1.p1.2.m2.2.2.4.3.2">ğ¶</ci><ci id="S3.SS1.p1.2.m2.2.2.4.3.3.cmml" xref="S3.SS1.p1.2.m2.2.2.4.3.3">ğ‘Š</ci><ci id="S3.SS1.p1.2.m2.2.2.4.3.4.cmml" xref="S3.SS1.p1.2.m2.2.2.4.3.4">ğ»</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.2c">\mathbf{I}_{t},\mathbf{I}_{t-1}\in\mathcal{R}^{C\times W\times H}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.2d">bold_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_I start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT âˆˆ caligraphic_R start_POSTSUPERSCRIPT italic_C Ã— italic_W Ã— italic_H end_POSTSUPERSCRIPT</annotation></semantics></math> and IMU measurements <math alttext="\mathbf{a}_{t,t-1},\mathbf{\omega}_{t,t-1}" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.6"><semantics id="S3.SS1.p1.3.m3.6a"><mrow id="S3.SS1.p1.3.m3.6.6.2" xref="S3.SS1.p1.3.m3.6.6.3.cmml"><msub id="S3.SS1.p1.3.m3.5.5.1.1" xref="S3.SS1.p1.3.m3.5.5.1.1.cmml"><mi id="S3.SS1.p1.3.m3.5.5.1.1.2" xref="S3.SS1.p1.3.m3.5.5.1.1.2.cmml">ğš</mi><mrow id="S3.SS1.p1.3.m3.2.2.2.2" xref="S3.SS1.p1.3.m3.2.2.2.3.cmml"><mi id="S3.SS1.p1.3.m3.1.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.1.cmml">t</mi><mo id="S3.SS1.p1.3.m3.2.2.2.2.2" xref="S3.SS1.p1.3.m3.2.2.2.3.cmml">,</mo><mrow id="S3.SS1.p1.3.m3.2.2.2.2.1" xref="S3.SS1.p1.3.m3.2.2.2.2.1.cmml"><mi id="S3.SS1.p1.3.m3.2.2.2.2.1.2" xref="S3.SS1.p1.3.m3.2.2.2.2.1.2.cmml">t</mi><mo id="S3.SS1.p1.3.m3.2.2.2.2.1.1" xref="S3.SS1.p1.3.m3.2.2.2.2.1.1.cmml">âˆ’</mo><mn id="S3.SS1.p1.3.m3.2.2.2.2.1.3" xref="S3.SS1.p1.3.m3.2.2.2.2.1.3.cmml">1</mn></mrow></mrow></msub><mo id="S3.SS1.p1.3.m3.6.6.2.3" xref="S3.SS1.p1.3.m3.6.6.3.cmml">,</mo><msub id="S3.SS1.p1.3.m3.6.6.2.2" xref="S3.SS1.p1.3.m3.6.6.2.2.cmml"><mi id="S3.SS1.p1.3.m3.6.6.2.2.2" xref="S3.SS1.p1.3.m3.6.6.2.2.2.cmml">Ï‰</mi><mrow id="S3.SS1.p1.3.m3.4.4.2.2" xref="S3.SS1.p1.3.m3.4.4.2.3.cmml"><mi id="S3.SS1.p1.3.m3.3.3.1.1" xref="S3.SS1.p1.3.m3.3.3.1.1.cmml">t</mi><mo id="S3.SS1.p1.3.m3.4.4.2.2.2" xref="S3.SS1.p1.3.m3.4.4.2.3.cmml">,</mo><mrow id="S3.SS1.p1.3.m3.4.4.2.2.1" xref="S3.SS1.p1.3.m3.4.4.2.2.1.cmml"><mi id="S3.SS1.p1.3.m3.4.4.2.2.1.2" xref="S3.SS1.p1.3.m3.4.4.2.2.1.2.cmml">t</mi><mo id="S3.SS1.p1.3.m3.4.4.2.2.1.1" xref="S3.SS1.p1.3.m3.4.4.2.2.1.1.cmml">âˆ’</mo><mn id="S3.SS1.p1.3.m3.4.4.2.2.1.3" xref="S3.SS1.p1.3.m3.4.4.2.2.1.3.cmml">1</mn></mrow></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.6b"><list id="S3.SS1.p1.3.m3.6.6.3.cmml" xref="S3.SS1.p1.3.m3.6.6.2"><apply id="S3.SS1.p1.3.m3.5.5.1.1.cmml" xref="S3.SS1.p1.3.m3.5.5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.5.5.1.1.1.cmml" xref="S3.SS1.p1.3.m3.5.5.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.5.5.1.1.2.cmml" xref="S3.SS1.p1.3.m3.5.5.1.1.2">ğš</ci><list id="S3.SS1.p1.3.m3.2.2.2.3.cmml" xref="S3.SS1.p1.3.m3.2.2.2.2"><ci id="S3.SS1.p1.3.m3.1.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1">ğ‘¡</ci><apply id="S3.SS1.p1.3.m3.2.2.2.2.1.cmml" xref="S3.SS1.p1.3.m3.2.2.2.2.1"><minus id="S3.SS1.p1.3.m3.2.2.2.2.1.1.cmml" xref="S3.SS1.p1.3.m3.2.2.2.2.1.1"></minus><ci id="S3.SS1.p1.3.m3.2.2.2.2.1.2.cmml" xref="S3.SS1.p1.3.m3.2.2.2.2.1.2">ğ‘¡</ci><cn id="S3.SS1.p1.3.m3.2.2.2.2.1.3.cmml" type="integer" xref="S3.SS1.p1.3.m3.2.2.2.2.1.3">1</cn></apply></list></apply><apply id="S3.SS1.p1.3.m3.6.6.2.2.cmml" xref="S3.SS1.p1.3.m3.6.6.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.6.6.2.2.1.cmml" xref="S3.SS1.p1.3.m3.6.6.2.2">subscript</csymbol><ci id="S3.SS1.p1.3.m3.6.6.2.2.2.cmml" xref="S3.SS1.p1.3.m3.6.6.2.2.2">ğœ”</ci><list id="S3.SS1.p1.3.m3.4.4.2.3.cmml" xref="S3.SS1.p1.3.m3.4.4.2.2"><ci id="S3.SS1.p1.3.m3.3.3.1.1.cmml" xref="S3.SS1.p1.3.m3.3.3.1.1">ğ‘¡</ci><apply id="S3.SS1.p1.3.m3.4.4.2.2.1.cmml" xref="S3.SS1.p1.3.m3.4.4.2.2.1"><minus id="S3.SS1.p1.3.m3.4.4.2.2.1.1.cmml" xref="S3.SS1.p1.3.m3.4.4.2.2.1.1"></minus><ci id="S3.SS1.p1.3.m3.4.4.2.2.1.2.cmml" xref="S3.SS1.p1.3.m3.4.4.2.2.1.2">ğ‘¡</ci><cn id="S3.SS1.p1.3.m3.4.4.2.2.1.3.cmml" type="integer" xref="S3.SS1.p1.3.m3.4.4.2.2.1.3">1</cn></apply></list></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.6c">\mathbf{a}_{t,t-1},\mathbf{\omega}_{t,t-1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.6d">bold_a start_POSTSUBSCRIPT italic_t , italic_t - 1 end_POSTSUBSCRIPT , italic_Ï‰ start_POSTSUBSCRIPT italic_t , italic_t - 1 end_POSTSUBSCRIPT</annotation></semantics></math> between frames, consisting of accelerometer and gyroscope readings.
Visual and inertial measurements are processed with different encoders for their modality as in previous deep VIO methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib27" title="">27</a>]</cite>, following the work of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib27" title="">27</a>]</cite> we use FlowNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib8" title="">8</a>]</cite> based image encoder and 1D CNN based inertial encoder.
Visual measurements are processed by image encoder <math alttext="\mathbf{E}_{v}" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4.1"><semantics id="S3.SS1.p1.4.m4.1a"><msub id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mi id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml">ğ„</mi><mi id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2">ğ„</ci><ci id="S3.SS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3">ğ‘£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">\mathbf{E}_{v}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.1d">bold_E start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT</annotation></semantics></math> to produce one-dimensional visual encodings <math alttext="\mathbf{x}^{v}_{t}" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m5.1"><semantics id="S3.SS1.p1.5.m5.1a"><msubsup id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mi id="S3.SS1.p1.5.m5.1.1.2.2" xref="S3.SS1.p1.5.m5.1.1.2.2.cmml">ğ±</mi><mi id="S3.SS1.p1.5.m5.1.1.3" xref="S3.SS1.p1.5.m5.1.1.3.cmml">t</mi><mi id="S3.SS1.p1.5.m5.1.1.2.3" xref="S3.SS1.p1.5.m5.1.1.2.3.cmml">v</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">subscript</csymbol><apply id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.2.1.cmml" xref="S3.SS1.p1.5.m5.1.1">superscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.2.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2.2">ğ±</ci><ci id="S3.SS1.p1.5.m5.1.1.2.3.cmml" xref="S3.SS1.p1.5.m5.1.1.2.3">ğ‘£</ci></apply><ci id="S3.SS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">\mathbf{x}^{v}_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.5.m5.1d">bold_x start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{x}^{v}_{t}=\mathbf{E}_{v}(\mathbf{I}_{t},\mathbf{I_{t-1}})" class="ltx_Math" display="block" id="S3.E1.m1.2"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml"><msubsup id="S3.E1.m1.2.2.4" xref="S3.E1.m1.2.2.4.cmml"><mi id="S3.E1.m1.2.2.4.2.2" xref="S3.E1.m1.2.2.4.2.2.cmml">ğ±</mi><mi id="S3.E1.m1.2.2.4.3" xref="S3.E1.m1.2.2.4.3.cmml">t</mi><mi id="S3.E1.m1.2.2.4.2.3" xref="S3.E1.m1.2.2.4.2.3.cmml">v</mi></msubsup><mo id="S3.E1.m1.2.2.3" xref="S3.E1.m1.2.2.3.cmml">=</mo><mrow id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml"><msub id="S3.E1.m1.2.2.2.4" xref="S3.E1.m1.2.2.2.4.cmml"><mi id="S3.E1.m1.2.2.2.4.2" xref="S3.E1.m1.2.2.2.4.2.cmml">ğ„</mi><mi id="S3.E1.m1.2.2.2.4.3" xref="S3.E1.m1.2.2.2.4.3.cmml">v</mi></msub><mo id="S3.E1.m1.2.2.2.3" xref="S3.E1.m1.2.2.2.3.cmml">â¢</mo><mrow id="S3.E1.m1.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.3.cmml"><mo id="S3.E1.m1.2.2.2.2.2.3" stretchy="false" xref="S3.E1.m1.2.2.2.2.3.cmml">(</mo><msub id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.2.cmml">ğˆ</mi><mi id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S3.E1.m1.2.2.2.2.2.4" xref="S3.E1.m1.2.2.2.2.3.cmml">,</mo><msub id="S3.E1.m1.2.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.2.cmml"><mi id="S3.E1.m1.2.2.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.2.2.cmml">ğˆ</mi><mrow id="S3.E1.m1.2.2.2.2.2.2.3" xref="S3.E1.m1.2.2.2.2.2.2.3.cmml"><mi id="S3.E1.m1.2.2.2.2.2.2.3.2" xref="S3.E1.m1.2.2.2.2.2.2.3.2.cmml">ğ­</mi><mo id="S3.E1.m1.2.2.2.2.2.2.3.1" xref="S3.E1.m1.2.2.2.2.2.2.3.1.cmml">âˆ’</mo><mn id="S3.E1.m1.2.2.2.2.2.2.3.3" xref="S3.E1.m1.2.2.2.2.2.2.3.3.cmml">ğŸ</mn></mrow></msub><mo id="S3.E1.m1.2.2.2.2.2.5" stretchy="false" xref="S3.E1.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2"><eq id="S3.E1.m1.2.2.3.cmml" xref="S3.E1.m1.2.2.3"></eq><apply id="S3.E1.m1.2.2.4.cmml" xref="S3.E1.m1.2.2.4"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.4.1.cmml" xref="S3.E1.m1.2.2.4">subscript</csymbol><apply id="S3.E1.m1.2.2.4.2.cmml" xref="S3.E1.m1.2.2.4"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.4.2.1.cmml" xref="S3.E1.m1.2.2.4">superscript</csymbol><ci id="S3.E1.m1.2.2.4.2.2.cmml" xref="S3.E1.m1.2.2.4.2.2">ğ±</ci><ci id="S3.E1.m1.2.2.4.2.3.cmml" xref="S3.E1.m1.2.2.4.2.3">ğ‘£</ci></apply><ci id="S3.E1.m1.2.2.4.3.cmml" xref="S3.E1.m1.2.2.4.3">ğ‘¡</ci></apply><apply id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2"><times id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.3"></times><apply id="S3.E1.m1.2.2.2.4.cmml" xref="S3.E1.m1.2.2.2.4"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.4.1.cmml" xref="S3.E1.m1.2.2.2.4">subscript</csymbol><ci id="S3.E1.m1.2.2.2.4.2.cmml" xref="S3.E1.m1.2.2.2.4.2">ğ„</ci><ci id="S3.E1.m1.2.2.2.4.3.cmml" xref="S3.E1.m1.2.2.2.4.3">ğ‘£</ci></apply><interval closure="open" id="S3.E1.m1.2.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.2.2"><apply id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2">ğˆ</ci><ci id="S3.E1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3">ğ‘¡</ci></apply><apply id="S3.E1.m1.2.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.2.2.2">subscript</csymbol><ci id="S3.E1.m1.2.2.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2">ğˆ</ci><apply id="S3.E1.m1.2.2.2.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.2.2.2.3"><minus id="S3.E1.m1.2.2.2.2.2.2.3.1.cmml" xref="S3.E1.m1.2.2.2.2.2.2.3.1"></minus><ci id="S3.E1.m1.2.2.2.2.2.2.3.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2.3.2">ğ­</ci><cn id="S3.E1.m1.2.2.2.2.2.2.3.3.cmml" type="integer" xref="S3.E1.m1.2.2.2.2.2.2.3.3">1</cn></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">\mathbf{x}^{v}_{t}=\mathbf{E}_{v}(\mathbf{I}_{t},\mathbf{I_{t-1}})</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.2d">bold_x start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_E start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( bold_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_I start_POSTSUBSCRIPT bold_t - bold_1 end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p1.7">Inertial measurements are processed by inertial encoder <math alttext="\mathbf{E}_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.6.m1.1"><semantics id="S3.SS1.p1.6.m1.1a"><msub id="S3.SS1.p1.6.m1.1.1" xref="S3.SS1.p1.6.m1.1.1.cmml"><mi id="S3.SS1.p1.6.m1.1.1.2" xref="S3.SS1.p1.6.m1.1.1.2.cmml">ğ„</mi><mi id="S3.SS1.p1.6.m1.1.1.3" xref="S3.SS1.p1.6.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m1.1b"><apply id="S3.SS1.p1.6.m1.1.1.cmml" xref="S3.SS1.p1.6.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m1.1.1.1.cmml" xref="S3.SS1.p1.6.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.6.m1.1.1.2.cmml" xref="S3.SS1.p1.6.m1.1.1.2">ğ„</ci><ci id="S3.SS1.p1.6.m1.1.1.3.cmml" xref="S3.SS1.p1.6.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m1.1c">\mathbf{E}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.6.m1.1d">bold_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> to produce one-dimensional inertial encodings <math alttext="\mathbf{x}^{i}_{t}" class="ltx_Math" display="inline" id="S3.SS1.p1.7.m2.1"><semantics id="S3.SS1.p1.7.m2.1a"><msubsup id="S3.SS1.p1.7.m2.1.1" xref="S3.SS1.p1.7.m2.1.1.cmml"><mi id="S3.SS1.p1.7.m2.1.1.2.2" xref="S3.SS1.p1.7.m2.1.1.2.2.cmml">ğ±</mi><mi id="S3.SS1.p1.7.m2.1.1.3" xref="S3.SS1.p1.7.m2.1.1.3.cmml">t</mi><mi id="S3.SS1.p1.7.m2.1.1.2.3" xref="S3.SS1.p1.7.m2.1.1.2.3.cmml">i</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m2.1b"><apply id="S3.SS1.p1.7.m2.1.1.cmml" xref="S3.SS1.p1.7.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m2.1.1.1.cmml" xref="S3.SS1.p1.7.m2.1.1">subscript</csymbol><apply id="S3.SS1.p1.7.m2.1.1.2.cmml" xref="S3.SS1.p1.7.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m2.1.1.2.1.cmml" xref="S3.SS1.p1.7.m2.1.1">superscript</csymbol><ci id="S3.SS1.p1.7.m2.1.1.2.2.cmml" xref="S3.SS1.p1.7.m2.1.1.2.2">ğ±</ci><ci id="S3.SS1.p1.7.m2.1.1.2.3.cmml" xref="S3.SS1.p1.7.m2.1.1.2.3">ğ‘–</ci></apply><ci id="S3.SS1.p1.7.m2.1.1.3.cmml" xref="S3.SS1.p1.7.m2.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m2.1c">\mathbf{x}^{i}_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.7.m2.1d">bold_x start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{x}^{i}_{t}=\mathbf{E}_{i}(\mathbf{a}_{t,t-1},\mathbf{\omega}_{t,t-1})" class="ltx_Math" display="block" id="S3.E2.m1.6"><semantics id="S3.E2.m1.6a"><mrow id="S3.E2.m1.6.6" xref="S3.E2.m1.6.6.cmml"><msubsup id="S3.E2.m1.6.6.4" xref="S3.E2.m1.6.6.4.cmml"><mi id="S3.E2.m1.6.6.4.2.2" xref="S3.E2.m1.6.6.4.2.2.cmml">ğ±</mi><mi id="S3.E2.m1.6.6.4.3" xref="S3.E2.m1.6.6.4.3.cmml">t</mi><mi id="S3.E2.m1.6.6.4.2.3" xref="S3.E2.m1.6.6.4.2.3.cmml">i</mi></msubsup><mo id="S3.E2.m1.6.6.3" xref="S3.E2.m1.6.6.3.cmml">=</mo><mrow id="S3.E2.m1.6.6.2" xref="S3.E2.m1.6.6.2.cmml"><msub id="S3.E2.m1.6.6.2.4" xref="S3.E2.m1.6.6.2.4.cmml"><mi id="S3.E2.m1.6.6.2.4.2" xref="S3.E2.m1.6.6.2.4.2.cmml">ğ„</mi><mi id="S3.E2.m1.6.6.2.4.3" xref="S3.E2.m1.6.6.2.4.3.cmml">i</mi></msub><mo id="S3.E2.m1.6.6.2.3" xref="S3.E2.m1.6.6.2.3.cmml">â¢</mo><mrow id="S3.E2.m1.6.6.2.2.2" xref="S3.E2.m1.6.6.2.2.3.cmml"><mo id="S3.E2.m1.6.6.2.2.2.3" stretchy="false" xref="S3.E2.m1.6.6.2.2.3.cmml">(</mo><msub id="S3.E2.m1.5.5.1.1.1.1" xref="S3.E2.m1.5.5.1.1.1.1.cmml"><mi id="S3.E2.m1.5.5.1.1.1.1.2" xref="S3.E2.m1.5.5.1.1.1.1.2.cmml">ğš</mi><mrow id="S3.E2.m1.2.2.2.2" xref="S3.E2.m1.2.2.2.3.cmml"><mi id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml">t</mi><mo id="S3.E2.m1.2.2.2.2.2" xref="S3.E2.m1.2.2.2.3.cmml">,</mo><mrow id="S3.E2.m1.2.2.2.2.1" xref="S3.E2.m1.2.2.2.2.1.cmml"><mi id="S3.E2.m1.2.2.2.2.1.2" xref="S3.E2.m1.2.2.2.2.1.2.cmml">t</mi><mo id="S3.E2.m1.2.2.2.2.1.1" xref="S3.E2.m1.2.2.2.2.1.1.cmml">âˆ’</mo><mn id="S3.E2.m1.2.2.2.2.1.3" xref="S3.E2.m1.2.2.2.2.1.3.cmml">1</mn></mrow></mrow></msub><mo id="S3.E2.m1.6.6.2.2.2.4" xref="S3.E2.m1.6.6.2.2.3.cmml">,</mo><msub id="S3.E2.m1.6.6.2.2.2.2" xref="S3.E2.m1.6.6.2.2.2.2.cmml"><mi id="S3.E2.m1.6.6.2.2.2.2.2" xref="S3.E2.m1.6.6.2.2.2.2.2.cmml">Ï‰</mi><mrow id="S3.E2.m1.4.4.2.2" xref="S3.E2.m1.4.4.2.3.cmml"><mi id="S3.E2.m1.3.3.1.1" xref="S3.E2.m1.3.3.1.1.cmml">t</mi><mo id="S3.E2.m1.4.4.2.2.2" xref="S3.E2.m1.4.4.2.3.cmml">,</mo><mrow id="S3.E2.m1.4.4.2.2.1" xref="S3.E2.m1.4.4.2.2.1.cmml"><mi id="S3.E2.m1.4.4.2.2.1.2" xref="S3.E2.m1.4.4.2.2.1.2.cmml">t</mi><mo id="S3.E2.m1.4.4.2.2.1.1" xref="S3.E2.m1.4.4.2.2.1.1.cmml">âˆ’</mo><mn id="S3.E2.m1.4.4.2.2.1.3" xref="S3.E2.m1.4.4.2.2.1.3.cmml">1</mn></mrow></mrow></msub><mo id="S3.E2.m1.6.6.2.2.2.5" stretchy="false" xref="S3.E2.m1.6.6.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.6b"><apply id="S3.E2.m1.6.6.cmml" xref="S3.E2.m1.6.6"><eq id="S3.E2.m1.6.6.3.cmml" xref="S3.E2.m1.6.6.3"></eq><apply id="S3.E2.m1.6.6.4.cmml" xref="S3.E2.m1.6.6.4"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.4.1.cmml" xref="S3.E2.m1.6.6.4">subscript</csymbol><apply id="S3.E2.m1.6.6.4.2.cmml" xref="S3.E2.m1.6.6.4"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.4.2.1.cmml" xref="S3.E2.m1.6.6.4">superscript</csymbol><ci id="S3.E2.m1.6.6.4.2.2.cmml" xref="S3.E2.m1.6.6.4.2.2">ğ±</ci><ci id="S3.E2.m1.6.6.4.2.3.cmml" xref="S3.E2.m1.6.6.4.2.3">ğ‘–</ci></apply><ci id="S3.E2.m1.6.6.4.3.cmml" xref="S3.E2.m1.6.6.4.3">ğ‘¡</ci></apply><apply id="S3.E2.m1.6.6.2.cmml" xref="S3.E2.m1.6.6.2"><times id="S3.E2.m1.6.6.2.3.cmml" xref="S3.E2.m1.6.6.2.3"></times><apply id="S3.E2.m1.6.6.2.4.cmml" xref="S3.E2.m1.6.6.2.4"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.2.4.1.cmml" xref="S3.E2.m1.6.6.2.4">subscript</csymbol><ci id="S3.E2.m1.6.6.2.4.2.cmml" xref="S3.E2.m1.6.6.2.4.2">ğ„</ci><ci id="S3.E2.m1.6.6.2.4.3.cmml" xref="S3.E2.m1.6.6.2.4.3">ğ‘–</ci></apply><interval closure="open" id="S3.E2.m1.6.6.2.2.3.cmml" xref="S3.E2.m1.6.6.2.2.2"><apply id="S3.E2.m1.5.5.1.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.5.5.1.1.1.1.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.2">ğš</ci><list id="S3.E2.m1.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.2"><ci id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1">ğ‘¡</ci><apply id="S3.E2.m1.2.2.2.2.1.cmml" xref="S3.E2.m1.2.2.2.2.1"><minus id="S3.E2.m1.2.2.2.2.1.1.cmml" xref="S3.E2.m1.2.2.2.2.1.1"></minus><ci id="S3.E2.m1.2.2.2.2.1.2.cmml" xref="S3.E2.m1.2.2.2.2.1.2">ğ‘¡</ci><cn id="S3.E2.m1.2.2.2.2.1.3.cmml" type="integer" xref="S3.E2.m1.2.2.2.2.1.3">1</cn></apply></list></apply><apply id="S3.E2.m1.6.6.2.2.2.2.cmml" xref="S3.E2.m1.6.6.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.2.2.2.2.1.cmml" xref="S3.E2.m1.6.6.2.2.2.2">subscript</csymbol><ci id="S3.E2.m1.6.6.2.2.2.2.2.cmml" xref="S3.E2.m1.6.6.2.2.2.2.2">ğœ”</ci><list id="S3.E2.m1.4.4.2.3.cmml" xref="S3.E2.m1.4.4.2.2"><ci id="S3.E2.m1.3.3.1.1.cmml" xref="S3.E2.m1.3.3.1.1">ğ‘¡</ci><apply id="S3.E2.m1.4.4.2.2.1.cmml" xref="S3.E2.m1.4.4.2.2.1"><minus id="S3.E2.m1.4.4.2.2.1.1.cmml" xref="S3.E2.m1.4.4.2.2.1.1"></minus><ci id="S3.E2.m1.4.4.2.2.1.2.cmml" xref="S3.E2.m1.4.4.2.2.1.2">ğ‘¡</ci><cn id="S3.E2.m1.4.4.2.2.1.3.cmml" type="integer" xref="S3.E2.m1.4.4.2.2.1.3">1</cn></apply></list></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.6c">\mathbf{x}^{i}_{t}=\mathbf{E}_{i}(\mathbf{a}_{t,t-1},\mathbf{\omega}_{t,t-1})</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.6d">bold_x start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_a start_POSTSUBSCRIPT italic_t , italic_t - 1 end_POSTSUBSCRIPT , italic_Ï‰ start_POSTSUBSCRIPT italic_t , italic_t - 1 end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p1.10">At the end, we concatenate visual and inertial latent vectors.</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{x}_{t}=\textit{Concat}(\mathbf{x}^{v}_{t},\mathbf{x}^{i}_{t})" class="ltx_Math" display="block" id="S3.E3.m1.2"><semantics id="S3.E3.m1.2a"><mrow id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2.cmml"><msub id="S3.E3.m1.2.2.4" xref="S3.E3.m1.2.2.4.cmml"><mi id="S3.E3.m1.2.2.4.2" xref="S3.E3.m1.2.2.4.2.cmml">ğ±</mi><mi id="S3.E3.m1.2.2.4.3" xref="S3.E3.m1.2.2.4.3.cmml">t</mi></msub><mo id="S3.E3.m1.2.2.3" xref="S3.E3.m1.2.2.3.cmml">=</mo><mrow id="S3.E3.m1.2.2.2" xref="S3.E3.m1.2.2.2.cmml"><mtext class="ltx_mathvariant_italic" id="S3.E3.m1.2.2.2.4" xref="S3.E3.m1.2.2.2.4a.cmml">Concat</mtext><mo id="S3.E3.m1.2.2.2.3" xref="S3.E3.m1.2.2.2.3.cmml">â¢</mo><mrow id="S3.E3.m1.2.2.2.2.2" xref="S3.E3.m1.2.2.2.2.3.cmml"><mo id="S3.E3.m1.2.2.2.2.2.3" stretchy="false" xref="S3.E3.m1.2.2.2.2.3.cmml">(</mo><msubsup id="S3.E3.m1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.2.2" xref="S3.E3.m1.1.1.1.1.1.1.2.2.cmml">ğ±</mi><mi id="S3.E3.m1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.3.cmml">t</mi><mi id="S3.E3.m1.1.1.1.1.1.1.2.3" xref="S3.E3.m1.1.1.1.1.1.1.2.3.cmml">v</mi></msubsup><mo id="S3.E3.m1.2.2.2.2.2.4" xref="S3.E3.m1.2.2.2.2.3.cmml">,</mo><msubsup id="S3.E3.m1.2.2.2.2.2.2" xref="S3.E3.m1.2.2.2.2.2.2.cmml"><mi id="S3.E3.m1.2.2.2.2.2.2.2.2" xref="S3.E3.m1.2.2.2.2.2.2.2.2.cmml">ğ±</mi><mi id="S3.E3.m1.2.2.2.2.2.2.3" xref="S3.E3.m1.2.2.2.2.2.2.3.cmml">t</mi><mi id="S3.E3.m1.2.2.2.2.2.2.2.3" xref="S3.E3.m1.2.2.2.2.2.2.2.3.cmml">i</mi></msubsup><mo id="S3.E3.m1.2.2.2.2.2.5" stretchy="false" xref="S3.E3.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.2b"><apply id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2"><eq id="S3.E3.m1.2.2.3.cmml" xref="S3.E3.m1.2.2.3"></eq><apply id="S3.E3.m1.2.2.4.cmml" xref="S3.E3.m1.2.2.4"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.4.1.cmml" xref="S3.E3.m1.2.2.4">subscript</csymbol><ci id="S3.E3.m1.2.2.4.2.cmml" xref="S3.E3.m1.2.2.4.2">ğ±</ci><ci id="S3.E3.m1.2.2.4.3.cmml" xref="S3.E3.m1.2.2.4.3">ğ‘¡</ci></apply><apply id="S3.E3.m1.2.2.2.cmml" xref="S3.E3.m1.2.2.2"><times id="S3.E3.m1.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.3"></times><ci id="S3.E3.m1.2.2.2.4a.cmml" xref="S3.E3.m1.2.2.2.4"><mtext class="ltx_mathvariant_italic" id="S3.E3.m1.2.2.2.4.cmml" xref="S3.E3.m1.2.2.2.4">Concat</mtext></ci><interval closure="open" id="S3.E3.m1.2.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.2.2"><apply id="S3.E3.m1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E3.m1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.2">ğ±</ci><ci id="S3.E3.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.3">ğ‘£</ci></apply><ci id="S3.E3.m1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3">ğ‘¡</ci></apply><apply id="S3.E3.m1.2.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.2.2.2.2.1.cmml" xref="S3.E3.m1.2.2.2.2.2.2">subscript</csymbol><apply id="S3.E3.m1.2.2.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.2.2.2.2.2.1.cmml" xref="S3.E3.m1.2.2.2.2.2.2">superscript</csymbol><ci id="S3.E3.m1.2.2.2.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2.2.2.2.2">ğ±</ci><ci id="S3.E3.m1.2.2.2.2.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.2.2.2.2.3">ğ‘–</ci></apply><ci id="S3.E3.m1.2.2.2.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.2.2.2.3">ğ‘¡</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.2c">\mathbf{x}_{t}=\textit{Concat}(\mathbf{x}^{v}_{t},\mathbf{x}^{i}_{t})</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.2d">bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = Concat ( bold_x start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_x start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p1.9">The corresponding <math alttext="\mathbf{x_{t}}" class="ltx_Math" display="inline" id="S3.SS1.p1.8.m1.1"><semantics id="S3.SS1.p1.8.m1.1a"><msub id="S3.SS1.p1.8.m1.1.1" xref="S3.SS1.p1.8.m1.1.1.cmml"><mi id="S3.SS1.p1.8.m1.1.1.2" xref="S3.SS1.p1.8.m1.1.1.2.cmml">ğ±</mi><mi id="S3.SS1.p1.8.m1.1.1.3" xref="S3.SS1.p1.8.m1.1.1.3.cmml">ğ­</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m1.1b"><apply id="S3.SS1.p1.8.m1.1.1.cmml" xref="S3.SS1.p1.8.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.8.m1.1.1.1.cmml" xref="S3.SS1.p1.8.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.8.m1.1.1.2.cmml" xref="S3.SS1.p1.8.m1.1.1.2">ğ±</ci><ci id="S3.SS1.p1.8.m1.1.1.3.cmml" xref="S3.SS1.p1.8.m1.1.1.3">ğ­</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m1.1c">\mathbf{x_{t}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.8.m1.1d">bold_x start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT</annotation></semantics></math> is labeled as Latent Vector in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry"><span class="ltx_text ltx_ref_tag">1</span></a>. The concatenated vector is passed to the transformer-based fusion and pose estimation module. The expected output is the translation and rotation of the camera between two timesteps <math alttext="\mathbf{T}_{t-1}^{t}\in\textbf{SE}(3)" class="ltx_Math" display="inline" id="S3.SS1.p1.9.m2.1"><semantics id="S3.SS1.p1.9.m2.1a"><mrow id="S3.SS1.p1.9.m2.1.2" xref="S3.SS1.p1.9.m2.1.2.cmml"><msubsup id="S3.SS1.p1.9.m2.1.2.2" xref="S3.SS1.p1.9.m2.1.2.2.cmml"><mi id="S3.SS1.p1.9.m2.1.2.2.2.2" xref="S3.SS1.p1.9.m2.1.2.2.2.2.cmml">ğ“</mi><mrow id="S3.SS1.p1.9.m2.1.2.2.2.3" xref="S3.SS1.p1.9.m2.1.2.2.2.3.cmml"><mi id="S3.SS1.p1.9.m2.1.2.2.2.3.2" xref="S3.SS1.p1.9.m2.1.2.2.2.3.2.cmml">t</mi><mo id="S3.SS1.p1.9.m2.1.2.2.2.3.1" xref="S3.SS1.p1.9.m2.1.2.2.2.3.1.cmml">âˆ’</mo><mn id="S3.SS1.p1.9.m2.1.2.2.2.3.3" xref="S3.SS1.p1.9.m2.1.2.2.2.3.3.cmml">1</mn></mrow><mi id="S3.SS1.p1.9.m2.1.2.2.3" xref="S3.SS1.p1.9.m2.1.2.2.3.cmml">t</mi></msubsup><mo id="S3.SS1.p1.9.m2.1.2.1" xref="S3.SS1.p1.9.m2.1.2.1.cmml">âˆˆ</mo><mrow id="S3.SS1.p1.9.m2.1.2.3" xref="S3.SS1.p1.9.m2.1.2.3.cmml"><mtext class="ltx_mathvariant_bold" id="S3.SS1.p1.9.m2.1.2.3.2" xref="S3.SS1.p1.9.m2.1.2.3.2a.cmml">SE</mtext><mo id="S3.SS1.p1.9.m2.1.2.3.1" xref="S3.SS1.p1.9.m2.1.2.3.1.cmml">â¢</mo><mrow id="S3.SS1.p1.9.m2.1.2.3.3.2" xref="S3.SS1.p1.9.m2.1.2.3.cmml"><mo id="S3.SS1.p1.9.m2.1.2.3.3.2.1" stretchy="false" xref="S3.SS1.p1.9.m2.1.2.3.cmml">(</mo><mn id="S3.SS1.p1.9.m2.1.1" xref="S3.SS1.p1.9.m2.1.1.cmml">3</mn><mo id="S3.SS1.p1.9.m2.1.2.3.3.2.2" stretchy="false" xref="S3.SS1.p1.9.m2.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m2.1b"><apply id="S3.SS1.p1.9.m2.1.2.cmml" xref="S3.SS1.p1.9.m2.1.2"><in id="S3.SS1.p1.9.m2.1.2.1.cmml" xref="S3.SS1.p1.9.m2.1.2.1"></in><apply id="S3.SS1.p1.9.m2.1.2.2.cmml" xref="S3.SS1.p1.9.m2.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.9.m2.1.2.2.1.cmml" xref="S3.SS1.p1.9.m2.1.2.2">superscript</csymbol><apply id="S3.SS1.p1.9.m2.1.2.2.2.cmml" xref="S3.SS1.p1.9.m2.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.9.m2.1.2.2.2.1.cmml" xref="S3.SS1.p1.9.m2.1.2.2">subscript</csymbol><ci id="S3.SS1.p1.9.m2.1.2.2.2.2.cmml" xref="S3.SS1.p1.9.m2.1.2.2.2.2">ğ“</ci><apply id="S3.SS1.p1.9.m2.1.2.2.2.3.cmml" xref="S3.SS1.p1.9.m2.1.2.2.2.3"><minus id="S3.SS1.p1.9.m2.1.2.2.2.3.1.cmml" xref="S3.SS1.p1.9.m2.1.2.2.2.3.1"></minus><ci id="S3.SS1.p1.9.m2.1.2.2.2.3.2.cmml" xref="S3.SS1.p1.9.m2.1.2.2.2.3.2">ğ‘¡</ci><cn id="S3.SS1.p1.9.m2.1.2.2.2.3.3.cmml" type="integer" xref="S3.SS1.p1.9.m2.1.2.2.2.3.3">1</cn></apply></apply><ci id="S3.SS1.p1.9.m2.1.2.2.3.cmml" xref="S3.SS1.p1.9.m2.1.2.2.3">ğ‘¡</ci></apply><apply id="S3.SS1.p1.9.m2.1.2.3.cmml" xref="S3.SS1.p1.9.m2.1.2.3"><times id="S3.SS1.p1.9.m2.1.2.3.1.cmml" xref="S3.SS1.p1.9.m2.1.2.3.1"></times><ci id="S3.SS1.p1.9.m2.1.2.3.2a.cmml" xref="S3.SS1.p1.9.m2.1.2.3.2"><mtext class="ltx_mathvariant_bold" id="S3.SS1.p1.9.m2.1.2.3.2.cmml" xref="S3.SS1.p1.9.m2.1.2.3.2">SE</mtext></ci><cn id="S3.SS1.p1.9.m2.1.1.cmml" type="integer" xref="S3.SS1.p1.9.m2.1.1">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m2.1c">\mathbf{T}_{t-1}^{t}\in\textbf{SE}(3)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.9.m2.1d">bold_T start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT âˆˆ SE ( 3 )</annotation></semantics></math>.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="294" id="S3.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S3.F2.3.2" style="font-size:90%;">Causal transformer based architecture for fusion and pose estimation.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Transformers For Fusion and Pose Estimation</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We use transformer encoder layers with causal masks to update latent vectors for pose estimation. The fusion and pose estimation module modifies each latent vector with an attention mechanism. Weighting the latent vectors based on data-dependent masks is introduced in Soft Fusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib4" title="">4</a>]</cite>, where a soft mask function is determined based on the current latent visual inertial vector.
The difference in VIFT is determining the mask based on previous measurements in the local window and applying several masks multiple times through the layers of the transformer. In the end, the corresponding vector is modified with past information to provide more accurate pose estimation.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.2">The fusion and pose estimation module of VIFT can be seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#S3.F2" title="Figure 2 â€£ 3.1 Feature Encoder â€£ 3 Method â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry"><span class="ltx_text ltx_ref_tag">2</span></a>. In Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#S3.F2" title="Figure 2 â€£ 3.1 Feature Encoder â€£ 3 Method â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry"><span class="ltx_text ltx_ref_tag">2</span></a>, the input latent vectors <math alttext="\mathbf{x}_{t},\dots,\mathbf{x}_{t-N}" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.3"><semantics id="S3.SS2.p2.1.m1.3a"><mrow id="S3.SS2.p2.1.m1.3.3.2" xref="S3.SS2.p2.1.m1.3.3.3.cmml"><msub id="S3.SS2.p2.1.m1.2.2.1.1" xref="S3.SS2.p2.1.m1.2.2.1.1.cmml"><mi id="S3.SS2.p2.1.m1.2.2.1.1.2" xref="S3.SS2.p2.1.m1.2.2.1.1.2.cmml">ğ±</mi><mi id="S3.SS2.p2.1.m1.2.2.1.1.3" xref="S3.SS2.p2.1.m1.2.2.1.1.3.cmml">t</mi></msub><mo id="S3.SS2.p2.1.m1.3.3.2.3" xref="S3.SS2.p2.1.m1.3.3.3.cmml">,</mo><mi id="S3.SS2.p2.1.m1.1.1" mathvariant="normal" xref="S3.SS2.p2.1.m1.1.1.cmml">â€¦</mi><mo id="S3.SS2.p2.1.m1.3.3.2.4" xref="S3.SS2.p2.1.m1.3.3.3.cmml">,</mo><msub id="S3.SS2.p2.1.m1.3.3.2.2" xref="S3.SS2.p2.1.m1.3.3.2.2.cmml"><mi id="S3.SS2.p2.1.m1.3.3.2.2.2" xref="S3.SS2.p2.1.m1.3.3.2.2.2.cmml">ğ±</mi><mrow id="S3.SS2.p2.1.m1.3.3.2.2.3" xref="S3.SS2.p2.1.m1.3.3.2.2.3.cmml"><mi id="S3.SS2.p2.1.m1.3.3.2.2.3.2" xref="S3.SS2.p2.1.m1.3.3.2.2.3.2.cmml">t</mi><mo id="S3.SS2.p2.1.m1.3.3.2.2.3.1" xref="S3.SS2.p2.1.m1.3.3.2.2.3.1.cmml">âˆ’</mo><mi id="S3.SS2.p2.1.m1.3.3.2.2.3.3" xref="S3.SS2.p2.1.m1.3.3.2.2.3.3.cmml">N</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.3b"><list id="S3.SS2.p2.1.m1.3.3.3.cmml" xref="S3.SS2.p2.1.m1.3.3.2"><apply id="S3.SS2.p2.1.m1.2.2.1.1.cmml" xref="S3.SS2.p2.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.2.2.1.1.1.cmml" xref="S3.SS2.p2.1.m1.2.2.1.1">subscript</csymbol><ci id="S3.SS2.p2.1.m1.2.2.1.1.2.cmml" xref="S3.SS2.p2.1.m1.2.2.1.1.2">ğ±</ci><ci id="S3.SS2.p2.1.m1.2.2.1.1.3.cmml" xref="S3.SS2.p2.1.m1.2.2.1.1.3">ğ‘¡</ci></apply><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">â€¦</ci><apply id="S3.SS2.p2.1.m1.3.3.2.2.cmml" xref="S3.SS2.p2.1.m1.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.3.3.2.2.1.cmml" xref="S3.SS2.p2.1.m1.3.3.2.2">subscript</csymbol><ci id="S3.SS2.p2.1.m1.3.3.2.2.2.cmml" xref="S3.SS2.p2.1.m1.3.3.2.2.2">ğ±</ci><apply id="S3.SS2.p2.1.m1.3.3.2.2.3.cmml" xref="S3.SS2.p2.1.m1.3.3.2.2.3"><minus id="S3.SS2.p2.1.m1.3.3.2.2.3.1.cmml" xref="S3.SS2.p2.1.m1.3.3.2.2.3.1"></minus><ci id="S3.SS2.p2.1.m1.3.3.2.2.3.2.cmml" xref="S3.SS2.p2.1.m1.3.3.2.2.3.2">ğ‘¡</ci><ci id="S3.SS2.p2.1.m1.3.3.2.2.3.3.cmml" xref="S3.SS2.p2.1.m1.3.3.2.2.3.3">ğ‘</ci></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.3c">\mathbf{x}_{t},\dots,\mathbf{x}_{t-N}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.3d">bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , â€¦ , bold_x start_POSTSUBSCRIPT italic_t - italic_N end_POSTSUBSCRIPT</annotation></semantics></math> are shown on the left. In the first step, similar to ViT, we apply linear projection to visual-inertial latent vector <math alttext="\mathbf{x}_{t}" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m2.1"><semantics id="S3.SS2.p2.2.m2.1a"><msub id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">ğ±</mi><mi id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">ğ±</ci><ci id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">\mathbf{x}_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m2.1d">bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>. This projection applies learned weights to visual and inertial encodings. We keep the embedding dimension of the transformer the same with a concatenated visual inertial latent vector.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.2">After this step, transformer Layers take <math alttext="\mathbf{x}_{t}" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.1"><semantics id="S3.SS2.p3.1.m1.1a"><msub id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml">ğ±</mi><mi id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2">ğ±</ci><ci id="S3.SS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">\mathbf{x}_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.1d">bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> together with N-1 past measurements <math alttext="\mathbf{x}_{t-1},\dots,\mathbf{x}_{t-N}" class="ltx_Math" display="inline" id="S3.SS2.p3.2.m2.3"><semantics id="S3.SS2.p3.2.m2.3a"><mrow id="S3.SS2.p3.2.m2.3.3.2" xref="S3.SS2.p3.2.m2.3.3.3.cmml"><msub id="S3.SS2.p3.2.m2.2.2.1.1" xref="S3.SS2.p3.2.m2.2.2.1.1.cmml"><mi id="S3.SS2.p3.2.m2.2.2.1.1.2" xref="S3.SS2.p3.2.m2.2.2.1.1.2.cmml">ğ±</mi><mrow id="S3.SS2.p3.2.m2.2.2.1.1.3" xref="S3.SS2.p3.2.m2.2.2.1.1.3.cmml"><mi id="S3.SS2.p3.2.m2.2.2.1.1.3.2" xref="S3.SS2.p3.2.m2.2.2.1.1.3.2.cmml">t</mi><mo id="S3.SS2.p3.2.m2.2.2.1.1.3.1" xref="S3.SS2.p3.2.m2.2.2.1.1.3.1.cmml">âˆ’</mo><mn id="S3.SS2.p3.2.m2.2.2.1.1.3.3" xref="S3.SS2.p3.2.m2.2.2.1.1.3.3.cmml">1</mn></mrow></msub><mo id="S3.SS2.p3.2.m2.3.3.2.3" xref="S3.SS2.p3.2.m2.3.3.3.cmml">,</mo><mi id="S3.SS2.p3.2.m2.1.1" mathvariant="normal" xref="S3.SS2.p3.2.m2.1.1.cmml">â€¦</mi><mo id="S3.SS2.p3.2.m2.3.3.2.4" xref="S3.SS2.p3.2.m2.3.3.3.cmml">,</mo><msub id="S3.SS2.p3.2.m2.3.3.2.2" xref="S3.SS2.p3.2.m2.3.3.2.2.cmml"><mi id="S3.SS2.p3.2.m2.3.3.2.2.2" xref="S3.SS2.p3.2.m2.3.3.2.2.2.cmml">ğ±</mi><mrow id="S3.SS2.p3.2.m2.3.3.2.2.3" xref="S3.SS2.p3.2.m2.3.3.2.2.3.cmml"><mi id="S3.SS2.p3.2.m2.3.3.2.2.3.2" xref="S3.SS2.p3.2.m2.3.3.2.2.3.2.cmml">t</mi><mo id="S3.SS2.p3.2.m2.3.3.2.2.3.1" xref="S3.SS2.p3.2.m2.3.3.2.2.3.1.cmml">âˆ’</mo><mi id="S3.SS2.p3.2.m2.3.3.2.2.3.3" xref="S3.SS2.p3.2.m2.3.3.2.2.3.3.cmml">N</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.3b"><list id="S3.SS2.p3.2.m2.3.3.3.cmml" xref="S3.SS2.p3.2.m2.3.3.2"><apply id="S3.SS2.p3.2.m2.2.2.1.1.cmml" xref="S3.SS2.p3.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.2.2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.2.2.1.1">subscript</csymbol><ci id="S3.SS2.p3.2.m2.2.2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.2.2.1.1.2">ğ±</ci><apply id="S3.SS2.p3.2.m2.2.2.1.1.3.cmml" xref="S3.SS2.p3.2.m2.2.2.1.1.3"><minus id="S3.SS2.p3.2.m2.2.2.1.1.3.1.cmml" xref="S3.SS2.p3.2.m2.2.2.1.1.3.1"></minus><ci id="S3.SS2.p3.2.m2.2.2.1.1.3.2.cmml" xref="S3.SS2.p3.2.m2.2.2.1.1.3.2">ğ‘¡</ci><cn id="S3.SS2.p3.2.m2.2.2.1.1.3.3.cmml" type="integer" xref="S3.SS2.p3.2.m2.2.2.1.1.3.3">1</cn></apply></apply><ci id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">â€¦</ci><apply id="S3.SS2.p3.2.m2.3.3.2.2.cmml" xref="S3.SS2.p3.2.m2.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.3.3.2.2.1.cmml" xref="S3.SS2.p3.2.m2.3.3.2.2">subscript</csymbol><ci id="S3.SS2.p3.2.m2.3.3.2.2.2.cmml" xref="S3.SS2.p3.2.m2.3.3.2.2.2">ğ±</ci><apply id="S3.SS2.p3.2.m2.3.3.2.2.3.cmml" xref="S3.SS2.p3.2.m2.3.3.2.2.3"><minus id="S3.SS2.p3.2.m2.3.3.2.2.3.1.cmml" xref="S3.SS2.p3.2.m2.3.3.2.2.3.1"></minus><ci id="S3.SS2.p3.2.m2.3.3.2.2.3.2.cmml" xref="S3.SS2.p3.2.m2.3.3.2.2.3.2">ğ‘¡</ci><ci id="S3.SS2.p3.2.m2.3.3.2.2.3.3.cmml" xref="S3.SS2.p3.2.m2.3.3.2.2.3.3">ğ‘</ci></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.3c">\mathbf{x}_{t-1},\dots,\mathbf{x}_{t-N}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.2.m2.3d">bold_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT , â€¦ , bold_x start_POSTSUBSCRIPT italic_t - italic_N end_POSTSUBSCRIPT</annotation></semantics></math> as input.
Then, we add Sinusoidal Positional Encodings to each latent vector according to their location in the sequence.
No temporal information flows between the linear projectionâ€™s latent visual inertial vector sequence and the Positional Encoding steps.
Transformer Layers modify the resulting latent vectors with attention by considering a local window.
In this step, latent vectors are weighted based on the previous and current measurements.
The operation of the transformer layer can be summarized as follows.
A transformer layer consists of a masked multi-head attention (MMHA) layer, feed-forward layer, residual connections, and layer normalizations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib23" title="">23</a>]</cite>.
The attention operation computes a weighted sum of value vectors based on the normalized dot product of query and key vectors. With a causal mask, we ensure that current estimates are not influenced by future measurements, which is crucial for real-time applications.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.3">After transformer layers, we apply 2-layer MLP to every feature to obtain pose output. We follow Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib27" title="">27</a>]</cite> and give a 6-dimensional output consisting of translation and Euler angles for rotation. Ultimately, we obtain <math alttext="N" class="ltx_Math" display="inline" id="S3.SS2.p4.1.m1.1"><semantics id="S3.SS2.p4.1.m1.1a"><mi id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><ci id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.1.m1.1d">italic_N</annotation></semantics></math> relative poses for <math alttext="N+1" class="ltx_Math" display="inline" id="S3.SS2.p4.2.m2.1"><semantics id="S3.SS2.p4.2.m2.1a"><mrow id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml"><mi id="S3.SS2.p4.2.m2.1.1.2" xref="S3.SS2.p4.2.m2.1.1.2.cmml">N</mi><mo id="S3.SS2.p4.2.m2.1.1.1" xref="S3.SS2.p4.2.m2.1.1.1.cmml">+</mo><mn id="S3.SS2.p4.2.m2.1.1.3" xref="S3.SS2.p4.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b"><apply id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1"><plus id="S3.SS2.p4.2.m2.1.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1.1"></plus><ci id="S3.SS2.p4.2.m2.1.1.2.cmml" xref="S3.SS2.p4.2.m2.1.1.2">ğ‘</ci><cn id="S3.SS2.p4.2.m2.1.1.3.cmml" type="integer" xref="S3.SS2.p4.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">N+1</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.2.m2.1d">italic_N + 1</annotation></semantics></math> input images.
During inference, for the first <math alttext="N+1" class="ltx_Math" display="inline" id="S3.SS2.p4.3.m3.1"><semantics id="S3.SS2.p4.3.m3.1a"><mrow id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml"><mi id="S3.SS2.p4.3.m3.1.1.2" xref="S3.SS2.p4.3.m3.1.1.2.cmml">N</mi><mo id="S3.SS2.p4.3.m3.1.1.1" xref="S3.SS2.p4.3.m3.1.1.1.cmml">+</mo><mn id="S3.SS2.p4.3.m3.1.1.3" xref="S3.SS2.p4.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.1b"><apply id="S3.SS2.p4.3.m3.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1"><plus id="S3.SS2.p4.3.m3.1.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1.1"></plus><ci id="S3.SS2.p4.3.m3.1.1.2.cmml" xref="S3.SS2.p4.3.m3.1.1.2">ğ‘</ci><cn id="S3.SS2.p4.3.m3.1.1.3.cmml" type="integer" xref="S3.SS2.p4.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.1c">N+1</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.3.m3.1d">italic_N + 1</annotation></semantics></math> images, we use all the output pose estimates to initialize the process and take only the last estimate after shifting latent vectors and adding a new latent visual inertial vector to the end of the sequence.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Deep Rotation Regression</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">The manifold structure of <span class="ltx_text ltx_font_bold" id="S3.SS3.p1.1.1">SO<math alttext="(3)" class="ltx_Math" display="inline" id="S3.SS3.p1.1.1.m1.1"><semantics id="S3.SS3.p1.1.1.m1.1a"><mrow id="S3.SS3.p1.1.1.m1.1.2.2"><mo id="S3.SS3.p1.1.1.m1.1.2.2.1" stretchy="false">(</mo><mn id="S3.SS3.p1.1.1.m1.1.1" xref="S3.SS3.p1.1.1.m1.1.1.cmml">3</mn><mo id="S3.SS3.p1.1.1.m1.1.2.2.2" stretchy="false">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.1.m1.1b"><cn id="S3.SS3.p1.1.1.m1.1.1.cmml" type="integer" xref="S3.SS3.p1.1.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.1.m1.1c">(3)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.1.m1.1d">( 3 )</annotation></semantics></math></span> space should be considered while performing optimizations on rotations. Zhou et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib29" title="">29</a>]</cite> discuss the importance of continuous representations in neural network optimization. A continuous subset of rotations can be discontinuous in the Euler angle representation of rotations, which could create discontinuous training signals in training. Moreover, the interpolation problems in Euler angles and gimbal lock phenomena further motivate using the optimization techniques for manifolds.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.2">We use the Regularized Projective Manifold Gradient (RPMG) layer proposed by Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib5" title="">5</a>]</cite> to obtain training signals for rotations, which produces manifold-aware gradients in backward passes. We first convert our 3D estimation to a 9D rotation matrix. Then, we calculate the loss between ground truth rotation <math alttext="\mathbf{R}_{\text{gt}}" class="ltx_Math" display="inline" id="S3.SS3.p2.1.m1.1"><semantics id="S3.SS3.p2.1.m1.1a"><msub id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">ğ‘</mi><mtext id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3a.cmml">gt</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">ğ‘</ci><ci id="S3.SS3.p2.1.m1.1.1.3a.cmml" xref="S3.SS3.p2.1.m1.1.1.3"><mtext id="S3.SS3.p2.1.m1.1.1.3.cmml" mathsize="70%" xref="S3.SS3.p2.1.m1.1.1.3">gt</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">\mathbf{R}_{\text{gt}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.1.m1.1d">bold_R start_POSTSUBSCRIPT gt end_POSTSUBSCRIPT</annotation></semantics></math> and estimated rotation <math alttext="\mathbf{R}" class="ltx_Math" display="inline" id="S3.SS3.p2.2.m2.1"><semantics id="S3.SS3.p2.2.m2.1a"><mi id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">ğ‘</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><ci id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">\mathbf{R}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.2.m2.1d">bold_R</annotation></semantics></math>.
For translations, we use direct regression as the space is already Euclidean. The total loss is the weighted sum of rotation and translation losses.</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{r}=||\mathbf{R}-\mathbf{R}_{\text{gt}}||,\quad\mathcal{L}_{t}=||%
\mathbf{t}-\mathbf{t}_{\text{gt}}||,\quad\mathcal{L}=\mathcal{L}_{t}+\alpha%
\mathcal{L}_{r}" class="ltx_Math" display="block" id="S3.E4.m1.2"><semantics id="S3.E4.m1.2a"><mrow id="S3.E4.m1.2.2.2" xref="S3.E4.m1.2.2.3.cmml"><mrow id="S3.E4.m1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml"><msub id="S3.E4.m1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.1.1.1.1.3.2" xref="S3.E4.m1.1.1.1.1.3.2.cmml">â„’</mi><mi id="S3.E4.m1.1.1.1.1.3.3" xref="S3.E4.m1.1.1.1.1.3.3.cmml">r</mi></msub><mo id="S3.E4.m1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E4.m1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.2.cmml"><mo id="S3.E4.m1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E4.m1.1.1.1.1.1.2.1.cmml">â€–</mo><mrow id="S3.E4.m1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.1.2.cmml">ğ‘</mi><mo id="S3.E4.m1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><msub id="S3.E4.m1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.3.2" xref="S3.E4.m1.1.1.1.1.1.1.1.3.2.cmml">ğ‘</mi><mtext id="S3.E4.m1.1.1.1.1.1.1.1.3.3" xref="S3.E4.m1.1.1.1.1.1.1.1.3.3a.cmml">gt</mtext></msub></mrow><mo id="S3.E4.m1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E4.m1.1.1.1.1.1.2.1.cmml">â€–</mo></mrow></mrow><mo id="S3.E4.m1.2.2.2.3" rspace="1.167em" xref="S3.E4.m1.2.2.3a.cmml">,</mo><mrow id="S3.E4.m1.2.2.2.2.2" xref="S3.E4.m1.2.2.2.2.3.cmml"><mrow id="S3.E4.m1.2.2.2.2.1.1" xref="S3.E4.m1.2.2.2.2.1.1.cmml"><msub id="S3.E4.m1.2.2.2.2.1.1.3" xref="S3.E4.m1.2.2.2.2.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.2.2.2.2.1.1.3.2" xref="S3.E4.m1.2.2.2.2.1.1.3.2.cmml">â„’</mi><mi id="S3.E4.m1.2.2.2.2.1.1.3.3" xref="S3.E4.m1.2.2.2.2.1.1.3.3.cmml">t</mi></msub><mo id="S3.E4.m1.2.2.2.2.1.1.2" xref="S3.E4.m1.2.2.2.2.1.1.2.cmml">=</mo><mrow id="S3.E4.m1.2.2.2.2.1.1.1.1" xref="S3.E4.m1.2.2.2.2.1.1.1.2.cmml"><mo id="S3.E4.m1.2.2.2.2.1.1.1.1.2" stretchy="false" xref="S3.E4.m1.2.2.2.2.1.1.1.2.1.cmml">â€–</mo><mrow id="S3.E4.m1.2.2.2.2.1.1.1.1.1" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.cmml"><mi id="S3.E4.m1.2.2.2.2.1.1.1.1.1.2" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.2.cmml">ğ­</mi><mo id="S3.E4.m1.2.2.2.2.1.1.1.1.1.1" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.1.cmml">âˆ’</mo><msub id="S3.E4.m1.2.2.2.2.1.1.1.1.1.3" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.cmml"><mi id="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.2" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.2.cmml">ğ­</mi><mtext id="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.3" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.3a.cmml">gt</mtext></msub></mrow><mo id="S3.E4.m1.2.2.2.2.1.1.1.1.3" stretchy="false" xref="S3.E4.m1.2.2.2.2.1.1.1.2.1.cmml">â€–</mo></mrow></mrow><mo id="S3.E4.m1.2.2.2.2.2.3" rspace="1.167em" xref="S3.E4.m1.2.2.2.2.3a.cmml">,</mo><mrow id="S3.E4.m1.2.2.2.2.2.2" xref="S3.E4.m1.2.2.2.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.2.2.2.2.2.2.2" xref="S3.E4.m1.2.2.2.2.2.2.2.cmml">â„’</mi><mo id="S3.E4.m1.2.2.2.2.2.2.1" xref="S3.E4.m1.2.2.2.2.2.2.1.cmml">=</mo><mrow id="S3.E4.m1.2.2.2.2.2.2.3" xref="S3.E4.m1.2.2.2.2.2.2.3.cmml"><msub id="S3.E4.m1.2.2.2.2.2.2.3.2" xref="S3.E4.m1.2.2.2.2.2.2.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.2.2.2.2.2.2.3.2.2" xref="S3.E4.m1.2.2.2.2.2.2.3.2.2.cmml">â„’</mi><mi id="S3.E4.m1.2.2.2.2.2.2.3.2.3" xref="S3.E4.m1.2.2.2.2.2.2.3.2.3.cmml">t</mi></msub><mo id="S3.E4.m1.2.2.2.2.2.2.3.1" xref="S3.E4.m1.2.2.2.2.2.2.3.1.cmml">+</mo><mrow id="S3.E4.m1.2.2.2.2.2.2.3.3" xref="S3.E4.m1.2.2.2.2.2.2.3.3.cmml"><mi id="S3.E4.m1.2.2.2.2.2.2.3.3.2" xref="S3.E4.m1.2.2.2.2.2.2.3.3.2.cmml">Î±</mi><mo id="S3.E4.m1.2.2.2.2.2.2.3.3.1" xref="S3.E4.m1.2.2.2.2.2.2.3.3.1.cmml">â¢</mo><msub id="S3.E4.m1.2.2.2.2.2.2.3.3.3" xref="S3.E4.m1.2.2.2.2.2.2.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.2.2.2.2.2.2.3.3.3.2" xref="S3.E4.m1.2.2.2.2.2.2.3.3.3.2.cmml">â„’</mi><mi id="S3.E4.m1.2.2.2.2.2.2.3.3.3.3" xref="S3.E4.m1.2.2.2.2.2.2.3.3.3.3.cmml">r</mi></msub></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.2b"><apply id="S3.E4.m1.2.2.3.cmml" xref="S3.E4.m1.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.3a.cmml" xref="S3.E4.m1.2.2.2.3">formulae-sequence</csymbol><apply id="S3.E4.m1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1"><eq id="S3.E4.m1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.2"></eq><apply id="S3.E4.m1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.3.2">â„’</ci><ci id="S3.E4.m1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.1.1.3.3">ğ‘Ÿ</ci></apply><apply id="S3.E4.m1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E4.m1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E4.m1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1"><minus id="S3.E4.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1"></minus><ci id="S3.E4.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.2">ğ‘</ci><apply id="S3.E4.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.3.2">ğ‘</ci><ci id="S3.E4.m1.1.1.1.1.1.1.1.3.3a.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.3.3"><mtext id="S3.E4.m1.1.1.1.1.1.1.1.3.3.cmml" mathsize="70%" xref="S3.E4.m1.1.1.1.1.1.1.1.3.3">gt</mtext></ci></apply></apply></apply></apply><apply id="S3.E4.m1.2.2.2.2.3.cmml" xref="S3.E4.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.2.2.3a.cmml" xref="S3.E4.m1.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="S3.E4.m1.2.2.2.2.1.1.cmml" xref="S3.E4.m1.2.2.2.2.1.1"><eq id="S3.E4.m1.2.2.2.2.1.1.2.cmml" xref="S3.E4.m1.2.2.2.2.1.1.2"></eq><apply id="S3.E4.m1.2.2.2.2.1.1.3.cmml" xref="S3.E4.m1.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.2.2.1.1.3.1.cmml" xref="S3.E4.m1.2.2.2.2.1.1.3">subscript</csymbol><ci id="S3.E4.m1.2.2.2.2.1.1.3.2.cmml" xref="S3.E4.m1.2.2.2.2.1.1.3.2">â„’</ci><ci id="S3.E4.m1.2.2.2.2.1.1.3.3.cmml" xref="S3.E4.m1.2.2.2.2.1.1.3.3">ğ‘¡</ci></apply><apply id="S3.E4.m1.2.2.2.2.1.1.1.2.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1"><csymbol cd="latexml" id="S3.E4.m1.2.2.2.2.1.1.1.2.1.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.2">norm</csymbol><apply id="S3.E4.m1.2.2.2.2.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1"><minus id="S3.E4.m1.2.2.2.2.1.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.1"></minus><ci id="S3.E4.m1.2.2.2.2.1.1.1.1.1.2.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.2">ğ­</ci><apply id="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.2">ğ­</ci><ci id="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.3a.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.3"><mtext id="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.3.cmml" mathsize="70%" xref="S3.E4.m1.2.2.2.2.1.1.1.1.1.3.3">gt</mtext></ci></apply></apply></apply></apply><apply id="S3.E4.m1.2.2.2.2.2.2.cmml" xref="S3.E4.m1.2.2.2.2.2.2"><eq id="S3.E4.m1.2.2.2.2.2.2.1.cmml" xref="S3.E4.m1.2.2.2.2.2.2.1"></eq><ci id="S3.E4.m1.2.2.2.2.2.2.2.cmml" xref="S3.E4.m1.2.2.2.2.2.2.2">â„’</ci><apply id="S3.E4.m1.2.2.2.2.2.2.3.cmml" xref="S3.E4.m1.2.2.2.2.2.2.3"><plus id="S3.E4.m1.2.2.2.2.2.2.3.1.cmml" xref="S3.E4.m1.2.2.2.2.2.2.3.1"></plus><apply id="S3.E4.m1.2.2.2.2.2.2.3.2.cmml" xref="S3.E4.m1.2.2.2.2.2.2.3.2"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.2.2.2.2.3.2.1.cmml" xref="S3.E4.m1.2.2.2.2.2.2.3.2">subscript</csymbol><ci id="S3.E4.m1.2.2.2.2.2.2.3.2.2.cmml" xref="S3.E4.m1.2.2.2.2.2.2.3.2.2">â„’</ci><ci id="S3.E4.m1.2.2.2.2.2.2.3.2.3.cmml" xref="S3.E4.m1.2.2.2.2.2.2.3.2.3">ğ‘¡</ci></apply><apply id="S3.E4.m1.2.2.2.2.2.2.3.3.cmml" xref="S3.E4.m1.2.2.2.2.2.2.3.3"><times id="S3.E4.m1.2.2.2.2.2.2.3.3.1.cmml" xref="S3.E4.m1.2.2.2.2.2.2.3.3.1"></times><ci id="S3.E4.m1.2.2.2.2.2.2.3.3.2.cmml" xref="S3.E4.m1.2.2.2.2.2.2.3.3.2">ğ›¼</ci><apply id="S3.E4.m1.2.2.2.2.2.2.3.3.3.cmml" xref="S3.E4.m1.2.2.2.2.2.2.3.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.2.2.2.2.3.3.3.1.cmml" xref="S3.E4.m1.2.2.2.2.2.2.3.3.3">subscript</csymbol><ci id="S3.E4.m1.2.2.2.2.2.2.3.3.3.2.cmml" xref="S3.E4.m1.2.2.2.2.2.2.3.3.3.2">â„’</ci><ci id="S3.E4.m1.2.2.2.2.2.2.3.3.3.3.cmml" xref="S3.E4.m1.2.2.2.2.2.2.3.3.3.3">ğ‘Ÿ</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.2c">\mathcal{L}_{r}=||\mathbf{R}-\mathbf{R}_{\text{gt}}||,\quad\mathcal{L}_{t}=||%
\mathbf{t}-\mathbf{t}_{\text{gt}}||,\quad\mathcal{L}=\mathcal{L}_{t}+\alpha%
\mathcal{L}_{r}</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.2d">caligraphic_L start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT = | | bold_R - bold_R start_POSTSUBSCRIPT gt end_POSTSUBSCRIPT | | , caligraphic_L start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = | | bold_t - bold_t start_POSTSUBSCRIPT gt end_POSTSUBSCRIPT | | , caligraphic_L = caligraphic_L start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_Î± caligraphic_L start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.3">where <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.SS3.p3.1.m1.1"><semantics id="S3.SS3.p3.1.m1.1a"><mi id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><ci id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.1.m1.1d">italic_Î±</annotation></semantics></math> is a constant factor to balance rotational and translational loss terms and <math alttext="||\cdot||" class="ltx_math_unparsed" display="inline" id="S3.SS3.p3.2.m2.1"><semantics id="S3.SS3.p3.2.m2.1a"><mrow id="S3.SS3.p3.2.m2.1b"><mo fence="false" id="S3.SS3.p3.2.m2.1.1" rspace="0.167em" stretchy="false">|</mo><mo fence="false" id="S3.SS3.p3.2.m2.1.2" stretchy="false">|</mo><mo id="S3.SS3.p3.2.m2.1.3" lspace="0em" rspace="0em">â‹…</mo><mo fence="false" id="S3.SS3.p3.2.m2.1.4" rspace="0.167em" stretchy="false">|</mo><mo fence="false" id="S3.SS3.p3.2.m2.1.5" stretchy="false">|</mo></mrow><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">||\cdot||</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.2.m2.1d">| | â‹… | |</annotation></semantics></math> is a norm. In the backward pass, we calculate RPMG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib5" title="">5</a>]</cite> for rotation loss, which uses Riemannian optimization to get a goal rotation <math alttext="\mathbf{R}_{g}" class="ltx_Math" display="inline" id="S3.SS3.p3.3.m3.1"><semantics id="S3.SS3.p3.3.m3.1a"><msub id="S3.SS3.p3.3.m3.1.1" xref="S3.SS3.p3.3.m3.1.1.cmml"><mi id="S3.SS3.p3.3.m3.1.1.2" xref="S3.SS3.p3.3.m3.1.1.2.cmml">ğ‘</mi><mi id="S3.SS3.p3.3.m3.1.1.3" xref="S3.SS3.p3.3.m3.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m3.1b"><apply id="S3.SS3.p3.3.m3.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.3.m3.1.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p3.3.m3.1.1.2.cmml" xref="S3.SS3.p3.3.m3.1.1.2">ğ‘</ci><ci id="S3.SS3.p3.3.m3.1.1.3.cmml" xref="S3.SS3.p3.3.m3.1.1.3">ğ‘”</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m3.1c">\mathbf{R}_{g}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.3.m3.1d">bold_R start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT</annotation></semantics></math> and maps it back to the representation manifold to find the closest element of ambient space to estimation, which is used for obtaining the gradients in the backward pass.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We evaluate VIFT and our choices in training settings and model selection. We show transformer based modules are effective in fusion and pose estimation in deep VIO.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experiment Setup</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.2">We utilize KITTI Odometry Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib10" title="">10</a>]</cite> for training and benchmarking in this study. The dataset is widely used amongst the visual-inertial odometry research community and consists of 22 sequences where those sequences have stereo-recorded images and 6-Degree-of-Freedom IMU measurements. Following <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib27" title="">27</a>]</cite>, we train our method with Sequences 00, 01, 02, 04, 06, and 09 while choosing Sequences 05, 07, and 10 for testing. We do not use Sequence 03 as it misses the IMU. The input images and ground truth poses are recorded at a rate of 10 Hz, whereas IMU data is recorded at 100 Hz. As the challenge requires monocular images, only left-camera frames are used throughout the study. For evaluation metric, relative translation error <math alttext="\textit{t}_{\textit{rel}}" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><msub id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mtext class="ltx_mathvariant_italic" id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2a.cmml">t</mtext><mtext class="ltx_mathvariant_italic" id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3a.cmml">rel</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.p1.1.m1.1.1.2a.cmml" xref="S4.SS1.p1.1.m1.1.1.2"><mtext class="ltx_mathvariant_italic" id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">t</mtext></ci><ci id="S4.SS1.p1.1.m1.1.1.3a.cmml" xref="S4.SS1.p1.1.m1.1.1.3"><mtext class="ltx_mathvariant_italic" id="S4.SS1.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="S4.SS1.p1.1.m1.1.1.3">rel</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\textit{t}_{\textit{rel}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">t start_POSTSUBSCRIPT rel end_POSTSUBSCRIPT</annotation></semantics></math> and relative rotation error <math alttext="\textit{r}_{\textit{rel}}" class="ltx_Math" display="inline" id="S4.SS1.p1.2.m2.1"><semantics id="S4.SS1.p1.2.m2.1a"><msub id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml"><mtext class="ltx_mathvariant_italic" id="S4.SS1.p1.2.m2.1.1.2" xref="S4.SS1.p1.2.m2.1.1.2a.cmml">r</mtext><mtext class="ltx_mathvariant_italic" id="S4.SS1.p1.2.m2.1.1.3" xref="S4.SS1.p1.2.m2.1.1.3a.cmml">rel</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><apply id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.p1.2.m2.1.1.2a.cmml" xref="S4.SS1.p1.2.m2.1.1.2"><mtext class="ltx_mathvariant_italic" id="S4.SS1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.p1.2.m2.1.1.2">r</mtext></ci><ci id="S4.SS1.p1.2.m2.1.1.3a.cmml" xref="S4.SS1.p1.2.m2.1.1.3"><mtext class="ltx_mathvariant_italic" id="S4.SS1.p1.2.m2.1.1.3.cmml" mathsize="70%" xref="S4.SS1.p1.2.m2.1.1.3">rel</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">\textit{r}_{\textit{rel}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.2.m2.1d">r start_POSTSUBSCRIPT rel end_POSTSUBSCRIPT</annotation></semantics></math> are calculated, indicating the averaged translation and rotation drift of all subsequences with length of (100 m, â€¦, 800 m).</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.8">We utilize pretrained FlowNet-S <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib8" title="">8</a>]</cite> based image encoder and 1D CNN based inertial encoder from Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib27" title="">27</a>]</cite>, and keep them fixed during training. Input images are resized to <math alttext="512\times 256" class="ltx_Math" display="inline" id="S4.SS1.p2.1.m1.1"><semantics id="S4.SS1.p2.1.m1.1a"><mrow id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><mn id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2.cmml">512</mn><mo id="S4.SS1.p2.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.p2.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.SS1.p2.1.m1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><times id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1.1"></times><cn id="S4.SS1.p2.1.m1.1.1.2.cmml" type="integer" xref="S4.SS1.p2.1.m1.1.1.2">512</cn><cn id="S4.SS1.p2.1.m1.1.1.3.cmml" type="integer" xref="S4.SS1.p2.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">512\times 256</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.1.m1.1d">512 Ã— 256</annotation></semantics></math> resolution. Our training scheduler follows a cosine annealing learning rate with warm restarts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib15" title="">15</a>]</cite>, with restarts occurring every 25 epochs. We employ the AdamW <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib16" title="">16</a>]</cite> optimizer with a learning rate of <math alttext="1\times 10^{-4}" class="ltx_Math" display="inline" id="S4.SS1.p2.2.m2.1"><semantics id="S4.SS1.p2.2.m2.1a"><mrow id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml"><mn id="S4.SS1.p2.2.m2.1.1.2" xref="S4.SS1.p2.2.m2.1.1.2.cmml">1</mn><mo id="S4.SS1.p2.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.p2.2.m2.1.1.1.cmml">Ã—</mo><msup id="S4.SS1.p2.2.m2.1.1.3" xref="S4.SS1.p2.2.m2.1.1.3.cmml"><mn id="S4.SS1.p2.2.m2.1.1.3.2" xref="S4.SS1.p2.2.m2.1.1.3.2.cmml">10</mn><mrow id="S4.SS1.p2.2.m2.1.1.3.3" xref="S4.SS1.p2.2.m2.1.1.3.3.cmml"><mo id="S4.SS1.p2.2.m2.1.1.3.3a" xref="S4.SS1.p2.2.m2.1.1.3.3.cmml">âˆ’</mo><mn id="S4.SS1.p2.2.m2.1.1.3.3.2" xref="S4.SS1.p2.2.m2.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><apply id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1"><times id="S4.SS1.p2.2.m2.1.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1.1"></times><cn id="S4.SS1.p2.2.m2.1.1.2.cmml" type="integer" xref="S4.SS1.p2.2.m2.1.1.2">1</cn><apply id="S4.SS1.p2.2.m2.1.1.3.cmml" xref="S4.SS1.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p2.2.m2.1.1.3.1.cmml" xref="S4.SS1.p2.2.m2.1.1.3">superscript</csymbol><cn id="S4.SS1.p2.2.m2.1.1.3.2.cmml" type="integer" xref="S4.SS1.p2.2.m2.1.1.3.2">10</cn><apply id="S4.SS1.p2.2.m2.1.1.3.3.cmml" xref="S4.SS1.p2.2.m2.1.1.3.3"><minus id="S4.SS1.p2.2.m2.1.1.3.3.1.cmml" xref="S4.SS1.p2.2.m2.1.1.3.3"></minus><cn id="S4.SS1.p2.2.m2.1.1.3.3.2.cmml" type="integer" xref="S4.SS1.p2.2.m2.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">1\times 10^{-4}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.2.m2.1d">1 Ã— 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math>, we set <math alttext="\beta_{1}=0.9" class="ltx_Math" display="inline" id="S4.SS1.p2.3.m3.1"><semantics id="S4.SS1.p2.3.m3.1a"><mrow id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml"><msub id="S4.SS1.p2.3.m3.1.1.2" xref="S4.SS1.p2.3.m3.1.1.2.cmml"><mi id="S4.SS1.p2.3.m3.1.1.2.2" xref="S4.SS1.p2.3.m3.1.1.2.2.cmml">Î²</mi><mn id="S4.SS1.p2.3.m3.1.1.2.3" xref="S4.SS1.p2.3.m3.1.1.2.3.cmml">1</mn></msub><mo id="S4.SS1.p2.3.m3.1.1.1" xref="S4.SS1.p2.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS1.p2.3.m3.1.1.3" xref="S4.SS1.p2.3.m3.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><apply id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1"><eq id="S4.SS1.p2.3.m3.1.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1.1"></eq><apply id="S4.SS1.p2.3.m3.1.1.2.cmml" xref="S4.SS1.p2.3.m3.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p2.3.m3.1.1.2.1.cmml" xref="S4.SS1.p2.3.m3.1.1.2">subscript</csymbol><ci id="S4.SS1.p2.3.m3.1.1.2.2.cmml" xref="S4.SS1.p2.3.m3.1.1.2.2">ğ›½</ci><cn id="S4.SS1.p2.3.m3.1.1.2.3.cmml" type="integer" xref="S4.SS1.p2.3.m3.1.1.2.3">1</cn></apply><cn id="S4.SS1.p2.3.m3.1.1.3.cmml" type="float" xref="S4.SS1.p2.3.m3.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">\beta_{1}=0.9</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.3.m3.1d">italic_Î² start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.9</annotation></semantics></math>, <math alttext="\beta_{2}=0.999" class="ltx_Math" display="inline" id="S4.SS1.p2.4.m4.1"><semantics id="S4.SS1.p2.4.m4.1a"><mrow id="S4.SS1.p2.4.m4.1.1" xref="S4.SS1.p2.4.m4.1.1.cmml"><msub id="S4.SS1.p2.4.m4.1.1.2" xref="S4.SS1.p2.4.m4.1.1.2.cmml"><mi id="S4.SS1.p2.4.m4.1.1.2.2" xref="S4.SS1.p2.4.m4.1.1.2.2.cmml">Î²</mi><mn id="S4.SS1.p2.4.m4.1.1.2.3" xref="S4.SS1.p2.4.m4.1.1.2.3.cmml">2</mn></msub><mo id="S4.SS1.p2.4.m4.1.1.1" xref="S4.SS1.p2.4.m4.1.1.1.cmml">=</mo><mn id="S4.SS1.p2.4.m4.1.1.3" xref="S4.SS1.p2.4.m4.1.1.3.cmml">0.999</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.4.m4.1b"><apply id="S4.SS1.p2.4.m4.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1"><eq id="S4.SS1.p2.4.m4.1.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1.1"></eq><apply id="S4.SS1.p2.4.m4.1.1.2.cmml" xref="S4.SS1.p2.4.m4.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p2.4.m4.1.1.2.1.cmml" xref="S4.SS1.p2.4.m4.1.1.2">subscript</csymbol><ci id="S4.SS1.p2.4.m4.1.1.2.2.cmml" xref="S4.SS1.p2.4.m4.1.1.2.2">ğ›½</ci><cn id="S4.SS1.p2.4.m4.1.1.2.3.cmml" type="integer" xref="S4.SS1.p2.4.m4.1.1.2.3">2</cn></apply><cn id="S4.SS1.p2.4.m4.1.1.3.cmml" type="float" xref="S4.SS1.p2.4.m4.1.1.3">0.999</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.4.m4.1c">\beta_{2}=0.999</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.4.m4.1d">italic_Î² start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.999</annotation></semantics></math>. The loss function in Equation <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#S3.E4" title="Equation 4 â€£ 3.3 Deep Rotation Regression â€£ 3 Method â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry"><span class="ltx_text ltx_ref_tag">4</span></a> includes a rotation weight of <math alttext="\alpha=40" class="ltx_Math" display="inline" id="S4.SS1.p2.5.m5.1"><semantics id="S4.SS1.p2.5.m5.1a"><mrow id="S4.SS1.p2.5.m5.1.1" xref="S4.SS1.p2.5.m5.1.1.cmml"><mi id="S4.SS1.p2.5.m5.1.1.2" xref="S4.SS1.p2.5.m5.1.1.2.cmml">Î±</mi><mo id="S4.SS1.p2.5.m5.1.1.1" xref="S4.SS1.p2.5.m5.1.1.1.cmml">=</mo><mn id="S4.SS1.p2.5.m5.1.1.3" xref="S4.SS1.p2.5.m5.1.1.3.cmml">40</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.5.m5.1b"><apply id="S4.SS1.p2.5.m5.1.1.cmml" xref="S4.SS1.p2.5.m5.1.1"><eq id="S4.SS1.p2.5.m5.1.1.1.cmml" xref="S4.SS1.p2.5.m5.1.1.1"></eq><ci id="S4.SS1.p2.5.m5.1.1.2.cmml" xref="S4.SS1.p2.5.m5.1.1.2">ğ›¼</ci><cn id="S4.SS1.p2.5.m5.1.1.3.cmml" type="integer" xref="S4.SS1.p2.5.m5.1.1.3">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.5.m5.1c">\alpha=40</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.5.m5.1d">italic_Î± = 40</annotation></semantics></math> and uses the L1 norm. We use RPMG layer for rotation estimation with <math alttext="\tau=\frac{1}{4}" class="ltx_Math" display="inline" id="S4.SS1.p2.6.m6.1"><semantics id="S4.SS1.p2.6.m6.1a"><mrow id="S4.SS1.p2.6.m6.1.1" xref="S4.SS1.p2.6.m6.1.1.cmml"><mi id="S4.SS1.p2.6.m6.1.1.2" xref="S4.SS1.p2.6.m6.1.1.2.cmml">Ï„</mi><mo id="S4.SS1.p2.6.m6.1.1.1" xref="S4.SS1.p2.6.m6.1.1.1.cmml">=</mo><mfrac id="S4.SS1.p2.6.m6.1.1.3" xref="S4.SS1.p2.6.m6.1.1.3.cmml"><mn id="S4.SS1.p2.6.m6.1.1.3.2" xref="S4.SS1.p2.6.m6.1.1.3.2.cmml">1</mn><mn id="S4.SS1.p2.6.m6.1.1.3.3" xref="S4.SS1.p2.6.m6.1.1.3.3.cmml">4</mn></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.6.m6.1b"><apply id="S4.SS1.p2.6.m6.1.1.cmml" xref="S4.SS1.p2.6.m6.1.1"><eq id="S4.SS1.p2.6.m6.1.1.1.cmml" xref="S4.SS1.p2.6.m6.1.1.1"></eq><ci id="S4.SS1.p2.6.m6.1.1.2.cmml" xref="S4.SS1.p2.6.m6.1.1.2">ğœ</ci><apply id="S4.SS1.p2.6.m6.1.1.3.cmml" xref="S4.SS1.p2.6.m6.1.1.3"><divide id="S4.SS1.p2.6.m6.1.1.3.1.cmml" xref="S4.SS1.p2.6.m6.1.1.3"></divide><cn id="S4.SS1.p2.6.m6.1.1.3.2.cmml" type="integer" xref="S4.SS1.p2.6.m6.1.1.3.2">1</cn><cn id="S4.SS1.p2.6.m6.1.1.3.3.cmml" type="integer" xref="S4.SS1.p2.6.m6.1.1.3.3">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.6.m6.1c">\tau=\frac{1}{4}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.6.m6.1d">italic_Ï„ = divide start_ARG 1 end_ARG start_ARG 4 end_ARG</annotation></semantics></math> and <math alttext="\lambda=0.01" class="ltx_Math" display="inline" id="S4.SS1.p2.7.m7.1"><semantics id="S4.SS1.p2.7.m7.1a"><mrow id="S4.SS1.p2.7.m7.1.1" xref="S4.SS1.p2.7.m7.1.1.cmml"><mi id="S4.SS1.p2.7.m7.1.1.2" xref="S4.SS1.p2.7.m7.1.1.2.cmml">Î»</mi><mo id="S4.SS1.p2.7.m7.1.1.1" xref="S4.SS1.p2.7.m7.1.1.1.cmml">=</mo><mn id="S4.SS1.p2.7.m7.1.1.3" xref="S4.SS1.p2.7.m7.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.7.m7.1b"><apply id="S4.SS1.p2.7.m7.1.1.cmml" xref="S4.SS1.p2.7.m7.1.1"><eq id="S4.SS1.p2.7.m7.1.1.1.cmml" xref="S4.SS1.p2.7.m7.1.1.1"></eq><ci id="S4.SS1.p2.7.m7.1.1.2.cmml" xref="S4.SS1.p2.7.m7.1.1.2">ğœ†</ci><cn id="S4.SS1.p2.7.m7.1.1.3.cmml" type="float" xref="S4.SS1.p2.7.m7.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.7.m7.1c">\lambda=0.01</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.7.m7.1d">italic_Î» = 0.01</annotation></semantics></math>. We use a sequence length of <math alttext="N=11" class="ltx_Math" display="inline" id="S4.SS1.p2.8.m8.1"><semantics id="S4.SS1.p2.8.m8.1a"><mrow id="S4.SS1.p2.8.m8.1.1" xref="S4.SS1.p2.8.m8.1.1.cmml"><mi id="S4.SS1.p2.8.m8.1.1.2" xref="S4.SS1.p2.8.m8.1.1.2.cmml">N</mi><mo id="S4.SS1.p2.8.m8.1.1.1" xref="S4.SS1.p2.8.m8.1.1.1.cmml">=</mo><mn id="S4.SS1.p2.8.m8.1.1.3" xref="S4.SS1.p2.8.m8.1.1.3.cmml">11</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.8.m8.1b"><apply id="S4.SS1.p2.8.m8.1.1.cmml" xref="S4.SS1.p2.8.m8.1.1"><eq id="S4.SS1.p2.8.m8.1.1.1.cmml" xref="S4.SS1.p2.8.m8.1.1.1"></eq><ci id="S4.SS1.p2.8.m8.1.1.2.cmml" xref="S4.SS1.p2.8.m8.1.1.2">ğ‘</ci><cn id="S4.SS1.p2.8.m8.1.1.3.cmml" type="integer" xref="S4.SS1.p2.8.m8.1.1.3">11</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.8.m8.1c">N=11</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.8.m8.1d">italic_N = 11</annotation></semantics></math> during training.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">Our transformer model is configured with an embedding dimension of 768 and a feed-forward layer dimension of 128. The embedding dimension is selected to be equal to the sum of 512 and 256, which are the output dimensions of the image encoder and inertial encoder, respectively. The model uses Sinusoidal Positional Encodings, consists of 4 transformer encoder layers with 6 attention heads, and employs masked self-attention with a causal mask. We do not apply dropout in the transformer. The network is trained for 200 epochs with a batch size of 128, totaling 27k training steps. Experiments are conducted using an NVIDIA GeForce RTX 4060 Laptop GPU.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Main Results</h3>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.10.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S4.T1.11.2" style="font-size:90%;">Comparison with prior VIO works in translational &amp; rotational error metrics of KITTI Odometry Benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib10" title="">10</a>]</cite>. The best performances in each block are marked in <span class="ltx_text ltx_font_bold" id="S4.T1.11.2.1">bold</span> and overall bests are shown with <span class="ltx_text" id="S4.T1.11.2.2" style="background-color:#BFFFBF;">green</span> background. Loop closure is excluded for VINS-Mono. Results are taken from Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib27" title="">27</a>]</cite> except ours.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.6">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.6.7.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T1.6.7.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.6.7.1.2" rowspan="2"><span class="ltx_text" id="S4.T1.6.7.1.2.1">Method</span></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" colspan="2" id="S4.T1.6.7.1.3">Seq. 05</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" colspan="2" id="S4.T1.6.7.1.4">Seq. 07</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="2" id="S4.T1.6.7.1.5">Seq. 10</th>
</tr>
<tr class="ltx_tr" id="S4.T1.6.6">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T1.6.6.7"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.1.1"><math alttext="t_{rel}(\%)" class="ltx_math_unparsed" display="inline" id="S4.T1.1.1.1.m1.1"><semantics id="S4.T1.1.1.1.m1.1a"><mrow id="S4.T1.1.1.1.m1.1b"><msub id="S4.T1.1.1.1.m1.1.1"><mi id="S4.T1.1.1.1.m1.1.1.2" mathsize="50%">t</mi><mrow id="S4.T1.1.1.1.m1.1.1.3"><mi id="S4.T1.1.1.1.m1.1.1.3.2" mathsize="50%">r</mi><mo id="S4.T1.1.1.1.m1.1.1.3.1">â¢</mo><mi id="S4.T1.1.1.1.m1.1.1.3.3" mathsize="50%">e</mi><mo id="S4.T1.1.1.1.m1.1.1.3.1a">â¢</mo><mi id="S4.T1.1.1.1.m1.1.1.3.4" mathsize="50%">l</mi></mrow></msub><mrow id="S4.T1.1.1.1.m1.1.2"><mo id="S4.T1.1.1.1.m1.1.2.1" maxsize="50%" minsize="50%">(</mo><mo id="S4.T1.1.1.1.m1.1.2.2" mathsize="50%">%</mo><mo id="S4.T1.1.1.1.m1.1.2.3" maxsize="50%" minsize="50%">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">t_{rel}(\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T1.1.1.1.m1.1d">italic_t start_POSTSUBSCRIPT italic_r italic_e italic_l end_POSTSUBSCRIPT ( % )</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.2.2.2"><math alttext="r_{rel}(^{\circ})" class="ltx_math_unparsed" display="inline" id="S4.T1.2.2.2.m1.1"><semantics id="S4.T1.2.2.2.m1.1a"><mrow id="S4.T1.2.2.2.m1.1b"><msub id="S4.T1.2.2.2.m1.1.1"><mi id="S4.T1.2.2.2.m1.1.1.2" mathsize="50%">r</mi><mrow id="S4.T1.2.2.2.m1.1.1.3"><mi id="S4.T1.2.2.2.m1.1.1.3.2" mathsize="50%">r</mi><mo id="S4.T1.2.2.2.m1.1.1.3.1">â¢</mo><mi id="S4.T1.2.2.2.m1.1.1.3.3" mathsize="50%">e</mi><mo id="S4.T1.2.2.2.m1.1.1.3.1a">â¢</mo><mi id="S4.T1.2.2.2.m1.1.1.3.4" mathsize="50%">l</mi></mrow></msub><mrow id="S4.T1.2.2.2.m1.1.2"><msup id="S4.T1.2.2.2.m1.1.2.1"><mo id="S4.T1.2.2.2.m1.1.2.1.2" maxsize="50%" minsize="50%">(</mo><mo id="S4.T1.2.2.2.m1.1.2.1.3" mathsize="50%">âˆ˜</mo></msup><mo id="S4.T1.2.2.2.m1.1.2.2" maxsize="50%" minsize="50%">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.T1.2.2.2.m1.1c">r_{rel}(^{\circ})</annotation><annotation encoding="application/x-llamapun" id="S4.T1.2.2.2.m1.1d">italic_r start_POSTSUBSCRIPT italic_r italic_e italic_l end_POSTSUBSCRIPT ( start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT )</annotation></semantics></math></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.3.3.3"><math alttext="t_{rel}(\%)" class="ltx_math_unparsed" display="inline" id="S4.T1.3.3.3.m1.1"><semantics id="S4.T1.3.3.3.m1.1a"><mrow id="S4.T1.3.3.3.m1.1b"><msub id="S4.T1.3.3.3.m1.1.1"><mi id="S4.T1.3.3.3.m1.1.1.2" mathsize="50%">t</mi><mrow id="S4.T1.3.3.3.m1.1.1.3"><mi id="S4.T1.3.3.3.m1.1.1.3.2" mathsize="50%">r</mi><mo id="S4.T1.3.3.3.m1.1.1.3.1">â¢</mo><mi id="S4.T1.3.3.3.m1.1.1.3.3" mathsize="50%">e</mi><mo id="S4.T1.3.3.3.m1.1.1.3.1a">â¢</mo><mi id="S4.T1.3.3.3.m1.1.1.3.4" mathsize="50%">l</mi></mrow></msub><mrow id="S4.T1.3.3.3.m1.1.2"><mo id="S4.T1.3.3.3.m1.1.2.1" maxsize="50%" minsize="50%">(</mo><mo id="S4.T1.3.3.3.m1.1.2.2" mathsize="50%">%</mo><mo id="S4.T1.3.3.3.m1.1.2.3" maxsize="50%" minsize="50%">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.T1.3.3.3.m1.1c">t_{rel}(\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T1.3.3.3.m1.1d">italic_t start_POSTSUBSCRIPT italic_r italic_e italic_l end_POSTSUBSCRIPT ( % )</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.4.4.4"><math alttext="r_{rel}(^{\circ})" class="ltx_math_unparsed" display="inline" id="S4.T1.4.4.4.m1.1"><semantics id="S4.T1.4.4.4.m1.1a"><mrow id="S4.T1.4.4.4.m1.1b"><msub id="S4.T1.4.4.4.m1.1.1"><mi id="S4.T1.4.4.4.m1.1.1.2" mathsize="50%">r</mi><mrow id="S4.T1.4.4.4.m1.1.1.3"><mi id="S4.T1.4.4.4.m1.1.1.3.2" mathsize="50%">r</mi><mo id="S4.T1.4.4.4.m1.1.1.3.1">â¢</mo><mi id="S4.T1.4.4.4.m1.1.1.3.3" mathsize="50%">e</mi><mo id="S4.T1.4.4.4.m1.1.1.3.1a">â¢</mo><mi id="S4.T1.4.4.4.m1.1.1.3.4" mathsize="50%">l</mi></mrow></msub><mrow id="S4.T1.4.4.4.m1.1.2"><msup id="S4.T1.4.4.4.m1.1.2.1"><mo id="S4.T1.4.4.4.m1.1.2.1.2" maxsize="50%" minsize="50%">(</mo><mo id="S4.T1.4.4.4.m1.1.2.1.3" mathsize="50%">âˆ˜</mo></msup><mo id="S4.T1.4.4.4.m1.1.2.2" maxsize="50%" minsize="50%">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.T1.4.4.4.m1.1c">r_{rel}(^{\circ})</annotation><annotation encoding="application/x-llamapun" id="S4.T1.4.4.4.m1.1d">italic_r start_POSTSUBSCRIPT italic_r italic_e italic_l end_POSTSUBSCRIPT ( start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT )</annotation></semantics></math></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.5.5.5"><math alttext="t_{rel}(\%)" class="ltx_math_unparsed" display="inline" id="S4.T1.5.5.5.m1.1"><semantics id="S4.T1.5.5.5.m1.1a"><mrow id="S4.T1.5.5.5.m1.1b"><msub id="S4.T1.5.5.5.m1.1.1"><mi id="S4.T1.5.5.5.m1.1.1.2" mathsize="50%">t</mi><mrow id="S4.T1.5.5.5.m1.1.1.3"><mi id="S4.T1.5.5.5.m1.1.1.3.2" mathsize="50%">r</mi><mo id="S4.T1.5.5.5.m1.1.1.3.1">â¢</mo><mi id="S4.T1.5.5.5.m1.1.1.3.3" mathsize="50%">e</mi><mo id="S4.T1.5.5.5.m1.1.1.3.1a">â¢</mo><mi id="S4.T1.5.5.5.m1.1.1.3.4" mathsize="50%">l</mi></mrow></msub><mrow id="S4.T1.5.5.5.m1.1.2"><mo id="S4.T1.5.5.5.m1.1.2.1" maxsize="50%" minsize="50%">(</mo><mo id="S4.T1.5.5.5.m1.1.2.2" mathsize="50%">%</mo><mo id="S4.T1.5.5.5.m1.1.2.3" maxsize="50%" minsize="50%">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.T1.5.5.5.m1.1c">t_{rel}(\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T1.5.5.5.m1.1d">italic_t start_POSTSUBSCRIPT italic_r italic_e italic_l end_POSTSUBSCRIPT ( % )</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center" id="S4.T1.6.6.6"><math alttext="r_{rel}(^{\circ})" class="ltx_math_unparsed" display="inline" id="S4.T1.6.6.6.m1.1"><semantics id="S4.T1.6.6.6.m1.1a"><mrow id="S4.T1.6.6.6.m1.1b"><msub id="S4.T1.6.6.6.m1.1.1"><mi id="S4.T1.6.6.6.m1.1.1.2" mathsize="50%">r</mi><mrow id="S4.T1.6.6.6.m1.1.1.3"><mi id="S4.T1.6.6.6.m1.1.1.3.2" mathsize="50%">r</mi><mo id="S4.T1.6.6.6.m1.1.1.3.1">â¢</mo><mi id="S4.T1.6.6.6.m1.1.1.3.3" mathsize="50%">e</mi><mo id="S4.T1.6.6.6.m1.1.1.3.1a">â¢</mo><mi id="S4.T1.6.6.6.m1.1.1.3.4" mathsize="50%">l</mi></mrow></msub><mrow id="S4.T1.6.6.6.m1.1.2"><msup id="S4.T1.6.6.6.m1.1.2.1"><mo id="S4.T1.6.6.6.m1.1.2.1.2" maxsize="50%" minsize="50%">(</mo><mo id="S4.T1.6.6.6.m1.1.2.1.3" mathsize="50%">âˆ˜</mo></msup><mo id="S4.T1.6.6.6.m1.1.2.2" maxsize="50%" minsize="50%">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.T1.6.6.6.m1.1c">r_{rel}(^{\circ})</annotation><annotation encoding="application/x-llamapun" id="S4.T1.6.6.6.m1.1d">italic_r start_POSTSUBSCRIPT italic_r italic_e italic_l end_POSTSUBSCRIPT ( start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT )</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.8.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" id="S4.T1.6.8.2.1">Geo</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.6.8.2.2">VINS-Mono <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib18" title="">18</a>]</cite>
</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" id="S4.T1.6.8.2.3"><span class="ltx_text ltx_font_bold" id="S4.T1.6.8.2.3.1">11.6</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.6.8.2.4"><span class="ltx_text ltx_font_bold" id="S4.T1.6.8.2.4.1">1.26</span></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" id="S4.T1.6.8.2.5"><span class="ltx_text ltx_font_bold" id="S4.T1.6.8.2.5.1">10.0</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.6.8.2.6"><span class="ltx_text ltx_font_bold" id="S4.T1.6.8.2.6.1">1.72</span></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" id="S4.T1.6.8.2.7"><span class="ltx_text ltx_font_bold" id="S4.T1.6.8.2.7.1">16.5</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.6.8.2.8"><span class="ltx_text ltx_font_bold" id="S4.T1.6.8.2.8.1">2.34</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.9.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T1.6.9.3.1" rowspan="2"><span class="ltx_text" id="S4.T1.6.9.3.1.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.6.9.3.1.1.1">
<span class="ltx_tr" id="S4.T1.6.9.3.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.6.9.3.1.1.1.1.1">Self-</span></span>
<span class="ltx_tr" id="S4.T1.6.9.3.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.6.9.3.1.1.1.2.1">Sup.</span></span>
</span></span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.6.9.3.2">VIOLearner <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib19" title="">19</a>]</cite>
</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T1.6.9.3.3">3.00</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.6.9.3.4"><span class="ltx_text ltx_font_bold" id="S4.T1.6.9.3.4.1">1.40</span></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T1.6.9.3.5">3.60</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.6.9.3.6">2.06</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T1.6.9.3.7">2.04</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.6.9.3.8">1.37</td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.10.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.10.4.1">DeepVIO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib12" title="">12</a>]</cite>
</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.6.10.4.2"><span class="ltx_text ltx_font_bold" id="S4.T1.6.10.4.2.1">2.86</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.10.4.3">2.32</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.6.10.4.4"><span class="ltx_text ltx_font_bold" id="S4.T1.6.10.4.4.1">2.71</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.10.4.5"><span class="ltx_text ltx_font_bold" id="S4.T1.6.10.4.5.1">1.66</span></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.6.10.4.6" style="background-color:#BFFFBF;"><span class="ltx_text ltx_font_bold" id="S4.T1.6.10.4.6.1" style="background-color:#BFFFBF;">0.85</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.6.10.4.7"><span class="ltx_text ltx_font_bold" id="S4.T1.6.10.4.7.1">1.03</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.11.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_t" id="S4.T1.6.11.5.1" rowspan="6"><span class="ltx_text" id="S4.T1.6.11.5.1.1">Sup.</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.6.11.5.2">ATVIO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib14" title="">14</a>]</cite>
</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T1.6.11.5.3">4.93</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.6.11.5.4">2.4</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T1.6.11.5.5">3.78</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.6.11.5.6">2.59</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T1.6.11.5.7">5.71</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.6.11.5.8">2.96</td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.12.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.12.6.1">Soft Fusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib4" title="">4</a>]</cite>
</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.6.12.6.2">4.44</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.12.6.3">1.69</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.6.12.6.4">2.95</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.12.6.5">1.32</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.6.12.6.6">3.41</th>
<td class="ltx_td ltx_align_center" id="S4.T1.6.12.6.7">1.41</td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.13.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.13.7.1">Hard Fusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib4" title="">4</a>]</cite>
</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.6.13.7.2">4.11</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.13.7.3">1.49</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.6.13.7.4">3.44</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.13.7.5">1.86</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.6.13.7.6"><span class="ltx_text ltx_font_bold" id="S4.T1.6.13.7.6.1">1.51</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.6.13.7.7">0.91</td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.14.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.14.8.1">Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib27" title="">27</a>]</cite>
</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.6.14.8.2">2.01</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.14.8.3">0.75</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.6.14.8.4">1.79</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.14.8.5">0.76</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.6.14.8.6">3.41</th>
<td class="ltx_td ltx_align_center" id="S4.T1.6.14.8.7">1.08</td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.15.9">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.6.15.9.1">(Ours) Baseline</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T1.6.15.9.2" style="background-color:#BFFFBF;"><span class="ltx_text ltx_font_bold" id="S4.T1.6.15.9.2.1" style="background-color:#BFFFBF;">1.93</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.6.15.9.3">0.68</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T1.6.15.9.4" style="background-color:#BFFFBF;"><span class="ltx_text ltx_font_bold" id="S4.T1.6.15.9.4.1" style="background-color:#BFFFBF;">1.55</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.6.15.9.5">0.91</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T1.6.15.9.6">2.57</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.6.15.9.7">0.54</td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.16.10">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T1.6.16.10.1">(Ours) w. RPMG</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="S4.T1.6.16.10.2">2.02</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T1.6.16.10.3" style="background-color:#BFFFBF;"><span class="ltx_text ltx_font_bold" id="S4.T1.6.16.10.3.1" style="background-color:#BFFFBF;">0.53</span></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="S4.T1.6.16.10.4">1.75</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T1.6.16.10.5" style="background-color:#BFFFBF;"><span class="ltx_text ltx_font_bold" id="S4.T1.6.16.10.5.1" style="background-color:#BFFFBF;">0.47</span></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="S4.T1.6.16.10.6">2.11</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.6.16.10.7" style="background-color:#BFFFBF;"><span class="ltx_text ltx_font_bold" id="S4.T1.6.16.10.7.1" style="background-color:#BFFFBF;">0.39</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.2">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#S4.T1" title="Table 1 â€£ 4.2 Main Results â€£ 4 Experiments â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry"><span class="ltx_text ltx_ref_tag">1</span></a> compares our method against geometry-based, self-supervised, and supervised VIO approaches. Self-supervised methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib1" title="">1</a>]</cite> are trained on KITTI sequences <math alttext="00" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><mn id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">00</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><cn id="S4.SS2.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS2.p1.1.m1.1.1">00</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">00</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">00</annotation></semantics></math> to <math alttext="08" class="ltx_Math" display="inline" id="S4.SS2.p1.2.m2.1"><semantics id="S4.SS2.p1.2.m2.1a"><mn id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">08</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><cn id="S4.SS2.p1.2.m2.1.1.cmml" type="integer" xref="S4.SS2.p1.2.m2.1.1">08</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">08</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.2.m2.1d">08</annotation></semantics></math>. We include supervised methods that utilize the same training and testing splits as our approach <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib27" title="">27</a>]</cite> on the KITTI dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib10" title="">10</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Monocular VIO with geometry-based methods requires excitation of all axes in initialization to correctly determine IMU biases and scale. Cars in the KITTI dataset mostly move forward and rotate in the yaw axis. This type of motion makes it hard to evaluate their performance fairly for VIO methods that require IMU initialization. ORB-SLAM3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib3" title="">3</a>]</cite> does not initialize in monocular inertial mode, and VINS-Mono <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib18" title="">18</a>]</cite> produces high errors even if it can initialize.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.5">As seen in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#S4.T1" title="Table 1 â€£ 4.2 Main Results â€£ 4 Experiments â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry"><span class="ltx_text ltx_ref_tag">1</span></a>, VIFT obtains state-of-the-art performance compared to learning-based methods. VIFT, without other additional modules, provides the lowest translation errors in Sequences <math alttext="05" class="ltx_Math" display="inline" id="S4.SS2.p3.1.m1.1"><semantics id="S4.SS2.p3.1.m1.1a"><mn id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml">05</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><cn id="S4.SS2.p3.1.m1.1.1.cmml" type="integer" xref="S4.SS2.p3.1.m1.1.1">05</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">05</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.1.m1.1d">05</annotation></semantics></math> and <math alttext="07" class="ltx_Math" display="inline" id="S4.SS2.p3.2.m2.1"><semantics id="S4.SS2.p3.2.m2.1a"><mn id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml">07</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><cn id="S4.SS2.p3.2.m2.1.1.cmml" type="integer" xref="S4.SS2.p3.2.m2.1.1">07</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">07</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.2.m2.1d">07</annotation></semantics></math> and the lowest rotation errors in Sequences <math alttext="05" class="ltx_Math" display="inline" id="S4.SS2.p3.3.m3.1"><semantics id="S4.SS2.p3.3.m3.1a"><mn id="S4.SS2.p3.3.m3.1.1" xref="S4.SS2.p3.3.m3.1.1.cmml">05</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.3.m3.1b"><cn id="S4.SS2.p3.3.m3.1.1.cmml" type="integer" xref="S4.SS2.p3.3.m3.1.1">05</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.3.m3.1c">05</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.3.m3.1d">05</annotation></semantics></math> and <math alttext="10" class="ltx_Math" display="inline" id="S4.SS2.p3.4.m4.1"><semantics id="S4.SS2.p3.4.m4.1a"><mn id="S4.SS2.p3.4.m4.1.1" xref="S4.SS2.p3.4.m4.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.4.m4.1b"><cn id="S4.SS2.p3.4.m4.1.1.cmml" type="integer" xref="S4.SS2.p3.4.m4.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.4.m4.1c">10</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.4.m4.1d">10</annotation></semantics></math> while obtaining comparable performances in other metrics. Moreover, with RPMG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib5" title="">5</a>]</cite>, VIFT decreases the rotation error by <math alttext="\approx 63.8\%" class="ltx_Math" display="inline" id="S4.SS2.p3.5.m5.1"><semantics id="S4.SS2.p3.5.m5.1a"><mrow id="S4.SS2.p3.5.m5.1.1" xref="S4.SS2.p3.5.m5.1.1.cmml"><mi id="S4.SS2.p3.5.m5.1.1.2" xref="S4.SS2.p3.5.m5.1.1.2.cmml"></mi><mo id="S4.SS2.p3.5.m5.1.1.1" xref="S4.SS2.p3.5.m5.1.1.1.cmml">â‰ˆ</mo><mrow id="S4.SS2.p3.5.m5.1.1.3" xref="S4.SS2.p3.5.m5.1.1.3.cmml"><mn id="S4.SS2.p3.5.m5.1.1.3.2" xref="S4.SS2.p3.5.m5.1.1.3.2.cmml">63.8</mn><mo id="S4.SS2.p3.5.m5.1.1.3.1" xref="S4.SS2.p3.5.m5.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.5.m5.1b"><apply id="S4.SS2.p3.5.m5.1.1.cmml" xref="S4.SS2.p3.5.m5.1.1"><approx id="S4.SS2.p3.5.m5.1.1.1.cmml" xref="S4.SS2.p3.5.m5.1.1.1"></approx><csymbol cd="latexml" id="S4.SS2.p3.5.m5.1.1.2.cmml" xref="S4.SS2.p3.5.m5.1.1.2">absent</csymbol><apply id="S4.SS2.p3.5.m5.1.1.3.cmml" xref="S4.SS2.p3.5.m5.1.1.3"><csymbol cd="latexml" id="S4.SS2.p3.5.m5.1.1.3.1.cmml" xref="S4.SS2.p3.5.m5.1.1.3.1">percent</csymbol><cn id="S4.SS2.p3.5.m5.1.1.3.2.cmml" type="float" xref="S4.SS2.p3.5.m5.1.1.3.2">63.8</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.5.m5.1c">\approx 63.8\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.5.m5.1d">â‰ˆ 63.8 %</annotation></semantics></math> in test Sequence 10 compared to Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib27" title="">27</a>]</cite>. Our experiments show transformer-based fusion and pose estimation surpass the performance of methods that use the same visual and inertial features with the RNN-based networks.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Ablation Study</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">In this section, we look at the effect of different modules to understand the performance of VIFT. We show the KITTI evaluation metric results in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#S4.T2" title="Table 2 â€£ 4.3 Ablation Study â€£ 4 Experiments â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry"><span class="ltx_text ltx_ref_tag">2</span></a> and plot the estimated trajectories against ground truth trajectories in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#S4.F3" title="Figure 3 â€£ 4.3 Ablation Study â€£ 4 Experiments â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry"><span class="ltx_text ltx_ref_tag">3</span></a>. We mark trajectories every 5 seconds to obtain intuition about the vehicleâ€™s speed along the trajectory and to make it easy to distinguish results. We emphasize that the camera and IMU provide 10 FPS and 100 Hz measurements, respectively, which are much more frequent than marked locations. We show the estimated trajectory in test sequences from above in the top row and vertical trajectory versus the bottom row. All trajectories start from the origin, and relative pose estimates from VIFT are applied sequentially to obtain absolute pose estimates for each time index.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.10.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S4.T2.11.2" style="font-size:90%;">Ablation study. Modified parts from VIFT model are shown with <span class="ltx_text" id="S4.T2.11.2.1" style="background-color:#BFFFBF;">green</span> background. Best results are shown <span class="ltx_text ltx_font_bold" id="S4.T2.11.2.2">bold</span>.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.6.7.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.6.7.1.1"><span class="ltx_text" id="S4.T2.6.7.1.1.1" style="font-size:90%;">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.6.7.1.2"><span class="ltx_text" id="S4.T2.6.7.1.2.1" style="font-size:90%;">Sequence</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.6.7.1.3" rowspan="2"><span class="ltx_text" id="S4.T2.6.7.1.3.1" style="font-size:90%;">Criterion</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.6.7.1.4"><span class="ltx_text" id="S4.T2.6.7.1.4.1" style="font-size:90%;">Data</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.6.7.1.5" rowspan="2"><span class="ltx_text" id="S4.T2.6.7.1.5.1" style="font-size:90%;">RPMG</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2" id="S4.T2.6.7.1.6"><span class="ltx_text" id="S4.T2.6.7.1.6.1" style="font-size:90%;">Seq. 05</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2" id="S4.T2.6.7.1.7"><span class="ltx_text" id="S4.T2.6.7.1.7.1" style="font-size:90%;">Seq. 07</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2" id="S4.T2.6.7.1.8"><span class="ltx_text" id="S4.T2.6.7.1.8.1" style="font-size:90%;">Seq. 10</span></th>
</tr>
<tr class="ltx_tr" id="S4.T2.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T2.6.6.7"><span class="ltx_text" id="S4.T2.6.6.7.1" style="font-size:90%;">Type</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T2.6.6.8"><span class="ltx_text" id="S4.T2.6.6.8.1" style="font-size:90%;">Length</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T2.6.6.9"><span class="ltx_text" id="S4.T2.6.6.9.1" style="font-size:90%;">Balancing</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.1.1.1"><math alttext="t_{rel}(\%)" class="ltx_math_unparsed" display="inline" id="S4.T2.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.m1.1a"><mrow id="S4.T2.1.1.1.m1.1b"><msub id="S4.T2.1.1.1.m1.1.1"><mi id="S4.T2.1.1.1.m1.1.1.2" mathsize="50%">t</mi><mrow id="S4.T2.1.1.1.m1.1.1.3"><mi id="S4.T2.1.1.1.m1.1.1.3.2" mathsize="50%">r</mi><mo id="S4.T2.1.1.1.m1.1.1.3.1">â¢</mo><mi id="S4.T2.1.1.1.m1.1.1.3.3" mathsize="50%">e</mi><mo id="S4.T2.1.1.1.m1.1.1.3.1a">â¢</mo><mi id="S4.T2.1.1.1.m1.1.1.3.4" mathsize="50%">l</mi></mrow></msub><mrow id="S4.T2.1.1.1.m1.1.2"><mo id="S4.T2.1.1.1.m1.1.2.1" maxsize="50%" minsize="50%">(</mo><mo id="S4.T2.1.1.1.m1.1.2.2" mathsize="50%">%</mo><mo id="S4.T2.1.1.1.m1.1.2.3" maxsize="50%" minsize="50%">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">t_{rel}(\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.m1.1d">italic_t start_POSTSUBSCRIPT italic_r italic_e italic_l end_POSTSUBSCRIPT ( % )</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T2.2.2.2"><math alttext="r_{rel}(^{\circ})" class="ltx_math_unparsed" display="inline" id="S4.T2.2.2.2.m1.1"><semantics id="S4.T2.2.2.2.m1.1a"><mrow id="S4.T2.2.2.2.m1.1b"><msub id="S4.T2.2.2.2.m1.1.1"><mi id="S4.T2.2.2.2.m1.1.1.2" mathsize="50%">r</mi><mrow id="S4.T2.2.2.2.m1.1.1.3"><mi id="S4.T2.2.2.2.m1.1.1.3.2" mathsize="50%">r</mi><mo id="S4.T2.2.2.2.m1.1.1.3.1">â¢</mo><mi id="S4.T2.2.2.2.m1.1.1.3.3" mathsize="50%">e</mi><mo id="S4.T2.2.2.2.m1.1.1.3.1a">â¢</mo><mi id="S4.T2.2.2.2.m1.1.1.3.4" mathsize="50%">l</mi></mrow></msub><mrow id="S4.T2.2.2.2.m1.1.2"><msup id="S4.T2.2.2.2.m1.1.2.1"><mo id="S4.T2.2.2.2.m1.1.2.1.2" maxsize="50%" minsize="50%">(</mo><mo id="S4.T2.2.2.2.m1.1.2.1.3" mathsize="50%">âˆ˜</mo></msup><mo id="S4.T2.2.2.2.m1.1.2.2" maxsize="50%" minsize="50%">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.T2.2.2.2.m1.1c">r_{rel}(^{\circ})</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.2.m1.1d">italic_r start_POSTSUBSCRIPT italic_r italic_e italic_l end_POSTSUBSCRIPT ( start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT )</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.3.3.3"><math alttext="t_{rel}(\%)" class="ltx_math_unparsed" display="inline" id="S4.T2.3.3.3.m1.1"><semantics id="S4.T2.3.3.3.m1.1a"><mrow id="S4.T2.3.3.3.m1.1b"><msub id="S4.T2.3.3.3.m1.1.1"><mi id="S4.T2.3.3.3.m1.1.1.2" mathsize="50%">t</mi><mrow id="S4.T2.3.3.3.m1.1.1.3"><mi id="S4.T2.3.3.3.m1.1.1.3.2" mathsize="50%">r</mi><mo id="S4.T2.3.3.3.m1.1.1.3.1">â¢</mo><mi id="S4.T2.3.3.3.m1.1.1.3.3" mathsize="50%">e</mi><mo id="S4.T2.3.3.3.m1.1.1.3.1a">â¢</mo><mi id="S4.T2.3.3.3.m1.1.1.3.4" mathsize="50%">l</mi></mrow></msub><mrow id="S4.T2.3.3.3.m1.1.2"><mo id="S4.T2.3.3.3.m1.1.2.1" maxsize="50%" minsize="50%">(</mo><mo id="S4.T2.3.3.3.m1.1.2.2" mathsize="50%">%</mo><mo id="S4.T2.3.3.3.m1.1.2.3" maxsize="50%" minsize="50%">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.T2.3.3.3.m1.1c">t_{rel}(\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.3.3.m1.1d">italic_t start_POSTSUBSCRIPT italic_r italic_e italic_l end_POSTSUBSCRIPT ( % )</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T2.4.4.4"><math alttext="r_{rel}(^{\circ})" class="ltx_math_unparsed" display="inline" id="S4.T2.4.4.4.m1.1"><semantics id="S4.T2.4.4.4.m1.1a"><mrow id="S4.T2.4.4.4.m1.1b"><msub id="S4.T2.4.4.4.m1.1.1"><mi id="S4.T2.4.4.4.m1.1.1.2" mathsize="50%">r</mi><mrow id="S4.T2.4.4.4.m1.1.1.3"><mi id="S4.T2.4.4.4.m1.1.1.3.2" mathsize="50%">r</mi><mo id="S4.T2.4.4.4.m1.1.1.3.1">â¢</mo><mi id="S4.T2.4.4.4.m1.1.1.3.3" mathsize="50%">e</mi><mo id="S4.T2.4.4.4.m1.1.1.3.1a">â¢</mo><mi id="S4.T2.4.4.4.m1.1.1.3.4" mathsize="50%">l</mi></mrow></msub><mrow id="S4.T2.4.4.4.m1.1.2"><msup id="S4.T2.4.4.4.m1.1.2.1"><mo id="S4.T2.4.4.4.m1.1.2.1.2" maxsize="50%" minsize="50%">(</mo><mo id="S4.T2.4.4.4.m1.1.2.1.3" mathsize="50%">âˆ˜</mo></msup><mo id="S4.T2.4.4.4.m1.1.2.2" maxsize="50%" minsize="50%">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.T2.4.4.4.m1.1c">r_{rel}(^{\circ})</annotation><annotation encoding="application/x-llamapun" id="S4.T2.4.4.4.m1.1d">italic_r start_POSTSUBSCRIPT italic_r italic_e italic_l end_POSTSUBSCRIPT ( start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT )</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.5.5.5"><math alttext="t_{rel}(\%)" class="ltx_math_unparsed" display="inline" id="S4.T2.5.5.5.m1.1"><semantics id="S4.T2.5.5.5.m1.1a"><mrow id="S4.T2.5.5.5.m1.1b"><msub id="S4.T2.5.5.5.m1.1.1"><mi id="S4.T2.5.5.5.m1.1.1.2" mathsize="50%">t</mi><mrow id="S4.T2.5.5.5.m1.1.1.3"><mi id="S4.T2.5.5.5.m1.1.1.3.2" mathsize="50%">r</mi><mo id="S4.T2.5.5.5.m1.1.1.3.1">â¢</mo><mi id="S4.T2.5.5.5.m1.1.1.3.3" mathsize="50%">e</mi><mo id="S4.T2.5.5.5.m1.1.1.3.1a">â¢</mo><mi id="S4.T2.5.5.5.m1.1.1.3.4" mathsize="50%">l</mi></mrow></msub><mrow id="S4.T2.5.5.5.m1.1.2"><mo id="S4.T2.5.5.5.m1.1.2.1" maxsize="50%" minsize="50%">(</mo><mo id="S4.T2.5.5.5.m1.1.2.2" mathsize="50%">%</mo><mo id="S4.T2.5.5.5.m1.1.2.3" maxsize="50%" minsize="50%">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.T2.5.5.5.m1.1c">t_{rel}(\%)</annotation><annotation encoding="application/x-llamapun" id="S4.T2.5.5.5.m1.1d">italic_t start_POSTSUBSCRIPT italic_r italic_e italic_l end_POSTSUBSCRIPT ( % )</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.6.6.6"><math alttext="r_{rel}(^{\circ})" class="ltx_math_unparsed" display="inline" id="S4.T2.6.6.6.m1.1"><semantics id="S4.T2.6.6.6.m1.1a"><mrow id="S4.T2.6.6.6.m1.1b"><msub id="S4.T2.6.6.6.m1.1.1"><mi id="S4.T2.6.6.6.m1.1.1.2" mathsize="50%">r</mi><mrow id="S4.T2.6.6.6.m1.1.1.3"><mi id="S4.T2.6.6.6.m1.1.1.3.2" mathsize="50%">r</mi><mo id="S4.T2.6.6.6.m1.1.1.3.1">â¢</mo><mi id="S4.T2.6.6.6.m1.1.1.3.3" mathsize="50%">e</mi><mo id="S4.T2.6.6.6.m1.1.1.3.1a">â¢</mo><mi id="S4.T2.6.6.6.m1.1.1.3.4" mathsize="50%">l</mi></mrow></msub><mrow id="S4.T2.6.6.6.m1.1.2"><msup id="S4.T2.6.6.6.m1.1.2.1"><mo id="S4.T2.6.6.6.m1.1.2.1.2" maxsize="50%" minsize="50%">(</mo><mo id="S4.T2.6.6.6.m1.1.2.1.3" mathsize="50%">âˆ˜</mo></msup><mo id="S4.T2.6.6.6.m1.1.2.2" maxsize="50%" minsize="50%">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.T2.6.6.6.m1.1c">r_{rel}(^{\circ})</annotation><annotation encoding="application/x-llamapun" id="S4.T2.6.6.6.m1.1d">italic_r start_POSTSUBSCRIPT italic_r italic_e italic_l end_POSTSUBSCRIPT ( start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT )</annotation></semantics></math></th>
</tr>
<tr class="ltx_tr" id="S4.T2.6.8.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T2.6.8.2.1" style="background-color:#BFFFBF;"><span class="ltx_text" id="S4.T2.6.8.2.1.1" style="font-size:90%;background-color:#BFFFBF;">MLP</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T2.6.8.2.2" style="background-color:#BFFFBF;"><span class="ltx_text" id="S4.T2.6.8.2.2.1" style="font-size:90%;background-color:#BFFFBF;">2</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T2.6.8.2.3"><span class="ltx_text" id="S4.T2.6.8.2.3.1" style="font-size:90%;">L1</span></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T2.6.8.2.4"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T2.6.8.2.5"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.6.8.2.6"><span class="ltx_text" id="S4.T2.6.8.2.6.1" style="font-size:90%;">2.03</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T2.6.8.2.7"><span class="ltx_text" id="S4.T2.6.8.2.7.1" style="font-size:90%;">0.67</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.6.8.2.8"><span class="ltx_text" id="S4.T2.6.8.2.8.1" style="font-size:90%;">3.04</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T2.6.8.2.9"><span class="ltx_text" id="S4.T2.6.8.2.9.1" style="font-size:90%;">1.19</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.6.8.2.10"><span class="ltx_text" id="S4.T2.6.8.2.10.1" style="font-size:90%;">3.60</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.6.8.2.11"><span class="ltx_text" id="S4.T2.6.8.2.11.1" style="font-size:90%;">1.14</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.6.9.1">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.6.9.1.1" rowspan="6"><span class="ltx_text" id="S4.T2.6.9.1.1.1" style="font-size:90%;">Ours</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.6.9.1.2"><span class="ltx_text" id="S4.T2.6.9.1.2.1" style="font-size:90%;">11</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.6.9.1.3" style="background-color:#BFFFBF;"><span class="ltx_text" id="S4.T2.6.9.1.3.1" style="font-size:90%;background-color:#BFFFBF;">L2</span></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T2.6.9.1.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T2.6.9.1.5"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.9.1.6"><span class="ltx_text" id="S4.T2.6.9.1.6.1" style="font-size:90%;">4.35</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.6.9.1.7"><span class="ltx_text" id="S4.T2.6.9.1.7.1" style="font-size:90%;">1.91</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.9.1.8"><span class="ltx_text" id="S4.T2.6.9.1.8.1" style="font-size:90%;">2.97</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.6.9.1.9"><span class="ltx_text" id="S4.T2.6.9.1.9.1" style="font-size:90%;">2.23</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.9.1.10"><span class="ltx_text" id="S4.T2.6.9.1.10.1" style="font-size:90%;">3.66</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.9.1.11"><span class="ltx_text" id="S4.T2.6.9.1.11.1" style="font-size:90%;">1.92</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.10.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.6.10.2.1"><span class="ltx_text" id="S4.T2.6.10.2.1.1" style="font-size:90%;">11</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.6.10.2.2"><span class="ltx_text" id="S4.T2.6.10.2.2.1" style="font-size:90%;">L1</span></td>
<td class="ltx_td ltx_border_r" id="S4.T2.6.10.2.3"></td>
<td class="ltx_td ltx_border_r" id="S4.T2.6.10.2.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.10.2.5"><span class="ltx_text" id="S4.T2.6.10.2.5.1" style="font-size:90%;">1.93</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.6.10.2.6"><span class="ltx_text" id="S4.T2.6.10.2.6.1" style="font-size:90%;">0.68</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.10.2.7"><span class="ltx_text ltx_font_bold" id="S4.T2.6.10.2.7.1" style="font-size:90%;">1.55</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.6.10.2.8"><span class="ltx_text" id="S4.T2.6.10.2.8.1" style="font-size:90%;">0.91</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.10.2.9"><span class="ltx_text" id="S4.T2.6.10.2.9.1" style="font-size:90%;">2.57</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.10.2.10"><span class="ltx_text" id="S4.T2.6.10.2.10.1" style="font-size:90%;">0.54</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.11.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.6.11.3.1"><span class="ltx_text" id="S4.T2.6.11.3.1.1" style="font-size:90%;">11</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.6.11.3.2"><span class="ltx_text" id="S4.T2.6.11.3.2.1" style="font-size:90%;">L1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.6.11.3.3" style="background-color:#BFFFBF;"><span class="ltx_text" id="S4.T2.6.11.3.3.1" style="font-size:90%;background-color:#BFFFBF;">âœ“</span></td>
<td class="ltx_td ltx_border_r" id="S4.T2.6.11.3.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.11.3.5"><span class="ltx_text" id="S4.T2.6.11.3.5.1" style="font-size:90%;">2.37</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.6.11.3.6"><span class="ltx_text ltx_font_bold" id="S4.T2.6.11.3.6.1" style="font-size:90%;">0.52</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.11.3.7"><span class="ltx_text ltx_font_bold" id="S4.T2.6.11.3.7.1" style="font-size:90%;">1.55</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.6.11.3.8"><span class="ltx_text" id="S4.T2.6.11.3.8.1" style="font-size:90%;">0.85</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.11.3.9"><span class="ltx_text" id="S4.T2.6.11.3.9.1" style="font-size:90%;">2.32</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.11.3.10"><span class="ltx_text" id="S4.T2.6.11.3.10.1" style="font-size:90%;">0.75</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.12.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.6.12.4.1" style="background-color:#BFFFBF;"><span class="ltx_text" id="S4.T2.6.12.4.1.1" style="font-size:90%;background-color:#BFFFBF;">65</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.6.12.4.2"><span class="ltx_text" id="S4.T2.6.12.4.2.1" style="font-size:90%;">L1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.6.12.4.3" style="background-color:#BFFFBF;"><span class="ltx_text" id="S4.T2.6.12.4.3.1" style="font-size:90%;background-color:#BFFFBF;">âœ“</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.6.12.4.4"><span class="ltx_text" id="S4.T2.6.12.4.4.1" style="font-size:90%;">âœ“</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.12.4.5"><span class="ltx_text" id="S4.T2.6.12.4.5.1" style="font-size:90%;">2.37</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.6.12.4.6"><span class="ltx_text" id="S4.T2.6.12.4.6.1" style="font-size:90%;">0.64</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.12.4.7"><span class="ltx_text" id="S4.T2.6.12.4.7.1" style="font-size:90%;">1.98</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.6.12.4.8"><span class="ltx_text" id="S4.T2.6.12.4.8.1" style="font-size:90%;">0.58</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.12.4.9"><span class="ltx_text" id="S4.T2.6.12.4.9.1" style="font-size:90%;">2.97</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.12.4.10"><span class="ltx_text" id="S4.T2.6.12.4.10.1" style="font-size:90%;">0.69</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.13.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.6.13.5.1"><span class="ltx_text" id="S4.T2.6.13.5.1.1" style="font-size:90%;">11</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.6.13.5.2"><span class="ltx_text" id="S4.T2.6.13.5.2.1" style="font-size:90%;">L1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.6.13.5.3" style="background-color:#BFFFBF;"><span class="ltx_text" id="S4.T2.6.13.5.3.1" style="font-size:90%;background-color:#BFFFBF;">âœ“</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.6.13.5.4"><span class="ltx_text" id="S4.T2.6.13.5.4.1" style="font-size:90%;">âœ“</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.13.5.5"><span class="ltx_text ltx_font_bold" id="S4.T2.6.13.5.5.1" style="font-size:90%;">1.90</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.6.13.5.6"><span class="ltx_text" id="S4.T2.6.13.5.6.1" style="font-size:90%;">0.53</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.13.5.7"><span class="ltx_text" id="S4.T2.6.13.5.7.1" style="font-size:90%;">1.79</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.6.13.5.8"><span class="ltx_text ltx_font_bold" id="S4.T2.6.13.5.8.1" style="font-size:90%;">0.45</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.13.5.9"><span class="ltx_text" id="S4.T2.6.13.5.9.1" style="font-size:90%;">2.40</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.13.5.10"><span class="ltx_text" id="S4.T2.6.13.5.10.1" style="font-size:90%;">0.60</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.14.6">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T2.6.14.6.1"><span class="ltx_text" id="S4.T2.6.14.6.1.1" style="font-size:90%;">11</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T2.6.14.6.2"><span class="ltx_text" id="S4.T2.6.14.6.2.1" style="font-size:90%;">L1</span></td>
<td class="ltx_td ltx_border_b ltx_border_r" id="S4.T2.6.14.6.3"></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T2.6.14.6.4"><span class="ltx_text" id="S4.T2.6.14.6.4.1" style="font-size:90%;">âœ“</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.6.14.6.5"><span class="ltx_text" id="S4.T2.6.14.6.5.1" style="font-size:90%;">2.02</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T2.6.14.6.6"><span class="ltx_text" id="S4.T2.6.14.6.6.1" style="font-size:90%;">0.53</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.6.14.6.7"><span class="ltx_text" id="S4.T2.6.14.6.7.1" style="font-size:90%;">1.75</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T2.6.14.6.8"><span class="ltx_text" id="S4.T2.6.14.6.8.1" style="font-size:90%;">0.47</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.6.14.6.9"><span class="ltx_text ltx_font_bold" id="S4.T2.6.14.6.9.1" style="font-size:90%;">2.11</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.6.14.6.10"><span class="ltx_text ltx_font_bold" id="S4.T2.6.14.6.10.1" style="font-size:90%;">0.39</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S4.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="281" id="S4.F3.g1" src="x3.png" width="830"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="286" id="S4.F3.g2" src="x4.png" width="830"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S4.F3.3.2" style="font-size:90%;">Proposed transformer based fusion and pose estimation module in VIFT evaluated under different training settings. We mark trajectories every 5 seconds for intuition about the vehicleâ€™s speed along the trajectory and easy distinction of results. We emphasize that the camera and IMU provide 10 FPS and 100 Hz measurements, respectively, which are much more frequent than marked locations. We show the estimated trajectory in test sequences from above in the top row and vertical trajectory versus the bottom row. All trajectories start from the origin, and relative pose estimates from VIFT are applied sequentially to obtain absolute pose estimates for each time index.</span></figcaption>
</figure>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Model Type</h4>
<div class="ltx_para" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.1">We first look at the performance of 4-layer MLP trained on latent visual inertial feature vectors. From results in rows 1 and 2 of Table <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#S4.T2" title="Table 2 â€£ 4.3 Ablation Study â€£ 4 Experiments â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry"><span class="ltx_text ltx_ref_tag">2</span></a>, we observe that the odometry performance is reasonably good even with a small MLP network. This performance supports the primary motivation of our architecture. The vectors in latent space already contain good properties for pose estimation. We use transformer-based fusion to correct these latent vectors with the transformer, based on past measurements, and a 2-layer MLP is used at the end of the transformer. VIFT utilizes history to improve pose estimation with transformer-based architecture.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Norm Type in Training Criterion</h4>
<div class="ltx_para" id="S4.SS3.SSS2.p1">
<p class="ltx_p" id="S4.SS3.SSS2.p1.6">We found that using the L1 loss function resulted in better performance within the same training steps. As the errors decrease after the initial epochs, the gradients in L2 loss become smaller, leading to slower convergence and requiring more training iterations, according to our observations. Consequently, we observed that training with L2 loss was slower overall. We also tuned each scenarioâ€™s <math alttext="\alpha" class="ltx_Math" display="inline" id="S4.SS3.SSS2.p1.1.m1.1"><semantics id="S4.SS3.SSS2.p1.1.m1.1a"><mi id="S4.SS3.SSS2.p1.1.m1.1.1" xref="S4.SS3.SSS2.p1.1.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.1.m1.1b"><ci id="S4.SS3.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS2.p1.1.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS2.p1.1.m1.1d">italic_Î±</annotation></semantics></math> parameter in the Equation <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#S3.E4" title="Equation 4 â€£ 3.3 Deep Rotation Regression â€£ 3 Method â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry"><span class="ltx_text ltx_ref_tag">4</span></a>. Following previous work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib27" title="">27</a>]</cite>, we used <math alttext="\alpha=100" class="ltx_Math" display="inline" id="S4.SS3.SSS2.p1.2.m2.1"><semantics id="S4.SS3.SSS2.p1.2.m2.1a"><mrow id="S4.SS3.SSS2.p1.2.m2.1.1" xref="S4.SS3.SSS2.p1.2.m2.1.1.cmml"><mi id="S4.SS3.SSS2.p1.2.m2.1.1.2" xref="S4.SS3.SSS2.p1.2.m2.1.1.2.cmml">Î±</mi><mo id="S4.SS3.SSS2.p1.2.m2.1.1.1" xref="S4.SS3.SSS2.p1.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS3.SSS2.p1.2.m2.1.1.3" xref="S4.SS3.SSS2.p1.2.m2.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.2.m2.1b"><apply id="S4.SS3.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS3.SSS2.p1.2.m2.1.1"><eq id="S4.SS3.SSS2.p1.2.m2.1.1.1.cmml" xref="S4.SS3.SSS2.p1.2.m2.1.1.1"></eq><ci id="S4.SS3.SSS2.p1.2.m2.1.1.2.cmml" xref="S4.SS3.SSS2.p1.2.m2.1.1.2">ğ›¼</ci><cn id="S4.SS3.SSS2.p1.2.m2.1.1.3.cmml" type="integer" xref="S4.SS3.SSS2.p1.2.m2.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.2.m2.1c">\alpha=100</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS2.p1.2.m2.1d">italic_Î± = 100</annotation></semantics></math> with the L2 criterion. For L1 loss, we found that <math alttext="\alpha=10" class="ltx_Math" display="inline" id="S4.SS3.SSS2.p1.3.m3.1"><semantics id="S4.SS3.SSS2.p1.3.m3.1a"><mrow id="S4.SS3.SSS2.p1.3.m3.1.1" xref="S4.SS3.SSS2.p1.3.m3.1.1.cmml"><mi id="S4.SS3.SSS2.p1.3.m3.1.1.2" xref="S4.SS3.SSS2.p1.3.m3.1.1.2.cmml">Î±</mi><mo id="S4.SS3.SSS2.p1.3.m3.1.1.1" xref="S4.SS3.SSS2.p1.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS3.SSS2.p1.3.m3.1.1.3" xref="S4.SS3.SSS2.p1.3.m3.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.3.m3.1b"><apply id="S4.SS3.SSS2.p1.3.m3.1.1.cmml" xref="S4.SS3.SSS2.p1.3.m3.1.1"><eq id="S4.SS3.SSS2.p1.3.m3.1.1.1.cmml" xref="S4.SS3.SSS2.p1.3.m3.1.1.1"></eq><ci id="S4.SS3.SSS2.p1.3.m3.1.1.2.cmml" xref="S4.SS3.SSS2.p1.3.m3.1.1.2">ğ›¼</ci><cn id="S4.SS3.SSS2.p1.3.m3.1.1.3.cmml" type="integer" xref="S4.SS3.SSS2.p1.3.m3.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.3.m3.1c">\alpha=10</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS2.p1.3.m3.1d">italic_Î± = 10</annotation></semantics></math> worked better for Euler angles, while <math alttext="\alpha=40" class="ltx_Math" display="inline" id="S4.SS3.SSS2.p1.4.m4.1"><semantics id="S4.SS3.SSS2.p1.4.m4.1a"><mrow id="S4.SS3.SSS2.p1.4.m4.1.1" xref="S4.SS3.SSS2.p1.4.m4.1.1.cmml"><mi id="S4.SS3.SSS2.p1.4.m4.1.1.2" xref="S4.SS3.SSS2.p1.4.m4.1.1.2.cmml">Î±</mi><mo id="S4.SS3.SSS2.p1.4.m4.1.1.1" xref="S4.SS3.SSS2.p1.4.m4.1.1.1.cmml">=</mo><mn id="S4.SS3.SSS2.p1.4.m4.1.1.3" xref="S4.SS3.SSS2.p1.4.m4.1.1.3.cmml">40</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.4.m4.1b"><apply id="S4.SS3.SSS2.p1.4.m4.1.1.cmml" xref="S4.SS3.SSS2.p1.4.m4.1.1"><eq id="S4.SS3.SSS2.p1.4.m4.1.1.1.cmml" xref="S4.SS3.SSS2.p1.4.m4.1.1.1"></eq><ci id="S4.SS3.SSS2.p1.4.m4.1.1.2.cmml" xref="S4.SS3.SSS2.p1.4.m4.1.1.2">ğ›¼</ci><cn id="S4.SS3.SSS2.p1.4.m4.1.1.3.cmml" type="integer" xref="S4.SS3.SSS2.p1.4.m4.1.1.3">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.4.m4.1c">\alpha=40</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS2.p1.4.m4.1d">italic_Î± = 40</annotation></semantics></math> yielded better results in models incorporating the RPMG layer. We experimented with different <math alttext="\alpha" class="ltx_Math" display="inline" id="S4.SS3.SSS2.p1.5.m5.1"><semantics id="S4.SS3.SSS2.p1.5.m5.1a"><mi id="S4.SS3.SSS2.p1.5.m5.1.1" xref="S4.SS3.SSS2.p1.5.m5.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.5.m5.1b"><ci id="S4.SS3.SSS2.p1.5.m5.1.1.cmml" xref="S4.SS3.SSS2.p1.5.m5.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.5.m5.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS2.p1.5.m5.1d">italic_Î±</annotation></semantics></math> values for each method and reported the results using the values that best balanced rotational and translational errors. Since the rotation loss is calculated based on the mean difference between elements of rotation matrices in RPMG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib5" title="">5</a>]</cite>, we fine-tuned the <math alttext="\alpha" class="ltx_Math" display="inline" id="S4.SS3.SSS2.p1.6.m6.1"><semantics id="S4.SS3.SSS2.p1.6.m6.1a"><mi id="S4.SS3.SSS2.p1.6.m6.1.1" xref="S4.SS3.SSS2.p1.6.m6.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.6.m6.1b"><ci id="S4.SS3.SSS2.p1.6.m6.1.1.cmml" xref="S4.SS3.SSS2.p1.6.m6.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.6.m6.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS2.p1.6.m6.1d">italic_Î±</annotation></semantics></math> parameter to identify the optimal balance.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3 </span>Data Balancing</h4>
<div class="ltx_para" id="S4.SS3.SSS3.p1">
<p class="ltx_p" id="S4.SS3.SSS3.p1.1">In datasets like KITTI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib10" title="">10</a>]</cite>, specific rotational movements, such as sharp turns and sudden stops, are underrepresented. These motions are critical because they impact overall trajectory accuracy more, making errors in these scenarios more costly. To address this, we experimented with increasing the weights of these less frequent rotational updates during training with the histogram of rotations proposed by Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib28" title="">28</a>]</cite>, aiming to enhance the modelâ€™s performance in these critical cases.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS3.p2">
<p class="ltx_p" id="S4.SS3.SSS3.p2.1">A comparison of rows 3,4,6, and 7 in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#S4.T2" title="Table 2 â€£ 4.3 Ablation Study â€£ 4 Experiments â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry"><span class="ltx_text ltx_ref_tag">2</span></a> shows that introducing data balancing during training does not lead to consistent improvements across the test sequences. These results indicate that data balancing might negatively affect the modelâ€™s ability. Although we obtained the best results in Sequences 05 and 07 when we applied data balancing, the improvements are inconsistent across all sequences.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS3.p3">
<p class="ltx_p" id="S4.SS3.SSS3.p3.1">Our observations suggest that while data balancing can be a helpful strategy, it must be carefully tuned, especially when combined with advanced optimization techniques like RPMG. Overemphasizing underrepresented rotations does not always yield the desired improvements and could potentially degrade performance, particularly in orientation accuracy. Therefore, a more nuanced approach may be required to balance the representation of various movements in the training data without compromising the modelâ€™s overall robustness.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.4 </span>RPMG</h4>
<div class="ltx_para" id="S4.SS3.SSS4.p1">
<p class="ltx_p" id="S4.SS3.SSS4.p1.2">The RPMG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#bib.bib5" title="">5</a>]</cite> layer has proven to be a crucial enhancement in the models we evaluated, particularly in terms of reducing orientation errors, denoted as <math alttext="\textit{r}_{\textit{rel}}" class="ltx_Math" display="inline" id="S4.SS3.SSS4.p1.1.m1.1"><semantics id="S4.SS3.SSS4.p1.1.m1.1a"><msub id="S4.SS3.SSS4.p1.1.m1.1.1" xref="S4.SS3.SSS4.p1.1.m1.1.1.cmml"><mtext class="ltx_mathvariant_italic" id="S4.SS3.SSS4.p1.1.m1.1.1.2" xref="S4.SS3.SSS4.p1.1.m1.1.1.2a.cmml">r</mtext><mtext class="ltx_mathvariant_italic" id="S4.SS3.SSS4.p1.1.m1.1.1.3" xref="S4.SS3.SSS4.p1.1.m1.1.1.3a.cmml">rel</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS4.p1.1.m1.1b"><apply id="S4.SS3.SSS4.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS4.p1.1.m1.1.1.1.cmml" xref="S4.SS3.SSS4.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.SSS4.p1.1.m1.1.1.2a.cmml" xref="S4.SS3.SSS4.p1.1.m1.1.1.2"><mtext class="ltx_mathvariant_italic" id="S4.SS3.SSS4.p1.1.m1.1.1.2.cmml" xref="S4.SS3.SSS4.p1.1.m1.1.1.2">r</mtext></ci><ci id="S4.SS3.SSS4.p1.1.m1.1.1.3a.cmml" xref="S4.SS3.SSS4.p1.1.m1.1.1.3"><mtext class="ltx_mathvariant_italic" id="S4.SS3.SSS4.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="S4.SS3.SSS4.p1.1.m1.1.1.3">rel</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS4.p1.1.m1.1c">\textit{r}_{\textit{rel}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS4.p1.1.m1.1d">r start_POSTSUBSCRIPT rel end_POSTSUBSCRIPT</annotation></semantics></math>. When comparing models trained with RPMG to those without, we consistently observe significant improvements in orientation accuracy across all tested sequences. This consistent reduction in <math alttext="\textit{r}_{\textit{rel}}" class="ltx_Math" display="inline" id="S4.SS3.SSS4.p1.2.m2.1"><semantics id="S4.SS3.SSS4.p1.2.m2.1a"><msub id="S4.SS3.SSS4.p1.2.m2.1.1" xref="S4.SS3.SSS4.p1.2.m2.1.1.cmml"><mtext class="ltx_mathvariant_italic" id="S4.SS3.SSS4.p1.2.m2.1.1.2" xref="S4.SS3.SSS4.p1.2.m2.1.1.2a.cmml">r</mtext><mtext class="ltx_mathvariant_italic" id="S4.SS3.SSS4.p1.2.m2.1.1.3" xref="S4.SS3.SSS4.p1.2.m2.1.1.3a.cmml">rel</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS4.p1.2.m2.1b"><apply id="S4.SS3.SSS4.p1.2.m2.1.1.cmml" xref="S4.SS3.SSS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS4.p1.2.m2.1.1.1.cmml" xref="S4.SS3.SSS4.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS3.SSS4.p1.2.m2.1.1.2a.cmml" xref="S4.SS3.SSS4.p1.2.m2.1.1.2"><mtext class="ltx_mathvariant_italic" id="S4.SS3.SSS4.p1.2.m2.1.1.2.cmml" xref="S4.SS3.SSS4.p1.2.m2.1.1.2">r</mtext></ci><ci id="S4.SS3.SSS4.p1.2.m2.1.1.3a.cmml" xref="S4.SS3.SSS4.p1.2.m2.1.1.3"><mtext class="ltx_mathvariant_italic" id="S4.SS3.SSS4.p1.2.m2.1.1.3.cmml" mathsize="70%" xref="S4.SS3.SSS4.p1.2.m2.1.1.3">rel</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS4.p1.2.m2.1c">\textit{r}_{\textit{rel}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS4.p1.2.m2.1d">r start_POSTSUBSCRIPT rel end_POSTSUBSCRIPT</annotation></semantics></math> highlights the effectiveness of incorporating Riemannian optimization techniques in the training process.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS4.p2">
<p class="ltx_p" id="S4.SS3.SSS4.p2.1">RPMG is particularly advantageous in scenarios involving rotations, where traditional optimization methods might struggle due to the non-Euclidean nature of orientation spaces. By operating directly on the manifold of rotations, RPMG ensures that updates to the orientation parameters are more geometrically appropriate, leading to better convergence properties and, ultimately, more accurate predictions.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS4.p3">
<p class="ltx_p" id="S4.SS3.SSS4.p3.1">In our ablation study, including RPMG improved orientation accuracy and demonstrated robustness across different sequence lengths and data balancing strategies. For instance, in Sequence 07, the use of RPMG led to a marked decrease in <math alttext="\textit{r}_{\textit{rel}}" class="ltx_Math" display="inline" id="S4.SS3.SSS4.p3.1.m1.1"><semantics id="S4.SS3.SSS4.p3.1.m1.1a"><msub id="S4.SS3.SSS4.p3.1.m1.1.1" xref="S4.SS3.SSS4.p3.1.m1.1.1.cmml"><mtext class="ltx_mathvariant_italic" id="S4.SS3.SSS4.p3.1.m1.1.1.2" xref="S4.SS3.SSS4.p3.1.m1.1.1.2a.cmml">r</mtext><mtext class="ltx_mathvariant_italic" id="S4.SS3.SSS4.p3.1.m1.1.1.3" xref="S4.SS3.SSS4.p3.1.m1.1.1.3a.cmml">rel</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS4.p3.1.m1.1b"><apply id="S4.SS3.SSS4.p3.1.m1.1.1.cmml" xref="S4.SS3.SSS4.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS4.p3.1.m1.1.1.1.cmml" xref="S4.SS3.SSS4.p3.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.SSS4.p3.1.m1.1.1.2a.cmml" xref="S4.SS3.SSS4.p3.1.m1.1.1.2"><mtext class="ltx_mathvariant_italic" id="S4.SS3.SSS4.p3.1.m1.1.1.2.cmml" xref="S4.SS3.SSS4.p3.1.m1.1.1.2">r</mtext></ci><ci id="S4.SS3.SSS4.p3.1.m1.1.1.3a.cmml" xref="S4.SS3.SSS4.p3.1.m1.1.1.3"><mtext class="ltx_mathvariant_italic" id="S4.SS3.SSS4.p3.1.m1.1.1.3.cmml" mathsize="70%" xref="S4.SS3.SSS4.p3.1.m1.1.1.3">rel</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS4.p3.1.m1.1c">\textit{r}_{\textit{rel}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS4.p3.1.m1.1d">r start_POSTSUBSCRIPT rel end_POSTSUBSCRIPT</annotation></semantics></math>, from 0.91 to 0.47, highlighting its substantial impact. We observed similar trends across Sequence 05 and Sequence 10, where RPMG consistently yielded lower orientation errors.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.5 </span>Sequence Length</h4>
<div class="ltx_para" id="S4.SS3.SSS5.p1">
<p class="ltx_p" id="S4.SS3.SSS5.p1.1">When we compare models in rows 5 and 6 of Table <a class="ltx_ref" href="https://arxiv.org/html/2409.08769v1#S4.T2" title="Table 2 â€£ 4.3 Ablation Study â€£ 4 Experiments â€£ Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry"><span class="ltx_text ltx_ref_tag">2</span></a>, we observe a performance drop on the same model trained with increased sequence length. We tried a sequence length of 65 compared to the original sequence length of 11 in the experiment. Modeling the relationships in longer sequences is a more complex task, and larger sequence lengths could require more training data.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We introduce deep VIO network VIFT, which performs sensor fusion and pose estimation with a causal transformer. We show that our method outperforms previous methods with our experiments on the KITTI dataset. We also improve the VIFT by including the manifold optimization technique RPMG inside our pipeline. Our studyâ€™s consistent performance gains across different configurations demonstrate that VIFT and RPMG provide a fundamental enhancement that can significantly elevate the performance of visual-inertial odometry models.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">We gratefully acknowledge the computational resources provided by TÃœBÄ°TAK ULAKBÄ°M High Performance and Grid Computing Center (TRUBA). Yunus Bilge Kurt is supported by the TÃœBÄ°TAK under the 2210-National MSc/MA Scholarship Program.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Almalioglu, Y., Turan, M., Saputra, M.R.U., deÂ GusmÃ£o, P.P., Markham, A., Trigoni, N.: Selfvio: Self-supervised deep monocular visualâ€“inertial odometry and depth estimation. Neural Networks <span class="ltx_text ltx_font_bold" id="bib.bib1.1.1">150</span>, 119â€“136 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Bloesch, M., Omari, S., Hutter, M., Siegwart, R.: Robust visual inertial odometry using a direct ekf-based approach. In: 2015 IEEE/RSJ international conference on intelligent robots and systems (IROS). pp. 298â€“304. IEEE (2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Campos, C., Elvira, R., RodrÃ­guez, J.J.G., Montiel, J.M., TardÃ³s, J.D.: Orb-slam3: An accurate open-source library for visual, visualâ€“inertial, and multimap slam. IEEE Transactions on Robotics <span class="ltx_text ltx_font_bold" id="bib.bib3.1.1">37</span>(6), 1874â€“1890 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Chen, C., Rosa, S., Miao, Y., Lu, C.X., Wu, W., Markham, A., Trigoni, N.: Selective sensor fusion for neural visual-inertial odometry. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10542â€“10551 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Chen, J., Yin, Y., Birdal, T., Chen, B., Guibas, L.J., Wang, H.: Projective manifold gradient layer for deep rotation regression. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6646â€“6655 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Clark, R., Wang, S., Wen, H., Markham, A., Trigoni, N.: Vinet: Visual-inertial odometry as a sequence-to-sequence learning problem. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol.Â 31 (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An image is worth 16x16 words: Transformers for image recognition at scale. ArXiv <span class="ltx_text ltx_font_bold" id="bib.bib7.1.1">abs/2010.11929</span> (2020), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:225039882" title="">https://api.semanticscholar.org/CorpusID:225039882</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Dosovitskiy, A., Fischer, P., Ilg, E., Hausser, P., Hazirbas, C., Golkov, V., Van DerÂ Smagt, P., Cremers, D., Brox, T.: Flownet: Learning optical flow with convolutional networks. In: Proceedings of the IEEE international conference on computer vision. pp. 2758â€“2766 (2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Forster, C., Carlone, L., Dellaert, F., Scaramuzza, D.: On-manifold preintegration for real-time visualâ€“inertial odometry. IEEE Transactions on Robotics <span class="ltx_text ltx_font_bold" id="bib.bib9.1.1">33</span>(1), 1â€“21 (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Geiger, A., Lenz, P., Urtasun, R.: Are we ready for autonomous driving? the kitti vision benchmark suite. In: Conference on Computer Vision and Pattern Recognition (CVPR) (2012)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
van Goor, P., Mahony, R.: Eqvio: An equivariant filter for visual-inertial odometry. IEEE Transactions on Robotics (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Han, L., Lin, Y., Du, G., Lian, S.: Deepvio: Self-supervised deep learning of monocular visual inertial odometry using 3d geometric constraints. In: 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). pp. 6906â€“6913. IEEE (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Leutenegger, S., Lynen, S., Bosse, M., Siegwart, R., Furgale, P.: Keyframe-based visualâ€“inertial odometry using nonlinear optimization. The International Journal of Robotics Research <span class="ltx_text ltx_font_bold" id="bib.bib13.1.1">34</span>(3), 314â€“334 (2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Liu, L., Li, G., Li, T.H.: Atvio: attention guided visual-inertial odometry. In: ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 4125â€“4129. IEEE (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Loshchilov, I., Hutter, F.: SGDR: stochastic gradient descent with warm restarts. In: 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net (2017), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=Skq89Scxx" title="">https://openreview.net/forum?id=Skq89Scxx</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net (2019), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=Bkg6RiCqY7" title="">https://openreview.net/forum?id=Bkg6RiCqY7</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Mourikis, A.I., Roumeliotis, S.I.: A multi-state constraint kalman filter for vision-aided inertial navigation. In: Proceedings 2007 IEEE international conference on robotics and automation. pp. 3565â€“3572. IEEE (2007)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Qin, T., Li, P., Shen, S.: Vins-mono: A robust and versatile monocular visual-inertial state estimator. IEEE Transactions on Robotics <span class="ltx_text ltx_font_bold" id="bib.bib18.1.1">34</span>(4), 1004â€“1020 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Shamwell, E.J., Leung, S., Nothwang, W.D.: Vision-aided absolute trajectory estimation using an unsupervised deep network with online error correction. In: 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). pp. 2524â€“2531. IEEE (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Shamwell, E.J., Lindgren, K., Leung, S., Nothwang, W.D.: Unsupervised deep visual-inertial odometry with online error correction for rgb-d imagery. IEEE Transactions on Pattern Analysis and Machine Intelligence <span class="ltx_text ltx_font_bold" id="bib.bib20.1.1">42</span>(10), 2478â€“2493 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
von Stumberg, L., Cremers, D.: DM-VIO: Delayed marginalization visual-inertial odometry. IEEE Robotics and Automation Letters (RA-L) &amp; International Conference on Robotics and Automation (ICRA) <span class="ltx_text ltx_font_bold" id="bib.bib21.1.1">7</span>(2), 1408â€“1415 (2022). https://doi.org/10.1109/LRA.2021.3140129

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Tu, Z., Chen, C., Pan, X., Liu, R., Cui, J., Mao, J.: Ema-vio: Deep visualâ€“inertial odometry with external memory attention. IEEE Sensors Journal <span class="ltx_text ltx_font_bold" id="bib.bib22.1.1">22</span>(21), 20877â€“20885 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L.u., Polosukhin, I.: Attention is all you need. In: Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R. (eds.) Advances in Neural Information Processing Systems. vol.Â 30. Curran Associates, Inc. (2017), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Wagstaff, B., Wise, E., Kelly, J.: A self-supervised, differentiable kalman filter for uncertainty-aware visual-inertial odometry. In: 2022 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM). pp. 1388â€“1395. IEEE (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Wang, Z., Zhu, Y., Lu, K., Freer, D., Wu, H., Chen, H.: Attention guided unsupervised learning of monocular visual-inertial odometry. In: 2022 IEEE Intelligent Vehicles Symposium (IV). pp. 651â€“657. IEEE (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Wei, P., Hua, G., Huang, W., Meng, F., Liu, H.: Unsupervised monocular visual-inertial odometry network. In: Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence. pp. 2347â€“2354 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Yang, M., Chen, Y., Kim, H.S.: Efficient deep visual and inertial odometry with adaptive visual modality selection. In: Computer Visionâ€“ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23â€“27, 2022, Proceedings, Part XXXVIII. pp. 233â€“250. Springer (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Yang, Y., Zha, K., Chen, Y., Wang, H., Katabi, D.: Delving into deep imbalanced regression. In: International conference on machine learning. pp. 11842â€“11851. PMLR (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Zhou, Y., Barnes, C., Lu, J., Yang, J., Li, H.: On the continuity of rotation representations in neural networks. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 5745â€“5753 (2019)

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Sep 13 12:00:25 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
