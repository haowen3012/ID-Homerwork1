<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects</title>
<!--Generated on Wed Jul 10 22:49:25 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="datasets,  pose estimation,  interaction,  mixed reality,  deep learning" lang="en" name="keywords"/>
<base href="/html/2407.08081v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S1" title="In RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S2" title="In RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S2.SS1" title="In 2. Related Work ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Object pose estimation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S2.SS2" title="In 2. Related Work ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Pose data collection</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S2.SS2.SSS1" title="In 2.2. Pose data collection ‣ 2. Related Work ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>Synthetic data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S2.SS2.SSS2" title="In 2.2. Pose data collection ‣ 2. Related Work ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>Real-world data</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S3" title="In RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Appearance-changing objects</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S3.SS1" title="In 3. Appearance-changing objects ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Deformation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S3.SS2" title="In 3. Appearance-changing objects ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Viewing-angle dependent</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S3.SS3" title="In 3. Appearance-changing objects ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Articulated</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S4" title="In RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>RoCap pipeline</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S4.SS1" title="In 4. RoCap pipeline ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Eye-to-hand camera calibration</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S4.SS2" title="In 4. RoCap pipeline ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Data collection</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S4.SS2.SSS1" title="In 4.2. Data collection ‣ 4. RoCap pipeline ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Pose coverage</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S4.SS2.SSS2" title="In 4.2. Data collection ‣ 4. RoCap pipeline ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Capturing process</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S4.SS3" title="In 4. RoCap pipeline ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Data labeling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S4.SS4" title="In 4. RoCap pipeline ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Data processing and augmentation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S4.SS4.SSS1" title="In 4.4. Data processing and augmentation ‣ 4. RoCap pipeline ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.1 </span>Generating masks</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S4.SS4.SSS1.Px1" title="In 4.4.1. Generating masks ‣ 4.4. Data processing and augmentation ‣ 4. RoCap pipeline ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_title">Bounding box</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S4.SS4.SSS1.Px2" title="In 4.4.1. Generating masks ‣ 4.4. Data processing and augmentation ‣ 4. RoCap pipeline ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_title">Filtering</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S4.SS4.SSS2" title="In 4.4. Data processing and augmentation ‣ 4. RoCap pipeline ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.2 </span>Data augmentation</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S5" title="In RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S5.SS1" title="In 5. Evaluation ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Implementation of pose estimation pipeline</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S5.SS2" title="In 5. Evaluation ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Quantitative evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S5.SS3" title="In 5. Evaluation ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Qualitative evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S6" title="In RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S6.SS1" title="In 6. Discussion ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Limitations</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S6.SS1.SSS0.Px1" title="In 6.1. Limitations ‣ 6. Discussion ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_title">Data capturing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S6.SS1.SSS0.Px2" title="In 6.1. Limitations ‣ 6. Discussion ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_title">Model performance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S6.SS1.SSS0.Px3" title="In 6.1. Limitations ‣ 6. Discussion ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_title">Other constraints</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S6.SS2" title="In 6. Discussion ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Handling Occlusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S6.SS3" title="In 6. Discussion ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Automatic Changing of Mechanical States</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S6.SS4" title="In 6. Discussion ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span>Leveraging Robots for Large-Scale Data Collection</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S7" title="In RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<figure class="ltx_figure ltx_teaserfigure" id="S0.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="170" id="S0.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>
The RoCap pipeline is a robotic system designed to collect datasets for the purpose of pose estimation of appearance-changing objects, <span class="ltx_text ltx_font_italic" id="S0.F1.2.1">e.g.</span>, a deformable plush toy (a). The system consists of a robotic arm and an RGB camera, which allows for data collection (c) of objects with appearance-changing features (b). Through data augmentation and training on off-the-shelf deep learning models using the collected data, the system can effectively estimate the pose of the plush toy during manipulation, even as it transitions through deformation (d).
</figcaption>
</figure>
<h1 class="ltx_title ltx_title_document">RoCap: A Robotic Data Collection Pipeline for the Pose Estimation
of Appearance-Changing Objects</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jiahao “Nick” Li
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">UCLA HCI Research</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Los Angeles</span><span class="ltx_text ltx_affiliation_country" id="id3.3.id3">United States</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:ljhnick@g.ucla.edu">ljhnick@g.ucla.edu</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Toby Chong
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id4.1.id1">TOEI Zukun Research</span><span class="ltx_text ltx_affiliation_city" id="id5.2.id2">Tokyo</span><span class="ltx_text ltx_affiliation_country" id="id6.3.id3">Japan</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:tobyclh@gmail.com">tobyclh@gmail.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhongyi Zhou
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">University of Tokyo</span><span class="ltx_text ltx_affiliation_city" id="id8.2.id2">Tokyo</span><span class="ltx_text ltx_affiliation_country" id="id9.3.id3">Japan</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:zhongyi.zhou.work@gmail.com">zhongyi.zhou.work@gmail.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hironori Yoshida
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id10.1.id1">Future University Hakodate</span><span class="ltx_text ltx_affiliation_city" id="id11.2.id2">Hakodate</span><span class="ltx_text ltx_affiliation_country" id="id12.3.id3">Japan</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:hyoshida@fun.ac.jp">hyoshida@fun.ac.jp</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Koji Yatani
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id13.1.id1">University of Tokyo</span><span class="ltx_text ltx_affiliation_city" id="id14.2.id2">Tokyo</span><span class="ltx_text ltx_affiliation_country" id="id15.3.id3">Japan</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:koji@iis-lab.org">koji@iis-lab.org</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiang ‘Anthony’ Chen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id16.1.id1">UCLA HCI Research</span><span class="ltx_text ltx_affiliation_city" id="id17.2.id2">Los Angeles</span><span class="ltx_text ltx_affiliation_country" id="id18.3.id3">United States</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:xac@ucla.edu">xac@ucla.edu</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Takeo Igarashi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id19.1.id1">University of Tokyo</span><span class="ltx_text ltx_affiliation_city" id="id20.2.id2">Tokyo</span><span class="ltx_text ltx_affiliation_country" id="id21.3.id3">Japan</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:takeo@acm.org">takeo@acm.org</a>
</span></span></span>
</div>
<div class="ltx_dates">(2018)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id22.id1">Object pose estimation plays a vital role in mixed reality interactions when user manipulate tangible objects as controllers.
Traditional vision-based object pose estimation methods leverage 3D reconstruction to synthesize training data.
However, these methods are designed for static objects with diffuse colors and do not work well for objects that change their appearance during manipulation, such as deformable objects like plush toys, transparent objects like chemical flasks, reflective objects like metal pitcher, and articulated objects like scissors.
To address this limitation, we propose RoCap, a robotic pipeline that emulates human manipulation of target objects while generating data labeled with ground truth pose information. The user first gives the target object to a robotic arm, and the system captures many pictures of the object in various 6D configurations. The system trains a model by using captured images
and their ground truth pose information automatically calculated from the joint angles of the robotic arm.
We showcase pose estimation
for appearance-changing objects by training simple deep-learning models using the collected data and comparing the results with a model trained with synthetic data based on 3D reconstruction via quantitative and qualitative evaluation. The findings underscore the promising capabilities of RoCap.

</p>
</div>
<div class="ltx_keywords">datasets, pose estimation, interaction, mixed reality, deep learning
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmcopyright</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2018</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>XXXXXXX.XXXXXXX</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Make sure to enter the correct
conference title from your rights confirmation email; June 03–05,
2018; Woodstock, NY</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_booktitle" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">booktitle: </span>Woodstock ’18: ACM Symposium on Neural Gaze Detection,
June 03–05, 2018, Woodstock, NY</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_price" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">price: </span>15.00</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id7"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>978-1-4503-XXXX-X/18/06</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_submissionid" id="id8"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">submissionid: </span>2831</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Leveraging existing tangible objects as controllers in mixed reality (MR) can significantly enhance the immersive experience and allow for a wider range of tangible motions, particularly for applications such as storytelling, skill training, and education.
For example, employing plush toys to guide storytelling allows for a personalized and engaging experience, and using various handheld tools can facilitate more precise and diverse interaction mechanisms to make tasks feel natural and intuitive.
Such an approach demands the capabilities for accurate predictions of 6D pose estimation — identifying an object’s location and orientation in the 3D space.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Vision-based pose estimation has gained popularity in the past few years over tracker or sensor based methods, as it does not require additional hardware, alter the appearance or interfere with the normal use of the objects and it is cost effective and accessible.
Researchers have adopted different approaches including mapping image feature to the 3D model of the object <cite class="ltx_cite ltx_citemacro_citep">(Hettiarachchi and Wigdor, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib21" title="">2016</a>)</cite> and matching point cloud constructed by depth camera <cite class="ltx_cite ltx_citemacro_citep">(Barnes et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib3" title="">2008</a>)</cite>.
More recently, data-driven deep learning methods <cite class="ltx_cite ltx_citemacro_citep">(Xiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib48" title="">2017</a>; Peng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib39" title="">2019</a>)</cite> demonstrated accurate predictions of the 6D pose of pre-defined sets of object included in carefully crafted datasets <cite class="ltx_cite ltx_citemacro_citep">(Calli et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib6" title="">2015</a>; Krull et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib27" title="">2015</a>)</cite>.
However, it remains unclear how well they work on objects where carefully labeled data do not exist such as personal objects.
To address this issue, some prior work enables end-users to collect datasets for everyday objects 6D pose estimation <cite class="ltx_cite ltx_citemacro_citep">(Marion et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib36" title="">2018</a>; Qian et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib40" title="">2022</a>)</cite>, introducing synthetic approaches to generate a large amount synthetic data given the 3D model of the objects <cite class="ltx_cite ltx_citemacro_citep">(Dwibedi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib14" title="">2017</a>)</cite>, or adopts a few-shot learning method by training on 3D mesh reconstructed from a short clip of video <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib33" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">A limitation of these existing methods is that they mainly focus on objects that are static objects with diffuse colors, with a less focus on objects that <span class="ltx_text ltx_font_bold" id="S1.p3.1.1">change their appearances</span> when being manipulated, including objects with challenging appearance materials (<span class="ltx_text ltx_font_italic" id="S1.p3.1.2">e.g.</span>, transparent and specular objects), deformable objects and articulated objects <cite class="ltx_cite ltx_citemacro_citep">(Thalhammer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib46" title="">2023</a>)</cite>.
A pair of scissors will dramatically change its physical appearance due to mechanical operation and a model trained on the image of a closed pair of scissors might produce lower accuracy at recognizing the same pair in an open configuration.
Similarly, a plush toy that changes its shape during manipulation when being affected by gravity will affect the performance of the pose estimation.
While one intuitive approach is to capture data while a human user is manipulating the objects, annotating such data at scale would be costly and error-prone.
</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To address the challenge, we propose RoCap, an automated pipeline to collect image data of appearance-changing object for 6D pose estimation using a robotic arm with minimum human intervention.
We deploy a robot arm to mimic human’s hand to manipulate the objects while capturing the image data as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S0.F1" title="Figure 1 ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_tag">1</span></a>.
The 6D poses of the object of each image can be obtained with robotic forward kinematics as each joint of the robotic arm is precisely controlled. Specifically, RoCap performs the data capturing process for eight different appearance-changing objects with deformable, transparent, reflective and articulated properties (Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S2.F3" title="Figure 3 ‣ 2.1. Object pose estimation ‣ 2. Related Work ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">We also implemented a simple pose estimation pipeline to quantitatively and qualitatively evaluate the pose estimation performance of the model trained on our collected data comparing against a few-shot learning pose estimation approach based on 3D reconstruction (Gen6D <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib33" title="">2022</a>)</cite>).
Both the quantitative and qualitative evaluation results demonstrate that existing work struggles with appearance-changing objects and our approach shows promise in overcoming these limitations with improved pose estimation accuracy.
</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">In summary, our contributions are two-fold:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">A robotic data collection pipeline</span> with a 6 DoF robotic arm which captures and annotates 6D pose data for objects that change their appearance during manipulation, addressing limitations in existing data collection methods.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">Quantitative and qualitative evaluations</span> to demonstrate the feasibility of the pipeline via improved accuracy of appearance-changing objects pose estimation by comparing with an advanced pose estimation method in the field of computer vision.
</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="656" id="S2.F2.g1" src="x2.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>3D reconstructed results for a transparent flask.</figcaption>
</figure>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Object pose estimation</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Object pose estimation plays a crucial role in various HCI applications such as augmented reality <cite class="ltx_cite ltx_citemacro_citep">(Suzuki et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib45" title="">2020</a>; Hettiarachchi and Wigdor, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib21" title="">2016</a>; Barnes et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib3" title="">2008</a>; Held et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib20" title="">2012</a>)</cite> and robotics and automation <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib30" title="">2022</a>)</cite>. Over recent decades, researchers have explored diverse approaches to predict an object’s pose.
These includes sensor applications like IMUs, physical marker techniques such as fiducial markers <cite class="ltx_cite ltx_citemacro_citep">(Kalaitzakis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib25" title="">2021</a>; Ulrich et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib47" title="">2022</a>; Garrido-Jurado et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib18" title="">2014</a>)</cite>, optic trackers <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib50" title="">2020</a>)</cite> 3D printed embedded QR code <cite class="ltx_cite ltx_citemacro_citep">(Dogan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib13" title="">2022</a>)</cite>, computer vision techniques such as color-based tracking <cite class="ltx_cite ltx_citemacro_citep">(Suzuki et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib45" title="">2020</a>)</cite>, feature point tracking <cite class="ltx_cite ltx_citemacro_citep">(Barnes et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib3" title="">2008</a>)</cite> and point cloud alignment <cite class="ltx_cite ltx_citemacro_citep">(Hettiarachchi and Wigdor, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib21" title="">2016</a>)</cite>.
Recent advancement in deep learning has unlocked new challenging tasks such as predicting the poses of hand-object interaction <cite class="ltx_cite ltx_citemacro_citep">(Hampali et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib19" title="">2020</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib32" title="">2021</a>; Chao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib7" title="">2021</a>)</cite>, articulated objects <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib31" title="">2020b</a>)</cite> and other problem setups <cite class="ltx_cite ltx_citemacro_citep">(Lugaresi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib34" title="">2019</a>; Ahmadyan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib2" title="">2021</a>; Danila Rukhovich, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib9" title="">2022</a>; Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib49" title="">2022</a>)</cite>.
Extending this line of research, RoCap focuses on a new problem setup where the objects will change their appearance during the manipulation.
Note that RoCap does not contribute new model architecture or algorithm to improve the performance in the field of deep learning. Instead, RoCap contributes a novel data collection method and
the data captured by the system can serve as great resources for researchers in the community of computer vision and machine learning to solve the downstream tracking problems.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="455" id="S2.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Example objects for each category that RoCap is focusing on, <span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.F3.4.1">Viewing-angle dependent</span>: (1) flask, (2) water bottle and, (3) pitcher, <span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.F3.5.2">Deformable</span>: (4) flexible frog and (5) stiff anpanman,
<span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.F3.6.3">Articulated</span>: (6) scissors, (7) spray head and (8) clamp.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Pose data collection</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Data-driven deep learning approaches require data annotated with ground truth labels. Yet, annotating 6D pose data is challenging, as it is hard to specify 3D bounding box on a 2D image.
To address this, researchers have investigated various methods including three primary strategies:
<span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.1">(i)</span> training on synthesized data, <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.2">(ii)</span> utilizing publicly available datasets and <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.3">(iii)</span> designing interactive tools for data collection.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1. </span>Synthetic data</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">One typical way is synthesizing data with the available resources such as the 3D model of the objects.
This approach is commonly used in tasks such as object segmentation <cite class="ltx_cite ltx_citemacro_citep">(Ros et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib41" title="">2016</a>)</cite> and object detection <cite class="ltx_cite ltx_citemacro_citep">(Dwibedi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib14" title="">2017</a>)</cite>.
And a standard way of using synthetic data in pose estimation is to obtain the 3D model and texture of the objects first and then render them with different target background <cite class="ltx_cite ltx_citemacro_citep">(Su et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib44" title="">2021</a>)</cite>.
Although synthetic data can be easily scaled, it comes with the drawback of a disparity between real and virtual data, which might impact model performance. Moreover, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S2.F2" title="Figure 2 ‣ 2. Related Work ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_tag">2</span></a>, the necessary step of object reconstruction may fail for our target objects, such as the flask.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2. </span>Real-world data</h4>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">An intuitive way to bypass the issue of synthesized data is to collect data in the real world.
In the recent years, researchers have adopted two major types of data collection methods.
The first is <span class="ltx_text ltx_font_bold" id="S2.SS2.SSS2.p1.1.1">“static object + moving camera”</span>, where the pose of the object is calculated from the pose of the camera, which can be read from the embedded sensor. Normally it requires a certain level of human labor as first couple frames need to be manually annotated by matching the 3D model to the physical object. For example, several publically available datasets have been collected in this way for benchmarking in the pose estimation domain, such as YCB Video dataset <cite class="ltx_cite ltx_citemacro_citep">(Xiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib48" title="">2017</a>)</cite>, Linemod <cite class="ltx_cite ltx_citemacro_citep">(Hinterstoisser et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib22" title="">2013</a>; Brachmann et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib4" title="">2014</a>)</cite> and T-Less <cite class="ltx_cite ltx_citemacro_citep">(Hodan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib23" title="">2017</a>)</cite>. Additionally, researchers have also developed interactive data collection pipeline to collect data on custom objects (<span class="ltx_text ltx_font_italic" id="S2.SS2.SSS2.p1.1.2">e.g.</span>, Label Fusion <cite class="ltx_cite ltx_citemacro_citep">(Marion et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib36" title="">2018</a>)</cite>. However, since the objects remain static, it is challenging to capture the appearance-changing features.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS2.p2">
<p class="ltx_p" id="S2.SS2.SSS2.p2.1">Another approach is <span class="ltx_text ltx_font_bold" id="S2.SS2.SSS2.p2.1.1">“moving objects + static or moving camera”</span>. While effective for capturing appearance-changing objects, this approach poses challenging for labeling ground truth.
For instance, ARnnotate, used in augmented reality <cite class="ltx_cite ltx_citemacro_citep">(Qian et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib40" title="">2022</a>)</cite>, requires users to hold and move the object along a recorded path, leading to potential errors, especially with objects like articulated items or deformed plush toys.
RoCap adopts this approach and ensures the labeled ground truth to be precise by calculating the robotic arm’s forward kinematics while it manipulates the object to capture the appearance-changing features.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Appearance-changing objects</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section we define and explain the importance of three categories of appearance-changing objects that we aim to track using RoCap. We collected and captured eight items from the three categories with RoCap.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Deformation</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Deformation refers to changes in the shape or size of an object due to external forces applied during manipulation (i.e., force of the hand and gravity).
Objects with naturally deformable features can include soft and malleable objects such as fabric materials, clothing and plush toys/stuffed animals.
During manipulation, the objects are affected by gravity all the time, leading to the deformation while the user is moving the objects into different orientation.
We picked two plush toys of different stiffness, anpanman (stiffer) (Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S2.F3" title="Figure 3 ‣ 2.1. Object pose estimation ‣ 2. Related Work ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_tag">3</span></a>(5)) and frog (more flexible)(Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S2.F3" title="Figure 3 ‣ 2.1. Object pose estimation ‣ 2. Related Work ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_tag">3</span></a>(4)) as examples of the deformable objects.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Viewing-angle dependent</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The visual appearance of viewing-angle dependent objects includes two main sub-categories of objects, transparent objects (e.g., glass) and reflective objects (e.g., polished metal).
Appearance of transparent objects depends on the background behind them, which may contain the environment and the user’s hands. Tracking and estimating the pose of such transparent objects is a known challenge <cite class="ltx_cite ltx_citemacro_citep">(Fan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib15" title="">2021</a>)</cite> and hand manipulation may make this even harder.
Appearance of reflective objects on the other hand depends on the environment in front and around it.
We picked a conical flask and a plastic bottle(Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S2.F3" title="Figure 3 ‣ 2.1. Object pose estimation ‣ 2. Related Work ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_tag">3</span></a>(1, 2)) as representations of transparent objects of different level of translucency (Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S4.F7" title="Figure 7 ‣ 4.2.1. Pose coverage ‣ 4.2. Data collection ‣ 4. RoCap pipeline ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_tag">7</span></a>b). We also included a reflective pitcher to represent reflective object.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Articulated</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Objects with articulated features refer to objects whose appearance changes through manual manipulation or interaction.
These changes can occur due to the inherent function of the physical objects. For examples, various handheld tools will change their mechanical forms while being manipulated by human.
We selected three manually-changing objects: a clamp, a pair of scissors and, a head of spray bottle to represent two different types of manual gripping and hand operation (holding and pinching).
</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="356" id="S3.F4.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span><span class="ltx_text ltx_font_italic" id="S3.F4.2.1">Overview of RoCap.</span> RoCap pipeline consists of camera calibration (§<a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S4.SS1" title="4.1. Eye-to-hand camera calibration ‣ 4. RoCap pipeline ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_tag">4.1</span></a>), data capturing (§<a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S4.SS2" title="4.2. Data collection ‣ 4. RoCap pipeline ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_tag">4.2</span></a>), data labeling (§<a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S4.SS3" title="4.3. Data labeling ‣ 4. RoCap pipeline ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_tag">4.3</span></a>), data processing (§<a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S4.SS4" title="4.4. Data processing and augmentation ‣ 4. RoCap pipeline ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_tag">4.4</span></a>) and data augmentation (§<a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S4.SS4" title="4.4. Data processing and augmentation ‣ 4. RoCap pipeline ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_tag">4.4</span></a>). By training on an existing deep learning framework, RoCap achieves object segmentation, state classification and pose estimation for appearance-changing objects.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>RoCap pipeline</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we will introduce the design of the RoCap pipeline, which is easily replicable using any 6-DoF robotic arm, we document the essential knowledge and technical challenges addressed including <span class="ltx_text ltx_font_italic" id="S4.p1.1.1">(i)</span> camera calibration, <span class="ltx_text ltx_font_italic" id="S4.p1.1.2">(ii)</span> data collection, <span class="ltx_text ltx_font_italic" id="S4.p1.1.3">(iii)</span> data labeling and <span class="ltx_text ltx_font_italic" id="S4.p1.1.4">(iv)</span> data pre-processing.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S3.F4" title="Figure 4 ‣ 3.3. Articulated ‣ 3. Appearance-changing objects ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_tag">4</span></a> shows the overview of the RoCap pipeline and we discuss each step in details as follows.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Eye-to-hand camera calibration</h3>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="212" id="S4.F5.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>Illustration of the eye-to-hand camera calibration (a). The robotic arm grip a checkerboard and move to multiple positions and orientations for an accurate calibration (b).</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The first step of RoCap pipeline is to calibrate the camera to the robotic arm (Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S3.F4" title="Figure 4 ‣ 3.3. Articulated ‣ 3. Appearance-changing objects ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_tag">4</span></a>a).
During data collection, the robotic arm will hold the target object using a gripper and the camera is standing on the side to capture the images.
In this setup, the pose of the object in the image refers to the homogeneous transformation of the object from its reference frame to the camera’s reference frame.
This is a typical hand-eye calibration problem because as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S4.F5" title="Figure 5 ‣ 4.1. Eye-to-hand camera calibration ‣ 4. RoCap pipeline ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_tag">5</span></a>.
Assuming the object has the same pose as the end-effector, the goal is to calculate the transformation matrix of the gripper to the camera: <math alttext="{}^{c}T_{g}" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><mmultiscripts id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mi id="S4.SS1.p1.1.m1.1.1.2.2" xref="S4.SS1.p1.1.m1.1.1.2.2.cmml">T</mi><mi id="S4.SS1.p1.1.m1.1.1.2.3" xref="S4.SS1.p1.1.m1.1.1.2.3.cmml">g</mi><mrow id="S4.SS1.p1.1.m1.1.1a" xref="S4.SS1.p1.1.m1.1.1.cmml"></mrow><mprescripts id="S4.SS1.p1.1.m1.1.1b" xref="S4.SS1.p1.1.m1.1.1.cmml"></mprescripts><mrow id="S4.SS1.p1.1.m1.1.1c" xref="S4.SS1.p1.1.m1.1.1.cmml"></mrow><mi id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml">c</mi></mmultiscripts><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">superscript</csymbol><apply id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.1.m1.1.1.2.1.cmml" xref="S4.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.p1.1.m1.1.1.2.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2.2">𝑇</ci><ci id="S4.SS1.p1.1.m1.1.1.2.3.cmml" xref="S4.SS1.p1.1.m1.1.1.2.3">𝑔</ci></apply><ci id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">{}^{c}T_{g}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">start_FLOATSUPERSCRIPT italic_c end_FLOATSUPERSCRIPT italic_T start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT</annotation></semantics></math>, which can be calculated from the following equation:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="^{c}T_{g}\ =\ ^{c}T_{b}\cdot^{b}T_{g}" class="ltx_math_unparsed" display="block" id="S4.E1.m1.1"><semantics id="S4.E1.m1.1a"><mrow id="S4.E1.m1.1b"><msup id="S4.E1.m1.1.1"><mi id="S4.E1.m1.1.1a"></mi><mi id="S4.E1.m1.1.1.1">c</mi></msup><msub id="S4.E1.m1.1.2"><mi id="S4.E1.m1.1.2.2">T</mi><mi id="S4.E1.m1.1.2.3">g</mi></msub><msup id="S4.E1.m1.1.3"><mo id="S4.E1.m1.1.3.2" rspace="0.778em">=</mo><mi id="S4.E1.m1.1.3.3">c</mi></msup><msub id="S4.E1.m1.1.4"><mi id="S4.E1.m1.1.4.2">T</mi><mi id="S4.E1.m1.1.4.3">b</mi></msub><msup id="S4.E1.m1.1.5"><mo id="S4.E1.m1.1.5.2" lspace="0.222em" rspace="0.222em">⋅</mo><mi id="S4.E1.m1.1.5.3">b</mi></msup><msub id="S4.E1.m1.1.6"><mi id="S4.E1.m1.1.6.2">T</mi><mi id="S4.E1.m1.1.6.3">g</mi></msub></mrow><annotation encoding="application/x-tex" id="S4.E1.m1.1c">^{c}T_{g}\ =\ ^{c}T_{b}\cdot^{b}T_{g}</annotation><annotation encoding="application/x-llamapun" id="S4.E1.m1.1d">start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT italic_T start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT = start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT italic_T start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ⋅ start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT italic_T start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS1.p1.3">In which <math alttext="{}^{b}T_{g}" class="ltx_Math" display="inline" id="S4.SS1.p1.2.m1.1"><semantics id="S4.SS1.p1.2.m1.1a"><mmultiscripts id="S4.SS1.p1.2.m1.1.1" xref="S4.SS1.p1.2.m1.1.1.cmml"><mi id="S4.SS1.p1.2.m1.1.1.2.2" xref="S4.SS1.p1.2.m1.1.1.2.2.cmml">T</mi><mi id="S4.SS1.p1.2.m1.1.1.2.3" xref="S4.SS1.p1.2.m1.1.1.2.3.cmml">g</mi><mrow id="S4.SS1.p1.2.m1.1.1a" xref="S4.SS1.p1.2.m1.1.1.cmml"></mrow><mprescripts id="S4.SS1.p1.2.m1.1.1b" xref="S4.SS1.p1.2.m1.1.1.cmml"></mprescripts><mrow id="S4.SS1.p1.2.m1.1.1c" xref="S4.SS1.p1.2.m1.1.1.cmml"></mrow><mi id="S4.SS1.p1.2.m1.1.1.3" xref="S4.SS1.p1.2.m1.1.1.3.cmml">b</mi></mmultiscripts><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m1.1b"><apply id="S4.SS1.p1.2.m1.1.1.cmml" xref="S4.SS1.p1.2.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.2.m1.1.1.1.cmml" xref="S4.SS1.p1.2.m1.1.1">superscript</csymbol><apply id="S4.SS1.p1.2.m1.1.1.2.cmml" xref="S4.SS1.p1.2.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.2.m1.1.1.2.1.cmml" xref="S4.SS1.p1.2.m1.1.1">subscript</csymbol><ci id="S4.SS1.p1.2.m1.1.1.2.2.cmml" xref="S4.SS1.p1.2.m1.1.1.2.2">𝑇</ci><ci id="S4.SS1.p1.2.m1.1.1.2.3.cmml" xref="S4.SS1.p1.2.m1.1.1.2.3">𝑔</ci></apply><ci id="S4.SS1.p1.2.m1.1.1.3.cmml" xref="S4.SS1.p1.2.m1.1.1.3">𝑏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m1.1c">{}^{b}T_{g}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.2.m1.1d">start_FLOATSUPERSCRIPT italic_b end_FLOATSUPERSCRIPT italic_T start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT</annotation></semantics></math> refers to the transformation from the gripper to the base of the robotic arm which could be calculated by forward kinematics <cite class="ltx_cite ltx_citemacro_citep">(Denavit and Hartenberg, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib11" title="">1955</a>)</cite> while <math alttext="{}^{c}T_{b}" class="ltx_Math" display="inline" id="S4.SS1.p1.3.m2.1"><semantics id="S4.SS1.p1.3.m2.1a"><mmultiscripts id="S4.SS1.p1.3.m2.1.1" xref="S4.SS1.p1.3.m2.1.1.cmml"><mi id="S4.SS1.p1.3.m2.1.1.2.2" xref="S4.SS1.p1.3.m2.1.1.2.2.cmml">T</mi><mi id="S4.SS1.p1.3.m2.1.1.2.3" xref="S4.SS1.p1.3.m2.1.1.2.3.cmml">b</mi><mrow id="S4.SS1.p1.3.m2.1.1a" xref="S4.SS1.p1.3.m2.1.1.cmml"></mrow><mprescripts id="S4.SS1.p1.3.m2.1.1b" xref="S4.SS1.p1.3.m2.1.1.cmml"></mprescripts><mrow id="S4.SS1.p1.3.m2.1.1c" xref="S4.SS1.p1.3.m2.1.1.cmml"></mrow><mi id="S4.SS1.p1.3.m2.1.1.3" xref="S4.SS1.p1.3.m2.1.1.3.cmml">c</mi></mmultiscripts><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m2.1b"><apply id="S4.SS1.p1.3.m2.1.1.cmml" xref="S4.SS1.p1.3.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.3.m2.1.1.1.cmml" xref="S4.SS1.p1.3.m2.1.1">superscript</csymbol><apply id="S4.SS1.p1.3.m2.1.1.2.cmml" xref="S4.SS1.p1.3.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.3.m2.1.1.2.1.cmml" xref="S4.SS1.p1.3.m2.1.1">subscript</csymbol><ci id="S4.SS1.p1.3.m2.1.1.2.2.cmml" xref="S4.SS1.p1.3.m2.1.1.2.2">𝑇</ci><ci id="S4.SS1.p1.3.m2.1.1.2.3.cmml" xref="S4.SS1.p1.3.m2.1.1.2.3">𝑏</ci></apply><ci id="S4.SS1.p1.3.m2.1.1.3.cmml" xref="S4.SS1.p1.3.m2.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m2.1c">{}^{c}T_{b}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.3.m2.1d">start_FLOATSUPERSCRIPT italic_c end_FLOATSUPERSCRIPT italic_T start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT</annotation></semantics></math> refers to the transformation from the base of the robotic arm to the camera frame, which is unknown.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.3">To calculate <math alttext="{}^{c}T_{b}" class="ltx_Math" display="inline" id="S4.SS1.p2.1.m1.1"><semantics id="S4.SS1.p2.1.m1.1a"><mmultiscripts id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><mi id="S4.SS1.p2.1.m1.1.1.2.2" xref="S4.SS1.p2.1.m1.1.1.2.2.cmml">T</mi><mi id="S4.SS1.p2.1.m1.1.1.2.3" xref="S4.SS1.p2.1.m1.1.1.2.3.cmml">b</mi><mrow id="S4.SS1.p2.1.m1.1.1a" xref="S4.SS1.p2.1.m1.1.1.cmml"></mrow><mprescripts id="S4.SS1.p2.1.m1.1.1b" xref="S4.SS1.p2.1.m1.1.1.cmml"></mprescripts><mrow id="S4.SS1.p2.1.m1.1.1c" xref="S4.SS1.p2.1.m1.1.1.cmml"></mrow><mi id="S4.SS1.p2.1.m1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.3.cmml">c</mi></mmultiscripts><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">superscript</csymbol><apply id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.1.m1.1.1.2.1.cmml" xref="S4.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.p2.1.m1.1.1.2.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2.2">𝑇</ci><ci id="S4.SS1.p2.1.m1.1.1.2.3.cmml" xref="S4.SS1.p2.1.m1.1.1.2.3">𝑏</ci></apply><ci id="S4.SS1.p2.1.m1.1.1.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">{}^{c}T_{b}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.1.m1.1d">start_FLOATSUPERSCRIPT italic_c end_FLOATSUPERSCRIPT italic_T start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT</annotation></semantics></math>, a camera calibration step is required which can be accomplished by using a checkerboard with known size of the squares, which is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S4.F5" title="Figure 5 ‣ 4.1. Eye-to-hand camera calibration ‣ 4. RoCap pipeline ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_tag">5</span></a>b .
By moving the robotic arm to multiple configuration, <math alttext="{}^{c}T_{g}" class="ltx_Math" display="inline" id="S4.SS1.p2.2.m2.1"><semantics id="S4.SS1.p2.2.m2.1a"><mmultiscripts id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml"><mi id="S4.SS1.p2.2.m2.1.1.2.2" xref="S4.SS1.p2.2.m2.1.1.2.2.cmml">T</mi><mi id="S4.SS1.p2.2.m2.1.1.2.3" xref="S4.SS1.p2.2.m2.1.1.2.3.cmml">g</mi><mrow id="S4.SS1.p2.2.m2.1.1a" xref="S4.SS1.p2.2.m2.1.1.cmml"></mrow><mprescripts id="S4.SS1.p2.2.m2.1.1b" xref="S4.SS1.p2.2.m2.1.1.cmml"></mprescripts><mrow id="S4.SS1.p2.2.m2.1.1c" xref="S4.SS1.p2.2.m2.1.1.cmml"></mrow><mi id="S4.SS1.p2.2.m2.1.1.3" xref="S4.SS1.p2.2.m2.1.1.3.cmml">c</mi></mmultiscripts><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><apply id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.2.m2.1.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">superscript</csymbol><apply id="S4.SS1.p2.2.m2.1.1.2.cmml" xref="S4.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.2.m2.1.1.2.1.cmml" xref="S4.SS1.p2.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.p2.2.m2.1.1.2.2.cmml" xref="S4.SS1.p2.2.m2.1.1.2.2">𝑇</ci><ci id="S4.SS1.p2.2.m2.1.1.2.3.cmml" xref="S4.SS1.p2.2.m2.1.1.2.3">𝑔</ci></apply><ci id="S4.SS1.p2.2.m2.1.1.3.cmml" xref="S4.SS1.p2.2.m2.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">{}^{c}T_{g}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.2.m2.1d">start_FLOATSUPERSCRIPT italic_c end_FLOATSUPERSCRIPT italic_T start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT</annotation></semantics></math> can be calculated from the following <math alttext="AX=XB" class="ltx_Math" display="inline" id="S4.SS1.p2.3.m3.1"><semantics id="S4.SS1.p2.3.m3.1a"><mrow id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml"><mrow id="S4.SS1.p2.3.m3.1.1.2" xref="S4.SS1.p2.3.m3.1.1.2.cmml"><mi id="S4.SS1.p2.3.m3.1.1.2.2" xref="S4.SS1.p2.3.m3.1.1.2.2.cmml">A</mi><mo id="S4.SS1.p2.3.m3.1.1.2.1" xref="S4.SS1.p2.3.m3.1.1.2.1.cmml">⁢</mo><mi id="S4.SS1.p2.3.m3.1.1.2.3" xref="S4.SS1.p2.3.m3.1.1.2.3.cmml">X</mi></mrow><mo id="S4.SS1.p2.3.m3.1.1.1" xref="S4.SS1.p2.3.m3.1.1.1.cmml">=</mo><mrow id="S4.SS1.p2.3.m3.1.1.3" xref="S4.SS1.p2.3.m3.1.1.3.cmml"><mi id="S4.SS1.p2.3.m3.1.1.3.2" xref="S4.SS1.p2.3.m3.1.1.3.2.cmml">X</mi><mo id="S4.SS1.p2.3.m3.1.1.3.1" xref="S4.SS1.p2.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p2.3.m3.1.1.3.3" xref="S4.SS1.p2.3.m3.1.1.3.3.cmml">B</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><apply id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1"><eq id="S4.SS1.p2.3.m3.1.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1.1"></eq><apply id="S4.SS1.p2.3.m3.1.1.2.cmml" xref="S4.SS1.p2.3.m3.1.1.2"><times id="S4.SS1.p2.3.m3.1.1.2.1.cmml" xref="S4.SS1.p2.3.m3.1.1.2.1"></times><ci id="S4.SS1.p2.3.m3.1.1.2.2.cmml" xref="S4.SS1.p2.3.m3.1.1.2.2">𝐴</ci><ci id="S4.SS1.p2.3.m3.1.1.2.3.cmml" xref="S4.SS1.p2.3.m3.1.1.2.3">𝑋</ci></apply><apply id="S4.SS1.p2.3.m3.1.1.3.cmml" xref="S4.SS1.p2.3.m3.1.1.3"><times id="S4.SS1.p2.3.m3.1.1.3.1.cmml" xref="S4.SS1.p2.3.m3.1.1.3.1"></times><ci id="S4.SS1.p2.3.m3.1.1.3.2.cmml" xref="S4.SS1.p2.3.m3.1.1.3.2">𝑋</ci><ci id="S4.SS1.p2.3.m3.1.1.3.3.cmml" xref="S4.SS1.p2.3.m3.1.1.3.3">𝐵</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">AX=XB</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.3.m3.1d">italic_A italic_X = italic_X italic_B</annotation></semantics></math> equations:</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<table class="ltx_equation ltx_eqn_table" id="S4.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\begin{gathered}{}^{g}T_{b}^{\left(1\right)}\ {}^{b}T_{c}\ ^{c}T_{t}^{\left(1%
\right)}=^{g}T_{b}^{\left(2\right)}\ {}^{b}T_{c}\ ^{c}T_{t}^{\left(2\right)}\\
\bigl{(}{{}^{g}T_{b}^{\left(2\right)}}\bigr{)}^{-1}\ {}^{b}T_{g}^{\left(1%
\right)}\ {}^{g}T_{c}=^{g}T_{c}\ ^{c}T_{t}^{\left(2\right)}\bigl{(}^{c}T_{t}^{%
\left(1\right)}\bigr{)}^{-1}\\
A_{i}X=XB_{i}\end{gathered}" class="ltx_math_unparsed" display="block" id="S4.E2.m1.59"><semantics id="S4.E2.m1.59a"><mtable displaystyle="true" id="S4.E2.m1.59.59" rowspacing="0pt"><mtr id="S4.E2.m1.59.59a"><mtd id="S4.E2.m1.59.59b"><mrow id="S4.E2.m1.23.23.23.23.23"><mrow id="S4.E2.m1.23.23.23.23.23.25"><mmultiscripts id="S4.E2.m1.23.23.23.23.23.25.2"><mi id="S4.E2.m1.2.2.2.2.2.2">T</mi><mi id="S4.E2.m1.3.3.3.3.3.3.1">b</mi><mrow id="S4.E2.m1.4.4.4.4.4.4.1.3"><mo id="S4.E2.m1.4.4.4.4.4.4.1.3.1">(</mo><mn id="S4.E2.m1.4.4.4.4.4.4.1.1">1</mn><mo id="S4.E2.m1.4.4.4.4.4.4.1.3.2">)</mo></mrow><mprescripts id="S4.E2.m1.23.23.23.23.23.25.2a"></mprescripts><mrow id="S4.E2.m1.23.23.23.23.23.25.2b"></mrow><mi id="S4.E2.m1.1.1.1.1.1.1.1">g</mi></mmultiscripts><mo id="S4.E2.m1.23.23.23.23.23.25.1">⁢</mo><mmultiscripts id="S4.E2.m1.23.23.23.23.23.25.3"><mi id="S4.E2.m1.6.6.6.6.6.6">T</mi><mi id="S4.E2.m1.7.7.7.7.7.7.1">c</mi><mi id="S4.E2.m1.8.8.8.8.8.8.1">c</mi><mprescripts id="S4.E2.m1.23.23.23.23.23.25.3a"></mprescripts><mrow id="S4.E2.m1.23.23.23.23.23.25.3b"></mrow><mi id="S4.E2.m1.5.5.5.5.5.5.1">b</mi></mmultiscripts><mo id="S4.E2.m1.23.23.23.23.23.25.1a">⁢</mo><msubsup id="S4.E2.m1.23.23.23.23.23.25.4"><mi id="S4.E2.m1.9.9.9.9.9.9">T</mi><mi id="S4.E2.m1.10.10.10.10.10.10.1">t</mi><mrow id="S4.E2.m1.11.11.11.11.11.11.1.3"><mo id="S4.E2.m1.11.11.11.11.11.11.1.3.1">(</mo><mn id="S4.E2.m1.11.11.11.11.11.11.1.1">1</mn><mo id="S4.E2.m1.11.11.11.11.11.11.1.3.2">)</mo></mrow></msubsup></mrow><msup id="S4.E2.m1.23.23.23.23.23.24"><mo id="S4.E2.m1.12.12.12.12.12.12">=</mo><mi id="S4.E2.m1.13.13.13.13.13.13.1">g</mi></msup><mrow id="S4.E2.m1.23.23.23.23.23.26"><msubsup id="S4.E2.m1.23.23.23.23.23.26.2"><mi id="S4.E2.m1.14.14.14.14.14.14">T</mi><mi id="S4.E2.m1.15.15.15.15.15.15.1">b</mi><mrow id="S4.E2.m1.16.16.16.16.16.16.1.3"><mo id="S4.E2.m1.16.16.16.16.16.16.1.3.1">(</mo><mn id="S4.E2.m1.16.16.16.16.16.16.1.1">2</mn><mo id="S4.E2.m1.16.16.16.16.16.16.1.3.2">)</mo></mrow></msubsup><mo id="S4.E2.m1.23.23.23.23.23.26.1">⁢</mo><mmultiscripts id="S4.E2.m1.23.23.23.23.23.26.3"><mi id="S4.E2.m1.18.18.18.18.18.18">T</mi><mi id="S4.E2.m1.19.19.19.19.19.19.1">c</mi><mi id="S4.E2.m1.20.20.20.20.20.20.1">c</mi><mprescripts id="S4.E2.m1.23.23.23.23.23.26.3a"></mprescripts><mrow id="S4.E2.m1.23.23.23.23.23.26.3b"></mrow><mi id="S4.E2.m1.17.17.17.17.17.17.1">b</mi></mmultiscripts><mo id="S4.E2.m1.23.23.23.23.23.26.1a">⁢</mo><msubsup id="S4.E2.m1.23.23.23.23.23.26.4"><mi id="S4.E2.m1.21.21.21.21.21.21">T</mi><mi id="S4.E2.m1.22.22.22.22.22.22.1">t</mi><mrow id="S4.E2.m1.23.23.23.23.23.23.1.3"><mo id="S4.E2.m1.23.23.23.23.23.23.1.3.1">(</mo><mn id="S4.E2.m1.23.23.23.23.23.23.1.1">2</mn><mo id="S4.E2.m1.23.23.23.23.23.23.1.3.2">)</mo></mrow></msubsup></mrow></mrow></mtd></mtr><mtr id="S4.E2.m1.59.59c"><mtd id="S4.E2.m1.59.59d"><mrow id="S4.E2.m1.52.52.52.29.29"><msup id="S4.E2.m1.52.52.52.29.29.30"><mrow id="S4.E2.m1.52.52.52.29.29.30.2"><mo id="S4.E2.m1.24.24.24.1.1.1" maxsize="120%" minsize="120%">(</mo><mmultiscripts id="S4.E2.m1.52.52.52.29.29.30.2.1"><mi id="S4.E2.m1.26.26.26.3.3.3">T</mi><mi id="S4.E2.m1.27.27.27.4.4.4.1">b</mi><mrow id="S4.E2.m1.28.28.28.5.5.5.1.3"><mo id="S4.E2.m1.28.28.28.5.5.5.1.3.1">(</mo><mn id="S4.E2.m1.28.28.28.5.5.5.1.1">2</mn><mo id="S4.E2.m1.28.28.28.5.5.5.1.3.2">)</mo></mrow><mprescripts id="S4.E2.m1.52.52.52.29.29.30.2.1a"></mprescripts><mrow id="S4.E2.m1.52.52.52.29.29.30.2.1b"></mrow><mi id="S4.E2.m1.25.25.25.2.2.2.1">g</mi></mmultiscripts><mo id="S4.E2.m1.29.29.29.6.6.6" maxsize="120%" minsize="120%">)</mo></mrow><mrow id="S4.E2.m1.30.30.30.7.7.7.1"><mo id="S4.E2.m1.30.30.30.7.7.7.1a">−</mo><mn id="S4.E2.m1.30.30.30.7.7.7.1.2">1</mn></mrow></msup><mmultiscripts id="S4.E2.m1.52.52.52.29.29.31"><mi id="S4.E2.m1.32.32.32.9.9.9">T</mi><mi id="S4.E2.m1.33.33.33.10.10.10.1">g</mi><mrow id="S4.E2.m1.34.34.34.11.11.11.1.3"><mo id="S4.E2.m1.34.34.34.11.11.11.1.3.1">(</mo><mn id="S4.E2.m1.34.34.34.11.11.11.1.1">1</mn><mo id="S4.E2.m1.34.34.34.11.11.11.1.3.2">)</mo></mrow><mprescripts id="S4.E2.m1.52.52.52.29.29.31a"></mprescripts><mrow id="S4.E2.m1.52.52.52.29.29.31b"></mrow><mi id="S4.E2.m1.31.31.31.8.8.8.1">b</mi></mmultiscripts><mmultiscripts id="S4.E2.m1.52.52.52.29.29.32"><mi id="S4.E2.m1.36.36.36.13.13.13">T</mi><mi id="S4.E2.m1.37.37.37.14.14.14.1">c</mi><mrow id="S4.E2.m1.52.52.52.29.29.32a"></mrow><mprescripts id="S4.E2.m1.52.52.52.29.29.32b"></mprescripts><mrow id="S4.E2.m1.52.52.52.29.29.32c"></mrow><mi id="S4.E2.m1.35.35.35.12.12.12.1">g</mi></mmultiscripts><msup id="S4.E2.m1.52.52.52.29.29.33"><mo id="S4.E2.m1.38.38.38.15.15.15">=</mo><mi id="S4.E2.m1.39.39.39.16.16.16.1">g</mi></msup><msubsup id="S4.E2.m1.52.52.52.29.29.34"><mi id="S4.E2.m1.40.40.40.17.17.17">T</mi><mi id="S4.E2.m1.41.41.41.18.18.18.1">c</mi><mi id="S4.E2.m1.42.42.42.19.19.19.1">c</mi></msubsup><msubsup id="S4.E2.m1.52.52.52.29.29.35"><mi id="S4.E2.m1.43.43.43.20.20.20">T</mi><mi id="S4.E2.m1.44.44.44.21.21.21.1">t</mi><mrow id="S4.E2.m1.45.45.45.22.22.22.1.3"><mo id="S4.E2.m1.45.45.45.22.22.22.1.3.1">(</mo><mn id="S4.E2.m1.45.45.45.22.22.22.1.1">2</mn><mo id="S4.E2.m1.45.45.45.22.22.22.1.3.2">)</mo></mrow></msubsup><msup id="S4.E2.m1.52.52.52.29.29.36"><mrow id="S4.E2.m1.52.52.52.29.29.36.2"><msup id="S4.E2.m1.52.52.52.29.29.36.2.1"><mo id="S4.E2.m1.46.46.46.23.23.23" maxsize="120%" minsize="120%">(</mo><mi id="S4.E2.m1.47.47.47.24.24.24.1">c</mi></msup><msubsup id="S4.E2.m1.52.52.52.29.29.36.2.2"><mi id="S4.E2.m1.48.48.48.25.25.25">T</mi><mi id="S4.E2.m1.49.49.49.26.26.26.1">t</mi><mrow id="S4.E2.m1.50.50.50.27.27.27.1.3"><mo id="S4.E2.m1.50.50.50.27.27.27.1.3.1">(</mo><mn id="S4.E2.m1.50.50.50.27.27.27.1.1">1</mn><mo id="S4.E2.m1.50.50.50.27.27.27.1.3.2">)</mo></mrow></msubsup><mo id="S4.E2.m1.51.51.51.28.28.28" maxsize="120%" minsize="120%">)</mo></mrow><mrow id="S4.E2.m1.52.52.52.29.29.29.1"><mo id="S4.E2.m1.52.52.52.29.29.29.1a">−</mo><mn id="S4.E2.m1.52.52.52.29.29.29.1.2">1</mn></mrow></msup></mrow></mtd></mtr><mtr id="S4.E2.m1.59.59e"><mtd id="S4.E2.m1.59.59f"><mrow id="S4.E2.m1.59.59.59.7.7"><mrow id="S4.E2.m1.59.59.59.7.7.8"><msub id="S4.E2.m1.59.59.59.7.7.8.2"><mi id="S4.E2.m1.53.53.53.1.1.1">A</mi><mi id="S4.E2.m1.54.54.54.2.2.2.1">i</mi></msub><mo id="S4.E2.m1.59.59.59.7.7.8.1">⁢</mo><mi id="S4.E2.m1.55.55.55.3.3.3">X</mi></mrow><mo id="S4.E2.m1.56.56.56.4.4.4">=</mo><mrow id="S4.E2.m1.59.59.59.7.7.9"><mi id="S4.E2.m1.57.57.57.5.5.5">X</mi><mo id="S4.E2.m1.59.59.59.7.7.9.1">⁢</mo><msub id="S4.E2.m1.59.59.59.7.7.9.2"><mi id="S4.E2.m1.58.58.58.6.6.6">B</mi><mi id="S4.E2.m1.59.59.59.7.7.7.1">i</mi></msub></mrow></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex" id="S4.E2.m1.59b">\begin{gathered}{}^{g}T_{b}^{\left(1\right)}\ {}^{b}T_{c}\ ^{c}T_{t}^{\left(1%
\right)}=^{g}T_{b}^{\left(2\right)}\ {}^{b}T_{c}\ ^{c}T_{t}^{\left(2\right)}\\
\bigl{(}{{}^{g}T_{b}^{\left(2\right)}}\bigr{)}^{-1}\ {}^{b}T_{g}^{\left(1%
\right)}\ {}^{g}T_{c}=^{g}T_{c}\ ^{c}T_{t}^{\left(2\right)}\bigl{(}^{c}T_{t}^{%
\left(1\right)}\bigr{)}^{-1}\\
A_{i}X=XB_{i}\end{gathered}</annotation><annotation encoding="application/x-llamapun" id="S4.E2.m1.59c">start_ROW start_CELL start_FLOATSUPERSCRIPT italic_g end_FLOATSUPERSCRIPT italic_T start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT italic_b end_FLOATSUPERSCRIPT italic_T start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT italic_T start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT = start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT italic_T start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 2 ) end_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT italic_b end_FLOATSUPERSCRIPT italic_T start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT italic_T start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 2 ) end_POSTSUPERSCRIPT end_CELL end_ROW start_ROW start_CELL ( start_FLOATSUPERSCRIPT italic_g end_FLOATSUPERSCRIPT italic_T start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 2 ) end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT italic_b end_FLOATSUPERSCRIPT italic_T start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT italic_g end_FLOATSUPERSCRIPT italic_T start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT = start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT italic_T start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT italic_T start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 2 ) end_POSTSUPERSCRIPT ( start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT italic_T start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_CELL end_ROW start_ROW start_CELL italic_A start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_X = italic_X italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS1.p3.2">Here <math alttext="{}^{c}T_{t}" class="ltx_Math" display="inline" id="S4.SS1.p3.1.m1.1"><semantics id="S4.SS1.p3.1.m1.1a"><mmultiscripts id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml"><mi id="S4.SS1.p3.1.m1.1.1.2.2" xref="S4.SS1.p3.1.m1.1.1.2.2.cmml">T</mi><mi id="S4.SS1.p3.1.m1.1.1.2.3" xref="S4.SS1.p3.1.m1.1.1.2.3.cmml">t</mi><mrow id="S4.SS1.p3.1.m1.1.1a" xref="S4.SS1.p3.1.m1.1.1.cmml"></mrow><mprescripts id="S4.SS1.p3.1.m1.1.1b" xref="S4.SS1.p3.1.m1.1.1.cmml"></mprescripts><mrow id="S4.SS1.p3.1.m1.1.1c" xref="S4.SS1.p3.1.m1.1.1.cmml"></mrow><mi id="S4.SS1.p3.1.m1.1.1.3" xref="S4.SS1.p3.1.m1.1.1.3.cmml">c</mi></mmultiscripts><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><apply id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.1.m1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1">superscript</csymbol><apply id="S4.SS1.p3.1.m1.1.1.2.cmml" xref="S4.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.1.m1.1.1.2.1.cmml" xref="S4.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.p3.1.m1.1.1.2.2.cmml" xref="S4.SS1.p3.1.m1.1.1.2.2">𝑇</ci><ci id="S4.SS1.p3.1.m1.1.1.2.3.cmml" xref="S4.SS1.p3.1.m1.1.1.2.3">𝑡</ci></apply><ci id="S4.SS1.p3.1.m1.1.1.3.cmml" xref="S4.SS1.p3.1.m1.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">{}^{c}T_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.1.m1.1d">start_FLOATSUPERSCRIPT italic_c end_FLOATSUPERSCRIPT italic_T start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> refers to the transformation from the checkerboard to the camera frame, which can be calculated knowing the size of the pattern <cite class="ltx_cite ltx_citemacro_citep">(Park and Martin, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib38" title="">1994</a>)</cite>.
Then the calibration target <math alttext="{}^{c}T_{b}" class="ltx_Math" display="inline" id="S4.SS1.p3.2.m2.1"><semantics id="S4.SS1.p3.2.m2.1a"><mmultiscripts id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml"><mi id="S4.SS1.p3.2.m2.1.1.2.2" xref="S4.SS1.p3.2.m2.1.1.2.2.cmml">T</mi><mi id="S4.SS1.p3.2.m2.1.1.2.3" xref="S4.SS1.p3.2.m2.1.1.2.3.cmml">b</mi><mrow id="S4.SS1.p3.2.m2.1.1a" xref="S4.SS1.p3.2.m2.1.1.cmml"></mrow><mprescripts id="S4.SS1.p3.2.m2.1.1b" xref="S4.SS1.p3.2.m2.1.1.cmml"></mprescripts><mrow id="S4.SS1.p3.2.m2.1.1c" xref="S4.SS1.p3.2.m2.1.1.cmml"></mrow><mi id="S4.SS1.p3.2.m2.1.1.3" xref="S4.SS1.p3.2.m2.1.1.3.cmml">c</mi></mmultiscripts><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><apply id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.2.m2.1.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1">superscript</csymbol><apply id="S4.SS1.p3.2.m2.1.1.2.cmml" xref="S4.SS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.2.m2.1.1.2.1.cmml" xref="S4.SS1.p3.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.p3.2.m2.1.1.2.2.cmml" xref="S4.SS1.p3.2.m2.1.1.2.2">𝑇</ci><ci id="S4.SS1.p3.2.m2.1.1.2.3.cmml" xref="S4.SS1.p3.2.m2.1.1.2.3">𝑏</ci></apply><ci id="S4.SS1.p3.2.m2.1.1.3.cmml" xref="S4.SS1.p3.2.m2.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">{}^{c}T_{b}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.2.m2.1d">start_FLOATSUPERSCRIPT italic_c end_FLOATSUPERSCRIPT italic_T start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT</annotation></semantics></math> can be calculated from Eq. <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S4.E1" title="In 4.1. Eye-to-hand camera calibration ‣ 4. RoCap pipeline ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">After the camera is calibrated, the next step is to capture the image data of the objects.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Data collection</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">As mentioned in the previous sections, RoCap collect data of the objects that exhibit the appearance-changing features.
More specifically, RoCap collects objects categorized in four types of appearance-changing features: deformable, reflective, transparent and articulated.
</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="667" id="S4.F6.g1" src="x6.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>Pose coverage in RoCap capturing pipeline.</figcaption>
</figure>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1. </span>Pose coverage</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">The goal of the capturing is simple: capture the images of the objects from as many angles as possible to have a good coverage of all the potential pose.
Quaternions possess the advantage of representing each rotation without introducing any ambiguity. However, directly sampling quaternions proves to be a challenging task. To overcome this obstacle and achieve comprehensive coverage of poses, we opt for sampling Euler angles with a specific step of degrees for each yaw, pitch, and roll channel.
Once we have obtained the Euler angles, they are converted into quaternions. These quaternions are then utilized to calculate the arc distance between each orientation. This methodology is employed due to the inherent redundancies that can arise from sampling Euler angles. By computing the arc distance of quaternions, we effectively eliminate these redundancies. The threshold for eliminating redundancies is set at 0.35, roughly equivalent to a 20<sup class="ltx_sup" id="S4.SS2.SSS1.p1.1.1">∘</sup> azimuth angle.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p2">
<p class="ltx_p" id="S4.SS2.SSS1.p2.1">However, due to the hardware limitation of the robotic arm (<span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p2.1.1">e.g.</span>, the joints may have a limited range of motion), RoCap cannot cover the whole possible poses sampled in this process.
We use the inverse kinematics solver and path planners in ROS and achieve the final sampling of the poses RoCap supports.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S4.F6" title="Figure 6 ‣ 4.2. Data collection ‣ 4. RoCap pipeline ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_tag">6</span></a> visualizes the coverage of the poses in RoCap.
Noted that in existing data collection method where the objects are placed on floor, there will be at least half of the poses not capturable because it is occluded by the contacting ground.</p>
</div>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="417" id="S4.F7.g1" src="x7.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7. </span>RoCap captures the appearance-changing feature of deformable objects (a), viewing-angle dependent objects including transparent objects (b) and reflective objects (c), and objects with articulated features (d). Human operator is needed if the robotic arm is not able to change the states automatically (d).</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2. </span>Capturing process</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">During the capturing, a human user will be required to hand the target object to the robotic arm and the robotic arm will move along the designed path and the camera capture the RGB images on each sampled point.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p2">
<p class="ltx_p" id="S4.SS2.SSS2.p2.1">For deformable, reflective and transparent objects, there is no further actions from the users as the change of the appearance happens naturally when the object is oriented to different direction while being manipulated by the robotic arm <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S4.F7" title="Figure 7 ‣ 4.2.1. Pose coverage ‣ 4.2. Data collection ‣ 4. RoCap pipeline ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_tag">7</span></a>abc.
For articulated objects, actions need to be taken in order to change the mechanical states of the objects. The manually-changing action can be achieved either by human or the robotic arm automatically depending on the capability of the robotic arm to change the appearance.
As is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S0.F1" title="Figure 1 ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_tag">1</span></a>b, the size of the clamp is small enough to be grasped by the gripper. And the clamp is expected to have multiple states such as closed, open, and mid-open states. Without the help of human, the gripper could be able to change the states of the clamp by applying different forces on the parallel grippers.
However, for the pair shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S4.F7" title="Figure 7 ‣ 4.2.1. Pose coverage ‣ 4.2. Data collection ‣ 4. RoCap pipeline ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_tag">7</span></a>d, the robotic gripper is not able to automatically change the states because the handle is too wide for the parallel gripper when it is in open state.
This is a typical robotic manipulability problem as mentioned in <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib30" title="">2022</a>)</cite>.
For the case that a robotic arm cannot establish firm gripping on the object, a human operator will be required to manually change the opening angle of the scissors in the interval between the capturing of different states.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Data labeling</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">The transformation from the base frame of the robotic arm to the target object is logged for each captured image in a <math alttext="4\times 4" class="ltx_Math" display="inline" id="S4.SS3.p1.1.m1.1"><semantics id="S4.SS3.p1.1.m1.1a"><mrow id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mn id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml">4</mn><mo id="S4.SS3.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS3.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><times id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1.1"></times><cn id="S4.SS3.p1.1.m1.1.1.2.cmml" type="integer" xref="S4.SS3.p1.1.m1.1.1.2">4</cn><cn id="S4.SS3.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.SS3.p1.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">4\times 4</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.1.m1.1d">4 × 4</annotation></semantics></math> homogeneous transformation matrix.
Then the transformation from camera frame to the object could be calculated using Equation <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S4.E1" title="In 4.1. Eye-to-hand camera calibration ‣ 4. RoCap pipeline ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_tag">1</span></a>. The rotation and the translation serve as the 6D pose label for the object in each image as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S3.F4" title="Figure 4 ‣ 3.3. Articulated ‣ 3. Appearance-changing objects ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_tag">4</span></a>c left.</p>
</div>
<figure class="ltx_figure" id="S4.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="189" id="S4.F8.g1" src="x8.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8. </span>Data processing of data collected in by RoCap. RoCap generates mask for each image (d) by prompting SAM with bounding box (b) and points (c).
</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>Data processing and augmentation</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">After capturing the data with the ground truth labels of objects using RoCap, crucial processing step must be performed to facilitate subsequent pose estimation training.
A typical object pose estimation task comprises two subtasks: <span class="ltx_text ltx_font_italic" id="S4.SS4.p1.1.1">(i)</span> segmenting the object from the scene, and <span class="ltx_text ltx_font_italic" id="S4.SS4.p1.1.2">(ii)</span> predicting the orientation of the segmented object.
Therefore, the processing steps involves generating object masks for each label and augmenting the data to adapt to various environment in application.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1. </span>Generating masks</h4>
<div class="ltx_para" id="S4.SS4.SSS1.p1">
<p class="ltx_p" id="S4.SS4.SSS1.p1.1">RoCap leverages the recent emergence of Segment-Anything Model (SAM) <cite class="ltx_cite ltx_citemacro_citep">(Kirillov et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib26" title="">2023</a>)</cite> which is capable of producing high quality segmentations given points or bounding boxes as prompts.
For each image captured by RoCap, the subsequent procedures must be executed:</p>
</div>
<section class="ltx_paragraph" id="S4.SS4.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">Bounding box</h5>
<div class="ltx_para" id="S4.SS4.SSS1.Px1.p1">
<p class="ltx_p" id="S4.SS4.SSS1.Px1.p1.1">As the camera is calibrated to the robotic arm’s coordinate frame, we generate the initial bounding box of the object by assuming the robotic arm is holding a 15x15x15 cm cube. We then project the cube’s coordinates onto the camera’s 2D plane to obtain the bounding box (Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S4.F8" title="Figure 8 ‣ 4.3. Data labeling ‣ 4. RoCap pipeline ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_tag">8</span></a>b).
Generally, this method yields satisfactory masks for objects that are distinct and easily identifiable in the image. However, complications arise when objects are partially obscured by the robotic arm, difficult to distinguish (e.g., a flask whose appearance is influenced by the background), or even entirely invisible. To address these challenges, an additional filtering process is introduced. This process either segments the semi-occluded objects or discards the invisible data.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS4.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">Filtering</h5>
<div class="ltx_para" id="S4.SS4.SSS1.Px2.p1">
<p class="ltx_p" id="S4.SS4.SSS1.Px2.p1.1">To improve the quality of the masked objects, we leverages the interaction with SAM by providing additional prompts (points) to specify the objects and background (Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S4.F8" title="Figure 8 ‣ 4.3. Data labeling ‣ 4. RoCap pipeline ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_tag">8</span></a>c). Specifically, we incorporated two additional steps: <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS1.Px2.p1.1.1">(i)</span> we provide additional prompts for the SAM to highlight the object’s location and <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS1.Px2.p1.1.2">(ii)</span> we wrap the gripper in green tape to reduce its potential interference with segmentation performance.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS1.Px2.p2">
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1">Providing additional prompts for the SAM to highlight the object’s location</span>. Given that the gripper consistently holds the objects, we can infer that the center of the 15x15x15 cm cube corresponds to the object.Thus, we add the projected pixel coordinate of this center as a point prompt for the SAM, indicating the object’s location.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1">Removing green background.</span> Given that the gripper can partially obscure the object, it might predominantly appear within the bounding box. This could lead the SAM to mistakenly segment the gripper as the target object. To counteract this, we detect the green regions in the image, which are presumed to represent the gripper. We then calculate the geometric center of these regions and use its coordinates to provide the SAM with a prompt, pointing out the undesired areas.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2. </span>Data augmentation</h4>
<div class="ltx_para" id="S4.SS4.SSS2.p1">
<p class="ltx_p" id="S4.SS4.SSS2.p1.1">We augment each masked image of the object with random exposure, contrast, saturation, etc. via Albumentations <cite class="ltx_cite ltx_citemacro_citep">(Buslaev et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib5" title="">2020</a>)</cite> to achieve better generalizability.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Evaluation</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">To demonstrate the feasibility of our data collection pipeline,
we conducted both quantitative and qualitative evaluation of the model trained on our data to compare with a few-shot learning pose estimation approach Gen6D <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib33" title="">2022</a>)</cite>. Gen6D has shown competitive performance on any custom object by
using a single video as input for 3D reconstruction (via COLMAP <cite class="ltx_cite ltx_citemacro_citep">(Schonberger and Frahm, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib42" title="">2016</a>)</cite>) and performed feature matching based on the image and resulting pointcloud.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">We evaluated the model in two settings, <span class="ltx_text ltx_font_italic" id="S5.p2.1.1">controlled</span> setting where the ground truth can be reliably obtained for quantitative evaluation, and <span class="ltx_text ltx_font_italic" id="S5.p2.1.2">application</span> setting, where the user manipulates the object during pose estimation for qualitative evaluation, as the ground truth pose cannot be obtained easily.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T1.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.3.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S5.T1.3.1.1.1" style="padding-top:0.5pt;padding-bottom:0.5pt;"></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.3.1.1.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">anpanman</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.3.1.1.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">frog</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.3.1.1.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">pitcher</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.3.1.1.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">flask</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.3.1.1.6" style="padding-top:0.5pt;padding-bottom:0.5pt;">bottle</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.3.1.1.7" style="padding-top:0.5pt;padding-bottom:0.5pt;">scissors</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.3.1.1.8" style="padding-top:0.5pt;padding-bottom:0.5pt;">clamp</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.3.1.1.9" style="padding-top:0.5pt;padding-bottom:0.5pt;">spray</td>
</tr>
<tr class="ltx_tr" id="S5.T1.3.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T1.3.2.2.1" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.3.2.2.1.1">RoCap</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.2.2.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">91.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.2.2.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">61.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.2.2.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">73.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.2.2.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">87.1(66.9)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.2.2.6" style="padding-top:0.5pt;padding-bottom:0.5pt;">71.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.2.2.7" style="padding-top:0.5pt;padding-bottom:0.5pt;">83.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.2.2.8" style="padding-top:0.5pt;padding-bottom:0.5pt;">42.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.2.2.9" style="padding-top:0.5pt;padding-bottom:0.5pt;">87.6</td>
</tr>
<tr class="ltx_tr" id="S5.T1.3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.3.3.3.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">Gen6D <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib33" title="">2022</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.3.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">19.6</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.3.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">12.9</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.3.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">12.7</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.3.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">16.2</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.3.6" style="padding-top:0.5pt;padding-bottom:0.5pt;">16.9</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.3.7" style="padding-top:0.5pt;padding-bottom:0.5pt;">38.3</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.3.8" style="padding-top:0.5pt;padding-bottom:0.5pt;">19.4</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.3.9" style="padding-top:0.5pt;padding-bottom:0.5pt;">28.4</td>
</tr>
</tbody>
<tfoot class="ltx_tfoot">
<tr class="ltx_tr" id="S5.T1.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" colspan="9" id="S5.T1.3.4.1.1" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text" id="S5.T1.3.4.1.1.1" style="font-size:80%;">*The number in parentheses indicates flask accuracy in a different background (Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S5.F9" title="Figure 9 ‣ 5.1. Implementation of pose estimation pipeline ‣ 5. Evaluation ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_tag">9</span></a>).</span></th>
</tr>
</tfoot>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>Quantitative evaluation result. The numbers indicate the average precision at 20<sup class="ltx_sup" id="S5.T1.5.1">∘</sup>  azimuth error.</figcaption>
</figure>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Implementation of pose estimation pipeline</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Before we delve into the result of quantitative evaluation, We will discuss the pose estimation pipeline first.
As mentioned earlier, the pose estimation pipeline should consists of one model for segmenting the target object and another for predicting the orientation based on the segmented output.
For objects with manually-modifiable states (e.g., scissors), an additional state classifier is employed.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">For the segmentation task, we leveraged a recent advancement based on SAM: HQTrack <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib51" title="">2023</a>)</cite>. It is a zero-shot approach and requires no training while being able to consistently produce high-quality segmentation of target objects in videos.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">For the orientation estimation, the model is a VGG16 model, pretrained on ImageNet <cite class="ltx_cite ltx_citemacro_citep">(Deng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib12" title="">2009</a>)</cite>, followed by a fully connected layer outputting the quaternion and the 2D pixel location of the object.
The loss function is a combined loss of the Geodesic Loss on the quaternion prediction and the MSE Loss of the displacement prediction.
We train the model on the augmented data for 120 epochs, using the Adam optimizer with a learning rate of 0.0001.</p>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1">For the state classification, the model is a MobileNet V3 <cite class="ltx_cite ltx_citemacro_citep">(Howard et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib24" title="">2019</a>)</cite>, pretrained on ImageNet <cite class="ltx_cite ltx_citemacro_citep">(Deng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib12" title="">2009</a>)</cite>, followed by a fully connected layer, and the output dimension is equivalent to the number of the states of the object.
We train the model on the augmented data for 120 epochs, using the Adam optimizer with a learning rate of 0.0001.</p>
</div>
<figure class="ltx_figure" id="S5.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="730" id="S5.F9.g1" src="x9.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9. </span>Quantitative evaluation setup. The green bounding box represents the ground truth and the read bounding box represents the predicted pose.
</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Quantitative evaluation</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">For quantitative evaluation, we modified the environment by changing the camera angle and updating the background (Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S5.F9" title="Figure 9 ‣ 5.1. Implementation of pose estimation pipeline ‣ 5. Evaluation ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_tag">9</span></a>). Using a newly designed trajectory for the robotic arm, we sampled 1041 data entries. The accuracy threshold for pose estimation remained consistent with our training data parameters: set at 0.35 or an azimuth angle of 20<sup class="ltx_sup" id="S5.SS2.p1.1.1">∘</sup>.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">To clarify, our evaluation only focused on the accuracy of the orientation prediction, since RoCap rely on prior to determine the position of the object.
For Gen6D, we could not modify its training pipeline to incorporate HQTrack to enhance its object detection. Instead, we adhered to the guidelines provided for pose estimation on custom objects as outlined in Gen6D’s guidelines<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/liuyuan-pal/Gen6D/blob/main/custom_object.md" title="">https://github.com/liuyuan-pal/Gen6D/blob/main/custom_object.md</a></span></span></span>.
Specifically, we performed 3D reconstruction of the object using COLMAP <cite class="ltx_cite ltx_citemacro_citep">(Schonberger and Frahm, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib42" title="">2016</a>)</cite> and followed the preprocessing procedure in the guideline.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">For objects with multiple manual states, test data is gathered for each state, and the model is evaluated accordingly. The results represent the mean accuracy across all states. Specifically, the flask was tested against two different backgrounds: its original setting (a black background) and an alternate setting with a typical orange-colored desk surface.
Table <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S5.T1" title="Table 1 ‣ 5. Evaluation ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_tag">1</span></a> shows the accuracy comparison between our approach and Gen6D.</p>
</div>
<div class="ltx_para" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1">We note that the accuracy is much lower in our testing result as compared to the result Gen6D demonstrated in their paper.
This could be due to several factors:</p>
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1">3D reconstruction failures (<span class="ltx_text ltx_font_italic" id="S5.I1.i1.p1.1.1">e.g.</span>, Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S2.F2" title="Figure 2 ‣ 2. Related Work ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1">Data collection with objects in static positions, leading to challenges when the object’s unseen side becomes visible during manipulation (<span class="ltx_text ltx_font_italic" id="S5.I1.i2.p1.1.1">e.g.</span>, a plush toy might be placed face-up on a table during data collection).</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1">Gen6D’s documented issue with size-changing objects in frames (as the object moves closer and further away from the camera), as mentioned in their GitHub issues<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://github.com/liuyuan-pal/Gen6D/issues/29</span></span></span>.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S5.SS2.p5">
<p class="ltx_p" id="S5.SS2.p5.1">The results indicate that a simple pose estimator trained with data from RoCap can deliver relative working pose estimation performance. However, the quantitative findings also reveal some limitations.
For example, the accuracy of clamp is relatively low compared to other objects due to ambiguity caused by its symmetry. Additionally, objects whose appearances are environment-dependent demonstrate inconsistent performance under varying backgrounds. More details are discussed in the limitation in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S6.SS1" title="6.1. Limitations ‣ 6. Discussion ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_tag">6.1</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Qualitative evaluation</h3>
<figure class="ltx_figure" id="S5.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="429" id="S5.F10.g1" src="x10.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10. </span>Qualitative evaluation of the eight objects.
</figcaption>
</figure>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">To test the performance in the application setting, due to the difficulty in collecting ground truth, we conducted a qualitative evaluation on videos of humans manipulating the objects. Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S5.F10" title="Figure 10 ‣ 5.3. Qualitative evaluation ‣ 5. Evaluation ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_tag">10</span></a> shows the qualitative comparison between model trained on RoCap data and Gen6D.
For example, RoCap recorded both closed and open states during data collection for the pair of scissors. This allowed it to provide viable pose estimation for the open state (Gen6D which struggled with the unobserved state).
Please refer to the supplementary materials for the video.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Discussion</h2>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Limitations</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">The quantitative and qualitative evaluation has demonstrated the feasibility and potential of our data collection method. However, the result also shows certain limitations. Below, we will discuss the limitations from the perspectives of data capturing, model performance and other constraints.</p>
</div>
<figure class="ltx_figure" id="S6.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="379" id="S6.F11.g1" src="x11.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11. </span>Failure case for a highly deformable cloth.
</figcaption>
</figure>
<section class="ltx_paragraph" id="S6.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Data capturing</h5>
<div class="ltx_para" id="S6.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S6.SS1.SSS0.Px1.p1.1">While RoCap addresses the data capturing of appearance-changing objects, it requires the objects have distinct appearances in different defined poses. One typically example that is challenging for RoCap is cloth, which is highly deformable.
Its extreme flexibility results in a loss of the pose information when being manipulated by the robotic arm. As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#S6.F11" title="Figure 11 ‣ 6.1. Limitations ‣ 6. Discussion ‣ RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects"><span class="ltx_text ltx_ref_tag">11</span></a>, the piece of cloth is nearly identical in two different poses manipulated by the robotic arm.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS0.Px1.p2">
<p class="ltx_p" id="S6.SS1.SSS0.Px1.p2.1">On the other hand, as currently we target objects that people can easily change their appearances with hands, leading to the target object size ranging from 0.5x~1.5x of a palm size. Additionally, our robotic arm’s mechanical gripper, with a maximum gripping width of 80mm, further constrains the size of objects it can handle.
However, this limitation can be resolved when a system applies our method to a larger scale robotic arm (<span class="ltx_text ltx_font_italic" id="S6.SS1.SSS0.Px1.p2.1.1">e.g.</span>, in a mass manufacturing setting).</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Model performance</h5>
<div class="ltx_para" id="S6.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S6.SS1.SSS0.Px2.p1.1">Indicated in the evaluation results, the pose estimation pipeline does not handle symmetric object well. While this has been an open challenge in object pose estimation <cite class="ltx_cite ltx_citemacro_citep">(Thalhammer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib46" title="">2023</a>)</cite>, recent work have proposed different network architecture to address this issue <cite class="ltx_cite ltx_citemacro_citep">(Xiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib48" title="">2017</a>; Song et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib43" title="">2020</a>)</cite>. While addressing symmetry is beyond the purview of this paper, future enhancements could incorporate a sophisticated pose estimator or gather supplementary data like depth via depth cameras.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS0.Px2.p2">
<p class="ltx_p" id="S6.SS1.SSS0.Px2.p2.1">Furthermore, variations in the environment from the capturing stage may also affect the model performance, especially for viewing-angle dependent objects. While it is feasible to maintain an environment similar to the capturing setup (<span class="ltx_text ltx_font_italic" id="S6.SS1.SSS0.Px2.p2.1.1">e.g.</span>, using a black background when operating a transparent flask), future improvements could include varying environmental factors such as different lighting conditions <cite class="ltx_cite ltx_citemacro_citep">(Debevec et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib10" title="">2000</a>; Okabe et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib37" title="">2007</a>)</cite>. Additionally, future work could introduce other augmentation method such as <cite class="ltx_cite ltx_citemacro_citep">(Zongker et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib52" title="">1999</a>)</cite> to adapt to various environment.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Other constraints</h5>
<div class="ltx_para" id="S6.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S6.SS1.SSS0.Px3.p1.1">Currently the need of a robotic arm may require a lab setting. However, the pipeline can also be applied to scenarios such as <span class="ltx_text ltx_font_italic" id="S6.SS1.SSS0.Px3.p1.1.1">(i)</span> product manufacturers collect data and train a model for their product, and include it as part of their solution package and <span class="ltx_text ltx_font_italic" id="S6.SS1.SSS0.Px3.p1.1.2">(ii)</span> home users asks their robot to train a model for their own object when robots are more accessible in the future, and later the user uses the model to estimation the pose in specific applications.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>Handling Occlusion</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">Occlusion happens in different scenarios, including the objects being manipulated by hands during interaction, or the objects being held by the robotic arm during data capturing.
While the hand occlusion does have an impact on the model performance trained on RoCap data, our pipeline is less impacted compared to Gen6D as shown in the qualitative results. This is due to the fact that during the capture phase, the robotic arm may partially obscure the object throughout the capturing process, which simulates hand occlusion in the training data.</p>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1">To further address the occlusion problem,
one possible approach is
to introduce a hand-like robotic hand during the data capturing process. For example, anthropomorphic robotic hands, such as those presented in <cite class="ltx_cite ltx_citemacro_citep">(Gama Melo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib16" title="">2014</a>)</cite>, can closely mimic human hand movements and provide more realistic interaction scenarios for data collection. By using a robotic hand, it is possible to better account for occlusions that occur during human-object interactions and develop models that can better predict user intent in such cases.
Additionally, to address occlusion caused by the robotic arm during data capturing, multiple cameras could be employed to ensure the complete visibility of the objects being captured.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3. </span>Automatic Changing of Mechanical States</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">As mentioned in the paper, certain articulated objects necessitate human intervention to change their states, as they cannot be manipulated by the parallel gripper of the robotic arm <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib30" title="">2022</a>)</cite>. Examples of such objects include those that require a large range of motion or those that demand bi-manual operation.
Recent research in HCI has proposed different methods of attaching mechanisms to the physical object to automatically actuate the motion without human intervention <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib30" title="">2022</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib29" title="">2019</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib28" title="">2020a</a>)</cite>, which can be potentially leveraged by future data collection system using robotic arms to automatically collect a large amount of data.
By automating the data collection process, it is possible to scale up the dataset and sample object states at smaller intervals. For instance, instead of having discrete states of a clamp, we can sample from a continuous parameter space evenly while capturing. This would enable the prediction of the continuous parameter such as the angle of a pair of scissors, thus opening up a wider range of applications.
Future direction should include how to design mechanisms that will not affect the apperance of the objects during capturing while being able to actuate the objects.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4. </span>Leveraging Robots for Large-Scale Data Collection</h3>
<div class="ltx_para" id="S6.SS4.p1">
<p class="ltx_p" id="S6.SS4.p1.1">Robots possess the capability to perform repetitive tasks consistently and efficiently.
Researchers in computer vision and HCI have explored various approaches to employing robots for data collection across a diverse range of applications <cite class="ltx_cite ltx_citemacro_citep">(Chong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib8" title="">2021</a>; Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib17" title="">2022</a>; Mandlekar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08081v1#bib.bib35" title="">2019</a>)</cite>. This has opened up new opportunities for augmenting tasks that necessitate a substantial amount of repetitive work, such as data collection for multiple objects, through the integration of robotic systems.
By leveraging robotic systems, researchers can not only streamline the data collection process but also minimize human error and fatigue. This can lead to the acquisition of more accurate and reliable datasets, which are critical for the development and evaluation of advanced algorithms and models.</p>
</div>
<div class="ltx_para" id="S6.SS4.p2">
<p class="ltx_p" id="S6.SS4.p2.1">In addition to automating repetitive tasks, robotic systems can be equipped with various sensors and end effectors to collect multimodal data, such as visual, tactile, and auditory information. This can significantly enrich the datasets and provide researchers with a more comprehensive understanding of the objects and environments being studied.
As robotics technology continues to advance, we can expect even more sophisticated and versatile robotic systems to be employed in the data collection process. This will ultimately lead to more robust, accurate, and diverse datasets, which will contribute to the improvement of various computer vision and HCI applications.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this paper, we present RoCap, a robotic pipeline for data collection of appearance-changing objects.
This system addresses the challenge of pose estimation for objects with deformable properties (<span class="ltx_text ltx_font_italic" id="S7.p1.1.1">e.g.</span>, plush toys), viewing-angle dependent properties including transparent materials (<span class="ltx_text ltx_font_italic" id="S7.p1.1.2">e.g.</span>, glass flasks) and reflective materials (<span class="ltx_text ltx_font_italic" id="S7.p1.1.3">e.g.</span>, pitcher) or objects to be actuated with multiple mechanical states (<span class="ltx_text ltx_font_italic" id="S7.p1.1.4">e.g.</span>, clamps or spray bottle heads). By employing a robotic arm to hold these objects and capture image data, which can be utilized by anyone possessing a 6 DoF robotic arm, we can train a simple deep learning model to perform pose estimation on the objects.
We conducted both quantitative and qualitative evaluation of our approach comparing to a few-shot learning framework,
which was trained on 3D mesh reconstructed from a video.
The results demonstrated the feasibility and potential of RoCap.

</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
To Robert, for the bagels and explaining CMYK and color spaces.

</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahmadyan et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Adel Ahmadyan, Liangkai Zhang, Artsiom Ablavatski, Jianing Wei, and Matthias Grundmann. 2021.

</span>
<span class="ltx_bibblock">Objectron: A Large Scale Dataset of Object-Centric Videos in the Wild With Pose Annotations. In <em class="ltx_emph ltx_font_italic" id="bib.bib2.3.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>. 7822–7831.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barnes et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2008)</span>
<span class="ltx_bibblock">
Connelly Barnes, David E Jacobs, Jason Sanders, Dan B Goldman, Szymon Rusinkiewicz, Adam Finkelstein, and Maneesh Agrawala. 2008.

</span>
<span class="ltx_bibblock">Video puppetry: a performative interface for cutout animation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">ACM SIGGRAPH Asia 2008 papers</em>. 1–9.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brachmann et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Eric Brachmann, Alexander Krull, Frank Michel, Stefan Gumhold, Jamie Shotton, and Carsten Rother. 2014.

</span>
<span class="ltx_bibblock">Learning 6d object pose estimation using 3d object coordinates. In <em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">European conference on computer vision</em>. Springer, 536–551.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Buslaev et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Alexander Buslaev, Vladimir I. Iglovikov, Eugene Khvedchenya, Alex Parinov, Mikhail Druzhinin, and Alexandr A. Kalinin. 2020.

</span>
<span class="ltx_bibblock">Albumentations: Fast and Flexible Image Augmentations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">Information</em> 11, 2 (2020).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.3390/info11020125" title="">https://doi.org/10.3390/info11020125</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Calli et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Berk Calli, Arjun Singh, Aaron Walsman, Siddhartha Srinivasa, Pieter Abbeel, and Aaron M Dollar. 2015.

</span>
<span class="ltx_bibblock">The ycb object and model set: Towards common benchmarks for manipulation research. In <em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">2015 international conference on advanced robotics (ICAR)</em>. IEEE, 510–517.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chao et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yu-Wei Chao, Wei Yang, Yu Xiang, Pavlo Molchanov, Ankur Handa, Jonathan Tremblay, Yashraj S Narang, Karl Van Wyk, Umar Iqbal, Stan Birchfield, et al<span class="ltx_text" id="bib.bib7.3.1">.</span> 2021.

</span>
<span class="ltx_bibblock">DexYCB: A benchmark for capturing hand grasping of objects. In <em class="ltx_emph ltx_font_italic" id="bib.bib7.4.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 9044–9053.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chong et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Toby Chong, I-Chao Shen, Nobuyuki Umetani, and Takeo Igarashi. 2021.

</span>
<span class="ltx_bibblock">Per Garment Capture and Synthesis for Real-time Virtual Try-on. In <em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">The 34th Annual ACM Symposium on User Interface Software and Technology</em>. 457–469.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Danila Rukhovich (2022)</span>
<span class="ltx_bibblock">
Anton Konushin Danila Rukhovich, Anna Vorontsova. 2022.

</span>
<span class="ltx_bibblock">ImVoxelNet: Image to Voxels Projection for Monocular and Multi-View General-Purpose 3D Object Detection.

</span>
<span class="ltx_bibblock">(2022), 2397–2406.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Debevec et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2000)</span>
<span class="ltx_bibblock">
Paul Debevec, Tim Hawkins, Chris Tchou, Haarm-Pieter Duiker, Westley Sarokin, and Mark Sagar. 2000.

</span>
<span class="ltx_bibblock">Acquiring the reflectance field of a human face. In <em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">Proceedings of the 27th annual conference on Computer graphics and interactive techniques</em>. 145–156.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Denavit and Hartenberg (1955)</span>
<span class="ltx_bibblock">
Jacques Denavit and Richard S Hartenberg. 1955.

</span>
<span class="ltx_bibblock">A kinematic notation for lower-pair mechanisms based on matrices.

</span>
<span class="ltx_bibblock">(1955).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2009)</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009.

</span>
<span class="ltx_bibblock">ImageNet: A large-scale hierarchical image database. In <em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">2009 IEEE Conference on Computer Vision and Pattern Recognition</em>. 248–255.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/CVPR.2009.5206848" title="">https://doi.org/10.1109/CVPR.2009.5206848</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dogan et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Mustafa Doga Dogan, Ahmad Taka, Michael Lu, Yunyi Zhu, Akshat Kumar, Aakar Gupta, and Stefanie Mueller. 2022.

</span>
<span class="ltx_bibblock">InfraredTags: Embedding Invisible AR Markers and Barcodes Using Low-Cost, Infrared-Based 3D Printing and Imaging Tools. In <em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">CHI Conference on Human Factors in Computing Systems</em>. 1–12.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dwibedi et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Debidatta Dwibedi, Ishan Misra, and Martial Hebert. 2017.

</span>
<span class="ltx_bibblock">Cut, paste and learn: Surprisingly easy synthesis for instance detection. In <em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">Proceedings of the IEEE international conference on computer vision</em>. 1301–1310.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Heng Fan, Halady Akhilesha Miththanthaya, Siranjiv Ramana Rajan, Xiaoqiong Liu, Zhilin Zou, Yuewei Lin, Haibin Ling, et al<span class="ltx_text" id="bib.bib15.3.1">.</span> 2021.

</span>
<span class="ltx_bibblock">Transparent object tracking benchmark. In <em class="ltx_emph ltx_font_italic" id="bib.bib15.4.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>. 10734–10743.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gama Melo et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Erika Nathalia Gama Melo, Oscar Fernando Aviles Sanchez, and Darlo Amaya Hurtado. 2014.

</span>
<span class="ltx_bibblock">Anthropomorphic robotic hands: a review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">Ingeniería y desarrollo</em> 32, 2 (2014), 279–313.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Ruohan Gao, Zilin Si, Yen-Yu Chang, Samuel Clarke, Jeannette Bohg, Li Fei-Fei, Wenzhen Yuan, and Jiajun Wu. 2022.

</span>
<span class="ltx_bibblock">Objectfolder 2.0: A multisensory object dataset for sim2real transfer. In <em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 10598–10608.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Garrido-Jurado et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Sergio Garrido-Jurado, Rafael Muñoz-Salinas, Francisco José Madrid-Cuevas, and Manuel Jesús Marín-Jiménez. 2014.

</span>
<span class="ltx_bibblock">Automatic generation and detection of highly reliable fiducial markers under occlusion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">Pattern Recognition</em> 47, 6 (2014), 2280–2292.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hampali et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Shreyas Hampali, Mahdi Rad, Markus Oberweger, and Vincent Lepetit. 2020.

</span>
<span class="ltx_bibblock">Honnotate: A method for 3d annotation of hand and object poses. In <em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 3196–3206.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Held et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2012)</span>
<span class="ltx_bibblock">
Robert Held, Ankit Gupta, Brian Curless, and Maneesh Agrawala. 2012.

</span>
<span class="ltx_bibblock">3D Puppetry: A Kinect-Based Interface for 3D Animation. In <em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology</em> (Cambridge, Massachusetts, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib20.4.2">(UIST ’12)</em>. Association for Computing Machinery, New York, NY, USA, 423–434.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/2380116.2380170" title="">https://doi.org/10.1145/2380116.2380170</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hettiarachchi and Wigdor (2016)</span>
<span class="ltx_bibblock">
Anuruddha Hettiarachchi and Daniel Wigdor. 2016.

</span>
<span class="ltx_bibblock">Annexing reality: Enabling opportunistic use of everyday objects as tangible proxies in augmented reality. In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems</em>. 1957–1967.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hinterstoisser et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Stefan Hinterstoisser, Vincent Lepetit, Slobodan Ilic, Stefan Holzer, Gary Bradski, Kurt Konolige, and Nassir Navab. 2013.

</span>
<span class="ltx_bibblock">Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes. In <em class="ltx_emph ltx_font_italic" id="bib.bib22.3.1">Computer Vision–ACCV 2012: 11th Asian Conference on Computer Vision, Daejeon, Korea, November 5-9, 2012, Revised Selected Papers, Part I 11</em>. Springer, 548–562.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hodan et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Tomáš Hodan, Pavel Haluza, Štepán Obdržálek, Jiri Matas, Manolis Lourakis, and Xenophon Zabulis. 2017.

</span>
<span class="ltx_bibblock">T-LESS: An RGB-D dataset for 6D pose estimation of texture-less objects. In <em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">2017 IEEE Winter Conference on Applications of Computer Vision (WACV)</em>. IEEE, 880–888.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Howard et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, Quoc V. Le, and Hartwig Adam. 2019.

</span>
<span class="ltx_bibblock">Searching for MobileNetV3.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1905.02244 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kalaitzakis et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Michail Kalaitzakis, Brennan Cain, Sabrina Carroll, Anand Ambrosi, Camden Whitehead, and Nikolaos Vitzilaios. 2021.

</span>
<span class="ltx_bibblock">Fiducial markers for pose estimation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">Journal of Intelligent &amp; Robotic Systems</em> 101, 4 (2021), 1–26.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kirillov et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al<span class="ltx_text" id="bib.bib26.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Segment anything.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.4.1">arXiv preprint arXiv:2304.02643</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krull et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Alexander Krull, Eric Brachmann, Frank Michel, Michael Ying Yang, Stefan Gumhold, and Carsten Rother. 2015.

</span>
<span class="ltx_bibblock">Learning analysis-by-synthesis for 6D pose estimation in RGB-D images. In <em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">Proceedings of the IEEE international conference on computer vision</em>. 954–962.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2020a)</span>
<span class="ltx_bibblock">
Jiahao Li, Meilin Cui, Jeeeun Kim, and Xiang’Anthony’ Chen. 2020a.

</span>
<span class="ltx_bibblock">Romeo: A design tool for embedding transformable parts in 3d models to robotically augment default functionalities. In <em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">Proceedings of the 33rd Annual Acm Symposium on User Interface Software and Technology</em>. 897–911.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Jiahao Li, Jeeeun Kim, and Xiang’Anthony’ Chen. 2019.

</span>
<span class="ltx_bibblock">Robiot: A design tool for actuating everyday objects with automatically generated 3D printable mechanisms. In <em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology</em>. 673–685.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Jiahao Li, Alexis Samoylov, Jeeeun Kim, and Xiang ’Anthony’ Chen. 2022.

</span>
<span class="ltx_bibblock">Roman: Making Everyday Objects Robotically Manipulable with 3D-Printable Add-on Mechanisms. In <em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems</em> (New Orleans, LA, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib30.4.2">(CHI ’22)</em>. Association for Computing Machinery, New York, NY, USA, Article 272, 17 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3491102.3501818" title="">https://doi.org/10.1145/3491102.3501818</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2020b)</span>
<span class="ltx_bibblock">
Xiaolong Li, He Wang, Li Yi, Leonidas J Guibas, A Lynn Abbott, and Shuran Song. 2020b.

</span>
<span class="ltx_bibblock">Category-level articulated object pose estimation. In <em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 3706–3715.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Shaowei Liu, Hanwen Jiang, Jiarui Xu, Sifei Liu, and Xiaolong Wang. 2021.

</span>
<span class="ltx_bibblock">Semi-supervised 3d hand-object poses estimation with interactions in time. In <em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 14687–14697.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Yuan Liu, Yilin Wen, Sida Peng, Cheng Lin, Xiaoxiao Long, Taku Komura, and Wenping Wang. 2022.

</span>
<span class="ltx_bibblock">Gen6D: Generalizable Model-Free 6-DoF Object Pose Estimation from RGB Images.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.3.1">arXiv preprint arXiv:2204.10776</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lugaresi et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris McClanahan, Esha Uboweja, Michael Hays, Fan Zhang, Chuo-Ling Chang, Ming Guang Yong, Juhyun Lee, Wan-Teh Chang, Wei Hua, Manfred Georg, and Matthias Grundmann. 2019.

</span>
<span class="ltx_bibblock">MediaPipe: A Framework for Building Perception Pipelines.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.1906.08172" title="">https://doi.org/10.48550/ARXIV.1906.08172</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mandlekar et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Ajay Mandlekar, Jonathan Booher, Max Spero, Albert Tung, Anchit Gupta, Yuke Zhu, Animesh Garg, Silvio Savarese, and Li Fei-Fei. 2019.

</span>
<span class="ltx_bibblock">Scaling robot supervision to hundreds of hours with roboturk: Robotic manipulation dataset through human reasoning and dexterity. In <em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>. IEEE, 1048–1055.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marion et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Pat Marion, Peter R Florence, Lucas Manuelli, and Russ Tedrake. 2018.

</span>
<span class="ltx_bibblock">Label fusion: A pipeline for generating ground truth labels for real rgbd data of cluttered scenes. In <em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">2018 IEEE International Conference on Robotics and Automation (ICRA)</em>. IEEE, 3235–3242.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Okabe et al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2007)</span>
<span class="ltx_bibblock">
Makoto Okabe, Kenshi Takayama, Takashi Ijiri, and Takeo Igarashi. 2007.

</span>
<span class="ltx_bibblock">Light shower: a poor man’s light stage built with an off-the-shelf umbrella and projector.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.3.1">ACM SIGGRAPH 2007 sketches</em>. 62–es.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park and Martin (1994)</span>
<span class="ltx_bibblock">
Frank C Park and Bryan J Martin. 1994.

</span>
<span class="ltx_bibblock">Robot sensor calibration: solving AX= XB on the Euclidean group.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">IEEE Transactions on Robotics and Automation</em> 10, 5 (1994), 717–721.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. 2019.

</span>
<span class="ltx_bibblock">Pvnet: Pixel-wise voting network for 6dof pose estimation. In <em class="ltx_emph ltx_font_italic" id="bib.bib39.3.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 4561–4570.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qian et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Xun Qian, Fengming He, Xiyun Hu, Tianyi Wang, and Karthik Ramani. 2022.

</span>
<span class="ltx_bibblock">ARnnotate: An Augmented Reality Interface for Collecting Custom Dataset of 3D Hand-Object Interaction Pose Estimation. In <em class="ltx_emph ltx_font_italic" id="bib.bib40.3.1">Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology</em>. 1–14.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ros et al<span class="ltx_text" id="bib.bib41.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio M Lopez. 2016.

</span>
<span class="ltx_bibblock">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes. In <em class="ltx_emph ltx_font_italic" id="bib.bib41.3.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 3234–3243.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schonberger and Frahm (2016)</span>
<span class="ltx_bibblock">
Johannes L Schonberger and Jan-Michael Frahm. 2016.

</span>
<span class="ltx_bibblock">Structure-from-motion revisited. In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 4104–4113.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al<span class="ltx_text" id="bib.bib43.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Chen Song, Jiaru Song, and Qixing Huang. 2020.

</span>
<span class="ltx_bibblock">Hybridpose: 6d object pose estimation under hybrid representations. In <em class="ltx_emph ltx_font_italic" id="bib.bib43.3.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 431–440.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al<span class="ltx_text" id="bib.bib44.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yongzhi Su, Jason Rambach, Alain Pagani, and Didier Stricker. 2021.

</span>
<span class="ltx_bibblock">Synpo-net—Accurate and fast CNN-based 6DoF object pose estimation using synthetic training.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.3.1">Sensors</em> 21, 1 (2021), 300.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suzuki et al<span class="ltx_text" id="bib.bib45.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Ryo Suzuki, Rubaiat Habib Kazi, Li-yi Wei, Stephen DiVerdi, Wilmot Li, and Daniel Leithinger. 2020.

</span>
<span class="ltx_bibblock">RealitySketch: Embedding Responsive Graphics and Visualizations in AR through Dynamic Sketching. In <em class="ltx_emph ltx_font_italic" id="bib.bib45.3.1">Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology</em> (Virtual Event, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib45.4.2">(UIST ’20)</em>. Association for Computing Machinery, New York, NY, USA, 166–181.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3379337.3415892" title="">https://doi.org/10.1145/3379337.3415892</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thalhammer et al<span class="ltx_text" id="bib.bib46.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Stefan Thalhammer, Dominik Bauer, Peter Hönig, Jean-Baptiste Weibel, José García-Rodríguez, and Markus Vincze. 2023.

</span>
<span class="ltx_bibblock">Challenges for Monocular 6D Object Pose Estimation in Robotics.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.3.1">arXiv preprint arXiv:2307.12172</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ulrich et al<span class="ltx_text" id="bib.bib47.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Jiří Ulrich, Ahmad Alsayed, Farshad Arvin, and Tomáš Krajník. 2022.

</span>
<span class="ltx_bibblock">Towards Fast Fiducial Marker with Full 6 DOF Pose Estimation. In <em class="ltx_emph ltx_font_italic" id="bib.bib47.3.1">Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing</em> (Virtual Event) <em class="ltx_emph ltx_font_italic" id="bib.bib47.4.2">(SAC ’22)</em>. Association for Computing Machinery, New York, NY, USA, 723–730.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3477314.3507043" title="">https://doi.org/10.1145/3477314.3507043</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiang et al<span class="ltx_text" id="bib.bib48.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, and Dieter Fox. 2017.

</span>
<span class="ltx_bibblock">Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.3.1">arXiv preprint arXiv:1711.00199</em> (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib49.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Hao Yang, Chen Shi, Yihong Chen, and Liwei Wang. 2022.

</span>
<span class="ltx_bibblock">Boosting 3D Object Detection via Object-Focused Image Fusion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.3.1">arXiv preprint arXiv:2207.10589</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib50.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Qian Zhou, Sarah Sykes, Sidney Fels, and Kenrick Kin. 2020.

</span>
<span class="ltx_bibblock">Gripmarks: Using Hand Grips to Transform In-Hand Objects into Mixed Reality Input. In <em class="ltx_emph ltx_font_italic" id="bib.bib50.3.1">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</em>. 1–11.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span class="ltx_text" id="bib.bib51.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jiawen Zhu, Zhenyu Chen, Zeqi Hao, Shijie Chang, Lu Zhang, Dong Wang, Huchuan Lu, Bin Luo, Jun-Yan He, Jin-Peng Lan, et al<span class="ltx_text" id="bib.bib51.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Tracking Anything in High Quality.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.4.1">arXiv preprint arXiv:2307.13974</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zongker et al<span class="ltx_text" id="bib.bib52.2.2.1">.</span> (1999)</span>
<span class="ltx_bibblock">
Douglas E Zongker, Dawn M Werner, Brian Curless, and David H Salesin. 1999.

</span>
<span class="ltx_bibblock">Environment matting and compositing. In <em class="ltx_emph ltx_font_italic" id="bib.bib52.3.1">Proceedings of the 26th annual conference on Computer graphics and interactive techniques</em>. 205–214.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Jul 10 22:49:25 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
