<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>NeRF-Feat: 6D Object Pose Estimation using Feature Rendering</title>
<!--Generated on Wed Jun 19 19:32:00 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2406.13796v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S1" title="In NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S2" title="In NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S3" title="In NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S3.SS1" title="In 3 Method ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Stage 1 - NeRF pretraining</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S3.SS2" title="In 3 Method ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Stage 2 - Feature Learning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S3.SS3" title="In 3 Method ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Inference</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S4" title="In NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S4.SS1" title="In 4 Experiments ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Training Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S4.SS2" title="In 4 Experiments ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>NeRF-Pose Comparison</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S4.SS3" title="In 4 Experiments ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>LineMOD</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S4.SS4" title="In 4 Experiments ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>LineMOD-Occlusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S4.SS5" title="In 4 Experiments ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>T-Less</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S4.SS6" title="In 4 Experiments ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6 </span>Ablation Studies</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S5" title="In NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S5.SS0.SSS0.Px1" title="In 5 Conclusion ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_title">Acknowledgements</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S6" title="In NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Noisy Initial Pose Refinement</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S7" title="In NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Faster Inference For Symmetric Objects</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S8" title="In NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Training Details</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S8.SS0.SSS0.Px1" title="In 8 Training Details ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_title">Density MLP</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S8.SS0.SSS0.Px2" title="In 8 Training Details ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_title">Color MLP</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S8.SS0.SSS0.Px3" title="In 8 Training Details ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_title">Feature MLP</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S8.SS0.SSS0.Px4" title="In 8 Training Details ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_title">CNN</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S8.SS1" title="In 8 Training Details ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.1 </span>Data Preparation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S8.SS2" title="In 8 Training Details ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.2 </span>Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S8.SS3" title="In 8 Training Details ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.3 </span>Learning Consistent Features</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S8.SS4" title="In 8 Training Details ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.4 </span>Inference</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S8.SS4.SSS0.Px1" title="In 8.4 Inference ‣ 8 Training Details ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_title">Point cloud extraction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S8.SS4.SSS0.Px2" title="In 8.4 Inference ‣ 8 Training Details ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_title">Inference with Image</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S9" title="In NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9 </span>Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S10" title="In NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10 </span>Additional Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S10.SS1" title="In 10 Additional Results ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10.1 </span>Linemod Test Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S10.SS2" title="In 10 Additional Results ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10.2 </span>T-Less Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S10.SS3" title="In 10 Additional Results ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10.3 </span>Qualitative Results</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">NeRF-Feat: 6D Object Pose Estimation using Feature Rendering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Shishir Reddy Vutukur<sup class="ltx_sup" id="id10.2.id1"><span class="ltx_text ltx_font_italic" id="id10.2.id1.1">1,2</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Heike Brock<sup class="ltx_sup" id="id11.2.id1"><span class="ltx_text ltx_font_italic" id="id11.2.id1.1">2</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Benjamin Busam<sup class="ltx_sup" id="id12.2.id1"><span class="ltx_text ltx_font_italic" id="id12.2.id1.1">1</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tolga Birdal<sup class="ltx_sup" id="id13.2.id1"><span class="ltx_text ltx_font_italic" id="id13.2.id1.1">3</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Andreas Hutter<sup class="ltx_sup" id="id14.2.id1"><span class="ltx_text ltx_font_italic" id="id14.2.id1.1">2</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Slobodan Ilic<sup class="ltx_sup" id="id15.2.id1"><span class="ltx_text ltx_font_italic" id="id15.2.id1.1">1,2</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">  <sup class="ltx_sup" id="id16.4.id1"><span class="ltx_text ltx_font_italic" id="id16.4.id1.1">1</span></sup> Technical University of Munich  <sup class="ltx_sup" id="id17.5.id2"><span class="ltx_text ltx_font_italic" id="id17.5.id2.1">2</span></sup> Siemens AG  <sup class="ltx_sup" id="id18.6.id3"><span class="ltx_text ltx_font_italic" id="id18.6.id3.1">3</span></sup> Imperial College London
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id19.id1">Object Pose Estimation is a crucial component in robotic grasping and augmented reality. Learning based approaches typically require training data from a highly accurate CAD model or labeled training data acquired using a complex setup. We address this by learning to estimate pose from weakly labeled data without a known CAD model. We propose to use a NeRF to learn object shape implicitly which is later used to learn view-invariant features in conjunction with CNN using a contrastive loss. While NeRF helps in learning features that are view-consistent, CNN ensures that the learned features respect symmetry. During inference, CNN is used to predict view-invariant features which can be used to establish correspondences with the implicit 3d model in NeRF. The correspondences are then used to estimate the pose in the reference frame of NeRF. Our approach can also handle symmetric objects unlike other approaches using a similar training setup. Specifically, we learn viewpoint invariant, discriminative features using NeRF which are later used for pose estimation. We evaluated our approach on LM, LM-Occlusion, and T-Less dataset and achieved benchmark accuracy despite using weakly labeled data.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">6D object pose estimation is a crucial component in robotics and augmented reality and is widely deployed in robotic grasping for pick-and-place tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">12</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib49" title=""><span class="ltx_text" style="font-size:90%;">49</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib55" title=""><span class="ltx_text" style="font-size:90%;">55</span></a>]</cite>. 6D object pose estimation involves estimating rotation and translation from the image reference frame to the object reference frame.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">With the rise in learning-based approaches for solving 6d pose estimation, it has become crucial to develop approaches that work with training data that are easier to obtain without compromising performance. Acquiring real data and obtaining 6D pose annotations requires a highly complex setup and cumbersome effort <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib29" title=""><span class="ltx_text" style="font-size:90%;">29</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib50" title=""><span class="ltx_text" style="font-size:90%;">50</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite>. Rendering synthetic training data using a CAD model yields decent performance, but is still not comparable to the performance obtained using real training data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">10</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">We propose a pipeline that simplifies the real training data acquisition process and training procedure using weaker labels compared to the strong labels needed and generated by current pipelines. We acquire real training images with relative poses which can be acquired much more easily from the device or using markerboards compared to object pose which requires a complex setup to annotate. We use the relative pose labels to learn the object shape implicitly followed by learning a feature representation of the implicit object shape to enable 6D pose estimation. We employ Neural Radiance Field, NeRF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite>, to learn object shape implicitly from posed real images.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">A similar training setup has been employed by NeRF-Pose<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite>, RLLG<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>. NeRF-Pose is a coordinate regression approach and it cannot handle symmetric objects without knowing symmetric configurations which is the case with the proposed data acquisition pipeline. We propose an approach that takes advantage of 3D representation in NeRF to learn 3D invariant feature space on the surface of the object while respecting symmetries. Specifically, we train NeRF and CNN to learn 3D invariant discriminative feature space by training them together. The learned feature space can be used to establish correspondences between 2D images and learned 3D object representation enabling 6D pose estimation. NeRF encodes the 3D knowledge by design to enable novel view rendering. CNN is invariant to images belonging to symmetric configurations of the object as it only works based on appearance. By training CNN and NeRF together, NeRF transfers 3D knowledge to CNN and CNN forces the features to obey symmetries. This bidirectional learning helps in handling symmetries, unlike NeRF-pose which only transfers 3D knowledge from NeRF to CNN. We employ a contrastive learning approach similar to SurfEmb<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite> and Continuous Surface Embeddings <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite> to learn discriminative 3D consistent feature space using InfoNCE loss <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib44" title=""><span class="ltx_text" style="font-size:90%;">44</span></a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To summarize, our contributions are listed as follows:</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">1. Propose a novel pipeline to distill 3D knowledge to 2D images by rendering features from NeRF.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">2. Providing bidirectional feature learning between CNN and NeRF enables handling symmetric objects.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">3. Simplify the pose estimation training data acquisition and training by working with only relative pose labels and RGB images which are much easier to obtain in practice.</p>
</div>
<div class="ltx_para" id="S1.p9">
<p class="ltx_p" id="S1.p9.1">4. Faster inference on symmetric objects as correspondences are biased to one symmetric configuration when trained with only one object with contrastive learning.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">With the rapid prominence of deep learning, learning-based approaches such as <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">51</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib46" title=""><span class="ltx_text" style="font-size:90%;">46</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib54" title=""><span class="ltx_text" style="font-size:90%;">54</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib37" title=""><span class="ltx_text" style="font-size:90%;">37</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">15</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">16</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib9" title=""><span class="ltx_text" style="font-size:90%;">9</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">13</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">32</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite> are proposed which address pose estimation using different input training data modalities and different training data generation pipelines. Dpod<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib54" title=""><span class="ltx_text" style="font-size:90%;">54</span></a>]</cite>, DpodV2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib37" title=""><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite> and Pix2Pose<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite> address the problem of solving pose estimation from RGB images using synthetic data generated from CAD models. BlenderProc <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite> introduced a way to conveniently generate high quality photo-realistic rendering of objects from CAD models using Blender which has improved the accuracy of synthetic data-based approaches. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">51</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib46" title=""><span class="ltx_text" style="font-size:90%;">46</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">15</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">16</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib9" title=""><span class="ltx_text" style="font-size:90%;">9</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib48" title=""><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite> leverage real training data acquired using complex setup to train the pipeline and have performed better than the approaches trained with synthetic data.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Recent Methods, Ove6D, OSOP, Latent Fusion, OnePose, OnePose++<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">38</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib33" title=""><span class="ltx_text" style="font-size:90%;">33</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib42" title=""><span class="ltx_text" style="font-size:90%;">42</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> propose approaches that can generalize to any object. However, the performance is still not comparable to the approaches using training data from objects under test. To address this issue, Self6D, RLLG, WeLSA, NeRF-Pose, TexPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib47" title=""><span class="ltx_text" style="font-size:90%;">47</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">6</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib45" title=""><span class="ltx_text" style="font-size:90%;">45</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite> try to use real training data for training by simplifying the acquisition process and by assuming weaker annotation compared to acquiring full 6d real pose annotations which require a complex setup. Self6D pretrains the network using synthetic data from CAD models and then obtains pseudo labels for unlabeled real training data. WeLSA labels a large data of real training images using few labeled real data. RLLG, NeRF-Pose try to learn object pose using relative pose between real training images. Self6D requires a textured CAD model, RGB-D data and WeLSA requires few labeled RGB-D data to learn to generate labels for real training images. RLLG, NeRF-Pose work with RGB data and weaker labels, but they cannot handle symmetric objects without prior knowledge about the symmetry of the object. We propose an approach to work with weaker labels similar to RLLG and NeRF-Pose and propose an approach that is robust to occlusions and handling symmetric objects without prior symmetry knowledge.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Neural Radiance Fields, NeRF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite>, proposed a promising way to perform novel view synthesis which has then been adapted to various other tasks like N3F, Distilled feature fields<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib43" title=""><span class="ltx_text" style="font-size:90%;">43</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite>. N3F learns to predict features in addition to colors using a frozen Self-Supervised network, DINO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>, to perform scene editing and scene segmentation. In our approach, we also propose to learn features using NeRF, but we can not use a pre-trained self-supervised CNN like DINO to learn features for our NeRF as the learned features are not trained to be invariant to out-of-plane rotations. Besides, Dino is not suited for completely texture-less objects such as T-Less <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite> dataset objects.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">Tex-Pose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite> uses CAD model to pre-train NeRF and later leverages real data to generate poses for real images by freezing geometry and optimizing texture, and pose for real images. NeRF-Pose<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite> and NeRF-Supervision<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib53" title=""><span class="ltx_text" style="font-size:90%;">53</span></a>]</cite> employ NeRF to estimate correspondences which they use to train another pose estimation network and do not take advantage of robust 3D representation of NeRF for pose estimation. iNeRF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib52" title=""><span class="ltx_text" style="font-size:90%;">52</span></a>]</cite> acts as a refinement network and needs a good initial estimate to estimate refined pose. NeRF-supervision requires depth maps from Colmap <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite> in addition to 2D images to train the pipeline. NeRF-Pose uses only 2D images with relative poses, but it cannot handle symmetric objects and employs a unidirectional learning approach from NeRF to CNN. We exploit the 3D knowledge encoded in NeRF to learn 3D invariant features from only 2D images by jointly optimizing NeRF and CNN which in turn enables us to handle symmetric objects.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">SurfEmb <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite> employs a contrastive feature learning approach that estimates viewpoint invariant features using InfoNCE loss <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib44" title=""><span class="ltx_text" style="font-size:90%;">44</span></a>]</cite>. SurfEmb requires a texture-less CAD model and 6D object pose-labeled images for training or a textured CAD model. We employ a similar contrastive learning approach and additionally make the training procedure simpler by assuming that the relative poses between images are available which are much easier to obtain in practice compared to accurate CAD models and real 6D pose labels. Specifically, we leverage the neural radiance fields, NeRF, to learn object implicit representation, and distill 3D knowledge into 2D CNN while CNN implicitly handles symmetry forcing the features to obey both symmetry and 3D consistency. We address the particularly challenging problem of learning 3D consistent, discriminative features when an accurate CAD model is not present in our pipeline. Learned feature representation is useful for downstream pose estimation tasks by establishing correspondences between 2D images and 3D NeRF representation.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="573" id="S3.F1.g1" src="x1.png" width="926"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F1.32.16.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S3.F1.30.15" style="font-size:90%;">Architecture: Our architecture comprises a Ray Generator, NeRF Block, and a U-Net CNN. Ray Generator generates rays from a specific viewpoint which are passed to NeRF Block as discrete 3D points, <math alttext="x" class="ltx_Math" display="inline" id="S3.F1.16.1.m1.1"><semantics id="S3.F1.16.1.m1.1b"><mi id="S3.F1.16.1.m1.1.1" xref="S3.F1.16.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.F1.16.1.m1.1c"><ci id="S3.F1.16.1.m1.1.1.cmml" xref="S3.F1.16.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.16.1.m1.1d">x</annotation><annotation encoding="application/x-llamapun" id="S3.F1.16.1.m1.1e">italic_x</annotation></semantics></math>, along the ray including ray direction, <math alttext="r_{d}" class="ltx_Math" display="inline" id="S3.F1.17.2.m2.1"><semantics id="S3.F1.17.2.m2.1b"><msub id="S3.F1.17.2.m2.1.1" xref="S3.F1.17.2.m2.1.1.cmml"><mi id="S3.F1.17.2.m2.1.1.2" xref="S3.F1.17.2.m2.1.1.2.cmml">r</mi><mi id="S3.F1.17.2.m2.1.1.3" xref="S3.F1.17.2.m2.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F1.17.2.m2.1c"><apply id="S3.F1.17.2.m2.1.1.cmml" xref="S3.F1.17.2.m2.1.1"><csymbol cd="ambiguous" id="S3.F1.17.2.m2.1.1.1.cmml" xref="S3.F1.17.2.m2.1.1">subscript</csymbol><ci id="S3.F1.17.2.m2.1.1.2.cmml" xref="S3.F1.17.2.m2.1.1.2">𝑟</ci><ci id="S3.F1.17.2.m2.1.1.3.cmml" xref="S3.F1.17.2.m2.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.17.2.m2.1d">r_{d}</annotation><annotation encoding="application/x-llamapun" id="S3.F1.17.2.m2.1e">italic_r start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT</annotation></semantics></math>. NeRF Block comprises three MLPs. Density MLP takes in a 3D coordinate, <math alttext="x" class="ltx_Math" display="inline" id="S3.F1.18.3.m3.1"><semantics id="S3.F1.18.3.m3.1b"><mi id="S3.F1.18.3.m3.1.1" xref="S3.F1.18.3.m3.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.F1.18.3.m3.1c"><ci id="S3.F1.18.3.m3.1.1.cmml" xref="S3.F1.18.3.m3.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.18.3.m3.1d">x</annotation><annotation encoding="application/x-llamapun" id="S3.F1.18.3.m3.1e">italic_x</annotation></semantics></math>, and predicts the corresponding density, <math alttext="d" class="ltx_Math" display="inline" id="S3.F1.19.4.m4.1"><semantics id="S3.F1.19.4.m4.1b"><mi id="S3.F1.19.4.m4.1.1" xref="S3.F1.19.4.m4.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.F1.19.4.m4.1c"><ci id="S3.F1.19.4.m4.1.1.cmml" xref="S3.F1.19.4.m4.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.19.4.m4.1d">d</annotation><annotation encoding="application/x-llamapun" id="S3.F1.19.4.m4.1e">italic_d</annotation></semantics></math>. Color MLP takes in the intermediate feature from Density MLP and Ray Direction, <math alttext="r_{d}" class="ltx_Math" display="inline" id="S3.F1.20.5.m5.1"><semantics id="S3.F1.20.5.m5.1b"><msub id="S3.F1.20.5.m5.1.1" xref="S3.F1.20.5.m5.1.1.cmml"><mi id="S3.F1.20.5.m5.1.1.2" xref="S3.F1.20.5.m5.1.1.2.cmml">r</mi><mi id="S3.F1.20.5.m5.1.1.3" xref="S3.F1.20.5.m5.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F1.20.5.m5.1c"><apply id="S3.F1.20.5.m5.1.1.cmml" xref="S3.F1.20.5.m5.1.1"><csymbol cd="ambiguous" id="S3.F1.20.5.m5.1.1.1.cmml" xref="S3.F1.20.5.m5.1.1">subscript</csymbol><ci id="S3.F1.20.5.m5.1.1.2.cmml" xref="S3.F1.20.5.m5.1.1.2">𝑟</ci><ci id="S3.F1.20.5.m5.1.1.3.cmml" xref="S3.F1.20.5.m5.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.20.5.m5.1d">r_{d}</annotation><annotation encoding="application/x-llamapun" id="S3.F1.20.5.m5.1e">italic_r start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT</annotation></semantics></math> as input to predict color, <math alttext="c" class="ltx_Math" display="inline" id="S3.F1.21.6.m6.1"><semantics id="S3.F1.21.6.m6.1b"><mi id="S3.F1.21.6.m6.1.1" xref="S3.F1.21.6.m6.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.F1.21.6.m6.1c"><ci id="S3.F1.21.6.m6.1.1.cmml" xref="S3.F1.21.6.m6.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.21.6.m6.1d">c</annotation><annotation encoding="application/x-llamapun" id="S3.F1.21.6.m6.1e">italic_c</annotation></semantics></math>, at the point. Ray direction is added to color MLP to model view-dependent color changes. Feature MLP takes in a 3D point, <math alttext="x" class="ltx_Math" display="inline" id="S3.F1.22.7.m7.1"><semantics id="S3.F1.22.7.m7.1b"><mi id="S3.F1.22.7.m7.1.1" xref="S3.F1.22.7.m7.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.F1.22.7.m7.1c"><ci id="S3.F1.22.7.m7.1.1.cmml" xref="S3.F1.22.7.m7.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.22.7.m7.1d">x</annotation><annotation encoding="application/x-llamapun" id="S3.F1.22.7.m7.1e">italic_x</annotation></semantics></math>, to predict feature vector, <math alttext="f" class="ltx_Math" display="inline" id="S3.F1.23.8.m8.1"><semantics id="S3.F1.23.8.m8.1b"><mi id="S3.F1.23.8.m8.1.1" xref="S3.F1.23.8.m8.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S3.F1.23.8.m8.1c"><ci id="S3.F1.23.8.m8.1.1.cmml" xref="S3.F1.23.8.m8.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.23.8.m8.1d">f</annotation><annotation encoding="application/x-llamapun" id="S3.F1.23.8.m8.1e">italic_f</annotation></semantics></math>. Ray Integration accumulates densities, <math alttext="d" class="ltx_Math" display="inline" id="S3.F1.24.9.m9.1"><semantics id="S3.F1.24.9.m9.1b"><mi id="S3.F1.24.9.m9.1.1" xref="S3.F1.24.9.m9.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.F1.24.9.m9.1c"><ci id="S3.F1.24.9.m9.1.1.cmml" xref="S3.F1.24.9.m9.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.24.9.m9.1d">d</annotation><annotation encoding="application/x-llamapun" id="S3.F1.24.9.m9.1e">italic_d</annotation></semantics></math>, and color values, <math alttext="c" class="ltx_Math" display="inline" id="S3.F1.25.10.m10.1"><semantics id="S3.F1.25.10.m10.1b"><mi id="S3.F1.25.10.m10.1.1" xref="S3.F1.25.10.m10.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.F1.25.10.m10.1c"><ci id="S3.F1.25.10.m10.1.1.cmml" xref="S3.F1.25.10.m10.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.25.10.m10.1d">c</annotation><annotation encoding="application/x-llamapun" id="S3.F1.25.10.m10.1e">italic_c</annotation></semantics></math>, along a ray to get the final color, <math alttext="C" class="ltx_Math" display="inline" id="S3.F1.26.11.m11.1"><semantics id="S3.F1.26.11.m11.1b"><mi id="S3.F1.26.11.m11.1.1" xref="S3.F1.26.11.m11.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.F1.26.11.m11.1c"><ci id="S3.F1.26.11.m11.1.1.cmml" xref="S3.F1.26.11.m11.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.26.11.m11.1d">C</annotation><annotation encoding="application/x-llamapun" id="S3.F1.26.11.m11.1e">italic_C</annotation></semantics></math>. Similarly, density and feature values along a ray are accumulated to generate feature value, <math alttext="G" class="ltx_Math" display="inline" id="S3.F1.27.12.m12.1"><semantics id="S3.F1.27.12.m12.1b"><mi id="S3.F1.27.12.m12.1.1" xref="S3.F1.27.12.m12.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.F1.27.12.m12.1c"><ci id="S3.F1.27.12.m12.1.1.cmml" xref="S3.F1.27.12.m12.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.27.12.m12.1d">G</annotation><annotation encoding="application/x-llamapun" id="S3.F1.27.12.m12.1e">italic_G</annotation></semantics></math>, and silhouette value, <math alttext="S" class="ltx_Math" display="inline" id="S3.F1.28.13.m13.1"><semantics id="S3.F1.28.13.m13.1b"><mi id="S3.F1.28.13.m13.1.1" xref="S3.F1.28.13.m13.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.F1.28.13.m13.1c"><ci id="S3.F1.28.13.m13.1.1.cmml" xref="S3.F1.28.13.m13.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.28.13.m13.1d">S</annotation><annotation encoding="application/x-llamapun" id="S3.F1.28.13.m13.1e">italic_S</annotation></semantics></math>. Each ray corresponds to a pixel in an image. By generating rays for all the pixels, we can render our final image. Our CNN takes in the input image, <math alttext="C^{\prime}" class="ltx_Math" display="inline" id="S3.F1.29.14.m14.1"><semantics id="S3.F1.29.14.m14.1b"><msup id="S3.F1.29.14.m14.1.1" xref="S3.F1.29.14.m14.1.1.cmml"><mi id="S3.F1.29.14.m14.1.1.2" xref="S3.F1.29.14.m14.1.1.2.cmml">C</mi><mo id="S3.F1.29.14.m14.1.1.3" xref="S3.F1.29.14.m14.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.F1.29.14.m14.1c"><apply id="S3.F1.29.14.m14.1.1.cmml" xref="S3.F1.29.14.m14.1.1"><csymbol cd="ambiguous" id="S3.F1.29.14.m14.1.1.1.cmml" xref="S3.F1.29.14.m14.1.1">superscript</csymbol><ci id="S3.F1.29.14.m14.1.1.2.cmml" xref="S3.F1.29.14.m14.1.1.2">𝐶</ci><ci id="S3.F1.29.14.m14.1.1.3.cmml" xref="S3.F1.29.14.m14.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.29.14.m14.1d">C^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.F1.29.14.m14.1e">italic_C start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math> corresponding to the same viewpoint and predicts the feature image <math alttext="F" class="ltx_Math" display="inline" id="S3.F1.30.15.m15.1"><semantics id="S3.F1.30.15.m15.1b"><mi id="S3.F1.30.15.m15.1.1" xref="S3.F1.30.15.m15.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S3.F1.30.15.m15.1c"><ci id="S3.F1.30.15.m15.1.1.cmml" xref="S3.F1.30.15.m15.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.30.15.m15.1d">F</annotation><annotation encoding="application/x-llamapun" id="S3.F1.30.15.m15.1e">italic_F</annotation></semantics></math>. We formulate a contrastive feature loss between feature images from NeRF and CNN. We train the orange blocks during stage 1 and freeze them during stage 2 when blue blocks are trained. </span></figcaption>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Our goal is to learn view-invariant per-pixel features from a 2D input image of an object. We employ NeRF to learn an implicit 3D representation of the object using real images with known relative poses without requiring a CAD model. Specifically, we employ a Neural Radiance Field, NeRF, and a U-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib35" title=""><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite> based CNN to learn 3D consistent feature images. We force CNN and NeRF to learn consistent and discriminative features using contrastive learning. NeRF ensures that the learned features are consistent across viewpoints as it implicitly encodes the 3D information while learning. CNN implicitly handles symmetry as it works only based on appearance and forces the learned features to be similar for visually similar images although from different viewpoints. Moreover, NeRF and CNN compete while learning features as NeRF enforces 3D consistency and CNN enforces similar features to be learned for symmetric configurations.
After the features are learned, we can establish correspondences through features between an image and the 3D model in NeRF which can be used for pose estimation.
We train our pipeline in two stages. We initially train a NeRF network to learn object representation using posed real images. Then, we train CNN and NeRF together to learn features useful for pose estimation. We describe the stages in detail in the following sections:
</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Stage 1 - NeRF pretraining</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.7">The training data comprises of real object images and relative poses. We extract segmentation masks using Segment Any Thing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite> to extract segmented images. We train the NeRF using segmentation masks and segmented real images to learn object representation. Our training data comprises real images, <math alttext="C^{\prime}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><msup id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">C</mi><mo id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">superscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">𝐶</ci><ci id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">C^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">italic_C start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math>, segmentation masks, <math alttext="S^{\prime}" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><msup id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">S</mi><mo id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">superscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">𝑆</ci><ci id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">S^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">italic_S start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math>, and relative poses, <math alttext="T" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><mi id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">italic_T</annotation></semantics></math>.
NeRF generates rays from the camera center through each pixel of the image to pass through the volume where the object is implicitly reconstructed. NeRF employs a volumetric rendering approach to render images from a viewpoint by shooting rays through the volumetric representation of the object. At each 3D point, <math alttext="x" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4.1"><semantics id="S3.SS1.p1.4.m4.1a"><mi id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.1d">italic_x</annotation></semantics></math>, in the volume, NeRF network, <math alttext="\hat{N}" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m5.1"><semantics id="S3.SS1.p1.5.m5.1a"><mover accent="true" id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mi id="S3.SS1.p1.5.m5.1.1.2" xref="S3.SS1.p1.5.m5.1.1.2.cmml">N</mi><mo id="S3.SS1.p1.5.m5.1.1.1" xref="S3.SS1.p1.5.m5.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><ci id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1.1">^</ci><ci id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">\hat{N}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.5.m5.1d">over^ start_ARG italic_N end_ARG</annotation></semantics></math>, predicts density, <math alttext="d" class="ltx_Math" display="inline" id="S3.SS1.p1.6.m6.1"><semantics id="S3.SS1.p1.6.m6.1a"><mi id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><ci id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">d</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.6.m6.1d">italic_d</annotation></semantics></math>, representing the 3D object geometry using Density MLP and also predicts color, <math alttext="c" class="ltx_Math" display="inline" id="S3.SS1.p1.7.m7.1"><semantics id="S3.SS1.p1.7.m7.1a"><mi id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><ci id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">c</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.7.m7.1d">italic_c</annotation></semantics></math> using Color MLP.</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{N}(x)=\left(d,c\right)" class="ltx_Math" display="block" id="S3.E1.m1.3"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.4" xref="S3.E1.m1.3.4.cmml"><mrow id="S3.E1.m1.3.4.2" xref="S3.E1.m1.3.4.2.cmml"><mover accent="true" id="S3.E1.m1.3.4.2.2" xref="S3.E1.m1.3.4.2.2.cmml"><mi id="S3.E1.m1.3.4.2.2.2" xref="S3.E1.m1.3.4.2.2.2.cmml">N</mi><mo id="S3.E1.m1.3.4.2.2.1" xref="S3.E1.m1.3.4.2.2.1.cmml">^</mo></mover><mo id="S3.E1.m1.3.4.2.1" xref="S3.E1.m1.3.4.2.1.cmml">⁢</mo><mrow id="S3.E1.m1.3.4.2.3.2" xref="S3.E1.m1.3.4.2.cmml"><mo id="S3.E1.m1.3.4.2.3.2.1" stretchy="false" xref="S3.E1.m1.3.4.2.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">x</mi><mo id="S3.E1.m1.3.4.2.3.2.2" stretchy="false" xref="S3.E1.m1.3.4.2.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.3.4.1" xref="S3.E1.m1.3.4.1.cmml">=</mo><mrow id="S3.E1.m1.3.4.3.2" xref="S3.E1.m1.3.4.3.1.cmml"><mo id="S3.E1.m1.3.4.3.2.1" xref="S3.E1.m1.3.4.3.1.cmml">(</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">d</mi><mo id="S3.E1.m1.3.4.3.2.2" xref="S3.E1.m1.3.4.3.1.cmml">,</mo><mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">c</mi><mo id="S3.E1.m1.3.4.3.2.3" xref="S3.E1.m1.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.4.cmml" xref="S3.E1.m1.3.4"><eq id="S3.E1.m1.3.4.1.cmml" xref="S3.E1.m1.3.4.1"></eq><apply id="S3.E1.m1.3.4.2.cmml" xref="S3.E1.m1.3.4.2"><times id="S3.E1.m1.3.4.2.1.cmml" xref="S3.E1.m1.3.4.2.1"></times><apply id="S3.E1.m1.3.4.2.2.cmml" xref="S3.E1.m1.3.4.2.2"><ci id="S3.E1.m1.3.4.2.2.1.cmml" xref="S3.E1.m1.3.4.2.2.1">^</ci><ci id="S3.E1.m1.3.4.2.2.2.cmml" xref="S3.E1.m1.3.4.2.2.2">𝑁</ci></apply><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝑥</ci></apply><interval closure="open" id="S3.E1.m1.3.4.3.1.cmml" xref="S3.E1.m1.3.4.3.2"><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">𝑑</ci><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">𝑐</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">\hat{N}(x)=\left(d,c\right)</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.3d">over^ start_ARG italic_N end_ARG ( italic_x ) = ( italic_d , italic_c )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p1.9">In addition to rendering color, we render features with our NeRF network, <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.p1.8.m1.1"><semantics id="S3.SS1.p1.8.m1.1a"><mi id="S3.SS1.p1.8.m1.1.1" xref="S3.SS1.p1.8.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m1.1b"><ci id="S3.SS1.p1.8.m1.1.1.cmml" xref="S3.SS1.p1.8.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.8.m1.1d">italic_N</annotation></semantics></math>, using another MLP, Feature MLP, to carry out feature learning in the second stage. So, our network predicts per-point feature, <math alttext="f" class="ltx_Math" display="inline" id="S3.SS1.p1.9.m2.1"><semantics id="S3.SS1.p1.9.m2.1a"><mi id="S3.SS1.p1.9.m2.1.1" xref="S3.SS1.p1.9.m2.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m2.1b"><ci id="S3.SS1.p1.9.m2.1.1.cmml" xref="S3.SS1.p1.9.m2.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m2.1c">f</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.9.m2.1d">italic_f</annotation></semantics></math>, in addition to density and color as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="N(x)=\left(d,c,f\right)" class="ltx_Math" display="block" id="S3.E2.m1.4"><semantics id="S3.E2.m1.4a"><mrow id="S3.E2.m1.4.5" xref="S3.E2.m1.4.5.cmml"><mrow id="S3.E2.m1.4.5.2" xref="S3.E2.m1.4.5.2.cmml"><mi id="S3.E2.m1.4.5.2.2" xref="S3.E2.m1.4.5.2.2.cmml">N</mi><mo id="S3.E2.m1.4.5.2.1" xref="S3.E2.m1.4.5.2.1.cmml">⁢</mo><mrow id="S3.E2.m1.4.5.2.3.2" xref="S3.E2.m1.4.5.2.cmml"><mo id="S3.E2.m1.4.5.2.3.2.1" stretchy="false" xref="S3.E2.m1.4.5.2.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">x</mi><mo id="S3.E2.m1.4.5.2.3.2.2" stretchy="false" xref="S3.E2.m1.4.5.2.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.4.5.1" xref="S3.E2.m1.4.5.1.cmml">=</mo><mrow id="S3.E2.m1.4.5.3.2" xref="S3.E2.m1.4.5.3.1.cmml"><mo id="S3.E2.m1.4.5.3.2.1" xref="S3.E2.m1.4.5.3.1.cmml">(</mo><mi id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">d</mi><mo id="S3.E2.m1.4.5.3.2.2" xref="S3.E2.m1.4.5.3.1.cmml">,</mo><mi id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml">c</mi><mo id="S3.E2.m1.4.5.3.2.3" xref="S3.E2.m1.4.5.3.1.cmml">,</mo><mi id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml">f</mi><mo id="S3.E2.m1.4.5.3.2.4" xref="S3.E2.m1.4.5.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.4b"><apply id="S3.E2.m1.4.5.cmml" xref="S3.E2.m1.4.5"><eq id="S3.E2.m1.4.5.1.cmml" xref="S3.E2.m1.4.5.1"></eq><apply id="S3.E2.m1.4.5.2.cmml" xref="S3.E2.m1.4.5.2"><times id="S3.E2.m1.4.5.2.1.cmml" xref="S3.E2.m1.4.5.2.1"></times><ci id="S3.E2.m1.4.5.2.2.cmml" xref="S3.E2.m1.4.5.2.2">𝑁</ci><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝑥</ci></apply><vector id="S3.E2.m1.4.5.3.1.cmml" xref="S3.E2.m1.4.5.3.2"><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">𝑑</ci><ci id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3">𝑐</ci><ci id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4">𝑓</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.4c">N(x)=\left(d,c,f\right)</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.4d">italic_N ( italic_x ) = ( italic_d , italic_c , italic_f )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<figure class="ltx_figure" id="S3.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F2.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="1339" id="S3.F2.1.g1" src="x2.png" width="1370"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="1385" id="S3.F2.2.g1" src="x3.png" width="1370"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F2.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="673" id="S3.F2.3.g1" src="extracted/5679340/images/4.png" width="343"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F2.4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1354" id="S3.F2.4.g1" src="x4.png" width="726"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.6.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S3.F2.7.2" style="font-size:90%;">Visualization of learned feature representation of symmetric T-Less objects along with meshes </span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.7">Our architecture is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S3.F1" title="Figure 1 ‣ 3 Method ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_tag">1</span></a>. Note that we do not send ray direction as input to Feature MLP as we want to learn view-independent features that resemble the object’s intrinsic properties such as albedo and geometry. A ray generated from the camera center through the pixel passes through the volume where the object is being reconstructed. In volumetric rendering, the final color for a specific ray generated for a pixel in the image is obtained by integrating densities and color values along various points along the rays. We query the NeRF network at every point along the ray to obtain per-point density, color and feature. The final color, <math alttext="C" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">C</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">italic_C</annotation></semantics></math>, feature, <math alttext="G" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">G</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">italic_G</annotation></semantics></math>, and silhouette, <math alttext="S" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3.1"><semantics id="S3.SS1.p2.3.m3.1a"><mi id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">S</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m3.1d">italic_S</annotation></semantics></math>, for a ray with color, <math alttext="c_{i}" class="ltx_Math" display="inline" id="S3.SS1.p2.4.m4.1"><semantics id="S3.SS1.p2.4.m4.1a"><msub id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><mi id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml">c</mi><mi id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2">𝑐</ci><ci id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">c_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.4.m4.1d">italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, feature, <math alttext="f_{i}" class="ltx_Math" display="inline" id="S3.SS1.p2.5.m5.1"><semantics id="S3.SS1.p2.5.m5.1a"><msub id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml"><mi id="S3.SS1.p2.5.m5.1.1.2" xref="S3.SS1.p2.5.m5.1.1.2.cmml">f</mi><mi id="S3.SS1.p2.5.m5.1.1.3" xref="S3.SS1.p2.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><apply id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.1.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p2.5.m5.1.1.2.cmml" xref="S3.SS1.p2.5.m5.1.1.2">𝑓</ci><ci id="S3.SS1.p2.5.m5.1.1.3.cmml" xref="S3.SS1.p2.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">f_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.5.m5.1d">italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, and density, <math alttext="d_{i}" class="ltx_Math" display="inline" id="S3.SS1.p2.6.m6.1"><semantics id="S3.SS1.p2.6.m6.1a"><msub id="S3.SS1.p2.6.m6.1.1" xref="S3.SS1.p2.6.m6.1.1.cmml"><mi id="S3.SS1.p2.6.m6.1.1.2" xref="S3.SS1.p2.6.m6.1.1.2.cmml">d</mi><mi id="S3.SS1.p2.6.m6.1.1.3" xref="S3.SS1.p2.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><apply id="S3.SS1.p2.6.m6.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.1.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.p2.6.m6.1.1.2.cmml" xref="S3.SS1.p2.6.m6.1.1.2">𝑑</ci><ci id="S3.SS1.p2.6.m6.1.1.3.cmml" xref="S3.SS1.p2.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">d_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.6.m6.1d">italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, at a certain point <math alttext="i" class="ltx_Math" display="inline" id="S3.SS1.p2.7.m7.1"><semantics id="S3.SS1.p2.7.m7.1a"><mi id="S3.SS1.p2.7.m7.1.1" xref="S3.SS1.p2.7.m7.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.1b"><ci id="S3.SS1.p2.7.m7.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.7.m7.1d">italic_i</annotation></semantics></math> along the ray with M points, is computed as follows:</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.Ex1">
<tbody id="S3.Ex1X"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle w_{i}" class="ltx_Math" display="inline" id="S3.Ex1X.2.1.1.m1.1"><semantics id="S3.Ex1X.2.1.1.m1.1a"><msub id="S3.Ex1X.2.1.1.m1.1.1" xref="S3.Ex1X.2.1.1.m1.1.1.cmml"><mi id="S3.Ex1X.2.1.1.m1.1.1.2" xref="S3.Ex1X.2.1.1.m1.1.1.2.cmml">w</mi><mi id="S3.Ex1X.2.1.1.m1.1.1.3" xref="S3.Ex1X.2.1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.Ex1X.2.1.1.m1.1b"><apply id="S3.Ex1X.2.1.1.m1.1.1.cmml" xref="S3.Ex1X.2.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.Ex1X.2.1.1.m1.1.1.1.cmml" xref="S3.Ex1X.2.1.1.m1.1.1">subscript</csymbol><ci id="S3.Ex1X.2.1.1.m1.1.1.2.cmml" xref="S3.Ex1X.2.1.1.m1.1.1.2">𝑤</ci><ci id="S3.Ex1X.2.1.1.m1.1.1.3.cmml" xref="S3.Ex1X.2.1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1X.2.1.1.m1.1c">\displaystyle w_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.Ex1X.2.1.1.m1.1d">italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\exp(-\sum_{j=1}^{i-1}(-d_{j}))" class="ltx_Math" display="inline" id="S3.Ex1X.3.2.2.m1.2"><semantics id="S3.Ex1X.3.2.2.m1.2a"><mrow id="S3.Ex1X.3.2.2.m1.2.2" xref="S3.Ex1X.3.2.2.m1.2.2.cmml"><mi id="S3.Ex1X.3.2.2.m1.2.2.3" xref="S3.Ex1X.3.2.2.m1.2.2.3.cmml"></mi><mo id="S3.Ex1X.3.2.2.m1.2.2.2" xref="S3.Ex1X.3.2.2.m1.2.2.2.cmml">=</mo><mrow id="S3.Ex1X.3.2.2.m1.2.2.1.1" xref="S3.Ex1X.3.2.2.m1.2.2.1.2.cmml"><mi id="S3.Ex1X.3.2.2.m1.1.1" xref="S3.Ex1X.3.2.2.m1.1.1.cmml">exp</mi><mo id="S3.Ex1X.3.2.2.m1.2.2.1.1a" xref="S3.Ex1X.3.2.2.m1.2.2.1.2.cmml">⁡</mo><mrow id="S3.Ex1X.3.2.2.m1.2.2.1.1.1" xref="S3.Ex1X.3.2.2.m1.2.2.1.2.cmml"><mo id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.2" stretchy="false" xref="S3.Ex1X.3.2.2.m1.2.2.1.2.cmml">(</mo><mrow id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.cmml"><mo id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1a" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.cmml">−</mo><mrow id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.cmml"><munderover id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2a" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.cmml"><mo id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.2.2" movablelimits="false" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.2.3" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.2.3.cmml"><mi id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.2.3.2" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.2.3.2.cmml">j</mi><mo id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.2.3.1" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.2.3.3" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mrow id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.3" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.3.cmml"><mi id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.3.2" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.3.2.cmml">i</mi><mo id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.3.1" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.3.1.cmml">−</mo><mn id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.3.3" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.3.3.cmml">1</mn></mrow></munderover></mstyle><mrow id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.1.1" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.cmml"><mo id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.cmml"><mo id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.1.1.1a" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.2" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.2.2" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.2.2.cmml">d</mi><mi id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.2.3" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.2.3.cmml">j</mi></msub></mrow><mo id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.3" stretchy="false" xref="S3.Ex1X.3.2.2.m1.2.2.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1X.3.2.2.m1.2b"><apply id="S3.Ex1X.3.2.2.m1.2.2.cmml" xref="S3.Ex1X.3.2.2.m1.2.2"><eq id="S3.Ex1X.3.2.2.m1.2.2.2.cmml" xref="S3.Ex1X.3.2.2.m1.2.2.2"></eq><csymbol cd="latexml" id="S3.Ex1X.3.2.2.m1.2.2.3.cmml" xref="S3.Ex1X.3.2.2.m1.2.2.3">absent</csymbol><apply id="S3.Ex1X.3.2.2.m1.2.2.1.2.cmml" xref="S3.Ex1X.3.2.2.m1.2.2.1.1"><exp id="S3.Ex1X.3.2.2.m1.1.1.cmml" xref="S3.Ex1X.3.2.2.m1.1.1"></exp><apply id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.cmml" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1"><minus id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.2.cmml" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1"></minus><apply id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.cmml" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1"><apply id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.1.cmml" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2">superscript</csymbol><apply id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.2.cmml" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.2.1.cmml" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2">subscript</csymbol><sum id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.2.2.cmml" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.2.2"></sum><apply id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.2.3.cmml" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.2.3"><eq id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.2.3.1.cmml" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.2.3.1"></eq><ci id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.2.3.2.cmml" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.2.3.2">𝑗</ci><cn id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.2.3.3.cmml" type="integer" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><apply id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.3.cmml" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.3"><minus id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.3.1.cmml" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.3.1"></minus><ci id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.3.2.cmml" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.3.2">𝑖</ci><cn id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.3.3.cmml" type="integer" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.2.3.3">1</cn></apply></apply><apply id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.1.1"><minus id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.1.1"></minus><apply id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.2.2">𝑑</ci><ci id="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.Ex1X.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.2.3">𝑗</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1X.3.2.2.m1.2c">\displaystyle=\exp(-\sum_{j=1}^{i-1}(-d_{j}))</annotation><annotation encoding="application/x-llamapun" id="S3.Ex1X.3.2.2.m1.2d">= roman_exp ( - ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i - 1 end_POSTSUPERSCRIPT ( - italic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.Ex1Xa"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle C" class="ltx_Math" display="inline" id="S3.Ex1Xa.2.1.1.m1.1"><semantics id="S3.Ex1Xa.2.1.1.m1.1a"><mi id="S3.Ex1Xa.2.1.1.m1.1.1" xref="S3.Ex1Xa.2.1.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.Ex1Xa.2.1.1.m1.1b"><ci id="S3.Ex1Xa.2.1.1.m1.1.1.cmml" xref="S3.Ex1Xa.2.1.1.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1Xa.2.1.1.m1.1c">\displaystyle C</annotation><annotation encoding="application/x-llamapun" id="S3.Ex1Xa.2.1.1.m1.1d">italic_C</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\sum_{i=0}^{M}w_{i}(1-\exp(-d_{i}))c_{i}" class="ltx_Math" display="inline" id="S3.Ex1Xa.3.2.2.m1.2"><semantics id="S3.Ex1Xa.3.2.2.m1.2a"><mrow id="S3.Ex1Xa.3.2.2.m1.2.2" xref="S3.Ex1Xa.3.2.2.m1.2.2.cmml"><mi id="S3.Ex1Xa.3.2.2.m1.2.2.3" xref="S3.Ex1Xa.3.2.2.m1.2.2.3.cmml"></mi><mo id="S3.Ex1Xa.3.2.2.m1.2.2.2" xref="S3.Ex1Xa.3.2.2.m1.2.2.2.cmml">=</mo><mrow id="S3.Ex1Xa.3.2.2.m1.2.2.1" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.cmml"><mstyle displaystyle="true" id="S3.Ex1Xa.3.2.2.m1.2.2.1.2" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.2.cmml"><munderover id="S3.Ex1Xa.3.2.2.m1.2.2.1.2a" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.2.cmml"><mo id="S3.Ex1Xa.3.2.2.m1.2.2.1.2.2.2" movablelimits="false" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.2.2.2.cmml">∑</mo><mrow id="S3.Ex1Xa.3.2.2.m1.2.2.1.2.2.3" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.2.2.3.cmml"><mi id="S3.Ex1Xa.3.2.2.m1.2.2.1.2.2.3.2" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.2.2.3.2.cmml">i</mi><mo id="S3.Ex1Xa.3.2.2.m1.2.2.1.2.2.3.1" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.2.2.3.1.cmml">=</mo><mn id="S3.Ex1Xa.3.2.2.m1.2.2.1.2.2.3.3" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.2.2.3.3.cmml">0</mn></mrow><mi id="S3.Ex1Xa.3.2.2.m1.2.2.1.2.3" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.2.3.cmml">M</mi></munderover></mstyle><mrow id="S3.Ex1Xa.3.2.2.m1.2.2.1.1" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.cmml"><msub id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.3" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.3.cmml"><mi id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.3.2" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.3.2.cmml">w</mi><mi id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.3.3" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.3.3.cmml">i</mi></msub><mo id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.2" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.2.cmml">⁢</mo><mrow id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.cmml"><mo id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.2" stretchy="false" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.cmml"><mn id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.3" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.3.cmml">1</mn><mo id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.2" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.2.cmml">−</mo><mrow id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.1" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.2.cmml"><mi id="S3.Ex1Xa.3.2.2.m1.1.1" xref="S3.Ex1Xa.3.2.2.m1.1.1.cmml">exp</mi><mo id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.1a" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.2.cmml">⁡</mo><mrow id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.2.cmml"><mo id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.2.cmml">(</mo><mrow id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1a" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.2" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.2.2" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.cmml">d</mi><mi id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.2.3" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub></mrow><mo id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.3" stretchy="false" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.2a" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.2.cmml">⁢</mo><msub id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.4" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.4.cmml"><mi id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.4.2" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.4.2.cmml">c</mi><mi id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.4.3" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.4.3.cmml">i</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1Xa.3.2.2.m1.2b"><apply id="S3.Ex1Xa.3.2.2.m1.2.2.cmml" xref="S3.Ex1Xa.3.2.2.m1.2.2"><eq id="S3.Ex1Xa.3.2.2.m1.2.2.2.cmml" xref="S3.Ex1Xa.3.2.2.m1.2.2.2"></eq><csymbol cd="latexml" id="S3.Ex1Xa.3.2.2.m1.2.2.3.cmml" xref="S3.Ex1Xa.3.2.2.m1.2.2.3">absent</csymbol><apply id="S3.Ex1Xa.3.2.2.m1.2.2.1.cmml" xref="S3.Ex1Xa.3.2.2.m1.2.2.1"><apply id="S3.Ex1Xa.3.2.2.m1.2.2.1.2.cmml" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.2"><csymbol cd="ambiguous" id="S3.Ex1Xa.3.2.2.m1.2.2.1.2.1.cmml" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.2">superscript</csymbol><apply id="S3.Ex1Xa.3.2.2.m1.2.2.1.2.2.cmml" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.2"><csymbol cd="ambiguous" id="S3.Ex1Xa.3.2.2.m1.2.2.1.2.2.1.cmml" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.2">subscript</csymbol><sum id="S3.Ex1Xa.3.2.2.m1.2.2.1.2.2.2.cmml" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.2.2.2"></sum><apply id="S3.Ex1Xa.3.2.2.m1.2.2.1.2.2.3.cmml" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.2.2.3"><eq id="S3.Ex1Xa.3.2.2.m1.2.2.1.2.2.3.1.cmml" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.2.2.3.1"></eq><ci id="S3.Ex1Xa.3.2.2.m1.2.2.1.2.2.3.2.cmml" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.2.2.3.2">𝑖</ci><cn id="S3.Ex1Xa.3.2.2.m1.2.2.1.2.2.3.3.cmml" type="integer" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.2.2.3.3">0</cn></apply></apply><ci id="S3.Ex1Xa.3.2.2.m1.2.2.1.2.3.cmml" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.2.3">𝑀</ci></apply><apply id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.cmml" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1"><times id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.2.cmml" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.2"></times><apply id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.3.cmml" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.3.1.cmml" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.3">subscript</csymbol><ci id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.3.2.cmml" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.3.2">𝑤</ci><ci id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.3.3.cmml" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.3.3">𝑖</ci></apply><apply id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.cmml" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1"><minus id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.2"></minus><cn id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.3.cmml" type="integer" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.3">1</cn><apply id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.1"><exp id="S3.Ex1Xa.3.2.2.m1.1.1.cmml" xref="S3.Ex1Xa.3.2.2.m1.1.1"></exp><apply id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1"><minus id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1"></minus><apply id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.2.2">𝑑</ci><ci id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply></apply></apply></apply><apply id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.4.cmml" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.4"><csymbol cd="ambiguous" id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.4.1.cmml" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.4">subscript</csymbol><ci id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.4.2.cmml" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.4.2">𝑐</ci><ci id="S3.Ex1Xa.3.2.2.m1.2.2.1.1.4.3.cmml" xref="S3.Ex1Xa.3.2.2.m1.2.2.1.1.4.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1Xa.3.2.2.m1.2c">\displaystyle=\sum_{i=0}^{M}w_{i}(1-\exp(-d_{i}))c_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.Ex1Xa.3.2.2.m1.2d">= ∑ start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( 1 - roman_exp ( - italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.Ex1Xb"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle G" class="ltx_Math" display="inline" id="S3.Ex1Xb.2.1.1.m1.1"><semantics id="S3.Ex1Xb.2.1.1.m1.1a"><mi id="S3.Ex1Xb.2.1.1.m1.1.1" xref="S3.Ex1Xb.2.1.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.Ex1Xb.2.1.1.m1.1b"><ci id="S3.Ex1Xb.2.1.1.m1.1.1.cmml" xref="S3.Ex1Xb.2.1.1.m1.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1Xb.2.1.1.m1.1c">\displaystyle G</annotation><annotation encoding="application/x-llamapun" id="S3.Ex1Xb.2.1.1.m1.1d">italic_G</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\sum_{i=0}^{M}w_{i}(1-\exp(-d_{i}))f_{i}" class="ltx_Math" display="inline" id="S3.Ex1Xb.3.2.2.m1.2"><semantics id="S3.Ex1Xb.3.2.2.m1.2a"><mrow id="S3.Ex1Xb.3.2.2.m1.2.2" xref="S3.Ex1Xb.3.2.2.m1.2.2.cmml"><mi id="S3.Ex1Xb.3.2.2.m1.2.2.3" xref="S3.Ex1Xb.3.2.2.m1.2.2.3.cmml"></mi><mo id="S3.Ex1Xb.3.2.2.m1.2.2.2" xref="S3.Ex1Xb.3.2.2.m1.2.2.2.cmml">=</mo><mrow id="S3.Ex1Xb.3.2.2.m1.2.2.1" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.cmml"><mstyle displaystyle="true" id="S3.Ex1Xb.3.2.2.m1.2.2.1.2" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.2.cmml"><munderover id="S3.Ex1Xb.3.2.2.m1.2.2.1.2a" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.2.cmml"><mo id="S3.Ex1Xb.3.2.2.m1.2.2.1.2.2.2" movablelimits="false" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.2.2.2.cmml">∑</mo><mrow id="S3.Ex1Xb.3.2.2.m1.2.2.1.2.2.3" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.2.2.3.cmml"><mi id="S3.Ex1Xb.3.2.2.m1.2.2.1.2.2.3.2" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.2.2.3.2.cmml">i</mi><mo id="S3.Ex1Xb.3.2.2.m1.2.2.1.2.2.3.1" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.2.2.3.1.cmml">=</mo><mn id="S3.Ex1Xb.3.2.2.m1.2.2.1.2.2.3.3" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.2.2.3.3.cmml">0</mn></mrow><mi id="S3.Ex1Xb.3.2.2.m1.2.2.1.2.3" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.2.3.cmml">M</mi></munderover></mstyle><mrow id="S3.Ex1Xb.3.2.2.m1.2.2.1.1" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.cmml"><msub id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.3" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.3.cmml"><mi id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.3.2" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.3.2.cmml">w</mi><mi id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.3.3" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.3.3.cmml">i</mi></msub><mo id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.2" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.2.cmml">⁢</mo><mrow id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.cmml"><mo id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.2" stretchy="false" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.cmml"><mn id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.3" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.3.cmml">1</mn><mo id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.2" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.2.cmml">−</mo><mrow id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.1.1" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.1.2.cmml"><mi id="S3.Ex1Xb.3.2.2.m1.1.1" xref="S3.Ex1Xb.3.2.2.m1.1.1.cmml">exp</mi><mo id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.1.1a" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.1.2.cmml">⁡</mo><mrow id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.1.2.cmml"><mo id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.1.2.cmml">(</mo><mrow id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1a" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.2" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.2.2" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.cmml">d</mi><mi id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.2.3" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub></mrow><mo id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.3" stretchy="false" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.2a" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.2.cmml">⁢</mo><msub id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.4" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.4.cmml"><mi id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.4.2" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.4.2.cmml">f</mi><mi id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.4.3" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.4.3.cmml">i</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1Xb.3.2.2.m1.2b"><apply id="S3.Ex1Xb.3.2.2.m1.2.2.cmml" xref="S3.Ex1Xb.3.2.2.m1.2.2"><eq id="S3.Ex1Xb.3.2.2.m1.2.2.2.cmml" xref="S3.Ex1Xb.3.2.2.m1.2.2.2"></eq><csymbol cd="latexml" id="S3.Ex1Xb.3.2.2.m1.2.2.3.cmml" xref="S3.Ex1Xb.3.2.2.m1.2.2.3">absent</csymbol><apply id="S3.Ex1Xb.3.2.2.m1.2.2.1.cmml" xref="S3.Ex1Xb.3.2.2.m1.2.2.1"><apply id="S3.Ex1Xb.3.2.2.m1.2.2.1.2.cmml" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.2"><csymbol cd="ambiguous" id="S3.Ex1Xb.3.2.2.m1.2.2.1.2.1.cmml" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.2">superscript</csymbol><apply id="S3.Ex1Xb.3.2.2.m1.2.2.1.2.2.cmml" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.2"><csymbol cd="ambiguous" id="S3.Ex1Xb.3.2.2.m1.2.2.1.2.2.1.cmml" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.2">subscript</csymbol><sum id="S3.Ex1Xb.3.2.2.m1.2.2.1.2.2.2.cmml" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.2.2.2"></sum><apply id="S3.Ex1Xb.3.2.2.m1.2.2.1.2.2.3.cmml" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.2.2.3"><eq id="S3.Ex1Xb.3.2.2.m1.2.2.1.2.2.3.1.cmml" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.2.2.3.1"></eq><ci id="S3.Ex1Xb.3.2.2.m1.2.2.1.2.2.3.2.cmml" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.2.2.3.2">𝑖</ci><cn id="S3.Ex1Xb.3.2.2.m1.2.2.1.2.2.3.3.cmml" type="integer" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.2.2.3.3">0</cn></apply></apply><ci id="S3.Ex1Xb.3.2.2.m1.2.2.1.2.3.cmml" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.2.3">𝑀</ci></apply><apply id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.cmml" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1"><times id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.2.cmml" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.2"></times><apply id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.3.cmml" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.3.1.cmml" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.3">subscript</csymbol><ci id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.3.2.cmml" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.3.2">𝑤</ci><ci id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.3.3.cmml" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.3.3">𝑖</ci></apply><apply id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.cmml" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1"><minus id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.2"></minus><cn id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.3.cmml" type="integer" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.3">1</cn><apply id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.1.1"><exp id="S3.Ex1Xb.3.2.2.m1.1.1.cmml" xref="S3.Ex1Xb.3.2.2.m1.1.1"></exp><apply id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1"><minus id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1"></minus><apply id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.2.2">𝑑</ci><ci id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply></apply></apply></apply><apply id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.4.cmml" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.4"><csymbol cd="ambiguous" id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.4.1.cmml" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.4">subscript</csymbol><ci id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.4.2.cmml" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.4.2">𝑓</ci><ci id="S3.Ex1Xb.3.2.2.m1.2.2.1.1.4.3.cmml" xref="S3.Ex1Xb.3.2.2.m1.2.2.1.1.4.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1Xb.3.2.2.m1.2c">\displaystyle=\sum_{i=0}^{M}w_{i}(1-\exp(-d_{i}))f_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.Ex1Xb.3.2.2.m1.2d">= ∑ start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( 1 - roman_exp ( - italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.Ex1Xc"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle S" class="ltx_Math" display="inline" id="S3.Ex1Xc.2.1.1.m1.1"><semantics id="S3.Ex1Xc.2.1.1.m1.1a"><mi id="S3.Ex1Xc.2.1.1.m1.1.1" xref="S3.Ex1Xc.2.1.1.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.Ex1Xc.2.1.1.m1.1b"><ci id="S3.Ex1Xc.2.1.1.m1.1.1.cmml" xref="S3.Ex1Xc.2.1.1.m1.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1Xc.2.1.1.m1.1c">\displaystyle S</annotation><annotation encoding="application/x-llamapun" id="S3.Ex1Xc.2.1.1.m1.1d">italic_S</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=1-\exp(\sum_{i=0}^{M}d_{i})" class="ltx_Math" display="inline" id="S3.Ex1Xc.3.2.2.m1.2"><semantics id="S3.Ex1Xc.3.2.2.m1.2a"><mrow id="S3.Ex1Xc.3.2.2.m1.2.2" xref="S3.Ex1Xc.3.2.2.m1.2.2.cmml"><mi id="S3.Ex1Xc.3.2.2.m1.2.2.3" xref="S3.Ex1Xc.3.2.2.m1.2.2.3.cmml"></mi><mo id="S3.Ex1Xc.3.2.2.m1.2.2.2" xref="S3.Ex1Xc.3.2.2.m1.2.2.2.cmml">=</mo><mrow id="S3.Ex1Xc.3.2.2.m1.2.2.1" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.cmml"><mn id="S3.Ex1Xc.3.2.2.m1.2.2.1.3" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.3.cmml">1</mn><mo id="S3.Ex1Xc.3.2.2.m1.2.2.1.2" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.2.cmml">−</mo><mrow id="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.1.2.cmml"><mi id="S3.Ex1Xc.3.2.2.m1.1.1" xref="S3.Ex1Xc.3.2.2.m1.1.1.cmml">exp</mi><mo id="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1a" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.1.2.cmml">⁡</mo><mrow id="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.1.2.cmml"><mo id="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.2" stretchy="false" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.1.2.cmml">(</mo><mrow id="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.1" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.1.cmml"><munderover id="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.1a" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.1.cmml"><mo id="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.1.2.2" movablelimits="false" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.1.2.2.cmml">∑</mo><mrow id="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.1.2.3" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.1.2.3.cmml"><mi id="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.1.2.3.2" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.1.2.3.2.cmml">i</mi><mo id="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.1.2.3.1" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.1.2.3.1.cmml">=</mo><mn id="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.1.2.3.3" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.1.2.3.3.cmml">0</mn></mrow><mi id="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.1.3" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.1.3.cmml">M</mi></munderover></mstyle><msub id="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.2" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.2.cmml"><mi id="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.2.2" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.2.2.cmml">d</mi><mi id="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.2.3" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.2.3.cmml">i</mi></msub></mrow><mo id="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.3" stretchy="false" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1Xc.3.2.2.m1.2b"><apply id="S3.Ex1Xc.3.2.2.m1.2.2.cmml" xref="S3.Ex1Xc.3.2.2.m1.2.2"><eq id="S3.Ex1Xc.3.2.2.m1.2.2.2.cmml" xref="S3.Ex1Xc.3.2.2.m1.2.2.2"></eq><csymbol cd="latexml" id="S3.Ex1Xc.3.2.2.m1.2.2.3.cmml" xref="S3.Ex1Xc.3.2.2.m1.2.2.3">absent</csymbol><apply id="S3.Ex1Xc.3.2.2.m1.2.2.1.cmml" xref="S3.Ex1Xc.3.2.2.m1.2.2.1"><minus id="S3.Ex1Xc.3.2.2.m1.2.2.1.2.cmml" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.2"></minus><cn id="S3.Ex1Xc.3.2.2.m1.2.2.1.3.cmml" type="integer" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.3">1</cn><apply id="S3.Ex1Xc.3.2.2.m1.2.2.1.1.2.cmml" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1"><exp id="S3.Ex1Xc.3.2.2.m1.1.1.cmml" xref="S3.Ex1Xc.3.2.2.m1.1.1"></exp><apply id="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.cmml" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1"><apply id="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.1">superscript</csymbol><apply id="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.1.2.1.cmml" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.1">subscript</csymbol><sum id="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.1.2.2.cmml" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.1.2.2"></sum><apply id="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.1.2.3.cmml" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.1.2.3"><eq id="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.1.2.3.1.cmml" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.1.2.3.1"></eq><ci id="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.1.2.3.2.cmml" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.1.2.3.2">𝑖</ci><cn id="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.1.2.3.3.cmml" type="integer" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.1.2.3.3">0</cn></apply></apply><ci id="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.1.3">𝑀</ci></apply><apply id="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.2.1.cmml" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.2">subscript</csymbol><ci id="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.2.2.cmml" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.2.2">𝑑</ci><ci id="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.2.3.cmml" xref="S3.Ex1Xc.3.2.2.m1.2.2.1.1.1.1.1.2.3">𝑖</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1Xc.3.2.2.m1.2c">\displaystyle=1-\exp(\sum_{i=0}^{M}d_{i})</annotation><annotation encoding="application/x-llamapun" id="S3.Ex1Xc.3.2.2.m1.2d">= 1 - roman_exp ( ∑ start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p2.15">We train our NeRF using the loss formulated between ground truth color image, <math alttext="C^{\prime}" class="ltx_Math" display="inline" id="S3.SS1.p2.8.m1.1"><semantics id="S3.SS1.p2.8.m1.1a"><msup id="S3.SS1.p2.8.m1.1.1" xref="S3.SS1.p2.8.m1.1.1.cmml"><mi id="S3.SS1.p2.8.m1.1.1.2" xref="S3.SS1.p2.8.m1.1.1.2.cmml">C</mi><mo id="S3.SS1.p2.8.m1.1.1.3" xref="S3.SS1.p2.8.m1.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m1.1b"><apply id="S3.SS1.p2.8.m1.1.1.cmml" xref="S3.SS1.p2.8.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.8.m1.1.1.1.cmml" xref="S3.SS1.p2.8.m1.1.1">superscript</csymbol><ci id="S3.SS1.p2.8.m1.1.1.2.cmml" xref="S3.SS1.p2.8.m1.1.1.2">𝐶</ci><ci id="S3.SS1.p2.8.m1.1.1.3.cmml" xref="S3.SS1.p2.8.m1.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m1.1c">C^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.8.m1.1d">italic_C start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math>, segmentation image, <math alttext="S^{\prime}" class="ltx_Math" display="inline" id="S3.SS1.p2.9.m2.1"><semantics id="S3.SS1.p2.9.m2.1a"><msup id="S3.SS1.p2.9.m2.1.1" xref="S3.SS1.p2.9.m2.1.1.cmml"><mi id="S3.SS1.p2.9.m2.1.1.2" xref="S3.SS1.p2.9.m2.1.1.2.cmml">S</mi><mo id="S3.SS1.p2.9.m2.1.1.3" xref="S3.SS1.p2.9.m2.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.9.m2.1b"><apply id="S3.SS1.p2.9.m2.1.1.cmml" xref="S3.SS1.p2.9.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.9.m2.1.1.1.cmml" xref="S3.SS1.p2.9.m2.1.1">superscript</csymbol><ci id="S3.SS1.p2.9.m2.1.1.2.cmml" xref="S3.SS1.p2.9.m2.1.1.2">𝑆</ci><ci id="S3.SS1.p2.9.m2.1.1.3.cmml" xref="S3.SS1.p2.9.m2.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.9.m2.1c">S^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.9.m2.1d">italic_S start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math>, and rendered color, <math alttext="C" class="ltx_Math" display="inline" id="S3.SS1.p2.10.m3.1"><semantics id="S3.SS1.p2.10.m3.1a"><mi id="S3.SS1.p2.10.m3.1.1" xref="S3.SS1.p2.10.m3.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.10.m3.1b"><ci id="S3.SS1.p2.10.m3.1.1.cmml" xref="S3.SS1.p2.10.m3.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.10.m3.1c">C</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.10.m3.1d">italic_C</annotation></semantics></math>, accumulated using volumetric rendering parameters, density, and colors acquired from NeRF network. We also employ loss between rendered segmentation mask, <math alttext="S" class="ltx_Math" display="inline" id="S3.SS1.p2.11.m4.1"><semantics id="S3.SS1.p2.11.m4.1a"><mi id="S3.SS1.p2.11.m4.1.1" xref="S3.SS1.p2.11.m4.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.11.m4.1b"><ci id="S3.SS1.p2.11.m4.1.1.cmml" xref="S3.SS1.p2.11.m4.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.11.m4.1c">S</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.11.m4.1d">italic_S</annotation></semantics></math>, and groundtruth segmentation mask, <math alttext="S^{\prime}" class="ltx_Math" display="inline" id="S3.SS1.p2.12.m5.1"><semantics id="S3.SS1.p2.12.m5.1a"><msup id="S3.SS1.p2.12.m5.1.1" xref="S3.SS1.p2.12.m5.1.1.cmml"><mi id="S3.SS1.p2.12.m5.1.1.2" xref="S3.SS1.p2.12.m5.1.1.2.cmml">S</mi><mo id="S3.SS1.p2.12.m5.1.1.3" xref="S3.SS1.p2.12.m5.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.12.m5.1b"><apply id="S3.SS1.p2.12.m5.1.1.cmml" xref="S3.SS1.p2.12.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.12.m5.1.1.1.cmml" xref="S3.SS1.p2.12.m5.1.1">superscript</csymbol><ci id="S3.SS1.p2.12.m5.1.1.2.cmml" xref="S3.SS1.p2.12.m5.1.1.2">𝑆</ci><ci id="S3.SS1.p2.12.m5.1.1.3.cmml" xref="S3.SS1.p2.12.m5.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.12.m5.1c">S^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.12.m5.1d">italic_S start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math>, to formulate silhouette loss. So, the color loss, <math alttext="L_{c}" class="ltx_Math" display="inline" id="S3.SS1.p2.13.m6.1"><semantics id="S3.SS1.p2.13.m6.1a"><msub id="S3.SS1.p2.13.m6.1.1" xref="S3.SS1.p2.13.m6.1.1.cmml"><mi id="S3.SS1.p2.13.m6.1.1.2" xref="S3.SS1.p2.13.m6.1.1.2.cmml">L</mi><mi id="S3.SS1.p2.13.m6.1.1.3" xref="S3.SS1.p2.13.m6.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.13.m6.1b"><apply id="S3.SS1.p2.13.m6.1.1.cmml" xref="S3.SS1.p2.13.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.13.m6.1.1.1.cmml" xref="S3.SS1.p2.13.m6.1.1">subscript</csymbol><ci id="S3.SS1.p2.13.m6.1.1.2.cmml" xref="S3.SS1.p2.13.m6.1.1.2">𝐿</ci><ci id="S3.SS1.p2.13.m6.1.1.3.cmml" xref="S3.SS1.p2.13.m6.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.13.m6.1c">L_{c}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.13.m6.1d">italic_L start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math>, and silhouette loss, <math alttext="L_{s}" class="ltx_Math" display="inline" id="S3.SS1.p2.14.m7.1"><semantics id="S3.SS1.p2.14.m7.1a"><msub id="S3.SS1.p2.14.m7.1.1" xref="S3.SS1.p2.14.m7.1.1.cmml"><mi id="S3.SS1.p2.14.m7.1.1.2" xref="S3.SS1.p2.14.m7.1.1.2.cmml">L</mi><mi id="S3.SS1.p2.14.m7.1.1.3" xref="S3.SS1.p2.14.m7.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.14.m7.1b"><apply id="S3.SS1.p2.14.m7.1.1.cmml" xref="S3.SS1.p2.14.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.14.m7.1.1.1.cmml" xref="S3.SS1.p2.14.m7.1.1">subscript</csymbol><ci id="S3.SS1.p2.14.m7.1.1.2.cmml" xref="S3.SS1.p2.14.m7.1.1.2">𝐿</ci><ci id="S3.SS1.p2.14.m7.1.1.3.cmml" xref="S3.SS1.p2.14.m7.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.14.m7.1c">L_{s}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.14.m7.1d">italic_L start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math>, for <math alttext="L" class="ltx_Math" display="inline" id="S3.SS1.p2.15.m8.1"><semantics id="S3.SS1.p2.15.m8.1a"><mi id="S3.SS1.p2.15.m8.1.1" xref="S3.SS1.p2.15.m8.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.15.m8.1b"><ci id="S3.SS1.p2.15.m8.1.1.cmml" xref="S3.SS1.p2.15.m8.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.15.m8.1c">L</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.15.m8.1d">italic_L</annotation></semantics></math> pixels in an image are formulated as follows:</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.Ex2">
<tbody id="S3.Ex2X"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle L_{c}" class="ltx_Math" display="inline" id="S3.Ex2X.2.1.1.m1.1"><semantics id="S3.Ex2X.2.1.1.m1.1a"><msub id="S3.Ex2X.2.1.1.m1.1.1" xref="S3.Ex2X.2.1.1.m1.1.1.cmml"><mi id="S3.Ex2X.2.1.1.m1.1.1.2" xref="S3.Ex2X.2.1.1.m1.1.1.2.cmml">L</mi><mi id="S3.Ex2X.2.1.1.m1.1.1.3" xref="S3.Ex2X.2.1.1.m1.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S3.Ex2X.2.1.1.m1.1b"><apply id="S3.Ex2X.2.1.1.m1.1.1.cmml" xref="S3.Ex2X.2.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.Ex2X.2.1.1.m1.1.1.1.cmml" xref="S3.Ex2X.2.1.1.m1.1.1">subscript</csymbol><ci id="S3.Ex2X.2.1.1.m1.1.1.2.cmml" xref="S3.Ex2X.2.1.1.m1.1.1.2">𝐿</ci><ci id="S3.Ex2X.2.1.1.m1.1.1.3.cmml" xref="S3.Ex2X.2.1.1.m1.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2X.2.1.1.m1.1c">\displaystyle L_{c}</annotation><annotation encoding="application/x-llamapun" id="S3.Ex2X.2.1.1.m1.1d">italic_L start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\sum_{i=0}^{L}\|C_{i}-C_{i}^{\prime}\|_{1}" class="ltx_Math" display="inline" id="S3.Ex2X.3.2.2.m1.1"><semantics id="S3.Ex2X.3.2.2.m1.1a"><mrow id="S3.Ex2X.3.2.2.m1.1.1" xref="S3.Ex2X.3.2.2.m1.1.1.cmml"><mi id="S3.Ex2X.3.2.2.m1.1.1.3" xref="S3.Ex2X.3.2.2.m1.1.1.3.cmml"></mi><mo id="S3.Ex2X.3.2.2.m1.1.1.2" xref="S3.Ex2X.3.2.2.m1.1.1.2.cmml">=</mo><mrow id="S3.Ex2X.3.2.2.m1.1.1.1" xref="S3.Ex2X.3.2.2.m1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.Ex2X.3.2.2.m1.1.1.1.2" xref="S3.Ex2X.3.2.2.m1.1.1.1.2.cmml"><munderover id="S3.Ex2X.3.2.2.m1.1.1.1.2a" xref="S3.Ex2X.3.2.2.m1.1.1.1.2.cmml"><mo id="S3.Ex2X.3.2.2.m1.1.1.1.2.2.2" movablelimits="false" xref="S3.Ex2X.3.2.2.m1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.Ex2X.3.2.2.m1.1.1.1.2.2.3" xref="S3.Ex2X.3.2.2.m1.1.1.1.2.2.3.cmml"><mi id="S3.Ex2X.3.2.2.m1.1.1.1.2.2.3.2" xref="S3.Ex2X.3.2.2.m1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.Ex2X.3.2.2.m1.1.1.1.2.2.3.1" xref="S3.Ex2X.3.2.2.m1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.Ex2X.3.2.2.m1.1.1.1.2.2.3.3" xref="S3.Ex2X.3.2.2.m1.1.1.1.2.2.3.3.cmml">0</mn></mrow><mi id="S3.Ex2X.3.2.2.m1.1.1.1.2.3" xref="S3.Ex2X.3.2.2.m1.1.1.1.2.3.cmml">L</mi></munderover></mstyle><msub id="S3.Ex2X.3.2.2.m1.1.1.1.1" xref="S3.Ex2X.3.2.2.m1.1.1.1.1.cmml"><mrow id="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1" xref="S3.Ex2X.3.2.2.m1.1.1.1.1.1.2.cmml"><mo id="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.2" stretchy="false" xref="S3.Ex2X.3.2.2.m1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1" xref="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.cmml"><msub id="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.2" xref="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.2.2" xref="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.2.2.cmml">C</mi><mi id="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.2.3" xref="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.1" xref="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.1.cmml">−</mo><msubsup id="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.3" xref="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.3.2.2" xref="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.3.2.2.cmml">C</mi><mi id="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.3.2.3" xref="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.3.2.3.cmml">i</mi><mo id="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.3.3" xref="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.3.3.cmml">′</mo></msubsup></mrow><mo id="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.3" stretchy="false" xref="S3.Ex2X.3.2.2.m1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S3.Ex2X.3.2.2.m1.1.1.1.1.3" xref="S3.Ex2X.3.2.2.m1.1.1.1.1.3.cmml">1</mn></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex2X.3.2.2.m1.1b"><apply id="S3.Ex2X.3.2.2.m1.1.1.cmml" xref="S3.Ex2X.3.2.2.m1.1.1"><eq id="S3.Ex2X.3.2.2.m1.1.1.2.cmml" xref="S3.Ex2X.3.2.2.m1.1.1.2"></eq><csymbol cd="latexml" id="S3.Ex2X.3.2.2.m1.1.1.3.cmml" xref="S3.Ex2X.3.2.2.m1.1.1.3">absent</csymbol><apply id="S3.Ex2X.3.2.2.m1.1.1.1.cmml" xref="S3.Ex2X.3.2.2.m1.1.1.1"><apply id="S3.Ex2X.3.2.2.m1.1.1.1.2.cmml" xref="S3.Ex2X.3.2.2.m1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex2X.3.2.2.m1.1.1.1.2.1.cmml" xref="S3.Ex2X.3.2.2.m1.1.1.1.2">superscript</csymbol><apply id="S3.Ex2X.3.2.2.m1.1.1.1.2.2.cmml" xref="S3.Ex2X.3.2.2.m1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex2X.3.2.2.m1.1.1.1.2.2.1.cmml" xref="S3.Ex2X.3.2.2.m1.1.1.1.2">subscript</csymbol><sum id="S3.Ex2X.3.2.2.m1.1.1.1.2.2.2.cmml" xref="S3.Ex2X.3.2.2.m1.1.1.1.2.2.2"></sum><apply id="S3.Ex2X.3.2.2.m1.1.1.1.2.2.3.cmml" xref="S3.Ex2X.3.2.2.m1.1.1.1.2.2.3"><eq id="S3.Ex2X.3.2.2.m1.1.1.1.2.2.3.1.cmml" xref="S3.Ex2X.3.2.2.m1.1.1.1.2.2.3.1"></eq><ci id="S3.Ex2X.3.2.2.m1.1.1.1.2.2.3.2.cmml" xref="S3.Ex2X.3.2.2.m1.1.1.1.2.2.3.2">𝑖</ci><cn id="S3.Ex2X.3.2.2.m1.1.1.1.2.2.3.3.cmml" type="integer" xref="S3.Ex2X.3.2.2.m1.1.1.1.2.2.3.3">0</cn></apply></apply><ci id="S3.Ex2X.3.2.2.m1.1.1.1.2.3.cmml" xref="S3.Ex2X.3.2.2.m1.1.1.1.2.3">𝐿</ci></apply><apply id="S3.Ex2X.3.2.2.m1.1.1.1.1.cmml" xref="S3.Ex2X.3.2.2.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex2X.3.2.2.m1.1.1.1.1.2.cmml" xref="S3.Ex2X.3.2.2.m1.1.1.1.1">subscript</csymbol><apply id="S3.Ex2X.3.2.2.m1.1.1.1.1.1.2.cmml" xref="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.Ex2X.3.2.2.m1.1.1.1.1.1.2.1.cmml" xref="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.cmml" xref="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1"><minus id="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.1"></minus><apply id="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.2.2">𝐶</ci><ci id="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.3.2.2">𝐶</ci><ci id="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.3.2.3">𝑖</ci></apply><ci id="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.Ex2X.3.2.2.m1.1.1.1.1.1.1.1.3.3">′</ci></apply></apply></apply><cn id="S3.Ex2X.3.2.2.m1.1.1.1.1.3.cmml" type="integer" xref="S3.Ex2X.3.2.2.m1.1.1.1.1.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2X.3.2.2.m1.1c">\displaystyle=\sum_{i=0}^{L}\|C_{i}-C_{i}^{\prime}\|_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.Ex2X.3.2.2.m1.1d">= ∑ start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ∥ italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.Ex2Xa"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle L_{s}" class="ltx_Math" display="inline" id="S3.Ex2Xa.2.1.1.m1.1"><semantics id="S3.Ex2Xa.2.1.1.m1.1a"><msub id="S3.Ex2Xa.2.1.1.m1.1.1" xref="S3.Ex2Xa.2.1.1.m1.1.1.cmml"><mi id="S3.Ex2Xa.2.1.1.m1.1.1.2" xref="S3.Ex2Xa.2.1.1.m1.1.1.2.cmml">L</mi><mi id="S3.Ex2Xa.2.1.1.m1.1.1.3" xref="S3.Ex2Xa.2.1.1.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.Ex2Xa.2.1.1.m1.1b"><apply id="S3.Ex2Xa.2.1.1.m1.1.1.cmml" xref="S3.Ex2Xa.2.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.Ex2Xa.2.1.1.m1.1.1.1.cmml" xref="S3.Ex2Xa.2.1.1.m1.1.1">subscript</csymbol><ci id="S3.Ex2Xa.2.1.1.m1.1.1.2.cmml" xref="S3.Ex2Xa.2.1.1.m1.1.1.2">𝐿</ci><ci id="S3.Ex2Xa.2.1.1.m1.1.1.3.cmml" xref="S3.Ex2Xa.2.1.1.m1.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2Xa.2.1.1.m1.1c">\displaystyle L_{s}</annotation><annotation encoding="application/x-llamapun" id="S3.Ex2Xa.2.1.1.m1.1d">italic_L start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\sum_{i=0}^{L}\|S_{i}-S_{i}^{\prime}\|_{1}" class="ltx_Math" display="inline" id="S3.Ex2Xa.3.2.2.m1.1"><semantics id="S3.Ex2Xa.3.2.2.m1.1a"><mrow id="S3.Ex2Xa.3.2.2.m1.1.1" xref="S3.Ex2Xa.3.2.2.m1.1.1.cmml"><mi id="S3.Ex2Xa.3.2.2.m1.1.1.3" xref="S3.Ex2Xa.3.2.2.m1.1.1.3.cmml"></mi><mo id="S3.Ex2Xa.3.2.2.m1.1.1.2" xref="S3.Ex2Xa.3.2.2.m1.1.1.2.cmml">=</mo><mrow id="S3.Ex2Xa.3.2.2.m1.1.1.1" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.Ex2Xa.3.2.2.m1.1.1.1.2" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.2.cmml"><munderover id="S3.Ex2Xa.3.2.2.m1.1.1.1.2a" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.2.cmml"><mo id="S3.Ex2Xa.3.2.2.m1.1.1.1.2.2.2" movablelimits="false" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.Ex2Xa.3.2.2.m1.1.1.1.2.2.3" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.2.2.3.cmml"><mi id="S3.Ex2Xa.3.2.2.m1.1.1.1.2.2.3.2" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.Ex2Xa.3.2.2.m1.1.1.1.2.2.3.1" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.Ex2Xa.3.2.2.m1.1.1.1.2.2.3.3" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.2.2.3.3.cmml">0</mn></mrow><mi id="S3.Ex2Xa.3.2.2.m1.1.1.1.2.3" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.2.3.cmml">L</mi></munderover></mstyle><msub id="S3.Ex2Xa.3.2.2.m1.1.1.1.1" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.1.cmml"><mrow id="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.2.cmml"><mo id="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.2" stretchy="false" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.cmml"><msub id="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.2" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.2.2" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.2.2.cmml">S</mi><mi id="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.2.3" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.1" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.1.cmml">−</mo><msubsup id="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.3" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.3.2.2" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.3.2.2.cmml">S</mi><mi id="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.3.2.3" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.3.2.3.cmml">i</mi><mo id="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.3.3" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.3.3.cmml">′</mo></msubsup></mrow><mo id="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.3" stretchy="false" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S3.Ex2Xa.3.2.2.m1.1.1.1.1.3" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.1.3.cmml">1</mn></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex2Xa.3.2.2.m1.1b"><apply id="S3.Ex2Xa.3.2.2.m1.1.1.cmml" xref="S3.Ex2Xa.3.2.2.m1.1.1"><eq id="S3.Ex2Xa.3.2.2.m1.1.1.2.cmml" xref="S3.Ex2Xa.3.2.2.m1.1.1.2"></eq><csymbol cd="latexml" id="S3.Ex2Xa.3.2.2.m1.1.1.3.cmml" xref="S3.Ex2Xa.3.2.2.m1.1.1.3">absent</csymbol><apply id="S3.Ex2Xa.3.2.2.m1.1.1.1.cmml" xref="S3.Ex2Xa.3.2.2.m1.1.1.1"><apply id="S3.Ex2Xa.3.2.2.m1.1.1.1.2.cmml" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex2Xa.3.2.2.m1.1.1.1.2.1.cmml" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.2">superscript</csymbol><apply id="S3.Ex2Xa.3.2.2.m1.1.1.1.2.2.cmml" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex2Xa.3.2.2.m1.1.1.1.2.2.1.cmml" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.2">subscript</csymbol><sum id="S3.Ex2Xa.3.2.2.m1.1.1.1.2.2.2.cmml" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.2.2.2"></sum><apply id="S3.Ex2Xa.3.2.2.m1.1.1.1.2.2.3.cmml" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.2.2.3"><eq id="S3.Ex2Xa.3.2.2.m1.1.1.1.2.2.3.1.cmml" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.2.2.3.1"></eq><ci id="S3.Ex2Xa.3.2.2.m1.1.1.1.2.2.3.2.cmml" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.2.2.3.2">𝑖</ci><cn id="S3.Ex2Xa.3.2.2.m1.1.1.1.2.2.3.3.cmml" type="integer" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.2.2.3.3">0</cn></apply></apply><ci id="S3.Ex2Xa.3.2.2.m1.1.1.1.2.3.cmml" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.2.3">𝐿</ci></apply><apply id="S3.Ex2Xa.3.2.2.m1.1.1.1.1.cmml" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex2Xa.3.2.2.m1.1.1.1.1.2.cmml" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.1">subscript</csymbol><apply id="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.2.cmml" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.2.1.cmml" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.cmml" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1"><minus id="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.1"></minus><apply id="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.2.2">𝑆</ci><ci id="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.3.2.2">𝑆</ci><ci id="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.3.2.3">𝑖</ci></apply><ci id="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.1.1.1.1.3.3">′</ci></apply></apply></apply><cn id="S3.Ex2Xa.3.2.2.m1.1.1.1.1.3.cmml" type="integer" xref="S3.Ex2Xa.3.2.2.m1.1.1.1.1.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2Xa.3.2.2.m1.1c">\displaystyle=\sum_{i=0}^{L}\|S_{i}-S_{i}^{\prime}\|_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.Ex2Xa.3.2.2.m1.1d">= ∑ start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ∥ italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">We do not formulate any loss over features as we learn features in the next stage of training. While training NeRF, we optimize both Density MLP and Color MLP to learn geometry through color and silhouette loss. We train the current pipeline to learn robust object representation using the total loss, <math alttext="L_{1}" class="ltx_Math" display="inline" id="S3.SS1.p3.1.m1.1"><semantics id="S3.SS1.p3.1.m1.1a"><msub id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><mi id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml">L</mi><mn id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2">𝐿</ci><cn id="S3.SS1.p3.1.m1.1.1.3.cmml" type="integer" xref="S3.SS1.p3.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">L_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.1.m1.1d">italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="L_{1}=L_{c}+L_{s}" class="ltx_Math" display="block" id="S3.E3.m1.1"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><msub id="S3.E3.m1.1.1.2" xref="S3.E3.m1.1.1.2.cmml"><mi id="S3.E3.m1.1.1.2.2" xref="S3.E3.m1.1.1.2.2.cmml">L</mi><mn id="S3.E3.m1.1.1.2.3" xref="S3.E3.m1.1.1.2.3.cmml">1</mn></msub><mo id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.cmml">=</mo><mrow id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml"><msub id="S3.E3.m1.1.1.3.2" xref="S3.E3.m1.1.1.3.2.cmml"><mi id="S3.E3.m1.1.1.3.2.2" xref="S3.E3.m1.1.1.3.2.2.cmml">L</mi><mi id="S3.E3.m1.1.1.3.2.3" xref="S3.E3.m1.1.1.3.2.3.cmml">c</mi></msub><mo id="S3.E3.m1.1.1.3.1" xref="S3.E3.m1.1.1.3.1.cmml">+</mo><msub id="S3.E3.m1.1.1.3.3" xref="S3.E3.m1.1.1.3.3.cmml"><mi id="S3.E3.m1.1.1.3.3.2" xref="S3.E3.m1.1.1.3.3.2.cmml">L</mi><mi id="S3.E3.m1.1.1.3.3.3" xref="S3.E3.m1.1.1.3.3.3.cmml">s</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><eq id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"></eq><apply id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.2">subscript</csymbol><ci id="S3.E3.m1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.2.2">𝐿</ci><cn id="S3.E3.m1.1.1.2.3.cmml" type="integer" xref="S3.E3.m1.1.1.2.3">1</cn></apply><apply id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3"><plus id="S3.E3.m1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.3.1"></plus><apply id="S3.E3.m1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.2.1.cmml" xref="S3.E3.m1.1.1.3.2">subscript</csymbol><ci id="S3.E3.m1.1.1.3.2.2.cmml" xref="S3.E3.m1.1.1.3.2.2">𝐿</ci><ci id="S3.E3.m1.1.1.3.2.3.cmml" xref="S3.E3.m1.1.1.3.2.3">𝑐</ci></apply><apply id="S3.E3.m1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.3.1.cmml" xref="S3.E3.m1.1.1.3.3">subscript</csymbol><ci id="S3.E3.m1.1.1.3.3.2.cmml" xref="S3.E3.m1.1.1.3.3.2">𝐿</ci><ci id="S3.E3.m1.1.1.3.3.3.cmml" xref="S3.E3.m1.1.1.3.3.3">𝑠</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">L_{1}=L_{c}+L_{s}</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.1d">italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = italic_L start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT + italic_L start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<figure class="ltx_figure" id="S3.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F3.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="685" id="S3.F3.sf1.g1" src="extracted/5679340/images/rgb2.png" width="685"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S3.F3.sf1.3.2" style="font-size:90%;">Input Image</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F3.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="685" id="S3.F3.sf2.g1" src="extracted/5679340/images/m2.png" width="685"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S3.F3.sf2.3.2" style="font-size:90%;">Estimated Mask</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F3.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="685" id="S3.F3.sf3.g1" src="extracted/5679340/images/seg2.jpg" width="685"/>
<br class="ltx_break ltx_centering"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.sf3.2.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S3.F3.sf3.3.2" style="font-size:90%;">Segmented Image</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F3.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="685" id="S3.F3.sf4.g1" src="extracted/5679340/images/vs1.png" width="685"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.sf4.2.1.1" style="font-size:90%;">(d)</span> </span><span class="ltx_text" id="S3.F3.sf4.3.2" style="font-size:90%;">View 1</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F3.sf5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="656" id="S3.F3.sf5.g1" src="extracted/5679340/images/vs2.png" width="685"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.sf5.2.1.1" style="font-size:90%;">(e)</span> </span><span class="ltx_text" id="S3.F3.sf5.3.2" style="font-size:90%;">View 2</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F3.sf6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="649" id="S3.F3.sf6.g1" src="extracted/5679340/images/vs3.png" width="685"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.sf6.2.1.1" style="font-size:90%;">(f)</span> </span><span class="ltx_text" id="S3.F3.sf6.3.2" style="font-size:90%;"> View 3</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.3.2" style="font-size:90%;">Correspondence Visualization for continuous symmetric object in T-Less. The Input image, the estimated mask, and the segmented images are shown in the first three images. 2D-3D Correspondences are visualized
the next 3 images. Correspondences from 2D segmented region to the 3D point cloud are connected using lines. Blue point cloud is the full point cloud of the object. Red points are the matched correspondences to 2D points. The different views of the correspondence show that the correspondences are biased towards one symmetric configuration. Ideally, for a continuous symmetric object, the correspondences should have been distributed around the object. This bias towards one symmetric configuration helps us in performing inference faster as we can use naive PnP Ransac to estimate the final pose instead of the intensive render and compare inference employed in surfEmb to handle symmetric objects.
</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Stage 2 - Feature Learning</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The training in stage 1 provides an implicit object representation learned in NeRF network. In stage 2, we leverage this learned network and freeze layers which predict the density and color of the 3D point. We freeze the layers so that the geometry of the object is not optimized during feature learning. In this stage, we render features from NeRF for a specific viewpoint based on the relative transformation of the image. We freeze the MLPs trained in the previous stage and optimize the Feature MLP, CNN predicting feature images.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.3">We send an image corresponding to a viewpoint, <math alttext="k" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">italic_k</annotation></semantics></math>, as input to U-Net CNN, <math alttext="U" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m2.1"><semantics id="S3.SS2.p2.2.m2.1a"><mi id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">U</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><ci id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">𝑈</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">U</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m2.1d">italic_U</annotation></semantics></math>, to predict a feature image, <math alttext="F" class="ltx_Math" display="inline" id="S3.SS2.p2.3.m3.1"><semantics id="S3.SS2.p2.3.m3.1a"><mi id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><ci id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">F</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m3.1d">italic_F</annotation></semantics></math>, at the same resolution as the input image as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="F_{k}=U(C^{\prime}_{k})" class="ltx_Math" display="block" id="S3.E4.m1.1"><semantics id="S3.E4.m1.1a"><mrow id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml"><msub id="S3.E4.m1.1.1.3" xref="S3.E4.m1.1.1.3.cmml"><mi id="S3.E4.m1.1.1.3.2" xref="S3.E4.m1.1.1.3.2.cmml">F</mi><mi id="S3.E4.m1.1.1.3.3" xref="S3.E4.m1.1.1.3.3.cmml">k</mi></msub><mo id="S3.E4.m1.1.1.2" xref="S3.E4.m1.1.1.2.cmml">=</mo><mrow id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.3" xref="S3.E4.m1.1.1.1.3.cmml">U</mi><mo id="S3.E4.m1.1.1.1.2" xref="S3.E4.m1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E4.m1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.cmml"><mo id="S3.E4.m1.1.1.1.1.1.2" stretchy="false" xref="S3.E4.m1.1.1.1.1.1.1.cmml">(</mo><msubsup id="S3.E4.m1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.2.2" xref="S3.E4.m1.1.1.1.1.1.1.2.2.cmml">C</mi><mi id="S3.E4.m1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.3.cmml">k</mi><mo id="S3.E4.m1.1.1.1.1.1.1.2.3" xref="S3.E4.m1.1.1.1.1.1.1.2.3.cmml">′</mo></msubsup><mo id="S3.E4.m1.1.1.1.1.1.3" stretchy="false" xref="S3.E4.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1"><eq id="S3.E4.m1.1.1.2.cmml" xref="S3.E4.m1.1.1.2"></eq><apply id="S3.E4.m1.1.1.3.cmml" xref="S3.E4.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.3.2">𝐹</ci><ci id="S3.E4.m1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.3.3">𝑘</ci></apply><apply id="S3.E4.m1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"><times id="S3.E4.m1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.2"></times><ci id="S3.E4.m1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.3">𝑈</ci><apply id="S3.E4.m1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1">subscript</csymbol><apply id="S3.E4.m1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1">superscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.2">𝐶</ci><ci id="S3.E4.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.3">′</ci></apply><ci id="S3.E4.m1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.3">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">F_{k}=U(C^{\prime}_{k})</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.1d">italic_F start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_U ( italic_C start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.4">Similarly, we render feature image using NeRF from a viewpoint, <math alttext="k" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.1"><semantics id="S3.SS2.p3.1.m1.1a"><mi id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><ci id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.1d">italic_k</annotation></semantics></math>, using relative transformation, <math alttext="T_{k}" class="ltx_Math" display="inline" id="S3.SS2.p3.2.m2.1"><semantics id="S3.SS2.p3.2.m2.1a"><msub id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml"><mi id="S3.SS2.p3.2.m2.1.1.2" xref="S3.SS2.p3.2.m2.1.1.2.cmml">T</mi><mi id="S3.SS2.p3.2.m2.1.1.3" xref="S3.SS2.p3.2.m2.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2">𝑇</ci><ci id="S3.SS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">T_{k}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.2.m2.1d">italic_T start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>. For the sake of simplicity, we omit the integration part of the NeRF to get the final feature image from 3d points along the ray. We denote NeRF with integration as <math alttext="NI" class="ltx_Math" display="inline" id="S3.SS2.p3.3.m3.1"><semantics id="S3.SS2.p3.3.m3.1a"><mrow id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml"><mi id="S3.SS2.p3.3.m3.1.1.2" xref="S3.SS2.p3.3.m3.1.1.2.cmml">N</mi><mo id="S3.SS2.p3.3.m3.1.1.1" xref="S3.SS2.p3.3.m3.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p3.3.m3.1.1.3" xref="S3.SS2.p3.3.m3.1.1.3.cmml">I</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><apply id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1"><times id="S3.SS2.p3.3.m3.1.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1.1"></times><ci id="S3.SS2.p3.3.m3.1.1.2.cmml" xref="S3.SS2.p3.3.m3.1.1.2">𝑁</ci><ci id="S3.SS2.p3.3.m3.1.1.3.cmml" xref="S3.SS2.p3.3.m3.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">NI</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.3.m3.1d">italic_N italic_I</annotation></semantics></math> to generate the feature image, <math alttext="G_{k}" class="ltx_Math" display="inline" id="S3.SS2.p3.4.m4.1"><semantics id="S3.SS2.p3.4.m4.1a"><msub id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml"><mi id="S3.SS2.p3.4.m4.1.1.2" xref="S3.SS2.p3.4.m4.1.1.2.cmml">G</mi><mi id="S3.SS2.p3.4.m4.1.1.3" xref="S3.SS2.p3.4.m4.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b"><apply id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.4.m4.1.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p3.4.m4.1.1.2.cmml" xref="S3.SS2.p3.4.m4.1.1.2">𝐺</ci><ci id="S3.SS2.p3.4.m4.1.1.3.cmml" xref="S3.SS2.p3.4.m4.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">G_{k}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.4.m4.1d">italic_G start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> from a viewpoint as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="G_{k}=NI(T_{k})" class="ltx_Math" display="block" id="S3.E5.m1.1"><semantics id="S3.E5.m1.1a"><mrow id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml"><msub id="S3.E5.m1.1.1.3" xref="S3.E5.m1.1.1.3.cmml"><mi id="S3.E5.m1.1.1.3.2" xref="S3.E5.m1.1.1.3.2.cmml">G</mi><mi id="S3.E5.m1.1.1.3.3" xref="S3.E5.m1.1.1.3.3.cmml">k</mi></msub><mo id="S3.E5.m1.1.1.2" xref="S3.E5.m1.1.1.2.cmml">=</mo><mrow id="S3.E5.m1.1.1.1" xref="S3.E5.m1.1.1.1.cmml"><mi id="S3.E5.m1.1.1.1.3" xref="S3.E5.m1.1.1.1.3.cmml">N</mi><mo id="S3.E5.m1.1.1.1.2" xref="S3.E5.m1.1.1.1.2.cmml">⁢</mo><mi id="S3.E5.m1.1.1.1.4" xref="S3.E5.m1.1.1.1.4.cmml">I</mi><mo id="S3.E5.m1.1.1.1.2a" xref="S3.E5.m1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E5.m1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.cmml"><mo id="S3.E5.m1.1.1.1.1.1.2" stretchy="false" xref="S3.E5.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E5.m1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.cmml"><mi id="S3.E5.m1.1.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.1.1.2.cmml">T</mi><mi id="S3.E5.m1.1.1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.1.1.3.cmml">k</mi></msub><mo id="S3.E5.m1.1.1.1.1.1.3" stretchy="false" xref="S3.E5.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.1b"><apply id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1"><eq id="S3.E5.m1.1.1.2.cmml" xref="S3.E5.m1.1.1.2"></eq><apply id="S3.E5.m1.1.1.3.cmml" xref="S3.E5.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.3.1.cmml" xref="S3.E5.m1.1.1.3">subscript</csymbol><ci id="S3.E5.m1.1.1.3.2.cmml" xref="S3.E5.m1.1.1.3.2">𝐺</ci><ci id="S3.E5.m1.1.1.3.3.cmml" xref="S3.E5.m1.1.1.3.3">𝑘</ci></apply><apply id="S3.E5.m1.1.1.1.cmml" xref="S3.E5.m1.1.1.1"><times id="S3.E5.m1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.2"></times><ci id="S3.E5.m1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.3">𝑁</ci><ci id="S3.E5.m1.1.1.1.4.cmml" xref="S3.E5.m1.1.1.1.4">𝐼</ci><apply id="S3.E5.m1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.E5.m1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.2">𝑇</ci><ci id="S3.E5.m1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.3">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.1c">G_{k}=NI(T_{k})</annotation><annotation encoding="application/x-llamapun" id="S3.E5.m1.1d">italic_G start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_N italic_I ( italic_T start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.7">We formulate a contrastive loss for learning consistent and discriminative feature representation similar to SurfEmb. To formulate a contrastive loss, we need positive samples and negative samples. Positive samples are pixel-aligned feature pairs between two feature images from NeRF and CNN. To generate negative samples, we shoot rays from all the available viewpoints in training data covering the entire object and render features from different viewpoints. We sample <math alttext="J" class="ltx_Math" display="inline" id="S3.SS2.p4.1.m1.1"><semantics id="S3.SS2.p4.1.m1.1a"><mi id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml">J</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><ci id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">𝐽</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">J</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.1.m1.1d">italic_J</annotation></semantics></math> negative feature samples and denote them as <math alttext="Z" class="ltx_Math" display="inline" id="S3.SS2.p4.2.m2.1"><semantics id="S3.SS2.p4.2.m2.1a"><mi id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b"><ci id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1">𝑍</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">Z</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.2.m2.1d">italic_Z</annotation></semantics></math>. We formulate InfoNCE loss which forces positive pairs to be similar and distinctive from negative samples. This helps learn distinctive feature space on the object based on the geometry, and texture of the object. Since we also consider features from CNN, symmetry is automatically handled as viewpoints corresponding to symmetric configurations yield the same feature images. We denote the loss for a specific viewpoint by ignoring <math alttext="k" class="ltx_Math" display="inline" id="S3.SS2.p4.3.m3.1"><semantics id="S3.SS2.p4.3.m3.1a"><mi id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.1b"><ci id="S3.SS2.p4.3.m3.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.3.m3.1d">italic_k</annotation></semantics></math> in the notation while formulating the loss. However, the negative samples are independent of the viewpoint as they are sampled from different viewpoints randomly. The loss is formulated for <math alttext="L" class="ltx_Math" display="inline" id="S3.SS2.p4.4.m4.1"><semantics id="S3.SS2.p4.4.m4.1a"><mi id="S3.SS2.p4.4.m4.1.1" xref="S3.SS2.p4.4.m4.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.4.m4.1b"><ci id="S3.SS2.p4.4.m4.1.1.cmml" xref="S3.SS2.p4.4.m4.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.4.m4.1c">L</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.4.m4.1d">italic_L</annotation></semantics></math> pixels over CNN feature image, <math alttext="F" class="ltx_Math" display="inline" id="S3.SS2.p4.5.m5.1"><semantics id="S3.SS2.p4.5.m5.1a"><mi id="S3.SS2.p4.5.m5.1.1" xref="S3.SS2.p4.5.m5.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.5.m5.1b"><ci id="S3.SS2.p4.5.m5.1.1.cmml" xref="S3.SS2.p4.5.m5.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.5.m5.1c">F</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.5.m5.1d">italic_F</annotation></semantics></math>, NeRF Feature Image, <math alttext="G" class="ltx_Math" display="inline" id="S3.SS2.p4.6.m6.1"><semantics id="S3.SS2.p4.6.m6.1a"><mi id="S3.SS2.p4.6.m6.1.1" xref="S3.SS2.p4.6.m6.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.6.m6.1b"><ci id="S3.SS2.p4.6.m6.1.1.cmml" xref="S3.SS2.p4.6.m6.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.6.m6.1c">G</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.6.m6.1d">italic_G</annotation></semantics></math>, and negative samples, <math alttext="Z" class="ltx_Math" display="inline" id="S3.SS2.p4.7.m7.1"><semantics id="S3.SS2.p4.7.m7.1a"><mi id="S3.SS2.p4.7.m7.1.1" xref="S3.SS2.p4.7.m7.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.7.m7.1b"><ci id="S3.SS2.p4.7.m7.1.1.cmml" xref="S3.SS2.p4.7.m7.1.1">𝑍</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.7.m7.1c">Z</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.7.m7.1d">italic_Z</annotation></semantics></math>, as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="L_{c}=-\sum_{i=0}^{L}\log\frac{\exp(F^{i}G^{i})}{\sum_{j=1}^{J}\exp(F^{i}Z^{j})}" class="ltx_Math" display="block" id="S3.E6.m1.4"><semantics id="S3.E6.m1.4a"><mrow id="S3.E6.m1.4.5" xref="S3.E6.m1.4.5.cmml"><msub id="S3.E6.m1.4.5.2" xref="S3.E6.m1.4.5.2.cmml"><mi id="S3.E6.m1.4.5.2.2" xref="S3.E6.m1.4.5.2.2.cmml">L</mi><mi id="S3.E6.m1.4.5.2.3" xref="S3.E6.m1.4.5.2.3.cmml">c</mi></msub><mo id="S3.E6.m1.4.5.1" xref="S3.E6.m1.4.5.1.cmml">=</mo><mrow id="S3.E6.m1.4.5.3" xref="S3.E6.m1.4.5.3.cmml"><mo id="S3.E6.m1.4.5.3a" xref="S3.E6.m1.4.5.3.cmml">−</mo><mrow id="S3.E6.m1.4.5.3.2" xref="S3.E6.m1.4.5.3.2.cmml"><munderover id="S3.E6.m1.4.5.3.2.1" xref="S3.E6.m1.4.5.3.2.1.cmml"><mo id="S3.E6.m1.4.5.3.2.1.2.2" movablelimits="false" xref="S3.E6.m1.4.5.3.2.1.2.2.cmml">∑</mo><mrow id="S3.E6.m1.4.5.3.2.1.2.3" xref="S3.E6.m1.4.5.3.2.1.2.3.cmml"><mi id="S3.E6.m1.4.5.3.2.1.2.3.2" xref="S3.E6.m1.4.5.3.2.1.2.3.2.cmml">i</mi><mo id="S3.E6.m1.4.5.3.2.1.2.3.1" xref="S3.E6.m1.4.5.3.2.1.2.3.1.cmml">=</mo><mn id="S3.E6.m1.4.5.3.2.1.2.3.3" xref="S3.E6.m1.4.5.3.2.1.2.3.3.cmml">0</mn></mrow><mi id="S3.E6.m1.4.5.3.2.1.3" xref="S3.E6.m1.4.5.3.2.1.3.cmml">L</mi></munderover><mrow id="S3.E6.m1.4.5.3.2.2" xref="S3.E6.m1.4.5.3.2.2.cmml"><mi id="S3.E6.m1.4.5.3.2.2.1" xref="S3.E6.m1.4.5.3.2.2.1.cmml">log</mi><mo id="S3.E6.m1.4.5.3.2.2a" lspace="0.167em" xref="S3.E6.m1.4.5.3.2.2.cmml">⁡</mo><mfrac id="S3.E6.m1.4.4" xref="S3.E6.m1.4.4.cmml"><mrow id="S3.E6.m1.2.2.2.2" xref="S3.E6.m1.2.2.2.3.cmml"><mi id="S3.E6.m1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.cmml">exp</mi><mo id="S3.E6.m1.2.2.2.2a" xref="S3.E6.m1.2.2.2.3.cmml">⁡</mo><mrow id="S3.E6.m1.2.2.2.2.1" xref="S3.E6.m1.2.2.2.3.cmml"><mo id="S3.E6.m1.2.2.2.2.1.2" stretchy="false" xref="S3.E6.m1.2.2.2.3.cmml">(</mo><mrow id="S3.E6.m1.2.2.2.2.1.1" xref="S3.E6.m1.2.2.2.2.1.1.cmml"><msup id="S3.E6.m1.2.2.2.2.1.1.2" xref="S3.E6.m1.2.2.2.2.1.1.2.cmml"><mi id="S3.E6.m1.2.2.2.2.1.1.2.2" xref="S3.E6.m1.2.2.2.2.1.1.2.2.cmml">F</mi><mi id="S3.E6.m1.2.2.2.2.1.1.2.3" xref="S3.E6.m1.2.2.2.2.1.1.2.3.cmml">i</mi></msup><mo id="S3.E6.m1.2.2.2.2.1.1.1" xref="S3.E6.m1.2.2.2.2.1.1.1.cmml">⁢</mo><msup id="S3.E6.m1.2.2.2.2.1.1.3" xref="S3.E6.m1.2.2.2.2.1.1.3.cmml"><mi id="S3.E6.m1.2.2.2.2.1.1.3.2" xref="S3.E6.m1.2.2.2.2.1.1.3.2.cmml">G</mi><mi id="S3.E6.m1.2.2.2.2.1.1.3.3" xref="S3.E6.m1.2.2.2.2.1.1.3.3.cmml">i</mi></msup></mrow><mo id="S3.E6.m1.2.2.2.2.1.3" stretchy="false" xref="S3.E6.m1.2.2.2.3.cmml">)</mo></mrow></mrow><mrow id="S3.E6.m1.4.4.4" xref="S3.E6.m1.4.4.4.cmml"><msubsup id="S3.E6.m1.4.4.4.3" xref="S3.E6.m1.4.4.4.3.cmml"><mo id="S3.E6.m1.4.4.4.3.2.2" xref="S3.E6.m1.4.4.4.3.2.2.cmml">∑</mo><mrow id="S3.E6.m1.4.4.4.3.2.3" xref="S3.E6.m1.4.4.4.3.2.3.cmml"><mi id="S3.E6.m1.4.4.4.3.2.3.2" xref="S3.E6.m1.4.4.4.3.2.3.2.cmml">j</mi><mo id="S3.E6.m1.4.4.4.3.2.3.1" xref="S3.E6.m1.4.4.4.3.2.3.1.cmml">=</mo><mn id="S3.E6.m1.4.4.4.3.2.3.3" xref="S3.E6.m1.4.4.4.3.2.3.3.cmml">1</mn></mrow><mi id="S3.E6.m1.4.4.4.3.3" xref="S3.E6.m1.4.4.4.3.3.cmml">J</mi></msubsup><mrow id="S3.E6.m1.4.4.4.2.1" xref="S3.E6.m1.4.4.4.2.2.cmml"><mi id="S3.E6.m1.3.3.3.1" xref="S3.E6.m1.3.3.3.1.cmml">exp</mi><mo id="S3.E6.m1.4.4.4.2.1a" xref="S3.E6.m1.4.4.4.2.2.cmml">⁡</mo><mrow id="S3.E6.m1.4.4.4.2.1.1" xref="S3.E6.m1.4.4.4.2.2.cmml"><mo id="S3.E6.m1.4.4.4.2.1.1.2" stretchy="false" xref="S3.E6.m1.4.4.4.2.2.cmml">(</mo><mrow id="S3.E6.m1.4.4.4.2.1.1.1" xref="S3.E6.m1.4.4.4.2.1.1.1.cmml"><msup id="S3.E6.m1.4.4.4.2.1.1.1.2" xref="S3.E6.m1.4.4.4.2.1.1.1.2.cmml"><mi id="S3.E6.m1.4.4.4.2.1.1.1.2.2" xref="S3.E6.m1.4.4.4.2.1.1.1.2.2.cmml">F</mi><mi id="S3.E6.m1.4.4.4.2.1.1.1.2.3" xref="S3.E6.m1.4.4.4.2.1.1.1.2.3.cmml">i</mi></msup><mo id="S3.E6.m1.4.4.4.2.1.1.1.1" xref="S3.E6.m1.4.4.4.2.1.1.1.1.cmml">⁢</mo><msup id="S3.E6.m1.4.4.4.2.1.1.1.3" xref="S3.E6.m1.4.4.4.2.1.1.1.3.cmml"><mi id="S3.E6.m1.4.4.4.2.1.1.1.3.2" xref="S3.E6.m1.4.4.4.2.1.1.1.3.2.cmml">Z</mi><mi id="S3.E6.m1.4.4.4.2.1.1.1.3.3" xref="S3.E6.m1.4.4.4.2.1.1.1.3.3.cmml">j</mi></msup></mrow><mo id="S3.E6.m1.4.4.4.2.1.1.3" stretchy="false" xref="S3.E6.m1.4.4.4.2.2.cmml">)</mo></mrow></mrow></mrow></mfrac></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.4b"><apply id="S3.E6.m1.4.5.cmml" xref="S3.E6.m1.4.5"><eq id="S3.E6.m1.4.5.1.cmml" xref="S3.E6.m1.4.5.1"></eq><apply id="S3.E6.m1.4.5.2.cmml" xref="S3.E6.m1.4.5.2"><csymbol cd="ambiguous" id="S3.E6.m1.4.5.2.1.cmml" xref="S3.E6.m1.4.5.2">subscript</csymbol><ci id="S3.E6.m1.4.5.2.2.cmml" xref="S3.E6.m1.4.5.2.2">𝐿</ci><ci id="S3.E6.m1.4.5.2.3.cmml" xref="S3.E6.m1.4.5.2.3">𝑐</ci></apply><apply id="S3.E6.m1.4.5.3.cmml" xref="S3.E6.m1.4.5.3"><minus id="S3.E6.m1.4.5.3.1.cmml" xref="S3.E6.m1.4.5.3"></minus><apply id="S3.E6.m1.4.5.3.2.cmml" xref="S3.E6.m1.4.5.3.2"><apply id="S3.E6.m1.4.5.3.2.1.cmml" xref="S3.E6.m1.4.5.3.2.1"><csymbol cd="ambiguous" id="S3.E6.m1.4.5.3.2.1.1.cmml" xref="S3.E6.m1.4.5.3.2.1">superscript</csymbol><apply id="S3.E6.m1.4.5.3.2.1.2.cmml" xref="S3.E6.m1.4.5.3.2.1"><csymbol cd="ambiguous" id="S3.E6.m1.4.5.3.2.1.2.1.cmml" xref="S3.E6.m1.4.5.3.2.1">subscript</csymbol><sum id="S3.E6.m1.4.5.3.2.1.2.2.cmml" xref="S3.E6.m1.4.5.3.2.1.2.2"></sum><apply id="S3.E6.m1.4.5.3.2.1.2.3.cmml" xref="S3.E6.m1.4.5.3.2.1.2.3"><eq id="S3.E6.m1.4.5.3.2.1.2.3.1.cmml" xref="S3.E6.m1.4.5.3.2.1.2.3.1"></eq><ci id="S3.E6.m1.4.5.3.2.1.2.3.2.cmml" xref="S3.E6.m1.4.5.3.2.1.2.3.2">𝑖</ci><cn id="S3.E6.m1.4.5.3.2.1.2.3.3.cmml" type="integer" xref="S3.E6.m1.4.5.3.2.1.2.3.3">0</cn></apply></apply><ci id="S3.E6.m1.4.5.3.2.1.3.cmml" xref="S3.E6.m1.4.5.3.2.1.3">𝐿</ci></apply><apply id="S3.E6.m1.4.5.3.2.2.cmml" xref="S3.E6.m1.4.5.3.2.2"><log id="S3.E6.m1.4.5.3.2.2.1.cmml" xref="S3.E6.m1.4.5.3.2.2.1"></log><apply id="S3.E6.m1.4.4.cmml" xref="S3.E6.m1.4.4"><divide id="S3.E6.m1.4.4.5.cmml" xref="S3.E6.m1.4.4"></divide><apply id="S3.E6.m1.2.2.2.3.cmml" xref="S3.E6.m1.2.2.2.2"><exp id="S3.E6.m1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1"></exp><apply id="S3.E6.m1.2.2.2.2.1.1.cmml" xref="S3.E6.m1.2.2.2.2.1.1"><times id="S3.E6.m1.2.2.2.2.1.1.1.cmml" xref="S3.E6.m1.2.2.2.2.1.1.1"></times><apply id="S3.E6.m1.2.2.2.2.1.1.2.cmml" xref="S3.E6.m1.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.2.2.1.1.2.1.cmml" xref="S3.E6.m1.2.2.2.2.1.1.2">superscript</csymbol><ci id="S3.E6.m1.2.2.2.2.1.1.2.2.cmml" xref="S3.E6.m1.2.2.2.2.1.1.2.2">𝐹</ci><ci id="S3.E6.m1.2.2.2.2.1.1.2.3.cmml" xref="S3.E6.m1.2.2.2.2.1.1.2.3">𝑖</ci></apply><apply id="S3.E6.m1.2.2.2.2.1.1.3.cmml" xref="S3.E6.m1.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.2.2.1.1.3.1.cmml" xref="S3.E6.m1.2.2.2.2.1.1.3">superscript</csymbol><ci id="S3.E6.m1.2.2.2.2.1.1.3.2.cmml" xref="S3.E6.m1.2.2.2.2.1.1.3.2">𝐺</ci><ci id="S3.E6.m1.2.2.2.2.1.1.3.3.cmml" xref="S3.E6.m1.2.2.2.2.1.1.3.3">𝑖</ci></apply></apply></apply><apply id="S3.E6.m1.4.4.4.cmml" xref="S3.E6.m1.4.4.4"><apply id="S3.E6.m1.4.4.4.3.cmml" xref="S3.E6.m1.4.4.4.3"><csymbol cd="ambiguous" id="S3.E6.m1.4.4.4.3.1.cmml" xref="S3.E6.m1.4.4.4.3">superscript</csymbol><apply id="S3.E6.m1.4.4.4.3.2.cmml" xref="S3.E6.m1.4.4.4.3"><csymbol cd="ambiguous" id="S3.E6.m1.4.4.4.3.2.1.cmml" xref="S3.E6.m1.4.4.4.3">subscript</csymbol><sum id="S3.E6.m1.4.4.4.3.2.2.cmml" xref="S3.E6.m1.4.4.4.3.2.2"></sum><apply id="S3.E6.m1.4.4.4.3.2.3.cmml" xref="S3.E6.m1.4.4.4.3.2.3"><eq id="S3.E6.m1.4.4.4.3.2.3.1.cmml" xref="S3.E6.m1.4.4.4.3.2.3.1"></eq><ci id="S3.E6.m1.4.4.4.3.2.3.2.cmml" xref="S3.E6.m1.4.4.4.3.2.3.2">𝑗</ci><cn id="S3.E6.m1.4.4.4.3.2.3.3.cmml" type="integer" xref="S3.E6.m1.4.4.4.3.2.3.3">1</cn></apply></apply><ci id="S3.E6.m1.4.4.4.3.3.cmml" xref="S3.E6.m1.4.4.4.3.3">𝐽</ci></apply><apply id="S3.E6.m1.4.4.4.2.2.cmml" xref="S3.E6.m1.4.4.4.2.1"><exp id="S3.E6.m1.3.3.3.1.cmml" xref="S3.E6.m1.3.3.3.1"></exp><apply id="S3.E6.m1.4.4.4.2.1.1.1.cmml" xref="S3.E6.m1.4.4.4.2.1.1.1"><times id="S3.E6.m1.4.4.4.2.1.1.1.1.cmml" xref="S3.E6.m1.4.4.4.2.1.1.1.1"></times><apply id="S3.E6.m1.4.4.4.2.1.1.1.2.cmml" xref="S3.E6.m1.4.4.4.2.1.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.4.4.4.2.1.1.1.2.1.cmml" xref="S3.E6.m1.4.4.4.2.1.1.1.2">superscript</csymbol><ci id="S3.E6.m1.4.4.4.2.1.1.1.2.2.cmml" xref="S3.E6.m1.4.4.4.2.1.1.1.2.2">𝐹</ci><ci id="S3.E6.m1.4.4.4.2.1.1.1.2.3.cmml" xref="S3.E6.m1.4.4.4.2.1.1.1.2.3">𝑖</ci></apply><apply id="S3.E6.m1.4.4.4.2.1.1.1.3.cmml" xref="S3.E6.m1.4.4.4.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E6.m1.4.4.4.2.1.1.1.3.1.cmml" xref="S3.E6.m1.4.4.4.2.1.1.1.3">superscript</csymbol><ci id="S3.E6.m1.4.4.4.2.1.1.1.3.2.cmml" xref="S3.E6.m1.4.4.4.2.1.1.1.3.2">𝑍</ci><ci id="S3.E6.m1.4.4.4.2.1.1.1.3.3.cmml" xref="S3.E6.m1.4.4.4.2.1.1.1.3.3">𝑗</ci></apply></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.4c">L_{c}=-\sum_{i=0}^{L}\log\frac{\exp(F^{i}G^{i})}{\sum_{j=1}^{J}\exp(F^{i}Z^{j})}</annotation><annotation encoding="application/x-llamapun" id="S3.E6.m1.4d">italic_L start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT = - ∑ start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT roman_log divide start_ARG roman_exp ( italic_F start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT italic_G start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_J end_POSTSUPERSCRIPT roman_exp ( italic_F start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT italic_Z start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT ) end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.2">Stage 2 is trained with loss, <math alttext="L_{2}" class="ltx_Math" display="inline" id="S3.SS2.p5.1.m1.1"><semantics id="S3.SS2.p5.1.m1.1a"><msub id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml"><mi id="S3.SS2.p5.1.m1.1.1.2" xref="S3.SS2.p5.1.m1.1.1.2.cmml">L</mi><mn id="S3.SS2.p5.1.m1.1.1.3" xref="S3.SS2.p5.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.1b"><apply id="S3.SS2.p5.1.m1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p5.1.m1.1.1.2.cmml" xref="S3.SS2.p5.1.m1.1.1.2">𝐿</ci><cn id="S3.SS2.p5.1.m1.1.1.3.cmml" type="integer" xref="S3.SS2.p5.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.1c">L_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.1.m1.1d">italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> for <math alttext="k" class="ltx_Math" display="inline" id="S3.SS2.p5.2.m2.1"><semantics id="S3.SS2.p5.2.m2.1a"><mi id="S3.SS2.p5.2.m2.1.1" xref="S3.SS2.p5.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.m2.1b"><ci id="S3.SS2.p5.2.m2.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.2.m2.1d">italic_k</annotation></semantics></math> images as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="L_{2}=\sum_{k}L_{c}^{k}" class="ltx_Math" display="block" id="S3.E7.m1.1"><semantics id="S3.E7.m1.1a"><mrow id="S3.E7.m1.1.1" xref="S3.E7.m1.1.1.cmml"><msub id="S3.E7.m1.1.1.2" xref="S3.E7.m1.1.1.2.cmml"><mi id="S3.E7.m1.1.1.2.2" xref="S3.E7.m1.1.1.2.2.cmml">L</mi><mn id="S3.E7.m1.1.1.2.3" xref="S3.E7.m1.1.1.2.3.cmml">2</mn></msub><mo id="S3.E7.m1.1.1.1" rspace="0.111em" xref="S3.E7.m1.1.1.1.cmml">=</mo><mrow id="S3.E7.m1.1.1.3" xref="S3.E7.m1.1.1.3.cmml"><munder id="S3.E7.m1.1.1.3.1" xref="S3.E7.m1.1.1.3.1.cmml"><mo id="S3.E7.m1.1.1.3.1.2" movablelimits="false" xref="S3.E7.m1.1.1.3.1.2.cmml">∑</mo><mi id="S3.E7.m1.1.1.3.1.3" xref="S3.E7.m1.1.1.3.1.3.cmml">k</mi></munder><msubsup id="S3.E7.m1.1.1.3.2" xref="S3.E7.m1.1.1.3.2.cmml"><mi id="S3.E7.m1.1.1.3.2.2.2" xref="S3.E7.m1.1.1.3.2.2.2.cmml">L</mi><mi id="S3.E7.m1.1.1.3.2.2.3" xref="S3.E7.m1.1.1.3.2.2.3.cmml">c</mi><mi id="S3.E7.m1.1.1.3.2.3" xref="S3.E7.m1.1.1.3.2.3.cmml">k</mi></msubsup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m1.1b"><apply id="S3.E7.m1.1.1.cmml" xref="S3.E7.m1.1.1"><eq id="S3.E7.m1.1.1.1.cmml" xref="S3.E7.m1.1.1.1"></eq><apply id="S3.E7.m1.1.1.2.cmml" xref="S3.E7.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.2.1.cmml" xref="S3.E7.m1.1.1.2">subscript</csymbol><ci id="S3.E7.m1.1.1.2.2.cmml" xref="S3.E7.m1.1.1.2.2">𝐿</ci><cn id="S3.E7.m1.1.1.2.3.cmml" type="integer" xref="S3.E7.m1.1.1.2.3">2</cn></apply><apply id="S3.E7.m1.1.1.3.cmml" xref="S3.E7.m1.1.1.3"><apply id="S3.E7.m1.1.1.3.1.cmml" xref="S3.E7.m1.1.1.3.1"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.3.1.1.cmml" xref="S3.E7.m1.1.1.3.1">subscript</csymbol><sum id="S3.E7.m1.1.1.3.1.2.cmml" xref="S3.E7.m1.1.1.3.1.2"></sum><ci id="S3.E7.m1.1.1.3.1.3.cmml" xref="S3.E7.m1.1.1.3.1.3">𝑘</ci></apply><apply id="S3.E7.m1.1.1.3.2.cmml" xref="S3.E7.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.3.2.1.cmml" xref="S3.E7.m1.1.1.3.2">superscript</csymbol><apply id="S3.E7.m1.1.1.3.2.2.cmml" xref="S3.E7.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.3.2.2.1.cmml" xref="S3.E7.m1.1.1.3.2">subscript</csymbol><ci id="S3.E7.m1.1.1.3.2.2.2.cmml" xref="S3.E7.m1.1.1.3.2.2.2">𝐿</ci><ci id="S3.E7.m1.1.1.3.2.2.3.cmml" xref="S3.E7.m1.1.1.3.2.2.3">𝑐</ci></apply><ci id="S3.E7.m1.1.1.3.2.3.cmml" xref="S3.E7.m1.1.1.3.2.3">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m1.1c">L_{2}=\sum_{k}L_{c}^{k}</annotation><annotation encoding="application/x-llamapun" id="S3.E7.m1.1d">italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = ∑ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Inference</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">After training the pipeline, we established a relation between 2D images and the 3D implicit model through feature space. The learned feature representation is visualized in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S3.F2" title="Figure 2 ‣ 3.1 Stage 1 - NeRF pretraining ‣ 3 Method ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_tag">2</span></a>. We leverage this feature space to estimate 6D pose from an RGB image. We shoot rays from different cameras to generate features from different viewpoints and estimate the surface point where the ray hits the surface. We estimate the surface point as the first point along the ray where the density of the point from NeRF network is above a certain threshold. We estimate features at the surface points by passing the 3D points through Feature MLP. In essence, we extract the 3D point cloud with features, <math alttext="P" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">P</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">italic_P</annotation></semantics></math>, that can be matched with features from CNN feature image.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.4">For a given image crop, we send the image through CNN to predict the Feature image, <math alttext="F" class="ltx_Math" display="inline" id="S3.SS3.p2.1.m1.1"><semantics id="S3.SS3.p2.1.m1.1a"><mi id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><ci id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">F</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.1.m1.1d">italic_F</annotation></semantics></math>. A segmentation mask is applied to the feature image to only focus on the features inside the object region. We estimate the 3D correspondence at the <math alttext="i^{th}" class="ltx_Math" display="inline" id="S3.SS3.p2.2.m2.1"><semantics id="S3.SS3.p2.2.m2.1a"><msup id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml"><mi id="S3.SS3.p2.2.m2.1.1.2" xref="S3.SS3.p2.2.m2.1.1.2.cmml">i</mi><mrow id="S3.SS3.p2.2.m2.1.1.3" xref="S3.SS3.p2.2.m2.1.1.3.cmml"><mi id="S3.SS3.p2.2.m2.1.1.3.2" xref="S3.SS3.p2.2.m2.1.1.3.2.cmml">t</mi><mo id="S3.SS3.p2.2.m2.1.1.3.1" xref="S3.SS3.p2.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S3.SS3.p2.2.m2.1.1.3.3" xref="S3.SS3.p2.2.m2.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><apply id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">superscript</csymbol><ci id="S3.SS3.p2.2.m2.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2">𝑖</ci><apply id="S3.SS3.p2.2.m2.1.1.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3"><times id="S3.SS3.p2.2.m2.1.1.3.1.cmml" xref="S3.SS3.p2.2.m2.1.1.3.1"></times><ci id="S3.SS3.p2.2.m2.1.1.3.2.cmml" xref="S3.SS3.p2.2.m2.1.1.3.2">𝑡</ci><ci id="S3.SS3.p2.2.m2.1.1.3.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">i^{th}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.2.m2.1d">italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT</annotation></semantics></math> pixel in CNN Feature image, <math alttext="F" class="ltx_Math" display="inline" id="S3.SS3.p2.3.m3.1"><semantics id="S3.SS3.p2.3.m3.1a"><mi id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><ci id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">F</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.3.m3.1d">italic_F</annotation></semantics></math>, using point cloud features, <math alttext="P" class="ltx_Math" display="inline" id="S3.SS3.p2.4.m4.1"><semantics id="S3.SS3.p2.4.m4.1a"><mi id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><ci id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">P</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.4.m4.1d">italic_P</annotation></semantics></math>, as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\operatorname*{arg\,max}_{j}\frac{F^{i}P^{j}}{\sum_{j=1}^{J}F^{i}P^{j}}" class="ltx_Math" display="block" id="S3.E8.m1.1"><semantics id="S3.E8.m1.1a"><mrow id="S3.E8.m1.1.1" xref="S3.E8.m1.1.1.cmml"><munder id="S3.E8.m1.1.1.1" xref="S3.E8.m1.1.1.1.cmml"><mrow id="S3.E8.m1.1.1.1.2" xref="S3.E8.m1.1.1.1.2.cmml"><mi id="S3.E8.m1.1.1.1.2.2" xref="S3.E8.m1.1.1.1.2.2.cmml">arg</mi><mo id="S3.E8.m1.1.1.1.2.1" lspace="0.170em" xref="S3.E8.m1.1.1.1.2.1.cmml">⁢</mo><mi id="S3.E8.m1.1.1.1.2.3" xref="S3.E8.m1.1.1.1.2.3.cmml">max</mi></mrow><mi id="S3.E8.m1.1.1.1.3" xref="S3.E8.m1.1.1.1.3.cmml">j</mi></munder><mo id="S3.E8.m1.1.1a" lspace="0.167em" xref="S3.E8.m1.1.1.cmml">⁡</mo><mfrac id="S3.E8.m1.1.1.2" xref="S3.E8.m1.1.1.2.cmml"><mrow id="S3.E8.m1.1.1.2.2" xref="S3.E8.m1.1.1.2.2.cmml"><msup id="S3.E8.m1.1.1.2.2.2" xref="S3.E8.m1.1.1.2.2.2.cmml"><mi id="S3.E8.m1.1.1.2.2.2.2" xref="S3.E8.m1.1.1.2.2.2.2.cmml">F</mi><mi id="S3.E8.m1.1.1.2.2.2.3" xref="S3.E8.m1.1.1.2.2.2.3.cmml">i</mi></msup><mo id="S3.E8.m1.1.1.2.2.1" xref="S3.E8.m1.1.1.2.2.1.cmml">⁢</mo><msup id="S3.E8.m1.1.1.2.2.3" xref="S3.E8.m1.1.1.2.2.3.cmml"><mi id="S3.E8.m1.1.1.2.2.3.2" xref="S3.E8.m1.1.1.2.2.3.2.cmml">P</mi><mi id="S3.E8.m1.1.1.2.2.3.3" xref="S3.E8.m1.1.1.2.2.3.3.cmml">j</mi></msup></mrow><mrow id="S3.E8.m1.1.1.2.3" xref="S3.E8.m1.1.1.2.3.cmml"><msubsup id="S3.E8.m1.1.1.2.3.1" xref="S3.E8.m1.1.1.2.3.1.cmml"><mo id="S3.E8.m1.1.1.2.3.1.2.2" xref="S3.E8.m1.1.1.2.3.1.2.2.cmml">∑</mo><mrow id="S3.E8.m1.1.1.2.3.1.2.3" xref="S3.E8.m1.1.1.2.3.1.2.3.cmml"><mi id="S3.E8.m1.1.1.2.3.1.2.3.2" xref="S3.E8.m1.1.1.2.3.1.2.3.2.cmml">j</mi><mo id="S3.E8.m1.1.1.2.3.1.2.3.1" xref="S3.E8.m1.1.1.2.3.1.2.3.1.cmml">=</mo><mn id="S3.E8.m1.1.1.2.3.1.2.3.3" xref="S3.E8.m1.1.1.2.3.1.2.3.3.cmml">1</mn></mrow><mi id="S3.E8.m1.1.1.2.3.1.3" xref="S3.E8.m1.1.1.2.3.1.3.cmml">J</mi></msubsup><mrow id="S3.E8.m1.1.1.2.3.2" xref="S3.E8.m1.1.1.2.3.2.cmml"><msup id="S3.E8.m1.1.1.2.3.2.2" xref="S3.E8.m1.1.1.2.3.2.2.cmml"><mi id="S3.E8.m1.1.1.2.3.2.2.2" xref="S3.E8.m1.1.1.2.3.2.2.2.cmml">F</mi><mi id="S3.E8.m1.1.1.2.3.2.2.3" xref="S3.E8.m1.1.1.2.3.2.2.3.cmml">i</mi></msup><mo id="S3.E8.m1.1.1.2.3.2.1" xref="S3.E8.m1.1.1.2.3.2.1.cmml">⁢</mo><msup id="S3.E8.m1.1.1.2.3.2.3" xref="S3.E8.m1.1.1.2.3.2.3.cmml"><mi id="S3.E8.m1.1.1.2.3.2.3.2" xref="S3.E8.m1.1.1.2.3.2.3.2.cmml">P</mi><mi id="S3.E8.m1.1.1.2.3.2.3.3" xref="S3.E8.m1.1.1.2.3.2.3.3.cmml">j</mi></msup></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E8.m1.1b"><apply id="S3.E8.m1.1.1.cmml" xref="S3.E8.m1.1.1"><apply id="S3.E8.m1.1.1.1.cmml" xref="S3.E8.m1.1.1.1"><csymbol cd="ambiguous" id="S3.E8.m1.1.1.1.1.cmml" xref="S3.E8.m1.1.1.1">subscript</csymbol><apply id="S3.E8.m1.1.1.1.2.cmml" xref="S3.E8.m1.1.1.1.2"><times id="S3.E8.m1.1.1.1.2.1.cmml" xref="S3.E8.m1.1.1.1.2.1"></times><ci id="S3.E8.m1.1.1.1.2.2.cmml" xref="S3.E8.m1.1.1.1.2.2">arg</ci><ci id="S3.E8.m1.1.1.1.2.3.cmml" xref="S3.E8.m1.1.1.1.2.3">max</ci></apply><ci id="S3.E8.m1.1.1.1.3.cmml" xref="S3.E8.m1.1.1.1.3">𝑗</ci></apply><apply id="S3.E8.m1.1.1.2.cmml" xref="S3.E8.m1.1.1.2"><divide id="S3.E8.m1.1.1.2.1.cmml" xref="S3.E8.m1.1.1.2"></divide><apply id="S3.E8.m1.1.1.2.2.cmml" xref="S3.E8.m1.1.1.2.2"><times id="S3.E8.m1.1.1.2.2.1.cmml" xref="S3.E8.m1.1.1.2.2.1"></times><apply id="S3.E8.m1.1.1.2.2.2.cmml" xref="S3.E8.m1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E8.m1.1.1.2.2.2.1.cmml" xref="S3.E8.m1.1.1.2.2.2">superscript</csymbol><ci id="S3.E8.m1.1.1.2.2.2.2.cmml" xref="S3.E8.m1.1.1.2.2.2.2">𝐹</ci><ci id="S3.E8.m1.1.1.2.2.2.3.cmml" xref="S3.E8.m1.1.1.2.2.2.3">𝑖</ci></apply><apply id="S3.E8.m1.1.1.2.2.3.cmml" xref="S3.E8.m1.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.E8.m1.1.1.2.2.3.1.cmml" xref="S3.E8.m1.1.1.2.2.3">superscript</csymbol><ci id="S3.E8.m1.1.1.2.2.3.2.cmml" xref="S3.E8.m1.1.1.2.2.3.2">𝑃</ci><ci id="S3.E8.m1.1.1.2.2.3.3.cmml" xref="S3.E8.m1.1.1.2.2.3.3">𝑗</ci></apply></apply><apply id="S3.E8.m1.1.1.2.3.cmml" xref="S3.E8.m1.1.1.2.3"><apply id="S3.E8.m1.1.1.2.3.1.cmml" xref="S3.E8.m1.1.1.2.3.1"><csymbol cd="ambiguous" id="S3.E8.m1.1.1.2.3.1.1.cmml" xref="S3.E8.m1.1.1.2.3.1">superscript</csymbol><apply id="S3.E8.m1.1.1.2.3.1.2.cmml" xref="S3.E8.m1.1.1.2.3.1"><csymbol cd="ambiguous" id="S3.E8.m1.1.1.2.3.1.2.1.cmml" xref="S3.E8.m1.1.1.2.3.1">subscript</csymbol><sum id="S3.E8.m1.1.1.2.3.1.2.2.cmml" xref="S3.E8.m1.1.1.2.3.1.2.2"></sum><apply id="S3.E8.m1.1.1.2.3.1.2.3.cmml" xref="S3.E8.m1.1.1.2.3.1.2.3"><eq id="S3.E8.m1.1.1.2.3.1.2.3.1.cmml" xref="S3.E8.m1.1.1.2.3.1.2.3.1"></eq><ci id="S3.E8.m1.1.1.2.3.1.2.3.2.cmml" xref="S3.E8.m1.1.1.2.3.1.2.3.2">𝑗</ci><cn id="S3.E8.m1.1.1.2.3.1.2.3.3.cmml" type="integer" xref="S3.E8.m1.1.1.2.3.1.2.3.3">1</cn></apply></apply><ci id="S3.E8.m1.1.1.2.3.1.3.cmml" xref="S3.E8.m1.1.1.2.3.1.3">𝐽</ci></apply><apply id="S3.E8.m1.1.1.2.3.2.cmml" xref="S3.E8.m1.1.1.2.3.2"><times id="S3.E8.m1.1.1.2.3.2.1.cmml" xref="S3.E8.m1.1.1.2.3.2.1"></times><apply id="S3.E8.m1.1.1.2.3.2.2.cmml" xref="S3.E8.m1.1.1.2.3.2.2"><csymbol cd="ambiguous" id="S3.E8.m1.1.1.2.3.2.2.1.cmml" xref="S3.E8.m1.1.1.2.3.2.2">superscript</csymbol><ci id="S3.E8.m1.1.1.2.3.2.2.2.cmml" xref="S3.E8.m1.1.1.2.3.2.2.2">𝐹</ci><ci id="S3.E8.m1.1.1.2.3.2.2.3.cmml" xref="S3.E8.m1.1.1.2.3.2.2.3">𝑖</ci></apply><apply id="S3.E8.m1.1.1.2.3.2.3.cmml" xref="S3.E8.m1.1.1.2.3.2.3"><csymbol cd="ambiguous" id="S3.E8.m1.1.1.2.3.2.3.1.cmml" xref="S3.E8.m1.1.1.2.3.2.3">superscript</csymbol><ci id="S3.E8.m1.1.1.2.3.2.3.2.cmml" xref="S3.E8.m1.1.1.2.3.2.3.2">𝑃</ci><ci id="S3.E8.m1.1.1.2.3.2.3.3.cmml" xref="S3.E8.m1.1.1.2.3.2.3.3">𝑗</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E8.m1.1c">\operatorname*{arg\,max}_{j}\frac{F^{i}P^{j}}{\sum_{j=1}^{J}F^{i}P^{j}}</annotation><annotation encoding="application/x-llamapun" id="S3.E8.m1.1d">start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT divide start_ARG italic_F start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT italic_P start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_J end_POSTSUPERSCRIPT italic_F start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT italic_P start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.p2.5">The 3D point in the point cloud corresponding to the returned index from the above equation is treated as a correspondence for the 2D pixel. This is evaluated for each pixel in the segmented image to extract many 2D-3D correspondences. We use PnP-Ransac on the estimated 2D-3D matches to extract the final 6D pose. We also perform experiments in case the sensor depth image is available during inference. We render a depth map from NeRF using the pose estimated from the 2D feature image. We adjust the z-axis translation by computing the median difference between a given depth map and rendered depth map. This improves the pose only if the estimated rotation is very good.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We perform experiments on object pose datasets like LineMOD<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib17" title=""><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite>, LineMOD-Occlusion<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite> and T-Less<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite>. We employ widely used ADD, ADD-S metric to evaluate pose on LineMOD(LM) and LineMOD-Occlusion(LMO) datasets. We employ AR metric from BOP challenge<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite> to evaluate on TLess dataset. We train DpodV2<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib37" title=""><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite> segmentation network to extract segmentation masks for inference.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.2.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S4.T1.3.2" style="font-size:90%;">LineMOD Results: We present results using RGB, RGB-D data, and CAD model.NP refers to NeRF-Pose with naive PnP ransac, NP(Mask) refers to NeRF-Pose with evaluation scheme comparing masks during each ransac iteration</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T1.4">
<tr class="ltx_tr" id="S4.T1.4.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T1.4.1.1"><span class="ltx_text" id="S4.T1.4.1.1.1" style="font-size:70%;">Object</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T1.4.1.2">
<span class="ltx_text" id="S4.T1.4.1.2.1" style="font-size:70%;">DpodV2</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.4.1.2.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib37" title=""><span class="ltx_text" style="font-size:90%;">37</span></a><span class="ltx_text" id="S4.T1.4.1.2.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T1.4.1.3">
<span class="ltx_text" id="S4.T1.4.1.3.1" style="font-size:70%;">GDR-Net</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.4.1.3.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib48" title=""><span class="ltx_text" style="font-size:90%;">48</span></a><span class="ltx_text" id="S4.T1.4.1.3.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T1.4.1.4">
<span class="ltx_text" id="S4.T1.4.1.4.1" style="font-size:70%;">FFB-6D</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.4.1.4.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">16</span></a><span class="ltx_text" id="S4.T1.4.1.4.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T1.4.1.5">
<span class="ltx_text" id="S4.T1.4.1.5.1" style="font-size:70%;">LieNet</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.4.1.5.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">21</span></a><span class="ltx_text" id="S4.T1.4.1.5.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T1.4.1.6">
<span class="ltx_text" id="S4.T1.4.1.6.1" style="font-size:70%;">RLLG </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.4.1.6.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">6</span></a><span class="ltx_text" id="S4.T1.4.1.6.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T1.4.1.7">
<span class="ltx_text" id="S4.T1.4.1.7.1" style="font-size:70%;">NP</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.4.1.7.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a><span class="ltx_text" id="S4.T1.4.1.7.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T1.4.1.8"><span class="ltx_text" id="S4.T1.4.1.8.1" style="font-size:70%;">NP(Mask)</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T1.4.1.9"><span class="ltx_text" id="S4.T1.4.1.9.1" style="font-size:70%;">Ours</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T1.4.1.10"><span class="ltx_text" id="S4.T1.4.1.10.1" style="font-size:70%;">Ours</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.4.2.1"><span class="ltx_text" id="S4.T1.4.2.1.1" style="font-size:70%;">CAD</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.4.2.2"><span class="ltx_text" id="S4.T1.4.2.2.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.4.2.3"><span class="ltx_text" id="S4.T1.4.2.3.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.4.2.4"><span class="ltx_text" id="S4.T1.4.2.4.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.4.2.5"><span class="ltx_text" id="S4.T1.4.2.5.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.4.2.6"><span class="ltx_text" id="S4.T1.4.2.6.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.4.2.7"><span class="ltx_text" id="S4.T1.4.2.7.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.4.2.8"><span class="ltx_text" id="S4.T1.4.2.8.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.4.2.9"><span class="ltx_text" id="S4.T1.4.2.9.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.4.2.10"><span class="ltx_text" id="S4.T1.4.2.10.1" style="font-size:70%;">✗</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.4.3.1"><span class="ltx_text" id="S4.T1.4.3.1.1" style="font-size:70%;">Data</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.4.3.2"><span class="ltx_text" id="S4.T1.4.3.2.1" style="font-size:70%;">RGB</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.4.3.3"><span class="ltx_text" id="S4.T1.4.3.3.1" style="font-size:70%;">RGB</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.4.3.4"><span class="ltx_text" id="S4.T1.4.3.4.1" style="font-size:70%;">RGB-D</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.4.3.5"><span class="ltx_text" id="S4.T1.4.3.5.1" style="font-size:70%;">RGB</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.4.3.6"><span class="ltx_text" id="S4.T1.4.3.6.1" style="font-size:70%;">RGB</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.4.3.7"><span class="ltx_text" id="S4.T1.4.3.7.1" style="font-size:70%;">RGB</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.4.3.8"><span class="ltx_text" id="S4.T1.4.3.8.1" style="font-size:70%;">RGB</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.4.3.9"><span class="ltx_text" id="S4.T1.4.3.9.1" style="font-size:70%;">RGB</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.4.3.10"><span class="ltx_text" id="S4.T1.4.3.10.1" style="font-size:70%;">RGB-D</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.4">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T1.4.4.1"><span class="ltx_text" id="S4.T1.4.4.1.1" style="font-size:70%;">ADD</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T1.4.4.2"><span class="ltx_text" id="S4.T1.4.4.2.1" style="font-size:70%;">93.5</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T1.4.4.3"><span class="ltx_text" id="S4.T1.4.4.3.1" style="font-size:70%;">93.7</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T1.4.4.4"><span class="ltx_text" id="S4.T1.4.4.4.1" style="font-size:70%;">99.7</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T1.4.4.5"><span class="ltx_text" id="S4.T1.4.4.5.1" style="font-size:70%;">65.2</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T1.4.4.6"><span class="ltx_text" id="S4.T1.4.4.6.1" style="font-size:70%;">82.9</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T1.4.4.7"><span class="ltx_text" id="S4.T1.4.4.7.1" style="font-size:70%;">91.8</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T1.4.4.8"><span class="ltx_text" id="S4.T1.4.4.8.1" style="font-size:70%;">96.6</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T1.4.4.9"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.9.1" style="font-size:70%;">94.3</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T1.4.4.10"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.10.1" style="font-size:70%;">99.8</span></td>
</tr>
</table>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Training Data</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We train our pipeline with images and relative poses. We use Segment Anything <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite> to extract segmentation masks from the images using a bounding box as input to the network along with the image. We assume the object bounding box labels are available if the object is in a cluttered scene as they are much easier to label compared to the segmentation mask. We do not need bounding box labels for objects if the scene only contains the target object as in the T-Less dataset. For inference, we use the 6d pose label of the first frame in our training data to transfer estimated pose to the reference frame of the object defined in the dataset.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>NeRF-Pose Comparison</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.3">We compare our approach with NeRF-Pose to show the importance of bidirectional feature based learning instead of employing coordinate regression which is not suited for symmetric objects. NeRF-Pose reports NeRF-Pose(Weak) and NeRF-Pose(Pose). In NeRF-Pose(Weak), they refine the ground truth pose and claim that the accuracy improves the refined poses. Besides, they also propose a mask comparison based intensive ransac where they render and compare segmentation masks with every ransac iteration. They show that they achieve better accuracy using refined poses and intensive ransac. However, these additions are not part of the core approach or core network and can also be applied to our pipeline. To compare the core networks and have a fair evaluation, we compare our approach with NeRF-Pose(pose) since we also use ground truth poses without refinement and also compare with NeRF-Pose with naive PnP ransac which we also employ to estimate pose. We do this to have a fair common ground to evaluate the core network’s capability and to show the effectiveness of bidirectional feature learning compared to the correspondence regression based approach. We reimplemented NeRF-Pose on T-Less dataset and also on the LM dataset as they do not provide results with naive PnP ransac. Our approach performs better than NeRF-Pose in LM(<math alttext="2.1\%" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mn id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">2.1</mn><mo id="S4.SS2.p1.1.m1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1">percent</csymbol><cn id="S4.SS2.p1.1.m1.1.1.2.cmml" type="float" xref="S4.SS2.p1.1.m1.1.1.2">2.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">2.1\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">2.1 %</annotation></semantics></math>), LMO(<math alttext="4.2\%" class="ltx_Math" display="inline" id="S4.SS2.p1.2.m2.1"><semantics id="S4.SS2.p1.2.m2.1a"><mrow id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mn id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml">4.2</mn><mo id="S4.SS2.p1.2.m2.1.1.1" xref="S4.SS2.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><csymbol cd="latexml" id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1">percent</csymbol><cn id="S4.SS2.p1.2.m2.1.1.2.cmml" type="float" xref="S4.SS2.p1.2.m2.1.1.2">4.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">4.2\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.2.m2.1d">4.2 %</annotation></semantics></math>) and T-Less(<math alttext="4\%" class="ltx_Math" display="inline" id="S4.SS2.p1.3.m3.1"><semantics id="S4.SS2.p1.3.m3.1a"><mrow id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml"><mn id="S4.SS2.p1.3.m3.1.1.2" xref="S4.SS2.p1.3.m3.1.1.2.cmml">4</mn><mo id="S4.SS2.p1.3.m3.1.1.1" xref="S4.SS2.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><apply id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1"><csymbol cd="latexml" id="S4.SS2.p1.3.m3.1.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1.1">percent</csymbol><cn id="S4.SS2.p1.3.m3.1.1.2.cmml" type="integer" xref="S4.SS2.p1.3.m3.1.1.2">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">4\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.3.m3.1d">4 %</annotation></semantics></math>). In a similar setting using ground truth poses and naive PnP ransac, we achieve better accuracy compared to NeRF-Pose.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>LineMOD</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">LineMOD comprises 13 objects of different sizes and textures. We split the dataset similar to previous approaches using <math alttext="15\%" class="ltx_Math" display="inline" id="S4.SS3.p1.1.m1.1"><semantics id="S4.SS3.p1.1.m1.1a"><mrow id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mn id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml">15</mn><mo id="S4.SS3.p1.1.m1.1.1.1" xref="S4.SS3.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1.1">percent</csymbol><cn id="S4.SS3.p1.1.m1.1.1.2.cmml" type="integer" xref="S4.SS3.p1.1.m1.1.1.2">15</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">15\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.1.m1.1d">15 %</annotation></semantics></math> data for training and the rest for evaluation. We use ground-truth segmentation during training to have a fair comparison with the NeRF-Pose(NP) pipeline which uses ground-truth segmentation masks for training. However, we perform an ablation study in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S4.T4" title="Table 4 ‣ 4.6 Ablation Studies ‣ 4 Experiments ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_tag">4</span></a> with masks generated using Segment Anything to evaluate performance and observed that the accuracy drop is minimal even with generated masks. In Table <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S4.T1" title="Table 1 ‣ 4 Experiments ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_tag">1</span></a>, we observe that we achieve closer to benchmark accuracy despite using weaker labels and assuming no CAD model. We observe that our approach performs better than NeRF-pose when we employ naive PnP ransac to evaluate both approaches. However, NeRF-pose(mask) has slightly higher accuracy because of the intensive mask comparison at every ransac iteration for pose estimation which could reduce translation error. The translation error arises mostly along the z-axis component of translation. This is evident from our results using RGB-D data. We render depth using the estimated pose from NeRF network and adjust the translation along the z-axis using the depth map from the sensor by computing the median difference in depth on aligned pixels. This approach will only help if the estimated rotation is very accurate. The results with RGB-D data reflect that our rotation accuracy is much higher and adding depth to the pipeline can help resolve the translation error in some cases. We achieve benchmark accuracy using RGB-D data. Note that the translation adjustment can’t be treated as refinement as it is estimated as the median in one shot and not optimized over iterations like ICP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite>.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.2.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S4.T2.3.2" style="font-size:90%;">LineMOD Occlusion: CAD refers to the approaches using CAD model for training. NP refers to NeRF-Pose and NP(mask) refers to NeRF-Pose with evaluation scheme using mask</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T2.4">
<tr class="ltx_tr" id="S4.T2.4.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T2.4.1.1"><span class="ltx_text" id="S4.T2.4.1.1.1" style="font-size:70%;">Approach</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T2.4.1.2">
<span class="ltx_text" id="S4.T2.4.1.2.1" style="font-size:70%;">GDR-Net</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.4.1.2.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib48" title=""><span class="ltx_text" style="font-size:90%;">48</span></a><span class="ltx_text" id="S4.T2.4.1.2.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T2.4.1.3">
<span class="ltx_text" id="S4.T2.4.1.3.1" style="font-size:70%;">ZebraPose</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.4.1.3.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a><span class="ltx_text" id="S4.T2.4.1.3.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T2.4.1.4">
<span class="ltx_text" id="S4.T2.4.1.4.1" style="font-size:70%;">FFB-6D</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.4.1.4.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">16</span></a><span class="ltx_text" id="S4.T2.4.1.4.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T2.4.1.5">
<span class="ltx_text" id="S4.T2.4.1.5.1" style="font-size:70%;">RLLG</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.4.1.5.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">6</span></a><span class="ltx_text" id="S4.T2.4.1.5.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T2.4.1.6">
<span class="ltx_text" id="S4.T2.4.1.6.1" style="font-size:70%;">NP</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.4.1.6.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a><span class="ltx_text" id="S4.T2.4.1.6.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T2.4.1.7">
<span class="ltx_text" id="S4.T2.4.1.7.1" style="font-size:70%;">NP(Mask)</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.4.1.7.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a><span class="ltx_text" id="S4.T2.4.1.7.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T2.4.1.8"><span class="ltx_text" id="S4.T2.4.1.8.1" style="font-size:70%;">Ours</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T2.4.1.9"><span class="ltx_text" id="S4.T2.4.1.9.1" style="font-size:70%;">Ours</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.4.2.1"><span class="ltx_text" id="S4.T2.4.2.1.1" style="font-size:70%;">Modality</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.4.2.2"><span class="ltx_text" id="S4.T2.4.2.2.1" style="font-size:70%;">RGB</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.4.2.3"><span class="ltx_text" id="S4.T2.4.2.3.1" style="font-size:70%;">RGB</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.4.2.4"><span class="ltx_text" id="S4.T2.4.2.4.1" style="font-size:70%;">RGB-D</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.4.2.5"><span class="ltx_text" id="S4.T2.4.2.5.1" style="font-size:70%;">RGB</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.4.2.6"><span class="ltx_text" id="S4.T2.4.2.6.1" style="font-size:70%;">RGB</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.4.2.7"><span class="ltx_text" id="S4.T2.4.2.7.1" style="font-size:70%;">RGB</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.4.2.8"><span class="ltx_text" id="S4.T2.4.2.8.1" style="font-size:70%;">RGB</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.4.2.9"><span class="ltx_text" id="S4.T2.4.2.9.1" style="font-size:70%;">RGB-D</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.4.3.1"><span class="ltx_text" id="S4.T2.4.3.1.1" style="font-size:70%;">CAD</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.4.3.2"><span class="ltx_text" id="S4.T2.4.3.2.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.4.3.3"><span class="ltx_text" id="S4.T2.4.3.3.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.4.3.4"><span class="ltx_text" id="S4.T2.4.3.4.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.4.3.5"><span class="ltx_text" id="S4.T2.4.3.5.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.4.3.6"><span class="ltx_text" id="S4.T2.4.3.6.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.4.3.7"><span class="ltx_text" id="S4.T2.4.3.7.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.4.3.8"><span class="ltx_text" id="S4.T2.4.3.8.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.4.3.9"><span class="ltx_text" id="S4.T2.4.3.9.1" style="font-size:70%;">✗</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T2.4.4.1"><span class="ltx_text" id="S4.T2.4.4.1.1" style="font-size:70%;">ADD</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T2.4.4.2"><span class="ltx_text" id="S4.T2.4.4.2.1" style="font-size:70%;">62.2</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T2.4.4.3"><span class="ltx_text ltx_font_bold" id="S4.T2.4.4.3.1" style="font-size:70%;">76.9</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T2.4.4.4"><span class="ltx_text ltx_font_bold" id="S4.T2.4.4.4.1" style="font-size:70%;">66.2</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T2.4.4.5"><span class="ltx_text" id="S4.T2.4.4.5.1" style="font-size:70%;">30.3</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T2.4.4.6"><span class="ltx_text" id="S4.T2.4.4.6.1" style="font-size:70%;">44.1</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T2.4.4.7"><span class="ltx_text" id="S4.T2.4.4.7.1" style="font-size:70%;">49.2</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T2.4.4.8"><span class="ltx_text ltx_font_bold" id="S4.T2.4.4.8.1" style="font-size:70%;">48.3</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T2.4.4.9"><span class="ltx_text ltx_font_bold" id="S4.T2.4.4.9.1" style="font-size:70%;">66.9</span></td>
</tr>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>LineMOD-Occlusion</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">LineMOD-Occlusion has a subset of objects in LineMOD with images heavily occluded which tests the robustness of approaches with occlusions. In Table <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S4.T2" title="Table 2 ‣ 4.3 LineMOD ‣ 4 Experiments ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_tag">2</span></a>, we show that our accuracy is better than NeRF-Pose using PnP ransac without specialized PnP involving projecting mask at every ransac iteration. We also compare with NeRF-pose(pose) reported in their paper instead of NeRF-pose(weak) since we want to compare the core approaches with the same training setup. This shows that our approach performs better in terms of occlusions.
We also achieve higher accuracy after including a depth map in the inference pipeline indicating that the accuracy loss is happening because of translation error. ZebraPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite> achieves better accuracy even when compared with approaches using RGB-D data, but its parameterized splitting is not extendable for symmetric objects trivially. Our method slightly lags behind the counterparts using the CAD model which is expected because they have much more accurate synthetic training data generated with natural occlusions in synthetic scenes. We achieve similar performance to RGB-D based approach FFB-6D<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite> using depth.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T3.2.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S4.T3.3.2" style="font-size:90%;">T-Less Results: Evaluation on the T-Less dataset. NP refers to NeRF-Pose approach. We also present AR on continuous symmetric(CS) and Discrete Symmetric(DS) objects on both NeRF-Pose(NP) and our approach. CAD refers to the models using the CAD model for training. Ours-G refers to our model trained without NeRF density network and by querying densities from texture-less CAD model directly to train the pipeline. We report the results of SurfEmb without refinement and Test-Time augmentations for fair comparison.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T3.4">
<tr class="ltx_tr" id="S4.T3.4.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T3.4.1.1"><span class="ltx_text" id="S4.T3.4.1.1.1" style="font-size:70%;">Appr</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T3.4.1.2"><span class="ltx_text" id="S4.T3.4.1.2.1" style="font-size:70%;">DpodV2</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T3.4.1.3"><span class="ltx_text" id="S4.T3.4.1.3.1" style="font-size:70%;">DpodV2</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T3.4.1.4"><span class="ltx_text" id="S4.T3.4.1.4.1" style="font-size:70%;">SurfEmb</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T3.4.1.5"><span class="ltx_text" id="S4.T3.4.1.5.1" style="font-size:70%;">NP</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T3.4.1.6"><span class="ltx_text" id="S4.T3.4.1.6.1" style="font-size:70%;">NP(CS)</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T3.4.1.7"><span class="ltx_text" id="S4.T3.4.1.7.1" style="font-size:70%;">Ours(CS)</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T3.4.1.8"><span class="ltx_text" id="S4.T3.4.1.8.1" style="font-size:70%;">NP(DS)</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T3.4.1.9"><span class="ltx_text" id="S4.T3.4.1.9.1" style="font-size:70%;">Ours(DS)</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T3.4.1.10"><span class="ltx_text" id="S4.T3.4.1.10.1" style="font-size:70%;">Ours-G</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T3.4.1.11"><span class="ltx_text" id="S4.T3.4.1.11.1" style="font-size:70%;">Ours</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.4.2.1"><span class="ltx_text" id="S4.T3.4.2.1.1" style="font-size:70%;">CAD</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.4.2.2"><span class="ltx_text" id="S4.T3.4.2.2.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.4.2.3"><span class="ltx_text" id="S4.T3.4.2.3.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.4.2.4"><span class="ltx_text" id="S4.T3.4.2.4.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.4.2.5"><span class="ltx_text" id="S4.T3.4.2.5.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.4.2.6"><span class="ltx_text" id="S4.T3.4.2.6.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.4.2.7"><span class="ltx_text" id="S4.T3.4.2.7.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.4.2.8"><span class="ltx_text" id="S4.T3.4.2.8.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.4.2.9"><span class="ltx_text" id="S4.T3.4.2.9.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.4.2.10"><span class="ltx_text" id="S4.T3.4.2.10.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.4.2.11"><span class="ltx_text" id="S4.T3.4.2.11.1" style="font-size:70%;">✗</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.3">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T3.4.3.1"><span class="ltx_text" id="S4.T3.4.3.1.1" style="font-size:70%;">AR</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T3.4.3.2"><span class="ltx_text" id="S4.T3.4.3.2.1" style="font-size:70%;">0.65</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T3.4.3.3"><span class="ltx_text" id="S4.T3.4.3.3.1" style="font-size:70%;">0.51</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T3.4.3.4"><span class="ltx_text" id="S4.T3.4.3.4.1" style="font-size:70%;">0.62</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T3.4.3.5"><span class="ltx_text" id="S4.T3.4.3.5.1" style="font-size:70%;">0.54</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T3.4.3.6"><span class="ltx_text" id="S4.T3.4.3.6.1" style="font-size:70%;">0.48</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T3.4.3.7"><span class="ltx_text ltx_font_bold" id="S4.T3.4.3.7.1" style="font-size:70%;">0.52</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T3.4.3.8"><span class="ltx_text" id="S4.T3.4.3.8.1" style="font-size:70%;">0.58</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T3.4.3.9"><span class="ltx_text ltx_font_bold" id="S4.T3.4.3.9.1" style="font-size:70%;">0.63</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T3.4.3.10"><span class="ltx_text" id="S4.T3.4.3.10.1" style="font-size:70%;">0.58</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T3.4.3.11"><span class="ltx_text ltx_font_bold" id="S4.T3.4.3.11.1" style="font-size:70%;">0.58</span></td>
</tr>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>T-Less</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">T-Less has 30 objects comprising 11 continuous symmetric objects, 16 discrete symmetric objects and 3 asymmetric objects. Our approach handles symmetric objects better than NeRF-Pose with a performance gap of <math alttext="4\%" class="ltx_Math" display="inline" id="S4.SS5.p1.1.m1.1"><semantics id="S4.SS5.p1.1.m1.1a"><mrow id="S4.SS5.p1.1.m1.1.1" xref="S4.SS5.p1.1.m1.1.1.cmml"><mn id="S4.SS5.p1.1.m1.1.1.2" xref="S4.SS5.p1.1.m1.1.1.2.cmml">4</mn><mo id="S4.SS5.p1.1.m1.1.1.1" xref="S4.SS5.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.1.m1.1b"><apply id="S4.SS5.p1.1.m1.1.1.cmml" xref="S4.SS5.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS5.p1.1.m1.1.1.1.cmml" xref="S4.SS5.p1.1.m1.1.1.1">percent</csymbol><cn id="S4.SS5.p1.1.m1.1.1.2.cmml" type="integer" xref="S4.SS5.p1.1.m1.1.1.2">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.1.m1.1c">4\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p1.1.m1.1d">4 %</annotation></semantics></math> as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S10.T9" title="Table 9 ‣ 10.2 T-Less Results ‣ 10 Additional Results ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_tag">9</span></a>. We observe that our approach achieves closer to benchmark accuracy without using CAD models. We performed an ablation experiment(Ours-G) by assuming that a texture-less CAD model is available during training to check the robustness of NeRF representation and performance drop in the absence of the CAD model. We perform this experiment by replacing Density MLP and querying density directly from the CAD model. We observe that there is no drop in accuracy indicating that NeRF is a robust representation and was able to estimate good pose estimates even when some finer details are lost in NeRF compared to the CAD model. Our approach is better suited compared to NeRF-Pose and DpodV2 which are regression-based approaches and need special handling for symmetric objects to achieve better performance. Our approach achieves the best performance in the training setup using real images and pose labels compared to NeRF-Pose and DpodV2. We demonstrate the performance gains using our bidirectional feature learning compared to regression based NeRF-Pose on continuous symmetric and discrete symmetric objects in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S10.T9" title="Table 9 ‣ 10.2 T-Less Results ‣ 10 Additional Results ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F4.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="1541" id="S4.F4.sf1.g1" src="x5.png" width="1370"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S4.F4.sf1.3.2" style="font-size:90%;">Original mesh</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F4.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="699" id="S4.F4.sf2.g1" src="extracted/5679340/images/f123cp.png" width="685"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S4.F4.sf2.3.2" style="font-size:90%;">Ours</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F4.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="1533" id="S4.F4.sf3.g1" src="x6.png" width="1370"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.sf3.2.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S4.F4.sf3.3.2" style="font-size:90%;">SoftRas</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S4.F4.3.2" style="font-size:90%;">Visualization of mesh reconstructions of Can object in LM. The figure shows the original mesh, our reconstruction from NeRF using marching cubes, SoftRas refers to mesh optimized using SoftRas differentiable renderer. SoftRas mesh cannot reconstruct holes as it is optimized from genus zero sphere mesh.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Ablation Studies</h3>
<div class="ltx_para" id="S4.SS6.p1">
<p class="ltx_p" id="S4.SS6.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS6.p1.1.1">Segmentation Mask and Training Views</span>: We perform ablation studies on three objects from LM. To test the robustness of employing Segment Anything, We consider a non-convex object, Can, and objects with textured patches, Driller and Iron which are difficult to segment and we observed that sometimes the masks were missing some parts in Driller and Iron and sometimes the holes in the masks of Can object were closed. However, we observed that NeRF representation was able to learn a robust object representation despite some wrongly labeled masks. This is clearly reflected in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S4.T4" title="Table 4 ‣ 4.6 Ablation Studies ‣ 4 Experiments ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_tag">4</span></a> as there is no drop in accuracy when we use Segment Anything masks compared to ground truth masks. We perform an ablation on the number of training views. We observe that even with just 12 images, we achieve 88% accuracy which shows that the approach is robust even with a small number of views. We also perform an ablation on one T-Less object to understand the impact of the augmentation pipeline proposed to compensate for the lack of occluded training data. We introduce synthetic occlusions in training data by performing augmentations on object masks and pasting objects on random coco <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib27" title=""><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite> backgrounds. We observe that our augmentation pipeline boosts the accuracy by 0.32(32%) in the AR metric.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T4.2.1.1" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text" id="S4.T4.3.2" style="font-size:90%;">Ablations: We perform 4 experiments with different numbers of views for training and different segmentation masks. SAM refers to segmentation masks generated using Segment Anything and GT refers to using ground truth segmentation masks.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T4.4">
<tr class="ltx_tr" id="S4.T4.4.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T4.4.1.1"><span class="ltx_text" id="S4.T4.4.1.1.1" style="font-size:70%;">Experiment</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T4.4.1.2"><span class="ltx_text" id="S4.T4.4.1.2.1" style="font-size:70%;">Exp1</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T4.4.1.3"><span class="ltx_text" id="S4.T4.4.1.3.1" style="font-size:70%;">Exp2</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T4.4.1.4"><span class="ltx_text" id="S4.T4.4.1.4.1" style="font-size:70%;">Exp3</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T4.4.1.5"><span class="ltx_text" id="S4.T4.4.1.5.1" style="font-size:70%;">Exp4</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.4.2.1"><span class="ltx_text" id="S4.T4.4.2.1.1" style="font-size:70%;">Training Views</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T4.4.2.2"><span class="ltx_text" id="S4.T4.4.2.2.1" style="font-size:70%;">12</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T4.4.2.3"><span class="ltx_text" id="S4.T4.4.2.3.1" style="font-size:70%;">50</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T4.4.2.4"><span class="ltx_text" id="S4.T4.4.2.4.1" style="font-size:70%;">150</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T4.4.2.5"><span class="ltx_text" id="S4.T4.4.2.5.1" style="font-size:70%;">150</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.4.3.1"><span class="ltx_text" id="S4.T4.4.3.1.1" style="font-size:70%;">Segmentation</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T4.4.3.2"><span class="ltx_text" id="S4.T4.4.3.2.1" style="font-size:70%;">GT</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T4.4.3.3"><span class="ltx_text" id="S4.T4.4.3.3.1" style="font-size:70%;">GT</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T4.4.3.4"><span class="ltx_text" id="S4.T4.4.3.4.1" style="font-size:70%;">SAM</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T4.4.3.5"><span class="ltx_text" id="S4.T4.4.3.5.1" style="font-size:70%;">GT</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.4.4.1"><span class="ltx_text" id="S4.T4.4.4.1.1" style="font-size:70%;">Can</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T4.4.4.2"><span class="ltx_text" id="S4.T4.4.4.2.1" style="font-size:70%;">76.8</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T4.4.4.3"><span class="ltx_text" id="S4.T4.4.4.3.1" style="font-size:70%;">93.5</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T4.4.4.4"><span class="ltx_text" id="S4.T4.4.4.4.1" style="font-size:70%;">98.4</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T4.4.4.5"><span class="ltx_text" id="S4.T4.4.4.5.1" style="font-size:70%;">99.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.5">
<td class="ltx_td ltx_align_left" id="S4.T4.4.5.1"><span class="ltx_text" id="S4.T4.4.5.1.1" style="font-size:70%;">Driller</span></td>
<td class="ltx_td ltx_align_right" id="S4.T4.4.5.2"><span class="ltx_text" id="S4.T4.4.5.2.1" style="font-size:70%;">91.3</span></td>
<td class="ltx_td ltx_align_right" id="S4.T4.4.5.3"><span class="ltx_text" id="S4.T4.4.5.3.1" style="font-size:70%;">95.5</span></td>
<td class="ltx_td ltx_align_right" id="S4.T4.4.5.4"><span class="ltx_text" id="S4.T4.4.5.4.1" style="font-size:70%;">98.3</span></td>
<td class="ltx_td ltx_align_right" id="S4.T4.4.5.5"><span class="ltx_text" id="S4.T4.4.5.5.1" style="font-size:70%;">98.1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.6">
<td class="ltx_td ltx_align_left" id="S4.T4.4.6.1"><span class="ltx_text" id="S4.T4.4.6.1.1" style="font-size:70%;">Iron</span></td>
<td class="ltx_td ltx_align_right" id="S4.T4.4.6.2"><span class="ltx_text" id="S4.T4.4.6.2.1" style="font-size:70%;">96.5</span></td>
<td class="ltx_td ltx_align_right" id="S4.T4.4.6.3"><span class="ltx_text" id="S4.T4.4.6.3.1" style="font-size:70%;">94.2</span></td>
<td class="ltx_td ltx_align_right" id="S4.T4.4.6.4"><span class="ltx_text" id="S4.T4.4.6.4.1" style="font-size:70%;">99.5</span></td>
<td class="ltx_td ltx_align_right" id="S4.T4.4.6.5"><span class="ltx_text" id="S4.T4.4.6.5.1" style="font-size:70%;">99.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.7">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T4.4.7.1"><span class="ltx_text" id="S4.T4.4.7.1.1" style="font-size:70%;">ADD</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T4.4.7.2"><span class="ltx_text" id="S4.T4.4.7.2.1" style="font-size:70%;">88.2</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T4.4.7.3"><span class="ltx_text" id="S4.T4.4.7.3.1" style="font-size:70%;">94.4</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T4.4.7.4"><span class="ltx_text" id="S4.T4.4.7.4.1" style="font-size:70%;">98.7</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T4.4.7.5"><span class="ltx_text" id="S4.T4.4.7.5.1" style="font-size:70%;">98.7</span></td>
</tr>
</table>
</figure>
<div class="ltx_para" id="S4.SS6.p2">
<p class="ltx_p" id="S4.SS6.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS6.p2.1.1">Robustness of NeRF representation</span>: We perform an ablation of comparing NeRF with SoftRas <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> differentiable renderer to emphasize the importance of NeRF in our pipeline. We use a SoftRas-based differentiable renderer from PyTorch3D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite> to optimize a mesh based on the pose labels, color images, and segmentation masks. The 3D reconstructions of the mesh are shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S4.F4" title="Figure 4 ‣ 4.5 T-Less ‣ 4 Experiments ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_tag">4</span></a>. Since we do not know the genus of the object beforehand, a differentiable renderer initialized with a zero genus sphere can’t reconstruct objects with a genus greater than zero. After reconstructing the mesh, we train the second stage of our pipeline by sampling density from the reconstructed mesh directly instead of using density MLP from NeRF. We evaluate the pose estimation accuracy on LM and LMO in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S4.T5" title="Table 5 ‣ 4.6 Ablation Studies ‣ 4 Experiments ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_tag">5</span></a>. The performance is similar on LM, but we observe that the NeRF based approach is better on the LMO dataset compared to the differentiable renderer based approach. Also, the mesh reconstructions show that NeRF clearly learns better geometry when the genus of the object is unknown.</p>
</div>
<div class="ltx_para" id="S4.SS6.p3">
<p class="ltx_p" id="S4.SS6.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS6.p3.1.1">Upperbound in Learned Geometry</span>: To understand the performance loss in the absence of perfect geometry i.e., a perfect CAD model, we evaluate our approach assuming that a perfect geometry, texture-less CAD model, is available during training. Since we assume that we have a CAD model, we skip stage 1 in our training pipeline where geometry is learned. In the second stage of our approach, the density values are queried from the Density MLP learned in the first stage. In the presence of the CAD model, we skip the training of the first stage and train the second stage by querying density values directly from the CAD model. We refer to this experiment as Ours-G in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S10.T9" title="Table 9 ‣ 10.2 T-Less Results ‣ 10 Additional Results ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_tag">9</span></a>. We observe that there is no loss in performance between Ours and Ours-G. This shows that geometry from NeRF is already at the upper bound in terms of geometry extraction.</p>
</div>
<figure class="ltx_table" id="S4.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T5.2.1.1" style="font-size:90%;">Table 5</span>: </span><span class="ltx_text" id="S4.T5.3.2" style="font-size:90%;">Ablation studies on three objects on LM and LMO. We replace our NeRF with SoftRas based differentiable rendering. Stages refer to the number of training stages used during training.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T5.4">
<tr class="ltx_tr" id="S4.T5.4.1">
<td class="ltx_td ltx_border_tt" id="S4.T5.4.1.1"></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T5.4.1.2"><span class="ltx_text" id="S4.T5.4.1.2.1" style="font-size:70%;">SoftRas</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T5.4.1.3"><span class="ltx_text" id="S4.T5.4.1.3.1" style="font-size:70%;">Ours</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T5.4.1.4"><span class="ltx_text" id="S4.T5.4.1.4.1" style="font-size:70%;">Ours</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T5.4.1.5"><span class="ltx_text" id="S4.T5.4.1.5.1" style="font-size:70%;">SoftRas</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T5.4.1.6"><span class="ltx_text" id="S4.T5.4.1.6.1" style="font-size:70%;">Ours</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T5.4.1.7"><span class="ltx_text" id="S4.T5.4.1.7.1" style="font-size:70%;">Ours</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.4.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.4.2.1"><span class="ltx_text" id="S4.T5.4.2.1.1" style="font-size:70%;">Dataset</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T5.4.2.2"><span class="ltx_text" id="S4.T5.4.2.2.1" style="font-size:70%;">LM</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T5.4.2.3"><span class="ltx_text" id="S4.T5.4.2.3.1" style="font-size:70%;">LM</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T5.4.2.4"><span class="ltx_text" id="S4.T5.4.2.4.1" style="font-size:70%;">LM</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T5.4.2.5"><span class="ltx_text" id="S4.T5.4.2.5.1" style="font-size:70%;">LMO</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T5.4.2.6"><span class="ltx_text" id="S4.T5.4.2.6.1" style="font-size:70%;">LMO</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T5.4.2.7"><span class="ltx_text" id="S4.T5.4.2.7.1" style="font-size:70%;">LMO</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.4.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.4.3.1"><span class="ltx_text" id="S4.T5.4.3.1.1" style="font-size:70%;">Stages</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T5.4.3.2"><span class="ltx_text" id="S4.T5.4.3.2.1" style="font-size:70%;">2</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T5.4.3.3"><span class="ltx_text" id="S4.T5.4.3.3.1" style="font-size:70%;">2</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T5.4.3.4"><span class="ltx_text" id="S4.T5.4.3.4.1" style="font-size:70%;">1</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T5.4.3.5"><span class="ltx_text" id="S4.T5.4.3.5.1" style="font-size:70%;">2</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T5.4.3.6"><span class="ltx_text" id="S4.T5.4.3.6.1" style="font-size:70%;">2</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T5.4.3.7"><span class="ltx_text" id="S4.T5.4.3.7.1" style="font-size:70%;">1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.4.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.4.4.1"><span class="ltx_text" id="S4.T5.4.4.1.1" style="font-size:70%;">Can</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T5.4.4.2"><span class="ltx_text" id="S4.T5.4.4.2.1" style="font-size:70%;">99.3</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T5.4.4.3"><span class="ltx_text" id="S4.T5.4.4.3.1" style="font-size:70%;">99.3</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T5.4.4.4"><span class="ltx_text" id="S4.T5.4.4.4.1" style="font-size:70%;">66.8</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T5.4.4.5"><span class="ltx_text" id="S4.T5.4.4.5.1" style="font-size:70%;">82.2</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T5.4.4.6"><span class="ltx_text" id="S4.T5.4.4.6.1" style="font-size:70%;">85.6</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T5.4.4.7"><span class="ltx_text" id="S4.T5.4.4.7.1" style="font-size:70%;">50.1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.4.5">
<td class="ltx_td ltx_align_left" id="S4.T5.4.5.1"><span class="ltx_text" id="S4.T5.4.5.1.1" style="font-size:70%;">Driller</span></td>
<td class="ltx_td ltx_align_right" id="S4.T5.4.5.2"><span class="ltx_text" id="S4.T5.4.5.2.1" style="font-size:70%;">98.2</span></td>
<td class="ltx_td ltx_align_right" id="S4.T5.4.5.3"><span class="ltx_text" id="S4.T5.4.5.3.1" style="font-size:70%;">98.3</span></td>
<td class="ltx_td ltx_align_right" id="S4.T5.4.5.4"><span class="ltx_text" id="S4.T5.4.5.4.1" style="font-size:70%;">85.0</span></td>
<td class="ltx_td ltx_align_right" id="S4.T5.4.5.5"><span class="ltx_text" id="S4.T5.4.5.5.1" style="font-size:70%;">63.9</span></td>
<td class="ltx_td ltx_align_right" id="S4.T5.4.5.6"><span class="ltx_text" id="S4.T5.4.5.6.1" style="font-size:70%;">65.0</span></td>
<td class="ltx_td ltx_align_right" id="S4.T5.4.5.7"><span class="ltx_text" id="S4.T5.4.5.7.1" style="font-size:70%;">38.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.4.6">
<td class="ltx_td ltx_align_left" id="S4.T5.4.6.1"><span class="ltx_text" id="S4.T5.4.6.1.1" style="font-size:70%;">Iron box</span></td>
<td class="ltx_td ltx_align_right" id="S4.T5.4.6.2"><span class="ltx_text" id="S4.T5.4.6.2.1" style="font-size:70%;">99.8</span></td>
<td class="ltx_td ltx_align_right" id="S4.T5.4.6.3"><span class="ltx_text" id="S4.T5.4.6.3.1" style="font-size:70%;">99.8</span></td>
<td class="ltx_td ltx_align_right" id="S4.T5.4.6.4"><span class="ltx_text" id="S4.T5.4.6.4.1" style="font-size:70%;">94.6</span></td>
<td class="ltx_td ltx_align_right" id="S4.T5.4.6.5"><span class="ltx_text" id="S4.T5.4.6.5.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_right" id="S4.T5.4.6.6"><span class="ltx_text" id="S4.T5.4.6.6.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_right" id="S4.T5.4.6.7"><span class="ltx_text" id="S4.T5.4.6.7.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.4.7">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T5.4.7.1"><span class="ltx_text" id="S4.T5.4.7.1.1" style="font-size:70%;">ADD</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T5.4.7.2"><span class="ltx_text" id="S4.T5.4.7.2.1" style="font-size:70%;">99.1</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T5.4.7.3"><span class="ltx_text" id="S4.T5.4.7.3.1" style="font-size:70%;">99.1</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T5.4.7.4"><span class="ltx_text" id="S4.T5.4.7.4.1" style="font-size:70%;">82.1</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T5.4.7.5"><span class="ltx_text" id="S4.T5.4.7.5.1" style="font-size:70%;">73</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T5.4.7.6"><span class="ltx_text" id="S4.T5.4.7.6.1" style="font-size:70%;">75.3</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T5.4.7.7"><span class="ltx_text" id="S4.T5.4.7.7.1" style="font-size:70%;">44.2</span></td>
</tr>
</table>
</figure>
<div class="ltx_para" id="S4.SS6.p4">
<p class="ltx_p" id="S4.SS6.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS6.p4.1.1">Two Stage vs Single Stage Training</span>:
Our pipeline comprises two stages. In the first stage, we train the NeRF to learn geometry. In the second stage, we freeze the geometry of NeRF and learn the feature space between CNN and NeRF. We perform an ablation to justify our two-stage training. In Table <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S4.T5" title="Table 5 ‣ 4.6 Ablation Studies ‣ 4 Experiments ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_tag">5</span></a>, we see that the performance of a two-stage pipeline is better than a single-stage. This happens because feature learning gets intertwined with geometry learning leading to subpar performance if they are trained together.</p>
</div>
<div class="ltx_para" id="S4.SS6.p5">
<p class="ltx_p" id="S4.SS6.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS6.p5.1.1">PnP Ransac for Symmetric objects</span>
During inference, 2D-3D correspondences are estimated by finding the closest features between 3D point cloud features and 2D image features. We apply PnP ransac to estimate the final pose. Ideally, this should not work effectively for symmetric objects as the 2D correspondences could match to 3D correspondences in different symmetric configurations. However, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S3.F3" title="Figure 3 ‣ 3.1 Stage 1 - NeRF pretraining ‣ 3 Method ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_tag">3</span></a>, we observe that the correspondences are biased towards one symmetric configuration instead of distributing correspondences across multiple symmetric configurations. This is the reason for getting better performance despite using only PnP ransac which makes the approach much faster. We believe that this happens because of the way contrastive learning is formulated in our scenario. In our scenario, for symmetric objects, we sample negative samples uniformly from around the object which will definitely contain some positive samples from another symmetric configuration. Besides, a training batch could contain images from other symmetric configurations. The network chooses to prioritize one symmetric configuration over others based on the negative samples and the images in the batch. Because of this, the network might be preferring one symmetric configuration over others. So, the naive PnP ransac is enough to estimate the final 6d pose which makes the inference faster. This happens in the scenario where the batch of training images contains only one object. SurfEmb trains all the objects together and the batch contains different objects during each iteration which makes the SurfEmb learn features that spread correspondences similarly for all symmetric configurations. They had to employ a render and compare based intensive computational inference step to finalize the best hypothesis. Our inference takes 35ms which is much faster than SurfEmb which takes 1.2 secs for inference.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We propose a novel pipeline combining Nerf and CNN to perform object pose estimation using weakly labeled data without requiring a cad model. The proposed feature learning approach enables us to handle symmetric objects by enforcing 3D and symmetry constraints and also facilitates faster inference. We achieve benchmark accuracy among approaches using only real images and relative pose labels on LM, LMO, T-Less datasets. Rotation estimation is robust with the pipeline which is evident from much improved results on LM and LM-Occlusion datasets in RGB-D setting. We propose a pose estimation pipeline that enables real training data based approach easier by working with weaker labels compared to the complex setup required to annotate 6D pose datasets without requiring a CAD model.</p>
</div>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Acknowledgements</h4>
<div class="ltx_para" id="S5.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px1.p1.1">This work was partially funded by the German BMWK
under grant GEMIMEG-II-01MT20001A.

We present some additional ablations, qualitative and quantitative results in the following sections.
</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Noisy Initial Pose Refinement</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In our pipeline, we propose an approach assuming that relative pose labels and bounding box labels are available for training. However, in reality, the relative poses acquired are pretty noisy. To demonstrate the applicability of our approach in real life, we perform an ablation with noisy initial relative pose labels. We preprocess the relative pose labels and refine them using Bundle Adjusting Radiance Fields, BARF<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>. We employ BARF to refine the noisy initial pose labels. BARF optimizes initial poses and networks together while training a NeRF. We employ BARF to refine our noisy initial poses and use the refined poses to train our pipeline.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">We tweak the ground truth pose labels by adding random noise of 15 degrees on rotation matrices and 10 percent of the object’s diameter as noise on the translation component. We present results in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S6.T6" title="Table 6 ‣ 6 Noisy Initial Pose Refinement ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_tag">6</span></a> by running our pipeline with noisy poses and refined poses initialized with noisy poses. We observe that the accuracy with refined poses generated using BARF achieved 93.8% accuracy which shows that employing BARF can handle the errors in initial pose labels. Although there is a performance drop(4.9%) compared to ground truth poses, employing BARF improves the performance(23.3%) compared to training with noisy poses. This demonstrates that our approach can be easily deployed in real life even in the presence of noisy pose labels.</p>
</div>
<figure class="ltx_table" id="S6.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S6.T6.2.1.1" style="font-size:90%;">Table 6</span>: </span><span class="ltx_text" id="S6.T6.3.2" style="font-size:90%;">Ablations: We perform an ablation with noisy initial pose estimates. We randomly perturb rotations with upto 15-degree error and translation with 10 percent of the diameter of the object on 3 objects, Can, Driller, and Iron from LineMOD Dataset. Labels refer to the pose labels used for training our pipeline. Noisy refers to our approach trained using noisy initial poses. BARF refers to applying training BARF on noisy pose labels to obtain refined poses. GT refers to our approach using ground truth pose labels. </span></figcaption>
<table class="ltx_tabular ltx_align_middle" id="S6.T6.4">
<tr class="ltx_tr" id="S6.T6.4.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T6.4.1.1"><span class="ltx_text" id="S6.T6.4.1.1.1" style="font-size:70%;">Labels</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S6.T6.4.1.2"><span class="ltx_text" id="S6.T6.4.1.2.1" style="font-size:70%;">Noisy</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S6.T6.4.1.3"><span class="ltx_text" id="S6.T6.4.1.3.1" style="font-size:70%;">Noisy</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S6.T6.4.1.4"><span class="ltx_text" id="S6.T6.4.1.4.1" style="font-size:70%;">GT</span></td>
</tr>
<tr class="ltx_tr" id="S6.T6.4.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.4.2.1"><span class="ltx_text" id="S6.T6.4.2.1.1" style="font-size:70%;">Refinement</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T6.4.2.2"><span class="ltx_text" id="S6.T6.4.2.2.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T6.4.2.3"><span class="ltx_text" id="S6.T6.4.2.3.1" style="font-size:70%;">BARF</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T6.4.2.4"><span class="ltx_text" id="S6.T6.4.2.4.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S6.T6.4.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.4.3.1"><span class="ltx_text" id="S6.T6.4.3.1.1" style="font-size:70%;">Can</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T6.4.3.2"><span class="ltx_text" id="S6.T6.4.3.2.1" style="font-size:70%;">69.9</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T6.4.3.3"><span class="ltx_text" id="S6.T6.4.3.3.1" style="font-size:70%;">96.2</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T6.4.3.4"><span class="ltx_text" id="S6.T6.4.3.4.1" style="font-size:70%;">99.0</span></td>
</tr>
<tr class="ltx_tr" id="S6.T6.4.4">
<td class="ltx_td ltx_align_center" id="S6.T6.4.4.1"><span class="ltx_text" id="S6.T6.4.4.1.1" style="font-size:70%;">Driller</span></td>
<td class="ltx_td ltx_align_right" id="S6.T6.4.4.2"><span class="ltx_text" id="S6.T6.4.4.2.1" style="font-size:70%;">65.7</span></td>
<td class="ltx_td ltx_align_right" id="S6.T6.4.4.3"><span class="ltx_text" id="S6.T6.4.4.3.1" style="font-size:70%;">87.9</span></td>
<td class="ltx_td ltx_align_right" id="S6.T6.4.4.4"><span class="ltx_text" id="S6.T6.4.4.4.1" style="font-size:70%;">98.1</span></td>
</tr>
<tr class="ltx_tr" id="S6.T6.4.5">
<td class="ltx_td ltx_align_center" id="S6.T6.4.5.1"><span class="ltx_text" id="S6.T6.4.5.1.1" style="font-size:70%;">Iron</span></td>
<td class="ltx_td ltx_align_right" id="S6.T6.4.5.2"><span class="ltx_text" id="S6.T6.4.5.2.1" style="font-size:70%;">76.0</span></td>
<td class="ltx_td ltx_align_right" id="S6.T6.4.5.3"><span class="ltx_text" id="S6.T6.4.5.3.1" style="font-size:70%;">97.5</span></td>
<td class="ltx_td ltx_align_right" id="S6.T6.4.5.4"><span class="ltx_text" id="S6.T6.4.5.4.1" style="font-size:70%;">99.0</span></td>
</tr>
<tr class="ltx_tr" id="S6.T6.4.6">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S6.T6.4.6.1"><span class="ltx_text" id="S6.T6.4.6.1.1" style="font-size:70%;">Mean</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S6.T6.4.6.2"><span class="ltx_text" id="S6.T6.4.6.2.1" style="font-size:70%;">70.5</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S6.T6.4.6.3"><span class="ltx_text" id="S6.T6.4.6.3.1" style="font-size:70%;">93.8</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S6.T6.4.6.4"><span class="ltx_text" id="S6.T6.4.6.4.1" style="font-size:70%;">98.7</span></td>
</tr>
</table>
</figure>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Faster Inference For Symmetric Objects</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In the ablation section of the main paper, we show that the naive PnP Ransac is enough to estimate 6D pose even for symmetric objects. Although the features look similar visually as shown in <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S10.F9" title="Figure 9 ‣ 10.2 T-Less Results ‣ 10 Additional Results ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_tag">9</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S10.F10" title="Figure 10 ‣ 10.2 T-Less Results ‣ 10 Additional Results ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_tag">10</span></a>, there is still some discrepancy numerically between features which should have been the exact same value obeying their symmetries. This discrepancy leads to a bias towards one symmetric configuration when matched with a 2D image from CNN. SurfEmb trains the pipeline with all the objects comprising different objects in each batch with batch size 16. As they train with 30 objects, their batch contains different objects at each iteration. In our approach, a single batch contains 16 images of the same object. In Figures <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S7.F5" title="Figure 5 ‣ 7 Faster Inference For Symmetric Objects ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_tag">5</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S7.F6" title="Figure 6 ‣ 7 Faster Inference For Symmetric Objects ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_tag">6</span></a>, we visualize the correspondences and compare how our correspondences are biased towards one symmetric configuration compared to SurfEmb.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">In our approach, we train only one object at a time with a batch size of 16. We perform an ablation on how PnP Ransac is well suited even for symmetric objects in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S7.T7" title="Table 7 ‣ 7 Faster Inference For Symmetric Objects ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_tag">7</span></a>. We perform ablations on a continuous symmetric object, object 1, in T-Less. We observe that the network doesn’t perform well with batch size 1. So, a higher batch size is necessary for better performance. When the model is trained on one object, PnP Ransac which is faster also performs better(0.7%) than SurfEmb’s intensive evaluation pipeline. When the model is trained on 30 objects, PnP Ransac performs worse(6.1%) compared to SurfEmb’s evaluation. This shows that the correspondences are distributed around the object for symmetric objects when trained with 30 objects(a batch containing the utmost 1 image per object). By employing a higher batch size for a single object, we are able to achieve correspondences on the 3D model which are biased to one symmetric configuration for symmetric objects. This enables the handling of symmetric objects much easier compared to SurfEmb. For symmetric objects, SurfEmb needs an intense render and compare approach to find the best hypothesis as their correspondences are distributed around the object which makes it less suited to apply PnP Ransac. We can achieve comparable accuracy with faster inference time without any special handling for symmetric objects. Even though we managed to make the inference faster for symmetric objects, it comes at the cost of training a single model per object. So, our approach is still useful if real-time performance is desired at the cost of having a separate model per object. Also, we do not assume that we know about the object’s symmetry before training and inference. We can handle symmetric objects despite not knowing about them being symmetric or not.</p>
</div>
<figure class="ltx_table" id="S7.T7">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S7.T7.2.1.1" style="font-size:90%;">Table 7</span>: </span><span class="ltx_text" id="S7.T7.3.2" style="font-size:90%;">We perform an ablation with different batch sizes and different numbers of training objects per model. We evaluate the performance of the first object in T-Less which is Continuous Symmetric. Batch Size refers to the number of images used per batch during training. Training Objects refers to the number of objects trained in the model. PnP refers to naive PnP Ransac used to evaluate 6D pose from 2D-3D correspondences. SE refers to the evaluation pipeline proposed by SurfEmb which samples 10000 pose hypotheses and finds the best hypothesis by using intensive render and compare. PnP Ransac is much faster than SurfEmb’s evaluation. Ransac is clearly suitable even for symmetric objects when only one object is trained with a higher batch size. </span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S7.T7.4">
<tr class="ltx_tr" id="S7.T7.4.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S7.T7.4.1.1"><span class="ltx_text" id="S7.T7.4.1.1.1" style="font-size:70%;">Experiment</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S7.T7.4.1.2"><span class="ltx_text" id="S7.T7.4.1.2.1" style="font-size:70%;">Exp1</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S7.T7.4.1.3"><span class="ltx_text" id="S7.T7.4.1.3.1" style="font-size:70%;">Exp2</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S7.T7.4.1.4"><span class="ltx_text" id="S7.T7.4.1.4.1" style="font-size:70%;">Exp3</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S7.T7.4.1.5"><span class="ltx_text" id="S7.T7.4.1.5.1" style="font-size:70%;">Exp4</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S7.T7.4.1.6"><span class="ltx_text" id="S7.T7.4.1.6.1" style="font-size:70%;">Exp5</span></td>
</tr>
<tr class="ltx_tr" id="S7.T7.4.2">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S7.T7.4.2.1"><span class="ltx_text" id="S7.T7.4.2.1.1" style="font-size:70%;">Batch Size</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S7.T7.4.2.2"><span class="ltx_text" id="S7.T7.4.2.2.1" style="font-size:70%;">1</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S7.T7.4.2.3"><span class="ltx_text" id="S7.T7.4.2.3.1" style="font-size:70%;">16</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S7.T7.4.2.4"><span class="ltx_text" id="S7.T7.4.2.4.1" style="font-size:70%;">16</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S7.T7.4.2.5"><span class="ltx_text" id="S7.T7.4.2.5.1" style="font-size:70%;">16</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S7.T7.4.2.6"><span class="ltx_text" id="S7.T7.4.2.6.1" style="font-size:70%;">16</span></td>
</tr>
<tr class="ltx_tr" id="S7.T7.4.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S7.T7.4.3.1"><span class="ltx_text" id="S7.T7.4.3.1.1" style="font-size:70%;">Training Objects</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S7.T7.4.3.2"><span class="ltx_text" id="S7.T7.4.3.2.1" style="font-size:70%;">1</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S7.T7.4.3.3"><span class="ltx_text" id="S7.T7.4.3.3.1" style="font-size:70%;">1</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S7.T7.4.3.4"><span class="ltx_text" id="S7.T7.4.3.4.1" style="font-size:70%;">1</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S7.T7.4.3.5"><span class="ltx_text" id="S7.T7.4.3.5.1" style="font-size:70%;">30</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S7.T7.4.3.6"><span class="ltx_text" id="S7.T7.4.3.6.1" style="font-size:70%;">30</span></td>
</tr>
<tr class="ltx_tr" id="S7.T7.4.4">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S7.T7.4.4.1"><span class="ltx_text" id="S7.T7.4.4.1.1" style="font-size:70%;">Evaluation</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S7.T7.4.4.2"><span class="ltx_text" id="S7.T7.4.4.2.1" style="font-size:70%;">PnP</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S7.T7.4.4.3"><span class="ltx_text" id="S7.T7.4.4.3.1" style="font-size:70%;">PnP</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S7.T7.4.4.4"><span class="ltx_text" id="S7.T7.4.4.4.1" style="font-size:70%;">SE</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S7.T7.4.4.5"><span class="ltx_text" id="S7.T7.4.4.5.1" style="font-size:70%;">PnP</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S7.T7.4.4.6"><span class="ltx_text" id="S7.T7.4.4.6.1" style="font-size:70%;">SE</span></td>
</tr>
<tr class="ltx_tr" id="S7.T7.4.5">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="S7.T7.4.5.1"><span class="ltx_text" id="S7.T7.4.5.1.1" style="font-size:70%;">Mean</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S7.T7.4.5.2"><span class="ltx_text" id="S7.T7.4.5.2.1" style="font-size:70%;">3.0</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S7.T7.4.5.3"><span class="ltx_text" id="S7.T7.4.5.3.1" style="font-size:70%;">47.8</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S7.T7.4.5.4"><span class="ltx_text" id="S7.T7.4.5.4.1" style="font-size:70%;">47.1</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S7.T7.4.5.5"><span class="ltx_text" id="S7.T7.4.5.5.1" style="font-size:70%;">34.9</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S7.T7.4.5.6"><span class="ltx_text" id="S7.T7.4.5.6.1" style="font-size:70%;">41.0</span></td>
</tr>
</table>
</figure>
<figure class="ltx_figure" id="S7.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S7.F5.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="617" id="S7.F5.sf1.g1" src="extracted/5679340/images/28semb1.png" width="617"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S7.F5.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S7.F5.sf1.3.2" style="font-size:90%;">SurfEmb:View 1</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S7.F5.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="618" id="S7.F5.sf2.g1" src="extracted/5679340/images/28semb2.png" width="617"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S7.F5.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S7.F5.sf2.3.2" style="font-size:90%;">SurfEmb:View 2</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S7.F5.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="615" id="S7.F5.sf3.g1" src="extracted/5679340/images/28semb3.png" width="617"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S7.F5.sf3.2.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S7.F5.sf3.3.2" style="font-size:90%;">SurfEmb:View 2</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S7.F5.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="617" id="S7.F5.sf4.g1" src="extracted/5679340/images/128nemb1.png" width="617"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S7.F5.sf4.2.1.1" style="font-size:90%;">(d)</span> </span><span class="ltx_text" id="S7.F5.sf4.3.2" style="font-size:90%;">Ours:View 1</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S7.F5.sf5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="617" id="S7.F5.sf5.g1" src="extracted/5679340/images/128nemb2.png" width="617"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S7.F5.sf5.2.1.1" style="font-size:90%;">(e)</span> </span><span class="ltx_text" id="S7.F5.sf5.3.2" style="font-size:90%;">Ours:View 2</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S7.F5.sf6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="615" id="S7.F5.sf6.g1" src="extracted/5679340/images/128nemb3.png" width="617"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S7.F5.sf6.2.1.1" style="font-size:90%;">(f)</span> </span><span class="ltx_text" id="S7.F5.sf6.3.2" style="font-size:90%;">Ours:View 3</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S7.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S7.F5.3.2" style="font-size:90%;">Visualization of correspondences for object 28 in T-Less: Discrete symmetric object. The visualization shows the 2D-3D correspondences between 2D masked pixels and the 3D point cloud of the object. We join lines between matched 2D-3D correspondences. The segmented 2D image points are indicated with their RGB color from the image. The blue pointcloud indicates the full point cloud of the object. The red point cloud indicates the correspondence to the current image(indicated with masked pixels). We visualize the correspondences in different views to show where the 3D correspondences on the object are matched. The first row indicates the correspondence visualization for SurfEmb. The second row indicates the correspondences visualization of our approach. In SurfEmb, the correspondences are distributed around the object for the symmetric object. In our approach, the correspondences are biased towards only one symmetric configuration</span></figcaption>
</figure>
<figure class="ltx_figure" id="S7.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S7.F6.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="617" id="S7.F6.sf1.g1" src="extracted/5679340/images/semb1.png" width="617"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S7.F6.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S7.F6.sf1.3.2" style="font-size:90%;">SurfEmb:View 1</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S7.F6.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="591" id="S7.F6.sf2.g1" src="extracted/5679340/images/semb2.png" width="617"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S7.F6.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S7.F6.sf2.3.2" style="font-size:90%;">SurfEmb:View 2</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S7.F6.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="617" id="S7.F6.sf3.g1" src="extracted/5679340/images/semb3.png" width="617"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S7.F6.sf3.2.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S7.F6.sf3.3.2" style="font-size:90%;">SurfEmb:View 2</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S7.F6.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="617" id="S7.F6.sf4.g1" src="extracted/5679340/images/nemb1.png" width="617"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S7.F6.sf4.2.1.1" style="font-size:90%;">(d)</span> </span><span class="ltx_text" id="S7.F6.sf4.3.2" style="font-size:90%;">Ours:View 1</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S7.F6.sf5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="617" id="S7.F6.sf5.g1" src="extracted/5679340/images/nemb2.png" width="617"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S7.F6.sf5.2.1.1" style="font-size:90%;">(e)</span> </span><span class="ltx_text" id="S7.F6.sf5.3.2" style="font-size:90%;">Ours:View 2</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S7.F6.sf6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="617" id="S7.F6.sf6.g1" src="extracted/5679340/images/nemb3.png" width="617"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S7.F6.sf6.2.1.1" style="font-size:90%;">(f)</span> </span><span class="ltx_text" id="S7.F6.sf6.3.2" style="font-size:90%;">Ours:View 3</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S7.F6.2.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S7.F6.3.2" style="font-size:90%;">Visualization of correspondences for object 1 in T-Less: Continuous symmetric object. The visualization shows the 2D-3D correspondences between 2D masked pixels and the 3D point cloud of the object. We join lines between matched 2D-3D correspondences. The masked 2D image points are indicated by their color from the image. The blue point cloud indicates the full point cloud of the object. The red point cloud indicates the correspondences to the current image(indicated with masked pixels). We visualize the correspondences in different views to show where the 3D correspondences on the object are matched. The first row indicates the correspondence visualization for SurfEmb. The second row indicates the corresponding visualization of our approach. In SurfEmb, the correspondences are distributed around the object for the symmetric object. In our approach, the correspondences are biased towards only one symmetric configuration</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Training Details</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">We present some additional training details and network architecture details in this section. We employ a U-Net<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib35" title=""><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite> based CNN and an adapted version of NeRF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite> to train our pipeline. Our Nerf consists of 3 MLPs, Density MLP, Color MLP, and Feature MLP. The architecture is depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S8.F7" title="Figure 7 ‣ CNN ‣ 8 Training Details ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<section class="ltx_paragraph" id="S8.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Density MLP</h4>
<div class="ltx_para" id="S8.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S8.SS0.SSS0.Px1.p1.1">Density MLP takes in a single 3D coordinate and predicts density value. We use positional encoding for encoding 3D points with 60 harmonic functions as specified in Nerf<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite>. We employ an MLP with three fully connected layers with Softplus activation. A 3D point is transformed into 360-dimensional positional encoding using 60 harmonic functions. The intermediate layers in MLP have 256-dimensional output and the final layer outputs a 1-dimensional density value.</p>
</div>
</section>
<section class="ltx_paragraph" id="S8.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Color MLP</h4>
<div class="ltx_para" id="S8.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S8.SS0.SSS0.Px2.p1.1">Color MLP takes in the ray direction and intermediate feature vector from the second layer of Density MLP as input to predict 3-dimensional color value. The three-dimensional ray direction vector is passed through the same position encoding as Density MLP to encode the 3D vector to create 360-dimensional output. The position-encoded ray direction vector and intermediate feature vector from Density MLP are concatenated together to create input for Color MLP. Color MLP is a two-layered MLP that employs the Softplus activation function after linear layers. The intermediate layer has 256-dimensional output.</p>
</div>
</section>
<section class="ltx_paragraph" id="S8.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Feature MLP</h4>
<div class="ltx_para" id="S8.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S8.SS0.SSS0.Px3.p1.1">Feature MLP takes in a 3D coordinate and predicts a 12-dimensional feature vector using a 2-layer MLP. We employ Siren MLP similar to SurfEmb<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite>. As position encoding is already present in the intermediate layers in Siren<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">39</span></a>]</cite> MLP implicitly, we do not use positional encoding to transform 3D coordinates. The intermediate layer has 256-dimensional output.</p>
</div>
</section>
<section class="ltx_paragraph" id="S8.SS0.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">CNN</h4>
<div class="ltx_para" id="S8.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="S8.SS0.SSS0.Px4.p1.2">We employ a U-Net<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib35" title=""><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite> based CNN to predict feature images. The CNN takes in an image with the resolution, <math alttext="224\times 224\times 3" class="ltx_Math" display="inline" id="S8.SS0.SSS0.Px4.p1.1.m1.1"><semantics id="S8.SS0.SSS0.Px4.p1.1.m1.1a"><mrow id="S8.SS0.SSS0.Px4.p1.1.m1.1.1" xref="S8.SS0.SSS0.Px4.p1.1.m1.1.1.cmml"><mn id="S8.SS0.SSS0.Px4.p1.1.m1.1.1.2" xref="S8.SS0.SSS0.Px4.p1.1.m1.1.1.2.cmml">224</mn><mo id="S8.SS0.SSS0.Px4.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S8.SS0.SSS0.Px4.p1.1.m1.1.1.1.cmml">×</mo><mn id="S8.SS0.SSS0.Px4.p1.1.m1.1.1.3" xref="S8.SS0.SSS0.Px4.p1.1.m1.1.1.3.cmml">224</mn><mo id="S8.SS0.SSS0.Px4.p1.1.m1.1.1.1a" lspace="0.222em" rspace="0.222em" xref="S8.SS0.SSS0.Px4.p1.1.m1.1.1.1.cmml">×</mo><mn id="S8.SS0.SSS0.Px4.p1.1.m1.1.1.4" xref="S8.SS0.SSS0.Px4.p1.1.m1.1.1.4.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S8.SS0.SSS0.Px4.p1.1.m1.1b"><apply id="S8.SS0.SSS0.Px4.p1.1.m1.1.1.cmml" xref="S8.SS0.SSS0.Px4.p1.1.m1.1.1"><times id="S8.SS0.SSS0.Px4.p1.1.m1.1.1.1.cmml" xref="S8.SS0.SSS0.Px4.p1.1.m1.1.1.1"></times><cn id="S8.SS0.SSS0.Px4.p1.1.m1.1.1.2.cmml" type="integer" xref="S8.SS0.SSS0.Px4.p1.1.m1.1.1.2">224</cn><cn id="S8.SS0.SSS0.Px4.p1.1.m1.1.1.3.cmml" type="integer" xref="S8.SS0.SSS0.Px4.p1.1.m1.1.1.3">224</cn><cn id="S8.SS0.SSS0.Px4.p1.1.m1.1.1.4.cmml" type="integer" xref="S8.SS0.SSS0.Px4.p1.1.m1.1.1.4">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.SS0.SSS0.Px4.p1.1.m1.1c">224\times 224\times 3</annotation><annotation encoding="application/x-llamapun" id="S8.SS0.SSS0.Px4.p1.1.m1.1d">224 × 224 × 3</annotation></semantics></math> to predict the feature image of the resolution, <math alttext="224\times 224\times 12" class="ltx_Math" display="inline" id="S8.SS0.SSS0.Px4.p1.2.m2.1"><semantics id="S8.SS0.SSS0.Px4.p1.2.m2.1a"><mrow id="S8.SS0.SSS0.Px4.p1.2.m2.1.1" xref="S8.SS0.SSS0.Px4.p1.2.m2.1.1.cmml"><mn id="S8.SS0.SSS0.Px4.p1.2.m2.1.1.2" xref="S8.SS0.SSS0.Px4.p1.2.m2.1.1.2.cmml">224</mn><mo id="S8.SS0.SSS0.Px4.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S8.SS0.SSS0.Px4.p1.2.m2.1.1.1.cmml">×</mo><mn id="S8.SS0.SSS0.Px4.p1.2.m2.1.1.3" xref="S8.SS0.SSS0.Px4.p1.2.m2.1.1.3.cmml">224</mn><mo id="S8.SS0.SSS0.Px4.p1.2.m2.1.1.1a" lspace="0.222em" rspace="0.222em" xref="S8.SS0.SSS0.Px4.p1.2.m2.1.1.1.cmml">×</mo><mn id="S8.SS0.SSS0.Px4.p1.2.m2.1.1.4" xref="S8.SS0.SSS0.Px4.p1.2.m2.1.1.4.cmml">12</mn></mrow><annotation-xml encoding="MathML-Content" id="S8.SS0.SSS0.Px4.p1.2.m2.1b"><apply id="S8.SS0.SSS0.Px4.p1.2.m2.1.1.cmml" xref="S8.SS0.SSS0.Px4.p1.2.m2.1.1"><times id="S8.SS0.SSS0.Px4.p1.2.m2.1.1.1.cmml" xref="S8.SS0.SSS0.Px4.p1.2.m2.1.1.1"></times><cn id="S8.SS0.SSS0.Px4.p1.2.m2.1.1.2.cmml" type="integer" xref="S8.SS0.SSS0.Px4.p1.2.m2.1.1.2">224</cn><cn id="S8.SS0.SSS0.Px4.p1.2.m2.1.1.3.cmml" type="integer" xref="S8.SS0.SSS0.Px4.p1.2.m2.1.1.3">224</cn><cn id="S8.SS0.SSS0.Px4.p1.2.m2.1.1.4.cmml" type="integer" xref="S8.SS0.SSS0.Px4.p1.2.m2.1.1.4">12</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.SS0.SSS0.Px4.p1.2.m2.1c">224\times 224\times 12</annotation><annotation encoding="application/x-llamapun" id="S8.SS0.SSS0.Px4.p1.2.m2.1d">224 × 224 × 12</annotation></semantics></math>. We predict 12-dimensional feature vectors for each pixel.</p>
</div>
<figure class="ltx_figure" id="S8.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="460" id="S8.F7.g1" src="x7.png" width="327"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S8.F7.15.7.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text ltx_font_bold" id="S8.F7.12.6" style="font-size:90%;">NeRF Architecture<span class="ltx_text ltx_font_medium" id="S8.F7.12.6.6">: Our NeRF comprises three MLPs, Density MLP, Color MLP and Feature MLP. Density MLP takes in a discrete 3D point, <math alttext="x" class="ltx_Math" display="inline" id="S8.F7.7.1.1.m1.1"><semantics id="S8.F7.7.1.1.m1.1b"><mi id="S8.F7.7.1.1.m1.1.1" xref="S8.F7.7.1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S8.F7.7.1.1.m1.1c"><ci id="S8.F7.7.1.1.m1.1.1.cmml" xref="S8.F7.7.1.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.F7.7.1.1.m1.1d">x</annotation><annotation encoding="application/x-llamapun" id="S8.F7.7.1.1.m1.1e">italic_x</annotation></semantics></math>, and passes through the Position Encoding (PE) layer and three Fully Connected (FC) layers to predict density, <math alttext="d" class="ltx_Math" display="inline" id="S8.F7.8.2.2.m2.1"><semantics id="S8.F7.8.2.2.m2.1b"><mi id="S8.F7.8.2.2.m2.1.1" xref="S8.F7.8.2.2.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S8.F7.8.2.2.m2.1c"><ci id="S8.F7.8.2.2.m2.1.1.cmml" xref="S8.F7.8.2.2.m2.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.F7.8.2.2.m2.1d">d</annotation><annotation encoding="application/x-llamapun" id="S8.F7.8.2.2.m2.1e">italic_d</annotation></semantics></math>. The green blocks are fully connected layers with Softplus activation function. Color MLP takes in a concatenated input from Ray Direction, <math alttext="r_{d}" class="ltx_Math" display="inline" id="S8.F7.9.3.3.m3.1"><semantics id="S8.F7.9.3.3.m3.1b"><msub id="S8.F7.9.3.3.m3.1.1" xref="S8.F7.9.3.3.m3.1.1.cmml"><mi id="S8.F7.9.3.3.m3.1.1.2" xref="S8.F7.9.3.3.m3.1.1.2.cmml">r</mi><mi id="S8.F7.9.3.3.m3.1.1.3" xref="S8.F7.9.3.3.m3.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S8.F7.9.3.3.m3.1c"><apply id="S8.F7.9.3.3.m3.1.1.cmml" xref="S8.F7.9.3.3.m3.1.1"><csymbol cd="ambiguous" id="S8.F7.9.3.3.m3.1.1.1.cmml" xref="S8.F7.9.3.3.m3.1.1">subscript</csymbol><ci id="S8.F7.9.3.3.m3.1.1.2.cmml" xref="S8.F7.9.3.3.m3.1.1.2">𝑟</ci><ci id="S8.F7.9.3.3.m3.1.1.3.cmml" xref="S8.F7.9.3.3.m3.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.F7.9.3.3.m3.1d">r_{d}</annotation><annotation encoding="application/x-llamapun" id="S8.F7.9.3.3.m3.1e">italic_r start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT</annotation></semantics></math>, and an intermediate feature from the second layer in Density MLP to predict color, <math alttext="c" class="ltx_Math" display="inline" id="S8.F7.10.4.4.m4.1"><semantics id="S8.F7.10.4.4.m4.1b"><mi id="S8.F7.10.4.4.m4.1.1" xref="S8.F7.10.4.4.m4.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S8.F7.10.4.4.m4.1c"><ci id="S8.F7.10.4.4.m4.1.1.cmml" xref="S8.F7.10.4.4.m4.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.F7.10.4.4.m4.1d">c</annotation><annotation encoding="application/x-llamapun" id="S8.F7.10.4.4.m4.1e">italic_c</annotation></semantics></math>.
Feature MLP takes in a discrete 3D point, <math alttext="x" class="ltx_Math" display="inline" id="S8.F7.11.5.5.m5.1"><semantics id="S8.F7.11.5.5.m5.1b"><mi id="S8.F7.11.5.5.m5.1.1" xref="S8.F7.11.5.5.m5.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S8.F7.11.5.5.m5.1c"><ci id="S8.F7.11.5.5.m5.1.1.cmml" xref="S8.F7.11.5.5.m5.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.F7.11.5.5.m5.1d">x</annotation><annotation encoding="application/x-llamapun" id="S8.F7.11.5.5.m5.1e">italic_x</annotation></semantics></math>, and passed through two fully connected layers to predict feature value, <math alttext="f" class="ltx_Math" display="inline" id="S8.F7.12.6.6.m6.1"><semantics id="S8.F7.12.6.6.m6.1b"><mi id="S8.F7.12.6.6.m6.1.1" xref="S8.F7.12.6.6.m6.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S8.F7.12.6.6.m6.1c"><ci id="S8.F7.12.6.6.m6.1.1.cmml" xref="S8.F7.12.6.6.m6.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.F7.12.6.6.m6.1d">f</annotation><annotation encoding="application/x-llamapun" id="S8.F7.12.6.6.m6.1e">italic_f</annotation></semantics></math>. The pink blocks in Feature MLP employ Sine activation functions as proposed in Siren <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">39</span></a>]</cite>. The orange blocks are trained during Stage 1 and the blue block along with CNN is trained during Stage 2.</span></span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S8.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.1 </span>Data Preparation</h3>
<div class="ltx_para" id="S8.SS1.p1">
<p class="ltx_p" id="S8.SS1.p1.3">We assume that the bounding box labels and relative pose labels between images are available for training. We use Segment Anything to segment the objects using the bounding box as input. We use segmentation masks, given images, and relative pose labels to train our pipeline. We use object crops and scale them to fit into <math alttext="190\times 190" class="ltx_Math" display="inline" id="S8.SS1.p1.1.m1.1"><semantics id="S8.SS1.p1.1.m1.1a"><mrow id="S8.SS1.p1.1.m1.1.1" xref="S8.SS1.p1.1.m1.1.1.cmml"><mn id="S8.SS1.p1.1.m1.1.1.2" xref="S8.SS1.p1.1.m1.1.1.2.cmml">190</mn><mo id="S8.SS1.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S8.SS1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S8.SS1.p1.1.m1.1.1.3" xref="S8.SS1.p1.1.m1.1.1.3.cmml">190</mn></mrow><annotation-xml encoding="MathML-Content" id="S8.SS1.p1.1.m1.1b"><apply id="S8.SS1.p1.1.m1.1.1.cmml" xref="S8.SS1.p1.1.m1.1.1"><times id="S8.SS1.p1.1.m1.1.1.1.cmml" xref="S8.SS1.p1.1.m1.1.1.1"></times><cn id="S8.SS1.p1.1.m1.1.1.2.cmml" type="integer" xref="S8.SS1.p1.1.m1.1.1.2">190</cn><cn id="S8.SS1.p1.1.m1.1.1.3.cmml" type="integer" xref="S8.SS1.p1.1.m1.1.1.3">190</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.SS1.p1.1.m1.1c">190\times 190</annotation><annotation encoding="application/x-llamapun" id="S8.SS1.p1.1.m1.1d">190 × 190</annotation></semantics></math> resolution while preserving the aspect ratio. To create synthetic occlusions with some part of the object removed, we create occlusion in the crops by randomly transforming the object out of the image and applying the inverse transformation so that the part of the image outside the image after the initial transformation is lost which gives us an image with some part of the object removed. We also remove some rectangular regions randomly from the object. We place the object crop in <math alttext="224\times 224" class="ltx_Math" display="inline" id="S8.SS1.p1.2.m2.1"><semantics id="S8.SS1.p1.2.m2.1a"><mrow id="S8.SS1.p1.2.m2.1.1" xref="S8.SS1.p1.2.m2.1.1.cmml"><mn id="S8.SS1.p1.2.m2.1.1.2" xref="S8.SS1.p1.2.m2.1.1.2.cmml">224</mn><mo id="S8.SS1.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S8.SS1.p1.2.m2.1.1.1.cmml">×</mo><mn id="S8.SS1.p1.2.m2.1.1.3" xref="S8.SS1.p1.2.m2.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S8.SS1.p1.2.m2.1b"><apply id="S8.SS1.p1.2.m2.1.1.cmml" xref="S8.SS1.p1.2.m2.1.1"><times id="S8.SS1.p1.2.m2.1.1.1.cmml" xref="S8.SS1.p1.2.m2.1.1.1"></times><cn id="S8.SS1.p1.2.m2.1.1.2.cmml" type="integer" xref="S8.SS1.p1.2.m2.1.1.2">224</cn><cn id="S8.SS1.p1.2.m2.1.1.3.cmml" type="integer" xref="S8.SS1.p1.2.m2.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.SS1.p1.2.m2.1c">224\times 224</annotation><annotation encoding="application/x-llamapun" id="S8.SS1.p1.2.m2.1d">224 × 224</annotation></semantics></math> resolution image at a random location and apply random scale, and rotations on the new image to create more augmentations. We also apply filters from Albumentations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib4" title=""><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite> to add noise, contrast, brightness, and blur augmentations. We follow the same procedure to create an image of size <math alttext="224\times 224" class="ltx_Math" display="inline" id="S8.SS1.p1.3.m3.1"><semantics id="S8.SS1.p1.3.m3.1a"><mrow id="S8.SS1.p1.3.m3.1.1" xref="S8.SS1.p1.3.m3.1.1.cmml"><mn id="S8.SS1.p1.3.m3.1.1.2" xref="S8.SS1.p1.3.m3.1.1.2.cmml">224</mn><mo id="S8.SS1.p1.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="S8.SS1.p1.3.m3.1.1.1.cmml">×</mo><mn id="S8.SS1.p1.3.m3.1.1.3" xref="S8.SS1.p1.3.m3.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S8.SS1.p1.3.m3.1b"><apply id="S8.SS1.p1.3.m3.1.1.cmml" xref="S8.SS1.p1.3.m3.1.1"><times id="S8.SS1.p1.3.m3.1.1.1.cmml" xref="S8.SS1.p1.3.m3.1.1.1"></times><cn id="S8.SS1.p1.3.m3.1.1.2.cmml" type="integer" xref="S8.SS1.p1.3.m3.1.1.2">224</cn><cn id="S8.SS1.p1.3.m3.1.1.3.cmml" type="integer" xref="S8.SS1.p1.3.m3.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.SS1.p1.3.m3.1c">224\times 224</annotation><annotation encoding="application/x-llamapun" id="S8.SS1.p1.3.m3.1d">224 × 224</annotation></semantics></math> from the object crop for inference. We do not perform augmentations on the inference image.</p>
</div>
</section>
<section class="ltx_subsection" id="S8.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.2 </span>Training</h3>
<div class="ltx_para" id="S8.SS2.p1">
<p class="ltx_p" id="S8.SS2.p1.4">We use an Nvidia GeForce GV102 GPU with 12GB memory to train our models. Training Stage 1 takes 8 hours and training Stage 2 takes 6 hours. Inference time for a test sample takes 35 milliseconds. We train NeRF in stage 1 for 60K iterations with a batch size of 3. We use a learning rate of 1e-3. We use the NeRF implementation with ray sampler and renderer from Pytorch3d <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite>. We sample 650 rays for an image with 512 points per ray during stage 1 as we need to learn geometry. In stage 2, we use stratified sampling to sample points only near the surface. We initially find the blend weights using the ray sampler with 512 points. We perform stratified sampling using blend weights to sample 8 points along the ray based on blend weights. Stratified sampling ensures that points very close to the surface are sampled. We use only samples from stratified sampling to perform feature rendering in stage 2. We train Feature MLP and CNN during stage 2 using a learning rate of 3e-5 and 3e-4 respectively for 60K iterations with a batch size of 16. Instead of formulating the loss over the entire feature image, we sample 1024 samples from the feature image from CNN and 1024 samples at the same pixel locations in the feature image from NeRF. We sample 1024 negative samples by shooting rays from all the available cameras to get uniformly distributed negative samples across the object. This is done for each batch sample. So, a batch sample contains 1024 positive samples from both NeRF feature image and CNN feature image and 1024 negative samples. All the samples are 12-dimensional feature vectors. The contrastive loss, <math alttext="L_{c}" class="ltx_Math" display="inline" id="S8.SS2.p1.1.m1.1"><semantics id="S8.SS2.p1.1.m1.1a"><msub id="S8.SS2.p1.1.m1.1.1" xref="S8.SS2.p1.1.m1.1.1.cmml"><mi id="S8.SS2.p1.1.m1.1.1.2" xref="S8.SS2.p1.1.m1.1.1.2.cmml">L</mi><mi id="S8.SS2.p1.1.m1.1.1.3" xref="S8.SS2.p1.1.m1.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S8.SS2.p1.1.m1.1b"><apply id="S8.SS2.p1.1.m1.1.1.cmml" xref="S8.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S8.SS2.p1.1.m1.1.1.1.cmml" xref="S8.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S8.SS2.p1.1.m1.1.1.2.cmml" xref="S8.SS2.p1.1.m1.1.1.2">𝐿</ci><ci id="S8.SS2.p1.1.m1.1.1.3.cmml" xref="S8.SS2.p1.1.m1.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.SS2.p1.1.m1.1c">L_{c}</annotation><annotation encoding="application/x-llamapun" id="S8.SS2.p1.1.m1.1d">italic_L start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math>, is formulated for 1024 pixels over 12-dimensional feature vectors from CNN feature image, <math alttext="F" class="ltx_Math" display="inline" id="S8.SS2.p1.2.m2.1"><semantics id="S8.SS2.p1.2.m2.1a"><mi id="S8.SS2.p1.2.m2.1.1" xref="S8.SS2.p1.2.m2.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S8.SS2.p1.2.m2.1b"><ci id="S8.SS2.p1.2.m2.1.1.cmml" xref="S8.SS2.p1.2.m2.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.SS2.p1.2.m2.1c">F</annotation><annotation encoding="application/x-llamapun" id="S8.SS2.p1.2.m2.1d">italic_F</annotation></semantics></math>, NeRF Feature Image, <math alttext="G" class="ltx_Math" display="inline" id="S8.SS2.p1.3.m3.1"><semantics id="S8.SS2.p1.3.m3.1a"><mi id="S8.SS2.p1.3.m3.1.1" xref="S8.SS2.p1.3.m3.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S8.SS2.p1.3.m3.1b"><ci id="S8.SS2.p1.3.m3.1.1.cmml" xref="S8.SS2.p1.3.m3.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.SS2.p1.3.m3.1c">G</annotation><annotation encoding="application/x-llamapun" id="S8.SS2.p1.3.m3.1d">italic_G</annotation></semantics></math> and 1024 negative samples, <math alttext="Z" class="ltx_Math" display="inline" id="S8.SS2.p1.4.m4.1"><semantics id="S8.SS2.p1.4.m4.1a"><mi id="S8.SS2.p1.4.m4.1.1" xref="S8.SS2.p1.4.m4.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S8.SS2.p1.4.m4.1b"><ci id="S8.SS2.p1.4.m4.1.1.cmml" xref="S8.SS2.p1.4.m4.1.1">𝑍</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.SS2.p1.4.m4.1c">Z</annotation><annotation encoding="application/x-llamapun" id="S8.SS2.p1.4.m4.1d">italic_Z</annotation></semantics></math>, as follows:</p>
</div>
<div class="ltx_para" id="S8.SS2.p2">
<table class="ltx_equation ltx_eqn_table" id="S8.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="L_{c}=-\sum_{i=1}^{1024}\log\frac{\exp(F^{i}G^{i})}{\sum_{j=1}^{1024}\exp(F^{i%
}Z^{j})}" class="ltx_Math" display="block" id="S8.E9.m1.4"><semantics id="S8.E9.m1.4a"><mrow id="S8.E9.m1.4.5" xref="S8.E9.m1.4.5.cmml"><msub id="S8.E9.m1.4.5.2" xref="S8.E9.m1.4.5.2.cmml"><mi id="S8.E9.m1.4.5.2.2" xref="S8.E9.m1.4.5.2.2.cmml">L</mi><mi id="S8.E9.m1.4.5.2.3" xref="S8.E9.m1.4.5.2.3.cmml">c</mi></msub><mo id="S8.E9.m1.4.5.1" xref="S8.E9.m1.4.5.1.cmml">=</mo><mrow id="S8.E9.m1.4.5.3" xref="S8.E9.m1.4.5.3.cmml"><mo id="S8.E9.m1.4.5.3a" xref="S8.E9.m1.4.5.3.cmml">−</mo><mrow id="S8.E9.m1.4.5.3.2" xref="S8.E9.m1.4.5.3.2.cmml"><munderover id="S8.E9.m1.4.5.3.2.1" xref="S8.E9.m1.4.5.3.2.1.cmml"><mo id="S8.E9.m1.4.5.3.2.1.2.2" movablelimits="false" xref="S8.E9.m1.4.5.3.2.1.2.2.cmml">∑</mo><mrow id="S8.E9.m1.4.5.3.2.1.2.3" xref="S8.E9.m1.4.5.3.2.1.2.3.cmml"><mi id="S8.E9.m1.4.5.3.2.1.2.3.2" xref="S8.E9.m1.4.5.3.2.1.2.3.2.cmml">i</mi><mo id="S8.E9.m1.4.5.3.2.1.2.3.1" xref="S8.E9.m1.4.5.3.2.1.2.3.1.cmml">=</mo><mn id="S8.E9.m1.4.5.3.2.1.2.3.3" xref="S8.E9.m1.4.5.3.2.1.2.3.3.cmml">1</mn></mrow><mn id="S8.E9.m1.4.5.3.2.1.3" xref="S8.E9.m1.4.5.3.2.1.3.cmml">1024</mn></munderover><mrow id="S8.E9.m1.4.5.3.2.2" xref="S8.E9.m1.4.5.3.2.2.cmml"><mi id="S8.E9.m1.4.5.3.2.2.1" xref="S8.E9.m1.4.5.3.2.2.1.cmml">log</mi><mo id="S8.E9.m1.4.5.3.2.2a" lspace="0.167em" xref="S8.E9.m1.4.5.3.2.2.cmml">⁡</mo><mfrac id="S8.E9.m1.4.4" xref="S8.E9.m1.4.4.cmml"><mrow id="S8.E9.m1.2.2.2.2" xref="S8.E9.m1.2.2.2.3.cmml"><mi id="S8.E9.m1.1.1.1.1" xref="S8.E9.m1.1.1.1.1.cmml">exp</mi><mo id="S8.E9.m1.2.2.2.2a" xref="S8.E9.m1.2.2.2.3.cmml">⁡</mo><mrow id="S8.E9.m1.2.2.2.2.1" xref="S8.E9.m1.2.2.2.3.cmml"><mo id="S8.E9.m1.2.2.2.2.1.2" stretchy="false" xref="S8.E9.m1.2.2.2.3.cmml">(</mo><mrow id="S8.E9.m1.2.2.2.2.1.1" xref="S8.E9.m1.2.2.2.2.1.1.cmml"><msup id="S8.E9.m1.2.2.2.2.1.1.2" xref="S8.E9.m1.2.2.2.2.1.1.2.cmml"><mi id="S8.E9.m1.2.2.2.2.1.1.2.2" xref="S8.E9.m1.2.2.2.2.1.1.2.2.cmml">F</mi><mi id="S8.E9.m1.2.2.2.2.1.1.2.3" xref="S8.E9.m1.2.2.2.2.1.1.2.3.cmml">i</mi></msup><mo id="S8.E9.m1.2.2.2.2.1.1.1" xref="S8.E9.m1.2.2.2.2.1.1.1.cmml">⁢</mo><msup id="S8.E9.m1.2.2.2.2.1.1.3" xref="S8.E9.m1.2.2.2.2.1.1.3.cmml"><mi id="S8.E9.m1.2.2.2.2.1.1.3.2" xref="S8.E9.m1.2.2.2.2.1.1.3.2.cmml">G</mi><mi id="S8.E9.m1.2.2.2.2.1.1.3.3" xref="S8.E9.m1.2.2.2.2.1.1.3.3.cmml">i</mi></msup></mrow><mo id="S8.E9.m1.2.2.2.2.1.3" stretchy="false" xref="S8.E9.m1.2.2.2.3.cmml">)</mo></mrow></mrow><mrow id="S8.E9.m1.4.4.4" xref="S8.E9.m1.4.4.4.cmml"><msubsup id="S8.E9.m1.4.4.4.3" xref="S8.E9.m1.4.4.4.3.cmml"><mo id="S8.E9.m1.4.4.4.3.2.2" xref="S8.E9.m1.4.4.4.3.2.2.cmml">∑</mo><mrow id="S8.E9.m1.4.4.4.3.2.3" xref="S8.E9.m1.4.4.4.3.2.3.cmml"><mi id="S8.E9.m1.4.4.4.3.2.3.2" xref="S8.E9.m1.4.4.4.3.2.3.2.cmml">j</mi><mo id="S8.E9.m1.4.4.4.3.2.3.1" xref="S8.E9.m1.4.4.4.3.2.3.1.cmml">=</mo><mn id="S8.E9.m1.4.4.4.3.2.3.3" xref="S8.E9.m1.4.4.4.3.2.3.3.cmml">1</mn></mrow><mn id="S8.E9.m1.4.4.4.3.3" xref="S8.E9.m1.4.4.4.3.3.cmml">1024</mn></msubsup><mrow id="S8.E9.m1.4.4.4.2.1" xref="S8.E9.m1.4.4.4.2.2.cmml"><mi id="S8.E9.m1.3.3.3.1" xref="S8.E9.m1.3.3.3.1.cmml">exp</mi><mo id="S8.E9.m1.4.4.4.2.1a" xref="S8.E9.m1.4.4.4.2.2.cmml">⁡</mo><mrow id="S8.E9.m1.4.4.4.2.1.1" xref="S8.E9.m1.4.4.4.2.2.cmml"><mo id="S8.E9.m1.4.4.4.2.1.1.2" stretchy="false" xref="S8.E9.m1.4.4.4.2.2.cmml">(</mo><mrow id="S8.E9.m1.4.4.4.2.1.1.1" xref="S8.E9.m1.4.4.4.2.1.1.1.cmml"><msup id="S8.E9.m1.4.4.4.2.1.1.1.2" xref="S8.E9.m1.4.4.4.2.1.1.1.2.cmml"><mi id="S8.E9.m1.4.4.4.2.1.1.1.2.2" xref="S8.E9.m1.4.4.4.2.1.1.1.2.2.cmml">F</mi><mi id="S8.E9.m1.4.4.4.2.1.1.1.2.3" xref="S8.E9.m1.4.4.4.2.1.1.1.2.3.cmml">i</mi></msup><mo id="S8.E9.m1.4.4.4.2.1.1.1.1" xref="S8.E9.m1.4.4.4.2.1.1.1.1.cmml">⁢</mo><msup id="S8.E9.m1.4.4.4.2.1.1.1.3" xref="S8.E9.m1.4.4.4.2.1.1.1.3.cmml"><mi id="S8.E9.m1.4.4.4.2.1.1.1.3.2" xref="S8.E9.m1.4.4.4.2.1.1.1.3.2.cmml">Z</mi><mi id="S8.E9.m1.4.4.4.2.1.1.1.3.3" xref="S8.E9.m1.4.4.4.2.1.1.1.3.3.cmml">j</mi></msup></mrow><mo id="S8.E9.m1.4.4.4.2.1.1.3" stretchy="false" xref="S8.E9.m1.4.4.4.2.2.cmml">)</mo></mrow></mrow></mrow></mfrac></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S8.E9.m1.4b"><apply id="S8.E9.m1.4.5.cmml" xref="S8.E9.m1.4.5"><eq id="S8.E9.m1.4.5.1.cmml" xref="S8.E9.m1.4.5.1"></eq><apply id="S8.E9.m1.4.5.2.cmml" xref="S8.E9.m1.4.5.2"><csymbol cd="ambiguous" id="S8.E9.m1.4.5.2.1.cmml" xref="S8.E9.m1.4.5.2">subscript</csymbol><ci id="S8.E9.m1.4.5.2.2.cmml" xref="S8.E9.m1.4.5.2.2">𝐿</ci><ci id="S8.E9.m1.4.5.2.3.cmml" xref="S8.E9.m1.4.5.2.3">𝑐</ci></apply><apply id="S8.E9.m1.4.5.3.cmml" xref="S8.E9.m1.4.5.3"><minus id="S8.E9.m1.4.5.3.1.cmml" xref="S8.E9.m1.4.5.3"></minus><apply id="S8.E9.m1.4.5.3.2.cmml" xref="S8.E9.m1.4.5.3.2"><apply id="S8.E9.m1.4.5.3.2.1.cmml" xref="S8.E9.m1.4.5.3.2.1"><csymbol cd="ambiguous" id="S8.E9.m1.4.5.3.2.1.1.cmml" xref="S8.E9.m1.4.5.3.2.1">superscript</csymbol><apply id="S8.E9.m1.4.5.3.2.1.2.cmml" xref="S8.E9.m1.4.5.3.2.1"><csymbol cd="ambiguous" id="S8.E9.m1.4.5.3.2.1.2.1.cmml" xref="S8.E9.m1.4.5.3.2.1">subscript</csymbol><sum id="S8.E9.m1.4.5.3.2.1.2.2.cmml" xref="S8.E9.m1.4.5.3.2.1.2.2"></sum><apply id="S8.E9.m1.4.5.3.2.1.2.3.cmml" xref="S8.E9.m1.4.5.3.2.1.2.3"><eq id="S8.E9.m1.4.5.3.2.1.2.3.1.cmml" xref="S8.E9.m1.4.5.3.2.1.2.3.1"></eq><ci id="S8.E9.m1.4.5.3.2.1.2.3.2.cmml" xref="S8.E9.m1.4.5.3.2.1.2.3.2">𝑖</ci><cn id="S8.E9.m1.4.5.3.2.1.2.3.3.cmml" type="integer" xref="S8.E9.m1.4.5.3.2.1.2.3.3">1</cn></apply></apply><cn id="S8.E9.m1.4.5.3.2.1.3.cmml" type="integer" xref="S8.E9.m1.4.5.3.2.1.3">1024</cn></apply><apply id="S8.E9.m1.4.5.3.2.2.cmml" xref="S8.E9.m1.4.5.3.2.2"><log id="S8.E9.m1.4.5.3.2.2.1.cmml" xref="S8.E9.m1.4.5.3.2.2.1"></log><apply id="S8.E9.m1.4.4.cmml" xref="S8.E9.m1.4.4"><divide id="S8.E9.m1.4.4.5.cmml" xref="S8.E9.m1.4.4"></divide><apply id="S8.E9.m1.2.2.2.3.cmml" xref="S8.E9.m1.2.2.2.2"><exp id="S8.E9.m1.1.1.1.1.cmml" xref="S8.E9.m1.1.1.1.1"></exp><apply id="S8.E9.m1.2.2.2.2.1.1.cmml" xref="S8.E9.m1.2.2.2.2.1.1"><times id="S8.E9.m1.2.2.2.2.1.1.1.cmml" xref="S8.E9.m1.2.2.2.2.1.1.1"></times><apply id="S8.E9.m1.2.2.2.2.1.1.2.cmml" xref="S8.E9.m1.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S8.E9.m1.2.2.2.2.1.1.2.1.cmml" xref="S8.E9.m1.2.2.2.2.1.1.2">superscript</csymbol><ci id="S8.E9.m1.2.2.2.2.1.1.2.2.cmml" xref="S8.E9.m1.2.2.2.2.1.1.2.2">𝐹</ci><ci id="S8.E9.m1.2.2.2.2.1.1.2.3.cmml" xref="S8.E9.m1.2.2.2.2.1.1.2.3">𝑖</ci></apply><apply id="S8.E9.m1.2.2.2.2.1.1.3.cmml" xref="S8.E9.m1.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S8.E9.m1.2.2.2.2.1.1.3.1.cmml" xref="S8.E9.m1.2.2.2.2.1.1.3">superscript</csymbol><ci id="S8.E9.m1.2.2.2.2.1.1.3.2.cmml" xref="S8.E9.m1.2.2.2.2.1.1.3.2">𝐺</ci><ci id="S8.E9.m1.2.2.2.2.1.1.3.3.cmml" xref="S8.E9.m1.2.2.2.2.1.1.3.3">𝑖</ci></apply></apply></apply><apply id="S8.E9.m1.4.4.4.cmml" xref="S8.E9.m1.4.4.4"><apply id="S8.E9.m1.4.4.4.3.cmml" xref="S8.E9.m1.4.4.4.3"><csymbol cd="ambiguous" id="S8.E9.m1.4.4.4.3.1.cmml" xref="S8.E9.m1.4.4.4.3">superscript</csymbol><apply id="S8.E9.m1.4.4.4.3.2.cmml" xref="S8.E9.m1.4.4.4.3"><csymbol cd="ambiguous" id="S8.E9.m1.4.4.4.3.2.1.cmml" xref="S8.E9.m1.4.4.4.3">subscript</csymbol><sum id="S8.E9.m1.4.4.4.3.2.2.cmml" xref="S8.E9.m1.4.4.4.3.2.2"></sum><apply id="S8.E9.m1.4.4.4.3.2.3.cmml" xref="S8.E9.m1.4.4.4.3.2.3"><eq id="S8.E9.m1.4.4.4.3.2.3.1.cmml" xref="S8.E9.m1.4.4.4.3.2.3.1"></eq><ci id="S8.E9.m1.4.4.4.3.2.3.2.cmml" xref="S8.E9.m1.4.4.4.3.2.3.2">𝑗</ci><cn id="S8.E9.m1.4.4.4.3.2.3.3.cmml" type="integer" xref="S8.E9.m1.4.4.4.3.2.3.3">1</cn></apply></apply><cn id="S8.E9.m1.4.4.4.3.3.cmml" type="integer" xref="S8.E9.m1.4.4.4.3.3">1024</cn></apply><apply id="S8.E9.m1.4.4.4.2.2.cmml" xref="S8.E9.m1.4.4.4.2.1"><exp id="S8.E9.m1.3.3.3.1.cmml" xref="S8.E9.m1.3.3.3.1"></exp><apply id="S8.E9.m1.4.4.4.2.1.1.1.cmml" xref="S8.E9.m1.4.4.4.2.1.1.1"><times id="S8.E9.m1.4.4.4.2.1.1.1.1.cmml" xref="S8.E9.m1.4.4.4.2.1.1.1.1"></times><apply id="S8.E9.m1.4.4.4.2.1.1.1.2.cmml" xref="S8.E9.m1.4.4.4.2.1.1.1.2"><csymbol cd="ambiguous" id="S8.E9.m1.4.4.4.2.1.1.1.2.1.cmml" xref="S8.E9.m1.4.4.4.2.1.1.1.2">superscript</csymbol><ci id="S8.E9.m1.4.4.4.2.1.1.1.2.2.cmml" xref="S8.E9.m1.4.4.4.2.1.1.1.2.2">𝐹</ci><ci id="S8.E9.m1.4.4.4.2.1.1.1.2.3.cmml" xref="S8.E9.m1.4.4.4.2.1.1.1.2.3">𝑖</ci></apply><apply id="S8.E9.m1.4.4.4.2.1.1.1.3.cmml" xref="S8.E9.m1.4.4.4.2.1.1.1.3"><csymbol cd="ambiguous" id="S8.E9.m1.4.4.4.2.1.1.1.3.1.cmml" xref="S8.E9.m1.4.4.4.2.1.1.1.3">superscript</csymbol><ci id="S8.E9.m1.4.4.4.2.1.1.1.3.2.cmml" xref="S8.E9.m1.4.4.4.2.1.1.1.3.2">𝑍</ci><ci id="S8.E9.m1.4.4.4.2.1.1.1.3.3.cmml" xref="S8.E9.m1.4.4.4.2.1.1.1.3.3">𝑗</ci></apply></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.E9.m1.4c">L_{c}=-\sum_{i=1}^{1024}\log\frac{\exp(F^{i}G^{i})}{\sum_{j=1}^{1024}\exp(F^{i%
}Z^{j})}</annotation><annotation encoding="application/x-llamapun" id="S8.E9.m1.4d">italic_L start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT = - ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1024 end_POSTSUPERSCRIPT roman_log divide start_ARG roman_exp ( italic_F start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT italic_G start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1024 end_POSTSUPERSCRIPT roman_exp ( italic_F start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT italic_Z start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT ) end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S8.SS2.p3">
<p class="ltx_p" id="S8.SS2.p3.7"><math alttext="F_{i}" class="ltx_Math" display="inline" id="S8.SS2.p3.1.m1.1"><semantics id="S8.SS2.p3.1.m1.1a"><msub id="S8.SS2.p3.1.m1.1.1" xref="S8.SS2.p3.1.m1.1.1.cmml"><mi id="S8.SS2.p3.1.m1.1.1.2" xref="S8.SS2.p3.1.m1.1.1.2.cmml">F</mi><mi id="S8.SS2.p3.1.m1.1.1.3" xref="S8.SS2.p3.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S8.SS2.p3.1.m1.1b"><apply id="S8.SS2.p3.1.m1.1.1.cmml" xref="S8.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S8.SS2.p3.1.m1.1.1.1.cmml" xref="S8.SS2.p3.1.m1.1.1">subscript</csymbol><ci id="S8.SS2.p3.1.m1.1.1.2.cmml" xref="S8.SS2.p3.1.m1.1.1.2">𝐹</ci><ci id="S8.SS2.p3.1.m1.1.1.3.cmml" xref="S8.SS2.p3.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.SS2.p3.1.m1.1c">F_{i}</annotation><annotation encoding="application/x-llamapun" id="S8.SS2.p3.1.m1.1d">italic_F start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="G_{i}" class="ltx_Math" display="inline" id="S8.SS2.p3.2.m2.1"><semantics id="S8.SS2.p3.2.m2.1a"><msub id="S8.SS2.p3.2.m2.1.1" xref="S8.SS2.p3.2.m2.1.1.cmml"><mi id="S8.SS2.p3.2.m2.1.1.2" xref="S8.SS2.p3.2.m2.1.1.2.cmml">G</mi><mi id="S8.SS2.p3.2.m2.1.1.3" xref="S8.SS2.p3.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S8.SS2.p3.2.m2.1b"><apply id="S8.SS2.p3.2.m2.1.1.cmml" xref="S8.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S8.SS2.p3.2.m2.1.1.1.cmml" xref="S8.SS2.p3.2.m2.1.1">subscript</csymbol><ci id="S8.SS2.p3.2.m2.1.1.2.cmml" xref="S8.SS2.p3.2.m2.1.1.2">𝐺</ci><ci id="S8.SS2.p3.2.m2.1.1.3.cmml" xref="S8.SS2.p3.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.SS2.p3.2.m2.1c">G_{i}</annotation><annotation encoding="application/x-llamapun" id="S8.SS2.p3.2.m2.1d">italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> are 12-dimensional feature vectors from the CNN feature image,<math alttext="F" class="ltx_Math" display="inline" id="S8.SS2.p3.3.m3.1"><semantics id="S8.SS2.p3.3.m3.1a"><mi id="S8.SS2.p3.3.m3.1.1" xref="S8.SS2.p3.3.m3.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S8.SS2.p3.3.m3.1b"><ci id="S8.SS2.p3.3.m3.1.1.cmml" xref="S8.SS2.p3.3.m3.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.SS2.p3.3.m3.1c">F</annotation><annotation encoding="application/x-llamapun" id="S8.SS2.p3.3.m3.1d">italic_F</annotation></semantics></math>, and NeRF image, <math alttext="G" class="ltx_Math" display="inline" id="S8.SS2.p3.4.m4.1"><semantics id="S8.SS2.p3.4.m4.1a"><mi id="S8.SS2.p3.4.m4.1.1" xref="S8.SS2.p3.4.m4.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S8.SS2.p3.4.m4.1b"><ci id="S8.SS2.p3.4.m4.1.1.cmml" xref="S8.SS2.p3.4.m4.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.SS2.p3.4.m4.1c">G</annotation><annotation encoding="application/x-llamapun" id="S8.SS2.p3.4.m4.1d">italic_G</annotation></semantics></math>, at pixel <math alttext="i" class="ltx_Math" display="inline" id="S8.SS2.p3.5.m5.1"><semantics id="S8.SS2.p3.5.m5.1a"><mi id="S8.SS2.p3.5.m5.1.1" xref="S8.SS2.p3.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S8.SS2.p3.5.m5.1b"><ci id="S8.SS2.p3.5.m5.1.1.cmml" xref="S8.SS2.p3.5.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.SS2.p3.5.m5.1c">i</annotation><annotation encoding="application/x-llamapun" id="S8.SS2.p3.5.m5.1d">italic_i</annotation></semantics></math>. <math alttext="Z_{j}" class="ltx_Math" display="inline" id="S8.SS2.p3.6.m6.1"><semantics id="S8.SS2.p3.6.m6.1a"><msub id="S8.SS2.p3.6.m6.1.1" xref="S8.SS2.p3.6.m6.1.1.cmml"><mi id="S8.SS2.p3.6.m6.1.1.2" xref="S8.SS2.p3.6.m6.1.1.2.cmml">Z</mi><mi id="S8.SS2.p3.6.m6.1.1.3" xref="S8.SS2.p3.6.m6.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S8.SS2.p3.6.m6.1b"><apply id="S8.SS2.p3.6.m6.1.1.cmml" xref="S8.SS2.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S8.SS2.p3.6.m6.1.1.1.cmml" xref="S8.SS2.p3.6.m6.1.1">subscript</csymbol><ci id="S8.SS2.p3.6.m6.1.1.2.cmml" xref="S8.SS2.p3.6.m6.1.1.2">𝑍</ci><ci id="S8.SS2.p3.6.m6.1.1.3.cmml" xref="S8.SS2.p3.6.m6.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.SS2.p3.6.m6.1c">Z_{j}</annotation><annotation encoding="application/x-llamapun" id="S8.SS2.p3.6.m6.1d">italic_Z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> is a <math alttext="j^{th}" class="ltx_Math" display="inline" id="S8.SS2.p3.7.m7.1"><semantics id="S8.SS2.p3.7.m7.1a"><msup id="S8.SS2.p3.7.m7.1.1" xref="S8.SS2.p3.7.m7.1.1.cmml"><mi id="S8.SS2.p3.7.m7.1.1.2" xref="S8.SS2.p3.7.m7.1.1.2.cmml">j</mi><mrow id="S8.SS2.p3.7.m7.1.1.3" xref="S8.SS2.p3.7.m7.1.1.3.cmml"><mi id="S8.SS2.p3.7.m7.1.1.3.2" xref="S8.SS2.p3.7.m7.1.1.3.2.cmml">t</mi><mo id="S8.SS2.p3.7.m7.1.1.3.1" xref="S8.SS2.p3.7.m7.1.1.3.1.cmml">⁢</mo><mi id="S8.SS2.p3.7.m7.1.1.3.3" xref="S8.SS2.p3.7.m7.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S8.SS2.p3.7.m7.1b"><apply id="S8.SS2.p3.7.m7.1.1.cmml" xref="S8.SS2.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S8.SS2.p3.7.m7.1.1.1.cmml" xref="S8.SS2.p3.7.m7.1.1">superscript</csymbol><ci id="S8.SS2.p3.7.m7.1.1.2.cmml" xref="S8.SS2.p3.7.m7.1.1.2">𝑗</ci><apply id="S8.SS2.p3.7.m7.1.1.3.cmml" xref="S8.SS2.p3.7.m7.1.1.3"><times id="S8.SS2.p3.7.m7.1.1.3.1.cmml" xref="S8.SS2.p3.7.m7.1.1.3.1"></times><ci id="S8.SS2.p3.7.m7.1.1.3.2.cmml" xref="S8.SS2.p3.7.m7.1.1.3.2">𝑡</ci><ci id="S8.SS2.p3.7.m7.1.1.3.3.cmml" xref="S8.SS2.p3.7.m7.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.SS2.p3.7.m7.1c">j^{th}</annotation><annotation encoding="application/x-llamapun" id="S8.SS2.p3.7.m7.1d">italic_j start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT</annotation></semantics></math>12 dimensional feature vector from 1024 negative samples.</p>
</div>
</section>
<section class="ltx_subsection" id="S8.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.3 </span>Learning Consistent Features</h3>
<div class="ltx_para" id="S8.SS3.p1">
<p class="ltx_p" id="S8.SS3.p1.1">We extract positive samples from NeRF feature image and CNN feature image to make sure that they predict the same features at the same pixel for a specific pose. We sample negative samples from different poses randomly to ensure that the negative samples are uniformly distributed across the shape. In the case of asymmetric objects, this helps to ensure that learned features are distinguishable based on their surface location on the object. In the case of symmetric objects, multiple points in symmetric configurations will map to similar features and the negative samples ensure that some form of discriminative feature space is learned while still obeying symmetry.</p>
</div>
</section>
<section class="ltx_subsection" id="S8.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.4 </span>Inference</h3>
<section class="ltx_paragraph" id="S8.SS4.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Point cloud extraction</h4>
<div class="ltx_para" id="S8.SS4.SSS0.Px1.p1">
<p class="ltx_p" id="S8.SS4.SSS0.Px1.p1.1">Before the inference phase, we extract the point cloud from the NeRF network by shooting rays from different viewpoints in training data and estimating the surface point and its corresponding feature. We extract the point cloud by estimating the point with blend weights along the ray and extracting the point corresponding to the maximum blend weights as the surface point. We can query per-point features for surface points by performing a forward pass for each 3D point through Feature MLP. So, we extract a point cloud with a per-point feature from our NeRF which is used for inference to establish correspondences.</p>
</div>
</section>
<section class="ltx_paragraph" id="S8.SS4.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Inference with Image</h4>
<div class="ltx_para" id="S8.SS4.SSS0.Px2.p1">
<p class="ltx_p" id="S8.SS4.SSS0.Px2.p1.1">During inference, we preprocess our RGB image and pass it through CNN to extract the corresponding feature image. We also estimate segmentation mask using DpodV2. We extract features inside the segmentation mask from the feature image using the segmentation mask. We use these features to find correspondences with our point cloud with features. We compare each feature vector in the feature image with all feature vectors in point clouds by taking the dot product. We extract a match as the point corresponding to the feature with the maximum dot product score. After the 2D-3D correspondences are estimated, we simply use PnP Ransac with just 500 iterations to estimate the final 6D pose.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S9">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span>Limitations</h2>
<div class="ltx_para" id="S9.p1">
<p class="ltx_p" id="S9.p1.1">The translation estimation can still be improved by decoupling it from rotation estimation. Our approach lags behind approaches using data from both CAD and real data. This needs to be addressed by a potential future work on Nerf to render realistic occlusions from real data. We need relative pose labels and a completely unsupervised approach without these labels is desired.</p>
</div>
</section>
<section class="ltx_section" id="S10">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">10 </span>Additional Results</h2>
<section class="ltx_subsection" id="S10.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">10.1 </span>Linemod Test Results</h3>
<div class="ltx_para" id="S10.SS1.p1">
<p class="ltx_p" id="S10.SS1.p1.3">We add object-wise test results for Linemod Dataset and also extend the ablation studies to all objects. We use Segment anything masks and evaluate all the objects and observe a drop in accuracy of <math alttext="3\%" class="ltx_Math" display="inline" id="S10.SS1.p1.1.m1.1"><semantics id="S10.SS1.p1.1.m1.1a"><mrow id="S10.SS1.p1.1.m1.1.1" xref="S10.SS1.p1.1.m1.1.1.cmml"><mn id="S10.SS1.p1.1.m1.1.1.2" xref="S10.SS1.p1.1.m1.1.1.2.cmml">3</mn><mo id="S10.SS1.p1.1.m1.1.1.1" xref="S10.SS1.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S10.SS1.p1.1.m1.1b"><apply id="S10.SS1.p1.1.m1.1.1.cmml" xref="S10.SS1.p1.1.m1.1.1"><csymbol cd="latexml" id="S10.SS1.p1.1.m1.1.1.1.cmml" xref="S10.SS1.p1.1.m1.1.1.1">percent</csymbol><cn id="S10.SS1.p1.1.m1.1.1.2.cmml" type="integer" xref="S10.SS1.p1.1.m1.1.1.2">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S10.SS1.p1.1.m1.1c">3\%</annotation><annotation encoding="application/x-llamapun" id="S10.SS1.p1.1.m1.1d">3 %</annotation></semantics></math>. We also add results on all objects using 50 sample views for training instead of employing <math alttext="~{}180" class="ltx_Math" display="inline" id="S10.SS1.p1.2.m2.1"><semantics id="S10.SS1.p1.2.m2.1a"><mn id="S10.SS1.p1.2.m2.1.1" xref="S10.SS1.p1.2.m2.1.1.cmml">180</mn><annotation-xml encoding="MathML-Content" id="S10.SS1.p1.2.m2.1b"><cn id="S10.SS1.p1.2.m2.1.1.cmml" type="integer" xref="S10.SS1.p1.2.m2.1.1">180</cn></annotation-xml><annotation encoding="application/x-tex" id="S10.SS1.p1.2.m2.1c">~{}180</annotation><annotation encoding="application/x-llamapun" id="S10.SS1.p1.2.m2.1d">180</annotation></semantics></math> views using in general setup. We improved our evaluation by tuning some hyperparameters in the inference pipeline and observed an increase in accuracy of <math alttext="0.4\%" class="ltx_Math" display="inline" id="S10.SS1.p1.3.m3.1"><semantics id="S10.SS1.p1.3.m3.1a"><mrow id="S10.SS1.p1.3.m3.1.1" xref="S10.SS1.p1.3.m3.1.1.cmml"><mn id="S10.SS1.p1.3.m3.1.1.2" xref="S10.SS1.p1.3.m3.1.1.2.cmml">0.4</mn><mo id="S10.SS1.p1.3.m3.1.1.1" xref="S10.SS1.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S10.SS1.p1.3.m3.1b"><apply id="S10.SS1.p1.3.m3.1.1.cmml" xref="S10.SS1.p1.3.m3.1.1"><csymbol cd="latexml" id="S10.SS1.p1.3.m3.1.1.1.cmml" xref="S10.SS1.p1.3.m3.1.1.1">percent</csymbol><cn id="S10.SS1.p1.3.m3.1.1.2.cmml" type="float" xref="S10.SS1.p1.3.m3.1.1.2">0.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S10.SS1.p1.3.m3.1c">0.4\%</annotation><annotation encoding="application/x-llamapun" id="S10.SS1.p1.3.m3.1d">0.4 %</annotation></semantics></math> compared to the results presented in the main paper. Additional results are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S10.T8" title="Table 8 ‣ 10.1 Linemod Test Results ‣ 10 Additional Results ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<figure class="ltx_table" id="S10.T8">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S10.T8.4.2.1" style="font-size:90%;">Table 8</span>: </span><span class="ltx_text" id="S10.T8.2.1" style="font-size:90%;">Linemod Results: We present results using RGB, RGB-D data, and CAD model. NP(Mask) refers to NeRF-Pose with an evaluation scheme comparing masks during each ransac iteration. Ours(S) refers to results obtained by segmentation masks generated using Segment Anything <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite> instead of using ground truth segmentation masks. Ours(50) refers to our approach trained using just 50 views instead of using <math alttext="~{}180" class="ltx_Math" display="inline" id="S10.T8.2.1.m1.1"><semantics id="S10.T8.2.1.m1.1b"><mn id="S10.T8.2.1.m1.1.1" xref="S10.T8.2.1.m1.1.1.cmml">180</mn><annotation-xml encoding="MathML-Content" id="S10.T8.2.1.m1.1c"><cn id="S10.T8.2.1.m1.1.1.cmml" type="integer" xref="S10.T8.2.1.m1.1.1">180</cn></annotation-xml><annotation encoding="application/x-tex" id="S10.T8.2.1.m1.1d">~{}180</annotation><annotation encoding="application/x-llamapun" id="S10.T8.2.1.m1.1e">180</annotation></semantics></math> views in the general evaluation setup. Ours(D) refers to our results with translation adjustment along the z-axis using a sensor depth map during inference.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S10.T8.5">
<tr class="ltx_tr" id="S10.T8.5.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S10.T8.5.1.1"><span class="ltx_text" id="S10.T8.5.1.1.1" style="font-size:70%;">Object</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S10.T8.5.1.2">
<span class="ltx_text" id="S10.T8.5.1.2.1" style="font-size:70%;">DpodV2</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S10.T8.5.1.2.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib37" title=""><span class="ltx_text" style="font-size:90%;">37</span></a><span class="ltx_text" id="S10.T8.5.1.2.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S10.T8.5.1.3">
<span class="ltx_text" id="S10.T8.5.1.3.1" style="font-size:70%;">FFB-6D</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S10.T8.5.1.3.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">16</span></a><span class="ltx_text" id="S10.T8.5.1.3.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S10.T8.5.1.4">
<span class="ltx_text" id="S10.T8.5.1.4.1" style="font-size:70%;">LieNet</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S10.T8.5.1.4.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">21</span></a><span class="ltx_text" id="S10.T8.5.1.4.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S10.T8.5.1.5">
<span class="ltx_text" id="S10.T8.5.1.5.1" style="font-size:70%;">Cai</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S10.T8.5.1.5.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">6</span></a><span class="ltx_text" id="S10.T8.5.1.5.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S10.T8.5.1.6">
<span class="ltx_text" id="S10.T8.5.1.6.1" style="font-size:70%;">NP(Mask)</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S10.T8.5.1.6.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a><span class="ltx_text" id="S10.T8.5.1.6.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S10.T8.5.1.7"><span class="ltx_text" id="S10.T8.5.1.7.1" style="font-size:70%;">Ours(50)</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S10.T8.5.1.8"><span class="ltx_text" id="S10.T8.5.1.8.1" style="font-size:70%;">Ours(S)</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S10.T8.5.1.9"><span class="ltx_text" id="S10.T8.5.1.9.1" style="font-size:70%;">Ours</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S10.T8.5.1.10"><span class="ltx_text" id="S10.T8.5.1.10.1" style="font-size:70%;">Ours(D)</span></td>
</tr>
<tr class="ltx_tr" id="S10.T8.5.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S10.T8.5.2.1"><span class="ltx_text" id="S10.T8.5.2.1.1" style="font-size:70%;">CAD</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T8.5.2.2"><span class="ltx_text" id="S10.T8.5.2.2.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T8.5.2.3"><span class="ltx_text" id="S10.T8.5.2.3.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T8.5.2.4"><span class="ltx_text" id="S10.T8.5.2.4.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T8.5.2.5"><span class="ltx_text" id="S10.T8.5.2.5.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T8.5.2.6"><span class="ltx_text" id="S10.T8.5.2.6.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T8.5.2.7"><span class="ltx_text" id="S10.T8.5.2.7.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T8.5.2.8"><span class="ltx_text" id="S10.T8.5.2.8.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T8.5.2.9"><span class="ltx_text" id="S10.T8.5.2.9.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T8.5.2.10"><span class="ltx_text" id="S10.T8.5.2.10.1" style="font-size:70%;">✗</span></td>
</tr>
<tr class="ltx_tr" id="S10.T8.5.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S10.T8.5.3.1"><span class="ltx_text" id="S10.T8.5.3.1.1" style="font-size:70%;">Data</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T8.5.3.2"><span class="ltx_text" id="S10.T8.5.3.2.1" style="font-size:70%;">RGB</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T8.5.3.3"><span class="ltx_text" id="S10.T8.5.3.3.1" style="font-size:70%;">RGB-D</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T8.5.3.4"><span class="ltx_text" id="S10.T8.5.3.4.1" style="font-size:70%;">RGB</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T8.5.3.5"><span class="ltx_text" id="S10.T8.5.3.5.1" style="font-size:70%;">RGB</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T8.5.3.6"><span class="ltx_text" id="S10.T8.5.3.6.1" style="font-size:70%;">RGB</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T8.5.3.7"><span class="ltx_text" id="S10.T8.5.3.7.1" style="font-size:70%;">RGB</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T8.5.3.8"><span class="ltx_text" id="S10.T8.5.3.8.1" style="font-size:70%;">RGB</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T8.5.3.9"><span class="ltx_text" id="S10.T8.5.3.9.1" style="font-size:70%;">RGB</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T8.5.3.10"><span class="ltx_text" id="S10.T8.5.3.10.1" style="font-size:70%;">RGB-D</span></td>
</tr>
<tr class="ltx_tr" id="S10.T8.5.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S10.T8.5.4.1"><span class="ltx_text" id="S10.T8.5.4.1.1" style="font-size:70%;">Ape</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T8.5.4.2"><span class="ltx_text" id="S10.T8.5.4.2.1" style="font-size:70%;">80</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T8.5.4.3"><span class="ltx_text" id="S10.T8.5.4.3.1" style="font-size:70%;">98.4</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T8.5.4.4"><span class="ltx_text" id="S10.T8.5.4.4.1" style="font-size:70%;">38.8</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T8.5.4.5"><span class="ltx_text" id="S10.T8.5.4.5.1" style="font-size:70%;">52.9</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T8.5.4.6"><span class="ltx_text" id="S10.T8.5.4.6.1" style="font-size:70%;">89.1</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T8.5.4.7"><span class="ltx_text" id="S10.T8.5.4.7.1" style="font-size:70%;">55.2</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T8.5.4.8"><span class="ltx_text" id="S10.T8.5.4.8.1" style="font-size:70%;">69.9</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T8.5.4.9"><span class="ltx_text" id="S10.T8.5.4.9.1" style="font-size:70%;">71.5</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T8.5.4.10"><span class="ltx_text" id="S10.T8.5.4.10.1" style="font-size:70%;">98.6</span></td>
</tr>
<tr class="ltx_tr" id="S10.T8.5.5">
<td class="ltx_td ltx_align_left" id="S10.T8.5.5.1"><span class="ltx_text" id="S10.T8.5.5.1.1" style="font-size:70%;">BenchVice</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.5.2"><span class="ltx_text" id="S10.T8.5.5.2.1" style="font-size:70%;">99.7</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.5.3"><span class="ltx_text" id="S10.T8.5.5.3.1" style="font-size:70%;">100</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.5.4"><span class="ltx_text" id="S10.T8.5.5.4.1" style="font-size:70%;">71.2</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.5.5"><span class="ltx_text" id="S10.T8.5.5.5.1" style="font-size:70%;">96.5</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.5.6"><span class="ltx_text" id="S10.T8.5.5.6.1" style="font-size:70%;">99.3</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.5.7"><span class="ltx_text" id="S10.T8.5.5.7.1" style="font-size:70%;">97.0</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.5.8"><span class="ltx_text" id="S10.T8.5.5.8.1" style="font-size:70%;">99.3</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.5.9"><span class="ltx_text" id="S10.T8.5.5.9.1" style="font-size:70%;">99.0</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.5.10"><span class="ltx_text" id="S10.T8.5.5.10.1" style="font-size:70%;">100</span></td>
</tr>
<tr class="ltx_tr" id="S10.T8.5.6">
<td class="ltx_td ltx_align_left" id="S10.T8.5.6.1"><span class="ltx_text" id="S10.T8.5.6.1.1" style="font-size:70%;">Camera</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.6.2"><span class="ltx_text" id="S10.T8.5.6.2.1" style="font-size:70%;">99.2</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.6.3"><span class="ltx_text" id="S10.T8.5.6.3.1" style="font-size:70%;">99.9</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.6.4"><span class="ltx_text" id="S10.T8.5.6.4.1" style="font-size:70%;">52.5</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.6.5"><span class="ltx_text" id="S10.T8.5.6.5.1" style="font-size:70%;">87.8</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.6.6"><span class="ltx_text" id="S10.T8.5.6.6.1" style="font-size:70%;">98.7</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.6.7"><span class="ltx_text" id="S10.T8.5.6.7.1" style="font-size:70%;">84.0</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.6.8"><span class="ltx_text" id="S10.T8.5.6.8.1" style="font-size:70%;">94.6</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.6.9"><span class="ltx_text" id="S10.T8.5.6.9.1" style="font-size:70%;">96.3</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.6.10"><span class="ltx_text" id="S10.T8.5.6.10.1" style="font-size:70%;">99.9</span></td>
</tr>
<tr class="ltx_tr" id="S10.T8.5.7">
<td class="ltx_td ltx_align_left" id="S10.T8.5.7.1"><span class="ltx_text" id="S10.T8.5.7.1.1" style="font-size:70%;">Can</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.7.2"><span class="ltx_text" id="S10.T8.5.7.2.1" style="font-size:70%;">99.6</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.7.3"><span class="ltx_text" id="S10.T8.5.7.3.1" style="font-size:70%;">99.8</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.7.4"><span class="ltx_text" id="S10.T8.5.7.4.1" style="font-size:70%;">86.1</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.7.5"><span class="ltx_text" id="S10.T8.5.7.5.1" style="font-size:70%;">86.8</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.7.6"><span class="ltx_text" id="S10.T8.5.7.6.1" style="font-size:70%;">99.1</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.7.7"><span class="ltx_text" id="S10.T8.5.7.7.1" style="font-size:70%;">93.5</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.7.8"><span class="ltx_text" id="S10.T8.5.7.8.1" style="font-size:70%;">99.6</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.7.9"><span class="ltx_text" id="S10.T8.5.7.9.1" style="font-size:70%;">99.3</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.7.10"><span class="ltx_text" id="S10.T8.5.7.10.1" style="font-size:70%;">100</span></td>
</tr>
<tr class="ltx_tr" id="S10.T8.5.8">
<td class="ltx_td ltx_align_left" id="S10.T8.5.8.1"><span class="ltx_text" id="S10.T8.5.8.1.1" style="font-size:70%;">Cat</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.8.2"><span class="ltx_text" id="S10.T8.5.8.2.1" style="font-size:70%;">95.1</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.8.3"><span class="ltx_text" id="S10.T8.5.8.3.1" style="font-size:70%;">99.9</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.8.4"><span class="ltx_text" id="S10.T8.5.8.4.1" style="font-size:70%;">66.2</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.8.5"><span class="ltx_text" id="S10.T8.5.8.5.1" style="font-size:70%;">67.3</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.8.6"><span class="ltx_text" id="S10.T8.5.8.6.1" style="font-size:70%;">97.1</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.8.7"><span class="ltx_text" id="S10.T8.5.8.7.1" style="font-size:70%;">71.2</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.8.8"><span class="ltx_text" id="S10.T8.5.8.8.1" style="font-size:70%;">73.3</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.8.9"><span class="ltx_text" id="S10.T8.5.8.9.1" style="font-size:70%;">94.8</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.8.10"><span class="ltx_text" id="S10.T8.5.8.10.1" style="font-size:70%;">100</span></td>
</tr>
<tr class="ltx_tr" id="S10.T8.5.9">
<td class="ltx_td ltx_align_left" id="S10.T8.5.9.1"><span class="ltx_text" id="S10.T8.5.9.1.1" style="font-size:70%;">Drill</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.9.2"><span class="ltx_text" id="S10.T8.5.9.2.1" style="font-size:70%;">98.9</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.9.3"><span class="ltx_text" id="S10.T8.5.9.3.1" style="font-size:70%;">100</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.9.4"><span class="ltx_text" id="S10.T8.5.9.4.1" style="font-size:70%;">82.3</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.9.5"><span class="ltx_text" id="S10.T8.5.9.5.1" style="font-size:70%;">88.7</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.9.6"><span class="ltx_text" id="S10.T8.5.9.6.1" style="font-size:70%;">97.4</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.9.7"><span class="ltx_text" id="S10.T8.5.9.7.1" style="font-size:70%;">95.5</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.9.8"><span class="ltx_text" id="S10.T8.5.9.8.1" style="font-size:70%;">98.6</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.9.9"><span class="ltx_text" id="S10.T8.5.9.9.1" style="font-size:70%;">98.3</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.9.10"><span class="ltx_text" id="S10.T8.5.9.10.1" style="font-size:70%;">100</span></td>
</tr>
<tr class="ltx_tr" id="S10.T8.5.10">
<td class="ltx_td ltx_align_left" id="S10.T8.5.10.1"><span class="ltx_text" id="S10.T8.5.10.1.1" style="font-size:70%;">Duck</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.10.2"><span class="ltx_text" id="S10.T8.5.10.2.1" style="font-size:70%;">79.5</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.10.3"><span class="ltx_text" id="S10.T8.5.10.3.1" style="font-size:70%;">98.4</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.10.4"><span class="ltx_text" id="S10.T8.5.10.4.1" style="font-size:70%;">32.5</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.10.5"><span class="ltx_text" id="S10.T8.5.10.5.1" style="font-size:70%;">54.7</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.10.6"><span class="ltx_text" id="S10.T8.5.10.6.1" style="font-size:70%;">90.3</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.10.7"><span class="ltx_text" id="S10.T8.5.10.7.1" style="font-size:70%;">52.0</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.10.8"><span class="ltx_text" id="S10.T8.5.10.8.1" style="font-size:70%;">63.7</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.10.9"><span class="ltx_text" id="S10.T8.5.10.9.1" style="font-size:70%;">81.4</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.10.10"><span class="ltx_text" id="S10.T8.5.10.10.1" style="font-size:70%;">99.5</span></td>
</tr>
<tr class="ltx_tr" id="S10.T8.5.11">
<td class="ltx_td ltx_align_left" id="S10.T8.5.11.1"><span class="ltx_text" id="S10.T8.5.11.1.1" style="font-size:70%;">EggBox</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.11.2"><span class="ltx_text" id="S10.T8.5.11.2.1" style="font-size:70%;">99.6</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.11.3"><span class="ltx_text" id="S10.T8.5.11.3.1" style="font-size:70%;">100</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.11.4"><span class="ltx_text" id="S10.T8.5.11.4.1" style="font-size:70%;">79.4</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.11.5"><span class="ltx_text" id="S10.T8.5.11.5.1" style="font-size:70%;">94.7</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.11.6"><span class="ltx_text" id="S10.T8.5.11.6.1" style="font-size:70%;">99.6</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.11.7"><span class="ltx_text" id="S10.T8.5.11.7.1" style="font-size:70%;">99.1</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.11.8"><span class="ltx_text" id="S10.T8.5.11.8.1" style="font-size:70%;">100</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.11.9"><span class="ltx_text ltx_font_bold" id="S10.T8.5.11.9.1" style="font-size:70%;">100</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.11.10"><span class="ltx_text" id="S10.T8.5.11.10.1" style="font-size:70%;">100</span></td>
</tr>
<tr class="ltx_tr" id="S10.T8.5.12">
<td class="ltx_td ltx_align_left" id="S10.T8.5.12.1"><span class="ltx_text" id="S10.T8.5.12.1.1" style="font-size:70%;">Glue</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.12.2"><span class="ltx_text" id="S10.T8.5.12.2.1" style="font-size:70%;">99.8</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.12.3"><span class="ltx_text" id="S10.T8.5.12.3.1" style="font-size:70%;">100</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.12.4"><span class="ltx_text" id="S10.T8.5.12.4.1" style="font-size:70%;">63.7</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.12.5"><span class="ltx_text" id="S10.T8.5.12.5.1" style="font-size:70%;">91.9</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.12.6"><span class="ltx_text" id="S10.T8.5.12.6.1" style="font-size:70%;">98.1</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.12.7"><span class="ltx_text" id="S10.T8.5.12.7.1" style="font-size:70%;">87.6</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.12.8"><span class="ltx_text" id="S10.T8.5.12.8.1" style="font-size:70%;">95.0</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.12.9"><span class="ltx_text" id="S10.T8.5.12.9.1" style="font-size:70%;">98.1</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.12.10"><span class="ltx_text" id="S10.T8.5.12.10.1" style="font-size:70%;">99.9</span></td>
</tr>
<tr class="ltx_tr" id="S10.T8.5.13">
<td class="ltx_td ltx_align_left" id="S10.T8.5.13.1"><span class="ltx_text" id="S10.T8.5.13.1.1" style="font-size:70%;">Holep</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.13.2"><span class="ltx_text" id="S10.T8.5.13.2.1" style="font-size:70%;">72.3</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.13.3"><span class="ltx_text" id="S10.T8.5.13.3.1" style="font-size:70%;">99.8</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.13.4"><span class="ltx_text" id="S10.T8.5.13.4.1" style="font-size:70%;">56.4</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.13.5"><span class="ltx_text" id="S10.T8.5.13.5.1" style="font-size:70%;">75.4</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.13.6"><span class="ltx_text" id="S10.T8.5.13.6.1" style="font-size:70%;">94.3</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.13.7"><span class="ltx_text" id="S10.T8.5.13.7.1" style="font-size:70%;">67.0</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.13.8"><span class="ltx_text" id="S10.T8.5.13.8.1" style="font-size:70%;">90.1</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.13.9"><span class="ltx_text" id="S10.T8.5.13.9.1" style="font-size:70%;">93.5</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.13.10"><span class="ltx_text" id="S10.T8.5.13.10.1" style="font-size:70%;">99.9</span></td>
</tr>
<tr class="ltx_tr" id="S10.T8.5.14">
<td class="ltx_td ltx_align_left" id="S10.T8.5.14.1"><span class="ltx_text" id="S10.T8.5.14.1.1" style="font-size:70%;">Iron</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.14.2"><span class="ltx_text" id="S10.T8.5.14.2.1" style="font-size:70%;">99.4</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.14.3"><span class="ltx_text" id="S10.T8.5.14.3.1" style="font-size:70%;">99.9</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.14.4"><span class="ltx_text" id="S10.T8.5.14.4.1" style="font-size:70%;">65.1</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.14.5"><span class="ltx_text" id="S10.T8.5.14.5.1" style="font-size:70%;">94.5</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.14.6"><span class="ltx_text" id="S10.T8.5.14.6.1" style="font-size:70%;">98.1</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.14.7"><span class="ltx_text" id="S10.T8.5.14.7.1" style="font-size:70%;">94.2</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.14.8"><span class="ltx_text" id="S10.T8.5.14.8.1" style="font-size:70%;">99.5</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.14.9"><span class="ltx_text" id="S10.T8.5.14.9.1" style="font-size:70%;">99.8</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.14.10"><span class="ltx_text" id="S10.T8.5.14.10.1" style="font-size:70%;">100</span></td>
</tr>
<tr class="ltx_tr" id="S10.T8.5.15">
<td class="ltx_td ltx_align_left" id="S10.T8.5.15.1"><span class="ltx_text" id="S10.T8.5.15.1.1" style="font-size:70%;">Lamp</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.15.2"><span class="ltx_text" id="S10.T8.5.15.2.1" style="font-size:70%;">96.3</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.15.3"><span class="ltx_text" id="S10.T8.5.15.3.1" style="font-size:70%;">99.9</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.15.4"><span class="ltx_text" id="S10.T8.5.15.4.1" style="font-size:70%;">89.4</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.15.5"><span class="ltx_text" id="S10.T8.5.15.5.1" style="font-size:70%;">96.6</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.15.6"><span class="ltx_text" id="S10.T8.5.15.6.1" style="font-size:70%;">97.9</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.15.7"><span class="ltx_text" id="S10.T8.5.15.7.1" style="font-size:70%;">96.9</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.15.8"><span class="ltx_text" id="S10.T8.5.15.8.1" style="font-size:70%;">98.7</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.15.9"><span class="ltx_text" id="S10.T8.5.15.9.1" style="font-size:70%;">99.9</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.15.10"><span class="ltx_text" id="S10.T8.5.15.10.1" style="font-size:70%;">99.9</span></td>
</tr>
<tr class="ltx_tr" id="S10.T8.5.16">
<td class="ltx_td ltx_align_left" id="S10.T8.5.16.1"><span class="ltx_text" id="S10.T8.5.16.1.1" style="font-size:70%;">Phone</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.16.2"><span class="ltx_text" id="S10.T8.5.16.2.1" style="font-size:70%;">96.8</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.16.3"><span class="ltx_text" id="S10.T8.5.16.3.1" style="font-size:70%;">99.7</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.16.4"><span class="ltx_text" id="S10.T8.5.16.4.1" style="font-size:70%;">65.0</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.16.5"><span class="ltx_text" id="S10.T8.5.16.5.1" style="font-size:70%;">89.2</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.16.6"><span class="ltx_text" id="S10.T8.5.16.6.1" style="font-size:70%;">96.4</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.16.7"><span class="ltx_text" id="S10.T8.5.16.7.1" style="font-size:70%;">88.0</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.16.8"><span class="ltx_text" id="S10.T8.5.16.8.1" style="font-size:70%;">90.0</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.16.9"><span class="ltx_text" id="S10.T8.5.16.9.1" style="font-size:70%;">93.9</span></td>
<td class="ltx_td ltx_align_right" id="S10.T8.5.16.10"><span class="ltx_text" id="S10.T8.5.16.10.1" style="font-size:70%;">100</span></td>
</tr>
<tr class="ltx_tr" id="S10.T8.5.17">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S10.T8.5.17.1"><span class="ltx_text" id="S10.T8.5.17.1.1" style="font-size:70%;">Mean</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S10.T8.5.17.2"><span class="ltx_text" id="S10.T8.5.17.2.1" style="font-size:70%;">93.5</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S10.T8.5.17.3"><span class="ltx_text" id="S10.T8.5.17.3.1" style="font-size:70%;">99.7</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S10.T8.5.17.4"><span class="ltx_text" id="S10.T8.5.17.4.1" style="font-size:70%;">65.2</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S10.T8.5.17.5"><span class="ltx_text" id="S10.T8.5.17.5.1" style="font-size:70%;">82.9</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S10.T8.5.17.6"><span class="ltx_text" id="S10.T8.5.17.6.1" style="font-size:70%;">96.6</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S10.T8.5.17.7"><span class="ltx_text" id="S10.T8.5.17.7.1" style="font-size:70%;">83.2</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S10.T8.5.17.8"><span class="ltx_text" id="S10.T8.5.17.8.1" style="font-size:70%;">90.2</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S10.T8.5.17.9"><span class="ltx_text ltx_font_bold" id="S10.T8.5.17.9.1" style="font-size:70%;">94.3</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S10.T8.5.17.10"><span class="ltx_text ltx_font_bold" id="S10.T8.5.17.10.1" style="font-size:70%;">99.8</span></td>
</tr>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S10.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">10.2 </span>T-Less Results</h3>
<div class="ltx_para" id="S10.SS2.p1">
<p class="ltx_p" id="S10.SS2.p1.1">We also add results with RGB-D data with the T-Less dataset to show that using depth in inference helps rectify translation. As we do not change rotation and adjust only translation, this shows that our method is robust in terms of rotation estimation. T-Less Dataset is licensed under a CC BY 4.0 license. Ours results are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S10.T9" title="Table 9 ‣ 10.2 T-Less Results ‣ 10 Additional Results ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_tag">9</span></a></p>
</div>
<figure class="ltx_table" id="S10.T9">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S10.T9.2.1.1" style="font-size:90%;">Table 9</span>: </span><span class="ltx_text" id="S10.T9.3.2" style="font-size:90%;">T-Less Results: Evaluation on the T-Less dataset. NP refers to the NeRF-Pose approach. We also present AR on continuous symmetric(CS) and Discrete Symmetric(DS) objects on both NeRF-Pose(NP) and our approach. CAD refers to the models assuming the presence of a CAD model for training. Ours(D) refers to the results with translation adjusted using an input depth map from the sensor. </span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S10.T9.4">
<tr class="ltx_tr" id="S10.T9.4.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S10.T9.4.1.1"><span class="ltx_text" id="S10.T9.4.1.1.1" style="font-size:70%;">Appr</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S10.T9.4.1.2">
<span class="ltx_text" id="S10.T9.4.1.2.1" style="font-size:70%;">DpodV2</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S10.T9.4.1.2.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib37" title=""><span class="ltx_text" style="font-size:90%;">37</span></a><span class="ltx_text" id="S10.T9.4.1.2.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S10.T9.4.1.3"><span class="ltx_text" id="S10.T9.4.1.3.1" style="font-size:70%;">DpodV2</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S10.T9.4.1.4">
<span class="ltx_text" id="S10.T9.4.1.4.1" style="font-size:70%;">SurfEmb</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S10.T9.4.1.4.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">13</span></a><span class="ltx_text" id="S10.T9.4.1.4.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S10.T9.4.1.5">
<span class="ltx_text" id="S10.T9.4.1.5.1" style="font-size:70%;">NP</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S10.T9.4.1.5.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a><span class="ltx_text" id="S10.T9.4.1.5.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S10.T9.4.1.6"><span class="ltx_text" id="S10.T9.4.1.6.1" style="font-size:70%;">NP(CS)</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S10.T9.4.1.7"><span class="ltx_text" id="S10.T9.4.1.7.1" style="font-size:70%;">Ours(CS)</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S10.T9.4.1.8"><span class="ltx_text" id="S10.T9.4.1.8.1" style="font-size:70%;">NP(DS)</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S10.T9.4.1.9"><span class="ltx_text" id="S10.T9.4.1.9.1" style="font-size:70%;">Ours(DS)</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S10.T9.4.1.10"><span class="ltx_text" id="S10.T9.4.1.10.1" style="font-size:70%;">Ours</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S10.T9.4.1.11"><span class="ltx_text" id="S10.T9.4.1.11.1" style="font-size:70%;">Ours(D)</span></td>
</tr>
<tr class="ltx_tr" id="S10.T9.4.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S10.T9.4.2.1"><span class="ltx_text" id="S10.T9.4.2.1.1" style="font-size:70%;">CAD</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T9.4.2.2"><span class="ltx_text" id="S10.T9.4.2.2.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T9.4.2.3"><span class="ltx_text" id="S10.T9.4.2.3.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T9.4.2.4"><span class="ltx_text" id="S10.T9.4.2.4.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T9.4.2.5"><span class="ltx_text" id="S10.T9.4.2.5.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T9.4.2.6"><span class="ltx_text" id="S10.T9.4.2.6.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T9.4.2.7"><span class="ltx_text" id="S10.T9.4.2.7.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T9.4.2.8"><span class="ltx_text" id="S10.T9.4.2.8.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T9.4.2.9"><span class="ltx_text" id="S10.T9.4.2.9.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T9.4.2.10"><span class="ltx_text" id="S10.T9.4.2.10.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T9.4.2.11"><span class="ltx_text" id="S10.T9.4.2.11.1" style="font-size:70%;">✗</span></td>
</tr>
<tr class="ltx_tr" id="S10.T9.4.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S10.T9.4.3.1"><span class="ltx_text" id="S10.T9.4.3.1.1" style="font-size:70%;">VSD</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T9.4.3.2"><span class="ltx_text" id="S10.T9.4.3.2.1" style="font-size:70%;">0.57</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T9.4.3.3"><span class="ltx_text" id="S10.T9.4.3.3.1" style="font-size:70%;">0.46</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T9.4.3.4"><span class="ltx_text" id="S10.T9.4.3.4.1" style="font-size:70%;">0.5</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T9.4.3.5"><span class="ltx_text" id="S10.T9.4.3.5.1" style="font-size:70%;">0.45</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T9.4.3.6"><span class="ltx_text" id="S10.T9.4.3.6.1" style="font-size:70%;">0.378</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T9.4.3.7"><span class="ltx_text" id="S10.T9.4.3.7.1" style="font-size:70%;">0.42</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T9.4.3.8"><span class="ltx_text" id="S10.T9.4.3.8.1" style="font-size:70%;">0.51</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T9.4.3.9"><span class="ltx_text" id="S10.T9.4.3.9.1" style="font-size:70%;">0.56</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T9.4.3.10"><span class="ltx_text" id="S10.T9.4.3.10.1" style="font-size:70%;">0.5</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S10.T9.4.3.11"><span class="ltx_text" id="S10.T9.4.3.11.1" style="font-size:70%;">0.58</span></td>
</tr>
<tr class="ltx_tr" id="S10.T9.4.4">
<td class="ltx_td ltx_align_left" id="S10.T9.4.4.1"><span class="ltx_text" id="S10.T9.4.4.1.1" style="font-size:70%;">MSSD</span></td>
<td class="ltx_td ltx_align_right" id="S10.T9.4.4.2"><span class="ltx_text" id="S10.T9.4.4.2.1" style="font-size:70%;">0.62</span></td>
<td class="ltx_td ltx_align_right" id="S10.T9.4.4.3"><span class="ltx_text" id="S10.T9.4.4.3.1" style="font-size:70%;">0.49</span></td>
<td class="ltx_td ltx_align_right" id="S10.T9.4.4.4"><span class="ltx_text" id="S10.T9.4.4.4.1" style="font-size:70%;">0.53</span></td>
<td class="ltx_td ltx_align_right" id="S10.T9.4.4.5"><span class="ltx_text" id="S10.T9.4.4.5.1" style="font-size:70%;">0.49</span></td>
<td class="ltx_td ltx_align_right" id="S10.T9.4.4.6"><span class="ltx_text" id="S10.T9.4.4.6.1" style="font-size:70%;">0.42</span></td>
<td class="ltx_td ltx_align_right" id="S10.T9.4.4.7"><span class="ltx_text" id="S10.T9.4.4.7.1" style="font-size:70%;">0.462</span></td>
<td class="ltx_td ltx_align_right" id="S10.T9.4.4.8"><span class="ltx_text" id="S10.T9.4.4.8.1" style="font-size:70%;">0.56</span></td>
<td class="ltx_td ltx_align_right" id="S10.T9.4.4.9"><span class="ltx_text" id="S10.T9.4.4.9.1" style="font-size:70%;">0.6</span></td>
<td class="ltx_td ltx_align_right" id="S10.T9.4.4.10"><span class="ltx_text" id="S10.T9.4.4.10.1" style="font-size:70%;">0.54</span></td>
<td class="ltx_td ltx_align_right" id="S10.T9.4.4.11"><span class="ltx_text" id="S10.T9.4.4.11.1" style="font-size:70%;">0.66</span></td>
</tr>
<tr class="ltx_tr" id="S10.T9.4.5">
<td class="ltx_td ltx_align_left" id="S10.T9.4.5.1"><span class="ltx_text" id="S10.T9.4.5.1.1" style="font-size:70%;">MSPD</span></td>
<td class="ltx_td ltx_align_right" id="S10.T9.4.5.2"><span class="ltx_text" id="S10.T9.4.5.2.1" style="font-size:70%;">0.76</span></td>
<td class="ltx_td ltx_align_right" id="S10.T9.4.5.3"><span class="ltx_text" id="S10.T9.4.5.3.1" style="font-size:70%;">0.59</span></td>
<td class="ltx_td ltx_align_right" id="S10.T9.4.5.4"><span class="ltx_text" id="S10.T9.4.5.4.1" style="font-size:70%;">0.83</span></td>
<td class="ltx_td ltx_align_right" id="S10.T9.4.5.5"><span class="ltx_text" id="S10.T9.4.5.5.1" style="font-size:70%;">0.66</span></td>
<td class="ltx_td ltx_align_right" id="S10.T9.4.5.6"><span class="ltx_text" id="S10.T9.4.5.6.1" style="font-size:70%;">0.672</span></td>
<td class="ltx_td ltx_align_right" id="S10.T9.4.5.7"><span class="ltx_text" id="S10.T9.4.5.7.1" style="font-size:70%;">0.693</span></td>
<td class="ltx_td ltx_align_right" id="S10.T9.4.5.8"><span class="ltx_text" id="S10.T9.4.5.8.1" style="font-size:70%;">0.65</span></td>
<td class="ltx_td ltx_align_right" id="S10.T9.4.5.9"><span class="ltx_text" id="S10.T9.4.5.9.1" style="font-size:70%;">0.69</span></td>
<td class="ltx_td ltx_align_right" id="S10.T9.4.5.10"><span class="ltx_text" id="S10.T9.4.5.10.1" style="font-size:70%;">0.7</span></td>
<td class="ltx_td ltx_align_right" id="S10.T9.4.5.11"><span class="ltx_text" id="S10.T9.4.5.11.1" style="font-size:70%;">0.69</span></td>
</tr>
<tr class="ltx_tr" id="S10.T9.4.6">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S10.T9.4.6.1"><span class="ltx_text" id="S10.T9.4.6.1.1" style="font-size:70%;">AR</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S10.T9.4.6.2"><span class="ltx_text" id="S10.T9.4.6.2.1" style="font-size:70%;">0.65</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S10.T9.4.6.3"><span class="ltx_text" id="S10.T9.4.6.3.1" style="font-size:70%;">0.51</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S10.T9.4.6.4"><span class="ltx_text" id="S10.T9.4.6.4.1" style="font-size:70%;">0.62</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S10.T9.4.6.5"><span class="ltx_text" id="S10.T9.4.6.5.1" style="font-size:70%;">0.54</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S10.T9.4.6.6"><span class="ltx_text" id="S10.T9.4.6.6.1" style="font-size:70%;">0.48</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S10.T9.4.6.7"><span class="ltx_text ltx_font_bold" id="S10.T9.4.6.7.1" style="font-size:70%;">0.52</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S10.T9.4.6.8"><span class="ltx_text" id="S10.T9.4.6.8.1" style="font-size:70%;">0.58</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S10.T9.4.6.9"><span class="ltx_text ltx_font_bold" id="S10.T9.4.6.9.1" style="font-size:70%;">0.63</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S10.T9.4.6.10"><span class="ltx_text ltx_font_bold" id="S10.T9.4.6.10.1" style="font-size:70%;">0.58</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S10.T9.4.6.11"><span class="ltx_text" id="S10.T9.4.6.11.1" style="font-size:70%;">0.65</span></td>
</tr>
</table>
</figure>
<figure class="ltx_figure" id="S10.F8">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S10.F8.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="617" id="S10.F8.1.g1" src="extracted/5679340/images/RGB9.jpg" width="617"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S10.F8.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="617" id="S10.F8.2.g1" src="extracted/5679340/images/cnnFEAT9.jpg" width="617"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S10.F8.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="617" id="S10.F8.3.g1" src="extracted/5679340/images/rendFeat9.jpg" width="617"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S10.F8.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="617" id="S10.F8.4.g1" src="extracted/5679340/images/RGB13.jpg" width="617"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S10.F8.5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="617" id="S10.F8.5.g1" src="extracted/5679340/images/cnnFEAT13.jpg" width="617"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S10.F8.6"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="617" id="S10.F8.6.g1" src="extracted/5679340/images/rendFeat13.jpg" width="617"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S10.F8.7"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="617" id="S10.F8.7.g1" src="extracted/5679340/images/RGB15.jpg" width="617"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S10.F8.8"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="617" id="S10.F8.8.g1" src="extracted/5679340/images/cnnFEAT15.jpg" width="617"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S10.F8.9"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="617" id="S10.F8.9.g1" src="extracted/5679340/images/rendFeat15.jpg" width="617"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S10.F8.10"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="617" id="S10.F8.10.g1" src="extracted/5679340/images/RGB12.jpg" width="617"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S10.F8.11"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="617" id="S10.F8.11.g1" src="extracted/5679340/images/cnnFEAT12.jpg" width="617"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S10.F8.12"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="617" id="S10.F8.12.g1" src="extracted/5679340/images/rendFeat12.jpg" width="617"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S10.F8.14.1.1" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text" id="S10.F8.15.2" style="font-size:90%;">Visualization of learned feature representation of Linemod objects. The first image in each row is the input RGB image and the second image is the output feature image from CNN and the third image is the feature image rendered from NeRF at the given pose. Visualizations are from objects, Duck, Ironbox, Phone and Holepunch in Linemod dataset.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S10.F9">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S10.F9.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="617" id="S10.F9.1.g1" src="extracted/5679340/images/TLRGB5.jpg" width="617"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S10.F9.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="617" id="S10.F9.2.g1" src="extracted/5679340/images/TLcnnFEAT5.jpg" width="617"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S10.F9.3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="617" id="S10.F9.3.g1" src="extracted/5679340/images/TLrendFeat5.jpg" width="617"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S10.F9.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="617" id="S10.F9.4.g1" src="extracted/5679340/images/TLRGB6.jpg" width="617"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S10.F9.5"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="617" id="S10.F9.5.g1" src="extracted/5679340/images/TLcnnFEAT6.jpg" width="617"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S10.F9.6"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="617" id="S10.F9.6.g1" src="extracted/5679340/images/TLrendFeat6.jpg" width="617"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S10.F9.7"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="617" id="S10.F9.7.g1" src="extracted/5679340/images/TLRGB7.jpg" width="617"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S10.F9.8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="617" id="S10.F9.8.g1" src="extracted/5679340/images/TLcnnFEAT7.jpg" width="617"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S10.F9.9"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="617" id="S10.F9.9.g1" src="extracted/5679340/images/TLrendFeat7.jpg" width="617"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S10.F9.10"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="617" id="S10.F9.10.g1" src="extracted/5679340/images/TLRGB9.jpg" width="617"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S10.F9.11"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="617" id="S10.F9.11.g1" src="extracted/5679340/images/TLcnnFEAT9.jpg" width="617"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S10.F9.12"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="617" id="S10.F9.12.g1" src="extracted/5679340/images/TLrendFeat9.jpg" width="617"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S10.F9.14.1.1" style="font-size:90%;">Figure 9</span>: </span><span class="ltx_text" id="S10.F9.15.2" style="font-size:90%;">Visualization of learned feature representation of Discrete Symmetric objects in T-Less objects. The first image in each row is the input RGB image and the second image is the output feature image from CNN and the third image is the feature image rendered from NeRF at the given pose. Visualizations are from objects 5, 6, 7, 9 in T-Less dataset.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S10.F10">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S10.F10.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="617" id="S10.F10.1.g1" src="extracted/5679340/images/TLRGB14.jpg" width="617"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S10.F10.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="617" id="S10.F10.2.g1" src="extracted/5679340/images/TLcnnFEAT14.jpg" width="617"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S10.F10.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="617" id="S10.F10.3.g1" src="extracted/5679340/images/TLrendFeat14.jpg" width="617"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S10.F10.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="617" id="S10.F10.4.g1" src="extracted/5679340/images/TLRGB16.jpg" width="617"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S10.F10.5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="617" id="S10.F10.5.g1" src="extracted/5679340/images/TLcnnFEAT16.jpg" width="617"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S10.F10.6"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="617" id="S10.F10.6.g1" src="extracted/5679340/images/TLrendFeat16.jpg" width="617"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S10.F10.7"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="617" id="S10.F10.7.g1" src="extracted/5679340/images/TLRGB3.jpg" width="617"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S10.F10.8"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="617" id="S10.F10.8.g1" src="extracted/5679340/images/TLcnnFEAT3.jpg" width="617"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S10.F10.9"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="617" id="S10.F10.9.g1" src="extracted/5679340/images/TLrendFeat3.jpg" width="617"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S10.F10.10"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="617" id="S10.F10.10.g1" src="extracted/5679340/images/TLRGB30.jpg" width="617"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S10.F10.11"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="617" id="S10.F10.11.g1" src="extracted/5679340/images/TLcnnFEAT30.jpg" width="617"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S10.F10.12"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="617" id="S10.F10.12.g1" src="extracted/5679340/images/TLrendFeat30.jpg" width="617"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S10.F10.14.1.1" style="font-size:90%;">Figure 10</span>: </span><span class="ltx_text" id="S10.F10.15.2" style="font-size:90%;">Visualization of learned feature representation of Continuous Symmetric objects in T-Less objects. The first image in each row is the input RGB image and the second image is the output feature image from CNN and the third image is the feature image rendered from NeRF at the given pose. Visualizations are from objects 14,16,3,30 in T-Less dataset.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S10.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">10.3 </span>Qualitative Results</h3>
<div class="ltx_para" id="S10.SS3.p1">
<p class="ltx_p" id="S10.SS3.p1.1">We present some qualitative results and learned feature maps in NeRF renderings and predicted feature maps from CNN in Figures <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S10.F8" title="Figure 8 ‣ 10.2 T-Less Results ‣ 10 Additional Results ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_tag">8</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S10.F9" title="Figure 9 ‣ 10.2 T-Less Results ‣ 10 Additional Results ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_tag">9</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.13796v1#S10.F10" title="Figure 10 ‣ 10.2 T-Less Results ‣ 10 Additional Results ‣ NeRF-Feat: 6D Object Pose Estimation using Feature Rendering"><span class="ltx_text ltx_ref_tag">10</span></a> in this section.</p>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib1.5.5.1" style="font-size:90%;">Arun et al. [1987]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.7.1" style="font-size:90%;">
K. S. Arun, T. S. Huang, and S. D. Blostein.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.8.1" style="font-size:90%;">Least-squares fitting of two 3-d point sets.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.9.1" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</em><span class="ltx_text" id="bib.bib1.10.2" style="font-size:90%;">, 1987.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib2.5.5.1" style="font-size:90%;">Brachmann et al. [2014]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.7.1" style="font-size:90%;">
Eric Brachmann, Alexander Krull, Frank Michel, Stefan Gumhold, Jamie Shotton, and Carsten Rother.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.8.1" style="font-size:90%;">Learning 6d object pose estimation using 3d object coordinates.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib2.10.2" style="font-size:90%;">ECCV</em><span class="ltx_text" id="bib.bib2.11.3" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib3.5.5.1" style="font-size:90%;">Busam et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.7.1" style="font-size:90%;">
Benjamin Busam, Hyun Jun Jung, and Nassir Navab.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.8.1" style="font-size:90%;">I like to move it: 6d pose estimation as an action decision process.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.9.1" style="font-size:90%;">arXiv preprint arXiv:2009.12678</em><span class="ltx_text" id="bib.bib3.10.2" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib4.5.5.1" style="font-size:90%;">Buslaev et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.7.1" style="font-size:90%;">
Alexander Buslaev, Vladimir I. Iglovikov, Eugene Khvedchenya, Alex Parinov, Mikhail Druzhinin, and Alexandr A. Kalinin.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.8.1" style="font-size:90%;">Albumentations: Fast and flexible image augmentations.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.9.1" style="font-size:90%;">Information</em><span class="ltx_text" id="bib.bib4.10.2" style="font-size:90%;">, 11(2), 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib5.5.5.1" style="font-size:90%;">Cai et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.7.1" style="font-size:90%;">
Dingding Cai, Janne Heikkilä, and Esa Rahtu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.8.1" style="font-size:90%;">Ove6d: Object viewpoint encoding for depth-based 6d object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib5.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib5.11.3" style="font-size:90%;">, pages 6803–6813, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib6.4.4.1" style="font-size:90%;">Cai and Reid [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.6.1" style="font-size:90%;">
Ming Cai and Ian Reid.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.7.1" style="font-size:90%;">Reconstruct locally, localize globally: A model free method for object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.8.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib6.9.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span class="ltx_text" id="bib.bib6.10.3" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib7.5.5.1" style="font-size:90%;">Caron et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.7.1" style="font-size:90%;">
Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.8.1" style="font-size:90%;">Emerging properties in self-supervised vision transformers.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib7.10.2" style="font-size:90%;">Proceedings of the International Conference on Computer Vision (ICCV)</em><span class="ltx_text" id="bib.bib7.11.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib8.5.5.1" style="font-size:90%;">Chen et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.7.1" style="font-size:90%;">
Hanzhi Chen, Fabian Manhardt, Nassir Navab, and Benjamin Busam.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.8.1" style="font-size:90%;">Texpose: Neural texture learning for self-supervised 6d object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib8.10.2" style="font-size:90%;">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span class="ltx_text" id="bib.bib8.11.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib9.5.5.1" style="font-size:90%;">Chen et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.7.1" style="font-size:90%;">
Wei Chen, Xi Jia, Hyung Jin Chang, Jinming Duan, and Ales Leonardis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.8.1" style="font-size:90%;">G2l-net: Global to local network for real-time 6d pose estimation with embedding vector features.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib9.10.2" style="font-size:90%;">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span class="ltx_text" id="bib.bib9.11.3" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib10.5.5.1" style="font-size:90%;">Denninger et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.7.1" style="font-size:90%;">
Maximilian Denninger, Martin Sundermeyer, Dominik Winkelbauer, Youssef Zidan, Dmitry Olefir, Mohamad Elbadrawy, Ahsan Lodhi, and Harinandan Katam.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.8.1" style="font-size:90%;">Blenderproc.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.9.1" style="font-size:90%;">arXiv preprint arXiv:1911.01911</em><span class="ltx_text" id="bib.bib10.10.2" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib11.5.5.1" style="font-size:90%;">Denninger et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.7.1" style="font-size:90%;">
Maximilian Denninger, Dominik Winkelbauer, Martin Sundermeyer, Wout Boerdijk, Markus Knauer, Klaus H. Strobl, Matthias Humt, and Rudolph Triebel.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.8.1" style="font-size:90%;">Blenderproc2: A procedural pipeline for photorealistic rendering.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.9.1" style="font-size:90%;">Journal of Open Source Software</em><span class="ltx_text" id="bib.bib11.10.2" style="font-size:90%;">, 8(82):4901, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib12.5.5.1" style="font-size:90%;">Fang et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.7.1" style="font-size:90%;">
Hao-Shu Fang, Chenxi Wang, Minghao Gou, and Cewu Lu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.8.1" style="font-size:90%;">Graspnet-1billion: A large-scale benchmark for general object grasping.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib12.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em><span class="ltx_text" id="bib.bib12.11.3" style="font-size:90%;">, pages 11444–11453, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib13.4.4.1" style="font-size:90%;">Haugaard and Buch [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.6.1" style="font-size:90%;">
R. Haugaard and A. Buch.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.7.1" style="font-size:90%;">Surfemb: Dense and continuous correspondence distributions for object pose estimation with learnt surface embeddings.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.8.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib13.9.2" style="font-size:90%;">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span class="ltx_text" id="bib.bib13.10.3" style="font-size:90%;">, pages 6739–6748, Los Alamitos, CA, USA, 2022. IEEE Computer Society.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib14.5.5.1" style="font-size:90%;">He et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.7.1" style="font-size:90%;">
Xingyi He, Jiaming Sun, Yuang Wang, Di Huang, Hujun Bao, and Xiaowei Zhou.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.8.1" style="font-size:90%;">Onepose++: Keypoint-free one-shot object pose estimation without CAD models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib14.10.2" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span class="ltx_text" id="bib.bib14.11.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib15.5.5.1" style="font-size:90%;">He et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.7.1" style="font-size:90%;">
Yisheng He, Wei Sun, Haibin Huang, Jianran Liu, Haoqiang Fan, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.8.1" style="font-size:90%;">Pvn3d: A deep point-wise 3d keypoints voting network for 6dof pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib15.10.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib15.11.3" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib16.5.5.1" style="font-size:90%;">He et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.7.1" style="font-size:90%;">
Yisheng He, Haibin Huang, Haoqiang Fan, Qifeng Chen, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.8.1" style="font-size:90%;">Ffb6d: A full flow bidirectional fusion network for 6d pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib16.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib16.11.3" style="font-size:90%;">, pages 3003–3013, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib17.5.5.1" style="font-size:90%;">Hinterstoisser et al. [2012]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.7.1" style="font-size:90%;">
S. Hinterstoisser, V. Lepetit, S. Ilic, S. Holzer, G. Bradski, K. Konolige, , and N. Navab.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.8.1" style="font-size:90%;">Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib17.10.2" style="font-size:90%;">Asian Conference on Computer Vision</em><span class="ltx_text" id="bib.bib17.11.3" style="font-size:90%;">, 2012.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib18.4.4.1" style="font-size:90%;">Hodan and Melenovsky [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.6.1" style="font-size:90%;">
Tomas Hodan and Antonin Melenovsky.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.7.1" style="font-size:90%;">Bop: Benchmark for 6d object pose estimation: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://bop.felk.cvut.cz/home/" style="font-size:90%;" title="">https://bop.felk.cvut.cz/home/</a><span class="ltx_text" id="bib.bib18.8.2" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib19.5.5.1" style="font-size:90%;">Hodaň et al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.7.1" style="font-size:90%;">
Tomáš Hodaň, Pavel Haluza, Štěpán Obdržálek, Jiří Matas, Manolis Lourakis, and Xenophon Zabulis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.8.1" style="font-size:90%;">T-LESS: An RGB-D dataset for 6D pose estimation of texture-less objects.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.9.1" style="font-size:90%;">IEEE Winter Conference on Applications of Computer Vision (WACV)</em><span class="ltx_text" id="bib.bib19.10.2" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib20.5.5.1" style="font-size:90%;">Jung et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.7.1" style="font-size:90%;">
HyunJun Jung, Shun-Cheng Wu, Patrick Ruhkamp, Hannah Schieber, Pengyuan Wang, Giulia Rizzoli, Hongcheng Zhao, Sven Damian Meier, Daniel Roth, Nassir Navab, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.8.1" style="font-size:90%;">Housecat6d–a large-scale multi-modal category level 6d object pose dataset with household objects in realistic scenarios.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.9.1" style="font-size:90%;">arXiv preprint arXiv:2212.10428</em><span class="ltx_text" id="bib.bib20.10.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib21.5.5.1" style="font-size:90%;">Karnati et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.7.1" style="font-size:90%;">
Mohan Karnati, Ayan Seal, Anis Yazidi, and Ondrej Krejcar.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.8.1" style="font-size:90%;">Lienet: A deep convolution neural network framework for detecting deception.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.9.1" style="font-size:90%;">IEEE Transactions on Cognitive and Developmental Systems</em><span class="ltx_text" id="bib.bib21.10.2" style="font-size:90%;">, 14(3):971–984, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib22.5.5.1" style="font-size:90%;">Kirillov et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.7.1" style="font-size:90%;">
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.8.1" style="font-size:90%;">Segment anything.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.9.1" style="font-size:90%;">arXiv:2304.02643</em><span class="ltx_text" id="bib.bib22.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib23.5.5.1" style="font-size:90%;">Kobayashi et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.7.1" style="font-size:90%;">
Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitzmann.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.8.1" style="font-size:90%;">Decomposing nerf for editing via feature field distillation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib23.10.2" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span class="ltx_text" id="bib.bib23.11.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib24.5.5.1" style="font-size:90%;">Labbe et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.7.1" style="font-size:90%;">
Y. Labbe, J. Carpentier, M. Aubry, and J. Sivic.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.8.1" style="font-size:90%;">Cosypose: Consistent multi-view multi-object 6d pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib24.10.2" style="font-size:90%;">Proceedings of the European Conference on Computer Vision (ECCV)</em><span class="ltx_text" id="bib.bib24.11.3" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib25.5.5.1" style="font-size:90%;">Li et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.7.1" style="font-size:90%;">
Fu Li, Shishir Reddy Vutukur, Hao Yu, Ivan Shugurov, Benjamin Busam, Shaowu Yang, and Slobodan Ilic.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.8.1" style="font-size:90%;">Nerf-pose: A first-reconstruct-then-regress approach for weakly-supervised 6d object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib25.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span class="ltx_text" id="bib.bib25.11.3" style="font-size:90%;">, pages 2123–2133, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib26.5.5.1" style="font-size:90%;">Lin et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.7.1" style="font-size:90%;">
Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Simon Lucey.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.8.1" style="font-size:90%;">Barf: Bundle-adjusting neural radiance fields.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib26.10.2" style="font-size:90%;">IEEE International Conference on Computer Vision (ICCV)</em><span class="ltx_text" id="bib.bib26.11.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib27.5.5.1" style="font-size:90%;">Lin et al. [2014]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.7.1" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.8.1" style="font-size:90%;">Microsoft coco: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib27.10.2" style="font-size:90%;">Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13</em><span class="ltx_text" id="bib.bib27.11.3" style="font-size:90%;">, pages 740–755. Springer, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib28.5.5.1" style="font-size:90%;">Liu et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.7.1" style="font-size:90%;">
Shichen Liu, Tianye Li, Weikai Chen, and Hao Li.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.8.1" style="font-size:90%;">Soft rasterizer: A differentiable renderer for image-based 3d reasoning.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.9.1" style="font-size:90%;">The IEEE International Conference on Computer Vision (ICCV)</em><span class="ltx_text" id="bib.bib28.10.2" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib29.5.5.1" style="font-size:90%;">Marion et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.7.1" style="font-size:90%;">
Pat Marion, Peter R Florence, Lucas Manuelli, and Russ Tedrake.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.8.1" style="font-size:90%;">Label fusion: A pipeline for generating ground truth labels for real rgbd data of cluttered scenes.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib29.10.2" style="font-size:90%;">2018 IEEE International Conference on Robotics and Automation (ICRA)</em><span class="ltx_text" id="bib.bib29.11.3" style="font-size:90%;">, pages 3235–3242. IEEE, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib30.5.5.1" style="font-size:90%;">Mildenhall et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.7.1" style="font-size:90%;">
Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.8.1" style="font-size:90%;">Nerf: Representing scenes as neural radiance fields for view synthesis.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.9.1" style="font-size:90%;">CoRR</em><span class="ltx_text" id="bib.bib30.10.2" style="font-size:90%;">, abs/2003.08934, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib31.5.5.1" style="font-size:90%;">Neverova et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.7.1" style="font-size:90%;">
Natalia Neverova, David Novotný, Vasil Khalidov, Marc Szafraniec, Patrick Labatut, and Andrea Vedaldi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.8.1" style="font-size:90%;">Continuous surface embeddings.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.9.1" style="font-size:90%;">CoRR</em><span class="ltx_text" id="bib.bib31.10.2" style="font-size:90%;">, abs/2011.12438, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib32.5.5.1" style="font-size:90%;">Park et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.7.1" style="font-size:90%;">
Kiru Park, Timothy Patten, and Markus Vincze.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.8.1" style="font-size:90%;">Pix2pose: Pix2pose: Pixel-wise coordinate regression of objects for 6d pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib32.10.2" style="font-size:90%;">The IEEE International Conference on Computer Vision (ICCV)</em><span class="ltx_text" id="bib.bib32.11.3" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib33.5.5.1" style="font-size:90%;">Park et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.7.1" style="font-size:90%;">
Keunhong Park, Arsalan Mousavian, Yu Xiang, and Dieter Fox.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.8.1" style="font-size:90%;">Latentfusion: End-to-end differentiable reconstruction and rendering for unseen object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib33.10.2" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib33.11.3" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib34.5.5.1" style="font-size:90%;">Ravi et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.7.1" style="font-size:90%;">
Nikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor Gordon, Wan-Yen Lo, Justin Johnson, and Georgia Gkioxari.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.8.1" style="font-size:90%;">Accelerating 3d deep learning with pytorch3d.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.9.1" style="font-size:90%;">arXiv:2007.08501</em><span class="ltx_text" id="bib.bib34.10.2" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib35.5.5.1" style="font-size:90%;">Ronneberger et al. [2015]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.7.1" style="font-size:90%;">
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.8.1" style="font-size:90%;">U-net: Convolutional networks for biomedical image segmentation, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib36.4.4.1" style="font-size:90%;">Schönberger and Frahm [2016]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.6.1" style="font-size:90%;">
Johannes Lutz Schönberger and Jan-Michael Frahm.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.7.1" style="font-size:90%;">Structure-from-motion revisited.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.8.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib36.9.2" style="font-size:90%;">Conference on Computer Vision and Pattern Recognition (CVPR)</em><span class="ltx_text" id="bib.bib36.10.3" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib37.5.5.1" style="font-size:90%;">Shugurov et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.7.1" style="font-size:90%;">
Ivan Shugurov, Sergey Zakharov, and Slobodan Ilic.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.8.1" style="font-size:90%;">Dpodv2: Dense correspondence-based 6 dof pose estimation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.9.1" style="font-size:90%;">IEEE transactions on pattern analysis and machine intelligence</em><span class="ltx_text" id="bib.bib37.10.2" style="font-size:90%;">, PP, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib38.5.5.1" style="font-size:90%;">Shugurov et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.7.1" style="font-size:90%;">
Ivan Shugurov, Fu Li, Benjamin Busam, and Slobodan Ilic.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.8.1" style="font-size:90%;">Osop: A multi-stage one shot object pose estimation framework, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib39.5.5.1" style="font-size:90%;">Sitzmann et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.7.1" style="font-size:90%;">
Vincent Sitzmann, Julien N.P. Martel, Alexander W. Bergman, David B. Lindell, and Gordon Wetzstein.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.8.1" style="font-size:90%;">Implicit neural representations with periodic activation functions.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib39.10.2" style="font-size:90%;">Proc. NeurIPS</em><span class="ltx_text" id="bib.bib39.11.3" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib40.5.5.1" style="font-size:90%;">Su et al. [2022a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.7.1" style="font-size:90%;">
Yongzhi Su, Mahdi Saleh, Torben Fetzer, Jason Rambach, Nassir Navab, Benjamin Busam, Didier Stricker, and Federico Tombari.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.8.1" style="font-size:90%;">Zebrapose: Coarse to fine surface encoding for 6dof object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib40.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib40.11.3" style="font-size:90%;">, pages 6738–6748, 2022a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib41.5.5.1" style="font-size:90%;">Su et al. [2022b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.7.1" style="font-size:90%;">
Yongzhi Su, Mahdi Saleh, Torben Fetzer, Jason Rambach, Nassir Navab, Benjamin Busam, Didier Stricker, and Federico Tombari.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.8.1" style="font-size:90%;">Zebrapose: Coarse to fine surface encoding for 6dof object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.9.1" style="font-size:90%;">arXiv preprint arXiv:2203.09418</em><span class="ltx_text" id="bib.bib41.10.2" style="font-size:90%;">, 2022b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib42.5.5.1" style="font-size:90%;">Sun et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.7.1" style="font-size:90%;">
Jiaming Sun, Zihao Wang, Siyu Zhang, Xingyi He, Hongcheng Zhao, Guofeng Zhang, and Xiaowei Zhou.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.8.1" style="font-size:90%;">OnePose: One-shot object pose estimation without CAD models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.9.1" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib42.10.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib43.5.5.1" style="font-size:90%;">Tschernezki et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.7.1" style="font-size:90%;">
Vadim Tschernezki, Iro Laina, Diane Larlus, and Andrea Vedaldi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.8.1" style="font-size:90%;">Neural feature fusion fields: 3D distillation of self-supervised 2D image representations.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib43.10.2" style="font-size:90%;">Proceedings of the International Conference on 3D Vision (3DV)</em><span class="ltx_text" id="bib.bib43.11.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib44.5.5.1" style="font-size:90%;">van den Oord et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.7.1" style="font-size:90%;">
Aaron van den Oord, Yazhe Li, and Oriol Vinyals.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.8.1" style="font-size:90%;">Representation learning with contrastive predictive coding, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib45.5.5.1" style="font-size:90%;">Vutukur et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.7.1" style="font-size:90%;">
Shishir Reddy Vutukur, Ivan Shugurov, Benjamin Busam, Andreas Hutter, and Slobodan Ilic.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.8.1" style="font-size:90%;">Welsa: Learning to predict 6d pose from weakly labeled data using shape alignment.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib45.10.2" style="font-size:90%;">Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part VIII</em><span class="ltx_text" id="bib.bib45.11.3" style="font-size:90%;">, pages 645–661. Springer, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib46.5.5.1" style="font-size:90%;">Wang et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.7.1" style="font-size:90%;">
Chen Wang, Danfei Xu, Yuke Zhu, Roberto Martín-Martín, Cewu Lu, Li Fei-Fei, and Silvio Savarese.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.8.1" style="font-size:90%;">Densefusion: 6d object pose estimation by iterative dense fusion.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib46.10.2" style="font-size:90%;">Computer Vision and Pattern Recognition (CVPR)</em><span class="ltx_text" id="bib.bib46.11.3" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib47.5.5.1" style="font-size:90%;">Wang et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.7.1" style="font-size:90%;">
Gu Wang, Fabian Manhardt, Jianzhun Shao, Xiangyang Ji, Nassir Navab, and Federico Tombari.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.8.1" style="font-size:90%;">Self6d: Self-supervised monocular 6d object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib47.10.2" style="font-size:90%;">Computer Vision – ECCV 2020</em><span class="ltx_text" id="bib.bib47.11.3" style="font-size:90%;">, pages 108–125, Cham, 2020. Springer International Publishing.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib48.5.5.1" style="font-size:90%;">Wang et al. [2021a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.7.1" style="font-size:90%;">
Gu Wang, Fabian Manhardt, Federico Tombari, and Xiangyang Ji.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.8.1" style="font-size:90%;">GDR-Net: Geometry-guided direct regression network for monocular 6d object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib48.10.2" style="font-size:90%;">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span class="ltx_text" id="bib.bib48.11.3" style="font-size:90%;">, pages 16611–16621, 2021a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib49.5.5.1" style="font-size:90%;">Wang et al. [2021b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.7.1" style="font-size:90%;">
Pengyuan Wang, Fabian Manhardt, Luca Minciullo, Lorenzo Garattoni, Sven Meier, Nassir Navab, and Benjamin Busam.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.8.1" style="font-size:90%;">Demograsp: Few-shot learning for robotic grasping with human demonstration.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib49.10.2" style="font-size:90%;">2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em><span class="ltx_text" id="bib.bib49.11.3" style="font-size:90%;">, pages 5733–5740. IEEE, 2021b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib50.5.5.1" style="font-size:90%;">Wang et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.7.1" style="font-size:90%;">
Pengyuan Wang, HyunJun Jung, Yitong Li, Siyuan Shen, Rahul Parthasarathy Srikanth, Lorenzo Garattoni, Sven Meier, Nassir Navab, and Benjamin Busam.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.8.1" style="font-size:90%;">Phocal: A multi-modal dataset for category-level object pose estimation with photometrically challenging objects.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib50.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib50.11.3" style="font-size:90%;">, pages 21222–21231, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib51.5.5.1" style="font-size:90%;">Xiang et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib51.7.1" style="font-size:90%;">
Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, and Dieter Fox.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib51.8.1" style="font-size:90%;">Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.9.1" style="font-size:90%;">Robotics: Science and Systems (RSS)</em><span class="ltx_text" id="bib.bib51.10.2" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib52.5.5.1" style="font-size:90%;">Yen-Chen et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib52.7.1" style="font-size:90%;">
Lin Yen-Chen, Pete Florence, Jonathan T. Barron, Alberto Rodriguez, Phillip Isola, and Tsung-Yi Lin.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib52.8.1" style="font-size:90%;">iNeRF: Inverting neural radiance fields for pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib52.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib52.10.2" style="font-size:90%;">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em><span class="ltx_text" id="bib.bib52.11.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib53.5.5.1" style="font-size:90%;">Yen-Chen et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib53.7.1" style="font-size:90%;">
Lin Yen-Chen, Pete Florence, Jonathan T. Barron, Tsung-Yi Lin, Alberto Rodriguez, and Phillip Isola.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib53.8.1" style="font-size:90%;">NeRF-Supervision: Learning dense object descriptors from neural radiance fields.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib53.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib53.10.2" style="font-size:90%;">IEEE Conference on Robotics and Automation (ICRA)</em><span class="ltx_text" id="bib.bib53.11.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib54.5.5.1" style="font-size:90%;">Zakharov et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.7.1" style="font-size:90%;">
Sergey Zakharov, Ivan Shugurov, and Slobodan Ilic.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.8.1" style="font-size:90%;">Dpod: 6d pose object detector and refiner.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib54.10.2" style="font-size:90%;">International Conference on Computer Vision (ICCV)</em><span class="ltx_text" id="bib.bib54.11.3" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib55.5.5.1" style="font-size:90%;">Zhai et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib55.7.1" style="font-size:90%;">
Guangyao Zhai, Dianye Huang, Shun-Cheng Wu, Hyunjun Jung, Yan Di, Fabian Manhardt, Federico Tombari, Nassir Navab, and Benjamin Busam.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib55.8.1" style="font-size:90%;">Monograspnet: 6-dof grasping with a single rgb image.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib55.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib55.10.2" style="font-size:90%;">IEEE Conference on Robotics and Automation (ICRA)</em><span class="ltx_text" id="bib.bib55.11.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Jun 19 19:32:00 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
