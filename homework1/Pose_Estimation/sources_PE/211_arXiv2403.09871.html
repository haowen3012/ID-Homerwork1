<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images</title>
<!--Generated on Thu Jun 13 16:49:39 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2403.09871v3/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S1" title="In ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S2" title="In ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Works</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S2.SS1" title="In 2 Related Works ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>3D Hand Pose Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S2.SS2" title="In 2 Related Works ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Image-based 3D Hand Pose Estimation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S2.SS3" title="In 2 Related Works ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Thermal Computer Vision</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S3" title="In ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>The ThermoHands Benchmark</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S3.SS1" title="In 3 The ThermoHands Benchmark ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Multi-Spectral Hand Pose Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S3.SS2" title="In 3 The ThermoHands Benchmark ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Hand Pose Annotation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S3.SS3" title="In 3 The ThermoHands Benchmark ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>TherFormer: A Baseline Method</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S4" title="In ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiment</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S4.SS1" title="In 4 Experiment ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Evaluation of the Annotation Method</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S4.SS2" title="In 4 Experiment ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Experiment Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S4.SS3" title="In 4 Experiment ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Thermal Image-based 3D Hand Pose Estimation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S4.SS4" title="In 4 Experiment ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Comparison between Spectrum</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S4.SS4.SSS1" title="In 4.4 Comparison between Spectrum ‣ 4 Experiment ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.1 </span>Qualitative Results under Challenging Conditions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S4.SS4.SSS2" title="In 4.4 Comparison between Spectrum ‣ 4 Experiment ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.2 </span>Quantitative Results under Normal Conditions</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S5" title="In ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Limitation and Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S6" title="In ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_text ltx_font_bold" id="id1.1.id1">Fangqiang Ding<sup class="ltx_sup" id="id1.1.id1.1"><span class="ltx_text ltx_font_medium" id="id1.1.id1.1.1">1,</span></sup></span>
 <span class="ltx_text ltx_font_bold" id="id2.2.id2">Lawrence Zhu<sup class="ltx_sup" id="id2.2.id2.1"><span class="ltx_text ltx_font_medium" id="id2.2.id2.1.1">1,</span></sup><span class="ltx_note ltx_role_footnotemark" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnotex1.1.1.1">1</span></span></span></span></span></span>  <span class="ltx_text ltx_font_bold" id="id3.3.id3">Xiangyu Wen<sup class="ltx_sup" id="id3.3.id3.1"><span class="ltx_text ltx_font_medium" id="id3.3.id3.1.1">1</span></sup></span>  <span class="ltx_text ltx_font_bold" id="id4.4.id4">Gaowen Liu<sup class="ltx_sup" id="id4.4.id4.1"><span class="ltx_text ltx_font_medium" id="id4.4.id4.1.1">2</span></sup></span>  <span class="ltx_text ltx_font_bold" id="id5.5.id5">Chris Xiaoxuan Lu<sup class="ltx_sup" id="id5.5.id5.1"><span class="ltx_text ltx_font_medium" id="id5.5.id5.1.1">3,</span></sup></span>
<br class="ltx_break"/><sup class="ltx_sup" id="id6.6.id6">1</sup>University of Edinburgh  <sup class="ltx_sup" id="id7.7.id7">2</sup>Cisco  <sup class="ltx_sup" id="id8.8.id8">3</sup>UCL 
<br class="ltx_break"/>
</span><span class="ltx_author_notes">Equal contributionCorreponding author: xiaoxuan.lu@ucl.ac.uk</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id9.id1">In this work, we present ThermoHands, a new benchmark for thermal image-based egocentric 3D hand pose estimation, aimed at overcoming challenges like varying lighting conditions and obstructions (e.g., handwear). The benchmark includes a multi-view and multi-spectral dataset collected from 28 subjects performing hand-object and hand-virtual interactions under diverse scenarios, accurately annotated with 3D hand poses through an automated process. We introduce a new baseline method, TherFormer, utilizing dual transformer modules for effective egocentric 3D hand pose estimation in thermal imagery. Our experimental results highlight TherFormer’s leading performance and affirm thermal imaging’s effectiveness in enabling robust 3D hand pose estimation in adverse conditions.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Egocentric 3D hand pose estimation is critically important for interpreting hand gestures across various applications, ranging from extended reality (XR) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib4" title="">4</a>]</cite>, to human-robot interaction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib7" title="">7</a>]</cite>, and to imitation learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib10" title="">10</a>]</cite>. Its importance has been magnified with the advent of advanced XR headsets such as the Meta Quest series <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib11" title="">11</a>]</cite> and Apple Vision Pro <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib12" title="">12</a>]</cite>, where it serves as a cornerstone for spatial interaction and immersive digital experiences.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">While current research of hand pose estimation primarily focuses on RGB image-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib17" title="">17</a>]</cite>, these approaches are particularly vulnerable to issues related to lighting variation and occlusions caused by handwear, <em class="ltx_emph ltx_font_italic" id="S1.p2.1.1">e.g</em>., gloves or large jewellery <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib19" title="">19</a>]</cite>. These challenges underscore the imperative for robust egocentric 3D hand pose estimation capable of performing reliably in a variety of common yet complex daily scenarios.
The prevailing approach to facilitate robust hand pose estimation in low-light conditions utilizes <em class="ltx_emph ltx_font_italic" id="S1.p2.1.2">near infrared</em> (NIR) cameras paired with active NIR emitters. This technology, invisible to the human eye, leverages active NIR emitter-receiver configurations for depth estimation through time-of-flight (ToF) or structured lighting. Nevertheless, active NIR systems are more power-intensive compared to passive sensing technologies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib21" title="">21</a>]</cite> and are prone to interference from external NIR sources, such as sunlight <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib22" title="">22</a>]</cite> and other NIR-equipped devices <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib23" title="">23</a>]</cite>. Consequently, these vulnerabilities restrict the effectiveness of hand pose estimation under bright daylight conditions and in situations where multiple augmented reality (AR) or virtual reality (VR) systems are used for collaborative works.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In contrast to NIR-based methods, thermal imaging cameras offer a passive sensing solution for hand pose estimation by capturing long-wave infrared (LWIR) radiation emitted from objects, thereby eliminating reliance on the visible light spectrum <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib24" title="">24</a>]</cite>. This unique attribute of thermal imaging introduces several benefits for hand pose estimation. Primarily, it accentuates the hand’s structure via temperature differentials, negating the effects of lighting variability. Moreover, thermal cameras can detect hands even under handwear such as gloves by identifying heat transmission patterns. This ability ensures a stable and consistent representation of hands, independent of any coverings, thereby broadening the scope and reliability of hand pose estimation across various scenarios.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Building on the above insights, this study probes the following research question: <em class="ltx_emph ltx_font_italic" id="S1.p4.1.1">Can egocentric thermal imagery be effectively used for 3D hand pose estimation under various conditions (such as different lighting and handwear), and how does it compare to techniques using RGB, NIR, and depth<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_upright" id="footnote1.1.1.1">1</span></span><span class="ltx_text ltx_font_upright" id="footnote1.9">For readability, we treat depth and NIR as two ‘spectra’, despite their usual overlap.</span></span></span></span> spectral imagery?</em> To answer this, we introduce <span class="ltx_text ltx_font_typewriter" id="S1.p4.1.2">ThermoHands</span>, the first benchmark specifically tailored for egocentric 3D hand pose estimation utilizing thermal imaging. This benchmark is supported by a novel multi-spectral and multi-view dataset
designed for egocentric 3D hand pose estimation and is unique in comprising thermal, NIR, depth, and RGB images (<em class="ltx_emph ltx_font_italic" id="S1.p4.1.3">cf</em>. <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S2.F1" title="In 2.1 3D Hand Pose Datasets ‣ 2 Related Works ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>). Our dataset emulates real-world application contexts by incorporating both hand-object and hand-virtual interaction activities, with participation from 28 subjects to ensure a broad representation of actions. To offer a thorough comparison across spectral types, we gather data under five distinct scenarios, each characterized by varying environments, handwear, and lighting conditions (<em class="ltx_emph ltx_font_italic" id="S1.p4.1.4">cf</em>. <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S3.T1" title="In 3.1 Multi-Spectral Hand Pose Dataset ‣ 3 The ThermoHands Benchmark ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a>). Considering the challenges associated with manually annotating large-scale 3D hand poses, we developed an automated annotation pipeline. This pipeline leverages multi-view RGB and depth imagery to accurately and efficiently generate 3D hand pose ground truths through optimization based on the MANO model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib25" title="">25</a>]</cite> (<em class="ltx_emph ltx_font_italic" id="S1.p4.1.5">cf</em>. <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S3.F3" title="In 3.2 Hand Pose Annotation ‣ 3 The ThermoHands Benchmark ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Together with the multi-spectral dataset, we introduce a new baseline method named <em class="ltx_emph ltx_font_italic" id="S1.p5.1.1">TherFormer</em>, specifically designed for thermal image-based egocentric 3D hand pose estimation (<em class="ltx_emph ltx_font_italic" id="S1.p5.1.2">cf</em>. <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S3.F4" title="In 3.3 TherFormer: A Baseline Method ‣ 3 The ThermoHands Benchmark ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>). This approach is notable for its two consecutive transformer modules, <em class="ltx_emph ltx_font_italic" id="S1.p5.1.3">i.e</em>., mask-guided spatial transformer and temporal transformer, which encode spatio-temporal relationship for 3D hand joints without losing the computation efficiency.
Our validation process begins with verifying the annotation quality, which averages an accuracy of 1cm (<em class="ltx_emph ltx_font_italic" id="S1.p5.1.4">cf</em>. <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S4.T2" title="In 4.1 Evaluation of the Annotation Method ‣ 4 Experiment ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">2</span></a>). We then benchmark <em class="ltx_emph ltx_font_italic" id="S1.p5.1.5">TherFormer</em> against leading methods (<em class="ltx_emph ltx_font_italic" id="S1.p5.1.6">cf</em>. <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S4.T3" title="In 4.3 Thermal Image-based 3D Hand Pose Estimation ‣ 4 Experiment ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">3</span></a>) and compare the performance of various spectral images (<em class="ltx_emph ltx_font_italic" id="S1.p5.1.7">cf</em>. <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S4.T4" title="In 4.4.2 Quantitative Results under Normal Conditions ‣ 4.4 Comparison between Spectrum ‣ 4 Experiment ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">4</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S4.F6" title="In 4.4.1 Qualitative Results under Challenging Conditions ‣ 4.4 Comparison between Spectrum ‣ 4 Experiment ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a>). The findings underscore thermal imagery’s advantages in difficult lighting conditions and when hands are gloved, showing superior performance and better adaptability to challenging settings than other spectral techniques.
Our main contributions are summarized as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.I1.i1.1.1.m1.1"><semantics id="S1.I1.i1.1.1.m1.1b"><mo id="S1.I1.i1.1.1.m1.1.1" xref="S1.I1.i1.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.I1.i1.1.1.m1.1c"><ci id="S1.I1.i1.1.1.m1.1.1.cmml" xref="S1.I1.i1.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i1.1.1.m1.1d">\bullet</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i1.1.1.m1.1e">∙</annotation></semantics></math></span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We introduce the first-of-its-kind benchmark, dubbed <span class="ltx_text ltx_font_typewriter" id="S1.I1.i1.p1.1.1">ThermoHands</span>, to investigate the potential of thermal imaging for egocentric 3D hand pose estimation.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.I1.i2.1.1.m1.1"><semantics id="S1.I1.i2.1.1.m1.1b"><mo id="S1.I1.i2.1.1.m1.1.1" xref="S1.I1.i2.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.I1.i2.1.1.m1.1c"><ci id="S1.I1.i2.1.1.m1.1.1.cmml" xref="S1.I1.i2.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i2.1.1.m1.1d">\bullet</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i2.1.1.m1.1e">∙</annotation></semantics></math></span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We collected a diverse dataset comprising approximately 96,000 synchronized multi-spectral, multi-view images capturing hand-object and hand-virtual interactions from 28 participants across various environments. This dataset is enriched with 3D hand pose ground truths through an innovative automatic annotation process.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.I1.i3.1.1.m1.1"><semantics id="S1.I1.i3.1.1.m1.1b"><mo id="S1.I1.i3.1.1.m1.1.1" xref="S1.I1.i3.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.I1.i3.1.1.m1.1c"><ci id="S1.I1.i3.1.1.m1.1.1.cmml" xref="S1.I1.i3.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i3.1.1.m1.1d">\bullet</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i3.1.1.m1.1e">∙</annotation></semantics></math></span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We introduce a new baseline method, termed <em class="ltx_emph ltx_font_italic" id="S1.I1.i3.p1.1.1">TherFormer</em>, and implement state-of-the-art image-based methods on our dataset for benchmarking.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.I1.i4.1.1.m1.1"><semantics id="S1.I1.i4.1.1.m1.1b"><mo id="S1.I1.i4.1.1.m1.1.1" xref="S1.I1.i4.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.I1.i4.1.1.m1.1c"><ci id="S1.I1.i4.1.1.m1.1.1.cmml" xref="S1.I1.i4.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i4.1.1.m1.1d">\bullet</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i4.1.1.m1.1e">∙</annotation></semantics></math></span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">Based on the <span class="ltx_text ltx_font_typewriter" id="S1.I1.i4.p1.1.1">ThermoHands</span> benchmark, we conduct comprehensive experiments and analysis on TherFormer and state-of-the-art methods.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.I1.i5.1.1.m1.1"><semantics id="S1.I1.i5.1.1.m1.1b"><mo id="S1.I1.i5.1.1.m1.1.1" xref="S1.I1.i5.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.I1.i5.1.1.m1.1c"><ci id="S1.I1.i5.1.1.m1.1.1.cmml" xref="S1.I1.i5.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i5.1.1.m1.1d">\bullet</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i5.1.1.m1.1e">∙</annotation></semantics></math></span>
<div class="ltx_para ltx_noindent" id="S1.I1.i5.p1">
<p class="ltx_p" id="S1.I1.i5.p1.1">We will release our dataset, code and models and maintain the benchmark to serve as a new challenge in 3D hand pose estimation.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>3D Hand Pose Datasets</h3>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Datasets with 3D hand pose annotations are imperative for training and evaluating <em class="ltx_emph ltx_font_italic" id="S2.SS1.p1.1.1">ad-hoc</em> models. Existing datasets, according to their approaches of annotation acquisition, can be summarized as four types in general, <em class="ltx_emph ltx_font_italic" id="S2.SS1.p1.1.2">i.e</em>., marker-based <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib29" title="">29</a>]</cite>, synthetic <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib34" title="">34</a>]</cite>, manual <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib35" title="">35</a>]</cite> or hybrid <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib40" title="">40</a>]</cite>, and automatic <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib44" title="">44</a>]</cite> annotated datasets. Marker-based approaches, using magnetic sensor <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib27" title="">27</a>]</cite> or Mocap markers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib29" title="">29</a>]</cite>, can alter and induce bias to the hand appearance. Synthetic data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib32" title="">32</a>]</cite> suffers from the <em class="ltx_emph ltx_font_italic" id="S2.SS1.p1.1.3">sim2real</em> gap in terms of hand motion and texture features. Introducing human annotators to fully <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib35" title="">35</a>]</cite> or partly <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib40" title="">40</a>]</cite> annotate 2D/3D keypoints circumvents the issues above, but it either limits the scale of datasets or manifests costly and laborious in practice. Most similar to ours, some datasets adopt fully automatic pipelines to obtain 3D hand pose annotations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib43" title="">43</a>]</cite>, which leverage pre-trained models (<em class="ltx_emph ltx_font_italic" id="S2.SS1.p1.1.4">e.g</em>. OpenPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib45" title="">45</a>]</cite>) to infer the prior hand information and rely on optimization to fit the MANO hand model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib25" title="">25</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Despite the existing progress, previous datasets only provide depth <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib32" title="">32</a>]</cite>, RGB images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib40" title="">40</a>]</cite> or both of them <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib43" title="">43</a>]</cite> as the input spectra, unable to support the study of NIR or thermal image-based 3D hand pose estimation. <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p2.1.1">ThermoHands</span> fills the gap by providing a moderate amount of multi-spectral image data, from infrared to visual light, paired with depth images. Moreover, we capture bimanual actions from both egocentric and exocentric viewpoints and design hand-object as well as hand-virtual interaction actions to facilitate a wide range of applications.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="269" id="S2.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span class="ltx_text ltx_font_bold" id="S2.F1.2.1">Data capture setup</span> with the HMSP and exocentric platform recording multi-view multi-spectral images of two-hand actions performed by participants. </figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Image-based 3D Hand Pose Estimation</h3>
<div class="ltx_para ltx_noindent" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">As a key computer vision task, 3D hand pose estimation from images is highly demanded by applications like XR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib4" title="">4</a>]</cite>, human-robot interaction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib7" title="">7</a>]</cite> and imitation learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib10" title="">10</a>]</cite>. Therefore, this field has been extensively explored in previous arts that uses single RGB <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib50" title="">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib52" title="">52</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib57" title="">57</a>]</cite> or
depth <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib58" title="">58</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib59" title="">59</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib61" title="">61</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib62" title="">62</a>]</cite> image as input.
These methods can be roughly categorized into two fashions, <em class="ltx_emph ltx_font_italic" id="S2.SS2.p1.1.1">i.e</em>., model-based and model-free methods. Model-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib52" title="">52</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib57" title="">57</a>]</cite> utilize the prior knowledge of the MANO hand model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib25" title="">25</a>]</cite> by estimating its shape and pose parameters, while model-free methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib50" title="">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib58" title="">58</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib59" title="">59</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib61" title="">61</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib62" title="">62</a>]</cite> learn the direct regression of 3D hand joints or vertices coordinates.
Recently there has been growing interest in leveraging the temporal supervision <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib63" title="">63</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib64" title="">64</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib65" title="">65</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib66" title="">66</a>]</cite> or leveraging sequential images as input <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib67" title="">67</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib68" title="">68</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib69" title="">69</a>]</cite> for 3D hand pose estimation. In this study, we evaluate existing methods and our baseline method in both single image-based and video-based problem settings, respectively.
Apart from the previous approaches, we investigate the potential of thermal imagery for tackling various challenges in 3D hand pose estimation.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Thermal Computer Vision</h3>
<div class="ltx_para ltx_noindent" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Thermal cameras achieve imaging by capturing the radiation emitted
in the LWIR spectrum and deducing the temperature distribution on the surfaces <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib24" title="">24</a>]</cite>. Leveraging its robustness to variable illumination and unique temperature information, numerous efforts have been made to address various computer vision tasks, including super-resolution <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib70" title="">70</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib71" title="">71</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib72" title="">72</a>]</cite>, human detection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib73" title="">73</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib74" title="">74</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib75" title="">75</a>]</cite>, action recognition <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib76" title="">76</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib77" title="">77</a>]</cite> and pose estimation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib78" title="">78</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib79" title="">79</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib80" title="">80</a>]</cite>, semantic segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib81" title="">81</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib82" title="">82</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib83" title="">83</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib84" title="">84</a>]</cite>, depth estimation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib85" title="">85</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib86" title="">86</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib87" title="">87</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib88" title="">88</a>]</cite>, visual(-inertial) odometry/SLAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib89" title="">89</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib90" title="">90</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib91" title="">91</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib92" title="">92</a>]</cite>, 3D reconstruction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib93" title="">93</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib94" title="">94</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib95" title="">95</a>]</cite>, <em class="ltx_emph ltx_font_italic" id="S2.SS3.p1.1.1">etc</em>. In this work, we focus on 3D hand pose estimation, which is an under-exploited task based on thermal images.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>The ThermoHands Benchmark</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Multi-Spectral Hand Pose Dataset</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p1.1.1">Overview.</span> At the core of our benchmark lies a multi-spectral dataset for 3D hand pose estimation (<em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.1.2">cf</em>. <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S3.T1" title="In 3.1 Multi-Spectral Hand Pose Dataset ‣ 3 The ThermoHands Benchmark ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a>), capturing hand actions performed by 28 subjects of various ethnicities and genders<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>The study has received the ethical approval from School of Informatics, University of Edinburgh,
and participant consent forms were signed before the collection.</span></span></span>. As shown in <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S2.F1" title="In 2.1 3D Hand Pose Datasets ‣ 2 Related Works ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>, we develop a customized head-mounted sensor platform (HMSP) and an exocentric platform to record multi-view data. During the capture, our participants are asked to perform pre-defined hand-object and hand-virtual interaction actions within the playground above the table. The main part is captured in the normal office scenario. To facilitate the evaluation under different settings, four auxiliary parts are recorded i) under the darkness, ii) under the sun glare, iii) with gloves on hand, and iv) in the kitchen environment with different actions, respectively.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T1.3" style="width:433.6pt;height:72.4pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-55.9pt,9.2pt) scale(0.795145585293831,0.795145585293831) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.3.1">
<tr class="ltx_tr" id="S3.T1.3.1.1">
<td class="ltx_td ltx_border_tt" id="S3.T1.3.1.1.1" style="padding-left:12.0pt;padding-right:12.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S3.T1.3.1.1.2" style="padding-left:12.0pt;padding-right:12.0pt;">Normal office (Main)</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S3.T1.3.1.1.3" style="padding-left:12.0pt;padding-right:12.0pt;">Other settings</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.3.1.1.4" rowspan="2" style="padding-left:12.0pt;padding-right:12.0pt;"><span class="ltx_text" id="S3.T1.3.1.1.4.1">Total</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.1.2">
<td class="ltx_td ltx_align_left" id="S3.T1.3.1.2.1" style="padding-left:12.0pt;padding-right:12.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.3.1.2.1.1">Setting</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.1.2.2" style="padding-left:12.0pt;padding-right:12.0pt;">train</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.1.2.3" style="padding-left:12.0pt;padding-right:12.0pt;">val</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.1.2.4" style="padding-left:12.0pt;padding-right:12.0pt;">test</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.1.2.5" style="padding-left:12.0pt;padding-right:12.0pt;">sum</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.1.2.6" style="padding-left:12.0pt;padding-right:12.0pt;">darkness</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.1.2.7" style="padding-left:12.0pt;padding-right:12.0pt;">sun glare</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.1.2.8" style="padding-left:12.0pt;padding-right:12.0pt;">gloves</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.1.2.9" style="padding-left:12.0pt;padding-right:12.0pt;">kitchen</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.1.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.3.1.3.1" style="padding-left:12.0pt;padding-right:12.0pt;">#frames</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.1.3.2" style="padding-left:12.0pt;padding-right:12.0pt;">47,436</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.1.3.3" style="padding-left:12.0pt;padding-right:12.0pt;">12,914</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.1.3.4" style="padding-left:12.0pt;padding-right:12.0pt;">24,002</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.1.3.5" style="padding-left:12.0pt;padding-right:12.0pt;">84,352</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.1.3.6" style="padding-left:12.0pt;padding-right:12.0pt;">3,188</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.1.3.7" style="padding-left:12.0pt;padding-right:12.0pt;">2,508</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.1.3.8" style="padding-left:12.0pt;padding-right:12.0pt;">3,068</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.1.3.9" style="padding-left:12.0pt;padding-right:12.0pt;">2,808</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T1.3.1.3.10" style="padding-left:12.0pt;padding-right:12.0pt;">95,924</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.1.4">
<td class="ltx_td ltx_align_left" id="S3.T1.3.1.4.1" style="padding-left:12.0pt;padding-right:12.0pt;">#seqs</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.1.4.2" style="padding-left:12.0pt;padding-right:12.0pt;">172</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.1.4.3" style="padding-left:12.0pt;padding-right:12.0pt;">43</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.1.4.4" style="padding-left:12.0pt;padding-right:12.0pt;">86</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.1.4.5" style="padding-left:12.0pt;padding-right:12.0pt;">301</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.1.4.6" style="padding-left:12.0pt;padding-right:12.0pt;">12</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.1.4.7" style="padding-left:12.0pt;padding-right:12.0pt;">12</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.1.4.8" style="padding-left:12.0pt;padding-right:12.0pt;">12</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.1.4.9" style="padding-left:12.0pt;padding-right:12.0pt;">14</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.3.1.4.10" style="padding-left:12.0pt;padding-right:12.0pt;">352</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.1.5">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.3.1.5.1" style="padding-left:12.0pt;padding-right:12.0pt;">#subjects</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.3.1.5.2" style="padding-left:12.0pt;padding-right:12.0pt;">16</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.3.1.5.3" style="padding-left:12.0pt;padding-right:12.0pt;">4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.3.1.5.4" style="padding-left:12.0pt;padding-right:12.0pt;">8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.3.1.5.5" style="padding-left:12.0pt;padding-right:12.0pt;">28</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.3.1.5.6" style="padding-left:12.0pt;padding-right:12.0pt;">1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.3.1.5.7" style="padding-left:12.0pt;padding-right:12.0pt;">1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.3.1.5.8" style="padding-left:12.0pt;padding-right:12.0pt;">1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.3.1.5.9" style="padding-left:12.0pt;padding-right:12.0pt;">2</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S3.T1.3.1.5.10" style="padding-left:12.0pt;padding-right:12.0pt;">-</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span><span class="ltx_text ltx_font_bold" id="S3.T1.5.1">Benchmark Dataset Statistics</span>. The overall duration of our dataset is over 3 hours with <math alttext="\sim" class="ltx_Math" display="inline" id="S3.T1.2.m1.1"><semantics id="S3.T1.2.m1.1b"><mo id="S3.T1.2.m1.1.1" xref="S3.T1.2.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.T1.2.m1.1c"><csymbol cd="latexml" id="S3.T1.2.m1.1.1.cmml" xref="S3.T1.2.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.m1.1d">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.T1.2.m1.1e">∼</annotation></semantics></math>96K synchronized frame of all types of images collected. </figcaption>
</figure>
<figure class="ltx_figure" id="S3.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel" id="S3.F2.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="380" id="S3.F2.sf1.g1" src="extracted/5665483/figures/calibration/thermal_cali_board.jpeg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel" id="S3.F2.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="380" id="S3.F2.sf2.g1" src="extracted/5665483/figures/calibration/thermal_calib_rgb.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel" id="S3.F2.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="380" id="S3.F2.sf3.g1" src="extracted/5665483/figures/calibration/thermal_calib_ir.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel" id="S3.F2.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="380" id="S3.F2.sf4.g1" src="extracted/5665483/figures/calibration/thermal_calib_thermal.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(d) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span class="ltx_text ltx_font_bold" id="S3.F2.2.1">Thermal calibration chessboard</span> containing a black base board and multiple removable white cubes (a). By cooling down the base board, it shows similar patterns and allows automatic corner detection in all (b) RGB, (c) NIR and (d) thermal images.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">Sensor Platforms.</span>
The HMSP is mounted with an Intel RealSense L515 LiDAR depth camera <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib20" title="">20</a>]</cite> streaming egocentric RGB, depth and NIR images, and a FLIR Boson 640 thermal camera <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib96" title="">96</a>]</cite> receiving the LWIR to obtain the thermal images. To better simulate XR devices, the HMSP is modularly designed as two components: a base and a sensor board with a fixed 30-degree downward tilt. An extra exocentric platform equipped with an Intel RealSense D455 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib21" title="">21</a>]</cite>
is leveraged to support multi-view annotation (<em class="ltx_emph ltx_font_italic" id="S3.SS1.p2.1.2">cf</em>. <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S3.SS2" title="3.2 Hand Pose Annotation ‣ 3 The ThermoHands Benchmark ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>) as well as provide the RGB-D image data from the third-person viewpoint. As exhibited in <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S2.F1" title="In 2.1 3D Hand Pose Datasets ‣ 2 Related Works ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>, we place the two depth sensors outside each other’s field of view (FoV) to minimize interference caused by their NIR emitters <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib21" title="">21</a>]</cite>. In the office environment, random heat sources, <em class="ltx_emph ltx_font_italic" id="S3.SS1.p2.1.3">e.g</em>., servers and chargers, are strategically placed in the background to increase realism and introduce challenging factors into thermal images.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.1">Synchronization.</span> We use a single PC to simultaneously gather data streams from two sensor platforms, ensuring the synchronization of their timestamps. After collection, we synchronize six types of images, each with distinct frame rates, w.r.t. the timestamps of thermal images (8.5fps), thereby generating well-aligned multi-spectral, multi-view data samples as our released data.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.1">Calibration.</span> For accuracy, we use factory-calibrated intrinsic parameters from the D455 and L515 cameras <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib21" title="">21</a>]</cite>. To better calibrate the parameters of the thermal camera, we self-design a modular calibration chessboard as shown in <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S3.F2" title="In 3.1 Multi-Spectral Hand Pose Dataset ‣ 3 The ThermoHands Benchmark ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>. Before calibration, we cool down the black base board while keeping the white cubes at room temperature to create a visible chessboard pattern in all three spectra, which can be seen in <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S3.F2" title="In 3.1 Multi-Spectral Hand Pose Dataset ‣ 3 The ThermoHands Benchmark ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>. In this way, we can simultaneously calibrate the intrinsic parameter of the L515 thermal camera and its extrinsic parameter w.r.t. the D455 RGB-D camera. To enable the calibration between the two viewpoints, we place 11 IR reflective spheres at random locations and heights above the table. At the first frame of each sequence, we manually annotate their 2D locations from two viewpoints, retrieve their depth values and compute the transformation between two viewpoints by solving the PnP problem <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib97" title="">97</a>]</cite>. For the subsequent frames, as the exocentric platform keeps stationary, we only track the the motion of the ego-head using KISS-ICP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib98" title="">98</a>]</cite> odometry method, which takes the point clouds converted from the egocentric depth image as input.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p5.1.1">Dataset Statistics.</span>
As seen in <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S3.T1" title="In 3.1 Multi-Spectral Hand Pose Dataset ‣ 3 The ThermoHands Benchmark ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a>, our dataset consists of approximately 96K synchronized multi-spectral multi-view frames (<em class="ltx_emph ltx_font_italic" id="S3.SS1.p5.1.2">cf</em>. <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S2.F1" title="In 2.1 3D Hand Pose Datasets ‣ 2 Related Works ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>) and 352 independent sequences in total. The main part is collected under the normal office scenario, where each participant<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Due to their limited time, 7 participants only perform the hand-object actions.</span></span></span> performs 7 hand-object interaction actions: <em class="ltx_emph ltx_font_italic" id="S3.SS1.p5.1.3">cut paper, fold paper, pour water, read book, staple paper, write with pen, write with pencil</em>, and 5 hand-virtual interaction actions: <em class="ltx_emph ltx_font_italic" id="S3.SS1.p5.1.4">pinch and drag, pinch and hold, swipe, tap, touch</em>, with two hands. This main part is divided into the training, validation and testing splits by subjects with a ratio of 4:1:2. We also collect four auxiliary testing sets by asking one subject to perform the aforementioned 12 actions in the darkness, sun glare and gloves settings individually, and two subjects to perform 7 scenarios-specific interaction actions: <em class="ltx_emph ltx_font_italic" id="S3.SS1.p5.1.5">cut, spray, stir, wash hands, wash mug, wash plate, wipe</em> in the kitchen. Please refer to the supplementary for more dataset details.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Hand Pose Annotation</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.2">To avoid employing tedious human efforts for annotation, we implement a fully automatic annotation pipeline, similar to the approaches in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib43" title="">43</a>]</cite>, to obtain the 3D hand pose ground truth for our dataset.
In particular, we use the MANO statistical hand model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib25" title="">25</a>]</cite> to represent 3D hand pose. The MANO model parameterizes the hand mesh vertices
into two low-dimensional embeddings, <em class="ltx_emph ltx_font_italic" id="S3.SS2.p1.2.1">i.e</em>., the shape parameters <math alttext="\beta\in\mathbb{R}^{10}" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">β</mi><mo id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.p1.1.m1.1.1.3.2" xref="S3.SS2.p1.1.m1.1.1.3.2.cmml">ℝ</mi><mn id="S3.SS2.p1.1.m1.1.1.3.3" xref="S3.SS2.p1.1.m1.1.1.3.3.cmml">10</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><in id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1"></in><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">𝛽</ci><apply id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.p1.1.m1.1.1.3.2">ℝ</ci><cn id="S3.SS2.p1.1.m1.1.1.3.3.cmml" type="integer" xref="S3.SS2.p1.1.m1.1.1.3.3">10</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\beta\in\mathbb{R}^{10}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_β ∈ blackboard_R start_POSTSUPERSCRIPT 10 end_POSTSUPERSCRIPT</annotation></semantics></math> and the pose parameters <math alttext="\theta\in\mathbb{R}^{51}" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><mrow id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">θ</mi><mo id="S3.SS2.p1.2.m2.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml"><mi id="S3.SS2.p1.2.m2.1.1.3.2" xref="S3.SS2.p1.2.m2.1.1.3.2.cmml">ℝ</mi><mn id="S3.SS2.p1.2.m2.1.1.3.3" xref="S3.SS2.p1.2.m2.1.1.3.3.cmml">51</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><in id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1"></in><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">𝜃</ci><apply id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.3.1.cmml" xref="S3.SS2.p1.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.3.2.cmml" xref="S3.SS2.p1.2.m2.1.1.3.2">ℝ</ci><cn id="S3.SS2.p1.2.m2.1.1.3.3.cmml" type="integer" xref="S3.SS2.p1.2.m2.1.1.3.3">51</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">\theta\in\mathbb{R}^{51}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">italic_θ ∈ blackboard_R start_POSTSUPERSCRIPT 51 end_POSTSUPERSCRIPT</annotation></semantics></math>.
The MANO fitting is performed by minimizing the following optimization objective per frame for each hand:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\theta^{*}=\operatorname*{arg\,min}_{\theta}\lambda_{j2d}\mathcal{L}_{j2d}^{{%
\color[rgb]{1,.5,0}\bullet}}+\lambda_{mask}\mathcal{L}_{mask}^{{\color[rgb]{%
1,0,1}\bullet}}+\lambda_{j3d}\mathcal{L}_{j3d}^{{\color[rgb]{0,1,0}\bullet}}+%
\lambda_{mesh}\mathcal{L}_{mesh}^{{\color[rgb]{0,1,1}\bullet}}+\lambda_{reg}%
\mathcal{L}_{reg}^{{\color[rgb]{1,0,0}\bullet}}" class="ltx_Math" display="block" id="S3.E1.m1.1"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><msup id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.2.2" xref="S3.E1.m1.1.1.2.2.cmml">θ</mi><mo id="S3.E1.m1.1.1.2.3" xref="S3.E1.m1.1.1.2.3.cmml">∗</mo></msup><mo id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><mrow id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml"><munder id="S3.E1.m1.1.1.3.2.1" xref="S3.E1.m1.1.1.3.2.1.cmml"><mrow id="S3.E1.m1.1.1.3.2.1.2" xref="S3.E1.m1.1.1.3.2.1.2.cmml"><mi id="S3.E1.m1.1.1.3.2.1.2.2" xref="S3.E1.m1.1.1.3.2.1.2.2.cmml">arg</mi><mo id="S3.E1.m1.1.1.3.2.1.2.1" lspace="0.170em" xref="S3.E1.m1.1.1.3.2.1.2.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.2.1.2.3" xref="S3.E1.m1.1.1.3.2.1.2.3.cmml">min</mi></mrow><mi id="S3.E1.m1.1.1.3.2.1.3" xref="S3.E1.m1.1.1.3.2.1.3.cmml">θ</mi></munder><mo id="S3.E1.m1.1.1.3.2a" xref="S3.E1.m1.1.1.3.2.cmml">⁡</mo><mrow id="S3.E1.m1.1.1.3.2.2" xref="S3.E1.m1.1.1.3.2.2.cmml"><msub id="S3.E1.m1.1.1.3.2.2.2" xref="S3.E1.m1.1.1.3.2.2.2.cmml"><mi id="S3.E1.m1.1.1.3.2.2.2.2" xref="S3.E1.m1.1.1.3.2.2.2.2.cmml">λ</mi><mrow id="S3.E1.m1.1.1.3.2.2.2.3" xref="S3.E1.m1.1.1.3.2.2.2.3.cmml"><mi id="S3.E1.m1.1.1.3.2.2.2.3.2" xref="S3.E1.m1.1.1.3.2.2.2.3.2.cmml">j</mi><mo id="S3.E1.m1.1.1.3.2.2.2.3.1" xref="S3.E1.m1.1.1.3.2.2.2.3.1.cmml">⁢</mo><mn id="S3.E1.m1.1.1.3.2.2.2.3.3" xref="S3.E1.m1.1.1.3.2.2.2.3.3.cmml">2</mn><mo id="S3.E1.m1.1.1.3.2.2.2.3.1a" xref="S3.E1.m1.1.1.3.2.2.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.2.2.2.3.4" xref="S3.E1.m1.1.1.3.2.2.2.3.4.cmml">d</mi></mrow></msub><mo id="S3.E1.m1.1.1.3.2.2.1" xref="S3.E1.m1.1.1.3.2.2.1.cmml">⁢</mo><msubsup id="S3.E1.m1.1.1.3.2.2.3" xref="S3.E1.m1.1.1.3.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1.3.2.2.3.2.2" xref="S3.E1.m1.1.1.3.2.2.3.2.2.cmml">ℒ</mi><mrow id="S3.E1.m1.1.1.3.2.2.3.2.3" xref="S3.E1.m1.1.1.3.2.2.3.2.3.cmml"><mi id="S3.E1.m1.1.1.3.2.2.3.2.3.2" xref="S3.E1.m1.1.1.3.2.2.3.2.3.2.cmml">j</mi><mo id="S3.E1.m1.1.1.3.2.2.3.2.3.1" xref="S3.E1.m1.1.1.3.2.2.3.2.3.1.cmml">⁢</mo><mn id="S3.E1.m1.1.1.3.2.2.3.2.3.3" xref="S3.E1.m1.1.1.3.2.2.3.2.3.3.cmml">2</mn><mo id="S3.E1.m1.1.1.3.2.2.3.2.3.1a" xref="S3.E1.m1.1.1.3.2.2.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.2.2.3.2.3.4" xref="S3.E1.m1.1.1.3.2.2.3.2.3.4.cmml">d</mi></mrow><mo id="S3.E1.m1.1.1.3.2.2.3.3" mathcolor="#FF8000" xref="S3.E1.m1.1.1.3.2.2.3.3.cmml">∙</mo></msubsup></mrow></mrow><mo id="S3.E1.m1.1.1.3.1" xref="S3.E1.m1.1.1.3.1.cmml">+</mo><mrow id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3.cmml"><msub id="S3.E1.m1.1.1.3.3.2" xref="S3.E1.m1.1.1.3.3.2.cmml"><mi id="S3.E1.m1.1.1.3.3.2.2" xref="S3.E1.m1.1.1.3.3.2.2.cmml">λ</mi><mrow id="S3.E1.m1.1.1.3.3.2.3" xref="S3.E1.m1.1.1.3.3.2.3.cmml"><mi id="S3.E1.m1.1.1.3.3.2.3.2" xref="S3.E1.m1.1.1.3.3.2.3.2.cmml">m</mi><mo id="S3.E1.m1.1.1.3.3.2.3.1" xref="S3.E1.m1.1.1.3.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.3.2.3.3" xref="S3.E1.m1.1.1.3.3.2.3.3.cmml">a</mi><mo id="S3.E1.m1.1.1.3.3.2.3.1a" xref="S3.E1.m1.1.1.3.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.3.2.3.4" xref="S3.E1.m1.1.1.3.3.2.3.4.cmml">s</mi><mo id="S3.E1.m1.1.1.3.3.2.3.1b" xref="S3.E1.m1.1.1.3.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.3.2.3.5" xref="S3.E1.m1.1.1.3.3.2.3.5.cmml">k</mi></mrow></msub><mo id="S3.E1.m1.1.1.3.3.1" xref="S3.E1.m1.1.1.3.3.1.cmml">⁢</mo><msubsup id="S3.E1.m1.1.1.3.3.3" xref="S3.E1.m1.1.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1.3.3.3.2.2" xref="S3.E1.m1.1.1.3.3.3.2.2.cmml">ℒ</mi><mrow id="S3.E1.m1.1.1.3.3.3.2.3" xref="S3.E1.m1.1.1.3.3.3.2.3.cmml"><mi id="S3.E1.m1.1.1.3.3.3.2.3.2" xref="S3.E1.m1.1.1.3.3.3.2.3.2.cmml">m</mi><mo id="S3.E1.m1.1.1.3.3.3.2.3.1" xref="S3.E1.m1.1.1.3.3.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.3.3.2.3.3" xref="S3.E1.m1.1.1.3.3.3.2.3.3.cmml">a</mi><mo id="S3.E1.m1.1.1.3.3.3.2.3.1a" xref="S3.E1.m1.1.1.3.3.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.3.3.2.3.4" xref="S3.E1.m1.1.1.3.3.3.2.3.4.cmml">s</mi><mo id="S3.E1.m1.1.1.3.3.3.2.3.1b" xref="S3.E1.m1.1.1.3.3.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.3.3.2.3.5" xref="S3.E1.m1.1.1.3.3.3.2.3.5.cmml">k</mi></mrow><mo id="S3.E1.m1.1.1.3.3.3.3" mathcolor="#FF00FF" xref="S3.E1.m1.1.1.3.3.3.3.cmml">∙</mo></msubsup></mrow><mo id="S3.E1.m1.1.1.3.1a" xref="S3.E1.m1.1.1.3.1.cmml">+</mo><mrow id="S3.E1.m1.1.1.3.4" xref="S3.E1.m1.1.1.3.4.cmml"><msub id="S3.E1.m1.1.1.3.4.2" xref="S3.E1.m1.1.1.3.4.2.cmml"><mi id="S3.E1.m1.1.1.3.4.2.2" xref="S3.E1.m1.1.1.3.4.2.2.cmml">λ</mi><mrow id="S3.E1.m1.1.1.3.4.2.3" xref="S3.E1.m1.1.1.3.4.2.3.cmml"><mi id="S3.E1.m1.1.1.3.4.2.3.2" xref="S3.E1.m1.1.1.3.4.2.3.2.cmml">j</mi><mo id="S3.E1.m1.1.1.3.4.2.3.1" xref="S3.E1.m1.1.1.3.4.2.3.1.cmml">⁢</mo><mn id="S3.E1.m1.1.1.3.4.2.3.3" xref="S3.E1.m1.1.1.3.4.2.3.3.cmml">3</mn><mo id="S3.E1.m1.1.1.3.4.2.3.1a" xref="S3.E1.m1.1.1.3.4.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.4.2.3.4" xref="S3.E1.m1.1.1.3.4.2.3.4.cmml">d</mi></mrow></msub><mo id="S3.E1.m1.1.1.3.4.1" xref="S3.E1.m1.1.1.3.4.1.cmml">⁢</mo><msubsup id="S3.E1.m1.1.1.3.4.3" xref="S3.E1.m1.1.1.3.4.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1.3.4.3.2.2" xref="S3.E1.m1.1.1.3.4.3.2.2.cmml">ℒ</mi><mrow id="S3.E1.m1.1.1.3.4.3.2.3" xref="S3.E1.m1.1.1.3.4.3.2.3.cmml"><mi id="S3.E1.m1.1.1.3.4.3.2.3.2" xref="S3.E1.m1.1.1.3.4.3.2.3.2.cmml">j</mi><mo id="S3.E1.m1.1.1.3.4.3.2.3.1" xref="S3.E1.m1.1.1.3.4.3.2.3.1.cmml">⁢</mo><mn id="S3.E1.m1.1.1.3.4.3.2.3.3" xref="S3.E1.m1.1.1.3.4.3.2.3.3.cmml">3</mn><mo id="S3.E1.m1.1.1.3.4.3.2.3.1a" xref="S3.E1.m1.1.1.3.4.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.4.3.2.3.4" xref="S3.E1.m1.1.1.3.4.3.2.3.4.cmml">d</mi></mrow><mo id="S3.E1.m1.1.1.3.4.3.3" mathcolor="#00FF00" xref="S3.E1.m1.1.1.3.4.3.3.cmml">∙</mo></msubsup></mrow><mo id="S3.E1.m1.1.1.3.1b" xref="S3.E1.m1.1.1.3.1.cmml">+</mo><mrow id="S3.E1.m1.1.1.3.5" xref="S3.E1.m1.1.1.3.5.cmml"><msub id="S3.E1.m1.1.1.3.5.2" xref="S3.E1.m1.1.1.3.5.2.cmml"><mi id="S3.E1.m1.1.1.3.5.2.2" xref="S3.E1.m1.1.1.3.5.2.2.cmml">λ</mi><mrow id="S3.E1.m1.1.1.3.5.2.3" xref="S3.E1.m1.1.1.3.5.2.3.cmml"><mi id="S3.E1.m1.1.1.3.5.2.3.2" xref="S3.E1.m1.1.1.3.5.2.3.2.cmml">m</mi><mo id="S3.E1.m1.1.1.3.5.2.3.1" xref="S3.E1.m1.1.1.3.5.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.5.2.3.3" xref="S3.E1.m1.1.1.3.5.2.3.3.cmml">e</mi><mo id="S3.E1.m1.1.1.3.5.2.3.1a" xref="S3.E1.m1.1.1.3.5.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.5.2.3.4" xref="S3.E1.m1.1.1.3.5.2.3.4.cmml">s</mi><mo id="S3.E1.m1.1.1.3.5.2.3.1b" xref="S3.E1.m1.1.1.3.5.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.5.2.3.5" xref="S3.E1.m1.1.1.3.5.2.3.5.cmml">h</mi></mrow></msub><mo id="S3.E1.m1.1.1.3.5.1" xref="S3.E1.m1.1.1.3.5.1.cmml">⁢</mo><msubsup id="S3.E1.m1.1.1.3.5.3" xref="S3.E1.m1.1.1.3.5.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1.3.5.3.2.2" xref="S3.E1.m1.1.1.3.5.3.2.2.cmml">ℒ</mi><mrow id="S3.E1.m1.1.1.3.5.3.2.3" xref="S3.E1.m1.1.1.3.5.3.2.3.cmml"><mi id="S3.E1.m1.1.1.3.5.3.2.3.2" xref="S3.E1.m1.1.1.3.5.3.2.3.2.cmml">m</mi><mo id="S3.E1.m1.1.1.3.5.3.2.3.1" xref="S3.E1.m1.1.1.3.5.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.5.3.2.3.3" xref="S3.E1.m1.1.1.3.5.3.2.3.3.cmml">e</mi><mo id="S3.E1.m1.1.1.3.5.3.2.3.1a" xref="S3.E1.m1.1.1.3.5.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.5.3.2.3.4" xref="S3.E1.m1.1.1.3.5.3.2.3.4.cmml">s</mi><mo id="S3.E1.m1.1.1.3.5.3.2.3.1b" xref="S3.E1.m1.1.1.3.5.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.5.3.2.3.5" xref="S3.E1.m1.1.1.3.5.3.2.3.5.cmml">h</mi></mrow><mo id="S3.E1.m1.1.1.3.5.3.3" mathcolor="#00FFFF" xref="S3.E1.m1.1.1.3.5.3.3.cmml">∙</mo></msubsup></mrow><mo id="S3.E1.m1.1.1.3.1c" xref="S3.E1.m1.1.1.3.1.cmml">+</mo><mrow id="S3.E1.m1.1.1.3.6" xref="S3.E1.m1.1.1.3.6.cmml"><msub id="S3.E1.m1.1.1.3.6.2" xref="S3.E1.m1.1.1.3.6.2.cmml"><mi id="S3.E1.m1.1.1.3.6.2.2" xref="S3.E1.m1.1.1.3.6.2.2.cmml">λ</mi><mrow id="S3.E1.m1.1.1.3.6.2.3" xref="S3.E1.m1.1.1.3.6.2.3.cmml"><mi id="S3.E1.m1.1.1.3.6.2.3.2" xref="S3.E1.m1.1.1.3.6.2.3.2.cmml">r</mi><mo id="S3.E1.m1.1.1.3.6.2.3.1" xref="S3.E1.m1.1.1.3.6.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.6.2.3.3" xref="S3.E1.m1.1.1.3.6.2.3.3.cmml">e</mi><mo id="S3.E1.m1.1.1.3.6.2.3.1a" xref="S3.E1.m1.1.1.3.6.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.6.2.3.4" xref="S3.E1.m1.1.1.3.6.2.3.4.cmml">g</mi></mrow></msub><mo id="S3.E1.m1.1.1.3.6.1" xref="S3.E1.m1.1.1.3.6.1.cmml">⁢</mo><msubsup id="S3.E1.m1.1.1.3.6.3" xref="S3.E1.m1.1.1.3.6.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1.3.6.3.2.2" xref="S3.E1.m1.1.1.3.6.3.2.2.cmml">ℒ</mi><mrow id="S3.E1.m1.1.1.3.6.3.2.3" xref="S3.E1.m1.1.1.3.6.3.2.3.cmml"><mi id="S3.E1.m1.1.1.3.6.3.2.3.2" xref="S3.E1.m1.1.1.3.6.3.2.3.2.cmml">r</mi><mo id="S3.E1.m1.1.1.3.6.3.2.3.1" xref="S3.E1.m1.1.1.3.6.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.6.3.2.3.3" xref="S3.E1.m1.1.1.3.6.3.2.3.3.cmml">e</mi><mo id="S3.E1.m1.1.1.3.6.3.2.3.1a" xref="S3.E1.m1.1.1.3.6.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.6.3.2.3.4" xref="S3.E1.m1.1.1.3.6.3.2.3.4.cmml">g</mi></mrow><mo id="S3.E1.m1.1.1.3.6.3.3" mathcolor="#FF0000" xref="S3.E1.m1.1.1.3.6.3.3.cmml">∙</mo></msubsup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><eq id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"></eq><apply id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.2">superscript</csymbol><ci id="S3.E1.m1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.2.2">𝜃</ci><times id="S3.E1.m1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.2.3"></times></apply><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><plus id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3.1"></plus><apply id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2"><apply id="S3.E1.m1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.3.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.2.1.1.cmml" xref="S3.E1.m1.1.1.3.2.1">subscript</csymbol><apply id="S3.E1.m1.1.1.3.2.1.2.cmml" xref="S3.E1.m1.1.1.3.2.1.2"><times id="S3.E1.m1.1.1.3.2.1.2.1.cmml" xref="S3.E1.m1.1.1.3.2.1.2.1"></times><ci id="S3.E1.m1.1.1.3.2.1.2.2.cmml" xref="S3.E1.m1.1.1.3.2.1.2.2">arg</ci><ci id="S3.E1.m1.1.1.3.2.1.2.3.cmml" xref="S3.E1.m1.1.1.3.2.1.2.3">min</ci></apply><ci id="S3.E1.m1.1.1.3.2.1.3.cmml" xref="S3.E1.m1.1.1.3.2.1.3">𝜃</ci></apply><apply id="S3.E1.m1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.3.2.2"><times id="S3.E1.m1.1.1.3.2.2.1.cmml" xref="S3.E1.m1.1.1.3.2.2.1"></times><apply id="S3.E1.m1.1.1.3.2.2.2.cmml" xref="S3.E1.m1.1.1.3.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.2.2.2.1.cmml" xref="S3.E1.m1.1.1.3.2.2.2">subscript</csymbol><ci id="S3.E1.m1.1.1.3.2.2.2.2.cmml" xref="S3.E1.m1.1.1.3.2.2.2.2">𝜆</ci><apply id="S3.E1.m1.1.1.3.2.2.2.3.cmml" xref="S3.E1.m1.1.1.3.2.2.2.3"><times id="S3.E1.m1.1.1.3.2.2.2.3.1.cmml" xref="S3.E1.m1.1.1.3.2.2.2.3.1"></times><ci id="S3.E1.m1.1.1.3.2.2.2.3.2.cmml" xref="S3.E1.m1.1.1.3.2.2.2.3.2">𝑗</ci><cn id="S3.E1.m1.1.1.3.2.2.2.3.3.cmml" type="integer" xref="S3.E1.m1.1.1.3.2.2.2.3.3">2</cn><ci id="S3.E1.m1.1.1.3.2.2.2.3.4.cmml" xref="S3.E1.m1.1.1.3.2.2.2.3.4">𝑑</ci></apply></apply><apply id="S3.E1.m1.1.1.3.2.2.3.cmml" xref="S3.E1.m1.1.1.3.2.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.2.2.3.1.cmml" xref="S3.E1.m1.1.1.3.2.2.3">superscript</csymbol><apply id="S3.E1.m1.1.1.3.2.2.3.2.cmml" xref="S3.E1.m1.1.1.3.2.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.2.2.3.2.1.cmml" xref="S3.E1.m1.1.1.3.2.2.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.2.2.3.2.2.cmml" xref="S3.E1.m1.1.1.3.2.2.3.2.2">ℒ</ci><apply id="S3.E1.m1.1.1.3.2.2.3.2.3.cmml" xref="S3.E1.m1.1.1.3.2.2.3.2.3"><times id="S3.E1.m1.1.1.3.2.2.3.2.3.1.cmml" xref="S3.E1.m1.1.1.3.2.2.3.2.3.1"></times><ci id="S3.E1.m1.1.1.3.2.2.3.2.3.2.cmml" xref="S3.E1.m1.1.1.3.2.2.3.2.3.2">𝑗</ci><cn id="S3.E1.m1.1.1.3.2.2.3.2.3.3.cmml" type="integer" xref="S3.E1.m1.1.1.3.2.2.3.2.3.3">2</cn><ci id="S3.E1.m1.1.1.3.2.2.3.2.3.4.cmml" xref="S3.E1.m1.1.1.3.2.2.3.2.3.4">𝑑</ci></apply></apply><ci id="S3.E1.m1.1.1.3.2.2.3.3.cmml" xref="S3.E1.m1.1.1.3.2.2.3.3">∙</ci></apply></apply></apply><apply id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3"><times id="S3.E1.m1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.3.3.1"></times><apply id="S3.E1.m1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.3.2.1.cmml" xref="S3.E1.m1.1.1.3.3.2">subscript</csymbol><ci id="S3.E1.m1.1.1.3.3.2.2.cmml" xref="S3.E1.m1.1.1.3.3.2.2">𝜆</ci><apply id="S3.E1.m1.1.1.3.3.2.3.cmml" xref="S3.E1.m1.1.1.3.3.2.3"><times id="S3.E1.m1.1.1.3.3.2.3.1.cmml" xref="S3.E1.m1.1.1.3.3.2.3.1"></times><ci id="S3.E1.m1.1.1.3.3.2.3.2.cmml" xref="S3.E1.m1.1.1.3.3.2.3.2">𝑚</ci><ci id="S3.E1.m1.1.1.3.3.2.3.3.cmml" xref="S3.E1.m1.1.1.3.3.2.3.3">𝑎</ci><ci id="S3.E1.m1.1.1.3.3.2.3.4.cmml" xref="S3.E1.m1.1.1.3.3.2.3.4">𝑠</ci><ci id="S3.E1.m1.1.1.3.3.2.3.5.cmml" xref="S3.E1.m1.1.1.3.3.2.3.5">𝑘</ci></apply></apply><apply id="S3.E1.m1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.3.3.1.cmml" xref="S3.E1.m1.1.1.3.3.3">superscript</csymbol><apply id="S3.E1.m1.1.1.3.3.3.2.cmml" xref="S3.E1.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.3.3.2.1.cmml" xref="S3.E1.m1.1.1.3.3.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.3.3.2.2.cmml" xref="S3.E1.m1.1.1.3.3.3.2.2">ℒ</ci><apply id="S3.E1.m1.1.1.3.3.3.2.3.cmml" xref="S3.E1.m1.1.1.3.3.3.2.3"><times id="S3.E1.m1.1.1.3.3.3.2.3.1.cmml" xref="S3.E1.m1.1.1.3.3.3.2.3.1"></times><ci id="S3.E1.m1.1.1.3.3.3.2.3.2.cmml" xref="S3.E1.m1.1.1.3.3.3.2.3.2">𝑚</ci><ci id="S3.E1.m1.1.1.3.3.3.2.3.3.cmml" xref="S3.E1.m1.1.1.3.3.3.2.3.3">𝑎</ci><ci id="S3.E1.m1.1.1.3.3.3.2.3.4.cmml" xref="S3.E1.m1.1.1.3.3.3.2.3.4">𝑠</ci><ci id="S3.E1.m1.1.1.3.3.3.2.3.5.cmml" xref="S3.E1.m1.1.1.3.3.3.2.3.5">𝑘</ci></apply></apply><ci id="S3.E1.m1.1.1.3.3.3.3.cmml" xref="S3.E1.m1.1.1.3.3.3.3">∙</ci></apply></apply><apply id="S3.E1.m1.1.1.3.4.cmml" xref="S3.E1.m1.1.1.3.4"><times id="S3.E1.m1.1.1.3.4.1.cmml" xref="S3.E1.m1.1.1.3.4.1"></times><apply id="S3.E1.m1.1.1.3.4.2.cmml" xref="S3.E1.m1.1.1.3.4.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.4.2.1.cmml" xref="S3.E1.m1.1.1.3.4.2">subscript</csymbol><ci id="S3.E1.m1.1.1.3.4.2.2.cmml" xref="S3.E1.m1.1.1.3.4.2.2">𝜆</ci><apply id="S3.E1.m1.1.1.3.4.2.3.cmml" xref="S3.E1.m1.1.1.3.4.2.3"><times id="S3.E1.m1.1.1.3.4.2.3.1.cmml" xref="S3.E1.m1.1.1.3.4.2.3.1"></times><ci id="S3.E1.m1.1.1.3.4.2.3.2.cmml" xref="S3.E1.m1.1.1.3.4.2.3.2">𝑗</ci><cn id="S3.E1.m1.1.1.3.4.2.3.3.cmml" type="integer" xref="S3.E1.m1.1.1.3.4.2.3.3">3</cn><ci id="S3.E1.m1.1.1.3.4.2.3.4.cmml" xref="S3.E1.m1.1.1.3.4.2.3.4">𝑑</ci></apply></apply><apply id="S3.E1.m1.1.1.3.4.3.cmml" xref="S3.E1.m1.1.1.3.4.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.4.3.1.cmml" xref="S3.E1.m1.1.1.3.4.3">superscript</csymbol><apply id="S3.E1.m1.1.1.3.4.3.2.cmml" xref="S3.E1.m1.1.1.3.4.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.4.3.2.1.cmml" xref="S3.E1.m1.1.1.3.4.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.4.3.2.2.cmml" xref="S3.E1.m1.1.1.3.4.3.2.2">ℒ</ci><apply id="S3.E1.m1.1.1.3.4.3.2.3.cmml" xref="S3.E1.m1.1.1.3.4.3.2.3"><times id="S3.E1.m1.1.1.3.4.3.2.3.1.cmml" xref="S3.E1.m1.1.1.3.4.3.2.3.1"></times><ci id="S3.E1.m1.1.1.3.4.3.2.3.2.cmml" xref="S3.E1.m1.1.1.3.4.3.2.3.2">𝑗</ci><cn id="S3.E1.m1.1.1.3.4.3.2.3.3.cmml" type="integer" xref="S3.E1.m1.1.1.3.4.3.2.3.3">3</cn><ci id="S3.E1.m1.1.1.3.4.3.2.3.4.cmml" xref="S3.E1.m1.1.1.3.4.3.2.3.4">𝑑</ci></apply></apply><ci id="S3.E1.m1.1.1.3.4.3.3.cmml" xref="S3.E1.m1.1.1.3.4.3.3">∙</ci></apply></apply><apply id="S3.E1.m1.1.1.3.5.cmml" xref="S3.E1.m1.1.1.3.5"><times id="S3.E1.m1.1.1.3.5.1.cmml" xref="S3.E1.m1.1.1.3.5.1"></times><apply id="S3.E1.m1.1.1.3.5.2.cmml" xref="S3.E1.m1.1.1.3.5.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.5.2.1.cmml" xref="S3.E1.m1.1.1.3.5.2">subscript</csymbol><ci id="S3.E1.m1.1.1.3.5.2.2.cmml" xref="S3.E1.m1.1.1.3.5.2.2">𝜆</ci><apply id="S3.E1.m1.1.1.3.5.2.3.cmml" xref="S3.E1.m1.1.1.3.5.2.3"><times id="S3.E1.m1.1.1.3.5.2.3.1.cmml" xref="S3.E1.m1.1.1.3.5.2.3.1"></times><ci id="S3.E1.m1.1.1.3.5.2.3.2.cmml" xref="S3.E1.m1.1.1.3.5.2.3.2">𝑚</ci><ci id="S3.E1.m1.1.1.3.5.2.3.3.cmml" xref="S3.E1.m1.1.1.3.5.2.3.3">𝑒</ci><ci id="S3.E1.m1.1.1.3.5.2.3.4.cmml" xref="S3.E1.m1.1.1.3.5.2.3.4">𝑠</ci><ci id="S3.E1.m1.1.1.3.5.2.3.5.cmml" xref="S3.E1.m1.1.1.3.5.2.3.5">ℎ</ci></apply></apply><apply id="S3.E1.m1.1.1.3.5.3.cmml" xref="S3.E1.m1.1.1.3.5.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.5.3.1.cmml" xref="S3.E1.m1.1.1.3.5.3">superscript</csymbol><apply id="S3.E1.m1.1.1.3.5.3.2.cmml" xref="S3.E1.m1.1.1.3.5.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.5.3.2.1.cmml" xref="S3.E1.m1.1.1.3.5.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.5.3.2.2.cmml" xref="S3.E1.m1.1.1.3.5.3.2.2">ℒ</ci><apply id="S3.E1.m1.1.1.3.5.3.2.3.cmml" xref="S3.E1.m1.1.1.3.5.3.2.3"><times id="S3.E1.m1.1.1.3.5.3.2.3.1.cmml" xref="S3.E1.m1.1.1.3.5.3.2.3.1"></times><ci id="S3.E1.m1.1.1.3.5.3.2.3.2.cmml" xref="S3.E1.m1.1.1.3.5.3.2.3.2">𝑚</ci><ci id="S3.E1.m1.1.1.3.5.3.2.3.3.cmml" xref="S3.E1.m1.1.1.3.5.3.2.3.3">𝑒</ci><ci id="S3.E1.m1.1.1.3.5.3.2.3.4.cmml" xref="S3.E1.m1.1.1.3.5.3.2.3.4">𝑠</ci><ci id="S3.E1.m1.1.1.3.5.3.2.3.5.cmml" xref="S3.E1.m1.1.1.3.5.3.2.3.5">ℎ</ci></apply></apply><ci id="S3.E1.m1.1.1.3.5.3.3.cmml" xref="S3.E1.m1.1.1.3.5.3.3">∙</ci></apply></apply><apply id="S3.E1.m1.1.1.3.6.cmml" xref="S3.E1.m1.1.1.3.6"><times id="S3.E1.m1.1.1.3.6.1.cmml" xref="S3.E1.m1.1.1.3.6.1"></times><apply id="S3.E1.m1.1.1.3.6.2.cmml" xref="S3.E1.m1.1.1.3.6.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.6.2.1.cmml" xref="S3.E1.m1.1.1.3.6.2">subscript</csymbol><ci id="S3.E1.m1.1.1.3.6.2.2.cmml" xref="S3.E1.m1.1.1.3.6.2.2">𝜆</ci><apply id="S3.E1.m1.1.1.3.6.2.3.cmml" xref="S3.E1.m1.1.1.3.6.2.3"><times id="S3.E1.m1.1.1.3.6.2.3.1.cmml" xref="S3.E1.m1.1.1.3.6.2.3.1"></times><ci id="S3.E1.m1.1.1.3.6.2.3.2.cmml" xref="S3.E1.m1.1.1.3.6.2.3.2">𝑟</ci><ci id="S3.E1.m1.1.1.3.6.2.3.3.cmml" xref="S3.E1.m1.1.1.3.6.2.3.3">𝑒</ci><ci id="S3.E1.m1.1.1.3.6.2.3.4.cmml" xref="S3.E1.m1.1.1.3.6.2.3.4">𝑔</ci></apply></apply><apply id="S3.E1.m1.1.1.3.6.3.cmml" xref="S3.E1.m1.1.1.3.6.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.6.3.1.cmml" xref="S3.E1.m1.1.1.3.6.3">superscript</csymbol><apply id="S3.E1.m1.1.1.3.6.3.2.cmml" xref="S3.E1.m1.1.1.3.6.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.6.3.2.1.cmml" xref="S3.E1.m1.1.1.3.6.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.6.3.2.2.cmml" xref="S3.E1.m1.1.1.3.6.3.2.2">ℒ</ci><apply id="S3.E1.m1.1.1.3.6.3.2.3.cmml" xref="S3.E1.m1.1.1.3.6.3.2.3"><times id="S3.E1.m1.1.1.3.6.3.2.3.1.cmml" xref="S3.E1.m1.1.1.3.6.3.2.3.1"></times><ci id="S3.E1.m1.1.1.3.6.3.2.3.2.cmml" xref="S3.E1.m1.1.1.3.6.3.2.3.2">𝑟</ci><ci id="S3.E1.m1.1.1.3.6.3.2.3.3.cmml" xref="S3.E1.m1.1.1.3.6.3.2.3.3">𝑒</ci><ci id="S3.E1.m1.1.1.3.6.3.2.3.4.cmml" xref="S3.E1.m1.1.1.3.6.3.2.3.4">𝑔</ci></apply></apply><ci id="S3.E1.m1.1.1.3.6.3.3.cmml" xref="S3.E1.m1.1.1.3.6.3.3">∙</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\theta^{*}=\operatorname*{arg\,min}_{\theta}\lambda_{j2d}\mathcal{L}_{j2d}^{{%
\color[rgb]{1,.5,0}\bullet}}+\lambda_{mask}\mathcal{L}_{mask}^{{\color[rgb]{%
1,0,1}\bullet}}+\lambda_{j3d}\mathcal{L}_{j3d}^{{\color[rgb]{0,1,0}\bullet}}+%
\lambda_{mesh}\mathcal{L}_{mesh}^{{\color[rgb]{0,1,1}\bullet}}+\lambda_{reg}%
\mathcal{L}_{reg}^{{\color[rgb]{1,0,0}\bullet}}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.1d">italic_θ start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT = start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT italic_λ start_POSTSUBSCRIPT italic_j 2 italic_d end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_j 2 italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∙ end_POSTSUPERSCRIPT + italic_λ start_POSTSUBSCRIPT italic_m italic_a italic_s italic_k end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_m italic_a italic_s italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∙ end_POSTSUPERSCRIPT + italic_λ start_POSTSUBSCRIPT italic_j 3 italic_d end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_j 3 italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∙ end_POSTSUPERSCRIPT + italic_λ start_POSTSUBSCRIPT italic_m italic_e italic_s italic_h end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_m italic_e italic_s italic_h end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∙ end_POSTSUPERSCRIPT + italic_λ start_POSTSUBSCRIPT italic_r italic_e italic_g end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_r italic_e italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∙ end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p1.3">where <math alttext="\lambda_{j2d},\lambda_{mask},\lambda_{j3d},\lambda_{mesh},\lambda_{reg}" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m1.5"><semantics id="S3.SS2.p1.3.m1.5a"><mrow id="S3.SS2.p1.3.m1.5.5.5" xref="S3.SS2.p1.3.m1.5.5.6.cmml"><msub id="S3.SS2.p1.3.m1.1.1.1.1" xref="S3.SS2.p1.3.m1.1.1.1.1.cmml"><mi id="S3.SS2.p1.3.m1.1.1.1.1.2" xref="S3.SS2.p1.3.m1.1.1.1.1.2.cmml">λ</mi><mrow id="S3.SS2.p1.3.m1.1.1.1.1.3" xref="S3.SS2.p1.3.m1.1.1.1.1.3.cmml"><mi id="S3.SS2.p1.3.m1.1.1.1.1.3.2" xref="S3.SS2.p1.3.m1.1.1.1.1.3.2.cmml">j</mi><mo id="S3.SS2.p1.3.m1.1.1.1.1.3.1" xref="S3.SS2.p1.3.m1.1.1.1.1.3.1.cmml">⁢</mo><mn id="S3.SS2.p1.3.m1.1.1.1.1.3.3" xref="S3.SS2.p1.3.m1.1.1.1.1.3.3.cmml">2</mn><mo id="S3.SS2.p1.3.m1.1.1.1.1.3.1a" xref="S3.SS2.p1.3.m1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p1.3.m1.1.1.1.1.3.4" xref="S3.SS2.p1.3.m1.1.1.1.1.3.4.cmml">d</mi></mrow></msub><mo id="S3.SS2.p1.3.m1.5.5.5.6" xref="S3.SS2.p1.3.m1.5.5.6.cmml">,</mo><msub id="S3.SS2.p1.3.m1.2.2.2.2" xref="S3.SS2.p1.3.m1.2.2.2.2.cmml"><mi id="S3.SS2.p1.3.m1.2.2.2.2.2" xref="S3.SS2.p1.3.m1.2.2.2.2.2.cmml">λ</mi><mrow id="S3.SS2.p1.3.m1.2.2.2.2.3" xref="S3.SS2.p1.3.m1.2.2.2.2.3.cmml"><mi id="S3.SS2.p1.3.m1.2.2.2.2.3.2" xref="S3.SS2.p1.3.m1.2.2.2.2.3.2.cmml">m</mi><mo id="S3.SS2.p1.3.m1.2.2.2.2.3.1" xref="S3.SS2.p1.3.m1.2.2.2.2.3.1.cmml">⁢</mo><mi id="S3.SS2.p1.3.m1.2.2.2.2.3.3" xref="S3.SS2.p1.3.m1.2.2.2.2.3.3.cmml">a</mi><mo id="S3.SS2.p1.3.m1.2.2.2.2.3.1a" xref="S3.SS2.p1.3.m1.2.2.2.2.3.1.cmml">⁢</mo><mi id="S3.SS2.p1.3.m1.2.2.2.2.3.4" xref="S3.SS2.p1.3.m1.2.2.2.2.3.4.cmml">s</mi><mo id="S3.SS2.p1.3.m1.2.2.2.2.3.1b" xref="S3.SS2.p1.3.m1.2.2.2.2.3.1.cmml">⁢</mo><mi id="S3.SS2.p1.3.m1.2.2.2.2.3.5" xref="S3.SS2.p1.3.m1.2.2.2.2.3.5.cmml">k</mi></mrow></msub><mo id="S3.SS2.p1.3.m1.5.5.5.7" xref="S3.SS2.p1.3.m1.5.5.6.cmml">,</mo><msub id="S3.SS2.p1.3.m1.3.3.3.3" xref="S3.SS2.p1.3.m1.3.3.3.3.cmml"><mi id="S3.SS2.p1.3.m1.3.3.3.3.2" xref="S3.SS2.p1.3.m1.3.3.3.3.2.cmml">λ</mi><mrow id="S3.SS2.p1.3.m1.3.3.3.3.3" xref="S3.SS2.p1.3.m1.3.3.3.3.3.cmml"><mi id="S3.SS2.p1.3.m1.3.3.3.3.3.2" xref="S3.SS2.p1.3.m1.3.3.3.3.3.2.cmml">j</mi><mo id="S3.SS2.p1.3.m1.3.3.3.3.3.1" xref="S3.SS2.p1.3.m1.3.3.3.3.3.1.cmml">⁢</mo><mn id="S3.SS2.p1.3.m1.3.3.3.3.3.3" xref="S3.SS2.p1.3.m1.3.3.3.3.3.3.cmml">3</mn><mo id="S3.SS2.p1.3.m1.3.3.3.3.3.1a" xref="S3.SS2.p1.3.m1.3.3.3.3.3.1.cmml">⁢</mo><mi id="S3.SS2.p1.3.m1.3.3.3.3.3.4" xref="S3.SS2.p1.3.m1.3.3.3.3.3.4.cmml">d</mi></mrow></msub><mo id="S3.SS2.p1.3.m1.5.5.5.8" xref="S3.SS2.p1.3.m1.5.5.6.cmml">,</mo><msub id="S3.SS2.p1.3.m1.4.4.4.4" xref="S3.SS2.p1.3.m1.4.4.4.4.cmml"><mi id="S3.SS2.p1.3.m1.4.4.4.4.2" xref="S3.SS2.p1.3.m1.4.4.4.4.2.cmml">λ</mi><mrow id="S3.SS2.p1.3.m1.4.4.4.4.3" xref="S3.SS2.p1.3.m1.4.4.4.4.3.cmml"><mi id="S3.SS2.p1.3.m1.4.4.4.4.3.2" xref="S3.SS2.p1.3.m1.4.4.4.4.3.2.cmml">m</mi><mo id="S3.SS2.p1.3.m1.4.4.4.4.3.1" xref="S3.SS2.p1.3.m1.4.4.4.4.3.1.cmml">⁢</mo><mi id="S3.SS2.p1.3.m1.4.4.4.4.3.3" xref="S3.SS2.p1.3.m1.4.4.4.4.3.3.cmml">e</mi><mo id="S3.SS2.p1.3.m1.4.4.4.4.3.1a" xref="S3.SS2.p1.3.m1.4.4.4.4.3.1.cmml">⁢</mo><mi id="S3.SS2.p1.3.m1.4.4.4.4.3.4" xref="S3.SS2.p1.3.m1.4.4.4.4.3.4.cmml">s</mi><mo id="S3.SS2.p1.3.m1.4.4.4.4.3.1b" xref="S3.SS2.p1.3.m1.4.4.4.4.3.1.cmml">⁢</mo><mi id="S3.SS2.p1.3.m1.4.4.4.4.3.5" xref="S3.SS2.p1.3.m1.4.4.4.4.3.5.cmml">h</mi></mrow></msub><mo id="S3.SS2.p1.3.m1.5.5.5.9" xref="S3.SS2.p1.3.m1.5.5.6.cmml">,</mo><msub id="S3.SS2.p1.3.m1.5.5.5.5" xref="S3.SS2.p1.3.m1.5.5.5.5.cmml"><mi id="S3.SS2.p1.3.m1.5.5.5.5.2" xref="S3.SS2.p1.3.m1.5.5.5.5.2.cmml">λ</mi><mrow id="S3.SS2.p1.3.m1.5.5.5.5.3" xref="S3.SS2.p1.3.m1.5.5.5.5.3.cmml"><mi id="S3.SS2.p1.3.m1.5.5.5.5.3.2" xref="S3.SS2.p1.3.m1.5.5.5.5.3.2.cmml">r</mi><mo id="S3.SS2.p1.3.m1.5.5.5.5.3.1" xref="S3.SS2.p1.3.m1.5.5.5.5.3.1.cmml">⁢</mo><mi id="S3.SS2.p1.3.m1.5.5.5.5.3.3" xref="S3.SS2.p1.3.m1.5.5.5.5.3.3.cmml">e</mi><mo id="S3.SS2.p1.3.m1.5.5.5.5.3.1a" xref="S3.SS2.p1.3.m1.5.5.5.5.3.1.cmml">⁢</mo><mi id="S3.SS2.p1.3.m1.5.5.5.5.3.4" xref="S3.SS2.p1.3.m1.5.5.5.5.3.4.cmml">g</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m1.5b"><list id="S3.SS2.p1.3.m1.5.5.6.cmml" xref="S3.SS2.p1.3.m1.5.5.5"><apply id="S3.SS2.p1.3.m1.1.1.1.1.cmml" xref="S3.SS2.p1.3.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m1.1.1.1.1.1.cmml" xref="S3.SS2.p1.3.m1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p1.3.m1.1.1.1.1.2.cmml" xref="S3.SS2.p1.3.m1.1.1.1.1.2">𝜆</ci><apply id="S3.SS2.p1.3.m1.1.1.1.1.3.cmml" xref="S3.SS2.p1.3.m1.1.1.1.1.3"><times id="S3.SS2.p1.3.m1.1.1.1.1.3.1.cmml" xref="S3.SS2.p1.3.m1.1.1.1.1.3.1"></times><ci id="S3.SS2.p1.3.m1.1.1.1.1.3.2.cmml" xref="S3.SS2.p1.3.m1.1.1.1.1.3.2">𝑗</ci><cn id="S3.SS2.p1.3.m1.1.1.1.1.3.3.cmml" type="integer" xref="S3.SS2.p1.3.m1.1.1.1.1.3.3">2</cn><ci id="S3.SS2.p1.3.m1.1.1.1.1.3.4.cmml" xref="S3.SS2.p1.3.m1.1.1.1.1.3.4">𝑑</ci></apply></apply><apply id="S3.SS2.p1.3.m1.2.2.2.2.cmml" xref="S3.SS2.p1.3.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m1.2.2.2.2.1.cmml" xref="S3.SS2.p1.3.m1.2.2.2.2">subscript</csymbol><ci id="S3.SS2.p1.3.m1.2.2.2.2.2.cmml" xref="S3.SS2.p1.3.m1.2.2.2.2.2">𝜆</ci><apply id="S3.SS2.p1.3.m1.2.2.2.2.3.cmml" xref="S3.SS2.p1.3.m1.2.2.2.2.3"><times id="S3.SS2.p1.3.m1.2.2.2.2.3.1.cmml" xref="S3.SS2.p1.3.m1.2.2.2.2.3.1"></times><ci id="S3.SS2.p1.3.m1.2.2.2.2.3.2.cmml" xref="S3.SS2.p1.3.m1.2.2.2.2.3.2">𝑚</ci><ci id="S3.SS2.p1.3.m1.2.2.2.2.3.3.cmml" xref="S3.SS2.p1.3.m1.2.2.2.2.3.3">𝑎</ci><ci id="S3.SS2.p1.3.m1.2.2.2.2.3.4.cmml" xref="S3.SS2.p1.3.m1.2.2.2.2.3.4">𝑠</ci><ci id="S3.SS2.p1.3.m1.2.2.2.2.3.5.cmml" xref="S3.SS2.p1.3.m1.2.2.2.2.3.5">𝑘</ci></apply></apply><apply id="S3.SS2.p1.3.m1.3.3.3.3.cmml" xref="S3.SS2.p1.3.m1.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m1.3.3.3.3.1.cmml" xref="S3.SS2.p1.3.m1.3.3.3.3">subscript</csymbol><ci id="S3.SS2.p1.3.m1.3.3.3.3.2.cmml" xref="S3.SS2.p1.3.m1.3.3.3.3.2">𝜆</ci><apply id="S3.SS2.p1.3.m1.3.3.3.3.3.cmml" xref="S3.SS2.p1.3.m1.3.3.3.3.3"><times id="S3.SS2.p1.3.m1.3.3.3.3.3.1.cmml" xref="S3.SS2.p1.3.m1.3.3.3.3.3.1"></times><ci id="S3.SS2.p1.3.m1.3.3.3.3.3.2.cmml" xref="S3.SS2.p1.3.m1.3.3.3.3.3.2">𝑗</ci><cn id="S3.SS2.p1.3.m1.3.3.3.3.3.3.cmml" type="integer" xref="S3.SS2.p1.3.m1.3.3.3.3.3.3">3</cn><ci id="S3.SS2.p1.3.m1.3.3.3.3.3.4.cmml" xref="S3.SS2.p1.3.m1.3.3.3.3.3.4">𝑑</ci></apply></apply><apply id="S3.SS2.p1.3.m1.4.4.4.4.cmml" xref="S3.SS2.p1.3.m1.4.4.4.4"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m1.4.4.4.4.1.cmml" xref="S3.SS2.p1.3.m1.4.4.4.4">subscript</csymbol><ci id="S3.SS2.p1.3.m1.4.4.4.4.2.cmml" xref="S3.SS2.p1.3.m1.4.4.4.4.2">𝜆</ci><apply id="S3.SS2.p1.3.m1.4.4.4.4.3.cmml" xref="S3.SS2.p1.3.m1.4.4.4.4.3"><times id="S3.SS2.p1.3.m1.4.4.4.4.3.1.cmml" xref="S3.SS2.p1.3.m1.4.4.4.4.3.1"></times><ci id="S3.SS2.p1.3.m1.4.4.4.4.3.2.cmml" xref="S3.SS2.p1.3.m1.4.4.4.4.3.2">𝑚</ci><ci id="S3.SS2.p1.3.m1.4.4.4.4.3.3.cmml" xref="S3.SS2.p1.3.m1.4.4.4.4.3.3">𝑒</ci><ci id="S3.SS2.p1.3.m1.4.4.4.4.3.4.cmml" xref="S3.SS2.p1.3.m1.4.4.4.4.3.4">𝑠</ci><ci id="S3.SS2.p1.3.m1.4.4.4.4.3.5.cmml" xref="S3.SS2.p1.3.m1.4.4.4.4.3.5">ℎ</ci></apply></apply><apply id="S3.SS2.p1.3.m1.5.5.5.5.cmml" xref="S3.SS2.p1.3.m1.5.5.5.5"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m1.5.5.5.5.1.cmml" xref="S3.SS2.p1.3.m1.5.5.5.5">subscript</csymbol><ci id="S3.SS2.p1.3.m1.5.5.5.5.2.cmml" xref="S3.SS2.p1.3.m1.5.5.5.5.2">𝜆</ci><apply id="S3.SS2.p1.3.m1.5.5.5.5.3.cmml" xref="S3.SS2.p1.3.m1.5.5.5.5.3"><times id="S3.SS2.p1.3.m1.5.5.5.5.3.1.cmml" xref="S3.SS2.p1.3.m1.5.5.5.5.3.1"></times><ci id="S3.SS2.p1.3.m1.5.5.5.5.3.2.cmml" xref="S3.SS2.p1.3.m1.5.5.5.5.3.2">𝑟</ci><ci id="S3.SS2.p1.3.m1.5.5.5.5.3.3.cmml" xref="S3.SS2.p1.3.m1.5.5.5.5.3.3">𝑒</ci><ci id="S3.SS2.p1.3.m1.5.5.5.5.3.4.cmml" xref="S3.SS2.p1.3.m1.5.5.5.5.3.4">𝑔</ci></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m1.5c">\lambda_{j2d},\lambda_{mask},\lambda_{j3d},\lambda_{mesh},\lambda_{reg}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.3.m1.5d">italic_λ start_POSTSUBSCRIPT italic_j 2 italic_d end_POSTSUBSCRIPT , italic_λ start_POSTSUBSCRIPT italic_m italic_a italic_s italic_k end_POSTSUBSCRIPT , italic_λ start_POSTSUBSCRIPT italic_j 3 italic_d end_POSTSUBSCRIPT , italic_λ start_POSTSUBSCRIPT italic_m italic_e italic_s italic_h end_POSTSUBSCRIPT , italic_λ start_POSTSUBSCRIPT italic_r italic_e italic_g end_POSTSUBSCRIPT</annotation></semantics></math> are used to balance the weight of different errors. The diagram illustrating our annotation process is shown in <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S3.F3" title="In 3.2 Hand Pose Annotation ‣ 3 The ThermoHands Benchmark ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>. For more explanation of error terms and implementation details of our dataset annotation process, please refer to our supplementary document.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="240" id="S3.F3.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span class="ltx_text ltx_font_bold" id="S3.F3.2.1">Automatic annotation pipeline</span> of 3D hand pose. We utilize the multi-view RGB and depth images as the input source and retrieve constraint information with off-the-shelf MediaPipe Hands <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib99" title="">99</a>]</cite> and SAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib100" title="">100</a>]</cite>. Various error terms are formulated to optimize the MANO parameters.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>TherFormer: A Baseline Method</h3>
<div class="ltx_para ltx_noindent" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">For our benchmark, we setup a baseline method, dubbed TherFormer, for thermal image-based 3D hand pose estimation. As exhibited in <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S3.F4" title="In 3.3 TherFormer: A Baseline Method ‣ 3 The ThermoHands Benchmark ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>, TherFormer features in its two consecuative transformer modules to model the spatio-temporal relationship of hand joints while being computationally efficient. Note that TherFormer is also capable of processing other spectral images due to their format consistency with thermal images. However, our primary objective is to establish a baseline method for future research, rather than focusing exclusively on methods for thermal images.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="150" id="S3.F4.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span class="ltx_text ltx_font_bold" id="S3.F4.2.1">Overall Framework of TherFormer.</span> Backbone features are input to the mask-guided spatial transformer and temporal transformer to enhance the spatial representation and temporal interaction. Spatio-temporal embeddings are fed into the pose head to regress the 3D hand pose. </figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.6"><span class="ltx_text ltx_font_bold" id="S3.SS3.p2.6.1">Problem Definition.</span> We consider two problem settings: single image-based and video-based egocentric 3D hand pose estimation for our benchmark. In the former setting, we aim to estimate the 3D joint positions <math alttext="\mathcal{J}_{t}" class="ltx_Math" display="inline" id="S3.SS3.p2.1.m1.1"><semantics id="S3.SS3.p2.1.m1.1a"><msub id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">𝒥</mi><mi id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">𝒥</ci><ci id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">\mathcal{J}_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.1.m1.1d">caligraphic_J start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> for two hands given the single thermal image <math alttext="\mathcal{I}_{t}" class="ltx_Math" display="inline" id="S3.SS3.p2.2.m2.1"><semantics id="S3.SS3.p2.2.m2.1a"><msub id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.2.m2.1.1.2" xref="S3.SS3.p2.2.m2.1.1.2.cmml">ℐ</mi><mi id="S3.SS3.p2.2.m2.1.1.3" xref="S3.SS3.p2.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><apply id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p2.2.m2.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2">ℐ</ci><ci id="S3.SS3.p2.2.m2.1.1.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">\mathcal{I}_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.2.m2.1d">caligraphic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> captured for the <math alttext="t" class="ltx_Math" display="inline" id="S3.SS3.p2.3.m3.1"><semantics id="S3.SS3.p2.3.m3.1a"><mi id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><ci id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.3.m3.1d">italic_t</annotation></semantics></math>-th frame. For the video-based one, our input is a sequence of thermal images <math alttext="\mathcal{S}=\{\mathcal{I}_{i}\}_{i=1}^{T}" class="ltx_Math" display="inline" id="S3.SS3.p2.4.m4.1"><semantics id="S3.SS3.p2.4.m4.1a"><mrow id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.4.m4.1.1.3" xref="S3.SS3.p2.4.m4.1.1.3.cmml">𝒮</mi><mo id="S3.SS3.p2.4.m4.1.1.2" xref="S3.SS3.p2.4.m4.1.1.2.cmml">=</mo><msubsup id="S3.SS3.p2.4.m4.1.1.1" xref="S3.SS3.p2.4.m4.1.1.1.cmml"><mrow id="S3.SS3.p2.4.m4.1.1.1.1.1.1" xref="S3.SS3.p2.4.m4.1.1.1.1.1.2.cmml"><mo id="S3.SS3.p2.4.m4.1.1.1.1.1.1.2" stretchy="false" xref="S3.SS3.p2.4.m4.1.1.1.1.1.2.cmml">{</mo><msub id="S3.SS3.p2.4.m4.1.1.1.1.1.1.1" xref="S3.SS3.p2.4.m4.1.1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.4.m4.1.1.1.1.1.1.1.2" xref="S3.SS3.p2.4.m4.1.1.1.1.1.1.1.2.cmml">ℐ</mi><mi id="S3.SS3.p2.4.m4.1.1.1.1.1.1.1.3" xref="S3.SS3.p2.4.m4.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS3.p2.4.m4.1.1.1.1.1.1.3" stretchy="false" xref="S3.SS3.p2.4.m4.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS3.p2.4.m4.1.1.1.1.3" xref="S3.SS3.p2.4.m4.1.1.1.1.3.cmml"><mi id="S3.SS3.p2.4.m4.1.1.1.1.3.2" xref="S3.SS3.p2.4.m4.1.1.1.1.3.2.cmml">i</mi><mo id="S3.SS3.p2.4.m4.1.1.1.1.3.1" xref="S3.SS3.p2.4.m4.1.1.1.1.3.1.cmml">=</mo><mn id="S3.SS3.p2.4.m4.1.1.1.1.3.3" xref="S3.SS3.p2.4.m4.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS3.p2.4.m4.1.1.1.3" xref="S3.SS3.p2.4.m4.1.1.1.3.cmml">T</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><apply id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1"><eq id="S3.SS3.p2.4.m4.1.1.2.cmml" xref="S3.SS3.p2.4.m4.1.1.2"></eq><ci id="S3.SS3.p2.4.m4.1.1.3.cmml" xref="S3.SS3.p2.4.m4.1.1.3">𝒮</ci><apply id="S3.SS3.p2.4.m4.1.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m4.1.1.1.2.cmml" xref="S3.SS3.p2.4.m4.1.1.1">superscript</csymbol><apply id="S3.SS3.p2.4.m4.1.1.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m4.1.1.1.1.2.cmml" xref="S3.SS3.p2.4.m4.1.1.1">subscript</csymbol><set id="S3.SS3.p2.4.m4.1.1.1.1.1.2.cmml" xref="S3.SS3.p2.4.m4.1.1.1.1.1.1"><apply id="S3.SS3.p2.4.m4.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m4.1.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p2.4.m4.1.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p2.4.m4.1.1.1.1.1.1.1.2">ℐ</ci><ci id="S3.SS3.p2.4.m4.1.1.1.1.1.1.1.3.cmml" xref="S3.SS3.p2.4.m4.1.1.1.1.1.1.1.3">𝑖</ci></apply></set><apply id="S3.SS3.p2.4.m4.1.1.1.1.3.cmml" xref="S3.SS3.p2.4.m4.1.1.1.1.3"><eq id="S3.SS3.p2.4.m4.1.1.1.1.3.1.cmml" xref="S3.SS3.p2.4.m4.1.1.1.1.3.1"></eq><ci id="S3.SS3.p2.4.m4.1.1.1.1.3.2.cmml" xref="S3.SS3.p2.4.m4.1.1.1.1.3.2">𝑖</ci><cn id="S3.SS3.p2.4.m4.1.1.1.1.3.3.cmml" type="integer" xref="S3.SS3.p2.4.m4.1.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS3.p2.4.m4.1.1.1.3.cmml" xref="S3.SS3.p2.4.m4.1.1.1.3">𝑇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">\mathcal{S}=\{\mathcal{I}_{i}\}_{i=1}^{T}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.4.m4.1d">caligraphic_S = { caligraphic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT</annotation></semantics></math> and we estimate the per-frame 3D hand joint positions <math alttext="\mathcal{J}_{i}" class="ltx_Math" display="inline" id="S3.SS3.p2.5.m5.1"><semantics id="S3.SS3.p2.5.m5.1a"><msub id="S3.SS3.p2.5.m5.1.1" xref="S3.SS3.p2.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.5.m5.1.1.2" xref="S3.SS3.p2.5.m5.1.1.2.cmml">𝒥</mi><mi id="S3.SS3.p2.5.m5.1.1.3" xref="S3.SS3.p2.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m5.1b"><apply id="S3.SS3.p2.5.m5.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.5.m5.1.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS3.p2.5.m5.1.1.2.cmml" xref="S3.SS3.p2.5.m5.1.1.2">𝒥</ci><ci id="S3.SS3.p2.5.m5.1.1.3.cmml" xref="S3.SS3.p2.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m5.1c">\mathcal{J}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.5.m5.1d">caligraphic_J start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> together.
Without losing the generality, here we illustrate our network architecture for the video-based setting, <em class="ltx_emph ltx_font_italic" id="S3.SS3.p2.6.2">i.e</em>., TherFormer-V. In practice, our network can be flexibly adapted to the single image-based version (<em class="ltx_emph ltx_font_italic" id="S3.SS3.p2.6.3">i.e</em>., TherFormer-S) by setting <math alttext="T=1" class="ltx_Math" display="inline" id="S3.SS3.p2.6.m6.1"><semantics id="S3.SS3.p2.6.m6.1a"><mrow id="S3.SS3.p2.6.m6.1.1" xref="S3.SS3.p2.6.m6.1.1.cmml"><mi id="S3.SS3.p2.6.m6.1.1.2" xref="S3.SS3.p2.6.m6.1.1.2.cmml">T</mi><mo id="S3.SS3.p2.6.m6.1.1.1" xref="S3.SS3.p2.6.m6.1.1.1.cmml">=</mo><mn id="S3.SS3.p2.6.m6.1.1.3" xref="S3.SS3.p2.6.m6.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m6.1b"><apply id="S3.SS3.p2.6.m6.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1"><eq id="S3.SS3.p2.6.m6.1.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1.1"></eq><ci id="S3.SS3.p2.6.m6.1.1.2.cmml" xref="S3.SS3.p2.6.m6.1.1.2">𝑇</ci><cn id="S3.SS3.p2.6.m6.1.1.3.cmml" type="integer" xref="S3.SS3.p2.6.m6.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m6.1c">T=1</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.6.m6.1d">italic_T = 1</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p3.1.1">Mask-guided Spatial Transformer.</span> Human hands are highly articulated objects that can adopt a wide variety of poses, often against complex backgrounds. We propose a mask-guided spatial transformer module to accurately identify and focus on the intricacies of hand poses during spatial feature interaction. Given backbone features, we first utilize a mask head to estimate the binary hand mask in the thermal image. Then, we leverage the deformable self-attention <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib101" title="">101</a>]</cite> to refine the hand spatial features under the guidance of the estimated hand mask. Specifically, we only take feature elements whose spatial locations are within the hand area as queries and sample keys from only the hand area and its surrounding locations. In this way, we not only reduce the computation waste on the irrelevant region but also increase the robustness to background clutter. Lastly, we reduce the spatial dimensions of the spatial features with a series of convolutions layers for efficiency.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p4.1.1">Temporal Transformer.</span> Temporal information is crucial for 3D hand pose estimation when coping with occlusion and solving ambiguities. To model temporal relationships, we first flatten the spatial features into 1D feature vectors and then employed the temporal self-attention <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib102" title="">102</a>]</cite> to explicitly attend to the feature vector of every frame. The output is frame-wise spatio-temporal feature embeddings. Note that the temporal self-attention degrades to an MLP for single-image based setting.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p5">
<p class="ltx_p" id="S3.SS3.p5.1">In the pose head, we use the MLP to project the embeddings to the output space and obtain the per-frame 3D joint <math alttext="\mathcal{J}_{i}" class="ltx_Math" display="inline" id="S3.SS3.p5.1.m1.1"><semantics id="S3.SS3.p5.1.m1.1a"><msub id="S3.SS3.p5.1.m1.1.1" xref="S3.SS3.p5.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p5.1.m1.1.1.2" xref="S3.SS3.p5.1.m1.1.1.2.cmml">𝒥</mi><mi id="S3.SS3.p5.1.m1.1.1.3" xref="S3.SS3.p5.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.1.m1.1b"><apply id="S3.SS3.p5.1.m1.1.1.cmml" xref="S3.SS3.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p5.1.m1.1.1.1.cmml" xref="S3.SS3.p5.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p5.1.m1.1.1.2.cmml" xref="S3.SS3.p5.1.m1.1.1.2">𝒥</ci><ci id="S3.SS3.p5.1.m1.1.1.3.cmml" xref="S3.SS3.p5.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.1.m1.1c">\mathcal{J}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.1.m1.1d">caligraphic_J start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. We leverage the binary cross-entropy loss to supervise the hand segmentation with the mask ground truth rendered from the annotated hand mesh (<em class="ltx_emph ltx_font_italic" id="S3.SS3.p5.1.1">cf</em>. <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S3.SS2" title="3.2 Hand Pose Annotation ‣ 3 The ThermoHands Benchmark ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>). For 3D hand joint positions, we measure the <em class="ltx_emph ltx_font_italic" id="S3.SS3.p5.1.2">L1</em> distance of its 2D projection and depth to that of the ground truth separately. Please see more details of TherFormer in our supplementary document.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiment</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Evaluation of the Annotation Method</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">As the first step, we validate the accuracy of our 3D hand pose annotation (<em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.1.1">cf</em>. <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S3.SS2" title="3.2 Hand Pose Annotation ‣ 3 The ThermoHands Benchmark ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>) and analyze the impact for optimization results. For evaluation, we manually annotate two random sequences from our main dataset, with a total of over 600 frames. To that end, we first annotate the 2D joint locations from both egocentric and exocentric images and obtain the 3D joint positions by triangulation. We calculate the average 3D joint errors across all frames to measure the accuracy.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.4">As shown in <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S4.T2" title="In 4.1 Evaluation of the Annotation Method ‣ 4 Experiment ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">2</span></a>, our annotation method achieves an average joint error of nearly 1cm, comparable to the results of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib37" title="">37</a>]</cite>. The multi-view setting shows remarkably better precision than the ego-view only optimization, demonstrating the necessity of multi-camera capture. We also observe that only combining <math alttext="\mathcal{L}_{mask}" class="ltx_Math" display="inline" id="S4.SS1.p2.1.m1.1"><semantics id="S4.SS1.p2.1.m1.1a"><msub id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2.cmml">ℒ</mi><mrow id="S4.SS1.p2.1.m1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.3.cmml"><mi id="S4.SS1.p2.1.m1.1.1.3.2" xref="S4.SS1.p2.1.m1.1.1.3.2.cmml">m</mi><mo id="S4.SS1.p2.1.m1.1.1.3.1" xref="S4.SS1.p2.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p2.1.m1.1.1.3.3" xref="S4.SS1.p2.1.m1.1.1.3.3.cmml">a</mi><mo id="S4.SS1.p2.1.m1.1.1.3.1a" xref="S4.SS1.p2.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p2.1.m1.1.1.3.4" xref="S4.SS1.p2.1.m1.1.1.3.4.cmml">s</mi><mo id="S4.SS1.p2.1.m1.1.1.3.1b" xref="S4.SS1.p2.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p2.1.m1.1.1.3.5" xref="S4.SS1.p2.1.m1.1.1.3.5.cmml">k</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2">ℒ</ci><apply id="S4.SS1.p2.1.m1.1.1.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3"><times id="S4.SS1.p2.1.m1.1.1.3.1.cmml" xref="S4.SS1.p2.1.m1.1.1.3.1"></times><ci id="S4.SS1.p2.1.m1.1.1.3.2.cmml" xref="S4.SS1.p2.1.m1.1.1.3.2">𝑚</ci><ci id="S4.SS1.p2.1.m1.1.1.3.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3.3">𝑎</ci><ci id="S4.SS1.p2.1.m1.1.1.3.4.cmml" xref="S4.SS1.p2.1.m1.1.1.3.4">𝑠</ci><ci id="S4.SS1.p2.1.m1.1.1.3.5.cmml" xref="S4.SS1.p2.1.m1.1.1.3.5">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">\mathcal{L}_{mask}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_m italic_a italic_s italic_k end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\mathcal{L}_{j2d}" class="ltx_Math" display="inline" id="S4.SS1.p2.2.m2.1"><semantics id="S4.SS1.p2.2.m2.1a"><msub id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p2.2.m2.1.1.2" xref="S4.SS1.p2.2.m2.1.1.2.cmml">ℒ</mi><mrow id="S4.SS1.p2.2.m2.1.1.3" xref="S4.SS1.p2.2.m2.1.1.3.cmml"><mi id="S4.SS1.p2.2.m2.1.1.3.2" xref="S4.SS1.p2.2.m2.1.1.3.2.cmml">j</mi><mo id="S4.SS1.p2.2.m2.1.1.3.1" xref="S4.SS1.p2.2.m2.1.1.3.1.cmml">⁢</mo><mn id="S4.SS1.p2.2.m2.1.1.3.3" xref="S4.SS1.p2.2.m2.1.1.3.3.cmml">2</mn><mo id="S4.SS1.p2.2.m2.1.1.3.1a" xref="S4.SS1.p2.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p2.2.m2.1.1.3.4" xref="S4.SS1.p2.2.m2.1.1.3.4.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><apply id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.2.m2.1.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.p2.2.m2.1.1.2.cmml" xref="S4.SS1.p2.2.m2.1.1.2">ℒ</ci><apply id="S4.SS1.p2.2.m2.1.1.3.cmml" xref="S4.SS1.p2.2.m2.1.1.3"><times id="S4.SS1.p2.2.m2.1.1.3.1.cmml" xref="S4.SS1.p2.2.m2.1.1.3.1"></times><ci id="S4.SS1.p2.2.m2.1.1.3.2.cmml" xref="S4.SS1.p2.2.m2.1.1.3.2">𝑗</ci><cn id="S4.SS1.p2.2.m2.1.1.3.3.cmml" type="integer" xref="S4.SS1.p2.2.m2.1.1.3.3">2</cn><ci id="S4.SS1.p2.2.m2.1.1.3.4.cmml" xref="S4.SS1.p2.2.m2.1.1.3.4">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">\mathcal{L}_{j2d}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.2.m2.1d">caligraphic_L start_POSTSUBSCRIPT italic_j 2 italic_d end_POSTSUBSCRIPT</annotation></semantics></math> can already provide a plausible accuracy since they fit the projection of the 3D hand pose to two heterogeneous views. <math alttext="\mathcal{L}_{mesh}" class="ltx_Math" display="inline" id="S4.SS1.p2.3.m3.1"><semantics id="S4.SS1.p2.3.m3.1a"><msub id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p2.3.m3.1.1.2" xref="S4.SS1.p2.3.m3.1.1.2.cmml">ℒ</mi><mrow id="S4.SS1.p2.3.m3.1.1.3" xref="S4.SS1.p2.3.m3.1.1.3.cmml"><mi id="S4.SS1.p2.3.m3.1.1.3.2" xref="S4.SS1.p2.3.m3.1.1.3.2.cmml">m</mi><mo id="S4.SS1.p2.3.m3.1.1.3.1" xref="S4.SS1.p2.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p2.3.m3.1.1.3.3" xref="S4.SS1.p2.3.m3.1.1.3.3.cmml">e</mi><mo id="S4.SS1.p2.3.m3.1.1.3.1a" xref="S4.SS1.p2.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p2.3.m3.1.1.3.4" xref="S4.SS1.p2.3.m3.1.1.3.4.cmml">s</mi><mo id="S4.SS1.p2.3.m3.1.1.3.1b" xref="S4.SS1.p2.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p2.3.m3.1.1.3.5" xref="S4.SS1.p2.3.m3.1.1.3.5.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><apply id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.3.m3.1.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1">subscript</csymbol><ci id="S4.SS1.p2.3.m3.1.1.2.cmml" xref="S4.SS1.p2.3.m3.1.1.2">ℒ</ci><apply id="S4.SS1.p2.3.m3.1.1.3.cmml" xref="S4.SS1.p2.3.m3.1.1.3"><times id="S4.SS1.p2.3.m3.1.1.3.1.cmml" xref="S4.SS1.p2.3.m3.1.1.3.1"></times><ci id="S4.SS1.p2.3.m3.1.1.3.2.cmml" xref="S4.SS1.p2.3.m3.1.1.3.2">𝑚</ci><ci id="S4.SS1.p2.3.m3.1.1.3.3.cmml" xref="S4.SS1.p2.3.m3.1.1.3.3">𝑒</ci><ci id="S4.SS1.p2.3.m3.1.1.3.4.cmml" xref="S4.SS1.p2.3.m3.1.1.3.4">𝑠</ci><ci id="S4.SS1.p2.3.m3.1.1.3.5.cmml" xref="S4.SS1.p2.3.m3.1.1.3.5">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">\mathcal{L}_{mesh}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.3.m3.1d">caligraphic_L start_POSTSUBSCRIPT italic_m italic_e italic_s italic_h end_POSTSUBSCRIPT</annotation></semantics></math>, though it hardly improves the joint accuracy, can result in more natural hand mesh. Adding <math alttext="\mathcal{L}_{j3d}" class="ltx_Math" display="inline" id="S4.SS1.p2.4.m4.1"><semantics id="S4.SS1.p2.4.m4.1a"><msub id="S4.SS1.p2.4.m4.1.1" xref="S4.SS1.p2.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p2.4.m4.1.1.2" xref="S4.SS1.p2.4.m4.1.1.2.cmml">ℒ</mi><mrow id="S4.SS1.p2.4.m4.1.1.3" xref="S4.SS1.p2.4.m4.1.1.3.cmml"><mi id="S4.SS1.p2.4.m4.1.1.3.2" xref="S4.SS1.p2.4.m4.1.1.3.2.cmml">j</mi><mo id="S4.SS1.p2.4.m4.1.1.3.1" xref="S4.SS1.p2.4.m4.1.1.3.1.cmml">⁢</mo><mn id="S4.SS1.p2.4.m4.1.1.3.3" xref="S4.SS1.p2.4.m4.1.1.3.3.cmml">3</mn><mo id="S4.SS1.p2.4.m4.1.1.3.1a" xref="S4.SS1.p2.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p2.4.m4.1.1.3.4" xref="S4.SS1.p2.4.m4.1.1.3.4.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.4.m4.1b"><apply id="S4.SS1.p2.4.m4.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.4.m4.1.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1">subscript</csymbol><ci id="S4.SS1.p2.4.m4.1.1.2.cmml" xref="S4.SS1.p2.4.m4.1.1.2">ℒ</ci><apply id="S4.SS1.p2.4.m4.1.1.3.cmml" xref="S4.SS1.p2.4.m4.1.1.3"><times id="S4.SS1.p2.4.m4.1.1.3.1.cmml" xref="S4.SS1.p2.4.m4.1.1.3.1"></times><ci id="S4.SS1.p2.4.m4.1.1.3.2.cmml" xref="S4.SS1.p2.4.m4.1.1.3.2">𝑗</ci><cn id="S4.SS1.p2.4.m4.1.1.3.3.cmml" type="integer" xref="S4.SS1.p2.4.m4.1.1.3.3">3</cn><ci id="S4.SS1.p2.4.m4.1.1.3.4.cmml" xref="S4.SS1.p2.4.m4.1.1.3.4">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.4.m4.1c">\mathcal{L}_{j3d}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.4.m4.1d">caligraphic_L start_POSTSUBSCRIPT italic_j 3 italic_d end_POSTSUBSCRIPT</annotation></semantics></math> further refines the joints as it induces the explicit constraint to their positions. We showcase some annotation examples in <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S4.F5" title="In 4.1 Evaluation of the Annotation Method ‣ 4 Experiment ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>. As can be seen, both hand joint and mesh can be accurately annotated across different actions despite the presence of occlusion and the variance in subjects’ hand color and shape.</p>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="166" id="S4.F5.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Examples of 3D hand pose annotations. Top row: left (<span class="ltx_text" id="S4.F5.3.1" style="color:#0000FF;">blue</span>) and right (<span class="ltx_text" id="S4.F5.4.2" style="color:#FF0000;">red</span>) hand 3D joints projected onto egocentric RGB images. Bottom row: visualization of hand mesh annotation.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.12" style="width:433.6pt;height:49.1pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-49.9pt,5.6pt) scale(0.812957115875051,0.812957115875051) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.12.12">
<tr class="ltx_tr" id="S4.T2.12.12.13">
<td class="ltx_td ltx_border_tt" id="S4.T2.12.12.13.1" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T2.12.12.13.2" style="padding-left:3.0pt;padding-right:3.0pt;">Ego-view optimization</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S4.T2.12.12.13.3" style="padding-left:3.0pt;padding-right:3.0pt;">Multi-view optimization</td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.6.6">
<td class="ltx_td ltx_align_left" id="S4.T2.6.6.6.7" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.6.6.6.7.1">Errors</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.1.1.1.1.1"><math alttext="\mathcal{L}_{mask}+\mathcal{L}_{j2d}" class="ltx_Math" display="inline" id="S4.T2.1.1.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.1.1.m1.1a"><mrow id="S4.T2.1.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.1.m1.1.1.cmml"><msub id="S4.T2.1.1.1.1.1.m1.1.1.2" xref="S4.T2.1.1.1.1.1.m1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.T2.1.1.1.1.1.m1.1.1.2.2" xref="S4.T2.1.1.1.1.1.m1.1.1.2.2.cmml">ℒ</mi><mrow id="S4.T2.1.1.1.1.1.m1.1.1.2.3" xref="S4.T2.1.1.1.1.1.m1.1.1.2.3.cmml"><mi id="S4.T2.1.1.1.1.1.m1.1.1.2.3.2" xref="S4.T2.1.1.1.1.1.m1.1.1.2.3.2.cmml">m</mi><mo id="S4.T2.1.1.1.1.1.m1.1.1.2.3.1" xref="S4.T2.1.1.1.1.1.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S4.T2.1.1.1.1.1.m1.1.1.2.3.3" xref="S4.T2.1.1.1.1.1.m1.1.1.2.3.3.cmml">a</mi><mo id="S4.T2.1.1.1.1.1.m1.1.1.2.3.1a" xref="S4.T2.1.1.1.1.1.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S4.T2.1.1.1.1.1.m1.1.1.2.3.4" xref="S4.T2.1.1.1.1.1.m1.1.1.2.3.4.cmml">s</mi><mo id="S4.T2.1.1.1.1.1.m1.1.1.2.3.1b" xref="S4.T2.1.1.1.1.1.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S4.T2.1.1.1.1.1.m1.1.1.2.3.5" xref="S4.T2.1.1.1.1.1.m1.1.1.2.3.5.cmml">k</mi></mrow></msub><mo id="S4.T2.1.1.1.1.1.m1.1.1.1" xref="S4.T2.1.1.1.1.1.m1.1.1.1.cmml">+</mo><msub id="S4.T2.1.1.1.1.1.m1.1.1.3" xref="S4.T2.1.1.1.1.1.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.T2.1.1.1.1.1.m1.1.1.3.2" xref="S4.T2.1.1.1.1.1.m1.1.1.3.2.cmml">ℒ</mi><mrow id="S4.T2.1.1.1.1.1.m1.1.1.3.3" xref="S4.T2.1.1.1.1.1.m1.1.1.3.3.cmml"><mi id="S4.T2.1.1.1.1.1.m1.1.1.3.3.2" xref="S4.T2.1.1.1.1.1.m1.1.1.3.3.2.cmml">j</mi><mo id="S4.T2.1.1.1.1.1.m1.1.1.3.3.1" xref="S4.T2.1.1.1.1.1.m1.1.1.3.3.1.cmml">⁢</mo><mn id="S4.T2.1.1.1.1.1.m1.1.1.3.3.3" xref="S4.T2.1.1.1.1.1.m1.1.1.3.3.3.cmml">2</mn><mo id="S4.T2.1.1.1.1.1.m1.1.1.3.3.1a" xref="S4.T2.1.1.1.1.1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S4.T2.1.1.1.1.1.m1.1.1.3.3.4" xref="S4.T2.1.1.1.1.1.m1.1.1.3.3.4.cmml">d</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.1.m1.1b"><apply id="S4.T2.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.1.m1.1.1"><plus id="S4.T2.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T2.1.1.1.1.1.m1.1.1.1"></plus><apply id="S4.T2.1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T2.1.1.1.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.T2.1.1.1.1.1.m1.1.1.2.1.cmml" xref="S4.T2.1.1.1.1.1.m1.1.1.2">subscript</csymbol><ci id="S4.T2.1.1.1.1.1.m1.1.1.2.2.cmml" xref="S4.T2.1.1.1.1.1.m1.1.1.2.2">ℒ</ci><apply id="S4.T2.1.1.1.1.1.m1.1.1.2.3.cmml" xref="S4.T2.1.1.1.1.1.m1.1.1.2.3"><times id="S4.T2.1.1.1.1.1.m1.1.1.2.3.1.cmml" xref="S4.T2.1.1.1.1.1.m1.1.1.2.3.1"></times><ci id="S4.T2.1.1.1.1.1.m1.1.1.2.3.2.cmml" xref="S4.T2.1.1.1.1.1.m1.1.1.2.3.2">𝑚</ci><ci id="S4.T2.1.1.1.1.1.m1.1.1.2.3.3.cmml" xref="S4.T2.1.1.1.1.1.m1.1.1.2.3.3">𝑎</ci><ci id="S4.T2.1.1.1.1.1.m1.1.1.2.3.4.cmml" xref="S4.T2.1.1.1.1.1.m1.1.1.2.3.4">𝑠</ci><ci id="S4.T2.1.1.1.1.1.m1.1.1.2.3.5.cmml" xref="S4.T2.1.1.1.1.1.m1.1.1.2.3.5">𝑘</ci></apply></apply><apply id="S4.T2.1.1.1.1.1.m1.1.1.3.cmml" xref="S4.T2.1.1.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T2.1.1.1.1.1.m1.1.1.3.1.cmml" xref="S4.T2.1.1.1.1.1.m1.1.1.3">subscript</csymbol><ci id="S4.T2.1.1.1.1.1.m1.1.1.3.2.cmml" xref="S4.T2.1.1.1.1.1.m1.1.1.3.2">ℒ</ci><apply id="S4.T2.1.1.1.1.1.m1.1.1.3.3.cmml" xref="S4.T2.1.1.1.1.1.m1.1.1.3.3"><times id="S4.T2.1.1.1.1.1.m1.1.1.3.3.1.cmml" xref="S4.T2.1.1.1.1.1.m1.1.1.3.3.1"></times><ci id="S4.T2.1.1.1.1.1.m1.1.1.3.3.2.cmml" xref="S4.T2.1.1.1.1.1.m1.1.1.3.3.2">𝑗</ci><cn id="S4.T2.1.1.1.1.1.m1.1.1.3.3.3.cmml" type="integer" xref="S4.T2.1.1.1.1.1.m1.1.1.3.3.3">2</cn><ci id="S4.T2.1.1.1.1.1.m1.1.1.3.3.4.cmml" xref="S4.T2.1.1.1.1.1.m1.1.1.3.3.4">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.1.m1.1c">\mathcal{L}_{mask}+\mathcal{L}_{j2d}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.1.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_m italic_a italic_s italic_k end_POSTSUBSCRIPT + caligraphic_L start_POSTSUBSCRIPT italic_j 2 italic_d end_POSTSUBSCRIPT</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.2.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.2.2.2.2.1"><span class="ltx_text" id="S4.T2.2.2.2.2.1.2"></span> <span class="ltx_text" id="S4.T2.2.2.2.2.1.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.2.2.2.2.1.1.1">
<span class="ltx_tr" id="S4.T2.2.2.2.2.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.2.2.2.2.1.1.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><math alttext="\mathcal{L}_{mask}+\mathcal{L}_{j2d}+\mathcal{L}_{mesh}" class="ltx_Math" display="inline" id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1"><semantics id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1a"><mrow id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.cmml"><msub id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2.2" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2.2.cmml">ℒ</mi><mrow id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2.3" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2.3.cmml"><mi id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2.3.2" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2.3.2.cmml">m</mi><mo id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2.3.1" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2.3.3" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2.3.3.cmml">a</mi><mo id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2.3.1a" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2.3.4" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2.3.4.cmml">s</mi><mo id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2.3.1b" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2.3.5" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2.3.5.cmml">k</mi></mrow></msub><mo id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.1" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.1.cmml">+</mo><msub id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.3" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.3.2" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.3.2.cmml">ℒ</mi><mrow id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.3.3" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.3.3.cmml"><mi id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.3.3.2" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.3.3.2.cmml">j</mi><mo id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.3.3.1" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.3.3.1.cmml">⁢</mo><mn id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.3.3.3" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.3.3.3.cmml">2</mn><mo id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.3.3.1a" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.3.3.4" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.3.3.4.cmml">d</mi></mrow></msub><mo id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.1a" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.1.cmml">+</mo><msub id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4.2" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4.2.cmml">ℒ</mi><mrow id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4.3" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4.3.cmml"><mi id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4.3.2" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4.3.2.cmml">m</mi><mo id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4.3.1" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4.3.1.cmml">⁢</mo><mi id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4.3.3" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4.3.3.cmml">e</mi><mo id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4.3.1a" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4.3.1.cmml">⁢</mo><mi id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4.3.4" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4.3.4.cmml">s</mi><mo id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4.3.1b" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4.3.1.cmml">⁢</mo><mi id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4.3.5" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4.3.5.cmml">h</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1b"><apply id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1"><plus id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.1"></plus><apply id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2.1.cmml" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2">subscript</csymbol><ci id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2.2.cmml" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2.2">ℒ</ci><apply id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2.3.cmml" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2.3"><times id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2.3.1.cmml" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2.3.1"></times><ci id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2.3.2.cmml" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2.3.2">𝑚</ci><ci id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2.3.3.cmml" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2.3.3">𝑎</ci><ci id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2.3.4.cmml" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2.3.4">𝑠</ci><ci id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2.3.5.cmml" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.2.3.5">𝑘</ci></apply></apply><apply id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.3.cmml" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.3.1.cmml" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.3">subscript</csymbol><ci id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.3.2.cmml" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.3.2">ℒ</ci><apply id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.3.3.cmml" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.3.3"><times id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.3.3.1.cmml" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.3.3.1"></times><ci id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.3.3.2.cmml" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.3.3.2">𝑗</ci><cn id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.3.3.3.cmml" type="integer" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.3.3.3">2</cn><ci id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.3.3.4.cmml" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.3.3.4">𝑑</ci></apply></apply><apply id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4.cmml" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4"><csymbol cd="ambiguous" id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4.1.cmml" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4">subscript</csymbol><ci id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4.2.cmml" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4.2">ℒ</ci><apply id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4.3.cmml" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4.3"><times id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4.3.1.cmml" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4.3.1"></times><ci id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4.3.2.cmml" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4.3.2">𝑚</ci><ci id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4.3.3.cmml" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4.3.3">𝑒</ci><ci id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4.3.4.cmml" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4.3.4">𝑠</ci><ci id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4.3.5.cmml" xref="S4.T2.2.2.2.2.1.1.1.1.1.m1.1.1.4.3.5">ℎ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1c">\mathcal{L}_{mask}+\mathcal{L}_{j2d}+\mathcal{L}_{mesh}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.2.2.1.1.1.1.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_m italic_a italic_s italic_k end_POSTSUBSCRIPT + caligraphic_L start_POSTSUBSCRIPT italic_j 2 italic_d end_POSTSUBSCRIPT + caligraphic_L start_POSTSUBSCRIPT italic_m italic_e italic_s italic_h end_POSTSUBSCRIPT</annotation></semantics></math></span></span>
</span></span> <span class="ltx_text" id="S4.T2.2.2.2.2.1.3"></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.3.3.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.3.3.3.3.1"><math alttext="\mathcal{L}_{mask}" class="ltx_Math" display="inline" id="S4.T2.3.3.3.3.1.m1.1"><semantics id="S4.T2.3.3.3.3.1.m1.1a"><msub id="S4.T2.3.3.3.3.1.m1.1.1" xref="S4.T2.3.3.3.3.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.T2.3.3.3.3.1.m1.1.1.2" xref="S4.T2.3.3.3.3.1.m1.1.1.2.cmml">ℒ</mi><mrow id="S4.T2.3.3.3.3.1.m1.1.1.3" xref="S4.T2.3.3.3.3.1.m1.1.1.3.cmml"><mi id="S4.T2.3.3.3.3.1.m1.1.1.3.2" xref="S4.T2.3.3.3.3.1.m1.1.1.3.2.cmml">m</mi><mo id="S4.T2.3.3.3.3.1.m1.1.1.3.1" xref="S4.T2.3.3.3.3.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.T2.3.3.3.3.1.m1.1.1.3.3" xref="S4.T2.3.3.3.3.1.m1.1.1.3.3.cmml">a</mi><mo id="S4.T2.3.3.3.3.1.m1.1.1.3.1a" xref="S4.T2.3.3.3.3.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.T2.3.3.3.3.1.m1.1.1.3.4" xref="S4.T2.3.3.3.3.1.m1.1.1.3.4.cmml">s</mi><mo id="S4.T2.3.3.3.3.1.m1.1.1.3.1b" xref="S4.T2.3.3.3.3.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.T2.3.3.3.3.1.m1.1.1.3.5" xref="S4.T2.3.3.3.3.1.m1.1.1.3.5.cmml">k</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.3.1.m1.1b"><apply id="S4.T2.3.3.3.3.1.m1.1.1.cmml" xref="S4.T2.3.3.3.3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.3.3.3.3.1.m1.1.1.1.cmml" xref="S4.T2.3.3.3.3.1.m1.1.1">subscript</csymbol><ci id="S4.T2.3.3.3.3.1.m1.1.1.2.cmml" xref="S4.T2.3.3.3.3.1.m1.1.1.2">ℒ</ci><apply id="S4.T2.3.3.3.3.1.m1.1.1.3.cmml" xref="S4.T2.3.3.3.3.1.m1.1.1.3"><times id="S4.T2.3.3.3.3.1.m1.1.1.3.1.cmml" xref="S4.T2.3.3.3.3.1.m1.1.1.3.1"></times><ci id="S4.T2.3.3.3.3.1.m1.1.1.3.2.cmml" xref="S4.T2.3.3.3.3.1.m1.1.1.3.2">𝑚</ci><ci id="S4.T2.3.3.3.3.1.m1.1.1.3.3.cmml" xref="S4.T2.3.3.3.3.1.m1.1.1.3.3">𝑎</ci><ci id="S4.T2.3.3.3.3.1.m1.1.1.3.4.cmml" xref="S4.T2.3.3.3.3.1.m1.1.1.3.4">𝑠</ci><ci id="S4.T2.3.3.3.3.1.m1.1.1.3.5.cmml" xref="S4.T2.3.3.3.3.1.m1.1.1.3.5">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.3.1.m1.1c">\mathcal{L}_{mask}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.3.3.3.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_m italic_a italic_s italic_k end_POSTSUBSCRIPT</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.4.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.4.4.4.4.1"><math alttext="\mathcal{L}_{mask}+\mathcal{L}_{j2d}" class="ltx_Math" display="inline" id="S4.T2.4.4.4.4.1.m1.1"><semantics id="S4.T2.4.4.4.4.1.m1.1a"><mrow id="S4.T2.4.4.4.4.1.m1.1.1" xref="S4.T2.4.4.4.4.1.m1.1.1.cmml"><msub id="S4.T2.4.4.4.4.1.m1.1.1.2" xref="S4.T2.4.4.4.4.1.m1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.T2.4.4.4.4.1.m1.1.1.2.2" xref="S4.T2.4.4.4.4.1.m1.1.1.2.2.cmml">ℒ</mi><mrow id="S4.T2.4.4.4.4.1.m1.1.1.2.3" xref="S4.T2.4.4.4.4.1.m1.1.1.2.3.cmml"><mi id="S4.T2.4.4.4.4.1.m1.1.1.2.3.2" xref="S4.T2.4.4.4.4.1.m1.1.1.2.3.2.cmml">m</mi><mo id="S4.T2.4.4.4.4.1.m1.1.1.2.3.1" xref="S4.T2.4.4.4.4.1.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S4.T2.4.4.4.4.1.m1.1.1.2.3.3" xref="S4.T2.4.4.4.4.1.m1.1.1.2.3.3.cmml">a</mi><mo id="S4.T2.4.4.4.4.1.m1.1.1.2.3.1a" xref="S4.T2.4.4.4.4.1.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S4.T2.4.4.4.4.1.m1.1.1.2.3.4" xref="S4.T2.4.4.4.4.1.m1.1.1.2.3.4.cmml">s</mi><mo id="S4.T2.4.4.4.4.1.m1.1.1.2.3.1b" xref="S4.T2.4.4.4.4.1.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S4.T2.4.4.4.4.1.m1.1.1.2.3.5" xref="S4.T2.4.4.4.4.1.m1.1.1.2.3.5.cmml">k</mi></mrow></msub><mo id="S4.T2.4.4.4.4.1.m1.1.1.1" xref="S4.T2.4.4.4.4.1.m1.1.1.1.cmml">+</mo><msub id="S4.T2.4.4.4.4.1.m1.1.1.3" xref="S4.T2.4.4.4.4.1.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.T2.4.4.4.4.1.m1.1.1.3.2" xref="S4.T2.4.4.4.4.1.m1.1.1.3.2.cmml">ℒ</mi><mrow id="S4.T2.4.4.4.4.1.m1.1.1.3.3" xref="S4.T2.4.4.4.4.1.m1.1.1.3.3.cmml"><mi id="S4.T2.4.4.4.4.1.m1.1.1.3.3.2" xref="S4.T2.4.4.4.4.1.m1.1.1.3.3.2.cmml">j</mi><mo id="S4.T2.4.4.4.4.1.m1.1.1.3.3.1" xref="S4.T2.4.4.4.4.1.m1.1.1.3.3.1.cmml">⁢</mo><mn id="S4.T2.4.4.4.4.1.m1.1.1.3.3.3" xref="S4.T2.4.4.4.4.1.m1.1.1.3.3.3.cmml">2</mn><mo id="S4.T2.4.4.4.4.1.m1.1.1.3.3.1a" xref="S4.T2.4.4.4.4.1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S4.T2.4.4.4.4.1.m1.1.1.3.3.4" xref="S4.T2.4.4.4.4.1.m1.1.1.3.3.4.cmml">d</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.4.1.m1.1b"><apply id="S4.T2.4.4.4.4.1.m1.1.1.cmml" xref="S4.T2.4.4.4.4.1.m1.1.1"><plus id="S4.T2.4.4.4.4.1.m1.1.1.1.cmml" xref="S4.T2.4.4.4.4.1.m1.1.1.1"></plus><apply id="S4.T2.4.4.4.4.1.m1.1.1.2.cmml" xref="S4.T2.4.4.4.4.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.T2.4.4.4.4.1.m1.1.1.2.1.cmml" xref="S4.T2.4.4.4.4.1.m1.1.1.2">subscript</csymbol><ci id="S4.T2.4.4.4.4.1.m1.1.1.2.2.cmml" xref="S4.T2.4.4.4.4.1.m1.1.1.2.2">ℒ</ci><apply id="S4.T2.4.4.4.4.1.m1.1.1.2.3.cmml" xref="S4.T2.4.4.4.4.1.m1.1.1.2.3"><times id="S4.T2.4.4.4.4.1.m1.1.1.2.3.1.cmml" xref="S4.T2.4.4.4.4.1.m1.1.1.2.3.1"></times><ci id="S4.T2.4.4.4.4.1.m1.1.1.2.3.2.cmml" xref="S4.T2.4.4.4.4.1.m1.1.1.2.3.2">𝑚</ci><ci id="S4.T2.4.4.4.4.1.m1.1.1.2.3.3.cmml" xref="S4.T2.4.4.4.4.1.m1.1.1.2.3.3">𝑎</ci><ci id="S4.T2.4.4.4.4.1.m1.1.1.2.3.4.cmml" xref="S4.T2.4.4.4.4.1.m1.1.1.2.3.4">𝑠</ci><ci id="S4.T2.4.4.4.4.1.m1.1.1.2.3.5.cmml" xref="S4.T2.4.4.4.4.1.m1.1.1.2.3.5">𝑘</ci></apply></apply><apply id="S4.T2.4.4.4.4.1.m1.1.1.3.cmml" xref="S4.T2.4.4.4.4.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T2.4.4.4.4.1.m1.1.1.3.1.cmml" xref="S4.T2.4.4.4.4.1.m1.1.1.3">subscript</csymbol><ci id="S4.T2.4.4.4.4.1.m1.1.1.3.2.cmml" xref="S4.T2.4.4.4.4.1.m1.1.1.3.2">ℒ</ci><apply id="S4.T2.4.4.4.4.1.m1.1.1.3.3.cmml" xref="S4.T2.4.4.4.4.1.m1.1.1.3.3"><times id="S4.T2.4.4.4.4.1.m1.1.1.3.3.1.cmml" xref="S4.T2.4.4.4.4.1.m1.1.1.3.3.1"></times><ci id="S4.T2.4.4.4.4.1.m1.1.1.3.3.2.cmml" xref="S4.T2.4.4.4.4.1.m1.1.1.3.3.2">𝑗</ci><cn id="S4.T2.4.4.4.4.1.m1.1.1.3.3.3.cmml" type="integer" xref="S4.T2.4.4.4.4.1.m1.1.1.3.3.3">2</cn><ci id="S4.T2.4.4.4.4.1.m1.1.1.3.3.4.cmml" xref="S4.T2.4.4.4.4.1.m1.1.1.3.3.4">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.4.1.m1.1c">\mathcal{L}_{mask}+\mathcal{L}_{j2d}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.4.4.4.4.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_m italic_a italic_s italic_k end_POSTSUBSCRIPT + caligraphic_L start_POSTSUBSCRIPT italic_j 2 italic_d end_POSTSUBSCRIPT</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.5.5.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.5.5.5.5.1"><span class="ltx_text" id="S4.T2.5.5.5.5.1.2"></span> <span class="ltx_text" id="S4.T2.5.5.5.5.1.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.5.5.5.5.1.1.1">
<span class="ltx_tr" id="S4.T2.5.5.5.5.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.5.5.5.5.1.1.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><math alttext="\mathcal{L}_{mask}+\mathcal{L}_{j2d}+\mathcal{L}_{mesh}" class="ltx_Math" display="inline" id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1"><semantics id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1a"><mrow id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.cmml"><msub id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2.2" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2.2.cmml">ℒ</mi><mrow id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2.3" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2.3.cmml"><mi id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2.3.2" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2.3.2.cmml">m</mi><mo id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2.3.1" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2.3.3" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2.3.3.cmml">a</mi><mo id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2.3.1a" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2.3.4" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2.3.4.cmml">s</mi><mo id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2.3.1b" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2.3.5" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2.3.5.cmml">k</mi></mrow></msub><mo id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.1" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.1.cmml">+</mo><msub id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.3" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.3.2" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.3.2.cmml">ℒ</mi><mrow id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.3.3" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.3.3.cmml"><mi id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.3.3.2" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.3.3.2.cmml">j</mi><mo id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.3.3.1" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.3.3.1.cmml">⁢</mo><mn id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.3.3.3" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.3.3.3.cmml">2</mn><mo id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.3.3.1a" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.3.3.4" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.3.3.4.cmml">d</mi></mrow></msub><mo id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.1a" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.1.cmml">+</mo><msub id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4.2" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4.2.cmml">ℒ</mi><mrow id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4.3" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4.3.cmml"><mi id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4.3.2" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4.3.2.cmml">m</mi><mo id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4.3.1" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4.3.1.cmml">⁢</mo><mi id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4.3.3" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4.3.3.cmml">e</mi><mo id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4.3.1a" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4.3.1.cmml">⁢</mo><mi id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4.3.4" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4.3.4.cmml">s</mi><mo id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4.3.1b" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4.3.1.cmml">⁢</mo><mi id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4.3.5" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4.3.5.cmml">h</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1b"><apply id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1"><plus id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.1"></plus><apply id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2.1.cmml" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2">subscript</csymbol><ci id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2.2.cmml" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2.2">ℒ</ci><apply id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2.3.cmml" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2.3"><times id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2.3.1.cmml" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2.3.1"></times><ci id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2.3.2.cmml" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2.3.2">𝑚</ci><ci id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2.3.3.cmml" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2.3.3">𝑎</ci><ci id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2.3.4.cmml" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2.3.4">𝑠</ci><ci id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2.3.5.cmml" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.2.3.5">𝑘</ci></apply></apply><apply id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.3.cmml" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.3.1.cmml" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.3">subscript</csymbol><ci id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.3.2.cmml" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.3.2">ℒ</ci><apply id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.3.3.cmml" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.3.3"><times id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.3.3.1.cmml" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.3.3.1"></times><ci id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.3.3.2.cmml" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.3.3.2">𝑗</ci><cn id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.3.3.3.cmml" type="integer" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.3.3.3">2</cn><ci id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.3.3.4.cmml" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.3.3.4">𝑑</ci></apply></apply><apply id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4.cmml" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4"><csymbol cd="ambiguous" id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4.1.cmml" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4">subscript</csymbol><ci id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4.2.cmml" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4.2">ℒ</ci><apply id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4.3.cmml" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4.3"><times id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4.3.1.cmml" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4.3.1"></times><ci id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4.3.2.cmml" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4.3.2">𝑚</ci><ci id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4.3.3.cmml" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4.3.3">𝑒</ci><ci id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4.3.4.cmml" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4.3.4">𝑠</ci><ci id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4.3.5.cmml" xref="S4.T2.5.5.5.5.1.1.1.1.1.m1.1.1.4.3.5">ℎ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1c">\mathcal{L}_{mask}+\mathcal{L}_{j2d}+\mathcal{L}_{mesh}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.5.5.5.5.1.1.1.1.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_m italic_a italic_s italic_k end_POSTSUBSCRIPT + caligraphic_L start_POSTSUBSCRIPT italic_j 2 italic_d end_POSTSUBSCRIPT + caligraphic_L start_POSTSUBSCRIPT italic_m italic_e italic_s italic_h end_POSTSUBSCRIPT</annotation></semantics></math></span></span>
</span></span> <span class="ltx_text" id="S4.T2.5.5.5.5.1.3"></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.6.6.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T2.6.6.6.6.1"><span class="ltx_text" id="S4.T2.6.6.6.6.1.2"></span> <span class="ltx_text" id="S4.T2.6.6.6.6.1.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.6.6.6.6.1.1.1">
<span class="ltx_tr" id="S4.T2.6.6.6.6.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.6.6.6.6.1.1.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><math alttext="\mathcal{L}_{mask}+\mathcal{L}_{j2d}+\mathcal{L}_{mesh}+\mathcal{L}_{j3d}" class="ltx_Math" display="inline" id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1"><semantics id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1a"><mrow id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.cmml"><msub id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2.2" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2.2.cmml">ℒ</mi><mrow id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2.3" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2.3.cmml"><mi id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2.3.2" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2.3.2.cmml">m</mi><mo id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2.3.1" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2.3.3" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2.3.3.cmml">a</mi><mo id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2.3.1a" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2.3.4" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2.3.4.cmml">s</mi><mo id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2.3.1b" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2.3.5" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2.3.5.cmml">k</mi></mrow></msub><mo id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.1" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.1.cmml">+</mo><msub id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.3" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.3.2" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.3.2.cmml">ℒ</mi><mrow id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.3.3" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.3.3.cmml"><mi id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.3.3.2" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.3.3.2.cmml">j</mi><mo id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.3.3.1" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.3.3.1.cmml">⁢</mo><mn id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.3.3.3" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.3.3.3.cmml">2</mn><mo id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.3.3.1a" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.3.3.4" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.3.3.4.cmml">d</mi></mrow></msub><mo id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.1a" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.1.cmml">+</mo><msub id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4.2" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4.2.cmml">ℒ</mi><mrow id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4.3" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4.3.cmml"><mi id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4.3.2" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4.3.2.cmml">m</mi><mo id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4.3.1" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4.3.1.cmml">⁢</mo><mi id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4.3.3" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4.3.3.cmml">e</mi><mo id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4.3.1a" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4.3.1.cmml">⁢</mo><mi id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4.3.4" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4.3.4.cmml">s</mi><mo id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4.3.1b" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4.3.1.cmml">⁢</mo><mi id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4.3.5" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4.3.5.cmml">h</mi></mrow></msub><mo id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.1b" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.1.cmml">+</mo><msub id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.5" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.5.2" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.5.2.cmml">ℒ</mi><mrow id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.5.3" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.5.3.cmml"><mi id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.5.3.2" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.5.3.2.cmml">j</mi><mo id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.5.3.1" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.5.3.1.cmml">⁢</mo><mn id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.5.3.3" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.5.3.3.cmml">3</mn><mo id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.5.3.1a" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.5.3.1.cmml">⁢</mo><mi id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.5.3.4" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.5.3.4.cmml">d</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1b"><apply id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1"><plus id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.1"></plus><apply id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2.1.cmml" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2">subscript</csymbol><ci id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2.2.cmml" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2.2">ℒ</ci><apply id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2.3.cmml" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2.3"><times id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2.3.1.cmml" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2.3.1"></times><ci id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2.3.2.cmml" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2.3.2">𝑚</ci><ci id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2.3.3.cmml" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2.3.3">𝑎</ci><ci id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2.3.4.cmml" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2.3.4">𝑠</ci><ci id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2.3.5.cmml" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.2.3.5">𝑘</ci></apply></apply><apply id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.3.cmml" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.3.1.cmml" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.3">subscript</csymbol><ci id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.3.2.cmml" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.3.2">ℒ</ci><apply id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.3.3.cmml" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.3.3"><times id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.3.3.1.cmml" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.3.3.1"></times><ci id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.3.3.2.cmml" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.3.3.2">𝑗</ci><cn id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.3.3.3.cmml" type="integer" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.3.3.3">2</cn><ci id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.3.3.4.cmml" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.3.3.4">𝑑</ci></apply></apply><apply id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4.cmml" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4"><csymbol cd="ambiguous" id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4.1.cmml" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4">subscript</csymbol><ci id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4.2.cmml" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4.2">ℒ</ci><apply id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4.3.cmml" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4.3"><times id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4.3.1.cmml" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4.3.1"></times><ci id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4.3.2.cmml" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4.3.2">𝑚</ci><ci id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4.3.3.cmml" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4.3.3">𝑒</ci><ci id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4.3.4.cmml" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4.3.4">𝑠</ci><ci id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4.3.5.cmml" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.4.3.5">ℎ</ci></apply></apply><apply id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.5.cmml" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.5"><csymbol cd="ambiguous" id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.5.1.cmml" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.5">subscript</csymbol><ci id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.5.2.cmml" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.5.2">ℒ</ci><apply id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.5.3.cmml" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.5.3"><times id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.5.3.1.cmml" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.5.3.1"></times><ci id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.5.3.2.cmml" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.5.3.2">𝑗</ci><cn id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.5.3.3.cmml" type="integer" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.5.3.3">3</cn><ci id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.5.3.4.cmml" xref="S4.T2.6.6.6.6.1.1.1.1.1.m1.1.1.5.3.4">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1c">\mathcal{L}_{mask}+\mathcal{L}_{j2d}+\mathcal{L}_{mesh}+\mathcal{L}_{j3d}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.6.6.6.6.1.1.1.1.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_m italic_a italic_s italic_k end_POSTSUBSCRIPT + caligraphic_L start_POSTSUBSCRIPT italic_j 2 italic_d end_POSTSUBSCRIPT + caligraphic_L start_POSTSUBSCRIPT italic_m italic_e italic_s italic_h end_POSTSUBSCRIPT + caligraphic_L start_POSTSUBSCRIPT italic_j 3 italic_d end_POSTSUBSCRIPT</annotation></semantics></math></span></span>
</span></span> <span class="ltx_text" id="S4.T2.6.6.6.6.1.3"></span></span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.12.12.12">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T2.12.12.12.7" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.12.12.12.7.1">mean (std)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.7.7.7.1" style="padding-left:3.0pt;padding-right:3.0pt;">37.29 (<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T2.7.7.7.1.m1.1"><semantics id="S4.T2.7.7.7.1.m1.1a"><mo id="S4.T2.7.7.7.1.m1.1.1" xref="S4.T2.7.7.7.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.7.7.7.1.m1.1b"><csymbol cd="latexml" id="S4.T2.7.7.7.1.m1.1.1.cmml" xref="S4.T2.7.7.7.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.7.7.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T2.7.7.7.1.m1.1d">±</annotation></semantics></math> 18.02)</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.8.8.8.2" style="padding-left:3.0pt;padding-right:3.0pt;">7.03 (<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T2.8.8.8.2.m1.1"><semantics id="S4.T2.8.8.8.2.m1.1a"><mo id="S4.T2.8.8.8.2.m1.1.1" xref="S4.T2.8.8.8.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.8.8.8.2.m1.1b"><csymbol cd="latexml" id="S4.T2.8.8.8.2.m1.1.1.cmml" xref="S4.T2.8.8.8.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.8.8.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T2.8.8.8.2.m1.1d">±</annotation></semantics></math> 2.57)</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.9.9.9.3" style="padding-left:3.0pt;padding-right:3.0pt;">8.13 (<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T2.9.9.9.3.m1.1"><semantics id="S4.T2.9.9.9.3.m1.1a"><mo id="S4.T2.9.9.9.3.m1.1.1" xref="S4.T2.9.9.9.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.9.9.9.3.m1.1b"><csymbol cd="latexml" id="S4.T2.9.9.9.3.m1.1.1.cmml" xref="S4.T2.9.9.9.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.9.9.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T2.9.9.9.3.m1.1d">±</annotation></semantics></math> 0.57)</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.10.10.10.4" style="padding-left:3.0pt;padding-right:3.0pt;">1.29 (<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T2.10.10.10.4.m1.1"><semantics id="S4.T2.10.10.10.4.m1.1a"><mo id="S4.T2.10.10.10.4.m1.1.1" xref="S4.T2.10.10.10.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.10.10.10.4.m1.1b"><csymbol cd="latexml" id="S4.T2.10.10.10.4.m1.1.1.cmml" xref="S4.T2.10.10.10.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.10.10.10.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T2.10.10.10.4.m1.1d">±</annotation></semantics></math> 0.43)</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.11.11.11.5" style="padding-left:3.0pt;padding-right:3.0pt;">1.28 (<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T2.11.11.11.5.m1.1"><semantics id="S4.T2.11.11.11.5.m1.1a"><mo id="S4.T2.11.11.11.5.m1.1.1" xref="S4.T2.11.11.11.5.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.11.11.11.5.m1.1b"><csymbol cd="latexml" id="S4.T2.11.11.11.5.m1.1.1.cmml" xref="S4.T2.11.11.11.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.11.11.11.5.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T2.11.11.11.5.m1.1d">±</annotation></semantics></math> 0.43)</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.12.12.12.6" style="padding-left:3.0pt;padding-right:3.0pt;">1.01 (<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T2.12.12.12.6.m1.1"><semantics id="S4.T2.12.12.12.6.m1.1a"><mo id="S4.T2.12.12.12.6.m1.1.1" xref="S4.T2.12.12.12.6.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.12.12.12.6.m1.1b"><csymbol cd="latexml" id="S4.T2.12.12.12.6.m1.1.1.cmml" xref="S4.T2.12.12.12.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.12.12.12.6.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T2.12.12.12.6.m1.1d">±</annotation></semantics></math> 0.34)</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Evaluation of annotation results. The average 3D joint errors across all frames are reported (in cm). <math alttext="\mathcal{L}_{reg}" class="ltx_Math" display="inline" id="S4.T2.14.m1.1"><semantics id="S4.T2.14.m1.1b"><msub id="S4.T2.14.m1.1.1" xref="S4.T2.14.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.T2.14.m1.1.1.2" xref="S4.T2.14.m1.1.1.2.cmml">ℒ</mi><mrow id="S4.T2.14.m1.1.1.3" xref="S4.T2.14.m1.1.1.3.cmml"><mi id="S4.T2.14.m1.1.1.3.2" xref="S4.T2.14.m1.1.1.3.2.cmml">r</mi><mo id="S4.T2.14.m1.1.1.3.1" xref="S4.T2.14.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.T2.14.m1.1.1.3.3" xref="S4.T2.14.m1.1.1.3.3.cmml">e</mi><mo id="S4.T2.14.m1.1.1.3.1b" xref="S4.T2.14.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.T2.14.m1.1.1.3.4" xref="S4.T2.14.m1.1.1.3.4.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T2.14.m1.1c"><apply id="S4.T2.14.m1.1.1.cmml" xref="S4.T2.14.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.14.m1.1.1.1.cmml" xref="S4.T2.14.m1.1.1">subscript</csymbol><ci id="S4.T2.14.m1.1.1.2.cmml" xref="S4.T2.14.m1.1.1.2">ℒ</ci><apply id="S4.T2.14.m1.1.1.3.cmml" xref="S4.T2.14.m1.1.1.3"><times id="S4.T2.14.m1.1.1.3.1.cmml" xref="S4.T2.14.m1.1.1.3.1"></times><ci id="S4.T2.14.m1.1.1.3.2.cmml" xref="S4.T2.14.m1.1.1.3.2">𝑟</ci><ci id="S4.T2.14.m1.1.1.3.3.cmml" xref="S4.T2.14.m1.1.1.3.3">𝑒</ci><ci id="S4.T2.14.m1.1.1.3.4.cmml" xref="S4.T2.14.m1.1.1.3.4">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.14.m1.1d">\mathcal{L}_{reg}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.14.m1.1e">caligraphic_L start_POSTSUBSCRIPT italic_r italic_e italic_g end_POSTSUBSCRIPT</annotation></semantics></math> is used for all to mitigate irregular hand poses.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Experiment Setup</h3>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.1">Dataset Preparation.</span> We utilize our own dataset for experiments as it uniquely contains egocentric images from multiple spectra, essentially for our benchmark experiments. We annotate the main part of our dataset (<em class="ltx_emph ltx_font_italic" id="S4.SS2.p1.1.2">cf</em>. <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S3.T1" title="In 3.1 Multi-Spectral Hand Pose Dataset ‣ 3 The ThermoHands Benchmark ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a>) automatically following <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S3.SS2" title="3.2 Hand Pose Annotation ‣ 3 The ThermoHands Benchmark ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>, of which the training and validation sets serve as the foundation for the training of all network models.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1">Method and Implementation.</span> To provide sufficient baselines for follow-up works, we selected two state-of-the-art methods in 3D hand pose estimation: HTT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib15" title="">15</a>]</cite> and A2J-Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib46" title="">46</a>]</cite>, and reproduced them on our dataset for experiments.
HTT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib15" title="">15</a>]</cite> is a video-based method thus enabling the evaluation in both two problem settings while A2J-Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib46" title="">46</a>]</cite> only works under the single image-based setting. For a feasible comparison, we use the same sequence length, <em class="ltx_emph ltx_font_italic" id="S4.SS2.p2.1.2">i.e</em>., <math alttext="T=8" class="ltx_Math" display="inline" id="S4.SS2.p2.1.m1.1"><semantics id="S4.SS2.p2.1.m1.1a"><mrow id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><mi id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2.cmml">T</mi><mo id="S4.SS2.p2.1.m1.1.1.1" xref="S4.SS2.p2.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS2.p2.1.m1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><eq id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1.1"></eq><ci id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2">𝑇</ci><cn id="S4.SS2.p2.1.m1.1.1.3.cmml" type="integer" xref="S4.SS2.p2.1.m1.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">T=8</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.1.m1.1d">italic_T = 8</annotation></semantics></math>, for HTT and TherFormer-V baselines. We exclude the additional action block used by HTT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib15" title="">15</a>]</cite>, focusing solely on hand pose estimation.
We adjusted the anchor initialization phase of A2J-Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib46" title="">46</a>]</cite> to better accommodate our dataset, without altering the density of its anchors. All trained models are tested on a single NVIDIA RTX 4090 GPU to fairly compare their inference speed.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.1">Evaluation Metrics.</span>
We evaluate the accuracy of 3D hand pose estimation with two metrics: <em class="ltx_emph ltx_font_italic" id="S4.SS2.p3.1.2">Percentage of Correct Keypoints</em> (PCK) and <em class="ltx_emph ltx_font_italic" id="S4.SS2.p3.1.3">Mean End-Point Error</em> (MEPE) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib34" title="">34</a>]</cite>, in both camera space and root-aligned (RA) space <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib38" title="">38</a>]</cite>. For RA space, we align the estimated wrist with its groundtruth position before measurement. For PCK, we report the corresponding <em class="ltx_emph ltx_font_italic" id="S4.SS2.p3.1.4">Area Under the Curve</em> (AUC) over the 0-50mm/80mm error thresholds for the camera/RA space.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Thermal Image-based 3D Hand Pose Estimation</h3>
<div class="ltx_para ltx_noindent" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.1">Main Results.</span>
To assess TherFormer’s performance in thermal image-based 3D hand pose estimation, we compare it against state-of-the-art methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib46" title="">46</a>]</cite> on the main testing set, as shown in <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S4.T3" title="In 4.3 Thermal Image-based 3D Hand Pose Estimation ‣ 4 Experiment ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">3</span></a>. TherFormer-S outforms two competing methods under the single image-based setting, while TherFormer-V surpasses the counterpart HTT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib15" title="">15</a>]</cite> given the same sequential images as input. Such an improvement mainly stems from our mask-guided spatial attention design that can adaptively encode the spatial interaction among hand joints with the guidance of the hand mask (<em class="ltx_emph ltx_font_italic" id="S4.SS3.p1.1.2">cf</em>. <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S3.SS3" title="3.3 TherFormer: A Baseline Method ‣ 3 The ThermoHands Benchmark ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.3</span></a>). A performance gap can be observed between single image-based and video-based settings for both HTT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib15" title="">15</a>]</cite> and TherFormer. We credit this to their usage of temporal information that helps to tackle the occlusion cases and solve ambiguities.
Thanks to our lightweight network design, TherFormer is highly efficient to run with fps of 136 and 52 for single and sequence input respectively, ensuring its real-time application to resource-constrained devices. Moreover, TherFormer-V also improves the performance over HTT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib15" title="">15</a>]</cite> for other spectra (<em class="ltx_emph ltx_font_italic" id="S4.SS3.p1.1.3">cf</em>. <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S4.T4" title="In 4.4.2 Quantitative Results under Normal Conditions ‣ 4.4 Comparison between Spectrum ‣ 4 Experiment ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">4</span></a>), proving its adaptability to different inputs.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.5" style="width:433.6pt;height:94.4pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-116.3pt,25.1pt) scale(0.650920621511858,0.650920621511858) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.5.5">
<tr class="ltx_tr" id="S4.T3.5.5.5">
<td class="ltx_td ltx_border_r ltx_border_tt" id="S4.T3.5.5.5.6" style="padding-left:14.0pt;padding-right:14.0pt;"></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T3.5.5.5.7" style="padding-left:14.0pt;padding-right:14.0pt;">Method</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.5.5.5.8" style="padding-left:14.0pt;padding-right:14.0pt;">Input</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.1.1.1.1" style="padding-left:14.0pt;padding-right:14.0pt;">MEPE (mm) <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.1.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.1.m1.1a"><mo id="S4.T3.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T3.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.1.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.2.2.2.2" style="padding-left:14.0pt;padding-right:14.0pt;">AUC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.2.2.2.2.m1.1"><semantics id="S4.T3.2.2.2.2.m1.1a"><mo id="S4.T3.2.2.2.2.m1.1.1" stretchy="false" xref="S4.T3.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.2.m1.1b"><ci id="S4.T3.2.2.2.2.m1.1.1.cmml" xref="S4.T3.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.2.2.2.m1.1d">↑</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.3.3.3.3" style="padding-left:14.0pt;padding-right:14.0pt;">MEPE-RA (mm) <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.3.3.3.3.m1.1"><semantics id="S4.T3.3.3.3.3.m1.1a"><mo id="S4.T3.3.3.3.3.m1.1.1" stretchy="false" xref="S4.T3.3.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.3.m1.1b"><ci id="S4.T3.3.3.3.3.m1.1.1.cmml" xref="S4.T3.3.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.3.3.3.3.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.4.4.4.4" style="padding-left:14.0pt;padding-right:14.0pt;">AUC-RA <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.4.4.4.4.m1.1"><semantics id="S4.T3.4.4.4.4.m1.1a"><mo id="S4.T3.4.4.4.4.m1.1.1" stretchy="false" xref="S4.T3.4.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.4.4.m1.1b"><ci id="S4.T3.4.4.4.4.m1.1.1.cmml" xref="S4.T3.4.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.4.4.4.4.m1.1d">↑</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.5.5.5.5" style="padding-left:14.0pt;padding-right:14.0pt;">fps <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.5.5.5.5.m1.1"><semantics id="S4.T3.5.5.5.5.m1.1a"><mo id="S4.T3.5.5.5.5.m1.1.1" stretchy="false" xref="S4.T3.5.5.5.5.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.5.5.m1.1b"><ci id="S4.T3.5.5.5.5.m1.1.1.cmml" xref="S4.T3.5.5.5.5.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.5.5.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.5.5.5.5.m1.1d">↑</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.5.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.5.5.6.1" style="padding-left:14.0pt;padding-right:14.0pt;">(a)</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.5.5.6.2" style="padding-left:14.0pt;padding-right:14.0pt;">A2J-Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib46" title="">46</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.5.6.3" style="padding-left:14.0pt;padding-right:14.0pt;">Single</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.5.6.4" style="padding-left:14.0pt;padding-right:14.0pt;">51.68</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.5.6.5" style="padding-left:14.0pt;padding-right:14.0pt;">0.474</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.5.6.6" style="padding-left:14.0pt;padding-right:14.0pt;">20.76</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.5.6.7" style="padding-left:14.0pt;padding-right:14.0pt;">0.603</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.5.5.6.8" style="padding-left:14.0pt;padding-right:14.0pt;">34</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.5.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.5.7.1" style="padding-left:14.0pt;padding-right:14.0pt;">(b)</td>
<td class="ltx_td ltx_align_left" id="S4.T3.5.5.7.2" style="padding-left:14.0pt;padding-right:14.0pt;">HTT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib15" title="">15</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.7.3" style="padding-left:14.0pt;padding-right:14.0pt;">Single</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.7.4" style="padding-left:14.0pt;padding-right:14.0pt;">49.09</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.7.5" style="padding-left:14.0pt;padding-right:14.0pt;">0.489</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.7.6" style="padding-left:14.0pt;padding-right:14.0pt;">20.69</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.7.7" style="padding-left:14.0pt;padding-right:14.0pt;">0.599</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.5.5.7.8" style="padding-left:14.0pt;padding-right:14.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.5.7.8.1">211</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.5.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.5.8.1" style="padding-left:14.0pt;padding-right:14.0pt;">(c)</td>
<td class="ltx_td ltx_align_left" id="S4.T3.5.5.8.2" style="padding-left:14.0pt;padding-right:14.0pt;">TherFormer-S</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.8.3" style="padding-left:14.0pt;padding-right:14.0pt;">Single</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.8.4" style="padding-left:14.0pt;padding-right:14.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.5.8.4.1">44.64</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.8.5" style="padding-left:14.0pt;padding-right:14.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.5.8.5.1">0.539</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.8.6" style="padding-left:14.0pt;padding-right:14.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.5.8.6.1">18.34</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.8.7" style="padding-left:14.0pt;padding-right:14.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.5.8.7.1">0.643</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.5.5.8.8" style="padding-left:14.0pt;padding-right:14.0pt;">136</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.5.9">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.5.5.9.1" style="padding-left:14.0pt;padding-right:14.0pt;">(d)</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.5.5.9.2" style="padding-left:14.0pt;padding-right:14.0pt;">(c) w/o spatial transformer</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.5.9.3" style="padding-left:14.0pt;padding-right:14.0pt;">Single</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.5.9.4" style="padding-left:14.0pt;padding-right:14.0pt;">48.79</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.5.9.5" style="padding-left:14.0pt;padding-right:14.0pt;">0.491</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.5.9.6" style="padding-left:14.0pt;padding-right:14.0pt;">20.15</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.5.9.7" style="padding-left:14.0pt;padding-right:14.0pt;">0.609</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.5.5.9.8" style="padding-left:14.0pt;padding-right:14.0pt;">174</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.5.10">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.5.10.1" style="padding-left:14.0pt;padding-right:14.0pt;">(e)</td>
<td class="ltx_td ltx_align_left" id="S4.T3.5.5.10.2" style="padding-left:14.0pt;padding-right:14.0pt;">(c) w/o mask guidance</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.10.3" style="padding-left:14.0pt;padding-right:14.0pt;">Single</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.10.4" style="padding-left:14.0pt;padding-right:14.0pt;">48.83</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.10.5" style="padding-left:14.0pt;padding-right:14.0pt;">0.494</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.10.6" style="padding-left:14.0pt;padding-right:14.0pt;">18.89</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.10.7" style="padding-left:14.0pt;padding-right:14.0pt;">0.625</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.5.5.10.8" style="padding-left:14.0pt;padding-right:14.0pt;">141</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.5.11">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.5.5.11.1" style="padding-left:14.0pt;padding-right:14.0pt;">(f)</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.5.5.11.2" style="padding-left:14.0pt;padding-right:14.0pt;">HTT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib15" title="">15</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.5.11.3" style="padding-left:14.0pt;padding-right:14.0pt;">Sequence</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.5.11.4" style="padding-left:14.0pt;padding-right:14.0pt;">47.07</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.5.11.5" style="padding-left:14.0pt;padding-right:14.0pt;">0.512</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.5.11.6" style="padding-left:14.0pt;padding-right:14.0pt;">17.49</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.5.11.7" style="padding-left:14.0pt;padding-right:14.0pt;">0.659</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.5.5.11.8" style="padding-left:14.0pt;padding-right:14.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.5.11.8.1">129</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.5.12">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.5.5.12.1" style="padding-left:14.0pt;padding-right:14.0pt;">(g)</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T3.5.5.12.2" style="padding-left:14.0pt;padding-right:14.0pt;">TherFormer-V</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.5.12.3" style="padding-left:14.0pt;padding-right:14.0pt;">Sequence</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.5.12.4" style="padding-left:14.0pt;padding-right:14.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.5.12.4.1">43.36</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.5.12.5" style="padding-left:14.0pt;padding-right:14.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.5.12.5.1">0.549</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.5.12.6" style="padding-left:14.0pt;padding-right:14.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.5.12.6.1">17.36</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.5.12.7" style="padding-left:14.0pt;padding-right:14.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.5.12.7.1">0.661</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T3.5.5.12.8" style="padding-left:14.0pt;padding-right:14.0pt;">52</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison between TherFormer and state of the arts on thermal camera-based 3D hand pose estimation. The fps indicates the number of inference steps that models can run per second. </figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p2.1.1">Ablation Studies.</span> We ablate our proposed mask-guided attention mechanism and the entire spatial transformer (<em class="ltx_emph ltx_font_italic" id="S4.SS3.p2.1.2">cf</em>. <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S3.SS3" title="3.3 TherFormer: A Baseline Method ‣ 3 The ThermoHands Benchmark ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.3</span></a>) to observe their impact. As can be seen in <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S4.T3" title="In 4.3 Thermal Image-based 3D Hand Pose Estimation ‣ 4 Experiment ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">3</span></a>, the mask-guided attention mechanism notably contributes to TherFormer’s performance growth (row (c) vs. (e)), demonstrating its effectiveness in mitigating the effect of background clutter.
Other components in the spatial transformer bring a large performance gain in the RA space (row (e) vs. (d)), confirming the importance of spatial feature enhancement for fine-grained hand joints localization.
</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Comparison between Spectrum</h3>
<section class="ltx_subsubsection" id="S4.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1 </span>Qualitative Results under Challenging Conditions</h4>
<div class="ltx_para ltx_noindent" id="S4.SS4.SSS1.p1">
<p class="ltx_p" id="S4.SS4.SSS1.p1.1">To justify the advantages of using thermal cameras for egocentric hand pose estimation under challenging scenarios, we conduct a comparison between four spectra on three of our auxiliary testing sets, <em class="ltx_emph ltx_font_italic" id="S4.SS4.SSS1.p1.1.1">i.e</em>., gloves, darkness and sun glare (<em class="ltx_emph ltx_font_italic" id="S4.SS4.SSS1.p1.1.2">cf</em>. <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S3.T1" title="In 3.1 Multi-Spectral Hand Pose Dataset ‣ 3 The ThermoHands Benchmark ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a>).
Our automatic annotation pipeline (<em class="ltx_emph ltx_font_italic" id="S4.SS4.SSS1.p1.1.3">cf</em>. <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S3.SS2" title="3.2 Hand Pose Annotation ‣ 3 The ThermoHands Benchmark ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>) becomes infeasible since hands appear corrupted in either RGB or depth images. Therefore, we conduct a qualitative analysis of their performance and present some representative examples in <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S4.F6" title="In 4.4.1 Qualitative Results under Challenging Conditions ‣ 4.4 Comparison between Spectrum ‣ 4 Experiment ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a>.
We use the same model, <em class="ltx_emph ltx_font_italic" id="S4.SS4.SSS1.p1.1.4">i.e</em>., HTT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib15" title="">15</a>]</cite>, trained on different spectra
for a fair comparison.
Please see the supplementary materials for more figure examples and demo videos under challenging conditions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS4.SSS1.p2">
<p class="ltx_p" id="S4.SS4.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.SSS1.p2.1.1">RGB vs. Thermal.</span> As can be seen, RGB-based methods fail with gloves wearing (<em class="ltx_emph ltx_font_italic" id="S4.SS4.SSS1.p2.1.2">cf</em>. <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S4.F6" title="In 4.4.1 Qualitative Results under Challenging Conditions ‣ 4.4 Comparison between Spectrum ‣ 4 Experiment ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a> (a-b)) and in darkness (<em class="ltx_emph ltx_font_italic" id="S4.SS4.SSS1.p2.1.3">cf</em>. <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S4.F6" title="In 4.4.1 Qualitative Results under Challenging Conditions ‣ 4.4 Comparison between Spectrum ‣ 4 Experiment ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a> (c-d)).
Gloves change how human hands look and hide their natural colors and textures. Since RGB algorithms rely on skin’s texture and color to identify hand parts and joints, gloves, particularly those with solid colors or textures unlike skin, can interfere with this identification process. Contrary to RGB techniques, as shown in <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S4.F6" title="In 4.4.1 Qualitative Results under Challenging Conditions ‣ 4.4 Comparison between Spectrum ‣ 4 Experiment ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a> (a-b), thermal imaging methods excel in identifying hands by leveraging the principles of heat conduction through gloves, effectively bypassing the limitations imposed by color and texture variations.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS4.SSS1.p3">
<p class="ltx_p" id="S4.SS4.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.SSS1.p3.1.1">NIR, Depth vs. Thermal.</span> NIR sensors are significantly disrupted by strong sunlight, affecting both NIR imaging and depth map creation, as shown in <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S4.F6" title="In 4.4.1 Qualitative Results under Challenging Conditions ‣ 4.4 Comparison between Spectrum ‣ 4 Experiment ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a> (e-f). Conversely, thermal imaging is immune to sunlight and outdoor conditions. The temperature difference between hands and their surroundings in the thermal spectrum facilitates effortless identification of the hands, leaving the thermal-based estimator unaffected.
The thermal camera’s ability to consistently capture hand features in diverse lighting conditions positions it as a suitable option for future XR applications.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="437" id="S4.F6.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Examples of results with various spectra under three challenging settings. For visualization, we show the projection of the estimated left (<span class="ltx_text" id="S4.F6.3.1" style="color:#FF0000;">red</span>) and right (<span class="ltx_text" id="S4.F6.4.2" style="color:#0000FF;">blue</span>) 3D hand joints on images. </figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2 </span>Quantitative Results under Normal Conditions</h4>
<div class="ltx_para ltx_noindent" id="S4.SS4.SSS2.p1">
<p class="ltx_p" id="S4.SS4.SSS2.p1.1">To validate the versatility of thermal-based methods, we also evaluate their performance compared to other spectra (<em class="ltx_emph ltx_font_italic" id="S4.SS4.SSS2.p1.1.1">i.e</em>., RGB, depth, NIR and thermal) under normal conditions for general-purpose use cases. We apply different spectral images as input to multiple baselines methods and report the results in <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S4.T4" title="In 4.4.2 Quantitative Results under Normal Conditions ‣ 4.4 Comparison between Spectrum ‣ 4 Experiment ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">4</span></a>. Not surprisingly, depth image-based methods yield the best performance on average, as they directly utilize the depth information to provide a detailed 3D structure of the hand. The NIR spectrum shows a marginal decrease in performance compared to depth but outperforms both RGB and thermal spectra. This can be attributed to NIR camera’s active sensing ability, which leads to more consistent and reliable imaging regardless of the variability of external illuminations (vs. RGB) and temperature (vs. thermal). However, active NIR sensors are prone to interference from external NIR sources like sun glare (<em class="ltx_emph ltx_font_italic" id="S4.SS4.SSS2.p1.1.2">cf</em>. <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S4.F6" title="In 4.4.1 Qualitative Results under Challenging Conditions ‣ 4.4 Comparison between Spectrum ‣ 4 Experiment ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a>). Thermal-based methods, despite using single-channel heat capture, achieves comparable performance to those using RGB images that contain rich color and textures. Such results suggest that thermal images can not only serve as supplements in challenging cases but also can be a viable alternative to other spectra in normal conditions for hand pose estimation.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T4.8" style="width:433.6pt;height:79.1pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-81.7pt,14.8pt) scale(0.726364362042705,0.726364362042705) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T4.8.8">
<tr class="ltx_tr" id="S4.T4.8.8.9">
<td class="ltx_td ltx_border_r ltx_border_tt" id="S4.T4.8.8.9.1" style="padding-left:10.0pt;padding-right:10.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S4.T4.8.8.9.2" style="padding-left:10.0pt;padding-right:10.0pt;">HTT (Sequence)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S4.T4.8.8.9.3" style="padding-left:10.0pt;padding-right:10.0pt;">TherFormer-V</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S4.T4.8.8.9.4" style="padding-left:10.0pt;padding-right:10.0pt;">TherFormer-S</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T4.8.8.9.5" style="padding-left:10.0pt;padding-right:10.0pt;">Average</td>
</tr>
<tr class="ltx_tr" id="S4.T4.8.8.8">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.8.8.8.9" style="padding-left:10.0pt;padding-right:10.0pt;">Spectrum</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.1.1" style="padding-left:10.0pt;padding-right:10.0pt;">MEPE (mm) <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T4.1.1.1.1.m1.1"><semantics id="S4.T4.1.1.1.1.m1.1a"><mo id="S4.T4.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T4.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.m1.1b"><ci id="S4.T4.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.1.1.1.1.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.2.2.2" style="padding-left:10.0pt;padding-right:10.0pt;">AUC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T4.2.2.2.2.m1.1"><semantics id="S4.T4.2.2.2.2.m1.1a"><mo id="S4.T4.2.2.2.2.m1.1.1" stretchy="false" xref="S4.T4.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.2.m1.1b"><ci id="S4.T4.2.2.2.2.m1.1.1.cmml" xref="S4.T4.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.2.2.2.2.m1.1d">↑</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.3.3.3.3" style="padding-left:10.0pt;padding-right:10.0pt;">MEPE (mm) <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T4.3.3.3.3.m1.1"><semantics id="S4.T4.3.3.3.3.m1.1a"><mo id="S4.T4.3.3.3.3.m1.1.1" stretchy="false" xref="S4.T4.3.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T4.3.3.3.3.m1.1b"><ci id="S4.T4.3.3.3.3.m1.1.1.cmml" xref="S4.T4.3.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.3.3.3.3.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.4.4.4.4" style="padding-left:10.0pt;padding-right:10.0pt;">AUC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T4.4.4.4.4.m1.1"><semantics id="S4.T4.4.4.4.4.m1.1a"><mo id="S4.T4.4.4.4.4.m1.1.1" stretchy="false" xref="S4.T4.4.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.4.4.4.4.m1.1b"><ci id="S4.T4.4.4.4.4.m1.1.1.cmml" xref="S4.T4.4.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.4.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.4.4.4.4.m1.1d">↑</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.5.5.5.5" style="padding-left:10.0pt;padding-right:10.0pt;">MEPE (mm) <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T4.5.5.5.5.m1.1"><semantics id="S4.T4.5.5.5.5.m1.1a"><mo id="S4.T4.5.5.5.5.m1.1.1" stretchy="false" xref="S4.T4.5.5.5.5.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T4.5.5.5.5.m1.1b"><ci id="S4.T4.5.5.5.5.m1.1.1.cmml" xref="S4.T4.5.5.5.5.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.5.5.5.5.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.5.5.5.5.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.6.6.6.6" style="padding-left:10.0pt;padding-right:10.0pt;">AUC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T4.6.6.6.6.m1.1"><semantics id="S4.T4.6.6.6.6.m1.1a"><mo id="S4.T4.6.6.6.6.m1.1.1" stretchy="false" xref="S4.T4.6.6.6.6.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.6.6.6.6.m1.1b"><ci id="S4.T4.6.6.6.6.m1.1.1.cmml" xref="S4.T4.6.6.6.6.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.6.6.6.6.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.6.6.6.6.m1.1d">↑</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.7.7.7.7" style="padding-left:10.0pt;padding-right:10.0pt;">MEPE (mm) <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T4.7.7.7.7.m1.1"><semantics id="S4.T4.7.7.7.7.m1.1a"><mo id="S4.T4.7.7.7.7.m1.1.1" stretchy="false" xref="S4.T4.7.7.7.7.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T4.7.7.7.7.m1.1b"><ci id="S4.T4.7.7.7.7.m1.1.1.cmml" xref="S4.T4.7.7.7.7.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.7.7.7.7.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.7.7.7.7.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T4.8.8.8.8" style="padding-left:10.0pt;padding-right:10.0pt;">AUC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T4.8.8.8.8.m1.1"><semantics id="S4.T4.8.8.8.8.m1.1a"><mo id="S4.T4.8.8.8.8.m1.1.1" stretchy="false" xref="S4.T4.8.8.8.8.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.8.8.8.8.m1.1b"><ci id="S4.T4.8.8.8.8.m1.1.1.cmml" xref="S4.T4.8.8.8.8.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.8.8.8.8.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.8.8.8.8.m1.1d">↑</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S4.T4.8.8.10">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.8.8.10.1" style="padding-left:10.0pt;padding-right:10.0pt;">RGB</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.8.8.10.2" style="padding-left:10.0pt;padding-right:10.0pt;">43.30</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.8.8.10.3" style="padding-left:10.0pt;padding-right:10.0pt;">0.542</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.8.8.10.4" style="padding-left:10.0pt;padding-right:10.0pt;">43.50</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.8.8.10.5" style="padding-left:10.0pt;padding-right:10.0pt;">0.542</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.8.8.10.6" style="padding-left:10.0pt;padding-right:10.0pt;">44.61</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.8.8.10.7" style="padding-left:10.0pt;padding-right:10.0pt;">0.529</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.8.8.10.8" style="padding-left:10.0pt;padding-right:10.0pt;">43.80</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T4.8.8.10.9" style="padding-left:10.0pt;padding-right:10.0pt;">0.538</td>
</tr>
<tr class="ltx_tr" id="S4.T4.8.8.11">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.8.8.11.1" style="padding-left:10.0pt;padding-right:10.0pt;">Depth</td>
<td class="ltx_td ltx_align_center" id="S4.T4.8.8.11.2" style="padding-left:10.0pt;padding-right:10.0pt;">41.62</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.8.8.11.3" style="padding-left:10.0pt;padding-right:10.0pt;">0.559</td>
<td class="ltx_td ltx_align_center" id="S4.T4.8.8.11.4" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.8.8.11.4.1">39.70</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.8.8.11.5" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.8.8.11.5.1">0.581</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.8.8.11.6" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.8.8.11.6.1">39.84</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.8.8.11.7" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.8.8.11.7.1">0.579</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.8.8.11.8" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.8.8.11.8.1">40.39</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.8.8.11.9" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.8.8.11.9.1">0.573</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.8.8.12">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.8.8.12.1" style="padding-left:10.0pt;padding-right:10.0pt;">NIR</td>
<td class="ltx_td ltx_align_center" id="S4.T4.8.8.12.2" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.8.8.12.2.1">41.57</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.8.8.12.3" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.8.8.12.3.1">0.562</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.8.8.12.4" style="padding-left:10.0pt;padding-right:10.0pt;">40.79</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.8.8.12.5" style="padding-left:10.0pt;padding-right:10.0pt;">0.575</td>
<td class="ltx_td ltx_align_center" id="S4.T4.8.8.12.6" style="padding-left:10.0pt;padding-right:10.0pt;">40.98</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.8.8.12.7" style="padding-left:10.0pt;padding-right:10.0pt;">0.575</td>
<td class="ltx_td ltx_align_center" id="S4.T4.8.8.12.8" style="padding-left:10.0pt;padding-right:10.0pt;">41.11</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.8.8.12.9" style="padding-left:10.0pt;padding-right:10.0pt;">0.571</td>
</tr>
<tr class="ltx_tr" id="S4.T4.8.8.13">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T4.8.8.13.1" style="padding-left:10.0pt;padding-right:10.0pt;">Thermal</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.8.8.13.2" style="padding-left:10.0pt;padding-right:10.0pt;">47.07</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T4.8.8.13.3" style="padding-left:10.0pt;padding-right:10.0pt;">0.512</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.8.8.13.4" style="padding-left:10.0pt;padding-right:10.0pt;">43.36</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T4.8.8.13.5" style="padding-left:10.0pt;padding-right:10.0pt;">0.549</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.8.8.13.6" style="padding-left:10.0pt;padding-right:10.0pt;">44.64</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T4.8.8.13.7" style="padding-left:10.0pt;padding-right:10.0pt;">0.539</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.8.8.13.8" style="padding-left:10.0pt;padding-right:10.0pt;">45.06</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T4.8.8.13.9" style="padding-left:10.0pt;padding-right:10.0pt;">0.533</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Comparison between different spectra under normal conditions. Models are trained with their corresponding spectrum images from the training set. We test them on the main testing set. </figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Limitation and Future Work</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Despite the advancements, we acknowledge certain limitations here guiding our future work. First, the scale of our current dataset is relatively small when compared to some datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib38" title="">38</a>]</cite>. We plan to continuously enlarge this dataset by incorporating data from new scenarios and hand actions. Second, we only annotate 3D hand pose for our dataset, which limits its usage of evaluation to other tasks relevant to human hands. Further efforts will be annotating the fine-grained hand action splits and contact heatmap for hand grasp <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib42" title="">42</a>]</cite>. Lastly, we currently rely on qualitative analysis for challenging conditions when our automatic annotation method becomes unreliable. In the future, we will leverage unaffected spectral images in individual cases to manually annotate the 3D hand pose.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">This paper introduces <span class="ltx_text ltx_font_typewriter" id="S6.p1.1.1">ThermoHands</span>, the first benchmark for egocentric 3D hand pose estimation using thermal images. It features a multi-spectral, multi-view dataset with automatically annotated 3D poses and a novel baseline method utilizing dual transformer modules for encoding spatio-temporal relationships. We demonstrate near 1cm annotation accuracy, show that TherFormer surpasses existing methods in thermal-based 3D hand pose estimation, and confirm thermal images’ effectiveness in challenging lighting and obstruction scenarios. We believe our endeavour could pave the way for further research in thermal-based 3D hand pose estimation and its wide application.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgments and Disclosure of Funding</h2>
<div class="ltx_para ltx_noindent" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This research is partially supported by the Engineering and Physical Sciences Research Council (EPSRC) under the Centre for Doctoral Training in Robotics and Autonomous Systems at the Edinburgh Centre of Robotics (EP/S023208/1), and partially supported by Cisco Research.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Min-Yu Wu, Pai-Wen Ting, Ya-Hui Tang, En-Te Chou, and Li-Chen Fu.

</span>
<span class="ltx_bibblock">Hand pose estimation in object-interaction based on deep learning for virtual reality applications.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">J. Vis. Commun. Image Represent.</span>, 70:102802, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
K Martin Sagayam and D Jude Hemanth.

</span>
<span class="ltx_bibblock">Hand posture and gesture recognition techniques for virtual reality applications: a survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">Virtual Reality</span>, 21:91–107, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Eric Marchand, Hideaki Uchiyama, and Fabien Spindler.

</span>
<span class="ltx_bibblock">Pose estimation for augmented reality: a hands-on survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">TVCG</span>, 22(12):2633–2651, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Hui Liang, Junsong Yuan, Daniel Thalmann, and Nadia Magnenat Thalmann.

</span>
<span class="ltx_bibblock">Ar in hand: Egocentric palm pose tracking and gesture recognition for augmented reality applications.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">ACM MM</span>, pages 743–744, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Alessio Sampieri, Guido Maria D’Amely di Melendugno, Andrea Avogaro, Federico Cunico, Francesco Setti, Geri Skenderi, Marco Cristani, and Fabio Galasso.

</span>
<span class="ltx_bibblock">Pose forecasting in industrial human-robot collaboration.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">ECCV</span>, pages 51–69. Springer, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Qing Gao, Yongquan Chen, Zhaojie Ju, and Yi Liang.

</span>
<span class="ltx_bibblock">Dynamic hand gesture recognition based on 3D hand pose estimation for human–robot interaction.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">IEEE Sensors Journal</span>, 22(18):17421–17430, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Qing Gao, Jinguo Liu, Zhaojie Ju, and Xin Zhang.

</span>
<span class="ltx_bibblock">Dual-hand detection for human–robot interaction by a parallel network based on hand detection and body pose estimation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">IEEE Transactions on Industrial Electronics</span>, 66(12):9663–9672, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Yuzhe Qin, Yueh-Hua Wu, Shaowei Liu, Hanwen Jiang, Ruihan Yang, Yang Fu, and Xiaolong Wang.

</span>
<span class="ltx_bibblock">Dexmv: Imitation learning for dexterous manipulation from human videos.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">ECCV</span>, pages 570–587. Springer, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Yuanpei Chen, Yiran Geng, Fangwei Zhong, Jiaming Ji, Jiechuang Jiang, Zongqing Lu, Hao Dong, and Yaodong Yang.

</span>
<span class="ltx_bibblock">Bi-DexHands: Towards Human-Level Bimanual Dexterous Manipulation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">PAMI</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Dafni Antotsiou, Guillermo Garcia-Hernando, and Tae-Kyun Kim.

</span>
<span class="ltx_bibblock">Task-oriented hand motion retargeting for dexterous manipulation imitation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">ECCVW</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Meta.

</span>
<span class="ltx_bibblock">Meta Quest.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.meta.com/gb/quest/" title="">https://www.meta.com/gb/quest/</a>, 2024.

</span>
<span class="ltx_bibblock">Accessed: 2024-02-23.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Apple.

</span>
<span class="ltx_bibblock">Apple Vision Pro.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.apple.com/apple-vision-pro/" title="">https://www.apple.com/apple-vision-pro/</a>, 2024.

</span>
<span class="ltx_bibblock">Accessed: 2024-02-23.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Fanqing Lin, Connor Wilhelm, and Tony Martinez.

</span>
<span class="ltx_bibblock">Two-hand global 3d pose estimation using monocular rgb.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">WACV</span>, pages 2373–2381, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Mengcheng Li, Liang An, Hongwen Zhang, Lianpeng Wu, Feng Chen, Tao Yu, and Yebin Liu.

</span>
<span class="ltx_bibblock">Interacting attention graph for single image two-hand reconstruction.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">CVPR</span>, pages 2761–2770, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Yilin Wen, Hao Pan, Lei Yang, Jia Pan, Taku Komura, and Wenping Wang.

</span>
<span class="ltx_bibblock">Hierarchical temporal transformer for 3d hand pose estimation and action recognition from egocentric rgb videos.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">CVPR</span>, pages 21243–21253, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Qichen Fu, Xingyu Liu, Ran Xu, Juan Carlos Niebles, and Kris M. Kitani.

</span>
<span class="ltx_bibblock">Deformer: Dynamic Fusion Transformer for Robust Hand Pose Estimation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">ICCV</span>, pages 23600–23611, October 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Hoseong Cho, Chanwoo Kim, Jihyeon Kim, Seongyeong Lee, Elkhan Ismayilzada, and Seungryul Baek.

</span>
<span class="ltx_bibblock">Transformer-Based Unified Recognition of Two Hands Manipulating Objects.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">CVPR</span>, pages 4769–4778, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Trenton’s Tech.

</span>
<span class="ltx_bibblock">Testing the Apple Vision Pro in Pitch Black Environments.

</span>
<span class="ltx_bibblock">YouTube, 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.youtube.com/watch?v=wlPpo_QFIRU" title="">https://www.youtube.com/watch?v=wlPpo_QFIRU</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Apple.

</span>
<span class="ltx_bibblock">Use gestures with Apple Vision Pro.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://support.apple.com/en-us/117741" title="">https://support.apple.com/en-us/117741</a>, 2024.

</span>
<span class="ltx_bibblock">Accessed: 2024-02-23.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Intel RealSense.

</span>
<span class="ltx_bibblock">LiDAR Camera L515.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.intelrealsense.com/lidar-camera-l515/" title="">https://www.intelrealsense.com/lidar-camera-l515/</a>, 2023.

</span>
<span class="ltx_bibblock">Accessed: 2024-02-27.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Intel RealSense.

</span>
<span class="ltx_bibblock">Depth Camera D455.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.intelrealsense.com/depth-camera-d455/" title="">https://www.intelrealsense.com/depth-camera-d455/</a>, 2023.

</span>
<span class="ltx_bibblock">Accessed: 2024-02-27.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Jesus Suarez and Robin R Murphy.

</span>
<span class="ltx_bibblock">Using the kinect for search and rescue robotics.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">SSRR</span>, pages 1–2. IEEE, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Lucas Adams Seewald, Vinicius Facco Rodrigues, Malte Ollenschläger, Rodolfo Stoffel Antunes, Cristiano André da Costa, Rodrigo da Rosa Righi, Luiz Gonzaga da Silveira Jr, Andreas Maier, Björn Eskofier, and Rebecca Fahrig.

</span>
<span class="ltx_bibblock">Toward analyzing mutual interference on infrared-enabled depth cameras.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">Computer Vision and Image Understanding</span>, 178:1–15, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
J Michael Lloyd.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">Thermal imaging systems</span>.

</span>
<span class="ltx_bibblock">Springer Science &amp; Business Media, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Javier Romero, Dimitrios Tzionas, and Michael J. Black.

</span>
<span class="ltx_bibblock">Embodied Hands: Modeling and Capturing Hands and Bodies Together.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">TOG</span>, 36(6), 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Shanxin Yuan, Qi Ye, Bjorn Stenger, Siddhant Jain, and Tae-Kyun Kim.

</span>
<span class="ltx_bibblock">Bighand2. 2m benchmark: Hand pose dataset and state of the art analysis.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">CVPR</span>, pages 4866–4874, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Guillermo Garcia-Hernando, Shanxin Yuan, Seungryul Baek, and Tae-Kyun Kim.

</span>
<span class="ltx_bibblock">First-person hand action benchmark with rgb-d videos and 3d hand pose annotations.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">CVPR</span>, pages 409–419, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Omid Taheri, Nima Ghorbani, Michael J Black, and Dimitrios Tzionas.

</span>
<span class="ltx_bibblock">GRAB: A dataset of whole-body human grasping of objects.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">ECCV</span>, pages 581–600. Springer, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Zicong Fan, Omid Taheri, Dimitrios Tzionas, Muhammed Kocabas, Manuel Kaufmann, Michael J Black, and Otmar Hilliges.

</span>
<span class="ltx_bibblock">ARCTIC: A Dataset for Dexterous Bimanual Hand-Object Manipulation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">CVPR</span>, pages 12943–12954, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Franziska Mueller, Dushyant Mehta, Oleksandr Sotnychenko, Srinath Sridhar, Dan Casas, and Christian Theobalt.

</span>
<span class="ltx_bibblock">Real-time hand tracking under occlusion from an egocentric rgb-d sensor.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">ICCV</span>, pages 1154–1163, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Yana Hasson, Gul Varol, Dimitrios Tzionas, Igor Kalevatykh, Michael J Black, Ivan Laptev, and Cordelia Schmid.

</span>
<span class="ltx_bibblock">Learning joint reconstruction of hands and manipulated objects.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">CVPR</span>, pages 11807–11816, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Franziska Mueller, Micah Davis, Florian Bernard, Oleksandr Sotnychenko, Mickeal Verschoor, Miguel A Otaduy, Dan Casas, and Christian Theobalt.

</span>
<span class="ltx_bibblock">Real-time pose and shape reconstruction of two interacting hands with a single depth camera.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">TOG</span>, 38(4):1–13, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Franziska Mueller, Florian Bernard, Oleksandr Sotnychenko, Dushyant Mehta, Srinath Sridhar, Dan Casas, and Christian Theobalt.

</span>
<span class="ltx_bibblock">Ganerated hands for real-time 3d hand tracking from monocular rgb.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">CVPR</span>, pages 49–59, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Christian Zimmermann and Thomas Brox.

</span>
<span class="ltx_bibblock">Learning to estimate 3d hand pose from single rgb images.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">ICCV</span>, pages 4903–4911, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Srinath Sridhar, Franziska Mueller, Michael Zollhöfer, Dan Casas, Antti Oulasvirta, and Christian Theobalt.

</span>
<span class="ltx_bibblock">Real-time joint tracking of a hand manipulating an object from rgb-d input.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">ECCV</span>, pages 294–310. Springer, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Yu-Wei Chao, Wei Yang, Yu Xiang, Pavlo Molchanov, Ankur Handa, Jonathan Tremblay, Yashraj S Narang, Karl Van Wyk, Umar Iqbal, Stan Birchfield, et al.

</span>
<span class="ltx_bibblock">DexYCB: A benchmark for capturing hand grasping of objects.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">CVPR</span>, pages 9044–9053, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Christian Zimmermann, Duygu Ceylan, Jimei Yang, Bryan Russell, Max Argus, and Thomas Brox.

</span>
<span class="ltx_bibblock">Freihand: A dataset for markerless capture of hand pose and shape from single rgb images.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">ICCV</span>, pages 813–822, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Gyeongsik Moon, Shoou-I Yu, He Wen, Takaaki Shiratori, and Kyoung Mu Lee.

</span>
<span class="ltx_bibblock">Interhand2. 6m: A dataset and baseline for 3d interacting hand pose estimation from a single rgb image.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">ECCV</span>, pages 548–564. Springer, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Yunze Liu, Yun Liu, Che Jiang, Kangbo Lyu, Weikang Wan, Hao Shen, Boqiang Liang, Zhoujie Fu, He Wang, and Li Yi.

</span>
<span class="ltx_bibblock">HOI4D: A 4D egocentric dataset for category-level human-object interaction.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">CVPR</span>, pages 21013–21022, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Takehiko Ohkawa, Kun He, Fadime Sener, Tomas Hodan, Luan Tran, and Cem Keskin.

</span>
<span class="ltx_bibblock">AssemblyHands: Towards Egocentric Activity Understanding via 3D Hand Pose Estimation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib40.1.1">CVPR</span>, pages 12999–13008, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Shreyas Hampali, Mahdi Rad, Markus Oberweger, and Vincent Lepetit.

</span>
<span class="ltx_bibblock">Honnotate: A method for 3d annotation of hand and object poses.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">CVPR</span>, pages 3196–3206, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Samarth Brahmbhatt, Chengcheng Tang, Christopher D Twigg, Charles C Kemp, and James Hays.

</span>
<span class="ltx_bibblock">ContactPose: A dataset of grasps with object contact and hand pose.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib42.1.1">ECCV</span>, pages 361–378. Springer, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Taein Kwon, Bugra Tekin, Jan Stühmer, Federica Bogo, and Marc Pollefeys.

</span>
<span class="ltx_bibblock">H2o: Two hands manipulating objects for first person interaction recognition.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib43.1.1">ICCV</span>, pages 10138–10148, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Fadime Sener, Dibyadip Chatterjee, Daniel Shelepov, Kun He, Dipika Singhania, Robert Wang, and Angela Yao.

</span>
<span class="ltx_bibblock">Assembly101: A large-scale multi-view video dataset for understanding procedural activities.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib44.1.1">CVPR</span>, pages 21096–21106, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh.

</span>
<span class="ltx_bibblock">OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib45.1.1">PAMI</span>, 43(1):172–186, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Changlong Jiang, Yang Xiao, Cunlin Wu, Mingyang Zhang, Jinghong Zheng, Zhiguo Cao, and Joey Tianyi Zhou.

</span>
<span class="ltx_bibblock">A2J-Transformer: Anchor-to-Joint Transformer Network for 3D Interacting Hand Pose Estimation from a Single RGB Image.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib46.1.1">CVPR</span>, pages 8846–8855, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Adrian Spurr, Umar Iqbal, Pavlo Molchanov, Otmar Hilliges, and Jan Kautz.

</span>
<span class="ltx_bibblock">Weakly supervised 3d hand pose estimation via biomechanical constraints.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib47.1.1">ECCV</span>, pages 211–228. Springer, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Linlin Yang and Angela Yao.

</span>
<span class="ltx_bibblock">Disentangling latent hands for image synthesis and pose estimation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib48.1.1">CVPR</span>, pages 9877–9886, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Wencan Cheng, Jae Hyun Park, and Jong Hwan Ko.

</span>
<span class="ltx_bibblock">Handfoldingnet: A 3d hand pose estimation network using multiscale-feature guided folding of a 2d hand skeleton.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib49.1.1">ICCV</span>, pages 11260–11269, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Dong Uk Kim, Kwang In Kim, and Seungryul Baek.

</span>
<span class="ltx_bibblock">End-to-end detection and pose estimation of two interacting hands.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib50.1.1">ICCV</span>, pages 11189–11198, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Breannan Smith, Chenglei Wu, He Wen, Patrick Peluse, Yaser Sheikh, Jessica K Hodgins, and Takaaki Shiratori.

</span>
<span class="ltx_bibblock">Constraining dense hand surface tracking with elasticity.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib51.1.1">TOG</span>, 39(6):1–14, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Jiayi Wang, Franziska Mueller, Florian Bernard, Suzanne Sorli, Oleksandr Sotnychenko, Neng Qian, Miguel A Otaduy, Dan Casas, and Christian Theobalt.

</span>
<span class="ltx_bibblock">Rgb2hands: real-time tracking of 3d hand interactions from monocular rgb video.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib52.1.1">TOG</span>, 39(6):1–16, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Baowen Zhang, Yangang Wang, Xiaoming Deng, Yinda Zhang, Ping Tan, Cuixia Ma, and Hongan Wang.

</span>
<span class="ltx_bibblock">Interacting two-hand 3d pose and shape reconstruction from single color image.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib53.1.1">ICCV</span>, pages 11354–11363, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Shaowei Liu, Hanwen Jiang, Jiarui Xu, Sifei Liu, and Xiaolong Wang.

</span>
<span class="ltx_bibblock">Semi-supervised 3d hand-object poses estimation with interactions in time.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib54.1.1">CVPR</span>, pages 14687–14697, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Adnane Boukhayma, Rodrigo de Bem, and Philip HS Torr.

</span>
<span class="ltx_bibblock">3d hand shape and pose from images in the wild.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib55.1.1">CVPR</span>, pages 10843–10852, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Seungryul Baek, Kwang In Kim, and Tae-Kyun Kim.

</span>
<span class="ltx_bibblock">Pushing the envelope for rgb-based dense 3d hand pose estimation via neural rendering.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib56.1.1">CVPR</span>, pages 1067–1076, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Xiong Zhang, Qiang Li, Hong Mo, Wenbo Zhang, and Wen Zheng.

</span>
<span class="ltx_bibblock">End-to-end hand mesh recovery from a monocular rgb image.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib57.1.1">ICCV</span>, pages 2354–2364, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Liuhao Ge, Hui Liang, Junsong Yuan, and Daniel Thalmann.

</span>
<span class="ltx_bibblock">Robust 3D hand pose estimation from single depth images using multi-view CNNs.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib58.1.1">IEEE Transactions on Image Processing</span>, 27(9):4422–4436, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Gyeongsik Moon, Ju Yong Chang, and Kyoung Mu Lee.

</span>
<span class="ltx_bibblock">V2v-posenet: Voxel-to-voxel prediction network for accurate 3d hand and human pose estimation from a single depth map.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib59.1.1">CVPR</span>, pages 5079–5088, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Fu Xiong, Boshen Zhang, Yang Xiao, Zhiguo Cao, Taidong Yu, Joey Tianyi Zhou, and Junsong Yuan.

</span>
<span class="ltx_bibblock">A2j: Anchor-to-joint regression network for 3d articulated pose estimation from a single depth image.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib60.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pages 793–802, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Zhaohui Zhang, Shipeng Xie, Mingxiu Chen, and Haichao Zhu.

</span>
<span class="ltx_bibblock">HandAugment: A simple data augmentation method for depth-based 3D hand pose estimation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib61.1.1">arXiv preprint arXiv:2001.00702</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Tianqiang Zhu, Yi Sun, Xiaohong Ma, and Xiangbo Lin.

</span>
<span class="ltx_bibblock">Hand Pose Ensemble Learning Based on Grouping Features of Hand Point Sets.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib62.1.1">ICCVW</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Yana Hasson, Bugra Tekin, Federica Bogo, Ivan Laptev, Marc Pollefeys, and Cordelia Schmid.

</span>
<span class="ltx_bibblock">Leveraging photometric consistency over time for sparsely supervised hand-object reconstruction.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib63.1.1">CVPR</span>, pages 571–580, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
John Yang, Hyung Jin Chang, Seungeui Lee, and Nojun Kwak.

</span>
<span class="ltx_bibblock">SeqHAND: RGB-sequence-based 3D hand pose and shape estimation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib64.1.1">ECCV</span>, pages 122–139. Springer, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Muhammed Kocabas, Nikos Athanasiou, and Michael J. Black.

</span>
<span class="ltx_bibblock">VIBE: Video Inference for Human Body Pose and Shape Estimation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib65.1.1">CVPR</span>, June 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
JoonKyu Park, Yeonguk Oh, Gyeongsik Moon, Hongsuk Choi, and Kyoung Mu Lee.

</span>
<span class="ltx_bibblock">Handoccnet: Occlusion-robust 3d hand mesh estimation network.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib66.1.1">CVPR</span>, pages 1496–1505, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Yujun Cai, Liuhao Ge, Jun Liu, Jianfei Cai, Tat-Jen Cham, Junsong Yuan, and Nadia Magnenat Thalmann.

</span>
<span class="ltx_bibblock">Exploiting Spatial-Temporal Relationships for 3D Pose Estimation via Graph Convolutional Networks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib67.1.1">ICCV</span>, October 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Leyla Khaleghi, Alireza Sepas-Moghaddam, Joshua Marshall, and Ali Etemad.

</span>
<span class="ltx_bibblock">Multi-view video-based 3D hand pose estimation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib68.1.1">IEEE Transactions on Artificial Intelligence</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Hongsuk Choi, Gyeongsik Moon, Ju Yong Chang, and Kyoung Mu Lee.

</span>
<span class="ltx_bibblock">Beyond static features for temporally consistent 3d human pose and shape from a video.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib69.1.1">CVPR</span>, pages 1964–1973, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
Rafael E Rivadeneira, Angel D Sappa, Boris X Vintimilla, Dai Bin, Li Ruodi, Li Shengye, Zhiwei Zhong, Xianming Liu, Junjun Jiang, and Chenyang Wang.

</span>
<span class="ltx_bibblock">Thermal Image Super-Resolution Challenge Results-PBVS 2023.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib70.1.1">CVPR</span>, pages 470–478, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Honey Gupta and Kaushik Mitra.

</span>
<span class="ltx_bibblock">Toward unaligned guided thermal super-resolution.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib71.1.1">TIP</span>, 31:433–445, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Priya Kansal and Sabari Nathan.

</span>
<span class="ltx_bibblock">A multi-level supervision model: A novel approach for thermal image super resolution.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib72.1.1">CVPR</span>, pages 94–95, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
Kaiwen Cai, Qiyue Xia, Peize Li, John Stankovic, and Chris Xiaoxuan Lu.

</span>
<span class="ltx_bibblock">Robust Human Detection under Visual Degradation via Thermal and mmWave Radar Fusion.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib73.1.1">EWSN</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Marina Ivašić-Kos, Mate Krišto, and Miran Pobar.

</span>
<span class="ltx_bibblock">Human detection in thermal imaging using YOLO.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib74.1.1">Proceedings of the 2019 5th International Conference on Computer and Technology Applications</span>, pages 20–24, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Ali Haider, Furqan Shaukat, and Junaid Mir.

</span>
<span class="ltx_bibblock">Human detection in aerial thermal imaging using a fully convolutional regression network.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib75.1.1">Infrared Physics &amp; Technology</span>, 116:103796, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
Ganbayar Batchuluun, Dat Tien Nguyen, Tuyen Danh Pham, Chanhum Park, and Kang Ryoung Park.

</span>
<span class="ltx_bibblock">Action recognition from thermal videos.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib76.1.1">IEEE Access</span>, 7:103893–103917, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
Meng Ding, Yuanyuan Ding, Li Wei, Yiming Xu, and Yunfeng Cao.

</span>
<span class="ltx_bibblock">Individual Surveillance Around Parked Aircraft at Nighttime: Thermal Infrared Vision-Based Human Action Recognition.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib77.1.1">IEEE Transactions on Systems, Man, and Cybernetics: Systems</span>, 53(2):1084–1094, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Marcos Lupión, Aurora Polo-Rodríguez, Javier Medina-Quero, Juan F Sanjuan, and Pilar M Ortigosa.

</span>
<span class="ltx_bibblock">3D Human Pose Estimation from multi-view thermal vision sensors.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib78.1.1">Information Fusion</span>, 104:102154, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
I-Chien Chen, Chang-Jen Wang, Chao-Kai Wen, and Shiow-Jyu Tzou.

</span>
<span class="ltx_bibblock">Multi-person pose estimation using thermal images.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib79.1.1">IEEE Access</span>, 8:174964–174971, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
Javier Smith, Patricio Loncomilla, and Javier Ruiz-Del-Solar.

</span>
<span class="ltx_bibblock">Human Pose Estimation using Thermal Images.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib80.1.1">IEEE Access</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
Zülfiye Kütük and Görkem Algan.

</span>
<span class="ltx_bibblock">Semantic segmentation for thermal images: A comparative survey.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib81.1.1">CVPR</span>, pages 286–295, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
Yeong-Hyeon Kim, Ukcheol Shin, Jinsun Park, and In So Kweon.

</span>
<span class="ltx_bibblock">MS-UDA: Multi-spectral unsupervised domain adaptation for thermal image semantic segmentation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib82.1.1">RA-L</span>, 6(4):6497–6504, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
Johan Vertens, Jannik Zürn, and Wolfram Burgard.

</span>
<span class="ltx_bibblock">Heatnet: Bridging the day-night domain gap in semantic segmentation with thermal images.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib83.1.1">IROS</span>, pages 8461–8468. IEEE, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
Chenglong Li, Wei Xia, Yan Yan, Bin Luo, and Jin Tang.

</span>
<span class="ltx_bibblock">Segmenting objects in day and night: Edge-conditioned CNN for thermal image semantic segmentation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib84.1.1">TNNLS</span>, 32(7):3069–3082, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
Ukcheol Shin, Jinsun Park, and In So Kweon.

</span>
<span class="ltx_bibblock">Deep Depth Estimation From Thermal Image.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib85.1.1">CVPR</span>, pages 1043–1053, June 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
Yawen Lu and Guoyu Lu.

</span>
<span class="ltx_bibblock">An alternative of lidar in nighttime: Unsupervised depth estimation based on single thermal image.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib86.1.1">WACV</span>, pages 3833–3843, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
Ukcheol Shin, Kyunghyun Lee, Seokju Lee, and In So Kweon.

</span>
<span class="ltx_bibblock">Self-supervised depth and ego-motion estimation for monocular thermal video using multi-spectral consistency loss.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib87.1.1">RA-L</span>, 7(2):1103–1110, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
Namil Kim, Yukyung Choi, Soonmin Hwang, and In So Kweon.

</span>
<span class="ltx_bibblock">Multispectral transfer network: Unsupervised depth estimation for all-day vision.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib88.1.1">AAAI</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
Shehryar Khattak, Christos Papachristos, and Kostas Alexis.

</span>
<span class="ltx_bibblock">Keyframe-based thermal–inertial odometry.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib89.1.1">Journal of Field Robotics</span>, 37(4):552–579, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
Saputra, Muhamad Risqi U. and Lu, Chris Xiaoxuan and de Gusmao, Pedro Porto B. and Wang, Bing and Markham, Andrew and Trigoni, Niki.

</span>
<span class="ltx_bibblock">Graph-based thermal–inertial slam with probabilistic neural networks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib90.1.1">TRO</span>, 38(3):1875–1893, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
Young-Sik Shin and Ayoung Kim.

</span>
<span class="ltx_bibblock">Sparse depth enhanced direct thermal-infrared SLAM beyond the visible spectrum.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib91.1.1">RA-L</span>, 4(3):2918–2925, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
Muhamad Risqi U Saputra, Pedro PB de Gusmao, Chris Xiaoxuan Lu, Yasin Almalioglu, Stefano Rosa, Changhao Chen, Johan Wahlström, Wei Wang, Andrew Markham, and Niki Trigoni.

</span>
<span class="ltx_bibblock">Deeptio: A deep thermal-inertial odometry with visual hallucination.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib92.1.1">RA-L</span>, 5(2):1672–1679, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
Ruoshi Liu and Carl Vondrick.

</span>
<span class="ltx_bibblock">Humans as Light Bulbs: 3D Human Reconstruction from Thermal Reflection.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib93.1.1">CVPR</span>, pages 12531–12542, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
Agata Sage, Daniel Ledwoń, Jan Juszczyk, and Paweł Badura.

</span>
<span class="ltx_bibblock">3D Thermal Volume Reconstruction from 2D Infrared Images—a Preliminary Study.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib94.1.1">Innovations in Biomedical Engineering</span>, pages 371–379. Springer, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
Sebastian Schramm, Phil Osterhold, Robert Schmoll, and Andreas Kroll.

</span>
<span class="ltx_bibblock">Combining modern 3D reconstruction and thermal imaging: Generation of large-scale 3D thermograms in real-time.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib95.1.1">Quantitative InfraRed Thermography Journal</span>, 19(5):295–311, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
Teledyne FLIR.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib96.1.1">Boson - Uncooled, Longwave Infrared (LWIR) OEM Thermal Camera Module</span>, 2024.

</span>
<span class="ltx_bibblock">Accessed: 2024-02-13.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
Martin A. Fischler and Robert C. Bolles.

</span>
<span class="ltx_bibblock">Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib97.1.1">Communications of the ACM</span>, 24(6):381–395, 1981.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
Ignacio Vizzo, Tiziano Guadagnino, Benedikt Mersch, Louis Wiesmann, Jens Behley, and Cyrill Stachniss.

</span>
<span class="ltx_bibblock">Kiss-icp: In defense of point-to-point icp–simple, accurate, and robust registration if done the right way.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib98.1.1">RA-L</span>, 8(2):1029–1036, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
Google LLC.

</span>
<span class="ltx_bibblock">MediaPipe Hands.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/google/mediapipe" title="">https://github.com/google/mediapipe</a>, 2020.

</span>
<span class="ltx_bibblock">Accessed: 2024-02-13.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.

</span>
<span class="ltx_bibblock">Segment anything.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib100.1.1">arXiv preprint arXiv:2304.02643</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai.

</span>
<span class="ltx_bibblock">Deformable detr: Deformable transformers for end-to-end object detection.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib101.1.1">ICLR</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib102.1.1">NIPS</span>, 30, 2017.

</span>
</li>
</ul>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Checklist</h2>
<div class="ltx_para ltx_noindent" id="Sx2.p1">
<ol class="ltx_enumerate" id="Sx2.I1">
<li class="ltx_item" id="Sx2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para ltx_noindent" id="Sx2.I1.i1.p1">
<p class="ltx_p" id="Sx2.I1.i1.p1.1">For all authors…</p>
<ol class="ltx_enumerate" id="Sx2.I1.i1.I1">
<li class="ltx_item" id="Sx2.I1.i1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span>
<div class="ltx_para" id="Sx2.I1.i1.I1.i1.p1">
<p class="ltx_p" id="Sx2.I1.i1.I1.i1.p1.1">Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope?
<span class="ltx_text" id="Sx2.I1.i1.I1.i1.p1.1.1" style="color:#0000FF;">[Yes] </span>We elaborate our contributions as a list in the introduction section and briefly explain them in the abstract.</p>
</div>
</li>
<li class="ltx_item" id="Sx2.I1.i1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span>
<div class="ltx_para" id="Sx2.I1.i1.I1.i2.p1">
<p class="ltx_p" id="Sx2.I1.i1.I1.i2.p1.1">Did you describe the limitations of your work?
<span class="ltx_text" id="Sx2.I1.i1.I1.i2.p1.1.1" style="color:#0000FF;">[Yes] </span>See <a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#S5" title="5 Limitation and Future Work ‣ ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">5</span></a></p>
</div>
</li>
<li class="ltx_item" id="Sx2.I1.i1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(c)</span>
<div class="ltx_para" id="Sx2.I1.i1.I1.i3.p1">
<p class="ltx_p" id="Sx2.I1.i1.I1.i3.p1.1">Did you discuss any potential negative societal impacts of your work?
<span class="ltx_text" id="Sx2.I1.i1.I1.i3.p1.1.1" style="color:#808080;">[N/A] </span>There is no societal impact of this work.</p>
</div>
</li>
<li class="ltx_item" id="Sx2.I1.i1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(d)</span>
<div class="ltx_para ltx_noindent" id="Sx2.I1.i1.I1.i4.p1">
<p class="ltx_p" id="Sx2.I1.i1.I1.i4.p1.1">Have you read the ethics review guidelines and ensured that your paper conforms to them?
<span class="ltx_text" id="Sx2.I1.i1.I1.i4.p1.1.1" style="color:#0000FF;">[Yes] </span>We have reviewed the ethics review guidelines and make sure we obey all rules for this research.</p>
</div>
</li>
</ol>
</div>
</li>
<li class="ltx_item" id="Sx2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para ltx_noindent" id="Sx2.I1.i2.p1">
<p class="ltx_p" id="Sx2.I1.i2.p1.1">If you are including theoretical results…</p>
<ol class="ltx_enumerate" id="Sx2.I1.i2.I1">
<li class="ltx_item" id="Sx2.I1.i2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span>
<div class="ltx_para" id="Sx2.I1.i2.I1.i1.p1">
<p class="ltx_p" id="Sx2.I1.i2.I1.i1.p1.1">Did you state the full set of assumptions of all theoretical results?
<span class="ltx_text" id="Sx2.I1.i2.I1.i1.p1.1.1" style="color:#808080;">[N/A] </span>This work does not contribute to the theoretical results.</p>
</div>
</li>
<li class="ltx_item" id="Sx2.I1.i2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span>
<div class="ltx_para ltx_noindent" id="Sx2.I1.i2.I1.i2.p1">
<p class="ltx_p" id="Sx2.I1.i2.I1.i2.p1.1">Did you include complete proofs of all theoretical results?
<span class="ltx_text" id="Sx2.I1.i2.I1.i2.p1.1.1" style="color:#808080;">[N/A] </span>This work does not contribute to the theoretical results.</p>
</div>
</li>
</ol>
</div>
</li>
<li class="ltx_item" id="Sx2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para ltx_noindent" id="Sx2.I1.i3.p1">
<p class="ltx_p" id="Sx2.I1.i3.p1.1">If you ran experiments (e.g. for benchmarks)…</p>
<ol class="ltx_enumerate" id="Sx2.I1.i3.I1">
<li class="ltx_item" id="Sx2.I1.i3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span>
<div class="ltx_para" id="Sx2.I1.i3.I1.i1.p1">
<p class="ltx_p" id="Sx2.I1.i3.I1.i1.p1.1">Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
<span class="ltx_text" id="Sx2.I1.i3.I1.i1.p1.1.1" style="color:#FF8000;">[No] </span>We will release our code upon the acceptance of this paper.</p>
</div>
</li>
<li class="ltx_item" id="Sx2.I1.i3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span>
<div class="ltx_para" id="Sx2.I1.i3.I1.i2.p1">
<p class="ltx_p" id="Sx2.I1.i3.I1.i2.p1.1">Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
<span class="ltx_text" id="Sx2.I1.i3.I1.i2.p1.1.1" style="color:#0000FF;">[Yes] </span>We describe them in our main paper and supplementary document.</p>
</div>
</li>
<li class="ltx_item" id="Sx2.I1.i3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(c)</span>
<div class="ltx_para" id="Sx2.I1.i3.I1.i3.p1">
<p class="ltx_p" id="Sx2.I1.i3.I1.i3.p1.1">Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
<span class="ltx_text" id="Sx2.I1.i3.I1.i3.p1.1.1" style="color:#FF8000;">[No] </span>In line with the state-of-the-art 3D hand pose estimation works, we do not report error bars for our experiment results.</p>
</div>
</li>
<li class="ltx_item" id="Sx2.I1.i3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(d)</span>
<div class="ltx_para ltx_noindent" id="Sx2.I1.i3.I1.i4.p1">
<p class="ltx_p" id="Sx2.I1.i3.I1.i4.p1.1">Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?
<span class="ltx_text" id="Sx2.I1.i3.I1.i4.p1.1.1" style="color:#0000FF;">[Yes] </span>We include the type of GPUs in the main paper.</p>
</div>
</li>
</ol>
</div>
</li>
<li class="ltx_item" id="Sx2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para ltx_noindent" id="Sx2.I1.i4.p1">
<p class="ltx_p" id="Sx2.I1.i4.p1.1">If you are using existing assets (e.g., code, data, models) or curating/releasing new assets…</p>
<ol class="ltx_enumerate" id="Sx2.I1.i4.I1">
<li class="ltx_item" id="Sx2.I1.i4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span>
<div class="ltx_para" id="Sx2.I1.i4.I1.i1.p1">
<p class="ltx_p" id="Sx2.I1.i4.I1.i1.p1.1">If your work uses existing assets, did you cite the creators?
<span class="ltx_text" id="Sx2.I1.i4.I1.i1.p1.1.1" style="color:#0000FF;">[Yes] </span>We use the offical code of HTT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib15" title="">15</a>]</cite> and A2J-transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.09871v3#bib.bib46" title="">46</a>]</cite> to reproduce their results on our dataset, which have been cited.</p>
</div>
</li>
<li class="ltx_item" id="Sx2.I1.i4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span>
<div class="ltx_para" id="Sx2.I1.i4.I1.i2.p1">
<p class="ltx_p" id="Sx2.I1.i4.I1.i2.p1.1">Did you mention the license of the assets?
<span class="ltx_text" id="Sx2.I1.i4.I1.i2.p1.1.1" style="color:#0000FF;">[Yes] </span>We mention the license of the offical codes of HTT and A2J-Transformer in the supplementary document.</p>
</div>
</li>
<li class="ltx_item" id="Sx2.I1.i4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(c)</span>
<div class="ltx_para" id="Sx2.I1.i4.I1.i3.p1">
<p class="ltx_p" id="Sx2.I1.i4.I1.i3.p1.1">Did you include any new assets either in the supplemental material or as a URL?
<span class="ltx_text" id="Sx2.I1.i4.I1.i3.p1.1.1" style="color:#0000FF;">[Yes] </span>We include the datasheet and the download URL of our dataset in the supplemental document.</p>
</div>
</li>
<li class="ltx_item" id="Sx2.I1.i4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(d)</span>
<div class="ltx_para" id="Sx2.I1.i4.I1.i4.p1">
<p class="ltx_p" id="Sx2.I1.i4.I1.i4.p1.1">Did you discuss whether and how consent was obtained from people whose data you’re using/curating?
<span class="ltx_text" id="Sx2.I1.i4.I1.i4.p1.1.1" style="color:#0000FF;">[Yes] </span>We claim in the footnote that participant consent forms were signed before the collection.</p>
</div>
</li>
<li class="ltx_item" id="Sx2.I1.i4.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(e)</span>
<div class="ltx_para ltx_noindent" id="Sx2.I1.i4.I1.i5.p1">
<p class="ltx_p" id="Sx2.I1.i4.I1.i5.p1.1">Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?
<span class="ltx_text" id="Sx2.I1.i4.I1.i5.p1.1.1" style="color:#0000FF;">[Yes] </span>We discuss in the supplementary document.</p>
</div>
</li>
</ol>
</div>
</li>
<li class="ltx_item" id="Sx2.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para ltx_noindent" id="Sx2.I1.i5.p1">
<p class="ltx_p" id="Sx2.I1.i5.p1.1">If you used crowdsourcing or conducted research with human subjects…</p>
<ol class="ltx_enumerate" id="Sx2.I1.i5.I1">
<li class="ltx_item" id="Sx2.I1.i5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span>
<div class="ltx_para" id="Sx2.I1.i5.I1.i1.p1">
<p class="ltx_p" id="Sx2.I1.i5.I1.i1.p1.1">Did you include the full text of instructions given to participants and screenshots, if applicable?
<span class="ltx_text" id="Sx2.I1.i5.I1.i1.p1.1.1" style="color:#808080;">[N/A] </span>We do not give written instructions to our participants.</p>
</div>
</li>
<li class="ltx_item" id="Sx2.I1.i5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span>
<div class="ltx_para" id="Sx2.I1.i5.I1.i2.p1">
<p class="ltx_p" id="Sx2.I1.i5.I1.i2.p1.1">Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?
<span class="ltx_text" id="Sx2.I1.i5.I1.i2.p1.1.1" style="color:#808080;">[N/A] </span>There is no potential participant risks.</p>
</div>
</li>
<li class="ltx_item" id="Sx2.I1.i5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(c)</span>
<div class="ltx_para ltx_noindent" id="Sx2.I1.i5.I1.i3.p1">
<p class="ltx_p" id="Sx2.I1.i5.I1.i3.p1.1">Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?
<span class="ltx_text" id="Sx2.I1.i5.I1.i3.p1.1.1" style="color:#808080;">[N/A] </span>Our participants are volunteer.</p>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Jun 13 16:49:39 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
