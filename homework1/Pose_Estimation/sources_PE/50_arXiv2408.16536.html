<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators</title>
<!--Generated on Wed Aug 28 15:07:32 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2408.16536v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S1" title="In Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S2" title="In Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S2.SS0.SSS0.Px1" title="In 2 Related Work ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title">3D human pose estimation benchmarks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S2.SS0.SSS0.Px2" title="In 2 Related Work ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title"></span> ‣ <span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S2.SS0.SSS0.Px3" title="In 2 Related Work ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title">Auditing with generative models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3" title="In Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3.SS1" title="In 3 Method ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Evaluation with Controlled Generated Test Sets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3.SS2" title="In 3 Method ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Pose-Conditioned Image Generation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3.SS2.SSS0.Px1" title="In 3.2 Pose-Conditioned Image Generation ‣ 3 Method ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title">CN-3DPose</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3.SS3" title="In 3 Method ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Dataset Generation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3.SS3.SSS0.Px1" title="In 3.3 Dataset Generation ‣ 3 Method ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title">Set of attributes</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3.SS3.SSS0.Px2" title="In 3.3 Dataset Generation ‣ 3 Method ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title">Variation reduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3.SS3.SSS0.Px3" title="In 3.3 Dataset Generation ‣ 3 Method ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title">Quality control</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3.SS4" title="In 3 Method ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Evaluation Protocol</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S4" title="In Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S4.SS1" title="In 4 Experiments ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Implementation Details</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S4.SS1.SSS0.Px1" title="In 4.1 Implementation Details ‣ 4 Experiments ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title">Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S4.SS1.SSS0.Px2" title="In 4.1 Implementation Details ‣ 4 Experiments ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title">Pose estimators</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S4.SS1.SSS0.Px3" title="In 4.1 Implementation Details ‣ 4 Experiments ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title">Data generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S4.SS2" title="In 4 Experiments ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Examining Sensitivity to Various Attributes</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S4.SS2.SSS0.Px1" title="In 4.2 Examining Sensitivity to Various Attributes ‣ 4 Experiments ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title">Summary</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S4.SS2.SSS0.Px2" title="In 4.2 Examining Sensitivity to Various Attributes ‣ 4 Experiments ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title">Diverse training data reduces sensitivity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S4.SS2.SSS0.Px3" title="In 4.2 Examining Sensitivity to Various Attributes ‣ 4 Experiments ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title">Architectural choices matter</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S4.SS2.SSS0.Px4" title="In 4.2 Examining Sensitivity to Various Attributes ‣ 4 Experiments ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title">Clothing significantly affects pose estimation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S4.SS2.SSS0.Px5" title="In 4.2 Examining Sensitivity to Various Attributes ‣ 4 Experiments ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title">Location and weather</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S4.SS2.SSS0.Px6" title="In 4.2 Examining Sensitivity to Various Attributes ‣ 4 Experiments ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title">Pose estimators are robust against protected attributes</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S4.SS3" title="In 4 Experiments ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Quality Assessment of STAGE Data</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S5" title="In Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S1a" title="In Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Further Analysis on Real <em class="ltx_emph ltx_font_italic">vs</em>.<span class="ltx_text"></span> Synthetic Gap</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S2a" title="In Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Implementation Details</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S2.SS1" title="In B Implementation Details ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Model Training</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S2.SS1.SSS0.Px1" title="In B.1 Model Training ‣ B Implementation Details ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title">Preprocessing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S2.SS1.SSS0.Px2" title="In B.1 Model Training ‣ B Implementation Details ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title">Caption generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S2.SS1.SSS0.Px3" title="In B.1 Model Training ‣ B Implementation Details ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title">Training</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S2.SS2" title="In B Implementation Details ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span>2D Pose Estimation Filter</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S2.SS3" title="In B Implementation Details ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.3 </span>Constructing the Synthetic Replica of 3DPW</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S2.SS4" title="In B Implementation Details ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.4 </span>Generating Data for Attribute Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S2.SS4.SSS0.Px1" title="In B.4 Generating Data for Attribute Experiments ‣ B Implementation Details ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title">Sampling poses</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S2.SS4.SSS0.Px2" title="In B.4 Generating Data for Attribute Experiments ‣ B Implementation Details ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title">Rendering model input</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S2.SS4.SSS0.Px3" title="In B.4 Generating Data for Attribute Experiments ‣ B Implementation Details ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title">Choosing a base prompt</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S2.SS4.SSS0.Px4" title="In B.4 Generating Data for Attribute Experiments ‣ B Implementation Details ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title">Generating images</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3a" title="In Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Full Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S4a" title="In Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Qualitative Results</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_pruned_first">
<h1 class="ltx_title ltx_title_document">Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Nikita Kister<sup class="ltx_sup" id="id1.1.id1">1</sup> István Sárándi<sup class="ltx_sup" id="id2.2.id2">2</sup> Anna Khoreva<sup class="ltx_sup" id="id3.3.id3">3</sup>
 Gerard Pons-Moll<sup class="ltx_sup" id="id4.4.id4">2,4</sup>
<br class="ltx_break"/>
<sup class="ltx_sup" id="id5.5.id5">1</sup>Bosch IoC Lab, University of Tübingen  <sup class="ltx_sup" id="id6.6.id6">2</sup>Tübingen AI Center, University of Tübingen
<br class="ltx_break"/><sup class="ltx_sup" id="id7.7.id7">3</sup>Bosch Center for Artificial Intelligence
<br class="ltx_break"/><sup class="ltx_sup" id="id8.8.id8">4</sup>Max Planck Institute for Informatics, Saarland Informatics Campus
<br class="ltx_break"/>
<span class="ltx_text ltx_font_typewriter" id="id9.9.id9" style="font-size:90%;">{nikita.kister,istvan.sarandi,gerard.pons-moll}@uni-tuebingen.de</span>
 <span class="ltx_text ltx_font_typewriter" id="id10.10.id10" style="font-size:90%;">anna.khoreva@bosch.com</span>
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id11.id1"><span class="ltx_text" id="id11.id1.1" style="font-size:90%;">The estimation of 3D human poses from images has progressed tremendously over the last few years as measured on standard benchmarks.
However, performance in the open world remains underexplored, as current benchmarks cannot capture its full extent.
Especially in safety-critical systems, it is crucial that 3D pose estimators are audited before deployment, and their
sensitivity towards single factors or attributes occurring in the operational domain is thoroughly examined.
Nevertheless, we currently lack a benchmark that would enable such fine-grained analysis.
We thus present STAGE, a GenAI data toolkit for auditing 3D human pose estimators.
We enable a text-to-image model to control the 3D human body pose in the generated image.
This allows us to create customized annotated data covering a wide range of open-world attributes.
We leverage STAGE and generate a series of benchmarks to audit the sensitivity of popular pose estimators towards attributes such as gender, ethnicity, age, clothing, location, and weather.
Our results show that the presence of such naturally occurring attributes can cause severe degradation in the performance of pose estimators and leads us to question if they are ready for open-world deployment.
Code, data, and models will be released at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://virtualhumans.mpi-inf.mpg.de/stage" title="">https://virtualhumans.mpi-inf.mpg.de/stage</a>.</span></p>
</div>
<div class="ltx_para" id="p2">
<table class="ltx_tabular ltx_align_middle" id="p2.27">
<tr class="ltx_tr" id="p2.27.28">
<td class="ltx_td ltx_align_center" id="p2.27.28.1">
<span class="ltx_ERROR undefined" id="p2.27.28.1.1">\pbox</span><span class="ltx_text" id="p2.27.28.1.2" style="font-size:90%;">0.13Ground truth pose</span>
</td>
<td class="ltx_td ltx_align_center" colspan="2" id="p2.27.28.2">
<span class="ltx_ERROR undefined" id="p2.27.28.2.1">\pbox</span><span class="ltx_text" id="p2.27.28.2.2" style="font-size:90%;">0.19Base image</span>
</td>
<td class="ltx_td ltx_align_center" colspan="4" id="p2.27.28.3">
<span class="ltx_ERROR undefined" id="p2.27.28.3.1">\pbox</span><span class="ltx_text" id="p2.27.28.3.2" style="font-size:90%;">0.56Image with attribute change</span>
</td>
</tr>
<tr class="ltx_tr" id="p2.2.2">
<td class="ltx_td ltx_align_center" id="p2.2.2.3">
<span class="ltx_ERROR undefined" id="p2.2.2.3.1">\pbox</span><span class="ltx_text" id="p2.2.2.3.2" style="font-size:90%;">0.13</span>
</td>
<td class="ltx_td ltx_align_center" colspan="2" id="p2.2.2.4">
<span class="ltx_ERROR undefined" id="p2.2.2.4.1">\pbox</span><span class="ltx_text" id="p2.2.2.4.2" style="font-size:90%;">0.28Young man in gym</span>
</td>
<td class="ltx_td ltx_align_center" colspan="2" id="p2.1.1.1">
<span class="ltx_ERROR undefined" id="p2.1.1.1.1">\pbox</span><span class="ltx_text" id="p2.1.1.1.2" style="font-size:90%;">0.28male </span><math alttext="\rightarrow" class="ltx_Math" display="inline" id="p2.1.1.1.m1.1"><semantics id="p2.1.1.1.m1.1a"><mo id="p2.1.1.1.m1.1.1" mathsize="90%" stretchy="false" xref="p2.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="p2.1.1.1.m1.1b"><ci id="p2.1.1.1.m1.1.1.cmml" xref="p2.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="p2.1.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="p2.1.1.1.m1.1d">→</annotation></semantics></math><span class="ltx_text" id="p2.1.1.1.3" style="font-size:90%;"> </span><span class="ltx_text" id="p2.1.1.1.4" style="font-size:90%;color:#BF0040;">female</span>
</td>
<td class="ltx_td ltx_align_center" colspan="2" id="p2.2.2.2">
<span class="ltx_ERROR undefined" id="p2.2.2.2.1">\pbox</span><span class="ltx_text" id="p2.2.2.2.2" style="font-size:90%;">0.28young </span><math alttext="\rightarrow" class="ltx_Math" display="inline" id="p2.2.2.2.m1.1"><semantics id="p2.2.2.2.m1.1a"><mo id="p2.2.2.2.m1.1.1" mathsize="90%" stretchy="false" xref="p2.2.2.2.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="p2.2.2.2.m1.1b"><ci id="p2.2.2.2.m1.1.1.cmml" xref="p2.2.2.2.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="p2.2.2.2.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="p2.2.2.2.m1.1d">→</annotation></semantics></math><span class="ltx_text" id="p2.2.2.2.3" style="font-size:90%;"> </span><span class="ltx_text" id="p2.2.2.2.4" style="font-size:90%;color:#BF0040;">elderly</span>
</td>
</tr>
<tr class="ltx_tr" id="p2.9.9">
<td class="ltx_td ltx_align_center" id="p2.3.3.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="90" id="p2.3.3.1.g1" src="extracted/5819071/data/teaser/teaser-1/gt_0.png" width="90"/></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="p2.4.4.2"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="90" id="p2.4.4.2.g1" src="extracted/5819071/data/teaser/teaser-1/base_0.png" width="90"/></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="p2.5.5.3"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="90" id="p2.5.5.3.g1" src="extracted/5819071/data/teaser/teaser-1/base_pred_0.png" width="90"/></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="p2.6.6.4"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="90" id="p2.6.6.4.g1" src="extracted/5819071/data/teaser/teaser-1/att_0_0.png" width="90"/></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="p2.7.7.5"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="90" id="p2.7.7.5.g1" src="extracted/5819071/data/teaser/teaser-1/att_pred_0_0.png" width="90"/></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="p2.8.8.6"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="90" id="p2.8.8.6.g1" src="extracted/5819071/data/teaser/teaser-1/att_0_1.png" width="90"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="p2.9.9.7"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="90" id="p2.9.9.7.g1" src="extracted/5819071/data/teaser/teaser-1/att_pred_0_1.png" width="90"/></td>
</tr>
<tr class="ltx_tr" id="p2.11.11">
<td class="ltx_td ltx_align_center" id="p2.11.11.3">
<span class="ltx_ERROR undefined" id="p2.11.11.3.1">\pbox</span><span class="ltx_text" id="p2.11.11.3.2" style="font-size:90%;">0.13</span>
</td>
<td class="ltx_td ltx_align_center" colspan="2" id="p2.11.11.4">
<span class="ltx_ERROR undefined" id="p2.11.11.4.1">\pbox</span><span class="ltx_text" id="p2.11.11.4.2" style="font-size:90%;">0.28Man wearing t-shirt in city</span>
</td>
<td class="ltx_td ltx_align_center" colspan="2" id="p2.10.10.1">
<span class="ltx_ERROR undefined" id="p2.10.10.1.1">\pbox</span><span class="ltx_text" id="p2.10.10.1.2" style="font-size:90%;">0.28t-shirt </span><math alttext="\rightarrow" class="ltx_Math" display="inline" id="p2.10.10.1.m1.1"><semantics id="p2.10.10.1.m1.1a"><mo id="p2.10.10.1.m1.1.1" mathsize="90%" stretchy="false" xref="p2.10.10.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="p2.10.10.1.m1.1b"><ci id="p2.10.10.1.m1.1.1.cmml" xref="p2.10.10.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="p2.10.10.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="p2.10.10.1.m1.1d">→</annotation></semantics></math><span class="ltx_text" id="p2.10.10.1.3" style="font-size:90%;"> </span><span class="ltx_text" id="p2.10.10.1.4" style="font-size:90%;color:#BF0040;">floral shirt</span>
</td>
<td class="ltx_td ltx_align_center" colspan="2" id="p2.11.11.2">
<span class="ltx_ERROR undefined" id="p2.11.11.2.1">\pbox</span><span class="ltx_text" id="p2.11.11.2.2" style="font-size:90%;">0.28t-shirt </span><math alttext="\rightarrow" class="ltx_Math" display="inline" id="p2.11.11.2.m1.1"><semantics id="p2.11.11.2.m1.1a"><mo id="p2.11.11.2.m1.1.1" mathsize="90%" stretchy="false" xref="p2.11.11.2.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="p2.11.11.2.m1.1b"><ci id="p2.11.11.2.m1.1.1.cmml" xref="p2.11.11.2.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="p2.11.11.2.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="p2.11.11.2.m1.1d">→</annotation></semantics></math><span class="ltx_text" id="p2.11.11.2.3" style="font-size:90%;"> </span><span class="ltx_text" id="p2.11.11.2.4" style="font-size:90%;color:#BF0040;">parka</span>
</td>
</tr>
<tr class="ltx_tr" id="p2.18.18">
<td class="ltx_td ltx_align_center" id="p2.12.12.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="90" id="p2.12.12.1.g1" src="extracted/5819071/data/teaser/teaser-1/gt_1.png" width="90"/></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="p2.13.13.2"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="90" id="p2.13.13.2.g1" src="extracted/5819071/data/teaser/teaser-1/base_1.png" width="90"/></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="p2.14.14.3"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="90" id="p2.14.14.3.g1" src="extracted/5819071/data/teaser/teaser-1/base_pred_1.png" width="90"/></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="p2.15.15.4"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="90" id="p2.15.15.4.g1" src="extracted/5819071/data/teaser/teaser-1/att_1_0.png" width="90"/></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="p2.16.16.5"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="90" id="p2.16.16.5.g1" src="extracted/5819071/data/teaser/teaser-1/att_pred_1_0.png" width="90"/></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="p2.17.17.6"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="90" id="p2.17.17.6.g1" src="extracted/5819071/data/teaser/teaser-1/att_1_1.png" width="90"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="p2.18.18.7"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="90" id="p2.18.18.7.g1" src="extracted/5819071/data/teaser/teaser-1/att_pred_1_1.png" width="90"/></td>
</tr>
<tr class="ltx_tr" id="p2.20.20">
<td class="ltx_td ltx_align_center" id="p2.20.20.3">
<span class="ltx_ERROR undefined" id="p2.20.20.3.1">\pbox</span><span class="ltx_text" id="p2.20.20.3.2" style="font-size:90%;">0.13</span>
</td>
<td class="ltx_td ltx_align_center" colspan="2" id="p2.20.20.4">
<span class="ltx_ERROR undefined" id="p2.20.20.4.1">\pbox</span><span class="ltx_text" id="p2.20.20.4.2" style="font-size:90%;">0.28Man wearing t-shirt in forest</span>
</td>
<td class="ltx_td ltx_align_center" colspan="2" id="p2.19.19.1">
<span class="ltx_ERROR undefined" id="p2.19.19.1.1">\pbox</span><span class="ltx_text" id="p2.19.19.1.2" style="font-size:90%;">0.28t-shirt </span><math alttext="\rightarrow" class="ltx_Math" display="inline" id="p2.19.19.1.m1.1"><semantics id="p2.19.19.1.m1.1a"><mo id="p2.19.19.1.m1.1.1" mathsize="90%" stretchy="false" xref="p2.19.19.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="p2.19.19.1.m1.1b"><ci id="p2.19.19.1.m1.1.1.cmml" xref="p2.19.19.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="p2.19.19.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="p2.19.19.1.m1.1d">→</annotation></semantics></math><span class="ltx_text" id="p2.19.19.1.3" style="font-size:90%;"> </span><span class="ltx_text" id="p2.19.19.1.4" style="font-size:90%;color:#BF0040;">suit</span>
</td>
<td class="ltx_td ltx_align_center" colspan="2" id="p2.20.20.2">
<span class="ltx_ERROR undefined" id="p2.20.20.2.1">\pbox</span><span class="ltx_text" id="p2.20.20.2.2" style="font-size:90%;">0.28t-shirt </span><math alttext="\rightarrow" class="ltx_Math" display="inline" id="p2.20.20.2.m1.1"><semantics id="p2.20.20.2.m1.1a"><mo id="p2.20.20.2.m1.1.1" mathsize="90%" stretchy="false" xref="p2.20.20.2.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="p2.20.20.2.m1.1b"><ci id="p2.20.20.2.m1.1.1.cmml" xref="p2.20.20.2.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="p2.20.20.2.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="p2.20.20.2.m1.1d">→</annotation></semantics></math><span class="ltx_text" id="p2.20.20.2.3" style="font-size:90%;"> </span><span class="ltx_text" id="p2.20.20.2.4" style="font-size:90%;color:#BF0040;">leather jacket</span>
</td>
</tr>
<tr class="ltx_tr" id="p2.27.27">
<td class="ltx_td ltx_align_center" id="p2.21.21.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="90" id="p2.21.21.1.g1" src="extracted/5819071/data/teaser/teaser-1/gt_2.png" width="90"/></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="p2.22.22.2"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="90" id="p2.22.22.2.g1" src="extracted/5819071/data/teaser/teaser-1/base_2.png" width="90"/></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="p2.23.23.3"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="90" id="p2.23.23.3.g1" src="extracted/5819071/data/teaser/teaser-1/base_pred_2.png" width="90"/></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="p2.24.24.4"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="90" id="p2.24.24.4.g1" src="extracted/5819071/data/teaser/teaser-1/att_2_0.png" width="90"/></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="p2.25.25.5"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="90" id="p2.25.25.5.g1" src="extracted/5819071/data/teaser/teaser-1/att_pred_2_0.png" width="90"/></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="p2.26.26.6"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="90" id="p2.26.26.6.g1" src="extracted/5819071/data/teaser/teaser-1/att_2_1.png" width="90"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="p2.27.27.7"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="90" id="p2.27.27.7.g1" src="extracted/5819071/data/teaser/teaser-1/att_pred_2_1.png" width="90"/></td>
</tr>
</table>
</div>
<figure class="ltx_figure" id="S0.F1">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
<span class="ltx_text ltx_font_bold" id="S0.F1.6.1">How do 3D human pose estimators perform in the presence of various personal or environmental attributes (<em class="ltx_emph ltx_font_italic" id="S0.F1.6.1.1">e.g</em>.<span class="ltx_text" id="S0.F1.6.1.2"></span>, gender or clothing)?</span>
Using text-to-image GenAI, our STAGE framework can answer such questions by dynamically generating custom benchmarks, targeting any user-specified attributes.
By generating image pairs (base image and attribute-changed image), we can measure the pose estimator’s performance gap, enabling customizable fine-grained analysis unsupported by existing benchmarks.</figcaption>
</figure>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1"><span class="ltx_text" id="S1.p1.1.1" style="font-size:90%;">3D human pose estimation (HPE) from monocular images has been a main area of research in computer vision for over 30 years, and we have witnessed tremendous progress in recent years.
HPE is a critical component of applications in collaborative robotics, systems that interact with or operate among humans, making it crucial to ensure its reliability.
However, especially for safety critical systems such as autonomous vehicles and robots, it is crucial that the deployed models are not only accurate on average but also </span><span class="ltx_text ltx_font_italic" id="S1.p1.1.2" style="font-size:90%;">robust towards elements naturally occurring in their operational domain</span><span class="ltx_text" id="S1.p1.1.3" style="font-size:90%;">.
A foremost precondition to ensuring robustness is the existence of rigorous and thorough evaluation protocols to stress-test these systems.
Controlled experiments are crucial for a systematic analysis, isolating the influence of various factors or </span><span class="ltx_text ltx_font_italic" id="S1.p1.1.4" style="font-size:90%;">attributes</span><span class="ltx_text" id="S1.p1.1.5" style="font-size:90%;"> one at a time, allowing more reliable conclusions about the causes of errors.
The need for such evaluations has also been recently recognized by legislators in proposals such as the EU AI Act </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p1.1.6.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">13</span></a><span class="ltx_text" id="S1.p1.1.7.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p1.1.8" style="font-size:90%;"> and the UL 4600 standard by the American National Standards Institute </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p1.1.9.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a><span class="ltx_text" id="S1.p1.1.10.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p1.1.11" style="font-size:90%;">.</span></p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1"><span class="ltx_text" id="S1.p2.1.1" style="font-size:90%;">Benchmarks consisting of real photographic data (Human3.6M </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p2.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">31</span></a><span class="ltx_text" id="S1.p2.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p2.1.4" style="font-size:90%;"> and 3DPW </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p2.1.5.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib74" title=""><span class="ltx_text" style="font-size:90%;">74</span></a><span class="ltx_text" id="S1.p2.1.6.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p2.1.7" style="font-size:90%;">) cannot provide such control – it is infeasible to capture a sufficient variety of people in the same exact pose while varying attributes. Furthermore, such benchmarks are laborious to capture, and attributes of interest vary depending on application.
Computer graphics generated synthetic benchmarks </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p2.1.8.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib72" title=""><span class="ltx_text" style="font-size:90%;">72</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib57" title=""><span class="ltx_text" style="font-size:90%;">57</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib9" title=""><span class="ltx_text" style="font-size:90%;">9</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">6</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib76" title=""><span class="ltx_text" style="font-size:90%;">76</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib4" title=""><span class="ltx_text" style="font-size:90%;">4</span></a><span class="ltx_text" id="S1.p2.1.9.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p2.1.10" style="font-size:90%;"> provide a high degree of control over pose, camera location, clothing, </span><em class="ltx_emph ltx_font_italic" id="S1.p2.1.11" style="font-size:90%;">etc</em><span class="ltx_text" id="S1.p2.1.12" style="font-size:90%;">.
However, to achieve photorealism and diversity, they require large amounts of high-quality 3D assets.
Clothing items have to be designed and physically simulated </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p2.1.13.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">6</span></a><span class="ltx_text" id="S1.p2.1.14.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p2.1.15" style="font-size:90%;">, and locations have to be modeled or captured.
In addition, effective use of modern graphics pipelines requires expertise that HPE practitioners may lack.
Overall, the current tools do not allow for a thorough auditing of HPE models as is required for their application in the open world.</span></p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1"><span class="ltx_text" id="S1.p3.1.1" style="font-size:90%;">We hence introduce STAGE, a new method to generate customizable HPE benchmarks, with which it becomes possible to measure the influence of individual attributes on HPE performance for the first time.
For this, we build on recent progress in large-scale text-to-image models </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p3.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib61" title=""><span class="ltx_text" style="font-size:90%;">61</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib63" title=""><span class="ltx_text" style="font-size:90%;">63</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib65" title=""><span class="ltx_text" style="font-size:90%;">65</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib80" title=""><span class="ltx_text" style="font-size:90%;">80</span></a><span class="ltx_text" id="S1.p3.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p3.1.4" style="font-size:90%;">, which have the potential to generate the required data quickly with an easy-to-use text-based user interface.</span></p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1"><span class="ltx_text" id="S1.p4.1.1" style="font-size:90%;">STAGE works as depicted in </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S0.F1" style="font-size:90%;" title="In Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a><span class="ltx_text" id="S1.p4.1.2" style="font-size:90%;">. Given a target attribute, </span><em class="ltx_emph ltx_font_italic" id="S1.p4.1.3" style="font-size:90%;">e.g</em><span class="ltx_text" id="S1.p4.1.4" style="font-size:90%;">.</span><span class="ltx_text" id="S1.p4.1.5"></span><span class="ltx_text" id="S1.p4.1.6" style="font-size:90%;"> a clothing item such as </span><span class="ltx_text ltx_inline-quote ltx_outerquote" id="S1.p4.1.7" style="font-size:90%;">“parka”</span><span class="ltx_text" id="S1.p4.1.8" style="font-size:90%;">, we generate two sets of images, one where people wear parka coats and one where they do not, with a strategy to keep the appearance of the images roughly equal in other aspects.
The performance difference between these two synthetic sets indicates the sensitivity of the pose estimator towards the attribute.
While image synthesis models introduce a real-to-synthetic distribution shift, including potential artifacts, this only has limited impact on the sensitivity score.
This is because we always compare between two sets of generated images, which differ only in the target attribute, and the sets of images are generated at scale for many different poses.
By enabling the generation of tailor-made benchmarks, STAGE allows researchers and practitioners to faithfully audit their models for their target application domains according to any attributes expressible in free-form text.</span></p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1"><span class="ltx_text" id="S1.p5.1.1" style="font-size:90%;">Creating a synthetic benchmark presents two main challenges: the generated images should 1) be sufficiently realistic and 2) contain cues coherent with the underlying 3D pose.
None of the current generation methods satisfies such requirements.
At best, recent methods allow control over 2D pose </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p5.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib83" title=""><span class="ltx_text" style="font-size:90%;">83</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib54" title=""><span class="ltx_text" style="font-size:90%;">54</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib33" title=""><span class="ltx_text" style="font-size:90%;">33</span></a><span class="ltx_text" id="S1.p5.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p5.1.4" style="font-size:90%;">, but this is insufficient, as several different 3D poses can project to the same 2D poses </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p5.1.5.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib70" title=""><span class="ltx_text" style="font-size:90%;">70</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib67" title=""><span class="ltx_text" style="font-size:90%;">67</span></a><span class="ltx_text" id="S1.p5.1.6.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p5.1.7" style="font-size:90%;">.
Other methods based on depth control </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p5.1.8.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib83" title=""><span class="ltx_text" style="font-size:90%;">83</span></a><span class="ltx_text" id="S1.p5.1.9.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p5.1.10" style="font-size:90%;"> result in a lack of diversity.
We therefore devise a new human image generator that is based on ControlNet </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p5.1.11.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib83" title=""><span class="ltx_text" style="font-size:90%;">83</span></a><span class="ltx_text" id="S1.p5.1.12.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p5.1.13" style="font-size:90%;"> but is conditioned on a 3D rendering of SMPL </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p5.1.14.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib49" title=""><span class="ltx_text" style="font-size:90%;">49</span></a><span class="ltx_text" id="S1.p5.1.15.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p5.1.16" style="font-size:90%;"> to control 3D pose and is trained with a carefully designed strategy on a combination of 3D pose datasets (providing accurate pose conditioning) and 2D datasets (providing more image diversity).
This model produces high-quality images that are also consistent with the conditioning 3D pose.
To demonstrate this, we generate a digital replica of the public 3DPW </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p5.1.17.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib74" title=""><span class="ltx_text" style="font-size:90%;">74</span></a><span class="ltx_text" id="S1.p5.1.18.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p5.1.19" style="font-size:90%;"> dataset and show that a SOTA pose estimator’s performance on the synthetic replica is comparable to its performance on the real 3DPW.</span></p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1"><span class="ltx_text" id="S1.p6.1.1" style="font-size:90%;">STAGE can shed new light on aspects of current SOTA model performance, which can not be evaluated in current benchmarks.
To demonstrate these capabilities, we use STAGE to extensively evaluate the sensitivity of popular SOTA HPE methods </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p6.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib69" title=""><span class="ltx_text" style="font-size:90%;">69</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">6</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">10</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib82" title=""><span class="ltx_text" style="font-size:90%;">82</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">38</span></a><span class="ltx_text" id="S1.p6.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p6.1.4" style="font-size:90%;"> to attribute categories such as clothing, location, weather, and protected attributes (gender, age, etc.), and make several interesting findings.
For example, the best-performing methods, on average, are not always the most robust to attributes. In summary, our contributions are threefold.</span></p>
</div>
<div class="ltx_para" id="S1.p7">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text" id="S1.I1.i1.p1.1.1" style="font-size:90%;">We propose STAGE, a customizable GenAI benchmark generator, along with a systematic evaluation protocol that allows, for the first time, to conduct controlled experiments to audit 3D pose estimators.</span></p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text" id="S1.I1.i2.p1.1.1" style="font-size:90%;">Using STAGE, we empirically evaluate popular pose estimators on their sensitivity against attributes such as gender, age, and clothing, making findings that were not possible before.</span></p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text" id="S1.I1.i3.p1.1.1" style="font-size:90%;">We build a text-to-image generative model that is able to create images that are coherent in terms of 3D pose.</span></p>
</div>
</li>
</ul>
<p class="ltx_p" id="S1.p7.1"><span class="ltx_text" id="S1.p7.1.1" style="font-size:90%;">We will release our code and data for future research.</span></p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:90%;">3D human pose estimation benchmarks</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1"><span class="ltx_text" id="S2.SS0.SSS0.Px1.p1.1.1" style="font-size:90%;">High-quality benchmarks have served as a major driving force behind progress in HPE.
Real-world benchmarks </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS0.SSS0.Px1.p1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib85" title=""><span class="ltx_text" style="font-size:90%;">85</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib74" title=""><span class="ltx_text" style="font-size:90%;">74</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">34</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib84" title=""><span class="ltx_text" style="font-size:90%;">84</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">31</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">30</span></a><span class="ltx_text" id="S2.SS0.SSS0.Px1.p1.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS0.SSS0.Px1.p1.1.4" style="font-size:90%;"> are desirable but expensive to create at scale.
They are either restricted to a studio </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS0.SSS0.Px1.p1.1.5.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib85" title=""><span class="ltx_text" style="font-size:90%;">85</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">31</span></a><span class="ltx_text" id="S2.SS0.SSS0.Px1.p1.1.6.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS0.SSS0.Px1.p1.1.7" style="font-size:90%;"> or have a limited number of subjects </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS0.SSS0.Px1.p1.1.8.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib74" title=""><span class="ltx_text" style="font-size:90%;">74</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">34</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">30</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib84" title=""><span class="ltx_text" style="font-size:90%;">84</span></a><span class="ltx_text" id="S2.SS0.SSS0.Px1.p1.1.9.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS0.SSS0.Px1.p1.1.10" style="font-size:90%;">.
High variation of clothing, locations, gender, and ethnicity has not been possible to achieve with real data.
Therefore, simulated benchmarks </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS0.SSS0.Px1.p1.1.11.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib57" title=""><span class="ltx_text" style="font-size:90%;">57</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib9" title=""><span class="ltx_text" style="font-size:90%;">9</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">6</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib76" title=""><span class="ltx_text" style="font-size:90%;">76</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib72" title=""><span class="ltx_text" style="font-size:90%;">72</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib4" title=""><span class="ltx_text" style="font-size:90%;">4</span></a><span class="ltx_text" id="S2.SS0.SSS0.Px1.p1.1.12.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS0.SSS0.Px1.p1.1.13" style="font-size:90%;"> have risen in popularity, allowing the generation of accurately annotated data at scale.
However, they require laborious asset design (clothes, 3D environments) as well as expertise to operate complex graphics, animation, and physics simulation pipelines.
In practice, therefore even computer-graphics-based synthetic benchmarks are limited regarding the factors that can impact performance.
In contrast, we argue that the </span><em class="ltx_emph ltx_font_italic" id="S2.SS0.SSS0.Px1.p1.1.14" style="font-size:90%;">text input</em><span class="ltx_text" id="S2.SS0.SSS0.Px1.p1.1.15" style="font-size:90%;"> of text-to-image models is a more flexible and easy-to-use interface for custom benchmark generation and control of attributes like clothing, locations, </span><em class="ltx_emph ltx_font_italic" id="S2.SS0.SSS0.Px1.p1.1.16" style="font-size:90%;">etc</em><span class="ltx_text" id="S2.SS0.SSS0.Px1.p1.1.17" style="font-size:90%;">.</span></p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:90%;">Controllable human image generation</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1"><span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.1" style="font-size:90%;">methods can be grouped into control given 2D pose </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib83" title=""><span class="ltx_text" style="font-size:90%;">83</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib54" title=""><span class="ltx_text" style="font-size:90%;">54</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib33" title=""><span class="ltx_text" style="font-size:90%;">33</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib48" title=""><span class="ltx_text" style="font-size:90%;">48</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">8</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib56" title=""><span class="ltx_text" style="font-size:90%;">56</span></a><span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.4" style="font-size:90%;"> and control given 3D pose </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.5.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib77" title=""><span class="ltx_text" style="font-size:90%;">77</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">18</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib27" title=""><span class="ltx_text" style="font-size:90%;">27</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib79" title=""><span class="ltx_text" style="font-size:90%;">79</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">11</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">39</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">28</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib55" title=""><span class="ltx_text" style="font-size:90%;">55</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib45" title=""><span class="ltx_text" style="font-size:90%;">45</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib35" title=""><span class="ltx_text" style="font-size:90%;">35</span></a><span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.6.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.7" style="font-size:90%;">.
2D pose conditioned methods such as ControlNet </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.8.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib83" title=""><span class="ltx_text" style="font-size:90%;">83</span></a><span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.9.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.10" style="font-size:90%;">, T2I </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.11.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib54" title=""><span class="ltx_text" style="font-size:90%;">54</span></a><span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.12.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.13" style="font-size:90%;">, HumanSD </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.14.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib33" title=""><span class="ltx_text" style="font-size:90%;">33</span></a><span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.15.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.16" style="font-size:90%;"> and HyperHuman </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.17.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib48" title=""><span class="ltx_text" style="font-size:90%;">48</span></a><span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.18.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.19" style="font-size:90%;"> extend StableDiffusion (SD) </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.20.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib63" title=""><span class="ltx_text" style="font-size:90%;">63</span></a><span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.21.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.22" style="font-size:90%;"> with control via 2D human keypoints.
They all exhibit good text control but do not allow precise control of human pose in 3D.
While ControlNet is able to combine multiple control signals (such as depth and 2D pose) in a training-free procedure, the resulting model achieves only limited 3D controllability, as we will show in </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S4.SS3" style="font-size:90%;" title="4.3 Quality Assessment of STAGE Data ‣ 4 Experiments ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.3</span></a><span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.23" style="font-size:90%;">.
For 3D pose control methods, we can broadly speak of GAN-based and Stable Diffusion-based methods.
GAN-based methods </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.24.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib55" title=""><span class="ltx_text" style="font-size:90%;">55</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib77" title=""><span class="ltx_text" style="font-size:90%;">77</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">28</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">18</span></a><span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.25.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.26" style="font-size:90%;"> learn to generate 3D humans from 2D single-view image collections.
Through techniques such as inverse skinning, they achieve excellent pose control.
However, they generate people without backgrounds and do not offer text control.
Methods based on SD </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.27.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib58" title=""><span class="ltx_text" style="font-size:90%;">58</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">39</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">11</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib45" title=""><span class="ltx_text" style="font-size:90%;">45</span></a><span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.28.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.29" style="font-size:90%;"> or CLIP </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.30.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">60</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib79" title=""><span class="ltx_text" style="font-size:90%;">79</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib27" title=""><span class="ltx_text" style="font-size:90%;">27</span></a><span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.31.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.32" style="font-size:90%;"> offer textual control.
However, for the task of benchmark creation, the image quality is often insufficient, and the generation process can take hours.</span></p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:90%;">Auditing with generative models</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p1.1"><span class="ltx_text" id="S2.SS0.SSS0.Px3.p1.1.1" style="font-size:90%;">The utility of generative models for benchmark creation and model auditing has been recognized before for the tasks of object classification and detection </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS0.SSS0.Px3.p1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">20</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib53" title=""><span class="ltx_text" style="font-size:90%;">53</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib75" title=""><span class="ltx_text" style="font-size:90%;">75</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">23</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib42" title=""><span class="ltx_text" style="font-size:90%;">42</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">1</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib50" title=""><span class="ltx_text" style="font-size:90%;">50</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib59" title=""><span class="ltx_text" style="font-size:90%;">59</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib73" title=""><span class="ltx_text" style="font-size:90%;">73</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib44" title=""><span class="ltx_text" style="font-size:90%;">44</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib64" title=""><span class="ltx_text" style="font-size:90%;">64</span></a><span class="ltx_text" id="S2.SS0.SSS0.Px3.p1.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS0.SSS0.Px3.p1.1.4" style="font-size:90%;">.
Some of these methods search for a semantically coherent sub-space of images that result in failure </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS0.SSS0.Px3.p1.1.5.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">20</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib53" title=""><span class="ltx_text" style="font-size:90%;">53</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib75" title=""><span class="ltx_text" style="font-size:90%;">75</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">23</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a><span class="ltx_text" id="S2.SS0.SSS0.Px3.p1.1.6.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS0.SSS0.Px3.p1.1.7" style="font-size:90%;">, while others create counterfactual images to measure classifier robustness </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS0.SSS0.Px3.p1.1.8.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">1</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib50" title=""><span class="ltx_text" style="font-size:90%;">50</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib59" title=""><span class="ltx_text" style="font-size:90%;">59</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib73" title=""><span class="ltx_text" style="font-size:90%;">73</span></a><span class="ltx_text" id="S2.SS0.SSS0.Px3.p1.1.9.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS0.SSS0.Px3.p1.1.10" style="font-size:90%;">.
ImageNet-E </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS0.SSS0.Px3.p1.1.11.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib42" title=""><span class="ltx_text" style="font-size:90%;">42</span></a><span class="ltx_text" id="S2.SS0.SSS0.Px3.p1.1.12.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS0.SSS0.Px3.p1.1.13" style="font-size:90%;"> is a synthetic benchmark to evaluate attributes such as size, pose, and background.
These methods typically rely on the availability of diverse benchmarks such as ImageNet </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS0.SSS0.Px3.p1.1.14.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">16</span></a><span class="ltx_text" id="S2.SS0.SSS0.Px3.p1.1.15.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS0.SSS0.Px3.p1.1.16" style="font-size:90%;"> or on large held-out test datasets, which are not available for HPE.
Additionally, unlike these classification and detection works, we are targeting HPE, a more fine-grained, high-dimensional regression problem requiring a more elaborate approach.</span></p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="351" id="S3.F2.g1" src="x1.png" width="951"/>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span class="ltx_text ltx_font_bold" id="S3.F2.14.1">Evaluation of estimators with STAGE</span>
We start by generating a pair of human images given text prompts
<math alttext="t^{\text{base}}" class="ltx_Math" display="inline" id="S3.F2.4.m1.1"><semantics id="S3.F2.4.m1.1b"><msup id="S3.F2.4.m1.1.1" xref="S3.F2.4.m1.1.1.cmml"><mi id="S3.F2.4.m1.1.1.2" xref="S3.F2.4.m1.1.1.2.cmml">t</mi><mtext id="S3.F2.4.m1.1.1.3" xref="S3.F2.4.m1.1.1.3a.cmml">base</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.F2.4.m1.1c"><apply id="S3.F2.4.m1.1.1.cmml" xref="S3.F2.4.m1.1.1"><csymbol cd="ambiguous" id="S3.F2.4.m1.1.1.1.cmml" xref="S3.F2.4.m1.1.1">superscript</csymbol><ci id="S3.F2.4.m1.1.1.2.cmml" xref="S3.F2.4.m1.1.1.2">𝑡</ci><ci id="S3.F2.4.m1.1.1.3a.cmml" xref="S3.F2.4.m1.1.1.3"><mtext id="S3.F2.4.m1.1.1.3.cmml" mathsize="70%" xref="S3.F2.4.m1.1.1.3">base</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.4.m1.1d">t^{\text{base}}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.4.m1.1e">italic_t start_POSTSUPERSCRIPT base end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="t^{\text{att}}" class="ltx_Math" display="inline" id="S3.F2.5.m2.1"><semantics id="S3.F2.5.m2.1b"><msup id="S3.F2.5.m2.1.1" xref="S3.F2.5.m2.1.1.cmml"><mi id="S3.F2.5.m2.1.1.2" xref="S3.F2.5.m2.1.1.2.cmml">t</mi><mtext id="S3.F2.5.m2.1.1.3" xref="S3.F2.5.m2.1.1.3a.cmml">att</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.F2.5.m2.1c"><apply id="S3.F2.5.m2.1.1.cmml" xref="S3.F2.5.m2.1.1"><csymbol cd="ambiguous" id="S3.F2.5.m2.1.1.1.cmml" xref="S3.F2.5.m2.1.1">superscript</csymbol><ci id="S3.F2.5.m2.1.1.2.cmml" xref="S3.F2.5.m2.1.1.2">𝑡</ci><ci id="S3.F2.5.m2.1.1.3a.cmml" xref="S3.F2.5.m2.1.1.3"><mtext id="S3.F2.5.m2.1.1.3.cmml" mathsize="70%" xref="S3.F2.5.m2.1.1.3">att</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.5.m2.1d">t^{\text{att}}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.5.m2.1e">italic_t start_POSTSUPERSCRIPT att end_POSTSUPERSCRIPT</annotation></semantics></math> and a desired 3D pose <math alttext="\mathbf{P}" class="ltx_Math" display="inline" id="S3.F2.6.m3.1"><semantics id="S3.F2.6.m3.1b"><mi id="S3.F2.6.m3.1.1" xref="S3.F2.6.m3.1.1.cmml">𝐏</mi><annotation-xml encoding="MathML-Content" id="S3.F2.6.m3.1c"><ci id="S3.F2.6.m3.1.1.cmml" xref="S3.F2.6.m3.1.1">𝐏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.6.m3.1d">\mathbf{P}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.6.m3.1e">bold_P</annotation></semantics></math> as a SMPL mesh.
We render the posed SMPL mesh as a depth map, a dense semantic encoding and a 2D skeleton.
These are input to our CN-3DPose to achieve 3D body pose control, while the text prompt control the overall appearance including the injected attributes to be tested.
We use the same initial noise to generate both images to preserve visual similarity between the images.
The result is an image pair that differ only by our attribute of interest.
We apply the pose estimator to be tested on the pair, compute the prediction error and further process it to estimate the sensitivity of the pose estimator towards the attribute.
</figcaption>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1"><span class="ltx_text" id="S3.p1.1.1" style="font-size:90%;">We introduce a synthetic data generation toolkit named STAGE, which enables the creation of custom benchmarks for auditing HPE.
We begin with a problem formalization in </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3.SS1" style="font-size:90%;" title="3.1 Evaluation with Controlled Generated Test Sets ‣ 3 Method ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.1</span></a><span class="ltx_text" id="S3.p1.1.2" style="font-size:90%;"> and introduce our 3D pose conditioned image synthesis method in </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3.SS2" style="font-size:90%;" title="3.2 Pose-Conditioned Image Generation ‣ 3 Method ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.2</span></a><span class="ltx_text" id="S3.p1.1.3" style="font-size:90%;">.
In </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3.SS3" style="font-size:90%;" title="3.3 Dataset Generation ‣ 3 Method ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.3</span></a><span class="ltx_text" id="S3.p1.1.4" style="font-size:90%;">, we further describe the dataset generation pipeline, and in </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3.SS4" style="font-size:90%;" title="3.4 Evaluation Protocol ‣ 3 Method ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.4</span></a><span class="ltx_text" id="S3.p1.1.5" style="font-size:90%;">, we introduce the evaluation protocol to assess the sensitivity of pose estimators towards certain attributes.</span></p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Evaluation with Controlled Generated Test Sets</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.8"><span class="ltx_text" id="S3.SS1.p1.8.1" style="font-size:90%;">A 3D pose estimator is a function </span><math alttext="f:\mathbf{x}\rightarrow\hat{\mathbf{P}}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" mathsize="90%" xref="S3.SS1.p1.1.m1.1.1.2.cmml">f</mi><mo id="S3.SS1.p1.1.m1.1.1.1" lspace="0.278em" mathsize="90%" rspace="0.278em" xref="S3.SS1.p1.1.m1.1.1.1.cmml">:</mo><mrow id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml"><mi id="S3.SS1.p1.1.m1.1.1.3.2" mathsize="90%" xref="S3.SS1.p1.1.m1.1.1.3.2.cmml">𝐱</mi><mo id="S3.SS1.p1.1.m1.1.1.3.1" mathsize="90%" stretchy="false" xref="S3.SS1.p1.1.m1.1.1.3.1.cmml">→</mo><mover accent="true" id="S3.SS1.p1.1.m1.1.1.3.3" xref="S3.SS1.p1.1.m1.1.1.3.3.cmml"><mi id="S3.SS1.p1.1.m1.1.1.3.3.2" mathsize="90%" xref="S3.SS1.p1.1.m1.1.1.3.3.2.cmml">𝐏</mi><mo id="S3.SS1.p1.1.m1.1.1.3.3.1" mathsize="90%" xref="S3.SS1.p1.1.m1.1.1.3.3.1.cmml">^</mo></mover></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><ci id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1">:</ci><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">𝑓</ci><apply id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3"><ci id="S3.SS1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS1.p1.1.m1.1.1.3.1">→</ci><ci id="S3.SS1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS1.p1.1.m1.1.1.3.2">𝐱</ci><apply id="S3.SS1.p1.1.m1.1.1.3.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3"><ci id="S3.SS1.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.1">^</ci><ci id="S3.SS1.p1.1.m1.1.1.3.3.2.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.2">𝐏</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">f:\mathbf{x}\rightarrow\hat{\mathbf{P}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">italic_f : bold_x → over^ start_ARG bold_P end_ARG</annotation></semantics></math><span class="ltx_text" id="S3.SS1.p1.8.2" style="font-size:90%;"> that maps an image </span><math alttext="\mathbf{x}\in\mathbb{R}^{H\times W\times 3}" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><mrow id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2" mathsize="90%" xref="S3.SS1.p1.2.m2.1.1.2.cmml">𝐱</mi><mo id="S3.SS1.p1.2.m2.1.1.1" mathsize="90%" xref="S3.SS1.p1.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml"><mi id="S3.SS1.p1.2.m2.1.1.3.2" mathsize="90%" xref="S3.SS1.p1.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p1.2.m2.1.1.3.3" xref="S3.SS1.p1.2.m2.1.1.3.3.cmml"><mi id="S3.SS1.p1.2.m2.1.1.3.3.2" mathsize="90%" xref="S3.SS1.p1.2.m2.1.1.3.3.2.cmml">H</mi><mo id="S3.SS1.p1.2.m2.1.1.3.3.1" lspace="0.222em" mathsize="90%" rspace="0.222em" xref="S3.SS1.p1.2.m2.1.1.3.3.1.cmml">×</mo><mi id="S3.SS1.p1.2.m2.1.1.3.3.3" mathsize="90%" xref="S3.SS1.p1.2.m2.1.1.3.3.3.cmml">W</mi><mo id="S3.SS1.p1.2.m2.1.1.3.3.1a" lspace="0.222em" mathsize="90%" rspace="0.222em" xref="S3.SS1.p1.2.m2.1.1.3.3.1.cmml">×</mo><mn id="S3.SS1.p1.2.m2.1.1.3.3.4" mathsize="90%" xref="S3.SS1.p1.2.m2.1.1.3.3.4.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><in id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1"></in><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">𝐱</ci><apply id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.3.1.cmml" xref="S3.SS1.p1.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.3.2.cmml" xref="S3.SS1.p1.2.m2.1.1.3.2">ℝ</ci><apply id="S3.SS1.p1.2.m2.1.1.3.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3.3"><times id="S3.SS1.p1.2.m2.1.1.3.3.1.cmml" xref="S3.SS1.p1.2.m2.1.1.3.3.1"></times><ci id="S3.SS1.p1.2.m2.1.1.3.3.2.cmml" xref="S3.SS1.p1.2.m2.1.1.3.3.2">𝐻</ci><ci id="S3.SS1.p1.2.m2.1.1.3.3.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3.3.3">𝑊</ci><cn id="S3.SS1.p1.2.m2.1.1.3.3.4.cmml" type="integer" xref="S3.SS1.p1.2.m2.1.1.3.3.4">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\mathbf{x}\in\mathbb{R}^{H\times W\times 3}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">bold_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_H × italic_W × 3 end_POSTSUPERSCRIPT</annotation></semantics></math><span class="ltx_text" id="S3.SS1.p1.8.3" style="font-size:90%;"> depicting a person to their corresponding 3D body pose </span><math alttext="\hat{\mathbf{P}}\in\mathbb{R}^{J\times 3}" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><mrow id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mover accent="true" id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2.2" mathsize="90%" xref="S3.SS1.p1.3.m3.1.1.2.2.cmml">𝐏</mi><mo id="S3.SS1.p1.3.m3.1.1.2.1" mathsize="90%" xref="S3.SS1.p1.3.m3.1.1.2.1.cmml">^</mo></mover><mo id="S3.SS1.p1.3.m3.1.1.1" mathsize="90%" xref="S3.SS1.p1.3.m3.1.1.1.cmml">∈</mo><msup id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml"><mi id="S3.SS1.p1.3.m3.1.1.3.2" mathsize="90%" xref="S3.SS1.p1.3.m3.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p1.3.m3.1.1.3.3" xref="S3.SS1.p1.3.m3.1.1.3.3.cmml"><mi id="S3.SS1.p1.3.m3.1.1.3.3.2" mathsize="90%" xref="S3.SS1.p1.3.m3.1.1.3.3.2.cmml">J</mi><mo id="S3.SS1.p1.3.m3.1.1.3.3.1" lspace="0.222em" mathsize="90%" rspace="0.222em" xref="S3.SS1.p1.3.m3.1.1.3.3.1.cmml">×</mo><mn id="S3.SS1.p1.3.m3.1.1.3.3.3" mathsize="90%" xref="S3.SS1.p1.3.m3.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><in id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1"></in><apply id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2"><ci id="S3.SS1.p1.3.m3.1.1.2.1.cmml" xref="S3.SS1.p1.3.m3.1.1.2.1">^</ci><ci id="S3.SS1.p1.3.m3.1.1.2.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2.2">𝐏</ci></apply><apply id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.3.1.cmml" xref="S3.SS1.p1.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.3.2.cmml" xref="S3.SS1.p1.3.m3.1.1.3.2">ℝ</ci><apply id="S3.SS1.p1.3.m3.1.1.3.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3.3"><times id="S3.SS1.p1.3.m3.1.1.3.3.1.cmml" xref="S3.SS1.p1.3.m3.1.1.3.3.1"></times><ci id="S3.SS1.p1.3.m3.1.1.3.3.2.cmml" xref="S3.SS1.p1.3.m3.1.1.3.3.2">𝐽</ci><cn id="S3.SS1.p1.3.m3.1.1.3.3.3.cmml" type="integer" xref="S3.SS1.p1.3.m3.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">\hat{\mathbf{P}}\in\mathbb{R}^{J\times 3}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">over^ start_ARG bold_P end_ARG ∈ blackboard_R start_POSTSUPERSCRIPT italic_J × 3 end_POSTSUPERSCRIPT</annotation></semantics></math><span class="ltx_text" id="S3.SS1.p1.8.4" style="font-size:90%;">, represented as </span><math alttext="J" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4.1"><semantics id="S3.SS1.p1.4.m4.1a"><mi id="S3.SS1.p1.4.m4.1.1" mathsize="90%" xref="S3.SS1.p1.4.m4.1.1.cmml">J</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">𝐽</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">J</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.1d">italic_J</annotation></semantics></math><span class="ltx_text" id="S3.SS1.p1.8.5" style="font-size:90%;"> 3D joint locations.
Typically, pose estimators are deployed in some operational domain where image </span><math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m5.1"><semantics id="S3.SS1.p1.5.m5.1a"><mi id="S3.SS1.p1.5.m5.1.1" mathsize="90%" xref="S3.SS1.p1.5.m5.1.1.cmml">𝐱</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><ci id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">𝐱</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.5.m5.1d">bold_x</annotation></semantics></math><span class="ltx_text" id="S3.SS1.p1.8.6" style="font-size:90%;"> and ground truth pose </span><math alttext="\mathbf{P}" class="ltx_Math" display="inline" id="S3.SS1.p1.6.m6.1"><semantics id="S3.SS1.p1.6.m6.1a"><mi id="S3.SS1.p1.6.m6.1.1" mathsize="90%" xref="S3.SS1.p1.6.m6.1.1.cmml">𝐏</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><ci id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">𝐏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">\mathbf{P}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.6.m6.1d">bold_P</annotation></semantics></math><span class="ltx_text" id="S3.SS1.p1.8.7" style="font-size:90%;"> follow a distribution </span><math alttext="p(\mathbf{x},\mathbf{P})" class="ltx_Math" display="inline" id="S3.SS1.p1.7.m7.2"><semantics id="S3.SS1.p1.7.m7.2a"><mrow id="S3.SS1.p1.7.m7.2.3" xref="S3.SS1.p1.7.m7.2.3.cmml"><mi id="S3.SS1.p1.7.m7.2.3.2" mathsize="90%" xref="S3.SS1.p1.7.m7.2.3.2.cmml">p</mi><mo id="S3.SS1.p1.7.m7.2.3.1" xref="S3.SS1.p1.7.m7.2.3.1.cmml">⁢</mo><mrow id="S3.SS1.p1.7.m7.2.3.3.2" xref="S3.SS1.p1.7.m7.2.3.3.1.cmml"><mo id="S3.SS1.p1.7.m7.2.3.3.2.1" maxsize="90%" minsize="90%" xref="S3.SS1.p1.7.m7.2.3.3.1.cmml">(</mo><mi id="S3.SS1.p1.7.m7.1.1" mathsize="90%" xref="S3.SS1.p1.7.m7.1.1.cmml">𝐱</mi><mo id="S3.SS1.p1.7.m7.2.3.3.2.2" mathsize="90%" xref="S3.SS1.p1.7.m7.2.3.3.1.cmml">,</mo><mi id="S3.SS1.p1.7.m7.2.2" mathsize="90%" xref="S3.SS1.p1.7.m7.2.2.cmml">𝐏</mi><mo id="S3.SS1.p1.7.m7.2.3.3.2.3" maxsize="90%" minsize="90%" xref="S3.SS1.p1.7.m7.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.2b"><apply id="S3.SS1.p1.7.m7.2.3.cmml" xref="S3.SS1.p1.7.m7.2.3"><times id="S3.SS1.p1.7.m7.2.3.1.cmml" xref="S3.SS1.p1.7.m7.2.3.1"></times><ci id="S3.SS1.p1.7.m7.2.3.2.cmml" xref="S3.SS1.p1.7.m7.2.3.2">𝑝</ci><interval closure="open" id="S3.SS1.p1.7.m7.2.3.3.1.cmml" xref="S3.SS1.p1.7.m7.2.3.3.2"><ci id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">𝐱</ci><ci id="S3.SS1.p1.7.m7.2.2.cmml" xref="S3.SS1.p1.7.m7.2.2">𝐏</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.2c">p(\mathbf{x},\mathbf{P})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.7.m7.2d">italic_p ( bold_x , bold_P )</annotation></semantics></math><span class="ltx_text" id="S3.SS1.p1.8.8" style="font-size:90%;"> and are evaluated with a metric </span><math alttext="L" class="ltx_Math" display="inline" id="S3.SS1.p1.8.m8.1"><semantics id="S3.SS1.p1.8.m8.1a"><mi id="S3.SS1.p1.8.m8.1.1" mathsize="90%" xref="S3.SS1.p1.8.m8.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.1b"><ci id="S3.SS1.p1.8.m8.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">L</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.8.m8.1d">italic_L</annotation></semantics></math><span class="ltx_text" id="S3.SS1.p1.8.9" style="font-size:90%;"> by computing the risk</span></p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="R_{f}=\mathbb{E}_{p(\mathbf{x},\mathbf{P})}\left[L(f(\mathbf{x}),\mathbf{P})%
\right]." class="ltx_Math" display="block" id="S3.E1.m1.5"><semantics id="S3.E1.m1.5a"><mrow id="S3.E1.m1.5.5.1" xref="S3.E1.m1.5.5.1.1.cmml"><mrow id="S3.E1.m1.5.5.1.1" xref="S3.E1.m1.5.5.1.1.cmml"><msub id="S3.E1.m1.5.5.1.1.3" xref="S3.E1.m1.5.5.1.1.3.cmml"><mi id="S3.E1.m1.5.5.1.1.3.2" mathsize="90%" xref="S3.E1.m1.5.5.1.1.3.2.cmml">R</mi><mi id="S3.E1.m1.5.5.1.1.3.3" mathsize="90%" xref="S3.E1.m1.5.5.1.1.3.3.cmml">f</mi></msub><mo id="S3.E1.m1.5.5.1.1.2" mathsize="90%" xref="S3.E1.m1.5.5.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.5.5.1.1.1" xref="S3.E1.m1.5.5.1.1.1.cmml"><msub id="S3.E1.m1.5.5.1.1.1.3" xref="S3.E1.m1.5.5.1.1.1.3.cmml"><mi id="S3.E1.m1.5.5.1.1.1.3.2" mathsize="90%" xref="S3.E1.m1.5.5.1.1.1.3.2.cmml">𝔼</mi><mrow id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml"><mi id="S3.E1.m1.2.2.2.4" mathsize="90%" xref="S3.E1.m1.2.2.2.4.cmml">p</mi><mo id="S3.E1.m1.2.2.2.3" xref="S3.E1.m1.2.2.2.3.cmml">⁢</mo><mrow id="S3.E1.m1.2.2.2.5.2" xref="S3.E1.m1.2.2.2.5.1.cmml"><mo id="S3.E1.m1.2.2.2.5.2.1" maxsize="90%" minsize="90%" xref="S3.E1.m1.2.2.2.5.1.cmml">(</mo><mi id="S3.E1.m1.1.1.1.1" mathsize="90%" xref="S3.E1.m1.1.1.1.1.cmml">𝐱</mi><mo id="S3.E1.m1.2.2.2.5.2.2" mathsize="90%" xref="S3.E1.m1.2.2.2.5.1.cmml">,</mo><mi id="S3.E1.m1.2.2.2.2" mathsize="90%" xref="S3.E1.m1.2.2.2.2.cmml">𝐏</mi><mo id="S3.E1.m1.2.2.2.5.2.3" maxsize="90%" minsize="90%" xref="S3.E1.m1.2.2.2.5.1.cmml">)</mo></mrow></mrow></msub><mo id="S3.E1.m1.5.5.1.1.1.2" xref="S3.E1.m1.5.5.1.1.1.2.cmml">⁢</mo><mrow id="S3.E1.m1.5.5.1.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.2.cmml"><mo id="S3.E1.m1.5.5.1.1.1.1.1.2" xref="S3.E1.m1.5.5.1.1.1.1.2.1.cmml">[</mo><mrow id="S3.E1.m1.5.5.1.1.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.5.5.1.1.1.1.1.1.3" mathsize="90%" xref="S3.E1.m1.5.5.1.1.1.1.1.1.3.cmml">L</mi><mo id="S3.E1.m1.5.5.1.1.1.1.1.1.2" xref="S3.E1.m1.5.5.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.2.cmml"><mo id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2" maxsize="90%" minsize="90%" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.2.cmml">(</mo><mrow id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2" mathsize="90%" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.cmml">f</mi><mo id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mrow id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.3.2.1" maxsize="90%" minsize="90%" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E1.m1.3.3" mathsize="90%" xref="S3.E1.m1.3.3.cmml">𝐱</mi><mo id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.3.2.2" maxsize="90%" minsize="90%" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.3" mathsize="90%" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.2.cmml">,</mo><mi id="S3.E1.m1.4.4" mathsize="90%" xref="S3.E1.m1.4.4.cmml">𝐏</mi><mo id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.4" maxsize="90%" minsize="90%" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.5.5.1.1.1.1.1.3" xref="S3.E1.m1.5.5.1.1.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><mo id="S3.E1.m1.5.5.1.2" lspace="0em" mathsize="90%" xref="S3.E1.m1.5.5.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.5b"><apply id="S3.E1.m1.5.5.1.1.cmml" xref="S3.E1.m1.5.5.1"><eq id="S3.E1.m1.5.5.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.2"></eq><apply id="S3.E1.m1.5.5.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.3.1.cmml" xref="S3.E1.m1.5.5.1.1.3">subscript</csymbol><ci id="S3.E1.m1.5.5.1.1.3.2.cmml" xref="S3.E1.m1.5.5.1.1.3.2">𝑅</ci><ci id="S3.E1.m1.5.5.1.1.3.3.cmml" xref="S3.E1.m1.5.5.1.1.3.3">𝑓</ci></apply><apply id="S3.E1.m1.5.5.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1"><times id="S3.E1.m1.5.5.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.2"></times><apply id="S3.E1.m1.5.5.1.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.1.3.1.cmml" xref="S3.E1.m1.5.5.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.5.5.1.1.1.3.2.cmml" xref="S3.E1.m1.5.5.1.1.1.3.2">𝔼</ci><apply id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2"><times id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.3"></times><ci id="S3.E1.m1.2.2.2.4.cmml" xref="S3.E1.m1.2.2.2.4">𝑝</ci><interval closure="open" id="S3.E1.m1.2.2.2.5.1.cmml" xref="S3.E1.m1.2.2.2.5.2"><ci id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">𝐱</ci><ci id="S3.E1.m1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2">𝐏</ci></interval></apply></apply><apply id="S3.E1.m1.5.5.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.5.5.1.1.1.1.2.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.E1.m1.5.5.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1"><times id="S3.E1.m1.5.5.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.2"></times><ci id="S3.E1.m1.5.5.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.3">𝐿</ci><interval closure="open" id="S3.E1.m1.5.5.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1"><apply id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1"><times id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.1"></times><ci id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2">𝑓</ci><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">𝐱</ci></apply><ci id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4">𝐏</ci></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.5c">R_{f}=\mathbb{E}_{p(\mathbf{x},\mathbf{P})}\left[L(f(\mathbf{x}),\mathbf{P})%
\right].</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.5d">italic_R start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT = blackboard_E start_POSTSUBSCRIPT italic_p ( bold_x , bold_P ) end_POSTSUBSCRIPT [ italic_L ( italic_f ( bold_x ) , bold_P ) ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p1.12"><span class="ltx_text" id="S3.SS1.p1.12.1" style="font-size:90%;">Sampling from </span><math alttext="p(\mathbf{x},\mathbf{P})" class="ltx_Math" display="inline" id="S3.SS1.p1.9.m1.2"><semantics id="S3.SS1.p1.9.m1.2a"><mrow id="S3.SS1.p1.9.m1.2.3" xref="S3.SS1.p1.9.m1.2.3.cmml"><mi id="S3.SS1.p1.9.m1.2.3.2" mathsize="90%" xref="S3.SS1.p1.9.m1.2.3.2.cmml">p</mi><mo id="S3.SS1.p1.9.m1.2.3.1" xref="S3.SS1.p1.9.m1.2.3.1.cmml">⁢</mo><mrow id="S3.SS1.p1.9.m1.2.3.3.2" xref="S3.SS1.p1.9.m1.2.3.3.1.cmml"><mo id="S3.SS1.p1.9.m1.2.3.3.2.1" maxsize="90%" minsize="90%" xref="S3.SS1.p1.9.m1.2.3.3.1.cmml">(</mo><mi id="S3.SS1.p1.9.m1.1.1" mathsize="90%" xref="S3.SS1.p1.9.m1.1.1.cmml">𝐱</mi><mo id="S3.SS1.p1.9.m1.2.3.3.2.2" mathsize="90%" xref="S3.SS1.p1.9.m1.2.3.3.1.cmml">,</mo><mi id="S3.SS1.p1.9.m1.2.2" mathsize="90%" xref="S3.SS1.p1.9.m1.2.2.cmml">𝐏</mi><mo id="S3.SS1.p1.9.m1.2.3.3.2.3" maxsize="90%" minsize="90%" xref="S3.SS1.p1.9.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m1.2b"><apply id="S3.SS1.p1.9.m1.2.3.cmml" xref="S3.SS1.p1.9.m1.2.3"><times id="S3.SS1.p1.9.m1.2.3.1.cmml" xref="S3.SS1.p1.9.m1.2.3.1"></times><ci id="S3.SS1.p1.9.m1.2.3.2.cmml" xref="S3.SS1.p1.9.m1.2.3.2">𝑝</ci><interval closure="open" id="S3.SS1.p1.9.m1.2.3.3.1.cmml" xref="S3.SS1.p1.9.m1.2.3.3.2"><ci id="S3.SS1.p1.9.m1.1.1.cmml" xref="S3.SS1.p1.9.m1.1.1">𝐱</ci><ci id="S3.SS1.p1.9.m1.2.2.cmml" xref="S3.SS1.p1.9.m1.2.2">𝐏</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m1.2c">p(\mathbf{x},\mathbf{P})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.9.m1.2d">italic_p ( bold_x , bold_P )</annotation></semantics></math><span class="ltx_text" id="S3.SS1.p1.12.2" style="font-size:90%;"> is commonly done by capturing real data, which is challenging to obtain in scale. This makes it difficult to evaluate the robustness of estimators to specific attributes in a controlled manner.
Computer graphics generated synthetic data is an alternative but it is limited in realism and diversity due
to the lack of large amounts of high-quality 3D assets available.
Instead, we use generative models trained on Internet-scale data to sample from </span><math alttext="p(\mathbf{x},\mathbf{P})=p(\mathbf{x}|\mathbf{P})p(\mathbf{P})" class="ltx_Math" display="inline" id="S3.SS1.p1.10.m2.4"><semantics id="S3.SS1.p1.10.m2.4a"><mrow id="S3.SS1.p1.10.m2.4.4" xref="S3.SS1.p1.10.m2.4.4.cmml"><mrow id="S3.SS1.p1.10.m2.4.4.3" xref="S3.SS1.p1.10.m2.4.4.3.cmml"><mi id="S3.SS1.p1.10.m2.4.4.3.2" mathsize="90%" xref="S3.SS1.p1.10.m2.4.4.3.2.cmml">p</mi><mo id="S3.SS1.p1.10.m2.4.4.3.1" xref="S3.SS1.p1.10.m2.4.4.3.1.cmml">⁢</mo><mrow id="S3.SS1.p1.10.m2.4.4.3.3.2" xref="S3.SS1.p1.10.m2.4.4.3.3.1.cmml"><mo id="S3.SS1.p1.10.m2.4.4.3.3.2.1" maxsize="90%" minsize="90%" xref="S3.SS1.p1.10.m2.4.4.3.3.1.cmml">(</mo><mi id="S3.SS1.p1.10.m2.1.1" mathsize="90%" xref="S3.SS1.p1.10.m2.1.1.cmml">𝐱</mi><mo id="S3.SS1.p1.10.m2.4.4.3.3.2.2" mathsize="90%" xref="S3.SS1.p1.10.m2.4.4.3.3.1.cmml">,</mo><mi id="S3.SS1.p1.10.m2.2.2" mathsize="90%" xref="S3.SS1.p1.10.m2.2.2.cmml">𝐏</mi><mo id="S3.SS1.p1.10.m2.4.4.3.3.2.3" maxsize="90%" minsize="90%" xref="S3.SS1.p1.10.m2.4.4.3.3.1.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p1.10.m2.4.4.2" mathsize="90%" xref="S3.SS1.p1.10.m2.4.4.2.cmml">=</mo><mrow id="S3.SS1.p1.10.m2.4.4.1" xref="S3.SS1.p1.10.m2.4.4.1.cmml"><mi id="S3.SS1.p1.10.m2.4.4.1.3" mathsize="90%" xref="S3.SS1.p1.10.m2.4.4.1.3.cmml">p</mi><mo id="S3.SS1.p1.10.m2.4.4.1.2" xref="S3.SS1.p1.10.m2.4.4.1.2.cmml">⁢</mo><mrow id="S3.SS1.p1.10.m2.4.4.1.1.1" xref="S3.SS1.p1.10.m2.4.4.1.1.1.1.cmml"><mo id="S3.SS1.p1.10.m2.4.4.1.1.1.2" maxsize="90%" minsize="90%" xref="S3.SS1.p1.10.m2.4.4.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p1.10.m2.4.4.1.1.1.1" xref="S3.SS1.p1.10.m2.4.4.1.1.1.1.cmml"><mi id="S3.SS1.p1.10.m2.4.4.1.1.1.1.2" mathsize="90%" xref="S3.SS1.p1.10.m2.4.4.1.1.1.1.2.cmml">𝐱</mi><mo fence="false" id="S3.SS1.p1.10.m2.4.4.1.1.1.1.1" mathsize="90%" xref="S3.SS1.p1.10.m2.4.4.1.1.1.1.1.cmml">|</mo><mi id="S3.SS1.p1.10.m2.4.4.1.1.1.1.3" mathsize="90%" xref="S3.SS1.p1.10.m2.4.4.1.1.1.1.3.cmml">𝐏</mi></mrow><mo id="S3.SS1.p1.10.m2.4.4.1.1.1.3" maxsize="90%" minsize="90%" xref="S3.SS1.p1.10.m2.4.4.1.1.1.1.cmml">)</mo></mrow><mo id="S3.SS1.p1.10.m2.4.4.1.2a" xref="S3.SS1.p1.10.m2.4.4.1.2.cmml">⁢</mo><mi id="S3.SS1.p1.10.m2.4.4.1.4" mathsize="90%" xref="S3.SS1.p1.10.m2.4.4.1.4.cmml">p</mi><mo id="S3.SS1.p1.10.m2.4.4.1.2b" xref="S3.SS1.p1.10.m2.4.4.1.2.cmml">⁢</mo><mrow id="S3.SS1.p1.10.m2.4.4.1.5.2" xref="S3.SS1.p1.10.m2.4.4.1.cmml"><mo id="S3.SS1.p1.10.m2.4.4.1.5.2.1" maxsize="90%" minsize="90%" xref="S3.SS1.p1.10.m2.4.4.1.cmml">(</mo><mi id="S3.SS1.p1.10.m2.3.3" mathsize="90%" xref="S3.SS1.p1.10.m2.3.3.cmml">𝐏</mi><mo id="S3.SS1.p1.10.m2.4.4.1.5.2.2" maxsize="90%" minsize="90%" xref="S3.SS1.p1.10.m2.4.4.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.10.m2.4b"><apply id="S3.SS1.p1.10.m2.4.4.cmml" xref="S3.SS1.p1.10.m2.4.4"><eq id="S3.SS1.p1.10.m2.4.4.2.cmml" xref="S3.SS1.p1.10.m2.4.4.2"></eq><apply id="S3.SS1.p1.10.m2.4.4.3.cmml" xref="S3.SS1.p1.10.m2.4.4.3"><times id="S3.SS1.p1.10.m2.4.4.3.1.cmml" xref="S3.SS1.p1.10.m2.4.4.3.1"></times><ci id="S3.SS1.p1.10.m2.4.4.3.2.cmml" xref="S3.SS1.p1.10.m2.4.4.3.2">𝑝</ci><interval closure="open" id="S3.SS1.p1.10.m2.4.4.3.3.1.cmml" xref="S3.SS1.p1.10.m2.4.4.3.3.2"><ci id="S3.SS1.p1.10.m2.1.1.cmml" xref="S3.SS1.p1.10.m2.1.1">𝐱</ci><ci id="S3.SS1.p1.10.m2.2.2.cmml" xref="S3.SS1.p1.10.m2.2.2">𝐏</ci></interval></apply><apply id="S3.SS1.p1.10.m2.4.4.1.cmml" xref="S3.SS1.p1.10.m2.4.4.1"><times id="S3.SS1.p1.10.m2.4.4.1.2.cmml" xref="S3.SS1.p1.10.m2.4.4.1.2"></times><ci id="S3.SS1.p1.10.m2.4.4.1.3.cmml" xref="S3.SS1.p1.10.m2.4.4.1.3">𝑝</ci><apply id="S3.SS1.p1.10.m2.4.4.1.1.1.1.cmml" xref="S3.SS1.p1.10.m2.4.4.1.1.1"><csymbol cd="latexml" id="S3.SS1.p1.10.m2.4.4.1.1.1.1.1.cmml" xref="S3.SS1.p1.10.m2.4.4.1.1.1.1.1">conditional</csymbol><ci id="S3.SS1.p1.10.m2.4.4.1.1.1.1.2.cmml" xref="S3.SS1.p1.10.m2.4.4.1.1.1.1.2">𝐱</ci><ci id="S3.SS1.p1.10.m2.4.4.1.1.1.1.3.cmml" xref="S3.SS1.p1.10.m2.4.4.1.1.1.1.3">𝐏</ci></apply><ci id="S3.SS1.p1.10.m2.4.4.1.4.cmml" xref="S3.SS1.p1.10.m2.4.4.1.4">𝑝</ci><ci id="S3.SS1.p1.10.m2.3.3.cmml" xref="S3.SS1.p1.10.m2.3.3">𝐏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.10.m2.4c">p(\mathbf{x},\mathbf{P})=p(\mathbf{x}|\mathbf{P})p(\mathbf{P})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.10.m2.4d">italic_p ( bold_x , bold_P ) = italic_p ( bold_x | bold_P ) italic_p ( bold_P )</annotation></semantics></math><span class="ltx_text" id="S3.SS1.p1.12.3" style="font-size:90%;">.
To sample from </span><math alttext="p(\mathbf{x}|\mathbf{P})" class="ltx_Math" display="inline" id="S3.SS1.p1.11.m3.1"><semantics id="S3.SS1.p1.11.m3.1a"><mrow id="S3.SS1.p1.11.m3.1.1" xref="S3.SS1.p1.11.m3.1.1.cmml"><mi id="S3.SS1.p1.11.m3.1.1.3" mathsize="90%" xref="S3.SS1.p1.11.m3.1.1.3.cmml">p</mi><mo id="S3.SS1.p1.11.m3.1.1.2" xref="S3.SS1.p1.11.m3.1.1.2.cmml">⁢</mo><mrow id="S3.SS1.p1.11.m3.1.1.1.1" xref="S3.SS1.p1.11.m3.1.1.1.1.1.cmml"><mo id="S3.SS1.p1.11.m3.1.1.1.1.2" maxsize="90%" minsize="90%" xref="S3.SS1.p1.11.m3.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p1.11.m3.1.1.1.1.1" xref="S3.SS1.p1.11.m3.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.11.m3.1.1.1.1.1.2" mathsize="90%" xref="S3.SS1.p1.11.m3.1.1.1.1.1.2.cmml">𝐱</mi><mo fence="false" id="S3.SS1.p1.11.m3.1.1.1.1.1.1" mathsize="90%" xref="S3.SS1.p1.11.m3.1.1.1.1.1.1.cmml">|</mo><mi id="S3.SS1.p1.11.m3.1.1.1.1.1.3" mathsize="90%" xref="S3.SS1.p1.11.m3.1.1.1.1.1.3.cmml">𝐏</mi></mrow><mo id="S3.SS1.p1.11.m3.1.1.1.1.3" maxsize="90%" minsize="90%" xref="S3.SS1.p1.11.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.11.m3.1b"><apply id="S3.SS1.p1.11.m3.1.1.cmml" xref="S3.SS1.p1.11.m3.1.1"><times id="S3.SS1.p1.11.m3.1.1.2.cmml" xref="S3.SS1.p1.11.m3.1.1.2"></times><ci id="S3.SS1.p1.11.m3.1.1.3.cmml" xref="S3.SS1.p1.11.m3.1.1.3">𝑝</ci><apply id="S3.SS1.p1.11.m3.1.1.1.1.1.cmml" xref="S3.SS1.p1.11.m3.1.1.1.1"><csymbol cd="latexml" id="S3.SS1.p1.11.m3.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.11.m3.1.1.1.1.1.1">conditional</csymbol><ci id="S3.SS1.p1.11.m3.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.11.m3.1.1.1.1.1.2">𝐱</ci><ci id="S3.SS1.p1.11.m3.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.11.m3.1.1.1.1.1.3">𝐏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.11.m3.1c">p(\mathbf{x}|\mathbf{P})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.11.m3.1d">italic_p ( bold_x | bold_P )</annotation></semantics></math><span class="ltx_text" id="S3.SS1.p1.12.4" style="font-size:90%;">, we develop a 3D pose-conditioned diffusion model, and for </span><math alttext="p(\mathbf{P})" class="ltx_Math" display="inline" id="S3.SS1.p1.12.m4.1"><semantics id="S3.SS1.p1.12.m4.1a"><mrow id="S3.SS1.p1.12.m4.1.2" xref="S3.SS1.p1.12.m4.1.2.cmml"><mi id="S3.SS1.p1.12.m4.1.2.2" mathsize="90%" xref="S3.SS1.p1.12.m4.1.2.2.cmml">p</mi><mo id="S3.SS1.p1.12.m4.1.2.1" xref="S3.SS1.p1.12.m4.1.2.1.cmml">⁢</mo><mrow id="S3.SS1.p1.12.m4.1.2.3.2" xref="S3.SS1.p1.12.m4.1.2.cmml"><mo id="S3.SS1.p1.12.m4.1.2.3.2.1" maxsize="90%" minsize="90%" xref="S3.SS1.p1.12.m4.1.2.cmml">(</mo><mi id="S3.SS1.p1.12.m4.1.1" mathsize="90%" xref="S3.SS1.p1.12.m4.1.1.cmml">𝐏</mi><mo id="S3.SS1.p1.12.m4.1.2.3.2.2" maxsize="90%" minsize="90%" xref="S3.SS1.p1.12.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.12.m4.1b"><apply id="S3.SS1.p1.12.m4.1.2.cmml" xref="S3.SS1.p1.12.m4.1.2"><times id="S3.SS1.p1.12.m4.1.2.1.cmml" xref="S3.SS1.p1.12.m4.1.2.1"></times><ci id="S3.SS1.p1.12.m4.1.2.2.cmml" xref="S3.SS1.p1.12.m4.1.2.2">𝑝</ci><ci id="S3.SS1.p1.12.m4.1.1.cmml" xref="S3.SS1.p1.12.m4.1.1">𝐏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.12.m4.1c">p(\mathbf{P})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.12.m4.1d">italic_p ( bold_P )</annotation></semantics></math><span class="ltx_text" id="S3.SS1.p1.12.5" style="font-size:90%;">, we sample poses from AMASS </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS1.p1.12.6.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib52" title=""><span class="ltx_text" style="font-size:90%;">52</span></a><span class="ltx_text" id="S3.SS1.p1.12.7.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S3.SS1.p1.12.8" style="font-size:90%;"> or Motion-X </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS1.p1.12.9.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib46" title=""><span class="ltx_text" style="font-size:90%;">46</span></a><span class="ltx_text" id="S3.SS1.p1.12.10.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S3.SS1.p1.12.11" style="font-size:90%;">.</span></p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.13"><span class="ltx_text" id="S3.SS1.p2.13.1" style="font-size:90%;">We further want control over specific scene attributes such as clothing, location, </span><em class="ltx_emph ltx_font_italic" id="S3.SS1.p2.13.2" style="font-size:90%;">etc</em><span class="ltx_text" id="S3.SS1.p2.13.3" style="font-size:90%;">.</span><span class="ltx_text" id="S3.SS1.p2.13.4"></span><span class="ltx_text" id="S3.SS1.p2.13.5" style="font-size:90%;">, which we summarize in the latent </span><math alttext="\mathbf{z}" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" mathsize="90%" xref="S3.SS1.p2.1.m1.1.1.cmml">𝐳</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝐳</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\mathbf{z}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">bold_z</annotation></semantics></math><span class="ltx_text" id="S3.SS1.p2.13.6" style="font-size:90%;">.
Thus, we aim to sample from </span><math alttext="p(\mathbf{x},\mathbf{P}|\mathbf{z})=p(\mathbf{x}|\mathbf{P},\mathbf{z})p(%
\mathbf{P}|\mathbf{z})" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.6"><semantics id="S3.SS1.p2.2.m2.6a"><mrow id="S3.SS1.p2.2.m2.6.6" xref="S3.SS1.p2.2.m2.6.6.cmml"><mrow id="S3.SS1.p2.2.m2.4.4.1" xref="S3.SS1.p2.2.m2.4.4.1.cmml"><mi id="S3.SS1.p2.2.m2.4.4.1.3" mathsize="90%" xref="S3.SS1.p2.2.m2.4.4.1.3.cmml">p</mi><mo id="S3.SS1.p2.2.m2.4.4.1.2" xref="S3.SS1.p2.2.m2.4.4.1.2.cmml">⁢</mo><mrow id="S3.SS1.p2.2.m2.4.4.1.1.1" xref="S3.SS1.p2.2.m2.4.4.1.1.2.cmml"><mo id="S3.SS1.p2.2.m2.4.4.1.1.1.2" maxsize="90%" minsize="90%" xref="S3.SS1.p2.2.m2.4.4.1.1.2.cmml">(</mo><mi id="S3.SS1.p2.2.m2.1.1" mathsize="90%" xref="S3.SS1.p2.2.m2.1.1.cmml">𝐱</mi><mo id="S3.SS1.p2.2.m2.4.4.1.1.1.3" mathsize="90%" xref="S3.SS1.p2.2.m2.4.4.1.1.2.cmml">,</mo><mrow id="S3.SS1.p2.2.m2.4.4.1.1.1.1" xref="S3.SS1.p2.2.m2.4.4.1.1.1.1.cmml"><mi id="S3.SS1.p2.2.m2.4.4.1.1.1.1.2" mathsize="90%" xref="S3.SS1.p2.2.m2.4.4.1.1.1.1.2.cmml">𝐏</mi><mo fence="false" id="S3.SS1.p2.2.m2.4.4.1.1.1.1.1" mathsize="90%" xref="S3.SS1.p2.2.m2.4.4.1.1.1.1.1.cmml">|</mo><mi id="S3.SS1.p2.2.m2.4.4.1.1.1.1.3" mathsize="90%" xref="S3.SS1.p2.2.m2.4.4.1.1.1.1.3.cmml">𝐳</mi></mrow><mo id="S3.SS1.p2.2.m2.4.4.1.1.1.4" maxsize="90%" minsize="90%" xref="S3.SS1.p2.2.m2.4.4.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p2.2.m2.6.6.4" mathsize="90%" xref="S3.SS1.p2.2.m2.6.6.4.cmml">=</mo><mrow id="S3.SS1.p2.2.m2.6.6.3" xref="S3.SS1.p2.2.m2.6.6.3.cmml"><mi id="S3.SS1.p2.2.m2.6.6.3.4" mathsize="90%" xref="S3.SS1.p2.2.m2.6.6.3.4.cmml">p</mi><mo id="S3.SS1.p2.2.m2.6.6.3.3" xref="S3.SS1.p2.2.m2.6.6.3.3.cmml">⁢</mo><mrow id="S3.SS1.p2.2.m2.5.5.2.1.1" xref="S3.SS1.p2.2.m2.5.5.2.1.1.1.cmml"><mo id="S3.SS1.p2.2.m2.5.5.2.1.1.2" maxsize="90%" minsize="90%" xref="S3.SS1.p2.2.m2.5.5.2.1.1.1.cmml">(</mo><mrow id="S3.SS1.p2.2.m2.5.5.2.1.1.1" xref="S3.SS1.p2.2.m2.5.5.2.1.1.1.cmml"><mi id="S3.SS1.p2.2.m2.5.5.2.1.1.1.2" mathsize="90%" xref="S3.SS1.p2.2.m2.5.5.2.1.1.1.2.cmml">𝐱</mi><mo fence="false" id="S3.SS1.p2.2.m2.5.5.2.1.1.1.1" mathsize="90%" xref="S3.SS1.p2.2.m2.5.5.2.1.1.1.1.cmml">|</mo><mrow id="S3.SS1.p2.2.m2.5.5.2.1.1.1.3.2" xref="S3.SS1.p2.2.m2.5.5.2.1.1.1.3.1.cmml"><mi id="S3.SS1.p2.2.m2.2.2" mathsize="90%" xref="S3.SS1.p2.2.m2.2.2.cmml">𝐏</mi><mo id="S3.SS1.p2.2.m2.5.5.2.1.1.1.3.2.1" mathsize="90%" xref="S3.SS1.p2.2.m2.5.5.2.1.1.1.3.1.cmml">,</mo><mi id="S3.SS1.p2.2.m2.3.3" mathsize="90%" xref="S3.SS1.p2.2.m2.3.3.cmml">𝐳</mi></mrow></mrow><mo id="S3.SS1.p2.2.m2.5.5.2.1.1.3" maxsize="90%" minsize="90%" xref="S3.SS1.p2.2.m2.5.5.2.1.1.1.cmml">)</mo></mrow><mo id="S3.SS1.p2.2.m2.6.6.3.3a" xref="S3.SS1.p2.2.m2.6.6.3.3.cmml">⁢</mo><mi id="S3.SS1.p2.2.m2.6.6.3.5" mathsize="90%" xref="S3.SS1.p2.2.m2.6.6.3.5.cmml">p</mi><mo id="S3.SS1.p2.2.m2.6.6.3.3b" xref="S3.SS1.p2.2.m2.6.6.3.3.cmml">⁢</mo><mrow id="S3.SS1.p2.2.m2.6.6.3.2.1" xref="S3.SS1.p2.2.m2.6.6.3.2.1.1.cmml"><mo id="S3.SS1.p2.2.m2.6.6.3.2.1.2" maxsize="90%" minsize="90%" xref="S3.SS1.p2.2.m2.6.6.3.2.1.1.cmml">(</mo><mrow id="S3.SS1.p2.2.m2.6.6.3.2.1.1" xref="S3.SS1.p2.2.m2.6.6.3.2.1.1.cmml"><mi id="S3.SS1.p2.2.m2.6.6.3.2.1.1.2" mathsize="90%" xref="S3.SS1.p2.2.m2.6.6.3.2.1.1.2.cmml">𝐏</mi><mo fence="false" id="S3.SS1.p2.2.m2.6.6.3.2.1.1.1" mathsize="90%" xref="S3.SS1.p2.2.m2.6.6.3.2.1.1.1.cmml">|</mo><mi id="S3.SS1.p2.2.m2.6.6.3.2.1.1.3" mathsize="90%" xref="S3.SS1.p2.2.m2.6.6.3.2.1.1.3.cmml">𝐳</mi></mrow><mo id="S3.SS1.p2.2.m2.6.6.3.2.1.3" maxsize="90%" minsize="90%" xref="S3.SS1.p2.2.m2.6.6.3.2.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.6b"><apply id="S3.SS1.p2.2.m2.6.6.cmml" xref="S3.SS1.p2.2.m2.6.6"><eq id="S3.SS1.p2.2.m2.6.6.4.cmml" xref="S3.SS1.p2.2.m2.6.6.4"></eq><apply id="S3.SS1.p2.2.m2.4.4.1.cmml" xref="S3.SS1.p2.2.m2.4.4.1"><times id="S3.SS1.p2.2.m2.4.4.1.2.cmml" xref="S3.SS1.p2.2.m2.4.4.1.2"></times><ci id="S3.SS1.p2.2.m2.4.4.1.3.cmml" xref="S3.SS1.p2.2.m2.4.4.1.3">𝑝</ci><interval closure="open" id="S3.SS1.p2.2.m2.4.4.1.1.2.cmml" xref="S3.SS1.p2.2.m2.4.4.1.1.1"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">𝐱</ci><apply id="S3.SS1.p2.2.m2.4.4.1.1.1.1.cmml" xref="S3.SS1.p2.2.m2.4.4.1.1.1.1"><csymbol cd="latexml" id="S3.SS1.p2.2.m2.4.4.1.1.1.1.1.cmml" xref="S3.SS1.p2.2.m2.4.4.1.1.1.1.1">conditional</csymbol><ci id="S3.SS1.p2.2.m2.4.4.1.1.1.1.2.cmml" xref="S3.SS1.p2.2.m2.4.4.1.1.1.1.2">𝐏</ci><ci id="S3.SS1.p2.2.m2.4.4.1.1.1.1.3.cmml" xref="S3.SS1.p2.2.m2.4.4.1.1.1.1.3">𝐳</ci></apply></interval></apply><apply id="S3.SS1.p2.2.m2.6.6.3.cmml" xref="S3.SS1.p2.2.m2.6.6.3"><times id="S3.SS1.p2.2.m2.6.6.3.3.cmml" xref="S3.SS1.p2.2.m2.6.6.3.3"></times><ci id="S3.SS1.p2.2.m2.6.6.3.4.cmml" xref="S3.SS1.p2.2.m2.6.6.3.4">𝑝</ci><apply id="S3.SS1.p2.2.m2.5.5.2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.5.5.2.1.1"><csymbol cd="latexml" id="S3.SS1.p2.2.m2.5.5.2.1.1.1.1.cmml" xref="S3.SS1.p2.2.m2.5.5.2.1.1.1.1">conditional</csymbol><ci id="S3.SS1.p2.2.m2.5.5.2.1.1.1.2.cmml" xref="S3.SS1.p2.2.m2.5.5.2.1.1.1.2">𝐱</ci><list id="S3.SS1.p2.2.m2.5.5.2.1.1.1.3.1.cmml" xref="S3.SS1.p2.2.m2.5.5.2.1.1.1.3.2"><ci id="S3.SS1.p2.2.m2.2.2.cmml" xref="S3.SS1.p2.2.m2.2.2">𝐏</ci><ci id="S3.SS1.p2.2.m2.3.3.cmml" xref="S3.SS1.p2.2.m2.3.3">𝐳</ci></list></apply><ci id="S3.SS1.p2.2.m2.6.6.3.5.cmml" xref="S3.SS1.p2.2.m2.6.6.3.5">𝑝</ci><apply id="S3.SS1.p2.2.m2.6.6.3.2.1.1.cmml" xref="S3.SS1.p2.2.m2.6.6.3.2.1"><csymbol cd="latexml" id="S3.SS1.p2.2.m2.6.6.3.2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.6.6.3.2.1.1.1">conditional</csymbol><ci id="S3.SS1.p2.2.m2.6.6.3.2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.6.6.3.2.1.1.2">𝐏</ci><ci id="S3.SS1.p2.2.m2.6.6.3.2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.6.6.3.2.1.1.3">𝐳</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.6c">p(\mathbf{x},\mathbf{P}|\mathbf{z})=p(\mathbf{x}|\mathbf{P},\mathbf{z})p(%
\mathbf{P}|\mathbf{z})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.6d">italic_p ( bold_x , bold_P | bold_z ) = italic_p ( bold_x | bold_P , bold_z ) italic_p ( bold_P | bold_z )</annotation></semantics></math><span class="ltx_text" id="S3.SS1.p2.13.7" style="font-size:90%;">.
For the purpose of evaluation, we assume that a person can adopt the same body pose regardless of scene attributes, </span><em class="ltx_emph ltx_font_italic" id="S3.SS1.p2.13.8" style="font-size:90%;">i.e</em><span class="ltx_text" id="S3.SS1.p2.13.9" style="font-size:90%;">.</span><span class="ltx_text" id="S3.SS1.p2.13.10"></span><span class="ltx_text" id="S3.SS1.p2.13.11" style="font-size:90%;">, </span><math alttext="p(\mathbf{P}|\mathbf{z})=p(\mathbf{P})" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3.2"><semantics id="S3.SS1.p2.3.m3.2a"><mrow id="S3.SS1.p2.3.m3.2.2" xref="S3.SS1.p2.3.m3.2.2.cmml"><mrow id="S3.SS1.p2.3.m3.2.2.1" xref="S3.SS1.p2.3.m3.2.2.1.cmml"><mi id="S3.SS1.p2.3.m3.2.2.1.3" mathsize="90%" xref="S3.SS1.p2.3.m3.2.2.1.3.cmml">p</mi><mo id="S3.SS1.p2.3.m3.2.2.1.2" xref="S3.SS1.p2.3.m3.2.2.1.2.cmml">⁢</mo><mrow id="S3.SS1.p2.3.m3.2.2.1.1.1" xref="S3.SS1.p2.3.m3.2.2.1.1.1.1.cmml"><mo id="S3.SS1.p2.3.m3.2.2.1.1.1.2" maxsize="90%" minsize="90%" xref="S3.SS1.p2.3.m3.2.2.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p2.3.m3.2.2.1.1.1.1" xref="S3.SS1.p2.3.m3.2.2.1.1.1.1.cmml"><mi id="S3.SS1.p2.3.m3.2.2.1.1.1.1.2" mathsize="90%" xref="S3.SS1.p2.3.m3.2.2.1.1.1.1.2.cmml">𝐏</mi><mo fence="false" id="S3.SS1.p2.3.m3.2.2.1.1.1.1.1" mathsize="90%" xref="S3.SS1.p2.3.m3.2.2.1.1.1.1.1.cmml">|</mo><mi id="S3.SS1.p2.3.m3.2.2.1.1.1.1.3" mathsize="90%" xref="S3.SS1.p2.3.m3.2.2.1.1.1.1.3.cmml">𝐳</mi></mrow><mo id="S3.SS1.p2.3.m3.2.2.1.1.1.3" maxsize="90%" minsize="90%" xref="S3.SS1.p2.3.m3.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p2.3.m3.2.2.2" mathsize="90%" xref="S3.SS1.p2.3.m3.2.2.2.cmml">=</mo><mrow id="S3.SS1.p2.3.m3.2.2.3" xref="S3.SS1.p2.3.m3.2.2.3.cmml"><mi id="S3.SS1.p2.3.m3.2.2.3.2" mathsize="90%" xref="S3.SS1.p2.3.m3.2.2.3.2.cmml">p</mi><mo id="S3.SS1.p2.3.m3.2.2.3.1" xref="S3.SS1.p2.3.m3.2.2.3.1.cmml">⁢</mo><mrow id="S3.SS1.p2.3.m3.2.2.3.3.2" xref="S3.SS1.p2.3.m3.2.2.3.cmml"><mo id="S3.SS1.p2.3.m3.2.2.3.3.2.1" maxsize="90%" minsize="90%" xref="S3.SS1.p2.3.m3.2.2.3.cmml">(</mo><mi id="S3.SS1.p2.3.m3.1.1" mathsize="90%" xref="S3.SS1.p2.3.m3.1.1.cmml">𝐏</mi><mo id="S3.SS1.p2.3.m3.2.2.3.3.2.2" maxsize="90%" minsize="90%" xref="S3.SS1.p2.3.m3.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.2b"><apply id="S3.SS1.p2.3.m3.2.2.cmml" xref="S3.SS1.p2.3.m3.2.2"><eq id="S3.SS1.p2.3.m3.2.2.2.cmml" xref="S3.SS1.p2.3.m3.2.2.2"></eq><apply id="S3.SS1.p2.3.m3.2.2.1.cmml" xref="S3.SS1.p2.3.m3.2.2.1"><times id="S3.SS1.p2.3.m3.2.2.1.2.cmml" xref="S3.SS1.p2.3.m3.2.2.1.2"></times><ci id="S3.SS1.p2.3.m3.2.2.1.3.cmml" xref="S3.SS1.p2.3.m3.2.2.1.3">𝑝</ci><apply id="S3.SS1.p2.3.m3.2.2.1.1.1.1.cmml" xref="S3.SS1.p2.3.m3.2.2.1.1.1"><csymbol cd="latexml" id="S3.SS1.p2.3.m3.2.2.1.1.1.1.1.cmml" xref="S3.SS1.p2.3.m3.2.2.1.1.1.1.1">conditional</csymbol><ci id="S3.SS1.p2.3.m3.2.2.1.1.1.1.2.cmml" xref="S3.SS1.p2.3.m3.2.2.1.1.1.1.2">𝐏</ci><ci id="S3.SS1.p2.3.m3.2.2.1.1.1.1.3.cmml" xref="S3.SS1.p2.3.m3.2.2.1.1.1.1.3">𝐳</ci></apply></apply><apply id="S3.SS1.p2.3.m3.2.2.3.cmml" xref="S3.SS1.p2.3.m3.2.2.3"><times id="S3.SS1.p2.3.m3.2.2.3.1.cmml" xref="S3.SS1.p2.3.m3.2.2.3.1"></times><ci id="S3.SS1.p2.3.m3.2.2.3.2.cmml" xref="S3.SS1.p2.3.m3.2.2.3.2">𝑝</ci><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">𝐏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.2c">p(\mathbf{P}|\mathbf{z})=p(\mathbf{P})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m3.2d">italic_p ( bold_P | bold_z ) = italic_p ( bold_P )</annotation></semantics></math><span class="ltx_text" id="S3.SS1.p2.13.12" style="font-size:90%;">.
Leveraging recent developments in text-to-image (T2I) generation, we use a T2I diffusion model to approximate </span><math alttext="p(\mathbf{x}|\mathbf{P},\mathbf{z})\approx p(\mathbf{x}|\mathbf{P},t,\mathbf{n})" class="ltx_Math" display="inline" id="S3.SS1.p2.4.m4.7"><semantics id="S3.SS1.p2.4.m4.7a"><mrow id="S3.SS1.p2.4.m4.7.7" xref="S3.SS1.p2.4.m4.7.7.cmml"><mrow id="S3.SS1.p2.4.m4.6.6.1" xref="S3.SS1.p2.4.m4.6.6.1.cmml"><mi id="S3.SS1.p2.4.m4.6.6.1.3" mathsize="90%" xref="S3.SS1.p2.4.m4.6.6.1.3.cmml">p</mi><mo id="S3.SS1.p2.4.m4.6.6.1.2" xref="S3.SS1.p2.4.m4.6.6.1.2.cmml">⁢</mo><mrow id="S3.SS1.p2.4.m4.6.6.1.1.1" xref="S3.SS1.p2.4.m4.6.6.1.1.1.1.cmml"><mo id="S3.SS1.p2.4.m4.6.6.1.1.1.2" maxsize="90%" minsize="90%" xref="S3.SS1.p2.4.m4.6.6.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p2.4.m4.6.6.1.1.1.1" xref="S3.SS1.p2.4.m4.6.6.1.1.1.1.cmml"><mi id="S3.SS1.p2.4.m4.6.6.1.1.1.1.2" mathsize="90%" xref="S3.SS1.p2.4.m4.6.6.1.1.1.1.2.cmml">𝐱</mi><mo fence="false" id="S3.SS1.p2.4.m4.6.6.1.1.1.1.1" mathsize="90%" xref="S3.SS1.p2.4.m4.6.6.1.1.1.1.1.cmml">|</mo><mrow id="S3.SS1.p2.4.m4.6.6.1.1.1.1.3.2" xref="S3.SS1.p2.4.m4.6.6.1.1.1.1.3.1.cmml"><mi id="S3.SS1.p2.4.m4.1.1" mathsize="90%" xref="S3.SS1.p2.4.m4.1.1.cmml">𝐏</mi><mo id="S3.SS1.p2.4.m4.6.6.1.1.1.1.3.2.1" mathsize="90%" xref="S3.SS1.p2.4.m4.6.6.1.1.1.1.3.1.cmml">,</mo><mi id="S3.SS1.p2.4.m4.2.2" mathsize="90%" xref="S3.SS1.p2.4.m4.2.2.cmml">𝐳</mi></mrow></mrow><mo id="S3.SS1.p2.4.m4.6.6.1.1.1.3" maxsize="90%" minsize="90%" xref="S3.SS1.p2.4.m4.6.6.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p2.4.m4.7.7.3" mathsize="90%" xref="S3.SS1.p2.4.m4.7.7.3.cmml">≈</mo><mrow id="S3.SS1.p2.4.m4.7.7.2" xref="S3.SS1.p2.4.m4.7.7.2.cmml"><mi id="S3.SS1.p2.4.m4.7.7.2.3" mathsize="90%" xref="S3.SS1.p2.4.m4.7.7.2.3.cmml">p</mi><mo id="S3.SS1.p2.4.m4.7.7.2.2" xref="S3.SS1.p2.4.m4.7.7.2.2.cmml">⁢</mo><mrow id="S3.SS1.p2.4.m4.7.7.2.1.1" xref="S3.SS1.p2.4.m4.7.7.2.1.1.1.cmml"><mo id="S3.SS1.p2.4.m4.7.7.2.1.1.2" maxsize="90%" minsize="90%" xref="S3.SS1.p2.4.m4.7.7.2.1.1.1.cmml">(</mo><mrow id="S3.SS1.p2.4.m4.7.7.2.1.1.1" xref="S3.SS1.p2.4.m4.7.7.2.1.1.1.cmml"><mi id="S3.SS1.p2.4.m4.7.7.2.1.1.1.2" mathsize="90%" xref="S3.SS1.p2.4.m4.7.7.2.1.1.1.2.cmml">𝐱</mi><mo fence="false" id="S3.SS1.p2.4.m4.7.7.2.1.1.1.1" mathsize="90%" xref="S3.SS1.p2.4.m4.7.7.2.1.1.1.1.cmml">|</mo><mrow id="S3.SS1.p2.4.m4.7.7.2.1.1.1.3.2" xref="S3.SS1.p2.4.m4.7.7.2.1.1.1.3.1.cmml"><mi id="S3.SS1.p2.4.m4.3.3" mathsize="90%" xref="S3.SS1.p2.4.m4.3.3.cmml">𝐏</mi><mo id="S3.SS1.p2.4.m4.7.7.2.1.1.1.3.2.1" mathsize="90%" xref="S3.SS1.p2.4.m4.7.7.2.1.1.1.3.1.cmml">,</mo><mi id="S3.SS1.p2.4.m4.4.4" mathsize="90%" xref="S3.SS1.p2.4.m4.4.4.cmml">t</mi><mo id="S3.SS1.p2.4.m4.7.7.2.1.1.1.3.2.2" mathsize="90%" xref="S3.SS1.p2.4.m4.7.7.2.1.1.1.3.1.cmml">,</mo><mi id="S3.SS1.p2.4.m4.5.5" mathsize="90%" xref="S3.SS1.p2.4.m4.5.5.cmml">𝐧</mi></mrow></mrow><mo id="S3.SS1.p2.4.m4.7.7.2.1.1.3" maxsize="90%" minsize="90%" xref="S3.SS1.p2.4.m4.7.7.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.7b"><apply id="S3.SS1.p2.4.m4.7.7.cmml" xref="S3.SS1.p2.4.m4.7.7"><approx id="S3.SS1.p2.4.m4.7.7.3.cmml" xref="S3.SS1.p2.4.m4.7.7.3"></approx><apply id="S3.SS1.p2.4.m4.6.6.1.cmml" xref="S3.SS1.p2.4.m4.6.6.1"><times id="S3.SS1.p2.4.m4.6.6.1.2.cmml" xref="S3.SS1.p2.4.m4.6.6.1.2"></times><ci id="S3.SS1.p2.4.m4.6.6.1.3.cmml" xref="S3.SS1.p2.4.m4.6.6.1.3">𝑝</ci><apply id="S3.SS1.p2.4.m4.6.6.1.1.1.1.cmml" xref="S3.SS1.p2.4.m4.6.6.1.1.1"><csymbol cd="latexml" id="S3.SS1.p2.4.m4.6.6.1.1.1.1.1.cmml" xref="S3.SS1.p2.4.m4.6.6.1.1.1.1.1">conditional</csymbol><ci id="S3.SS1.p2.4.m4.6.6.1.1.1.1.2.cmml" xref="S3.SS1.p2.4.m4.6.6.1.1.1.1.2">𝐱</ci><list id="S3.SS1.p2.4.m4.6.6.1.1.1.1.3.1.cmml" xref="S3.SS1.p2.4.m4.6.6.1.1.1.1.3.2"><ci id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">𝐏</ci><ci id="S3.SS1.p2.4.m4.2.2.cmml" xref="S3.SS1.p2.4.m4.2.2">𝐳</ci></list></apply></apply><apply id="S3.SS1.p2.4.m4.7.7.2.cmml" xref="S3.SS1.p2.4.m4.7.7.2"><times id="S3.SS1.p2.4.m4.7.7.2.2.cmml" xref="S3.SS1.p2.4.m4.7.7.2.2"></times><ci id="S3.SS1.p2.4.m4.7.7.2.3.cmml" xref="S3.SS1.p2.4.m4.7.7.2.3">𝑝</ci><apply id="S3.SS1.p2.4.m4.7.7.2.1.1.1.cmml" xref="S3.SS1.p2.4.m4.7.7.2.1.1"><csymbol cd="latexml" id="S3.SS1.p2.4.m4.7.7.2.1.1.1.1.cmml" xref="S3.SS1.p2.4.m4.7.7.2.1.1.1.1">conditional</csymbol><ci id="S3.SS1.p2.4.m4.7.7.2.1.1.1.2.cmml" xref="S3.SS1.p2.4.m4.7.7.2.1.1.1.2">𝐱</ci><list id="S3.SS1.p2.4.m4.7.7.2.1.1.1.3.1.cmml" xref="S3.SS1.p2.4.m4.7.7.2.1.1.1.3.2"><ci id="S3.SS1.p2.4.m4.3.3.cmml" xref="S3.SS1.p2.4.m4.3.3">𝐏</ci><ci id="S3.SS1.p2.4.m4.4.4.cmml" xref="S3.SS1.p2.4.m4.4.4">𝑡</ci><ci id="S3.SS1.p2.4.m4.5.5.cmml" xref="S3.SS1.p2.4.m4.5.5">𝐧</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.7c">p(\mathbf{x}|\mathbf{P},\mathbf{z})\approx p(\mathbf{x}|\mathbf{P},t,\mathbf{n})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.4.m4.7d">italic_p ( bold_x | bold_P , bold_z ) ≈ italic_p ( bold_x | bold_P , italic_t , bold_n )</annotation></semantics></math><span class="ltx_text" id="S3.SS1.p2.13.13" style="font-size:90%;">, where </span><math alttext="t" class="ltx_Math" display="inline" id="S3.SS1.p2.5.m5.1"><semantics id="S3.SS1.p2.5.m5.1a"><mi id="S3.SS1.p2.5.m5.1.1" mathsize="90%" xref="S3.SS1.p2.5.m5.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><ci id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.5.m5.1d">italic_t</annotation></semantics></math><span class="ltx_text" id="S3.SS1.p2.13.14" style="font-size:90%;"> is a text prompt and </span><math alttext="\mathbf{n}\sim\mathcal{N}\left(\mathbf{0},\mathbf{I}\right)" class="ltx_Math" display="inline" id="S3.SS1.p2.6.m6.2"><semantics id="S3.SS1.p2.6.m6.2a"><mrow id="S3.SS1.p2.6.m6.2.3" xref="S3.SS1.p2.6.m6.2.3.cmml"><mi id="S3.SS1.p2.6.m6.2.3.2" mathsize="90%" xref="S3.SS1.p2.6.m6.2.3.2.cmml">𝐧</mi><mo id="S3.SS1.p2.6.m6.2.3.1" mathsize="90%" xref="S3.SS1.p2.6.m6.2.3.1.cmml">∼</mo><mrow id="S3.SS1.p2.6.m6.2.3.3" xref="S3.SS1.p2.6.m6.2.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.6.m6.2.3.3.2" mathsize="90%" xref="S3.SS1.p2.6.m6.2.3.3.2.cmml">𝒩</mi><mo id="S3.SS1.p2.6.m6.2.3.3.1" xref="S3.SS1.p2.6.m6.2.3.3.1.cmml">⁢</mo><mrow id="S3.SS1.p2.6.m6.2.3.3.3.2" xref="S3.SS1.p2.6.m6.2.3.3.3.1.cmml"><mo id="S3.SS1.p2.6.m6.2.3.3.3.2.1" xref="S3.SS1.p2.6.m6.2.3.3.3.1.cmml">(</mo><mn id="S3.SS1.p2.6.m6.1.1" mathsize="90%" xref="S3.SS1.p2.6.m6.1.1.cmml">𝟎</mn><mo id="S3.SS1.p2.6.m6.2.3.3.3.2.2" mathsize="90%" xref="S3.SS1.p2.6.m6.2.3.3.3.1.cmml">,</mo><mi id="S3.SS1.p2.6.m6.2.2" mathsize="90%" xref="S3.SS1.p2.6.m6.2.2.cmml">𝐈</mi><mo id="S3.SS1.p2.6.m6.2.3.3.3.2.3" xref="S3.SS1.p2.6.m6.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.2b"><apply id="S3.SS1.p2.6.m6.2.3.cmml" xref="S3.SS1.p2.6.m6.2.3"><csymbol cd="latexml" id="S3.SS1.p2.6.m6.2.3.1.cmml" xref="S3.SS1.p2.6.m6.2.3.1">similar-to</csymbol><ci id="S3.SS1.p2.6.m6.2.3.2.cmml" xref="S3.SS1.p2.6.m6.2.3.2">𝐧</ci><apply id="S3.SS1.p2.6.m6.2.3.3.cmml" xref="S3.SS1.p2.6.m6.2.3.3"><times id="S3.SS1.p2.6.m6.2.3.3.1.cmml" xref="S3.SS1.p2.6.m6.2.3.3.1"></times><ci id="S3.SS1.p2.6.m6.2.3.3.2.cmml" xref="S3.SS1.p2.6.m6.2.3.3.2">𝒩</ci><interval closure="open" id="S3.SS1.p2.6.m6.2.3.3.3.1.cmml" xref="S3.SS1.p2.6.m6.2.3.3.3.2"><cn id="S3.SS1.p2.6.m6.1.1.cmml" type="integer" xref="S3.SS1.p2.6.m6.1.1">0</cn><ci id="S3.SS1.p2.6.m6.2.2.cmml" xref="S3.SS1.p2.6.m6.2.2">𝐈</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.2c">\mathbf{n}\sim\mathcal{N}\left(\mathbf{0},\mathbf{I}\right)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.6.m6.2d">bold_n ∼ caligraphic_N ( bold_0 , bold_I )</annotation></semantics></math><span class="ltx_text" id="S3.SS1.p2.13.15" style="font-size:90%;"> is the initial noise for the denoising process.
The </span><math alttext="\mathbf{z}" class="ltx_Math" display="inline" id="S3.SS1.p2.7.m7.1"><semantics id="S3.SS1.p2.7.m7.1a"><mi id="S3.SS1.p2.7.m7.1.1" mathsize="90%" xref="S3.SS1.p2.7.m7.1.1.cmml">𝐳</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.1b"><ci id="S3.SS1.p2.7.m7.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1">𝐳</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.1c">\mathbf{z}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.7.m7.1d">bold_z</annotation></semantics></math><span class="ltx_text" id="S3.SS1.p2.13.16" style="font-size:90%;"> is represented by </span><math alttext="t" class="ltx_Math" display="inline" id="S3.SS1.p2.8.m8.1"><semantics id="S3.SS1.p2.8.m8.1a"><mi id="S3.SS1.p2.8.m8.1.1" mathsize="90%" xref="S3.SS1.p2.8.m8.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m8.1b"><ci id="S3.SS1.p2.8.m8.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m8.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.8.m8.1d">italic_t</annotation></semantics></math><span class="ltx_text" id="S3.SS1.p2.13.17" style="font-size:90%;"> and </span><math alttext="\mathbf{n}" class="ltx_Math" display="inline" id="S3.SS1.p2.9.m9.1"><semantics id="S3.SS1.p2.9.m9.1a"><mi id="S3.SS1.p2.9.m9.1.1" mathsize="90%" xref="S3.SS1.p2.9.m9.1.1.cmml">𝐧</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.9.m9.1b"><ci id="S3.SS1.p2.9.m9.1.1.cmml" xref="S3.SS1.p2.9.m9.1.1">𝐧</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.9.m9.1c">\mathbf{n}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.9.m9.1d">bold_n</annotation></semantics></math><span class="ltx_text" id="S3.SS1.p2.13.18" style="font-size:90%;">.
The text prompt allows us to generate images containing the required attributes, </span><em class="ltx_emph ltx_font_italic" id="S3.SS1.p2.13.19" style="font-size:90%;">e.g</em><span class="ltx_text" id="S3.SS1.p2.13.20" style="font-size:90%;">., “Photo, a Caucasian elderly male wearing a t-shirt and pants in the city center during daytime”.
The noise vector encodes the rest of the details, such as the background layout and a specific appearance of the human not contained in the text prompt.
We note that with diffusion sampling algorithms, such as DDIM </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS1.p2.13.21.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib68" title=""><span class="ltx_text" style="font-size:90%;">68</span></a><span class="ltx_text" id="S3.SS1.p2.13.22.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S3.SS1.p2.13.23" style="font-size:90%;">, the generation process is deterministic given the initial noise </span><math alttext="\mathbf{n}" class="ltx_Math" display="inline" id="S3.SS1.p2.10.m10.1"><semantics id="S3.SS1.p2.10.m10.1a"><mi id="S3.SS1.p2.10.m10.1.1" mathsize="90%" xref="S3.SS1.p2.10.m10.1.1.cmml">𝐧</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.10.m10.1b"><ci id="S3.SS1.p2.10.m10.1.1.cmml" xref="S3.SS1.p2.10.m10.1.1">𝐧</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.10.m10.1c">\mathbf{n}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.10.m10.1d">bold_n</annotation></semantics></math><span class="ltx_text" id="S3.SS1.p2.13.24" style="font-size:90%;">.
Thus sampling from </span><math alttext="p(\mathbf{x}|\mathbf{P},t,\mathbf{n})" class="ltx_Math" display="inline" id="S3.SS1.p2.11.m11.4"><semantics id="S3.SS1.p2.11.m11.4a"><mrow id="S3.SS1.p2.11.m11.4.4" xref="S3.SS1.p2.11.m11.4.4.cmml"><mi id="S3.SS1.p2.11.m11.4.4.3" mathsize="90%" xref="S3.SS1.p2.11.m11.4.4.3.cmml">p</mi><mo id="S3.SS1.p2.11.m11.4.4.2" xref="S3.SS1.p2.11.m11.4.4.2.cmml">⁢</mo><mrow id="S3.SS1.p2.11.m11.4.4.1.1" xref="S3.SS1.p2.11.m11.4.4.1.1.1.cmml"><mo id="S3.SS1.p2.11.m11.4.4.1.1.2" maxsize="90%" minsize="90%" xref="S3.SS1.p2.11.m11.4.4.1.1.1.cmml">(</mo><mrow id="S3.SS1.p2.11.m11.4.4.1.1.1" xref="S3.SS1.p2.11.m11.4.4.1.1.1.cmml"><mi id="S3.SS1.p2.11.m11.4.4.1.1.1.2" mathsize="90%" xref="S3.SS1.p2.11.m11.4.4.1.1.1.2.cmml">𝐱</mi><mo fence="false" id="S3.SS1.p2.11.m11.4.4.1.1.1.1" mathsize="90%" xref="S3.SS1.p2.11.m11.4.4.1.1.1.1.cmml">|</mo><mrow id="S3.SS1.p2.11.m11.4.4.1.1.1.3.2" xref="S3.SS1.p2.11.m11.4.4.1.1.1.3.1.cmml"><mi id="S3.SS1.p2.11.m11.1.1" mathsize="90%" xref="S3.SS1.p2.11.m11.1.1.cmml">𝐏</mi><mo id="S3.SS1.p2.11.m11.4.4.1.1.1.3.2.1" mathsize="90%" xref="S3.SS1.p2.11.m11.4.4.1.1.1.3.1.cmml">,</mo><mi id="S3.SS1.p2.11.m11.2.2" mathsize="90%" xref="S3.SS1.p2.11.m11.2.2.cmml">t</mi><mo id="S3.SS1.p2.11.m11.4.4.1.1.1.3.2.2" mathsize="90%" xref="S3.SS1.p2.11.m11.4.4.1.1.1.3.1.cmml">,</mo><mi id="S3.SS1.p2.11.m11.3.3" mathsize="90%" xref="S3.SS1.p2.11.m11.3.3.cmml">𝐧</mi></mrow></mrow><mo id="S3.SS1.p2.11.m11.4.4.1.1.3" maxsize="90%" minsize="90%" xref="S3.SS1.p2.11.m11.4.4.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.11.m11.4b"><apply id="S3.SS1.p2.11.m11.4.4.cmml" xref="S3.SS1.p2.11.m11.4.4"><times id="S3.SS1.p2.11.m11.4.4.2.cmml" xref="S3.SS1.p2.11.m11.4.4.2"></times><ci id="S3.SS1.p2.11.m11.4.4.3.cmml" xref="S3.SS1.p2.11.m11.4.4.3">𝑝</ci><apply id="S3.SS1.p2.11.m11.4.4.1.1.1.cmml" xref="S3.SS1.p2.11.m11.4.4.1.1"><csymbol cd="latexml" id="S3.SS1.p2.11.m11.4.4.1.1.1.1.cmml" xref="S3.SS1.p2.11.m11.4.4.1.1.1.1">conditional</csymbol><ci id="S3.SS1.p2.11.m11.4.4.1.1.1.2.cmml" xref="S3.SS1.p2.11.m11.4.4.1.1.1.2">𝐱</ci><list id="S3.SS1.p2.11.m11.4.4.1.1.1.3.1.cmml" xref="S3.SS1.p2.11.m11.4.4.1.1.1.3.2"><ci id="S3.SS1.p2.11.m11.1.1.cmml" xref="S3.SS1.p2.11.m11.1.1">𝐏</ci><ci id="S3.SS1.p2.11.m11.2.2.cmml" xref="S3.SS1.p2.11.m11.2.2">𝑡</ci><ci id="S3.SS1.p2.11.m11.3.3.cmml" xref="S3.SS1.p2.11.m11.3.3">𝐧</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.11.m11.4c">p(\mathbf{x}|\mathbf{P},t,\mathbf{n})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.11.m11.4d">italic_p ( bold_x | bold_P , italic_t , bold_n )</annotation></semantics></math><span class="ltx_text" id="S3.SS1.p2.13.25" style="font-size:90%;"> amounts to first sampling the random noise vector </span><math alttext="\mathbf{n}\sim\mathcal{N}\left(\mathbf{0},\mathbf{I}\right)" class="ltx_Math" display="inline" id="S3.SS1.p2.12.m12.2"><semantics id="S3.SS1.p2.12.m12.2a"><mrow id="S3.SS1.p2.12.m12.2.3" xref="S3.SS1.p2.12.m12.2.3.cmml"><mi id="S3.SS1.p2.12.m12.2.3.2" mathsize="90%" xref="S3.SS1.p2.12.m12.2.3.2.cmml">𝐧</mi><mo id="S3.SS1.p2.12.m12.2.3.1" mathsize="90%" xref="S3.SS1.p2.12.m12.2.3.1.cmml">∼</mo><mrow id="S3.SS1.p2.12.m12.2.3.3" xref="S3.SS1.p2.12.m12.2.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.12.m12.2.3.3.2" mathsize="90%" xref="S3.SS1.p2.12.m12.2.3.3.2.cmml">𝒩</mi><mo id="S3.SS1.p2.12.m12.2.3.3.1" xref="S3.SS1.p2.12.m12.2.3.3.1.cmml">⁢</mo><mrow id="S3.SS1.p2.12.m12.2.3.3.3.2" xref="S3.SS1.p2.12.m12.2.3.3.3.1.cmml"><mo id="S3.SS1.p2.12.m12.2.3.3.3.2.1" xref="S3.SS1.p2.12.m12.2.3.3.3.1.cmml">(</mo><mn id="S3.SS1.p2.12.m12.1.1" mathsize="90%" xref="S3.SS1.p2.12.m12.1.1.cmml">𝟎</mn><mo id="S3.SS1.p2.12.m12.2.3.3.3.2.2" mathsize="90%" xref="S3.SS1.p2.12.m12.2.3.3.3.1.cmml">,</mo><mi id="S3.SS1.p2.12.m12.2.2" mathsize="90%" xref="S3.SS1.p2.12.m12.2.2.cmml">𝐈</mi><mo id="S3.SS1.p2.12.m12.2.3.3.3.2.3" xref="S3.SS1.p2.12.m12.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.12.m12.2b"><apply id="S3.SS1.p2.12.m12.2.3.cmml" xref="S3.SS1.p2.12.m12.2.3"><csymbol cd="latexml" id="S3.SS1.p2.12.m12.2.3.1.cmml" xref="S3.SS1.p2.12.m12.2.3.1">similar-to</csymbol><ci id="S3.SS1.p2.12.m12.2.3.2.cmml" xref="S3.SS1.p2.12.m12.2.3.2">𝐧</ci><apply id="S3.SS1.p2.12.m12.2.3.3.cmml" xref="S3.SS1.p2.12.m12.2.3.3"><times id="S3.SS1.p2.12.m12.2.3.3.1.cmml" xref="S3.SS1.p2.12.m12.2.3.3.1"></times><ci id="S3.SS1.p2.12.m12.2.3.3.2.cmml" xref="S3.SS1.p2.12.m12.2.3.3.2">𝒩</ci><interval closure="open" id="S3.SS1.p2.12.m12.2.3.3.3.1.cmml" xref="S3.SS1.p2.12.m12.2.3.3.3.2"><cn id="S3.SS1.p2.12.m12.1.1.cmml" type="integer" xref="S3.SS1.p2.12.m12.1.1">0</cn><ci id="S3.SS1.p2.12.m12.2.2.cmml" xref="S3.SS1.p2.12.m12.2.2">𝐈</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.12.m12.2c">\mathbf{n}\sim\mathcal{N}\left(\mathbf{0},\mathbf{I}\right)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.12.m12.2d">bold_n ∼ caligraphic_N ( bold_0 , bold_I )</annotation></semantics></math><span class="ltx_text" id="S3.SS1.p2.13.26" style="font-size:90%;"> and then applying our diffusion model </span><math alttext="\mathbf{x}=g(\mathbf{P},t,\mathbf{n})" class="ltx_Math" display="inline" id="S3.SS1.p2.13.m13.3"><semantics id="S3.SS1.p2.13.m13.3a"><mrow id="S3.SS1.p2.13.m13.3.4" xref="S3.SS1.p2.13.m13.3.4.cmml"><mi id="S3.SS1.p2.13.m13.3.4.2" mathsize="90%" xref="S3.SS1.p2.13.m13.3.4.2.cmml">𝐱</mi><mo id="S3.SS1.p2.13.m13.3.4.1" mathsize="90%" xref="S3.SS1.p2.13.m13.3.4.1.cmml">=</mo><mrow id="S3.SS1.p2.13.m13.3.4.3" xref="S3.SS1.p2.13.m13.3.4.3.cmml"><mi id="S3.SS1.p2.13.m13.3.4.3.2" mathsize="90%" xref="S3.SS1.p2.13.m13.3.4.3.2.cmml">g</mi><mo id="S3.SS1.p2.13.m13.3.4.3.1" xref="S3.SS1.p2.13.m13.3.4.3.1.cmml">⁢</mo><mrow id="S3.SS1.p2.13.m13.3.4.3.3.2" xref="S3.SS1.p2.13.m13.3.4.3.3.1.cmml"><mo id="S3.SS1.p2.13.m13.3.4.3.3.2.1" maxsize="90%" minsize="90%" xref="S3.SS1.p2.13.m13.3.4.3.3.1.cmml">(</mo><mi id="S3.SS1.p2.13.m13.1.1" mathsize="90%" xref="S3.SS1.p2.13.m13.1.1.cmml">𝐏</mi><mo id="S3.SS1.p2.13.m13.3.4.3.3.2.2" mathsize="90%" xref="S3.SS1.p2.13.m13.3.4.3.3.1.cmml">,</mo><mi id="S3.SS1.p2.13.m13.2.2" mathsize="90%" xref="S3.SS1.p2.13.m13.2.2.cmml">t</mi><mo id="S3.SS1.p2.13.m13.3.4.3.3.2.3" mathsize="90%" xref="S3.SS1.p2.13.m13.3.4.3.3.1.cmml">,</mo><mi id="S3.SS1.p2.13.m13.3.3" mathsize="90%" xref="S3.SS1.p2.13.m13.3.3.cmml">𝐧</mi><mo id="S3.SS1.p2.13.m13.3.4.3.3.2.4" maxsize="90%" minsize="90%" xref="S3.SS1.p2.13.m13.3.4.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.13.m13.3b"><apply id="S3.SS1.p2.13.m13.3.4.cmml" xref="S3.SS1.p2.13.m13.3.4"><eq id="S3.SS1.p2.13.m13.3.4.1.cmml" xref="S3.SS1.p2.13.m13.3.4.1"></eq><ci id="S3.SS1.p2.13.m13.3.4.2.cmml" xref="S3.SS1.p2.13.m13.3.4.2">𝐱</ci><apply id="S3.SS1.p2.13.m13.3.4.3.cmml" xref="S3.SS1.p2.13.m13.3.4.3"><times id="S3.SS1.p2.13.m13.3.4.3.1.cmml" xref="S3.SS1.p2.13.m13.3.4.3.1"></times><ci id="S3.SS1.p2.13.m13.3.4.3.2.cmml" xref="S3.SS1.p2.13.m13.3.4.3.2">𝑔</ci><vector id="S3.SS1.p2.13.m13.3.4.3.3.1.cmml" xref="S3.SS1.p2.13.m13.3.4.3.3.2"><ci id="S3.SS1.p2.13.m13.1.1.cmml" xref="S3.SS1.p2.13.m13.1.1">𝐏</ci><ci id="S3.SS1.p2.13.m13.2.2.cmml" xref="S3.SS1.p2.13.m13.2.2">𝑡</ci><ci id="S3.SS1.p2.13.m13.3.3.cmml" xref="S3.SS1.p2.13.m13.3.3">𝐧</ci></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.13.m13.3c">\mathbf{x}=g(\mathbf{P},t,\mathbf{n})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.13.m13.3d">bold_x = italic_g ( bold_P , italic_t , bold_n )</annotation></semantics></math><span class="ltx_text" id="S3.SS1.p2.13.27" style="font-size:90%;">.</span></p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.3"><span class="ltx_text" id="S3.SS1.p3.3.1" style="font-size:90%;">For controlled evaluation, we compare performance between generated images that differ only by our attribute of interest.
We achieve this by using text prompts </span><math alttext="t^{\text{base}}" class="ltx_Math" display="inline" id="S3.SS1.p3.1.m1.1"><semantics id="S3.SS1.p3.1.m1.1a"><msup id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><mi id="S3.SS1.p3.1.m1.1.1.2" mathsize="90%" xref="S3.SS1.p3.1.m1.1.1.2.cmml">t</mi><mtext id="S3.SS1.p3.1.m1.1.1.3" mathsize="90%" xref="S3.SS1.p3.1.m1.1.1.3a.cmml">base</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">superscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2">𝑡</ci><ci id="S3.SS1.p3.1.m1.1.1.3a.cmml" xref="S3.SS1.p3.1.m1.1.1.3"><mtext id="S3.SS1.p3.1.m1.1.1.3.cmml" mathsize="63%" xref="S3.SS1.p3.1.m1.1.1.3">base</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">t^{\text{base}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.1.m1.1d">italic_t start_POSTSUPERSCRIPT base end_POSTSUPERSCRIPT</annotation></semantics></math><span class="ltx_text" id="S3.SS1.p3.3.2" style="font-size:90%;"> and </span><math alttext="t^{\text{att}}" class="ltx_Math" display="inline" id="S3.SS1.p3.2.m2.1"><semantics id="S3.SS1.p3.2.m2.1a"><msup id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml"><mi id="S3.SS1.p3.2.m2.1.1.2" mathsize="90%" xref="S3.SS1.p3.2.m2.1.1.2.cmml">t</mi><mtext id="S3.SS1.p3.2.m2.1.1.3" mathsize="90%" xref="S3.SS1.p3.2.m2.1.1.3a.cmml">att</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">superscript</csymbol><ci id="S3.SS1.p3.2.m2.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.2">𝑡</ci><ci id="S3.SS1.p3.2.m2.1.1.3a.cmml" xref="S3.SS1.p3.2.m2.1.1.3"><mtext id="S3.SS1.p3.2.m2.1.1.3.cmml" mathsize="63%" xref="S3.SS1.p3.2.m2.1.1.3">att</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">t^{\text{att}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.2.m2.1d">italic_t start_POSTSUPERSCRIPT att end_POSTSUPERSCRIPT</annotation></semantics></math><span class="ltx_text" id="S3.SS1.p3.3.3" style="font-size:90%;"> that only differ by a single attribute description and using the same noise to generate both images. Note that </span><math alttext="t^{\text{base}}" class="ltx_Math" display="inline" id="S3.SS1.p3.3.m3.1"><semantics id="S3.SS1.p3.3.m3.1a"><msup id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml"><mi id="S3.SS1.p3.3.m3.1.1.2" mathsize="90%" xref="S3.SS1.p3.3.m3.1.1.2.cmml">t</mi><mtext id="S3.SS1.p3.3.m3.1.1.3" mathsize="90%" xref="S3.SS1.p3.3.m3.1.1.3a.cmml">base</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><apply id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.3.m3.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">superscript</csymbol><ci id="S3.SS1.p3.3.m3.1.1.2.cmml" xref="S3.SS1.p3.3.m3.1.1.2">𝑡</ci><ci id="S3.SS1.p3.3.m3.1.1.3a.cmml" xref="S3.SS1.p3.3.m3.1.1.3"><mtext id="S3.SS1.p3.3.m3.1.1.3.cmml" mathsize="63%" xref="S3.SS1.p3.3.m3.1.1.3">base</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">t^{\text{base}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.3.m3.1d">italic_t start_POSTSUPERSCRIPT base end_POSTSUPERSCRIPT</annotation></semantics></math><span class="ltx_text" id="S3.SS1.p3.3.4" style="font-size:90%;"> is constructed to describe a generic, most commonly occurring human appearance and scene, which in principle should not present a challenge for the pose estimator.
We refer to the resulting sets of images as </span><span class="ltx_text ltx_font_italic" id="S3.SS1.p3.3.5" style="font-size:90%;">base set</span><span class="ltx_text" id="S3.SS1.p3.3.6" style="font-size:90%;"> and </span><span class="ltx_text ltx_font_italic" id="S3.SS1.p3.3.7" style="font-size:90%;">attribute set</span><span class="ltx_text" id="S3.SS1.p3.3.8" style="font-size:90%;">.
To compare the performance, we choose an evaluation function (</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3.SS4" style="font-size:90%;" title="3.4 Evaluation Protocol ‣ 3 Method ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.4</span></a><span class="ltx_text" id="S3.SS1.p3.3.9" style="font-size:90%;">) and estimate the risk</span></p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\footnotesize R_{f}\left\lparen t^{\text{att}}\right\rparen=\mathbb{E}_{%
\mathbf{n},\mathbf{P}}\left[L\biggl{(}f\Bigl{(}g\left\lparen\mathbf{P},t^{%
\text{att}},\mathbf{n}\right\rparen\Bigr{)},\ f\Bigl{(}g\left\lparen\mathbf{P}%
,t^{\text{base}},\mathbf{n}\right\rparen\Bigr{)},\ \mathbf{P}\biggr{)}\right]." class="ltx_Math" display="block" id="S3.E2.m1.8"><semantics id="S3.E2.m1.8a"><mrow id="S3.E2.m1.8.8.1" xref="S3.E2.m1.8.8.1.1.cmml"><mrow id="S3.E2.m1.8.8.1.1" xref="S3.E2.m1.8.8.1.1.cmml"><mrow id="S3.E2.m1.8.8.1.1.1" xref="S3.E2.m1.8.8.1.1.1.cmml"><msub id="S3.E2.m1.8.8.1.1.1.3" xref="S3.E2.m1.8.8.1.1.1.3.cmml"><mi id="S3.E2.m1.8.8.1.1.1.3.2" mathsize="80%" xref="S3.E2.m1.8.8.1.1.1.3.2.cmml">R</mi><mi id="S3.E2.m1.8.8.1.1.1.3.3" mathsize="80%" xref="S3.E2.m1.8.8.1.1.1.3.3.cmml">f</mi></msub><mo id="S3.E2.m1.8.8.1.1.1.2" xref="S3.E2.m1.8.8.1.1.1.2.cmml">⁢</mo><mrow id="S3.E2.m1.8.8.1.1.1.1.1" xref="S3.E2.m1.8.8.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.8.8.1.1.1.1.1.2" xref="S3.E2.m1.8.8.1.1.1.1.1.1.cmml">(</mo><msup id="S3.E2.m1.8.8.1.1.1.1.1.1" xref="S3.E2.m1.8.8.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.8.8.1.1.1.1.1.1.2" mathsize="80%" xref="S3.E2.m1.8.8.1.1.1.1.1.1.2.cmml">t</mi><mtext id="S3.E2.m1.8.8.1.1.1.1.1.1.3" mathsize="80%" xref="S3.E2.m1.8.8.1.1.1.1.1.1.3a.cmml">att</mtext></msup><mo id="S3.E2.m1.8.8.1.1.1.1.1.3" xref="S3.E2.m1.8.8.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.8.8.1.1.3" mathsize="80%" xref="S3.E2.m1.8.8.1.1.3.cmml">=</mo><mrow id="S3.E2.m1.8.8.1.1.2" xref="S3.E2.m1.8.8.1.1.2.cmml"><msub id="S3.E2.m1.8.8.1.1.2.3" xref="S3.E2.m1.8.8.1.1.2.3.cmml"><mi id="S3.E2.m1.8.8.1.1.2.3.2" mathsize="80%" xref="S3.E2.m1.8.8.1.1.2.3.2.cmml">𝔼</mi><mrow id="S3.E2.m1.2.2.2.4" xref="S3.E2.m1.2.2.2.3.cmml"><mi id="S3.E2.m1.1.1.1.1" mathsize="80%" xref="S3.E2.m1.1.1.1.1.cmml">𝐧</mi><mo id="S3.E2.m1.2.2.2.4.1" mathsize="80%" xref="S3.E2.m1.2.2.2.3.cmml">,</mo><mi id="S3.E2.m1.2.2.2.2" mathsize="80%" xref="S3.E2.m1.2.2.2.2.cmml">𝐏</mi></mrow></msub><mo id="S3.E2.m1.8.8.1.1.2.2" xref="S3.E2.m1.8.8.1.1.2.2.cmml">⁢</mo><mrow id="S3.E2.m1.8.8.1.1.2.1.1" xref="S3.E2.m1.8.8.1.1.2.1.2.cmml"><mo id="S3.E2.m1.8.8.1.1.2.1.1.2" xref="S3.E2.m1.8.8.1.1.2.1.2.1.cmml">[</mo><mrow id="S3.E2.m1.8.8.1.1.2.1.1.1" xref="S3.E2.m1.8.8.1.1.2.1.1.1.cmml"><mi id="S3.E2.m1.8.8.1.1.2.1.1.1.4" mathsize="80%" xref="S3.E2.m1.8.8.1.1.2.1.1.1.4.cmml">L</mi><mo id="S3.E2.m1.8.8.1.1.2.1.1.1.3" xref="S3.E2.m1.8.8.1.1.2.1.1.1.3.cmml">⁢</mo><mrow id="S3.E2.m1.8.8.1.1.2.1.1.1.2.2" xref="S3.E2.m1.8.8.1.1.2.1.1.1.2.3.cmml"><mo id="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.3" maxsize="210%" minsize="210%" xref="S3.E2.m1.8.8.1.1.2.1.1.1.2.3.cmml">(</mo><mrow id="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1" xref="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.3" mathsize="80%" xref="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.3.cmml">f</mi><mo id="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.2" xref="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1" xref="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.2" maxsize="160%" minsize="160%" xref="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.3" mathsize="80%" xref="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.3.cmml">g</mi><mo id="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.1.2.cmml"><mo id="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.1.2.cmml">(</mo><mi id="S3.E2.m1.3.3" mathsize="80%" xref="S3.E2.m1.3.3.cmml">𝐏</mi><mo id="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.1.1.3" mathsize="80%" xref="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.1.2.cmml">,</mo><msup id="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.1.1.1.2" mathsize="80%" xref="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">t</mi><mtext id="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.1.1.1.3" mathsize="80%" xref="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.1.1.1.3a.cmml">att</mtext></msup><mo id="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.1.1.4" mathsize="80%" xref="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.1.2.cmml">,</mo><mi id="S3.E2.m1.4.4" mathsize="80%" xref="S3.E2.m1.4.4.cmml">𝐧</mi><mo id="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.1.1.5" xref="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.3" maxsize="160%" minsize="160%" xref="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.4" mathsize="80%" rspace="0.567em" xref="S3.E2.m1.8.8.1.1.2.1.1.1.2.3.cmml">,</mo><mrow id="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2" xref="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.cmml"><mi id="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.3" mathsize="80%" xref="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.3.cmml">f</mi><mo id="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.2" xref="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.2.cmml">⁢</mo><mrow id="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1" xref="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.cmml"><mo id="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.2" maxsize="160%" minsize="160%" xref="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1" xref="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.cmml"><mi id="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.3" mathsize="80%" xref="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.3.cmml">g</mi><mo id="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.2" xref="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.2.cmml">⁢</mo><mrow id="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.1.1" xref="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.1.2.cmml"><mo id="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.1.1.2" xref="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.1.2.cmml">(</mo><mi id="S3.E2.m1.5.5" mathsize="80%" xref="S3.E2.m1.5.5.cmml">𝐏</mi><mo id="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.1.1.3" mathsize="80%" xref="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.1.2.cmml">,</mo><msup id="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.1.1.1" xref="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.1.1.1.2" mathsize="80%" xref="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.1.1.1.2.cmml">t</mi><mtext id="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.1.1.1.3" mathsize="80%" xref="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.1.1.1.3a.cmml">base</mtext></msup><mo id="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.1.1.4" mathsize="80%" xref="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.1.2.cmml">,</mo><mi id="S3.E2.m1.6.6" mathsize="80%" xref="S3.E2.m1.6.6.cmml">𝐧</mi><mo id="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.1.1.5" xref="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.3" maxsize="160%" minsize="160%" xref="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.5" mathsize="80%" rspace="0.567em" xref="S3.E2.m1.8.8.1.1.2.1.1.1.2.3.cmml">,</mo><mi id="S3.E2.m1.7.7" mathsize="80%" xref="S3.E2.m1.7.7.cmml">𝐏</mi><mo id="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.6" maxsize="210%" minsize="210%" xref="S3.E2.m1.8.8.1.1.2.1.1.1.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.8.8.1.1.2.1.1.3" xref="S3.E2.m1.8.8.1.1.2.1.2.1.cmml">]</mo></mrow></mrow></mrow><mo id="S3.E2.m1.8.8.1.2" lspace="0em" mathsize="80%" xref="S3.E2.m1.8.8.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.8b"><apply id="S3.E2.m1.8.8.1.1.cmml" xref="S3.E2.m1.8.8.1"><eq id="S3.E2.m1.8.8.1.1.3.cmml" xref="S3.E2.m1.8.8.1.1.3"></eq><apply id="S3.E2.m1.8.8.1.1.1.cmml" xref="S3.E2.m1.8.8.1.1.1"><times id="S3.E2.m1.8.8.1.1.1.2.cmml" xref="S3.E2.m1.8.8.1.1.1.2"></times><apply id="S3.E2.m1.8.8.1.1.1.3.cmml" xref="S3.E2.m1.8.8.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.8.8.1.1.1.3.1.cmml" xref="S3.E2.m1.8.8.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.8.8.1.1.1.3.2.cmml" xref="S3.E2.m1.8.8.1.1.1.3.2">𝑅</ci><ci id="S3.E2.m1.8.8.1.1.1.3.3.cmml" xref="S3.E2.m1.8.8.1.1.1.3.3">𝑓</ci></apply><apply id="S3.E2.m1.8.8.1.1.1.1.1.1.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.8.8.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1">superscript</csymbol><ci id="S3.E2.m1.8.8.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.1.2">𝑡</ci><ci id="S3.E2.m1.8.8.1.1.1.1.1.1.3a.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.1.3"><mtext id="S3.E2.m1.8.8.1.1.1.1.1.1.3.cmml" mathsize="56%" xref="S3.E2.m1.8.8.1.1.1.1.1.1.3">att</mtext></ci></apply></apply><apply id="S3.E2.m1.8.8.1.1.2.cmml" xref="S3.E2.m1.8.8.1.1.2"><times id="S3.E2.m1.8.8.1.1.2.2.cmml" xref="S3.E2.m1.8.8.1.1.2.2"></times><apply id="S3.E2.m1.8.8.1.1.2.3.cmml" xref="S3.E2.m1.8.8.1.1.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.8.8.1.1.2.3.1.cmml" xref="S3.E2.m1.8.8.1.1.2.3">subscript</csymbol><ci id="S3.E2.m1.8.8.1.1.2.3.2.cmml" xref="S3.E2.m1.8.8.1.1.2.3.2">𝔼</ci><list id="S3.E2.m1.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.4"><ci id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1">𝐧</ci><ci id="S3.E2.m1.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2">𝐏</ci></list></apply><apply id="S3.E2.m1.8.8.1.1.2.1.2.cmml" xref="S3.E2.m1.8.8.1.1.2.1.1"><csymbol cd="latexml" id="S3.E2.m1.8.8.1.1.2.1.2.1.cmml" xref="S3.E2.m1.8.8.1.1.2.1.1.2">delimited-[]</csymbol><apply id="S3.E2.m1.8.8.1.1.2.1.1.1.cmml" xref="S3.E2.m1.8.8.1.1.2.1.1.1"><times id="S3.E2.m1.8.8.1.1.2.1.1.1.3.cmml" xref="S3.E2.m1.8.8.1.1.2.1.1.1.3"></times><ci id="S3.E2.m1.8.8.1.1.2.1.1.1.4.cmml" xref="S3.E2.m1.8.8.1.1.2.1.1.1.4">𝐿</ci><vector id="S3.E2.m1.8.8.1.1.2.1.1.1.2.3.cmml" xref="S3.E2.m1.8.8.1.1.2.1.1.1.2.2"><apply id="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.cmml" xref="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1"><times id="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.2"></times><ci id="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.3">𝑓</ci><apply id="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1"><times id="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.2"></times><ci id="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.3">𝑔</ci><vector id="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.1.1"><ci id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3">𝐏</ci><apply id="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.1.1.1.2">𝑡</ci><ci id="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.1.1.1.3a.cmml" xref="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.1.1.1.3"><mtext id="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" mathsize="56%" xref="S3.E2.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.1.1.1.3">att</mtext></ci></apply><ci id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4">𝐧</ci></vector></apply></apply><apply id="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.cmml" xref="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2"><times id="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.2.cmml" xref="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.2"></times><ci id="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.3.cmml" xref="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.3">𝑓</ci><apply id="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.cmml" xref="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1"><times id="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.2.cmml" xref="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.2"></times><ci id="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.3.cmml" xref="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.3">𝑔</ci><vector id="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.1.2.cmml" xref="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.1.1"><ci id="S3.E2.m1.5.5.cmml" xref="S3.E2.m1.5.5">𝐏</ci><apply id="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.1.1.1.cmml" xref="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.1.1.1.2">𝑡</ci><ci id="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.1.1.1.3a.cmml" xref="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.1.1.1.3"><mtext id="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.1.1.1.3.cmml" mathsize="56%" xref="S3.E2.m1.8.8.1.1.2.1.1.1.2.2.2.1.1.1.1.1.1.3">base</mtext></ci></apply><ci id="S3.E2.m1.6.6.cmml" xref="S3.E2.m1.6.6">𝐧</ci></vector></apply></apply><ci id="S3.E2.m1.7.7.cmml" xref="S3.E2.m1.7.7">𝐏</ci></vector></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.8c">\footnotesize R_{f}\left\lparen t^{\text{att}}\right\rparen=\mathbb{E}_{%
\mathbf{n},\mathbf{P}}\left[L\biggl{(}f\Bigl{(}g\left\lparen\mathbf{P},t^{%
\text{att}},\mathbf{n}\right\rparen\Bigr{)},\ f\Bigl{(}g\left\lparen\mathbf{P}%
,t^{\text{base}},\mathbf{n}\right\rparen\Bigr{)},\ \mathbf{P}\biggr{)}\right].</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.8d">italic_R start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT ( italic_t start_POSTSUPERSCRIPT att end_POSTSUPERSCRIPT ) = blackboard_E start_POSTSUBSCRIPT bold_n , bold_P end_POSTSUBSCRIPT [ italic_L ( italic_f ( italic_g ( bold_P , italic_t start_POSTSUPERSCRIPT att end_POSTSUPERSCRIPT , bold_n ) ) , italic_f ( italic_g ( bold_P , italic_t start_POSTSUPERSCRIPT base end_POSTSUPERSCRIPT , bold_n ) ) , bold_P ) ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p3.6"><span class="ltx_text" id="S3.SS1.p3.6.1" style="font-size:90%;">In contrast to (</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3.E1" style="font-size:90%;" title="Equation 1 ‣ 3.1 Evaluation with Controlled Generated Test Sets ‣ 3 Method ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">1</span></a><span class="ltx_text" id="S3.SS1.p3.6.2" style="font-size:90%;">), we compare the performance of estimator </span><math alttext="f" class="ltx_Math" display="inline" id="S3.SS1.p3.4.m1.1"><semantics id="S3.SS1.p3.4.m1.1a"><mi id="S3.SS1.p3.4.m1.1.1" mathsize="90%" xref="S3.SS1.p3.4.m1.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m1.1b"><ci id="S3.SS1.p3.4.m1.1.1.cmml" xref="S3.SS1.p3.4.m1.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m1.1c">f</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.4.m1.1d">italic_f</annotation></semantics></math><span class="ltx_text" id="S3.SS1.p3.6.3" style="font-size:90%;"> for synthetic base
</span><math alttext="\mathbf{x}^{\text{base}}=g(\mathbf{P},t^{\text{base}},\mathbf{n})" class="ltx_Math" display="inline" id="S3.SS1.p3.5.m2.3"><semantics id="S3.SS1.p3.5.m2.3a"><mrow id="S3.SS1.p3.5.m2.3.3" xref="S3.SS1.p3.5.m2.3.3.cmml"><msup id="S3.SS1.p3.5.m2.3.3.3" xref="S3.SS1.p3.5.m2.3.3.3.cmml"><mi id="S3.SS1.p3.5.m2.3.3.3.2" mathsize="90%" xref="S3.SS1.p3.5.m2.3.3.3.2.cmml">𝐱</mi><mtext id="S3.SS1.p3.5.m2.3.3.3.3" mathsize="90%" xref="S3.SS1.p3.5.m2.3.3.3.3a.cmml">base</mtext></msup><mo id="S3.SS1.p3.5.m2.3.3.2" mathsize="90%" xref="S3.SS1.p3.5.m2.3.3.2.cmml">=</mo><mrow id="S3.SS1.p3.5.m2.3.3.1" xref="S3.SS1.p3.5.m2.3.3.1.cmml"><mi id="S3.SS1.p3.5.m2.3.3.1.3" mathsize="90%" xref="S3.SS1.p3.5.m2.3.3.1.3.cmml">g</mi><mo id="S3.SS1.p3.5.m2.3.3.1.2" xref="S3.SS1.p3.5.m2.3.3.1.2.cmml">⁢</mo><mrow id="S3.SS1.p3.5.m2.3.3.1.1.1" xref="S3.SS1.p3.5.m2.3.3.1.1.2.cmml"><mo id="S3.SS1.p3.5.m2.3.3.1.1.1.2" maxsize="90%" minsize="90%" xref="S3.SS1.p3.5.m2.3.3.1.1.2.cmml">(</mo><mi id="S3.SS1.p3.5.m2.1.1" mathsize="90%" xref="S3.SS1.p3.5.m2.1.1.cmml">𝐏</mi><mo id="S3.SS1.p3.5.m2.3.3.1.1.1.3" mathsize="90%" xref="S3.SS1.p3.5.m2.3.3.1.1.2.cmml">,</mo><msup id="S3.SS1.p3.5.m2.3.3.1.1.1.1" xref="S3.SS1.p3.5.m2.3.3.1.1.1.1.cmml"><mi id="S3.SS1.p3.5.m2.3.3.1.1.1.1.2" mathsize="90%" xref="S3.SS1.p3.5.m2.3.3.1.1.1.1.2.cmml">t</mi><mtext id="S3.SS1.p3.5.m2.3.3.1.1.1.1.3" mathsize="90%" xref="S3.SS1.p3.5.m2.3.3.1.1.1.1.3a.cmml">base</mtext></msup><mo id="S3.SS1.p3.5.m2.3.3.1.1.1.4" mathsize="90%" xref="S3.SS1.p3.5.m2.3.3.1.1.2.cmml">,</mo><mi id="S3.SS1.p3.5.m2.2.2" mathsize="90%" xref="S3.SS1.p3.5.m2.2.2.cmml">𝐧</mi><mo id="S3.SS1.p3.5.m2.3.3.1.1.1.5" maxsize="90%" minsize="90%" xref="S3.SS1.p3.5.m2.3.3.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.m2.3b"><apply id="S3.SS1.p3.5.m2.3.3.cmml" xref="S3.SS1.p3.5.m2.3.3"><eq id="S3.SS1.p3.5.m2.3.3.2.cmml" xref="S3.SS1.p3.5.m2.3.3.2"></eq><apply id="S3.SS1.p3.5.m2.3.3.3.cmml" xref="S3.SS1.p3.5.m2.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p3.5.m2.3.3.3.1.cmml" xref="S3.SS1.p3.5.m2.3.3.3">superscript</csymbol><ci id="S3.SS1.p3.5.m2.3.3.3.2.cmml" xref="S3.SS1.p3.5.m2.3.3.3.2">𝐱</ci><ci id="S3.SS1.p3.5.m2.3.3.3.3a.cmml" xref="S3.SS1.p3.5.m2.3.3.3.3"><mtext id="S3.SS1.p3.5.m2.3.3.3.3.cmml" mathsize="63%" xref="S3.SS1.p3.5.m2.3.3.3.3">base</mtext></ci></apply><apply id="S3.SS1.p3.5.m2.3.3.1.cmml" xref="S3.SS1.p3.5.m2.3.3.1"><times id="S3.SS1.p3.5.m2.3.3.1.2.cmml" xref="S3.SS1.p3.5.m2.3.3.1.2"></times><ci id="S3.SS1.p3.5.m2.3.3.1.3.cmml" xref="S3.SS1.p3.5.m2.3.3.1.3">𝑔</ci><vector id="S3.SS1.p3.5.m2.3.3.1.1.2.cmml" xref="S3.SS1.p3.5.m2.3.3.1.1.1"><ci id="S3.SS1.p3.5.m2.1.1.cmml" xref="S3.SS1.p3.5.m2.1.1">𝐏</ci><apply id="S3.SS1.p3.5.m2.3.3.1.1.1.1.cmml" xref="S3.SS1.p3.5.m2.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.5.m2.3.3.1.1.1.1.1.cmml" xref="S3.SS1.p3.5.m2.3.3.1.1.1.1">superscript</csymbol><ci id="S3.SS1.p3.5.m2.3.3.1.1.1.1.2.cmml" xref="S3.SS1.p3.5.m2.3.3.1.1.1.1.2">𝑡</ci><ci id="S3.SS1.p3.5.m2.3.3.1.1.1.1.3a.cmml" xref="S3.SS1.p3.5.m2.3.3.1.1.1.1.3"><mtext id="S3.SS1.p3.5.m2.3.3.1.1.1.1.3.cmml" mathsize="63%" xref="S3.SS1.p3.5.m2.3.3.1.1.1.1.3">base</mtext></ci></apply><ci id="S3.SS1.p3.5.m2.2.2.cmml" xref="S3.SS1.p3.5.m2.2.2">𝐧</ci></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.5.m2.3c">\mathbf{x}^{\text{base}}=g(\mathbf{P},t^{\text{base}},\mathbf{n})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.5.m2.3d">bold_x start_POSTSUPERSCRIPT base end_POSTSUPERSCRIPT = italic_g ( bold_P , italic_t start_POSTSUPERSCRIPT base end_POSTSUPERSCRIPT , bold_n )</annotation></semantics></math><span class="ltx_text" id="S3.SS1.p3.6.4" style="font-size:90%;"> and attribute images </span><math alttext="\mathbf{x}^{\text{att}}=g(\mathbf{P},t^{\text{att}},\mathbf{n})" class="ltx_Math" display="inline" id="S3.SS1.p3.6.m3.3"><semantics id="S3.SS1.p3.6.m3.3a"><mrow id="S3.SS1.p3.6.m3.3.3" xref="S3.SS1.p3.6.m3.3.3.cmml"><msup id="S3.SS1.p3.6.m3.3.3.3" xref="S3.SS1.p3.6.m3.3.3.3.cmml"><mi id="S3.SS1.p3.6.m3.3.3.3.2" mathsize="90%" xref="S3.SS1.p3.6.m3.3.3.3.2.cmml">𝐱</mi><mtext id="S3.SS1.p3.6.m3.3.3.3.3" mathsize="90%" xref="S3.SS1.p3.6.m3.3.3.3.3a.cmml">att</mtext></msup><mo id="S3.SS1.p3.6.m3.3.3.2" mathsize="90%" xref="S3.SS1.p3.6.m3.3.3.2.cmml">=</mo><mrow id="S3.SS1.p3.6.m3.3.3.1" xref="S3.SS1.p3.6.m3.3.3.1.cmml"><mi id="S3.SS1.p3.6.m3.3.3.1.3" mathsize="90%" xref="S3.SS1.p3.6.m3.3.3.1.3.cmml">g</mi><mo id="S3.SS1.p3.6.m3.3.3.1.2" xref="S3.SS1.p3.6.m3.3.3.1.2.cmml">⁢</mo><mrow id="S3.SS1.p3.6.m3.3.3.1.1.1" xref="S3.SS1.p3.6.m3.3.3.1.1.2.cmml"><mo id="S3.SS1.p3.6.m3.3.3.1.1.1.2" maxsize="90%" minsize="90%" xref="S3.SS1.p3.6.m3.3.3.1.1.2.cmml">(</mo><mi id="S3.SS1.p3.6.m3.1.1" mathsize="90%" xref="S3.SS1.p3.6.m3.1.1.cmml">𝐏</mi><mo id="S3.SS1.p3.6.m3.3.3.1.1.1.3" mathsize="90%" xref="S3.SS1.p3.6.m3.3.3.1.1.2.cmml">,</mo><msup id="S3.SS1.p3.6.m3.3.3.1.1.1.1" xref="S3.SS1.p3.6.m3.3.3.1.1.1.1.cmml"><mi id="S3.SS1.p3.6.m3.3.3.1.1.1.1.2" mathsize="90%" xref="S3.SS1.p3.6.m3.3.3.1.1.1.1.2.cmml">t</mi><mtext id="S3.SS1.p3.6.m3.3.3.1.1.1.1.3" mathsize="90%" xref="S3.SS1.p3.6.m3.3.3.1.1.1.1.3a.cmml">att</mtext></msup><mo id="S3.SS1.p3.6.m3.3.3.1.1.1.4" mathsize="90%" xref="S3.SS1.p3.6.m3.3.3.1.1.2.cmml">,</mo><mi id="S3.SS1.p3.6.m3.2.2" mathsize="90%" xref="S3.SS1.p3.6.m3.2.2.cmml">𝐧</mi><mo id="S3.SS1.p3.6.m3.3.3.1.1.1.5" maxsize="90%" minsize="90%" xref="S3.SS1.p3.6.m3.3.3.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.6.m3.3b"><apply id="S3.SS1.p3.6.m3.3.3.cmml" xref="S3.SS1.p3.6.m3.3.3"><eq id="S3.SS1.p3.6.m3.3.3.2.cmml" xref="S3.SS1.p3.6.m3.3.3.2"></eq><apply id="S3.SS1.p3.6.m3.3.3.3.cmml" xref="S3.SS1.p3.6.m3.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p3.6.m3.3.3.3.1.cmml" xref="S3.SS1.p3.6.m3.3.3.3">superscript</csymbol><ci id="S3.SS1.p3.6.m3.3.3.3.2.cmml" xref="S3.SS1.p3.6.m3.3.3.3.2">𝐱</ci><ci id="S3.SS1.p3.6.m3.3.3.3.3a.cmml" xref="S3.SS1.p3.6.m3.3.3.3.3"><mtext id="S3.SS1.p3.6.m3.3.3.3.3.cmml" mathsize="63%" xref="S3.SS1.p3.6.m3.3.3.3.3">att</mtext></ci></apply><apply id="S3.SS1.p3.6.m3.3.3.1.cmml" xref="S3.SS1.p3.6.m3.3.3.1"><times id="S3.SS1.p3.6.m3.3.3.1.2.cmml" xref="S3.SS1.p3.6.m3.3.3.1.2"></times><ci id="S3.SS1.p3.6.m3.3.3.1.3.cmml" xref="S3.SS1.p3.6.m3.3.3.1.3">𝑔</ci><vector id="S3.SS1.p3.6.m3.3.3.1.1.2.cmml" xref="S3.SS1.p3.6.m3.3.3.1.1.1"><ci id="S3.SS1.p3.6.m3.1.1.cmml" xref="S3.SS1.p3.6.m3.1.1">𝐏</ci><apply id="S3.SS1.p3.6.m3.3.3.1.1.1.1.cmml" xref="S3.SS1.p3.6.m3.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.6.m3.3.3.1.1.1.1.1.cmml" xref="S3.SS1.p3.6.m3.3.3.1.1.1.1">superscript</csymbol><ci id="S3.SS1.p3.6.m3.3.3.1.1.1.1.2.cmml" xref="S3.SS1.p3.6.m3.3.3.1.1.1.1.2">𝑡</ci><ci id="S3.SS1.p3.6.m3.3.3.1.1.1.1.3a.cmml" xref="S3.SS1.p3.6.m3.3.3.1.1.1.1.3"><mtext id="S3.SS1.p3.6.m3.3.3.1.1.1.1.3.cmml" mathsize="63%" xref="S3.SS1.p3.6.m3.3.3.1.1.1.1.3">att</mtext></ci></apply><ci id="S3.SS1.p3.6.m3.2.2.cmml" xref="S3.SS1.p3.6.m3.2.2">𝐧</ci></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.6.m3.3c">\mathbf{x}^{\text{att}}=g(\mathbf{P},t^{\text{att}},\mathbf{n})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.6.m3.3d">bold_x start_POSTSUPERSCRIPT att end_POSTSUPERSCRIPT = italic_g ( bold_P , italic_t start_POSTSUPERSCRIPT att end_POSTSUPERSCRIPT , bold_n )</annotation></semantics></math><span class="ltx_text" id="S3.SS1.p3.6.5" style="font-size:90%;"> on the average. Using pairs of synthetic images reduces the estimation noise caused by generation flaws. Computing the risk over a large number of pairs reduces the estimation noise caused by random factors. In </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S2.F1" style="font-size:90%;" title="In Generating images ‣ B.4 Generating Data for Attribute Experiments ‣ B Implementation Details ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">S1</span></a><span class="ltx_text" id="S3.SS1.p3.6.6" style="font-size:90%;"> we show that we use a sufficient amount of samples.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Pose-Conditioned Image Generation</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.3"><span class="ltx_text" id="S3.SS2.p1.3.1" style="font-size:90%;">To condition, we render a depth map from the SMPL body model in a desired pose </span><math alttext="\theta\in\operatorname{SO}\left\lparen 3\right\rparen^{24}" class="ltx_math_unparsed" display="inline" id="S3.SS2.p1.1.m1.2"><semantics id="S3.SS2.p1.1.m1.2a"><mrow id="S3.SS2.p1.1.m1.2b"><mi id="S3.SS2.p1.1.m1.2.3" mathsize="90%">θ</mi><mo id="S3.SS2.p1.1.m1.2.4" mathsize="90%">∈</mo><mi id="S3.SS2.p1.1.m1.1.1" mathsize="90%">SO</mi><msup id="S3.SS2.p1.1.m1.2.5"><mrow id="S3.SS2.p1.1.m1.2.5.2"><mo id="S3.SS2.p1.1.m1.2.5.2.1">(</mo><mn id="S3.SS2.p1.1.m1.2.2" mathsize="90%">3</mn><mo id="S3.SS2.p1.1.m1.2.5.2.2">)</mo></mrow><mn id="S3.SS2.p1.1.m1.2.5.3" mathsize="90%">24</mn></msup></mrow><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.2c">\theta\in\operatorname{SO}\left\lparen 3\right\rparen^{24}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.2d">italic_θ ∈ roman_SO ( 3 ) start_POSTSUPERSCRIPT 24 end_POSTSUPERSCRIPT</annotation></semantics></math><span class="ltx_text" id="S3.SS2.p1.3.2" style="font-size:90%;"> and given shape. The model </span><math alttext="g(\mathbf{P},\mathbf{n},t)" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.3"><semantics id="S3.SS2.p1.2.m2.3a"><mrow id="S3.SS2.p1.2.m2.3.4" xref="S3.SS2.p1.2.m2.3.4.cmml"><mi id="S3.SS2.p1.2.m2.3.4.2" mathsize="90%" xref="S3.SS2.p1.2.m2.3.4.2.cmml">g</mi><mo id="S3.SS2.p1.2.m2.3.4.1" xref="S3.SS2.p1.2.m2.3.4.1.cmml">⁢</mo><mrow id="S3.SS2.p1.2.m2.3.4.3.2" xref="S3.SS2.p1.2.m2.3.4.3.1.cmml"><mo id="S3.SS2.p1.2.m2.3.4.3.2.1" maxsize="90%" minsize="90%" xref="S3.SS2.p1.2.m2.3.4.3.1.cmml">(</mo><mi id="S3.SS2.p1.2.m2.1.1" mathsize="90%" xref="S3.SS2.p1.2.m2.1.1.cmml">𝐏</mi><mo id="S3.SS2.p1.2.m2.3.4.3.2.2" mathsize="90%" xref="S3.SS2.p1.2.m2.3.4.3.1.cmml">,</mo><mi id="S3.SS2.p1.2.m2.2.2" mathsize="90%" xref="S3.SS2.p1.2.m2.2.2.cmml">𝐧</mi><mo id="S3.SS2.p1.2.m2.3.4.3.2.3" mathsize="90%" xref="S3.SS2.p1.2.m2.3.4.3.1.cmml">,</mo><mi id="S3.SS2.p1.2.m2.3.3" mathsize="90%" xref="S3.SS2.p1.2.m2.3.3.cmml">t</mi><mo id="S3.SS2.p1.2.m2.3.4.3.2.4" maxsize="90%" minsize="90%" xref="S3.SS2.p1.2.m2.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.3b"><apply id="S3.SS2.p1.2.m2.3.4.cmml" xref="S3.SS2.p1.2.m2.3.4"><times id="S3.SS2.p1.2.m2.3.4.1.cmml" xref="S3.SS2.p1.2.m2.3.4.1"></times><ci id="S3.SS2.p1.2.m2.3.4.2.cmml" xref="S3.SS2.p1.2.m2.3.4.2">𝑔</ci><vector id="S3.SS2.p1.2.m2.3.4.3.1.cmml" xref="S3.SS2.p1.2.m2.3.4.3.2"><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">𝐏</ci><ci id="S3.SS2.p1.2.m2.2.2.cmml" xref="S3.SS2.p1.2.m2.2.2">𝐧</ci><ci id="S3.SS2.p1.2.m2.3.3.cmml" xref="S3.SS2.p1.2.m2.3.3">𝑡</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.3c">g(\mathbf{P},\mathbf{n},t)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.3d">italic_g ( bold_P , bold_n , italic_t )</annotation></semantics></math><span class="ltx_text" id="S3.SS2.p1.3.3" style="font-size:90%;"> has to meet two requirements.
First, it must provide accurate </span><span class="ltx_text ltx_font_italic" id="S3.SS2.p1.3.4" style="font-size:90%;">3D pose alignment</span><span class="ltx_text" id="S3.SS2.p1.3.5" style="font-size:90%;"> between the condition and the generated images.
Second, the effect of the conditioning pose and text prompt has to be disentangled.
Meaning the pose should not influence their shape or clothing.
Our starting point to realize </span><math alttext="g(\mathbf{P},\mathbf{n},t)" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3.3"><semantics id="S3.SS2.p1.3.m3.3a"><mrow id="S3.SS2.p1.3.m3.3.4" xref="S3.SS2.p1.3.m3.3.4.cmml"><mi id="S3.SS2.p1.3.m3.3.4.2" mathsize="90%" xref="S3.SS2.p1.3.m3.3.4.2.cmml">g</mi><mo id="S3.SS2.p1.3.m3.3.4.1" xref="S3.SS2.p1.3.m3.3.4.1.cmml">⁢</mo><mrow id="S3.SS2.p1.3.m3.3.4.3.2" xref="S3.SS2.p1.3.m3.3.4.3.1.cmml"><mo id="S3.SS2.p1.3.m3.3.4.3.2.1" maxsize="90%" minsize="90%" xref="S3.SS2.p1.3.m3.3.4.3.1.cmml">(</mo><mi id="S3.SS2.p1.3.m3.1.1" mathsize="90%" xref="S3.SS2.p1.3.m3.1.1.cmml">𝐏</mi><mo id="S3.SS2.p1.3.m3.3.4.3.2.2" mathsize="90%" xref="S3.SS2.p1.3.m3.3.4.3.1.cmml">,</mo><mi id="S3.SS2.p1.3.m3.2.2" mathsize="90%" xref="S3.SS2.p1.3.m3.2.2.cmml">𝐧</mi><mo id="S3.SS2.p1.3.m3.3.4.3.2.3" mathsize="90%" xref="S3.SS2.p1.3.m3.3.4.3.1.cmml">,</mo><mi id="S3.SS2.p1.3.m3.3.3" mathsize="90%" xref="S3.SS2.p1.3.m3.3.3.cmml">t</mi><mo id="S3.SS2.p1.3.m3.3.4.3.2.4" maxsize="90%" minsize="90%" xref="S3.SS2.p1.3.m3.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.3b"><apply id="S3.SS2.p1.3.m3.3.4.cmml" xref="S3.SS2.p1.3.m3.3.4"><times id="S3.SS2.p1.3.m3.3.4.1.cmml" xref="S3.SS2.p1.3.m3.3.4.1"></times><ci id="S3.SS2.p1.3.m3.3.4.2.cmml" xref="S3.SS2.p1.3.m3.3.4.2">𝑔</ci><vector id="S3.SS2.p1.3.m3.3.4.3.1.cmml" xref="S3.SS2.p1.3.m3.3.4.3.2"><ci id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">𝐏</ci><ci id="S3.SS2.p1.3.m3.2.2.cmml" xref="S3.SS2.p1.3.m3.2.2">𝐧</ci><ci id="S3.SS2.p1.3.m3.3.3.cmml" xref="S3.SS2.p1.3.m3.3.3">𝑡</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.3c">g(\mathbf{P},\mathbf{n},t)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.3.m3.3d">italic_g ( bold_P , bold_n , italic_t )</annotation></semantics></math><span class="ltx_text" id="S3.SS2.p1.3.6" style="font-size:90%;"> is ControlNet (CN) </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS2.p1.3.7.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib83" title=""><span class="ltx_text" style="font-size:90%;">83</span></a><span class="ltx_text" id="S3.SS2.p1.3.8.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S3.SS2.p1.3.9" style="font-size:90%;">.
Zhang </span><em class="ltx_emph ltx_font_italic" id="S3.SS2.p1.3.10" style="font-size:90%;">et al</em><span class="ltx_text" id="S3.SS2.p1.3.11" style="font-size:90%;">.</span><span class="ltx_text" id="S3.SS2.p1.3.12"></span><span class="ltx_text" id="S3.SS2.p1.3.13" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS2.p1.3.14.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib83" title=""><span class="ltx_text" style="font-size:90%;">83</span></a><span class="ltx_text" id="S3.SS2.p1.3.15.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S3.SS2.p1.3.16" style="font-size:90%;"> provide two models referred to as CN-Pose and CN-Depth.
However, these models do not meet the desired requirements.</span></p>
</div>
<figure class="ltx_figure" id="S3.F3">
<table class="ltx_tabular ltx_align_middle" id="S3.F3.20">
<tr class="ltx_tr" id="S3.F3.20.21">
<td class="ltx_td ltx_align_right" id="S3.F3.20.21.1">
<span class="ltx_ERROR undefined" id="S3.F3.20.21.1.1">\pbox</span><span class="ltx_text" id="S3.F3.20.21.1.2" style="font-size:90%;">0.18Ground Truth Pose</span>
</td>
<td class="ltx_td ltx_align_right" id="S3.F3.20.21.2">
<span class="ltx_ERROR undefined" id="S3.F3.20.21.2.1">\pbox</span><span class="ltx_text" id="S3.F3.20.21.2.2" style="font-size:90%;">0.18High BMI</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.F3.20.21.3"><span class="ltx_text" id="S3.F3.20.21.3.1" style="font-size:90%;">Floral Texture</span></td>
<td class="ltx_td ltx_align_center" id="S3.F3.20.21.4"><span class="ltx_text" id="S3.F3.20.21.4.1" style="font-size:90%;">Trench Coat</span></td>
<td class="ltx_td ltx_align_center" id="S3.F3.20.21.5"><span class="ltx_text" id="S3.F3.20.21.5.1" style="font-size:90%;">Wine Cellar</span></td>
</tr>
<tr class="ltx_tr" id="S3.F3.5.5">
<td class="ltx_td ltx_align_center" id="S3.F3.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F3.1.1.1.g1" src="extracted/5819071/data/gallery/row-1/gt-1059.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S3.F3.2.2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F3.2.2.2.g1" src="extracted/5819071/data/gallery/row-1/1.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S3.F3.3.3.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F3.3.3.3.g1" src="extracted/5819071/data/gallery/row-1/2.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S3.F3.4.4.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F3.4.4.4.g1" src="extracted/5819071/data/gallery/row-1/3.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S3.F3.5.5.5"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F3.5.5.5.g1" src="extracted/5819071/data/gallery/row-1/4.png" width="108"/></td>
</tr>
<tr class="ltx_tr" id="S3.F3.10.10">
<td class="ltx_td ltx_align_center" id="S3.F3.6.6.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F3.6.6.1.g1" src="extracted/5819071/data/gallery/row-2/gt-968.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S3.F3.7.7.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F3.7.7.2.g1" src="extracted/5819071/data/gallery/row-2/1.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S3.F3.8.8.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F3.8.8.3.g1" src="extracted/5819071/data/gallery/row-2/2.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S3.F3.9.9.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F3.9.9.4.g1" src="extracted/5819071/data/gallery/row-2/3.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S3.F3.10.10.5"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F3.10.10.5.g1" src="extracted/5819071/data/gallery/row-2/4.png" width="108"/></td>
</tr>
<tr class="ltx_tr" id="S3.F3.15.15">
<td class="ltx_td ltx_align_center" id="S3.F3.11.11.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F3.11.11.1.g1" src="extracted/5819071/data/gallery/row-3/gt-846.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S3.F3.12.12.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F3.12.12.2.g1" src="extracted/5819071/data/gallery/row-3/1.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S3.F3.13.13.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F3.13.13.3.g1" src="extracted/5819071/data/gallery/row-3/2.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S3.F3.14.14.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F3.14.14.4.g1" src="extracted/5819071/data/gallery/row-3/3.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S3.F3.15.15.5"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F3.15.15.5.g1" src="extracted/5819071/data/gallery/row-3/4.png" width="108"/></td>
</tr>
<tr class="ltx_tr" id="S3.F3.20.20">
<td class="ltx_td ltx_align_center" id="S3.F3.16.16.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F3.16.16.1.g1" src="extracted/5819071/data/gallery/row-4/gt-29.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S3.F3.17.17.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F3.17.17.2.g1" src="extracted/5819071/data/gallery/row-4/1.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S3.F3.18.18.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F3.18.18.3.g1" src="extracted/5819071/data/gallery/row-4/2.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S3.F3.19.19.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F3.19.19.4.g1" src="extracted/5819071/data/gallery/row-4/3.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S3.F3.20.20.5"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F3.20.20.5.g1" src="extracted/5819071/data/gallery/row-4/4.png" width="108"/></td>
</tr>
</table>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span class="ltx_text ltx_font_bold" id="S3.F3.29.1">Images generated via STAGE.</span>
We are able to generate images of people with different body shapes and appearances and in different locations, well-aligned with the given 3D ground truth pose (leftmost column). We use a base prompt “Photo, adult caucasian male/female wearing a t-shirt in the city center at day time sunny day” and modify a single attribute, <em class="ltx_emph ltx_font_italic" id="S3.F3.30.2">e.g</em>.<span class="ltx_text" id="S3.F3.31.3"></span> “t-shirt” to “trench coat”.
</figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1"><span class="ltx_text" id="S3.SS2.p2.1.1" style="font-size:90%;">CN-Pose </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS2.p2.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib83" title=""><span class="ltx_text" style="font-size:90%;">83</span></a><span class="ltx_text" id="S3.SS2.p2.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S3.SS2.p2.1.4" style="font-size:90%;"> generates images conditioned on the 2D human pose.
This model does offer reasonable image quality but suffers from poor 3D pose alignment since multiple 3D poses can project to the same 2D pose.
Generated images can have swapped left and right limbs or generate back views instead of front views.
CN-Depth </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS2.p2.1.5.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib83" title=""><span class="ltx_text" style="font-size:90%;">83</span></a><span class="ltx_text" id="S3.SS2.p2.1.6.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S3.SS2.p2.1.7" style="font-size:90%;"> is conditioned on the given depth image, thus retaining the depth information and providing a better 3D pose alignment.
However, using body pose depth leads to low diversity as the backgrounds are bland and clothing is mostly body-tight.
This is because CN-Depth was trained with full image depth estimated from web images.
Providing full image depth would restrict us to a finite amount of depth images, lowering the diversity.
Combining CN-Pose and CN-Depth </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS2.p2.1.8.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib83" title=""><span class="ltx_text" style="font-size:90%;">83</span></a><span class="ltx_text" id="S3.SS2.p2.1.9.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S3.SS2.p2.1.10" style="font-size:90%;"> results in an improved 3D pose alignment, but also negatively affects the diversity as can be observed in </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S4.T2" style="font-size:90%;" title="In 4.3 Quality Assessment of STAGE Data ‣ 4 Experiments ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">2</span></a><span class="ltx_text" id="S3.SS2.p2.1.11" style="font-size:90%;"> and most obviously by visual inspection, see </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S4.F10" style="font-size:90%;" title="In 4.3 Quality Assessment of STAGE Data ‣ 4 Experiments ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">10</span></a><span class="ltx_text" id="S3.SS2.p2.1.12" style="font-size:90%;"> and </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S4.F9a" style="font-size:90%;" title="In D Qualitative Results ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">S9</span></a><span class="ltx_text" id="S3.SS2.p2.1.13" style="font-size:90%;"> in Supp. Mat.
We refer to this combination as CN-Multi.
None of these solutions fully meet our requirements.</span></p>
</div>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:90%;">CN-3DPose</h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.6"><span class="ltx_text" id="S3.SS2.SSS0.Px1.p1.6.1" style="font-size:90%;">Our image generation method, CN-3DPose, builds on the pre-trained CN-Depth and three key insights, which effectively leverage a combination of 3D and 2D pose datasets.
First, we condition on isolated human body depth maps </span><math alttext="0pt" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.1.m1.1"><semantics id="S3.SS2.SSS0.Px1.p1.1.m1.1a"><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml"><mn id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2" mathsize="90%" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml">0</mn><mo id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3" mathsize="90%" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml">p</mi><mo id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1a" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.4" mathsize="90%" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.4.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1"><times id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1"></times><cn id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2">0</cn><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3">𝑝</ci><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.4.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.4">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.1.m1.1c">0pt</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.1.m1.1d">0 italic_p italic_t</annotation></semantics></math><span class="ltx_text" id="S3.SS2.SSS0.Px1.p1.6.2" style="font-size:90%;">, as opposed to real depth maps that include the background.
This forces the model to use the text control and the initial noise to create diverse backgrounds and appearances while being coherent with the 3D pose.
To fine-tune for this new task, we leverage existing public 3D pose datasets (see </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S4.SS1" style="font-size:90%;" title="4.1 Implementation Details ‣ 4 Experiments ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.1</span></a><span class="ltx_text" id="S3.SS2.SSS0.Px1.p1.6.3" style="font-size:90%;">).
Second, to avoid confusion between body parts, we use an additional dense semantic conditioning </span><math alttext="I_{s}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.2.m2.1"><semantics id="S3.SS2.SSS0.Px1.p1.2.m2.1a"><msub id="S3.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2" mathsize="90%" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml">I</mi><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3" mathsize="90%" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.2.m2.1b"><apply id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2">𝐼</ci><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.2.m2.1c">I_{s}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.2.m2.1d">italic_I start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S3.SS2.SSS0.Px1.p1.6.4" style="font-size:90%;">: a rendering of SMPL with each vertex colored according to its position in a canonical A-pose.
Third, to mitigate over-fitting to the 3D pose dataset backgrounds and their limited variation in human appearance and poses, we also fine-tune using larger and diverse 2D pose datasets. Here, we simply condition on both 2D pose </span><math alttext="I_{k}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.3.m3.1"><semantics id="S3.SS2.SSS0.Px1.p1.3.m3.1a"><msub id="S3.SS2.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2" mathsize="90%" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2.cmml">I</mi><mi id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3" mathsize="90%" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.3.m3.1b"><apply id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2">𝐼</ci><ci id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.3.m3.1c">I_{k}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.3.m3.1d">italic_I start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S3.SS2.SSS0.Px1.p1.6.5" style="font-size:90%;"> and 3D pose,
but zero out the 3D pose conditioning inputs (</span><math alttext="0pt" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.4.m4.1"><semantics id="S3.SS2.SSS0.Px1.p1.4.m4.1a"><mrow id="S3.SS2.SSS0.Px1.p1.4.m4.1.1" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.cmml"><mn id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2" mathsize="90%" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2.cmml">0</mn><mo id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3" mathsize="90%" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.cmml">p</mi><mo id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1a" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.4" mathsize="90%" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.4.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.4.m4.1b"><apply id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1"><times id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1"></times><cn id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2.cmml" type="integer" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2">0</cn><ci id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3">𝑝</ci><ci id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.4.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.4">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.4.m4.1c">0pt</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.4.m4.1d">0 italic_p italic_t</annotation></semantics></math><span class="ltx_text" id="S3.SS2.SSS0.Px1.p1.6.6" style="font-size:90%;"> and </span><math alttext="I_{s}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.5.m5.1"><semantics id="S3.SS2.SSS0.Px1.p1.5.m5.1a"><msub id="S3.SS2.SSS0.Px1.p1.5.m5.1.1" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.2" mathsize="90%" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.2.cmml">I</mi><mi id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.3" mathsize="90%" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.5.m5.1b"><apply id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.2">𝐼</ci><ci id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.5.m5.1c">I_{s}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.5.m5.1d">italic_I start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S3.SS2.SSS0.Px1.p1.6.7" style="font-size:90%;">) when it is not available.
This new model meets our requirements for diversity and 3D pose alignment.
Overall, it provides us with a way to sample from
</span><math alttext="p(\mathbf{x}|\mathbf{P},\mathbf{z})\approx p(\mathbf{x}|t,0pt,I_{s},I_{k},%
\mathbf{n})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.6.m6.6"><semantics id="S3.SS2.SSS0.Px1.p1.6.m6.6a"><mrow id="S3.SS2.SSS0.Px1.p1.6.m6.6.6" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.cmml"><mrow id="S3.SS2.SSS0.Px1.p1.6.m6.5.5.1" xref="S3.SS2.SSS0.Px1.p1.6.m6.5.5.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.6.m6.5.5.1.3" mathsize="90%" xref="S3.SS2.SSS0.Px1.p1.6.m6.5.5.1.3.cmml">p</mi><mo id="S3.SS2.SSS0.Px1.p1.6.m6.5.5.1.2" xref="S3.SS2.SSS0.Px1.p1.6.m6.5.5.1.2.cmml">⁢</mo><mrow id="S3.SS2.SSS0.Px1.p1.6.m6.5.5.1.1.1" xref="S3.SS2.SSS0.Px1.p1.6.m6.5.5.1.1.1.1.cmml"><mo id="S3.SS2.SSS0.Px1.p1.6.m6.5.5.1.1.1.2" maxsize="90%" minsize="90%" xref="S3.SS2.SSS0.Px1.p1.6.m6.5.5.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.SSS0.Px1.p1.6.m6.5.5.1.1.1.1" xref="S3.SS2.SSS0.Px1.p1.6.m6.5.5.1.1.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.6.m6.5.5.1.1.1.1.2" mathsize="90%" xref="S3.SS2.SSS0.Px1.p1.6.m6.5.5.1.1.1.1.2.cmml">𝐱</mi><mo fence="false" id="S3.SS2.SSS0.Px1.p1.6.m6.5.5.1.1.1.1.1" mathsize="90%" xref="S3.SS2.SSS0.Px1.p1.6.m6.5.5.1.1.1.1.1.cmml">|</mo><mrow id="S3.SS2.SSS0.Px1.p1.6.m6.5.5.1.1.1.1.3.2" xref="S3.SS2.SSS0.Px1.p1.6.m6.5.5.1.1.1.1.3.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.6.m6.1.1" mathsize="90%" xref="S3.SS2.SSS0.Px1.p1.6.m6.1.1.cmml">𝐏</mi><mo id="S3.SS2.SSS0.Px1.p1.6.m6.5.5.1.1.1.1.3.2.1" mathsize="90%" xref="S3.SS2.SSS0.Px1.p1.6.m6.5.5.1.1.1.1.3.1.cmml">,</mo><mi id="S3.SS2.SSS0.Px1.p1.6.m6.2.2" mathsize="90%" xref="S3.SS2.SSS0.Px1.p1.6.m6.2.2.cmml">𝐳</mi></mrow></mrow><mo id="S3.SS2.SSS0.Px1.p1.6.m6.5.5.1.1.1.3" maxsize="90%" minsize="90%" xref="S3.SS2.SSS0.Px1.p1.6.m6.5.5.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.3" mathsize="90%" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.3.cmml">≈</mo><mrow id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.cmml"><mi id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.3" mathsize="90%" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.3.cmml">p</mi><mo id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.2" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.2.cmml">⁢</mo><mrow id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.cmml"><mo id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.2" maxsize="90%" minsize="90%" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.cmml">(</mo><mrow id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.5" mathsize="90%" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.5.cmml">𝐱</mi><mo fence="false" id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.4" mathsize="90%" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.4.cmml">|</mo><mrow id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.3.3" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.3.4.cmml"><mi id="S3.SS2.SSS0.Px1.p1.6.m6.3.3" mathsize="90%" xref="S3.SS2.SSS0.Px1.p1.6.m6.3.3.cmml">t</mi><mo id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.3.3.4" mathsize="90%" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.3.4.cmml">,</mo><mrow id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.1.1.1" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.1.1.1.cmml"><mn id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.1.1.1.2" mathsize="90%" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.1.1.1.2.cmml">0</mn><mo id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.1.1.1.1" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.1.1.1.3" mathsize="90%" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.1.1.1.3.cmml">p</mi><mo id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.1.1.1.1a" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.1.1.1.4" mathsize="90%" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.1.1.1.4.cmml">t</mi></mrow><mo id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.3.3.5" mathsize="90%" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.3.4.cmml">,</mo><msub id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.2.2.2" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.2.2.2.cmml"><mi id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.2.2.2.2" mathsize="90%" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.2.2.2.2.cmml">I</mi><mi id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.2.2.2.3" mathsize="90%" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.2.2.2.3.cmml">s</mi></msub><mo id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.3.3.6" mathsize="90%" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.3.4.cmml">,</mo><msub id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.3.3.3" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.3.3.3.cmml"><mi id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.3.3.3.2" mathsize="90%" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.3.3.3.2.cmml">I</mi><mi id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.3.3.3.3" mathsize="90%" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.3.3.3.3.cmml">k</mi></msub><mo id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.3.3.7" mathsize="90%" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.3.4.cmml">,</mo><mi id="S3.SS2.SSS0.Px1.p1.6.m6.4.4" mathsize="90%" xref="S3.SS2.SSS0.Px1.p1.6.m6.4.4.cmml">𝐧</mi></mrow></mrow><mo id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.3" maxsize="90%" minsize="90%" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.6.m6.6b"><apply id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6"><approx id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.3.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.3"></approx><apply id="S3.SS2.SSS0.Px1.p1.6.m6.5.5.1.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.5.5.1"><times id="S3.SS2.SSS0.Px1.p1.6.m6.5.5.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.5.5.1.2"></times><ci id="S3.SS2.SSS0.Px1.p1.6.m6.5.5.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.5.5.1.3">𝑝</ci><apply id="S3.SS2.SSS0.Px1.p1.6.m6.5.5.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.5.5.1.1.1"><csymbol cd="latexml" id="S3.SS2.SSS0.Px1.p1.6.m6.5.5.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.5.5.1.1.1.1.1">conditional</csymbol><ci id="S3.SS2.SSS0.Px1.p1.6.m6.5.5.1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.5.5.1.1.1.1.2">𝐱</ci><list id="S3.SS2.SSS0.Px1.p1.6.m6.5.5.1.1.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.5.5.1.1.1.1.3.2"><ci id="S3.SS2.SSS0.Px1.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.1.1">𝐏</ci><ci id="S3.SS2.SSS0.Px1.p1.6.m6.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.2.2">𝐳</ci></list></apply></apply><apply id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2"><times id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.2"></times><ci id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.3.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.3">𝑝</ci><apply id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1"><csymbol cd="latexml" id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.4.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.4">conditional</csymbol><ci id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.5.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.5">𝐱</ci><list id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.3.4.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.3.3"><ci id="S3.SS2.SSS0.Px1.p1.6.m6.3.3.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.3.3">𝑡</ci><apply id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.1.1.1"><times id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.1.1.1.1"></times><cn id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.1.1.1.2.cmml" type="integer" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.1.1.1.2">0</cn><ci id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.1.1.1.3">𝑝</ci><ci id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.1.1.1.4.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.1.1.1.4">𝑡</ci></apply><apply id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.2.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.2.2.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.2.2.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.2.2.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.2.2.2.2">𝐼</ci><ci id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.2.2.2.3.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.2.2.2.3">𝑠</ci></apply><apply id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.3.3.3.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.3.3.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.3.3.3">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.3.3.3.2.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.3.3.3.2">𝐼</ci><ci id="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.3.3.3.3.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.6.6.2.1.1.1.3.3.3.3">𝑘</ci></apply><ci id="S3.SS2.SSS0.Px1.p1.6.m6.4.4.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.4.4">𝐧</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.6.m6.6c">p(\mathbf{x}|\mathbf{P},\mathbf{z})\approx p(\mathbf{x}|t,0pt,I_{s},I_{k},%
\mathbf{n})</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.6.m6.6d">italic_p ( bold_x | bold_P , bold_z ) ≈ italic_p ( bold_x | italic_t , 0 italic_p italic_t , italic_I start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , italic_I start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , bold_n )</annotation></semantics></math><span class="ltx_text" id="S3.SS2.SSS0.Px1.p1.6.8" style="font-size:90%;">.
Our conditioning inputs and the key steps of our method are depicted in </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3.F2" style="font-size:90%;" title="In 3 Method ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a><span class="ltx_text" id="S3.SS2.SSS0.Px1.p1.6.9" style="font-size:90%;">.</span></p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Dataset Generation</h3>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.2">
<tr class="ltx_tr" id="S3.T1.2.2">
<td class="ltx_td ltx_align_left ltx_align_middle ltx_border_tt" id="S3.T1.2.2.3" rowspan="2" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.2.3.1" style="font-size:90%;"><span class="ltx_text" id="S3.T1.2.2.3.1.1"></span> <span class="ltx_text" id="S3.T1.2.2.3.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.2.2.3.1.2.1">
<span class="ltx_tr" id="S3.T1.2.2.3.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.2.2.3.1.2.1.1.1" style="padding-left:4.8pt;padding-right:4.8pt;">Pose</span></span>
<span class="ltx_tr" id="S3.T1.2.2.3.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.2.2.3.1.2.1.2.1" style="padding-left:4.8pt;padding-right:4.8pt;">estimator</span></span>
</span></span> <span class="ltx_text" id="S3.T1.2.2.3.1.3"></span></span></td>
<td class="ltx_td ltx_border_tt" id="S3.T1.2.2.4" style="padding-left:4.8pt;padding-right:4.8pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S3.T1.1.1.1" style="padding-left:4.8pt;padding-right:4.8pt;">
<span class="ltx_text" id="S3.T1.1.1.1.1" style="font-size:90%;">Base error (mm) </span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T1.1.1.1.m1.1"><semantics id="S3.T1.1.1.1.m1.1a"><mo id="S3.T1.1.1.1.m1.1.1" mathsize="90%" stretchy="false" xref="S3.T1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.m1.1b"><ci id="S3.T1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.1.1.1.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="7" id="S3.T1.2.2.2" style="padding-left:4.8pt;padding-right:4.8pt;">
<span class="ltx_text" id="S3.T1.2.2.2.1" style="font-size:90%;">Sensitivity to attributes (PDP, %) </span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T1.2.2.2.m1.1"><semantics id="S3.T1.2.2.2.m1.1a"><mo id="S3.T1.2.2.2.m1.1.1" mathsize="90%" stretchy="false" xref="S3.T1.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.m1.1b"><ci id="S3.T1.2.2.2.m1.1.1.cmml" xref="S3.T1.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.2.2.2.m1.1d">↓</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.3">
<td class="ltx_td" id="S3.T1.2.3.1" style="padding-left:4.8pt;padding-right:4.8pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.3.2" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.3.2.1" style="font-size:90%;">MPJPE</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.3.3" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.3.3.1" style="font-size:90%;">PA-MPJPE</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.3.4" style="padding-left:4.8pt;padding-right:4.8pt;">
<span class="ltx_text" id="S3.T1.2.3.4.1"></span><span class="ltx_text" id="S3.T1.2.3.4.2" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.2.3.4.2.1">
<span class="ltx_tr" id="S3.T1.2.3.4.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T1.2.3.4.2.1.1.1" style="padding-left:4.8pt;padding-right:4.8pt;">Location</span></span>
<span class="ltx_tr" id="S3.T1.2.3.4.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T1.2.3.4.2.1.2.1" style="padding-left:4.8pt;padding-right:4.8pt;">(outdoor)</span></span>
</span></span><span class="ltx_text" id="S3.T1.2.3.4.3"></span><span class="ltx_text" id="S3.T1.2.3.4.4" style="font-size:90%;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.3.5" style="padding-left:4.8pt;padding-right:4.8pt;">
<span class="ltx_text" id="S3.T1.2.3.5.1"></span><span class="ltx_text" id="S3.T1.2.3.5.2" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.2.3.5.2.1">
<span class="ltx_tr" id="S3.T1.2.3.5.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T1.2.3.5.2.1.1.1" style="padding-left:4.8pt;padding-right:4.8pt;">Location</span></span>
<span class="ltx_tr" id="S3.T1.2.3.5.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T1.2.3.5.2.1.2.1" style="padding-left:4.8pt;padding-right:4.8pt;">(indoor)</span></span>
</span></span><span class="ltx_text" id="S3.T1.2.3.5.3"></span><span class="ltx_text" id="S3.T1.2.3.5.4" style="font-size:90%;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.3.6" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.3.6.1" style="font-size:90%;">Fairness</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.3.7" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.3.7.1" style="font-size:90%;">Clothing</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.3.8" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.3.8.1" style="font-size:90%;">Weather</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.3.9" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.3.9.1" style="font-size:90%;">Texture</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.3.10" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.3.10.1" style="font-size:90%;">Mean</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.4.1" style="padding-left:4.8pt;padding-right:4.8pt;">
<span class="ltx_text" id="S3.T1.2.4.1.1" style="font-size:90%;">SPIN </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T1.2.4.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">38</span></a><span class="ltx_text" id="S3.T1.2.4.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_border_t" id="S3.T1.2.4.2" style="padding-left:4.8pt;padding-right:4.8pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.4.3" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.4.3.1" style="font-size:90%;">122.50</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.4.4" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.4.4.1" style="font-size:90%;">90.27</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.4.5" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.4.5.1" style="font-size:90%;">15.79</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.4.6" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.4.6.1" style="font-size:90%;">19.65</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.4.7" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.4.7.1" style="font-size:90%;">12.93</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.4.8" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.4.8.1" style="font-size:90%;">29.13</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.4.9" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.4.9.1" style="font-size:90%;">17.43</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.4.10" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.4.10.1" style="font-size:90%;">29.26</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.4.11" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.4.11.1" style="font-size:90%;">20.70</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.5">
<td class="ltx_td ltx_align_left" id="S3.T1.2.5.1" style="padding-left:4.8pt;padding-right:4.8pt;">
<span class="ltx_text" id="S3.T1.2.5.1.1" style="font-size:90%;">PARE </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T1.2.5.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a><span class="ltx_text" id="S3.T1.2.5.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td" id="S3.T1.2.5.2" style="padding-left:4.8pt;padding-right:4.8pt;"></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.5.3" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.5.3.1" style="font-size:90%;">118.81</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.5.4" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.5.4.1" style="font-size:90%;">88.98</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.5.5" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.5.5.1" style="font-size:90%;">13.34</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.5.6" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.5.6.1" style="font-size:90%;">15.84</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.5.7" style="padding-left:4.8pt;padding-right:4.8pt;">
<span class="ltx_text ltx_phantom" id="S3.T1.2.5.7.1" style="font-size:90%;"><span style="visibility:hidden">0</span></span><span class="ltx_text" id="S3.T1.2.5.7.2" style="font-size:90%;">9.36</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.5.8" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.5.8.1" style="font-size:90%;">21.66</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.5.9" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.5.9.1" style="font-size:90%;">12.97</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.5.10" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.5.10.1" style="font-size:90%;">18.43</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.5.11" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.5.11.1" style="font-size:90%;">15.27</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.6">
<td class="ltx_td ltx_align_left" id="S3.T1.2.6.1" style="padding-left:4.8pt;padding-right:4.8pt;">
<span class="ltx_text" id="S3.T1.2.6.1.1" style="font-size:90%;">MeTRAbs </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T1.2.6.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib69" title=""><span class="ltx_text" style="font-size:90%;">69</span></a><span class="ltx_text" id="S3.T1.2.6.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td" id="S3.T1.2.6.2" style="padding-left:4.8pt;padding-right:4.8pt;"></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.6.3" style="padding-left:4.8pt;padding-right:4.8pt;">
<span class="ltx_text ltx_phantom" id="S3.T1.2.6.3.1" style="font-size:90%;"><span style="visibility:hidden">0</span></span><span class="ltx_text" id="S3.T1.2.6.3.2" style="font-size:90%;">89.80</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.6.4" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.6.4.1" style="font-size:90%;">67.77</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.6.5" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.6.5.1" style="font-size:90%;">17.06</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.6.6" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.6.6.1" style="font-size:90%;">19.57</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.6.7" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.6.7.1" style="font-size:90%;">11.31</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.6.8" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.6.8.1" style="font-size:90%;">22.93</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.6.9" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.6.9.1" style="font-size:90%;">15.88</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.6.10" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.6.10.1" style="font-size:90%;">16.94</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.6.11" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.6.11.1" style="font-size:90%;">17.28</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.7">
<td class="ltx_td ltx_align_left" id="S3.T1.2.7.1" style="padding-left:4.8pt;padding-right:4.8pt;">
<span class="ltx_text" id="S3.T1.2.7.1.1" style="font-size:90%;">PyMAF-X </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T1.2.7.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib69" title=""><span class="ltx_text" style="font-size:90%;">69</span></a><span class="ltx_text" id="S3.T1.2.7.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td" id="S3.T1.2.7.2" style="padding-left:4.8pt;padding-right:4.8pt;"></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.7.3" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.7.3.1" style="font-size:90%;">115.81</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.7.4" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.7.4.1" style="font-size:90%;">84.03</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.7.5" style="padding-left:4.8pt;padding-right:4.8pt;">
<span class="ltx_text ltx_phantom" id="S3.T1.2.7.5.1" style="font-size:90%;"><span style="visibility:hidden">0</span></span><span class="ltx_text" id="S3.T1.2.7.5.2" style="font-size:90%;">9.83</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.7.6" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.7.6.1" style="font-size:90%;">12.49</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.7.7" style="padding-left:4.8pt;padding-right:4.8pt;">
<span class="ltx_text ltx_phantom" id="S3.T1.2.7.7.1" style="font-size:90%;"><span style="visibility:hidden">0</span></span><span class="ltx_text" id="S3.T1.2.7.7.2" style="font-size:90%;">5.40</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.7.8" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.7.8.1" style="font-size:90%;">14.61</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.7.9" style="padding-left:4.8pt;padding-right:4.8pt;">
<span class="ltx_text ltx_phantom" id="S3.T1.2.7.9.1" style="font-size:90%;"><span style="visibility:hidden">0</span></span><span class="ltx_text" id="S3.T1.2.7.9.2" style="font-size:90%;">8.51</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.7.10" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.7.10.1" style="font-size:90%;">10.60</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.7.11" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.7.11.1" style="font-size:90%;">10.24</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.8">
<td class="ltx_td ltx_align_left" id="S3.T1.2.8.1" style="padding-left:4.8pt;padding-right:4.8pt;">
<span class="ltx_text" id="S3.T1.2.8.1.1" style="font-size:90%;">HMR 2.0 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T1.2.8.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a><span class="ltx_text" id="S3.T1.2.8.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td" id="S3.T1.2.8.2" style="padding-left:4.8pt;padding-right:4.8pt;"></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.8.3" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.8.3.1" style="font-size:90%;">102.40</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.8.4" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.8.4.1" style="font-size:90%;">75.21</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.8.5" style="padding-left:4.8pt;padding-right:4.8pt;">
<span class="ltx_text ltx_phantom" id="S3.T1.2.8.5.1" style="font-size:90%;"><span style="visibility:hidden">0</span></span><span class="ltx_text" id="S3.T1.2.8.5.2" style="font-size:90%;">9.31</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.8.6" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.8.6.1" style="font-size:90%;">12.41</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.8.7" style="padding-left:4.8pt;padding-right:4.8pt;">
<span class="ltx_text ltx_phantom" id="S3.T1.2.8.7.1" style="font-size:90%;"><span style="visibility:hidden">0</span></span><span class="ltx_text" id="S3.T1.2.8.7.2" style="font-size:90%;">5.34</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.8.8" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.8.8.1" style="font-size:90%;">15.76</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.8.9" style="padding-left:4.8pt;padding-right:4.8pt;">
<span class="ltx_text ltx_phantom" id="S3.T1.2.8.9.1" style="font-size:90%;"><span style="visibility:hidden">0</span></span><span class="ltx_text" id="S3.T1.2.8.9.2" style="font-size:90%;">7.57</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.8.10" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.8.10.1" style="font-size:90%;">10.75</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.8.11" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.8.11.1" style="font-size:90%;">10.19</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.9">
<td class="ltx_td ltx_align_left" colspan="2" id="S3.T1.2.9.1" style="padding-left:4.8pt;padding-right:4.8pt;">
<span class="ltx_text" id="S3.T1.2.9.1.1" style="font-size:90%;">BEDL.-CLIFF</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T1.2.9.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">6</span></a><span class="ltx_text" id="S3.T1.2.9.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.9.2" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.9.2.1" style="font-size:90%;">113.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.9.3" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.9.3.1" style="font-size:90%;">84.22</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.9.4" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.9.4.1" style="font-size:90%;">17.30</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.9.5" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.9.5.1" style="font-size:90%;">20.77</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.9.6" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.9.6.1" style="font-size:90%;">15.61</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.9.7" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.9.7.1" style="font-size:90%;">23.36</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.9.8" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.9.8.1" style="font-size:90%;">15.47</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.9.9" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.9.9.1" style="font-size:90%;">19.42</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.9.10" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.9.10.1" style="font-size:90%;">18.65</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.10">
<td class="ltx_td ltx_align_left" id="S3.T1.2.10.1" style="padding-left:4.8pt;padding-right:4.8pt;">
<span class="ltx_ERROR undefined" id="S3.T1.2.10.1.1">\ldelim</span><span class="ltx_text" id="S3.T1.2.10.1.2" style="font-size:90%;">{4*[</span><span class="ltx_text" id="S3.T1.2.10.1.3"></span><span class="ltx_text" id="S3.T1.2.10.1.4" style="font-size:90%;"> </span><span class="ltx_text" id="S3.T1.2.10.1.5" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.2.10.1.5.1">
<span class="ltx_tr" id="S3.T1.2.10.1.5.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.2.10.1.5.1.1.1" style="padding-left:4.8pt;padding-right:4.8pt;">SMPLer-X</span></span>
<span class="ltx_tr" id="S3.T1.2.10.1.5.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.2.10.1.5.1.2.1" style="padding-left:4.8pt;padding-right:4.8pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite></span></span>
</span></span><span class="ltx_text" id="S3.T1.2.10.1.6" style="font-size:90%;"> </span><span class="ltx_text" id="S3.T1.2.10.1.7"></span><span class="ltx_text" id="S3.T1.2.10.1.8" style="font-size:90%;">]</span>
</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.10.2" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.10.2.1" style="font-size:90%;">S32</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.10.3" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.10.3.1" style="font-size:90%;">117.84</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.10.4" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.10.4.1" style="font-size:90%;">90.61</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.10.5" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.10.5.1" style="font-size:90%;">15.62</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.10.6" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.10.6.1" style="font-size:90%;">19.54</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.10.7" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.10.7.1" style="font-size:90%;">11.20</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.10.8" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.10.8.1" style="font-size:90%;">21.76</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.10.9" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.10.9.1" style="font-size:90%;">15.88</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.10.10" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.10.10.1" style="font-size:90%;">18.51</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.10.11" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.10.11.1" style="font-size:90%;">17.08</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.11">
<td class="ltx_td" id="S3.T1.2.11.1" style="padding-left:4.8pt;padding-right:4.8pt;"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.11.2" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.11.2.1" style="font-size:90%;">B32</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.11.3" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.11.3.1" style="font-size:90%;">107.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.11.4" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.11.4.1" style="font-size:90%;">80.40</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.11.5" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.11.5.1" style="font-size:90%;">13.30</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.11.6" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.11.6.1" style="font-size:90%;">17.24</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.11.7" style="padding-left:4.8pt;padding-right:4.8pt;">
<span class="ltx_text ltx_phantom" id="S3.T1.2.11.7.1" style="font-size:90%;"><span style="visibility:hidden">0</span></span><span class="ltx_text" id="S3.T1.2.11.7.2" style="font-size:90%;">7.81</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.11.8" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.11.8.1" style="font-size:90%;">17.38</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.11.9" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.11.9.1" style="font-size:90%;">12.57</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.11.10" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.11.10.1" style="font-size:90%;">13.13</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.11.11" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.11.11.1" style="font-size:90%;">13.57</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.12">
<td class="ltx_td" id="S3.T1.2.12.1" style="padding-left:4.8pt;padding-right:4.8pt;"></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.12.2" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.12.2.1" style="font-size:90%;">L32</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.12.3" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.12.3.1" style="font-size:90%;">101.83</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.12.4" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.12.4.1" style="font-size:90%;">75.98</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.12.5" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.12.5.1" style="font-size:90%;">14.48</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.12.6" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.12.6.1" style="font-size:90%;">17.84</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.12.7" style="padding-left:4.8pt;padding-right:4.8pt;">
<span class="ltx_text ltx_phantom" id="S3.T1.2.12.7.1" style="font-size:90%;"><span style="visibility:hidden">0</span></span><span class="ltx_text" id="S3.T1.2.12.7.2" style="font-size:90%;">7.73</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.12.8" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.12.8.1" style="font-size:90%;">16.53</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.12.9" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.12.9.1" style="font-size:90%;">12.30</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.12.10" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.12.10.1" style="font-size:90%;">12.40</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.12.11" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.12.11.1" style="font-size:90%;">13.54</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.13">
<td class="ltx_td ltx_border_bb" id="S3.T1.2.13.1" style="padding-left:4.8pt;padding-right:4.8pt;"></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.2.13.2" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.13.2.1" style="font-size:90%;">H32</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.2.13.3" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.13.3.1" style="font-size:90%;">104.79</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T1.2.13.4" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.13.4.1" style="font-size:90%;">76.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.2.13.5" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.13.5.1" style="font-size:90%;">14.35</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.2.13.6" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.13.6.1" style="font-size:90%;">18.55</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.2.13.7" style="padding-left:4.8pt;padding-right:4.8pt;">
<span class="ltx_text ltx_phantom" id="S3.T1.2.13.7.1" style="font-size:90%;"><span style="visibility:hidden">0</span></span><span class="ltx_text" id="S3.T1.2.13.7.2" style="font-size:90%;">6.75</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.2.13.8" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.13.8.1" style="font-size:90%;">15.11</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.2.13.9" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.13.9.1" style="font-size:90%;">10.54</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.2.13.10" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.13.10.1" style="font-size:90%;">10.74</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.2.13.11" style="padding-left:4.8pt;padding-right:4.8pt;"><span class="ltx_text" id="S3.T1.2.13.11.1" style="font-size:90%;">12.67</span></td>
</tr>
</table>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1: </span><span class="ltx_text ltx_font_bold" id="S3.T1.7.1">Evaluating sensitivity of various pose estimators with STAGE</span>.
We evaluate the percentage of degraded poses and contrast it against the general performance on the base set.
We find that appearance attributes such as clothing and texture are the most impactful. In addition, pose estimators are more sensitive to indoor locations compared to outdoor locations.</figcaption>
</figure>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:90%;">Set of attributes</h4>
<div class="ltx_para" id="S3.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px1.p1.1"><span class="ltx_text" id="S3.SS3.SSS0.Px1.p1.1.1" style="font-size:90%;">To evaluate the capabilities of pose estimators in the open world, we aim to cover several groups of attributes.
We cover attributes like clothing and location, as well as sensitive personal attributes such as ethnicity, age, and gender to detect potential biases in pose estimation methods.
Finally, we also consider adverse conditions such as night, snow, or rain as it is a topic recognized in the autonomous driving community </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS3.SSS0.Px1.p1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib66" title=""><span class="ltx_text" style="font-size:90%;">66</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib17" title=""><span class="ltx_text" style="font-size:90%;">17</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">51</span></a><span class="ltx_text" id="S3.SS3.SSS0.Px1.p1.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S3.SS3.SSS0.Px1.p1.1.4" style="font-size:90%;">, but not yet in the HPE community.
This leads to a high-level scene description captured in the following prompt template: </span><span class="ltx_text ltx_font_typewriter" id="S3.SS3.SSS0.Px1.p1.1.5" style="font-size:90%;">Photo, {ethnicity} {age} {gender} wearing {clothing} in {location} at {lighting condition} {weather}</span><span class="ltx_text" id="S3.SS3.SSS0.Px1.p1.1.6" style="font-size:90%;">.
For the base set construction, we populate the template to represent the average scene, with the intention not to present an extra challenge to the pose estimator, such as “</span><span class="ltx_text ltx_font_typewriter" id="S3.SS3.SSS0.Px1.p1.1.7" style="font-size:90%;">Photo, caucasian young male wearing a t-shirt in the city center at daytime sunny day</span><span class="ltx_text" id="S3.SS3.SSS0.Px1.p1.1.8" style="font-size:90%;">”.
Note that the template structure can also be adapted by the user depending on the target operational domain of the pose estimator.</span></p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:90%;">Variation reduction</h4>
<div class="ltx_para" id="S3.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px2.p1.1"><span class="ltx_text" id="S3.SS3.SSS0.Px2.p1.1.1" style="font-size:90%;">To reduce factors of variation between the base and attribute sets, we fully specify the template, even for attributes not directly relevant to the experiment.
For example, not specifying the daytime can lead to a mix of images that depict daytime and nighttime.
Additionally, the noise sharing scheme described in </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3.SS1" style="font-size:90%;" title="3.1 Evaluation with Controlled Generated Test Sets ‣ 3 Method ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.1</span></a><span class="ltx_text" id="S3.SS3.SSS0.Px2.p1.1.2" style="font-size:90%;"> reduces the variation between base and attribute image and thus the variance of our estimation of the risk </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3.E2" style="font-size:90%;" title="In 3.1 Evaluation with Controlled Generated Test Sets ‣ 3 Method ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Eq.</span> <span class="ltx_text ltx_ref_tag">2</span></a><span class="ltx_text" id="S3.SS3.SSS0.Px2.p1.1.3" style="font-size:90%;">.
The effect can be observed in </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3.F3" style="font-size:90%;" title="In 3.2 Pose-Conditioned Image Generation ‣ 3 Method ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a><span class="ltx_text" id="S3.SS3.SSS0.Px2.p1.1.4" style="font-size:90%;">.</span></p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:90%;">Quality control</h4>
<div class="ltx_para" id="S3.SS3.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px3.p1.1"><span class="ltx_text" id="S3.SS3.SSS0.Px3.p1.1.1" style="font-size:90%;">To deal with noise and errors in the synthesis process, we introduce a filtering mechanism.
First, the target attribute should be present in the image. Second, the person should exhibit our desired pose.
To ensure the attribute presence, we employ a Visual Question Answering (VQA) based filtering.
Based on the input prompt, we construct questions about the presence of the attributes, following TIFA </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS3.SSS0.Px3.p1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib29" title=""><span class="ltx_text" style="font-size:90%;">29</span></a><span class="ltx_text" id="S3.SS3.SSS0.Px3.p1.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S3.SS3.SSS0.Px3.p1.1.4" style="font-size:90%;">.
The questions, jointly with the image, are fed into the VQA model BLIP2 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS3.SSS0.Px3.p1.1.5.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a><span class="ltx_text" id="S3.SS3.SSS0.Px3.p1.1.6.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S3.SS3.SSS0.Px3.p1.1.7" style="font-size:90%;">.
If the answer of the VQA model regarding the attribute is negative, the image is discarded.
To ensure consitency between image and 3D pose, we apply filtering based on 2D human pose alignment. (Filtering using a </span><em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS0.Px3.p1.1.8" style="font-size:90%;">3D</em><span class="ltx_text" id="S3.SS3.SSS0.Px3.p1.1.9" style="font-size:90%;"> pose method would risk removing the challenging and interesting images.)
Hence, we discard an image if OpenPose’s </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS3.SSS0.Px3.p1.1.10.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">12</span></a><span class="ltx_text" id="S3.SS3.SSS0.Px3.p1.1.11.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S3.SS3.SSS0.Px3.p1.1.12" style="font-size:90%;"> 2D keypoints predictions in the generated image deviate from the projected 3D keypoints by more than a given threshold.
Since we jointly generate the base and attribute images, we discard the whole pair if at least one of them is discarded.</span></p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Evaluation Protocol</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.6"><span class="ltx_text" id="S3.SS4.p1.6.1" style="font-size:90%;">We aim to identify cases where the prediction is </span><span class="ltx_text ltx_font_italic" id="S3.SS4.p1.6.2" style="font-size:90%;">degraded</span><span class="ltx_text" id="S3.SS4.p1.6.3" style="font-size:90%;"> compared to the base set in the presence of the target attribute.
We consider an estimated pose degraded if any joint is significantly displaced.
Hence, we use the Maximum Joint Error (MaxJE) – the highest joint error of a predicted pose.
For a ground truth pose </span><math alttext="\mathbf{P}\in\mathbb{R}^{J\times 3}" class="ltx_Math" display="inline" id="S3.SS4.p1.1.m1.1"><semantics id="S3.SS4.p1.1.m1.1a"><mrow id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml"><mi id="S3.SS4.p1.1.m1.1.1.2" mathsize="90%" xref="S3.SS4.p1.1.m1.1.1.2.cmml">𝐏</mi><mo id="S3.SS4.p1.1.m1.1.1.1" mathsize="90%" xref="S3.SS4.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS4.p1.1.m1.1.1.3" xref="S3.SS4.p1.1.m1.1.1.3.cmml"><mi id="S3.SS4.p1.1.m1.1.1.3.2" mathsize="90%" xref="S3.SS4.p1.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS4.p1.1.m1.1.1.3.3" xref="S3.SS4.p1.1.m1.1.1.3.3.cmml"><mi id="S3.SS4.p1.1.m1.1.1.3.3.2" mathsize="90%" xref="S3.SS4.p1.1.m1.1.1.3.3.2.cmml">J</mi><mo id="S3.SS4.p1.1.m1.1.1.3.3.1" lspace="0.222em" mathsize="90%" rspace="0.222em" xref="S3.SS4.p1.1.m1.1.1.3.3.1.cmml">×</mo><mn id="S3.SS4.p1.1.m1.1.1.3.3.3" mathsize="90%" xref="S3.SS4.p1.1.m1.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><apply id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1"><in id="S3.SS4.p1.1.m1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1.1"></in><ci id="S3.SS4.p1.1.m1.1.1.2.cmml" xref="S3.SS4.p1.1.m1.1.1.2">𝐏</ci><apply id="S3.SS4.p1.1.m1.1.1.3.cmml" xref="S3.SS4.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p1.1.m1.1.1.3.1.cmml" xref="S3.SS4.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS4.p1.1.m1.1.1.3.2.cmml" xref="S3.SS4.p1.1.m1.1.1.3.2">ℝ</ci><apply id="S3.SS4.p1.1.m1.1.1.3.3.cmml" xref="S3.SS4.p1.1.m1.1.1.3.3"><times id="S3.SS4.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS4.p1.1.m1.1.1.3.3.1"></times><ci id="S3.SS4.p1.1.m1.1.1.3.3.2.cmml" xref="S3.SS4.p1.1.m1.1.1.3.3.2">𝐽</ci><cn id="S3.SS4.p1.1.m1.1.1.3.3.3.cmml" type="integer" xref="S3.SS4.p1.1.m1.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">\mathbf{P}\in\mathbb{R}^{J\times 3}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.1.m1.1d">bold_P ∈ blackboard_R start_POSTSUPERSCRIPT italic_J × 3 end_POSTSUPERSCRIPT</annotation></semantics></math><span class="ltx_text" id="S3.SS4.p1.6.4" style="font-size:90%;">, and predicted pose </span><math alttext="\hat{\mathbf{P}}\in\mathbb{R}^{J\times 3}" class="ltx_Math" display="inline" id="S3.SS4.p1.2.m2.1"><semantics id="S3.SS4.p1.2.m2.1a"><mrow id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml"><mover accent="true" id="S3.SS4.p1.2.m2.1.1.2" xref="S3.SS4.p1.2.m2.1.1.2.cmml"><mi id="S3.SS4.p1.2.m2.1.1.2.2" mathsize="90%" xref="S3.SS4.p1.2.m2.1.1.2.2.cmml">𝐏</mi><mo id="S3.SS4.p1.2.m2.1.1.2.1" mathsize="90%" xref="S3.SS4.p1.2.m2.1.1.2.1.cmml">^</mo></mover><mo id="S3.SS4.p1.2.m2.1.1.1" mathsize="90%" xref="S3.SS4.p1.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS4.p1.2.m2.1.1.3" xref="S3.SS4.p1.2.m2.1.1.3.cmml"><mi id="S3.SS4.p1.2.m2.1.1.3.2" mathsize="90%" xref="S3.SS4.p1.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS4.p1.2.m2.1.1.3.3" xref="S3.SS4.p1.2.m2.1.1.3.3.cmml"><mi id="S3.SS4.p1.2.m2.1.1.3.3.2" mathsize="90%" xref="S3.SS4.p1.2.m2.1.1.3.3.2.cmml">J</mi><mo id="S3.SS4.p1.2.m2.1.1.3.3.1" lspace="0.222em" mathsize="90%" rspace="0.222em" xref="S3.SS4.p1.2.m2.1.1.3.3.1.cmml">×</mo><mn id="S3.SS4.p1.2.m2.1.1.3.3.3" mathsize="90%" xref="S3.SS4.p1.2.m2.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.1b"><apply id="S3.SS4.p1.2.m2.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1"><in id="S3.SS4.p1.2.m2.1.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1.1"></in><apply id="S3.SS4.p1.2.m2.1.1.2.cmml" xref="S3.SS4.p1.2.m2.1.1.2"><ci id="S3.SS4.p1.2.m2.1.1.2.1.cmml" xref="S3.SS4.p1.2.m2.1.1.2.1">^</ci><ci id="S3.SS4.p1.2.m2.1.1.2.2.cmml" xref="S3.SS4.p1.2.m2.1.1.2.2">𝐏</ci></apply><apply id="S3.SS4.p1.2.m2.1.1.3.cmml" xref="S3.SS4.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p1.2.m2.1.1.3.1.cmml" xref="S3.SS4.p1.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS4.p1.2.m2.1.1.3.2.cmml" xref="S3.SS4.p1.2.m2.1.1.3.2">ℝ</ci><apply id="S3.SS4.p1.2.m2.1.1.3.3.cmml" xref="S3.SS4.p1.2.m2.1.1.3.3"><times id="S3.SS4.p1.2.m2.1.1.3.3.1.cmml" xref="S3.SS4.p1.2.m2.1.1.3.3.1"></times><ci id="S3.SS4.p1.2.m2.1.1.3.3.2.cmml" xref="S3.SS4.p1.2.m2.1.1.3.3.2">𝐽</ci><cn id="S3.SS4.p1.2.m2.1.1.3.3.3.cmml" type="integer" xref="S3.SS4.p1.2.m2.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.1c">\hat{\mathbf{P}}\in\mathbb{R}^{J\times 3}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.2.m2.1d">over^ start_ARG bold_P end_ARG ∈ blackboard_R start_POSTSUPERSCRIPT italic_J × 3 end_POSTSUPERSCRIPT</annotation></semantics></math><span class="ltx_text" id="S3.SS4.p1.6.5" style="font-size:90%;"> (Procrustes-aligned) with joints </span><math alttext="\mathbf{p}_{i},\hat{\mathbf{p}}_{i}\in\mathbb{R}^{3}" class="ltx_Math" display="inline" id="S3.SS4.p1.3.m3.2"><semantics id="S3.SS4.p1.3.m3.2a"><mrow id="S3.SS4.p1.3.m3.2.2" xref="S3.SS4.p1.3.m3.2.2.cmml"><mrow id="S3.SS4.p1.3.m3.2.2.2.2" xref="S3.SS4.p1.3.m3.2.2.2.3.cmml"><msub id="S3.SS4.p1.3.m3.1.1.1.1.1" xref="S3.SS4.p1.3.m3.1.1.1.1.1.cmml"><mi id="S3.SS4.p1.3.m3.1.1.1.1.1.2" mathsize="90%" xref="S3.SS4.p1.3.m3.1.1.1.1.1.2.cmml">𝐩</mi><mi id="S3.SS4.p1.3.m3.1.1.1.1.1.3" mathsize="90%" xref="S3.SS4.p1.3.m3.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS4.p1.3.m3.2.2.2.2.3" mathsize="90%" xref="S3.SS4.p1.3.m3.2.2.2.3.cmml">,</mo><msub id="S3.SS4.p1.3.m3.2.2.2.2.2" xref="S3.SS4.p1.3.m3.2.2.2.2.2.cmml"><mover accent="true" id="S3.SS4.p1.3.m3.2.2.2.2.2.2" xref="S3.SS4.p1.3.m3.2.2.2.2.2.2.cmml"><mi id="S3.SS4.p1.3.m3.2.2.2.2.2.2.2" mathsize="90%" xref="S3.SS4.p1.3.m3.2.2.2.2.2.2.2.cmml">𝐩</mi><mo id="S3.SS4.p1.3.m3.2.2.2.2.2.2.1" mathsize="90%" xref="S3.SS4.p1.3.m3.2.2.2.2.2.2.1.cmml">^</mo></mover><mi id="S3.SS4.p1.3.m3.2.2.2.2.2.3" mathsize="90%" xref="S3.SS4.p1.3.m3.2.2.2.2.2.3.cmml">i</mi></msub></mrow><mo id="S3.SS4.p1.3.m3.2.2.3" mathsize="90%" xref="S3.SS4.p1.3.m3.2.2.3.cmml">∈</mo><msup id="S3.SS4.p1.3.m3.2.2.4" xref="S3.SS4.p1.3.m3.2.2.4.cmml"><mi id="S3.SS4.p1.3.m3.2.2.4.2" mathsize="90%" xref="S3.SS4.p1.3.m3.2.2.4.2.cmml">ℝ</mi><mn id="S3.SS4.p1.3.m3.2.2.4.3" mathsize="90%" xref="S3.SS4.p1.3.m3.2.2.4.3.cmml">3</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.3.m3.2b"><apply id="S3.SS4.p1.3.m3.2.2.cmml" xref="S3.SS4.p1.3.m3.2.2"><in id="S3.SS4.p1.3.m3.2.2.3.cmml" xref="S3.SS4.p1.3.m3.2.2.3"></in><list id="S3.SS4.p1.3.m3.2.2.2.3.cmml" xref="S3.SS4.p1.3.m3.2.2.2.2"><apply id="S3.SS4.p1.3.m3.1.1.1.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1.1.1.1">subscript</csymbol><ci id="S3.SS4.p1.3.m3.1.1.1.1.1.2.cmml" xref="S3.SS4.p1.3.m3.1.1.1.1.1.2">𝐩</ci><ci id="S3.SS4.p1.3.m3.1.1.1.1.1.3.cmml" xref="S3.SS4.p1.3.m3.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS4.p1.3.m3.2.2.2.2.2.cmml" xref="S3.SS4.p1.3.m3.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS4.p1.3.m3.2.2.2.2.2.1.cmml" xref="S3.SS4.p1.3.m3.2.2.2.2.2">subscript</csymbol><apply id="S3.SS4.p1.3.m3.2.2.2.2.2.2.cmml" xref="S3.SS4.p1.3.m3.2.2.2.2.2.2"><ci id="S3.SS4.p1.3.m3.2.2.2.2.2.2.1.cmml" xref="S3.SS4.p1.3.m3.2.2.2.2.2.2.1">^</ci><ci id="S3.SS4.p1.3.m3.2.2.2.2.2.2.2.cmml" xref="S3.SS4.p1.3.m3.2.2.2.2.2.2.2">𝐩</ci></apply><ci id="S3.SS4.p1.3.m3.2.2.2.2.2.3.cmml" xref="S3.SS4.p1.3.m3.2.2.2.2.2.3">𝑖</ci></apply></list><apply id="S3.SS4.p1.3.m3.2.2.4.cmml" xref="S3.SS4.p1.3.m3.2.2.4"><csymbol cd="ambiguous" id="S3.SS4.p1.3.m3.2.2.4.1.cmml" xref="S3.SS4.p1.3.m3.2.2.4">superscript</csymbol><ci id="S3.SS4.p1.3.m3.2.2.4.2.cmml" xref="S3.SS4.p1.3.m3.2.2.4.2">ℝ</ci><cn id="S3.SS4.p1.3.m3.2.2.4.3.cmml" type="integer" xref="S3.SS4.p1.3.m3.2.2.4.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.3.m3.2c">\mathbf{p}_{i},\hat{\mathbf{p}}_{i}\in\mathbb{R}^{3}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.3.m3.2d">bold_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , over^ start_ARG bold_p end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math><span class="ltx_text" id="S3.SS4.p1.6.6" style="font-size:90%;"> respectively, the maximum joint error
</span><math alttext="\operatorname{MaxJE}\left\lparen\hat{\mathbf{P}},\mathbf{P}\right\rparen=\max_%
{1\leq i\leq J}\left\lVert\hat{\mathbf{p}}_{i}-\mathbf{p}_{i}\right\rVert." class="ltx_Math" display="inline" id="S3.SS4.p1.4.m4.4"><semantics id="S3.SS4.p1.4.m4.4a"><mrow id="S3.SS4.p1.4.m4.4.4.1" xref="S3.SS4.p1.4.m4.4.4.1.1.cmml"><mrow id="S3.SS4.p1.4.m4.4.4.1.1" xref="S3.SS4.p1.4.m4.4.4.1.1.cmml"><mrow id="S3.SS4.p1.4.m4.4.4.1.1.4.2" xref="S3.SS4.p1.4.m4.4.4.1.1.4.1.cmml"><mi id="S3.SS4.p1.4.m4.1.1" mathsize="90%" xref="S3.SS4.p1.4.m4.1.1.cmml">MaxJE</mi><mo id="S3.SS4.p1.4.m4.4.4.1.1.4.2a" xref="S3.SS4.p1.4.m4.4.4.1.1.4.1.cmml">⁡</mo><mrow id="S3.SS4.p1.4.m4.4.4.1.1.4.2.1" xref="S3.SS4.p1.4.m4.4.4.1.1.4.1.cmml"><mo id="S3.SS4.p1.4.m4.4.4.1.1.4.2.1.1" xref="S3.SS4.p1.4.m4.4.4.1.1.4.1.cmml">(</mo><mover accent="true" id="S3.SS4.p1.4.m4.2.2" xref="S3.SS4.p1.4.m4.2.2.cmml"><mi id="S3.SS4.p1.4.m4.2.2.2" mathsize="90%" xref="S3.SS4.p1.4.m4.2.2.2.cmml">𝐏</mi><mo id="S3.SS4.p1.4.m4.2.2.1" mathsize="90%" xref="S3.SS4.p1.4.m4.2.2.1.cmml">^</mo></mover><mo id="S3.SS4.p1.4.m4.4.4.1.1.4.2.1.2" mathsize="90%" xref="S3.SS4.p1.4.m4.4.4.1.1.4.1.cmml">,</mo><mi id="S3.SS4.p1.4.m4.3.3" mathsize="90%" xref="S3.SS4.p1.4.m4.3.3.cmml">𝐏</mi><mo id="S3.SS4.p1.4.m4.4.4.1.1.4.2.1.3" xref="S3.SS4.p1.4.m4.4.4.1.1.4.1.cmml">)</mo></mrow></mrow><mo id="S3.SS4.p1.4.m4.4.4.1.1.3" mathsize="90%" xref="S3.SS4.p1.4.m4.4.4.1.1.3.cmml">=</mo><mrow id="S3.SS4.p1.4.m4.4.4.1.1.2.2" xref="S3.SS4.p1.4.m4.4.4.1.1.2.3.cmml"><msub id="S3.SS4.p1.4.m4.4.4.1.1.1.1.1" xref="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.cmml"><mi id="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.2" mathsize="90%" xref="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.2.cmml">max</mi><mrow id="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.3" xref="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.3.cmml"><mn id="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.3.2" mathsize="90%" xref="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.3.2.cmml">1</mn><mo id="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.3.3" mathsize="90%" xref="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.3.3.cmml">≤</mo><mi id="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.3.4" mathsize="90%" xref="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.3.4.cmml">i</mi><mo id="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.3.5" mathsize="90%" xref="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.3.5.cmml">≤</mo><mi id="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.3.6" mathsize="90%" xref="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.3.6.cmml">J</mi></mrow></msub><mo id="S3.SS4.p1.4.m4.4.4.1.1.2.2a" lspace="0em" xref="S3.SS4.p1.4.m4.4.4.1.1.2.3.cmml">⁡</mo><mrow id="S3.SS4.p1.4.m4.4.4.1.1.2.2.2" xref="S3.SS4.p1.4.m4.4.4.1.1.2.3.cmml"><mo fence="true" id="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.2" rspace="0em" stretchy="true" xref="S3.SS4.p1.4.m4.4.4.1.1.2.3.cmml">∥</mo><mrow id="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1" xref="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.cmml"><msub id="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.2" xref="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.2.cmml"><mover accent="true" id="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.2.2" xref="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.2.2.cmml"><mi id="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.2.2.2" mathsize="90%" xref="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.2.2.2.cmml">𝐩</mi><mo id="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.2.2.1" mathsize="90%" xref="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.2.2.1.cmml">^</mo></mover><mi id="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.2.3" mathsize="90%" xref="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.2.3.cmml">i</mi></msub><mo id="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.1" mathsize="90%" xref="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.1.cmml">−</mo><msub id="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.3" xref="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.3.cmml"><mi id="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.3.2" mathsize="90%" xref="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.3.2.cmml">𝐩</mi><mi id="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.3.3" mathsize="90%" xref="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.3.3.cmml">i</mi></msub></mrow><mo fence="true" id="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.3" lspace="0em" rspace="0em" stretchy="true" xref="S3.SS4.p1.4.m4.4.4.1.1.2.3.cmml">∥</mo></mrow></mrow></mrow><mo id="S3.SS4.p1.4.m4.4.4.1.2" lspace="0em" mathsize="90%" xref="S3.SS4.p1.4.m4.4.4.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.4.m4.4b"><apply id="S3.SS4.p1.4.m4.4.4.1.1.cmml" xref="S3.SS4.p1.4.m4.4.4.1"><eq id="S3.SS4.p1.4.m4.4.4.1.1.3.cmml" xref="S3.SS4.p1.4.m4.4.4.1.1.3"></eq><apply id="S3.SS4.p1.4.m4.4.4.1.1.4.1.cmml" xref="S3.SS4.p1.4.m4.4.4.1.1.4.2"><ci id="S3.SS4.p1.4.m4.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1">MaxJE</ci><apply id="S3.SS4.p1.4.m4.2.2.cmml" xref="S3.SS4.p1.4.m4.2.2"><ci id="S3.SS4.p1.4.m4.2.2.1.cmml" xref="S3.SS4.p1.4.m4.2.2.1">^</ci><ci id="S3.SS4.p1.4.m4.2.2.2.cmml" xref="S3.SS4.p1.4.m4.2.2.2">𝐏</ci></apply><ci id="S3.SS4.p1.4.m4.3.3.cmml" xref="S3.SS4.p1.4.m4.3.3">𝐏</ci></apply><apply id="S3.SS4.p1.4.m4.4.4.1.1.2.3.cmml" xref="S3.SS4.p1.4.m4.4.4.1.1.2.2"><apply id="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.cmml" xref="S3.SS4.p1.4.m4.4.4.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.1.cmml" xref="S3.SS4.p1.4.m4.4.4.1.1.1.1.1">subscript</csymbol><max id="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.2.cmml" xref="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.2"></max><apply id="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.3.cmml" xref="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.3"><and id="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.3a.cmml" xref="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.3"></and><apply id="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.3b.cmml" xref="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.3"><leq id="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.3.3.cmml" xref="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.3.3"></leq><cn id="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.3.2.cmml" type="integer" xref="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.3.2">1</cn><ci id="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.3.4.cmml" xref="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.3.4">𝑖</ci></apply><apply id="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.3c.cmml" xref="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.3"><leq id="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.3.5.cmml" xref="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.3.5"></leq><share href="https://arxiv.org/html/2408.16536v1#S3.SS4.p1.4.m4.4.4.1.1.1.1.1.3.4.cmml" id="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.3d.cmml" xref="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.3"></share><ci id="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.3.6.cmml" xref="S3.SS4.p1.4.m4.4.4.1.1.1.1.1.3.6">𝐽</ci></apply></apply></apply><apply id="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.cmml" xref="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1"><minus id="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.1.cmml" xref="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.1"></minus><apply id="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.2.cmml" xref="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.2"><csymbol cd="ambiguous" id="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.2.1.cmml" xref="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.2">subscript</csymbol><apply id="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.2.2.cmml" xref="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.2.2"><ci id="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.2.2.1.cmml" xref="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.2.2.1">^</ci><ci id="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.2.2.2.cmml" xref="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.2.2.2">𝐩</ci></apply><ci id="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.2.3.cmml" xref="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.2.3">𝑖</ci></apply><apply id="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.3.cmml" xref="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.3"><csymbol cd="ambiguous" id="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.3.1.cmml" xref="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.3">subscript</csymbol><ci id="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.3.2.cmml" xref="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.3.2">𝐩</ci><ci id="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.3.3.cmml" xref="S3.SS4.p1.4.m4.4.4.1.1.2.2.2.1.3.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.4.m4.4c">\operatorname{MaxJE}\left\lparen\hat{\mathbf{P}},\mathbf{P}\right\rparen=\max_%
{1\leq i\leq J}\left\lVert\hat{\mathbf{p}}_{i}-\mathbf{p}_{i}\right\rVert.</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.4.m4.4d">roman_MaxJE ( over^ start_ARG bold_P end_ARG , bold_P ) = roman_max start_POSTSUBSCRIPT 1 ≤ italic_i ≤ italic_J end_POSTSUBSCRIPT ∥ over^ start_ARG bold_p end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - bold_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∥ .</annotation></semantics></math><span class="ltx_text" id="S3.SS4.p1.6.7" style="font-size:90%;">
We employ Procrustes alignment to account for differences in camera angle between the pair of images.
A predicted pose </span><math alttext="\hat{\mathbf{P}}_{\text{att}}=f(\mathbf{x}^{\text{att}})" class="ltx_Math" display="inline" id="S3.SS4.p1.5.m5.1"><semantics id="S3.SS4.p1.5.m5.1a"><mrow id="S3.SS4.p1.5.m5.1.1" xref="S3.SS4.p1.5.m5.1.1.cmml"><msub id="S3.SS4.p1.5.m5.1.1.3" xref="S3.SS4.p1.5.m5.1.1.3.cmml"><mover accent="true" id="S3.SS4.p1.5.m5.1.1.3.2" xref="S3.SS4.p1.5.m5.1.1.3.2.cmml"><mi id="S3.SS4.p1.5.m5.1.1.3.2.2" mathsize="90%" xref="S3.SS4.p1.5.m5.1.1.3.2.2.cmml">𝐏</mi><mo id="S3.SS4.p1.5.m5.1.1.3.2.1" mathsize="90%" xref="S3.SS4.p1.5.m5.1.1.3.2.1.cmml">^</mo></mover><mtext id="S3.SS4.p1.5.m5.1.1.3.3" mathsize="90%" xref="S3.SS4.p1.5.m5.1.1.3.3a.cmml">att</mtext></msub><mo id="S3.SS4.p1.5.m5.1.1.2" mathsize="90%" xref="S3.SS4.p1.5.m5.1.1.2.cmml">=</mo><mrow id="S3.SS4.p1.5.m5.1.1.1" xref="S3.SS4.p1.5.m5.1.1.1.cmml"><mi id="S3.SS4.p1.5.m5.1.1.1.3" mathsize="90%" xref="S3.SS4.p1.5.m5.1.1.1.3.cmml">f</mi><mo id="S3.SS4.p1.5.m5.1.1.1.2" xref="S3.SS4.p1.5.m5.1.1.1.2.cmml">⁢</mo><mrow id="S3.SS4.p1.5.m5.1.1.1.1.1" xref="S3.SS4.p1.5.m5.1.1.1.1.1.1.cmml"><mo id="S3.SS4.p1.5.m5.1.1.1.1.1.2" maxsize="90%" minsize="90%" xref="S3.SS4.p1.5.m5.1.1.1.1.1.1.cmml">(</mo><msup id="S3.SS4.p1.5.m5.1.1.1.1.1.1" xref="S3.SS4.p1.5.m5.1.1.1.1.1.1.cmml"><mi id="S3.SS4.p1.5.m5.1.1.1.1.1.1.2" mathsize="90%" xref="S3.SS4.p1.5.m5.1.1.1.1.1.1.2.cmml">𝐱</mi><mtext id="S3.SS4.p1.5.m5.1.1.1.1.1.1.3" mathsize="90%" xref="S3.SS4.p1.5.m5.1.1.1.1.1.1.3a.cmml">att</mtext></msup><mo id="S3.SS4.p1.5.m5.1.1.1.1.1.3" maxsize="90%" minsize="90%" xref="S3.SS4.p1.5.m5.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.5.m5.1b"><apply id="S3.SS4.p1.5.m5.1.1.cmml" xref="S3.SS4.p1.5.m5.1.1"><eq id="S3.SS4.p1.5.m5.1.1.2.cmml" xref="S3.SS4.p1.5.m5.1.1.2"></eq><apply id="S3.SS4.p1.5.m5.1.1.3.cmml" xref="S3.SS4.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p1.5.m5.1.1.3.1.cmml" xref="S3.SS4.p1.5.m5.1.1.3">subscript</csymbol><apply id="S3.SS4.p1.5.m5.1.1.3.2.cmml" xref="S3.SS4.p1.5.m5.1.1.3.2"><ci id="S3.SS4.p1.5.m5.1.1.3.2.1.cmml" xref="S3.SS4.p1.5.m5.1.1.3.2.1">^</ci><ci id="S3.SS4.p1.5.m5.1.1.3.2.2.cmml" xref="S3.SS4.p1.5.m5.1.1.3.2.2">𝐏</ci></apply><ci id="S3.SS4.p1.5.m5.1.1.3.3a.cmml" xref="S3.SS4.p1.5.m5.1.1.3.3"><mtext id="S3.SS4.p1.5.m5.1.1.3.3.cmml" mathsize="63%" xref="S3.SS4.p1.5.m5.1.1.3.3">att</mtext></ci></apply><apply id="S3.SS4.p1.5.m5.1.1.1.cmml" xref="S3.SS4.p1.5.m5.1.1.1"><times id="S3.SS4.p1.5.m5.1.1.1.2.cmml" xref="S3.SS4.p1.5.m5.1.1.1.2"></times><ci id="S3.SS4.p1.5.m5.1.1.1.3.cmml" xref="S3.SS4.p1.5.m5.1.1.1.3">𝑓</ci><apply id="S3.SS4.p1.5.m5.1.1.1.1.1.1.cmml" xref="S3.SS4.p1.5.m5.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.5.m5.1.1.1.1.1.1.1.cmml" xref="S3.SS4.p1.5.m5.1.1.1.1.1">superscript</csymbol><ci id="S3.SS4.p1.5.m5.1.1.1.1.1.1.2.cmml" xref="S3.SS4.p1.5.m5.1.1.1.1.1.1.2">𝐱</ci><ci id="S3.SS4.p1.5.m5.1.1.1.1.1.1.3a.cmml" xref="S3.SS4.p1.5.m5.1.1.1.1.1.1.3"><mtext id="S3.SS4.p1.5.m5.1.1.1.1.1.1.3.cmml" mathsize="63%" xref="S3.SS4.p1.5.m5.1.1.1.1.1.1.3">att</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.5.m5.1c">\hat{\mathbf{P}}_{\text{att}}=f(\mathbf{x}^{\text{att}})</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.5.m5.1d">over^ start_ARG bold_P end_ARG start_POSTSUBSCRIPT att end_POSTSUBSCRIPT = italic_f ( bold_x start_POSTSUPERSCRIPT att end_POSTSUPERSCRIPT )</annotation></semantics></math><span class="ltx_text" id="S3.SS4.p1.6.8" style="font-size:90%;"> is then called degraded if its error is worse than the base case by a larger margin than a given threshold </span><math alttext="\tau" class="ltx_Math" display="inline" id="S3.SS4.p1.6.m6.1"><semantics id="S3.SS4.p1.6.m6.1a"><mi id="S3.SS4.p1.6.m6.1.1" mathsize="90%" xref="S3.SS4.p1.6.m6.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.6.m6.1b"><ci id="S3.SS4.p1.6.m6.1.1.cmml" xref="S3.SS4.p1.6.m6.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.6.m6.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.6.m6.1d">italic_τ</annotation></semantics></math><span class="ltx_text" id="S3.SS4.p1.6.9" style="font-size:90%;">, </span><em class="ltx_emph ltx_font_italic" id="S3.SS4.p1.6.10" style="font-size:90%;">i.e</em><span class="ltx_text" id="S3.SS4.p1.6.11" style="font-size:90%;">.</span><span class="ltx_text" id="S3.SS4.p1.6.12"></span><span class="ltx_text" id="S3.SS4.p1.6.13" style="font-size:90%;">, when</span></p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\footnotesize L(\hat{\mathbf{P}}_{\text{att}},\hat{\mathbf{P}}_{\text{base}},%
\mathbf{P},\tau)=\left[\operatorname{MaxJE}\left\lparen\hat{\mathbf{P}}_{\text%
{att}},\mathbf{P}\right\rparen-\operatorname{MaxJE}\left\lparen\hat{\mathbf{P}%
}_{\text{base}},\mathbf{P}\right\rparen&gt;\tau\right]" class="ltx_Math" display="block" id="S3.E3.m1.9"><semantics id="S3.E3.m1.9a"><mrow id="S3.E3.m1.9.9" xref="S3.E3.m1.9.9.cmml"><mrow id="S3.E3.m1.8.8.2" xref="S3.E3.m1.8.8.2.cmml"><mi id="S3.E3.m1.8.8.2.4" mathsize="80%" xref="S3.E3.m1.8.8.2.4.cmml">L</mi><mo id="S3.E3.m1.8.8.2.3" xref="S3.E3.m1.8.8.2.3.cmml">⁢</mo><mrow id="S3.E3.m1.8.8.2.2.2" xref="S3.E3.m1.8.8.2.2.3.cmml"><mo id="S3.E3.m1.8.8.2.2.2.3" maxsize="80%" minsize="80%" xref="S3.E3.m1.8.8.2.2.3.cmml">(</mo><msub id="S3.E3.m1.7.7.1.1.1.1" xref="S3.E3.m1.7.7.1.1.1.1.cmml"><mover accent="true" id="S3.E3.m1.7.7.1.1.1.1.2" xref="S3.E3.m1.7.7.1.1.1.1.2.cmml"><mi id="S3.E3.m1.7.7.1.1.1.1.2.2" mathsize="80%" xref="S3.E3.m1.7.7.1.1.1.1.2.2.cmml">𝐏</mi><mo id="S3.E3.m1.7.7.1.1.1.1.2.1" mathsize="80%" xref="S3.E3.m1.7.7.1.1.1.1.2.1.cmml">^</mo></mover><mtext id="S3.E3.m1.7.7.1.1.1.1.3" mathsize="80%" xref="S3.E3.m1.7.7.1.1.1.1.3a.cmml">att</mtext></msub><mo id="S3.E3.m1.8.8.2.2.2.4" mathsize="80%" xref="S3.E3.m1.8.8.2.2.3.cmml">,</mo><msub id="S3.E3.m1.8.8.2.2.2.2" xref="S3.E3.m1.8.8.2.2.2.2.cmml"><mover accent="true" id="S3.E3.m1.8.8.2.2.2.2.2" xref="S3.E3.m1.8.8.2.2.2.2.2.cmml"><mi id="S3.E3.m1.8.8.2.2.2.2.2.2" mathsize="80%" xref="S3.E3.m1.8.8.2.2.2.2.2.2.cmml">𝐏</mi><mo id="S3.E3.m1.8.8.2.2.2.2.2.1" mathsize="80%" xref="S3.E3.m1.8.8.2.2.2.2.2.1.cmml">^</mo></mover><mtext id="S3.E3.m1.8.8.2.2.2.2.3" mathsize="80%" xref="S3.E3.m1.8.8.2.2.2.2.3a.cmml">base</mtext></msub><mo id="S3.E3.m1.8.8.2.2.2.5" mathsize="80%" xref="S3.E3.m1.8.8.2.2.3.cmml">,</mo><mi id="S3.E3.m1.1.1" mathsize="80%" xref="S3.E3.m1.1.1.cmml">𝐏</mi><mo id="S3.E3.m1.8.8.2.2.2.6" mathsize="80%" xref="S3.E3.m1.8.8.2.2.3.cmml">,</mo><mi id="S3.E3.m1.2.2" mathsize="80%" xref="S3.E3.m1.2.2.cmml">τ</mi><mo id="S3.E3.m1.8.8.2.2.2.7" maxsize="80%" minsize="80%" xref="S3.E3.m1.8.8.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.9.9.4" mathsize="80%" xref="S3.E3.m1.9.9.4.cmml">=</mo><mrow id="S3.E3.m1.9.9.3.1" xref="S3.E3.m1.9.9.3.2.cmml"><mo id="S3.E3.m1.9.9.3.1.2" xref="S3.E3.m1.9.9.3.2.1.cmml">[</mo><mrow id="S3.E3.m1.9.9.3.1.1" xref="S3.E3.m1.9.9.3.1.1.cmml"><mrow id="S3.E3.m1.9.9.3.1.1.2" xref="S3.E3.m1.9.9.3.1.1.2.cmml"><mrow id="S3.E3.m1.9.9.3.1.1.1.1.1" xref="S3.E3.m1.9.9.3.1.1.1.1.2.cmml"><mi id="S3.E3.m1.3.3" mathsize="80%" xref="S3.E3.m1.3.3.cmml">MaxJE</mi><mo id="S3.E3.m1.9.9.3.1.1.1.1.1a" xref="S3.E3.m1.9.9.3.1.1.1.1.2.cmml">⁡</mo><mrow id="S3.E3.m1.9.9.3.1.1.1.1.1.1" xref="S3.E3.m1.9.9.3.1.1.1.1.2.cmml"><mo id="S3.E3.m1.9.9.3.1.1.1.1.1.1.2" xref="S3.E3.m1.9.9.3.1.1.1.1.2.cmml">(</mo><msub id="S3.E3.m1.9.9.3.1.1.1.1.1.1.1" xref="S3.E3.m1.9.9.3.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.E3.m1.9.9.3.1.1.1.1.1.1.1.2" xref="S3.E3.m1.9.9.3.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E3.m1.9.9.3.1.1.1.1.1.1.1.2.2" mathsize="80%" xref="S3.E3.m1.9.9.3.1.1.1.1.1.1.1.2.2.cmml">𝐏</mi><mo id="S3.E3.m1.9.9.3.1.1.1.1.1.1.1.2.1" mathsize="80%" xref="S3.E3.m1.9.9.3.1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mtext id="S3.E3.m1.9.9.3.1.1.1.1.1.1.1.3" mathsize="80%" xref="S3.E3.m1.9.9.3.1.1.1.1.1.1.1.3a.cmml">att</mtext></msub><mo id="S3.E3.m1.9.9.3.1.1.1.1.1.1.3" mathsize="80%" xref="S3.E3.m1.9.9.3.1.1.1.1.2.cmml">,</mo><mi id="S3.E3.m1.4.4" mathsize="80%" xref="S3.E3.m1.4.4.cmml">𝐏</mi><mo id="S3.E3.m1.9.9.3.1.1.1.1.1.1.4" xref="S3.E3.m1.9.9.3.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.9.9.3.1.1.2.3" mathsize="80%" xref="S3.E3.m1.9.9.3.1.1.2.3.cmml">−</mo><mrow id="S3.E3.m1.9.9.3.1.1.2.2.1" xref="S3.E3.m1.9.9.3.1.1.2.2.2.cmml"><mi id="S3.E3.m1.5.5" mathsize="80%" xref="S3.E3.m1.5.5.cmml">MaxJE</mi><mo id="S3.E3.m1.9.9.3.1.1.2.2.1a" xref="S3.E3.m1.9.9.3.1.1.2.2.2.cmml">⁡</mo><mrow id="S3.E3.m1.9.9.3.1.1.2.2.1.1" xref="S3.E3.m1.9.9.3.1.1.2.2.2.cmml"><mo id="S3.E3.m1.9.9.3.1.1.2.2.1.1.2" xref="S3.E3.m1.9.9.3.1.1.2.2.2.cmml">(</mo><msub id="S3.E3.m1.9.9.3.1.1.2.2.1.1.1" xref="S3.E3.m1.9.9.3.1.1.2.2.1.1.1.cmml"><mover accent="true" id="S3.E3.m1.9.9.3.1.1.2.2.1.1.1.2" xref="S3.E3.m1.9.9.3.1.1.2.2.1.1.1.2.cmml"><mi id="S3.E3.m1.9.9.3.1.1.2.2.1.1.1.2.2" mathsize="80%" xref="S3.E3.m1.9.9.3.1.1.2.2.1.1.1.2.2.cmml">𝐏</mi><mo id="S3.E3.m1.9.9.3.1.1.2.2.1.1.1.2.1" mathsize="80%" xref="S3.E3.m1.9.9.3.1.1.2.2.1.1.1.2.1.cmml">^</mo></mover><mtext id="S3.E3.m1.9.9.3.1.1.2.2.1.1.1.3" mathsize="80%" xref="S3.E3.m1.9.9.3.1.1.2.2.1.1.1.3a.cmml">base</mtext></msub><mo id="S3.E3.m1.9.9.3.1.1.2.2.1.1.3" mathsize="80%" xref="S3.E3.m1.9.9.3.1.1.2.2.2.cmml">,</mo><mi id="S3.E3.m1.6.6" mathsize="80%" xref="S3.E3.m1.6.6.cmml">𝐏</mi><mo id="S3.E3.m1.9.9.3.1.1.2.2.1.1.4" xref="S3.E3.m1.9.9.3.1.1.2.2.2.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E3.m1.9.9.3.1.1.3" mathsize="80%" xref="S3.E3.m1.9.9.3.1.1.3.cmml">&gt;</mo><mi id="S3.E3.m1.9.9.3.1.1.4" mathsize="80%" xref="S3.E3.m1.9.9.3.1.1.4.cmml">τ</mi></mrow><mo id="S3.E3.m1.9.9.3.1.3" xref="S3.E3.m1.9.9.3.2.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.9b"><apply id="S3.E3.m1.9.9.cmml" xref="S3.E3.m1.9.9"><eq id="S3.E3.m1.9.9.4.cmml" xref="S3.E3.m1.9.9.4"></eq><apply id="S3.E3.m1.8.8.2.cmml" xref="S3.E3.m1.8.8.2"><times id="S3.E3.m1.8.8.2.3.cmml" xref="S3.E3.m1.8.8.2.3"></times><ci id="S3.E3.m1.8.8.2.4.cmml" xref="S3.E3.m1.8.8.2.4">𝐿</ci><vector id="S3.E3.m1.8.8.2.2.3.cmml" xref="S3.E3.m1.8.8.2.2.2"><apply id="S3.E3.m1.7.7.1.1.1.1.cmml" xref="S3.E3.m1.7.7.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.7.7.1.1.1.1.1.cmml" xref="S3.E3.m1.7.7.1.1.1.1">subscript</csymbol><apply id="S3.E3.m1.7.7.1.1.1.1.2.cmml" xref="S3.E3.m1.7.7.1.1.1.1.2"><ci id="S3.E3.m1.7.7.1.1.1.1.2.1.cmml" xref="S3.E3.m1.7.7.1.1.1.1.2.1">^</ci><ci id="S3.E3.m1.7.7.1.1.1.1.2.2.cmml" xref="S3.E3.m1.7.7.1.1.1.1.2.2">𝐏</ci></apply><ci id="S3.E3.m1.7.7.1.1.1.1.3a.cmml" xref="S3.E3.m1.7.7.1.1.1.1.3"><mtext id="S3.E3.m1.7.7.1.1.1.1.3.cmml" mathsize="56%" xref="S3.E3.m1.7.7.1.1.1.1.3">att</mtext></ci></apply><apply id="S3.E3.m1.8.8.2.2.2.2.cmml" xref="S3.E3.m1.8.8.2.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.8.8.2.2.2.2.1.cmml" xref="S3.E3.m1.8.8.2.2.2.2">subscript</csymbol><apply id="S3.E3.m1.8.8.2.2.2.2.2.cmml" xref="S3.E3.m1.8.8.2.2.2.2.2"><ci id="S3.E3.m1.8.8.2.2.2.2.2.1.cmml" xref="S3.E3.m1.8.8.2.2.2.2.2.1">^</ci><ci id="S3.E3.m1.8.8.2.2.2.2.2.2.cmml" xref="S3.E3.m1.8.8.2.2.2.2.2.2">𝐏</ci></apply><ci id="S3.E3.m1.8.8.2.2.2.2.3a.cmml" xref="S3.E3.m1.8.8.2.2.2.2.3"><mtext id="S3.E3.m1.8.8.2.2.2.2.3.cmml" mathsize="56%" xref="S3.E3.m1.8.8.2.2.2.2.3">base</mtext></ci></apply><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">𝐏</ci><ci id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2">𝜏</ci></vector></apply><apply id="S3.E3.m1.9.9.3.2.cmml" xref="S3.E3.m1.9.9.3.1"><csymbol cd="latexml" id="S3.E3.m1.9.9.3.2.1.cmml" xref="S3.E3.m1.9.9.3.1.2">delimited-[]</csymbol><apply id="S3.E3.m1.9.9.3.1.1.cmml" xref="S3.E3.m1.9.9.3.1.1"><gt id="S3.E3.m1.9.9.3.1.1.3.cmml" xref="S3.E3.m1.9.9.3.1.1.3"></gt><apply id="S3.E3.m1.9.9.3.1.1.2.cmml" xref="S3.E3.m1.9.9.3.1.1.2"><minus id="S3.E3.m1.9.9.3.1.1.2.3.cmml" xref="S3.E3.m1.9.9.3.1.1.2.3"></minus><apply id="S3.E3.m1.9.9.3.1.1.1.1.2.cmml" xref="S3.E3.m1.9.9.3.1.1.1.1.1"><ci id="S3.E3.m1.3.3.cmml" xref="S3.E3.m1.3.3">MaxJE</ci><apply id="S3.E3.m1.9.9.3.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.9.9.3.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.9.9.3.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.9.9.3.1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E3.m1.9.9.3.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.9.9.3.1.1.1.1.1.1.1.2"><ci id="S3.E3.m1.9.9.3.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.9.9.3.1.1.1.1.1.1.1.2.1">^</ci><ci id="S3.E3.m1.9.9.3.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.9.9.3.1.1.1.1.1.1.1.2.2">𝐏</ci></apply><ci id="S3.E3.m1.9.9.3.1.1.1.1.1.1.1.3a.cmml" xref="S3.E3.m1.9.9.3.1.1.1.1.1.1.1.3"><mtext id="S3.E3.m1.9.9.3.1.1.1.1.1.1.1.3.cmml" mathsize="56%" xref="S3.E3.m1.9.9.3.1.1.1.1.1.1.1.3">att</mtext></ci></apply><ci id="S3.E3.m1.4.4.cmml" xref="S3.E3.m1.4.4">𝐏</ci></apply><apply id="S3.E3.m1.9.9.3.1.1.2.2.2.cmml" xref="S3.E3.m1.9.9.3.1.1.2.2.1"><ci id="S3.E3.m1.5.5.cmml" xref="S3.E3.m1.5.5">MaxJE</ci><apply id="S3.E3.m1.9.9.3.1.1.2.2.1.1.1.cmml" xref="S3.E3.m1.9.9.3.1.1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.9.9.3.1.1.2.2.1.1.1.1.cmml" xref="S3.E3.m1.9.9.3.1.1.2.2.1.1.1">subscript</csymbol><apply id="S3.E3.m1.9.9.3.1.1.2.2.1.1.1.2.cmml" xref="S3.E3.m1.9.9.3.1.1.2.2.1.1.1.2"><ci id="S3.E3.m1.9.9.3.1.1.2.2.1.1.1.2.1.cmml" xref="S3.E3.m1.9.9.3.1.1.2.2.1.1.1.2.1">^</ci><ci id="S3.E3.m1.9.9.3.1.1.2.2.1.1.1.2.2.cmml" xref="S3.E3.m1.9.9.3.1.1.2.2.1.1.1.2.2">𝐏</ci></apply><ci id="S3.E3.m1.9.9.3.1.1.2.2.1.1.1.3a.cmml" xref="S3.E3.m1.9.9.3.1.1.2.2.1.1.1.3"><mtext id="S3.E3.m1.9.9.3.1.1.2.2.1.1.1.3.cmml" mathsize="56%" xref="S3.E3.m1.9.9.3.1.1.2.2.1.1.1.3">base</mtext></ci></apply><ci id="S3.E3.m1.6.6.cmml" xref="S3.E3.m1.6.6">𝐏</ci></apply></apply><ci id="S3.E3.m1.9.9.3.1.1.4.cmml" xref="S3.E3.m1.9.9.3.1.1.4">𝜏</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.9c">\footnotesize L(\hat{\mathbf{P}}_{\text{att}},\hat{\mathbf{P}}_{\text{base}},%
\mathbf{P},\tau)=\left[\operatorname{MaxJE}\left\lparen\hat{\mathbf{P}}_{\text%
{att}},\mathbf{P}\right\rparen-\operatorname{MaxJE}\left\lparen\hat{\mathbf{P}%
}_{\text{base}},\mathbf{P}\right\rparen&gt;\tau\right]</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.9d">italic_L ( over^ start_ARG bold_P end_ARG start_POSTSUBSCRIPT att end_POSTSUBSCRIPT , over^ start_ARG bold_P end_ARG start_POSTSUBSCRIPT base end_POSTSUBSCRIPT , bold_P , italic_τ ) = [ roman_MaxJE ( over^ start_ARG bold_P end_ARG start_POSTSUBSCRIPT att end_POSTSUBSCRIPT , bold_P ) - roman_MaxJE ( over^ start_ARG bold_P end_ARG start_POSTSUBSCRIPT base end_POSTSUBSCRIPT , bold_P ) &gt; italic_τ ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS4.p1.8"><span class="ltx_text" id="S3.SS4.p1.8.1" style="font-size:90%;">is one.
The Percentage of Degraded Poses (PDP) for a threshold </span><math alttext="\tau" class="ltx_Math" display="inline" id="S3.SS4.p1.7.m1.1"><semantics id="S3.SS4.p1.7.m1.1a"><mi id="S3.SS4.p1.7.m1.1.1" mathsize="90%" xref="S3.SS4.p1.7.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.7.m1.1b"><ci id="S3.SS4.p1.7.m1.1.1.cmml" xref="S3.SS4.p1.7.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.7.m1.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.7.m1.1d">italic_τ</annotation></semantics></math><span class="ltx_text" id="S3.SS4.p1.8.2" style="font-size:90%;"> for an attribute with dataset size </span><math alttext="N" class="ltx_Math" display="inline" id="S3.SS4.p1.8.m2.1"><semantics id="S3.SS4.p1.8.m2.1a"><mi id="S3.SS4.p1.8.m2.1.1" mathsize="90%" xref="S3.SS4.p1.8.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.8.m2.1b"><ci id="S3.SS4.p1.8.m2.1.1.cmml" xref="S3.SS4.p1.8.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.8.m2.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.8.m2.1d">italic_N</annotation></semantics></math><span class="ltx_text" id="S3.SS4.p1.8.3" style="font-size:90%;"> is then</span></p>
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\footnotesize\text{PDP}=\frac{1}{N}\sum_{i=1}^{N}L(\hat{\mathbf{P}}_{\text{att%
,i}},\hat{\mathbf{P}}_{\text{base,i}},\mathbf{P}_{i},\tau)," class="ltx_Math" display="block" id="S3.E4.m1.2"><semantics id="S3.E4.m1.2a"><mrow id="S3.E4.m1.2.2.1" xref="S3.E4.m1.2.2.1.1.cmml"><mrow id="S3.E4.m1.2.2.1.1" xref="S3.E4.m1.2.2.1.1.cmml"><mtext id="S3.E4.m1.2.2.1.1.5" mathsize="80%" xref="S3.E4.m1.2.2.1.1.5a.cmml">PDP</mtext><mo id="S3.E4.m1.2.2.1.1.4" mathsize="80%" xref="S3.E4.m1.2.2.1.1.4.cmml">=</mo><mrow id="S3.E4.m1.2.2.1.1.3" xref="S3.E4.m1.2.2.1.1.3.cmml"><mfrac id="S3.E4.m1.2.2.1.1.3.5" xref="S3.E4.m1.2.2.1.1.3.5.cmml"><mn id="S3.E4.m1.2.2.1.1.3.5.2" mathsize="80%" xref="S3.E4.m1.2.2.1.1.3.5.2.cmml">1</mn><mi id="S3.E4.m1.2.2.1.1.3.5.3" mathsize="80%" xref="S3.E4.m1.2.2.1.1.3.5.3.cmml">N</mi></mfrac><mo id="S3.E4.m1.2.2.1.1.3.4" xref="S3.E4.m1.2.2.1.1.3.4.cmml">⁢</mo><mrow id="S3.E4.m1.2.2.1.1.3.3" xref="S3.E4.m1.2.2.1.1.3.3.cmml"><munderover id="S3.E4.m1.2.2.1.1.3.3.4" xref="S3.E4.m1.2.2.1.1.3.3.4.cmml"><mo id="S3.E4.m1.2.2.1.1.3.3.4.2.2" maxsize="80%" minsize="80%" movablelimits="false" stretchy="true" xref="S3.E4.m1.2.2.1.1.3.3.4.2.2.cmml">∑</mo><mrow id="S3.E4.m1.2.2.1.1.3.3.4.2.3" xref="S3.E4.m1.2.2.1.1.3.3.4.2.3.cmml"><mi id="S3.E4.m1.2.2.1.1.3.3.4.2.3.2" mathsize="80%" xref="S3.E4.m1.2.2.1.1.3.3.4.2.3.2.cmml">i</mi><mo id="S3.E4.m1.2.2.1.1.3.3.4.2.3.1" mathsize="80%" xref="S3.E4.m1.2.2.1.1.3.3.4.2.3.1.cmml">=</mo><mn id="S3.E4.m1.2.2.1.1.3.3.4.2.3.3" mathsize="80%" xref="S3.E4.m1.2.2.1.1.3.3.4.2.3.3.cmml">1</mn></mrow><mi id="S3.E4.m1.2.2.1.1.3.3.4.3" mathsize="80%" xref="S3.E4.m1.2.2.1.1.3.3.4.3.cmml">N</mi></munderover><mrow id="S3.E4.m1.2.2.1.1.3.3.3" xref="S3.E4.m1.2.2.1.1.3.3.3.cmml"><mi id="S3.E4.m1.2.2.1.1.3.3.3.5" mathsize="80%" xref="S3.E4.m1.2.2.1.1.3.3.3.5.cmml">L</mi><mo id="S3.E4.m1.2.2.1.1.3.3.3.4" xref="S3.E4.m1.2.2.1.1.3.3.3.4.cmml">⁢</mo><mrow id="S3.E4.m1.2.2.1.1.3.3.3.3.3" xref="S3.E4.m1.2.2.1.1.3.3.3.3.4.cmml"><mo id="S3.E4.m1.2.2.1.1.3.3.3.3.3.4" maxsize="80%" minsize="80%" xref="S3.E4.m1.2.2.1.1.3.3.3.3.4.cmml">(</mo><msub id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.2.2" mathsize="80%" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.2.2.cmml">𝐏</mi><mo id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.2.1" mathsize="80%" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mtext id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.3" mathsize="80%" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.3a.cmml">att,i</mtext></msub><mo id="S3.E4.m1.2.2.1.1.3.3.3.3.3.5" mathsize="80%" xref="S3.E4.m1.2.2.1.1.3.3.3.3.4.cmml">,</mo><msub id="S3.E4.m1.2.2.1.1.2.2.2.2.2.2" xref="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.cmml"><mover accent="true" id="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.2" xref="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.2.cmml"><mi id="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.2.2" mathsize="80%" xref="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.2.2.cmml">𝐏</mi><mo id="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.2.1" mathsize="80%" xref="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.2.1.cmml">^</mo></mover><mtext id="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.3" mathsize="80%" xref="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.3a.cmml">base,i</mtext></msub><mo id="S3.E4.m1.2.2.1.1.3.3.3.3.3.6" mathsize="80%" xref="S3.E4.m1.2.2.1.1.3.3.3.3.4.cmml">,</mo><msub id="S3.E4.m1.2.2.1.1.3.3.3.3.3.3" xref="S3.E4.m1.2.2.1.1.3.3.3.3.3.3.cmml"><mi id="S3.E4.m1.2.2.1.1.3.3.3.3.3.3.2" mathsize="80%" xref="S3.E4.m1.2.2.1.1.3.3.3.3.3.3.2.cmml">𝐏</mi><mi id="S3.E4.m1.2.2.1.1.3.3.3.3.3.3.3" mathsize="80%" xref="S3.E4.m1.2.2.1.1.3.3.3.3.3.3.3.cmml">i</mi></msub><mo id="S3.E4.m1.2.2.1.1.3.3.3.3.3.7" mathsize="80%" xref="S3.E4.m1.2.2.1.1.3.3.3.3.4.cmml">,</mo><mi id="S3.E4.m1.1.1" mathsize="80%" xref="S3.E4.m1.1.1.cmml">τ</mi><mo id="S3.E4.m1.2.2.1.1.3.3.3.3.3.8" maxsize="80%" minsize="80%" xref="S3.E4.m1.2.2.1.1.3.3.3.3.4.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S3.E4.m1.2.2.1.2" mathsize="80%" xref="S3.E4.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.2b"><apply id="S3.E4.m1.2.2.1.1.cmml" xref="S3.E4.m1.2.2.1"><eq id="S3.E4.m1.2.2.1.1.4.cmml" xref="S3.E4.m1.2.2.1.1.4"></eq><ci id="S3.E4.m1.2.2.1.1.5a.cmml" xref="S3.E4.m1.2.2.1.1.5"><mtext id="S3.E4.m1.2.2.1.1.5.cmml" mathsize="80%" xref="S3.E4.m1.2.2.1.1.5">PDP</mtext></ci><apply id="S3.E4.m1.2.2.1.1.3.cmml" xref="S3.E4.m1.2.2.1.1.3"><times id="S3.E4.m1.2.2.1.1.3.4.cmml" xref="S3.E4.m1.2.2.1.1.3.4"></times><apply id="S3.E4.m1.2.2.1.1.3.5.cmml" xref="S3.E4.m1.2.2.1.1.3.5"><divide id="S3.E4.m1.2.2.1.1.3.5.1.cmml" xref="S3.E4.m1.2.2.1.1.3.5"></divide><cn id="S3.E4.m1.2.2.1.1.3.5.2.cmml" type="integer" xref="S3.E4.m1.2.2.1.1.3.5.2">1</cn><ci id="S3.E4.m1.2.2.1.1.3.5.3.cmml" xref="S3.E4.m1.2.2.1.1.3.5.3">𝑁</ci></apply><apply id="S3.E4.m1.2.2.1.1.3.3.cmml" xref="S3.E4.m1.2.2.1.1.3.3"><apply id="S3.E4.m1.2.2.1.1.3.3.4.cmml" xref="S3.E4.m1.2.2.1.1.3.3.4"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.3.3.4.1.cmml" xref="S3.E4.m1.2.2.1.1.3.3.4">superscript</csymbol><apply id="S3.E4.m1.2.2.1.1.3.3.4.2.cmml" xref="S3.E4.m1.2.2.1.1.3.3.4"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.3.3.4.2.1.cmml" xref="S3.E4.m1.2.2.1.1.3.3.4">subscript</csymbol><sum id="S3.E4.m1.2.2.1.1.3.3.4.2.2.cmml" xref="S3.E4.m1.2.2.1.1.3.3.4.2.2"></sum><apply id="S3.E4.m1.2.2.1.1.3.3.4.2.3.cmml" xref="S3.E4.m1.2.2.1.1.3.3.4.2.3"><eq id="S3.E4.m1.2.2.1.1.3.3.4.2.3.1.cmml" xref="S3.E4.m1.2.2.1.1.3.3.4.2.3.1"></eq><ci id="S3.E4.m1.2.2.1.1.3.3.4.2.3.2.cmml" xref="S3.E4.m1.2.2.1.1.3.3.4.2.3.2">𝑖</ci><cn id="S3.E4.m1.2.2.1.1.3.3.4.2.3.3.cmml" type="integer" xref="S3.E4.m1.2.2.1.1.3.3.4.2.3.3">1</cn></apply></apply><ci id="S3.E4.m1.2.2.1.1.3.3.4.3.cmml" xref="S3.E4.m1.2.2.1.1.3.3.4.3">𝑁</ci></apply><apply id="S3.E4.m1.2.2.1.1.3.3.3.cmml" xref="S3.E4.m1.2.2.1.1.3.3.3"><times id="S3.E4.m1.2.2.1.1.3.3.3.4.cmml" xref="S3.E4.m1.2.2.1.1.3.3.3.4"></times><ci id="S3.E4.m1.2.2.1.1.3.3.3.5.cmml" xref="S3.E4.m1.2.2.1.1.3.3.3.5">𝐿</ci><vector id="S3.E4.m1.2.2.1.1.3.3.3.3.4.cmml" xref="S3.E4.m1.2.2.1.1.3.3.3.3.3"><apply id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.2"><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.2.1">^</ci><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.2.2">𝐏</ci></apply><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.3a.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.3"><mtext id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.3.cmml" mathsize="56%" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.3">att,i</mtext></ci></apply><apply id="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.cmml" xref="S3.E4.m1.2.2.1.1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.1.cmml" xref="S3.E4.m1.2.2.1.1.2.2.2.2.2.2">subscript</csymbol><apply id="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.2.cmml" xref="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.2"><ci id="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.2.1.cmml" xref="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.2.1">^</ci><ci id="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.2.2.cmml" xref="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.2.2">𝐏</ci></apply><ci id="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.3a.cmml" xref="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.3"><mtext id="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.3.cmml" mathsize="56%" xref="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.3">base,i</mtext></ci></apply><apply id="S3.E4.m1.2.2.1.1.3.3.3.3.3.3.cmml" xref="S3.E4.m1.2.2.1.1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.3.3.3.3.3.3.1.cmml" xref="S3.E4.m1.2.2.1.1.3.3.3.3.3.3">subscript</csymbol><ci id="S3.E4.m1.2.2.1.1.3.3.3.3.3.3.2.cmml" xref="S3.E4.m1.2.2.1.1.3.3.3.3.3.3.2">𝐏</ci><ci id="S3.E4.m1.2.2.1.1.3.3.3.3.3.3.3.cmml" xref="S3.E4.m1.2.2.1.1.3.3.3.3.3.3.3">𝑖</ci></apply><ci id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1">𝜏</ci></vector></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.2c">\footnotesize\text{PDP}=\frac{1}{N}\sum_{i=1}^{N}L(\hat{\mathbf{P}}_{\text{att%
,i}},\hat{\mathbf{P}}_{\text{base,i}},\mathbf{P}_{i},\tau),</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.2d">PDP = divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_L ( over^ start_ARG bold_P end_ARG start_POSTSUBSCRIPT att,i end_POSTSUBSCRIPT , over^ start_ARG bold_P end_ARG start_POSTSUBSCRIPT base,i end_POSTSUBSCRIPT , bold_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_τ ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS4.p1.9"><span class="ltx_text" id="S3.SS4.p1.9.1" style="font-size:90%;">which is exactly the estimate for the risk </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3.E2" style="font-size:90%;" title="In 3.1 Evaluation with Controlled Generated Test Sets ‣ 3 Method ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Eq.</span> <span class="ltx_text ltx_ref_tag">2</span></a><span class="ltx_text" id="S3.SS4.p1.9.2" style="font-size:90%;"> when using the degradation criterion </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3.E3" style="font-size:90%;" title="In 3.4 Evaluation Protocol ‣ 3 Method ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Eq.</span> <span class="ltx_text ltx_ref_tag">3</span></a><span class="ltx_text" id="S3.SS4.p1.9.3" style="font-size:90%;"> as the loss </span><math alttext="L" class="ltx_Math" display="inline" id="S3.SS4.p1.9.m1.1"><semantics id="S3.SS4.p1.9.m1.1a"><mi id="S3.SS4.p1.9.m1.1.1" mathsize="90%" xref="S3.SS4.p1.9.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.9.m1.1b"><ci id="S3.SS4.p1.9.m1.1.1.cmml" xref="S3.SS4.p1.9.m1.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.9.m1.1c">L</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.9.m1.1d">italic_L</annotation></semantics></math><span class="ltx_text" id="S3.SS4.p1.9.4" style="font-size:90%;"> in </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3.E2" style="font-size:90%;" title="In 3.1 Evaluation with Controlled Generated Test Sets ‣ 3 Method ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Eq.</span> <span class="ltx_text ltx_ref_tag">2</span></a><span class="ltx_text" id="S3.SS4.p1.9.5" style="font-size:90%;">.
Note that the PDP is nonnegative by construction.
It aims to count the especially high-risk instances where a pose estimator performs well under a common benchmark scenario but degrades when moving to the open world, indicating non-robustness.
To compute the PDP for a category, we average the PDP for each attribute in the category.
The overall PDP score for a pose estimator is the mean across all categories.</span></p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<figure class="ltx_figure" id="S4.F4">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.F4.4">
<tr class="ltx_tr" id="S4.F4.4.5">
<td class="ltx_td ltx_align_center" colspan="2" id="S4.F4.4.5.1"><span class="ltx_text" id="S4.F4.4.5.1.1" style="font-size:90%;">BEDLAM CLIFF predictions</span></td>
<td class="ltx_td ltx_align_center" colspan="2" id="S4.F4.4.5.2"><span class="ltx_text" id="S4.F4.4.5.2.1" style="font-size:90%;">PARE predictions</span></td>
</tr>
<tr class="ltx_tr" id="S4.F4.4.4">
<td class="ltx_td ltx_align_center" id="S4.F4.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="137" id="S4.F4.1.1.1.g1" src="extracted/5819071/data/Experiments/texture/base_pred_1.png" width="137"/></td>
<td class="ltx_td ltx_align_center" id="S4.F4.2.2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="137" id="S4.F4.2.2.2.g1" src="extracted/5819071/data/Experiments/texture/att_pred_1.png" width="137"/></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.F4.3.3.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="137" id="S4.F4.3.3.3.g1" src="extracted/5819071/data/Experiments/texture/base_pred_2.png" width="137"/></td>
<td class="ltx_td ltx_align_center" id="S4.F4.4.4.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="137" id="S4.F4.4.4.4.g1" src="extracted/5819071/data/Experiments/texture/att_pred_2.png" width="137"/></td>
</tr>
<tr class="ltx_tr" id="S4.F4.4.6">
<td class="ltx_td ltx_align_center" id="S4.F4.4.6.1"><span class="ltx_text" id="S4.F4.4.6.1.1" style="font-size:90%;">Base image</span></td>
<td class="ltx_td ltx_align_center" id="S4.F4.4.6.2"><span class="ltx_text" id="S4.F4.4.6.2.1" style="font-size:90%;">Attribute image</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.F4.4.6.3"><span class="ltx_text" id="S4.F4.4.6.3.1" style="font-size:90%;">Base image</span></td>
<td class="ltx_td ltx_align_center" id="S4.F4.4.6.4"><span class="ltx_text" id="S4.F4.4.6.4.1" style="font-size:90%;">Attribute image</span></td>
</tr>
</table>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span class="ltx_text ltx_font_bold" id="S4.F4.9.1">State-of-the-art estimators can break with a simple texture change</span>. We use the base prompt “Photo, caucasian adult male/female wearing a shirt in the city center during day time.” and modify “shirt” to “floral shirt”(left image) and “checkered shirt”(right image).
</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="162" id="S4.F5.g1" src="x2.png" width="855"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span class="ltx_text ltx_font_bold" id="S4.F5.5.1">Pose estimators are susceptible to texture changes.</span> Clothing texture can have a large impact on performance of pose estimators, e.g. on PARE. The impact of each texture is roughly equal.
</figcaption>
</figure>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1"><span class="ltx_text" id="S4.p1.1.1" style="font-size:90%;">This section is divided into three parts. First, we provide implementation details.
Second, we use STAGE to evaluate the sensitivity of popular pose estimators towards certain attributes.
Third, we evaluate the quality of the images generated by our GenAI toolkit STAGE.</span></p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Implementation Details</h3>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:90%;">Training</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1"><span class="ltx_text" id="S4.SS1.SSS0.Px1.p1.1.1" style="font-size:90%;">We initialize our CN-3DPose with weights from a pre-trained ControlNet-Depth </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS1.SSS0.Px1.p1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib83" title=""><span class="ltx_text" style="font-size:90%;">83</span></a><span class="ltx_text" id="S4.SS1.SSS0.Px1.p1.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS1.SSS0.Px1.p1.1.4" style="font-size:90%;">.
We train it using AGORA </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS1.SSS0.Px1.p1.1.5.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib57" title=""><span class="ltx_text" style="font-size:90%;">57</span></a><span class="ltx_text" id="S4.SS1.SSS0.Px1.p1.1.6.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS1.SSS0.Px1.p1.1.7" style="font-size:90%;">, HUMBI </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS1.SSS0.Px1.p1.1.8.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib81" title=""><span class="ltx_text" style="font-size:90%;">81</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib78" title=""><span class="ltx_text" style="font-size:90%;">78</span></a><span class="ltx_text" id="S4.SS1.SSS0.Px1.p1.1.9.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS1.SSS0.Px1.p1.1.10" style="font-size:90%;">, SHHQ </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS1.SSS0.Px1.p1.1.11.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">22</span></a><span class="ltx_text" id="S4.SS1.SSS0.Px1.p1.1.12.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS1.SSS0.Px1.p1.1.13" style="font-size:90%;">, COCO </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS1.SSS0.Px1.p1.1.14.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib47" title=""><span class="ltx_text" style="font-size:90%;">47</span></a><span class="ltx_text" id="S4.SS1.SSS0.Px1.p1.1.15.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS1.SSS0.Px1.p1.1.16" style="font-size:90%;"> and a set of human scans </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS1.SSS0.Px1.p1.1.17.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib62" title=""><span class="ltx_text" style="font-size:90%;">62</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib71" title=""><span class="ltx_text" style="font-size:90%;">71</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">3</span></a><span class="ltx_text" id="S4.SS1.SSS0.Px1.p1.1.18.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS1.SSS0.Px1.p1.1.19" style="font-size:90%;">.
The remaining technical details can be found in the Supp. Mat.</span></p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:90%;">Pose estimators</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.1"><span class="ltx_text" id="S4.SS1.SSS0.Px2.p1.1.1" style="font-size:90%;">We evaluate, MeTRAbs </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS1.SSS0.Px2.p1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib69" title=""><span class="ltx_text" style="font-size:90%;">69</span></a><span class="ltx_text" id="S4.SS1.SSS0.Px2.p1.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS1.SSS0.Px2.p1.1.4" style="font-size:90%;">, HMR 2.0 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS1.SSS0.Px2.p1.1.5.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a><span class="ltx_text" id="S4.SS1.SSS0.Px2.p1.1.6.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS1.SSS0.Px2.p1.1.7" style="font-size:90%;">, PyMAF-X </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS1.SSS0.Px2.p1.1.8.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib82" title=""><span class="ltx_text" style="font-size:90%;">82</span></a><span class="ltx_text" id="S4.SS1.SSS0.Px2.p1.1.9.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS1.SSS0.Px2.p1.1.10" style="font-size:90%;">, SMPLer-X </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS1.SSS0.Px2.p1.1.11.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">10</span></a><span class="ltx_text" id="S4.SS1.SSS0.Px2.p1.1.12.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS1.SSS0.Px2.p1.1.13" style="font-size:90%;"> and BEDLAM-CLIFF </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS1.SSS0.Px2.p1.1.14.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib43" title=""><span class="ltx_text" style="font-size:90%;">43</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">6</span></a><span class="ltx_text" id="S4.SS1.SSS0.Px2.p1.1.15.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS1.SSS0.Px2.p1.1.16" style="font-size:90%;"> as these are the current state-of-the-art estimators.
Note BEDLAM-CLIFF achieves state-of-the-art performance by training only on the synthetic datasets AGORA and BEDLAM </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS1.SSS0.Px2.p1.1.17.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib57" title=""><span class="ltx_text" style="font-size:90%;">57</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">6</span></a><span class="ltx_text" id="S4.SS1.SSS0.Px2.p1.1.18.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS1.SSS0.Px2.p1.1.19" style="font-size:90%;">. Here, we examine how effective this approach is in generalizing to the open world.
In addition, we test the popular estimators SPIN </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS1.SSS0.Px2.p1.1.20.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">38</span></a><span class="ltx_text" id="S4.SS1.SSS0.Px2.p1.1.21.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS1.SSS0.Px2.p1.1.22" style="font-size:90%;"> and PARE </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS1.SSS0.Px2.p1.1.23.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a><span class="ltx_text" id="S4.SS1.SSS0.Px2.p1.1.24.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS1.SSS0.Px2.p1.1.25" style="font-size:90%;">.</span></p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:90%;">Data generation</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px3.p1.1"><span class="ltx_text" id="S4.SS1.SSS0.Px3.p1.1.1" style="font-size:90%;">We use a set of 1500 poses for generation. For experiments on 3DPW, we use farthest point sampling following </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS1.SSS0.Px3.p1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">15</span></a><span class="ltx_text" id="S4.SS1.SSS0.Px3.p1.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS1.SSS0.Px3.p1.1.4" style="font-size:90%;"> to obtain diverse poses.
For experiments on AMASS, we use the subset provided by </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS1.SSS0.Px3.p1.1.5.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">15</span></a><span class="ltx_text" id="S4.SS1.SSS0.Px3.p1.1.6.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS1.SSS0.Px3.p1.1.7" style="font-size:90%;"> and subsample it to 1500.
For AMASS, we balance the gender and select 750 male and 750 female labeled poses.
For our pose quality filter, we select a threshold of 50 px (see </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3.SS3" style="font-size:90%;" title="3.3 Dataset Generation ‣ 3 Method ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.3</span></a><span class="ltx_text" id="S4.SS1.SSS0.Px3.p1.1.8" style="font-size:90%;">), as we observed that it is a good trade-off between accuracy and generation speed.
We generate an image for a pose at most 13 times and check the quality filter.
If none of the 13 generated images passes the quality filter, the pose is discarded (details on this are in the Supp. Mat.).
Therefore, datasets for different attributes can have different poses.
To control for this, we compare results between different attributes always on the intersection of valid poses.</span></p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Examining Sensitivity to Various Attributes</h3>
<figure class="ltx_figure" id="S4.F6">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.F6.4">
<tr class="ltx_tr" id="S4.F6.4.5">
<td class="ltx_td ltx_align_center" colspan="2" id="S4.F6.4.5.1"><span class="ltx_text" id="S4.F6.4.5.1.1" style="font-size:90%;">SMPler-X B32 predictions</span></td>
<td class="ltx_td ltx_align_center" colspan="2" id="S4.F6.4.5.2"><span class="ltx_text" id="S4.F6.4.5.2.1" style="font-size:90%;">PyMAF-X predictions</span></td>
</tr>
<tr class="ltx_tr" id="S4.F6.4.4">
<td class="ltx_td ltx_align_center" id="S4.F6.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="137" id="S4.F6.1.1.1.g1" src="extracted/5819071/data/Experiments/clothing/base_pred_1.png" width="137"/></td>
<td class="ltx_td ltx_align_center" id="S4.F6.2.2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="137" id="S4.F6.2.2.2.g1" src="extracted/5819071/data/Experiments/clothing/att_pred_1.png" width="137"/></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.F6.3.3.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="137" id="S4.F6.3.3.3.g1" src="extracted/5819071/data/Experiments/clothing/base_pred_2.png" width="137"/></td>
<td class="ltx_td ltx_align_center" id="S4.F6.4.4.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="137" id="S4.F6.4.4.4.g1" src="extracted/5819071/data/Experiments/clothing/att_pred_2.png" width="137"/></td>
</tr>
<tr class="ltx_tr" id="S4.F6.4.6">
<td class="ltx_td ltx_align_center" id="S4.F6.4.6.1"><span class="ltx_text" id="S4.F6.4.6.1.1" style="font-size:90%;">Base image</span></td>
<td class="ltx_td ltx_align_center" id="S4.F6.4.6.2"><span class="ltx_text" id="S4.F6.4.6.2.1" style="font-size:90%;">Attribute image</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.F6.4.6.3"><span class="ltx_text" id="S4.F6.4.6.3.1" style="font-size:90%;">Base image</span></td>
<td class="ltx_td ltx_align_center" id="S4.F6.4.6.4"><span class="ltx_text" id="S4.F6.4.6.4.1" style="font-size:90%;">Attribute image</span></td>
</tr>
</table>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 6: </span><span class="ltx_text ltx_font_bold" id="S4.F6.9.1">State-of-the-art estimators can break with a simple clothing change.</span> We use the base prompt “Photo, caucasian adult male/female wearing a shirt in the city center during day time.” and modify “shirt” to “parka” (left image) and “trench coat” (right image).
</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="198" id="S4.F7.g1" src="x3.png" width="855"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 7: </span><span class="ltx_text ltx_font_bold" id="S4.F7.5.1">Clothing impacts performance.</span> Clothing has the largest impact on performance. Especially items that cover most of the body such as coats, can impact the performance in a negative way.</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F8">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.F8.4">
<tr class="ltx_tr" id="S4.F8.4.5">
<td class="ltx_td ltx_align_center" colspan="2" id="S4.F8.4.5.1"><span class="ltx_text" id="S4.F8.4.5.1.1" style="font-size:90%;">BEDLAM CLIFF predictions</span></td>
<td class="ltx_td ltx_align_center" colspan="2" id="S4.F8.4.5.2"><span class="ltx_text" id="S4.F8.4.5.2.1" style="font-size:90%;">SMPLer-X B32 predictions</span></td>
</tr>
<tr class="ltx_tr" id="S4.F8.4.4">
<td class="ltx_td ltx_align_center" id="S4.F8.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="137" id="S4.F8.1.1.1.g1" src="extracted/5819071/data/Experiments/fairness/base_pred_1.png" width="137"/></td>
<td class="ltx_td ltx_align_center" id="S4.F8.2.2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="137" id="S4.F8.2.2.2.g1" src="extracted/5819071/data/Experiments/fairness/att_pred_1.png" width="137"/></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.F8.3.3.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="137" id="S4.F8.3.3.3.g1" src="extracted/5819071/data/Experiments/fairness/base_pred_2.png" width="137"/></td>
<td class="ltx_td ltx_align_center" id="S4.F8.4.4.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="137" id="S4.F8.4.4.4.g1" src="extracted/5819071/data/Experiments/fairness/att_pred_2.png" width="137"/></td>
</tr>
<tr class="ltx_tr" id="S4.F8.4.6">
<td class="ltx_td ltx_align_center" id="S4.F8.4.6.1"><span class="ltx_text" id="S4.F8.4.6.1.1" style="font-size:90%;">Base image</span></td>
<td class="ltx_td ltx_align_center" id="S4.F8.4.6.2"><span class="ltx_text" id="S4.F8.4.6.2.1" style="font-size:90%;">Attribute image</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.F8.4.6.3"><span class="ltx_text" id="S4.F8.4.6.3.1" style="font-size:90%;">Base image</span></td>
<td class="ltx_td ltx_align_center" id="S4.F8.4.6.4"><span class="ltx_text" id="S4.F8.4.6.4.1" style="font-size:90%;">Attribute image</span></td>
</tr>
</table>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 8: </span><span class="ltx_text ltx_font_bold" id="S4.F8.9.1">State-of-the-art estimators can break with a gender or age change.</span> We use the base prompt “Photo, caucasian adult male wearing a shirt in the city center during day time.” and modify “male” to “female”(left image) and “adult” to “elderly”(right image).
</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="162" id="S4.F9.g1" src="x4.png" width="855"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 9: </span><span class="ltx_text ltx_font_bold" id="S4.F9.5.1">Fairness analysis.</span> We consider multiple attributes related to fairness in computer vision. A subset is presented here. Pose estimators seem to be influenced by gender and age attributes while being robust against different ethnicities.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1"><span class="ltx_text" id="S4.SS2.p1.1.1" style="font-size:90%;">In the following, we apply STAGE to examine the sensitivity of pose estimators with respect to different open-world attributes.
We emphasize that our attribute selection is to showcase the method, but users can choose an entirely different set of attributes – code for generation and evaluation will be made public.
The prompts used to create these datasets are provided in the Supp. Mat.
</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3.F3" style="font-size:90%;" title="In 3.2 Pose-Conditioned Image Generation ‣ 3 Method ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a><span class="ltx_text" id="S4.SS2.p1.1.2" style="font-size:90%;"> shows a sample of our generated images. Notice how we can generate diverse images in a controlled manner. We can keep the overall image appearance fixed while changing one specific attribute at a time – an ideal controlled scenario that is unfeasible with purely real data.
We use poses from AMASS to generate the data.
For evaluation of sensitivity, we use the (MaxJE and PDP with </span><math alttext="\tau=50\,\mathrm{mm}" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mi id="S4.SS2.p1.1.m1.1.1.2" mathsize="90%" xref="S4.SS2.p1.1.m1.1.1.2.cmml">τ</mi><mo id="S4.SS2.p1.1.m1.1.1.1" mathsize="90%" xref="S4.SS2.p1.1.m1.1.1.1.cmml">=</mo><mrow id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml"><mn id="S4.SS2.p1.1.m1.1.1.3.2" mathsize="90%" xref="S4.SS2.p1.1.m1.1.1.3.2.cmml">50</mn><mo id="S4.SS2.p1.1.m1.1.1.3.1" lspace="0.170em" xref="S4.SS2.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS2.p1.1.m1.1.1.3.3" mathsize="90%" xref="S4.SS2.p1.1.m1.1.1.3.3.cmml">mm</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><eq id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1"></eq><ci id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">𝜏</ci><apply id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3"><times id="S4.SS2.p1.1.m1.1.1.3.1.cmml" xref="S4.SS2.p1.1.m1.1.1.3.1"></times><cn id="S4.SS2.p1.1.m1.1.1.3.2.cmml" type="integer" xref="S4.SS2.p1.1.m1.1.1.3.2">50</cn><ci id="S4.SS2.p1.1.m1.1.1.3.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3.3">mm</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">\tau=50\,\mathrm{mm}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">italic_τ = 50 roman_mm</annotation></semantics></math><span class="ltx_text" id="S4.SS2.p1.1.3" style="font-size:90%;">) metrics defined in </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3.SS4" style="font-size:90%;" title="3.4 Evaluation Protocol ‣ 3 Method ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.4</span></a><span class="ltx_text" id="S4.SS2.p1.1.4" style="font-size:90%;">.</span></p>
</div>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:90%;">Summary</h4>
<div class="ltx_para" id="S4.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px1.p1.1"><span class="ltx_text" id="S4.SS2.SSS0.Px1.p1.1.1" style="font-size:90%;">Overall, we observe a significant percentage of degraded poses when testing for a specific attribute as summarized in </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3.T1" style="font-size:90%;" title="In 3.3 Dataset Generation ‣ 3 Method ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a><span class="ltx_text" id="S4.SS2.SSS0.Px1.p1.1.2" style="font-size:90%;">.
The attributes that target the body appearance directly (Clothing and Texture) have the most significant effect and can lead to large prediction errors (see </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S4.F6" style="font-size:90%;" title="In 4.2 Examining Sensitivity to Various Attributes ‣ 4 Experiments ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Figs.</span> <span class="ltx_text ltx_ref_tag">6</span></a><span class="ltx_text" id="S4.SS2.SSS0.Px1.p1.1.3" style="font-size:90%;"> and </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S4.F4" style="font-size:90%;" title="Figure 4 ‣ 4 Experiments ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">4</span></a><span class="ltx_text" id="S4.SS2.SSS0.Px1.p1.1.4" style="font-size:90%;">).
Full results are available in the Supp. Mat.</span></p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:90%;">Diverse training data reduces sensitivity</h4>
<div class="ltx_para" id="S4.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p1.1"><span class="ltx_text" id="S4.SS2.SSS0.Px2.p1.1.1" style="font-size:90%;">A natural question is how training data and model size affect the sensitivity of the HPE methods.
Our results indicate that a combination of large-scale data with a large model size is crucial for robustness.
This can be observed from the SMPLer-X model family.
Each variant uses a different-sized ViT </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS2.SSS0.Px2.p1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib37" title=""><span class="ltx_text" style="font-size:90%;">37</span></a><span class="ltx_text" id="S4.SS2.SSS0.Px2.p1.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS2.SSS0.Px2.p1.1.4" style="font-size:90%;"> backbone.
While all models were trained on the same amount of data, only large models use the data effectively, leading to a reduction of PDP across attributes.
This is most pronounced for attributes of fairness, clothing, and clothing texture attributes, </span><em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS0.Px2.p1.1.5" style="font-size:90%;">i.e</em><span class="ltx_text" id="S4.SS2.SSS0.Px2.p1.1.6" style="font-size:90%;">.</span><span class="ltx_text" id="S4.SS2.SSS0.Px2.p1.1.7"></span><span class="ltx_text" id="S4.SS2.SSS0.Px2.p1.1.8" style="font-size:90%;">, attributes that affect the appearance of the person.
HMR 2.0 is a piece of further evidence for that, as it was trained on a large-scale dataset with a ViT-H backbone.
Conversely, SPIN and PARE were trained with the least amount of data and are the most sensitive estimators (largest PDP).
However, diversity alone is not enough.
BEDLAM-CLIFF (SOTA on real benchmarks) was trained on purely synthetic data, designed to be highly diverse, but is very sensitive to attribute changes across the board.
This further evidences that our STAGE method sheds new light on the performance of SOTA methods in the open world, which can not be measured in existing benchmarks.</span></p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:90%;">Architectural choices matter</h4>
<div class="ltx_para" id="S4.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px3.p1.1"><span class="ltx_text" id="S4.SS2.SSS0.Px3.p1.1.1" style="font-size:90%;">While large-scale datasets lead to lower PDP, the performance of PyMAF-X indicates that smart architectural choices can be an efficient way to achieve robustness as well.
PyMAF-X was trained on roughly the same datasets as PARE but is less sensitive to attribute changes.
We attribute this to the iterative feedback loop to align the prediction to the input image in PyMAF-X.
While the general performance is only about 3 mm better than PARE, it results in a significantly more robust method, which again was impossible to verify before our STAGE method.</span></p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:90%;">Clothing significantly affects pose estimation</h4>
<div class="ltx_para" id="S4.SS2.SSS0.Px4.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px4.p1.1"><span class="ltx_text" id="S4.SS2.SSS0.Px4.p1.1.1" style="font-size:90%;">While intuitive, for the first time, we empirically verify that clothing significantly affects pose estimation performance.
</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S4.F7" style="font-size:90%;" title="In 4.2 Examining Sensitivity to Various Attributes ‣ 4 Experiments ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Figs.</span> <span class="ltx_text ltx_ref_tag">7</span></a><span class="ltx_text" id="S4.SS2.SSS0.Px4.p1.1.2" style="font-size:90%;"> and </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S4.F5" style="font-size:90%;" title="Figure 5 ‣ 4 Experiments ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">5</span></a><span class="ltx_text" id="S4.SS2.SSS0.Px4.p1.1.3" style="font-size:90%;"> show the sensitivity to each clothing item and to different texture patterns, respectively.
We observe that large body covering items such as coats and long jackets are most impactful, leading to a degradation of up to 30% of the predictions and over 15% across all estimators.
In addition, a simple change such as the pattern of a shirt can lead to up to 20% of degraded predictions.
Further, </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S4.F5" style="font-size:90%;" title="In 4 Experiments ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a><span class="ltx_text" id="S4.SS2.SSS0.Px4.p1.1.4" style="font-size:90%;"> suggests that each pattern is equally impactful.</span></p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px5">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:90%;">Location and weather</h4>
<div class="ltx_para" id="S4.SS2.SSS0.Px5.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px5.p1.1"><span class="ltx_text" id="S4.SS2.SSS0.Px5.p1.1.1" style="font-size:90%;">It is commonly accepted in the HPE community that outdoor scenes are more challenging than indoor scenes </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS2.SSS0.Px5.p1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">32</span></a><span class="ltx_text" id="S4.SS2.SSS0.Px5.p1.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS2.SSS0.Px5.p1.1.4" style="font-size:90%;">. However, our results show that SOTA methods degrade more when the attribute is indoor; see </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3.T1" style="font-size:90%;" title="In 3.3 Dataset Generation ‣ 3 Method ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a><span class="ltx_text" id="S4.SS2.SSS0.Px5.p1.1.5" style="font-size:90%;">. We attribute this to the fact that standard benchmarks captured indoors are typically lab settings with very simple backgrounds, whereas our generated images have more realistic backgrounds, including potential occlusions. This hints at the fact that indoor HPE in the open world is not necessarily easier than outdoors.
Perhaps unsurprisingly, bad weather conditions, in particular snow, result in the most degradation of poses.</span></p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px6">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:90%;">Pose estimators are robust against protected attributes</h4>
<div class="ltx_para" id="S4.SS2.SSS0.Px6.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px6.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S4.F9" style="font-size:90%;" title="In 4.2 Examining Sensitivity to Various Attributes ‣ 4 Experiments ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">9</span></a><span class="ltx_text" id="S4.SS2.SSS0.Px6.p1.1.1" style="font-size:90%;"> shows the sensitivity to protected attributes – attributes that should not be used to make decisions.
Overall, current pose estimators are less sensitive towards protected attributes (such as ethnicity) compared to clothing and location.
This might be due to the fact that these attributes usually only affect a small part of the body, making them less important overall.
We note, however, that gender does seem to affect predictions more (see also </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S4.F8" style="font-size:90%;" title="In 4.2 Examining Sensitivity to Various Attributes ‣ 4 Experiments ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">8</span></a><span class="ltx_text" id="S4.SS2.SSS0.Px6.p1.1.2" style="font-size:90%;">) than other attributes – which indicates that to make these methods fair, more work is needed.
One can also observe that BEDLAM-CLIFF is quite sensitive to every attribute despite being designed to cover a range of skin tones.
This indicates that computer graphics generated data might not be sufficient to achieve good open-world performance.</span></p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Quality Assessment of STAGE Data</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1"><span class="ltx_text" id="S4.SS3.p1.1.1" style="font-size:90%;">In this section we evaluate the quality of our generated images.
We use our CN-3DPose model to construct a synthetic replica of a HPE benchmark. We do not aim for an exact copy but for a similar attribute distribution, </span><em class="ltx_emph ltx_font_italic" id="S4.SS3.p1.1.2" style="font-size:90%;">e.g</em><span class="ltx_text" id="S4.SS3.p1.1.3" style="font-size:90%;">.</span><span class="ltx_text" id="S4.SS3.p1.1.4"></span><span class="ltx_text" id="S4.SS3.p1.1.5" style="font-size:90%;">, same clothing, gender, and location.
This way, the dominant distribution shift comes from the different source of images. We then compare two things: 1) pose estimation performance difference between real and synthetic data, which we refer to as </span><em class="ltx_emph ltx_font_italic" id="S4.SS3.p1.1.6" style="font-size:90%;">pose gap</em><span class="ltx_text" id="S4.SS3.p1.1.7" style="font-size:90%;">,
and 2) the FID </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS3.p1.1.8.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">26</span></a><span class="ltx_text" id="S4.SS3.p1.1.9.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS3.p1.1.10" style="font-size:90%;"> score to measure how close our replica is to the real-world dataset in terms of visual quality and diversity.</span></p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1"><span class="ltx_text" id="S4.SS3.p2.1.1" style="font-size:90%;">We use 3DPW </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS3.p2.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib74" title=""><span class="ltx_text" style="font-size:90%;">74</span></a><span class="ltx_text" id="S4.SS3.p2.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS3.p2.1.4" style="font-size:90%;"> for this experiment because it is well-established and covers different real-world scenes such as parks, city centers, and restaurants.
To capture the attributes within an image for our prompt template, we use a VQA model </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS3.p2.1.5.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a><span class="ltx_text" id="S4.SS3.p2.1.6.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS3.p2.1.7" style="font-size:90%;">.
See Supp. Mat. for details.
As image synthesis baselines, we use CN-Pose and CN-Multi, and we use the current SOTA HPE method (MeTRAbs-ACAE </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS3.p2.1.8.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib69" title=""><span class="ltx_text" style="font-size:90%;">69</span></a><span class="ltx_text" id="S4.SS3.p2.1.9.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS3.p2.1.10" style="font-size:90%;">) on 3DPW to compute the pose gap.</span></p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1"><span class="ltx_text" id="S4.SS3.p3.1.1" style="font-size:90%;">We present the results in </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S4.T2" style="font-size:90%;" title="In 4.3 Quality Assessment of STAGE Data ‣ 4 Experiments ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">2</span></a><span class="ltx_text" id="S4.SS3.p3.1.2" style="font-size:90%;">.
CN-Pose is the weakest generator in terms of pose alignment as the pose gap 32.83 mm is the largest.
Only 2D pose conditioning is not sufficient due to the aforementioned loss of 3D information (see </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3.SS3" style="font-size:90%;" title="3.3 Dataset Generation ‣ 3 Method ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.3</span></a><span class="ltx_text" id="S4.SS3.p3.1.3" style="font-size:90%;">).
In addition, it is prone to swap left and right limbs, as can be observed in </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S4.F10" style="font-size:90%;" title="In 4.3 Quality Assessment of STAGE Data ‣ 4 Experiments ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">10</span></a><span class="ltx_text" id="S4.SS3.p3.1.4" style="font-size:90%;"> (left and right legs are swapped).
In contrast, CN-Multi is able to generate images with accurate 3D pose alignment, achieving a smaller pose gap of 9.19 mm.
However, its diversity is limited, as indicated by the FID score of 60.8. In addition, it often does not follow the text prompt, creates flat backgrounds, and is biased towards generating body-tight clothing, limiting diversity even further.
Both phenomena can be observed in visual examples in </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S4.F10" style="font-size:90%;" title="In 4.3 Quality Assessment of STAGE Data ‣ 4 Experiments ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">10</span></a><span class="ltx_text" id="S4.SS3.p3.1.5" style="font-size:90%;"> (more examples can be found in the Supp. Mat.).
This makes pose estimation easier on the images and can be seen as good pose alignment, but at the expense of diversity, which is not what we want.</span></p>
</div>
<figure class="ltx_figure" id="S4.F10">
<table class="ltx_tabular ltx_align_middle" id="S4.F10.13">
<tr class="ltx_tr" id="S4.F10.13.14">
<td class="ltx_td" id="S4.F10.13.14.1"></td>
<td class="ltx_td ltx_nopad_l" id="S4.F10.13.14.2"></td>
<td class="ltx_td ltx_align_center" colspan="4" id="S4.F10.13.14.3"><span class="ltx_text" id="S4.F10.13.14.3.1" style="font-size:90%;">Photo, adult caucasian male wearing …</span></td>
</tr>
<tr class="ltx_tr" id="S4.F10.13.15">
<td class="ltx_td" id="S4.F10.13.15.1"></td>
<td class="ltx_td ltx_nopad_l" id="S4.F10.13.15.2"></td>
<td class="ltx_td ltx_nopad_l ltx_align_right" id="S4.F10.13.15.3">
<span class="ltx_ERROR undefined" id="S4.F10.13.15.3.1">\pbox</span><span class="ltx_text" id="S4.F10.13.15.3.2" style="font-size:90%;">0.17</span><span class="ltx_text ltx_font_bold" id="S4.F10.13.15.3.3" style="font-size:90%;">long coat</span><span class="ltx_text" id="S4.F10.13.15.3.4" style="font-size:90%;"> in </span><span class="ltx_text ltx_font_bold" id="S4.F10.13.15.3.5" style="font-size:90%;">city park</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_align_right" id="S4.F10.13.15.4">
<span class="ltx_ERROR undefined" id="S4.F10.13.15.4.1">\pbox</span><span class="ltx_text" id="S4.F10.13.15.4.2" style="font-size:90%;">0.17</span><span class="ltx_text ltx_font_bold" id="S4.F10.13.15.4.3" style="font-size:90%;">jacket</span><span class="ltx_text" id="S4.F10.13.15.4.4" style="font-size:90%;"> in </span><span class="ltx_text ltx_font_bold" id="S4.F10.13.15.4.5" style="font-size:90%;">city center</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_align_right" id="S4.F10.13.15.5">
<span class="ltx_ERROR undefined" id="S4.F10.13.15.5.1">\pbox</span><span class="ltx_text" id="S4.F10.13.15.5.2" style="font-size:90%;">0.17</span><span class="ltx_text ltx_font_bold ltx_align_center" id="S4.F10.13.15.5.3" style="font-size:90%;">long coat</span><span class="ltx_text" id="S4.F10.13.15.5.4" style="font-size:90%;"> in </span><span class="ltx_text ltx_font_bold ltx_align_center" id="S4.F10.13.15.5.5" style="font-size:90%;">snowy city center</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" id="S4.F10.13.15.6">
<span class="ltx_ERROR undefined" id="S4.F10.13.15.6.1">\pbox</span><span class="ltx_text" id="S4.F10.13.15.6.2" style="font-size:90%;">0.17</span><span class="ltx_text ltx_font_bold" id="S4.F10.13.15.6.3" style="font-size:90%;">jacket</span><span class="ltx_text" id="S4.F10.13.15.6.4" style="font-size:90%;"> in </span><span class="ltx_text ltx_font_bold" id="S4.F10.13.15.6.5" style="font-size:90%;">restaurant</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.F10.5.5">
<td class="ltx_td ltx_align_center ltx_align_middle" id="S4.F10.1.1.1" style="padding-bottom:3.0pt;"><span class="ltx_text" id="S4.F10.1.1.1.1" style="font-size:90%;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="90" id="S4.F10.1.1.1.1.g1" src="extracted/5819071/data/oursVersus/vs3/row-3/gt-16.png" width="90"/></span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.F10.5.5.6" style="padding-bottom:3.0pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.F10.5.5.6.1" style="width:6.1pt;height:55.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:55.8pt;transform:translate(-24.81pt,-24.81pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.F10.5.5.6.1.1"><span class="ltx_text" id="S4.F10.5.5.6.1.1.1" style="font-size:90%;">CN-Pose</span></p>
</span></div>
</td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.F10.2.2.2" style="padding-bottom:3.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="102" id="S4.F10.2.2.2.g1" src="extracted/5819071/data/supp/ours_vs/v3/row-1/1-red.png" width="102"/></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.F10.3.3.3" style="padding-bottom:3.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="102" id="S4.F10.3.3.3.g1" src="extracted/5819071/data/supp/ours_vs/v3/row-2/1-red.png" width="102"/></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.F10.4.4.4" style="padding-bottom:3.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="102" id="S4.F10.4.4.4.g1" src="extracted/5819071/data/supp/ours_vs/v3/row-3/1-red.png" width="102"/></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.F10.5.5.5" style="padding-bottom:3.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="102" id="S4.F10.5.5.5.g1" src="extracted/5819071/data/supp/ours_vs/v3/row-4/1-red.png" width="102"/></td>
</tr>
<tr class="ltx_tr" id="S4.F10.9.9">
<td class="ltx_td ltx_align_right ltx_align_middle" id="S4.F10.9.9.5" style="padding-bottom:3.0pt;"><span class="ltx_text" id="S4.F10.9.9.5.1" style="font-size:90%;">
<span class="ltx_ERROR undefined" id="S4.F10.9.9.5.1.1">\pbox</span>0.1Ground truth pose
</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.F10.9.9.6" style="padding-bottom:3.0pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.F10.9.9.6.1" style="width:6.3pt;height:58.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:58.7pt;transform:translate(-26.23pt,-26.23pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.F10.9.9.6.1.1"><span class="ltx_text" id="S4.F10.9.9.6.1.1.1" style="font-size:90%;">CN-Multi</span></p>
</span></div>
</td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.F10.6.6.1" style="padding-bottom:3.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="102" id="S4.F10.6.6.1.g1" src="extracted/5819071/data/supp/ours_vs/v3/row-1/3.png" width="102"/></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.F10.7.7.2" style="padding-bottom:3.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="102" id="S4.F10.7.7.2.g1" src="extracted/5819071/data/supp/ours_vs/v3/row-2/3.png" width="102"/></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.F10.8.8.3" style="padding-bottom:3.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="102" id="S4.F10.8.8.3.g1" src="extracted/5819071/data/supp/ours_vs/v3/row-3/3.png" width="102"/></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.F10.9.9.4" style="padding-bottom:3.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="102" id="S4.F10.9.9.4.g1" src="extracted/5819071/data/supp/ours_vs/v3/row-4/3.png" width="102"/></td>
</tr>
<tr class="ltx_tr" id="S4.F10.13.13">
<td class="ltx_td" id="S4.F10.13.13.5" style="padding-bottom:1.0pt;"></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.F10.13.13.6" style="padding-bottom:1.0pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.F10.13.13.6.1" style="width:9.0pt;height:78.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:78.6pt;transform:translate(-34.81pt,-33.69pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.F10.13.13.6.1.1"><span class="ltx_text" id="S4.F10.13.13.6.1.1.1" style="font-size:90%;">CN-3DPose (Ours)</span></p>
</span></div>
</td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.F10.10.10.1" style="padding-bottom:1.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="102" id="S4.F10.10.10.1.g1" src="extracted/5819071/data/supp/ours_vs/v3/row-1/4.png" width="102"/></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.F10.11.11.2" style="padding-bottom:1.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="102" id="S4.F10.11.11.2.g1" src="extracted/5819071/data/supp/ours_vs/v3/row-2/4.png" width="102"/></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.F10.12.12.3" style="padding-bottom:1.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="102" id="S4.F10.12.12.3.g1" src="extracted/5819071/data/supp/ours_vs/v3/row-3/4.png" width="102"/></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.F10.13.13.4" style="padding-bottom:1.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="102" id="S4.F10.13.13.4.g1" src="extracted/5819071/data/supp/ours_vs/v3/row-4/4.png" width="102"/></td>
</tr>
</table>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>
<span class="ltx_text ltx_font_bold" id="S4.F10.19.1">Our method CN-3DPose generates diverse images with good 3D pose alignment.</span>
Given a single pose and multiple prompts we generate images with CN-Pose, CN-Multi and CN-3DPose. CN-Multi fails to follow the prompt and generates flat backgrounds or the wrong clothing item (notice the flat wall for CN-Multi in row 2). CN-Pose fails to follow the pose and generates the feet consistently in the wrong order. Only CN-3DPose (Ours) is able to generate diverse images while offering good 3D pose alignment.
</figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1"><span class="ltx_text" id="S4.SS3.p4.1.1" style="font-size:90%;">CN-3DPose is able to achieve high diversity and accurate 3D pose alignment thanks to our proposed design choices explained in </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3.SS2" style="font-size:90%;" title="3.2 Pose-Conditioned Image Generation ‣ 3 Method ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.2</span></a><span class="ltx_text" id="S4.SS3.p4.1.2" style="font-size:90%;">.
While the pose gap is slightly higher 13.11 vs 9.19 of CN-Multi,
we attribute this to our higher diversity of backgrounds and clothing, making pose estimation harder. This can be clearly seen visually (see Supp. Mat.) and is also reflected in our much lower FID score 60.8 vs 39.7.
We further investigate the reason for a pose gap of 13.11 mm.
We hypothesize that there are two factors of influence. First, the backgrounds of our generated images are more complex and diverse compared to 3DPW, making the estimation task harder.
Second, we observe artifacts in faces, feet, and hands in our generated images, which might influence estimation performance.
Thus, we compare estimation performance again after blurring faces, feet, and hands, after background removal, and both.
Results are in </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S2.T1" style="font-size:90%;" title="In Preprocessing ‣ B.1 Model Training ‣ B Implementation Details ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">S1</span></a><span class="ltx_text" id="S4.SS3.p4.1.3" style="font-size:90%;">.
After applying the blur and removing the background, the pose gap shrinks to 6.5 mm, suggesting that more complex backgrounds and lower quality of faces, feet, and hands are causing the gap.
However, we compute the PDP for our experiments between sets of generated images, base and attribute set.
Thus, any flaw would be present in both sets, therefore not influencing the conclusion.
In addition, our main contribution is the evaluation framework and it will benefit from any future improvement of generative models </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS3.p4.1.4.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">19</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">21</span></a><span class="ltx_text" id="S4.SS3.p4.1.5.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS3.p4.1.6" style="font-size:90%;">.
We conclude our proposed CN-3DPose is sufficiently accurate and diverse to generate images for controlled auditing of HPE methods.</span></p>
</div>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.2">
<tr class="ltx_tr" id="S4.T2.2.2">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T2.2.2.3"><span class="ltx_text" id="S4.T2.2.2.3.1" style="font-size:90%;">Generator</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.1.1.1">
<span class="ltx_text" id="S4.T2.1.1.1.1" style="font-size:90%;">Pose Gap </span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.m1.1a"><mo id="S4.T2.1.1.1.m1.1.1" mathsize="90%" stretchy="false" xref="S4.T2.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.2.2.2">
<span class="ltx_text" id="S4.T2.2.2.2.1" style="font-size:90%;">FID </span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.2.2.2.m1.1"><semantics id="S4.T2.2.2.2.m1.1a"><mo id="S4.T2.2.2.2.m1.1.1" mathsize="90%" stretchy="false" xref="S4.T2.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.m1.1b"><ci id="S4.T2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.2.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.2.2.4">
<span class="ltx_text" id="S4.T2.2.2.4.1"></span><span class="ltx_text" id="S4.T2.2.2.4.2" style="font-size:90%;"> </span><span class="ltx_text" id="S4.T2.2.2.4.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.2.2.4.3.1">
<span class="ltx_tr" id="S4.T2.2.2.4.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.2.2.4.3.1.1.1">Pose</span></span>
<span class="ltx_tr" id="S4.T2.2.2.4.3.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.2.2.4.3.1.2.1">Accuracy</span></span>
</span></span><span class="ltx_text" id="S4.T2.2.2.4.4"></span><span class="ltx_text" id="S4.T2.2.2.4.5" style="font-size:90%;"></span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T2.2.2.5"><span class="ltx_text" id="S4.T2.2.2.5.1" style="font-size:90%;">Diversity</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.2.3.1"><span class="ltx_text" id="S4.T2.2.3.1.1" style="font-size:90%;">CN-Pose</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.3.2"><span class="ltx_text" id="S4.T2.2.3.2.1" style="font-size:90%;">32.83</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.3.3"><span class="ltx_text" id="S4.T2.2.3.3.1" style="font-size:90%;">58.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.3.4"><span class="ltx_text" id="S4.T2.2.3.4.1" style="font-size:90%;">✗</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.2.3.5"><span class="ltx_text" id="S4.T2.2.3.5.1" style="font-size:90%;">✗</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.4">
<td class="ltx_td ltx_align_left" id="S4.T2.2.4.1"><span class="ltx_text" id="S4.T2.2.4.1.1" style="font-size:90%;">CN-Multi</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.4.2">
<span class="ltx_text ltx_phantom" id="S4.T2.2.4.2.1" style="font-size:90%;"><span style="visibility:hidden">0</span></span><span class="ltx_text" id="S4.T2.2.4.2.2" style="font-size:90%;">9.19</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.4.3"><span class="ltx_text" id="S4.T2.2.4.3.1" style="font-size:90%;">60.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.4.4"><span class="ltx_text" id="S4.T2.2.4.4.1" style="font-size:90%;">✓</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.2.4.5"><span class="ltx_text" id="S4.T2.2.4.5.1" style="font-size:90%;">✗</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.5">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T2.2.5.1"><span class="ltx_text" id="S4.T2.2.5.1.1" style="font-size:90%;">CN-3DPose</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.5.2"><span class="ltx_text" id="S4.T2.2.5.2.1" style="font-size:90%;">13.11</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.5.3"><span class="ltx_text" id="S4.T2.2.5.3.1" style="font-size:90%;">39.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.5.4"><span class="ltx_text" id="S4.T2.2.5.4.1" style="font-size:90%;">✓</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T2.2.5.5"><span class="ltx_text" id="S4.T2.2.5.5.1" style="font-size:90%;">✓</span></td>
</tr>
</table>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2: </span><span class="ltx_text ltx_font_bold" id="S4.T2.7.1">Quantifying the domain gap.</span> We compare CN-3DPose with CN-Pose and CN-Multi.
“Pose alignment” denotes the MeTRAbs pose estimator’s performance gap (in PA-MPJPE) between the real 3DPW images and our synthetic replica.
CN-3DPose generates diverse, high-quality images with good 3D pose alignment.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1"><span class="ltx_text" id="S5.p1.1.1" style="font-size:90%;">3D human pose estimation has been a main area of research in computer vision for over 30 years, and the community has seen a lot of progress in terms of generalization and good benchmarks. However, are these methods robust to attributes such as clothing, weather, or gender?
For the first time, we can answer these questions empirically, shedding new light on pose estimation methods with STAGE, a method to build custom benchmarks for 3D human pose estimation at no cost.
We built upon text-to-image models and adapted their capabilities to generate diverse, realistic images with 3D human pose control. This allows us to vary one attribute at a time and do controlled experiments.
We use the model to create synthetic benchmarks, allowing one to audit their pose estimators for specific operational domains.
We use our method to create controlled benchmarks, and test current SOTA pose estimators against attributes such as clothing, background texture, weather, and fairness.
We make several interesting findings with our method: most estimators are sensitive to clothing (intuitive but never verified empirically) and texture.
To a lesser degree, most methods are also affected by certain protected attributes such as ethnicity, gender, or age.
Overall, the methods are sensitive to several attributes; this is concerning and requires further investigation to make pose estimation methods fair and safe.
Future work will investigate generating more complex scenes, including multiple people and control over objects.
We will release code and data to allow researchers and practitioners to evaluate their 3D pose estimation methods against their desired attributes, allowing a much deeper understanding of how they will perform in the target domain.</span></p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">Acknowledgments</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1"><span class="ltx_text" id="Sx1.p1.1.1" style="font-size:90%;">We thank Riccardo Marin for proofreading and the whole RVH team for the support.
Nikita Kister was supported by Bosch Industry on Campus Lab at the University of Tübingen.
Nikita Kister thanks the European Laboratory for Learning and Intelligent Systems (ELLIS) PhD program for support.
István Sárándi and Gerard Pons-Moll were supported by the German Federal Ministry of Education and Research (BMBF): Tübingen AI Center, FKZ: 01IS18039A, by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – 409792180 (Emmy Noether Programme, project: Real Virtual Humans).
GPM is a member of the Machine Learning Cluster of Excellence, EXC number 2064/1 – Project number 390727645 and is supported by the Carl Zeiss Foundation.</span></p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib1.5.5.1" style="font-size:90%;">Augustin et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.7.1" style="font-size:90%;">
Maximilian Augustin, Valentyn Boreiko, Francesco Croce, and Matthias Hein.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.8.1" style="font-size:90%;">Diffusion visual counterfactual explanations.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib1.10.2" style="font-size:90%;">NeurIPS</em><span class="ltx_text" id="bib.bib1.11.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib2.5.5.1" style="font-size:90%;">Augustin et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.7.1" style="font-size:90%;">
Maximilian Augustin, Yannic Neuhaus, and Matthias Hein.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.8.1" style="font-size:90%;">Analyzing and explaining image classifiers via diffusion guidance.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib2.10.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib2.11.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib3.3.3.1" style="font-size:90%;">[3]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.5.1" style="font-size:90%;">
AXZY Dataset.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.6.1" style="font-size:90%;">Axyz dataset.
</span>
</span>
<span class="ltx_bibblock">
<br class="ltx_break"/><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://secure.axyz-design.com" style="font-size:90%;" title="">https://secure.axyz-design.com</a><span class="ltx_text" id="bib.bib3.7.1" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.8.1" style="font-size:90%;">Accessed: 2024-03-07.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib4.5.5.1" style="font-size:90%;">Bazavan et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.7.1" style="font-size:90%;">
Eduard Gabriel Bazavan, Andrei Zanfir, Mihai Zanfir, William T. Freeman, Rahul
Sukthankar, and Cristian Sminchisescu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.8.1" style="font-size:90%;">HSPACE: Synthetic Parametric Humans Animated in Complex
Environments.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.9.1" style="font-size:90%;">arXiv:2112.12867</em><span class="ltx_text" id="bib.bib4.10.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib5.5.5.1" style="font-size:90%;">Bergman et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.7.1" style="font-size:90%;">
Alexander W. Bergman, Petr Kellnhofer, Yifan Wang, Eric R. Chan, David B.
Lindell, and Gordon Wetzstein.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.8.1" style="font-size:90%;">Generative neural articulated radiance fields.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib5.10.2" style="font-size:90%;">NeurIPS</em><span class="ltx_text" id="bib.bib5.11.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib6.5.5.1" style="font-size:90%;">Black et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.7.1" style="font-size:90%;">
Michael J. Black, Priyanka Patel, Joachim Tesch, and Jinlong Yang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.8.1" style="font-size:90%;">BEDLAM: A synthetic dataset of bodies exhibiting detailed lifelike
animated motion.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib6.10.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib6.11.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib7.5.5.1" style="font-size:90%;">Boreiko et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.7.1" style="font-size:90%;">
Valentyn Boreiko, Matthias Hein, and Jan Hendrik Metzen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.8.1" style="font-size:90%;">Identifying systematic errors in object detectors with the SCROD
pipeline.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib7.10.2" style="font-size:90%;">ICCV Workshops</em><span class="ltx_text" id="bib.bib7.11.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib8.4.4.1" style="font-size:90%;">Brooks and Efros [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.6.1" style="font-size:90%;">
Tim Brooks and Alexei A. Efros.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.7.1" style="font-size:90%;">Hallucinating pose-compatible scenes.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.8.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib8.9.2" style="font-size:90%;">ECCV</em><span class="ltx_text" id="bib.bib8.10.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib9.5.5.1" style="font-size:90%;">Cai et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.7.1" style="font-size:90%;">
Zhongang Cai, Mingyuan Zhang, Jiawei Ren, Chen Wei, Daxuan Ren, Zhengyu Lin,
Haiyu Zhao, Lei Yang, Chen Change Loy, and Ziwei Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.8.1" style="font-size:90%;">Playing for 3D human recovery.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.9.1" style="font-size:90%;">arXiv:2110.07588</em><span class="ltx_text" id="bib.bib9.10.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib10.5.5.1" style="font-size:90%;">Cai et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.7.1" style="font-size:90%;">
Zhongang Cai, Wanqi Yin, Ailing Zeng, Chen Wei, Qingping Sun, Yanjun Wang,
Hui En Pang, Haiyi Mei, Mingyuan Zhang, Lei Zhang, Chen Change Loy, Lei Yang,
and Ziwei Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.8.1" style="font-size:90%;">SMPLer-X: Scaling up expressive human pose and shape estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib10.10.2" style="font-size:90%;">NeurIPS: Datasets and Benchmarks</em><span class="ltx_text" id="bib.bib10.11.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib11.5.5.1" style="font-size:90%;">Cao et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.7.1" style="font-size:90%;">
Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, and Kwan-Yee K. Wong.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.8.1" style="font-size:90%;">DreamAvatar: Text-and-shape guided 3D human avatar generation via
diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib11.10.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib11.11.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib12.5.5.1" style="font-size:90%;">Cao et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.7.1" style="font-size:90%;">
Z. Cao, G. Hidalgo Martinez, T. Simon, S. Wei, and Y. A. Sheikh.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.8.1" style="font-size:90%;">OpenPose: Realtime multi-person 2D pose estimation using part
affinity fields.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.9.1" style="font-size:90%;">TPAMI</em><span class="ltx_text" id="bib.bib12.10.2" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib13.4.4.1" style="font-size:90%;">Council of European Union [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.6.1" style="font-size:90%;">
Council of European Union.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.7.1" style="font-size:90%;">Proposal for a regulation of the european parliament and of the
council laying down harmonised rules on artificial intelligence (artificial
intelligence act) and amending certain union legislative acts no 2021/0106
(cod).
</span>
</span>
<span class="ltx_bibblock">
<br class="ltx_break"/><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52021PC0206" style="font-size:90%;" title="">https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52021PC0206</a><span class="ltx_text" id="bib.bib13.8.1" style="font-size:90%;">,
2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib14.4.4.1" style="font-size:90%;">Dai and Van Gool [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.6.1" style="font-size:90%;">
Dengxin Dai and Luc Van Gool.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.7.1" style="font-size:90%;">Dark model adaptation: Semantic image segmentation from daytime to
nighttime.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.8.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib14.9.2" style="font-size:90%;">IEEE Conference on Intelligent Transportation Systems
(ITSC)</em><span class="ltx_text" id="bib.bib14.10.3" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib15.4.4.1" style="font-size:90%;">Delmas, Ginger and Weinzaepfel, Philippe and Lucas, Thomas and
Moreno-Noguer, Francesc and Rogez, Grégory [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.6.1" style="font-size:90%;">
Delmas, Ginger and Weinzaepfel, Philippe and Lucas, Thomas and Moreno-Noguer,
Francesc and Rogez, Grégory.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.7.1" style="font-size:90%;">PoseScript: 3D human poses from natural language.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.8.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib15.9.2" style="font-size:90%;">ECCV</em><span class="ltx_text" id="bib.bib15.10.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib16.5.5.1" style="font-size:90%;">Deng et al. [2009]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.7.1" style="font-size:90%;">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.8.1" style="font-size:90%;">ImageNet: A large-scale hierarchical image database.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib16.10.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib16.11.3" style="font-size:90%;">, 2009.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib17.5.5.1" style="font-size:90%;">Di et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.7.1" style="font-size:90%;">
Shuai Di, Qi Feng, Chun-Guang Li, Mei Zhang, Honggang Zhang, Semir Elezovikj,
Chiu C. Tan, and Haibin Ling.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.8.1" style="font-size:90%;">Rainy night scene understanding with near scene semantic adaptation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.9.1" style="font-size:90%;">IEEE Transactions on Intelligent Transportation Systems
(T-ITS)</em><span class="ltx_text" id="bib.bib17.10.2" style="font-size:90%;">, 22(3):1594–1602, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib18.5.5.1" style="font-size:90%;">Dong et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.7.1" style="font-size:90%;">
Zijian Dong, Xu Chen, Jinlong Yang, Michael J. Black, Otmar Hilliges, and
Andreas Geiger.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.8.1" style="font-size:90%;">AG3D: Learning to generate 3D avatars from 2D image
collections.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib18.10.2" style="font-size:90%;">ICCV</em><span class="ltx_text" id="bib.bib18.11.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib19.5.5.1" style="font-size:90%;">Esser et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.7.1" style="font-size:90%;">
Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas
Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic
Boesel, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.8.1" style="font-size:90%;">Scaling rectified flow transformers for high-resolution image
synthesis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib19.10.2" style="font-size:90%;">ICML</em><span class="ltx_text" id="bib.bib19.11.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib20.5.5.1" style="font-size:90%;">Eyuboglu et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.7.1" style="font-size:90%;">
Sabri Eyuboglu, Maya Varma, Khaled Saab, Jean-Benoit Delbrouck, Christopher
Lee-Messer, Jared Dunnmon, James Zou, and Christopher Ré.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.8.1" style="font-size:90%;">Domino: Discovering systematic errors with cross-modal embeddings.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib20.10.2" style="font-size:90%;">ICLR</em><span class="ltx_text" id="bib.bib20.11.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib21.3.3.1" style="font-size:90%;">[21]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.5.1" style="font-size:90%;">
Flux Model.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.6.1" style="font-size:90%;">Flux model.
</span>
</span>
<span class="ltx_bibblock">
<br class="ltx_break"/><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://blackforestlabs.ai/" style="font-size:90%;" title="">https://blackforestlabs.ai/</a><span class="ltx_text" id="bib.bib21.7.1" style="font-size:90%;">, 2020.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.8.1" style="font-size:90%;">Accessed: 2024-07-16.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib22.5.5.1" style="font-size:90%;">Fu et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.7.1" style="font-size:90%;">
Jianglin Fu, Shikai Li, Yuming Jiang, Kwan-Yee Lin, Chen Qian, Chen Change Loy,
Wayne Wu, and Ziwei Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.8.1" style="font-size:90%;">StyleGAN-Human: A data-centric odyssey of human generation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib22.10.2" style="font-size:90%;">ECCV</em><span class="ltx_text" id="bib.bib22.11.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib23.5.5.1" style="font-size:90%;">Gao et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.7.1" style="font-size:90%;">
Irena Gao, Gabriel Ilharco, Scott Lundberg, and Marco Tulio Ribeiro.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.8.1" style="font-size:90%;">Adaptive testing of computer vision models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib23.10.2" style="font-size:90%;">ICCV</em><span class="ltx_text" id="bib.bib23.11.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib24.5.5.1" style="font-size:90%;">Goel et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.7.1" style="font-size:90%;">
Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran, Angjoo Kanazawa, and
Jitendra Malik.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.8.1" style="font-size:90%;">Humans in 4D: Reconstructing and tracking humans with transformers.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib24.10.2" style="font-size:90%;">ICCV</em><span class="ltx_text" id="bib.bib24.11.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib25.5.5.1" style="font-size:90%;">Grigorev et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.7.1" style="font-size:90%;">
Artur Grigorev, Karim Iskakov, Anastasia Ianina, Renat Bashirov, Ilya
Zakharkin, Alexander Vakhitov, and Victor Lempitsky.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.8.1" style="font-size:90%;">StylePeople: A generative model of fullbody human avatars.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib25.10.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib25.11.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib26.5.5.1" style="font-size:90%;">Heusel et al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.7.1" style="font-size:90%;">
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler,
Günter Klambauer, and Sepp Hochreiter.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.8.1" style="font-size:90%;">GANs trained by a two time-scale update rule converge to a nash
equilibrium.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib26.10.2" style="font-size:90%;">NIPS</em><span class="ltx_text" id="bib.bib26.11.3" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib27.5.5.1" style="font-size:90%;">Hong et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.7.1" style="font-size:90%;">
Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang Cai, Lei Yang, and Ziwei
Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.8.1" style="font-size:90%;">AvatarCLIP: Zero-shot text-driven generation and animation of 3D
avatars.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.9.1" style="font-size:90%;">TOG</em><span class="ltx_text" id="bib.bib27.10.2" style="font-size:90%;">, 41(4):1–19, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib28.5.5.1" style="font-size:90%;">Hong et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.7.1" style="font-size:90%;">
Fangzhou Hong, Zhaoxi Chen, Yushi Lan, Liang Pan, and Ziwei Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.8.1" style="font-size:90%;">EVA3D: Compositional 3D human generation from 2D image
collections.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib28.10.2" style="font-size:90%;">ICLR</em><span class="ltx_text" id="bib.bib28.11.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib29.5.5.1" style="font-size:90%;">Hu et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.7.1" style="font-size:90%;">
Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay
Krishna, and Noah A Smith.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.8.1" style="font-size:90%;">TIFA: Accurate and interpretable text-to-image faithfulness
evaluation with question answering.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib29.10.2" style="font-size:90%;">ICCV</em><span class="ltx_text" id="bib.bib29.11.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib30.5.5.1" style="font-size:90%;">Huang et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.7.1" style="font-size:90%;">
Chun-Hao P. Huang, Hongwei Yi, Markus Höschle, Matvey Safroshkin,
Tsvetelina Alexiadis, Senya Polikovsky, Daniel Scharstein, and Michael J.
Black.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.8.1" style="font-size:90%;">Capturing and inferring dense full-body human-scene contact.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib30.10.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib30.11.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib31.5.5.1" style="font-size:90%;">Ionescu et al. [2014]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.7.1" style="font-size:90%;">
Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.8.1" style="font-size:90%;">Human3.6M: Large scale datasets and predictive methods for 3D
human sensing in natural environments.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.9.1" style="font-size:90%;">TPAMI</em><span class="ltx_text" id="bib.bib31.10.2" style="font-size:90%;">, 36(7):1325–1339, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib32.5.5.1" style="font-size:90%;">Joo et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.7.1" style="font-size:90%;">
Hanbyul Joo, Natalia Neverova, and Andrea Vedaldi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.8.1" style="font-size:90%;">Exemplar fine-tuning for 3D human pose fitting towards in-the-wild
3D human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib32.10.2" style="font-size:90%;">3DV</em><span class="ltx_text" id="bib.bib32.11.3" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib33.5.5.1" style="font-size:90%;">Ju et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.7.1" style="font-size:90%;">
Xuan Ju, Ailing Zeng, Chenchen Zhao, Jianan Wang, Lei Zhang, and Qiang Xu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.8.1" style="font-size:90%;">HumanSD: A native skeleton-guided diffusion model for human image
generation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib33.10.2" style="font-size:90%;">ICCV</em><span class="ltx_text" id="bib.bib33.11.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib34.5.5.1" style="font-size:90%;">Kaufmann et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.7.1" style="font-size:90%;">
Manuel Kaufmann, Jie Song, Chen Guo, Kaiyue Shen, Tianjian Jiang, Chengcheng
Tang, Juan José Zárate, and Otmar Hilliges.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.8.1" style="font-size:90%;">EMDB: The electromagnetic database of global 3D human pose and
shape in the wild.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib34.10.2" style="font-size:90%;">ICCV</em><span class="ltx_text" id="bib.bib34.11.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib35.5.5.1" style="font-size:90%;">Knoche et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.7.1" style="font-size:90%;">
Markus Knoche, István Sárándi, and Bastian Leibe.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.8.1" style="font-size:90%;">Reposing humans by warping 3D features.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib35.10.2" style="font-size:90%;">CVPR Workshops</em><span class="ltx_text" id="bib.bib35.11.3" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib36.5.5.1" style="font-size:90%;">Kocabas et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.7.1" style="font-size:90%;">
Muhammed Kocabas, Chun-Hao P. Huang, Otmar Hilliges, and Michael J. Black.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.8.1" style="font-size:90%;">PARE: Part attention regressor for 3D human body estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib36.10.2" style="font-size:90%;">ICCV</em><span class="ltx_text" id="bib.bib36.11.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib37.5.5.1" style="font-size:90%;">Kolesnikov et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.7.1" style="font-size:90%;">
Alexander Kolesnikov, Alexey Dosovitskiy, Dirk Weissenborn, Georg Heigold,
Jakob Uszkoreit, Lucas Beyer, Matthias Minderer, Mostafa Dehghani, Neil
Houlsby, Sylvain Gelly, Thomas Unterthiner, and Xiaohua Zhai.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.8.1" style="font-size:90%;">An image is worth 16x16 words: Transformers for image recognition at
scale.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib37.10.2" style="font-size:90%;">ICLR</em><span class="ltx_text" id="bib.bib37.11.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib38.5.5.1" style="font-size:90%;">Kolotouros et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.7.1" style="font-size:90%;">
Nikos Kolotouros, Georgios Pavlakos, Michael J Black, and Kostas Daniilidis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.8.1" style="font-size:90%;">Learning to reconstruct 3D human pose and shape via model-fitting
in the loop.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib38.10.2" style="font-size:90%;">ICCV</em><span class="ltx_text" id="bib.bib38.11.3" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib39.5.5.1" style="font-size:90%;">Kolotouros et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.7.1" style="font-size:90%;">
Nikos Kolotouros, Thiemo Alldieck, Andrei Zanfir, Eduard Gabriel Bazavan, Mihai
Fieraru, and Cristian Sminchisescu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.8.1" style="font-size:90%;">DreamHuman: Animatable 3D avatars from text.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib39.10.2" style="font-size:90%;">NeurIPS</em><span class="ltx_text" id="bib.bib39.11.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib40.4.4.1" style="font-size:90%;">Laboratories [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.6.1" style="font-size:90%;">
Underwriters Laboratories.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.7.1" style="font-size:90%;">Ul 4600 evaluation of autonomous products, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib41.5.5.1" style="font-size:90%;">Li et al. [2023a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.7.1" style="font-size:90%;">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.8.1" style="font-size:90%;">BLIP-2: bootstrapping language-image pre-training with frozen image
encoders and large language models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib41.10.2" style="font-size:90%;">ICML</em><span class="ltx_text" id="bib.bib41.11.3" style="font-size:90%;">, 2023a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib42.5.5.1" style="font-size:90%;">Li et al. [2023b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.7.1" style="font-size:90%;">
Xiaodan Li, Yuefeng Chen, Yao Zhu, Shuhui Wang, Rong Zhang, and Hui Xue.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.8.1" style="font-size:90%;">ImageNet-E: Benchmarking neural network robustness via attribute
editing.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib42.10.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib42.11.3" style="font-size:90%;">, 2023b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib43.5.5.1" style="font-size:90%;">Li et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.7.1" style="font-size:90%;">
Zhihao Li, Jianzhuang Liu, Zhensong Zhang, Songcen Xu, and Youliang Yan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.8.1" style="font-size:90%;">Cliff: Carrying location information in full frames into human pose
and shape estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib43.10.2" style="font-size:90%;">ECCV</em><span class="ltx_text" id="bib.bib43.11.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib44.5.5.1" style="font-size:90%;">Liang et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.7.1" style="font-size:90%;">
Hao Liang, Pietro Perona, and Guha Balakrishnan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.8.1" style="font-size:90%;">Benchmarking algorithmic bias in face recognition: An experimental
approach using synthetic faces and human evaluation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib44.10.2" style="font-size:90%;">ICCV</em><span class="ltx_text" id="bib.bib44.11.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib45.5.5.1" style="font-size:90%;">Liao et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.7.1" style="font-size:90%;">
Tingting Liao, Hongwei Yi, Yuliang Xiu, Jiaxaing Tang, Yangyi Huang, Justus
Thies, and Michael J. Black.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.8.1" style="font-size:90%;">TADA! text to animatable digital avatars.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib45.10.2" style="font-size:90%;">3DV</em><span class="ltx_text" id="bib.bib45.11.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib46.5.5.1" style="font-size:90%;">Lin et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.7.1" style="font-size:90%;">
Jing Lin, Ailing Zeng, Shunlin Lu, Yuanhao Cai, Ruimao Zhang, Haoqian Wang, and
Lei Zhang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.8.1" style="font-size:90%;">Motion-X: A large-scale 3D expressive whole-body human motion
dataset.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib46.10.2" style="font-size:90%;">NeurIPS: Datasets and Benchmarks</em><span class="ltx_text" id="bib.bib46.11.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib47.5.5.1" style="font-size:90%;">Lin et al. [2014]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.7.1" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C. Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.8.1" style="font-size:90%;">Microsoft COCO: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib47.10.2" style="font-size:90%;">ECCV</em><span class="ltx_text" id="bib.bib47.11.3" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib48.5.5.1" style="font-size:90%;">Liu et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.7.1" style="font-size:90%;">
Xian Liu, Jian Ren, Aliaksandr Siarohin, Ivan Skorokhodov, Dahua Lin, Xihui
Liu, Ziwei Liu, Sergey Tulyakov, and Yanyu Li.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.8.1" style="font-size:90%;">HyperHuman: Hyper-realistic human generation with latent structural
diffusion.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib48.10.2" style="font-size:90%;">ICLR</em><span class="ltx_text" id="bib.bib48.11.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib49.5.5.1" style="font-size:90%;">Loper et al. [2015]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.7.1" style="font-size:90%;">
Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J.
Black.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.8.1" style="font-size:90%;">SMPL: A skinned multi-person linear model.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.9.1" style="font-size:90%;">TOG (Proc. SIGGRAPH Asia)</em><span class="ltx_text" id="bib.bib49.10.2" style="font-size:90%;">, 34(6):248:1–248:16, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib50.5.5.1" style="font-size:90%;">Luo et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.7.1" style="font-size:90%;">
Jinqi Luo, Zhaoning Wang, Chen Henry Wu, Dong Huang, and Fernando De la Torre.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.8.1" style="font-size:90%;">Zero-shot model diagnosis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib50.10.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib50.11.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib51.5.5.1" style="font-size:90%;">Maddern et al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib51.7.1" style="font-size:90%;">
Will Maddern, Geoff Pascoe, Chris Linegar, and Paul Newman.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib51.8.1" style="font-size:90%;">1 Year, 1000km: The Oxford RobotCar Dataset.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.9.1" style="font-size:90%;">The International Journal of Robotics Research (IJRR)</em><span class="ltx_text" id="bib.bib51.10.2" style="font-size:90%;">,
36(1):3–15, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib52.5.5.1" style="font-size:90%;">Mahmood et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib52.7.1" style="font-size:90%;">
Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Gerard Pons-Moll, and
Michael J. Black.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib52.8.1" style="font-size:90%;">AMASS: Archive of motion capture as surface shapes.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib52.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib52.10.2" style="font-size:90%;">ICCV</em><span class="ltx_text" id="bib.bib52.11.3" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib53.5.5.1" style="font-size:90%;">Metzen et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib53.7.1" style="font-size:90%;">
Jan Hendrik Metzen, Robin Hutmacher, N. Grace Hua, Valentyn Boreiko, and Dan
Zhang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib53.8.1" style="font-size:90%;">Identification of systematic errors of image classifiers on rare
subgroups.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib53.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib53.10.2" style="font-size:90%;">ICCV</em><span class="ltx_text" id="bib.bib53.11.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib54.5.5.1" style="font-size:90%;">Mou et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.7.1" style="font-size:90%;">
Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, Ying
Shan, and Xiaohu Qie.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.8.1" style="font-size:90%;">T2I-Adapter: Learning adapters to dig out more controllable
ability for text-to-image diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib54.10.2" style="font-size:90%;">AAAI</em><span class="ltx_text" id="bib.bib54.11.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib55.5.5.1" style="font-size:90%;">Noguchi et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib55.7.1" style="font-size:90%;">
Atsuhiro Noguchi, Xiao Sun, Stephen Lin, and Tatsuya Harada.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib55.8.1" style="font-size:90%;">Unsupervised learning of efficient geometry-aware neural articulated
representations.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib55.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib55.10.2" style="font-size:90%;">ECCV</em><span class="ltx_text" id="bib.bib55.11.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib56.5.5.1" style="font-size:90%;">Ostrek et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib56.7.1" style="font-size:90%;">
Mirela Ostrek, Soubhik Sanyal, Carol O’Sullivan, Michael J. Black, and Justus
Thies.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib56.8.1" style="font-size:90%;">Environment-Specific People.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.9.1" style="font-size:90%;">arXiv:2312.14579</em><span class="ltx_text" id="bib.bib56.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib57.5.5.1" style="font-size:90%;">Patel et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib57.7.1" style="font-size:90%;">
Priyanka Patel, Chun-Hao P. Huang, Joachim Tesch, David T. Hoffmann, Shashank
Tripathi, and Michael J. Black.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib57.8.1" style="font-size:90%;">AGORA: Avatars in geography optimized for regression analysis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib57.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib57.10.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib57.11.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib58.5.5.1" style="font-size:90%;">Poole et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib58.7.1" style="font-size:90%;">
Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib58.8.1" style="font-size:90%;">DreamFusion: Text-to-3D using 2D diffusion.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.9.1" style="font-size:90%;">arXiv:2209.14988</em><span class="ltx_text" id="bib.bib58.10.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib59.5.5.1" style="font-size:90%;">Prabhu et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.7.1" style="font-size:90%;">
Viraj Prabhu, Sriram Yenamandra, Prithvijit Chattopadhyay, and Judy Hoffman.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.8.1" style="font-size:90%;">LANCE: Stress-testing visual models by generating language-guided
counterfactual images.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib59.10.2" style="font-size:90%;">NeurIPS</em><span class="ltx_text" id="bib.bib59.11.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib60.5.5.1" style="font-size:90%;">Radford et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib60.7.1" style="font-size:90%;">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
Gretchen Krueger, and Ilya Sutskever.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib60.8.1" style="font-size:90%;">Learning transferable visual models from natural language
supervision.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib60.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib60.10.2" style="font-size:90%;">ICML</em><span class="ltx_text" id="bib.bib60.11.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib61.5.5.1" style="font-size:90%;">Ramesh et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib61.7.1" style="font-size:90%;">
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib61.8.1" style="font-size:90%;">Hierarchical text-conditional image generation with CLIP latents.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.9.1" style="font-size:90%;">arXiv:2204.06125</em><span class="ltx_text" id="bib.bib61.10.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib62.3.3.1" style="font-size:90%;">[62]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib62.5.1" style="font-size:90%;">
RenderPeople Dataset.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib62.6.1" style="font-size:90%;">RenderPeople dataset.
</span>
</span>
<span class="ltx_bibblock">
<br class="ltx_break"/><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://renderpeople.com" style="font-size:90%;" title="">https://renderpeople.com</a><span class="ltx_text" id="bib.bib62.7.1" style="font-size:90%;">, 2020.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib62.8.1" style="font-size:90%;">Accessed: 2024-03-07.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib63.5.5.1" style="font-size:90%;">Rombach et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib63.7.1" style="font-size:90%;">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn
Ommer.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib63.8.1" style="font-size:90%;">High-resolution image synthesis with latent diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib63.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib63.10.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib63.11.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib64.5.5.1" style="font-size:90%;">Ruiz et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib64.7.1" style="font-size:90%;">
Nataniel Ruiz, Barry-John Theobald, Anurag Ranjan, Ahmed Hussein Abdelaziz, and
Nicholas Apostoloff.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib64.8.1" style="font-size:90%;">MorphGAN: One-shot face synthesis GAN for detecting recognition
bias.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib64.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib64.10.2" style="font-size:90%;">BMVC</em><span class="ltx_text" id="bib.bib64.11.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib65.5.5.1" style="font-size:90%;">Saharia et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib65.7.1" style="font-size:90%;">
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily
Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi,
Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad
Norouzi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib65.8.1" style="font-size:90%;">Photorealistic text-to-image diffusion models with deep language
understanding.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib65.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib65.10.2" style="font-size:90%;">NeurIPS</em><span class="ltx_text" id="bib.bib65.11.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib66.5.5.1" style="font-size:90%;">Sakaridis et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib66.7.1" style="font-size:90%;">
Christos Sakaridis, Dengxin Dai, Simon Hecker, and Luc Van Gool.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib66.8.1" style="font-size:90%;">Model adaptation with synthetic and real data for semantic dense
foggy scene understanding.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib66.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib66.10.2" style="font-size:90%;">ECCV</em><span class="ltx_text" id="bib.bib66.11.3" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib67.4.4.1" style="font-size:90%;">Sminchisescu [2008]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib67.6.1" style="font-size:90%;">
Cristian Sminchisescu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib67.7.1" style="font-size:90%;">3D human motion analysis in monocular video: Techniques and
challenges.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib67.8.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib67.9.2" style="font-size:90%;">Human Motion: Understanding, Modelling, Capture, and
Animation</em><span class="ltx_text" id="bib.bib67.10.3" style="font-size:90%;">, pages 185–211. Springer Netherlands, 2008.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib68.5.5.1" style="font-size:90%;">Song et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib68.7.1" style="font-size:90%;">
Jiaming Song, Chenlin Meng, and Stefano Ermon.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib68.8.1" style="font-size:90%;">Denoising diffusion implicit models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib68.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib68.10.2" style="font-size:90%;">ICLR</em><span class="ltx_text" id="bib.bib68.11.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib69.5.5.1" style="font-size:90%;">Sárándi et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib69.7.1" style="font-size:90%;">
István Sárándi, Alexander Hermans, and Bastian Leibe.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib69.8.1" style="font-size:90%;">Learning 3D human pose estimation from dozens of datasets using a
geometry-aware autoencoder to bridge between skeleton formats.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib69.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib69.10.2" style="font-size:90%;">WACV</em><span class="ltx_text" id="bib.bib69.11.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib70.4.4.1" style="font-size:90%;">Taylor [2000]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib70.6.1" style="font-size:90%;">
Camillo J. Taylor.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib70.7.1" style="font-size:90%;">Reconstruction of articulated objects from point correspondences in a
single uncalibrated image.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.8.1" style="font-size:90%;">CVIU</em><span class="ltx_text" id="bib.bib70.9.2" style="font-size:90%;">, 80(3):349–363, 2000.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib71.3.3.1" style="font-size:90%;">[71]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib71.5.1" style="font-size:90%;">
Twindom Dataset.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib71.6.1" style="font-size:90%;">Twindom dataset.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://web.twindom.com" style="font-size:90%;" title="">https://web.twindom.com</a><span class="ltx_text" id="bib.bib71.7.1" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib71.8.1" style="font-size:90%;">Accessed: 2024-03-07.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib72.5.5.1" style="font-size:90%;">Varol et al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib72.7.1" style="font-size:90%;">
Gül Varol, Javier Romero, Xavier Martin, Naureen Mahmood, Michael J. Black,
Ivan Laptev, and Cordelia Schmid.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib72.8.1" style="font-size:90%;">Learning from synthetic humans.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib72.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib72.10.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib72.11.3" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib73.5.5.1" style="font-size:90%;">Vendrow et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib73.7.1" style="font-size:90%;">
Joshua Vendrow, Saachi Jain, Logan Engstrom, and Aleksander Madry.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib73.8.1" style="font-size:90%;">Dataset interfaces: Diagnosing model failures using controllable
counterfactual generation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib73.9.1" style="font-size:90%;">arXiv:2302.07865</em><span class="ltx_text" id="bib.bib73.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib74.5.5.1" style="font-size:90%;">von Marcard et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib74.7.1" style="font-size:90%;">
Timo von Marcard, Roberto Henschel, Michael Black, Bodo Rosenhahn, and Gerard
Pons-Moll.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib74.8.1" style="font-size:90%;">Recovering accurate 3D human pose in the wild using IMUs and a
moving camera.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib74.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib74.10.2" style="font-size:90%;">ECCV</em><span class="ltx_text" id="bib.bib74.11.3" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib75.5.5.1" style="font-size:90%;">Wiles et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib75.7.1" style="font-size:90%;">
Olivia Wiles, Isabela Albuquerque, and Sven Gowal.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib75.8.1" style="font-size:90%;">Discovering bugs in vision models using off-the-shelf image
generation and captioning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib75.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib75.10.2" style="font-size:90%;">NeurIPS Workshops</em><span class="ltx_text" id="bib.bib75.11.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib76.5.5.1" style="font-size:90%;">Yang et al. [2023a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib76.7.1" style="font-size:90%;">
Zhitao Yang, Zhongang Cai, Haiyi Mei, Shuai Liu, Zhaoxi Chen, Weiye Xiao, Yukun
Wei, Zhongfei Qing, Chen Wei, Bo Dai, Wayne Wu, Chen Qian, Dahua Lin, Ziwei
Liu, and Lei Yang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib76.8.1" style="font-size:90%;">SynBody: Synthetic dataset with layered human models for 3D human
perception and modeling.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib76.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib76.10.2" style="font-size:90%;">ICCV</em><span class="ltx_text" id="bib.bib76.11.3" style="font-size:90%;">, 2023a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib77.5.5.1" style="font-size:90%;">Yang et al. [2023b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib77.7.1" style="font-size:90%;">
Zhuoqian Yang, Shikai Li, Wayne Wu, and Bo Dai.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib77.8.1" style="font-size:90%;">3DHumanGAN: 3D-aware human image generation with 3D pose
mapping.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib77.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib77.10.2" style="font-size:90%;">ICCV</em><span class="ltx_text" id="bib.bib77.11.3" style="font-size:90%;">, 2023b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib78.5.5.1" style="font-size:90%;">Yoon et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib78.7.1" style="font-size:90%;">
Jae Shin Yoon, Zhixuan Yu, Jaesik Park, and Hyun Soo Park.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib78.8.1" style="font-size:90%;">HUMBI: A large multiview dataset of human body expressions and
benchmark challenge.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib78.9.1" style="font-size:90%;">TPAMI</em><span class="ltx_text" id="bib.bib78.10.2" style="font-size:90%;">, 45(1):623–640, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib79.5.5.1" style="font-size:90%;">Youwang et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib79.7.1" style="font-size:90%;">
Kim Youwang, Kim Ji-Yeon, and Tae-Hyun Oh.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib79.8.1" style="font-size:90%;">CLIP-Actor: Text-driven recommendation and stylization for
animating Human meshes.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib79.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib79.10.2" style="font-size:90%;">ECCV</em><span class="ltx_text" id="bib.bib79.11.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib80.5.5.1" style="font-size:90%;">Yu et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib80.7.1" style="font-size:90%;">
Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang,
Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben
Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and
Yonghui Wu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib80.8.1" style="font-size:90%;">Scaling autoregressive models for content-rich text-to-image
generation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib80.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib80.10.2" style="font-size:90%;">ICLR</em><span class="ltx_text" id="bib.bib80.11.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib81.5.5.1" style="font-size:90%;">Yu et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib81.7.1" style="font-size:90%;">
Zhixuan Yu, Jae Shin Yoon, In Kyu Lee, Prashanth Venkatesh, Jaesik Park, Jihun
Yu, and Hyun Soo Park.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib81.8.1" style="font-size:90%;">HUMBI: A large multiview dataset of human body expressions.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib81.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib81.10.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib81.11.3" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib82.5.5.1" style="font-size:90%;">Zhang et al. [2023a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib82.7.1" style="font-size:90%;">
Hongwen Zhang, Yating Tian, Yuxiang Zhang, Mengcheng Li, Liang An, Zhenan Sun,
and Yebin Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib82.8.1" style="font-size:90%;">PyMAF-X: Towards well-aligned full-body model regression from
monocular images.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib82.9.1" style="font-size:90%;">TPAMI</em><span class="ltx_text" id="bib.bib82.10.2" style="font-size:90%;">, 45(10), 2023a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib83.5.5.1" style="font-size:90%;">Zhang et al. [2023b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib83.7.1" style="font-size:90%;">
Lvmin Zhang, Anyi Rao, and Maneesh Agrawala.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib83.8.1" style="font-size:90%;">Adding conditional control to text-to-image diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib83.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib83.10.2" style="font-size:90%;">ICCV</em><span class="ltx_text" id="bib.bib83.11.3" style="font-size:90%;">, 2023b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib84.5.5.1" style="font-size:90%;">Zhang et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib84.7.1" style="font-size:90%;">
Siwei Zhang, Qianli Ma, Yan Zhang, Zhiyin Qian, Taein Kwon, Marc Pollefeys,
Federica Bogo, and Siyu Tang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib84.8.1" style="font-size:90%;">EgoBody: Human body shape and motion of interacting people from
head-mounted devices.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib84.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib84.10.2" style="font-size:90%;">ECCV</em><span class="ltx_text" id="bib.bib84.11.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib85.5.5.1" style="font-size:90%;">Zhang et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib85.7.1" style="font-size:90%;">
Tianshu Zhang, Buzhen Huang, and Yangang Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib85.8.1" style="font-size:90%;">Object-occluded human shape and pose estimation from a single color
image.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib85.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib85.10.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib85.11.3" style="font-size:90%;">, 2020.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para ltx_align_center" id="p3">
<span class="ltx_ERROR undefined" id="p3.1">\thetitle</span>
<br class="ltx_break"/>
<p class="ltx_p" id="p3.2"><span class="ltx_text" id="p3.2.1" style="font-size:144%;">Supplementary Material 
<br class="ltx_break"/></span></p>
</div>
<section class="ltx_section ltx_centering" id="S1a">
<h2 class="ltx_title ltx_title_section" style="font-size:144%;">
<span class="ltx_tag ltx_tag_section">A </span>Further Analysis on Real <em class="ltx_emph ltx_font_italic" id="S1a.1.1">vs</em>.<span class="ltx_text" id="S1a.2.2"></span> Synthetic Gap</h2>
<div class="ltx_para" id="S1a.p1">
<p class="ltx_p" id="S1a.p1.1"><span class="ltx_text" id="S1a.p1.1.1" style="font-size:144%;">Here, we analyze in detail the reasons behind the error gap between 3DPW and its synthetic replica that we reported in </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S4.T2" style="font-size:144%;" title="In 4.3 Quality Assessment of STAGE Data ‣ 4 Experiments ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">2</span></a><span class="ltx_text" id="S1a.p1.1.2" style="font-size:144%;"> of the main paper.
We hypothesize that the differences between synthetic and real images are largely concentrated on the background, faces, hands and feet.
To verify this, we conduct experiments with blurring faces, hands and feet, and background removal in both real and generated images following the protocol from </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S4.SS3" style="font-size:144%;" title="4.3 Quality Assessment of STAGE Data ‣ 4 Experiments ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.3</span></a><span class="ltx_text" id="S1a.p1.1.3" style="font-size:144%;">.
As the results in </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S2.T1" style="font-size:144%;" title="In Preprocessing ‣ B.1 Model Training ‣ B Implementation Details ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">S1</span></a><span class="ltx_text" id="S1a.p1.1.4" style="font-size:144%;"> show, jointly these almost cancel the MPJPE gap, showing the body is recognizable for the pose estimator to a similar degree, validating the overall quality of generated images.
The quality of faces and limbs is an aspect that is actively being worked on in the image generation community, therefore we can expect future models to further reduce the gap.
Background removal improves the performance on synthetic images more than on real ones; this suggests that our synthetic backgrounds are more complex than those in 3DPW, enabling testing under more challenging conditions.</span></p>
</div>
</section>
<section class="ltx_section ltx_centering" id="S2a">
<h2 class="ltx_title ltx_title_section" style="font-size:144%;">
<span class="ltx_tag ltx_tag_section">B </span>Implementation Details</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection" style="font-size:144%;">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Model Training</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1"><span class="ltx_text" id="S2.SS1.p1.1.1" style="font-size:144%;">The CN-3DPose architecture is based on </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS1.p1.1.2.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib83" title=""><span class="ltx_text" style="font-size:90%;">83</span></a><span class="ltx_text" id="S2.SS1.p1.1.3.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S2.SS1.p1.1.4" style="font-size:144%;">.
We adapt the input layer to take nine channel dimension, and we initialize it with weights from a pre-trained CN-Depth </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS1.p1.1.5.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib83" title=""><span class="ltx_text" style="font-size:90%;">83</span></a><span class="ltx_text" id="S2.SS1.p1.1.6.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S2.SS1.p1.1.7" style="font-size:144%;">.
We train our model using AGORA </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS1.p1.1.8.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib57" title=""><span class="ltx_text" style="font-size:90%;">57</span></a><span class="ltx_text" id="S2.SS1.p1.1.9.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S2.SS1.p1.1.10" style="font-size:144%;">, HUMBI </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS1.p1.1.11.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib81" title=""><span class="ltx_text" style="font-size:90%;">81</span></a><span class="ltx_text" id="S2.SS1.p1.1.12.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S2.SS1.p1.1.13" style="font-size:144%;">, SHHQ </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS1.p1.1.14.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">22</span></a><span class="ltx_text" id="S2.SS1.p1.1.15.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S2.SS1.p1.1.16" style="font-size:144%;">, COCO </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS1.p1.1.17.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib47" title=""><span class="ltx_text" style="font-size:90%;">47</span></a><span class="ltx_text" id="S2.SS1.p1.1.18.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S2.SS1.p1.1.19" style="font-size:144%;"> and a set of human scans </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS1.p1.1.20.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib71" title=""><span class="ltx_text" style="font-size:90%;">71</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib62" title=""><span class="ltx_text" style="font-size:90%;">62</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">3</span></a><span class="ltx_text" id="S2.SS1.p1.1.21.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S2.SS1.p1.1.22" style="font-size:144%;">.
For the image datasets, we create training images by center-cropping around each person in the image.</span></p>
</div>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:144%;">Preprocessing</h4>
<div class="ltx_para" id="S2.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px1.p1.2"><span class="ltx_text" id="S2.SS1.SSS0.Px1.p1.2.1" style="font-size:144%;">In our evaluation, we do not consider occlusions, therefore we remove images where the person is significantly occluded.
Specifically, for COCO, we drop images where less than ten keypoints are visible, and for AGORA, we project the SMPL mesh vertices onto the image and drop it if more than 20% are outside the image.
We resize images to </span><math alttext="512\times 512" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.1.m1.1"><semantics id="S2.SS1.SSS0.Px1.p1.1.m1.1a"><mrow id="S2.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1.cmml"><mn id="S2.SS1.SSS0.Px1.p1.1.m1.1.1.2" mathsize="144%" xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml">512</mn><mo id="S2.SS1.SSS0.Px1.p1.1.m1.1.1.1" lspace="0.222em" mathsize="144%" rspace="0.222em" xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S2.SS1.SSS0.Px1.p1.1.m1.1.1.3" mathsize="144%" xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.1.m1.1b"><apply id="S2.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1"><times id="S2.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1.1"></times><cn id="S2.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml" type="integer" xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1.2">512</cn><cn id="S2.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml" type="integer" xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.1.m1.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS0.Px1.p1.1.m1.1d">512 × 512</annotation></semantics></math><span class="ltx_text" id="S2.SS1.SSS0.Px1.p1.2.2" style="font-size:144%;">.
The cropping can result in the image having black regions due to the required padding.
We mask these regions during training to prevent the model learning from generating them.
Our model is a latent diffusion model </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS1.SSS0.Px1.p1.2.3.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib63" title=""><span class="ltx_text" style="font-size:90%;">63</span></a><span class="ltx_text" id="S2.SS1.SSS0.Px1.p1.2.4.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S2.SS1.SSS0.Px1.p1.2.5" style="font-size:144%;">. Therefore, we resize the mask to </span><math alttext="64\times 64" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.2.m2.1"><semantics id="S2.SS1.SSS0.Px1.p1.2.m2.1a"><mrow id="S2.SS1.SSS0.Px1.p1.2.m2.1.1" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.1.cmml"><mn id="S2.SS1.SSS0.Px1.p1.2.m2.1.1.2" mathsize="144%" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.1.2.cmml">64</mn><mo id="S2.SS1.SSS0.Px1.p1.2.m2.1.1.1" lspace="0.222em" mathsize="144%" rspace="0.222em" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.1.1.cmml">×</mo><mn id="S2.SS1.SSS0.Px1.p1.2.m2.1.1.3" mathsize="144%" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.2.m2.1b"><apply id="S2.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.1"><times id="S2.SS1.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.1.1"></times><cn id="S2.SS1.SSS0.Px1.p1.2.m2.1.1.2.cmml" type="integer" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.1.2">64</cn><cn id="S2.SS1.SSS0.Px1.p1.2.m2.1.1.3.cmml" type="integer" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.2.m2.1c">64\times 64</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS0.Px1.p1.2.m2.1d">64 × 64</annotation></semantics></math><span class="ltx_text" id="S2.SS1.SSS0.Px1.p1.2.6" style="font-size:144%;"> (spatial dimension of the latent) and apply it to the denoising loss during training.
For our evaluation, we only consider single-person images.
Hence, when possible, we mask other people in the image during training.
COCO provides instance and crowd masks, and AGORA provides instance masks we can use.
The person mask is combined with the black border mask.</span></p>
</div>
<figure class="ltx_table" id="S2.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.T1.2.2">
<tr class="ltx_tr" id="S2.T1.2.2.2">
<td class="ltx_td ltx_border_tt" id="S2.T1.2.2.2.3" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_border_tt" id="S2.T1.2.2.2.4" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S2.T1.1.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S2.T1.1.1.1.1.1" style="font-size:90%;">PA-MPJPE </span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S2.T1.1.1.1.1.m1.1"><semantics id="S2.T1.1.1.1.1.m1.1a"><mo id="S2.T1.1.1.1.1.m1.1.1" mathsize="90%" stretchy="false" xref="S2.T1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.m1.1b"><ci id="S2.T1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.1.1.1.1.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S2.T1.2.2.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S2.T1.2.2.2.2.1" style="font-size:90%;">MPJPE </span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S2.T1.2.2.2.2.m1.1"><semantics id="S2.T1.2.2.2.2.m1.1a"><mo id="S2.T1.2.2.2.2.m1.1.1" mathsize="90%" stretchy="false" xref="S2.T1.2.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.2.2.m1.1b"><ci id="S2.T1.2.2.2.2.m1.1.1.cmml" xref="S2.T1.2.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.2.2.2.2.m1.1d">↓</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.2.3">
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.3.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S2.T1.2.2.3.1.1"></span><span class="ltx_text" id="S2.T1.2.2.3.1.2" style="font-size:90%;"> </span><span class="ltx_text" id="S2.T1.2.2.3.1.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.2.2.3.1.3.1">
<span class="ltx_tr" id="S2.T1.2.2.3.1.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.2.2.3.1.3.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">Blurred face,</span></span>
<span class="ltx_tr" id="S2.T1.2.2.3.1.3.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.2.2.3.1.3.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">hands, feet</span></span>
</span></span><span class="ltx_text" id="S2.T1.2.2.3.1.4"></span><span class="ltx_text" id="S2.T1.2.2.3.1.5" style="font-size:90%;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.3.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S2.T1.2.2.3.2.1"></span><span class="ltx_text" id="S2.T1.2.2.3.2.2" style="font-size:90%;"> </span><span class="ltx_text" id="S2.T1.2.2.3.2.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.2.2.3.2.3.1">
<span class="ltx_tr" id="S2.T1.2.2.3.2.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.2.2.3.2.3.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">Backgr.</span></span>
<span class="ltx_tr" id="S2.T1.2.2.3.2.3.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.2.2.3.2.3.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">removed</span></span>
</span></span><span class="ltx_text" id="S2.T1.2.2.3.2.4"></span><span class="ltx_text" id="S2.T1.2.2.3.2.5" style="font-size:90%;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.3.3" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S2.T1.2.2.3.3.1"></span><span class="ltx_text" id="S2.T1.2.2.3.3.2" style="font-size:90%;"> </span><span class="ltx_text" id="S2.T1.2.2.3.3.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.2.2.3.3.3.1">
<span class="ltx_tr" id="S2.T1.2.2.3.3.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.2.2.3.3.3.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">Real</span></span>
</span></span><span class="ltx_text" id="S2.T1.2.2.3.3.4"></span><span class="ltx_text" id="S2.T1.2.2.3.3.5" style="font-size:90%;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.3.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S2.T1.2.2.3.4.1"></span><span class="ltx_text" id="S2.T1.2.2.3.4.2" style="font-size:90%;"> </span><span class="ltx_text" id="S2.T1.2.2.3.4.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.2.2.3.4.3.1">
<span class="ltx_tr" id="S2.T1.2.2.3.4.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.2.2.3.4.3.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">Synth.</span></span>
</span></span><span class="ltx_text" id="S2.T1.2.2.3.4.4"></span><span class="ltx_text" id="S2.T1.2.2.3.4.5" style="font-size:90%;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.3.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S2.T1.2.2.3.5.1"></span><span class="ltx_text" id="S2.T1.2.2.3.5.2" style="font-size:90%;"> </span><span class="ltx_text" id="S2.T1.2.2.3.5.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.2.2.3.5.3.1">
<span class="ltx_tr" id="S2.T1.2.2.3.5.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.2.2.3.5.3.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">Gap</span></span>
</span></span><span class="ltx_text" id="S2.T1.2.2.3.5.4"></span><span class="ltx_text" id="S2.T1.2.2.3.5.5" style="font-size:90%;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.3.6" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S2.T1.2.2.3.6.1"></span><span class="ltx_text" id="S2.T1.2.2.3.6.2" style="font-size:90%;"> </span><span class="ltx_text" id="S2.T1.2.2.3.6.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.2.2.3.6.3.1">
<span class="ltx_tr" id="S2.T1.2.2.3.6.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.2.2.3.6.3.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">Real</span></span>
</span></span><span class="ltx_text" id="S2.T1.2.2.3.6.4"></span><span class="ltx_text" id="S2.T1.2.2.3.6.5" style="font-size:90%;"></span>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.3.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S2.T1.2.2.3.7.1"></span><span class="ltx_text" id="S2.T1.2.2.3.7.2" style="font-size:90%;"> </span><span class="ltx_text" id="S2.T1.2.2.3.7.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.2.2.3.7.3.1">
<span class="ltx_tr" id="S2.T1.2.2.3.7.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.2.2.3.7.3.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">Synth.</span></span>
</span></span><span class="ltx_text" id="S2.T1.2.2.3.7.4"></span><span class="ltx_text" id="S2.T1.2.2.3.7.5" style="font-size:90%;"></span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.2.2.3.8" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S2.T1.2.2.3.8.1"></span><span class="ltx_text" id="S2.T1.2.2.3.8.2" style="font-size:90%;"> </span><span class="ltx_text" id="S2.T1.2.2.3.8.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.2.2.3.8.3.1">
<span class="ltx_tr" id="S2.T1.2.2.3.8.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.2.2.3.8.3.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">Gap</span></span>
</span></span><span class="ltx_text" id="S2.T1.2.2.3.8.4"></span><span class="ltx_text" id="S2.T1.2.2.3.8.5" style="font-size:90%;"></span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.2.4">
<td class="ltx_td ltx_border_t" id="S2.T1.2.2.4.1" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_border_t" id="S2.T1.2.2.4.2" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.2.4.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S2.T1.2.2.4.3.1" style="font-size:90%;">47.16</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.2.4.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S2.T1.2.2.4.4.1" style="font-size:90%;">60.27</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.2.4.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S2.T1.2.2.4.5.1" style="font-size:90%;">13.11</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.2.4.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S2.T1.2.2.4.6.1" style="font-size:90%;">69.31</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.2.4.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S2.T1.2.2.4.7.1" style="font-size:90%;">79.76</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S2.T1.2.2.4.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S2.T1.2.2.4.8.1" style="font-size:90%;">10.45</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.2.5">
<td class="ltx_td" id="S2.T1.2.2.5.1" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.5.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S2.T1.2.2.5.2.1" style="font-size:90%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.5.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S2.T1.2.2.5.3.1" style="font-size:90%;">45.60</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.5.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S2.T1.2.2.5.4.1" style="font-size:90%;">55.25</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.5.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text ltx_phantom" id="S2.T1.2.2.5.5.1" style="font-size:90%;"><span style="visibility:hidden">0</span></span><span class="ltx_text" id="S2.T1.2.2.5.5.2" style="font-size:90%;">9.65</span>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.5.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S2.T1.2.2.5.6.1" style="font-size:90%;">64.84</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.5.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S2.T1.2.2.5.7.1" style="font-size:90%;">70.93</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.2.2.5.8" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text ltx_phantom" id="S2.T1.2.2.5.8.1" style="font-size:90%;"><span style="visibility:hidden">0</span></span><span class="ltx_text" id="S2.T1.2.2.5.8.2" style="font-size:90%;">6.09</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.2.6">
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.6.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S2.T1.2.2.6.1.1" style="font-size:90%;">✓</span></td>
<td class="ltx_td" id="S2.T1.2.2.6.2" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.6.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S2.T1.2.2.6.3.1" style="font-size:90%;">49.56</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.6.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S2.T1.2.2.6.4.1" style="font-size:90%;">60.30</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.6.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S2.T1.2.2.6.5.1" style="font-size:90%;">10.74</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.6.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S2.T1.2.2.6.6.1" style="font-size:90%;">70.90</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.6.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S2.T1.2.2.6.7.1" style="font-size:90%;">79.81</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.2.2.6.8" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text ltx_phantom" id="S2.T1.2.2.6.8.1" style="font-size:90%;"><span style="visibility:hidden">0</span></span><span class="ltx_text" id="S2.T1.2.2.6.8.2" style="font-size:90%;">8.91</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.2.7">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.2.2.7.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S2.T1.2.2.7.1.1" style="font-size:90%;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.2.2.7.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S2.T1.2.2.7.2.1" style="font-size:90%;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.2.2.7.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S2.T1.2.2.7.3.1" style="font-size:90%;">47.46</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.2.2.7.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S2.T1.2.2.7.4.1" style="font-size:90%;">53.96</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.2.2.7.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text ltx_phantom" id="S2.T1.2.2.7.5.1" style="font-size:90%;"><span style="visibility:hidden">0</span></span><span class="ltx_text ltx_font_bold" id="S2.T1.2.2.7.5.2" style="font-size:90%;">6.50</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.2.2.7.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S2.T1.2.2.7.6.1" style="font-size:90%;">68.52</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.2.2.7.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S2.T1.2.2.7.7.1" style="font-size:90%;">69.41</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S2.T1.2.2.7.8" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text ltx_phantom" id="S2.T1.2.2.7.8.1" style="font-size:90%;"><span style="visibility:hidden">0</span></span><span class="ltx_text ltx_font_bold" id="S2.T1.2.2.7.8.2" style="font-size:90%;">0.89</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption" style="font-size:144%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T1.7.1.1" style="font-size:63%;">Table S1</span>: </span><span class="ltx_text ltx_font_bold" id="S2.T1.8.2" style="font-size:63%;">Impact of background and certain body parts on the pose error gap.<span class="ltx_text ltx_font_medium" id="S2.T1.8.2.1"> The pose error gap between real and synthetic data reduces when we remove the background or blur faces, hands and feet, which are body parts that current image generators often struggle with.
</span></span></figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:144%;">Caption generation</h4>
<div class="ltx_para" id="S2.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px2.p1.1"><span class="ltx_text" id="S2.SS1.SSS0.Px2.p1.1.1" style="font-size:144%;">For training CN-3DPose, we need image captions.
We use the image captioning model BLIP2 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS1.SSS0.Px2.p1.1.2.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a><span class="ltx_text" id="S2.SS1.SSS0.Px2.p1.1.3.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S2.SS1.SSS0.Px2.p1.1.4" style="font-size:144%;"> to create textual descriptions for each image.
To avoid overfitting, we construct multiple captions for each image.
First, we sample five captions with BLIP2 and filter them using CLIP </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS1.SSS0.Px2.p1.1.5.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">60</span></a><span class="ltx_text" id="S2.SS1.SSS0.Px2.p1.1.6.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S2.SS1.SSS0.Px2.p1.1.7" style="font-size:144%;"> to ensure image-text alignment.
During training, a random caption from the set is selected for the image.
We are also using synthetic data for training (AGORA and the scans).
Thus, we want to prevent the model from generating their synthetic look.
We do so by adding the word “Rendering” at training time to captions of images from these datasets.
Using negative prompting at inference time further prevents the model from creating images similar to AGORA and human scans.</span></p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:144%;">Training</h4>
<div class="ltx_para" id="S2.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px3.p1.1"><span class="ltx_text" id="S2.SS1.SSS0.Px3.p1.1.1" style="font-size:144%;">For the hyperparameters, we follow </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS1.SSS0.Px3.p1.1.2.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib83" title=""><span class="ltx_text" style="font-size:90%;">83</span></a><span class="ltx_text" id="S2.SS1.SSS0.Px3.p1.1.3.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S2.SS1.SSS0.Px3.p1.1.4" style="font-size:144%;">.
We are training with datasets of different sizes.
To balance data, we sample from datasets with probability proportional to their size.
We train our model on 4 NVIDIA A100 40GB for nine days.</span></p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection" style="font-size:144%;">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>2D Pose Estimation Filter</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1"><span class="ltx_text" id="S2.SS2.p1.1.1" style="font-size:144%;">To identify low-quality samples in the generation, we apply a 2D keypoint-based filter mechanism.
We predict 2D keypoints for our generated images with OpenPose </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS2.p1.1.2.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">12</span></a><span class="ltx_text" id="S2.SS2.p1.1.3.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S2.SS2.p1.1.4" style="font-size:144%;"> and compare them against the projected ground truth keypoints.
We are generating images based on the SMPL mesh. To get a 2D keypoint in the OpenPose skeleton format, we first apply a joint regressor on the SMPL mesh to get 3D joints in the OpenPose format.
Then, we project them onto the image.
OpenPose predicted keypoints have different variances depending on their type, </span><em class="ltx_emph ltx_font_italic" id="S2.SS2.p1.1.5" style="font-size:144%;">e.g</em><span class="ltx_text" id="S2.SS2.p1.1.6" style="font-size:144%;">.</span><span class="ltx_text" id="S2.SS2.p1.1.7"></span><span class="ltx_text" id="S2.SS2.p1.1.8" style="font-size:144%;">,
wrists have lower variance than hips.
Therefore, we only consider wrist, ankle, shoulder, elbow, and knee keypoints for our filtering.
We compute the 2D position error for each keypoint and compare it against a threshold (50px in our case).
The image is invalid if the error of at least one selected keypoint is higher than the threshold.
The image is also invalid if multiple people are detected in the image.
Invalid images are dropped and not used for evaluation.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection" style="font-size:144%;">
<span class="ltx_tag ltx_tag_subsection">B.3 </span>Constructing the Synthetic Replica of 3DPW</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1"><span class="ltx_text" id="S2.SS3.p1.1.1" style="font-size:144%;">We sample poses from 3DPW with farthest point sampling to create a diverse subset of poses.
Each pose has a corresponding bounding box containing the target person that we use to create image crops.
Thus, each pose has a corresponding image.
The construction of the replica aims to recreate these images using STAGE.
We require poses, which are already given, and text descriptions of the images.
To create a textual description, we use our prompt template </span><span class="ltx_text ltx_font_typewriter" id="S2.SS3.p1.1.2" style="font-size:144%;">Photo, caucasian {gender} wearing {clothing} in {location} during {weather} at {daytime}</span><span class="ltx_text" id="S2.SS3.p1.1.3" style="font-size:144%;"> and fill in the values for the attributes.
The dataset was recorded during the day, and all participants were Caucasian.
We take the gender from the annotations.
To identify clothing and locations, we use a VQA model (BLIP2 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS3.p1.1.4.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a><span class="ltx_text" id="S2.SS3.p1.1.5.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S2.SS3.p1.1.6" style="font-size:144%;">) and ask
“What is the person in the foreground wearing?” and “Where is the person located?”.</span></p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1"><span class="ltx_text" id="S2.SS3.p2.1.1" style="font-size:144%;">In total, we use 1500 poses from 3DPW for the generation.
We use the provided camera parameters to create the inputs for our model.
Since we use pose-based and VQA-based quality filtering, we do not generate 1500 valid images, because we interrupt the generation process after 13 attempts.
The number of valid images the models generated is</span></p>
<ol class="ltx_enumerate" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1"><span class="ltx_text" id="S2.I1.i1.p1.1.1" style="font-size:144%;">CN-Pose: 1021</span></p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1"><span class="ltx_text" id="S2.I1.i2.p1.1.1" style="font-size:144%;">CN-Multi: 1367</span></p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1"><span class="ltx_text" id="S2.I1.i3.p1.1.1" style="font-size:144%;">CN-3DPose: 1372.</span></p>
</div>
</li>
</ol>
<p class="ltx_p" id="S2.SS3.p2.2"><span class="ltx_text" id="S2.SS3.p2.2.1" style="font-size:144%;">For a fair comparison, we only selected poses that resulted in valid images for all methods.
In total, we evaluate 981 poses.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection" style="font-size:144%;">
<span class="ltx_tag ltx_tag_subsection">B.4 </span>Generating Data for Attribute Experiments</h3>
<section class="ltx_paragraph" id="S2.SS4.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:144%;">Sampling poses</h4>
<div class="ltx_para" id="S2.SS4.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS4.SSS0.Px1.p1.1"><span class="ltx_text" id="S2.SS4.SSS0.Px1.p1.1.1" style="font-size:144%;">For our attribute sensitivity experiments, we used poses from AMASS.
Specifically, we use the subset provided by PoseScript </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS4.SSS0.Px1.p1.1.2.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">15</span></a><span class="ltx_text" id="S2.SS4.SSS0.Px1.p1.1.3.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S2.SS4.SSS0.Px1.p1.1.4" style="font-size:144%;"> and sample 1500 poses (750 male, 750 female) using furthest point sampling to create a set of diverse poses.</span></p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS4.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:144%;">Rendering model input</h4>
<div class="ltx_para" id="S2.SS4.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS4.SSS0.Px2.p1.1"><span class="ltx_text" id="S2.SS4.SSS0.Px2.p1.1.1" style="font-size:144%;">AMASS does not provide camera parameters in contrast to 3DPW.
We set the focal length to 4 by visual inspection.
The camera is placed such that its center is aligned with the root joint of the pose.
To have the whole body in the frame, we set the distance of the camera so that the body is barely visible in the frame.
We set the rotation to the identity matrix and instead rotate the pose around its vertical axis.
We want to avoid introducing variances that come from self-occluded joints.
For this, we rotate the body so that its front is facing the camera, and wrists, ankles, shoulder, and elbow joints are visible in the camera.</span></p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS4.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:144%;">Choosing a base prompt</h4>
<div class="ltx_para" id="S2.SS4.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS4.SSS0.Px3.p1.1"><span class="ltx_text" id="S2.SS4.SSS0.Px3.p1.1.1" style="font-size:144%;">For our experiments, we chose a base prompt that describes a scene that does not pose any challenges to the pose estimator.
In general, our base prompt is the following </span><span class="ltx_text ltx_font_typewriter" id="S2.SS4.SSS0.Px3.p1.1.2" style="font-size:144%;">Photo, caucasian {gender} wearing t-shirt and pants in city center at daytime sunny weather</span><span class="ltx_text" id="S2.SS4.SSS0.Px3.p1.1.3" style="font-size:144%;">, where gender is filled in based on the pose that is used for generation.
However, for specific experiments, we deviate from it.
For the location (indoor) experiment, we replace “city center” with “hallway” to provide a neutral indoor location.
For the location (outdoor), we replace “city center” with “village” to evaluate “city center” itself.
For the fairness experiments, only body shape and the gender experiment deviate from the base prompt.
For gender, we use “male” for our base dataset and “female” for our attribute dataset.
For the body shape experiment, we used “adult with average BMI” for our base dataset and “adult with low/high BMI” for the two attribute datasets.
We observed that specifying “average BMI” is not required since there is little deviation from the average body shape.</span></p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS4.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:144%;">Generating images</h4>
<div class="ltx_para" id="S2.SS4.SSS0.Px4.p1">
<p class="ltx_p" id="S2.SS4.SSS0.Px4.p1.1"><span class="ltx_text" id="S2.SS4.SSS0.Px4.p1.1.1" style="font-size:144%;">To generate the attribute datasets, we modify the corresponding base prompt by replacing the base attribute with the target attribute.
For example, if we generate data to examine coats, we replace “t-shirt and pants” with “coat” to create the attribute prompt.
Similar to the experiment on the synthetic 3DPW, we do not generate 1500 valid images because we use pose and VQA-based filtering.
Therefore, we use only poses that result in valid images for all attributes in the category.
For example, if we examine outdoor locations, we only use poses that result in valid images for all our chosen locations.
We list the number of used poses per category in </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S2.T2" style="font-size:144%;" title="In Generating images ‣ B.4 Generating Data for Attribute Experiments ‣ B Implementation Details ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">S2</span></a><span class="ltx_text" id="S2.SS4.SSS0.Px4.p1.1.2" style="font-size:144%;">.
One concern is that the number of poses is too low to estimate the PDP reliably.
Therefore, we investigate how the estimate changes with different numbers of samples.
To do so, we compute the PDP with a different number of poses.
We show the results in </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S2.F1" style="font-size:144%;" title="In Generating images ‣ B.4 Generating Data for Attribute Experiments ‣ B Implementation Details ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">S1</span></a><span class="ltx_text" id="S2.SS4.SSS0.Px4.p1.1.3" style="font-size:144%;">.
Overall, the PDP is stable after the first couple of hundred poses.
Thus, we conclude that we have used a sufficient number of samples.</span></p>
</div>
<figure class="ltx_table" id="S2.T2">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.T2.2">
<tr class="ltx_tr" id="S2.T2.2.1">
<td class="ltx_td ltx_border_tt" id="S2.T2.2.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.2.1.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S2.T2.2.1.2.1"></span><span class="ltx_text" id="S2.T2.2.1.2.2" style="font-size:90%;"> </span><span class="ltx_text" id="S2.T2.2.1.2.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S2.T2.2.1.2.3.1">
<span class="ltx_tr" id="S2.T2.2.1.2.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.2.1.2.3.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">Location</span></span>
<span class="ltx_tr" id="S2.T2.2.1.2.3.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.2.1.2.3.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">(outdoor)</span></span>
</span></span><span class="ltx_text" id="S2.T2.2.1.2.4"></span><span class="ltx_text" id="S2.T2.2.1.2.5" style="font-size:90%;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.2.1.3" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S2.T2.2.1.3.1"></span><span class="ltx_text" id="S2.T2.2.1.3.2" style="font-size:90%;"> </span><span class="ltx_text" id="S2.T2.2.1.3.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S2.T2.2.1.3.3.1">
<span class="ltx_tr" id="S2.T2.2.1.3.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.2.1.3.3.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">Location</span></span>
<span class="ltx_tr" id="S2.T2.2.1.3.3.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.2.1.3.3.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">(indoor)</span></span>
</span></span><span class="ltx_text" id="S2.T2.2.1.3.4"></span><span class="ltx_text" id="S2.T2.2.1.3.5" style="font-size:90%;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.2.1.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S2.T2.2.1.4.1" style="font-size:90%;">Fairness</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.2.1.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S2.T2.2.1.5.1" style="font-size:90%;">Clothing</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.2.1.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S2.T2.2.1.6.1" style="font-size:90%;">Weather</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="S2.T2.2.1.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S2.T2.2.1.7.1" style="font-size:90%;">Texture</span></td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.2">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S2.T2.2.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S2.T2.2.2.1.1"></span><span class="ltx_text" id="S2.T2.2.2.1.2" style="font-size:90%;"> </span><span class="ltx_text" id="S2.T2.2.2.1.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S2.T2.2.2.1.3.1">
<span class="ltx_tr" id="S2.T2.2.2.1.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.2.2.1.3.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"># poses</span></span>
</span></span><span class="ltx_text" id="S2.T2.2.2.1.4"></span><span class="ltx_text" id="S2.T2.2.2.1.5" style="font-size:90%;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S2.T2.2.2.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S2.T2.2.2.2.1" style="font-size:90%;">1062</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S2.T2.2.2.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S2.T2.2.2.3.1" style="font-size:90%;">1320</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S2.T2.2.2.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S2.T2.2.2.4.1" style="font-size:90%;">660</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S2.T2.2.2.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S2.T2.2.2.5.1" style="font-size:90%;">708</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S2.T2.2.2.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S2.T2.2.2.6.1" style="font-size:90%;">370</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S2.T2.2.2.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S2.T2.2.2.7.1" style="font-size:90%;">1085</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table S2: </span><span class="ltx_text ltx_font_bold" id="S2.T2.6.1">Remaining number of poses used for evaluation</span>. Due to our filtering mechanism it is possible that we never generate a valid image pair for a given pose. These poses are dropped from the evaluation. It is also possible that different attributes have different set of valid poses. Therefore, we use only poses that are valid for all attribute in a given category when we compute our metrics. Here we present the number of valid poses for each attribute category, that we used to compute our metrics.
</figcaption>
</figure>
<figure class="ltx_figure" id="S2.F1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.F1.6">
<tr class="ltx_tr" id="S2.F1.2.2">
<td class="ltx_td ltx_align_center" id="S2.F1.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="194" id="S2.F1.1.1.1.g1" src="x5.png" width="230"/></td>
<td class="ltx_td ltx_align_center" id="S2.F1.2.2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="194" id="S2.F1.2.2.2.g1" src="x6.png" width="230"/></td>
</tr>
<tr class="ltx_tr" id="S2.F1.4.4">
<td class="ltx_td ltx_align_center" id="S2.F1.3.3.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="194" id="S2.F1.3.3.1.g1" src="x7.png" width="230"/></td>
<td class="ltx_td ltx_align_center" id="S2.F1.4.4.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="194" id="S2.F1.4.4.2.g1" src="x8.png" width="230"/></td>
</tr>
<tr class="ltx_tr" id="S2.F1.6.6">
<td class="ltx_td ltx_align_center" id="S2.F1.5.5.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="194" id="S2.F1.5.5.1.g1" src="x9.png" width="230"/></td>
<td class="ltx_td ltx_align_center" id="S2.F1.6.6.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="194" id="S2.F1.6.6.2.g1" src="x10.png" width="230"/></td>
</tr>
</table>
<figcaption class="ltx_caption" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.12.1.1" style="font-size:63%;">Figure S1</span>: </span><span class="ltx_text" id="S2.F1.13.2" style="font-size:63%;">
<span class="ltx_text ltx_font_bold" id="S2.F1.13.2.1">Our number of samples is sufficient.</span>
We plot how our results per category change based on the number of valid poses used to compute them. Overall the PDP remains stable after the first couple of hundred of poses.
</span></figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section ltx_centering" id="S3a">
<h2 class="ltx_title ltx_title_section" style="font-size:144%;">
<span class="ltx_tag ltx_tag_section">C </span>Full Results</h2>
<figure class="ltx_figure" id="S3.F2a"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="242" id="S3.F2a.g1" src="x11.png" width="855"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2a.5.1.1" style="font-size:63%;">Figure S2</span>: </span><span class="ltx_text ltx_font_bold" id="S3.F2a.6.2" style="font-size:63%;">Clothing impacts performance.<span class="ltx_text ltx_font_medium" id="S3.F2a.6.2.1"> Clothing has the largest impact on performance. Especially items that cover most of the body, such as coats, can impact the performance in a negative way.</span></span></figcaption>
</figure>
<figure class="ltx_figure" id="S3.F3a"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="205" id="S3.F3a.g1" src="x12.png" width="855"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3a.6.1.1" style="font-size:63%;">Figure S3</span>: </span><span class="ltx_text" id="S3.F3a.7.2" style="font-size:63%;">
<span class="ltx_text ltx_font_bold" id="S3.F3a.7.2.1">Pose estimators are susceptible to texture changes.</span> All textures lead to about the same PDP, indicating that a texture change influences performance regardless of what that texture is. SPIN is particularly sensitive as up to 30 percent of the poses are degraded.
</span></figcaption>
</figure>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="242" id="S3.F4.g1" src="x13.png" width="855"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.5.1.1" style="font-size:63%;">Figure S4</span>: </span><span class="ltx_text ltx_font_bold" id="S3.F4.6.2" style="font-size:63%;">Influence of indoor locations.<span class="ltx_text ltx_font_medium" id="S3.F4.6.2.1"> The continual increase of the PDP indicates that some locations are more challenging than others. Especially, “Restaurant”, “Bar” and “Wine cellar” have the most impact on performance.
</span></span></figcaption>
</figure>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="242" id="S3.F5.g1" src="x14.png" width="855"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.5.1.1" style="font-size:63%;">Figure S5</span>: </span><span class="ltx_text ltx_font_bold" id="S3.F5.6.2" style="font-size:63%;">Influence of outdoor locations<span class="ltx_text ltx_font_medium" id="S3.F5.6.2.1">. Most outdoor locations have a similar impact on performance, “Swamp and “Wetlands” pose the most challenges to pose estimators.
</span></span></figcaption>
</figure>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="205" id="S3.F6.g1" src="x15.png" width="855"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F6.6.1.1" style="font-size:63%;">Figure S6</span>: </span><span class="ltx_text" id="S3.F6.7.2" style="font-size:63%;">
<span class="ltx_text ltx_font_bold" id="S3.F6.7.2.1">Fairness analysis.</span> We consider multiple attributes related to fairness in computer vision. Pose estimators are robust against protected attributes. However, gender and age.
</span></figcaption>
</figure>
<figure class="ltx_figure" id="S3.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="205" id="S3.F7.g1" src="x16.png" width="855"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F7.5.1.1" style="font-size:63%;">Figure S7</span>: </span><span class="ltx_text ltx_font_bold" id="S3.F7.6.2" style="font-size:63%;">Influence of adverse conditions<span class="ltx_text ltx_font_medium" id="S3.F7.6.2.1">. Adverse conditions such as snow and night influence the performance. Pose estimators seem less sensitive to fog and rain.
</span></span></figcaption>
</figure>
<figure class="ltx_figure" id="S3.F8">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.F8.25">
<tr class="ltx_tr" id="S3.F8.5.5">
<td class="ltx_td ltx_align_center" id="S3.F8.1.1.1" style="padding-bottom:-3.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F8.1.1.1.g1" src="extracted/5819071/data/supp/gallery_supp/row-eth/0.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S3.F8.2.2.2" style="padding-bottom:-3.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F8.2.2.2.g1" src="extracted/5819071/data/supp/gallery_supp/row-eth/1.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S3.F8.3.3.3" style="padding-bottom:-3.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F8.3.3.3.g1" src="extracted/5819071/data/supp/gallery_supp/row-eth/2.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S3.F8.4.4.4" style="padding-bottom:-3.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F8.4.4.4.g1" src="extracted/5819071/data/supp/gallery_supp/row-eth/3.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S3.F8.5.5.5" style="padding-bottom:-3.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F8.5.5.5.g1" src="extracted/5819071/data/supp/gallery_supp/row-eth/4.png" width="108"/></td>
</tr>
<tr class="ltx_tr" id="S3.F8.25.26">
<td class="ltx_td" id="S3.F8.25.26.1" style="padding-bottom:3.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S3.F8.25.26.2" style="padding-bottom:3.0pt;"><span class="ltx_text" id="S3.F8.25.26.2.1" style="font-size:144%;">Caucasian</span></td>
<td class="ltx_td ltx_align_center" id="S3.F8.25.26.3" style="padding-bottom:3.0pt;"><span class="ltx_text" id="S3.F8.25.26.3.1" style="font-size:144%;">Black</span></td>
<td class="ltx_td ltx_align_center" id="S3.F8.25.26.4" style="padding-bottom:3.0pt;"><span class="ltx_text" id="S3.F8.25.26.4.1" style="font-size:144%;">East Asian</span></td>
<td class="ltx_td ltx_align_center" id="S3.F8.25.26.5" style="padding-bottom:3.0pt;"><span class="ltx_text" id="S3.F8.25.26.5.1" style="font-size:144%;">Indian</span></td>
</tr>
<tr class="ltx_tr" id="S3.F8.10.10">
<td class="ltx_td ltx_align_center" id="S3.F8.6.6.1" style="padding-bottom:-3.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F8.6.6.1.g1" src="extracted/5819071/data/supp/gallery_supp/row-loc-1/0.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S3.F8.7.7.2" style="padding-bottom:-3.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F8.7.7.2.g1" src="extracted/5819071/data/supp/gallery_supp/row-loc-1/1.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S3.F8.8.8.3" style="padding-bottom:-3.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F8.8.8.3.g1" src="extracted/5819071/data/supp/gallery_supp/row-loc-1/2.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S3.F8.9.9.4" style="padding-bottom:-3.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F8.9.9.4.g1" src="extracted/5819071/data/supp/gallery_supp/row-loc-1/3.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S3.F8.10.10.5" style="padding-bottom:-3.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F8.10.10.5.g1" src="extracted/5819071/data/supp/gallery_supp/row-loc-1/4.png" width="108"/></td>
</tr>
<tr class="ltx_tr" id="S3.F8.25.27">
<td class="ltx_td" id="S3.F8.25.27.1" style="padding-bottom:3.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S3.F8.25.27.2" style="padding-bottom:3.0pt;"><span class="ltx_text" id="S3.F8.25.27.2.1" style="font-size:144%;">Church</span></td>
<td class="ltx_td ltx_align_center" id="S3.F8.25.27.3" style="padding-bottom:3.0pt;"><span class="ltx_text" id="S3.F8.25.27.3.1" style="font-size:144%;">Lecture hall</span></td>
<td class="ltx_td ltx_align_center" id="S3.F8.25.27.4" style="padding-bottom:3.0pt;"><span class="ltx_text" id="S3.F8.25.27.4.1" style="font-size:144%;">Gym</span></td>
<td class="ltx_td ltx_align_center" id="S3.F8.25.27.5" style="padding-bottom:3.0pt;"><span class="ltx_text" id="S3.F8.25.27.5.1" style="font-size:144%;">Art gallery</span></td>
</tr>
<tr class="ltx_tr" id="S3.F8.15.15">
<td class="ltx_td ltx_align_center" id="S3.F8.11.11.1" style="padding-bottom:-2.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F8.11.11.1.g1" src="extracted/5819071/data/supp/gallery_supp/row-loc-2/0.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S3.F8.12.12.2" style="padding-bottom:-2.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F8.12.12.2.g1" src="extracted/5819071/data/supp/gallery_supp/row-loc-2/1.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S3.F8.13.13.3" style="padding-bottom:-2.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F8.13.13.3.g1" src="extracted/5819071/data/supp/gallery_supp/row-loc-2/2.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S3.F8.14.14.4" style="padding-bottom:-2.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F8.14.14.4.g1" src="extracted/5819071/data/supp/gallery_supp/row-loc-2/3.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S3.F8.15.15.5" style="padding-bottom:-2.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F8.15.15.5.g1" src="extracted/5819071/data/supp/gallery_supp/row-loc-2/4.png" width="108"/></td>
</tr>
<tr class="ltx_tr" id="S3.F8.25.28">
<td class="ltx_td" id="S3.F8.25.28.1" style="padding-bottom:3.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S3.F8.25.28.2" style="padding-bottom:3.0pt;"><span class="ltx_text" id="S3.F8.25.28.2.1" style="font-size:144%;">Hallway</span></td>
<td class="ltx_td ltx_align_center" id="S3.F8.25.28.3" style="padding-bottom:3.0pt;"><span class="ltx_text" id="S3.F8.25.28.3.1" style="font-size:144%;">Kitchen</span></td>
<td class="ltx_td ltx_align_center" id="S3.F8.25.28.4" style="padding-bottom:3.0pt;"><span class="ltx_text" id="S3.F8.25.28.4.1" style="font-size:144%;">Library</span></td>
<td class="ltx_td ltx_align_center" id="S3.F8.25.28.5" style="padding-bottom:3.0pt;"><span class="ltx_text" id="S3.F8.25.28.5.1" style="font-size:144%;">Office</span></td>
</tr>
<tr class="ltx_tr" id="S3.F8.20.20">
<td class="ltx_td ltx_align_center" id="S3.F8.16.16.1" style="padding-bottom:-3.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F8.16.16.1.g1" src="extracted/5819071/data/supp/gallery_supp/row-loc-3/0.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S3.F8.17.17.2" style="padding-bottom:-3.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F8.17.17.2.g1" src="extracted/5819071/data/supp/gallery_supp/row-loc-3/1.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S3.F8.18.18.3" style="padding-bottom:-3.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F8.18.18.3.g1" src="extracted/5819071/data/supp/gallery_supp/row-loc-3/2.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S3.F8.19.19.4" style="padding-bottom:-3.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F8.19.19.4.g1" src="extracted/5819071/data/supp/gallery_supp/row-loc-3/3.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S3.F8.20.20.5" style="padding-bottom:-3.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F8.20.20.5.g1" src="extracted/5819071/data/supp/gallery_supp/row-loc-3/4.png" width="108"/></td>
</tr>
<tr class="ltx_tr" id="S3.F8.25.29">
<td class="ltx_td" id="S3.F8.25.29.1" style="padding-bottom:3.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S3.F8.25.29.2" style="padding-bottom:3.0pt;"><span class="ltx_text" id="S3.F8.25.29.2.1" style="font-size:144%;">Village</span></td>
<td class="ltx_td ltx_align_center" id="S3.F8.25.29.3" style="padding-bottom:3.0pt;"><span class="ltx_text" id="S3.F8.25.29.3.1" style="font-size:144%;">Courtyard</span></td>
<td class="ltx_td ltx_align_center" id="S3.F8.25.29.4" style="padding-bottom:3.0pt;"><span class="ltx_text" id="S3.F8.25.29.4.1" style="font-size:144%;">Botanical garden</span></td>
<td class="ltx_td ltx_align_center" id="S3.F8.25.29.5" style="padding-bottom:3.0pt;"><span class="ltx_text" id="S3.F8.25.29.5.1" style="font-size:144%;">Swamp</span></td>
</tr>
<tr class="ltx_tr" id="S3.F8.25.25">
<td class="ltx_td ltx_align_center" id="S3.F8.21.21.1" style="padding-bottom:-3.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F8.21.21.1.g1" src="extracted/5819071/data/supp/gallery_supp/row-w/0.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S3.F8.22.22.2" style="padding-bottom:-3.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F8.22.22.2.g1" src="extracted/5819071/data/supp/gallery_supp/row-w/1.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S3.F8.23.23.3" style="padding-bottom:-3.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F8.23.23.3.g1" src="extracted/5819071/data/supp/gallery_supp/row-w/2.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S3.F8.24.24.4" style="padding-bottom:-3.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F8.24.24.4.g1" src="extracted/5819071/data/supp/gallery_supp/row-w/3.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S3.F8.25.25.5" style="padding-bottom:-3.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="108" id="S3.F8.25.25.5.g1" src="extracted/5819071/data/supp/gallery_supp/row-w/4.png" width="108"/></td>
</tr>
<tr class="ltx_tr" id="S3.F8.25.30">
<td class="ltx_td" id="S3.F8.25.30.1"></td>
<td class="ltx_td ltx_align_center" id="S3.F8.25.30.2"><span class="ltx_text" id="S3.F8.25.30.2.1" style="font-size:144%;">Sunny day</span></td>
<td class="ltx_td ltx_align_center" id="S3.F8.25.30.3"><span class="ltx_text" id="S3.F8.25.30.3.1" style="font-size:144%;">Rain</span></td>
<td class="ltx_td ltx_align_center" id="S3.F8.25.30.4"><span class="ltx_text" id="S3.F8.25.30.4.1" style="font-size:144%;">Snow</span></td>
<td class="ltx_td ltx_align_center" id="S3.F8.25.30.5"><span class="ltx_text" id="S3.F8.25.30.5.1" style="font-size:144%;">Night</span></td>
</tr>
</table>
<figcaption class="ltx_caption" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F8.35.1.1" style="font-size:63%;">Figure S8</span>: </span><span class="ltx_text" id="S3.F8.36.2" style="font-size:63%;">
<span class="ltx_text ltx_font_bold" id="S3.F8.36.2.1">Generated images for various attributes.</span> We present more examples of generated images. Based on a given ground truth pose (leftmost column), we generate images of people with different ethnicities, in different locations, or during different weather/lighting conditions. We start from a base prompt “Photo, adult caucasian male/female wearing a t-shirt in the city center at daytime.” and modify a single attribute, <em class="ltx_emph ltx_font_italic" id="S3.F8.36.2.2">e.g</em>.<span class="ltx_text" id="S3.F8.36.2.3"></span> “city center” to “gym”.
Images in the same row use the same initial noise for generation.
</span></figcaption>
</figure>
<div class="ltx_para" id="S3a.p1">
<p class="ltx_p" id="S3a.p1.1"><span class="ltx_text" id="S3a.p1.1.1" style="font-size:144%;">The full results of our attribute experiments using STAGE are depicted in </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3.F2a" style="font-size:144%;" title="In C Full Results ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Figs.</span> <span class="ltx_text ltx_ref_tag">S2</span></a><span class="ltx_text" id="S3a.p1.1.2" style="font-size:144%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3.F3a" style="font-size:144%;" title="Figure S3 ‣ C Full Results ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">S3</span></a><span class="ltx_text" id="S3a.p1.1.3" style="font-size:144%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3.F4" style="font-size:144%;" title="Figure S4 ‣ C Full Results ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">S4</span></a><span class="ltx_text" id="S3a.p1.1.4" style="font-size:144%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3.F5" style="font-size:144%;" title="Figure S5 ‣ C Full Results ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">S5</span></a><span class="ltx_text" id="S3a.p1.1.5" style="font-size:144%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3.F6" style="font-size:144%;" title="Figure S6 ‣ C Full Results ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">S6</span></a><span class="ltx_text" id="S3a.p1.1.6" style="font-size:144%;"> and </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3.F7" style="font-size:144%;" title="Figure S7 ‣ C Full Results ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">S7</span></a><span class="ltx_text" id="S3a.p1.1.7" style="font-size:144%;">.
We also provide more visual examples in </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3.F8" style="font-size:144%;" title="In C Full Results ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">S8</span></a><span class="ltx_text" id="S3a.p1.1.8" style="font-size:144%;">, showing the utility of STAGE to test pose estimators with various attributes.</span></p>
</div>
<div class="ltx_para" id="S3a.p2">
<p class="ltx_p" id="S3a.p2.1"><span class="ltx_text" id="S3a.p2.1.1" style="font-size:144%;">Overall, we observe that none of the pose estimators are completely robust against attribute changes and can change their prediction if one aspect of the image is changed.
This can be best observed in </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3.F2a" style="font-size:144%;" title="In C Full Results ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Figs.</span> <span class="ltx_text ltx_ref_tag">S2</span></a><span class="ltx_text" id="S3a.p2.1.2" style="font-size:144%;"> and </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S3.F4" style="font-size:144%;" title="Figure S4 ‣ C Full Results ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">S4</span></a><span class="ltx_text" id="S3a.p2.1.3" style="font-size:144%;">, where we see a continual increase of PDP across all estimators from left to right.
This continual increase indicates that the more difficult attributes also affect the best models, even if only by a little.</span></p>
</div>
<div class="ltx_para" id="S3a.p3">
<p class="ltx_p" id="S3a.p3.1"><span class="ltx_text" id="S3a.p3.1.1" style="font-size:144%;">For the outdoor location experiments, we observe that each location has a similar impact, suggesting that they are equally challenging.
Only “swamp” and “wetlands” show a slightly higher impact compared to the rest of the attributes.
In contrast, the indoor locations show a clear increase in PDP from left to right.
The most difficult indoor locations such as “bar”, “restaurant” or “kitchen” have in common that tables are a frequently occurring object that can introduce occlusions.
Indeed, we observe that the images of these attributes can depict occluded body limbs, which suggests the leading cause of error for these attributes to be occlusions.</span></p>
</div>
</section>
<section class="ltx_section ltx_centering" id="S4a">
<h2 class="ltx_title ltx_title_section" style="font-size:144%;">
<span class="ltx_tag ltx_tag_section">D </span>Qualitative Results</h2>
<figure class="ltx_figure" id="S4.F9a">
<table class="ltx_tabular ltx_align_middle" id="S4.F9a.16">
<tr class="ltx_tr" id="S4.F9a.16.17">
<td class="ltx_td ltx_align_right" id="S4.F9a.16.17.1">
<span class="ltx_ERROR undefined" id="S4.F9a.16.17.1.1">\pbox</span><span class="ltx_text" id="S4.F9a.16.17.1.2" style="font-size:144%;">0.18CN-Pose</span>
</td>
<td class="ltx_td ltx_align_right" id="S4.F9a.16.17.2">
<span class="ltx_ERROR undefined" id="S4.F9a.16.17.2.1">\pbox</span><span class="ltx_text" id="S4.F9a.16.17.2.2" style="font-size:144%;">0.18CN-Depth</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.F9a.16.17.3"><span class="ltx_text" id="S4.F9a.16.17.3.1" style="font-size:144%;">CN-Multi</span></td>
<td class="ltx_td ltx_align_center" id="S4.F9a.16.17.4"><span class="ltx_text" id="S4.F9a.16.17.4.1" style="font-size:144%;">CN-3DPose (Ours)</span></td>
</tr>
<tr class="ltx_tr" id="S4.F9a.4.4">
<td class="ltx_td ltx_align_center" id="S4.F9a.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="138" id="S4.F9a.1.1.1.g1" src="extracted/5819071/data/supp/ours_vs/v2/row-1/1.png" width="138"/></td>
<td class="ltx_td ltx_align_center" id="S4.F9a.2.2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="138" id="S4.F9a.2.2.2.g1" src="extracted/5819071/data/supp/ours_vs/v2/row-1/2.png" width="138"/></td>
<td class="ltx_td ltx_align_center" id="S4.F9a.3.3.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="138" id="S4.F9a.3.3.3.g1" src="extracted/5819071/data/supp/ours_vs/v2/row-1/3.png" width="138"/></td>
<td class="ltx_td ltx_align_center" id="S4.F9a.4.4.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="138" id="S4.F9a.4.4.4.g1" src="extracted/5819071/data/supp/ours_vs/v2/row-1/4.png" width="138"/></td>
</tr>
<tr class="ltx_tr" id="S4.F9a.16.18">
<td class="ltx_td ltx_align_center" colspan="4" id="S4.F9a.16.18.1">
<span class="ltx_text" id="S4.F9a.16.18.1.1" style="font-size:144%;">Photo, adult caucasian male wearing </span><span class="ltx_text ltx_font_bold" id="S4.F9a.16.18.1.2" style="font-size:144%;">long coat</span><span class="ltx_text" id="S4.F9a.16.18.1.3" style="font-size:144%;"> in </span><span class="ltx_text ltx_font_bold" id="S4.F9a.16.18.1.4" style="font-size:144%;">city park</span><span class="ltx_text" id="S4.F9a.16.18.1.5" style="font-size:144%;"> at daytime</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.F9a.8.8">
<td class="ltx_td ltx_align_center" id="S4.F9a.5.5.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="138" id="S4.F9a.5.5.1.g1" src="extracted/5819071/data/supp/ours_vs/v2/row-2/1.png" width="138"/></td>
<td class="ltx_td ltx_align_center" id="S4.F9a.6.6.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="138" id="S4.F9a.6.6.2.g1" src="extracted/5819071/data/supp/ours_vs/v2/row-2/2.png" width="138"/></td>
<td class="ltx_td ltx_align_center" id="S4.F9a.7.7.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="138" id="S4.F9a.7.7.3.g1" src="extracted/5819071/data/supp/ours_vs/v2/row-2/3.png" width="138"/></td>
<td class="ltx_td ltx_align_center" id="S4.F9a.8.8.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="138" id="S4.F9a.8.8.4.g1" src="extracted/5819071/data/supp/ours_vs/v2/row-2/4.png" width="138"/></td>
</tr>
<tr class="ltx_tr" id="S4.F9a.16.19">
<td class="ltx_td ltx_align_center" colspan="4" id="S4.F9a.16.19.1">
<span class="ltx_text" id="S4.F9a.16.19.1.1" style="font-size:144%;">Photo, adult caucasian male wearing </span><span class="ltx_text ltx_font_bold" id="S4.F9a.16.19.1.2" style="font-size:144%;">jacket</span><span class="ltx_text" id="S4.F9a.16.19.1.3" style="font-size:144%;"> in </span><span class="ltx_text ltx_font_bold" id="S4.F9a.16.19.1.4" style="font-size:144%;">city center</span><span class="ltx_text" id="S4.F9a.16.19.1.5" style="font-size:144%;"> at daytime</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.F9a.12.12">
<td class="ltx_td ltx_align_center" id="S4.F9a.9.9.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="138" id="S4.F9a.9.9.1.g1" src="extracted/5819071/data/supp/ours_vs/v2/row-3/1.png" width="138"/></td>
<td class="ltx_td ltx_align_center" id="S4.F9a.10.10.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="138" id="S4.F9a.10.10.2.g1" src="extracted/5819071/data/supp/ours_vs/v2/row-3/2.png" width="138"/></td>
<td class="ltx_td ltx_align_center" id="S4.F9a.11.11.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="138" id="S4.F9a.11.11.3.g1" src="extracted/5819071/data/supp/ours_vs/v2/row-3/3.png" width="138"/></td>
<td class="ltx_td ltx_align_center" id="S4.F9a.12.12.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="138" id="S4.F9a.12.12.4.g1" src="extracted/5819071/data/supp/ours_vs/v2/row-3/4.png" width="138"/></td>
</tr>
<tr class="ltx_tr" id="S4.F9a.16.20">
<td class="ltx_td ltx_align_center" colspan="4" id="S4.F9a.16.20.1">
<span class="ltx_text" id="S4.F9a.16.20.1.1" style="font-size:144%;">Photo, adult caucasian male wearing </span><span class="ltx_text ltx_font_bold" id="S4.F9a.16.20.1.2" style="font-size:144%;">long coat</span><span class="ltx_text" id="S4.F9a.16.20.1.3" style="font-size:144%;"> in </span><span class="ltx_text ltx_font_bold" id="S4.F9a.16.20.1.4" style="font-size:144%;">city center</span><span class="ltx_text" id="S4.F9a.16.20.1.5" style="font-size:144%;"> during snow</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.F9a.16.16">
<td class="ltx_td ltx_align_center" id="S4.F9a.13.13.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="138" id="S4.F9a.13.13.1.g1" src="extracted/5819071/data/supp/ours_vs/v2/row-4/1.png" width="138"/></td>
<td class="ltx_td ltx_align_center" id="S4.F9a.14.14.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="138" id="S4.F9a.14.14.2.g1" src="extracted/5819071/data/supp/ours_vs/v2/row-4/2.png" width="138"/></td>
<td class="ltx_td ltx_align_center" id="S4.F9a.15.15.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="138" id="S4.F9a.15.15.3.g1" src="extracted/5819071/data/supp/ours_vs/v2/row-4/3.png" width="138"/></td>
<td class="ltx_td ltx_align_center" id="S4.F9a.16.16.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="138" id="S4.F9a.16.16.4.g1" src="extracted/5819071/data/supp/ours_vs/v2/row-4/4.png" width="138"/></td>
</tr>
<tr class="ltx_tr" id="S4.F9a.16.21">
<td class="ltx_td ltx_align_center" colspan="4" id="S4.F9a.16.21.1">
<span class="ltx_text" id="S4.F9a.16.21.1.1" style="font-size:144%;">Photo, adult caucasian male wearing </span><span class="ltx_text ltx_font_bold" id="S4.F9a.16.21.1.2" style="font-size:144%;">jacket</span><span class="ltx_text" id="S4.F9a.16.21.1.3" style="font-size:144%;"> in </span><span class="ltx_text ltx_font_bold" id="S4.F9a.16.21.1.4" style="font-size:144%;">restaurant</span><span class="ltx_text" id="S4.F9a.16.21.1.5" style="font-size:144%;"> at daytime</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F9a.22.1.1" style="font-size:63%;">Figure S9</span>: </span><span class="ltx_text" id="S4.F9a.23.2" style="font-size:63%;">
<span class="ltx_text ltx_font_bold" id="S4.F9a.23.2.1">Our method CN-3DPose generates diverse images.</span>
Given a single pose and multiple prompts we generate images with CN-Pose, CN-Depth, CN-Multi and CN-3DPose. CN-Depth and CN-Multi fail to follow the prompt and generate flat backgrounds or the wrong clothing item (notice the absence of a coat for CN-Depth and CN-Multi in row 1). Each image is generated from a different randomly sampled noise.
</span></figcaption>
</figure>
<div class="ltx_para" id="S4a.p1">
<p class="ltx_p" id="S4a.p1.1"><span class="ltx_text" id="S4a.p1.1.1" style="font-size:144%;">We compare our method CN-3DPose with CN-Pose, CN-Depth, and CN-Multi in terms of diversity in </span><a class="ltx_ref" href="https://arxiv.org/html/2408.16536v1#S4.F9a" style="font-size:144%;" title="In D Qualitative Results ‣ Are Pose Estimators Ready for the Open World? STAGE: Synthetic Data Generation Toolkit for Auditing 3D Human Pose Estimators"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">S9</span></a><span class="ltx_text" id="S4a.p1.1.2" style="font-size:144%;"> by generating images with fixed input pose (per figure) and different prompts.
CN-Depth and CN-Multi achieve good pose alignment but are not able to follow the prompt and generate only flat backgrounds.
CN-Pose creates diverse images but does not provide good pose alignment.
Overall, only our method, CN-3DPose, can generate diverse images while having good pose alignment.</span></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Aug 28 15:07:32 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
