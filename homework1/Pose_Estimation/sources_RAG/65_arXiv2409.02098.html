<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation</title>
<!--Generated on Mon Sep  2 15:00:06 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.02098v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S1" title="In CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S2" title="In CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S2.SS1" title="In 2 Related Work ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Optimizing LLMs for Specific Tasks</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S2.SS1.SSS0.Px1" title="In 2.1 Optimizing LLMs for Specific Tasks ‣ 2 Related Work ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title">Prompting:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S2.SS1.SSS0.Px2" title="In 2.1 Optimizing LLMs for Specific Tasks ‣ 2 Related Work ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title">Zero-Shot Inference:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S2.SS1.SSS0.Px3" title="In 2.1 Optimizing LLMs for Specific Tasks ‣ 2 Related Work ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title">Few-Shot Learning:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S2.SS1.SSS0.Px4" title="In 2.1 Optimizing LLMs for Specific Tasks ‣ 2 Related Work ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title">Full Fine-Tuning:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S2.SS1.SSS0.Px5" title="In 2.1 Optimizing LLMs for Specific Tasks ‣ 2 Related Work ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title">Instruction Tuning:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S2.SS1.SSS0.Px6" title="In 2.1 Optimizing LLMs for Specific Tasks ‣ 2 Related Work ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title">Low-Rank Adaptation:</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S2.SS2" title="In 2 Related Work ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Synthetic Data Generation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S2.SS2.SSS0.Px1" title="In 2.2 Synthetic Data Generation ‣ 2 Related Work ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title">Fully Synthetic Data Generation:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S2.SS2.SSS0.Px2" title="In 2.2 Synthetic Data Generation ‣ 2 Related Work ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title">Partially Synthetic Data Generation:</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S3" title="In CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>The CRAFT Approach</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S3.SS1" title="In 3 The CRAFT Approach ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Architecture Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S3.SS2" title="In 3 The CRAFT Approach ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Few-Shot Examples</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S3.SS3" title="In 3 The CRAFT Approach ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Corpora and Embedding Database</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S3.SS4" title="In 3 The CRAFT Approach ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Document Retrieval</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S3.SS5" title="In 3 The CRAFT Approach ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Task Sample Synthesis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S4" title="In CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S4.SS1" title="In 4 Experimental Setup ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Tasks</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S4.SS1.SSS0.Px1" title="In 4.1 Tasks ‣ 4 Experimental Setup ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title">Multiple-Choice QA:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S4.SS1.SSS0.Px2" title="In 4.1 Tasks ‣ 4 Experimental Setup ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title">Generative:</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S4.SS2" title="In 4 Experimental Setup ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S4.SS2.SSS1" title="In 4.2 Evaluation ‣ 4 Experimental Setup ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Metrics</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S4.SS2.SSS1.Px1" title="In 4.2.1 Metrics ‣ 4.2 Evaluation ‣ 4 Experimental Setup ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title">QA Tasks:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S4.SS2.SSS1.Px2" title="In 4.2.1 Metrics ‣ 4.2 Evaluation ‣ 4 Experimental Setup ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title">Generative Tasks:</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S4.SS2.SSS2" title="In 4.2 Evaluation ‣ 4 Experimental Setup ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Datasets</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S4.SS2.SSS2.Px1" title="In 4.2.2 Datasets ‣ 4.2 Evaluation ‣ 4 Experimental Setup ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title">BioQA:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S4.SS2.SSS2.Px2" title="In 4.2.2 Datasets ‣ 4.2 Evaluation ‣ 4 Experimental Setup ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title">MedQA:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S4.SS2.SSS2.Px3" title="In 4.2.2 Datasets ‣ 4.2 Evaluation ‣ 4 Experimental Setup ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title">CSQA:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S4.SS2.SSS2.Px4" title="In 4.2.2 Datasets ‣ 4.2 Evaluation ‣ 4 Experimental Setup ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title">RecipeGen:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S4.SS2.SSS2.Px5" title="In 4.2.2 Datasets ‣ 4.2 Evaluation ‣ 4 Experimental Setup ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title">Summarization:</span></a></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S5" title="In CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S5.SS1" title="In 5 Results ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Baselines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S5.SS2" title="In 5 Results ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Scaling the Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S5.SS3" title="In 5 Results ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>OOD Generalization and Data Contamination</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S5.SS4" title="In 5 Results ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Negative Results: Recipe Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S5.SS5" title="In 5 Results ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Base Model Comparison</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S6" title="In CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#A1" title="In CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Implementation Details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#A1.SS1" title="In Appendix A Implementation Details ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Few-Shot Design</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#A1.SS2" title="In Appendix A Implementation Details ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Embedding Database</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#A1.SS3" title="In Appendix A Implementation Details ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Corpora</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#A1.SS4" title="In Appendix A Implementation Details ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4 </span>Document Retrieval</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#A1.SS5" title="In Appendix A Implementation Details ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.5 </span>Task Sample Synthesis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#A1.SS6" title="In Appendix A Implementation Details ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.6 </span>Training and Optimization</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#A2" title="In CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Filtering Statistics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#A3" title="In CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Dataset Statistics</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation
<br class="ltx_break"/>Through Corpus Retrieval and Augmentation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Ingo Ziegler<sup class="ltx_sup" id="id1.1.id1">1*</sup>  Abdullatif Köksal<sup class="ltx_sup" id="id2.2.id2">2,3*</sup>  Desmond Elliott<sup class="ltx_sup" id="id3.3.id3">1</sup>  Hinrich Schütze<sup class="ltx_sup" id="id4.4.id4">2,3</sup>
<br class="ltx_break"/> 
<br class="ltx_break"/><sup class="ltx_sup" id="id5.5.id5">1</sup>Department of Computer Science, University of Copenhagen 
<br class="ltx_break"/><sup class="ltx_sup" id="id6.6.id6">2</sup>Center for Information and Language Processing (CIS), LMU Munich 
<br class="ltx_break"/><sup class="ltx_sup" id="id7.7.id7">3</sup>Munich Center for Machine Learning (MCML) 
<br class="ltx_break"/><sup class="ltx_sup" id="id8.8.id8">*</sup>Shared first authorship 
<br class="ltx_break"/><a class="ltx_ref ltx_href" href="mailto:inzi@di.ku.dk" title=""><span class="ltx_ref ltx_nolink">inzi@di.ku.dk</span></a>, <a class="ltx_ref ltx_href" href="mailto:akoksal@cis.lmu.de" title=""><span class="ltx_ref ltx_nolink">akoksal@cis.lmu.de</span></a>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id9.id1">Building high-quality datasets for specialized tasks is a time-consuming and resource-intensive process that often requires specialized domain knowledge.
We propose Corpus Retrieval and Augmentation for Fine-Tuning (CRAFT), a method for generating synthetic datasets, given a small number of user-written few-shots that demonstrate the task to be performed.
Given the few-shot examples, we use large-scale public web-crawled corpora and similarity-based document retrieval to find other relevant human-written documents.
Lastly, instruction-tuned large language models (LLMs) augment the retrieved documents into custom-formatted task samples, which then can be used for fine-tuning.
We demonstrate that CRAFT can efficiently generate
large-scale task-specific training datasets for four diverse
tasks: biology question-answering (QA), medicine QA and commonsense QA as well as summarization.
Our experiments show that CRAFT-based models outperform or achieve comparable performance to general LLMs for QA tasks, while CRAFT-based summarization models outperform models trained on human-curated data by 46 preference points.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large language models (LLMs) demonstrate strong
generalization capabilities across diverse tasks
<cite class="ltx_cite ltx_citemacro_cite">Dubey et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib21" title="">2024</a>); Ouyang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib65" title="">2022</a>)</cite>, but optimizing these models for specific tasks remains a
considerable challenge. Although zero-shot and few-shot
prompting methods provide some degree of adaptability
<cite class="ltx_cite ltx_citemacro_cite">Dong et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib20" title="">2024</a>)</cite>, task-specific
fine-tuning generally delivers better performance,
particularly for specialized and out-of-domain tasks
<cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib50" title="">2022</a>)</cite>. A key challenge for effective fine-tuning
is obtaining high-quality task-specific examples at large scale.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Traditionally, creating high-quality datasets for specific tasks involves a time-consuming and resource-intensive process, often requiring extensive manual curation and annotation (e.g. <cite class="ltx_cite ltx_citemacro_citet">Marcus et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib59" title="">1993</a>)</cite>). This challenge is particularly acute for low-resource domains or novel tasks where existing datasets may be limited or non-existent.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">On the other hand, “raw” (i.e., unannotated, free-text) web-crawled corpora are
known for their diversity and potential utility for various
tasks <cite class="ltx_cite ltx_citemacro_cite">Maini et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib56" title="">2024</a>)</cite>. Prior work has used raw data by targeted crawling of recipe websites <cite class="ltx_cite ltx_citemacro_citep">(Bień et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib9" title="">2020</a>)</cite> or word-specific filtering of crawling metadata to gather examples from pre-training corpora for sentiment analysis and summarization tasks via ratings <cite class="ltx_cite ltx_citemacro_citep">(Maas et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib55" title="">2011</a>)</cite> and bullet point summaries found in news articles <cite class="ltx_cite ltx_citemacro_citep">(See et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib76" title="">2017</a>)</cite>.
These approaches either rely on a predefined task definition
based on keywords, or on the targeted crawling of websites
which are expected to contain the desired content.
This reliance hinders the generalization of these methods to tasks where such prior knowledge is unavailable, difficult to define, or highly context-dependent.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this work, we propose Corpus Retrieval and
Augmentation for Fine-Tuning (CRAFT) to curate task-specific
samples from raw data for a wide variety of tasks. CRAFT
only requires a small set of few-shot examples from a
user to initiate the process of
crawling and structuring task examples. CRAFT
first detects relevant corpus examples from
large-scale unannotated corpora using similarity-based
retrieval. Then it uses LLMs to structure these examples
into a proper task format, effectively transforming
free-text documents into custom-formatted task samples
for fine-tuning.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">We demonstrate the effectiveness of CRAFT on four diverse
tasks: three
QA tasks – in biology, medicine and
commonsense – as well as a text
summarization generative task. Our results show that models
fine-tuned on CRAFT-generated datasets achieve performance
that is either better than or comparable to
instruction-tuned LLMs. This holds across diverse tasks,
LLMs, and dataset sizes, highlighting the effectiveness of
our approach. We publicly release the code to craft datasets
for other tasks as well as all datasets and checkpoints at
<a class="ltx_ref ltx_href" href="https://github.com/ziegler-ingo/CRAFT" title="">github.com/ziegler-ingo/CRAFT.</a></p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Optimizing LLMs for Specific Tasks</h3>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Prompting:</h5>
<div class="ltx_para" id="S2.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px1.p1.1">Prompts are added to the input to provide additional context that guides the computation and output of a model <cite class="ltx_cite ltx_citemacro_citep">(Gu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib29" title="">2023</a>)</cite>. A prompt usually takes the form of a natural language instruction <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib69" title="">2019</a>; Brown et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib10" title="">2020</a>)</cite>.
Prompting is commonly used with instruction-tuned models to define tasks and extract responses from language models, using natural language, without gradient updates.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Zero-Shot Inference:</h5>
<div class="ltx_para" id="S2.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px2.p1.1">Originally discovered in the vision domain, zero-shot inference <cite class="ltx_cite ltx_citemacro_citep">(Larochelle et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib42" title="">2008</a>)</cite> is a technique that allows models to generalize their learned knowledge from pre-training to previously unseen classes, tasks, or sample instance variants at inference time without gradient updates.
Pre-training LLMs on large corpora produces semantic representations that are generally applicable to multiple downstream tasks.
GPT-2 <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib69" title="">2019</a>)</cite> demonstrated that the
acquired capabilities can then be activated by prompting a new task in natural language.
However, zero-shot inference often falls short of the performance achieved by few-shot learning <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib10" title="">2020</a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Few-Shot Learning:</h5>
<div class="ltx_para" id="S2.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px3.p1.1">In few-shot learning, the model is provided with a small
number of task-specific examples at inference time. The few-shot examples are given to the model in the prompt, in a technique known as in-context learning <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib10" title="">2020</a>)</cite>.
While full fine-tuning generally requires a substantial amount of labeled data, few-shot learning offers an inexpensive alternative to adapt a model to a new task with a limited number of examples <cite class="ltx_cite ltx_citemacro_citep">(Dong et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib20" title="">2024</a>)</cite>.
Nonetheless, few-shot learning faces several challenges,
including inaccurate assessment of the underlying data
distribution <cite class="ltx_cite ltx_citemacro_citep">(Song et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib77" title="">2023</a>)</cite>, biases related
to small sample sizes <cite class="ltx_cite ltx_citemacro_citep">(Song et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib77" title="">2023</a>)</cite>, and
sensitivity to shot length <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib51" title="">2024</a>)</cite>, shot quality and noise <cite class="ltx_cite ltx_citemacro_citep">(Perez et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib68" title="">2021</a>; Chang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib12" title="">2021</a>; Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib15" title="">2022</a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Full Fine-Tuning:</h5>
<div class="ltx_para" id="S2.SS1.SSS0.Px4.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px4.p1.1">During full fine-tuning, all model parameters are updated on
a large dataset with the goal of
adapting the model to a domain, task or dataset <cite class="ltx_cite ltx_citemacro_citep">(Howard and Ruder, <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib36" title="">2018</a>)</cite>.
This approach usually provides the best performance by learning task-specific patterns and relationships that may not be captured by pre-training and zero- or few-shot learning alone.
However, it requires a dataset of appropriate size.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px5">
<h5 class="ltx_title ltx_title_paragraph">Instruction Tuning:</h5>
<div class="ltx_para" id="S2.SS1.SSS0.Px5.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px5.p1.1">Instruction tuning <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib88" title="">2022</a>)</cite> is a type of full fine-tuning that optimizes a model to produce more relevant answers to questions or instructions <cite class="ltx_cite ltx_citemacro_citep">(Leike et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib44" title="">2018</a>; Askell et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib5" title="">2021</a>)</cite>.
This approach enables language models to better understand and follow user intents rather than simply continuing the input text.
Instruction-tuned models regularly produce answers that are preferred by humans for tasks ranging from question-answering to summarization <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib65" title="">2022</a>)</cite>.
The main challenge is to obtain a large high-quality dataset that is both task-specific and in the desired instruction-output format.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px6">
<h5 class="ltx_title ltx_title_paragraph">Low-Rank Adaptation:</h5>
<div class="ltx_para" id="S2.SS1.SSS0.Px6.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px6.p1.1">Full fine-tuning may be too expensive for LLMs but the
difference between pre-trained weights and their fine-tuned
counterparts often has low rank <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib46" title="">2018</a>; Aghajanyan et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib3" title="">2021</a>)</cite>.
Low-Rank Adaptation <cite class="ltx_cite ltx_citemacro_cite">(Hu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib37" title="">2021</a>, LoRA)</cite> approximates these low-rank matrices during fine-tuning, and is efficient because it freezes the full model and only learns the low-rank matrices, which typically results in learning the equivalent of 2% of the model’s parameters.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Synthetic Data Generation</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Synthetic data refers to artificially generated data that mimics the characteristics of real-world data <cite class="ltx_cite ltx_citemacro_citep">(Little et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib49" title="">1993</a>)</cite>.
It can be generated using statistical <cite class="ltx_cite ltx_citemacro_citep">(Sue, <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib79" title="">1987</a>; Maqsud, <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib58" title="">2015</a>)</cite> or deep neural approaches <cite class="ltx_cite ltx_citemacro_citep">(Sutskever et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib80" title="">2011</a>)</cite> with the aim of replicating the patterns, distributions, and structures found in real-world datasets.</p>
</div>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Fully Synthetic Data Generation:</h5>
<div class="ltx_para" id="S2.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px1.p1.1">A dataset is fully synthetic if the question or instruction, the possible context, as well as the answers are generated synthetically.
For instance, Self-Instruct <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib87" title="">2023b</a>)</cite>, Unnatural
Instructions <cite class="ltx_cite ltx_citemacro_citep">(Honovich et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib35" title="">2023</a>)</cite>,
Alpaca <cite class="ltx_cite ltx_citemacro_citep">(Taori et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib82" title="">2023</a>)</cite>, and
Evol-Instruct <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib91" title="">2023</a>)</cite> are examples of fully synthetic general-purpose data generated by LLMs.
More focused approaches for task-specific fine-tuning data generation have also been proposed, especially based around the rephrasing of already existing task-specific datasets <cite class="ltx_cite ltx_citemacro_citep">(Yin et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib92" title="">2023</a>; Gandhi et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib27" title="">2024</a>)</cite>.
Methods that use general-purpose corpora have recently been proposed for generating pre-training data <cite class="ltx_cite ltx_citemacro_citep">(Maini et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib56" title="">2024</a>)</cite>.
When used for fine-tuning data generation, these methods are either based around complex and resource-intensive multi-agent workflows <cite class="ltx_cite ltx_citemacro_citep">(Mitra et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib61" title="">2024</a>)</cite> or are restricted to a small set of tasks, as the generation process relies on a model that has been fine-tuned for those tasks <cite class="ltx_cite ltx_citemacro_citep">(Nayak et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib63" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px1.p2">
<p class="ltx_p" id="S2.SS2.SSS0.Px1.p2.1">The two greatest drawbacks of current approaches to fully
synthetic data generation are repetition and low quality.
Unnatural Instructions reported that a majority of
their samples have a BERTScore <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib93" title="">2020</a>)</cite> of
above 45% when compared to other samples in the generated
dataset. Self-Instruct faces similar issues, with generated
instructions often having ROUGE-L
scores <cite class="ltx_cite ltx_citemacro_citep">(Lin, <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib48" title="">2004</a>)</cite> greater than 0.4 compared to
the provided seed instructions. Both approaches also only
contain about 54%-56.5% correct samples, while the
correctness rate in Alpaca is as low as
17% <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib13" title="">2024a</a>)</cite>. This suggests that a large
portion of the samples in these datasets may not be useful
for fine-tuning models.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="113" id="S2.F1.g1" src="x1.png" width="626"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Synthetic dataset generation process.
Given a small set of task-specific few-shots  <svg class="ltx_picture" height="19.62" id="S2.F1.5.pic1" overflow="visible" version="1.1" width="19.62"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)"><path d="M 9.54 0 C 9.54 5.27 5.27 9.54 0 9.54 C -5.27 9.54 -9.54 5.27 -9.54 0 C -9.54 -5.27 -5.27 -9.54 0 -9.54 C 5.27 -9.54 9.54 -5.27 9.54 0 Z M 0 0" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S2.F1.5.pic1.1.1.1.1.1">1</span></foreignobject></g></g></svg>, we retrieve the top-k most similar free-text documents  <svg class="ltx_picture" height="19.62" id="S2.F1.6.pic2" overflow="visible" version="1.1" width="19.62"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)"><path d="M 9.54 0 C 9.54 5.27 5.27 9.54 0 9.54 C -5.27 9.54 -9.54 5.27 -9.54 0 C -9.54 -5.27 -5.27 -9.54 0 -9.54 C 5.27 -9.54 9.54 -5.27 9.54 0 Z M 0 0" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S2.F1.6.pic2.1.1.1.1.1">2</span></foreignobject></g></g></svg> from an embedding database.
Each document is then integrated into a task template  <svg class="ltx_picture" height="19.62" id="S2.F1.7.pic3" overflow="visible" version="1.1" width="19.62"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)"><path d="M 9.54 0 C 9.54 5.27 5.27 9.54 0 9.54 C -5.27 9.54 -9.54 5.27 -9.54 0 C -9.54 -5.27 -5.27 -9.54 0 -9.54 C 5.27 -9.54 9.54 -5.27 9.54 0 Z M 0 0" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S2.F1.7.pic3.1.1.1.1.1">3</span></foreignobject></g></g></svg> alongside original few-shots and an instruction prompt.
An instruction-tuned LLM generates new synthetic task samples  <svg class="ltx_picture" height="19.62" id="S2.F1.8.pic4" overflow="visible" version="1.1" width="19.62"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)"><path d="M 9.54 0 C 9.54 5.27 5.27 9.54 0 9.54 C -5.27 9.54 -9.54 5.27 -9.54 0 C -9.54 -5.27 -5.27 -9.54 0 -9.54 C 5.27 -9.54 9.54 -5.27 9.54 0 Z M 0 0" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S2.F1.8.pic4.1.1.1.1.1">4</span></foreignobject></g></g></svg> by augmenting the content of the corpus samples to mimic the style of the few-shots.
The transformation process for each numbered step is illustrated with example documents in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S3.F2" title="Figure 2 ‣ 3.1 Architecture Overview ‣ 3 The CRAFT Approach ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_tag">2</span></a>.

</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Partially Synthetic Data Generation:</h5>
<div class="ltx_para" id="S2.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px2.p1.1">In partially synthetic data generation, a portion of the input, context, or output is generated synthetically, while the remaining portion is human-curated.
It is distinct from approaches that combine fully synthetic and purely human-curated samples at the dataset level, such as Phi <cite class="ltx_cite ltx_citemacro_citep">(Gunasekar et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib30" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px2.p2">
<p class="ltx_p" id="S2.SS2.SSS0.Px2.p2.1">One recent approach is reverse instruction
generation <cite class="ltx_cite ltx_citemacro_citep">(Köksal et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib41" title="">2023</a>)</cite>, where a language
model,
provided with a human-curated output in context,
generates
the instruction that would have prompted this output.
This produces more coherent and
correct input-output pairs because the LLM does not need to
generate the longer and more complex component of the data
sample. There are also approaches where, conversely, the
output is synthetically generated from human-curated input
samples. Such methods employ distillation techniques to
extract patterns from larger, more capable models to teach
those patterns and skills to smaller
models <cite class="ltx_cite ltx_citemacro_citep">(Mukherjee et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib62" title="">2023</a>; Mitra et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib60" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px2.p3">
<p class="ltx_p" id="S2.SS2.SSS0.Px2.p3.1">Partially synthetic data generation alleviates some of the
quality and diversity concerns of fully synthetic data generation.
However, taking a raw corpus document as the output can result in generating noisy or unnecessary information <cite class="ltx_cite ltx_citemacro_citep">(Agarwal et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib2" title="">2007</a>)</cite>, but data augmentation can mitigate these problems when generating pre-training data <cite class="ltx_cite ltx_citemacro_citep">(Maini et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib56" title="">2024</a>)</cite>.
However, when data augmentation was used to generate fine-tuning data, it required GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib64" title="">2023</a>)</cite> to build an intermediate synthetic dataset to fine-tune a sample creator model <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib14" title="">2024b</a>)</cite>.
This approach can result in a sample creator model that is a distilled version of the larger model’s knowledge and data, while also limiting the model’s task flexibility, depending on the synthesized training data.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px2.p4">
<p class="ltx_p" id="S2.SS2.SSS0.Px2.p4.1">In contrast, CRAFT produces fully synthetic data but leverages the quality and diversity advantages of human-written documents from partially synthetic data generation approaches while removing noise through augmentation.
Our approach does not require intermediate datasets, nor a
separately fine-tuned model, nor knowledge distillation from
a larger model; instead, it relies only on a small number of human-curated examples, retrieval, and in-context learning.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>The CRAFT Approach</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Architecture Overview</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">CRAFT is used to fine-tune language models by generating task-specific synthetic datasets, given a few human-curated examples of the task.
During CRAFT (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S2.F1" title="Figure 1 ‣ Fully Synthetic Data Generation: ‣ 2.2 Synthetic Data Generation ‣ 2 Related Work ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_tag">1</span></a>), we retrieve
human-written, free-text documents from a large collection
of corpora by calculating their similarity to the provided
few-shots and transforming them into
the task-specific format
through augmentation.
The only human effort required is in writing a small number of high-quality examples of the target task.
CRAFT has two phases: In the initial phase, an embedding database is created from large corpora.
While this phase can be resource-intensive, its cost is incurred only once for all subsequent tasks, and it can be easily expanded with new corpora.
In the second phase, the user-generated, task-specific few-shot examples are embedded,
enabling the retrieval of relevant documents by calculating similarity measures between few-shots and corpus documents.
Once relevant documents are retrieved,
an instruction-tuned LLM is used to
augment the retrieved free-text documents into a
task-specific design, generating synthetic task samples in
the layout that is needed for instruction-tuning (illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S2.F1" title="Figure 1 ‣ Fully Synthetic Data Generation: ‣ 2.2 Synthetic Data Generation ‣ 2 Related Work ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_tag">1</span></a>).
Finally, the synthetic dataset is used to fine-tune a task-specific language model.
We report implementation details for the whole CRAFT framework in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#A1" title="Appendix A Implementation Details ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><svg class="ltx_picture ltx_centering ltx_figure_panel" height="268.42" id="S3.F2.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,268.42) matrix(1 0 0 -1 0 0)"><g fill="#7BAFDE" fill-opacity="1.0"><path d="M 0 5.91 L 0 262.52 C 0 265.78 2.64 268.42 5.91 268.42 L 594.09 268.42 C 597.36 268.42 600 265.78 600 262.52 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F5F5F5" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 234.3 L 598.03 234.3 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill="#7BAFDE" fill-opacity="1.0"><path d="M 1.97 236.27 L 1.97 262.52 C 1.97 264.69 3.73 266.46 5.91 266.46 L 594.09 266.46 C 596.27 266.46 598.03 264.69 598.03 262.52 L 598.03 236.27 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 240.2)"><foreignobject color="#FFFFFF" height="22.31" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.F2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="width:402.3pt;"><svg class="ltx_picture" height="19.62" id="S3.F2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.pic1" overflow="visible" version="1.1" width="19.62"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)"><path d="M 9.54 0 C 9.54 5.27 5.27 9.54 0 9.54 C -5.27 9.54 -9.54 5.27 -9.54 0 C -9.54 -5.27 -5.27 -9.54 0 -9.54 C 5.27 -9.54 9.54 -5.27 9.54 0 Z M 0 0" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text ltx_font_bold" id="S3.F2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.pic1.1.1.1.1.1">1</span></foreignobject></g></g></svg>
<span class="ltx_p" id="S3.F2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.F2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">Few-Shot Design</span></span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="208.71" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.F2.pic1.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S3.F2.pic1.2.2.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.F2.pic1.2.2.2.1.1.1.1">Text:</span> However, it has become clear that human chromosomes also carry a great deal of information that is epigenetic, and not contained in the sequence of the DNA itself. Imprinting is one example. Another is seen in the phenomenon of mono-allelic expression, in which only one of the two copies of certain human genes is expressed.


<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S3.F2.pic1.2.2.2.1.1.1.2">Question:</span> What is epigenetic inheritance, and what are two examples of epigenetic changes? 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S3.F2.pic1.2.2.2.1.1.1.3">Options:</span> A. Epigenetic inheritance signifies any heritable difference in the phenotype […] 
<br class="ltx_break"/>B. Epigenetic inheritance refers to inheriting variations in the number of chromosomes […] 
<br class="ltx_break"/>C. Epigenetic inheritance implies inheriting acquired traits during lifetime, whereas two […] 
<br class="ltx_break"/>D. Epigenetic inheritance denotes acquiring beneficial mutations via natural selection, and […] 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S3.F2.pic1.2.2.2.1.1.1.4">Answer:</span> A.</span>
</span></foreignobject></g></g></svg></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><svg class="ltx_picture ltx_centering ltx_figure_panel" height="121.83" id="S3.F2.pic2" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,121.83) matrix(1 0 0 -1 0 0)"><g fill="#7BAFDE" fill-opacity="1.0"><path d="M 0 5.91 L 0 115.92 C 0 119.18 2.64 121.83 5.91 121.83 L 594.09 121.83 C 597.36 121.83 600 119.18 600 115.92 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F5F5F5" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 87.7 L 598.03 87.7 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill="#7BAFDE" fill-opacity="1.0"><path d="M 1.97 89.67 L 1.97 115.92 C 1.97 118.1 3.73 119.86 5.91 119.86 L 594.09 119.86 C 596.27 119.86 598.03 118.1 598.03 115.92 L 598.03 89.67 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 93.61)"><foreignobject color="#FFFFFF" height="22.31" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.F2.pic2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="width:402.3pt;"><svg class="ltx_picture" height="19.62" id="S3.F2.pic2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.pic1" overflow="visible" version="1.1" width="19.62"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)"><path d="M 9.54 0 C 9.54 5.27 5.27 9.54 0 9.54 C -5.27 9.54 -9.54 5.27 -9.54 0 C -9.54 -5.27 -5.27 -9.54 0 -9.54 C 5.27 -9.54 9.54 -5.27 9.54 0 Z M 0 0" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text ltx_font_bold" id="S3.F2.pic2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.pic1.1.1.1.1.1">2</span></foreignobject></g></g></svg>
<span class="ltx_p" id="S3.F2.pic2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.F2.pic2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">Corpus Sample</span></span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="62.11" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.F2.pic2.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S3.F2.pic2.2.2.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.F2.pic2.2.2.2.1.1.1.1">Text:</span> Proteins are involved in the formation of higher-ordered chromosome structures, such as chromosome loops. Some proteins, including special AT-rich sequence-binding protein-1 (SATB1), CCCTC-binding factor (CTCF) and cohesin, play key roles in disease development and recovery.</span>
</span></foreignobject></g></g></svg></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><svg class="ltx_picture ltx_centering ltx_figure_panel" height="242.36" id="S3.F2.pic3" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,242.36) matrix(1 0 0 -1 0 0)"><g fill="#7BAFDE" fill-opacity="1.0"><path d="M 0 5.91 L 0 236.46 C 0 239.72 2.64 242.36 5.91 242.36 L 594.09 242.36 C 597.36 242.36 600 239.72 600 236.46 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F5F5F5" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 208.24 L 598.03 208.24 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill="#7BAFDE" fill-opacity="1.0"><path d="M 1.97 210.21 L 1.97 236.46 C 1.97 238.63 3.73 240.4 5.91 240.4 L 594.09 240.4 C 596.27 240.4 598.03 238.63 598.03 236.46 L 598.03 210.21 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 214.14)"><foreignobject color="#FFFFFF" height="22.31" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.F2.pic3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="width:402.3pt;"><svg class="ltx_picture" height="19.62" id="S3.F2.pic3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.pic1" overflow="visible" version="1.1" width="19.62"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)"><path d="M 9.54 0 C 9.54 5.27 5.27 9.54 0 9.54 C -5.27 9.54 -9.54 5.27 -9.54 0 C -9.54 -5.27 -5.27 -9.54 0 -9.54 C 5.27 -9.54 9.54 -5.27 9.54 0 Z M 0 0" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text ltx_font_bold" id="S3.F2.pic3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.pic1.1.1.1.1.1">3</span></foreignobject></g></g></svg>
<span class="ltx_p" id="S3.F2.pic3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.F2.pic3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">Few-Shot Task Template</span></span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="182.65" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.F2.pic3.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S3.F2.pic3.2.2.2.1.1.1"><span class="ltx_text ltx_font_typewriter" id="S3.F2.pic3.2.2.2.1.1.1.1">&lt;s&gt;[INST]</span> Please carefully read the text below. Then, generate exactly one question along with four answer choices designated as A, B, C, and D based on the provided text. Then, respond to the question with the correct answer using only the corresponding letter label. Return the output only as a JSON structure in this format: <span class="ltx_text ltx_font_typewriter" id="S3.F2.pic3.2.2.2.1.1.1.2">{"question":</span> […]<span class="ltx_text ltx_font_typewriter" id="S3.F2.pic3.2.2.2.1.1.1.3">,</span> <span class="ltx_text ltx_font_typewriter" id="S3.F2.pic3.2.2.2.1.1.1.4">"options":</span> […]<span class="ltx_text ltx_font_typewriter" id="S3.F2.pic3.2.2.2.1.1.1.5">,</span> <span class="ltx_text ltx_font_typewriter" id="S3.F2.pic3.2.2.2.1.1.1.6">"answer":</span> […]<span class="ltx_text ltx_font_typewriter" id="S3.F2.pic3.2.2.2.1.1.1.7">}</span>
<br class="ltx_break"/>However, it has become clear that human chromosomes also carry a great deal of […] <span class="ltx_text ltx_font_typewriter" id="S3.F2.pic3.2.2.2.1.1.1.8">[/INST]</span> <span class="ltx_text ltx_font_typewriter" id="S3.F2.pic3.2.2.2.1.1.1.9">{"question":</span> […]<span class="ltx_text ltx_font_typewriter" id="S3.F2.pic3.2.2.2.1.1.1.10">,</span> <span class="ltx_text ltx_font_typewriter" id="S3.F2.pic3.2.2.2.1.1.1.11">"options":</span> […]<span class="ltx_text ltx_font_typewriter" id="S3.F2.pic3.2.2.2.1.1.1.12">,</span> <span class="ltx_text ltx_font_typewriter" id="S3.F2.pic3.2.2.2.1.1.1.13">"answer":</span> […]<span class="ltx_text ltx_font_typewriter" id="S3.F2.pic3.2.2.2.1.1.1.14">}&lt;/s&gt;</span></span>
<span class="ltx_p" id="S3.F2.pic3.2.2.2.1.1.2"><span class="ltx_text ltx_font_italic" id="S3.F2.pic3.2.2.2.1.1.2.1">Repeat for randomly sampled few-shots 2 and 3</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="S3.F2.pic3.2.2.2.1.1.2.2">&lt;s&gt;[INST]</span> Please carefully read the text below. Then, generate exactly one question along […] 
<br class="ltx_break"/>Proteins are involved in the formation of higher-ordered chromosome structures, […] <span class="ltx_text ltx_font_typewriter" id="S3.F2.pic3.2.2.2.1.1.2.3">[/INST]</span></span>
</span></foreignobject></g></g></svg></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><svg class="ltx_picture ltx_centering ltx_figure_panel" height="168.8" id="S3.F2.pic4" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,168.8) matrix(1 0 0 -1 0 0)"><g fill="#7BAFDE" fill-opacity="1.0"><path d="M 0 5.91 L 0 162.89 C 0 166.15 2.64 168.8 5.91 168.8 L 594.09 168.8 C 597.36 168.8 600 166.15 600 162.89 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F5F5F5" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 134.67 L 598.03 134.67 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill="#7BAFDE" fill-opacity="1.0"><path d="M 1.97 136.64 L 1.97 162.89 C 1.97 165.07 3.73 166.83 5.91 166.83 L 594.09 166.83 C 596.27 166.83 598.03 165.07 598.03 162.89 L 598.03 136.64 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 140.58)"><foreignobject color="#FFFFFF" height="22.31" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.F2.pic4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="width:402.3pt;"><svg class="ltx_picture" height="19.62" id="S3.F2.pic4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.pic1" overflow="visible" version="1.1" width="19.62"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)"><path d="M 9.54 0 C 9.54 5.27 5.27 9.54 0 9.54 C -5.27 9.54 -9.54 5.27 -9.54 0 C -9.54 -5.27 -5.27 -9.54 0 -9.54 C 5.27 -9.54 9.54 -5.27 9.54 0 Z M 0 0" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text ltx_font_bold" id="S3.F2.pic4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.pic1.1.1.1.1.1">4</span></foreignobject></g></g></svg>
<span class="ltx_p" id="S3.F2.pic4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.F2.pic4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">Synthetic Task Sample</span></span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="109.08" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.F2.pic4.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S3.F2.pic4.2.2.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.F2.pic4.2.2.2.1.1.1.1">Question:</span> Which proteins play key roles in the formation of higher-ordered chromosome structures and disease development? 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S3.F2.pic4.2.2.2.1.1.1.2">Options:</span> A. SATB1, CTCF, and cohesin 
<br class="ltx_break"/>B. Histone proteins only 
<br class="ltx_break"/>C. Transcription factors and co-TFs 
<br class="ltx_break"/>D. RNA polymerase II and transcription factors 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S3.F2.pic4.2.2.2.1.1.1.3">Answer:</span> A.</span>
</span></foreignobject></g></g></svg></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Step-by-step synthetic task sample generation process for BioQA.
The color coding indicates where each section is reused throughout the process.
For readability, we shorten text sections in this figure,
indicated by “[…]”.
 <svg class="ltx_picture" height="19.62" id="S3.F2.7.pic1" overflow="visible" version="1.1" width="19.62"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)"><path d="M 9.54 0 C 9.54 5.27 5.27 9.54 0 9.54 C -5.27 9.54 -9.54 5.27 -9.54 0 C -9.54 -5.27 -5.27 -9.54 0 -9.54 C 5.27 -9.54 9.54 -5.27 9.54 0 Z M 0 0" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S3.F2.7.pic1.1.1.1.1.1">1</span></foreignobject></g></g></svg> Few-shot design: the layout of a user-written few-shot sample that is used to guide the retrieval and task sample creation process.
 <svg class="ltx_picture" height="19.62" id="S3.F2.8.pic2" overflow="visible" version="1.1" width="19.62"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)"><path d="M 9.54 0 C 9.54 5.27 5.27 9.54 0 9.54 C -5.27 9.54 -9.54 5.27 -9.54 0 C -9.54 -5.27 -5.27 -9.54 0 -9.54 C 5.27 -9.54 9.54 -5.27 9.54 0 Z M 0 0" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S3.F2.8.pic2.1.1.1.1.1">2</span></foreignobject></g></g></svg> Corpus sample: a retrieved free-text document from the embedding database based on cosine similarity to the user-written few-shot.
 <svg class="ltx_picture" height="19.62" id="S3.F2.9.pic3" overflow="visible" version="1.1" width="19.62"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)"><path d="M 9.54 0 C 9.54 5.27 5.27 9.54 0 9.54 C -5.27 9.54 -9.54 5.27 -9.54 0 C -9.54 -5.27 -5.27 -9.54 0 -9.54 C 5.27 -9.54 9.54 -5.27 9.54 0 Z M 0 0" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S3.F2.9.pic3.1.1.1.1.1">3</span></foreignobject></g></g></svg> Few-shot task template: the prompting template that is used to augment the retrieved corpus sample into a synthetic task sample by using multiple few-shots as in-context examples.
 <svg class="ltx_picture" height="19.62" id="S3.F2.10.pic4" overflow="visible" version="1.1" width="19.62"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)"><path d="M 9.54 0 C 9.54 5.27 5.27 9.54 0 9.54 C -5.27 9.54 -9.54 5.27 -9.54 0 C -9.54 -5.27 -5.27 -9.54 0 -9.54 C 5.27 -9.54 9.54 -5.27 9.54 0 Z M 0 0" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S3.F2.10.pic4.1.1.1.1.1">4</span></foreignobject></g></g></svg> Synthetic task sample: this is an actual synthetic task sample that is generated from the corpus sample  <svg class="ltx_picture" height="19.62" id="S3.F2.11.pic5" overflow="visible" version="1.1" width="19.62"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)"><path d="M 9.54 0 C 9.54 5.27 5.27 9.54 0 9.54 C -5.27 9.54 -9.54 5.27 -9.54 0 C -9.54 -5.27 -5.27 -9.54 0 -9.54 C 5.27 -9.54 9.54 -5.27 9.54 0 Z M 0 0" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S3.F2.11.pic5.1.1.1.1.1">2</span></foreignobject></g></g></svg> using the few-shot task template  <svg class="ltx_picture" height="19.62" id="S3.F2.12.pic6" overflow="visible" version="1.1" width="19.62"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)"><path d="M 9.54 0 C 9.54 5.27 5.27 9.54 0 9.54 C -5.27 9.54 -9.54 5.27 -9.54 0 C -9.54 -5.27 -5.27 -9.54 0 -9.54 C 5.27 -9.54 9.54 -5.27 9.54 0 Z M 0 0" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S3.F2.12.pic6.1.1.1.1.1">3</span></foreignobject></g></g></svg>.
</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Few-Shot Examples</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">A small number of human-curated few-shots serve as the
“definition” of the task, i.e., they indicate how the task
is to be performed.
The few-shot samples consist of three elements:
(i) a long text that mirrors in language, content, and accuracy what a high-quality corpus sample from the web should look like,
(ii) a natural language instruction for the task to be performed, which can take the form of a direct instruction or a question about the text, and
(iii) an output that satisfies the instruction or answers the question the way that the final model should later respond.
Length statistics for texts, instructions, and outputs of our few-shots can be found in the XS row of Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#A3" title="Appendix C Dataset Statistics ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_tag">C</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">We note that the task does not need to be explicitly
specified. For example, there is no need to state the task
as “biology question-answering”; it is sufficient for the
human-curated few shots to focus on QA in the domain of
biology.
If multiple-choice questions or single-letter outputs are in the few-shots, this will result in a corresponding dataset and fine-tuned model behavior.
These examples show that CRAFT is highly customizable: Few-shot
examples enable users to tailor the model’s behavior to
specific formats, use cases, or domains. Users can create
few-shots with unique terminology, style preferences, or
domain-specific constraints, optimizing the retrieval and
the final model’s performance for particular tasks.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Corpora and Embedding Database</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">The embedding database is a key element of CRAFT as it
provides, for all corpora, embeddings of human-written documents that should be retrievable for task-specific augmentation.
It is, therefore, important that the embedding database encompasses a wide variety of linguistically and semantically diverse documents.
This diversity can be achieved by including corpora that exhibit different writing styles, tones, and vocabularies.
Task-specific, task-agnostic, public, and also private documents can provide a comprehensive coverage of relevant information.
The more varied the documents in the embedding database, the
better the coverage
will be for diverse or rare tasks.
Notably, CRAFT can also handle sensitive company data, as the encoding, storage, and retrieval can be performed on-site.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Document Retrieval</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Our retrieval system is task agnostic, both in terms of domain and complexity, in contrast to previous approaches <cite class="ltx_cite ltx_citemacro_citep">(Ein-Dor et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib23" title="">2020</a>; Dai et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib17" title="">2022</a>; Lewis et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib45" title="">2021</a>)</cite>.
The CRAFT approach relies on human-curated few-shot examples as query documents and can dynamically retrieve any document of the base corpora.
As the few-shot samples include a text containing the domain, the instruction or question, as well as the output, the resulting embedding representation of the sample contains contextualized <cite class="ltx_cite ltx_citemacro_citep">(Reimers and Gurevych, <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib72" title="">2019</a>)</cite> semantic information about both the domain and the nature of task to be performed.
Relevant text documents that contain similar latent features as the few-shots are retrieved from the corpora by calculating similarity scores based on the embedded few-shots and corpus samples.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">As corpus size increases, the risk of retrieving redundant or similar corpus samples also increases.
This is partly due to the growing volume of documents, but also because the diversity of documents within the corpora may plateau, resulting in a higher proportion of similar documents. Designing few-shots that are sufficiently diverse in topic may alleviate this issue.
For example, when creating few-shots for biology question-answering, various subtopics of biology, such as genetics, anatomy, or physiology, should be covered to broaden the range of retrieved documents.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Task Sample Synthesis</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">The retrieved documents naturally contain noise <cite class="ltx_cite ltx_citemacro_citep">(Agarwal et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib2" title="">2007</a>)</cite> and lack the formatting required for fine-tuning.
Therefore, it is necessary to convert these free-text
documents into appropriate task samples
by removing noise and undesired sections.</p>
</div>
<div class="ltx_para" id="S3.SS5.p2">
<p class="ltx_p" id="S3.SS5.p2.1">To address this, we utilize instruction-tuning prompt templates <cite class="ltx_cite ltx_citemacro_citep">(Sanh et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib75" title="">2022</a>; Maini et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib56" title="">2024</a>)</cite> to augment the free-text documents into task-specific training data while simultaneously eliminating noise.
A few-shot task template consists of three elements:
(i) one or more few-shots,
(ii) a corpus sample,
(iii) and a brief instruction for the model to generate instruction-output pairs from the content of the corpus sample.
Aside from the brief instruction,
it is easy to assemble
these templates
from material we already have.
The template only structures all information from the instruction, the few-shots, and the retrieved corpus samples to generate one continuous string that serves as input for the model generating the synthetic task samples.
In this setup, the contents of the few-shots serve as in-context examples for the completion of the instruction.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S3.F2" title="Figure 2 ‣ 3.1 Architecture Overview ‣ 3 The CRAFT Approach ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_tag">2</span></a>, step 3, shows an example of how these templates guide the model in augmenting the corpus samples into synthetic task samples.
Any instruction-tuned language model can be used for this purpose.
This augmentation step not only rephrases the text but also condenses the retrieved document down to the essential information required for the task.
The result of this step produces the final synthetic instruction-output pairs that can be used to fine-tune a language model.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S3.F2" title="Figure 2 ‣ 3.1 Architecture Overview ‣ 3 The CRAFT Approach ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_tag">2</span></a>, step 4, shows an actual example output from the generated pool of synthetic training samples, and Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#A3" title="Appendix C Dataset Statistics ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_tag">C</span></a> provides an overview of length statistics from the stages of corpus retrieval up to the synthesized input-output pairs.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Setup</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Tasks</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We generate datasets for all tasks in sizes 100, 500, 5000, and 25,000.
We refer to the few-shots as a dataset of size XS, and to the sizes ranging from 100 to 25,000 as S, M, L, and XL, respectively.
Implementation details related to fine-tuning can be found in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#A1.SS6" title="A.6 Training and Optimization ‣ Appendix A Implementation Details ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_tag">A.6</span></a>.</p>
</div>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Multiple-Choice QA:</h5>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1">We generate three synthetic QA datasets for biology (BioQA), medicine (MedQA), and commonsense (CSQA).
The datasets all follow the MMLU multiple-choice format <cite class="ltx_cite ltx_citemacro_citep">(Hendrycks et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib33" title="">2021</a>)</cite>, where each question is accompanied by a number of answer options.
Exactly one of these options is correct, and the task is to identify the correct answer.
The expected output is the corresponding letter label of the correct answer.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Generative:</h5>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.1">In addition, we develop two synthetic datasets for the generative tasks of summarization and recipe generation (RecipeGen).
The goal of summarization is to convey accurate and concise information; recipe generation is additionally focused on creating coherent and structured text that adheres to specific formatting and stylistic conventions <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib84" title="">2022</a>)</cite>.
To build a synthetic summarization dataset, we first select a corpus sample and instruct the model to extract an extensive but unsummarized section of text.
In the second step, the extracted section is transformed into a summary format, optionally incorporating elements from the raw text, such as abstracts, conclusions, or TLDRs.
This approach avoids using the original corpus samples as to-be-summarized text documents, which can be lengthy and overly broad and may result in uninformative summaries.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Evaluation</h3>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Metrics</h4>
<section class="ltx_paragraph" id="S4.SS2.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">QA Tasks:</h5>
<div class="ltx_para" id="S4.SS2.SSS1.Px1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.Px1.p1.1">All multiple-choice QA tasks are evaluated using accuracy.
We follow the evaluation approach of MMLU and assess the logarithmic probabilities for the vocabulary options corresponding to the letter labels of the answer choices <cite class="ltx_cite ltx_citemacro_citep">(Hendrycks et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib33" title="">2021</a>)</cite>.
Accordingly, greedy decoding without temperature scaling is performed.
Depending on the number of answer choices, this evaluation can range from options A and B to options A through E.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">Generative Tasks:</h5>
<div class="ltx_para" id="S4.SS2.SSS1.Px2.p1">
<p class="ltx_p" id="S4.SS2.SSS1.Px2.p1.1">Automated metrics like ROUGE <cite class="ltx_cite ltx_citemacro_citep">(Lin, <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib48" title="">2004</a>)</cite> and METEOR <cite class="ltx_cite ltx_citemacro_citep">(Banerjee and Lavie, <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib7" title="">2005</a>)</cite> are resource-efficient as evaluation metrics but face limitations in generative tasks.
They rely heavily on n-gram overlap, which may not accurately reflect the true quality of the generated text <cite class="ltx_cite ltx_citemacro_citep">(Barbella et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib8" title="">2021</a>)</cite>.
They also assume that the reference text provides a complete and accurate representation of the desired output, which is not always guaranteed <cite class="ltx_cite ltx_citemacro_citep">(Graham, <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib28" title="">2015</a>; Sai et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib74" title="">2019</a>)</cite>.
Reference texts can be low-quality, contain errors and ambiguities, which leads to unreliable evaluation results. This issue is illustrated by the findings of <cite class="ltx_cite ltx_citemacro_citet">Sottana et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib78" title="">2023</a>)</cite>, who demonstrated that human reviewers frequently rate gold-standard benchmark answers among the worst answer options.
This issue worsens when generated texts differ significantly in length from references, as subsequence-based metrics struggle to capture such variations <cite class="ltx_cite ltx_citemacro_citep">(Celikyilmaz et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib11" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.Px2.p2">
<p class="ltx_p" id="S4.SS2.SSS1.Px2.p2.1">As an alternative, we opt to evaluate generations using LLMs as a judge <cite class="ltx_cite ltx_citemacro_citep">(Eldan and Li, <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib24" title="">2023</a>)</cite>.
In this setup, the LLM effectively acts as a human annotator, providing a binary preference score for each pair of outputs, resulting in a win rate as the final metric <cite class="ltx_cite ltx_citemacro_citep">(Chiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib16" title="">2024</a>)</cite>.
This approach has become increasingly common: LLMs have been successfully employed as annotators while demonstrating high inter-rater reliability <cite class="ltx_cite ltx_citemacro_citep">(Hackl et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib31" title="">2023</a>; Sottana et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib78" title="">2023</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib52" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.Px2.p3">
<p class="ltx_p" id="S4.SS2.SSS1.Px2.p3.1">For general-purpose outputs, we use the popular Alpaca-Eval
benchmark <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib47" title="">2023</a>)</cite>
that evaluates
multiple LLMs on about 650 human-curated questions <cite class="ltx_cite ltx_citemacro_citep">(Dubois et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib22" title="">2023</a>)</cite>. We select Llama 3 70B <cite class="ltx_cite ltx_citemacro_citep">(Dubey et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib21" title="">2024</a>)</cite> as our annotator model due to its open nature and cost-efficiency for high-volume experiments. As of July 2024, Llama 3 70B ranks 4th in human agreement with a score of 67.5, close to customized GPT-4 versions at 69.2.</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Datasets</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">To assess the quality of our synthetically generated datasets, we compare the performance of models trained on them to those trained on human-curated datasets.</p>
</div>
<section class="ltx_paragraph" id="S4.SS2.SSS2.Px1">
<h5 class="ltx_title ltx_title_paragraph">BioQA:</h5>
<div class="ltx_para" id="S4.SS2.SSS2.Px1.p1">
<p class="ltx_p" id="S4.SS2.SSS2.Px1.p1.1">We use the 800 sample test split from the biology subsection of ScienceQA <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib54" title="">2022</a>)</cite>.
ScienceQA sourced expert-curated question-answer pairs from online learning platforms, ensuring a high level of quality and accuracy.
The dataset’s answer options range from two to five, have a single correct answer per question, and are randomized to prevent pattern recognition.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS2.Px2">
<h5 class="ltx_title ltx_title_paragraph">MedQA:</h5>
<div class="ltx_para" id="S4.SS2.SSS2.Px2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.Px2.p1.1">We use the 4183 samples from the MedMCQA validation split <cite class="ltx_cite ltx_citemacro_citep">(Pal et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib66" title="">2022</a>)</cite>.
The dataset is comprised of entrance exam questions to two of India’s postgraduate institutions.
All dataset samples are sourced from preparation or real exams created by medical professionals.
All questions have 4 answer options with one correct answer option per question.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS2.Px3">
<h5 class="ltx_title ltx_title_paragraph">CSQA:</h5>
<div class="ltx_para" id="S4.SS2.SSS2.Px3.p1">
<p class="ltx_p" id="S4.SS2.SSS2.Px3.p1.1">We benchmark on the 2541 validation set samples in CommonsenseQA 2.0 <cite class="ltx_cite ltx_citemacro_citep">(Talmor et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib81" title="">2021</a>)</cite>.
The dataset was generated through a gamified but controlled question generation process, where players were incentivized to design challenging yes/no questions by earning points when beating an AI model.
The generated questions were then cross-validated by other players, independent validators, and another model, to ensure that the questions were well-formed, answerable, and representative of common sense.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS2.Px4">
<h5 class="ltx_title ltx_title_paragraph">RecipeGen:</h5>
<div class="ltx_para" id="S4.SS2.SSS2.Px4.p1">
<p class="ltx_p" id="S4.SS2.SSS2.Px4.p1.1">We randomly sample 1000 samples from the higher-quality subsection of the RecipeNLG dataset <cite class="ltx_cite ltx_citemacro_citep">(Bień et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib9" title="">2020</a>)</cite>.
The recipes were scraped from cooking websites and post-processed using a fine-grained and standardized cleaning and formatting process to ensure correctness.
Each recipe features a title, the list of required ingredients, as well as the steps to produce the meal.
We only include samples not present in C4, based on the URL in each dataset.</p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="458" id="S4.F3.g1" src="x2.png" width="813"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Performance scaling with increasing data size across multiple tasks when running CRAFT with 8 (solid blue line) and 32 (dashed black line) few-shots.
The graphs illustrate consistent improvement as the training data size increases from few-shots (XS) to 25,000 synthetic task samples (XL). CRAFT-trained models consistently outperform or match the Mistral 7B Instruct v0.2 baseline (dotted red line).
Shaded regions represent the standard deviation across three runs.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS2.Px5">
<h5 class="ltx_title ltx_title_paragraph">Summarization:</h5>
<div class="ltx_para" id="S4.SS2.SSS2.Px5.p1">
<p class="ltx_p" id="S4.SS2.SSS2.Px5.p1.1">We use 1000 samples from the test split of the CNN-DailyMail dataset <cite class="ltx_cite ltx_citemacro_citep">(See et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib76" title="">2017</a>)</cite>.
The dataset is commonly used for summarization because it consists of articles and stories from CNN and DailyMail alongside their highlights in abstract or bullet point format presented at the top of newspaper pages or websites.</p>
</div>
</section>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Baselines</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">We compare the performance of CRAFT, trained on our synthetic datasets, against three baselines.
The few-shot baseline is a model fine-tuned only on the XS size CRAFT dataset, with human-curated few-shots.
It serves as the primary baseline since this model uses all
human-curated
data available in our pipeline.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">The second baseline is the instruction-tuned model, Mistral 7B Instruct v0.2 <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib38" title="">2023</a>)</cite>,
which has been fine-tuned on proprietary instruction-following datasets that mix various tasks and sources.
This baseline provides a meaningful comparison, as it is similar in size and instruction-tuned like CRAFT models, though it is trained on undisclosed datasets of unknown quality and quantity.
Thus, matching or exceeding the performance of instruction-tuned models with our synthetic data would indicate that CRAFT can produce high-quality datasets.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">The upper bound of expected performance is fine-tuning the models on the in-domain human-curated training splits from the chosen evaluation datasets.
This baseline represents the optimal performance achievable with human-quality datasets.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Scaling the Data</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We report the performance gain when scaling up our training data in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S4.F3" title="Figure 3 ‣ RecipeGen: ‣ 4.2.2 Datasets ‣ 4.2 Evaluation ‣ 4 Experimental Setup ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_tag">3</span></a>.
We report the mean
and standard deviation
across three seeds.
We observe consistent improvements across four tasks as we
increase the data size. Relative to the baseline models
trained with only few-shot examples, we see improvements of
17% (from 66.9 to 78.1), 12% (from 55.3 to 62.1), 23%
(from 39.1 to 48.0), and 124% (from 43.7 to 97.9) for
BioQA, CSQA, MedQA, and Summarization, respectively. This
shows that CRAFT can be used for diverse tasks, starting
with just a few curated examples. We also find
appropriate scaling for each set of examples, ranging from
100 to 25,000 across all tasks. Additionally,
that models trained with fewer examples (32, 100) exhibit
much more variance than those trained with 5,000 and 25,000
examples, as indicated by the gray regions in the plots that visualize the standard deviation.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">For all tasks, we achieve
results
that are are clearly better of comparable to
Mistral Instruct. It is worth
noting that CRAFT uses an LLM in a limited way (to
restructure and rewrite existing corpora) that seem to
exclude the possibility that distillation may have played a
role here. However, even if distillation were to be considered the
reason for good CRAFT performance, the results indicate
otherwise:
we use the same model (Mistral
Instruct) to paraphrase existing corpora examples but
achieve even stronger results. Finally, we observe that
CRAFT models outperform those trained with official
human-curated large-scale data in Summarization. For other
tasks, while we observe lower performance than with official
data, we speculate that this could be due to in-domain
evaluation for official human-curated data. We use their
test split to evaluate our models, which may give these
models an unfair advantage. We investigate this further in
the next section.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T1.1" style="width:227.6pt;height:115.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-10.7pt,5.4pt) scale(0.913735366314922,0.913735366314922) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T1.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T1.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T1.1.1.1.1.1">Dataset</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.1.1.1.1.2">Baseline</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.1.1.1.1.3">CRAFT<sub class="ltx_sub" id="S5.T1.1.1.1.1.3.1">XL</sub>
</th>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T1.1.1.2.2.1">In-domain</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.1.1.2.2.2"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.2.2.2.1">89.9</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.1.1.2.2.3">78.1</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.1.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T1.1.1.3.1.1">MMLU<sub class="ltx_sub" id="S5.T1.1.1.3.1.1.1">Medical Genetics</sub>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.3.1.2">60.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.3.1.3"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.3.1.3.1">69.0</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.1.4.2.1">MMLU<sub class="ltx_sub" id="S5.T1.1.1.4.2.1.1">Anatomy</sub>
</th>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.4.2.2">55.6</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.4.2.3"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.4.2.3.1">57.0</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.1.5.3.1">MMLU<sub class="ltx_sub" id="S5.T1.1.1.5.3.1.1">High School Biology</sub>
</th>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.5.3.2"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.5.3.2.1">69.3</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.5.3.3">67.4</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.1.6.4.1">MMLU<sub class="ltx_sub" id="S5.T1.1.1.6.4.1.1">College Biology</sub>
</th>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.6.4.2">66.7</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.6.4.3"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.6.4.3.1">74.3</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S5.T1.1.1.7.5.1">MMLU-Avg</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.1.1.7.5.2">62.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.1.1.7.5.3"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.7.5.3.1">66.9</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>OOD evaluation of baseline and CRAFT<sub class="ltx_sub" id="S5.T1.3.1">XL</sub>.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>OOD Generalization and Data Contamination</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">We now report experiments to understand the level of data contamination or similarity between test and training examples in the experiments introduced in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S4.F3" title="Figure 3 ‣ RecipeGen: ‣ 4.2.2 Datasets ‣ 4.2 Evaluation ‣ 4 Experimental Setup ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_tag">3</span></a>. We conduct 5-gram weighted Jaccard similarity analyses between CRAFT datasets and the test data.
For each sample, we combine the instruction and output and gather 5-gram frequencies for the whole dataset.
We then calculate the Jaccard similarity between the 5-gram
frequency distributions of the respective CRAFT and test
dataset, where 5-grams receive weight proportional to their frequency.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">This analysis shows that all CRAFT datasets have less than
0.4% similarity with the task test set, whereas the
original human-authored datasets show much higher
similarities: BioQA (17.9%); CSQA (4.4%), MedQA (1.1%),
and Summarization (0.3%); this may indicate some overlap
between train and test splits. Since we have curated the
few-shots manually, rather than copying from
existing datasets, our low overlap (0.4%) may be expected. To
further investigate CRAFT’s performance improvement, we
select four out-of-domain datasets for the biology question
answering task and compare CRAFT with the human-curated
baseline.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1">In Table <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S5.T1" title="Table 1 ‣ 5.2 Scaling the Data ‣ 5 Results ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_tag">1</span></a>, we compare the human-curated baseline with CRAFT on the in-domain dataset for the baseline and four out-of-domain (OOD) datasets selected from MMLU <cite class="ltx_cite ltx_citemacro_cite">Hendrycks et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib33" title="">2021</a>)</cite>. Although the baseline outperforms CRAFT by more than 11% in the biology subset of the ScienceQA test set (in-domain for the baseline), CRAFT outperforms the baseline on 3 out of 4 OOD datasets. On average, CRAFT outperforms the baseline by 4%.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T2.1" style="width:455.2pt;height:141.4pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-34.8pt,10.7pt) scale(0.86745078806349,0.86745078806349) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T2.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.1.1">Task</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.1.2">Model</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.1.3">CRAFT<sub class="ltx_sub" id="S5.T2.1.1.1.1.3.1">XS</sub>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.1.4">CRAFT<sub class="ltx_sub" id="S5.T2.1.1.1.1.4.1">S</sub>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.1.5">CRAFT<sub class="ltx_sub" id="S5.T2.1.1.1.1.5.1">M</sub>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.1.6">CRAFT<sub class="ltx_sub" id="S5.T2.1.1.1.1.6.1">L</sub>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.1.7">CRAFT<sub class="ltx_sub" id="S5.T2.1.1.1.1.7.1">XL</sub>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.1.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.1.2.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.2.1.1.1">BioQA</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.1.2.1.2">Mistral</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.1.2.1.3">66.88<span class="ltx_text" id="S5.T2.1.1.2.1.3.1" style="font-size:80%;">±1.24</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.1.2.1.4">65.71<span class="ltx_text" id="S5.T2.1.1.2.1.4.1" style="font-size:80%;">±1.74 <span class="ltx_text" id="S5.T2.1.1.2.1.4.1.1" style="color:#330000;">(-1.74%)</span></span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.1.2.1.5">71.00<span class="ltx_text" id="S5.T2.1.1.2.1.5.1" style="font-size:80%;">±1.87 <span class="ltx_text" id="S5.T2.1.1.2.1.5.1.1" style="color:#001F00;">(6.17%)</span></span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.1.2.1.6">76.75<span class="ltx_text" id="S5.T2.1.1.2.1.6.1" style="font-size:80%;">±0.64 <span class="ltx_text" id="S5.T2.1.1.2.1.6.1.1" style="color:#004700;">(14.76%)</span></span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.1.2.1.7">78.13<span class="ltx_text" id="S5.T2.1.1.2.1.7.1" style="font-size:80%;">±0.35 <span class="ltx_text" id="S5.T2.1.1.2.1.7.1.1" style="color:#005200;">(16.82%)</span></span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.3.2">
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.3.2.1">Llama</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.3.2.2">64.38<span class="ltx_text" id="S5.T2.1.1.3.2.2.1" style="font-size:80%;">±1.98</span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.3.2.3">69.09<span class="ltx_text" id="S5.T2.1.1.3.2.3.1" style="font-size:80%;">±0.78 <span class="ltx_text" id="S5.T2.1.1.3.2.3.1.1" style="color:#002400;">(7.32%)</span></span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.3.2.4">70.38<span class="ltx_text" id="S5.T2.1.1.3.2.4.1" style="font-size:80%;">±0.57 <span class="ltx_text" id="S5.T2.1.1.3.2.4.1.1" style="color:#002E00;">(9.32%)</span></span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.3.2.5">73.38<span class="ltx_text" id="S5.T2.1.1.3.2.5.1" style="font-size:80%;">±1.24 <span class="ltx_text" id="S5.T2.1.1.3.2.5.1.1" style="color:#004200;">(13.99%)</span></span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.3.2.6">76.63<span class="ltx_text" id="S5.T2.1.1.3.2.6.1" style="font-size:80%;">±1.22 <span class="ltx_text" id="S5.T2.1.1.3.2.6.1.1" style="color:#006100;">(19.03%)</span></span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.4.3">
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.4.3.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.4.3.1.1">CSQA</span></td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.4.3.2">Mistral</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.4.3.3">55.33<span class="ltx_text" id="S5.T2.1.1.4.3.3.1" style="font-size:80%;">±5.09</span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.4.3.4">56.47<span class="ltx_text" id="S5.T2.1.1.4.3.4.1" style="font-size:80%;">±4.76 <span class="ltx_text" id="S5.T2.1.1.4.3.4.1.1" style="color:#000A00;">(2.06%)</span></span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.4.3.5">60.68<span class="ltx_text" id="S5.T2.1.1.4.3.5.1" style="font-size:80%;">±0.31 <span class="ltx_text" id="S5.T2.1.1.4.3.5.1.1" style="color:#002E00;">(9.67%)</span></span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.4.3.6">61.77<span class="ltx_text" id="S5.T2.1.1.4.3.6.1" style="font-size:80%;">±0.80 <span class="ltx_text" id="S5.T2.1.1.4.3.6.1.1" style="color:#003800;">(11.63%)</span></span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.4.3.7">62.10<span class="ltx_text" id="S5.T2.1.1.4.3.7.1" style="font-size:80%;">±0.54 <span class="ltx_text" id="S5.T2.1.1.4.3.7.1.1" style="color:#004000;">(12.23%)</span></span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.5.4">
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.5.4.1">Llama</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.5.4.2">56.72<span class="ltx_text" id="S5.T2.1.1.5.4.2.1" style="font-size:80%;">±0.82</span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.5.4.3">54.60<span class="ltx_text" id="S5.T2.1.1.5.4.3.1" style="font-size:80%;">±0.44 <span class="ltx_text" id="S5.T2.1.1.5.4.3.1.1" style="color:#330000;">(-3.74%)</span></span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.5.4.4">59.57<span class="ltx_text" id="S5.T2.1.1.5.4.4.1" style="font-size:80%;">±0.85 <span class="ltx_text" id="S5.T2.1.1.5.4.4.1.1" style="color:#001A00;">(5.02%)</span></span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.5.4.5">60.45<span class="ltx_text" id="S5.T2.1.1.5.4.5.1" style="font-size:80%;">±0.40 <span class="ltx_text" id="S5.T2.1.1.5.4.5.1.1" style="color:#001F00;">(6.58%)</span></span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.5.4.6">61.64<span class="ltx_text" id="S5.T2.1.1.5.4.6.1" style="font-size:80%;">±0.64 <span class="ltx_text" id="S5.T2.1.1.5.4.6.1.1" style="color:#002900;">(8.67%)</span></span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.6.5">
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.6.5.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.6.5.1.1">MedQA</span></td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.6.5.2">Mistral</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.6.5.3">39.17<span class="ltx_text" id="S5.T2.1.1.6.5.3.1" style="font-size:80%;">±0.48</span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.6.5.4">41.81<span class="ltx_text" id="S5.T2.1.1.6.5.4.1" style="font-size:80%;">±1.79 <span class="ltx_text" id="S5.T2.1.1.6.5.4.1.1" style="color:#002E00;">(6.72%)</span></span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.6.5.5">46.49<span class="ltx_text" id="S5.T2.1.1.6.5.5.1" style="font-size:80%;">±1.13 <span class="ltx_text" id="S5.T2.1.1.6.5.5.1.1" style="color:#005C00;">(18.68%)</span></span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.6.5.6">46.16<span class="ltx_text" id="S5.T2.1.1.6.5.6.1" style="font-size:80%;">±0.27 <span class="ltx_text" id="S5.T2.1.1.6.5.6.1.1" style="color:#005700;">(17.83%)</span></span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.6.5.7">47.99<span class="ltx_text" id="S5.T2.1.1.6.5.7.1" style="font-size:80%;">±0.36 <span class="ltx_text" id="S5.T2.1.1.6.5.7.1.1" style="color:#007000;">(22.52%)</span></span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.7.6">
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.7.6.1">Llama</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.7.6.2">49.65<span class="ltx_text" id="S5.T2.1.1.7.6.2.1" style="font-size:80%;">±0.35</span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.7.6.3">49.70<span class="ltx_text" id="S5.T2.1.1.7.6.3.1" style="font-size:80%;">±0.07 <span class="ltx_text" id="S5.T2.1.1.7.6.3.1.1" style="color:#000D00;">(0.10%)</span></span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.7.6.4">52.38<span class="ltx_text" id="S5.T2.1.1.7.6.4.1" style="font-size:80%;">±0.11 <span class="ltx_text" id="S5.T2.1.1.7.6.4.1.1" style="color:#001A00;">(5.49%)</span></span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.7.6.5">52.07<span class="ltx_text" id="S5.T2.1.1.7.6.5.1" style="font-size:80%;">±0.42 <span class="ltx_text" id="S5.T2.1.1.7.6.5.1.1" style="color:#001400;">(4.86%)</span></span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.7.6.6">52.82<span class="ltx_text" id="S5.T2.1.1.7.6.6.1" style="font-size:80%;">±0.29 <span class="ltx_text" id="S5.T2.1.1.7.6.6.1.1" style="color:#002100;">(6.37%)</span></span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.8.7">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T2.1.1.8.7.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.8.7.1.1">Summ</span></td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.8.7.2">Mistral</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.8.7.3">43.73<span class="ltx_text" id="S5.T2.1.1.8.7.3.1" style="font-size:80%;">±2.78</span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.8.7.4">52.80<span class="ltx_text" id="S5.T2.1.1.8.7.4.1" style="font-size:80%;">±3.35 <span class="ltx_text" id="S5.T2.1.1.8.7.4.1.1" style="color:#006600;">(20.73%)</span></span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.8.7.5">91.90<span class="ltx_text" id="S5.T2.1.1.8.7.5.1" style="font-size:80%;">±1.02 <span class="ltx_text" id="S5.T2.1.1.8.7.5.1.1" style="color:#00FF00;">(110.14%)</span></span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.8.7.6">96.23<span class="ltx_text" id="S5.T2.1.1.8.7.6.1" style="font-size:80%;">±0.40 <span class="ltx_text" id="S5.T2.1.1.8.7.6.1.1" style="color:#00FF00;">(120.05%)</span></span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.8.7.7">97.90<span class="ltx_text" id="S5.T2.1.1.8.7.7.1" style="font-size:80%;">±0.37 <span class="ltx_text" id="S5.T2.1.1.8.7.7.1.1" style="color:#00FF00;">(123.86%)</span></span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.9.8">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T2.1.1.9.8.1">Llama</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T2.1.1.9.8.2">31.70<span class="ltx_text" id="S5.T2.1.1.9.8.2.1" style="font-size:80%;">±3.97</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T2.1.1.9.8.3">28.20<span class="ltx_text" id="S5.T2.1.1.9.8.3.1" style="font-size:80%;">±3.28 <span class="ltx_text" id="S5.T2.1.1.9.8.3.1.1" style="color:#330000;">(-11.04%)</span></span>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T2.1.1.9.8.4">88.27<span class="ltx_text" id="S5.T2.1.1.9.8.4.1" style="font-size:80%;">±1.18 <span class="ltx_text" id="S5.T2.1.1.9.8.4.1.1" style="color:#00FF00;">(178.44%)</span></span>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T2.1.1.9.8.5">96.63<span class="ltx_text" id="S5.T2.1.1.9.8.5.1" style="font-size:80%;">±0.24 <span class="ltx_text" id="S5.T2.1.1.9.8.5.1.1" style="color:#00FF00;">(204.84%)</span></span>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T2.1.1.9.8.6">97.17<span class="ltx_text" id="S5.T2.1.1.9.8.6.1" style="font-size:80%;">±0.38 <span class="ltx_text" id="S5.T2.1.1.9.8.6.1.1" style="color:#00FF00;">(206.52%)</span></span>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Performance comparison of Llama and Mistral across different tasks and few-shot settings. Values show accuracy ± standard deviation, with relative improvement in parentheses compared to the CRAFT<sub class="ltx_sub" id="S5.T2.3.1">XS</sub> model.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Negative Results: Recipe Generation</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">Out of the five tasks we selected, we observe non-scaling behavior in one task: Recipe Generation. While our manually curated few-shots are of high quality, we see a drop when scaling from 32 to 25,000 examples, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S5.F4" title="Figure 4 ‣ 5.4 Negative Results: Recipe Generation ‣ 5 Results ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_tag">4</span></a>. CRAFT’s performance is still better than the baseline with official human data, which means that the final dataset is usable. However, we explore why this reverse scaling occurs and examine the drop in performance.</p>
</div>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="566" id="S5.F4.g1" src="x3.png" width="828"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Non-scaling behavior observed in the RecipeGen
task. Unlike the general trend, performance does not
consistently improve with increasing data size.
We report the overall dataset quality with a solid orange square line. The
graph shows an inverse relationship
between dataset size and quality. This anomaly in
scaling behavior is attributed to a decline in data
quality as the number of examples increases.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.1">An initial analysis suggested that the CRAFT
pipeline tends to find less relevant examples over time. We conducted automated data
quality analysis to
analyze this on a larger scale. For 500 randomly sampled instructions from
different sizes of CRAFT datasets (i.e., the training sets), we used the
Llama 3 8B Instruct model to answer the instructions. Then,
using Llama 3 70B Instruct as a judge, we compared win
rates, i.e., which output the model preferred: the gold
output in the CRAFT dataset or the output generated by
Llama 3 8B Instruct. We report the average win rate against the Llama outputs as the data quality metric. Higher scores
indicate that the pipeline created higher quality output
than Llama 3 8B Instruct’s answers.</p>
</div>
<div class="ltx_para" id="S5.SS4.p3">
<p class="ltx_p" id="S5.SS4.p3.1">We observe that data quality drops when scaling up to
25,000 examples. The 100 and 500 example sets have win rates
around 0.4, while the win rate for 25K drops to
0.3. We believe this is the cause of the performance drop
with scaling. While the final dataset
is still useful (it outperforms the baseline with
official human data), the next version of CRAFT should
include either effective stopping criteria or additional
validators for quality.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Base Model Comparison</h3>
<div class="ltx_para" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1">In previous sections, we fine-tuned CRAFT models using the pretrained Mistral 7B model. Now, we repeat the experiments using the pretrained Llama 3 8B model. We observe similar trends across all tasks, and the relative improvement is comparable when scaling up from few-shots to 25,000 examples, as illustrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S5.T2" title="Table 2 ‣ 5.3 OOD Generalization and Data Contamination ‣ 5 Results ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS5.p2">
<p class="ltx_p" id="S5.SS5.p2.1">In all experiments, we manually curated 32-shot examples and expanded our synthetic data examples from that point. However, even curating 32 examples can be time-consuming. We can limit the number of few-shots to just eight examples to bootstrap the CRAFT process. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S4.F3" title="Figure 3 ‣ RecipeGen: ‣ 4.2.2 Datasets ‣ 4.2 Evaluation ‣ 4 Experimental Setup ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_tag">3</span></a> shows the results compared to the CRAFT pipeline with 32-shots. While the final results with 25,000 examples are slightly lower for 8-shots, the trend is similar for both 8- and 32-shot examples. We observe that the model trained with 25,000 samples, based on running CRAFT with 8 few-shots, significantly outperforms the model trained with only 32 few-shots (i.e., no extra synthetic data). This suggests that if there are time and resource limitations, using CRAFT with fewer initial examples leads to better models than trying to curate and train only on more human-curated few-shot examples.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this work, we introduced CRAFT (Corpus Retrieval and
Augmentation for Fine-Tuning), a framework for generating
task-specific synthetic datasets grounded in
text
corpora. CRAFT requires only a small set of human-curated few-shot examples to bootstrap the creation of large-scale training data by leveraging existing corpora and instruction-tuned language models. Our experiments across multiple tasks, including biology, medicine, and commonsense question-answering, as well as summarization, demonstrate that models fine-tuned on CRAFT-generated datasets can match or outperform strong baselines, including instruction-tuned models and those trained on human-curated datasets. Notably, CRAFT-based models showed better generalization capabilities on out-of-domain datasets compared to models trained on human-curated data, highlighting the robustness of our approach.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">While CRAFT shows promising results for most tasks, we also identified limitations in scaling performance for recipe generation, emphasizing the need for careful quality control and potential stopping criteria in future iterations. Nevertheless, the overall success of CRAFT in producing high-quality synthetic datasets with minimal human effort opens up new possibilities for efficient and adaptable model fine-tuning across a wide range of domains and tasks.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">IZ and DE have been supported by the European Union’s Horizon 2020 research and innovation program under grant agreement No. 101135671 (TrustLLM).
AK and HS have been funded by Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) project SCHU 2246/14-1.
IZ was also supported by a G-Research Travel Grant to present this work at the ELLIS Doctoral Symposium 2024 in Paris.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abadi et al. (2016)</span>
<span class="ltx_bibblock">
Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1603.04467" title="">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agarwal et al. (2007)</span>
<span class="ltx_bibblock">
Sumeet Agarwal, Shantanu Godbole, Diwakar Punjani, and Shourya Roy. 2007.

</span>
<span class="ltx_bibblock">How much noise is too much: A study in automatic text classification.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Seventh IEEE International Conference on Data Mining (ICDM 2007)</em>, pages 3–12. IEEE.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aghajanyan et al. (2021)</span>
<span class="ltx_bibblock">
Armen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer. 2021.

</span>
<span class="ltx_bibblock">Intrinsic dimensionality explains the effectiveness of language model fine-tuning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, pages 7319–7328.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alberts (2017)</span>
<span class="ltx_bibblock">
Bruce Alberts. 2017.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Molecular biology of the cell</em>, 5th edition.

</span>
<span class="ltx_bibblock">Garland science.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Askell et al. (2021)</span>
<span class="ltx_bibblock">
Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. 2021.

</span>
<span class="ltx_bibblock">A general language assistant as a laboratory for alignment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:2112.00861</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Attardi (2015)</span>
<span class="ltx_bibblock">
Giusepppe Attardi. 2015.

</span>
<span class="ltx_bibblock">Wikiextractor.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/attardi/wikiextractor" title="">https://github.com/attardi/wikiextractor</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Banerjee and Lavie (2005)</span>
<span class="ltx_bibblock">
Satanjeev Banerjee and Alon Lavie. 2005.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/W05-0909" title="">METEOR: An automatic metric for MT evaluation with improved correlation with human judgments</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization</em>, pages 65–72, Ann Arbor, Michigan. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barbella et al. (2021)</span>
<span class="ltx_bibblock">
Marcello Barbella, Michele Risi, and Genoveffa Tortora. 2021.

</span>
<span class="ltx_bibblock">A comparison of methods for the evaluation of text summarization techniques.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">DATA</em>, pages 200–207.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bień et al. (2020)</span>
<span class="ltx_bibblock">
Michał Bień, Michał Gilski, Martyna Maciejewska, Wojciech Taisner, Dawid Wisniewski, and Agnieszka Lawrynowicz. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.inlg-1.4" title="">RecipeNLG: A cooking recipes dataset for semi-structured text generation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 13th International Conference on Natural Language Generation</em>, pages 22–28, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Advances in neural information processing systems</em>, 33:1877–1901.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Celikyilmaz et al. (2020)</span>
<span class="ltx_bibblock">
Asli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao. 2020.

</span>
<span class="ltx_bibblock">Evaluation of text generation: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2006.14799</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chang et al. (2021)</span>
<span class="ltx_bibblock">
Ernie Chang, Xiaoyu Shen, Alex Marin, and Vera Demberg. 2021.

</span>
<span class="ltx_bibblock">The selectgen challenge: Finding the best training samples for few-shot neural text generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the 14th International Conference on Natural Language Generation</em>, pages 325–330.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024a)</span>
<span class="ltx_bibblock">
Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et al. 2024a.

</span>
<span class="ltx_bibblock">Alpagasus: Training a better alpaca with fewer data.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">The Twelfth International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024b)</span>
<span class="ltx_bibblock">
Yongrui Chen, Haiyun Jiang, Xinting Huang, Shuming Shi, and Guilin Qi. 2024b.

</span>
<span class="ltx_bibblock">Dog-instruct: Towards premium instruction-tuning data via text-grounded instruction wrapping.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)</em>, pages 4125–4135.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2022)</span>
<span class="ltx_bibblock">
Zhikui Chen, Tiandong Ji, Suhua Zhang, and Fangming Zhong. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/ICASSP43922.2022.9746127" title="">Noise suppression for improved few-shot learning</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 1900–1904.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang et al. (2024)</span>
<span class="ltx_bibblock">
Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E Gonzalez, et al. 2024.

</span>
<span class="ltx_bibblock">Chatbot arena: An open platform for evaluating llms by human preference.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:2403.04132</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al. (2022)</span>
<span class="ltx_bibblock">
Zhuyun Dai, Arun Tejasvi Chaganty, Vincent Y Zhao, Aida Amini, Qazi Mamunur Rashid, Mike Green, and Kelvin Guu. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.mlr.press/v162/dai22a.html" title="">Dialog inpainting: Turning documents into dialogs</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the 39th International Conference on Machine Learning</em>, volume 162 of <em class="ltx_emph ltx_font_italic" id="bib.bib17.2.2">Proceedings of Machine Learning Research</em>, pages 4558–4586. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dettmers et al. (2024)</span>
<span class="ltx_bibblock">
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2024.

</span>
<span class="ltx_bibblock">Qlora: Efficient finetuning of quantized llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Advances in Neural Information Processing Systems</em>, 36.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deutsch (1996)</span>
<span class="ltx_bibblock">
Peter Deutsch. 1996.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://www.rfc-editor.org/rfc/rfc1952.html" title="">Gzip file format specification version 4.3</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al. (2024)</span>
<span class="ltx_bibblock">
Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Baobao Chang, Xu Sun, Lei Li, and Zhifang Sui. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2301.00234" title="">A survey on in-context learning</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubey et al. (2024)</span>
<span class="ltx_bibblock">
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,
Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier
Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu,
Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl
Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzmán, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli,
Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha
Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin Cho, Sunny
Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2407.21783" title="">The llama 3 herd of models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubois et al. (2023)</span>
<span class="ltx_bibblock">
Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2305.14387" title="">Alpacafarm: A simulation framework for methods that learn from human feedback</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ein-Dor et al. (2020)</span>
<span class="ltx_bibblock">
Liat Ein-Dor, Eyal Shnarch, Lena Dankin, Alon Halfon, Benjamin Sznajder, Ariel Gera, Carlos Alzate, Martin Gleize, Leshem Choshen, Yufang Hou, et al. 2020.

</span>
<span class="ltx_bibblock">Corpus wide argument mining—a working solution.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, volume 34, pages 7683–7691.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eldan and Li (2023)</span>
<span class="ltx_bibblock">
Ronen Eldan and Yuanzhi Li. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2305.07759" title="">Tinystories: How small can language models be and still speak coherent english?</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al. (2018)</span>
<span class="ltx_bibblock">
Angela Fan, Mike Lewis, and Yann Dauphin. 2018.

</span>
<span class="ltx_bibblock">Hierarchical neural story generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 889–898.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Flax Sentence Embeddings Team (2021)</span>
<span class="ltx_bibblock">
Flax Sentence Embeddings Team. 2021.

</span>
<span class="ltx_bibblock">Stack exchange question pairs.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/flax-sentence-embeddings/" title="">https://huggingface.co/datasets/flax-sentence-embeddings/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gandhi et al. (2024)</span>
<span class="ltx_bibblock">
Saumya Gandhi, Ritu Gala, Vijay Viswanathan, Tongshuang Wu, and Graham Neubig. 2024.

</span>
<span class="ltx_bibblock">Better synthetic data by retrieving and transforming existing datasets.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2404.14361</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graham (2015)</span>
<span class="ltx_bibblock">
Yvette Graham. 2015.

</span>
<span class="ltx_bibblock">Re-evaluating automatic summarization with bleu and 192 shades of rouge.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the 2015 conference on empirical methods in natural language processing</em>, pages 128–137.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al. (2023)</span>
<span class="ltx_bibblock">
Jindong Gu, Zhen Han, Shuo Chen, Ahmad Beirami, Bailan He, Gengyuan Zhang, Ruotong Liao, Yao Qin, Volker Tresp, and Philip Torr. 2023.

</span>
<span class="ltx_bibblock">A systematic survey of prompt engineering on vision-language foundation models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:2307.12980</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gunasekar et al. (2023)</span>
<span class="ltx_bibblock">
Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2306.11644" title="">Textbooks are all you need</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hackl et al. (2023)</span>
<span class="ltx_bibblock">
Veronika Hackl, Alexandra Elena Müller, Michael Granitzer, and Maximilian Sailer. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3389/feduc.2023.1272229" title="">Is gpt-4 a reliable rater? evaluating consistency in gpt-4’s text ratings</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Frontiers in Education</em>, 8.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Harris et al. (2020)</span>
<span class="ltx_bibblock">
Charles R. Harris, K. Jarrod Millman, Stéfan J. van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fernández del Río, Mark Wiebe, Pearu Peterson, Pierre Gérard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1038/s41586-020-2649-2" title="">Array programming with NumPy</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Nature</em>, 585(7825):357–362.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et al. (2021)</span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021.

</span>
<span class="ltx_bibblock">Measuring massive multitask language understanding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Holtzman et al. (2020)</span>
<span class="ltx_bibblock">
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020.

</span>
<span class="ltx_bibblock">The curious case of neural text degeneration.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Honovich et al. (2023)</span>
<span class="ltx_bibblock">
Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.acl-long.806" title="">Unnatural instructions: Tuning language models with (almost) no human labor</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 14409–14428, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Howard and Ruder (2018)</span>
<span class="ltx_bibblock">
Jeremy Howard and Sebastian Ruder. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P18-1031" title="">Universal language model fine-tuning for text classification</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 328–339, Melbourne, Australia. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2021)</span>
<span class="ltx_bibblock">
Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. 2021.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2023)</span>
<span class="ltx_bibblock">
Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2310.06825" title="">Mistral 7b</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">arXiv</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koupaee and Wang (2018)</span>
<span class="ltx_bibblock">
Mahnaz Koupaee and William Yang Wang. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1810.09305" title="">Wikihow: A large scale text summarization dataset</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwon et al. (2023)</span>
<span class="ltx_bibblock">
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023.

</span>
<span class="ltx_bibblock">Efficient memory management for large language model serving with pagedattention.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Köksal et al. (2023)</span>
<span class="ltx_bibblock">
Abdullatif Köksal, Timo Schick, Anna Korhonen, and Hinrich Schütze. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2304.08460" title="">Longform: Optimizing instruction tuning for long text generation with corpus extraction</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Larochelle et al. (2008)</span>
<span class="ltx_bibblock">
Hugo Larochelle, Dumitru Erhan, and Yoshua Bengio. 2008.

</span>
<span class="ltx_bibblock">Zero-data learning of new tasks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">AAAI</em>, volume 1, pages 646–651.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lei et al. (2023)</span>
<span class="ltx_bibblock">
Deren Lei, Yaxi Li, Mingyu Wang, Vincent Yun, Emily Ching, Eslam Kamal, et al. 2023.

</span>
<span class="ltx_bibblock">Chain of natural language inference for reducing large language model ungrounded hallucinations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">arXiv preprint arXiv:2310.03951</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Leike et al. (2018)</span>
<span class="ltx_bibblock">
Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. 2018.

</span>
<span class="ltx_bibblock">Scalable agent alignment via reward modeling: a research direction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">arXiv preprint arXiv:1811.07871</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2021)</span>
<span class="ltx_bibblock">
Patrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale Minervini, Heinrich Küttler, Aleksandra Piktus, Pontus Stenetorp, and Sebastian Riedel. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00415" title="">PAQ: 65 million probably-asked questions and what you can do with them</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Transactions of the Association for Computational Linguistics</em>, 9:1098–1115.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2018)</span>
<span class="ltx_bibblock">
Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. 2018.

</span>
<span class="ltx_bibblock">Measuring the intrinsic dimension of objective landscapes.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023)</span>
<span class="ltx_bibblock">
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023.

</span>
<span class="ltx_bibblock">Alpacaeval: An automatic evaluator of instruction-following models.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/tatsu-lab/alpaca_eval" title="">https://github.com/tatsu-lab/alpaca_eval</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin (2004)</span>
<span class="ltx_bibblock">
Chin-Yew Lin. 2004.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://www.aclweb.org/anthology/W04-1013" title="">ROUGE: A package for automatic evaluation of summaries</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Text Summarization Branches Out</em>, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Little et al. (1993)</span>
<span class="ltx_bibblock">
Roderick JA Little et al. 1993.

</span>
<span class="ltx_bibblock">Statistical analysis of masked data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">JOURNAL OF OFFICIAL STATISTICS-STOCKHOLM-</em>, 9:407–407.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2022)</span>
<span class="ltx_bibblock">
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper_files/paper/2022/file/0cde695b83bd186c1fd456302888454c-Paper-Conference.pdf" title="">Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Advances in Neural Information Processing Systems</em>, volume 35, pages 1950–1965. Curran Associates, Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024)</span>
<span class="ltx_bibblock">
Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024.

</span>
<span class="ltx_bibblock">Lost in the middle: How language models use long contexts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">Transactions of the Association for Computational Linguistics</em>, 11:157–173.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023.

</span>
<span class="ltx_bibblock">G-eval: Nlg evaluation using gpt-4 with better human alignment.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pages 2511–2522.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov and Hutter (2019)</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=Bkg6RiCqY7" title="">Decoupled weight decay regularization</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019</em>. OpenReview.net.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2022)</span>
<span class="ltx_bibblock">
Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022.

</span>
<span class="ltx_bibblock">Learn to explain: Multimodal reasoning via thought chains for science question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">The 36th Conference on Neural Information Processing Systems (NeurIPS)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maas et al. (2011)</span>
<span class="ltx_bibblock">
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/P11-1015" title="">Learning word vectors for sentiment analysis</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</em>, pages 142–150, Portland, Oregon, USA. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maini et al. (2024)</span>
<span class="ltx_bibblock">
Pratyush Maini, Skyler Seto, He Bai, David Grangier, Yizhe Zhang, and Navdeep Jaitly. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2401.16380" title="">Rephrasing the web: A recipe for compute and data-efficient language modeling</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">Data Problems for Foundation Models Workshop at ICLR</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malmquist and Prescott (2022)</span>
<span class="ltx_bibblock">
Sarah Malmquist and Kristina Prescott. 2022.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">Human Biology</em>, 2nd edition.

</span>
<span class="ltx_bibblock">Pressbooks.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maqsud (2015)</span>
<span class="ltx_bibblock">
Umar Maqsud. 2015.

</span>
<span class="ltx_bibblock">Synthetic text generation for sentiment analysis.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Proceedings of the 6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</em>, pages 156–161.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marcus et al. (1993)</span>
<span class="ltx_bibblock">
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/J93-2004" title="">Building a large annotated corpus of English: The Penn Treebank</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Computational Linguistics</em>, 19(2):313–330.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mitra et al. (2023)</span>
<span class="ltx_bibblock">
Arindam Mitra, Luciano Del Corro, Shweti Mahajan, Andres Codas, Clarisse Simoes, Sahaj Agarwal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, Hamid Palangi, Guoqing Zheng, Corby Rosset, Hamed Khanpour, and Ahmed Awadallah. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2311.11045" title="">Orca 2: Teaching small language models how to reason</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mitra et al. (2024)</span>
<span class="ltx_bibblock">
Arindam Mitra, Luciano Del Corro, Guoqing Zheng, Shweti Mahajan, Dany Rouhana, Andres Codas, Yadong Lu, Wei-ge Chen, Olga Vrousgos, Corby Rosset, et al. 2024.

</span>
<span class="ltx_bibblock">Agentinstruct: Toward generative teaching with agentic flows.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">arXiv preprint arXiv:2407.03502</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mukherjee et al. (2023)</span>
<span class="ltx_bibblock">
Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2306.02707" title="">Orca: Progressive learning from complex explanation traces of gpt-4</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nayak et al. (2024)</span>
<span class="ltx_bibblock">
Nihal V Nayak, Yiyang Nan, Avi Trost, and Stephen H Bach. 2024.

</span>
<span class="ltx_bibblock">Learning to generate instruction tuning datasets for zero-shot task adaptation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">arXiv preprint arXiv:2402.18334</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2303.08774" title="">Gpt-4 technical report</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al. (2022)</span>
<span class="ltx_bibblock">
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf" title="">Training language models to follow instructions with human feedback</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">Advances in Neural Information Processing Systems</em>, volume 35, pages 27730–27744. Curran Associates, Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pal et al. (2022)</span>
<span class="ltx_bibblock">
Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.mlr.press/v174/pal22a.html" title="">Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">Proceedings of the Conference on Health, Inference, and Learning</em>, volume 174 of <em class="ltx_emph ltx_font_italic" id="bib.bib66.2.2">Proceedings of Machine Learning Research</em>, pages 248–260. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paszke et al. (2019)</span>
<span class="ltx_bibblock">
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019.

</span>
<span class="ltx_bibblock">Pytorch: an imperative style, high-performance deep learning library.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">Proceedings of the 33rd International Conference on Neural Information Processing Systems</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Perez et al. (2021)</span>
<span class="ltx_bibblock">
Ethan Perez, Douwe Kiela, and Kyunghyun Cho. 2021.

</span>
<span class="ltx_bibblock">True few-shot learning with language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">Advances in neural information processing systems</em>, 34:11054–11070.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2019)</span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">OpenAI blog</em>, 1(8):9.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al. (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">The Journal of Machine Learning Research</em>, 21(1):5485–5551.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ranjith et al. (2022)</span>
<span class="ltx_bibblock">
V Ranjith, MK Dhananjaya, P Yamini Sahukar, M Akshara, and Partho Sharothi Biswas. 2022.

</span>
<span class="ltx_bibblock">A review of deduplicate and significance of using fuzzy logic.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">ICT Analysis and Applications</em>, pages 281–287.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers and Gurevych (2019)</span>
<span class="ltx_bibblock">
Nils Reimers and Iryna Gurevych. 2019.

</span>
<span class="ltx_bibblock">Sentence-bert: Sentence embeddings using siamese bert-networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, pages 3982–3992.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rye et al. (2016)</span>
<span class="ltx_bibblock">
Connie Rye, Robert Wise, Vladimir Jurukovski, Jean DeSaix, Jung Choi, and Yael Avissar. 2016.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">Behavioral Biology: Proximate and Ultimate Causes of Behavior</em>.

</span>
<span class="ltx_bibblock">OpenStax, Houston, Texas.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sai et al. (2019)</span>
<span class="ltx_bibblock">
Ananya B Sai, Mithun Das Gupta, Mitesh M Khapra, and Mukundhan Srinivasan. 2019.

</span>
<span class="ltx_bibblock">Re-evaluating adem: A deeper look at scoring dialogue responses.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, volume 33, pages 6220–6227.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanh et al. (2022)</span>
<span class="ltx_bibblock">
Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2022.

</span>
<span class="ltx_bibblock">Multitask prompted training enables zero-shot task generalization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">ICLR 2022-Tenth International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">See et al. (2017)</span>
<span class="ltx_bibblock">
Abigail See, Peter J. Liu, and Christopher D. Manning. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P17-1099" title="">Get to the point: Summarization with pointer-generator networks</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1073–1083, Vancouver, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. (2023)</span>
<span class="ltx_bibblock">
Yisheng Song, Ting Wang, Puyu Cai, Subrota K Mondal, and Jyoti Prakash Sahoo. 2023.

</span>
<span class="ltx_bibblock">A comprehensive survey of few-shot learning: Evolution, applications, challenges, and opportunities.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">ACM Computing Surveys</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sottana et al. (2023)</span>
<span class="ltx_bibblock">
Andrea Sottana, Bin Liang, Kai Zou, and Zheng Yuan. 2023.

</span>
<span class="ltx_bibblock">Evaluation metrics in the era of gpt-4: Reliably evaluating large language models on sequence to sequence tasks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pages 8776–8788.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sue (1987)</span>
<span class="ltx_bibblock">
Leurgans Sue. 1987.

</span>
<span class="ltx_bibblock">Linear models, random censoring and synthetic data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib79.1.1">Biometrika</em>, 74(2):301–309.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sutskever et al. (2011)</span>
<span class="ltx_bibblock">
Ilya Sutskever, James Martens, and Geoffrey E Hinton. 2011.

</span>
<span class="ltx_bibblock">Generating text with recurrent neural networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">Proceedings of the 28th international conference on machine learning (ICML-11)</em>, pages 1017–1024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Talmor et al. (2021)</span>
<span class="ltx_bibblock">
Alon Talmor, Ori Yoran, Ronan Le Bras, Chandra Bhagavatula, Yoav Goldberg, Yejin Choi, and Jonathan Berant. 2021.

</span>
<span class="ltx_bibblock">Commonsenseqa 2.0: Exposing the limits of ai through gamification.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taori et al. (2023)</span>
<span class="ltx_bibblock">
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023.

</span>
<span class="ltx_bibblock">Stanford alpaca: An instruction-following llama model.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/tatsu-lab/stanford_alpaca" title="">https://github.com/tatsu-lab/stanford_alpaca</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">The HDF Group (2002)</span>
<span class="ltx_bibblock">
The HDF Group. 2002.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://github.com/HDFGroup/hdf5" title="">Hierarchical Data Format, version 5</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022)</span>
<span class="ltx_bibblock">
Hao Wang, Guosheng Lin, Steven CH Hoi, and Chunyan Miao. 2022.

</span>
<span class="ltx_bibblock">Learning structural representations for recipe generation and food retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 45(3):3363–3377.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2020)</span>
<span class="ltx_bibblock">
Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020.

</span>
<span class="ltx_bibblock">Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">Advances in Neural Information Processing Systems</em>, 33:5776–5788.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023a)</span>
<span class="ltx_bibblock">
Yanshan Wang, Shyam Visweswaran, Sumit Kappor, Shravan Kooragayalu, and Xizhi Wu. 2023a.

</span>
<span class="ltx_bibblock">Chatgpt, enhanced with clinical practice guidelines, is a superior decision support tool.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib86.1.1">medRxiv</em>, pages 2023–08.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023b)</span>
<span class="ltx_bibblock">
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023b.

</span>
<span class="ltx_bibblock">Self-instruct: Aligning language models with self-generated instructions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib87.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 13484–13508.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2022.

</span>
<span class="ltx_bibblock">Finetuned language models are zero-shot learners.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib88.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wilkin and Brainard (2016)</span>
<span class="ltx_bibblock">
Douglas Wilkin and Jean Brainard. 2016.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib89.1.1">Communication Behavior in Animals - Advanced</em>.

</span>
<span class="ltx_bibblock">CK-12.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolf et al. (2020)</span>
<span class="ltx_bibblock">
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. 2020.

</span>
<span class="ltx_bibblock">Transformers: State-of-the-art natural language processing.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib90.1.1">Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations</em>, pages 38–45.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2023)</span>
<span class="ltx_bibblock">
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023.

</span>
<span class="ltx_bibblock">Wizardlm: Empowering large language models to follow complex instructions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib91.1.1">arXiv preprint arXiv:2304.12244</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al. (2023)</span>
<span class="ltx_bibblock">
Da Yin, Xiao Liu, Fan Yin, Ming Zhong, Hritik Bansal, Jiawei Han, and Kai-Wei Chang. 2023.

</span>
<span class="ltx_bibblock">Dynosaur: A dynamic growth paradigm for instruction-tuning data curation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib92.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pages 4031–4047.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2020)</span>
<span class="ltx_bibblock">
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1904.09675" title="">Bertscore: Evaluating text generation with bert</a>.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Implementation Details</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Few-Shot Design</h3>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">The BioQA few-shot text samples were compiled from a diverse range of sources, including textbooks <cite class="ltx_cite ltx_citemacro_citep">(Alberts, <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib4" title="">2017</a>; Malmquist and Prescott, <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib57" title="">2022</a>; Wilkin and Brainard, <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib89" title="">2016</a>; Rye et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib73" title="">2016</a>)</cite>, openly accessible materials, and Encyclopedia Britannica.
For MedQA, we primarily drew upon openly accessible websites, such as the National Institutes of Health, National Health Service, Food and Drug Administration, Mayo and Cleveland Clinics, and other medicine-related websites, to generate our few-shot text samples.
The CSQA few-shot text samples were sourced from a variety of online resources, including blogs, articles, and other websites tailored to the specific topic at hand.
From sources that are not openly accessible through websites, continuous text snippets were directly extracted and used as texts, while all other material was shortened, rephrased, or restructured by the authors.
This process ensures that articles which may have been crawled through C4 <cite class="ltx_cite ltx_citemacro_citep">(Raffel et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib70" title="">2020</a>)</cite> do not produce exact matches at retrieval time.
Since none of these texts have direct question-answer pairs available, the question, answer options, and the answer were generated by the authors for each sample.
Table <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S3.F2" title="Figure 2 ‣ 3.1 Architecture Overview ‣ 3 The CRAFT Approach ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_tag">2</span></a>, step 1, shows an example.</p>
</div>
<div class="ltx_para" id="A1.SS1.p2">
<p class="ltx_p" id="A1.SS1.p2.1">The recipe generation few-shots were taken from blogs and other openly accessible websites.
Their design always includes a simple instruction or question to cook a meal, as well as bulleted or numbered lists of ingredients and steps to produce the meal.
Sometimes, the recipes on websites were described only as continuous text.
In such cases, the authors added the list of ingredients and steps to create the few-shots.
Similar to the QA few-shots, it is desirable to increase the vocabulary diversity for retrieval.
Therefore, we made sure to cover a wide range of recipes in the few-shots.
To design summarization few-shots, we collected a wide variety of texts from websites, blogs, and magazines, as well as specialized sources such as GitHub issues.
Here, the few-shots feature a text, an instruction to create a summary, as well as the summary output.
On some websites, summarized versions of the main text are given, while in other cases continuous or bulleted summaries were created by the authors.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Embedding Database</h3>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">To create the embeddings, SentenceTransformers <cite class="ltx_cite ltx_citemacro_citep">(Reimers and Gurevych, <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib72" title="">2019</a>)</cite> is used, specifically a MiniLM <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib85" title="">2020</a>)</cite> version (<span class="ltx_text ltx_font_typewriter" id="A1.SS2.p1.1.1">multi-qa-MiniLM-L6-cos-v1</span>) optimized for cosine-similarity search between two document pairs.
This model creates 384D embeddings,
which are stored in an HDF5 database <cite class="ltx_cite ltx_citemacro_citep">(The HDF Group, <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib83" title="">2002</a>)</cite>, that allows for native storage and retrieval of array-like data.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Corpora</h3>
<div class="ltx_para" id="A1.SS3.p1">
<p class="ltx_p" id="A1.SS3.p1.1">To enable retrieving human-written documents for general-purpose as well as specialized domains, we include four large corpora.</p>
</div>
<div class="ltx_para" id="A1.SS3.p2">
<p class="ltx_p" id="A1.SS3.p2.1">The Colossal Clean Crawled Corpus <cite class="ltx_cite ltx_citemacro_citep">(Raffel et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib70" title="">2020</a>, C4)</cite> dataset consists of approximately 750GB of English data, pre-cleaned by the creators to exclude non-informative texts.
We use a 305GB subset of the dataset, which excludes not-safe-for-work or offensive content.</p>
</div>
<div class="ltx_para" id="A1.SS3.p3">
<p class="ltx_p" id="A1.SS3.p3.1">The English Wikipedia corpus comprises a diverse and high-quality collection of textual information on a wide range of topics.
We use the January 2nd, 2024 dump of English Wikipedia, which we pre-process using WikiExtractor <cite class="ltx_cite ltx_citemacro_citep">(Attardi, <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib6" title="">2015</a>)</cite> to extract clean text documents.</p>
</div>
<div class="ltx_para" id="A1.SS3.p4">
<p class="ltx_p" id="A1.SS3.p4.1">The Stack Exchange corpus <cite class="ltx_cite ltx_citemacro_citep">(Flax Sentence Embeddings Team, <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib26" title="">2021</a>)</cite> features a structured format with title, body, and best-voted answer collected from the Stack Exchange network.
The dataset encompasses 173 distinct sub-communities, covering both technical and non-technical topics.</p>
</div>
<div class="ltx_para" id="A1.SS3.p5">
<p class="ltx_p" id="A1.SS3.p5.1">The WikiHow corpus <cite class="ltx_cite ltx_citemacro_citep">(Koupaee and Wang, <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib39" title="">2018</a>)</cite> presents information in a step-by-step instructional layout, making it a valuable resource for tasks such as summarization or recipe generation.
Each document consists of a title, clear instructions, and accompanying text.</p>
</div>
<div class="ltx_para" id="A1.SS3.p6">
<p class="ltx_p" id="A1.SS3.p6.1">After filtering out documents with fewer than 200 characters or more than 25,000 characters, the resulting datasets contain approximately 362 million documents from C4, 10.5 million documents from English Wikipedia, 9.5 million documents from Stack Exchange, and 190,000 documents from WikiHow.
The resulting 383 million documents take up approximately 247GB of storage when GZIP compressed <cite class="ltx_cite ltx_citemacro_citep">(Deutsch, <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib19" title="">1996</a>)</cite> and represented as 16-bit NumPy arrays <cite class="ltx_cite ltx_citemacro_citep">(Harris et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib32" title="">2020</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>Document Retrieval</h3>
<div class="ltx_para" id="A1.SS4.p1">
<p class="ltx_p" id="A1.SS4.p1.1">To retrieve relevant human-written samples, we employ a two-step process that approximates a global similarity search.
Due to the large size of the embedding database (383 million documents), a direct global pair-wise comparison is impractical.</p>
</div>
<div class="ltx_para" id="A1.SS4.p2">
<p class="ltx_p" id="A1.SS4.p2.1">In the first step, we divide the embedding database sequentially into subsections of approximately 350,000 documents each.
We calculate cosine similarity between each subsection and the few-shot samples, and store the top 5% most similar documents in memory.
This reduces the number of documents considered from 383 million to approximately 19 million.
In the second step, we recalculate cosine similarity and perform traditional top-k similarity retrieval on the filtered documents.
This yields the indices of the top-k most similar documents to the few-shot examples.</p>
</div>
<div class="ltx_para" id="A1.SS4.p3">
<p class="ltx_p" id="A1.SS4.p3.1">Optimizing solely for top-k similarity between individual few-shot samples and the embedding database may lead to limited variation and high similarity among the retrieved documents.
Conversely, optimizing for similarity by averaging the embedding representations of all few-shot samples may prioritize a single dominant topic, potentially biasing the selection.
To balance these approaches, we adopt a mixed similarity retrieval strategy, combining two methods:
(i) 50% of samples are retrieved based on individual top-k similarity to each few-shot sample, ensuring a minimum number of similar samples for each few-shot and mitigating topic dominance, and
(ii) the remaining 50% of samples are retrieved based on fully averaged similarity, which aims to help uncover more latent topics.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.5 </span>Task Sample Synthesis</h3>
<div class="ltx_para" id="A1.SS5.p1">
<p class="ltx_p" id="A1.SS5.p1.1">To facilitate the creation of high-quality synthetic task samples, we align the text generation process to the few-shot design through in-context learning <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib10" title="">2020</a>)</cite>.
This approach helps reduce issues like hallucinations <cite class="ltx_cite ltx_citemacro_citep">(Lei et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib43" title="">2023</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib86" title="">2023a</a>)</cite> and formatting errors, although it does not entirely prevent them.
Specifically, three few-shot examples from the human-curated few-shots are dynamically and randomly sampled for each input prompt and corpus sample, interleaving them with the instruction to augment the text.
Since each few-shot example includes a text similar in length to corpus samples, the three-shot prompting technique can result in input prompts that often exceed 10,000 tokens, sometimes reaching 20,000 tokens.
For optimal performance, it is recommended to use models offering long context lengths.</p>
</div>
<div class="ltx_para" id="A1.SS5.p2">
<p class="ltx_p" id="A1.SS5.p2.1">In our experiments, all task samples are created using Mistral 7B Instruct v0.2 <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib38" title="">2023</a>)</cite> along with the few-shot task template shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#S3.F2" title="Figure 2 ‣ 3.1 Architecture Overview ‣ 3 The CRAFT Approach ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_tag">2</span></a>, step 3.
During the generation process, vLLM <cite class="ltx_cite ltx_citemacro_citep">(Kwon et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib40" title="">2023</a>)</cite> is used with temperature 0.7, top-k <cite class="ltx_cite ltx_citemacro_citep">(Fan et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib25" title="">2018</a>)</cite> and top-p <cite class="ltx_cite ltx_citemacro_citep">(Holtzman et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib34" title="">2020</a>)</cite> sampling at 40 and 0.9, respectively.
The maximum generation length was determined based on preliminary experiments with the goal of accommodating full-length sample generation.
For all QA datasets we limited samples to 256 tokens, whereas recipe generation and summarization were limited at 1280 and 1536 tokens, respectively.</p>
</div>
<div class="ltx_para" id="A1.SS5.p3">
<p class="ltx_p" id="A1.SS5.p3.1">To enable quality control checks such as checking whether the QA datasets have the desired number of answer options, or whether the samples have been fully generated, we instruct the model to produce all task samples as valid JSON objects with a set number of keys and proper naming.
We perform checks on each JSON object to ensure that the questions, instructions, and generative task answers exceed a reasonable minimum length, as well as that the answer option generated for the QA tasks contain valid letters.
If no valid JSON is found, or keys are missing, or any formatting check fails, we discard the sample.
Furthermore, we filter task samples that are either too similar to the few-shots or to other generated task samples using fuzzy string matching <cite class="ltx_cite ltx_citemacro_citep">(Ranjith et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib71" title="">2022</a>)</cite> at a token set similarity ratio of 0.85.
To account for the removal of samples during deduplication, we recommend retrieving approximately twice as many corpus samples as the number of desired final synthetic task samples.
A detailed breakdown of the retrieval and deduplication quantities for each task is provided in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#A2" title="Appendix B Filtering Statistics ‣ CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.6 </span>Training and Optimization</h3>
<div class="ltx_para" id="A1.SS6.p1">
<p class="ltx_p" id="A1.SS6.p1.2">For all experiments, low-rank adaptation <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib37" title="">2021</a>, LoRA)</cite> fine-tuning is performed with 16-bit BrainFloat <cite class="ltx_cite ltx_citemacro_citep">(Abadi et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib1" title="">2016</a>)</cite> as the computation data type.
All experiments are implemented using PyTorch <cite class="ltx_cite ltx_citemacro_citep">(Paszke et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib67" title="">2019</a>)</cite> and the Hugging Face packages <cite class="ltx_cite ltx_citemacro_citep">(Wolf et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib90" title="">2020</a>)</cite>.
For optimization, the adaptive momentum optimizer with decoupled weight decay <cite class="ltx_cite ltx_citemacro_citep">(Loshchilov and Hutter, <a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib53" title="">2019</a>)</cite> of 5% and a learning rate of <math alttext="1\times 10^{-4}" class="ltx_Math" display="inline" id="A1.SS6.p1.1.m1.1"><semantics id="A1.SS6.p1.1.m1.1a"><mrow id="A1.SS6.p1.1.m1.1.1" xref="A1.SS6.p1.1.m1.1.1.cmml"><mn id="A1.SS6.p1.1.m1.1.1.2" xref="A1.SS6.p1.1.m1.1.1.2.cmml">1</mn><mo id="A1.SS6.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.SS6.p1.1.m1.1.1.1.cmml">×</mo><msup id="A1.SS6.p1.1.m1.1.1.3" xref="A1.SS6.p1.1.m1.1.1.3.cmml"><mn id="A1.SS6.p1.1.m1.1.1.3.2" xref="A1.SS6.p1.1.m1.1.1.3.2.cmml">10</mn><mrow id="A1.SS6.p1.1.m1.1.1.3.3" xref="A1.SS6.p1.1.m1.1.1.3.3.cmml"><mo id="A1.SS6.p1.1.m1.1.1.3.3a" xref="A1.SS6.p1.1.m1.1.1.3.3.cmml">−</mo><mn id="A1.SS6.p1.1.m1.1.1.3.3.2" xref="A1.SS6.p1.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A1.SS6.p1.1.m1.1b"><apply id="A1.SS6.p1.1.m1.1.1.cmml" xref="A1.SS6.p1.1.m1.1.1"><times id="A1.SS6.p1.1.m1.1.1.1.cmml" xref="A1.SS6.p1.1.m1.1.1.1"></times><cn id="A1.SS6.p1.1.m1.1.1.2.cmml" type="integer" xref="A1.SS6.p1.1.m1.1.1.2">1</cn><apply id="A1.SS6.p1.1.m1.1.1.3.cmml" xref="A1.SS6.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="A1.SS6.p1.1.m1.1.1.3.1.cmml" xref="A1.SS6.p1.1.m1.1.1.3">superscript</csymbol><cn id="A1.SS6.p1.1.m1.1.1.3.2.cmml" type="integer" xref="A1.SS6.p1.1.m1.1.1.3.2">10</cn><apply id="A1.SS6.p1.1.m1.1.1.3.3.cmml" xref="A1.SS6.p1.1.m1.1.1.3.3"><minus id="A1.SS6.p1.1.m1.1.1.3.3.1.cmml" xref="A1.SS6.p1.1.m1.1.1.3.3"></minus><cn id="A1.SS6.p1.1.m1.1.1.3.3.2.cmml" type="integer" xref="A1.SS6.p1.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS6.p1.1.m1.1c">1\times 10^{-4}</annotation><annotation encoding="application/x-llamapun" id="A1.SS6.p1.1.m1.1d">1 × 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math> is employed.
A linear learning rate schedule is applied with a warm-up ratio of 10%.
Fine-tuning is performed for 3 epochs across all tasks and dataset size variations.
When fine-tuning on only the human-curated few-shots, a batch size of 2 is adopted.
In all other scenarios, fine-tuning is performed with a batch size of 16 or the equivalent with gradient accumulation steps.
Following <cite class="ltx_cite ltx_citemacro_citet">Dettmers et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.02098v1#bib.bib18" title="">2024</a>)</cite>, LoRA adapters are used on every linear layer, specifically on the query<span class="ltx_text" id="A1.SS6.p1.2.1">-</span>, key<span class="ltx_text" id="A1.SS6.p1.2.2">-</span>, value<span class="ltx_text" id="A1.SS6.p1.2.3">-</span>, output<span class="ltx_text" id="A1.SS6.p1.2.4">-</span>, gate<span class="ltx_text" id="A1.SS6.p1.2.5">-</span>, up<span class="ltx_text" id="A1.SS6.p1.2.6">-</span>, and down-projection matrices.
We use a rank and <math alttext="\alpha" class="ltx_Math" display="inline" id="A1.SS6.p1.2.m2.1"><semantics id="A1.SS6.p1.2.m2.1a"><mi id="A1.SS6.p1.2.m2.1.1" xref="A1.SS6.p1.2.m2.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="A1.SS6.p1.2.m2.1b"><ci id="A1.SS6.p1.2.m2.1.1.cmml" xref="A1.SS6.p1.2.m2.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS6.p1.2.m2.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="A1.SS6.p1.2.m2.1d">italic_α</annotation></semantics></math> parameter of 64, the adapter’s dropout rate is set to 0.1, and the bias terms of the update matrices are deactivated.
In total, this adds 2.3%, or about 160 million parameters, of the model’s 7 billion base parameters as LoRA adapters.
The rest of the model remains frozen during fine-tuning, and the updated parameters will be merged with the base model after fine-tuning.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Filtering Statistics</h2>
<figure class="ltx_table" id="A2.T3">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A2.T3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T3.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="A2.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A2.T3.1.1.1.1.1">Filter Criteria</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A2.T3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A2.T3.1.1.1.2.1">S</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A2.T3.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A2.T3.1.1.1.3.1">M</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A2.T3.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A2.T3.1.1.1.4.1">L</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A2.T3.1.1.1.5"><span class="ltx_text ltx_font_bold" id="A2.T3.1.1.1.5.1">XL</span></td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="A2.T3.1.2.2.1">Retrieved Corpus Samples</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T3.1.2.2.2">240</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T3.1.2.2.3">1,200</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T3.1.2.2.4">12,000</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T3.1.2.2.5">60,000</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="A2.T3.1.3.3.1">Exact duplicates</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T3.1.3.3.2">25</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T3.1.3.3.3">37</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T3.1.3.3.4">819</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T3.1.3.3.5">8,551</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.4.4">
<td class="ltx_td ltx_align_left" id="A2.T3.1.4.4.1">Excessive length</td>
<td class="ltx_td ltx_align_center" id="A2.T3.1.4.4.2">2</td>
<td class="ltx_td ltx_align_center" id="A2.T3.1.4.4.3">14</td>
<td class="ltx_td ltx_align_center" id="A2.T3.1.4.4.4">266</td>
<td class="ltx_td ltx_align_center" id="A2.T3.1.4.4.5">1,632</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.5.5">
<td class="ltx_td ltx_align_left" id="A2.T3.1.5.5.1">Format errors</td>
<td class="ltx_td ltx_align_center" id="A2.T3.1.5.5.2">10</td>
<td class="ltx_td ltx_align_center" id="A2.T3.1.5.5.3">40</td>
<td class="ltx_td ltx_align_center" id="A2.T3.1.5.5.4">466</td>
<td class="ltx_td ltx_align_center" id="A2.T3.1.5.5.5">2,174</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.6.6">
<td class="ltx_td ltx_align_left" id="A2.T3.1.6.6.1">Similarity to few-shots</td>
<td class="ltx_td ltx_align_center" id="A2.T3.1.6.6.2">0</td>
<td class="ltx_td ltx_align_center" id="A2.T3.1.6.6.3">1</td>
<td class="ltx_td ltx_align_center" id="A2.T3.1.6.6.4">22</td>
<td class="ltx_td ltx_align_center" id="A2.T3.1.6.6.5">45</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.7.7">
<td class="ltx_td ltx_align_left" id="A2.T3.1.7.7.1">Similarity to other task samples</td>
<td class="ltx_td ltx_align_center" id="A2.T3.1.7.7.2">9</td>
<td class="ltx_td ltx_align_center" id="A2.T3.1.7.7.3">117</td>
<td class="ltx_td ltx_align_center" id="A2.T3.1.7.7.4">1,469</td>
<td class="ltx_td ltx_align_center" id="A2.T3.1.7.7.5">5,961</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.8.8">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="A2.T3.1.8.8.1">Available synthetic task samples</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A2.T3.1.8.8.2">194</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A2.T3.1.8.8.3">991</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A2.T3.1.8.8.4">8,958</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A2.T3.1.8.8.5">41,637</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>BioQA corpus and task sample filtering process.
Corpus samples are turned into task samples after filtering for excessive length.
</figcaption>
</figure>
<figure class="ltx_table" id="A2.T4">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A2.T4.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T4.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="A2.T4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A2.T4.1.1.1.1.1">Filter Criteria</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A2.T4.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A2.T4.1.1.1.2.1">S</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A2.T4.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A2.T4.1.1.1.3.1">M</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A2.T4.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A2.T4.1.1.1.4.1">L</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A2.T4.1.1.1.5"><span class="ltx_text ltx_font_bold" id="A2.T4.1.1.1.5.1">XL</span></td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="A2.T4.1.2.2.1">Retrieved Corpus Samples</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T4.1.2.2.2">240</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T4.1.2.2.3">1,200</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T4.1.2.2.4">12,000</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T4.1.2.2.5">60,000</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="A2.T4.1.3.3.1">Exact duplicates</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T4.1.3.3.2">24</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T4.1.3.3.3">30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T4.1.3.3.4">165</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T4.1.3.3.5">1,348</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.4.4">
<td class="ltx_td ltx_align_left" id="A2.T4.1.4.4.1">Excessive length</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.4.4.2">2</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.4.4.3">8</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.4.4.4">64</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.4.4.5">307</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.5.5">
<td class="ltx_td ltx_align_left" id="A2.T4.1.5.5.1">Format errors</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.5.5.2">5</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.5.5.3">30</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.5.5.4">364</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.5.5.5">1,879</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.6.6">
<td class="ltx_td ltx_align_left" id="A2.T4.1.6.6.1">Similarity to few-shots</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.6.6.2">11</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.6.6.3">19</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.6.6.4">141</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.6.6.5">410</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.7.7">
<td class="ltx_td ltx_align_left" id="A2.T4.1.7.7.1">Similarity to other task samples</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.7.7.2">14</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.7.7.3">129</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.7.7.4">2,655</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.7.7.5">17,749</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.8.8">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="A2.T4.1.8.8.1">Available synthetic task samples</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A2.T4.1.8.8.2">184</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A2.T4.1.8.8.3">984</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A2.T4.1.8.8.4">8,611</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A2.T4.1.8.8.5">38,307</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>CSQA corpus and task sample filtering process.
Corpus samples are turned into task samples after filtering for excessive length.
</figcaption>
</figure>
<figure class="ltx_table" id="A2.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A2.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A2.T5.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A2.T5.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A2.T5.1.1.1.1.1">Filter Criteria</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A2.T5.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A2.T5.1.1.1.2.1">S</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A2.T5.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A2.T5.1.1.1.3.1">M</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A2.T5.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A2.T5.1.1.1.4.1">L</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A2.T5.1.1.1.5"><span class="ltx_text ltx_font_bold" id="A2.T5.1.1.1.5.1">XL</span></th>
</tr>
<tr class="ltx_tr" id="A2.T5.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="A2.T5.1.2.2.1">Retrieved Corpus Samples</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T5.1.2.2.2">240</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T5.1.2.2.3">1,200</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T5.1.2.2.4">12,000</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T5.1.2.2.5">60,000</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T5.1.3.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="A2.T5.1.3.1.1">Exact duplicates</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.3.1.2">24</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.3.1.3">24</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.3.1.4">50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.3.1.5">773</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.4.2">
<td class="ltx_td ltx_align_left" id="A2.T5.1.4.2.1">Excessive length</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.4.2.2">1</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.4.2.3">10</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.4.2.4">141</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.4.2.5">890</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.5.3">
<td class="ltx_td ltx_align_left" id="A2.T5.1.5.3.1">Format errors</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.5.3.2">15</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.5.3.3">36</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.5.3.4">540</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.5.3.5">2,911</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.6.4">
<td class="ltx_td ltx_align_left" id="A2.T5.1.6.4.1">Similarity to few-shots</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.6.4.2">0</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.6.4.3">10</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.6.4.4">55</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.6.4.5">204</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.7.5">
<td class="ltx_td ltx_align_left" id="A2.T5.1.7.5.1">Similarity to other task samples</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.7.5.2">3</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.7.5.3">40</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.7.5.4">813</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.7.5.5">5,221</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.8.6">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="A2.T5.1.8.6.1">Available synthetic task samples</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A2.T5.1.8.6.2">197</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A2.T5.1.8.6.3">1,080</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A2.T5.1.8.6.4">10,401</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A2.T5.1.8.6.5">50,001</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>MedQA corpus and task sample filtering process.
Corpus samples are turned into task samples after filtering for excessive length.
</figcaption>
</figure>
<figure class="ltx_table" id="A2.T6">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A2.T6.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A2.T6.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A2.T6.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A2.T6.1.1.1.1.1">Filter Criteria</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A2.T6.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A2.T6.1.1.1.2.1">S</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A2.T6.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A2.T6.1.1.1.3.1">M</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A2.T6.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A2.T6.1.1.1.4.1">L</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A2.T6.1.1.1.5"><span class="ltx_text ltx_font_bold" id="A2.T6.1.1.1.5.1">XL</span></th>
</tr>
<tr class="ltx_tr" id="A2.T6.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="A2.T6.1.2.2.1">Retrieved Corpus Samples</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T6.1.2.2.2">240</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T6.1.2.2.3">1,200</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T6.1.2.2.4">12,000</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T6.1.2.2.5">60,000</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T6.1.3.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="A2.T6.1.3.1.1">Exact duplicates</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T6.1.3.1.2">24</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T6.1.3.1.3">24</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T6.1.3.1.4">28</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T6.1.3.1.5">620</td>
</tr>
<tr class="ltx_tr" id="A2.T6.1.4.2">
<td class="ltx_td ltx_align_left" id="A2.T6.1.4.2.1">Excessive length</td>
<td class="ltx_td ltx_align_center" id="A2.T6.1.4.2.2">1</td>
<td class="ltx_td ltx_align_center" id="A2.T6.1.4.2.3">1</td>
<td class="ltx_td ltx_align_center" id="A2.T6.1.4.2.4">20</td>
<td class="ltx_td ltx_align_center" id="A2.T6.1.4.2.5">54</td>
</tr>
<tr class="ltx_tr" id="A2.T6.1.5.3">
<td class="ltx_td ltx_align_left" id="A2.T6.1.5.3.1">Format errors</td>
<td class="ltx_td ltx_align_center" id="A2.T6.1.5.3.2">87</td>
<td class="ltx_td ltx_align_center" id="A2.T6.1.5.3.3">417</td>
<td class="ltx_td ltx_align_center" id="A2.T6.1.5.3.4">4,035</td>
<td class="ltx_td ltx_align_center" id="A2.T6.1.5.3.5">18,684</td>
</tr>
<tr class="ltx_tr" id="A2.T6.1.6.4">
<td class="ltx_td ltx_align_left" id="A2.T6.1.6.4.1">Similarity to few-shots</td>
<td class="ltx_td ltx_align_center" id="A2.T6.1.6.4.2">6</td>
<td class="ltx_td ltx_align_center" id="A2.T6.1.6.4.3">18</td>
<td class="ltx_td ltx_align_center" id="A2.T6.1.6.4.4">111</td>
<td class="ltx_td ltx_align_center" id="A2.T6.1.6.4.5">389</td>
</tr>
<tr class="ltx_tr" id="A2.T6.1.7.5">
<td class="ltx_td ltx_align_left" id="A2.T6.1.7.5.1">Similarity to other task samples</td>
<td class="ltx_td ltx_align_center" id="A2.T6.1.7.5.2">0</td>
<td class="ltx_td ltx_align_center" id="A2.T6.1.7.5.3">7</td>
<td class="ltx_td ltx_align_center" id="A2.T6.1.7.5.4">473</td>
<td class="ltx_td ltx_align_center" id="A2.T6.1.7.5.5">3,711</td>
</tr>
<tr class="ltx_tr" id="A2.T6.1.8.6">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="A2.T6.1.8.6.1">Available synthetic task samples</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A2.T6.1.8.6.2">122</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A2.T6.1.8.6.3">733</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A2.T6.1.8.6.4">7,333</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A2.T6.1.8.6.5">36,542</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>RecipeGen corpus and task sample filtering process.
Corpus samples are turned into task samples after filtering for excessive length.
</figcaption>
</figure>
<figure class="ltx_table" id="A2.T7">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A2.T7.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A2.T7.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A2.T7.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A2.T7.1.1.1.1.1">Filter Criteria</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A2.T7.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A2.T7.1.1.1.2.1">S</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A2.T7.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A2.T7.1.1.1.3.1">M</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A2.T7.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A2.T7.1.1.1.4.1">L</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A2.T7.1.1.1.5"><span class="ltx_text ltx_font_bold" id="A2.T7.1.1.1.5.1">XL</span></th>
</tr>
<tr class="ltx_tr" id="A2.T7.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="A2.T7.1.2.2.1">Retrieved Corpus Samples</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T7.1.2.2.2">240</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T7.1.2.2.3">1,200</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T7.1.2.2.4">12,000</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T7.1.2.2.5">60,000</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T7.1.3.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="A2.T7.1.3.1.1">Exact duplicates</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T7.1.3.1.2">24</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T7.1.3.1.3">24</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T7.1.3.1.4">19</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T7.1.3.1.5">101</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.4.2">
<td class="ltx_td ltx_align_left" id="A2.T7.1.4.2.1">Excessive length</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.4.2.2">34</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.4.2.3">189</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.4.2.4">1,793</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.4.2.5">8,964</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.5.3">
<td class="ltx_td ltx_align_left" id="A2.T7.1.5.3.1">Format errors</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.5.3.2">55</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.5.3.3">336</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.5.3.4">3,119</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.5.3.5">14,803</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.6.4">
<td class="ltx_td ltx_align_left" id="A2.T7.1.6.4.1">Similarity to few-shots</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.6.4.2">21</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.6.4.3">28</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.6.4.4">99</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.6.4.5">379</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.7.5">
<td class="ltx_td ltx_align_left" id="A2.T7.1.7.5.1">Similarity to other task samples</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.7.5.2">1</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.7.5.3">1</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.7.5.4">32</td>
<td class="ltx_td ltx_align_center" id="A2.T7.1.7.5.5">394</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.8.6">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="A2.T7.1.8.6.1">Available synthetic task samples</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A2.T7.1.8.6.2">105</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A2.T7.1.8.6.3">622</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A2.T7.1.8.6.4">6,938</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A2.T7.1.8.6.5">35,359</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Summarization corpus and task sample filtering process.
Corpus samples are turned into task samples after filtering for excessive length.
</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Dataset Statistics</h2>
<figure class="ltx_table" id="A3.T8">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A3.T8.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A3.T8.1.1.1">
<th class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A3.T8.1.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="A3.T8.1.1.1.1.1">Dataset</span></th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T8.1.1.1.2" rowspan="2"><span class="ltx_text ltx_font_bold" id="A3.T8.1.1.1.2.1">Size</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A3.T8.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A3.T8.1.1.1.3.1">Corpus Samples</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A3.T8.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A3.T8.1.1.1.4.1">TS Instruction</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A3.T8.1.1.1.5"><span class="ltx_text ltx_font_bold" id="A3.T8.1.1.1.5.1">TS Output</span></th>
</tr>
<tr class="ltx_tr" id="A3.T8.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T8.1.2.2.1">Mean</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T8.1.2.2.2">Median</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T8.1.2.2.3">Mean</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T8.1.2.2.4">Median</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T8.1.2.2.5">Mean</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T8.1.2.2.6">Median</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.T8.1.3.1">
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="A3.T8.1.3.1.1" rowspan="5"><span class="ltx_text" id="A3.T8.1.3.1.1.1">BioQA</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="A3.T8.1.3.1.2">XS</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.3.1.3">1,109</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.3.1.4">1,088</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.3.1.5">93</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.3.1.6">91</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.3.1.7" rowspan="5"><span class="ltx_text" id="A3.T8.1.3.1.7.1">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.3.1.8" rowspan="5"><span class="ltx_text" id="A3.T8.1.3.1.8.1">1</span></td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.4.2">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="A3.T8.1.4.2.1">S</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.4.2.2">1,786</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.4.2.3">1,170</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.4.2.4">83</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.4.2.5">77</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.5.3">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="A3.T8.1.5.3.1">M</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.5.3.2">1,858</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.5.3.3">1,093</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.5.3.4">76</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.5.3.5">64</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.6.4">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="A3.T8.1.6.4.1">L</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.6.4.2">2,033</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.6.4.3">1,038</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.6.4.4">80</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.6.4.5">69</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.7.5">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="A3.T8.1.7.5.1">XL</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.7.5.2">2,122</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.7.5.3">972</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.7.5.4">86</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.7.5.5">77</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.8.6">
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="A3.T8.1.8.6.1" rowspan="5"><span class="ltx_text" id="A3.T8.1.8.6.1.1">CSQA</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="A3.T8.1.8.6.2">XS</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.8.6.3">1,496</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.8.6.4">1,444</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.8.6.5">25</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.8.6.6">26</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.8.6.7" rowspan="5"><span class="ltx_text" id="A3.T8.1.8.6.7.1">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.8.6.8" rowspan="5"><span class="ltx_text" id="A3.T8.1.8.6.8.1">1</span></td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.9.7">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="A3.T8.1.9.7.1">S</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.9.7.2">1,265</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.9.7.3">851</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.9.7.4">25</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.9.7.5">25</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.10.8">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="A3.T8.1.10.8.1">M</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.10.8.2">1,399</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.10.8.3">884</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.10.8.4">26</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.10.8.5">25</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.11.9">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="A3.T8.1.11.9.1">L</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.11.9.2">1,324</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.11.9.3">864</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.11.9.4">26</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.11.9.5">25</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.12.10">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="A3.T8.1.12.10.1">XL</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.12.10.2">1,300</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.12.10.3">848</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.12.10.4">27</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.12.10.5">26</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.13.11">
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="A3.T8.1.13.11.1" rowspan="5"><span class="ltx_text" id="A3.T8.1.13.11.1.1">MedQA</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="A3.T8.1.13.11.2">XS</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.13.11.3">1,755</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.13.11.4">1,815</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.13.11.5">117</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.13.11.6">118</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.13.11.7" rowspan="5"><span class="ltx_text" id="A3.T8.1.13.11.7.1">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.13.11.8" rowspan="5"><span class="ltx_text" id="A3.T8.1.13.11.8.1">1</span></td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.14.12">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="A3.T8.1.14.12.1">S</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.14.12.2">1,612</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.14.12.3">1,203</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.14.12.4">85</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.14.12.5">77</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.15.13">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="A3.T8.1.15.13.1">M</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.15.13.2">1,577</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.15.13.3">1,053</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.15.13.4">79</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.15.13.5">67</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.16.14">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="A3.T8.1.16.14.1">L</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.16.14.2">1,599</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.16.14.3">1,013</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.16.14.4">78</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.16.14.5">68</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.17.15">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="A3.T8.1.17.15.1">XL</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.17.15.2">1,691</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.17.15.3">1,001</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.17.15.4">81</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.17.15.5">71</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.18.16">
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="A3.T8.1.18.16.1" rowspan="5"><span class="ltx_text" id="A3.T8.1.18.16.1.1">RecipeGen</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="A3.T8.1.18.16.2">XS</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.18.16.3">1,277</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.18.16.4">1,223</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.18.16.5">16</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.18.16.6">16</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.18.16.7">593</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.18.16.8">528</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.19.17">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="A3.T8.1.19.17.1">S</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.19.17.2">1,168</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.19.17.3">823</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.19.17.4">20</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.19.17.5">19</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.19.17.6">433</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.19.17.7">363</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.20.18">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="A3.T8.1.20.18.1">M</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.20.18.2">1,107</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.20.18.3">807</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.20.18.4">24</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.20.18.5">22</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.20.18.6">369</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.20.18.7">326</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.21.19">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="A3.T8.1.21.19.1">L</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.21.19.2">1,058</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.21.19.3">786</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.21.19.4">24</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.21.19.5">23</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.21.19.6">355</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.21.19.7">319</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.22.20">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="A3.T8.1.22.20.1">XL</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.22.20.2">1,005</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.22.20.3">754</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.22.20.4">24</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.22.20.5">23</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.22.20.6">345</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.22.20.7">316</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.23.21">
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_t" id="A3.T8.1.23.21.1" rowspan="5"><span class="ltx_text" id="A3.T8.1.23.21.1.1">Summarization</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="A3.T8.1.23.21.2">XS</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.23.21.3">1,595</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.23.21.4">734</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.23.21.5">1,019</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.23.21.6">690</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.23.21.7">82</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.23.21.8">61</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.24.22">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="A3.T8.1.24.22.1">S</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.24.22.2">1,442</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.24.22.3">829</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.24.22.4">612</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.24.22.5">442</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.24.22.6">107</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.24.22.7">92</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.25.23">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="A3.T8.1.25.23.1">M</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.25.23.2">1,440</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.25.23.3">852</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.25.23.4">471</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.25.23.5">366</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.25.23.6">116</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.25.23.7">106</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.26.24">
<td class="ltx_td ltx_nopad_l ltx_align_center" id="A3.T8.1.26.24.1">L</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.26.24.2">1,396</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.26.24.3">880</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.26.24.4">432</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.26.24.5">358</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.26.24.6">122</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.26.24.7">110</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.27.25">
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="A3.T8.1.27.25.1">XL</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T8.1.27.25.2">1,369</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T8.1.27.25.3">882</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T8.1.27.25.4">433</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T8.1.27.25.5">355</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T8.1.27.25.6">117</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T8.1.27.25.7">107</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Dataset Statistics.
TS is short for task sample.
For summarization, the instruction includes the model-generated long but cleaned text augmentation from a corpus sample that will subsequently be summarized.
</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep  2 15:00:06 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
