<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>WeQA: A Benchmark for Retrieval Augmented Generation in Wind Energy Domain</title>
<!--Generated on Tue Sep 24 23:17:04 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2408.11800v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#S1" title="In WeQA: A Benchmark for Retrieval Augmented Generation in Wind Energy Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#S2" title="In WeQA: A Benchmark for Retrieval Augmented Generation in Wind Energy Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Works</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#S3" title="In WeQA: A Benchmark for Retrieval Augmented Generation in Wind Energy Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Dataset Creation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#S4" title="In WeQA: A Benchmark for Retrieval Augmented Generation in Wind Energy Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Methodology</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#S5" title="In WeQA: A Benchmark for Retrieval Augmented Generation in Wind Energy Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results and Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#S6" title="In WeQA: A Benchmark for Retrieval Augmented Generation in Wind Energy Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#S7" title="In WeQA: A Benchmark for Retrieval Augmented Generation in Wind Energy Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#S8" title="In WeQA: A Benchmark for Retrieval Augmented Generation in Wind Energy Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Ethical Considerations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#S9" title="In WeQA: A Benchmark for Retrieval Augmented Generation in Wind Energy Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9 </span>Acknowledgment</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">WeQA: A Benchmark for Retrieval Augmented Generation in Wind Energy Domain</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rounak Meyur,
Hung Phan,
Sridevi Wagle,
Jan Strube,
Mahantesh Halappanavar,
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id1.1.id1">Sameera Horawalavithana,
Anurag Acharya,
Sai Munikoti
<br class="ltx_break"/></span>Pacific Northwest National Laboratory
<br class="ltx_break"/>Richland, WA 99354 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id2.2.id2">{rounak.meyur, hung.phan, sridevi.wagle, jan.strube, mahantesh.halappanavar,
<br class="ltx_break"/>yasanka.horawalavithana, anurag.acharya, sai.munikoti}@pnnl.gov</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id3.id1">In the rapidly evolving landscape of <span class="ltx_glossaryref" title="">Natural Language Processing (NLP)</span> and text generation, the emergence of <span class="ltx_glossaryref" title="">Retrieval Augmented Generation (RAG)</span> presents a promising avenue for improving the quality and reliability of generated text by leveraging information retrieved from user specified database. Benchmarking is essential to evaluate and compare the performance of the different <span class="ltx_glossaryref" title="">RAG</span> configurations in terms of <em class="ltx_emph ltx_font_italic" id="id3.id1.1">retriever</em> and <em class="ltx_emph ltx_font_italic" id="id3.id1.2">generator</em>, providing insights into their effectiveness, scalability, and suitability for the specific domain and applications.
In this paper, we present a comprehensive framework to generate a domain relevant <span class="ltx_glossaryref" title="">RAG</span> benchmark. Our framework is based on automatic question-answer generation with Human (domain experts)-AI (<span class="ltx_glossaryref" title="">Large Language Model (LLM)</span>) teaming. As a case study, we demonstrate the framework by introducing WeQA, a first-of-its-kind benchmark on the wind energy domain which comprises of multiple scientific documents/reports related to environmental impact of wind energy projects. Our framework systematically evaluates RAG performance using diverse metrics and multiple question types with varying complexity level. We also demonstrate the performance of different models on our benchmark.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.1">
<p class="ltx_p" id="p1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1.1">WeQA: A Benchmark for Retrieval Augmented Generation in Wind Energy Domain</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.1.2" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.1.2.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.2.1.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.1.1.1.1">Rounak Meyur,
Hung Phan,
Sridevi Wagle,
Jan Strube,
Mahantesh Halappanavar,</span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.2.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.2.2.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.2.2.1.1">Sameera Horawalavithana,
Anurag Acharya,
Sai Munikoti</span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.3.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.3.3.1">Pacific Northwest National Laboratory</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.4.4">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.4.4.1">Richland, WA 99354</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.5.5">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.5.5.1"><span class="ltx_text ltx_font_typewriter" id="p1.1.2.1.1.5.5.1.1">{rounak.meyur, hung.phan, sridevi.wagle, jan.strube, mahantesh.halappanavar,</span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.6.6">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.6.6.1"><span class="ltx_text ltx_font_typewriter" id="p1.1.2.1.1.6.6.1.1">yasanka.horawalavithana, anurag.acharya, sai.munikoti}@pnnl.gov</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In recent years, the advancements in <span class="ltx_glossaryref" title="">LLM</span> have revolutionized various natural language processing tasks, including text and response generation.
However, text generation using <span class="ltx_glossaryref" title="">LLM</span> often encounters challenges such as generating irrelevant or incoherent outputs, perpetuating biases ingrained in the training data, and struggling to maintain context and factual accuracy. These issues pose significant obstacles to achieving human-level performance in automated text generation systems. <span class="ltx_glossaryref" title="">RAG</span> effectively mitigates these common challenges by incorporating retrieved information to enhance coherence and factual accuracy, thus minimizing the generation of fictitious or irrelevant content <cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib11" title="">2024</a>); Lewis et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib18" title="">2021</a>)</cite>. Furthermore, concurrent works suggest RAG is the most sought approach for adapting models towards niche scientific domain such as nuclear, renewable energy, environmental policy, etc. <cite class="ltx_cite ltx_citemacro_cite">Munikoti et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib21" title="">2024a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib22" title="">b</a>); Phan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib27" title="">2023</a>)</cite></p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">As this innovative approach gains traction within the research community and industry applications, its effectiveness and robustness must be rigorously assessed against established benchmarks to ensure its practical utility and reliability <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib7" title="">2023a</a>)</cite>. Benchmarking is essential to establish standardized evaluation metrics and dataset that can effectively capture the nuances of text quality, coherence, factual accuracy, and relevance. Further, it facilitates comparison between <span class="ltx_glossaryref" title="">RAG</span> and existing text generation methods, shedding light on its strengths, limitations, and potential areas for improvement <cite class="ltx_cite ltx_citemacro_cite">Xiong et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib41" title="">2024</a>)</cite>. A robust benchmarking framework can enable researchers and practitioners to systematically investigate the impact of various parameters, such as retrieval strategies, model architectures, and training data, on the performance of RAG <cite class="ltx_cite ltx_citemacro_cite">Ray (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib29" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In benchmarking <span class="ltx_glossaryref" title="">RAG</span> for text generation, it is crucial to evaluate its performance across a diverse set of questions to ensure its efficacy in handling various linguistic contexts and user intents <cite class="ltx_cite ltx_citemacro_cite">Lyu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib20" title="">2024</a>)</cite>. A set of well curated and diverse questions enable a comprehensive assessment of <span class="ltx_glossaryref" title="">RAG</span>’s ability to generate coherent and relevant responses across various domains, ensuring its applicability in real-world scenarios. To generate such questions, automated methods leveraging <span class="ltx_glossaryref" title="">NLP</span> techniques can be employed. These methods include rule-based approaches, template filling, and neural network-based models, which enable the efficient creation of diverse question sets by leveraging linguistic patterns and semantic transformations.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Human-curated questions offer a level of linguistic richness and contextual relevance that may be challenging to achieve solely through automated generation methods <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib42" title="">2024</a>)</cite>. By leveraging human expertise and domain knowledge, curated question sets can encompass a broader spectrum of linguistic variations, domain-specific considerations, and nuanced semantics, providing a more comprehensive evaluation of RAG’s performance across diverse scenarios and applications. Combining automated generation with human curation for benchmarking <span class="ltx_glossaryref" title="">RAG</span> offers a synergistic approach to ensure both efficiency and quality in question sets. This hybrid approach leverages the strengths of both automated and human-driven processes, that provide efficient and robust evaluation metrics for RAG’s performance.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">In this work, we present a hybrid workflow to benchmark <span class="ltx_glossaryref" title="">RAG</span> s, which combines rapid question generation through automated methods, augmented with properly designed human prompts to generate diverse set of questions.
Our proposed benchmarking framework is used to generate questions from documents related to wind energy domain.
These questions serve as a tool to evaluate the performance of <span class="ltx_glossaryref" title="">RAG</span>-based <span class="ltx_glossaryref" title="">LLM</span> s, which are designed to answer queries related to these extensive and comprehensive documents.
Given the vast amount of information contained in these documents, manually reviewing them is impractical, making <span class="ltx_glossaryref" title="">RAG</span>-based <span class="ltx_glossaryref" title="">LLM</span> s essential for generating accurate responses to specific queries.
Our benchmarking framework assesses the effectiveness of these models in accurately retrieving and responding to queries, ensuring that they can reliably process and provide relevant information from the documents.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p6">
<p class="ltx_p" id="S1.p6.1"><span class="ltx_text ltx_font_bold" id="S1.p6.1.1">Contributions</span>  The paper introduces a novel benchmark in a specific domain and also proposes a generic framework to evaluate the performance of <span class="ltx_glossaryref" title="">RAG</span>-based <span class="ltx_glossaryref" title="">LLM</span> s in responding to different types of questions. This framework is designed to be adaptable across various domains, with a specific focus on wind energy-related documents in this study. The contributions of this research are as follows: <span class="ltx_text ltx_font_bold" id="S1.p6.1.2">(i)</span> We present WeQA, <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>This benchmark will be made publicly available.</span></span></span> the first benchmark in the Wind Energy domain, <span class="ltx_text ltx_font_bold" id="S1.p6.1.3">(ii)</span> our proposed framework is domain-agnostic, so it can be tailored for any desired niche domain <span class="ltx_text ltx_font_bold" id="S1.p6.1.4">(iii)</span> we introduce a hybrid method to automatically generate various types of questions, producing both objective and subjective responses. The framework also generates questions from different sections of documents to evaluate <span class="ltx_glossaryref" title="">LLM</span> performance across various sections and question types, and <span class="ltx_text ltx_font_bold" id="S1.p6.1.5">(iv)</span> we utilize existing scoring frameworks like <math alttext="\mathsf{RAGAS}" class="ltx_Math" display="inline" id="S1.p6.1.m1.1"><semantics id="S1.p6.1.m1.1a"><mi id="S1.p6.1.m1.1.1" xref="S1.p6.1.m1.1.1.cmml">𝖱𝖠𝖦𝖠𝖲</mi><annotation-xml encoding="MathML-Content" id="S1.p6.1.m1.1b"><ci id="S1.p6.1.m1.1.1.cmml" xref="S1.p6.1.m1.1.1">𝖱𝖠𝖦𝖠𝖲</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.1.m1.1c">\mathsf{RAGAS}</annotation><annotation encoding="application/x-llamapun" id="S1.p6.1.m1.1d">sansserif_RAGAS</annotation></semantics></math> to evaluate model performance, incorporating different <span class="ltx_glossaryref" title="">LLM</span> s as evaluators for scoring. This approach ensures scalability and quick reproducibility of this approach, while also providing a holistic comparison of <span class="ltx_glossaryref" title="">LLM</span> performance in terms of responding to questions and assessing or comparing <span class="ltx_glossaryref" title="">LLM</span> responses with the ground truth answers.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="63" id="S1.F1.g1" src="extracted/5877533/images/data_extraction.png" width="287"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An overview of data extraction and curation pipeline to generate a vector database of relevant wind energy related documents.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">There have been a lot of work in the field of benchmarking, particularly for question answering (QA) task. These can be broadly divided into general QA and domain-specific QA.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.3">The Stanford Question Answering Dataset (SQuAD) <cite class="ltx_cite ltx_citemacro_cite">Rajpurkar et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib28" title="">2016</a>)</cite>, consisting of 100,000+ questions and a reading comprehension dataset, is arguably the most famous general QA benchmark of the field. They contain three subtasks within QA: reading comprehension, Open-domain QA, and missing word prediction. The AI2 Reasoning Challenge (ARC) <cite class="ltx_cite ltx_citemacro_cite">Clark et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib9" title="">2018</a>)</cite> is another major work, which contains almost <math alttext="8,000" class="ltx_Math" display="inline" id="S2.p2.1.m1.2"><semantics id="S2.p2.1.m1.2a"><mrow id="S2.p2.1.m1.2.3.2" xref="S2.p2.1.m1.2.3.1.cmml"><mn id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml">8</mn><mo id="S2.p2.1.m1.2.3.2.1" xref="S2.p2.1.m1.2.3.1.cmml">,</mo><mn id="S2.p2.1.m1.2.2" xref="S2.p2.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.2b"><list id="S2.p2.1.m1.2.3.1.cmml" xref="S2.p2.1.m1.2.3.2"><cn id="S2.p2.1.m1.1.1.cmml" type="integer" xref="S2.p2.1.m1.1.1">8</cn><cn id="S2.p2.1.m1.2.2.cmml" type="integer" xref="S2.p2.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.2c">8,000</annotation><annotation encoding="application/x-llamapun" id="S2.p2.1.m1.2d">8 , 000</annotation></semantics></math> science questions in English, and also included questions that neither a retrieval-based algorithm nor a word co-occurrence algorithm were able to answer correctly. Similarly, the MCTest dataset <cite class="ltx_cite ltx_citemacro_cite">Richardson et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib31" title="">2013</a>)</cite> consists of <math alttext="500" class="ltx_Math" display="inline" id="S2.p2.2.m2.1"><semantics id="S2.p2.2.m2.1a"><mn id="S2.p2.2.m2.1.1" xref="S2.p2.2.m2.1.1.cmml">500</mn><annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.1b"><cn id="S2.p2.2.m2.1.1.cmml" type="integer" xref="S2.p2.2.m2.1.1">500</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m2.1c">500</annotation><annotation encoding="application/x-llamapun" id="S2.p2.2.m2.1d">500</annotation></semantics></math> stories and <math alttext="2000" class="ltx_Math" display="inline" id="S2.p2.3.m3.1"><semantics id="S2.p2.3.m3.1a"><mn id="S2.p2.3.m3.1.1" xref="S2.p2.3.m3.1.1.cmml">2000</mn><annotation-xml encoding="MathML-Content" id="S2.p2.3.m3.1b"><cn id="S2.p2.3.m3.1.1.cmml" type="integer" xref="S2.p2.3.m3.1.1">2000</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.3.m3.1c">2000</annotation><annotation encoding="application/x-llamapun" id="S2.p2.3.m3.1d">2000</annotation></semantics></math> young children level multiple-choice reading comprehension questions. Some other notable QA benchmarks include Big Bench <cite class="ltx_cite ltx_citemacro_cite">Srivastava et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib34" title="">2022</a>)</cite>, ARC2 <cite class="ltx_cite ltx_citemacro_cite">Bhakthavatsalam et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib5" title="">2021</a>)</cite>, GLUE <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib37" title="">2018</a>)</cite>, CommonsenseQA <cite class="ltx_cite ltx_citemacro_cite">Talmor et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib35" title="">2018</a>)</cite>, TriviaQA: 650K QA pairs with evidence <cite class="ltx_cite ltx_citemacro_cite">Joshi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib16" title="">2017</a>)</cite>, Search QA <cite class="ltx_cite ltx_citemacro_cite">Dunn et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib10" title="">2017</a>)</cite>, NewsQA: 10K news articles <cite class="ltx_cite ltx_citemacro_cite">Trischler et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib36" title="">2016</a>)</cite>, <span class="ltx_text ltx_font_italic" id="S2.p2.3.1">inter alia</span>.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">More recently, there have been several benchmarks that focus on scientific and adjacent fields. The scientific portions of the MMLU <cite class="ltx_cite ltx_citemacro_cite">Hendrycks et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib13" title="">2020</a>)</cite> benchmark is perhaps one of the most widely used science benchmarks, which include college and high school level questions in Physics, Chemistry, Biology, Computer Science, etc. Science Questions: 1K multiple choice questions in AI2R <cite class="ltx_cite ltx_citemacro_cite">Talmor et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib35" title="">2018</a>)</cite> and SciQ Dataset: <cite class="ltx_cite ltx_citemacro_cite">Welbl et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib40" title="">2017</a>)</cite> 13,679 multiple choice science questions are two other major benchmarks in the scientific domain, as is the SciQA <cite class="ltx_cite ltx_citemacro_cite">Auer et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib4" title="">2023</a>)</cite>, a scientific QA benchmark created by using knowledge graphs of academic articles. SciRepEval<cite class="ltx_cite ltx_citemacro_cite">Singh et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib33" title="">2022</a>)</cite> is a benchmark that has four different task types – classification, regression, proximity – over scientific documents.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.2">Similarly, some of the other most recent works include SciBench<cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib38" title="">2023</a>)</cite>, a benchmark of <math alttext="\sim" class="ltx_Math" display="inline" id="S2.p4.1.m1.1"><semantics id="S2.p4.1.m1.1a"><mo id="S2.p4.1.m1.1.1" xref="S2.p4.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.p4.1.m1.1b"><csymbol cd="latexml" id="S2.p4.1.m1.1.1.cmml" xref="S2.p4.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S2.p4.1.m1.1d">∼</annotation></semantics></math>700 questions sourced from textbooks for college-level science problems and QASA <cite class="ltx_cite ltx_citemacro_cite">Lee et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib17" title="">2023</a>)</cite>, a QA benchmark of <math alttext="\sim" class="ltx_Math" display="inline" id="S2.p4.2.m2.1"><semantics id="S2.p4.2.m2.1a"><mo id="S2.p4.2.m2.1.1" xref="S2.p4.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.p4.2.m2.1b"><csymbol cd="latexml" id="S2.p4.2.m2.1.1.cmml" xref="S2.p4.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.2.m2.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S2.p4.2.m2.1d">∼</annotation></semantics></math>1800 questions to test reasoning on scientific articles, specifically in AI and ML domains. There are also benchmarks that address specific fields, with TheoremQA <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib8" title="">2023b</a>)</cite> for mathematics, emrQA <cite class="ltx_cite ltx_citemacro_cite">Pampari et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib24" title="">2018</a>)</cite> for medicine, and BioRead <cite class="ltx_cite ltx_citemacro_cite">Pappas et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib25" title="">2018</a>)</cite> and BioMRC <cite class="ltx_cite ltx_citemacro_cite">Pappas et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib26" title="">2020</a>)</cite> for biology, and NukeBERT <cite class="ltx_cite ltx_citemacro_cite">Jain et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib15" title="">2020</a>)</cite> and NuclearQA <cite class="ltx_cite ltx_citemacro_cite">Acharya et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib2" title="">2023</a>)</cite> for the nuclear domain.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">While these scientific domains are related to our task in terms of technological similarity, to our knowledge, there are no benchmarks for our specific field and this is the first such work. The only one that comes close is the NEPAQuAD benchmark <cite class="ltx_cite ltx_citemacro_cite">Phan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib27" title="">2023</a>)</cite> that deals with QA task for Environmental Impact Statement (EIS) documents.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Dataset Creation</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this paper, we focus on wind energy-related documents to enable the <span class="ltx_glossaryref" title="">RAG</span>-based <span class="ltx_glossaryref" title="">LLM</span> s to answer questions specific to this field.
We gather PDF documents, including research articles and environmental impact studies published by the Department of Energy (DOE) under the National Environmental Policy Act (NEPA). Accessing information from this vast database is not straightforward, necessitating the need for a trained <span class="ltx_glossaryref" title="">LLM</span> to accurately retrieve and answer questions from the provided context. The challenge is to ensure that the model’s responses are based on the actual documents and do not hallucinate information. By using <span class="ltx_glossaryref" title="">RAG</span>-based <span class="ltx_glossaryref" title="">LLM</span> s, we aim to enhance the reliability and accuracy of responses related to wind energy, leveraging the rich information within our extensive document collection. This approach ensures that the information provided is both relevant and grounded in the sourced material.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">We constructed a data extraction and curation pipeline to extract text, image, and table information from wind energy-related documents as depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ WeQA: A Benchmark for Retrieval Augmented Generation in Wind Energy Domain"><span class="ltx_text ltx_ref_tag">1</span></a>. Utilizing large language model (LLM) based methods such as the <em class="ltx_emph ltx_font_italic" id="S3.p2.1.1">Unstructured.io</em> tool <cite class="ltx_cite ltx_citemacro_cite">Raymond (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib30" title="">2023</a>)</cite>, we efficiently extracted information and converted it into JSON elements. These JSON elements were then organized into a schema, creating a page-wise assortment of text, table, and image elements. This structured format ensures that the extracted data is easily accessible and can be accurately referenced during model training and evaluation.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="138" id="S3.F2.g1" src="extracted/5877533/images/benchmark_framework.png" width="287"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An overview of the proposed <span class="ltx_glossaryref" title="">RAG</span> benchmarking framework. Multiple versions of hybrid questions are generated from specific text chunks of source documents with human-in-the-loop to review them. These questions are used as prompts for the <span class="ltx_glossaryref" title="">LLM</span> or <span class="ltx_glossaryref" title="">RAG</span> model under test.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Methodology</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">While past works have generally preferred to use crowdsourcing as a way to craft datasets and benchmarks <cite class="ltx_cite ltx_citemacro_cite">Sap et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib32" title="">2019</a>); Acharya et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib3" title="">2021</a>)</cite>, we choose to automated methods for benchmark question generation. Automatically generating benchmarking questions using GPT-4 allows for efficient and scalable evaluation of other <span class="ltx_glossaryref" title="">LLM</span> s and <span class="ltx_glossaryref" title="">RAG</span>. However, this approach can introduce errors, leading to poor quality of questions being generated. This makes it essential to incorporate a human-in-the-loop for reviewing and refining the questions and responses. This paper proposes hybrid approaches, where automated methods are combined with human curation to ensure the accuracy and reliability of the benchmarking process. By leveraging both machine and human expertise, we can achieve more robust and comprehensive benchmarking framework.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#S3.F2" title="Figure 2 ‣ 3 Dataset Creation ‣ WeQA: A Benchmark for Retrieval Augmented Generation in Wind Energy Domain"><span class="ltx_text ltx_ref_tag">2</span></a> provides an overview of the proposed <span class="ltx_glossaryref" title="">LLM</span> benchmarking framework.
The core of the benchmarking framework is the question generation aspect, where automatic generation of questions forms the foundation.
We combine this with human curation to select high-quality questions, ensuring relevance and clarity.
Corresponding answers to these questions are then validated by humans, establishing a reliable ground truth.
This curated set of questions and validated answers is used to evaluate the responses of other <span class="ltx_glossaryref" title="">LLM</span> s and <span class="ltx_glossaryref" title="">RAG</span> models.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p3">
<p class="ltx_p" id="S4.p3.1"><span class="ltx_text ltx_font_bold" id="S4.p3.1.1">Different question types.</span> We generate multiple types of questions, including closed, open, comparison, evaluation, recall, process, and rhetorical questions.
This diversity ensures a comprehensive benchmarking process, as each question type assesses different aspects of the models’ capabilities.
By incorporating a wide variety of questions, we can more effectively evaluate and compare the performance of <span class="ltx_glossaryref" title="">LLM</span> s and <span class="ltx_glossaryref" title="">RAG</span> models across various dimensions.
This approach provides a holistic view of their strengths and weaknesses.</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1">Each of these question type evaluates different capabilities of the <span class="ltx_glossaryref" title="">LLM</span> under test.
<em class="ltx_emph ltx_font_italic" id="S4.p4.1.1">Open questions</em> require models to generate detailed, free-form responses, testing their ability to construct coherent and informative answers.
<em class="ltx_emph ltx_font_italic" id="S4.p4.1.2">Comparison questions</em> ask models to compare and contrast different concepts or entities, assessing their analytical and comparative reasoning skills.
<em class="ltx_emph ltx_font_italic" id="S4.p4.1.3">Evaluation questions</em> require models to make judgments or provide assessments, gauging their ability to evaluate information critically.
<em class="ltx_emph ltx_font_italic" id="S4.p4.1.4">Recall questions</em> focus on the model’s ability to retrieve and reproduce specific information from memory, testing their factual accuracy.
<em class="ltx_emph ltx_font_italic" id="S4.p4.1.5">Process questions</em> ask models to explain processes or sequences of actions, evaluating their understanding of procedures and logical progression.
<em class="ltx_emph ltx_font_italic" id="S4.p4.1.6">Rhetorical questions</em> are used to test the models’ grasp of nuances in language and their ability to recognize and appropriately respond to questions that may not require direct answers.</p>
</div>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p" id="S4.p5.1">Next, we present two approaches for the hybrid question generation procedure required for <span class="ltx_glossaryref" title="">LLM</span> benchmarking purposes. The first approach engineers the prompt to generate well curated enhanced quality questions. The second approach summarizes the provided text chunks and generates questions from the summaries.
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p6">
<p class="ltx_p" id="S4.p6.1"><span class="ltx_text ltx_font_bold" id="S4.p6.1.1">Hybrid prompts.</span> 
We use GPT-4 to automatically generate questions from a given text chunk by providing particular <em class="ltx_emph ltx_font_italic" id="S4.p6.1.2">prompts</em> for each question type.
The prompt is structured as follows:</p>
</div>
<div class="ltx_para" id="S4.p7">
<blockquote class="ltx_quote" id="S4.p7.1">
<p class="ltx_p" id="S4.p7.1.1">Generate {<em class="ltx_emph ltx_font_italic" id="S4.p7.1.1.1">num</em>} questions given the content provided in the following paragraph. Restrict the type of questions to {<em class="ltx_emph ltx_font_italic" id="S4.p7.1.1.2">question type</em>} questions.
<br class="ltx_break"/>{<em class="ltx_emph ltx_font_italic" id="S4.p7.1.1.3">Text chunk to generate the questions.</em>}</p>
</blockquote>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="246" id="S4.F3.g1" src="extracted/5877533/images/v3_question.png" width="287"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Summary of “introduction” section of a report <cite class="ltx_cite ltx_citemacro_cite">Invenergy (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib14" title="">2014</a>)</cite> generated by GPT-4. The question and the answer are generated from the summarized text chunk. The answer is retrieved from a subset of text in the chunk, shown here in red.</figcaption>
</figure>
<div class="ltx_para" id="S4.p8">
<p class="ltx_p" id="S4.p8.1">An important aspect of our approach is curating the automatically generated questions to enhance the quality.
To this end, we manually identify questions which are best suited for the purpose of benchmarking <span class="ltx_glossaryref" title="">LLM</span> s.
We perform this process for each type of question, so that we include particular grammatical structures for each question type.
Thereafter, we use these identified questions as <em class="ltx_emph ltx_font_italic" id="S4.p8.1.1">few-shot examples</em> to regenerate questions using the automatic question generation framework.
The updated prompt looks as follows:</p>
</div>
<div class="ltx_para" id="S4.p9">
<blockquote class="ltx_quote" id="S4.p9.1">
<p class="ltx_p" id="S4.p9.1.1">Generate {<em class="ltx_emph ltx_font_italic" id="S4.p9.1.1.1">num</em>} questions given the content provided in the following paragraph. Restrict the type of questions to {<em class="ltx_emph ltx_font_italic" id="S4.p9.1.1.2">question type</em>} questions.
<br class="ltx_break"/>{<em class="ltx_emph ltx_font_italic" id="S4.p9.1.1.3">Text chunk to generate the questions.</em>}
<br class="ltx_break"/>You can generate similar questions (but not limited) to sample questions provided below.
<br class="ltx_break"/>{<em class="ltx_emph ltx_font_italic" id="S4.p9.1.1.4">List of sample questions</em>}</p>
</blockquote>
</div>
<div class="ltx_para ltx_noindent" id="S4.p10">
<p class="ltx_p" id="S4.p10.1"><span class="ltx_text ltx_font_bold" id="S4.p10.1.1">Hybrid text chunks.</span> 
A problem with the aforementioned approach is that a significant number of questions are generated on a single sentence basis.
This is obtained by substituting the subject or object of a sentence with a ‘wh’ word.
These generated questions are meaningful when we consider question types such as ‘closed’, ‘open’, or ‘recall’, where the answers can be a single sentence from the provided text chunk.
However, ‘process’, ‘evaluation’, and ‘comparison’ type questions of enhanced quality require the answer to be inferred from a larger portion of the given text chunk.
To this end, first we use GPT-4 to summarize the entire text chunk (consisting of more than 15 sentences) into a summarized text chunk (consisting 5-8 sentences) using a prompt as follows:</p>
</div>
<div class="ltx_para" id="S4.p11">
<blockquote class="ltx_quote" id="S4.p11.1">
<p class="ltx_p" id="S4.p11.1.1">You are a smart assistant. Can you summarize this input paragraph within {<em class="ltx_emph ltx_font_italic" id="S4.p11.1.1.1">num</em>} bullet points. Return the summarized text.
<br class="ltx_break"/>Input paragraph:
“‘
{<em class="ltx_emph ltx_font_italic" id="S4.p11.1.1.2">Text chunk to summarize</em>}
”’</p>
</blockquote>
<p class="ltx_p" id="S4.p11.2">Thereafter, we use GPT-4 with appropriate prompts to generate questions from these summarized text chunks using the previous hybrid prompt along with the list of sample questions.
Here, we show an example question generated using this approach.
We include the summary text chunk generated by GPT-4 in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#S4.F3" title="Figure 3 ‣ 4 Methodology ‣ WeQA: A Benchmark for Retrieval Augmented Generation in Wind Energy Domain"><span class="ltx_text ltx_ref_tag">3</span></a> and highlight the text in red color, from which the answer for the ‘comparison’ type question is retrieved.</p>
</div>
<div class="ltx_para" id="S4.p12">
<blockquote class="ltx_quote" id="S4.p12.2">
<p class="ltx_p" id="S4.p12.2.3"><span class="ltx_text ltx_font_bold" id="S4.p12.2.3.1">Question:</span> How does the proportion of cultivated cropland within the Pleasant Ridge Wind Resource Area (PRWRA) compared to the proportion of developed areas?</p>
<p class="ltx_p" id="S4.p12.2.2"><span class="ltx_text ltx_font_bold" id="S4.p12.2.2.1">Answer:</span> Cultivated cropland covers <math alttext="92.3\%" class="ltx_Math" display="inline" id="S4.p12.1.1.m1.1"><semantics id="S4.p12.1.1.m1.1a"><mrow id="S4.p12.1.1.m1.1.1" xref="S4.p12.1.1.m1.1.1.cmml"><mn id="S4.p12.1.1.m1.1.1.2" xref="S4.p12.1.1.m1.1.1.2.cmml">92.3</mn><mo id="S4.p12.1.1.m1.1.1.1" xref="S4.p12.1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p12.1.1.m1.1b"><apply id="S4.p12.1.1.m1.1.1.cmml" xref="S4.p12.1.1.m1.1.1"><csymbol cd="latexml" id="S4.p12.1.1.m1.1.1.1.cmml" xref="S4.p12.1.1.m1.1.1.1">percent</csymbol><cn id="S4.p12.1.1.m1.1.1.2.cmml" type="float" xref="S4.p12.1.1.m1.1.1.2">92.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p12.1.1.m1.1c">92.3\%</annotation><annotation encoding="application/x-llamapun" id="S4.p12.1.1.m1.1d">92.3 %</annotation></semantics></math> of the PRWRA while developed areas cover <math alttext="5.1\%" class="ltx_Math" display="inline" id="S4.p12.2.2.m2.1"><semantics id="S4.p12.2.2.m2.1a"><mrow id="S4.p12.2.2.m2.1.1" xref="S4.p12.2.2.m2.1.1.cmml"><mn id="S4.p12.2.2.m2.1.1.2" xref="S4.p12.2.2.m2.1.1.2.cmml">5.1</mn><mo id="S4.p12.2.2.m2.1.1.1" xref="S4.p12.2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p12.2.2.m2.1b"><apply id="S4.p12.2.2.m2.1.1.cmml" xref="S4.p12.2.2.m2.1.1"><csymbol cd="latexml" id="S4.p12.2.2.m2.1.1.1.cmml" xref="S4.p12.2.2.m2.1.1.1">percent</csymbol><cn id="S4.p12.2.2.m2.1.1.2.cmml" type="float" xref="S4.p12.2.2.m2.1.1.2">5.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p12.2.2.m2.1c">5.1\%</annotation><annotation encoding="application/x-llamapun" id="S4.p12.2.2.m2.1d">5.1 %</annotation></semantics></math>.</p>
</blockquote>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Land Cover Types, Coverage, and Composition within the Pleasant Ridge Project Area, Based on National Land Cover Database in May of 2014 <cite class="ltx_cite ltx_citemacro_cite">Invenergy (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib14" title="">2014</a>)</cite></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.1">Habitat</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.2.1">Acres [Hectares]</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.3.1">% Composition</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.2.1.1">Cultivated Crops</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.2">55,946[22,641]</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.3">92.6</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.3.2.1">Developed</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.2">3,432[1,389]</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.3">5.7</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.4.3.1">Deciduous Forest</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.2">451[183]</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.3">0.7</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.5.4.1">Hay/Pasture</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.2">347[140]</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.3">0.6</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.6.5.1">Open Water</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.5.2">122[49]</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.5.3">0.2</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.7.6.1">Woody Wetlands</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.6.2">111[45]</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.6.3">0.2</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.8.7.1">Barren Land</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.7.2">19[8]</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.7.3">0.0</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.9.8.1">Herbaceous</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.9.8.2">3[1]</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.9.8.3">0.0</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="S4.T1.1.10.9.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.10.9.1.1">Total</span></th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T1.1.10.9.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.10.9.2.1">60,431[24,456]</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T1.1.10.9.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.10.9.3.1">100</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.3.4.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S4.T2.3.4.1.1" style="padding-left:5.5pt;padding-right:5.5pt;"></th>
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T2.3.4.1.2" style="padding-left:5.5pt;padding-right:5.5pt;"></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="6" id="S4.T2.3.4.1.3" style="padding-left:5.5pt;padding-right:5.5pt;">GPT-4 as Evaluator</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="6" id="S4.T2.3.4.1.4" style="padding-left:5.5pt;padding-right:5.5pt;">Gemini 1.5 Pro as Evaluator</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T2.1.1.2" style="padding-left:5.5pt;padding-right:5.5pt;"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.1.1.1" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.1.1.1.1">Model <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T2.1.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.1.m1.1a"><mo id="S4.T2.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T2.1.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.1.m1.1d">→</annotation></semantics></math></span></th>
<td class="ltx_td ltx_align_center" colspan="2" id="S4.T2.1.1.3" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.1.1.3.1">  GPT</span></td>
<td class="ltx_td ltx_align_center" colspan="2" id="S4.T2.1.1.4" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.1.1.4.1">  Claude</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" colspan="2" id="S4.T2.1.1.5" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.1.1.5.1">  Gemini</span></td>
<td class="ltx_td ltx_align_center" colspan="2" id="S4.T2.1.1.6" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.1.1.6.1">  GPT</span></td>
<td class="ltx_td ltx_align_center" colspan="2" id="S4.T2.1.1.7" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.1.1.7.1">  Claude</span></td>
<td class="ltx_td ltx_align_center" colspan="2" id="S4.T2.1.1.8" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.1.1.8.1">  Gemini</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.2.2.1" style="padding-left:5.5pt;padding-right:5.5pt;">Section <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.2.2.1.m1.1"><semantics id="S4.T2.2.2.1.m1.1a"><mo id="S4.T2.2.2.1.m1.1.1" stretchy="false" xref="S4.T2.2.2.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.1.m1.1b"><ci id="S4.T2.2.2.1.m1.1.1.cmml" xref="S4.T2.2.2.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.1.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.3.3.2" style="padding-left:5.5pt;padding-right:5.5pt;">Type <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.3.3.2.m1.1"><semantics id="S4.T2.3.3.2.m1.1a"><mo id="S4.T2.3.3.2.m1.1.1" stretchy="false" xref="S4.T2.3.3.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.2.m1.1b"><ci id="S4.T2.3.3.2.m1.1.1.cmml" xref="S4.T2.3.3.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.3.2.m1.1d">↓</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_right" id="S4.T2.3.3.3" style="padding-left:5.5pt;padding-right:5.5pt;">Prec.</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.3.4" style="padding-left:5.5pt;padding-right:5.5pt;">Rec.</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.3.5" style="padding-left:5.5pt;padding-right:5.5pt;">Prec.</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.3.6" style="padding-left:5.5pt;padding-right:5.5pt;">Rec.</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.3.7" style="padding-left:5.5pt;padding-right:5.5pt;">Prec.</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.3.3.8" style="padding-left:5.5pt;padding-right:5.5pt;">Rec.</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.3.9" style="padding-left:5.5pt;padding-right:5.5pt;">Prec.</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.3.10" style="padding-left:5.5pt;padding-right:5.5pt;">Rec.</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.3.11" style="padding-left:5.5pt;padding-right:5.5pt;">Prec.</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.3.12" style="padding-left:5.5pt;padding-right:5.5pt;">Rec.</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.3.13" style="padding-left:5.5pt;padding-right:5.5pt;">Prec.</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.3.14" style="padding-left:5.5pt;padding-right:5.5pt;">Rec.</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.3.5.2.1" rowspan="5" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text" id="S4.T2.3.5.2.1.1">Introduction</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.3.5.2.2" style="padding-left:5.5pt;padding-right:5.5pt;">closed</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.5.2.3" style="padding-left:5.5pt;padding-right:5.5pt;">0.467</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.5.2.4" style="padding-left:5.5pt;padding-right:5.5pt;">0.314</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.5.2.5" style="padding-left:5.5pt;padding-right:5.5pt;">0.500</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.5.2.6" style="padding-left:5.5pt;padding-right:5.5pt;">0.330</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.5.2.7" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.5.2.7.1">0.570</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S4.T2.3.5.2.8" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.5.2.8.1">0.385</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.5.2.9" style="padding-left:5.5pt;padding-right:5.5pt;">0.392</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.5.2.10" style="padding-left:5.5pt;padding-right:5.5pt;">0.435</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.5.2.11" style="padding-left:5.5pt;padding-right:5.5pt;">0.424</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.5.2.12" style="padding-left:5.5pt;padding-right:5.5pt;">0.448</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.5.2.13" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.5.2.13.1">0.467</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.5.2.14" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.5.2.14.1">0.563</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.3.6.3.1" style="padding-left:5.5pt;padding-right:5.5pt;">comparison</th>
<td class="ltx_td ltx_align_right" id="S4.T2.3.6.3.2" style="padding-left:5.5pt;padding-right:5.5pt;">0.556</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.6.3.3" style="padding-left:5.5pt;padding-right:5.5pt;">0.596</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.6.3.4" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.6.3.4.1">0.607</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.6.3.5" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.6.3.5.1">0.672</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.6.3.6" style="padding-left:5.5pt;padding-right:5.5pt;">0.587</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.3.6.3.7" style="padding-left:5.5pt;padding-right:5.5pt;">0.628</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.6.3.8" style="padding-left:5.5pt;padding-right:5.5pt;">0.429</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.6.3.9" style="padding-left:5.5pt;padding-right:5.5pt;">0.597</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.6.3.10" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.6.3.10.1">0.480</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.6.3.11" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.6.3.11.1">0.637</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.6.3.12" style="padding-left:5.5pt;padding-right:5.5pt;">0.454</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.6.3.13" style="padding-left:5.5pt;padding-right:5.5pt;">0.632</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.3.7.4.1" style="padding-left:5.5pt;padding-right:5.5pt;">process</th>
<td class="ltx_td ltx_align_right" id="S4.T2.3.7.4.2" style="padding-left:5.5pt;padding-right:5.5pt;">0.565</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.7.4.3" style="padding-left:5.5pt;padding-right:5.5pt;">0.608</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.7.4.4" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.7.4.4.1">0.598</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.7.4.5" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.7.4.5.1">0.625</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.7.4.6" style="padding-left:5.5pt;padding-right:5.5pt;">0.586</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.3.7.4.7" style="padding-left:5.5pt;padding-right:5.5pt;">0.602</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.7.4.8" style="padding-left:5.5pt;padding-right:5.5pt;">0.457</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.7.4.9" style="padding-left:5.5pt;padding-right:5.5pt;">0.568</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.7.4.10" style="padding-left:5.5pt;padding-right:5.5pt;">0.467</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.7.4.11" style="padding-left:5.5pt;padding-right:5.5pt;">0.603</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.7.4.12" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.7.4.12.1">0.483</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.7.4.13" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.7.4.13.1">0.591</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.8.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.3.8.5.1" style="padding-left:5.5pt;padding-right:5.5pt;">recall</th>
<td class="ltx_td ltx_align_right" id="S4.T2.3.8.5.2" style="padding-left:5.5pt;padding-right:5.5pt;">0.529</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.8.5.3" style="padding-left:5.5pt;padding-right:5.5pt;">0.597</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.8.5.4" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.8.5.4.1">0.560</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.8.5.5" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.8.5.5.1">0.617</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.8.5.6" style="padding-left:5.5pt;padding-right:5.5pt;">0.540</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.3.8.5.7" style="padding-left:5.5pt;padding-right:5.5pt;">0.586</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.8.5.8" style="padding-left:5.5pt;padding-right:5.5pt;">0.491</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.8.5.9" style="padding-left:5.5pt;padding-right:5.5pt;">0.611</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.8.5.10" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.8.5.10.1">0.487</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.8.5.11" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.8.5.11.1">0.624</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.8.5.12" style="padding-left:5.5pt;padding-right:5.5pt;">0.483</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.8.5.13" style="padding-left:5.5pt;padding-right:5.5pt;">0.601</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.9.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.3.9.6.1" style="padding-left:5.5pt;padding-right:5.5pt;">rhetorical</th>
<td class="ltx_td ltx_align_right" id="S4.T2.3.9.6.2" style="padding-left:5.5pt;padding-right:5.5pt;">0.305</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.9.6.3" style="padding-left:5.5pt;padding-right:5.5pt;">0.296</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.9.6.4" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.9.6.4.1">0.365</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.9.6.5" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.9.6.5.1">0.353</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.9.6.6" style="padding-left:5.5pt;padding-right:5.5pt;">0.319</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.3.9.6.7" style="padding-left:5.5pt;padding-right:5.5pt;">0.306</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.9.6.8" style="padding-left:5.5pt;padding-right:5.5pt;">0.272</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.9.6.9" style="padding-left:5.5pt;padding-right:5.5pt;">0.299</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.9.6.10" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.9.6.10.1">0.323</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.9.6.11" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.9.6.11.1">0.339</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.9.6.12" style="padding-left:5.5pt;padding-right:5.5pt;">0.283</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.9.6.13" style="padding-left:5.5pt;padding-right:5.5pt;">0.299</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.10.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.3.10.7.1" rowspan="6" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text" id="S4.T2.3.10.7.1.1">Method</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.3.10.7.2" style="padding-left:5.5pt;padding-right:5.5pt;">closed</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.10.7.3" style="padding-left:5.5pt;padding-right:5.5pt;">0.162</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.10.7.4" style="padding-left:5.5pt;padding-right:5.5pt;">0.119</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.10.7.5" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.10.7.5.1">0.168</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.10.7.6" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.10.7.6.1">0.139</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.10.7.7" style="padding-left:5.5pt;padding-right:5.5pt;">0.094</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S4.T2.3.10.7.8" style="padding-left:5.5pt;padding-right:5.5pt;">0.082</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.10.7.9" style="padding-left:5.5pt;padding-right:5.5pt;">0.128</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.10.7.10" style="padding-left:5.5pt;padding-right:5.5pt;">0.176</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.10.7.11" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.10.7.11.1">0.144</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.10.7.12" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.10.7.12.1">0.174</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.10.7.13" style="padding-left:5.5pt;padding-right:5.5pt;">0.084</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.10.7.14" style="padding-left:5.5pt;padding-right:5.5pt;">0.093</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.11.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.3.11.8.1" style="padding-left:5.5pt;padding-right:5.5pt;">open</th>
<td class="ltx_td ltx_align_right" id="S4.T2.3.11.8.2" style="padding-left:5.5pt;padding-right:5.5pt;">0.364</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.11.8.3" style="padding-left:5.5pt;padding-right:5.5pt;">0.431</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.11.8.4" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.11.8.4.1">0.431</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.11.8.5" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.11.8.5.1">0.540</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.11.8.6" style="padding-left:5.5pt;padding-right:5.5pt;">0.378</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.3.11.8.7" style="padding-left:5.5pt;padding-right:5.5pt;">0.471</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.11.8.8" style="padding-left:5.5pt;padding-right:5.5pt;">0.333</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.11.8.9" style="padding-left:5.5pt;padding-right:5.5pt;">0.455</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.11.8.10" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.11.8.10.1">0.383</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.11.8.11" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.11.8.11.1">0.511</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.11.8.12" style="padding-left:5.5pt;padding-right:5.5pt;">0.367</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.11.8.13" style="padding-left:5.5pt;padding-right:5.5pt;">0.446</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.12.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.3.12.9.1" style="padding-left:5.5pt;padding-right:5.5pt;">evaluation</th>
<td class="ltx_td ltx_align_right" id="S4.T2.3.12.9.2" style="padding-left:5.5pt;padding-right:5.5pt;">0.400</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.12.9.3" style="padding-left:5.5pt;padding-right:5.5pt;">0.387</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.12.9.4" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.12.9.4.1">0.442</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.12.9.5" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.12.9.5.1">0.453</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.12.9.6" style="padding-left:5.5pt;padding-right:5.5pt;">0.416</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.3.12.9.7" style="padding-left:5.5pt;padding-right:5.5pt;">0.422</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.12.9.8" style="padding-left:5.5pt;padding-right:5.5pt;">0.311</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.12.9.9" style="padding-left:5.5pt;padding-right:5.5pt;">0.406</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.12.9.10" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.12.9.10.1">0.352</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.12.9.11" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.12.9.11.1">0.474</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.12.9.12" style="padding-left:5.5pt;padding-right:5.5pt;">0.316</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.12.9.13" style="padding-left:5.5pt;padding-right:5.5pt;">0.430</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.13.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.3.13.10.1" style="padding-left:5.5pt;padding-right:5.5pt;">process</th>
<td class="ltx_td ltx_align_right" id="S4.T2.3.13.10.2" style="padding-left:5.5pt;padding-right:5.5pt;">0.270</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.13.10.3" style="padding-left:5.5pt;padding-right:5.5pt;">0.275</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.13.10.4" style="padding-left:5.5pt;padding-right:5.5pt;">0.270</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.13.10.5" style="padding-left:5.5pt;padding-right:5.5pt;">0.293</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.13.10.6" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.13.10.6.1">0.282</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.3.13.10.7" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.13.10.7.1">0.302</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.13.10.8" style="padding-left:5.5pt;padding-right:5.5pt;">0.209</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.13.10.9" style="padding-left:5.5pt;padding-right:5.5pt;">0.282</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.13.10.10" style="padding-left:5.5pt;padding-right:5.5pt;">0.162</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.13.10.11" style="padding-left:5.5pt;padding-right:5.5pt;">0.268</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.13.10.12" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.13.10.12.1">0.210</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.13.10.13" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.13.10.13.1">0.306</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.14.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.3.14.11.1" style="padding-left:5.5pt;padding-right:5.5pt;">recall</th>
<td class="ltx_td ltx_align_right" id="S4.T2.3.14.11.2" style="padding-left:5.5pt;padding-right:5.5pt;">0.234</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.14.11.3" style="padding-left:5.5pt;padding-right:5.5pt;">0.277</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.14.11.4" style="padding-left:5.5pt;padding-right:5.5pt;">0.223</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.14.11.5" style="padding-left:5.5pt;padding-right:5.5pt;">0.268</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.14.11.6" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.14.11.6.1">0.250</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.3.14.11.7" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.14.11.7.1">0.285</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.14.11.8" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.14.11.8.1">0.223</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.14.11.9" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.14.11.9.1">0.270</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.14.11.10" style="padding-left:5.5pt;padding-right:5.5pt;">0.188</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.14.11.11" style="padding-left:5.5pt;padding-right:5.5pt;">0.251</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.14.11.12" style="padding-left:5.5pt;padding-right:5.5pt;">0.212</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.14.11.13" style="padding-left:5.5pt;padding-right:5.5pt;">0.278</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.15.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.3.15.12.1" style="padding-left:5.5pt;padding-right:5.5pt;">rhetorical</th>
<td class="ltx_td ltx_align_right" id="S4.T2.3.15.12.2" style="padding-left:5.5pt;padding-right:5.5pt;">0.229</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.15.12.3" style="padding-left:5.5pt;padding-right:5.5pt;">0.223</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.15.12.4" style="padding-left:5.5pt;padding-right:5.5pt;">0.241</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.15.12.5" style="padding-left:5.5pt;padding-right:5.5pt;">0.232</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.15.12.6" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.15.12.6.1">0.250</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.3.15.12.7" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.15.12.7.1">0.238</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.15.12.8" style="padding-left:5.5pt;padding-right:5.5pt;">0.208</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.15.12.9" style="padding-left:5.5pt;padding-right:5.5pt;">0.238</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.15.12.10" style="padding-left:5.5pt;padding-right:5.5pt;">0.193</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.15.12.11" style="padding-left:5.5pt;padding-right:5.5pt;">0.230</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.15.12.12" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.15.12.12.1">0.224</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.15.12.13" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.15.12.13.1">0.248</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.16.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.3.16.13.1" rowspan="5" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text" id="S4.T2.3.16.13.1.1">Results</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.3.16.13.2" style="padding-left:5.5pt;padding-right:5.5pt;">closed</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.16.13.3" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.16.13.3.1">0.143</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.16.13.4" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.16.13.4.1">0.077</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.16.13.5" style="padding-left:5.5pt;padding-right:5.5pt;">0.102</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.16.13.6" style="padding-left:5.5pt;padding-right:5.5pt;">0.072</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.16.13.7" style="padding-left:5.5pt;padding-right:5.5pt;">0.076</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S4.T2.3.16.13.8" style="padding-left:5.5pt;padding-right:5.5pt;">0.059</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.16.13.9" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.16.13.9.1">0.120</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.16.13.10" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.16.13.10.1">0.101</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.16.13.11" style="padding-left:5.5pt;padding-right:5.5pt;">0.093</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.16.13.12" style="padding-left:5.5pt;padding-right:5.5pt;">0.099</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.16.13.13" style="padding-left:5.5pt;padding-right:5.5pt;">0.070</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.16.13.14" style="padding-left:5.5pt;padding-right:5.5pt;">0.086</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.17.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.3.17.14.1" style="padding-left:5.5pt;padding-right:5.5pt;">open</th>
<td class="ltx_td ltx_align_right" id="S4.T2.3.17.14.2" style="padding-left:5.5pt;padding-right:5.5pt;">0.284</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.17.14.3" style="padding-left:5.5pt;padding-right:5.5pt;">0.328</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.17.14.4" style="padding-left:5.5pt;padding-right:5.5pt;">0.263</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.17.14.5" style="padding-left:5.5pt;padding-right:5.5pt;">0.280</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.17.14.6" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.17.14.6.1">0.325</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.3.17.14.7" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.17.14.7.1">0.320</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.17.14.8" style="padding-left:5.5pt;padding-right:5.5pt;">0.230</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.17.14.9" style="padding-left:5.5pt;padding-right:5.5pt;">0.306</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.17.14.10" style="padding-left:5.5pt;padding-right:5.5pt;">0.192</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.17.14.11" style="padding-left:5.5pt;padding-right:5.5pt;">0.265</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.17.14.12" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.17.14.12.1">0.253</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.17.14.13" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.17.14.13.1">0.320</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.18.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.3.18.15.1" style="padding-left:5.5pt;padding-right:5.5pt;">comparison</th>
<td class="ltx_td ltx_align_right" id="S4.T2.3.18.15.2" style="padding-left:5.5pt;padding-right:5.5pt;">0.167</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.18.15.3" style="padding-left:5.5pt;padding-right:5.5pt;">0.174</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.18.15.4" style="padding-left:5.5pt;padding-right:5.5pt;">0.139</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.18.15.5" style="padding-left:5.5pt;padding-right:5.5pt;">0.141</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.18.15.6" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.18.15.6.1">0.172</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.3.18.15.7" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.18.15.7.1">0.173</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.18.15.8" style="padding-left:5.5pt;padding-right:5.5pt;">0.128</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.18.15.9" style="padding-left:5.5pt;padding-right:5.5pt;">0.157</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.18.15.10" style="padding-left:5.5pt;padding-right:5.5pt;">0.098</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.18.15.11" style="padding-left:5.5pt;padding-right:5.5pt;">0.119</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.18.15.12" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.18.15.12.1">0.134</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.18.15.13" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.18.15.13.1">0.156</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.19.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.3.19.16.1" style="padding-left:5.5pt;padding-right:5.5pt;">evaluation</th>
<td class="ltx_td ltx_align_right" id="S4.T2.3.19.16.2" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.19.16.2.1">0.272</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.19.16.3" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.19.16.3.1">0.254</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.19.16.4" style="padding-left:5.5pt;padding-right:5.5pt;">0.217</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.19.16.5" style="padding-left:5.5pt;padding-right:5.5pt;">0.218</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.19.16.6" style="padding-left:5.5pt;padding-right:5.5pt;">0.257</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.3.19.16.7" style="padding-left:5.5pt;padding-right:5.5pt;">0.263</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.19.16.8" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.19.16.8.1">0.226</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.19.16.9" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.19.16.9.1">0.252</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.19.16.10" style="padding-left:5.5pt;padding-right:5.5pt;">0.171</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.19.16.11" style="padding-left:5.5pt;padding-right:5.5pt;">0.229</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.19.16.12" style="padding-left:5.5pt;padding-right:5.5pt;">0.209</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.19.16.13" style="padding-left:5.5pt;padding-right:5.5pt;">0.266</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.20.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.3.20.17.1" style="padding-left:5.5pt;padding-right:5.5pt;">rhetorical</th>
<td class="ltx_td ltx_align_right" id="S4.T2.3.20.17.2" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.20.17.2.1">0.192</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.20.17.3" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.20.17.3.1">0.182</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.20.17.4" style="padding-left:5.5pt;padding-right:5.5pt;">0.133</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.20.17.5" style="padding-left:5.5pt;padding-right:5.5pt;">0.126</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.20.17.6" style="padding-left:5.5pt;padding-right:5.5pt;">0.183</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.3.20.17.7" style="padding-left:5.5pt;padding-right:5.5pt;">0.175</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.20.17.8" style="padding-left:5.5pt;padding-right:5.5pt;">0.156</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.20.17.9" style="padding-left:5.5pt;padding-right:5.5pt;">0.180</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.20.17.10" style="padding-left:5.5pt;padding-right:5.5pt;">0.100</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.20.17.11" style="padding-left:5.5pt;padding-right:5.5pt;">0.136</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.20.17.12" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.20.17.12.1">0.160</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.20.17.13" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.20.17.13.1">0.176</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.21.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_b ltx_border_t" id="S4.T2.3.21.18.1" rowspan="3" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text" id="S4.T2.3.21.18.1.1">Conclusion</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.3.21.18.2" style="padding-left:5.5pt;padding-right:5.5pt;">comparison</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.21.18.3" style="padding-left:5.5pt;padding-right:5.5pt;">0.048</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.21.18.4" style="padding-left:5.5pt;padding-right:5.5pt;">0.051</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.21.18.5" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.21.18.5.1">0.059</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.21.18.6" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.21.18.6.1">0.065</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.21.18.7" style="padding-left:5.5pt;padding-right:5.5pt;">0.055</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S4.T2.3.21.18.8" style="padding-left:5.5pt;padding-right:5.5pt;">0.058</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.21.18.9" style="padding-left:5.5pt;padding-right:5.5pt;">0.045</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.21.18.10" style="padding-left:5.5pt;padding-right:5.5pt;">0.050</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.21.18.11" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.21.18.11.1">0.053</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.21.18.12" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.21.18.12.1">0.059</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.21.18.13" style="padding-left:5.5pt;padding-right:5.5pt;">0.050</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.3.21.18.14" style="padding-left:5.5pt;padding-right:5.5pt;">0.058</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.22.19">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.3.22.19.1" style="padding-left:5.5pt;padding-right:5.5pt;">evaluation</th>
<td class="ltx_td ltx_align_right" id="S4.T2.3.22.19.2" style="padding-left:5.5pt;padding-right:5.5pt;">0.082</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.22.19.3" style="padding-left:5.5pt;padding-right:5.5pt;">0.079</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.22.19.4" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.22.19.4.1">0.100</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.22.19.5" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.22.19.5.1">0.103</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.22.19.6" style="padding-left:5.5pt;padding-right:5.5pt;">0.086</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.3.22.19.7" style="padding-left:5.5pt;padding-right:5.5pt;">0.089</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.22.19.8" style="padding-left:5.5pt;padding-right:5.5pt;">0.073</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.22.19.9" style="padding-left:5.5pt;padding-right:5.5pt;">0.081</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.22.19.10" style="padding-left:5.5pt;padding-right:5.5pt;">0.072</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.22.19.11" style="padding-left:5.5pt;padding-right:5.5pt;">0.084</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.22.19.12" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.22.19.12.1">0.078</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.22.19.13" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.22.19.13.1">0.081</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.23.20">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_b ltx_border_r" id="S4.T2.3.23.20.1" style="padding-left:5.5pt;padding-right:5.5pt;">rhetorical</th>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_b" id="S4.T2.3.23.20.2" style="padding-left:5.5pt;padding-right:5.5pt;">0.138</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_b" id="S4.T2.3.23.20.3" style="padding-left:5.5pt;padding-right:5.5pt;">0.141</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_b" id="S4.T2.3.23.20.4" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.23.20.4.1">0.178</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_b" id="S4.T2.3.23.20.5" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.23.20.5.1">0.171</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_b" id="S4.T2.3.23.20.6" style="padding-left:5.5pt;padding-right:5.5pt;">0.148</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_b ltx_border_r" id="S4.T2.3.23.20.7" style="padding-left:5.5pt;padding-right:5.5pt;">0.147</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_b" id="S4.T2.3.23.20.8" style="padding-left:5.5pt;padding-right:5.5pt;">0.126</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_b" id="S4.T2.3.23.20.9" style="padding-left:5.5pt;padding-right:5.5pt;">0.148</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_b" id="S4.T2.3.23.20.10" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.23.20.10.1">0.149</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_b" id="S4.T2.3.23.20.11" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.23.20.11.1">0.165</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_b" id="S4.T2.3.23.20.12" style="padding-left:5.5pt;padding-right:5.5pt;">0.133</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_b" id="S4.T2.3.23.20.13" style="padding-left:5.5pt;padding-right:5.5pt;">0.144</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Performance of the models on the WeQA benchmark scored using the RAGAS framework across evaluators. The "Prec." and "Rec." mean Context Precision and Context Recall respectively, while "Type" refers to the Question Type. The best performance for each question type per evaluator is highlighted in bold.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.p13">
<p class="ltx_p" id="S4.p13.1"><span class="ltx_text ltx_font_bold" id="S4.p13.1.1">Questions from tables.</span> 
Another important aspect of benchmarking <span class="ltx_glossaryref" title="">RAG</span> models in the domain of research articles and reports is to evaluate their performance in retrieving information from tables.
Tables are important contents inside research documents and often contain useful summaries of the entire documents.</p>
</div>
<div class="ltx_para" id="S4.p14">
<blockquote class="ltx_quote" id="S4.p14.1">
<p class="ltx_p" id="S4.p14.1.1">Generate {<em class="ltx_emph ltx_font_italic" id="S4.p14.1.1.1">num</em>} questions given the table provided in HTML format in the following paragraph?
Generate the questions keeping in mind that the caption of the table is
“‘
{<em class="ltx_emph ltx_font_italic" id="S4.p14.1.1.2">Table caption obtained from document.</em>}
”’
Restrict the questions such that the answers are only from the provided table in the html format.
For each question, return 3 lines: question/ answer/ proof.
Make sure there are no newline characters in the proof.</p>
<p class="ltx_p" id="S4.p14.1.2">Input table:
<br class="ltx_break"/>“‘
{<em class="ltx_emph ltx_font_italic" id="S4.p14.1.2.1">Table in HTML format extracted from document</em>}
”’</p>
</blockquote>
</div>
<div class="ltx_para" id="S4.p15">
<p class="ltx_p" id="S4.p15.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#S4.T1" title="Table 1 ‣ 4 Methodology ‣ WeQA: A Benchmark for Retrieval Augmented Generation in Wind Energy Domain"><span class="ltx_text ltx_ref_tag">1</span></a> shows a table from the report <cite class="ltx_cite ltx_citemacro_cite">Invenergy (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib14" title="">2014</a>)</cite> and we generate questions from this table as follows.</p>
<blockquote class="ltx_quote" id="S4.p15.2">
<p class="ltx_p" id="S4.p15.2.1"><span class="ltx_text ltx_font_bold" id="S4.p15.2.1.1">Question:</span> What is the acreage of Cultivated Crops within the Pleasant Ridge Project Area based on the National Land Cover Database in May of 2014?
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.p15.2.1.2">Answer:</span> The acreage of Cultivated Crops within the Pleasant Ridge Project Area is 55,946 acres.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.p15.2.1.3">Proof:</span> The table entry under the “Habitat” column for “Cultivated Crops” corresponds with the entry under the “Acres [Hectares]” column that reads “55,946[22,641]”</p>
</blockquote>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results and Discussion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We evaluate three <span class="ltx_glossaryref" title="">RAG</span>-based <span class="ltx_glossaryref" title="">LLM</span> s, namely GPT-4, Gemini, and Claude, on our WeQA benchmark. The <math alttext="\mathsf{RAGAS}" class="ltx_Math" display="inline" id="S5.p1.1.m1.1"><semantics id="S5.p1.1.m1.1a"><mi id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml">𝖱𝖠𝖦𝖠𝖲</mi><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><ci id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1">𝖱𝖠𝖦𝖠𝖲</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">\mathsf{RAGAS}</annotation><annotation encoding="application/x-llamapun" id="S5.p1.1.m1.1d">sansserif_RAGAS</annotation></semantics></math> framework is employed for this evaluation, utilizing an evaluator LLM to assess the models’ performance. The assessment includes metrics such as answer correctness, context precision, and context recall, providing a comprehensive understanding of each model’s capabilities in retrieving and generating accurate information from the given context. In our case, we have used GPT-4 and Gemini-1.5Pro as choices for the evaluator <span class="ltx_glossaryref" title="">LLM</span> s. Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#S5.F4" title="Figure 4 ‣ 5 Results and Discussion ‣ WeQA: A Benchmark for Retrieval Augmented Generation in Wind Energy Domain"><span class="ltx_text ltx_ref_tag">4</span></a> presents the answer correctness score, while context precision and context recall depicted in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#S4.T2" title="Table 2 ‣ 4 Methodology ‣ WeQA: A Benchmark for Retrieval Augmented Generation in Wind Energy Domain"><span class="ltx_text ltx_ref_tag">2</span></a> show the ability of the models to retrieve the context accurately.</p>
</div>
<div class="ltx_theorem ltx_theorem_observation" id="Thmobservation1">
<h6 class="ltx_title ltx_runin ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold" id="Thmobservation1.1.1.1">Observation 1</span></span></h6>
<div class="ltx_para" id="Thmobservation1.p1">
<p class="ltx_p" id="Thmobservation1.p1.1"><span class="ltx_text ltx_font_italic" id="Thmobservation1.p1.1.1">The observed answer correctness scores are notably low, indicating a robust and challenging benchmark.</span></p>
</div>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Specifically, "evaluation" and "comparison" type questions yield nearly zero answer correctness scores for all models, highlighting their difficulty in responding. Recall that, these challenging questions were crafted from summaries of text chunks rather than the text chunks themselves, further complicating the models’ ability to generate correct answers. This underscores the complexity and rigor of the benchmarking process, emphasizing the need for models to improve their understanding and contextual extraction capabilities.</p>
</div>
<div class="ltx_theorem ltx_theorem_observation" id="Thmobservation2">
<h6 class="ltx_title ltx_runin ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold" id="Thmobservation2.1.1.1">Observation 2</span></span></h6>
<div class="ltx_para" id="Thmobservation2.p1">
<p class="ltx_p" id="Thmobservation2.p1.1"><span class="ltx_text ltx_font_italic" id="Thmobservation2.p1.1.1">There is an alignment in evaluations made by the two evaluator <span class="ltx_glossaryref" title="">LLM</span> s used within the <math alttext="\mathsf{RAGAS}" class="ltx_Math" display="inline" id="Thmobservation2.p1.1.1.m1.1"><semantics id="Thmobservation2.p1.1.1.m1.1a"><mi id="Thmobservation2.p1.1.1.m1.1.1" xref="Thmobservation2.p1.1.1.m1.1.1.cmml">𝖱𝖠𝖦𝖠𝖲</mi><annotation-xml encoding="MathML-Content" id="Thmobservation2.p1.1.1.m1.1b"><ci id="Thmobservation2.p1.1.1.m1.1.1.cmml" xref="Thmobservation2.p1.1.1.m1.1.1">𝖱𝖠𝖦𝖠𝖲</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmobservation2.p1.1.1.m1.1c">\mathsf{RAGAS}</annotation><annotation encoding="application/x-llamapun" id="Thmobservation2.p1.1.1.m1.1d">sansserif_RAGAS</annotation></semantics></math> framework, particularly visible for ‘closed’ type questions.</span></p>
</div>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">This similarity arises because the answers to these questions are objective (‘yes’ or ‘no’), leading to equivalent correctness evaluations by both models.
Although there are some mismatches in the evaluations made by the two evaluator models, the number of these discrepancies is insignificant compared to the number of matching evaluations.</p>
</div>
<figure class="ltx_figure" id="S5.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="209" id="S5.F4.g1" src="extracted/5877533/images/correctness_introduction.png" width="281"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="209" id="S5.F4.g2" src="extracted/5877533/images/correctness_method.png" width="281"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="205" id="S5.F4.g3" src="extracted/5877533/images/correctness_result.png" width="281"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="205" id="S5.F4.g4" src="extracted/5877533/images/correctness_conclusion.png" width="281"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Answer correctness scores computed using the RAGAS scoring framework with GPT-4 and Gemini-1.5Pro as evaluator models for response generated by all three models used.</figcaption>
</figure>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#S5.F5" title="Figure 5 ‣ 5 Results and Discussion ‣ WeQA: A Benchmark for Retrieval Augmented Generation in Wind Energy Domain"><span class="ltx_text ltx_ref_tag">5</span></a> displays the confusion matrix illustrating the evaluations made by the two evaluator <span class="ltx_glossaryref" title="">LLM</span> s (GPT-4 and Gemini-1.5Pro) on the responses provided by the <span class="ltx_glossaryref" title="">RAG</span>-based Claude and GPT-4 models to the benchmarking questions.
In this context, a true positive occurs when the <span class="ltx_glossaryref" title="">LLM</span> evaluator correctly identifies the model response as matching the ground truth. Conversely, a false positive arises when the <span class="ltx_glossaryref" title="">LLM</span> evaluator incorrectly states that the model response matches the ground truth, while it does not. This matrix helps visualize the accuracy and reliability of the evaluations conducted by the <span class="ltx_glossaryref" title="">LLM</span> s, when used within the <math alttext="\mathsf{RAGAS}" class="ltx_Math" display="inline" id="S5.p4.1.m1.1"><semantics id="S5.p4.1.m1.1a"><mi id="S5.p4.1.m1.1.1" xref="S5.p4.1.m1.1.1.cmml">𝖱𝖠𝖦𝖠𝖲</mi><annotation-xml encoding="MathML-Content" id="S5.p4.1.m1.1b"><ci id="S5.p4.1.m1.1.1.cmml" xref="S5.p4.1.m1.1.1">𝖱𝖠𝖦𝖠𝖲</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.1.m1.1c">\mathsf{RAGAS}</annotation><annotation encoding="application/x-llamapun" id="S5.p4.1.m1.1d">sansserif_RAGAS</annotation></semantics></math> framework.
We note that majority of evaluations made by either <span class="ltx_glossaryref" title="">LLM</span> evaluator matches the actual evaluation which indicates that both of them are reliable.</p>
</div>
<div class="ltx_theorem ltx_theorem_observation" id="Thmobservation3">
<h6 class="ltx_title ltx_runin ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold" id="Thmobservation3.1.1.1">Observation 3</span></span></h6>
<div class="ltx_para" id="Thmobservation3.p1">
<p class="ltx_p" id="Thmobservation3.p1.1"><span class="ltx_text ltx_font_italic" id="Thmobservation3.p1.1.1">Comparison between ‘closed’ and ‘open’ type questions within the same section reveals a higher answer correctness for responses to ‘open’ type questions than ‘closed’ type questions.</span></p>
</div>
</div>
<div class="ltx_para" id="S5.p5">
<p class="ltx_p" id="S5.p5.1">From this observation, we conclude that <span class="ltx_glossaryref" title="">RAG</span>-based models generate more accurate subjective responses to ‘open’ questions than objective (‘yes’ or ‘no’) responses for ‘closed’ questions. This suggests that these models perform better when tasked with generating detailed, context-rich answers rather than simple, binary ones, highlighting their strength in handling nuanced and complex queries.</p>
</div>
<figure class="ltx_figure" id="S5.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="123" id="S5.F5.g1" src="extracted/5877533/images/claude-confusion-matrix.png" width="287"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="123" id="S5.F5.g2" src="extracted/5877533/images/gpt4-confusion-matrix.png" width="287"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Confusion Matrix for evaluations by <span class="ltx_glossaryref" title="">LLM</span> evaluators on responses from Claude (top) and GPT-4 (bottom) models</figcaption>
</figure>
<div class="ltx_theorem ltx_theorem_observation" id="Thmobservation4">
<h6 class="ltx_title ltx_runin ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold" id="Thmobservation4.1.1.1">Observation 4</span></span></h6>
<div class="ltx_para" id="Thmobservation4.p1">
<p class="ltx_p" id="Thmobservation4.p1.1"><span class="ltx_text ltx_font_italic" id="Thmobservation4.p1.1.1">The answer correctness scores for questions derived from the “Introduction” section are higher compared to those from other sections.</span></p>
</div>
</div>
<div class="ltx_para" id="S5.p6">
<p class="ltx_p" id="S5.p6.1">This is because the “introduction” section is typically longer, more similar to other documents, and often includes a related works section, which aligns closely with content found in many other documents.
As a result, the <span class="ltx_glossaryref" title="">RAG</span>-based <span class="ltx_glossaryref" title="">LLM</span> s can more easily extract relevant information to answer questions accurately, leading to higher correctness scores.
Additionally, the content in the “introduction” section is primarily text-based, unlike other sections which contain equations, tables, and figures. Therefore, the models provide more accurate responses to questions from the “introduction” section compared to those from other sections.</p>
</div>
<figure class="ltx_figure" id="S5.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="182" id="S5.F6.g1" src="extracted/5877533/images/table_answer_correctness.png" width="287"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="182" id="S5.F6.g2" src="extracted/5877533/images/table_answer_similarity.png" width="287"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="182" id="S5.F6.g3" src="extracted/5877533/images/table_context_precision.png" width="287"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="182" id="S5.F6.g4" src="extracted/5877533/images/table_context_recall.png" width="287"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Answer correctness (top left), answer similarity (top right), context precision (bottom left) and recall (bottom right) scores across different evaluator and generator models.</figcaption>
</figure>
<div class="ltx_theorem ltx_theorem_observation" id="Thmobservation5">
<h6 class="ltx_title ltx_runin ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold" id="Thmobservation5.1.1.1">Observation 5</span></span></h6>
<div class="ltx_para" id="Thmobservation5.p1">
<p class="ltx_p" id="Thmobservation5.p1.1"><span class="ltx_text ltx_font_italic" id="Thmobservation5.p1.1.1">The answer correctness scores for ‘rhetorical’ questions are lower than those for other question types.</span></p>
</div>
</div>
<div class="ltx_para" id="S5.p7">
<p class="ltx_p" id="S5.p7.1">This is because ‘rhetorical’ questions lack definite answers in the documents, making it challenging for the models to retrieve the appropriate context and provide correct responses.
The absence of clear, concrete answers in the source material complicates the models’ ability to generate accurate and relevant responses, leading to lower correctness scores for this question type.</p>
</div>
<div class="ltx_theorem ltx_theorem_observation" id="Thmobservation6">
<h6 class="ltx_title ltx_runin ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold" id="Thmobservation6.1.1.1">Observation 6</span></span></h6>
<div class="ltx_para" id="Thmobservation6.p1">
<p class="ltx_p" id="Thmobservation6.p1.1"><span class="ltx_text ltx_font_italic" id="Thmobservation6.p1.1.1">Evaluations made by Gemini-1.5Pro on the responses generated by all three <span class="ltx_glossaryref" title="">LLM</span> s are higher than the evaluations made by GPT-4, with the responses from Gemini <span class="ltx_glossaryref" title="">LLM</span> receiving significantly higher scores.</span></p>
</div>
</div>
<div class="ltx_para" id="S5.p8">
<p class="ltx_p" id="S5.p8.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#S5.F6" title="Figure 6 ‣ 5 Results and Discussion ‣ WeQA: A Benchmark for Retrieval Augmented Generation in Wind Energy Domain"><span class="ltx_text ltx_ref_tag">6</span></a> shows the scores computed by the evaluators for the responses generated by the three <span class="ltx_glossaryref" title="">RAG</span>-based <span class="ltx_glossaryref" title="">LLM</span> s. The Gemini-1.5Pro evaluator tends to rate high scores even when the <span class="ltx_glossaryref" title="">LLM</span> s refuse to answer. An example is listed below:</p>
</div>
<div class="ltx_para" id="S5.p9">
<blockquote class="ltx_quote" id="S5.p9.3">
<p class="ltx_p" id="S5.p9.3.3"><span class="ltx_text ltx_font_bold" id="S5.p9.3.3.1">Question:</span> In the HTML table that estimates the annual number of bird collisions at different percentages of avoidance, what is the estimated number of collisions per year at <math alttext="98.0\%" class="ltx_Math" display="inline" id="S5.p9.1.1.m1.1"><semantics id="S5.p9.1.1.m1.1a"><mrow id="S5.p9.1.1.m1.1.1" xref="S5.p9.1.1.m1.1.1.cmml"><mn id="S5.p9.1.1.m1.1.1.2" xref="S5.p9.1.1.m1.1.1.2.cmml">98.0</mn><mo id="S5.p9.1.1.m1.1.1.1" xref="S5.p9.1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.p9.1.1.m1.1b"><apply id="S5.p9.1.1.m1.1.1.cmml" xref="S5.p9.1.1.m1.1.1"><csymbol cd="latexml" id="S5.p9.1.1.m1.1.1.1.cmml" xref="S5.p9.1.1.m1.1.1.1">percent</csymbol><cn id="S5.p9.1.1.m1.1.1.2.cmml" type="float" xref="S5.p9.1.1.m1.1.1.2">98.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p9.1.1.m1.1c">98.0\%</annotation><annotation encoding="application/x-llamapun" id="S5.p9.1.1.m1.1d">98.0 %</annotation></semantics></math> avoidance?
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S5.p9.3.3.2">Expected answer:</span> The estimated number of collisions per year at <math alttext="98.0\%" class="ltx_Math" display="inline" id="S5.p9.2.2.m2.1"><semantics id="S5.p9.2.2.m2.1a"><mrow id="S5.p9.2.2.m2.1.1" xref="S5.p9.2.2.m2.1.1.cmml"><mn id="S5.p9.2.2.m2.1.1.2" xref="S5.p9.2.2.m2.1.1.2.cmml">98.0</mn><mo id="S5.p9.2.2.m2.1.1.1" xref="S5.p9.2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.p9.2.2.m2.1b"><apply id="S5.p9.2.2.m2.1.1.cmml" xref="S5.p9.2.2.m2.1.1"><csymbol cd="latexml" id="S5.p9.2.2.m2.1.1.1.cmml" xref="S5.p9.2.2.m2.1.1.1">percent</csymbol><cn id="S5.p9.2.2.m2.1.1.2.cmml" type="float" xref="S5.p9.2.2.m2.1.1.2">98.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p9.2.2.m2.1c">98.0\%</annotation><annotation encoding="application/x-llamapun" id="S5.p9.2.2.m2.1d">98.0 %</annotation></semantics></math> avoidance is <math alttext="152" class="ltx_Math" display="inline" id="S5.p9.3.3.m3.1"><semantics id="S5.p9.3.3.m3.1a"><mn id="S5.p9.3.3.m3.1.1" xref="S5.p9.3.3.m3.1.1.cmml">152</mn><annotation-xml encoding="MathML-Content" id="S5.p9.3.3.m3.1b"><cn id="S5.p9.3.3.m3.1.1.cmml" type="integer" xref="S5.p9.3.3.m3.1.1">152</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p9.3.3.m3.1c">152</annotation><annotation encoding="application/x-llamapun" id="S5.p9.3.3.m3.1d">152</annotation></semantics></math>.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S5.p9.3.3.3">Gemini generated answer:</span> This question cannot be answered from the given source. While the text discusses bird collisions with wind turbines, it does not provide any data on estimated collision numbers at different avoidance rates.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S5.p9.3.3.4">GPT-4 score:</span> 0.194
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S5.p9.3.3.5">Gemini-1.5Pro score</span> 0.813</p>
</blockquote>
</div>
<div class="ltx_para" id="S5.p10">
<p class="ltx_p" id="S5.p10.1">Conversely, in instances where the LLMs generate correct answers, Gemini-1.5Pro has been observed to evaluate them as incorrect. An example is detailed below:</p>
</div>
<div class="ltx_para" id="S5.p11">
<blockquote class="ltx_quote" id="S5.p11.1">
<p class="ltx_p" id="S5.p11.1.1"><span class="ltx_text ltx_font_bold" id="S5.p11.1.1.1">Question:</span> Who is the GIS Technician in the ‘STUDY PARTICIPANTS’ table?
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S5.p11.1.1.2">Expected answer:</span> JR Boehrs
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S5.p11.1.1.3">Gemini generated answer:</span> Saif Nomani JR Boehrs was the GIS Technician.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S5.p11.1.1.4">GPT-4 score:</span> 0.703
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S5.p11.1.1.5">Gemini-1.5Pro score:</span> 0.200</p>
</blockquote>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In conclusion, this paper presents a versatile framework for evaluating the performance of <span class="ltx_glossaryref" title="">RAG</span>-based <span class="ltx_glossaryref" title="">LLM</span> s across various question types and document sections. We showcase this by introducing a hybrid, automated question-generation method that ensures comprehensive coverage of both objective and subjective queries, and implement this for the use case of wind energy related document and present the WeQA benchmark, which is a first of its kind benchmark in wind energy domain. However, the usefulness of our work goes beyond this niche domain as our approach is domain-agnostic, meaning it can be used for creating benchmark for any domain. Additionally, our use of the RAGAS scoring framework comes with multiple benefits; it allows for a thorough evaluation of model performance, offering a holistic assessment of LLM capabilities, while also having the advantage of being easy for other researchers to adapt this approach for their own work.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Limitations</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">A limitation of the proposed framework is that the automatic method of generating questions often produces queries that are too specific to the document from which they were derived.
When these questions are posed to an <span class="ltx_glossaryref" title="">LLM</span> with a large document corpus, the model may struggle to respond accurately, necessitating the filtering of ambiguous questions to ensure relevance and clarity.
Additionally, the <math alttext="\mathsf{RAGAS}" class="ltx_Math" display="inline" id="S7.p1.1.m1.1"><semantics id="S7.p1.1.m1.1a"><mi id="S7.p1.1.m1.1.1" xref="S7.p1.1.m1.1.1.cmml">𝖱𝖠𝖦𝖠𝖲</mi><annotation-xml encoding="MathML-Content" id="S7.p1.1.m1.1b"><ci id="S7.p1.1.m1.1.1.cmml" xref="S7.p1.1.m1.1.1">𝖱𝖠𝖦𝖠𝖲</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.1.m1.1c">\mathsf{RAGAS}</annotation><annotation encoding="application/x-llamapun" id="S7.p1.1.m1.1d">sansserif_RAGAS</annotation></semantics></math> scoring framework, which relies on <span class="ltx_glossaryref" title="">LLM</span> s as evaluators, introduces uncertainty in performance metrics, as different <span class="ltx_glossaryref" title="">LLM</span> evaluators may score responses differently.
While comparisons can be made for questions with objective responses, evaluating and comparing subjective responses across different <span class="ltx_glossaryref" title="">LLM</span> s remains challenging and less consistent.</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Ethical Considerations</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">While we do not anticipate the novel work presented here to introduce new ethical concerns in and by themselves, we do recognize that there may also be pre-existing concerns and issues of the data, models, and methodologies we have used for this paper. We acknowledge that researchers should not “simply assume that […] research will have a net positive impact on the world” <cite class="ltx_cite ltx_citemacro_cite">Hecht et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib12" title="">2021</a>)</cite>. In particular, it has been seen that Large Language Models (LLMs), like the ones used in this work, exhibit a wide variety of bias – <span class="ltx_text ltx_font_italic" id="S8.p1.1.1">e.g., </span>religious, gender, race, profession, and cultural – and frequently generate answers that are incorrect, misogynistic, antisemitic, and generally toxic <cite class="ltx_cite ltx_citemacro_cite">Abid et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib1" title="">2021</a>); Buolamwini and Gebru (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib6" title="">2018</a>); Liang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib19" title="">2021</a>); Nadeem et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib23" title="">2021</a>); Welbl et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11800v2#bib.bib39" title="">2021</a>)</cite>.
However, when used within the parameters of our experiments detailed in this paper, we did not see such behaviour from any of the models. To our knowledge, when used as intended, our models do not pose additional ethical concerns than any other LLM.</p>
</div>
</section>
<section class="ltx_section" id="S9">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span>Acknowledgment</h2>
<div class="ltx_para" id="S9.p1">
<p class="ltx_p" id="S9.p1.1">This research was sponsored by US DOE Wind Energy Technologies Office (WETO) and was done at Pacific Northwest National Laboratory, a multi-program national laboratory operated by Battelle for the U.S. Department of Energy under contract DE-AC05-76RLO1830. This article has been cleared by PNNL for public release as PNNL-SA-203893.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abid et al. (2021)</span>
<span class="ltx_bibblock">
Abubakar Abid, Maheen Farooqi, and James Zou. 2021.

</span>
<span class="ltx_bibblock">Persistent anti-muslim bias in large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics,
and Society</em>, pages 298–306.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Acharya et al. (2023)</span>
<span class="ltx_bibblock">
Anurag Acharya, Sai Munikoti, Aaron Hellinger, Sara Smith, Sridevi Wagle, and
Sameera Horawalavithana. 2023.

</span>
<span class="ltx_bibblock">Nuclearqa: A human-made benchmark for language models for the nuclear
domain.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2310.10920</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Acharya et al. (2021)</span>
<span class="ltx_bibblock">
Anurag Acharya, Kartik Talamadupula, and Mark A Finlayson. 2021.

</span>
<span class="ltx_bibblock">Towards an atlas of cultural commonsense for machine reasoning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Workshop on Common Sense Knowledge Graphs (CSKGs) @ AAAI
Conference on Artificial Intelligence</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Auer et al. (2023)</span>
<span class="ltx_bibblock">
Sören Auer, Dante AC Barone, Cassiano Bartz, Eduardo G Cortes,
Mohamad Yaser Jaradeh, Oliver Karras, Manolis Koubarakis, Dmitry Mouromtsev,
Dmitrii Pliukhin, Daniil Radyush, et al. 2023.

</span>
<span class="ltx_bibblock">The sciqa scientific question answering benchmark for scholarly
knowledge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Scientific Reports</em>, 13(1):7240.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bhakthavatsalam et al. (2021)</span>
<span class="ltx_bibblock">
Sumithra Bhakthavatsalam, Daniel Khashabi, Tushar Khot, Bhavana Dalvi Mishra,
Kyle Richardson, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, and
Peter Clark. 2021.

</span>
<span class="ltx_bibblock">Think you have solved direct-answer question answering? try arc-da,
the direct-answer ai2 reasoning challenge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:2102.03315</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Buolamwini and Gebru (2018)</span>
<span class="ltx_bibblock">
Joy Buolamwini and Timnit Gebru. 2018.

</span>
<span class="ltx_bibblock">Gender shades: Intersectional accuracy disparities in commercial
gender classification.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Conference on fairness, accountability and transparency</em>,
pages 77–91. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023a)</span>
<span class="ltx_bibblock">
Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2309.01431" title="">Benchmarking large language
models in retrieval-augmented generation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Preprint</em>, arXiv:2309.01431.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023b)</span>
<span class="ltx_bibblock">
Wenhu Chen, Ming Yin, Max Ku, Elaine Wan, Xueguang Ma, Jianyu Xu, Tony Xia,
Xinyi Wang, and Pan Lu. 2023b.

</span>
<span class="ltx_bibblock">Theoremqa: A theorem-driven question answering dataset.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2305.12524</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et al. (2018)</span>
<span class="ltx_bibblock">
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa
Schoenick, and Oyvind Tafjord. 2018.

</span>
<span class="ltx_bibblock">Think you have solved question answering? try arc, the ai2 reasoning
challenge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:1803.05457</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dunn et al. (2017)</span>
<span class="ltx_bibblock">
Matthew Dunn, Levent Sagun, Mike Higgins, V Ugur Guney, Volkan Cirik, and
Kyunghyun Cho. 2017.

</span>
<span class="ltx_bibblock">Searchqa: A new q&amp;a dataset augmented with context from a search
engine.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:1704.05179</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2024)</span>
<span class="ltx_bibblock">
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai,
Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2312.10997" title="">Retrieval-augmented
generation for large language models: A survey</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Preprint</em>, arXiv:2312.10997.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hecht et al. (2021)</span>
<span class="ltx_bibblock">
Brent Hecht, Lauren Wilcox, Jeffrey P Bigham, Johannes Schöning, Ehsan
Hoque, Jason Ernst, Yonatan Bisk, Luigi De Russis, Lana Yarosh, Bushra Anjum,
et al. 2021.

</span>
<span class="ltx_bibblock">It’s time to do something: Mitigating the negative impacts of
computing through a change to the peer review process.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2112.09544</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et al. (2020)</span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn
Song, and Jacob Steinhardt. 2020.

</span>
<span class="ltx_bibblock">Measuring massive multitask language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2009.03300</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Invenergy (2014)</span>
<span class="ltx_bibblock">
Invenergy. 2014.

</span>
<span class="ltx_bibblock">Bird and bat conservation strategy for Invenergy’s pleasant ridge
wind project.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain et al. (2020)</span>
<span class="ltx_bibblock">
Ayush Jain, Dr NM Meenachi, and Dr B Venkatraman. 2020.

</span>
<span class="ltx_bibblock">Nukebert: A pre-trained language model for low resource nuclear
domain.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">arXiv preprint arXiv:2003.13821</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi et al. (2017)</span>
<span class="ltx_bibblock">
Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017.

</span>
<span class="ltx_bibblock">Triviaqa: A large scale distantly supervised challenge dataset for
reading comprehension.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:1705.03551</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2023)</span>
<span class="ltx_bibblock">
Yoonjoo Lee, Kyungjae Lee, Sunghyun Park, Dasol Hwang, Jaehyeon Kim, Hong-in
Lee, and Moontae Lee. 2023.

</span>
<span class="ltx_bibblock">Qasa: advanced question answering on scientific articles.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the 40th International Conference on Machine
Learning</em>, ICML‘23. JMLR.org.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2021)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim
Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2005.11401" title="">Retrieval-augmented
generation for knowledge-intensive nlp tasks</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Preprint</em>, arXiv:2005.11401.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al. (2021)</span>
<span class="ltx_bibblock">
Paul Pu Liang, Chiyu Wu, Louis-Philippe Morency, and Ruslan Salakhutdinov.
2021.

</span>
<span class="ltx_bibblock">Towards understanding and mitigating social biases in language
models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">International Conference on Machine Learning</em>, pages
6565–6576. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu et al. (2024)</span>
<span class="ltx_bibblock">
Yuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong, Bo Tang, Wenjin Wang, Hao Wu,
Huanyong Liu, Tong Xu, Enhong Chen, Yi Luo, Peng Cheng, Haiying Deng,
Zhonghao Wang, and Zijia Lu. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2401.17043" title="">CRUD-RAG: A Comprehensive
Chinese Benchmark for Retrieval-Augmented Generation of Large Language
Models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Preprint</em>, arXiv:2401.17043.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Munikoti et al. (2024a)</span>
<span class="ltx_bibblock">
Sai Munikoti, Anurag Acharya, Sridevi Wagle, and Sameera Horawalavithana.
2024a.

</span>
<span class="ltx_bibblock">Atlantic: Structure-aware retrieval-augmented language model for
interdisciplinary science.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Workshop on AI to Accelerate Science and Engineering, The
Thirty-Eighth Annual AAAI Conference on Artificial Intelligence</em>, volume 3.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Munikoti et al. (2024b)</span>
<span class="ltx_bibblock">
Sai Munikoti, Anurag Acharya, Sridevi Wagle, and Sameera Horawalavithana.
2024b.

</span>
<span class="ltx_bibblock">Evaluating the effectiveness of retrieval-augmented large language
models in scientific document reasoning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the 4th Workshop on Scholarly Document
Processing @ ACL 2024</em>. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nadeem et al. (2021)</span>
<span class="ltx_bibblock">
Moin Nadeem, Anna Bethke, and Siva Reddy. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.acl-long.416" title="">StereoSet:
Measuring stereotypical bias in pretrained language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</em>, pages 5356–5371,
Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pampari et al. (2018)</span>
<span class="ltx_bibblock">
Anusri Pampari, Preethi Raghavan, Jennifer Liang, and Jian Peng. 2018.

</span>
<span class="ltx_bibblock">emrqa: A large corpus for question answering on electronic medical
records.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:1809.00732</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pappas et al. (2018)</span>
<span class="ltx_bibblock">
Dimitris Pappas, Ion Androutsopoulos, and Harris Papageorgiou. 2018.

</span>
<span class="ltx_bibblock">Bioread: A new dataset for biomedical reading comprehension.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the Eleventh International Conference on
Language Resources and Evaluation (LREC 2018)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pappas et al. (2020)</span>
<span class="ltx_bibblock">
Dimitris Pappas, Petros Stavropoulos, Ion Androutsopoulos, and Ryan McDonald.
2020.

</span>
<span class="ltx_bibblock">Biomrc: A dataset for biomedical machine reading comprehension.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the 19th SIGBioMed Workshop on Biomedical
Language Processing</em>, pages 140–149.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Phan et al. (2023)</span>
<span class="ltx_bibblock">
Hung Phan, Anurag Acharya, Sarthak Chaturvedi, Shivam Sharma, Mike Parker, Dan
Nally, Ali Jannesari, Karl Pazdernik, Mahantesh Halappanavar, Sai Munikoti,
et al. 2023.

</span>
<span class="ltx_bibblock">Rag vs. long context: Examining frontier large language models for
environmental review document comprehension.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2407.07321</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajpurkar et al. (2016)</span>
<span class="ltx_bibblock">
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.

</span>
<span class="ltx_bibblock">Squad: 100,000+ questions for machine comprehension of text.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">arXiv preprint arXiv:1606.05250</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ray (2023)</span>
<span class="ltx_bibblock">
Partha Pratim Ray. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.tbench.2023.100136" title="">Benchmarking,
ethical alignment, and evaluation framework for conversational ai: Advancing
responsible development of chatgpt</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">BenchCouncil Transactions on Benchmarks, Standards and
Evaluations</em>, 3(3):100136.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raymond (2023)</span>
<span class="ltx_bibblock">
Brian Raymond. 2023.

</span>
<span class="ltx_bibblock">UNSTRUCTURED.IO.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://unstructured.io/" title="">https://unstructured.io/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Richardson et al. (2013)</span>
<span class="ltx_bibblock">
Matthew Richardson, Christopher JC Burges, and Erin Renshaw. 2013.

</span>
<span class="ltx_bibblock">Mctest: A challenge dataset for the open-domain machine comprehension
of text.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the 2013 conference on empirical methods in
natural language processing</em>, pages 193–203.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sap et al. (2019)</span>
<span class="ltx_bibblock">
Maarten Sap, Ronan Le Bras, Emily Allaway, Chandra Bhagavatula, Nicholas
Lourie, Hannah Rashkin, Brendan Roof, Noah A Smith, and Yejin Choi. 2019.

</span>
<span class="ltx_bibblock">Atomic: An atlas of machine commonsense for if-then reasoning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Proceedings of the AAAI conference on artificial
intelligence</em>, volume 33, pages 3027–3035.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et al. (2022)</span>
<span class="ltx_bibblock">
Amanpreet Singh, Mike D’Arcy, Arman Cohan, Doug Downey, and Sergey Feldman.
2022.

</span>
<span class="ltx_bibblock">Scirepeval: A multi-format benchmark for scientific document
representations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:2211.13308</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srivastava et al. (2022)</span>
<span class="ltx_bibblock">
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar
Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià
Garriga-Alonso, et al. 2022.

</span>
<span class="ltx_bibblock">Beyond the imitation game: Quantifying and extrapolating the
capabilities of language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:2206.04615</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Talmor et al. (2018)</span>
<span class="ltx_bibblock">
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2018.

</span>
<span class="ltx_bibblock">Commonsenseqa: A question answering challenge targeting commonsense
knowledge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">arXiv preprint arXiv:1811.00937</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trischler et al. (2016)</span>
<span class="ltx_bibblock">
Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni,
Philip Bachman, and Kaheer Suleman. 2016.

</span>
<span class="ltx_bibblock">Newsqa: A machine comprehension dataset.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:1611.09830</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2018)</span>
<span class="ltx_bibblock">
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R
Bowman. 2018.

</span>
<span class="ltx_bibblock">Glue: A multi-task benchmark and analysis platform for natural
language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:1804.07461</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023)</span>
<span class="ltx_bibblock">
Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam,
Arjun R Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. 2023.

</span>
<span class="ltx_bibblock">Scibench: Evaluating college-level scientific problem-solving
abilities of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">arXiv preprint arXiv:2307.10635</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Welbl et al. (2021)</span>
<span class="ltx_bibblock">
Johannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor,
Lisa Anne Hendricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin, and Po-Sen
Huang. 2021.

</span>
<span class="ltx_bibblock">Challenges in detoxifying language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">arXiv preprint arXiv:2109.07445</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Welbl et al. (2017)</span>
<span class="ltx_bibblock">
Johannes Welbl, Nelson F Liu, and Matt Gardner. 2017.

</span>
<span class="ltx_bibblock">Crowdsourcing multiple choice science questions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">arXiv preprint arXiv:1707.06209</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiong et al. (2024)</span>
<span class="ltx_bibblock">
Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2402.13178" title="">Benchmarking
retrieval-augmented generation for medicine</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Preprint</em>, arXiv:2402.13178.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024)</span>
<span class="ltx_bibblock">
Liang Zhang, Katherine Jijo, Spurthi Setty, Eden Chung, Fatima Javid, Natan
Vidra, and Tommy Clifford. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2402.01722" title="">Enhancing large language
model performance to answer questions and extract information more
accurately</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Preprint</em>, arXiv:2402.01722.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Sep 24 23:17:04 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
