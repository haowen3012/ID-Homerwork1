<!DOCTYPE html>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Financial Report Chunking for Effective Retrieval Augmented Generation</title>
<!--Generated on Sat Mar 16 09:11:35 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Retrieval Augmented Generation Document Chunking Document Pre-Processing Financial Domain Large Language Models" lang="en" name="keywords"/>
<base href="/html/2402.05131v3/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#S1" title="1 Introduction ‣ Financial Report Chunking for Effective Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#S2" title="2 Related work ‣ Financial Report Chunking for Effective Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#S3" title="3 Methods ‣ Financial Report Chunking for Effective Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#S3.SS1" title="3.1 RAG setting for the experiments ‣ 3 Methods ‣ Financial Report Chunking for Effective Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>RAG setting for the experiments</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#S3.SS2" title="3.2 Indexing and retrieval ‣ 3 Methods ‣ Financial Report Chunking for Effective Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Indexing and retrieval</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#S3.SS3" title="3.3 Generation ‣ 3 Methods ‣ Financial Report Chunking for Effective Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#S3.SS4" title="3.4 Chunking ‣ 3 Methods ‣ Financial Report Chunking for Effective Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Chunking</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#S3.SS5" title="3.5 Dataset ‣ 3 Methods ‣ Financial Report Chunking for Effective Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Dataset</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#S4" title="4 Results ‣ Financial Report Chunking for Effective Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#S4.SS0.SSS1" title="4.0.1 Chunking Efficiency ‣ 4 Results ‣ Financial Report Chunking for Effective Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.0.1 </span>Chunking Efficiency</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#S4.SS0.SSS2" title="4.0.2 Retrieval Accuracy ‣ 4 Results ‣ Financial Report Chunking for Effective Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.0.2 </span>Retrieval Accuracy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#S4.SS0.SSS3" title="4.0.3 Q&amp;A Accuracy ‣ 4 Results ‣ Financial Report Chunking for Effective Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.0.3 </span>Q&amp;A Accuracy</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#S5" title="5 Discussion ‣ Financial Report Chunking for Effective Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#S6" title="6 Conclusions and Future Work ‣ Financial Report Chunking for Effective Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusions and Future Work</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content"><div class="section" id="target-section"><div id="license-tr">License: CC BY-NC-ND 4.0</div><div id="watermark-tr">arXiv:2402.05131v3 [cs.CL] 16 Mar 2024</div></div>
<article class="ltx_document ltx_authors_1line"><span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Unstructured Technologies
<br class="ltx_break"/>Sacramento, CA, USA 
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id1.1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>leah@unstructured.io</span></span></span>
<br class="ltx_break"/><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://unstructured.io" title="">https://unstructured.io</a></span></span></span>
<h1 class="ltx_title ltx_title_document">Financial Report Chunking for Effective Retrieval Augmented Generation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Antonio Jimeno Yepes
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yao You
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jan Milczek
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sebastian Laverde
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Leah Li
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Chunking information is a key step in Retrieval Augmented Generation (RAG). Current research primarily centers on paragraph-level chunking. This approach treats all texts as equal and neglects the information contained in the structure of documents. We propose an expanded approach to chunk documents by moving beyond mere paragraph-level chunking to chunk primary by structural element components of documents. Dissecting documents into these constituent elements creates a new way to chunk documents that yields the best chunk size without tuning. We introduce a novel framework that evaluates how chunking based on element types annotated by document understanding models contributes to the overall context and accuracy of the information retrieved. We also demonstrate how this approach impacts RAG assisted Question &amp; Answer task performance. Our research includes a comprehensive analysis of various element types, their role in effective information retrieval, and the impact they have on the quality of RAG outputs. Findings support that element type based chunking largely improve RAG results on financial reporting. Through this research, we are also able to answer how to uncover highly accurate RAG.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Retrieval Augmented Generation Document Chunking Document Pre-Processing Financial Domain Large Language Models
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Existing approaches for document understanding use a combination of methods from the computer vision and natural language processing domains to identify the different components in a document. In the rapidly evolving landscape of artificial intelligence, the capability to effectively process unstructured data is becoming increasingly critical. Large Language Models (LLMs) like GPT-4 have revolutionized natural language understanding and generation, as evidenced by their prompt-based functionalities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib31" title="">31</a>]</cite>, enabling a wide range of applications <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib5" title="">5</a>]</cite>. However, the efficacy of these models is often constrained by their reliance on the size and quality of the data they process. A notable limitation is the restricted contextual window of LLMs, which hampers their ability to fully comprehend the contents of extensive documents
 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib18" title="">18</a>]</cite>.
By dissecting large volumes of text into smaller, more focused segments, LLMs can process each part with greater precision, ensuring a thorough understanding of each section. This segmented approach allows for meticulous analysis of unstructured data, enabling LLMs to construct a more comprehensive and coherent understanding of the entire document <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib41" title="">41</a>]</cite>. There remains a challenge in
ensuring factual accuracy and relevance in the generated responses, especially when dealing with complex or extensive information.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Recently, Retrieval Augmented Generation (RAG) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib12" title="">12</a>]</cite> has been developed to address the <em class="ltx_emph ltx_font_italic" id="S1.p2.1.1">hallucination</em> problem with LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib43" title="">43</a>]</cite> when recovering factual information directly from an LLM. In RAG, instead of answering a user query directly using an LLM, the user query is used to retrieve documents or segments from a corpus and the top retrieved documents or segments are used to generate the answer in conjunction with an LLM.
In this way, RAG constraints the answer to the set of retrieved documents.
RAGs have been used as well to answer questions from single documents <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib14" title="">14</a>]</cite>.
The documents are split into smaller parts or chunks, indexed by a retrieval system and recovered and processed depending on the user information need.
In a sense, this process allows answering questions about information in a single document, thus contributing to the set of techniques available for document understanding.
</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Since documents need to be <em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">chunked</em> for RAG processing, this raises the question about what is the best practice to chunk documents for effective RAG document understanding.
There are several dimensions to consider when deciding how to chunk a document, which includes the size of the chunks.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">The retrieval system in RAG can use traditional retrieval systems using bag-of-words methods or a vector database. If a vector database is used, then an embedding needs to be obtained from each chunk, thus the number of tokens in the chunk is relevant since the neural networks processing the chunks might have constraints on the number of tokens. As well, different chunk sizes might have undesirable retrieval results.
Since the most relevant retrieved chunks need to be processed by an LLM, the number of tokens in retrieved chunks might have an effect in the generation of the answer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib25" title="">25</a>]</cite>.
As we see, chunking is required for RAG systems and there are several advantages and disadvantages when considering how to chunk a document.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">In this work, we study specifically the chunking of U.S. Securities and Exchange Commission (SEC)<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sec.gov" title="">https://www.sec.gov</a></span></span></span> Financial Reports<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sec.gov/files/cf-frm.pdf" title="">https://www.sec.gov/files/cf-frm.pdf</a></span></span></span>, including 10-Ks, 10-Qs, and 8-Ks. This study plays a critical role in offering insights into the financial health and operational dynamics of public companies. These documents present unique challenges in terms of document processing and information extraction as they consist of varying sizes and layouts, and contain a variety of tabular information.
Previous work has evaluated the processing of these reports with simple chunking strategies (e.g., tokens), but we believe that a more effective use of these reports might be achieved by a better pre-processing of the documents and chunking configuration<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.cnbc.com/2023/12/19/gpt-and-other-ai-models-cant-analyze-an-sec-filing-researchers-find.html" title="">https://www.cnbc.com/2023/12/19/gpt-and-other-ai-models-cant-analyze-an-sec-filing-researchers-find.html</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib14" title="">14</a>]</cite>.
To the best of our knowledge, this is the first systematic study on chunking for document understanding and more specifically for processing financial reports.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">RAG is an innovative method that has emerged to enhance the performance of LLMs by incorporating external knowledge, thereby boosting their capabilities. This technique has undergone substantial research, examining various configurations and applications. Key research includes Gao et al.’s <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib12" title="">12</a>]</cite> detailed analysis of RAG configurations and their role in enhancing Natural Language Processing (NLP) tasks, reducing errors, and improving factual accuracy. Several context retrieval methods are proposed to dynamically retrieve documents to improve the coherence of generated outputs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib1" title="">1</a>]</cite>. Other research introduced advancements in RAG, including reasoning chain storage and optimization strategies for retrieval, respectively, broadening the scope and efficiency of RAG applications in LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib21" title="">21</a>]</cite>. More recent work has compared RAG vs LLM fine-tuning, and identified that applying both improves the performance of each individual method <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib2" title="">2</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Chunking has been identified as the key factor in the success of RAG, improving the relevance of retrieved content by ensuring accurate embedding of text with minimal noise. Various strategies have been developed for text subdivision, each with its unique approach. They can be summarized as follows: the <span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.p2.1.1">fixed size strategy</span> divides text into uniform segments, but it often overlooks the underlying textual structure. In contrast, the <span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.p2.1.2">recursive strategy</span> iteratively subdivides text using separators like punctuation marks, allowing it to adapt more fluidly to the content. The <span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.p2.1.3">contextual strategy</span> takes this a step further by employing NLP techniques such as sentence segmentation to represent the meaning in context. Lastly, the <span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.p2.1.4">hybrid strategy</span> combines different approaches, offering greater flexibility in handling diverse text types <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib34" title="">34</a>]</cite>. However, an area yet to be explored in RAG chunking based on element types (document structure), which involves analyzing the inherent structure of documents, such as headings, paragraphs, tables, to guide the chunking process. Although chunking by Markdown and LaTeX comes closer to addressing element types, it’s not the same in nature as a dedicated approach that directly considers document structure and element types for chunking, which could potentially yield more contextually relevant chunks.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Exploring the structure of financial reports is an exceptional area for establishing optimal principles for chunking. The intricate nature of document structures and contents has resulted in most of the work processing financial reports focusing on the identification of structural elements. Among previous work, we find El-Haj et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib10" title="">10</a>]</cite> and the FinTOC challenges <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib11" title="">11</a>]</cite> that have worked at the document structure level for UK and French financial reports.
Additionally, there is recent work that considers U.S. SEC reports, which includes DocLayNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib33" title="">33</a>]</cite> and more specifically with the report tables in FinTabNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib45" title="">45</a>]</cite>.
</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">On the side of financial models, there is work in sentiment analysis in finance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib37" title="">37</a>]</cite>, which includes the pre-training of specialised models such as FinBERT by Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib26" title="">26</a>]</cite>, which is a BERT based model pre-trained on large corpora including large collections of financial news collected from different sites and FinBERT by DeSola et al, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib9" title="">9</a>]</cite> trained on Wikipedia, BookCorpus and U.S. SEC data.
Additional models include BloombergGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib40" title="">40</a>]</cite>, FinGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib42" title="">42</a>]</cite> and Instruct-FinGPT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib44" title="">44</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">More advance datasets in the financial domain include FinQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib6" title="">6</a>]</cite>, LLMWare <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib27" title="">27</a>]</cite>, ConFIRM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib8" title="">8</a>]</cite> and TAT-QA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib46" title="">46</a>]</cite> among others <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib19" title="">19</a>]</cite> that have been prepared for retrieval and or Questions and Answering (Q&amp;A) tasks over snippets of financial data that includes tabular data, which has allowed methods on large language models to be tested on them <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib39" title="">39</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p6">
<p class="ltx_p" id="S2.p6.1">Most of the previous work has focused on understanding the layout of financial documents or understanding specific snippets of existing reports with different levels of complexity, but there has not been much research in understanding financial report documents, except some more recent work that includes FinanceBench <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib14" title="">14</a>]</cite>, in which a set of questions about the content of financial reports are proposed that includes the evidence snippet.</p>
</div>
<div class="ltx_para" id="S2.p7">
<p class="ltx_p" id="S2.p7.1">More specifically on document chunking methods for RAG, there are standard approaches being considered such as chunking text into spans of a given token length (e.g. 128 and 256) or chunking based on sentences. Open source projects already allow simple processing of documents (e.g. Unstructured<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://unstructured.io" title="">https://unstructured.io</a></span></span></span>, Llamaindex<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.llamaindex.ai" title="">https://www.llamaindex.ai</a></span></span></span> or Langchain <span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.langchain.com" title="">https://www.langchain.com</a></span></span></span>), without explicitly considering the table structure on which these chunking strategies are applied.</p>
</div>
<div class="ltx_para" id="S2.p8">
<p class="ltx_p" id="S2.p8.1">Even though different approaches are available, an exhaustive evaluation of chunking applied to RAG and specifically to financial reporting, except for some limited chunking analysis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib36" title="">36</a>]</cite>, is non-existent.
In our work, we compare a broad range of chunking approaches in addition to more simple ones and provide an analysis of the outcomes of different methods when asking questions about different aspects of the reports.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methods</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we present the chunking strategies that we have evaluated.
Before describing the chunking strategies, we present the RAG environment in which these strategies have been evaluated and the dataset used for evaluation.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>RAG setting for the experiments</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The RAG pipeline used to process a user question is presented in figure <a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#S3.F1" title="Figure 1 ‣ 3.1 RAG setting for the experiments ‣ 3 Methods ‣ Financial Report Chunking for Effective Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">1</span></a> and is a common instance of a RAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib12" title="">12</a>]</cite>.
Prior to answering any question about a given document, the document is split into chunks and the chunks are indexed into a vector database (vectordb).
When a question is sent to the RAG system, the top-k chunks most similar to the question are retrieved from the vector database and used to generate the answer using a large language model as generator.
In order to retrieve chunks from the vector database, the question is encoded into a vector that is compared to the vector previously generated from the chunks.
To prompt the generator, the question is converted into a set of instructions that instruct the LLM to find the answer within the top-k retrieved chunks.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1074" id="S3.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>RAG steps to answer a question about a document</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">In our experiments, we modify the way documents are <em class="ltx_emph ltx_font_italic" id="S3.SS1.p2.1.1">chunked</em> prior to being indexed in the vector database.
All other settings remain constant.
In the following sections, we describe in more detail each one of the components and processes used.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Indexing and retrieval</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We have used the open source system Weaviate<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://weaviate.io/developers/weaviate" title="">https://weaviate.io/developers/weaviate</a></span></span></span> as our vector database.
As encoder model, we have used a sentence transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib35" title="">35</a>]</cite> trained on over 256M questions and answers, which is available from the HuggingFace system<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-dot-v1" title="">https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-dot-v1</a></span></span></span>.
</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">As shown in figure <a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#S3.F2" title="Figure 2 ‣ 3.2 Indexing and retrieval ‣ 3 Methods ‣ Financial Report Chunking for Effective Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">2</span></a>, to index a document, first the document is split into chunks, then each chunk is processed by an encoder model and then indexed into the vector database.
Based on the chunking strategy a document will be split into a larger or smaller set of chunks.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1074" id="S3.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Indexing of document chunks into the vector database</figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">As shown in figure <a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#S3.F1" title="Figure 1 ‣ 3.1 RAG setting for the experiments ‣ 3 Methods ‣ Financial Report Chunking for Effective Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">1</span></a>, to retrieve chunks relevant to a question, the question is converted into a vector representation and the vector database returns a ranked list of chunks based on the similarity between question vector and the chunks in the database.
Weaviate implements an approximate nearest neighbours algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib28" title="">28</a>]</cite> as their retrieval approach, which supports fast retrieval with high accuracy.
In our experiments, we retrieve the top-10 chunks for each question.
</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Generation</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Once the vector database has retrieved the top-10 chunks based on a question, the generation module generates the answer.
To do so, a prompt based on the question and the retrieved chunks are provided to a large language model that generates the answer of the system.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">We have used GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib31" title="">31</a>]</cite> as the generator, which has shown best performance compared to earlier versions. As well, its performance was better compared to existing open source alternatives <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib22" title="">22</a>]</cite> such as Mixtral <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib16" title="">16</a>]</cite>.
We used the prompt presented in figure <a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#S3.F3" title="Figure 3 ‣ 3.3 Generation ‣ 3 Methods ‣ Financial Report Chunking for Effective Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">3</span></a> that we designed on another similar RAG implementation with different document types.
The prompt conditions the answer to the query and the chunks, referred to as <em class="ltx_emph ltx_font_italic" id="S3.SS3.p2.1.1">source</em>, and if the generator cannot answer it should return <em class="ltx_emph ltx_font_italic" id="S3.SS3.p2.1.2">No answer</em>.</p>
</div>
<figure class="ltx_figure" id="S3.F3">
<span class="ltx_inline-block ltx_align_center ltx_framed_rectangle" id="S3.F3.1" style="border-color: black;"><pre class="ltx_verbatim ltx_font_typewriter" id="S3.F3.1.1">
please answer the question below by referencing the list of sources
provided after the question; if the question can not be answered just
respond ’No answer’. The sources are listed after "Sources:".

Question: {query}

Sources: {key} - {source}
...
</pre>
</span>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Example prompt template used by the generator</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Chunking</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.2">As a baseline chunking method, we have split the documents into chunks of size <math alttext="n" class="ltx_Math" display="inline" id="S3.SS4.p1.1.m1.1"><semantics id="S3.SS4.p1.1.m1.1a"><mi id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><ci id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.1.m1.1d">italic_n</annotation></semantics></math> tokens (<math alttext="n\in\{128,256,512\}" class="ltx_Math" display="inline" id="S3.SS4.p1.2.m2.3"><semantics id="S3.SS4.p1.2.m2.3a"><mrow id="S3.SS4.p1.2.m2.3.4" xref="S3.SS4.p1.2.m2.3.4.cmml"><mi id="S3.SS4.p1.2.m2.3.4.2" xref="S3.SS4.p1.2.m2.3.4.2.cmml">n</mi><mo id="S3.SS4.p1.2.m2.3.4.1" xref="S3.SS4.p1.2.m2.3.4.1.cmml">∈</mo><mrow id="S3.SS4.p1.2.m2.3.4.3.2" xref="S3.SS4.p1.2.m2.3.4.3.1.cmml"><mo id="S3.SS4.p1.2.m2.3.4.3.2.1" stretchy="false" xref="S3.SS4.p1.2.m2.3.4.3.1.cmml">{</mo><mn id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml">128</mn><mo id="S3.SS4.p1.2.m2.3.4.3.2.2" xref="S3.SS4.p1.2.m2.3.4.3.1.cmml">,</mo><mn id="S3.SS4.p1.2.m2.2.2" xref="S3.SS4.p1.2.m2.2.2.cmml">256</mn><mo id="S3.SS4.p1.2.m2.3.4.3.2.3" xref="S3.SS4.p1.2.m2.3.4.3.1.cmml">,</mo><mn id="S3.SS4.p1.2.m2.3.3" xref="S3.SS4.p1.2.m2.3.3.cmml">512</mn><mo id="S3.SS4.p1.2.m2.3.4.3.2.4" stretchy="false" xref="S3.SS4.p1.2.m2.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.3b"><apply id="S3.SS4.p1.2.m2.3.4.cmml" xref="S3.SS4.p1.2.m2.3.4"><in id="S3.SS4.p1.2.m2.3.4.1.cmml" xref="S3.SS4.p1.2.m2.3.4.1"></in><ci id="S3.SS4.p1.2.m2.3.4.2.cmml" xref="S3.SS4.p1.2.m2.3.4.2">𝑛</ci><set id="S3.SS4.p1.2.m2.3.4.3.1.cmml" xref="S3.SS4.p1.2.m2.3.4.3.2"><cn id="S3.SS4.p1.2.m2.1.1.cmml" type="integer" xref="S3.SS4.p1.2.m2.1.1">128</cn><cn id="S3.SS4.p1.2.m2.2.2.cmml" type="integer" xref="S3.SS4.p1.2.m2.2.2">256</cn><cn id="S3.SS4.p1.2.m2.3.3.cmml" type="integer" xref="S3.SS4.p1.2.m2.3.3">512</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.3c">n\in\{128,256,512\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.2.m2.3d">italic_n ∈ { 128 , 256 , 512 }</annotation></semantics></math>).
As well, an aggregation of the output by the indexing of different chunking configurations has been considered.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">In addition to chunking based on the number of tokens, we have processed the documents using computer vision and natural language processing to extract elements identified in the reports. The list of elements considered are provided by the Unstructured<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://unstructured-io.github.io/unstructured/introduction.html#elements" title="">https://unstructured-io.github.io/unstructured/introduction.html#elements</a></span></span></span> open source library. From the set of processing strategies, we use Chipper, a vision encoder decoder<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/docs/transformers/model_doc/vision-encoder-decoder" title="">https://huggingface.co/docs/transformers/model_doc/vision-encoder-decoder</a></span></span></span> model inspired by Donut <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib20" title="">20</a>]</cite> to showcase the performance difference. The Chipper model outputs results as a JSON representation of the document, listing elements per page characterized by their element type.
Additionally, Chipper provides a bounding box enclosing each element on the page and the corresponding element text.
</p>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.1">These elements are sometimes short to be considered as chunks, so to generate chunks from elements the following steps have been followed. Given the structure of finance reporting documents, our structural chunking efforts are concentrated on processing titles, texts, and tables. The steps to generate element-based chunks are:</p>
</div>
<div class="ltx_para" id="S3.SS4.p4">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">if the element text length is smaller than 2,048 characters, a merge with the following element is attempted</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">iteratively, element texts are merged following the step above till either the desired length is achieved, without breaking the element</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">if a title element is found, a new chunk is started</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1">if a table element is found, a new chunk is started, preserving the entire table
</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS4.p5">
<p class="ltx_p" id="S3.SS4.p5.1">After element-based chunks have been derived, three types of metadata are generated to enrich the content and support efficient indexing. The first two types, generated via predefined prompt templates with GPT-4, include: 1) up to 6 representative keywords of the composite chunk 2) a summarised paragraph of the composite chunk. The third type is
3) Naive representation using the first two sentences from a composite chunk (a kind of prefix) and in the case of tables, the description of the table, which is typically identified in the table caption.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Dataset</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">To evaluate the performance of the different chunking approaches, we have used the FinanceBench dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib14" title="">14</a>]</cite>. FinanceBench is a new benchmarking dataset designed to assess the capabilities of LLMs in answering open-book financial questions. The questions collected are realistic and applicable to real-world financial scenarios and include complex questions that require computational reasoning to arrive at conclusive answers.</p>
</div>
<div class="ltx_para" id="S3.SS5.p2">
<p class="ltx_p" id="S3.SS5.p2.1">This dataset is made of 150 instances with questions and answers from 84 unique reports.
The dataset does not include the source documents, which we have downloaded.
We were able to recover only 80 documents, which reduces the number of questions to 141 from the original 150.
The distribution of Unstructured elements predictions are shown in table <a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#S3.T1" title="Table 1 ‣ 3.5 Dataset ‣ 3 Methods ‣ Financial Report Chunking for Effective Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Unstructured element types distribution for <em class="ltx_emph ltx_font_italic" id="S3.T1.2.1">Chipper</em> predictions against documents in FinanceBench.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.3.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.3.1.1.1.1">Element Type</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T1.3.1.1.2">
<em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.T1.3.1.1.2.1">Chipper</em><span class="ltx_text ltx_font_bold" id="S3.T1.3.1.1.2.2"> Entities</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.3.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.3.2.1.1">NarrativeText</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.3.2.1.2">61,780</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S3.T1.3.3.2.1">Title</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.3.3.2.2">29,664</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S3.T1.3.4.3.1">ListItem</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.3.4.3.2">33,054</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S3.T1.3.5.4.1">UncategorizedText</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.3.5.4.2">9,400</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S3.T1.3.6.5.1">Footer</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.3.6.5.2">1,026</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S3.T1.3.7.6.1">Table</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.3.7.6.2">7,700</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S3.T1.3.8.7.1">Header</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.3.8.7.2">3,959</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S3.T1.3.9.8.1">Image</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.3.9.8.2">26</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S3.T1.3.10.9.1">FigureCaption</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.3.10.9.2">54</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S3.T1.3.11.10.1">Formula</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.3.11.10.2">29</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.12.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S3.T1.3.12.11.1">Address</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.3.12.11.2">229</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.13.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.3.13.12.1"><span class="ltx_text ltx_font_bold" id="S3.T1.3.13.12.1.1">Total</span></th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.3.13.12.2"><span class="ltx_text ltx_font_bold" id="S3.T1.3.13.12.2.1">146,921</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S3.SS5.p3">
<p class="ltx_p" id="S3.SS5.p3.1">Documents have a varying number of pages, spanning from 4 pages (FOOTLOCKER_2022_8K_dated-2022-05-20) to 549 pages (e.g. PEPSICO_2021_10K), with an average of 147.34 with std 97.78 with a total of 11,787 pages combined.
Each instance contains a link to the report, the question, a question type , the answer and supporting evidence, with page number where the evidence is located in the document, that allows for a closer evaluation of the results.
Based on the page number, evidence contexts are located in different areas in the documents, ranging from the first page in some cases up to page 304 in one instance. The mean page number to find the evidence is 54.58 with a standard deviation of 43.66, which shows that evidence contexts to answer the questions are spread within a document.
These characteristics make FinanceBench a perfect dataset for evaluating RAG. An example instance is available in table <a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#S3.T2" title="Table 2 ‣ 3.5 Dataset ‣ 3 Methods ‣ Financial Report Chunking for Effective Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Example question from the FinanceBench dataset</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.1.1.1">Field</th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T2.1.1.1.2" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="S3.T2.1.1.1.2.1">Value</p>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.2.1.1">financebench_id</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T2.1.2.1.2" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="S3.T2.1.2.1.2.1">financebench_id_00859</p>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.3.2.1">doc_name</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T2.1.3.2.2" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="S3.T2.1.3.2.2.1">VERIZON_2021_10K</p>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.4.3.1">doc_link</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T2.1.4.3.2" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="S3.T2.1.4.3.2.1">https://www.verizon.com/about/sites/default/files/2021-Annual-Report-on-Form-10-K.pdf</p>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.5.4.1">question_type</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T2.1.5.4.2" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="S3.T2.1.5.4.2.1">’novel-generated’</p>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.6.5.1">question</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T2.1.6.5.2" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="S3.T2.1.6.5.2.1">Among all of the derivative instruments that Verizon used to manage the exposure to fluctuations of foreign currencies exchange rates or interest rates, which one had the highest notional value in FY 2021?</p>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.7.6.1">answer</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T2.1.7.6.2" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="S3.T2.1.7.6.2.1">Cross currency swaps. Its notional value was $32,502 million.,</p>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.8.7.1">evidence_text</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S3.T2.1.8.7.2" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="S3.T2.1.8.7.2.1">Derivative Instruments We enter into derivative transactions primarily to manage our exposure to fluctuations in foreign currency exchange rates and interest rates. We employ risk management strategies, which may include the use of a variety of derivatives including interest rate swaps, cross currency swaps, forward starting interest rate swaps, treasury rate locks, interest rate caps, swaptions and foreign exchange forwards. We do not hold derivatives for trading purposes. The following table sets forth the notional amounts of our outstanding derivative instruments: (dollars in millions) At December 31, 2021 2020 Interest rate swaps $ 19,779 $ 17,768 Cross currency swaps 32,502 26,288 Forward starting interest rate swaps 1,000 2,000 Foreign exchange forwards 932 1,405</p>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.9.8.1">page_number</th>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.1.9.8.2" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="S3.T2.1.9.8.2.1">85</p>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we evaluate the different chunking strategies using the FinanceBench dataset. Our evaluation is grounded in factual accuracy, which allows us to measure the effectiveness of each configuration by its precision in retrieving answers that match the ground truth, as well as its generation abilities.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">We are considering 80 documents and 141 questions from FinanceBench.
Using the OpenAI tokenizer from the model <em class="ltx_emph ltx_font_italic" id="S4.p2.1.1">text-embedding-ada-002</em> that uses the tokenizer <em class="ltx_emph ltx_font_italic" id="S4.p2.1.2">cl100k_base</em><span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://platform.openai.com/docs/guides/embeddings/limitations-risks" title="">https://platform.openai.com/docs/guides/embeddings/limitations-risks</a></span></span></span>, there are on average 102,444.35 tokens with std of 61,979.45, which shows the large variability of document lengths as seen by the different number of pages per document presented above.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS0.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.0.1 </span>Chunking Efficiency</h4>
<div class="ltx_para" id="S4.SS0.SSS1.p1">
<p class="ltx_p" id="S4.SS0.SSS1.p1.1">The first thing we analyzed is the total number of chunks, as it impacts indexing time. We would like to observe the relationship between accuracy and total chunk size. Table <a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#S4.T3" title="Table 3 ‣ 4.0.1 Chunking Efficiency ‣ 4 Results ‣ Financial Report Chunking for Effective Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">3</span></a> shows the number of chunks derived from each one of the processing methods. Unstructured element-based chunks are closer in size to Base 512, and as the chunk size decreases for the basic chunking strategies, the total number of chunks increases linearly.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Chunks statistics for basic chunking elements and Unstructured elements</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.1.1.1.1">Processing</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.1.1.1.2">total chunks</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.1.1.1.3">mean chunks per document (std)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.1.1.1.4">tables mean (std)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.1.2.1.1">Base 128</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.2.1.2">64,058</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.2.1.3">800.73 (484.11)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.2.1.4">N/A</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T3.1.3.2.1">Base 256</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.3.2.2">32,051</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.3.2.3">400.64 (242.04)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.3.2.4">N/A</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T3.1.4.3.1">Base 512</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.4.3.2">16,046</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.4.3.3">200.58 (121.01)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.4.3.4">N/A</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.1.5.4.1">Chipper</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.1.5.4.2">20,843</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.1.5.4.3">260.57 (145.80)</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.1.5.4.4">96.20 (57.53)</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS0.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.0.2 </span>Retrieval Accuracy</h4>
<div class="ltx_para" id="S4.SS0.SSS2.p1">
<p class="ltx_p" id="S4.SS0.SSS2.p1.1">Secondly, we evaluate the capabilities of each chunking strategy in terms of retrieval accuracy. We use the page numbers in the ground truth to calculate the page-level retrieval accuracy, and we use ROUGE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib24" title="">24</a>]</cite> and BLEU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib32" title="">32</a>]</cite> scores to evaluate the accuracy of paragraph-level retrieval compared to the ground truth evidence paragraphs.</p>
</div>
<div class="ltx_para" id="S4.SS0.SSS2.p2">
<p class="ltx_p" id="S4.SS0.SSS2.p2.1">As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#S4.T4" title="Table 4 ‣ 4.0.3 Q&amp;A Accuracy ‣ 4 Results ‣ Financial Report Chunking for Effective Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">4</span></a>, when compared to Unstructured element-based chunking strategies, basic chunking strategies seem to have higher page-level retrieval accuracy but lower paragraph-level accuracy on average. Additionally, basic chunking strategies also lack consistency between page-level and paragraph-level accuracy; higher page-level accuracy doesn’t ensure higher paragraph-level accuracy. For example, Base 128 has the second highest page-level accuracy but the lowest paragraph-level scores among all. On the other hand, element-based chunking strategies showed more consistent results.</p>
</div>
<div class="ltx_para" id="S4.SS0.SSS2.p3">
<p class="ltx_p" id="S4.SS0.SSS2.p3.1">A fascinating discovery is that when various chunking strategies are combined, it results in enhanced retrieval scores, achieving superior performance at both the page level (84.4%) and paragraph level (with ROUGE at 0.568% and BLEU at 0.452%). This finding addresses an unresolved question: how to improve the accuracy of RAG.</p>
</div>
<div class="ltx_para" id="S4.SS0.SSS2.p4">
<p class="ltx_p" id="S4.SS0.SSS2.p4.1">The element based method provides the highest scores and it also provides a mechanism to chunk documents without the need to fine tune hyper-parameters like the number of tokens in a chunk. This suggests the element based method is more generalizable and can be applied to new types of documents.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS0.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.0.3 </span>Q&amp;A Accuracy</h4>
<div class="ltx_para" id="S4.SS0.SSS3.p1">
<p class="ltx_p" id="S4.SS0.SSS3.p1.1">Third, we evaluate the Q&amp;A accuracy for the chunking strategies. In addition to manual evaluation, we have investigated an automatic evaluation using GPT-4.
GPT-4 compares how the answers provided by our method are similar to or different from the FinanceBench gold standard, similar approaches have been previously evaluated <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib30" title="">30</a>]</cite>. The automatic evaluation allows scaling the evaluation efforts for the different chunking strategies that we have considered.
We used the prompt template in figure <a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#S4.F4" title="Figure 4 ‣ 4.0.3 Q&amp;A Accuracy ‣ 4 Results ‣ Financial Report Chunking for Effective Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F4">
<span class="ltx_inline-block ltx_align_center ltx_framed_rectangle" id="S4.F4.7" style="border-color: black;"><pre class="ltx_verbatim ltx_font_typewriter" id="S4.F4.7.1">
Begin with True or False. Are the two following answers (Answer 1 and
Answer 2) the same with respect to the question between single quotes
’{question}’?

Answer 1: ’{ground_truth_answer}’
Answer 2: ’{generated_answer}’
</pre>
</span>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Evaluation prompt template. The <math alttext="\{question\}" class="ltx_Math" display="inline" id="S4.F4.4.m1.1"><semantics id="S4.F4.4.m1.1b"><mrow id="S4.F4.4.m1.1.1.1" xref="S4.F4.4.m1.1.1.2.cmml"><mo id="S4.F4.4.m1.1.1.1.2" stretchy="false" xref="S4.F4.4.m1.1.1.2.cmml">{</mo><mrow id="S4.F4.4.m1.1.1.1.1" xref="S4.F4.4.m1.1.1.1.1.cmml"><mi id="S4.F4.4.m1.1.1.1.1.2" xref="S4.F4.4.m1.1.1.1.1.2.cmml">q</mi><mo id="S4.F4.4.m1.1.1.1.1.1" xref="S4.F4.4.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.4.m1.1.1.1.1.3" xref="S4.F4.4.m1.1.1.1.1.3.cmml">u</mi><mo id="S4.F4.4.m1.1.1.1.1.1b" xref="S4.F4.4.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.4.m1.1.1.1.1.4" xref="S4.F4.4.m1.1.1.1.1.4.cmml">e</mi><mo id="S4.F4.4.m1.1.1.1.1.1c" xref="S4.F4.4.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.4.m1.1.1.1.1.5" xref="S4.F4.4.m1.1.1.1.1.5.cmml">s</mi><mo id="S4.F4.4.m1.1.1.1.1.1d" xref="S4.F4.4.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.4.m1.1.1.1.1.6" xref="S4.F4.4.m1.1.1.1.1.6.cmml">t</mi><mo id="S4.F4.4.m1.1.1.1.1.1e" xref="S4.F4.4.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.4.m1.1.1.1.1.7" xref="S4.F4.4.m1.1.1.1.1.7.cmml">i</mi><mo id="S4.F4.4.m1.1.1.1.1.1f" xref="S4.F4.4.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.4.m1.1.1.1.1.8" xref="S4.F4.4.m1.1.1.1.1.8.cmml">o</mi><mo id="S4.F4.4.m1.1.1.1.1.1g" xref="S4.F4.4.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.4.m1.1.1.1.1.9" xref="S4.F4.4.m1.1.1.1.1.9.cmml">n</mi></mrow><mo id="S4.F4.4.m1.1.1.1.3" stretchy="false" xref="S4.F4.4.m1.1.1.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.F4.4.m1.1c"><set id="S4.F4.4.m1.1.1.2.cmml" xref="S4.F4.4.m1.1.1.1"><apply id="S4.F4.4.m1.1.1.1.1.cmml" xref="S4.F4.4.m1.1.1.1.1"><times id="S4.F4.4.m1.1.1.1.1.1.cmml" xref="S4.F4.4.m1.1.1.1.1.1"></times><ci id="S4.F4.4.m1.1.1.1.1.2.cmml" xref="S4.F4.4.m1.1.1.1.1.2">𝑞</ci><ci id="S4.F4.4.m1.1.1.1.1.3.cmml" xref="S4.F4.4.m1.1.1.1.1.3">𝑢</ci><ci id="S4.F4.4.m1.1.1.1.1.4.cmml" xref="S4.F4.4.m1.1.1.1.1.4">𝑒</ci><ci id="S4.F4.4.m1.1.1.1.1.5.cmml" xref="S4.F4.4.m1.1.1.1.1.5">𝑠</ci><ci id="S4.F4.4.m1.1.1.1.1.6.cmml" xref="S4.F4.4.m1.1.1.1.1.6">𝑡</ci><ci id="S4.F4.4.m1.1.1.1.1.7.cmml" xref="S4.F4.4.m1.1.1.1.1.7">𝑖</ci><ci id="S4.F4.4.m1.1.1.1.1.8.cmml" xref="S4.F4.4.m1.1.1.1.1.8">𝑜</ci><ci id="S4.F4.4.m1.1.1.1.1.9.cmml" xref="S4.F4.4.m1.1.1.1.1.9">𝑛</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.4.m1.1d">\{question\}</annotation><annotation encoding="application/x-llamapun" id="S4.F4.4.m1.1e">{ italic_q italic_u italic_e italic_s italic_t italic_i italic_o italic_n }</annotation></semantics></math>, <math alttext="\{ground\_truth\_answer\}" class="ltx_Math" display="inline" id="S4.F4.5.m2.1"><semantics id="S4.F4.5.m2.1b"><mrow id="S4.F4.5.m2.1.1.1" xref="S4.F4.5.m2.1.1.2.cmml"><mo id="S4.F4.5.m2.1.1.1.2" stretchy="false" xref="S4.F4.5.m2.1.1.2.cmml">{</mo><mrow id="S4.F4.5.m2.1.1.1.1" xref="S4.F4.5.m2.1.1.1.1.cmml"><mi id="S4.F4.5.m2.1.1.1.1.2" xref="S4.F4.5.m2.1.1.1.1.2.cmml">g</mi><mo id="S4.F4.5.m2.1.1.1.1.1" xref="S4.F4.5.m2.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.5.m2.1.1.1.1.3" xref="S4.F4.5.m2.1.1.1.1.3.cmml">r</mi><mo id="S4.F4.5.m2.1.1.1.1.1b" xref="S4.F4.5.m2.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.5.m2.1.1.1.1.4" xref="S4.F4.5.m2.1.1.1.1.4.cmml">o</mi><mo id="S4.F4.5.m2.1.1.1.1.1c" xref="S4.F4.5.m2.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.5.m2.1.1.1.1.5" xref="S4.F4.5.m2.1.1.1.1.5.cmml">u</mi><mo id="S4.F4.5.m2.1.1.1.1.1d" xref="S4.F4.5.m2.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.5.m2.1.1.1.1.6" xref="S4.F4.5.m2.1.1.1.1.6.cmml">n</mi><mo id="S4.F4.5.m2.1.1.1.1.1e" xref="S4.F4.5.m2.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.5.m2.1.1.1.1.7" xref="S4.F4.5.m2.1.1.1.1.7.cmml">d</mi><mo id="S4.F4.5.m2.1.1.1.1.1f" xref="S4.F4.5.m2.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.5.m2.1.1.1.1.8" mathvariant="normal" xref="S4.F4.5.m2.1.1.1.1.8.cmml">_</mi><mo id="S4.F4.5.m2.1.1.1.1.1g" xref="S4.F4.5.m2.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.5.m2.1.1.1.1.9" xref="S4.F4.5.m2.1.1.1.1.9.cmml">t</mi><mo id="S4.F4.5.m2.1.1.1.1.1h" xref="S4.F4.5.m2.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.5.m2.1.1.1.1.10" xref="S4.F4.5.m2.1.1.1.1.10.cmml">r</mi><mo id="S4.F4.5.m2.1.1.1.1.1i" xref="S4.F4.5.m2.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.5.m2.1.1.1.1.11" xref="S4.F4.5.m2.1.1.1.1.11.cmml">u</mi><mo id="S4.F4.5.m2.1.1.1.1.1j" xref="S4.F4.5.m2.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.5.m2.1.1.1.1.12" xref="S4.F4.5.m2.1.1.1.1.12.cmml">t</mi><mo id="S4.F4.5.m2.1.1.1.1.1k" xref="S4.F4.5.m2.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.5.m2.1.1.1.1.13" xref="S4.F4.5.m2.1.1.1.1.13.cmml">h</mi><mo id="S4.F4.5.m2.1.1.1.1.1l" xref="S4.F4.5.m2.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.5.m2.1.1.1.1.14" mathvariant="normal" xref="S4.F4.5.m2.1.1.1.1.14.cmml">_</mi><mo id="S4.F4.5.m2.1.1.1.1.1m" xref="S4.F4.5.m2.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.5.m2.1.1.1.1.15" xref="S4.F4.5.m2.1.1.1.1.15.cmml">a</mi><mo id="S4.F4.5.m2.1.1.1.1.1n" xref="S4.F4.5.m2.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.5.m2.1.1.1.1.16" xref="S4.F4.5.m2.1.1.1.1.16.cmml">n</mi><mo id="S4.F4.5.m2.1.1.1.1.1o" xref="S4.F4.5.m2.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.5.m2.1.1.1.1.17" xref="S4.F4.5.m2.1.1.1.1.17.cmml">s</mi><mo id="S4.F4.5.m2.1.1.1.1.1p" xref="S4.F4.5.m2.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.5.m2.1.1.1.1.18" xref="S4.F4.5.m2.1.1.1.1.18.cmml">w</mi><mo id="S4.F4.5.m2.1.1.1.1.1q" xref="S4.F4.5.m2.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.5.m2.1.1.1.1.19" xref="S4.F4.5.m2.1.1.1.1.19.cmml">e</mi><mo id="S4.F4.5.m2.1.1.1.1.1r" xref="S4.F4.5.m2.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.5.m2.1.1.1.1.20" xref="S4.F4.5.m2.1.1.1.1.20.cmml">r</mi></mrow><mo id="S4.F4.5.m2.1.1.1.3" stretchy="false" xref="S4.F4.5.m2.1.1.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.F4.5.m2.1c"><set id="S4.F4.5.m2.1.1.2.cmml" xref="S4.F4.5.m2.1.1.1"><apply id="S4.F4.5.m2.1.1.1.1.cmml" xref="S4.F4.5.m2.1.1.1.1"><times id="S4.F4.5.m2.1.1.1.1.1.cmml" xref="S4.F4.5.m2.1.1.1.1.1"></times><ci id="S4.F4.5.m2.1.1.1.1.2.cmml" xref="S4.F4.5.m2.1.1.1.1.2">𝑔</ci><ci id="S4.F4.5.m2.1.1.1.1.3.cmml" xref="S4.F4.5.m2.1.1.1.1.3">𝑟</ci><ci id="S4.F4.5.m2.1.1.1.1.4.cmml" xref="S4.F4.5.m2.1.1.1.1.4">𝑜</ci><ci id="S4.F4.5.m2.1.1.1.1.5.cmml" xref="S4.F4.5.m2.1.1.1.1.5">𝑢</ci><ci id="S4.F4.5.m2.1.1.1.1.6.cmml" xref="S4.F4.5.m2.1.1.1.1.6">𝑛</ci><ci id="S4.F4.5.m2.1.1.1.1.7.cmml" xref="S4.F4.5.m2.1.1.1.1.7">𝑑</ci><ci id="S4.F4.5.m2.1.1.1.1.8.cmml" xref="S4.F4.5.m2.1.1.1.1.8">_</ci><ci id="S4.F4.5.m2.1.1.1.1.9.cmml" xref="S4.F4.5.m2.1.1.1.1.9">𝑡</ci><ci id="S4.F4.5.m2.1.1.1.1.10.cmml" xref="S4.F4.5.m2.1.1.1.1.10">𝑟</ci><ci id="S4.F4.5.m2.1.1.1.1.11.cmml" xref="S4.F4.5.m2.1.1.1.1.11">𝑢</ci><ci id="S4.F4.5.m2.1.1.1.1.12.cmml" xref="S4.F4.5.m2.1.1.1.1.12">𝑡</ci><ci id="S4.F4.5.m2.1.1.1.1.13.cmml" xref="S4.F4.5.m2.1.1.1.1.13">ℎ</ci><ci id="S4.F4.5.m2.1.1.1.1.14.cmml" xref="S4.F4.5.m2.1.1.1.1.14">_</ci><ci id="S4.F4.5.m2.1.1.1.1.15.cmml" xref="S4.F4.5.m2.1.1.1.1.15">𝑎</ci><ci id="S4.F4.5.m2.1.1.1.1.16.cmml" xref="S4.F4.5.m2.1.1.1.1.16">𝑛</ci><ci id="S4.F4.5.m2.1.1.1.1.17.cmml" xref="S4.F4.5.m2.1.1.1.1.17">𝑠</ci><ci id="S4.F4.5.m2.1.1.1.1.18.cmml" xref="S4.F4.5.m2.1.1.1.1.18">𝑤</ci><ci id="S4.F4.5.m2.1.1.1.1.19.cmml" xref="S4.F4.5.m2.1.1.1.1.19">𝑒</ci><ci id="S4.F4.5.m2.1.1.1.1.20.cmml" xref="S4.F4.5.m2.1.1.1.1.20">𝑟</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.5.m2.1d">\{ground\_truth\_answer\}</annotation><annotation encoding="application/x-llamapun" id="S4.F4.5.m2.1e">{ italic_g italic_r italic_o italic_u italic_n italic_d _ italic_t italic_r italic_u italic_t italic_h _ italic_a italic_n italic_s italic_w italic_e italic_r }</annotation></semantics></math> and <math alttext="\{generated\_answer\}" class="ltx_Math" display="inline" id="S4.F4.6.m3.1"><semantics id="S4.F4.6.m3.1b"><mrow id="S4.F4.6.m3.1.1.1" xref="S4.F4.6.m3.1.1.2.cmml"><mo id="S4.F4.6.m3.1.1.1.2" stretchy="false" xref="S4.F4.6.m3.1.1.2.cmml">{</mo><mrow id="S4.F4.6.m3.1.1.1.1" xref="S4.F4.6.m3.1.1.1.1.cmml"><mi id="S4.F4.6.m3.1.1.1.1.2" xref="S4.F4.6.m3.1.1.1.1.2.cmml">g</mi><mo id="S4.F4.6.m3.1.1.1.1.1" xref="S4.F4.6.m3.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.6.m3.1.1.1.1.3" xref="S4.F4.6.m3.1.1.1.1.3.cmml">e</mi><mo id="S4.F4.6.m3.1.1.1.1.1b" xref="S4.F4.6.m3.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.6.m3.1.1.1.1.4" xref="S4.F4.6.m3.1.1.1.1.4.cmml">n</mi><mo id="S4.F4.6.m3.1.1.1.1.1c" xref="S4.F4.6.m3.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.6.m3.1.1.1.1.5" xref="S4.F4.6.m3.1.1.1.1.5.cmml">e</mi><mo id="S4.F4.6.m3.1.1.1.1.1d" xref="S4.F4.6.m3.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.6.m3.1.1.1.1.6" xref="S4.F4.6.m3.1.1.1.1.6.cmml">r</mi><mo id="S4.F4.6.m3.1.1.1.1.1e" xref="S4.F4.6.m3.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.6.m3.1.1.1.1.7" xref="S4.F4.6.m3.1.1.1.1.7.cmml">a</mi><mo id="S4.F4.6.m3.1.1.1.1.1f" xref="S4.F4.6.m3.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.6.m3.1.1.1.1.8" xref="S4.F4.6.m3.1.1.1.1.8.cmml">t</mi><mo id="S4.F4.6.m3.1.1.1.1.1g" xref="S4.F4.6.m3.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.6.m3.1.1.1.1.9" xref="S4.F4.6.m3.1.1.1.1.9.cmml">e</mi><mo id="S4.F4.6.m3.1.1.1.1.1h" xref="S4.F4.6.m3.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.6.m3.1.1.1.1.10" xref="S4.F4.6.m3.1.1.1.1.10.cmml">d</mi><mo id="S4.F4.6.m3.1.1.1.1.1i" xref="S4.F4.6.m3.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.6.m3.1.1.1.1.11" mathvariant="normal" xref="S4.F4.6.m3.1.1.1.1.11.cmml">_</mi><mo id="S4.F4.6.m3.1.1.1.1.1j" xref="S4.F4.6.m3.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.6.m3.1.1.1.1.12" xref="S4.F4.6.m3.1.1.1.1.12.cmml">a</mi><mo id="S4.F4.6.m3.1.1.1.1.1k" xref="S4.F4.6.m3.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.6.m3.1.1.1.1.13" xref="S4.F4.6.m3.1.1.1.1.13.cmml">n</mi><mo id="S4.F4.6.m3.1.1.1.1.1l" xref="S4.F4.6.m3.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.6.m3.1.1.1.1.14" xref="S4.F4.6.m3.1.1.1.1.14.cmml">s</mi><mo id="S4.F4.6.m3.1.1.1.1.1m" xref="S4.F4.6.m3.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.6.m3.1.1.1.1.15" xref="S4.F4.6.m3.1.1.1.1.15.cmml">w</mi><mo id="S4.F4.6.m3.1.1.1.1.1n" xref="S4.F4.6.m3.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.6.m3.1.1.1.1.16" xref="S4.F4.6.m3.1.1.1.1.16.cmml">e</mi><mo id="S4.F4.6.m3.1.1.1.1.1o" xref="S4.F4.6.m3.1.1.1.1.1.cmml">⁢</mo><mi id="S4.F4.6.m3.1.1.1.1.17" xref="S4.F4.6.m3.1.1.1.1.17.cmml">r</mi></mrow><mo id="S4.F4.6.m3.1.1.1.3" stretchy="false" xref="S4.F4.6.m3.1.1.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.F4.6.m3.1c"><set id="S4.F4.6.m3.1.1.2.cmml" xref="S4.F4.6.m3.1.1.1"><apply id="S4.F4.6.m3.1.1.1.1.cmml" xref="S4.F4.6.m3.1.1.1.1"><times id="S4.F4.6.m3.1.1.1.1.1.cmml" xref="S4.F4.6.m3.1.1.1.1.1"></times><ci id="S4.F4.6.m3.1.1.1.1.2.cmml" xref="S4.F4.6.m3.1.1.1.1.2">𝑔</ci><ci id="S4.F4.6.m3.1.1.1.1.3.cmml" xref="S4.F4.6.m3.1.1.1.1.3">𝑒</ci><ci id="S4.F4.6.m3.1.1.1.1.4.cmml" xref="S4.F4.6.m3.1.1.1.1.4">𝑛</ci><ci id="S4.F4.6.m3.1.1.1.1.5.cmml" xref="S4.F4.6.m3.1.1.1.1.5">𝑒</ci><ci id="S4.F4.6.m3.1.1.1.1.6.cmml" xref="S4.F4.6.m3.1.1.1.1.6">𝑟</ci><ci id="S4.F4.6.m3.1.1.1.1.7.cmml" xref="S4.F4.6.m3.1.1.1.1.7">𝑎</ci><ci id="S4.F4.6.m3.1.1.1.1.8.cmml" xref="S4.F4.6.m3.1.1.1.1.8">𝑡</ci><ci id="S4.F4.6.m3.1.1.1.1.9.cmml" xref="S4.F4.6.m3.1.1.1.1.9">𝑒</ci><ci id="S4.F4.6.m3.1.1.1.1.10.cmml" xref="S4.F4.6.m3.1.1.1.1.10">𝑑</ci><ci id="S4.F4.6.m3.1.1.1.1.11.cmml" xref="S4.F4.6.m3.1.1.1.1.11">_</ci><ci id="S4.F4.6.m3.1.1.1.1.12.cmml" xref="S4.F4.6.m3.1.1.1.1.12">𝑎</ci><ci id="S4.F4.6.m3.1.1.1.1.13.cmml" xref="S4.F4.6.m3.1.1.1.1.13">𝑛</ci><ci id="S4.F4.6.m3.1.1.1.1.14.cmml" xref="S4.F4.6.m3.1.1.1.1.14">𝑠</ci><ci id="S4.F4.6.m3.1.1.1.1.15.cmml" xref="S4.F4.6.m3.1.1.1.1.15">𝑤</ci><ci id="S4.F4.6.m3.1.1.1.1.16.cmml" xref="S4.F4.6.m3.1.1.1.1.16">𝑒</ci><ci id="S4.F4.6.m3.1.1.1.1.17.cmml" xref="S4.F4.6.m3.1.1.1.1.17">𝑟</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.6.m3.1d">\{generated\_answer\}</annotation><annotation encoding="application/x-llamapun" id="S4.F4.6.m3.1e">{ italic_g italic_e italic_n italic_e italic_r italic_a italic_t italic_e italic_d _ italic_a italic_n italic_s italic_w italic_e italic_r }</annotation></semantics></math> fields are substituted for each question accordingly.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS0.SSS3.p2">
<p class="ltx_p" id="S4.SS0.SSS3.p2.1">Results in table <a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#S4.T5" title="Table 5 ‣ 4.0.3 Q&amp;A Accuracy ‣ 4 Results ‣ Financial Report Chunking for Effective Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">5</span></a> show that element-based chunking strategies offer the best question-answering accuracy, which is consistent with page retrieval and paragraph retrieval accuracy.
</p>
</div>
<div class="ltx_para" id="S4.SS0.SSS3.p3">
<p class="ltx_p" id="S4.SS0.SSS3.p3.1">Lastly, our approach stands out for its efficiency. Not only is element-based chunking generalizable without the need to select the chunk size, but when compared to the aggregation results that yield the highest retrieval scores. Element-based chunking achieves the highest retrieval scores with only half the number of chunks required compared to methods that do not consider the structure of the documents (62,529 v.s. 112,155). This can reduce the indexing cost and improve query latency because there are only half as many vectors to index for the vectordb that stores the chunks. This underscores the effectiveness of our solution in optimizing the balance between performance and computational resource requirements.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Retrieval results. For each chunking strategy, we show the number of chunks for all the documents (Total Chunks), Page Accuracy, and ROUGE and BLEU scores. ROUGE and BLEU are calculated as the maximum score from the list of recovered contexts for a question when compared to the known evidence for that question.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T4.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.1.1.1.1">Chunking strategy</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.1.2">Total Chunks</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.1.3">Page Accuracy</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.1.4">ROUGE</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.1.5">BLEU</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.1.2.2.1">Base 128</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.2.2.2">64,058</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.2.2.3">72.34</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.2.2.4">0.383</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.2.2.5">0.181</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T4.1.3.3.1">Base 256</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.3.3.2">32,051</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.3.3.3">73.05</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.3.3.4">0.433</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.3.3.5">0.231</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T4.1.4.4.1">Base 512</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.4.4.2">16,046</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.4.4.3">68.09</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.4.4.4">0.455</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.4.4.5">0.250</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.1.5.5.1">Base Aggregation</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.5.5.2">112,155</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.5.5.3">83.69</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.5.5.4">0.536</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.5.5.5">0.277</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt" id="S4.T4.1.6.6.1">Keywords Chipper</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.1.6.6.2">20,843</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.1.6.6.3">46.10</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.1.6.6.4">0.444</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.1.6.6.5">0.315</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T4.1.7.7.1">Summary Chipper</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.7.7.2">20,843</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.7.7.3">62.41</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.7.7.4">0.473</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.7.7.5">0.350</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T4.1.8.8.1">Prefix &amp; Table Description Chipper</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.8.8.2">20,843</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.8.8.3">67.38</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.8.8.4">0.514</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.8.8.5">0.400</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.1.9.9.1">Chipper Aggregation</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.1.9.9.2">62,529</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.1.9.9.3"><span class="ltx_text ltx_font_bold" id="S4.T4.1.9.9.3.1">84.40</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.1.9.9.4"><span class="ltx_text ltx_font_bold" id="S4.T4.1.9.9.4.1">0.568</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.1.9.9.5"><span class="ltx_text ltx_font_bold" id="S4.T4.1.9.9.5.1">0.452</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S4.T5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Q&amp;A results. We show the percentage of questions with no answer and as well the accuracy either estimated automatically using GPT-4 or manually.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T5.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T5.1.1.1.1">Chunking strategy</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T5.1.1.1.2">No answer</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T5.1.1.1.3">GPT-4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T5.1.1.1.4">Manual</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T5.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T5.1.2.1.1">Base 128</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.1.2.1.2">35.46</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.1.2.1.3">29.08</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.1.2.1.4">35.46</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T5.1.3.2.1">Base 256</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.1.3.2.2">25.53</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.1.3.2.3">32.62</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.1.3.2.4">36.88</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T5.1.4.3.1">Base 512</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.1.4.3.2">24.82</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.1.4.3.3">41.84</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.1.4.3.4">48.23</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T5.1.5.4.1">Keywords Chipper</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.1.5.4.2">22.70</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.1.5.4.3"><span class="ltx_text ltx_font_bold" id="S4.T5.1.5.4.3.1">43.97</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.1.5.4.4"><span class="ltx_text ltx_font_bold" id="S4.T5.1.5.4.4.1">53.19</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T5.1.6.5.1">Summary Chipper</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.1.6.5.2">17.73</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.1.6.5.3"><span class="ltx_text ltx_font_bold" id="S4.T5.1.6.5.3.1">43.97</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.1.6.5.4">51.77</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r" id="S4.T5.1.7.6.1">Prefix &amp; Table Description Chipper</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T5.1.7.6.2">20.57</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T5.1.7.6.3">41.13</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T5.1.7.6.4"><span class="ltx_text ltx_font_bold" id="S4.T5.1.7.6.4.1">53.19</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Results demonstrate the efficacy of our approach in utilizing structural elements for chunking, which has enabled us to attain state-of-the-art performance on Q&amp;A tasks within the FinanceBench dataset (accuracy of 50% vs 53.19%) when an index is created from document chunks and used for generation. This method, which we refer to as <span class="ltx_text ltx_font_italic" id="S5.p1.1.1">element base chunking</span>, has shown to yield consistent results between retrieval and Q&amp;A accuracy.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">We have observed that using basic 512 chunking strategies produces results most similar to the Unstructured element-based approach, which may be due to the fact that 512 tokens share a similar length with the token size within our element-based chunks and capture a long context, but fail keep a coherent context in some cases, leaving out relevant information required for Q&amp;A.
This is further observed when considering the ROUGE and BLEU scores in table <a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#S4.T4" title="Table 4 ‣ 4.0.3 Q&amp;A Accuracy ‣ 4 Results ‣ Financial Report Chunking for Effective Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">4</span></a>, where the chunk contexts for the baseline have lower scores.
</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">These findings support existing research stating that the best basic chunk size varies from data to data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#bib.bib3" title="">3</a>]</cite>. These results show, as well, that our method adapts to different documents without tuning. Our method relies on the structural information that is present in the document’s layout to adjust the chunk size automatically.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">We have evaluated aggregating the output of different chunking methods in the retrieval experiments as sown in table <a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#S4.T4" title="Table 4 ‣ 4.0.3 Q&amp;A Accuracy ‣ 4 Results ‣ Financial Report Chunking for Effective Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">4</span></a>.
Even though the aggregation seems to be effective for retrieval, the Q&amp;A exceeded the GPT-4 token limit, which resulted in a non-effective Q&amp;A solution using the selected model.</p>
</div>
<div class="ltx_para" id="S5.p5">
<p class="ltx_p" id="S5.p5.1">As well, we evaluated variations of the prompt used to generate the answers (see figure <a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#S3.F3" title="Figure 3 ‣ 3.3 Generation ‣ 3 Methods ‣ Financial Report Chunking for Effective Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">3</span></a>).
Re-ordering the retrieval context and the question, but results were not statistically different.
We experimented as well with variations of the verbs using in the prompt, e.g. changing <span class="ltx_text ltx_font_italic" id="S5.p5.1.1">referencing</span> with <span class="ltx_text ltx_font_italic" id="S5.p5.1.2">using</span>, which seemed to lower the quality of the answers generated.
This shows that prompt engineering is a relevant factor in RAG.</p>
</div>
<div class="ltx_para" id="S5.p6">
<p class="ltx_p" id="S5.p6.1">We evaluated using GPT-4 for evaluation instead of relying on manual evaluation.
In most cases, GPT-4 evaluated correctly but failed when a more elaborate answer is provided.
As shown in figure <a class="ltx_ref" href="https://arxiv.org/html/2402.05131v3#S5.F5" title="Figure 5 ‣ 5 Discussion ‣ Financial Report Chunking for Effective Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">5</span></a>, the answer is 39.7% while the estimated answer is 39.73% but with a detailed explanation of the calculation.</p>
</div>
<figure class="ltx_figure" id="S5.F5">
<span class="ltx_inline-block ltx_align_center ltx_framed_rectangle" id="S5.F5.1" style="border-color: black;"><pre class="ltx_verbatim ltx_font_typewriter" id="S5.F5.1.1">
Question: ’What is Coca Cola’s FY2021 COGS % margin? Calculate what
was asked by utilizing the line items clearly shown in the income
statement.’?

Answer 1: ’39.7%’
Answer 2: ’From the income statement referenced on page 60 of
COCACOLA_2021_10K_embedded.json, we can see that Coca Cola’s total
revenue in FY2021 was $38,655 million and their cost of goods sold
(COGS) was $15,357 million. To calculate the COGS % margin, we divide
the COGS by the total revenue and multiply by 100:
(15,357 / 38,655) * 100 = 39.73%
So, Coca Cola’s FY2021 COGS % margin was approximately 39.73%.’
</pre>
</span>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Evaluation prompt template</figcaption>
</figure>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions and Future Work</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Results show that our element based chunking strategy improves the state-of-the-art Q&amp;A for the task, which is achieved by providing a better chunking strategy for the processed documents.
We provide comparison with baseline chunking strategies that allow us to draw conclusions about different chunking methods.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">As future work, we would like to perform a similar evaluation in other domains, e.g. biomedical, to understand how our findings apply outside financial reporting.
As well, we would like studying which additional element types and/or relation between elements would support better chunking strategies for RAG.
Furthermore, we would like to study the impact of RAG configuration and element type based chunking.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Anantha, R., Bethi, T., Vodianik, D., Chappidi, S.: Context Tuning for Retrieval Augmented Generation (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Balaguer, A., Benara, V., de Freitas Cunha, R.L., de M. Estevão Filho, R., Hendry, T., Holstein, D., Marsman, J., Mecklenburg, N., Malvar, S., Nunes, L.O., Padilha, R., Sharp, M., Silva, B., Sharma, S., Aski, V., Chandra, R.: Rag vs fine-tuning: Pipelines, tradeoffs, and a case study on agriculture (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Barnett, S., Kurniawan, S., Thudumu, S., Brannelly, Z., Abdelrazek, M.: Seven Failure Points When Engineering a Retrieval Augmented Generation System (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Bentabet, N.I., Juge, R., El Maarouf, I., Mouilleron, V., Valsamou-Stanislawski, D., El-Haj, M.: The financial document structure extraction shared task (fintoc 2020). In: Proceedings of the 1st Joint Workshop on Financial Narrative Processing and MultiLing Financial Summarisation. pp. 13–22 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Chen, H., Jiao, F., Li, X., Qin, C., Ravaut, M., Zhao, R., Xiong, C., Joty, S.: ChatGPT’s One-year Anniversary: Are Open-Source Large Language Models Catching up? arXiv preprint arXiv:2311.16989 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Chen, Z., Chen, W., Smiley, C., Shah, S., Borova, I., Langdon, D., Moussa, R., Beane, M., Huang, T.H., Routledge, B., et al.: Finqa: A dataset of numerical reasoning over financial data. arXiv preprint arXiv:2109.00122 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Chen, Z., Li, S., Smiley, C., Ma, Z., Shah, S., Wang, W.Y.: ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Choi, S., Gazeley, W., Wong, S.H., Li, T.: Conversational Financial Information Retrieval Model (ConFIRM). arXiv preprint arXiv:2310.13001 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
DeSola, V., Hanna, K., Nonis, P.: Finbert: pre-trained model on sec filings for financial natural language tasks. University of California (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
El-Haj, M., Rayson, P., Young, S., Walker, M.: Detecting document structure in a very large corpus of UK financial reports. European Language Resources Association (ELRA) (2014)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
El Maarouf, I., Kang, J., Azzi, A.A., Bellato, S., Gan, M., El-Haj, M.: The financial document structure extraction shared task (FinTOC2021). In: Proceedings of the 3rd Financial Narrative Processing Workshop. pp. 111–119 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J., Wang, H.: Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Hada, R., Gumma, V., de Wynter, A., Diddee, H., Ahmed, M., Choudhury, M., Bali, K., Sitaram, S.: Are large language model-based evaluators the solution to scaling up multilingual evaluation? arXiv preprint arXiv:2309.07462 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Islam, P., Kannappan, A., Kiela, D., Qian, R., Scherrer, N., Vidgen, B.: FinanceBench: A New Benchmark for Financial Question Answering. arXiv preprint arXiv:2311.11944 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y.J., Madotto, A., Fung, P.: Survey of Hallucination in Natural Language Generation. ACM Computing Surveys <span class="ltx_text ltx_font_bold" id="bib.bib15.1.1">55</span>(12), 1–38 (Mar 2023). https://doi.org/10.1145/3571730, <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1145/3571730" title="">http://dx.doi.org/10.1145/3571730</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Jiang, A.Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D.S., de las Casas, D., Hanna, E.B., Bressand, F., Lengyel, G., Bour, G., Lample, G., Lavaud, L.R., Saulnier, L., Lachaux, M.A., Stock, P., Subramanian, S., Yang, S., Antoniak, S., Scao, T.L., Gervet, T., Lavril, T., Wang, T., Lacroix, T., Sayed, W.E.: Mixtral of Experts (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Juge, R., Bentabet, I., Ferradans, S.: The fintoc-2019 shared task: Financial document structure extraction. In: Proceedings of the Second Financial Narrative Processing Workshop (FNP 2019). pp. 51–57 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Kaddour, J., Harris, J., Mozes, M., Bradley, H., Raileanu, R., McHardy, R.: Challenges and applications of large language models. arXiv preprint arXiv:2307.10169 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Kaur, S., Smiley, C., Gupta, A., Sain, J., Wang, D., Siddagangappa, S., Aguda, T., Shah, S.: REFinD: Relation Extraction Financial Dataset. In: Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. SIGIR ’23, ACM (Jul 2023). https://doi.org/10.1145/3539618.3591911, <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1145/3539618.3591911" title="">http://dx.doi.org/10.1145/3539618.3591911</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Kim, G., Hong, T., Yim, M., Park, J., Yim, J., Hwang, W., Yun, S., Han, D., Park, S.: Donut: Document understanding transformer without ocr. arXiv preprint arXiv:2111.15664 <span class="ltx_text ltx_font_bold" id="bib.bib20.1.1">7</span>,  15 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.t., Rocktäschel, T., et al.: Retrieval-augmented generation for knowledge-intensive NLP tasks. Advances in Neural Information Processing Systems <span class="ltx_text ltx_font_bold" id="bib.bib21.1.1">33</span>, 9459–9474 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Li, D., Shao, R., Xie, A., Sheng, Y., Zheng, L., Gonzalez, J.E., Stoica, I., Ma, X., Zhang, H.: How Long Can Open-Source LLMs Truly Promise on Context Length? (June 2023), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://lmsys.org/blog/2023-06-29-longchat" title="">https://lmsys.org/blog/2023-06-29-longchat</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Li, Y., Duan, Y.: The evaluation of experiments of artificial general intelligence with gpt-4 based on dikwp. arXiv preprint (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Lin, C.Y.: Rouge: A package for automatic evaluation of summaries. In: Text summarization branches out. pp. 74–81 (2004)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Liu, N.F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., Liang, P.: Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Liu, Z., Huang, D., Huang, K., Li, Z., Zhao, J.: Finbert: A pre-trained financial language representation model for financial text mining. In: Proceedings of the twenty-ninth international conference on international joint conferences on artificial intelligence. pp. 4513–4519 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
llmware: Rag Instruct Benchmark Tester. <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/llmware/rag_instruct_benchmark_tester" title="">https://huggingface.co/datasets/llmware/rag_instruct_benchmark_tester</a>, Accessed: January 15, 2024

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Malkov, Y.A., Yashunin, D.A.: Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE transactions on pattern analysis and machine intelligence <span class="ltx_text ltx_font_bold" id="bib.bib28.1.1">42</span>(4), 824–836 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Moore, S., Nguyen, H.A., Chen, T., Stamper, J.: Assessing the quality of multiple-choice questions using gpt-4 and rule-based methods. In: European Conference on Technology Enhanced Learning. pp. 229–245. Springer (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Naismith, B., Mulcaire, P., Burstein, J.: Automated evaluation of written discourse coherence using gpt-4. In: Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023). pp. 394–403 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
OpenAI, :, Achiam, J., Adler, S., Agarwal, S., et al.: GPT-4 Technical Report (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: a method for automatic evaluation of machine translation. In: Proceedings of the 40th annual meeting of the Association for Computational Linguistics. pp. 311–318 (2002)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Pfitzmann, B., Auer, C., Dolfi, M., Nassar, A.S., Staar, P.: Doclaynet: A large human-annotated dataset for document-layout segmentation. In: Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. pp. 3743–3751 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Pinecone: Chunking strategies for llm applications, <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.pinecone.io/learn/chunking-strategies/" title="">https://www.pinecone.io/learn/chunking-strategies/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Reimers, N., Gurevych, I.: Sentence-bert: Sentence embeddings using siamese bert-networks. In: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics (11 2019), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1908.10084" title="">https://arxiv.org/abs/1908.10084</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Retteter, J.: Mastering Table Extraction: Revolutionize Your Earnings Reports Analysis with AI. <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://medium.com/unstructured-io/mastering-table-extraction-revolutionize-your-earnings-reports-analysis-with-ai-1bc32c22720e" title="">https://medium.com/unstructured-io/mastering-table-extraction-revolutionize-your-earnings-reports-analysis-with-ai-1bc32c22720e</a>, Accessed: January 15, 2024

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Rizinski, M., Peshov, H., Mishev, K., Jovanovik, M., Trajanov, D.: Sentiment analysis in finance: From transformers back to explainable lexicons (xlex). IEEE Access <span class="ltx_text ltx_font_bold" id="bib.bib37.1.1">12</span>, 7170–7198 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Shah, R.S., Chawla, K., Eidnani, D., Shah, A., Du, W., Chava, S., Raman, N., Smiley, C., Chen, J., Yang, D.: WHEN FLUE MEETS FLANG: Benchmarks and Large Pre-trained Language Model for Financial Domain (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Singh Phogat, K., Harsha, C., Dasaratha, S., Ramakrishna, S., Akhil Puranam, S.: Zero-Shot Question Answering over Financial Documents using Large Language Models. arXiv e-prints pp. arXiv–2311 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Wu, S., Irsoy, O., Lu, S., Dabravolski, V., Dredze, M., Gehrmann, S., Kambadur, P., Rosenberg, D., Mann, G.: BloombergGPT: A Large Language Model for Finance (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Xu, P., Ping, W., Wu, X., McAfee, L., Zhu, C., Liu, Z., Subramanian, S., Bakhturina, E., Shoeybi, M., Catanzaro, B.: Retrieval meets Long Context Large Language Models (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Yang, H., Liu, X.Y., Wang, C.D.: FinGPT: Open-Source Financial Large Language Models (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Ye, H., Liu, T., Zhang, A., Hua, W., Jia, W.: Cognitive Mirage: A Review of Hallucinations in Large Language Models (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Zhang, B., Yang, H., Liu, X.Y.: Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Zheng, X., Burdick, D., Popa, L., Zhong, X., Wang, N.X.R.: Global table extractor (gte): A framework for joint table identification and cell structure recognition using visual context. In: Proceedings of the IEEE/CVF winter conference on applications of computer vision. pp. 697–706 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Zhu, F., Lei, W., Huang, Y., Wang, C., Zhang, S., Lv, J., Feng, F., Chua, T.S.: TAT-QA: A question answering benchmark on a hybrid of tabular and textual content in finance. arXiv preprint arXiv:2105.07624 (2021)

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sat Mar 16 09:11:35 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
