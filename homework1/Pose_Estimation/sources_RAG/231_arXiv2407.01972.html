<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>MeMemo: On-device Retrieval Augmentation for Private and Personalized Text Generation</title>
<!--Generated on Tue Jul  2 06:03:57 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Neural information retrieval,  On-device,  Large language models" lang="en" name="keywords"/>
<base href="/html/2407.01972v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#S1" title="In MeMemo: On-device Retrieval Augmentation for Private and Personalized Text Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#S2" title="In MeMemo: On-device Retrieval Augmentation for Private and Personalized Text Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span><span class="ltx_text ltx_font_smallcaps">MeMemo</span> in Action</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#S2.SS1" title="In 2. MeMemo in Action ‣ MeMemo: On-device Retrieval Augmentation for Private and Personalized Text Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Developing In-browser RAG Tools</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#S2.SS2" title="In 2. MeMemo in Action ‣ MeMemo: On-device Retrieval Augmentation for Private and Personalized Text Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Prototyping with <span class="ltx_text ltx_font_smallcaps">RAG Playground</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#S3" title="In MeMemo: On-device Retrieval Augmentation for Private and Personalized Text Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span><span class="ltx_text ltx_font_smallcaps">MeMemo</span> Design and Implementation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#S3.SS1" title="In 3. MeMemo Design and Implementation ‣ MeMemo: On-device Retrieval Augmentation for Private and Personalized Text Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Adapting HNSW</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#S3.SS2" title="In 3. MeMemo Design and Implementation ‣ MeMemo: On-device Retrieval Augmentation for Private and Personalized Text Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Optimizing for the Browsers</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#S3.SS3" title="In 3. MeMemo Design and Implementation ‣ MeMemo: On-device Retrieval Augmentation for Private and Personalized Text Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Open-source and Easy to Use</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#S4" title="In MeMemo: On-device Retrieval Augmentation for Private and Personalized Text Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#S5" title="In MeMemo: On-device Retrieval Augmentation for Private and Personalized Text Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion and Future Work</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">
<span class="ltx_text ltx_font_smallcaps" id="id1.id1">MeMemo</span>: On-device Retrieval Augmentation for Private and Personalized Text Generation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zijie J. Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0003-4360-1423" title="ORCID identifier">0000-0003-4360-1423</a></span>
<span class="ltx_contact ltx_role_email"><a href="mailto:jayw@gatech.edu">jayw@gatech.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id2.1.id1">Georgia Institute of Technology</span><span class="ltx_text ltx_affiliation_city" id="id3.2.id2">Atlanta</span><span class="ltx_text ltx_affiliation_state" id="id4.3.id3">Georgia</span><span class="ltx_text ltx_affiliation_country" id="id5.4.id4">USA</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Duen Horng Chau
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0001-9824-3323" title="ORCID identifier">0000-0001-9824-3323</a></span>
<span class="ltx_contact ltx_role_email"><a href="mailto:polo@gatech.edu">polo@gatech.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id6.1.id1">Georgia Institute of Technology</span><span class="ltx_text ltx_affiliation_city" id="id7.2.id2">Atlanta</span><span class="ltx_text ltx_affiliation_state" id="id8.3.id3">Georgia</span><span class="ltx_text ltx_affiliation_country" id="id9.4.id4">USA</span>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id10.id1">Retrieval-augmented text generation (RAG) addresses the common limitations of large language models (LLMs), such as hallucination, by retrieving information from an updatable external knowledge base.
However, existing approaches often require dedicated backend servers for data storage and retrieval, thereby limiting their applicability in use cases that require strict data privacy, such as personal finance, education, and medicine.
To address the pressing need for client-side dense retrieval, we introduce <span class="ltx_text ltx_font_smallcaps" id="id10.id1.1">MeMemo</span>, the first open-source JavaScript toolkit that adapts the state-of-the-art approximate nearest neighbor search technique HNSW to browser environments.
Developed with modern and native Web technologies, such as IndexedDB and Web Workers, our toolkit leverages client-side hardware capabilities to enable researchers and developers to efficiently search through millions of high-dimensional vectors in the browser.
<span class="ltx_text ltx_font_smallcaps" id="id10.id1.2">MeMemo</span> enables exciting new design and research opportunities, such as private and personalized content creation and interactive prototyping, as demonstrated in our example application <span class="ltx_text ltx_font_smallcaps" id="id10.id1.3">RAG Playground</span>.
Reflecting on our work, we discuss the opportunities and challenges for on-device dense retrieval.
<span class="ltx_text ltx_font_smallcaps" id="id10.id1.4">MeMemo</span> is available at <a class="ltx_ref ltx_href ltx_font_typewriter ltx_font_bold" href="https://github.com/poloclub/mememo" style="color:#1E88E5;" title="">https://github.com/poloclub/mememo</a>.</p>
</div>
<div class="ltx_keywords">Neural information retrieval, On-device, Large language models
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>rightsretained</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval; July 14–18, 2024; Washington, DC, USA</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_booktitle" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">booktitle: </span>Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’24), July 14–18, 2024, Washington, DC, USA</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>10.1145/3626772.3657662</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>979-8-4007-0431-4/24/07</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id7"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information systems Information retrieval</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id8"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing Human computer interaction (HCI)</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id9"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Machine learning</span></span></span>
<figure class="ltx_figure ltx_teaserfigure" id="S0.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="550" id="S0.F1.g1" src="x1.png" width="900"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S0.F1.6.1.1" style="font-size:90%;">Fig. 1</span>. </span><span class="ltx_text" id="S0.F1.7.2" style="font-size:90%;">
<span class="ltx_text ltx_font_smallcaps" id="S0.F1.7.2.1">MeMemo</span> is the first open-source JavaScript toolkit for in-browser dense neural retrieval.
We demonstrate the capabilities of <span class="ltx_text ltx_font_smallcaps" id="S0.F1.7.2.2">MeMemo</span> by developing <span class="ltx_text ltx_font_smallcaps" id="S0.F1.7.2.3">RAG Playground</span> that enables AI developers to prototype retrieval-augmented text generation (RAG) apps locally in their browsers.
With <span class="ltx_text ltx_font_smallcaps" id="S0.F1.7.2.4">RAG Playground</span>, developers can (A) enter various user queries, (B) search for semantically similar documents from an in-browser vector database, and (C) augment a text prompt with retrieved documents.
(D) This allows developers to rapidly test if in-browser large language models generate more reliable responses to the query.
</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S0.F1.8">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S0.F1.9">Teaser image for MeMemo.</p>
</div>
</div>
</figure>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Retrieval augmented generation (RAG) <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib40" title="">2020</a>)</cite> with large language models (LLMs) has gained immense popularity from both practitioners and researchers, especially in applications such as domain-specific chatbots <cite class="ltx_cite ltx_citemacro_citep">(Semnani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib65" title="">2023</a>; Prince et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib59" title="">2023</a>)</cite>, code generation <cite class="ltx_cite ltx_citemacro_citep">(Soare et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib68" title="">2022</a>; Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib81" title="">2023</a>)</cite>, and interactive agents <cite class="ltx_cite ltx_citemacro_citep">(Hsieh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib28" title="">2023</a>; Ruan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib63" title="">2023</a>)</cite>.
RAG can improve the accuracy and reliability of LLMs’ generated text <cite class="ltx_cite ltx_citemacro_citep">(Shuster et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib66" title="">2021</a>)</cite>, by providing these models, such as GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib57" title="">2023</a>)</cite> and Llama 2 <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib70" title="">2023</a>)</cite>, with context information retrieved from an updatable and external knowledge base.
Compared to other techniques, such as fine-tuning <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib29" title="">2021</a>)</cite> and prompt tuning <cite class="ltx_cite ltx_citemacro_citep">(Lester et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib39" title="">2021</a>)</cite>, that aim to improve LLM’s performance on new or specific domains, RAG is often favored by AI practitioners <cite class="ltx_cite ltx_citemacro_citep">(Martineau, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib49" title="">2023</a>)</cite> due to its ease of implementation, flexibility in maintenance, and superior performance <cite class="ltx_cite ltx_citemacro_citep">(Ovadia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib58" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">However, current RAG systems rely on dedicated backend servers to store and retrieve external documents relevant to the user’s query.
This is often achieved through nearest neighbor search using dense embedding vector representations of documents <cite class="ltx_cite ltx_citemacro_citep">(Balaguer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib5" title="">2024</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib41" title="">2023a</a>)</cite>.
The need for centralized backend servers limits the applicability of RAG in domains that prioritize data privacy, such as personal finance, education, and medicine <cite class="ltx_cite ltx_citemacro_citep">(e.g., Chung et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib12" title="">2023</a>; Wutschitz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib77" title="">2023</a>; Fuchsbauer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib22" title="">2021</a>; Ghodratnama and Zakershahrak, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib23" title="">2023</a>)</cite>.
Furthermore, implementing and hosting a vector storage and dense retriever pose additional challenges for AI novices and everyday LLM users <cite class="ltx_cite ltx_citemacro_citep">(Draxler et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib16" title="">2023</a>; Zamfirescu-Pereira et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib79" title="">2023</a>)</cite>, thereby increasing the barrier to entry for learning and applying RAG.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To address these pressing challenges, we present <span class="ltx_text ltx_font_smallcaps" id="S1.p3.1.1">MeMemo</span>, the first JavaScript toolkit that offloads vector storage and dense retrieval to the client—empowering a broader range of audiences to leverage cutting-edge retrieval techniques to enhance their LLM experiences.
Our work makes the following key <span class="ltx_text ltx_font_bold" id="S1.p3.1.2">contributions:</span></p>
</div>
<div class="ltx_para" id="S1.p4">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S1.I1.i1.p1.1.1">MeMemo<span class="ltx_text ltx_font_upright" id="S1.I1.i1.p1.1.1.1">, the first scalable JavaScript library</span></span> that enables users to store and retrieve large vector databases directly in their browsers.
Our toolkit adapts the state-of-the-art approximate nearest neighbor search Hierarchical Navigable Small World graphs (HNSW) <cite class="ltx_cite ltx_citemacro_citep">(Malkov and Yashunin, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib48" title="">2020</a>)</cite> to the Web environment.
By leveraging a novel prefetching strategy and modern Web technologies, such as IndexedDB and Web Workers, <span class="ltx_text ltx_font_smallcaps" id="S1.I1.i1.p1.1.2">MeMemo</span> empowers users to retrieve dense vectors with both privacy and efficiency (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.01972v1#S3" title="3. MeMemo Design and Implementation ‣ MeMemo: On-device Retrieval Augmentation for Private and Personalized Text Generation"><span class="ltx_text ltx_ref_tag">§ 3</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S1.I1.i2.p1.1.1">RAG Playground<span class="ltx_text ltx_font_upright" id="S1.I1.i2.p1.1.1.1">, an example application of on-device dense retrieval.</span></span>
We demonstrate the capabilities of <span class="ltx_text ltx_font_smallcaps" id="S1.I1.i2.p1.1.2">MeMemo</span> by developing <span class="ltx_text ltx_font_smallcaps" id="S1.I1.i2.p1.1.3">RAG Playground</span> (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.01972v1#S0.F1" title="Fig. 1 ‣ MeMemo: On-device Retrieval Augmentation for Private and Personalized Text Generation"><span class="ltx_text ltx_ref_tag">Fig. 1</span></a>), a novel client-side tool using on-device retrieval to enable interactive learning about RAG and rapid prototyping of RAG applications (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.01972v1#S2" title="2. MeMemo in Action ‣ MeMemo: On-device Retrieval Augmentation for Private and Personalized Text Generation"><span class="ltx_text ltx_ref_tag">§ 2</span></a>).
We highlight the benefit of on-device retrieval regarding <span class="ltx_text ltx_font_italic" id="S1.I1.i2.p1.1.4">privacy</span>, <span class="ltx_text ltx_font_italic" id="S1.I1.i2.p1.1.5">ubiquity</span>, and <span class="ltx_text ltx_font_italic" id="S1.I1.i2.p1.1.6">interactivity</span>.
Finally, we discuss the opportunities and challenges for future research on client-side retrieval augmentation and personalized text generation (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.01972v1#S5" title="5. Discussion and Future Work ‣ MeMemo: On-device Retrieval Augmentation for Private and Personalized Text Generation"><span class="ltx_text ltx_ref_tag">§ 5</span></a>).
<span class="ltx_text ltx_font_smallcaps" id="S1.I1.i2.p1.1.7">RAG Playground</span> is publicly accessible at <a class="ltx_ref ltx_href ltx_font_typewriter ltx_font_bold" href="https://poloclub.github.io/mememo" style="color:#1E88E5;" title="">https://poloclub.github.io/mememo</a>.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">An open-source<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnote1.1.1.1">1</span></span><span class="ltx_text ltx_font_smallcaps" id="footnote1.5">MeMemo</span><span class="ltx_text ltx_font_medium" id="footnote1.6"> code: </span><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://github.com/poloclub/mememo" style="color:#1E88E5;" title="">https://github.com/poloclub/mememo</a></span></span></span> implementation</span> that lowers the barrier for researchers and developers to apply retrieval augmentation to improve text generation on the client side.
We provide comprehensive documentation and an example application to help users use <span class="ltx_text ltx_font_smallcaps" id="S1.I1.i3.p1.1.2">MeMemo</span> to implement on-device retrieval augmentation across different Web environments.
<span class="ltx_text ltx_font_smallcaps" id="S1.I1.i3.p1.1.3">MeMemo</span> is developed with minimal dependencies and TypeScript, a statically typed programming language, making it a maintainable and easy-to-use resource for the information retrieval community.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">We hope our work will inspire the design, research, and development of on-device retrieval, enabling everyone to use text-generative models and other AI technologies more easily and privately.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">MeMemo</span> in Action</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">We present two hypothetical usage scenarios, developing (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.01972v1#S2.SS1" title="2.1. Developing In-browser RAG Tools ‣ 2. MeMemo in Action ‣ MeMemo: On-device Retrieval Augmentation for Private and Personalized Text Generation"><span class="ltx_text ltx_ref_tag">§ 2.1</span></a>) and using (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.01972v1#S2.SS2" title="2.2. Prototyping with RAG Playground ‣ 2. MeMemo in Action ‣ MeMemo: On-device Retrieval Augmentation for Private and Personalized Text Generation"><span class="ltx_text ltx_ref_tag">§ 2.2</span></a>) <span class="ltx_text ltx_font_smallcaps" id="S2.p1.1.1">RAG Playground</span>, to demonstrate how researchers and practitioners can use <span class="ltx_text ltx_font_smallcaps" id="S2.p1.1.2">MeMemo</span> to easily develop client-side applications that take advantage of on-device RAGs.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Developing In-browser RAG Tools</h3>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p1.1.1">Motivations.</span>
Assume an example scenario where Mei, a machine learning (ML) consultant, is currently developing an LLM-based chatbot for a large design studio.
The chatbot’s purpose is to assist new-hired designers in familiarizing themselves with the company’s internal design systems and tools.
To ensure accurate and reliable responses, Mei integrates RAG into this onboarding chatbot.
This integration allows the responses to be grounded by relevant documentation, design documents, and code.
Initially, Mei uses Jupyter Notebooks <cite class="ltx_cite ltx_citemacro_citep">(Kluyver et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib36" title="">2016</a>)</cite> to prototype the chatbot through prompt engineering in Python.
However, she realizes that this workflow is not ideal for collaborating with designers and introducing RAG to her clients.
This is because many of the collaborators and stakeholders are not experienced in programming and setting up notebook environments.
Therefore, Mei decides to develop <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.p1.1.2">RAG Playground</span> (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.01972v1#S0.F1" title="Fig. 1 ‣ MeMemo: On-device Retrieval Augmentation for Private and Personalized Text Generation"><span class="ltx_text ltx_ref_tag">Fig. 1</span></a>), a web-based no-code RAG prototyping tool.
This tool will enable her collaborators, who come from diverse backgrounds, to easily access and prototype RAG features for their chatbot through their web browsers.</p>
</div>
<figure class="ltx_float ltx_lstlisting" id="LST1">
<div class="ltx_listing ltx_lst_language_TypeScript ltx_lst_numbers_left ltx_lstlisting ltx_listing" id="LST1.1" style="background-color:#FFFFFF;">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,aW1wb3J0IHsgSE5TVyB9IGZyb20gJ21lbWVtbyc7CgovLyBDcmVhdGluZyBhIG5ldyBpbmRleApjb25zdCBpbmRleCA9IG5ldyBITlNXKHsgZGlzdGFuY2VGdW5jdGlvbjogJ2Nvc2luZScgfSk7CgovLyBJbnNlcnRpbmcgZWxlbWVudHMsIGtleXM6IHN0cmluZ1tdLCB2YWx1ZXM6IG51bWJlcltdW10KYXdhaXQgaW5kZXguYnVsa0luc2VydChrZXlzLCB2YWx1ZXMpOwoKLy8gRmluZCBrLW5lYXJlc3QgbmVpZ2hib3JzLCBxdWVyeTogbnVtYmVyW10sIGs6IG51bWJlcgovLyBrZXlzOiBzdHJpbmdbXSwgZGlzdGFuY2VzOiBudW1iZXJbXQpjb25zdCB7IGtleXMsIGRpc3RhbmNlcyB9ID0gYXdhaXQgaW5kZXgucXVlcnkocXVlcnksIGspOw==">⬇</a></div>
<div class="ltx_listingline" id="lstnumberx1">
<span class="ltx_tag ltx_tag_listingline">1</span><span class="ltx_text ltx_lst_keywords2 ltx_font_typewriter ltx_font_bold" id="lstnumberx1.1" style="font-size:80%;color:#404040;">import</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.2" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.3" style="font-size:80%;">{</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.4" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.5" style="font-size:80%;color:#000000;">HNSW</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.6" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.7" style="font-size:80%;">}</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.8" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.9" style="font-size:80%;color:#000000;">from</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.10" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx1.11" style="font-size:80%;color:#E91E63;">’mememo’</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.12" style="font-size:80%;">;</span>
</div>
<div class="ltx_listingline" id="lstnumberx2">
<span class="ltx_tag ltx_tag_listingline">2</span>
</div>
<div class="ltx_listingline" id="lstnumberx3">
<span class="ltx_tag ltx_tag_listingline">3</span><span class="ltx_text ltx_lst_comment ltx_font_typewriter" id="lstnumberx3.1" style="font-size:80%;color:#757575;">//<span class="ltx_text ltx_lst_space" id="lstnumberx3.1.1"> </span>Creating<span class="ltx_text ltx_lst_space" id="lstnumberx3.1.2"> </span>a<span class="ltx_text ltx_lst_space" id="lstnumberx3.1.3"> </span>new<span class="ltx_text ltx_lst_space" id="lstnumberx3.1.4"> </span>index</span>
</div>
<div class="ltx_listingline" id="lstnumberx4">
<span class="ltx_tag ltx_tag_listingline">4</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter ltx_font_bold" id="lstnumberx4.1" style="font-size:80%;color:#039BE5;">const</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.2" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.3" style="font-size:80%;color:#000000;">index</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.4" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.5" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.6" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter ltx_font_bold" id="lstnumberx4.7" style="font-size:80%;color:#039BE5;">new</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.8" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.9" style="font-size:80%;color:#000000;">HNSW</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.10" style="font-size:80%;">({</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.11" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.12" style="font-size:80%;color:#000000;">distanceFunction</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.13" style="font-size:80%;">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.14" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx4.15" style="font-size:80%;color:#E91E63;">’cosine’</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.16" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.17" style="font-size:80%;">});</span>
</div>
<div class="ltx_listingline" id="lstnumberx5">
<span class="ltx_tag ltx_tag_listingline">5</span>
</div>
<div class="ltx_listingline" id="lstnumberx6">
<span class="ltx_tag ltx_tag_listingline">6</span><span class="ltx_text ltx_lst_comment ltx_font_typewriter" id="lstnumberx6.1" style="font-size:80%;color:#757575;">//<span class="ltx_text ltx_lst_space" id="lstnumberx6.1.1"> </span>Inserting<span class="ltx_text ltx_lst_space" id="lstnumberx6.1.2"> </span>elements,<span class="ltx_text ltx_lst_space" id="lstnumberx6.1.3"> </span>keys:<span class="ltx_text ltx_lst_space" id="lstnumberx6.1.4"> </span>string[],<span class="ltx_text ltx_lst_space" id="lstnumberx6.1.5"> </span>values:<span class="ltx_text ltx_lst_space" id="lstnumberx6.1.6"> </span>number[][]</span>
</div>
<div class="ltx_listingline" id="lstnumberx7">
<span class="ltx_tag ltx_tag_listingline">7</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter ltx_font_bold" id="lstnumberx7.1" style="font-size:80%;color:#039BE5;">await</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.2" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.3" style="font-size:80%;color:#000000;">index</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx7.4" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.5" style="font-size:80%;color:#000000;">bulkInsert</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx7.6" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.7" style="font-size:80%;color:#000000;">keys</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx7.8" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.9" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.10" style="font-size:80%;color:#000000;">values</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx7.11" style="font-size:80%;">);</span>
</div>
<div class="ltx_listingline" id="lstnumberx8">
<span class="ltx_tag ltx_tag_listingline">8</span>
</div>
<div class="ltx_listingline" id="lstnumberx9">
<span class="ltx_tag ltx_tag_listingline">9</span><span class="ltx_text ltx_lst_comment ltx_font_typewriter" id="lstnumberx9.1" style="font-size:80%;color:#757575;">//<span class="ltx_text ltx_lst_space" id="lstnumberx9.1.1"> </span>Find<span class="ltx_text ltx_lst_space" id="lstnumberx9.1.2"> </span>k-nearest<span class="ltx_text ltx_lst_space" id="lstnumberx9.1.3"> </span>neighbors,<span class="ltx_text ltx_lst_space" id="lstnumberx9.1.4"> </span>query:<span class="ltx_text ltx_lst_space" id="lstnumberx9.1.5"> </span>number[],<span class="ltx_text ltx_lst_space" id="lstnumberx9.1.6"> </span>k:<span class="ltx_text ltx_lst_space" id="lstnumberx9.1.7"> </span>number</span>
</div>
<div class="ltx_listingline" id="lstnumberx10">
<span class="ltx_tag ltx_tag_listingline">10</span><span class="ltx_text ltx_lst_comment ltx_font_typewriter" id="lstnumberx10.1" style="font-size:80%;color:#757575;">//<span class="ltx_text ltx_lst_space" id="lstnumberx10.1.1"> </span>keys:<span class="ltx_text ltx_lst_space" id="lstnumberx10.1.2"> </span>string[],<span class="ltx_text ltx_lst_space" id="lstnumberx10.1.3"> </span>distances:<span class="ltx_text ltx_lst_space" id="lstnumberx10.1.4"> </span>number[]</span>
</div>
<div class="ltx_listingline" id="lstnumberx11">
<span class="ltx_tag ltx_tag_listingline">11</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter ltx_font_bold" id="lstnumberx11.1" style="font-size:80%;color:#039BE5;">const</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.2" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.3" style="font-size:80%;">{</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.4" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.5" style="font-size:80%;color:#000000;">keys</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.6" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.7" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.8" style="font-size:80%;color:#000000;">distances</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.9" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.10" style="font-size:80%;">}</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.11" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.12" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.13" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter ltx_font_bold" id="lstnumberx11.14" style="font-size:80%;color:#039BE5;">await</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.15" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.16" style="font-size:80%;color:#000000;">index</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.17" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.18" style="font-size:80%;color:#000000;">query</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.19" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.20" style="font-size:80%;color:#000000;">query</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.21" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.22" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.23" style="font-size:80%;color:#000000;">k</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.24" style="font-size:80%;">);</span>
</div>
</div>
<figcaption class="ltx_caption" style="background-color:#FFFFFF;"><span class="ltx_tag ltx_tag_float">Code 1: </span>Example TypeScript code that uses <span class="ltx_text ltx_font_smallcaps" id="LST1.5.1">MeMemo</span> to create an HNSW index and search for k-nearest neighbors.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p2.1.1">Vector storage and retrieval with <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.p2.1.1.1">MeMemo</span>.</span>
Mei uses <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.p2.1.2">MeMemo</span>, a JavaScript library, to enable dense vector storage and retrieval directly in the browser.
By installing the library with a single command <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p2.1.3">npm install mememo</span>, Mei can easily import it into her web app, regardless of her web development stack (e.g., JavaScript, TypeScript, React <cite class="ltx_cite ltx_citemacro_citep">(Facebook, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib17" title="">2013</a>)</cite>, Svelte <cite class="ltx_cite ltx_citemacro_citep">(Harris, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib26" title="">2016</a>)</cite>, or Lit <cite class="ltx_cite ltx_citemacro_citep">(Google, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib25" title="">2015</a>)</cite>).
With just a few lines of code (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.01972v1#LST1" title="Code 1 ‣ 2.1. Developing In-browser RAG Tools ‣ 2. MeMemo in Action ‣ MeMemo: On-device Retrieval Augmentation for Private and Personalized Text Generation"><span class="ltx_text ltx_ref_tag">Code 1</span></a>), Mei can create an HNSW vector index <cite class="ltx_cite ltx_citemacro_citep">(Malkov and Yashunin, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib48" title="">2020</a>)</cite> and efficiently search through millions of embedding vectors entirely within her browser.
Mei also uses <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.p2.1.4">MeMemo</span>’s <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p2.1.5">exportIndex()</span> and <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p2.1.6">loadIndex()</span> functions to export an index she has created into persistent local storage or as a JSON file.
This allows her collaborators to quickly load the HNSW index without the need to recreate it every time they use <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.p2.1.7">RAG Playground</span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p3.1.1">Smooth integration with existing Web ML technologies.</span>

Mei seamlessly integrates <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.p3.1.2">MeMemo</span> with other Web ML technologies.
For example, she uses IndexedDB, a client-side key-value browser storage, to store the raw documents.
Using the same keys, Mei creates the HNSW index with <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.p3.1.3">MeMemo</span>.
Then, Mei uses FlexSearch <cite class="ltx_cite ltx_citemacro_citep">(Wilkerling, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib76" title="">2019</a>)</cite> to implement fast full-text lexical search in the browser.
To enable semantic search, Mei first uses GTE-Small <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib42" title="">2023b</a>)</cite> to encode all documents into dense vectors with 384 dimensions in Python with SentenceTransformers <cite class="ltx_cite ltx_citemacro_citep">(Reimers and Gurevych, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib62" title="">2019</a>)</cite>.
For encoding the user’s query (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.01972v1#S0.F1" title="Fig. 1 ‣ MeMemo: On-device Retrieval Augmentation for Private and Personalized Text Generation"><span class="ltx_text ltx_ref_tag">Fig. 1</span></a><span class="ltx_text" id="S2.SS1.p3.1.4" style="color:#4D00D9;">A</span>), Mei uses ONNX <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib4" title="">2019</a>)</cite> and Transformer.js <cite class="ltx_cite ltx_citemacro_citep">(Lochner, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib44" title="">2023</a>)</cite> to run the same GTE-Small model in the browser.
After augmenting a text prompt with retrieved documents (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.01972v1#S0.F1" title="Fig. 1 ‣ MeMemo: On-device Retrieval Augmentation for Private and Personalized Text Generation"><span class="ltx_text ltx_ref_tag">Fig. 1</span></a><span class="ltx_text" id="S2.SS1.p3.1.5" style="color:#4D00D9;">C</span>), Mei runs the prompt with open-source LLMs, such as LLama 2 <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib70" title="">2023</a>)</cite> and Phi 2 <cite class="ltx_cite ltx_citemacro_citep">(Abdin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib2" title="">2023</a>)</cite>, in the browser through Web LLM <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">teamMLCLLM2023</span>)</cite>.
By combining <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.p3.1.6">MeMemo</span> with existing Web ML technologies, Mei quickly develops <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.p3.1.7">RAG Playground</span> and shares it with her collaborators.
With this tool, Mei’s team has made great progress as all stakeholders with diverse backgrounds can easily experiment with different user queries and prompts to improve their onboarding chatbot.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Prototyping with <span class="ltx_text ltx_font_smallcaps" id="S2.SS2.2.1">RAG Playground</span>
</h3>
<div class="ltx_para ltx_noindent" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p1.1.1">Motivations.</span>
Robaire, a graduate student studying human-computer interaction, is designing an interactive visualization tool to assist researchers in brainstorming and literature review.
After discovering RAG online, Robaire becomes interested in integrating it into his prototype.
The objective is to allow users to input a large corpus of academic papers and use natural language queries to discover related papers and visualize the connections between them.
Since Robaire has never implemented RAG before, he turns to <span class="ltx_text ltx_font_smallcaps" id="S2.SS2.p1.1.2">RAG Playground</span> to learn about the concept and prototype for his tool.</p>
</div>
<figure class="ltx_figure" id="S2.SS2.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="164" id="S2.SS2.1.g1" src="x2.png" width="141"/>
</figure>
<div class="ltx_para ltx_noindent" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p2.1.2">Learning and experimenting with RAG.</span>
After opening <span class="ltx_text ltx_font_smallcaps" id="S2.SS2.p2.1.3">RAG Playground</span> in the browser, Robaire creates a <span class="ltx_text ltx_font_smallcaps" id="S2.SS2.p2.1.4">MeMemo</span> database (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.01972v1#S0.F1" title="Fig. 1 ‣ MeMemo: On-device Retrieval Augmentation for Private and Personalized Text Generation"><span class="ltx_text ltx_ref_tag">Fig. 1</span></a><span class="ltx_text" id="S2.SS2.p2.1.5" style="color:#4D00D9;">B</span>) by uploading a JSON file containing the abstracts of 120k arXiv ML papers and 384-dimensional embeddings of the abstracts.
Robaire then pretends to be his end-users and types in a natural language query in the <span class="ltx_text ltx_font_italic" id="S2.SS2.p2.1.6">User Query View</span>, such as “how to integrate information retrieval into ML?” (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.01972v1#S0.F1" title="Fig. 1 ‣ MeMemo: On-device Retrieval Augmentation for Private and Personalized Text Generation"><span class="ltx_text ltx_ref_tag">Fig. 1</span></a><span class="ltx_text" id="S2.SS2.p2.1.7" style="color:#4D00D9;">A</span>).
In addition, he writes a simple system prompt template in the <span class="ltx_text ltx_font_italic" id="S2.SS2.p2.1.8">Prompt View</span> (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.01972v1#S0.F1" title="Fig. 1 ‣ MeMemo: On-device Retrieval Augmentation for Private and Personalized Text Generation"><span class="ltx_text ltx_ref_tag">Fig. 1</span></a><span class="ltx_text" id="S2.SS2.p2.1.9" style="color:#4D00D9;">C</span>) with placeholders <span class="ltx_text ltx_font_typewriter" id="S2.SS2.p2.1.10">{{user}}</span> and <span class="ltx_text ltx_font_typewriter" id="S2.SS2.p2.1.11">{{context}}</span>.
After clicking the <span class="ltx_text" id="S2.SS2.p2.1.1" style="position:relative; bottom:-1.5pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="17" id="S2.SS2.p2.1.1.g1" src="x3.png" width="40"/></span> button, Robaire sees 10 relevant paper abstracts with their Cosine distances highlighted in the <span class="ltx_text ltx_font_italic" id="S2.SS2.p2.1.12">Database View</span> (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.01972v1#S0.F1" title="Fig. 1 ‣ MeMemo: On-device Retrieval Augmentation for Private and Personalized Text Generation"><span class="ltx_text ltx_ref_tag">Fig. 1</span></a><span class="ltx_text" id="S2.SS2.p2.1.13" style="color:#4D00D9;">B</span>).
He also finds that the two placeholders in the <span class="ltx_text ltx_font_italic" id="S2.SS2.p2.1.14">Prompt View</span> are replaced with the user query and relevant documents.
Robaire then sees the LLM’s output in the <span class="ltx_text ltx_font_italic" id="S2.SS2.p2.1.15">Output View</span> (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.01972v1#S0.F1" title="Fig. 1 ‣ MeMemo: On-device Retrieval Augmentation for Private and Personalized Text Generation"><span class="ltx_text ltx_ref_tag">Fig. 1</span></a><span class="ltx_text" id="S2.SS2.p2.1.16" style="color:#4D00D9;">D</span>).
Finding the output helpful and grounded by the documents retrieved by <span class="ltx_text ltx_font_smallcaps" id="S2.SS2.p2.1.17">MeMemo</span>, Robaire experiments with more prompts and both remote and local LLMs (e.g., GPT 4 and Llama 2 shown in the figure above) in <span class="ltx_text ltx_font_smallcaps" id="S2.SS2.p2.1.18">RAG Playground</span> and gains a better understanding of RAG.
This increased understanding gives him more confidence to implement RAG in his tool.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">MeMemo</span> Design and Implementation</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S3.p1.1.1">MeMemo</span> is the first JavaScript toolkit that enables dense retrieval in the browser.
To enable fast and reliable retrieval for RAG, our tool adapts the state-of-the-art approximate nearest neighbor search technique HNSW (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.01972v1#S3.SS1" title="3.1. Adapting HNSW ‣ 3. MeMemo Design and Implementation ‣ MeMemo: On-device Retrieval Augmentation for Private and Personalized Text Generation"><span class="ltx_text ltx_ref_tag">§ 3.1</span></a>).
<span class="ltx_text ltx_font_smallcaps" id="S3.p1.1.2">MeMemo</span> leveraging modern and native Web technologies, such as IndexedDB and Web Workers (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.01972v1#S3.SS2" title="3.2. Optimizing for the Browsers ‣ 3. MeMemo Design and Implementation ‣ MeMemo: On-device Retrieval Augmentation for Private and Personalized Text Generation"><span class="ltx_text ltx_ref_tag">§ 3.2</span></a>), to optimize for browser environments.
To help researchers and developers adopt <span class="ltx_text ltx_font_smallcaps" id="S3.p1.1.3">MeMemo</span>, we have open-sourced it and provided detailed documentation, tutorial, and an example application (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.01972v1#S3.SS3" title="3.3. Open-source and Easy to Use ‣ 3. MeMemo Design and Implementation ‣ MeMemo: On-device Retrieval Augmentation for Private and Personalized Text Generation"><span class="ltx_text ltx_ref_tag">§ 3.3</span></a>).</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Adapting HNSW</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">HNSW is a state-of-the-art approximate k-nearest neighbor search technique introduced by <cite class="ltx_cite ltx_citemacro_citeauthor"><a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib48" title="">Malkov and Yashunin</a></cite>.
It is inspired by the greedy graph routing used in navigable small world networks <cite class="ltx_cite ltx_citemacro_citep">(Kleinberg, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib35" title="">2000</a>; Boguñá et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib7" title="">2009</a>)</cite> and the stochastic hierarchical structure in 1D probabilistic skip list <cite class="ltx_cite ltx_citemacro_citep">(Pugh, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib60" title="">1990</a>)</cite>.
HNSW uses a multilayered graph structure to connect high-dimensional dense vectors.
During the insertion process, each new element is assigned a layer level at random, determining its position within the graph’s multi-layered hierarchy.
The insertion process involves finding the element’s closest neighbors, starting from the top layer and working downwards using a greedy search approach.
When searching for the nearest neighbors of a query element, the algorithm follows a similar procedure.
It starts from the top layer and uses the connections established during the insertion phase to guide its search downwards.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">We use HNSW as the approximate nearest neighbor search technique in <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.p2.1.1">MeMemo</span> because it is the state-of-the-art regarding construction and query efficiency <cite class="ltx_cite ltx_citemacro_citep">(Malkov and Yashunin, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib48" title="">2020</a>)</cite>.
Additionally, HNSW has gained immense popularity among retrieval and AI practitioners and has been integrated into popular retrieval and RAG Python toolkits such as FAISS <cite class="ltx_cite ltx_citemacro_citep">(Douze et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib15" title="">2024</a>)</cite>, Pyserini <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib43" title="">2021</a>)</cite>, PGVector <cite class="ltx_cite ltx_citemacro_citep">(Kane, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib33" title="">2021</a>)</cite>, and LangChain <cite class="ltx_cite ltx_citemacro_citep">(Chase, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib9" title="">2022</a>)</cite>.
Our goal with <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.p2.1.2">MeMemo</span> is to seamlessly integrate into users’ existing workflows and preferences, providing a smooth and familiar experience when developing in-browser retrieval applications.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Optimizing for the Browsers</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.1">Memory management.</span>
Memory management is one of the main challenges for developing in-browser toolkits.
Depending on the device and browser, a webpage tab might have a RAM limit as low as 256MB <cite class="ltx_cite ltx_citemacro_citep">(Maitre, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib47" title="">2018</a>)</cite>.
This means that without considering any other memory usage on a webpage, it can store at most 83k 384-dimensional vectors in RAM.
Additionally, for security reasons, browsers do not allow access to the operating system’s file systems, so <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.p1.1.2">MeMemo</span> cannot directly store data in the user’s disk.
To overcome these challenges, <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.p1.1.3">MeMemo</span> leverages IndexedDB <cite class="ltx_cite ltx_citemacro_citep">(MDN, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib50" title="">2021</a>)</cite>, a cross-browser key-value storage that can use up to 80% of the client’s disk size <cite class="ltx_cite ltx_citemacro_citep">(MDN, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib51" title="">2023a</a>)</cite>.
In IndexedDB, <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.p1.1.4">MeMemo</span> stores all vector values, while only keeping the keys and HNSW graphs in the RAM.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.3"><span class="ltx_text ltx_font_bold" id="S3.SS2.p2.3.1">Prefetching for efficient data access.</span>
While IndexedDB addresses the memory constraints in the browser, reading or writing a large amount of data to IndexedDB with consecutive transactions is extremely slow <cite class="ltx_cite ltx_citemacro_citep">(RxDB, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib64" title="">2021</a>)</cite>.
Dexie.js <cite class="ltx_cite ltx_citemacro_citep">(Fahlander, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib18" title="">2021</a>)</cite> introduces techniques for fast batched read and write to IndexedDB.
However, the HNSW construction process requires consecutive reads and writes of vector values, as the algorithm relies on the previously constructed index for finding good neighbors <cite class="ltx_cite ltx_citemacro_citep">(Mendel-Gleason, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib54" title="">2024</a>; Malkov and Yashunin, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib48" title="">2020</a>)</cite>.
To address this challenge, <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.p2.3.2">MeMemo</span> introduces a prefetching mechanism.
When inserting multiple elements, <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.p2.3.3">MeMemo</span> first uses a batched write to store all vectors in IndexedDB.
During construction and search, <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.p2.3.4">MeMemo</span> maintains a cache of <math alttext="p" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">italic_p</annotation></semantics></math> vector values in RAM.
If it needs to read a vector value that is not in the cache, <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.p2.3.5">MeMemo</span> prefetches <math alttext="p" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m2.1"><semantics id="S3.SS2.p2.2.m2.1a"><mi id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><ci id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">p</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m2.1d">italic_p</annotation></semantics></math> neighbors of that element on the current graph layer from IndexedDB to RAM.
This mechanism reduces the number of IndexedDB transactions.
The parameter <math alttext="p" class="ltx_Math" display="inline" id="S3.SS2.p2.3.m3.1"><semantics id="S3.SS2.p2.3.m3.1a"><mi id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><ci id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">p</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m3.1d">italic_p</annotation></semantics></math> is automatically determined by the vector dimension and can be configured by users.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Open-source and Easy to Use</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.2">To help researchers and developers easily adopt <span class="ltx_text ltx_font_smallcaps" id="S3.SS3.p1.2.1">MeMemo</span>, we open source our implementation and design APIs similar to popular HNSW Python libraries <cite class="ltx_cite ltx_citemacro_citep">(e.g., Malkov and Yashunin, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib48" title="">2020</a>; Douze et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib15" title="">2024</a>; Zhu, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib82" title="">2016</a>)</cite>.
Users can easily configure all HNSW parameters, such as <math alttext="M" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">M</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">italic_M</annotation></semantics></math> (the number of neighbors a graph node can have) and <math alttext="\mathit{efConstruction}" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2.1"><semantics id="S3.SS3.p1.2.m2.1a"><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">𝑒𝑓𝐶𝑜𝑛𝑠𝑡𝑟𝑢𝑐𝑡𝑖𝑜𝑛</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">𝑒𝑓𝐶𝑜𝑛𝑠𝑡𝑟𝑢𝑐𝑡𝑖𝑜𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">\mathit{efConstruction}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m2.1d">italic_efConstruction</annotation></semantics></math> (the number of nodes to search during construction).
With just a few lines of code (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.01972v1#LST1" title="Code 1 ‣ 2.1. Developing In-browser RAG Tools ‣ 2. MeMemo in Action ‣ MeMemo: On-device Retrieval Augmentation for Private and Personalized Text Generation"><span class="ltx_text ltx_ref_tag">Code 1</span></a>), users can quickly implement dense retrieval in web browsers using <span class="ltx_text ltx_font_smallcaps" id="S3.SS3.p1.2.2">MeMemo</span>.
We provide detailed documentation and tutorials.
Additionally, we offer an open-source example application <span class="ltx_text ltx_font_smallcaps" id="S3.SS3.p1.2.3">RAG Playground</span> that demonstrates the integration of <span class="ltx_text ltx_font_smallcaps" id="S3.SS3.p1.2.4">MeMemo</span> with existing Web ML technologies (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.01972v1#S2.SS1" title="2.1. Developing In-browser RAG Tools ‣ 2. MeMemo in Action ‣ MeMemo: On-device Retrieval Augmentation for Private and Personalized Text Generation"><span class="ltx_text ltx_ref_tag">§ 2.1</span></a>).
<span class="ltx_text ltx_font_smallcaps" id="S3.SS3.p1.2.5">RAG Playground</span> also shows how to use <span class="ltx_text ltx_font_smallcaps" id="S3.SS3.p1.2.6">MeMemo</span> with modern Web APIs, including Web Workers <cite class="ltx_cite ltx_citemacro_citep">(MDN, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib53" title="">2023c</a>)</cite> to prevent blocking the main thread and Streams API <cite class="ltx_cite ltx_citemacro_citep">(MDN, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib52" title="">2023b</a>)</cite> for creating an HNSW index incrementally with small network-received chunk.
<span class="ltx_text ltx_font_smallcaps" id="S3.SS3.p1.2.7">MeMemo</span> is published in the popular <a class="ltx_ref ltx_href ltx_font_bold" href="https://www.npmjs.com/package/mememo" style="color:#1E88E5;" title="">Web package repository <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p1.2.8.1">npm</span> Registry</a>, and can be easily installed and used in both browser and Node.js <cite class="ltx_cite ltx_citemacro_citep">(Dahl, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib14" title="">2009</a>)</cite> environments.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Related Work</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1"><span class="ltx_text ltx_font_bold" id="S4.p1.1.1">Retrieval-augmented text generation.</span>
There has been a long history of using information retrieval to enhance text generation, such as developing language models through retrieval <cite class="ltx_cite ltx_citemacro_citep">(Lavrenko and Croft, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib38" title="">2001</a>)</cite>, using a retrieve-and-edit framework to improve code generation <cite class="ltx_cite ltx_citemacro_citep">(Hashimoto et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib27" title="">2018</a>)</cite>, and incorporating knowledge graphs to enhance language representation in language models <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib80" title="">2019</a>)</cite>.
The concept of RAG was popularized by <cite class="ltx_cite ltx_citemacro_citeauthor"><a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib40" title="">Lewis et al<span class="ltx_text">.</span></a></cite>, who introduced a model that combines a dense passage reliever and sequence-to-sequence models.
More recent approaches <cite class="ltx_cite ltx_citemacro_citep">(e.g., Neelakantan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib56" title="">2022</a>; Qu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib61" title="">2021</a>; Izacard and Grave, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib30" title="">2021</a>; Cuconasu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib13" title="">2024</a>)</cite> use pre-trained embedding models to encode external documents as dense vectors and retrieve relevant documents using dense retrievers such as HNSW <cite class="ltx_cite ltx_citemacro_citep">(Malkov and Yashunin, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib48" title="">2020</a>)</cite>, PQ <cite class="ltx_cite ltx_citemacro_citep">(Jégou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib31" title="">2011</a>)</cite>, and FAISS <cite class="ltx_cite ltx_citemacro_citep">(Douze et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib15" title="">2024</a>)</cite>.
<span class="ltx_text ltx_font_smallcaps" id="S4.p1.1.2">MeMemo</span> builds upon these works and extends RAG to the client side for more private and personalized text generation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p2">
<p class="ltx_p" id="S4.p2.1"><span class="ltx_text ltx_font_bold" id="S4.p2.1.1">On-device retrieval and machine learning.</span>
Traditional retrieval and machine learning (ML) systems are typically deployed on remote servers, and their outputs are sent to client devices.
However, there has been a recent surge of interest in deploying ML models directly on edge devices in the pursuit of private, ubiquitous, and interactive ML experiences.
Tools such as TensorFlow.js <cite class="ltx_cite ltx_citemacro_citep">(Smilkov et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib67" title="">2019</a>)</cite>, ONNX <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib4" title="">2019</a>)</cite>, MLC <cite class="ltx_cite ltx_citemacro_citep">(MLC, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib55" title="">2023</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib11" title="">2018</a>)</cite>, and Core ML <cite class="ltx_cite ltx_citemacro_citep">(Apple, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib3" title="">2017</a>)</cite> have significantly reduced the barriers to running complex ML models in browsers and mobile devices.
Researchers have proposed various on-device systems, including information retrieval <cite class="ltx_cite ltx_citemacro_citep">(Kamvar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib32" title="">2009</a>; Lam et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib37" title="">2023</a>)</cite>, recommender systems <cite class="ltx_cite ltx_citemacro_citep">(Gong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib24" title="">2020</a>; Xia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib78" title="">2023</a>)</cite>, prediction explanation <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib74" title="">2022</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib75" title="">2023b</a>; Wang and Chau, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib72" title="">2023</a>)</cite>, speech recognition <cite class="ltx_cite ltx_citemacro_citep">(Macoskey et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib46" title="">2021b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib45" title="">a</a>)</cite>, translation <cite class="ltx_cite ltx_citemacro_citep">(Tan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib69" title="">2022</a>)</cite>, and writing assistants <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib71" title="">2024</a>)</cite>.
Our tool contributes to the growing body of on-device ML research by introducing the first adaptation of dense retrieval to browsers.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Discussion and Future Work</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Reflecting on our development of <span class="ltx_text ltx_font_smallcaps" id="S5.p1.1.1">MeMemo</span>, we highlight the opportunities and challenges for in-browser dense retrieval.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text ltx_font_bold" id="S5.p2.1.1">Opportunities.</span>
Enabling dense retrieval and RAG in browsers offers significant advantages regarding <span class="ltx_text ltx_font_italic" id="S5.p2.1.2">privacy</span>, <span class="ltx_text ltx_font_italic" id="S5.p2.1.3">ubiquity</span>, and <span class="ltx_text ltx_font_italic" id="S5.p2.1.4">interactivity</span>.
With the browser’s ubiquity, <span class="ltx_text ltx_font_smallcaps" id="S5.p2.1.5">MeMemo</span> is accessible on various devices, including laptops, mobile phones, and IoT appliances like smart refrigerators.
Future research directions include:</p>
</div>
<div class="ltx_para" id="S5.p3">
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i1.p1.1.1">Intelligent personal information management.</span>
There is a large body of research on collecting all of one’s personal information into a searchable database <cite class="ltx_cite ltx_citemacro_citep">(e.g., Freeman and Gelernter, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib21" title="">1996</a>; Cai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib8" title="">2005</a>; Bell, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib6" title="">2001</a>; Chau et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib10" title="">2008</a>; Kiesel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib34" title="">2018</a>)</cite>.
Researchers can leverage on-device dense storage and retrieval to design browser extensions that automatically and privately encode and store a user’s visited web pages, photos, and academic papers.
These extensions can serve as an intelligent “second brain” <cite class="ltx_cite ltx_citemacro_citep">(Forte, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib20" title="">2022</a>)</cite> to help users capture and review knowledge.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i2.p1.1.1">Private and personalized content creation.</span>
If users maintain a personal vector database in browsers, content creators, such as book writers, can use on-device RAG to tailor their content privately based on readers’ preferences and reading history.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i3.p1.1.1">Interactive RAG prototyping.</span>
Future researchers can enhance the design of <span class="ltx_text ltx_font_smallcaps" id="S5.I1.i3.p1.1.2">RAG Playground</span> to improve interactive RAG prototyping experience, such as supporting collaborative prompt editing <cite class="ltx_cite ltx_citemacro_citep">(Feng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib19" title="">2023</a>)</cite> and interactive embedding visualizations <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib73" title="">2023a</a>)</cite>.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S5.p4">
<p class="ltx_p" id="S5.p4.2"><span class="ltx_text ltx_font_bold" id="S5.p4.2.1">Challenges.</span>
Due to limited computation resources in browsers, <span class="ltx_text ltx_font_smallcaps" id="S5.p4.2.2">MeMemo</span> is slower than heavily optimized libraries like <span class="ltx_text ltx_font_typewriter" id="S5.p4.2.3">HNSWLIB</span> <cite class="ltx_cite ltx_citemacro_citep">(Malkov and Yashunin, <a class="ltx_ref" href="https://arxiv.org/html/2407.01972v1#bib.bib48" title="">2020</a>)</cite> in terms of index creation and search.
In Chrome on a 64GB RAM MacBook, it took about 94 minutes to insert 1 million 384-dimensional vectors (<math alttext="M" class="ltx_Math" display="inline" id="S5.p4.1.m1.1"><semantics id="S5.p4.1.m1.1a"><mi id="S5.p4.1.m1.1.1" xref="S5.p4.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.p4.1.m1.1b"><ci id="S5.p4.1.m1.1.1.cmml" xref="S5.p4.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.1.m1.1c">M</annotation><annotation encoding="application/x-llamapun" id="S5.p4.1.m1.1d">italic_M</annotation></semantics></math>=5, <math alttext="\mathit{efConstruction}" class="ltx_Math" display="inline" id="S5.p4.2.m2.1"><semantics id="S5.p4.2.m2.1a"><mi id="S5.p4.2.m2.1.1" xref="S5.p4.2.m2.1.1.cmml">𝑒𝑓𝐶𝑜𝑛𝑠𝑡𝑟𝑢𝑐𝑡𝑖𝑜𝑛</mi><annotation-xml encoding="MathML-Content" id="S5.p4.2.m2.1b"><ci id="S5.p4.2.m2.1.1.cmml" xref="S5.p4.2.m2.1.1">𝑒𝑓𝐶𝑜𝑛𝑠𝑡𝑟𝑢𝑐𝑡𝑖𝑜𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.2.m2.1c">\mathit{efConstruction}</annotation><annotation encoding="application/x-llamapun" id="S5.p4.2.m2.1d">italic_efConstruction</annotation></semantics></math>=20).
However, querying this index with 1M items is still performed in real time.
Future researchers can optimize in-browser dense retrieval further by implementing parallelization and smarter prefetching techniques.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p5">
<p class="ltx_p" id="S5.p5.1"><span class="ltx_text ltx_font_bold" id="S5.p5.1.1">Conclusions.</span>
We present <span class="ltx_text ltx_font_smallcaps" id="S5.p5.1.2">MeMemo</span>, an open-source library that enables in-browser dense retrieval using HNSW and modern Web technologies.
We introduce <span class="ltx_text ltx_font_smallcaps" id="S5.p5.1.3">RAG Playground</span>, a novel client-side RAG prototyping tool to demonstrate the capabilities of <span class="ltx_text ltx_font_smallcaps" id="S5.p5.1.4">MeMemo</span>.
We hope <span class="ltx_text ltx_font_smallcaps" id="S5.p5.1.5">MeMemo</span> to be an easy-to-use resource for the information retrieval and ML community, inspiring future research and development of on-device retrieval and RAG applications.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abdin et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Marah Abdin, Jyoti Aneja,
Sebastien Bubeck, Caio César Teodoro
Mendes, Weizhu Chen, Allie Del Giorno,
Ronen Eldan, Sivakanth Gopi,
Suriya Gunasekar, Mojan Javaheripi,
Piero Kauffmann, Yin Tat Lee,
Yuanzhi Li, Anh Nguyen,
Gustavo de Rosa, Olli Saarikivi,
Adil Salim, Shital Shah,
Michael Santacroce, Harkirat Singh Behl,
Adam Taumann Kalai, Xin Wang,
Rachel Ward, Philipp Witte,
Cyril Zhang, and Yi Zhang.
2023.

</span>
<span class="ltx_bibblock">Phi-2: The Surprising Power of Small Language
Models.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/" title="">https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Apple (2017)</span>
<span class="ltx_bibblock">
Apple. 2017.

</span>
<span class="ltx_bibblock">Core ML: Integrate Machine Learning Models
into Your App.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developer.apple.com/documentation/coreml" title="">https://developer.apple.com/documentation/coreml</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Junjie Bai, Fang Lu,
and Ke Zhang. 2019.

</span>
<span class="ltx_bibblock">ONNX: Open Neural Network Exchange.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/onnx/onnx" title="">https://github.com/onnx/onnx</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Balaguer et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Angels Balaguer, Vinamra
Benara, Renato Luiz de Freitas Cunha,
Roberto de M. Estevão Filho, Todd
Hendry, Daniel Holstein, Jennifer
Marsman, Nick Mecklenburg, Sara Malvar,
Leonardo O. Nunes, Rafael Padilha,
Morris Sharp, Bruno Silva,
Swati Sharma, Vijay Aski, and
Ranveer Chandra. 2024.

</span>
<span class="ltx_bibblock">RAG vs Fine-tuning: Pipelines,
Tradeoffs, and a Case Study on Agriculture.

</span>
<span class="ltx_bibblock">(2024).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2401.08406" title="">https://doi.org/10.48550/ARXIV.2401.08406</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bell (2001)</span>
<span class="ltx_bibblock">
Gordon Bell.
2001.

</span>
<span class="ltx_bibblock">A Personal Digital Store.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Commun. ACM</em> 44
(2001).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/357489.357513" title="">https://doi.org/10.1145/357489.357513</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Boguñá et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2009)</span>
<span class="ltx_bibblock">
Marián Boguñá,
Dmitri Krioukov, and K. C. Claffy.
2009.

</span>
<span class="ltx_bibblock">Navigability of Complex Networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">Nature Physics</em> 5
(2009).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1038/nphys1130" title="">https://doi.org/10.1038/nphys1130</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2005)</span>
<span class="ltx_bibblock">
Yuhan Cai, Xin Luna Dong,
Alon Halevy, Jing Michelle Liu, and
Jayant Madhavan. 2005.

</span>
<span class="ltx_bibblock">Personal Information Management with SEMEX. In
<em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">Proceedings of the 2005 ACM SIGMOD
International Conference on Management of Data</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/1066157.1066289" title="">https://doi.org/10.1145/1066157.1066289</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chase (2022)</span>
<span class="ltx_bibblock">
Harrison Chase.
2022.

</span>
<span class="ltx_bibblock">LangChain: Building Applications with
LLMs through Composability.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/langchain-ai/langchain" title="">https://github.com/langchain-ai/langchain</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chau et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2008)</span>
<span class="ltx_bibblock">
Duen Horng Chau, Brad
Myers, and Andrew Faulring.
2008.

</span>
<span class="ltx_bibblock">What to Do When Search Fails: Finding Information
by Association. In <em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">Proceedings of the SIGCHI
Conference on Human Factors in Computing Systems</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/1357054.1357208" title="">https://doi.org/10.1145/1357054.1357208</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Tianqi Chen, Thierry
Moreau, Ziheng Jiang, Lianmin Zheng,
Eddie Yan, Haichen Shen,
Meghan Cowan, Leyuan Wang,
Yuwei Hu, Luis Ceze,
Carlos Guestrin, and Arvind
Krishnamurthy. 2018.

</span>
<span class="ltx_bibblock">TVM: An Automated End-to-End Optimizing
Compiler for Deep Learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">13th USENIX
Symposium on Operating Systems Design and Implementation (OSDI 18)</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.usenix.org/conference/osdi18/presentation/chen" title="">https://www.usenix.org/conference/osdi18/presentation/chen</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Neo Christopher Chung,
George Dyer, and Lennart Brocki.
2023.

</span>
<span class="ltx_bibblock">Challenges of Large Language Models for
Mental Health Counseling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">arXiv 2311.13857</em> (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2311.13857" title="">http://arxiv.org/abs/2311.13857</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cuconasu et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Florin Cuconasu, Giovanni
Trappolini, Federico Siciliano, Simone
Filice, Cesare Campagnano, Yoelle
Maarek, Nicola Tonellotto, and Fabrizio
Silvestri. 2024.

</span>
<span class="ltx_bibblock">The Power of Noise: Redefining
Retrieval for RAG Systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">arXiv 2401.14887</em> (2024).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2401.14887" title="">http://arxiv.org/abs/2401.14887</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dahl (2009)</span>
<span class="ltx_bibblock">
Ryan Dahl.
2009.

</span>
<span class="ltx_bibblock">Node.Js: An Open-Source, Cross-Platform
JavaScript Runtime Environment.

</span>
<span class="ltx_bibblock">(2009).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://nodejs.org/en/" title="">https://nodejs.org/en/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Douze et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Matthijs Douze, Alexandr
Guzhva, Chengqi Deng, Jeff Johnson,
Gergely Szilvasy, Pierre-Emmanuel
Mazaré, Maria Lomeli, Lucas
Hosseini, and Hervé Jégou.
2024.

</span>
<span class="ltx_bibblock">The Faiss Library.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">arXiv 2401.08281</em> (2024).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2401.08281" title="">http://arxiv.org/abs/2401.08281</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Draxler et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Fiona Draxler, Daniel
Buschek, Mikke Tavast, Perttu
Hämäläinen, Albrecht Schmidt,
Juhi Kulshrestha, and Robin Welsch.
2023.

</span>
<span class="ltx_bibblock">Gender, Age, and Technology Education
Influence the Adoption and Appropriation of LLMs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">arXiv 2310.06556</em> (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2310.06556" title="">http://arxiv.org/abs/2310.06556</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Facebook (2013)</span>
<span class="ltx_bibblock">
Facebook. 2013.

</span>
<span class="ltx_bibblock">React: The Library for Web and Native User
Interfaces.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://react.dev/" title="">https://react.dev/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fahlander (2021)</span>
<span class="ltx_bibblock">
David Fahlander.
2021.

</span>
<span class="ltx_bibblock">Dexie.Js - Minimalistic IndexedDB Wrapper.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dexie.org/" title="">https://dexie.org/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Felicia Li Feng, Ryan
Yen, Yuzhe You, Mingming Fan,
Jian Zhao, and Zhicong Lu.
2023.

</span>
<span class="ltx_bibblock">CoPrompt: Supporting Prompt Sharing and
Referring in Collaborative Natural Language Programming.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">arXiv 2310.09235</em> (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2310.09235" title="">http://arxiv.org/abs/2310.09235</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Forte (2022)</span>
<span class="ltx_bibblock">
Tiago Forte.
2022.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Building a Second Brain: A Proven Method to
Organize Your Digital Life and Unlock Your Creative Potential</em>
(first atria books hardcover edition ed.).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Freeman and Gelernter (1996)</span>
<span class="ltx_bibblock">
Eric Freeman and David
Gelernter. 1996.

</span>
<span class="ltx_bibblock">Lifestreams: A Storage Model for Personal Data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">ACM SIGMOD Record</em> 25
(1996).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/381854.381893" title="">https://doi.org/10.1145/381854.381893</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fuchsbauer et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Georg Fuchsbauer, Riddhi
Ghosal, Nathan Hauke, and Adam
O’Neill. 2021.

</span>
<span class="ltx_bibblock">Approximate Distance-Comparison-Preserving Symmetric
Encryption.

</span>
<span class="ltx_bibblock">Cryptology ePrint Archive, Paper 2021/1666.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://eprint.iacr.org/2021/1666" title="">https://eprint.iacr.org/2021/1666</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghodratnama and Zakershahrak (2023)</span>
<span class="ltx_bibblock">
Samira Ghodratnama and
Mehrdad Zakershahrak. 2023.

</span>
<span class="ltx_bibblock">Adapting LLMs for Efficient, Personalized
Information Retrieval: Methods and Implications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv 2311.12287</em> (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2311.12287" title="">http://arxiv.org/abs/2311.12287</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Yu Gong, Ziwen Jiang,
Yufei Feng, Binbin Hu,
Kaiqi Zhao, Qingwen Liu, and
Wenwu Ou. 2020.

</span>
<span class="ltx_bibblock">EdgeRec: Recommender System on Edge in
Mobile Taobao. In <em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">Proceedings of the 29th
ACM International Conference on Information &amp; Knowledge
Management</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3340531.3412700" title="">https://doi.org/10.1145/3340531.3412700</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Google (2015)</span>
<span class="ltx_bibblock">
Google. 2015.

</span>
<span class="ltx_bibblock">Lit: Simple Fast Web Components.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://lit.dev/" title="">https://lit.dev/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Harris (2016)</span>
<span class="ltx_bibblock">
Rich Harris.
2016.

</span>
<span class="ltx_bibblock">Svelte: Cybernetically Enhanced Web Apps.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://svelte.dev/" title="">https://svelte.dev/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hashimoto et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Tatsunori B Hashimoto,
Kelvin Guu, Yonatan Oren, and
Percy S Liang. 2018.

</span>
<span class="ltx_bibblock">A Retrieve-and-Edit Framework for Predicting
Structured Outputs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">Advances in Neural Information Processing
Systems</em> 31 (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsieh et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Cheng-Yu Hsieh, Si-An
Chen, Chun-Liang Li, Yasuhisa Fujii,
Alexander Ratner, Chen-Yu Lee,
Ranjay Krishna, and Tomas Pfister.
2023.

</span>
<span class="ltx_bibblock">Tool Documentation Enables Zero-Shot Tool-Usage
with Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">arXiv 2308.00675</em> (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2308.00675" title="">http://arxiv.org/abs/2308.00675</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Edward J. Hu, Yelong
Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang,
Lu Wang, and Weizhu Chen.
2021.

</span>
<span class="ltx_bibblock">LoRA: Low-Rank Adaptation of Large
Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">arXiv 2106.09685</em> (2021).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2106.09685" title="">http://arxiv.org/abs/2106.09685</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard and Grave (2021)</span>
<span class="ltx_bibblock">
Gautier Izacard and
Edouard Grave. 2021.

</span>
<span class="ltx_bibblock">Leveraging Passage Retrieval with Generative Models
for Open Domain Question Answering. In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings
of the 16th Conference of the European Chapter of the Association for
Computational Linguistics: Main Volume</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/v1/2021.eacl-main.74" title="">https://doi.org/10.18653/v1/2021.eacl-main.74</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jégou et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2011)</span>
<span class="ltx_bibblock">
H Jégou, M Douze,
and C Schmid. 2011.

</span>
<span class="ltx_bibblock">Product Quantization for Nearest Neighbor
Search.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">IEEE Transactions on Pattern Analysis and
Machine Intelligence</em> 33 (2011).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/TPAMI.2010.57" title="">https://doi.org/10.1109/TPAMI.2010.57</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kamvar et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2009)</span>
<span class="ltx_bibblock">
Maryam Kamvar, Melanie
Kellar, Rajan Patel, and Ya Xu.
2009.

</span>
<span class="ltx_bibblock">Computers and Iphones and Mobile Phones, Oh My!: A
Logs-Based Comparison of Search Users on Different Devices. In
<em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">Proceedings of the 18th International Conference on
World Wide Web</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/1526709.1526817" title="">https://doi.org/10.1145/1526709.1526817</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kane (2021)</span>
<span class="ltx_bibblock">
Andrew Kane.
2021.

</span>
<span class="ltx_bibblock">Pgvector: Open-source Vector Similarity Search
for Postgres.

</span>
<span class="ltx_bibblock">pgvector.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/pgvector/pgvector" title="">https://github.com/pgvector/pgvector</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kiesel et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Johannes Kiesel, Arjen P
de Vries, Matthias Hagen, Benno Stein,
and Martin Potthast. 2018.

</span>
<span class="ltx_bibblock">WASP: Web Archiving and Search Personalized.
In <em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">DESIRES</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kleinberg (2000)</span>
<span class="ltx_bibblock">
Jon M. Kleinberg.
2000.

</span>
<span class="ltx_bibblock">Navigation in a Small World.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Nature</em> 406
(2000).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1038/35022643" title="">https://doi.org/10.1038/35022643</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kluyver et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Thomas Kluyver, Benjamin
Ragan-Kelley, Fernando Pérez,
Brian E Granger, Matthias Bussonnier,
Jonathan Frederic, Kyle Kelley,
Jessica B Hamrick, Jason Grout, and
Sylvain Corlay. 2016.

</span>
<span class="ltx_bibblock">Jupyter Notebooks-a Publishing Format for
Reproducible Computational Workflows.

</span>
<span class="ltx_bibblock">2016 (2016).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.3233/978-1-61499-649-1-87" title="">https://doi.org/10.3233/978-1-61499-649-1-87</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lam et al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Maximilian Lam, Jeff
Johnson, Wenjie Xiong, Kiwan Maeng,
Udit Gupta, Yang Li,
Liangzhen Lai, Ilias Leontiadis,
Minsoo Rhu, Hsien-Hsin S. Lee,
Vijay Janapa Reddi, Gu-Yeon Wei,
David Brooks, and G. Edward Suh.
2023.

</span>
<span class="ltx_bibblock">GPU-based Private Information Retrieval for
On-Device Machine Learning Inference.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.3.1">arXiv 2301.10904</em> (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2301.10904" title="">http://arxiv.org/abs/2301.10904</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lavrenko and Croft (2001)</span>
<span class="ltx_bibblock">
Victor Lavrenko and
W. Bruce Croft. 2001.

</span>
<span class="ltx_bibblock">Relevance Based Language Models. In
<em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Proceedings of the 24th Annual International ACM
SIGIR Conference on Research and Development in Information
Retrieval</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/383952.383972" title="">https://doi.org/10.1145/383952.383972</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lester et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Brian Lester, Rami
Al-Rfou, and Noah Constant.
2021.

</span>
<span class="ltx_bibblock">The Power of Scale for
Parameter-Efficient Prompt Tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.3.1">arXiv 2104.08691</em> (2021).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2104.08691" title="">http://arxiv.org/abs/2104.08691</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan
Perez, Aleksandra Piktus, Fabio Petroni,
Vladimir Karpukhin, Naman Goyal,
Heinrich Küttler, Mike Lewis,
Wen-tau Yih, Tim Rocktäschel,
et al<span class="ltx_text" id="bib.bib40.3.1">.</span> 2020.

</span>
<span class="ltx_bibblock">Retrieval-Augmented Generation for
Knowledge-Intensive NLP Tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.4.1">Advances in Neural Information Processing
Systems</em> 33 (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib41.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Chaofan Li, Zheng Liu,
Shitao Xiao, Yingxia Shao,
Defu Lian, and Zhao Cao.
2023a.

</span>
<span class="ltx_bibblock">LibVQ: A Toolkit for Optimizing Vector
Quantization and Efficient Neural Retrieval. In
<em class="ltx_emph ltx_font_italic" id="bib.bib41.3.1">Proceedings of the 46th International ACM SIGIR
Conference on Research and Development in Information
Retrieval</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3539618.3591799" title="">https://doi.org/10.1145/3539618.3591799</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib42.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Zehan Li, Xin Zhang,
Yanzhao Zhang, Dingkun Long,
Pengjun Xie, and Meishan Zhang.
2023b.

</span>
<span class="ltx_bibblock">Towards General Text Embeddings with
Multi-stage Contrastive Learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.3.1">arXiv 2308.03281</em> (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2308.03281" title="">http://arxiv.org/abs/2308.03281</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span class="ltx_text" id="bib.bib43.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Jimmy Lin, Xueguang Ma,
Sheng-Chieh Lin, Jheng-Hong Yang,
Ronak Pradeep, and Rodrigo Nogueira.
2021.

</span>
<span class="ltx_bibblock">Pyserini: A Python Toolkit for Reproducible
Information Retrieval Research with Sparse and Dense
Representations. In <em class="ltx_emph ltx_font_italic" id="bib.bib43.3.1">Proceedings of the 44th
International ACM SIGIR Conference on Research and Development in
Information Retrieval</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3404835.3463238" title="">https://doi.org/10.1145/3404835.3463238</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lochner (2023)</span>
<span class="ltx_bibblock">
Joshua Lochner.
2023.

</span>
<span class="ltx_bibblock">Transformers.Js: State-of-the-art Machine
Learning for the Web.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/xenova/transformers.js" title="">https://github.com/xenova/transformers.js</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Macoskey et al<span class="ltx_text" id="bib.bib45.2.2.1">.</span> (2021a)</span>
<span class="ltx_bibblock">
Jonathan Macoskey, Grant
Strimel, and Ariya Rastrow.
2021a.

</span>
<span class="ltx_bibblock">Learning a Neural Diff for Speech Models. In
<em class="ltx_emph ltx_font_italic" id="bib.bib45.3.1">Interspeech 2021</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.amazon.science/publications/learning-a-neural-diff-for-speech-models" title="">https://www.amazon.science/publications/learning-a-neural-diff-for-speech-models</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Macoskey et al<span class="ltx_text" id="bib.bib46.2.2.1">.</span> (2021b)</span>
<span class="ltx_bibblock">
Jonathan Macoskey, Grant
Strimel, Jinru Su, and Ariya Rastrow.
2021b.

</span>
<span class="ltx_bibblock">Amortized Neural Networks for Low-Latency Speech
Recognition. In <em class="ltx_emph ltx_font_italic" id="bib.bib46.3.1">Interspeech 2021</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.amazon.science/publications/amortized-neural-networks-for-low-latency-speech-recognition" title="">https://www.amazon.science/publications/amortized-neural-networks-for-low-latency-speech-recognition</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maitre (2018)</span>
<span class="ltx_bibblock">
Ogier Maitre.
2018.

</span>
<span class="ltx_bibblock">Total Canvas Memory Use Exceeds the Maximum Limit
(Safari 12) - Stack Overflow.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://stackoverflow.com/questions/52532614/total-canvas-memory-use-exceeds-the-maximum-limit-safari-12" title="">https://stackoverflow.com/questions/52532614/total-canvas-memory-use-exceeds-the-maximum-limit-safari-12</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malkov and Yashunin (2020)</span>
<span class="ltx_bibblock">
Yu A. Malkov and D. A.
Yashunin. 2020.

</span>
<span class="ltx_bibblock">Efficient and Robust Approximate Nearest Neighbor
Search Using Hierarchical Navigable Small World Graphs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">IEEE Transactions on Pattern Analysis and
Machine Intelligence</em> 42 (2020).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/TPAMI.2018.2889473" title="">https://doi.org/10.1109/TPAMI.2018.2889473</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Martineau (2023)</span>
<span class="ltx_bibblock">
Kim Martineau.
2023.

</span>
<span class="ltx_bibblock">What Is Retrieval-Augmented Generation?

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://research.ibm.com/blog/retrieval-augmented-generation-RAG" title="">https://research.ibm.com/blog/retrieval-augmented-generation-RAG</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MDN (2021)</span>
<span class="ltx_bibblock">
MDN. 2021.

</span>
<span class="ltx_bibblock">IndexedDB API - Web APIs.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developer.mozilla.org/en-US/docs/Web/API/IndexedDB_API" title="">https://developer.mozilla.org/en-US/docs/Web/API/IndexedDB_API</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MDN (2023a)</span>
<span class="ltx_bibblock">
MDN. 2023a.

</span>
<span class="ltx_bibblock">Storage Quotas and Eviction Criteria - Web APIs
| MDN.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developer.mozilla.org/en-US/docs/Web/API/Storage_API/Storage_quotas_and_eviction_criteria" title="">https://developer.mozilla.org/en-US/docs/Web/API/Storage_API/Storage_quotas_and_eviction_criteria</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MDN (2023b)</span>
<span class="ltx_bibblock">
MDN. 2023b.

</span>
<span class="ltx_bibblock">Streams API - Web APIs.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developer.mozilla.org/en-US/docs/Web/API/Streams_API" title="">https://developer.mozilla.org/en-US/docs/Web/API/Streams_API</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MDN (2023c)</span>
<span class="ltx_bibblock">
MDN. 2023c.

</span>
<span class="ltx_bibblock">Web Workers API - Web APIs.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API" title="">https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mendel-Gleason (2024)</span>
<span class="ltx_bibblock">
Gavin Mendel-Gleason.
2024.

</span>
<span class="ltx_bibblock">Parallelising HNSW.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/GavinMendelGleason/blog/blob/main/entries/parallelising_hnsw.md" title="">https://github.com/GavinMendelGleason/blog/blob/main/entries/parallelising_hnsw.md</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MLC (2023)</span>
<span class="ltx_bibblock">
Team MLC. 2023.

</span>
<span class="ltx_bibblock">MLC-LLM.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/mlc-ai/mlc-llm" title="">https://github.com/mlc-ai/mlc-llm</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Neelakantan et al<span class="ltx_text" id="bib.bib56.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Arvind Neelakantan, Tao
Xu, Raul Puri, Alec Radford,
Jesse Michael Han, Jerry Tworek,
Qiming Yuan, Nikolas Tezak,
Jong Wook Kim, Chris Hallacy,
Johannes Heidecke, Pranav Shyam,
Boris Power, Tyna Eloundou Nekoul,
Girish Sastry, Gretchen Krueger,
David Schnurr, Felipe Petroski Such,
Kenny Hsu, Madeleine Thompson,
Tabarak Khan, Toki Sherbakov,
Joanne Jang, Peter Welinder, and
Lilian Weng. 2022.

</span>
<span class="ltx_bibblock">Text and Code Embeddings by Contrastive
Pre-Training.

</span>
<span class="ltx_bibblock">(2022).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2201.10005" title="">https://doi.org/10.48550/ARXIV.2201.10005</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock">GPT-4 Technical Report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">arXiv 2303.08774</em> (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2303.08774" title="">http://arxiv.org/abs/2303.08774</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ovadia et al<span class="ltx_text" id="bib.bib58.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Oded Ovadia, Menachem
Brief, Moshik Mishaeli, and Oren
Elisha. 2024.

</span>
<span class="ltx_bibblock">Fine-Tuning or Retrieval? Comparing
Knowledge Injection in LLMs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.3.1">arXiv 2312.05934</em> (2024).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2312.05934" title="">http://arxiv.org/abs/2312.05934</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Prince et al<span class="ltx_text" id="bib.bib59.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Michael H. Prince, Henry
Chan, Aikaterini Vriza, Tao Zhou,
Varuni K. Sastry, Matthew T. Dearing,
Ross J. Harder, Rama K. Vasudevan, and
Mathew J. Cherukara. 2023.

</span>
<span class="ltx_bibblock">Opportunities for Retrieval and Tool
Augmented Large Language Models in Scientific Facilities.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.3.1">arXiv 2312.01291</em> (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2312.01291" title="">http://arxiv.org/abs/2312.01291</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pugh (1990)</span>
<span class="ltx_bibblock">
William Pugh.
1990.

</span>
<span class="ltx_bibblock">Skip Lists: A Probabilistic Alternative to Balanced
Trees.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Commun. ACM</em> 33
(1990).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/78973.78977" title="">https://doi.org/10.1145/78973.78977</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qu et al<span class="ltx_text" id="bib.bib61.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Chen Qu, Hamed Zamani,
Liu Yang, W. Bruce Croft, and
Erik Learned-Miller. 2021.

</span>
<span class="ltx_bibblock">Passage Retrieval for Outside-Knowledge
Visual Question Answering. In <em class="ltx_emph ltx_font_italic" id="bib.bib61.3.1">Proceedings of the
44th International ACM SIGIR Conference on Research and
Development in Information Retrieval</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3404835.3462987" title="">https://doi.org/10.1145/3404835.3462987</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers and Gurevych (2019)</span>
<span class="ltx_bibblock">
Nils Reimers and Iryna
Gurevych. 2019.

</span>
<span class="ltx_bibblock">Sentence-BERT: Sentence Embeddings Using
Siamese BERT-Networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">Proceedings of the
2019 Conference on Empirical Methods in Natural Language
Processing</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1908.10084" title="">https://arxiv.org/abs/1908.10084</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ruan et al<span class="ltx_text" id="bib.bib63.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jingqing Ruan, Yihong
Chen, Bin Zhang, Zhiwei Xu,
Tianpeng Bao, Guoqing Du,
Shiwei Shi, Hangyu Mao,
Ziyue Li, Xingyu Zeng, and
Rui Zhao. 2023.

</span>
<span class="ltx_bibblock">TPTU: Large Language Model-based AI Agents
for Task Planning and Tool Usage.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.3.1">arXiv 2308.03427</em> (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2308.03427" title="">http://arxiv.org/abs/2308.03427</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">RxDB (2021)</span>
<span class="ltx_bibblock">
RxDB. 2021.

</span>
<span class="ltx_bibblock">Why IndexedDB Is Slow and What to Use Instead.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://rxdb.info/slow-indexeddb.html" title="">https://rxdb.info/slow-indexeddb.html</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Semnani et al<span class="ltx_text" id="bib.bib65.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Sina Semnani, Violet Yao,
Heidi Zhang, and Monica Lam.
2023.

</span>
<span class="ltx_bibblock">WikiChat: Stopping the Hallucination of
Large Language Model Chatbots by Few-Shot Grounding on
Wikipedia. In <em class="ltx_emph ltx_font_italic" id="bib.bib65.3.1">Findings of the Association
for Computational Linguistics: EMNLP 2023</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/v1/2023.findings-emnlp.157" title="">https://doi.org/10.18653/v1/2023.findings-emnlp.157</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shuster et al<span class="ltx_text" id="bib.bib66.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Kurt Shuster, Spencer
Poff, Moya Chen, Douwe Kiela, and
Jason Weston. 2021.

</span>
<span class="ltx_bibblock">Retrieval Augmentation Reduces Hallucination in
Conversation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.3.1">arXiv 2104.07567</em> (2021).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2104.07567" title="">http://arxiv.org/abs/2104.07567</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smilkov et al<span class="ltx_text" id="bib.bib67.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Daniel Smilkov, Nikhil
Thorat, Yannick Assogba, Ann Yuan,
Nick Kreeger, Ping Yu,
Kangyi Zhang, Shanqing Cai,
Eric Nielsen, David Soergel,
Stan Bileschi, Michael Terry,
Charles Nicholson, Sandeep N. Gupta,
Sarah Sirajuddin, D. Sculley,
Rajat Monga, Greg Corrado,
Fernanda B. Viégas, and Martin
Wattenberg. 2019.

</span>
<span class="ltx_bibblock">TensorFlow.Js: Machine Learning for the
Web and Beyond.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.3.1">arXiv</em> (2019).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1901.05350" title="">https://arxiv.org/abs/1901.05350</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Soare et al<span class="ltx_text" id="bib.bib68.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Elena Soare, Iain Mackie,
and Jeffrey Dalton. 2022.

</span>
<span class="ltx_bibblock">DocuT5: Seq2seq SQL Generation with Table
Documentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.3.1">arXiv 2211.06193</em> (2022).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2211.06193" title="">http://arxiv.org/abs/2211.06193</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan et al<span class="ltx_text" id="bib.bib69.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Zhixing Tan, Zeyuan Yang,
Meng Zhang, Qun Liu,
Maosong Sun, and Yang Liu.
2022.

</span>
<span class="ltx_bibblock">Dynamic Multi-Branch Layers for On-Device
Neural Machine Translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.3.1">IEEE/ACM Transactions on Audio, Speech, and
Language Processing</em> 30 (2022).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/TASLP.2022.3153257" title="">https://doi.org/10.1109/TASLP.2022.3153257</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al<span class="ltx_text" id="bib.bib70.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis
Martin, Kevin Stone, Peter Albert,
Amjad Almahairi, Yasmine Babaei,
Nikolay Bashlykov, Soumya Batra,
Prajjwal Bhargava, Shruti Bhosale,
Dan Bikel, Lukas Blecher,
Cristian Canton Ferrer, Moya Chen,
Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu,
Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami,
Naman Goyal, Anthony Hartshorn,
Saghar Hosseini, Rui Hou,
Hakan Inan, Marcin Kardas,
Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev,
Punit Singh Koura, Marie-Anne Lachaux,
Thibaut Lavril, Jenya Lee,
Diana Liskovich, Yinghai Lu,
Yuning Mao, Xavier Martinet,
Todor Mihaylov, Pushkar Mishra,
Igor Molybog, Yixin Nie,
Andrew Poulton, Jeremy Reizenstein,
Rashi Rungta, Kalyan Saladi,
Alan Schelten, Ruan Silva,
Eric Michael Smith, Ranjan Subramanian,
Xiaoqing Ellen Tan, Binh Tang,
Ross Taylor, Adina Williams,
Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov,
Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang,
Aurelien Rodriguez, Robert Stojnic,
Sergey Edunov, and Thomas Scialom.
2023.

</span>
<span class="ltx_bibblock">Llama 2: Open Foundation and Fine-Tuned Chat
Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.3.1">arXiv 2307.09288</em> (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2307.09288" title="">https://arxiv.org/abs/2307.09288</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib71.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Zijie J. Wang, Aishwarya
Chakravarthy, David Munechika, and
Duen Horng Chau. 2024.

</span>
<span class="ltx_bibblock">Wordflow: Social Prompt Engineering for Large
Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.3.1">arXiv 2401.14447</em> (2024).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2401.14447" title="">http://arxiv.org/abs/2401.14447</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Chau (2023)</span>
<span class="ltx_bibblock">
Zijie J. Wang and
Duen Horng Chau. 2023.

</span>
<span class="ltx_bibblock">WebSHAP: Towards Explaining Any Machine
Learning Models Anywhere. In <em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">Companion
Proceedings of the Web Conference 2023</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3543873.3587362" title="">https://doi.org/10.1145/3543873.3587362</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib73.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Zijie J. Wang, Fred
Hohman, and Duen Horng Chau.
2023a.

</span>
<span class="ltx_bibblock">WizMap: Scalable Interactive Visualization
for Exploring Large Machine Learning Embeddings. In
<em class="ltx_emph ltx_font_italic" id="bib.bib73.3.1">Proceedings of the 61st Annual Meeting of the
Association for Computational Linguistics (Volume 3: System
Demonstrations)</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.acl-demo.50" title="">https://aclanthology.org/2023.acl-demo.50</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib74.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Zijie J. Wang, Alex Kale,
Harsha Nori, Peter Stella,
Mark E. Nunnally, Duen Horng Chau,
Mihaela Vorvoreanu, Jennifer
Wortman Vaughan, and Rich Caruana.
2022.

</span>
<span class="ltx_bibblock">Interpretability, Then What? Editing Machine
Learning Models to Reflect Human Knowledge and Values. In
<em class="ltx_emph ltx_font_italic" id="bib.bib74.3.1">Proceedings of the 28th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining</em>
<em class="ltx_emph ltx_font_italic" id="bib.bib74.4.2">(KDD ’22)</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3534678.3539074" title="">https://doi.org/10.1145/3534678.3539074</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib75.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Zijie J. Wang,
Jennifer Wortman Vaughan, Rich Caruana,
and Duen Horng Chau. 2023b.

</span>
<span class="ltx_bibblock">GAM Coach: Towards Interactive and
User-centered Algorithmic Recourse. In <em class="ltx_emph ltx_font_italic" id="bib.bib75.3.1">CHI
Conference on Human Factors in Computing Systems</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3544548.3580816" title="">https://doi.org/10.1145/3544548.3580816</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wilkerling (2019)</span>
<span class="ltx_bibblock">
Thomas Wilkerling.
2019.

</span>
<span class="ltx_bibblock">FlexSearch: Next-Generation Full Text Search
Library for Browser and Node.Js.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/nextapps-de/flexsearch" title="">https://github.com/nextapps-de/flexsearch</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wutschitz et al<span class="ltx_text" id="bib.bib77.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Lukas Wutschitz, Boris
Köpf, Andrew Paverd, Saravan
Rajmohan, Ahmed Salem, Shruti Tople,
Santiago Zanella-Béguelin, Menglin
Xia, and Victor Rühle.
2023.

</span>
<span class="ltx_bibblock">Rethinking Privacy in Machine Learning
Pipelines from an Information Flow Control Perspective.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib77.3.1">arXiv 2311.15792</em> (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2311.15792" title="">http://arxiv.org/abs/2311.15792</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xia et al<span class="ltx_text" id="bib.bib78.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xin Xia, Junliang Yu,
Qinyong Wang, Chaoqun Yang,
Nguyen Quoc Viet Hung, and Hongzhi
Yin. 2023.

</span>
<span class="ltx_bibblock">Efficient On-Device Session-Based
Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib78.3.1">ACM Transactions on Information Systems</em>
(2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3580364" title="">https://doi.org/10.1145/3580364</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zamfirescu-Pereira et al<span class="ltx_text" id="bib.bib79.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
J.D. Zamfirescu-Pereira,
Richmond Y. Wong, Bjoern Hartmann, and
Qian Yang. 2023.

</span>
<span class="ltx_bibblock">Why Johnny Can’t Prompt: How Non-AI
Experts Try (and Fail) to Design LLM Prompts. In
<em class="ltx_emph ltx_font_italic" id="bib.bib79.3.1">Proceedings of the 2023 CHI Conference on
Human Factors in Computing Systems</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3544548.3581388" title="">https://doi.org/10.1145/3544548.3581388</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib80.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Zhengyan Zhang, Xu Han,
Zhiyuan Liu, Xin Jiang,
Maosong Sun, and Qun Liu.
2019.

</span>
<span class="ltx_bibblock">ERNIE: Enhanced Language Representation
with Informative Entities. In <em class="ltx_emph ltx_font_italic" id="bib.bib80.3.1">Proceedings of the
57th Annual Meeting of the Association for Computational Linguistics</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/v1/P19-1139" title="">https://doi.org/10.18653/v1/P19-1139</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib81.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shuyan Zhou, Uri Alon,
Frank F. Xu, Zhiruo Wang,
Zhengbao Jiang, and Graham Neubig.
2023.

</span>
<span class="ltx_bibblock">DocPrompting: Generating Code by
Retrieving the Docs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib81.3.1">arXiv 2207.05987</em> (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2207.05987" title="">http://arxiv.org/abs/2207.05987</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu (2016)</span>
<span class="ltx_bibblock">
Eric Zhu. 2016.

</span>
<span class="ltx_bibblock">Ekzhu/Datasketch: MinHash, LSH, LSH
Forest, Weighted MinHash, HyperLogLog, HyperLogLog++, LSH
Ensemble and HNSW.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ekzhu/datasketch" title="">https://github.com/ekzhu/datasketch</a>
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jul  2 06:03:57 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
