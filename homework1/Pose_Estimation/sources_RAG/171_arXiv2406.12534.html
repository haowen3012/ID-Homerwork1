<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Unified Active Retrieval for Retrieval Augmented Generation</title>
<!--Generated on Thu Oct  3 02:38:46 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2406.12534v4/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S1" title="In Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S2" title="In Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S2.SS1" title="In 2 Related Work â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Active Retrieval</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S2.SS2" title="In 2 Related Work â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Time-awareness of LLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S2.SS3" title="In 2 Related Work â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Self-awareness of LLMs</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S3" title="In Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S3.SS1" title="In 3 Methodology â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>UAR Classifiers Training</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S3.SS1.SSS0.Px1" title="In 3.1 UAR Classifiers Training â€£ 3 Methodology â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title">Self-aware</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S3.SS1.SSS0.Px2" title="In 3.1 UAR Classifiers Training â€£ 3 Methodology â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title">Time-aware</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S3.SS1.SSS0.Px3" title="In 3.1 UAR Classifiers Training â€£ 3 Methodology â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title">Knowledge-aware</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S3.SS1.SSS0.Px4" title="In 3.1 UAR Classifiers Training â€£ 3 Methodology â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title">Intent-aware</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S3.SS2" title="In 3 Methodology â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>UAR Criteria</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S3.SS3" title="In 3 Methodology â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Generation with Relevant Information</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S4" title="In Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S4.SS1" title="In 4 Experiments â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Benchmarking Retrieval Timing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S4.SS2" title="In 4 Experiments â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Downstream Tasks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S4.SS3" title="In 4 Experiments â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Baselines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S4.SS4" title="In 4 Experiments â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Retrievers</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S4.SS5" title="In 4 Experiments â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Evaluation Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S4.SS6" title="In 4 Experiments â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6 </span>Comparisons on AR-Bench</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S4.SS7" title="In 4 Experiments â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.7 </span>Comparisons on Downstream Tasks</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S4.SS7.SSS0.Px1" title="In 4.7 Comparisons on Downstream Tasks â€£ 4 Experiments â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title">UAR does not invoke retrieval when factual knowledge is not needed.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S4.SS7.SSS0.Px2" title="In 4.7 Comparisons on Downstream Tasks â€£ 4 Experiments â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title">UAR accurately invokes retrieval for time-sensitive questions.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S4.SS7.SSS0.Px3" title="In 4.7 Comparisons on Downstream Tasks â€£ 4 Experiments â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title">UAR accurately assesses the modelâ€™s knowledge, avoiding poor retrieval impacts.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S5" title="In Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S5.SS1" title="In 5 Analysis â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Single Classifiers vs UAR</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S5.SS2" title="In 5 Analysis â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Using the Whole LLM as Classifier</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S5.SS3" title="In 5 Analysis â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>The Impact of Document Number</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S6" title="In Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#A1" title="In Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Details of AR-Bench Construction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#A2" title="In Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Details of Baselines Re-implementation</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#A2.SS1" title="In Appendix B Details of Baselines Re-implementation â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>FLARE</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#A2.SS2" title="In Appendix B Details of Baselines Re-implementation â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span>SKR</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#A3" title="In Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>ChatGPT Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#A4" title="In Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Details of Generation</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#A4.SS1" title="In Appendix D Details of Generation â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.1 </span>Self-RAG</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#A4.SS2" title="In Appendix D Details of Generation â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.2 </span>Generation without Retrieval</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#A4.SS3" title="In Appendix D Details of Generation â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.3 </span>Generation with Retrieval</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#A5" title="In Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Details of UAR Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#A6" title="In Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F </span>Downstream Task Datasets</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Unified Active Retrieval for Retrieval Augmented Generation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Qinyuan Cheng<sup class="ltx_sup" id="id14.14.id1"><span class="ltx_text ltx_font_italic" id="id14.14.id1.1">1,2</span></sup> â€„
Xiaonan Li<sup class="ltx_sup" id="id15.15.id2"><span class="ltx_text ltx_font_italic" id="id15.15.id2.1">1</span></sup><span class="ltx_note ltx_role_footnotemark" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span> â€†
Shimin Li<sup class="ltx_sup" id="id16.16.id3"><span class="ltx_text ltx_font_italic" id="id16.16.id3.1">1</span></sup> â€†
Qin Zhu<sup class="ltx_sup" id="id17.17.id4"><span class="ltx_text ltx_font_italic" id="id17.17.id4.1">1</span></sup> â€†
Zhangyue Yin<sup class="ltx_sup" id="id18.18.id5"><span class="ltx_text ltx_font_italic" id="id18.18.id5.1">1</span></sup>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id10.10.5">
Yunfan Shao<sup class="ltx_sup" id="id10.10.5.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id10.10.5.1.1">1,2</span></sup> â€†
Linyang Li<sup class="ltx_sup" id="id10.10.5.2"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id10.10.5.2.1">2</span></sup> â€†
Tianxiang Sun<sup class="ltx_sup" id="id10.10.5.3"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id10.10.5.3.1">1</span></sup> â€†
Hang Yan<sup class="ltx_sup" id="id10.10.5.4"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id10.10.5.4.1">2</span></sup> â€†
Xipeng Qiu<sup class="ltx_sup" id="id10.10.5.5"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id10.10.5.5.1">1,3,</span></sup></span>
<br class="ltx_break"/>
<sup class="ltx_sup" id="id19.19.id6"><span class="ltx_text ltx_font_italic" id="id19.19.id6.1">1</span></sup>Fudan University 
<br class="ltx_break"/><sup class="ltx_sup" id="id20.20.id7"><span class="ltx_text ltx_font_italic" id="id20.20.id7.1">2</span></sup>Shanghai AI Laboratory 
<br class="ltx_break"/><sup class="ltx_sup" id="id21.21.id8"><span class="ltx_text ltx_font_italic" id="id21.21.id8.1">3</span></sup>Shanghai Collaborative Innovation Center of Intelligent Visual Computing 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id22.22.id9">chengqy21@m.fudan.edu.cn
{lixn20, xpqiu}@fudan.edu.cn
</span>
</span><span class="ltx_author_notes">Equal contribution<span class="ltx_text ltx_font_bold" id="id23.23.id1">Corresponding author.</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id24.id1">In Retrieval-Augmented Generation (RAG),
retrieval is not always helpful and applying it to every instruction is sub-optimal.
Therefore, determining whether to retrieve is crucial for RAG, which is usually referred to as Active Retrieval. However, existing active retrieval methods face two challenges: 1. They usually rely on a single criterion, which struggles with handling various types of instructions. 2. They depend on specialized and highly differentiated procedures, and thus combining them makes the RAG system more complicated and leads to higher response latency. To address these challenges, we propose <span class="ltx_text ltx_font_bold" id="id24.id1.1">U</span>nified <span class="ltx_text ltx_font_bold" id="id24.id1.2">A</span>ctive <span class="ltx_text ltx_font_bold" id="id24.id1.3">R</span>etrieval (<span class="ltx_text ltx_font_bold" id="id24.id1.4">UAR</span>). UAR contains four orthogonal criteria and casts them into plug-and-play classification tasks, which achieves multifaceted retrieval timing judgements with negligible extra inference cost.
We further introduce the Unified Active Retrieval Criteria (UAR-Criteria), designed to process diverse active retrieval scenarios through a standardized procedure.
Experiments on four representative types of user instructions show that UAR significantly outperforms existing work on the retrieval timing judgement and the performance of downstream tasks, which shows the effectiveness of UAR and its helpfulness to downstream tasks.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.13">
<p class="ltx_p" id="p1.13.14"><span class="ltx_text ltx_font_bold" id="p1.13.14.1">Unified Active Retrieval for Retrieval Augmented Generation</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.13.13" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.13.13.13" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.13.13.13.13">
<span class="ltx_tr" id="p1.5.5.5.5.5">
<span class="ltx_td ltx_align_center" id="p1.5.5.5.5.5.5"><span class="ltx_text ltx_font_bold" id="p1.5.5.5.5.5.5.5">
Qinyuan Cheng<sup class="ltx_sup" id="p1.5.5.5.5.5.5.5.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.5.5.5.5.5.5.5.1.1">1,2</span></sup><span class="ltx_note ltx_role_thanks" id="p1.5.5.5.5.5.5.5.2"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">thanks: </span>Equal contribution</span></span></span> â€„
Xiaonan Li<sup class="ltx_sup" id="p1.5.5.5.5.5.5.5.3"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.5.5.5.5.5.5.5.3.1">1</span></sup><span class="ltx_note ltx_role_footnotemark" id="footnotex2"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnotex2.1.1.1">1</span></span></span></span></span> â€†
Shimin Li<sup class="ltx_sup" id="p1.5.5.5.5.5.5.5.4"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.5.5.5.5.5.5.5.4.1">1</span></sup> â€†
Qin Zhu<sup class="ltx_sup" id="p1.5.5.5.5.5.5.5.5"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.5.5.5.5.5.5.5.5.1">1</span></sup> â€†
Zhangyue Yin<sup class="ltx_sup" id="p1.5.5.5.5.5.5.5.6"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.5.5.5.5.5.5.5.6.1">1</span></sup></span></span></span>
<span class="ltx_tr" id="p1.10.10.10.10.10">
<span class="ltx_td ltx_align_center" id="p1.10.10.10.10.10.5" style="padding-bottom:4.30554pt;"><span class="ltx_text ltx_font_bold" id="p1.10.10.10.10.10.5.5">
Yunfan Shao<sup class="ltx_sup" id="p1.10.10.10.10.10.5.5.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.10.10.10.10.10.5.5.1.1">1,2</span></sup> â€†
Linyang Li<sup class="ltx_sup" id="p1.10.10.10.10.10.5.5.2"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.10.10.10.10.10.5.5.2.1">2</span></sup> â€†
Tianxiang Sun<sup class="ltx_sup" id="p1.10.10.10.10.10.5.5.3"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.10.10.10.10.10.5.5.3.1">1</span></sup> â€†
Hang Yan<sup class="ltx_sup" id="p1.10.10.10.10.10.5.5.4"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.10.10.10.10.10.5.5.4.1">2</span></sup> â€†
Xipeng Qiu<sup class="ltx_sup" id="p1.10.10.10.10.10.5.5.5"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.10.10.10.10.10.5.5.5.1">1,3,</span></sup><span class="ltx_note ltx_role_thanks" id="p1.10.10.10.10.10.5.5.6"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">thanks: </span>Corresponding author.</span></span></span></span></span></span>
<span class="ltx_tr" id="p1.11.11.11.11.11">
<span class="ltx_td ltx_align_center" id="p1.11.11.11.11.11.1"><sup class="ltx_sup" id="p1.11.11.11.11.11.1.1"><span class="ltx_text ltx_font_italic" id="p1.11.11.11.11.11.1.1.1">1</span></sup>Fudan University</span></span>
<span class="ltx_tr" id="p1.12.12.12.12.12">
<span class="ltx_td ltx_align_center" id="p1.12.12.12.12.12.1"><sup class="ltx_sup" id="p1.12.12.12.12.12.1.1"><span class="ltx_text ltx_font_italic" id="p1.12.12.12.12.12.1.1.1">2</span></sup>Shanghai AI Laboratory</span></span>
<span class="ltx_tr" id="p1.13.13.13.13.13">
<span class="ltx_td ltx_align_center" id="p1.13.13.13.13.13.1"><sup class="ltx_sup" id="p1.13.13.13.13.13.1.1"><span class="ltx_text ltx_font_italic" id="p1.13.13.13.13.13.1.1.1">3</span></sup>Shanghai Collaborative Innovation Center of Intelligent Visual Computing</span></span>
<span class="ltx_tr" id="p1.13.13.13.13.14">
<span class="ltx_td ltx_align_center" id="p1.13.13.13.13.14.1"><span class="ltx_text ltx_font_typewriter" id="p1.13.13.13.13.14.1.1">chengqy21@m.fudan.edu.cn
{lixn20, xpqiu}@fudan.edu.cn</span></span></span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">With the rapid development of large language models (LLMs)Â <cite class="ltx_cite ltx_citemacro_cite">Brown etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib6" title="">2020</a>); Touvron etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib36" title="">2023</a>); Zeng etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib46" title="">2023</a>); Yang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib42" title="">2023</a>); Cai etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib7" title="">2024</a>); Bai etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib4" title="">2023</a>)</cite>, AI assistants based on LLMs become unbiquitous and show remarkable abilities on various types of instructions, e.g., coding, writing and reasoningÂ <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib29" title="">2022</a>); Taori etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib35" title="">2023</a>); Chiang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib11" title="">2023</a>); Sun etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib34" title="">2024</a>); OpenAI (<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib30" title="">2023</a>); Anthropic (<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib2" title="">2023</a>); Anil etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib1" title="">2023</a>)</cite>.
However, LLMs often generate fabricated and non-factual informationÂ <cite class="ltx_cite ltx_citemacro_cite">Lin etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib26" title="">2022b</a>); Cheng etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib10" title="">2023</a>); Wang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib38" title="">2023a</a>)</cite>, which is called â€œhallucinationâ€ and makes LLMsâ€™ responses not trustworthy in real-world scenarios.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Retrieval-Augmented Generation (RAG) is a prevailing approach to address LLMâ€™s hallucinationÂ <cite class="ltx_cite ltx_citemacro_citep">(Guu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib18" title="">2020</a>; Gao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib17" title="">2024</a>)</cite>. Given a user query, it usually first retrieves relevant documents and then uses them to augment the LLMâ€™s factual correctness.
However, retrieval is not always helpful and applying it to every instruction is sub-optimal.
When faced with instructions that do not require external knowledge, RAG can impair the creativity and versatility of LLMsÂ <cite class="ltx_cite ltx_citemacro_cite">Asai etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib3" title="">2023</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="514" id="S1.F1.g1" src="x1.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Different types of user instructions, which can not be handled by single active retrieval criteria.</figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">If irrelevant knowledge is retrieved, it will hinder the LLM from utilizing its internal knowledge effectively and make it produce low-quality responsesÂ <cite class="ltx_cite ltx_citemacro_cite">Shi etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib33" title="">2023</a>); Yoran etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib45" title="">2023</a>)</cite>.
Meanwhile, compared with only LLM, RAG involves an additional retrieval process and the longer LLM input,
resulting in significantly longer response latency.
Therefore, applying RAG for all instructions is sub-optimal and unnecessary, and determining the correct timing for retrieval is crucial for LLMsâ€™ real-world application, which is often reftered to as <span class="ltx_text ltx_font_italic" id="S1.p3.1.1">Active Retrieval</span>Â <cite class="ltx_cite ltx_citemacro_citep">(Jiang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib21" title="">2023</a>; Asai etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib3" title="">2023</a>)</cite>.</p>
</div>
<figure class="ltx_table" id="S1.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S1.T1.1">
<tr class="ltx_tr" id="S1.T1.1.1">
<td class="ltx_td ltx_border_tt" id="S1.T1.1.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S1.T1.1.1.2">
<span class="ltx_text" id="S1.T1.1.1.2.1"></span> <span class="ltx_text" id="S1.T1.1.1.2.2">
<span class="ltx_tabular ltx_align_middle" id="S1.T1.1.1.2.2.1">
<span class="ltx_tr" id="S1.T1.1.1.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S1.T1.1.1.2.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.2.2.1.1.1.1">UAR</span></span></span>
<span class="ltx_tr" id="S1.T1.1.1.2.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S1.T1.1.1.2.2.1.2.1">(our work)</span></span>
</span></span><span class="ltx_text" id="S1.T1.1.1.2.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S1.T1.1.1.3">
<span class="ltx_text" id="S1.T1.1.1.3.1"></span> <span class="ltx_text" id="S1.T1.1.1.3.2">
<span class="ltx_tabular ltx_align_middle" id="S1.T1.1.1.3.2.1">
<span class="ltx_tr" id="S1.T1.1.1.3.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S1.T1.1.1.3.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.3.2.1.1.1.1">FLARE</span></span></span>
<span class="ltx_tr" id="S1.T1.1.1.3.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S1.T1.1.1.3.2.1.2.1"><cite class="ltx_cite ltx_citemacro_citep">(Jiang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib21" title="">2023</a>)</cite></span></span>
</span></span><span class="ltx_text" id="S1.T1.1.1.3.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S1.T1.1.1.4">
<span class="ltx_text" id="S1.T1.1.1.4.1"></span> <span class="ltx_text" id="S1.T1.1.1.4.2">
<span class="ltx_tabular ltx_align_middle" id="S1.T1.1.1.4.2.1">
<span class="ltx_tr" id="S1.T1.1.1.4.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S1.T1.1.1.4.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.4.2.1.1.1.1">Self-RAG</span></span></span>
<span class="ltx_tr" id="S1.T1.1.1.4.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S1.T1.1.1.4.2.1.2.1"><cite class="ltx_cite ltx_citemacro_citep">(Asai etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib3" title="">2023</a>)</cite></span></span>
</span></span><span class="ltx_text" id="S1.T1.1.1.4.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S1.T1.1.1.5">
<span class="ltx_text" id="S1.T1.1.1.5.1"></span> <span class="ltx_text" id="S1.T1.1.1.5.2">
<span class="ltx_tabular ltx_align_middle" id="S1.T1.1.1.5.2.1">
<span class="ltx_tr" id="S1.T1.1.1.5.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S1.T1.1.1.5.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.5.2.1.1.1.1">SKR</span></span></span>
<span class="ltx_tr" id="S1.T1.1.1.5.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S1.T1.1.1.5.2.1.2.1"><cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib40" title="">2023b</a>)</cite></span></span>
</span></span><span class="ltx_text" id="S1.T1.1.1.5.3"></span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S1.T1.1.2.1">Intent Awareness?</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.1.2.2"><span class="ltx_text" id="S1.T1.1.2.2.1" style="color:#1E821E;">âœ“</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.1.2.3"><span class="ltx_text" id="S1.T1.1.2.3.1" style="color:#FF0000;">âœ—</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.1.2.4"><span class="ltx_text" id="S1.T1.1.2.4.1" style="color:#FF0000;">âœ—</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.1.2.5"><span class="ltx_text" id="S1.T1.1.2.5.1" style="color:#FF0000;">âœ—</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.3">
<td class="ltx_td ltx_align_left" id="S1.T1.1.3.1">Knowledge Awareness?</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.3.2"><span class="ltx_text" id="S1.T1.1.3.2.1" style="color:#1E821E;">âœ“</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.3.3"><span class="ltx_text" id="S1.T1.1.3.3.1" style="color:#FF0000;">âœ—</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.3.4"><span class="ltx_text" id="S1.T1.1.3.4.1" style="color:#1E821E;">âœ“</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.3.5"><span class="ltx_text" id="S1.T1.1.3.5.1" style="color:#FF0000;">âœ—</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.4">
<td class="ltx_td ltx_align_left" id="S1.T1.1.4.1">Time Awareness?</td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.4.2"><span class="ltx_text" id="S1.T1.1.4.2.1" style="color:#1E821E;">âœ“</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.4.3"><span class="ltx_text" id="S1.T1.1.4.3.1" style="color:#FF0000;">âœ—</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.4.4"><span class="ltx_text" id="S1.T1.1.4.4.1" style="color:#FF0000;">âœ—</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.1.4.5"><span class="ltx_text" id="S1.T1.1.4.5.1" style="color:#FF0000;">âœ—</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.5">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S1.T1.1.5.1">Self Awareness?</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S1.T1.1.5.2"><span class="ltx_text" id="S1.T1.1.5.2.1" style="color:#1E821E;">âœ“</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S1.T1.1.5.3"><span class="ltx_text" id="S1.T1.1.5.3.1" style="color:#1E821E;">âœ“</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S1.T1.1.5.4"><span class="ltx_text" id="S1.T1.1.5.4.1" style="color:#FF0000;">âœ—</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S1.T1.1.5.5"><span class="ltx_text" id="S1.T1.1.5.5.1" style="color:#1E821E;">âœ“</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>
Comparison of UAR to other active retrieval methods. Exciting methods only consider a single active retrieval criterion, while UAR unifies four orthogonal criteria and can handle various types of user instructions.
</figcaption>
</figure>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In general, there are two lines of active retrieval methods. One is the â€œknowledge-awareâ€ method, based on the instructionâ€™s factual relevance, e.g., Self-RAGÂ <cite class="ltx_cite ltx_citemacro_citep">(Asai etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib3" title="">2023</a>)</cite>. If the instruction requires factual information, the retrieval will be triggered. Another line of work is the â€œself-awareâ€ method,
based on the LLMâ€™s self awarenessÂ <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib40" title="">2023b</a>)</cite>. The retrieval is only triggered when the LLM thinks that it does not know the answer, i.e., when it is uncertain. In this way, the retrieval can supplement knowledge for the LLM when necessary and avoid unnecessary retrieval cost.
Although these methods can determine retrieval timing for specialized scenarios, they still face two challenges:
1. Previous work usually relies on a single criterion, which struggles with diverse scenarios.
For instance, the self-aware methodÂ <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib40" title="">2023b</a>; Liu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib27" title="">2024</a>; Ding etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib14" title="">2024</a>)</cite> struggles with various instructions such as time-sensitive queries or those with userâ€™s explicit retrieval intent.
For time-sensitive questions, it is challenging for a static LLM to judge whether it possesses the correct knowledge for a rapidly changing answer.
Additionally, these methods often overlook userâ€™s intent in real-world scenarios, such as when a user seeks a verifiable answer that requires external information sources, necessitating retrieval.
Therefore, correctly determining whether to retrieve requires multifaceted decision-making.
2. Existing methods rely on specialized procedures, complicating the integration within the RAG system and increasing computational load.
For example, FLAREÂ <cite class="ltx_cite ltx_citemacro_citep">(Jiang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib21" title="">2023</a>)</cite> uses the confidence of generation and RowenÂ <cite class="ltx_cite ltx_citemacro_citep">(Ding etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib14" title="">2024</a>)</cite> relies on response divergence for the same question.
These highly differentiated approaches are difficult to unify, making it very difficult to extend them to new scenarios.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.2">To address these challenges, we propose <span class="ltx_text ltx_font_bold" id="S1.p5.2.1">U</span>nified <span class="ltx_text ltx_font_bold" id="S1.p5.2.2">A</span>ctive <span class="ltx_text ltx_font_bold" id="S1.p5.2.3">R</span>etrieval (<span class="ltx_text ltx_font_bold" id="S1.p5.2.4">UAR</span>), a unified and comprehensive framework for judging whether to retrieve for various types of user instructions.
UAR consists of various orthogonal criteria of retrieval timing and casts them into unified classification tasks, and thus can judge the LLMâ€™s retrieval timing both comprehensively and efficiently.
Specifically, UAR consists of four orthogonal criteria for determining the retrieval timing:
1) <span class="ltx_text ltx_font_bold" id="S1.p5.2.5">Intent-aware</span>: whether the user desires retrieval / external information;
2) <span class="ltx_text ltx_font_bold" id="S1.p5.2.6">Knowledge-aware</span>: whether the question requires fact knowledge;
3) <span class="ltx_text ltx_font_bold" id="S1.p5.2.7">Time-Sensitive-aware</span>: whether the question is time-sensitive;
4) <span class="ltx_text ltx_font_bold" id="S1.p5.2.8">Self-aware</span>: whether the LLM has the internal knowledge.
As shown in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S1.T1" title="Table 1 â€£ 1 Introduction â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">1</span></a>, compared with previous methods of single criterionÂ <cite class="ltx_cite ltx_citemacro_citep">(Jiang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib21" title="">2023</a>; Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib40" title="">2023b</a>; Asai etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib3" title="">2023</a>)</cite>,
UAR can comprehensively handle various types of user instructions and call retrieval accurately considering multiple active retrieval criteria.
To efficiently achieve judgements of multiple criteria, UAR unifies each criterionâ€™s judgement into binary classification tasks using lightweight classifiers.
For each criterion <math alttext="c_{i}" class="ltx_Math" display="inline" id="S1.p5.1.m1.1"><semantics id="S1.p5.1.m1.1a"><msub id="S1.p5.1.m1.1.1" xref="S1.p5.1.m1.1.1.cmml"><mi id="S1.p5.1.m1.1.1.2" xref="S1.p5.1.m1.1.1.2.cmml">c</mi><mi id="S1.p5.1.m1.1.1.3" xref="S1.p5.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S1.p5.1.m1.1b"><apply id="S1.p5.1.m1.1.1.cmml" xref="S1.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S1.p5.1.m1.1.1.1.cmml" xref="S1.p5.1.m1.1.1">subscript</csymbol><ci id="S1.p5.1.m1.1.1.2.cmml" xref="S1.p5.1.m1.1.1.2">ğ‘</ci><ci id="S1.p5.1.m1.1.1.3.cmml" xref="S1.p5.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.1.m1.1c">c_{i}</annotation><annotation encoding="application/x-llamapun" id="S1.p5.1.m1.1d">italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, we train a plug-and-play binary classifier on the last layerâ€™s hidden states of a fixed LLM, to judge whether the input requires retrieval according to <math alttext="c_{i}" class="ltx_Math" display="inline" id="S1.p5.2.m2.1"><semantics id="S1.p5.2.m2.1a"><msub id="S1.p5.2.m2.1.1" xref="S1.p5.2.m2.1.1.cmml"><mi id="S1.p5.2.m2.1.1.2" xref="S1.p5.2.m2.1.1.2.cmml">c</mi><mi id="S1.p5.2.m2.1.1.3" xref="S1.p5.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S1.p5.2.m2.1b"><apply id="S1.p5.2.m2.1.1.cmml" xref="S1.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S1.p5.2.m2.1.1.1.cmml" xref="S1.p5.2.m2.1.1">subscript</csymbol><ci id="S1.p5.2.m2.1.1.2.cmml" xref="S1.p5.2.m2.1.1.2">ğ‘</ci><ci id="S1.p5.2.m2.1.1.3.cmml" xref="S1.p5.2.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.2.m2.1c">c_{i}</annotation><annotation encoding="application/x-llamapun" id="S1.p5.2.m2.1d">italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>.
In this way, UAR does not change LLMsâ€™ parameters, avoiding the costly LLM fine-tuning and performance degradationÂ <cite class="ltx_cite ltx_citemacro_citep">(Yang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib43" title="">2024</a>)</cite>.
Meanwhile, the classifiers and LLM generation share the same input encoding, which makes UAR only need to encode the input once and thus achieves multifaceted retrieval timing judgements with negligible extra inference cost.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">To handle various instructions in an unified procedure, we further propose Unified Active Retrieval Criteria (UAR-Criteria), which specifies priorities for multiple retrieval criteria and unifies them into a single multifaceted decision tree.
As shown in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S2.F2" title="Figure 2 â€£ 2.3 Self-awareness of LLMs â€£ 2 Related Work â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">2</span></a>, UAR-Criteria can trigger retrieval for time-sensitive or LLM-unknown instructions, which facilitates necessary external information supplement. Meanwhile, UAR-Criteria cancels retrieval for those non-knowledge-intensive or LLM-known instructions, which avoids the negative effect of unnecessary retrieval.
In this way, UAR-Criteria unifies the process to comprehensively decide whether to retrieval for various types of user instructions, which facilitates more effective RAG.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Experiments on four representative types of user instructions show that UAR significantly outperforms existing work on the retrieval timing judgement accuracy and the performance of downstream tasks, which verifies the effectiveness of UAR and its helpfulness to downstream tasks.
We summarize our contributions as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We propose an active retrieval framework named Unified Active Retrieval (UAR) for Retrieval-Augmented Generation (RAG). To the best of our knowledge, UAR is the first work to propose multifaceted criteria for active retrieval and demonstrate its necessity.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We curate the Active Retrieval benchmark (AR-Bench) for evaluating the accuracy of retrieval timing and conduct comprehensive experiments on AR-Bench and downstream tasks. The results show that UAR significantly outperforms existing work and achieves more efficient RAG.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We release the code, data, models and relevant resources to facilitate future research<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/xiami2019/UAR" title="">https://github.com/xiami2019/UAR</a></span></span></span>.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Active Retrieval</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Compared to applying retrieval for every instruction (passive retrieval), active retrieval has advantages such as not hurting the versatility of the model, reducing the number of retrievals, and preventing interference from low-quality retrieval results.
Self-RAG <cite class="ltx_cite ltx_citemacro_citep">(Asai etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib3" title="">2023</a>)</cite> construct active retrieval data using GPT-4 and teach the model to not retrieve when encounter non-knowledge-intensive instructions.
FLARE <cite class="ltx_cite ltx_citemacro_citep">(Jiang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib21" title="">2023</a>)</cite> proposes forward-looking active retrieval augmented generation based on modelâ€™s confidence, only retrieving information when the modelâ€™s uncertainty for the prediction is high.
SKR <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib40" title="">2023b</a>)</cite>, RA-ISF <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib27" title="">2024</a>)</cite> and Self-DC <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib39" title="">2024</a>)</cite> first determines whether the model knows the questions and then retrieves only when the model does not know.
However, current active retrieval methods mostly consider only a single scenario and are unable to adapt to complex situations in real-world applications.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Time-awareness of LLMs</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">There are some papers focus on the time awareness of large language models.
<cite class="ltx_cite ltx_citemacro_citet">Chen etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib8" title="">2021</a>)</cite> construct a time-sensitive QA dataset called TimeQA to evaluate the modelâ€™s ability to handle temporal questions.
<cite class="ltx_cite ltx_citemacro_citet">Fierro etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib16" title="">2024</a>)</cite> create a benchmark named MULAN for evaluating the ability of language models to predict mutable facts.
They find representations classification can distinct immutable and mutable facts, which means language models have a certain degree of temporal awareness.
<cite class="ltx_cite ltx_citemacro_citet">Zhao etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib48" title="">2024</a>)</cite> investigate whether language models can align their internal knowledge to a target year.
They construct a dataset which contains time-sensitive questions.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Self-awareness of LLMs</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Self-awareness means that large language model can be aware of what they know and what they donâ€™t know.
<cite class="ltx_cite ltx_citemacro_citet">Kadavath etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib23" title="">2022</a>)</cite> find that language models can be well-calibrated when using a multiple-choice template.
And they also finetune a value head to predict whether language models know the answer to the given question.
<cite class="ltx_cite ltx_citemacro_citet">Lin etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib25" title="">2022a</a>)</cite> finetune GPT-3 to express uncertainty in words on math questions.
<cite class="ltx_cite ltx_citemacro_citet">Yin etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib44" title="">2023</a>)</cite> collect some unanswerable questions to evaluate whether language models can express uncertainty to these unanswerable questions.
<cite class="ltx_cite ltx_citemacro_citet">Zhang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib47" title="">2023</a>)</cite> utilize supervised fine-tune to teach large language models to refuse questions which beyond their knowledge scope.
<cite class="ltx_cite ltx_citemacro_citet">Cheng etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib9" title="">2024</a>)</cite> explore more alignment methods beyond supervised fine-tuning to teach language models know and express what they donâ€™t know, like preference optimization.
Results of previous work show that we can enhance language modelsâ€™ self-awareness with corresponding dataset.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="382" id="S2.F2.g1" src="x2.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overview of the UAR framework. <img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="14" id="S2.F2.3.g1" src="extracted/5897613/images/snowflake.png" width="14"/> indicates that we freeze these parameters. <img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="14" id="S2.F2.4.g2" src="extracted/5897613/images/flame.png" width="14"/> indicates that we update these parameters. Each MLP is a fully connected layer, with an input dimension equal to the modelâ€™s hidden state dimension and an output dimension of 2.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">UAR is a plug-and-play active retrieval framework.
As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S2.F2" title="Figure 2 â€£ 2.3 Self-awareness of LLMs â€£ 2 Related Work â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">2</span></a>, we fix the parameters of the LLM and train a lightweight classifier for each active retrieval criteria using the modelâ€™s hidden states, which is far more efficient than fine-tuning the entire model.
Besides, UAR determines the need for active retrieval following the UAR-Criteria shown on the right side of Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S2.F2" title="Figure 2 â€£ 2.3 Self-awareness of LLMs â€£ 2 Related Work â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">2</span></a>, invoking retrieval when necessary and avoiding unnecessary across various scenarios, making RAG more effective and efficient.
For instructions requiring retrieval, we append the retrieved documents to the original instruction,
which means that UAR does not introduce extra LLM inference cost.
We introduce the details of our UAR framework in the following sections.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>UAR Classifiers Training</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We construct distinct training data tailored to each scenario.</p>
</div>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Self-aware</h4>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.1">In the self-aware scenario, the model must determine if it knows the answer to a given question.
Following the methodology in <cite class="ltx_cite ltx_citemacro_citet">Cheng etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib9" title="">2024</a>)</cite>, we create model-specific IDK (I donâ€™t know) datasets.
For example, with the Llama2-7B-chat model, we use the TriviaQA <cite class="ltx_cite ltx_citemacro_citep">(Joshi etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib22" title="">2017</a>)</cite> dataset, sampling ten responses for each question.
If all responses are correct, the question is marked as known; otherwise, it is unknown.
10% of the TriviaQA training set is used for validation, with the rest designated as the training set.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Time-aware</h4>
<div class="ltx_para" id="S3.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p1.1">In the time-aware scenario, it is critical to determine if a userâ€™s question is time-sensitive, meaning the answer changes over time.
We utilize questions from TAQAâ€™s <cite class="ltx_cite ltx_citemacro_citep">(Zhao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib48" title="">2024</a>)</cite> training and validation sets as time-sensitive questions.
In contrast, we sample an equivalent number of questions from the TriviaQA training set to represent non-time-sensitive questions, which typically have static answers.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Knowledge-aware</h4>
<div class="ltx_para" id="S3.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px3.p1.1">In the knowledge-aware scenario, identifying whether a userâ€™s instruction requires factual knowledge is essential.
We use non-retrieval instruct-following data from the Self-RAG <cite class="ltx_cite ltx_citemacro_citep">(Asai etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib3" title="">2023</a>)</cite> training set, which GPT-4 classifies as non-knowledge-intensive.
We select 2,000 entries for our validation set and 22,801 for training.
Additionally, we incorporate all entries from our time-aware dataâ€™s training and validation sets as knowledge-intensive instructions to complete the final knowledge-aware training and validation sets.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Intent-aware</h4>
<div class="ltx_para" id="S3.SS1.SSS0.Px4.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px4.p1.1">In the intent-aware scenario, itâ€™s crucial to identify usersâ€™ intentions to use retrieval-augmented generation.
Due to a lack of data with explicit retrieval intentions, we use Self-Instruct <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib41" title="">2023c</a>)</cite> to generate 3,000 user intents from ten handwritten intents.
We allocate 2,000 for training, 500 for validation, and 500 for testing.
We assemble user queries by sampling 52,949 entries from Self-RAGâ€™s non-retrieval-required data, and factual knowledge questions from TAQA and TriviaQA for the training set, with an additional 5,000 for validation.
We integrate half of these data with user retrieval intents, alternating the position of intents before and after user inputs, to create inputs with retrieval intents. The remaining data are used as inputs without retrieval intents.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS0.Px4.p2">
<p class="ltx_p" id="S3.SS1.SSS0.Px4.p2.1">For each scenario, we train a single-layer MLP as the classifier, using the hidden states from the last token in the input as the input to the classification head. In this way, UAR can achieve various criteriaâ€™s judgements with negligible extra computational cost.
We include details of classifiersâ€™ training in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#A5" title="Appendix E Details of UAR Training â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">E</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>UAR Criteria</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We further propose UAR-Criteria to unify the judgements of different types of user instructions in to one unified procedure.
During the inference stage, UAR sequentially utilizes four classifiers according to different priorities to determine the correct timing for retrieval calls, and we introduce its details as follows.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Initially, UAR checks whether the user intends to use retrieval augmentation.
If so, retrieval is triggered.
If not, UAR evaluates whether the input is knowledge-intensive.
For non-knowledge-intensive tasks, retrieval is not used.
For knowledge-intensive tasks, UAR further assesses whether the knowledge is time-sensitive.
Retrieval is necessary for time-sensitive questions.
For non-time-sensitive, knowledge-intensive tasks, UAR checks whether the model already has the relevant knowledge, activating retrieval only for unfamiliar questions.
In this way, UAR can handle various types of instructions.
Specifically, UAR-Criteria activates retrieval for instructions that are time-sensitive, unknown to the model, and have explicit retrieval intent, which facilitates necessary external information supplement.
Meanwhile, UAR-Criteria cancels retrieval for those non-knowledge-intensive or LLM-known instructions, which avoids the negative effect of unnecessary retrieval.
Meanwhile, since UAR achieves the judgement of multifaceted criteria by linear classifiers, the introduced extra computational cost is negligible.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Generation with Relevant Information</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">For instructions requiring retrieval augmentation, we append the retrieved external information with a RAG template to the original user input. Since most of the prevailing LLMs are based on the decoder-only architectureÂ <cite class="ltx_cite ltx_citemacro_citep">(Brown etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib6" title="">2020</a>)</cite>, UAR can avoid the need to recompute the original instruction.
The retriever might fetch information irrelevant to the question, our prompt instructs the model to utilize only the information relevant to the question.
This approach helps prevent irrelevant information from misleading the model.
An example of our RAG prompt is as follows:</p>
<div class="ltx_listing ltx_lstlisting ltx_framed ltx_framed_rectangle ltx_listing" id="S3.SS3.p1.2">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,e3F1ZXN0aW9ufQpIZXJlIGFyZSBzb21lIGFkZGl0aW9uYWwgcmVmZXJlbmNlIHBhc3NhZ2VzOgp7cmVmZXJlbmNlIHBhc3NhZ2VzfQpZb3UgY2FuIHJlZmVyIHRvIHRoZSBjb250ZW50IG9mIHJlbGV2YW50IHJlZmVyZW5jZSBwYXNzYWdlcyB0byBhbnN3ZXIgdGhlIHF1ZXN0aW9ucy4KTm93IGdpdmUgbWUgdGhlIGFuc3dlci4=">â¬‡</a></div>
<div class="ltx_listingline" id="lstnumberx1">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx1.1">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.2">question</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.3">}</span>
</div>
<div class="ltx_listingline" id="lstnumberx2">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.1">Here</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.3">are</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.5">some</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.7">additional</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.9">reference</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.11">passages</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx2.12">:</span>
</div>
<div class="ltx_listingline" id="lstnumberx3">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx3.1">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.2">reference</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.3"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.4">passages</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.5">}</span>
</div>
<div class="ltx_listingline" id="lstnumberx4">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.1">You</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.3">can</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.5">refer</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.7">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.9">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.11">content</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.13">of</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.15">relevant</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.16"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.17">reference</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.18"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.19">passages</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.20"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.21">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.22"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.23">answer</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.24"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.25">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.26"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.27">questions</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.28">.</span>
</div>
<div class="ltx_listingline" id="lstnumberx5">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.1">Now</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.3">give</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.5">me</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.7">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.9">answer</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.10">.</span>
</div>
</div>
<p class="ltx_p" id="S3.SS3.p1.3">For instructions that do not require retrieval, we allow the model to generate outputs in its original format.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Benchmarking Retrieval Timing</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We curate an Active Retrieval Benchmark (AR-Bench) to evaluate the accuracy of various active retrieval methods in determining the timing of retrieval.
The AR-Bench includes four sub-tasks: intent-aware, knowledge-aware, time-aware and self-aware, covering all the active retrieval scenarios mentioned in this paper.
Each sub-task is a binary classification task comprising 8,000 samples, with a 1:1 ratio of positive to negative examples, and these samples do not overlap with the training data of UAR.
These four sub-tasks separately evaluate one single active retrieval criterion and we control variables to ensure that each taskâ€™s retrieval decision solely depends on one single criterion.
We introduce details of AR-Bench construction in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#A1" title="Appendix A Details of AR-Bench Construction â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T2.2">
<tr class="ltx_tr" id="S4.T2.2.3">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T2.2.3.1" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.3.1.1">Scenario</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.2.3.2" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.3.2.1">Intent-aware</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.2.3.3" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.3.3.1">Knowledge-aware</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.2.3.4" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.3.4.1">Time-aware</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.2.3.5" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.3.5.1">Self-aware</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T2.2.3.6" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.3.6.1">Overall</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.4">
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="6" id="S4.T2.2.4.1" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_italic" id="S4.T2.2.4.1.1">7B Models</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.2.5.1" style="padding-left:7.0pt;padding-right:7.0pt;">FLARE</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.5.2" style="padding-left:7.0pt;padding-right:7.0pt;">61.95</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.5.3" style="padding-left:7.0pt;padding-right:7.0pt;">56.76</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.5.4" style="padding-left:7.0pt;padding-right:7.0pt;">53.69</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.5.5" style="padding-left:7.0pt;padding-right:7.0pt;">53.59</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.2.5.6" style="padding-left:7.0pt;padding-right:7.0pt;">56.50</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1">
<td class="ltx_td ltx_align_left" id="S4.T2.1.1.1" style="padding-left:7.0pt;padding-right:7.0pt;">Self-RAG<sup class="ltx_sup" id="S4.T2.1.1.1.1">â€ </sup>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.2" style="padding-left:7.0pt;padding-right:7.0pt;">64.26</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.3" style="padding-left:7.0pt;padding-right:7.0pt;">72.82</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.4" style="padding-left:7.0pt;padding-right:7.0pt;">47.45</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.5" style="padding-left:7.0pt;padding-right:7.0pt;">55.95</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.1.6" style="padding-left:7.0pt;padding-right:7.0pt;">60.12</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.6">
<td class="ltx_td ltx_align_left" id="S4.T2.2.6.1" style="padding-left:7.0pt;padding-right:7.0pt;">SKR</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.6.2" style="padding-left:7.0pt;padding-right:7.0pt;">58.73</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.6.3" style="padding-left:7.0pt;padding-right:7.0pt;">42.94</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.6.4" style="padding-left:7.0pt;padding-right:7.0pt;">76.61</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.6.5" style="padding-left:7.0pt;padding-right:7.0pt;">70.28</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.2.6.6" style="padding-left:7.0pt;padding-right:7.0pt;">62.14</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.2.7.1" style="padding-left:7.0pt;padding-right:7.0pt;">UAR</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.7.2" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.7.2.1">91.88</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.7.3" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.7.3.1">90.38</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.7.4" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.7.4.1">86.69</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.7.5" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.7.5.1">72.32</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.2.7.6" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.7.6.1">85.32</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.8">
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="6" id="S4.T2.2.8.1" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_italic" id="S4.T2.2.8.1.1">13B Models</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.9">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.2.9.1" style="padding-left:7.0pt;padding-right:7.0pt;">FLARE</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.9.2" style="padding-left:7.0pt;padding-right:7.0pt;">65.49</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.9.3" style="padding-left:7.0pt;padding-right:7.0pt;">53.54</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.9.4" style="padding-left:7.0pt;padding-right:7.0pt;">55.20</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.9.5" style="padding-left:7.0pt;padding-right:7.0pt;">54.61</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.2.9.6" style="padding-left:7.0pt;padding-right:7.0pt;">57.21</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2">
<td class="ltx_td ltx_align_left" id="S4.T2.2.2.1" style="padding-left:7.0pt;padding-right:7.0pt;">Self-RAG<sup class="ltx_sup" id="S4.T2.2.2.1.1">â€ </sup>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.2" style="padding-left:7.0pt;padding-right:7.0pt;">67.80</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.3" style="padding-left:7.0pt;padding-right:7.0pt;">64.85</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.4" style="padding-left:7.0pt;padding-right:7.0pt;">54.44</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.5" style="padding-left:7.0pt;padding-right:7.0pt;">52.49</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.2.2.6" style="padding-left:7.0pt;padding-right:7.0pt;">59.89</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.10">
<td class="ltx_td ltx_align_left" id="S4.T2.2.10.1" style="padding-left:7.0pt;padding-right:7.0pt;">SKR</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.10.2" style="padding-left:7.0pt;padding-right:7.0pt;">59.00</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.10.3" style="padding-left:7.0pt;padding-right:7.0pt;">43.18</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.10.4" style="padding-left:7.0pt;padding-right:7.0pt;">79.91</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.10.5" style="padding-left:7.0pt;padding-right:7.0pt;">68.70</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.2.10.6" style="padding-left:7.0pt;padding-right:7.0pt;">62.70</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.11">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T2.2.11.1" style="padding-left:7.0pt;padding-right:7.0pt;">UAR</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.2.11.2" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.11.2.1">92.49</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.2.11.3" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.11.3.1">91.04</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.2.11.4" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.11.4.1">87.94</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.2.11.5" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.11.5.1">73.84</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.2.11.6" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.11.6.1">86.33</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparisons of active retrieval accuracy on our active retrieval benchmark (AR-Bench). <math alttext="\dagger" class="ltx_Math" display="inline" id="S4.T2.4.m1.1"><semantics id="S4.T2.4.m1.1b"><mo id="S4.T2.4.m1.1.1" xref="S4.T2.4.m1.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.m1.1c"><ci id="S4.T2.4.m1.1.1.cmml" xref="S4.T2.4.m1.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.m1.1d">\dagger</annotation><annotation encoding="application/x-llamapun" id="S4.T2.4.m1.1e">â€ </annotation></semantics></math>: Self-RAG is fine-tuned from Llama2-base models. Other methods are based on Llama2-chat models.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Downstream Tasks</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We select six datasets to test UARâ€™s performance in real downstream tasks and its adaptability to different active retrieval scenarios.
Since the intent-aware judgement focuses on satisfying usersâ€™ retrieval intent, which is not reflected on the objective downstream performance, the selected datasets cover the remaining three scenarios: knowledge-aware, time-aware, and self-aware.
For knowledge-aware scenario, we use DROP <cite class="ltx_cite ltx_citemacro_citep">(Dua etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib15" title="">2019</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citep">(Cobbe etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib12" title="">2021</a>)</cite>.
For time-aware scenario, we use TAQA <cite class="ltx_cite ltx_citemacro_citep">(Zhao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib48" title="">2024</a>)</cite> and FreshQA <cite class="ltx_cite ltx_citemacro_citep">(Vu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib37" title="">2023</a>)</cite>.
For self-aware scenario, we use TriviaQA <cite class="ltx_cite ltx_citemacro_citep">(Joshi etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib22" title="">2017</a>)</cite> and WebQuestions (WQ) <cite class="ltx_cite ltx_citemacro_citep">(Berant etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib5" title="">2013</a>)</cite>.
We provide a detailed introduction to these datasets in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#A6" title="Appendix F Downstream Task Datasets â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">F</span></a>.
In these six datasets, we only use the training sets of TriviaQA anf TAQA for UARâ€™s training, and thus the remaining evaluation dataset can reflect the UARâ€™s out-of-distribution (OOD) performance, which can further verify the effectiveness of UAR in complicated real-world scenarios.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Baselines</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">We choose three active retrieval methods as our baseline methods: FLARE <cite class="ltx_cite ltx_citemacro_citep">(Jiang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib21" title="">2023</a>)</cite>, Self-RAG <cite class="ltx_cite ltx_citemacro_citep">(Asai etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib3" title="">2023</a>)</cite>, and SKR <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib40" title="">2023b</a>)</cite>, covering two main active retrieval criteria.
FLARE determines whether external retrieval is needed by assessing the modelâ€™s uncertainty about the generated responses.
SKR first collects modelâ€™s self-knowledge (knowns and unknowns) data, then trains a BERT-based <cite class="ltx_cite ltx_citemacro_citep">(Devlin etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib13" title="">2019</a>)</cite> classifier to determine whether the model knows a certain question.
For questions the model does not know, retrieval augmentation is used.
Self-RAG gathers a large amount of knowledge-intensive and instruction-following data (no fact knowledge required), then trains the pre-trained model to only use retrieval augmentation for knowledge-intensive tasks.
For downstream tasks, we also include generation with never-retrieval and always-retrieval as baseline methods.
The original SKR and FLARE are not based on Llama2, so we re-implement these methods on the Llama2 model.
The details of our re-implementation are provided in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#A2" title="Appendix B Details of Baselines Re-implementation â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Retrievers</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">For time-sensitive datasets TAQA and FreshQA, we follow the settings in FreshQA <cite class="ltx_cite ltx_citemacro_citet">Vu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib37" title="">2023</a>)</cite> and use Google Search.
For other datasets, following the settings in Self-RAG, we use off-the-shelf Contriever-MS MARCO <cite class="ltx_cite ltx_citemacro_citep">(Izacard etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib20" title="">2022</a>)</cite> and retrieve up to ten documents for each input.
During generation, we use the top five retrieved documents.
For other datasets, following the settings in Self-RAG, we adopt off-the-shelf Contriever-MS MARCOÂ <cite class="ltx_cite ltx_citemacro_citep">(Izacard etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib20" title="">2022</a>)</cite> and use the top-5 documents.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T3.74">
<tr class="ltx_tr" id="S4.T3.74.75">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T3.74.75.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.74.75.1.1">Dataset</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.74.75.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.74.75.2.1">Drop</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.74.75.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.74.75.3.1">GSM8K</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.74.75.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.74.75.4.1">TriviaQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.74.75.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.74.75.5.1">WQ</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.74.75.6" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.74.75.6.1">TAQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.74.75.7" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.74.75.7.1">FreshQA</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.74.75.8" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.74.75.8.1">Overall</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.74.76">
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="8" id="S4.T3.74.76.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_italic" id="S4.T3.74.76.1.1">7B Models</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.6.6">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T3.6.6.7" style="padding-left:4.0pt;padding-right:4.0pt;">Never-Ret</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">57.67<sub class="ltx_sub" id="S4.T3.1.1.1.1"><span class="ltx_text ltx_font_italic" id="S4.T3.1.1.1.1.1">(0%)</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.2.2.2" style="padding-left:4.0pt;padding-right:4.0pt;">26.91<sub class="ltx_sub" id="S4.T3.2.2.2.1"><span class="ltx_text ltx_font_italic" id="S4.T3.2.2.2.1.1">(0%)</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.3.3.3" style="padding-left:4.0pt;padding-right:4.0pt;">62.15<sub class="ltx_sub" id="S4.T3.3.3.3.1"><span class="ltx_text ltx_font_italic" id="S4.T3.3.3.3.1.1">(0%)</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.4.4.4" style="padding-left:4.0pt;padding-right:4.0pt;">59.79<sub class="ltx_sub" id="S4.T3.4.4.4.1"><span class="ltx_text ltx_font_italic" id="S4.T3.4.4.4.1.1">(0%)</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.5.5.5" style="padding-left:4.0pt;padding-right:4.0pt;">16.43<sub class="ltx_sub" id="S4.T3.5.5.5.1"><span class="ltx_text ltx_font_italic" id="S4.T3.5.5.5.1.1">(0%)</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.6.6.6" style="padding-left:4.0pt;padding-right:4.0pt;">35.64<sub class="ltx_sub" id="S4.T3.6.6.6.1"><span class="ltx_text ltx_font_italic" id="S4.T3.6.6.6.1.1">(0%)</span></sub>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.6.6.8" style="padding-left:4.0pt;padding-right:4.0pt;">43.10</td>
</tr>
<tr class="ltx_tr" id="S4.T3.12.12">
<td class="ltx_td ltx_align_left" id="S4.T3.12.12.7" style="padding-left:4.0pt;padding-right:4.0pt;">Always-Ret</td>
<td class="ltx_td ltx_align_center" id="S4.T3.7.7.1" style="padding-left:4.0pt;padding-right:4.0pt;">49.57<sub class="ltx_sub" id="S4.T3.7.7.1.1"><span class="ltx_text ltx_font_italic" id="S4.T3.7.7.1.1.1">(100%)</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.8.8.2" style="padding-left:4.0pt;padding-right:4.0pt;">23.65<sub class="ltx_sub" id="S4.T3.8.8.2.1"><span class="ltx_text ltx_font_italic" id="S4.T3.8.8.2.1.1">(100%)</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.9.9.3" style="padding-left:4.0pt;padding-right:4.0pt;">68.73<sub class="ltx_sub" id="S4.T3.9.9.3.1"><span class="ltx_text ltx_font_italic" id="S4.T3.9.9.3.1.1">(100%)</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.10.10.4" style="padding-left:4.0pt;padding-right:4.0pt;">53.99<sub class="ltx_sub" id="S4.T3.10.10.4.1"><span class="ltx_text ltx_font_italic" id="S4.T3.10.10.4.1.1">(100%)</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.11.5" style="padding-left:4.0pt;padding-right:4.0pt;">34.49<sub class="ltx_sub" id="S4.T3.11.11.5.1"><span class="ltx_text ltx_font_italic" id="S4.T3.11.11.5.1.1">(100%)</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.12.12.6" style="padding-left:4.0pt;padding-right:4.0pt;">65.35<sub class="ltx_sub" id="S4.T3.12.12.6.1"><span class="ltx_text ltx_font_italic" id="S4.T3.12.12.6.1.1">(100%)</span></sub>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.12.12.8" style="padding-left:4.0pt;padding-right:4.0pt;">49.23</td>
</tr>
<tr class="ltx_tr" id="S4.T3.74.77">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="8" id="S4.T3.74.77.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_italic" id="S4.T3.74.77.1.1">Active Retrieval</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.19.19">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.13.13.1" style="padding-left:4.0pt;padding-right:4.0pt;">Self-RAG<sup class="ltx_sup" id="S4.T3.13.13.1.1">â€ </sup>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.14.14.2" style="padding-left:4.0pt;padding-right:4.0pt;">39.17<sub class="ltx_sub" id="S4.T3.14.14.2.1"><span class="ltx_text ltx_font_italic" id="S4.T3.14.14.2.1.1">(5.7%)</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.15.15.3" style="padding-left:4.0pt;padding-right:4.0pt;">16.07<sub class="ltx_sub" id="S4.T3.15.15.3.1"><span class="ltx_text ltx_font_italic" id="S4.T3.15.15.3.1.1">(4.9%)</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.16.16.4" style="padding-left:4.0pt;padding-right:4.0pt;">61.68<sub class="ltx_sub" id="S4.T3.16.16.4.1"><span class="ltx_text ltx_font_italic" id="S4.T3.16.16.4.1.1">(53.5%)</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.17.17.5" style="padding-left:4.0pt;padding-right:4.0pt;">43.01<sub class="ltx_sub" id="S4.T3.17.17.5.1"><span class="ltx_text ltx_font_italic" id="S4.T3.17.17.5.1.1">(61.9%)</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.18.18.6" style="padding-left:4.0pt;padding-right:4.0pt;">11.09<sub class="ltx_sub" id="S4.T3.18.18.6.1"><span class="ltx_text ltx_font_italic" id="S4.T3.18.18.6.1.1">(42.1%)</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.19.19.7" style="padding-left:4.0pt;padding-right:4.0pt;">44.88<sub class="ltx_sub" id="S4.T3.19.19.7.1"><span class="ltx_text ltx_font_italic" id="S4.T3.19.19.7.1.1">(51.2%)</span></sub>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.19.19.8" style="padding-left:4.0pt;padding-right:4.0pt;">35.98</td>
</tr>
<tr class="ltx_tr" id="S4.T3.25.25">
<td class="ltx_td ltx_align_left" id="S4.T3.25.25.7" style="padding-left:4.0pt;padding-right:4.0pt;">SKR</td>
<td class="ltx_td ltx_align_center" id="S4.T3.20.20.1" style="padding-left:4.0pt;padding-right:4.0pt;">53.00<sub class="ltx_sub" id="S4.T3.20.20.1.1"><span class="ltx_text ltx_font_italic" id="S4.T3.20.20.1.1.1">(61.4%)</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.21.21.2" style="padding-left:4.0pt;padding-right:4.0pt;">26.38<sub class="ltx_sub" id="S4.T3.21.21.2.1"><span class="ltx_text ltx_font_italic" id="S4.T3.21.21.2.1.1">(35.3%)</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.22.22.3" style="padding-left:4.0pt;padding-right:4.0pt;">65.39<sub class="ltx_sub" id="S4.T3.22.22.3.1"><span class="ltx_text ltx_font_italic" id="S4.T3.22.22.3.1.1">(48.9%)</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.23.23.4" style="padding-left:4.0pt;padding-right:4.0pt;">58.96<sub class="ltx_sub" id="S4.T3.23.23.4.1"><span class="ltx_text ltx_font_italic" id="S4.T3.23.23.4.1.1">(26.8%)</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.24.24.5" style="padding-left:4.0pt;padding-right:4.0pt;">30.63<sub class="ltx_sub" id="S4.T3.24.24.5.1"><span class="ltx_text ltx_font_italic" id="S4.T3.24.24.5.1.1">(79.9%)</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.25.25.6" style="padding-left:4.0pt;padding-right:4.0pt;">48.84<sub class="ltx_sub" id="S4.T3.25.25.6.1"><span class="ltx_text ltx_font_italic" id="S4.T3.25.25.6.1.1">(39.3%)</span></sub>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.25.25.8" style="padding-left:4.0pt;padding-right:4.0pt;">47.17</td>
</tr>
<tr class="ltx_tr" id="S4.T3.31.31">
<td class="ltx_td ltx_align_left" id="S4.T3.31.31.7" style="padding-left:4.0pt;padding-right:4.0pt;">FLARE</td>
<td class="ltx_td ltx_align_center" id="S4.T3.26.26.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.26.26.1.1">56.98<sub class="ltx_sub" id="S4.T3.26.26.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T3.26.26.1.1.1.1">(9.6%)</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.27.27.2" style="padding-left:4.0pt;padding-right:4.0pt;">26.76<sub class="ltx_sub" id="S4.T3.27.27.2.1"><span class="ltx_text ltx_font_italic" id="S4.T3.27.27.2.1.1">(45.8%)</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.28.28.3" style="padding-left:4.0pt;padding-right:4.0pt;">65.98<sub class="ltx_sub" id="S4.T3.28.28.3.1"><span class="ltx_text ltx_font_italic" id="S4.T3.28.28.3.1.1">(58.8%)</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.29.29.4" style="padding-left:4.0pt;padding-right:4.0pt;">55.46<sub class="ltx_sub" id="S4.T3.29.29.4.1"><span class="ltx_text ltx_font_italic" id="S4.T3.29.29.4.1.1">(67.9%)</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.30.30.5" style="padding-left:4.0pt;padding-right:4.0pt;">28.08<sub class="ltx_sub" id="S4.T3.30.30.5.1"><span class="ltx_text ltx_font_italic" id="S4.T3.30.30.5.1.1">(63.5%)</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.31.31.6" style="padding-left:4.0pt;padding-right:4.0pt;">57.76<sub class="ltx_sub" id="S4.T3.31.31.6.1"><span class="ltx_text ltx_font_italic" id="S4.T3.31.31.6.1.1">(57.4%)</span></sub>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.31.31.8" style="padding-left:4.0pt;padding-right:4.0pt;">48.50</td>
</tr>
<tr class="ltx_tr" id="S4.T3.37.37">
<td class="ltx_td ltx_align_left" id="S4.T3.37.37.7" style="padding-left:4.0pt;padding-right:4.0pt;">UAR</td>
<td class="ltx_td ltx_align_center" id="S4.T3.32.32.1" style="padding-left:4.0pt;padding-right:4.0pt;">52.55<sub class="ltx_sub" id="S4.T3.32.32.1.1"><span class="ltx_text ltx_font_italic" id="S4.T3.32.32.1.1.1">(49.7%)</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.33.33.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.33.33.2.1">26.91<sub class="ltx_sub" id="S4.T3.33.33.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T3.33.33.2.1.1.1">(0.1%)</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.34.34.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.34.34.3.1">69.02<sub class="ltx_sub" id="S4.T3.34.34.3.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T3.34.34.3.1.1.1">(50.1%)</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.35.35.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.35.35.4.1">60.53<sub class="ltx_sub" id="S4.T3.35.35.4.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T3.35.35.4.1.1.1">(25.0%)</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.36.36.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.36.36.5.1">34.46<sub class="ltx_sub" id="S4.T3.36.36.5.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T3.36.36.5.1.1.1">(99.7%)</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.37.37.6" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.37.37.6.1">59.74<sub class="ltx_sub" id="S4.T3.37.37.6.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T3.37.37.6.1.1.1">(78.5%)</span></sub></span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.37.37.8" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.37.37.8.1">50.49</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.74.78">
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="8" id="S4.T3.74.78.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_italic" id="S4.T3.74.78.1.1">13B Models</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.43.43">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T3.43.43.7" style="padding-left:4.0pt;padding-right:4.0pt;">Never-Ret</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.38.38.1" style="padding-left:4.0pt;padding-right:4.0pt;">58.76<sub class="ltx_sub" id="S4.T3.38.38.1.1"><span class="ltx_text ltx_font_italic" id="S4.T3.38.38.1.1.1">(0%)</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.39.39.2" style="padding-left:4.0pt;padding-right:4.0pt;">40.64<sub class="ltx_sub" id="S4.T3.39.39.2.1"><span class="ltx_text ltx_font_italic" id="S4.T3.39.39.2.1.1">(0%)</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.40.40.3" style="padding-left:4.0pt;padding-right:4.0pt;">63.18<sub class="ltx_sub" id="S4.T3.40.40.3.1"><span class="ltx_text ltx_font_italic" id="S4.T3.40.40.3.1.1">(0%)</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.41.41.4" style="padding-left:4.0pt;padding-right:4.0pt;">57.63<sub class="ltx_sub" id="S4.T3.41.41.4.1"><span class="ltx_text ltx_font_italic" id="S4.T3.41.41.4.1.1">(0%)</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.42.42.5" style="padding-left:4.0pt;padding-right:4.0pt;">11.14<sub class="ltx_sub" id="S4.T3.42.42.5.1"><span class="ltx_text ltx_font_italic" id="S4.T3.42.42.5.1.1">(0%)</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.43.43.6" style="padding-left:4.0pt;padding-right:4.0pt;">34.98 <sub class="ltx_sub" id="S4.T3.43.43.6.1"><span class="ltx_text ltx_font_italic" id="S4.T3.43.43.6.1.1">(0%)</span></sub>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.43.43.8" style="padding-left:4.0pt;padding-right:4.0pt;">44.39</td>
</tr>
<tr class="ltx_tr" id="S4.T3.49.49">
<td class="ltx_td ltx_align_left" id="S4.T3.49.49.7" style="padding-left:4.0pt;padding-right:4.0pt;">Always-Ret</td>
<td class="ltx_td ltx_align_center" id="S4.T3.44.44.1" style="padding-left:4.0pt;padding-right:4.0pt;">54.16<sub class="ltx_sub" id="S4.T3.44.44.1.1"><span class="ltx_text ltx_font_italic" id="S4.T3.44.44.1.1.1">(100%)</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.45.45.2" style="padding-left:4.0pt;padding-right:4.0pt;">37.68<sub class="ltx_sub" id="S4.T3.45.45.2.1"><span class="ltx_text ltx_font_italic" id="S4.T3.45.45.2.1.1">(100%)</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.46.46.3" style="padding-left:4.0pt;padding-right:4.0pt;">71.02<sub class="ltx_sub" id="S4.T3.46.46.3.1"><span class="ltx_text ltx_font_italic" id="S4.T3.46.46.3.1.1">(100%)</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.47.47.4" style="padding-left:4.0pt;padding-right:4.0pt;">54.08<sub class="ltx_sub" id="S4.T3.47.47.4.1"><span class="ltx_text ltx_font_italic" id="S4.T3.47.47.4.1.1">(100%)</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.48.48.5" style="padding-left:4.0pt;padding-right:4.0pt;">34.20<sub class="ltx_sub" id="S4.T3.48.48.5.1"><span class="ltx_text ltx_font_italic" id="S4.T3.48.48.5.1.1">(100%)</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.49.49.6" style="padding-left:4.0pt;padding-right:4.0pt;">62.05<sub class="ltx_sub" id="S4.T3.49.49.6.1"><span class="ltx_text ltx_font_italic" id="S4.T3.49.49.6.1.1">(100%)</span></sub>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.49.49.8" style="padding-left:4.0pt;padding-right:4.0pt;">52.09</td>
</tr>
<tr class="ltx_tr" id="S4.T3.74.79">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="8" id="S4.T3.74.79.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_italic" id="S4.T3.74.79.1.1">Active Retrieval</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.56.56">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.50.50.1" style="padding-left:4.0pt;padding-right:4.0pt;">Self-RAG<sup class="ltx_sup" id="S4.T3.50.50.1.1">â€ </sup>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.51.51.2" style="padding-left:4.0pt;padding-right:4.0pt;">44.68<sub class="ltx_sub" id="S4.T3.51.51.2.1"><span class="ltx_text ltx_font_italic" id="S4.T3.51.51.2.1.1">(0.1%)</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.52.52.3" style="padding-left:4.0pt;padding-right:4.0pt;">21.00<sub class="ltx_sub" id="S4.T3.52.52.3.1"><span class="ltx_text ltx_font_italic" id="S4.T3.52.52.3.1.1">(0.0%)</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.53.53.4" style="padding-left:4.0pt;padding-right:4.0pt;">62.53<sub class="ltx_sub" id="S4.T3.53.53.4.1"><span class="ltx_text ltx_font_italic" id="S4.T3.53.53.4.1.1">(30.0%)</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.54.54.5" style="padding-left:4.0pt;padding-right:4.0pt;">42.37<sub class="ltx_sub" id="S4.T3.54.54.5.1"><span class="ltx_text ltx_font_italic" id="S4.T3.54.54.5.1.1">(51.9%)</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.55.55.6" style="padding-left:4.0pt;padding-right:4.0pt;">15.42<sub class="ltx_sub" id="S4.T3.55.55.6.1"><span class="ltx_text ltx_font_italic" id="S4.T3.55.55.6.1.1">(37.0%)</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.56.56.7" style="padding-left:4.0pt;padding-right:4.0pt;">39.60<sub class="ltx_sub" id="S4.T3.56.56.7.1"><span class="ltx_text ltx_font_italic" id="S4.T3.56.56.7.1.1">(39.3%)</span></sub>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.56.56.8" style="padding-left:4.0pt;padding-right:4.0pt;">37.60</td>
</tr>
<tr class="ltx_tr" id="S4.T3.62.62">
<td class="ltx_td ltx_align_left" id="S4.T3.62.62.7" style="padding-left:4.0pt;padding-right:4.0pt;">SKR</td>
<td class="ltx_td ltx_align_center" id="S4.T3.57.57.1" style="padding-left:4.0pt;padding-right:4.0pt;">56.58<sub class="ltx_sub" id="S4.T3.57.57.1.1"><span class="ltx_text ltx_font_italic" id="S4.T3.57.57.1.1.1">(50.9%)</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.58.58.2" style="padding-left:4.0pt;padding-right:4.0pt;">39.35<sub class="ltx_sub" id="S4.T3.58.58.2.1"><span class="ltx_text ltx_font_italic" id="S4.T3.58.58.2.1.1">(27.6%)</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.59.59.3" style="padding-left:4.0pt;padding-right:4.0pt;">67.21<sub class="ltx_sub" id="S4.T3.59.59.3.1"><span class="ltx_text ltx_font_italic" id="S4.T3.59.59.3.1.1">(49.2%)</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.60.60.4" style="padding-left:4.0pt;padding-right:4.0pt;">56.20<sub class="ltx_sub" id="S4.T3.60.60.4.1"><span class="ltx_text ltx_font_italic" id="S4.T3.60.60.4.1.1">(31.5%)</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.61.61.5" style="padding-left:4.0pt;padding-right:4.0pt;">31.66<sub class="ltx_sub" id="S4.T3.61.61.5.1"><span class="ltx_text ltx_font_italic" id="S4.T3.61.61.5.1.1">(89.2%)</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.62.62.6" style="padding-left:4.0pt;padding-right:4.0pt;">50.17<sub class="ltx_sub" id="S4.T3.62.62.6.1"><span class="ltx_text ltx_font_italic" id="S4.T3.62.62.6.1.1">(45.9%)</span></sub>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.62.62.8" style="padding-left:4.0pt;padding-right:4.0pt;">50.16</td>
</tr>
<tr class="ltx_tr" id="S4.T3.68.68">
<td class="ltx_td ltx_align_left" id="S4.T3.68.68.7" style="padding-left:4.0pt;padding-right:4.0pt;">FLARE</td>
<td class="ltx_td ltx_align_center" id="S4.T3.63.63.1" style="padding-left:4.0pt;padding-right:4.0pt;">58.12<sub class="ltx_sub" id="S4.T3.63.63.1.1"><span class="ltx_text ltx_font_italic" id="S4.T3.63.63.1.1.1">(17.5%)</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.64.64.2" style="padding-left:4.0pt;padding-right:4.0pt;">38.05<sub class="ltx_sub" id="S4.T3.64.64.2.1"><span class="ltx_text ltx_font_italic" id="S4.T3.64.64.2.1.1">(61.2%)</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.65.65.3" style="padding-left:4.0pt;padding-right:4.0pt;">68.00<sub class="ltx_sub" id="S4.T3.65.65.3.1"><span class="ltx_text ltx_font_italic" id="S4.T3.65.65.3.1.1">(54.9%)</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.66.66.4" style="padding-left:4.0pt;padding-right:4.0pt;">53.64<sub class="ltx_sub" id="S4.T3.66.66.4.1"><span class="ltx_text ltx_font_italic" id="S4.T3.66.66.4.1.1">(69.6%)</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.67.67.5" style="padding-left:4.0pt;padding-right:4.0pt;">25.40<sub class="ltx_sub" id="S4.T3.67.67.5.1"><span class="ltx_text ltx_font_italic" id="S4.T3.67.67.5.1.1">(60.9%)</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.68.68.6" style="padding-left:4.0pt;padding-right:4.0pt;">50.17<sub class="ltx_sub" id="S4.T3.68.68.6.1"><span class="ltx_text ltx_font_italic" id="S4.T3.68.68.6.1.1">(55.8%)</span></sub>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.68.68.8" style="padding-left:4.0pt;padding-right:4.0pt;">48.90</td>
</tr>
<tr class="ltx_tr" id="S4.T3.74.74">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T3.74.74.7" style="padding-left:4.0pt;padding-right:4.0pt;">UAR</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.69.69.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.69.69.1.1">58.55<sub class="ltx_sub" id="S4.T3.69.69.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T3.69.69.1.1.1.1">(3.7%)</span></sub></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.70.70.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.70.70.2.1">40.64<sub class="ltx_sub" id="S4.T3.70.70.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T3.70.70.2.1.1.1">(0.0%)</span></sub></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.71.71.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.71.71.3.1">71.71<sub class="ltx_sub" id="S4.T3.71.71.3.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T3.71.71.3.1.1.1">(48.5%)</span></sub></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.72.72.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.72.72.4.1">59.20<sub class="ltx_sub" id="S4.T3.72.72.4.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T3.72.72.4.1.1.1">(31.2%)</span></sub></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.73.73.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.73.73.5.1">34.14<sub class="ltx_sub" id="S4.T3.73.73.5.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T3.73.73.5.1.1.1">(99.6%)</span></sub></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.74.74.6" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text ltx_font_bold" id="S4.T3.74.74.6.1">55.45</span> <sub class="ltx_sub" id="S4.T3.74.74.6.2"><span class="ltx_text ltx_font_italic" id="S4.T3.74.74.6.2.1">(73.3%)</span></sub>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T3.74.74.8" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.74.74.8.1">53.26</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparisons of downstream tasks performance. Never-Ret means that retrieval augmentation is never used during generation, while Always-Ret means that retrieval augmentation is used in every generation.<math alttext="\dagger" class="ltx_Math" display="inline" id="S4.T3.76.m1.1"><semantics id="S4.T3.76.m1.1b"><mo id="S4.T3.76.m1.1.1" xref="S4.T3.76.m1.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="S4.T3.76.m1.1c"><ci id="S4.T3.76.m1.1.1.cmml" xref="S4.T3.76.m1.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.76.m1.1d">\dagger</annotation><annotation encoding="application/x-llamapun" id="S4.T3.76.m1.1e">â€ </annotation></semantics></math>: Self-RAG is fine-tuned from Llama2-base models. Other methods are based on Llama2-chat models.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Evaluation Metrics</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">Following previous work <cite class="ltx_cite ltx_citemacro_citep">(Asai etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib3" title="">2023</a>; Mallen etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib28" title="">2023</a>; Schick etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib31" title="">2023</a>)</cite>, we check whether gold answers are included in modelâ€™s generations to evaluate performance on the DROP, TriviaQA, and WQ datasets, instead of strictly requiring exact matching.
For GSM8K, we use the prompts for answer extraction in <cite class="ltx_cite ltx_citemacro_citet">Kojima etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib24" title="">2022</a>)</cite> to extract modelâ€™s answers and then use exact matching to calculate the accuracy.
For TAQA and FreshQA, since the golden answers are too long to conduct lexical matching, we use ChatGPT to evaluate whether the modelâ€™s answers are correct.
Details of ChatGPT evaluation are included in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#A3" title="Appendix C ChatGPT Evaluation â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">C</span></a>.
Furthermore, for downstream tasks, we also report the percentage of samples that invoke retrieval.
For AR-Bench, we use accuracy as the metric.
Since AR-Bench is a binary classification task with an equal number of positive and negative samples, accuracy and micro F1 score are equivalent.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Comparisons on AR-Bench</h3>
<div class="ltx_para" id="S4.SS6.p1">
<p class="ltx_p" id="S4.SS6.p1.1">We show the results in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S4.T2" title="Table 2 â€£ 4.1 Benchmarking Retrieval Timing â€£ 4 Experiments â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">2</span></a>.
We observe that UAR outperforms existing active retrieval methods across all AR-Bench scenarios, demonstrating its versatility and effectiveness.
Since baseline methods depend on a single criterion, they struggle with various active retrieval scenarios, which demonstrates the limitation of single criterion and the necessity of multifaceted decision for active retrieval.
Additionally, we find FLARE struggle with self-aware scenario, which it is targeted at.
We think it is because its uncertainty estimation heavily depends on model calibration and this leads to its poor performance on less calibrated models like chat modelsÂ <cite class="ltx_cite ltx_citemacro_citep">(He etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib19" title="">2023</a>)</cite> or those with fewer parameters.
Self-RAG uses the knowledge-intensive nature of tasks as the retrieval criterion, performing well in knowledge-aware scenarios but poorly in others.
SKR bases retrieval on the modelâ€™s knowledge of an answer, excelling in self-aware and time-aware scenarios but failing in others.
Additionally, since SKR uses BERT as the classifier,
whose internal knowledge has a significant gap with Llama,
it underperforms UAR with value heads based on the Llamaâ€™s representation, in the self-aware scenario.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.7 </span>Comparisons on Downstream Tasks</h3>
<div class="ltx_para" id="S4.SS7.p1">
<p class="ltx_p" id="S4.SS7.p1.1">For Self-RAG, we use inference scripts provided by the authors.
For FLARE, SKR, UAR, and always-retrieval methods, we use the same prompts to generate responses by incorporating the retrieved information.
We introduce the details of generation in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#A4" title="Appendix D Details of Generation â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">D</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS7.p2">
<p class="ltx_p" id="S4.SS7.p2.1">The results are shown in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S4.T3" title="Table 3 â€£ 4.4 Retrievers â€£ 4 Experiments â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">3</span></a>.
We see that UAR leads to the best overall performance across different downstream task scenarios, which indicates its effectiveness.
The percentage inside the parentheses represents the proportion of retrieval-invoked samples to the total samples.
We analyze each scenario as follows.</p>
</div>
<section class="ltx_paragraph" id="S4.SS7.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">UAR does not invoke retrieval when factual knowledge is not needed.</h4>
<div class="ltx_para" id="S4.SS7.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS7.SSS0.Px1.p1.1">The DROP and GSM8K dataset do not require fact knowledge, and using retrieval enhancement will interfere with the model.
The results of always-retrieval are worse than never-retrieval.
UAR only invokes a small amount of retrieval, while SKR and FLARE incorrectly invoke retrieval extensively.
And since UAR avoid unnecessary retrieval<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>UAR based on the 7B model incorrectly invokes retrieval 50% of the time on the DROP dataset.
We speculate that this may be due to the limited representation capacity of the 7B modelâ€™s hidden states.
In contrast, the 13B model only incorrectly invokes retrieval 3.7% of the time.</span></span></span> and thus prevents affecting the original capabilities of the LLM,
it achieves the best results among all active retrieval methods on DROP and GSM8K, coming close to the results of never-retrieval.
Although Self-RAG does not incorrectly invoke retrieval, its final performance is not very good because it is fine-tuned based on the base model rather than leveraging the capabilities of the chat model.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS7.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">UAR accurately invokes retrieval for time-sensitive questions.</h4>
<div class="ltx_para" id="S4.SS7.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS7.SSS0.Px2.p1.1">Since the questions in TAQA and FreshQA are time-sensitive and their answers keep changing, each question requires the retrieval of the latest information.
It is evident that the always-retrieval method based on Google Search performs significantly better than the never-retrieval method.
For TAQA, UAR almost perfectly invokes retrieval.
For FreshQA, UAR also invokes retrieval for most of the questions.
In contrast, other methods invoke retrieval less frequently and therefore do not use the latest information for responses, resulting in lower accuracy compared to UAR.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS7.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">UAR accurately assesses the modelâ€™s knowledge, avoiding poor retrieval impacts.</h4>
<div class="ltx_para" id="S4.SS7.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS7.SSS0.Px3.p1.1">For questions in TriviaQA and WQ whose answers do not change over time, always-retrieval is sub-optimal and the reason is two-fold: 1. For questions which model knows, retrieval increases unnecessary latency. 2. Potential incorrect external information will interfere correct internal knowledge.
Retrieving information only for knowledge that the model does not know can mitigate this issue.
Compared to SKR, UAR can more accurately determine whether the model knows a particular piece of knowledge.
Although SKR and UAR use a comparable number of retrieval calls, the accuracy of SKRâ€™s answers is lower than that of UAR, indicating that SKRâ€™s retrieval calls are less precise than UARâ€™s.
We believe this is because SKR uses independent models, whereas our approach uses hidden states of the original model, resulting in better generalization.
Moreover, UAR outperforms always-retrieval with fewer retrieval calls, demonstrating the superiority of the Active Retrieval method.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Analysis</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Single Classifiers vs UAR</h3>
<figure class="ltx_table" id="S5.T4">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T4.1">
<tr class="ltx_tr" id="S5.T4.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T4.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.1">Scenario</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T4.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.2.1">Single Classifier</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T4.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.3.1">UAR</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.1.2.1">Intent-aware</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.2.2">98.29</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.2.3">91.88</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.3">
<td class="ltx_td ltx_align_left" id="S5.T4.1.3.1">Knowledge-aware</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.3.2">99.66</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.3.3">90.38</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.4">
<td class="ltx_td ltx_align_left" id="S5.T4.1.4.1">Time-aware</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.4.2">99.41</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.4.3">86.69</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.5">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T4.1.5.1">Self-aware</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.1.5.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.1.5.2.1">72.56</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.1.5.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.1.5.3.1">72.32</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Comparison between single classifiers and UAR based on Llama2-7B-chat.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Different scenarios have varying levels of discrimination difficulty.
As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S5.T4" title="Table 4 â€£ 5.1 Single Classifiers vs UAR â€£ 5 Analysis â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">4</span></a>, the single classifier for the self-aware scenario has the lowest accuracy, which implies that determining whether the model is self-aware is a relatively challenging task.
We can also observe that the accuracy of each single classifier is higher than UAR in their respective scenarios.
The self-aware classifier may become the bottleneck restricting the performance of UAR, which also results in the accuracy of UAR on the AR-Bench being lower than the accuracy of using a single classifier alone.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Using the Whole LLM as Classifier</h3>
<figure class="ltx_table" id="S5.T5">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T5.1">
<tr class="ltx_tr" id="S5.T5.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T5.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.1">Self-aware</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.2.1">Only Value Head</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.3.1">Whole LLM</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.1.2.1">Llama2-7B-chat</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.2.2">72.56</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.2.3">75.65</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.3">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T5.1.3.1">Llama2-13B-chat</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.3.2">73.48</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.3.3">76.28</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Comparison of the performance between training a value head as the classifier and training a entire large language model as the classifier.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">To improve the performance bottleneck of the self-aware classifier, we attempt to fine-tune the entire large language model as the classifier.
From the results in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S5.T5" title="Table 5 â€£ 5.2 Using the Whole LLM as Classifier â€£ 5 Analysis â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">5</span></a>, we can observe that on both 7B and 13B models, fine-tuning the entire model only achieves slight higher accuracy compared to just fine-tuning a lightweight value head.
Using a whole LLM as the classifier, UARâ€™s inference latency and required parameters will significantly increase.
Therefore, we use lightweight value heads as classifiers, ensuring the efficiency of the entire framework with minimal performance loss.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>The Impact of Document Number</h3>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="553" id="S5.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The impact of the number of reference documents on model performance.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">We evaluate performance on the TriviaQA (TQ) and WebQuestions (WQ) datasets by varying the number of reference documents from 1 to 10.
The results, shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S5.F3" title="Figure 3 â€£ 5.3 The Impact of Document Number â€£ 5 Analysis â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">3</span></a>, indicate that on the WQ dataset, the always-retrieval method performs worse than the never-retrieval method, possibly because some documents disrupt the correct knowledge within the model.
UAR reduces retrieval frequency, enabling more precise retrieval calls and outperforming the never-retrieval method.
On the TQ dataset, always-retrieval outperforms never-retrieval, and performance improves with more documents, suggesting useful information might be in lower-ranked documents.
UAR performs best with fewer documents.
With more documents, it matches the performance of always-retrieval, although it requires significantly fewer retrieval calls.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we introduce UAR, a unified active retrieval framework for retrieval-augmented generation.
Unlike existing methods that rely on a single criterion, UAR incorporates four orthogonal criteria into plug-and-play classification tasks, enabling comprehensive retrieval timing judgments with minimal inference cost and no loss of model capabilities.
We also introduce UAR-Criteria for processing various active retrieval scenarios uniformly.
We curate the Active Retrieval Benchmark (AR-Bench) to assess the retrieval timing accuracy of active retrieval methods across different scenarios.
Experimental results demonstrate that UAR significantly outperforms existing methods on AR-Bench and downstream tasks, highlighting its effectiveness and benefits to downstream applications.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitations</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">We summarize limitations of our work as follows:</p>
<ul class="ltx_itemize" id="Sx1.I1">
<li class="ltx_item" id="Sx1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="Sx1.I1.i1.p1">
<p class="ltx_p" id="Sx1.I1.i1.p1.1">Our experiments primarily focus on the generation of short texts, such as in knowledge-based question answering, and involve only a single retrieval call.
How to implement multiple active retrieval calls within longer text responses remains an area for future investigation.</p>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="Sx1.I1.i2.p1">
<p class="ltx_p" id="Sx1.I1.i2.p1.1">Our active retrieval criteria are primarily derived from our experience in practical applications, which may overlook some active retrieval scenarios.</p>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="Sx1.I1.i3.p1">
<p class="ltx_p" id="Sx1.I1.i3.p1.1">Our classifier is based on a single-layer MLP network.
Whether using a deeper network can further enhance performance remains to be explored.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Acknowledgement</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">This work was supported by the National Natural Science Foundation of China (No. 62236004). The computations in this research were performed using the CFFF platform of Fudan University.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anil etÂ al. (2023)</span>
<span class="ltx_bibblock">
Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, AndrewÂ M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, TimothyÂ P. Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, PaulÂ Ronald Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, AnaÃ¯s White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, and etÂ al. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2312.11805" title="">Gemini: A family of highly capable multimodal models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">CoRR</em>, abs/2312.11805.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anthropic (2023)</span>
<span class="ltx_bibblock">
Anthropic. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://www.anthropic.com/index/introducing-claude" title="">Introducing claude</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asai etÂ al. (2023)</span>
<span class="ltx_bibblock">
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2310.11511" title="">Self-rag: Learning to retrieve, generate, and critique through self-reflection</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">CoRR</em>, abs/2310.11511.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai etÂ al. (2023)</span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, YuÂ Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, AnÂ Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2309.16609" title="">Qwen technical report</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">CoRR</em>, abs/2309.16609.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Berant etÂ al. (2013)</span>
<span class="ltx_bibblock">
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/D13-1160/" title="">Semantic parsing on freebase from question-answer pairs</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA, A meeting of SIGDAT, a Special Interest Group of the ACL</em>, pages 1533â€“1544. ACL.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown etÂ al. (2020)</span>
<span class="ltx_bibblock">
TomÂ B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, DanielÂ M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html" title="">Language models are few-shot learners</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai etÂ al. (2024)</span>
<span class="ltx_bibblock">
Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, QiÂ Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, LiÂ Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, YuÂ Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, and etÂ al. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2403.17297" title="">Internlm2 technical report</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">CoRR</em>, abs/2403.17297.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2021)</span>
<span class="ltx_bibblock">
Wenhu Chen, Xinyi Wang, and WilliamÂ Yang Wang. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/1f0e3dad99908345f7439f8ffabdffc4-Abstract-round2.html" title="">A dataset for answering time-sensitive questions</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng etÂ al. (2024)</span>
<span class="ltx_bibblock">
Qinyuan Cheng, Tianxiang Sun, Xiangyang Liu, Wenwei Zhang, Zhangyue Yin, Shimin Li, Linyang Li, Zhengfu He, Kai Chen, and Xipeng Qiu. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2401.13275" title="">Can AI assistants know what they donâ€™t know?</a>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">CoRR</em>, abs/2401.13275.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng etÂ al. (2023)</span>
<span class="ltx_bibblock">
Qinyuan Cheng, Tianxiang Sun, Wenwei Zhang, Siyin Wang, Xiangyang Liu, Mozhi Zhang, Junliang He, Mianqiu Huang, Zhangyue Yin, Kai Chen, and Xipeng Qiu. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2310.03368" title="">Evaluating hallucinations in chinese large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">CoRR</em>, abs/2310.03368.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Wei-Lin Chiang, Zhuohan Li, ZiÂ Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, JosephÂ E. Gonzalez, Ion Stoica, and EricÂ P. Xing. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://lmsys.org/blog/2023-03-30-vicuna/" title="">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cobbe etÂ al. (2021)</span>
<span class="ltx_bibblock">
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2110.14168" title="">Training verifiers to solve math word problems</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">CoRR</em>, abs/2110.14168.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin etÂ al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/N19-1423" title="">BERT: pre-training of deep bidirectional transformers for language understanding</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)</em>, pages 4171â€“4186. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding etÂ al. (2024)</span>
<span class="ltx_bibblock">
Hanxing Ding, Liang Pang, Zihao Wei, Huawei Shen, and Xueqi Cheng. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2402.10612" title="">Retrieve only when it needs: Adaptive retrieval augmentation for hallucination mitigation in large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Preprint</em>, arXiv:2402.10612.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dua etÂ al. (2019)</span>
<span class="ltx_bibblock">
Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/N19-1246" title="">DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)</em>, pages 2368â€“2378. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fierro etÂ al. (2024)</span>
<span class="ltx_bibblock">
Constanza Fierro, Nicolas Garneau, Emanuele Bugliarello, Yova Kementchedjhieva, and Anders SÃ¸gaard. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2404.03036" title="">Mulan: A study of fact mutability in language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">CoRR</em>, abs/2404.03036.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al. (2024)</span>
<span class="ltx_bibblock">
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, YiÂ Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2312.10997" title="">Retrieval-augmented generation for large language models: A survey</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Preprint</em>, arXiv:2312.10997.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guu etÂ al. (2020)</span>
<span class="ltx_bibblock">
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2002.08909" title="">Realm: Retrieval-augmented language model pre-training</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Preprint</em>, arXiv:2002.08909.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He etÂ al. (2023)</span>
<span class="ltx_bibblock">
Guande He, Peng Cui, Jianfei Chen, Wenbo Hu, and Jun Zhu. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2310.11732" title="">Investigating uncertainty calibration of aligned language models under the multiple-choice setting</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">CoRR</em>, abs/2310.11732.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard etÂ al. (2022)</span>
<span class="ltx_bibblock">
Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=jKN1pXi7b0" title="">Unsupervised dense information retrieval with contrastive learning</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Trans. Mach. Learn. Res.</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Zhengbao Jiang, FrankÂ F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2023.EMNLP-MAIN.495" title="">Active retrieval augmented generation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023</em>, pages 7969â€“7992. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi etÂ al. (2017)</span>
<span class="ltx_bibblock">
Mandar Joshi, Eunsol Choi, DanielÂ S. Weld, and Luke Zettlemoyer. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/P17-1147" title="">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers</em>, pages 1601â€“1611. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kadavath etÂ al. (2022)</span>
<span class="ltx_bibblock">
Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, SheerÂ El Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2207.05221" title="">Language models (mostly) know what they know</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">CoRR</em>, abs/2207.05221.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kojima etÂ al. (2022)</span>
<span class="ltx_bibblock">
Takeshi Kojima, ShixiangÂ Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://papers.nips.cc/paper_files/paper/2022/hash/8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html" title="">Large language models are zero-shot reasoners</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al. (2022a)</span>
<span class="ltx_bibblock">
Stephanie Lin, Jacob Hilton, and Owain Evans. 2022a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=8s8K2UZGTZ" title="">Teaching models to express their uncertainty in words</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Trans. Mach. Learn. Res.</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al. (2022b)</span>
<span class="ltx_bibblock">
Stephanie Lin, Jacob Hilton, and Owain Evans. 2022b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2022.ACL-LONG.229" title="">Truthfulqa: Measuring how models mimic human falsehoods</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022</em>, pages 3214â€“3252. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2024)</span>
<span class="ltx_bibblock">
Yanming Liu, Xinyue Peng, Xuhong Zhang, Weihao Liu, Jianwei Yin, Jiannan Cao, and Tianyu Du. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2403.06840" title="">RA-ISF: learning to answer and understand from retrieval augmentation via iterative self-feedback</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">CoRR</em>, abs/2403.06840.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mallen etÂ al. (2023)</span>
<span class="ltx_bibblock">
Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2023.ACL-LONG.546" title="">When not to trust language models: Investigating effectiveness of parametric and non-parametric memories</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023</em>, pages 9802â€“9822. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2022)</span>
<span class="ltx_bibblock">
OpenAI. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openai.com/blog/chatgpt" title="">Introducing chatgpt</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:257532815" title="">Gpt-4 technical report</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick etÂ al. (2023)</span>
<span class="ltx_bibblock">
Timo Schick, Jane Dwivedi-Yu, Roberto DessÃ¬, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://papers.nips.cc/paper_files/paper/2023/hash/d842425e4bf79ba039352da0f658a906-Abstract-Conference.html" title="">Toolformer: Language models can teach themselves to use tools</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao etÂ al. (2023)</span>
<span class="ltx_bibblock">
Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2023.FINDINGS-EMNLP.620" title="">Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023</em>, pages 9248â€“9274. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi etÂ al. (2023)</span>
<span class="ltx_bibblock">
Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, EdÂ H. Chi, Nathanael SchÃ¤rli, and Denny Zhou. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.mlr.press/v202/shi23a.html" title="">Large language models can be easily distracted by irrelevant context</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA</em>, volume 202 of <em class="ltx_emph ltx_font_italic" id="bib.bib33.2.2">Proceedings of Machine Learning Research</em>, pages 31210â€“31227. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun etÂ al. (2024)</span>
<span class="ltx_bibblock">
Tianxiang Sun, Xiaotian Zhang, Zhengfu He, Peng Li, Qinyuan Cheng, Xiangyang Liu, Hang Yan, Yunfan Shao, Qiong Tang, Shiduo Zhang, Xingjian Zhao, KeÂ Chen, Yining Zheng, Zhejian Zhou, Ruixiao Li, Jun Zhan, Yunhua Zhou, Linyang Li, Xiaogui Yang, Lingling Wu, Zhangyue Yin, Xuanjing Huang, Yu-Gang Jiang, and Xipeng Qiu. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1007/s11633-024-1502-8" title="">Moss: An open conversational large language model</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Machine Intelligence Research</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taori etÂ al. (2023)</span>
<span class="ltx_bibblock">
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and TatsunoriÂ B. Hashimoto. 2023.

</span>
<span class="ltx_bibblock">Stanford alpaca: An instruction-following llama model.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/tatsu-lab/stanford_alpaca" title="">https://github.com/tatsu-lab/stanford_alpaca</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron etÂ al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, PunitÂ Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, EricÂ Michael Smith, Ranjan Subramanian, XiaoqingÂ Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, JianÂ Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, AurÃ©lien Rodriguez, Robert Stojnic, Sergey Edunov,
and Thomas Scialom. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2307.09288" title="">Llama 2: Open foundation and fine-tuned chat models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">CoRR</em>, abs/2307.09288.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vu etÂ al. (2023)</span>
<span class="ltx_bibblock">
TuÂ Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, JerryÂ W. Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, QuocÂ V. Le, and Thang Luong. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2310.03214" title="">Freshllms: Refreshing large language models with search engine augmentation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">CoRR</em>, abs/2310.03214.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xiangru Tang, Tianhang Zhang, Jiayang Cheng, Yunzhi Yao, Wenyang Gao, Xuming Hu, Zehan Qi, Yidong Wang, Linyi Yang, Jindong Wang, Xing Xie, Zheng Zhang, and Yue Zhang. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2310.07521" title="">Survey on factuality in large language models: Knowledge, retrieval and domain-specificity</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">CoRR</em>, abs/2310.07521.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2024)</span>
<span class="ltx_bibblock">
Hongru Wang, Boyang Xue, Baohang Zhou, Tianhua Zhang, Cunxiang Wang, Guanhua Chen, Huimin Wang, and Kam-Fai Wong. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2402.13514" title="">Self-dc: When to retrieve and when to generate? self divide-and-conquer for compositional unknown questions</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">CoRR</em>, abs/2402.13514.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Yile Wang, Peng Li, Maosong Sun, and Yang Liu. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2023.FINDINGS-EMNLP.691" title="">Self-knowledge guided retrieval augmentation for large language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023</em>, pages 10303â€“10315. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2023c)</span>
<span class="ltx_bibblock">
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, NoahÂ A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023c.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2023.ACL-LONG.754" title="">Self-instruct: Aligning language models with self-generated instructions</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023</em>, pages 13484â€“13508. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, CeÂ Bian, Chao Yin, Chenxu Lv, DaÂ Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, Juntao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, and Zhiying Wu. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2309.10305" title="">Baichuan 2: Open large-scale language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">CoRR</em>, abs/2309.10305.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al. (2024)</span>
<span class="ltx_bibblock">
Zhaorui Yang, Qian Liu, Tianyu Pang, Han Wang, Haozhe Feng, Minfeng Zhu, and Wei Chen. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2402.13669" title="">Self-distillation bridges distribution gap in language model fine-tuning</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">CoRR</em>, abs/2402.13669.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin etÂ al. (2023)</span>
<span class="ltx_bibblock">
Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, and Xuanjing Huang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2023.FINDINGS-ACL.551" title="">Do large language models know what they donâ€™t know?</a>
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023</em>, pages 8653â€“8665. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yoran etÂ al. (2023)</span>
<span class="ltx_bibblock">
Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2310.01558" title="">Making retrieval-augmented language models robust to irrelevant context</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">CoRR</em>, abs/2310.01558.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng etÂ al. (2023)</span>
<span class="ltx_bibblock">
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, WengÂ Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie Tang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/pdf?id=-Aw0rrrPUF" title="">GLM-130B: an open bilingual pre-trained model</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023</em>. OpenReview.net.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Hanning Zhang, Shizhe Diao, Yong Lin, YiÂ R. Fung, Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji, and Tong Zhang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2311.09677" title="">R-tuning: Teaching large language models to refuse unknown questions</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">CoRR</em>, abs/2311.09677.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao etÂ al. (2024)</span>
<span class="ltx_bibblock">
Bowen Zhao, Zander Brumbaugh, Yizhong Wang, Hannaneh Hajishirzi, and NoahÂ A. Smith. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2402.16797" title="">Set the clock: Temporal alignment of pretrained language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">CoRR</em>, abs/2402.16797.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Details of AR-Bench Construction</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">For the self-aware task, we employ the same method as described in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#S3.SS1" title="3.1 UAR Classifiers Training â€£ 3 Methodology â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">3.1</span></a> to construct test samples on the TriviaQA validation set.
Questions the model does not know are marked as requiring retrieval.
The test set comprise 4000 questions the model knows and 4000 questions it does not.</p>
</div>
<div class="ltx_para" id="A1.p2">
<p class="ltx_p" id="A1.p2.1">For the time-aware task, we use 4000 time-sensitive questions from the TAQA test set as inputs requiring retrieval, and 4000 questions the model knows from the TriviaQA validation set as inputs not requiring retrieval.</p>
</div>
<div class="ltx_para" id="A1.p3">
<p class="ltx_p" id="A1.p3.1">For the knowledge-aware task, we use 4000 samples from the Self-RAG non-retrieval training data as inputs not requiring retrieval, and combine 2000 time-sensitive questions from the TAQA test set with 2000 questions the model does not know from the TriviaQA validation set as inputs requiring retrieval.</p>
</div>
<div class="ltx_para" id="A1.p4">
<p class="ltx_p" id="A1.p4.1">For the intent-aware task, we use 4000 questions the model knows from the TriviaQA validation set and 4000 instructions from the Self-RAG non-retrieval training data, half of which are concatenated with user retrieval intents as inputs requiring retrieval, and the other half as inputs not requiring retrieval.</p>
</div>
<div class="ltx_para" id="A1.p5">
<p class="ltx_p" id="A1.p5.1">It is important to note that the self-aware data for different models may vary, leading to different AR-Benches for different models.
In our experiments, we curate two separate AR-Benches for Llama2-7B-chat and Llama2-13B-chat respectively.</p>
</div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Details of Baselines Re-implementation</h2>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>FLARE</h3>
<div class="ltx_para" id="A2.SS1.p1">
<p class="ltx_p" id="A2.SS1.p1.1">In implementing FLARE, we make two modifications.
First, we conduct experiments based on the Llama2-chat series of models, rather than using text-davinci-003.
Second, we eliminate the initial retrieval step in FLARE since our setting is active retrieval rather than passive retrieval.
We find that FLARE based on Llama2 struggle to achieve satisfactory results, which we suspect may be due to poor calibration of the Llama2-7B-chat and Llama2-13B-chat models.
The uncertainty estimation in FLARE heavily relies on model calibration, making it challenging to adapt to poorly calibrated models.
Therefore, on the AR-Bench, we conduct a direct search for the best retrieval thresholds for FLARE, ultimately setting them at 0.006 and 0.02 for the Llama2-7B-chat and Llama2-13B-chat models, respectively.</p>
</div>
</section>
<section class="ltx_subsection" id="A2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>SKR</h3>
<figure class="ltx_table" id="A2.T6">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A2.T6.1">
<tr class="ltx_tr" id="A2.T6.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="A2.T6.1.1.1">Training Hyper-parameters</td>
</tr>
<tr class="ltx_tr" id="A2.T6.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="A2.T6.1.2.1">Optimizer</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="A2.T6.1.2.2">AdamW</td>
</tr>
<tr class="ltx_tr" id="A2.T6.1.3">
<td class="ltx_td ltx_align_left" id="A2.T6.1.3.1">Warmup Steps</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A2.T6.1.3.2">0</td>
</tr>
<tr class="ltx_tr" id="A2.T6.1.4">
<td class="ltx_td ltx_align_left" id="A2.T6.1.4.1">Learning Rate</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A2.T6.1.4.2">2e-5</td>
</tr>
<tr class="ltx_tr" id="A2.T6.1.5">
<td class="ltx_td ltx_align_left" id="A2.T6.1.5.1">Batch Size</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A2.T6.1.5.2">32</td>
</tr>
<tr class="ltx_tr" id="A2.T6.1.6">
<td class="ltx_td ltx_align_left" id="A2.T6.1.6.1">Train Epochs</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A2.T6.1.6.2">5</td>
</tr>
<tr class="ltx_tr" id="A2.T6.1.7">
<td class="ltx_td ltx_align_left" id="A2.T6.1.7.1">LR Scheduler</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A2.T6.1.7.2">Linear</td>
</tr>
<tr class="ltx_tr" id="A2.T6.1.8">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A2.T6.1.8.1">Max-seq-length</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="A2.T6.1.8.2">512</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Training hyper-parameters of SKR.</figcaption>
</figure>
<div class="ltx_para" id="A2.SS2.p1">
<p class="ltx_p" id="A2.SS2.p1.1">In implementing SKR, we first use the 849 original pieces of data provided by the authors of SKR and collect self-knowledge data for the Llama2-7B-chat model according to the scripts in SKRâ€™s code repository.
We obtain 15 questions that the model does not know and 143 questions that it knows, and find that these data are not sufficient to train an effective BERT classifier.
Therefore, we use the data from our training data of the self-aware classifier to train the BERT classifier for SKR.
Our training hyper-parameters are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#A2.T6" title="Table 6 â€£ B.2 SKR â€£ Appendix B Details of Baselines Re-implementation â€£ Unified Active Retrieval for Retrieval Augmented Generation"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>ChatGPT Evaluation</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">We use gpt-3.5-turbo-instruct as the evaluator.
During the evaluation, we input the correct answer and the answer to be evaluated into gpt-3.5, and then let the model compare the correct answer with the answer to be evaluated to determine if the latter is correct.
Following <cite class="ltx_cite ltx_citemacro_citet">Shao etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib32" title="">2023</a>)</cite>, we use the following prompt for evaluation.</p>
<div class="ltx_listing ltx_lstlisting ltx_framed ltx_framed_rectangle ltx_listing" id="A3.p1.2">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,SW4gdGhlIGZvbGxvd2luZyB0YXNrLCB5b3UgYXJlIGdpdmVuIGEgUXVlc3Rpb24sIGEgbW9kZWwgUHJlZGljdGlvbiBmb3IgdGhlIFF1ZXN0aW9uLCBhbmQgYSBHcm91bmQtdHJ1dGggQW5zd2VyIHRvIHRoZSBRdWVzdGlvbi4gWW91IHNob3VsZCBkZWNpZGUgd2hldGhlciB0aGUgbW9kZWwgUHJlZGljdGlvbiBpbXBsaWVzIHRoZSBHcm91bmQtdHJ1dGggQW5zd2VyLgoKUXVlc3Rpb246CntxdWVzdGlvbn0KClByZWRpY3Rpb246CntwcmVkaWN0ZWQgYW5zd2VyfQoKR3JvdW5kLXRydXRoIEFuc3dlcjoKe2dyb3VuZC10cnV0aCBhbnN3ZXJ9CkRvZXMgdGhlIFByZWRpY3Rpb24gaW1wbHkgdGhlIEdyb3VuZC10cnV0aCBBbnN3ZXI/IE91dHB1dCBZZXMgb3IgTm86">â¬‡</a></div>
<div class="ltx_listingline" id="lstnumberx6">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.1">In</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.3">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.5">following</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.7">task</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.8">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.9"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.10">you</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.11"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.12">are</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.13"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.14">given</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.15"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.16">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.17"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.18">Question</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.19">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.20"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.21">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.22"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.23">model</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.24"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.25">Prediction</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.26"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.27">for</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.28"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.29">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.30"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.31">Question</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.32">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.33"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.34">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.35"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.36">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.37"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.38">Ground</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.39">-</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.40">truth</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.41"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.42">Answer</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.43"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.44">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.45"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.46">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.47"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.48">Question</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.49">.</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.50"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.51">You</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.52"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.53">should</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.54"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.55">decide</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.56"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.57">whether</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.58"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.59">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.60"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.61">model</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.62"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.63">Prediction</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.64"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.65">implies</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.66"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.67">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.68"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.69">Ground</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.70">-</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.71">truth</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.72"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.73">Answer</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.74">.</span>
</div>
<div class="ltx_listingline" id="lstnumberx7">
</div>
<div class="ltx_listingline" id="lstnumberx8">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.1">Question</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.2">:</span>
</div>
<div class="ltx_listingline" id="lstnumberx9">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx9.1">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.2">question</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.3">}</span>
</div>
<div class="ltx_listingline" id="lstnumberx10">
</div>
<div class="ltx_listingline" id="lstnumberx11">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.1">Prediction</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.2">:</span>
</div>
<div class="ltx_listingline" id="lstnumberx12">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx12.1">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.2">predicted</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.3"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.4">answer</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.5">}</span>
</div>
<div class="ltx_listingline" id="lstnumberx13">
</div>
<div class="ltx_listingline" id="lstnumberx14">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.1">Ground</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.2">-</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.3">truth</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.5">Answer</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.6">:</span>
</div>
<div class="ltx_listingline" id="lstnumberx15">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx15.1">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.2">ground</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.3">-</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.4">truth</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.5"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.6">answer</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.7">}</span>
</div>
<div class="ltx_listingline" id="lstnumberx16">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.1">Does</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.3">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.5">Prediction</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.7">imply</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.9">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.11">Ground</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.12">-</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.13">truth</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.15">Answer</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.16">?</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.17"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.18">Output</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.19"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.20">Yes</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.21"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.22">or</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.23"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.24">No</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.25">:</span>
</div>
</div>
</div>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Details of Generation</h2>
<section class="ltx_subsection" id="A4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.1 </span>Self-RAG</h3>
<div class="ltx_para" id="A4.SS1.p1">
<p class="ltx_p" id="A4.SS1.p1.1">We use the inference script provided by the Self-RAG authors for generation.
We determine the need for retrieval by whether the retrieval special token appears in the generated response.
For datasets using Contriever-MS MARCO as the retriever, we provide all 10 documents retrieved to Self-RAG for generation.</p>
</div>
</section>
<section class="ltx_subsection" id="A4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.2 </span>Generation without Retrieval</h3>
<div class="ltx_para" id="A4.SS2.p1">
<p class="ltx_p" id="A4.SS2.p1.1">For the DROP dataset, we use the following prompt:</p>
<div class="ltx_listing ltx_lstlisting ltx_framed ltx_framed_rectangle ltx_listing" id="A4.SS2.p1.2">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,UGxlYXNlIGFuc3dlciB0aGUgcXVlc3Rpb24gYmFzZWQgb24gdGhlIGdpdmVuIHBhc3NhZ2UuClBhc3NhZ2U6IHtwYXNzYWdlIGluIHRoZSBkYXRhc2V0fQpRdWVzdGlvbjoge3F1ZXN0aW9ufQpOb3cgZ2l2ZSBtZSB0aGUgYW5zd2VyLg==">â¬‡</a></div>
<div class="ltx_listingline" id="lstnumberx17">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.1">Please</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.3">answer</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.5">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.7">question</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.9">based</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.11">on</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.13">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.15">given</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.16"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.17">passage</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.18">.</span>
</div>
<div class="ltx_listingline" id="lstnumberx18">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx18.1">Passage</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.2">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.3"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.4">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx18.5">passage</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx18.7">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx18.9">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx18.11">dataset</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.12">}</span>
</div>
<div class="ltx_listingline" id="lstnumberx19">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.1">Question</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx19.2">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.3"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx19.4">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.5">question</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx19.6">}</span>
</div>
<div class="ltx_listingline" id="lstnumberx20">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx20.1">Now</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx20.3">give</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx20.5">me</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx20.7">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx20.9">answer</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.10">.</span>
</div>
</div>
</div>
<div class="ltx_para" id="A4.SS2.p2">
<p class="ltx_p" id="A4.SS2.p2.1">For the GSM8K dataset, we use the following prompt:</p>
<div class="ltx_listing ltx_lstlisting ltx_framed ltx_framed_rectangle ltx_listing" id="A4.SS2.p2.2">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,QW5zd2VyIHRoZSBtYXRoIHdvcmQgcXVlc3Rpb24gc3RlcCBieSBzdGVwLiBZb3VyIGFuc3dlciBuZWVkcyB0byBlbmQgd2l0aCAnVGhlIGFuc3dlciBpcycuClF1ZXN0aW9uOiB7cXVlc3Rpb259CkxldCdzIHRoaW5rIHN0ZXAgYnkgc3RlcCBhbmQgZ2l2ZSBtZSB0aGUgYW5zd2VyLg==">â¬‡</a></div>
<div class="ltx_listingline" id="lstnumberx21">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx21.1">Answer</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx21.3">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx21.5">math</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx21.7">word</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx21.9">question</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx21.11">step</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx21.13">by</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx21.15">step</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx21.16">.</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.17"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx21.18">Your</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.19"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx21.20">answer</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.21"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx21.22">needs</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.23"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx21.24">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.25"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx21.26">end</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.27"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx21.28">with</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.29"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx21.30">â€™</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx21.31">The</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.32"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx21.33">answer</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.34"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx21.35">is</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx21.36">â€™.</span>
</div>
<div class="ltx_listingline" id="lstnumberx22">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx22.1">Question</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx22.2">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx22.3"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx22.4">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx22.5">question</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx22.6">}</span>
</div>
<div class="ltx_listingline" id="lstnumberx23">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.1">Let</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx23.2">â€™</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.3">s</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.5">think</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.7">step</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.9">by</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.11">step</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.13">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.15">give</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.16"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.17">me</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.18"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.19">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.20"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.21">answer</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx23.22">.</span>
</div>
</div>
</div>
<div class="ltx_para" id="A4.SS2.p3">
<p class="ltx_p" id="A4.SS2.p3.1">For other datasets, we directly input the question to the model:</p>
<div class="ltx_listing ltx_lstlisting ltx_framed ltx_framed_rectangle ltx_listing" id="A4.SS2.p3.2">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,e3F1ZXN0aW9ufQ==">â¬‡</a></div>
<div class="ltx_listingline" id="lstnumberx24">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx24.1">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx24.2">question</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx24.3">}</span>
</div>
</div>
</div>
</section>
<section class="ltx_subsection" id="A4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.3 </span>Generation with Retrieval</h3>
<div class="ltx_para" id="A4.SS3.p1">
<p class="ltx_p" id="A4.SS3.p1.1">For the DROP dataset, we use the following prompt:</p>
<div class="ltx_listing ltx_lstlisting ltx_framed ltx_framed_rectangle ltx_listing" id="A4.SS3.p1.2">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,UGxlYXNlIGFuc3dlciB0aGUgcXVlc3Rpb24gYmFzZWQgb24gdGhlIGdpdmVuIHBhc3NhZ2UuClBhc3NhZ2U6IHtwYXNzYWdlIGluIHRoZSBkYXRhc2V0fQpRdWVzdGlvbjoge3F1ZXN0aW9ufQoKSGVyZSBhcmUgc29tZSBhZGRpdGlvbmFsIHJlZmVyZW5jZSBwYXNzYWdlczoKe3JldHJpZXZlZCBkb2N1bWVudHN9CgpZb3UgY2FuIHJlZmVyIHRvIHRoZSBjb250ZW50IG9mIHJlbGV2YW50IHJlZmVyZW5jZSBwYXNzYWdlcyB0byBhbnN3ZXIgdGhlIHF1ZXN0aW9ucy4KTm93IGdpdmUgbWUgdGhlIGFuc3dlci4=">â¬‡</a></div>
<div class="ltx_listingline" id="lstnumberx25">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx25.1">Please</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx25.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx25.3">answer</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx25.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx25.5">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx25.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx25.7">question</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx25.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx25.9">based</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx25.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx25.11">on</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx25.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx25.13">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx25.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx25.15">given</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx25.16"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx25.17">passage</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx25.18">.</span>
</div>
<div class="ltx_listingline" id="lstnumberx26">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx26.1">Passage</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx26.2">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx26.3"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx26.4">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx26.5">passage</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx26.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx26.7">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx26.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx26.9">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx26.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx26.11">dataset</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx26.12">}</span>
</div>
<div class="ltx_listingline" id="lstnumberx27">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx27.1">Question</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx27.2">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx27.3"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx27.4">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx27.5">question</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx27.6">}</span>
</div>
<div class="ltx_listingline" id="lstnumberx28">
</div>
<div class="ltx_listingline" id="lstnumberx29">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx29.1">Here</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx29.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx29.3">are</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx29.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx29.5">some</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx29.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx29.7">additional</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx29.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx29.9">reference</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx29.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx29.11">passages</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx29.12">:</span>
</div>
<div class="ltx_listingline" id="lstnumberx30">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx30.1">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx30.2">retrieved</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx30.3"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx30.4">documents</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx30.5">}</span>
</div>
<div class="ltx_listingline" id="lstnumberx31">
</div>
<div class="ltx_listingline" id="lstnumberx32">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx32.1">You</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx32.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx32.3">can</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx32.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx32.5">refer</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx32.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx32.7">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx32.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx32.9">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx32.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx32.11">content</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx32.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx32.13">of</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx32.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx32.15">relevant</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx32.16"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx32.17">reference</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx32.18"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx32.19">passages</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx32.20"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx32.21">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx32.22"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx32.23">answer</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx32.24"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx32.25">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx32.26"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx32.27">questions</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx32.28">.</span>
</div>
<div class="ltx_listingline" id="lstnumberx33">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx33.1">Now</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx33.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx33.3">give</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx33.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx33.5">me</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx33.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx33.7">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx33.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx33.9">answer</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx33.10">.</span>
</div>
</div>
</div>
<div class="ltx_para" id="A4.SS3.p2">
<p class="ltx_p" id="A4.SS3.p2.1">For the GSM8K dataset, we use the following prompt:</p>
<div class="ltx_listing ltx_lstlisting ltx_framed ltx_framed_rectangle ltx_listing" id="A4.SS3.p2.2">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,QW5zd2VyIHRoZSBtYXRoIHdvcmQgcXVlc3Rpb24gc3RlcCBieSBzdGVwLiBZb3VyIGFuc3dlciBuZWVkcyB0byBlbmQgd2l0aCAnVGhlIGFuc3dlciBpcycKUXVlc3Rpb246IHtxdWVzdGlvbn0KCkhlcmUgYXJlIHNvbWUgYWRkaXRpb25hbCByZWZlcmVuY2UgcGFzc2FnZXM6CntyZXRyaWV2ZWQgZG9jdW1lbnRzfQoKWW91IGNhbiByZWZlciB0byB0aGUgY29udGVudCBvZiByZWxldmFudCByZWZlcmVuY2UgcGFzc2FnZXMgdG8gYW5zd2VyIHRoZSBxdWVzdGlvbnMuCkxldCdzIHRoaW5rIHN0ZXAgYnkgc3RlcCBhbmQgZ2l2ZSBtZSB0aGUgYW5zd2VyLg==">â¬‡</a></div>
<div class="ltx_listingline" id="lstnumberx34">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx34.1">Answer</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx34.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx34.3">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx34.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx34.5">math</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx34.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx34.7">word</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx34.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx34.9">question</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx34.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx34.11">step</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx34.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx34.13">by</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx34.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx34.15">step</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx34.16">.</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx34.17"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx34.18">Your</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx34.19"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx34.20">answer</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx34.21"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx34.22">needs</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx34.23"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx34.24">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx34.25"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx34.26">end</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx34.27"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx34.28">with</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx34.29"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx34.30">â€™</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx34.31">The</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx34.32"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx34.33">answer</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx34.34"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx34.35">is</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx34.36">â€™</span>
</div>
<div class="ltx_listingline" id="lstnumberx35">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx35.1">Question</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx35.2">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx35.3"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx35.4">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx35.5">question</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx35.6">}</span>
</div>
<div class="ltx_listingline" id="lstnumberx36">
</div>
<div class="ltx_listingline" id="lstnumberx37">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx37.1">Here</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx37.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx37.3">are</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx37.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx37.5">some</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx37.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx37.7">additional</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx37.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx37.9">reference</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx37.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx37.11">passages</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx37.12">:</span>
</div>
<div class="ltx_listingline" id="lstnumberx38">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx38.1">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx38.2">retrieved</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx38.3"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx38.4">documents</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx38.5">}</span>
</div>
<div class="ltx_listingline" id="lstnumberx39">
</div>
<div class="ltx_listingline" id="lstnumberx40">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx40.1">You</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx40.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx40.3">can</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx40.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx40.5">refer</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx40.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx40.7">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx40.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx40.9">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx40.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx40.11">content</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx40.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx40.13">of</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx40.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx40.15">relevant</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx40.16"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx40.17">reference</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx40.18"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx40.19">passages</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx40.20"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx40.21">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx40.22"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx40.23">answer</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx40.24"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx40.25">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx40.26"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx40.27">questions</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx40.28">.</span>
</div>
<div class="ltx_listingline" id="lstnumberx41">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx41.1">Let</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx41.2">â€™</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx41.3">s</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx41.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx41.5">think</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx41.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx41.7">step</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx41.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx41.9">by</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx41.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx41.11">step</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx41.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx41.13">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx41.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx41.15">give</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx41.16"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx41.17">me</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx41.18"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx41.19">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx41.20"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx41.21">answer</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx41.22">.</span>
</div>
</div>
</div>
<div class="ltx_para" id="A4.SS3.p3">
<p class="ltx_p" id="A4.SS3.p3.1">For other datasets, we use the following prompt:</p>
<div class="ltx_listing ltx_lstlisting ltx_framed ltx_framed_rectangle ltx_listing" id="A4.SS3.p3.2">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,e3F1ZXN0aW9ufQoKSGVyZSBhcmUgc29tZSBhZGRpdGlvbmFsIHJlZmVyZW5jZSBwYXNzYWdlczoKe3JldHJpZXZlZCBkb2N1bWVudHN9CgpZb3UgY2FuIHJlZmVyIHRvIHRoZSBjb250ZW50IG9mIHJlbGV2YW50IHJlZmVyZW5jZSBwYXNzYWdlcyB0byBhbnN3ZXIgdGhlIHF1ZXN0aW9ucy4KTm93IGdpdmUgbWUgdGhlIGFuc3dlci4=">â¬‡</a></div>
<div class="ltx_listingline" id="lstnumberx42">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx42.1">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx42.2">question</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx42.3">}</span>
</div>
<div class="ltx_listingline" id="lstnumberx43">
</div>
<div class="ltx_listingline" id="lstnumberx44">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx44.1">Here</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx44.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx44.3">are</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx44.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx44.5">some</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx44.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx44.7">additional</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx44.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx44.9">reference</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx44.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx44.11">passages</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx44.12">:</span>
</div>
<div class="ltx_listingline" id="lstnumberx45">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx45.1">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx45.2">retrieved</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx45.3"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx45.4">documents</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx45.5">}</span>
</div>
<div class="ltx_listingline" id="lstnumberx46">
</div>
<div class="ltx_listingline" id="lstnumberx47">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx47.1">You</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx47.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx47.3">can</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx47.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx47.5">refer</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx47.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx47.7">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx47.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx47.9">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx47.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx47.11">content</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx47.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx47.13">of</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx47.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx47.15">relevant</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx47.16"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx47.17">reference</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx47.18"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx47.19">passages</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx47.20"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx47.21">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx47.22"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx47.23">answer</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx47.24"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx47.25">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx47.26"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx47.27">questions</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx47.28">.</span>
</div>
<div class="ltx_listingline" id="lstnumberx48">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx48.1">Now</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx48.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx48.3">give</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx48.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx48.5">me</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx48.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx48.7">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx48.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx48.9">answer</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx48.10">.</span>
</div>
</div>
</div>
</section>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Details of UAR Training</h2>
<div class="ltx_para" id="A5.p1">
<p class="ltx_p" id="A5.p1.1">When training the UAR classifiers, we set the batch size to 32 and train for a total of 10 epochs, saving after each epoch and selecting the checkpoint that perform best on the validation set.
We conduct a grid search on the validation set and ultimately determine the learning rate to be 5e-5.
Our classifier is a fully connected layer with an input dimension equal to the hidden state dimension and an output dimension of 2.</p>
</div>
</section>
<section class="ltx_appendix" id="A6">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Downstream Task Datasets</h2>
<div class="ltx_para" id="A6.p1">
<p class="ltx_p" id="A6.p1.1">For knowledge-aware scenario, we use the validation set of DROP <cite class="ltx_cite ltx_citemacro_citep">(Dua etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib15" title="">2019</a>)</cite> and the test set of GSM8K <cite class="ltx_cite ltx_citemacro_citep">(Cobbe etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib12" title="">2021</a>)</cite> as the test sets.
DROP is a reading comprehension benchmark, which needs the model to answer questions based on given paragraphs.
GSM8K is a dataset containing diverse grade school math word problems, primarily used to assess the reasoning ability of models.
These two datasets evaluate the modelâ€™s abstract abilities, e.g., reading comprehension and math reasoning, and thus do not require extra fact knowledge.
Therefore, they can measure the ability of active retrieval methods to avoid unnecessary retrieval for scenarios that requires little fact knowledge.</p>
</div>
<div class="ltx_para" id="A6.p2">
<p class="ltx_p" id="A6.p2.1">For time-aware scenario, we use the test set of TAQA <cite class="ltx_cite ltx_citemacro_citep">(Zhao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib48" title="">2024</a>)</cite> and questions whose answers will change over time from FreshQA <cite class="ltx_cite ltx_citemacro_citep">(Vu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib37" title="">2023</a>)</cite> (We remove questions with false premises).
Since these questions are time-sensitive, the active retrieval system need to retrieve real-time information for every question.</p>
</div>
<div class="ltx_para" id="A6.p3">
<p class="ltx_p" id="A6.p3.1">For self-aware scenario, we use the validation set of TriviaQA <cite class="ltx_cite ltx_citemacro_citep">(Joshi etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib22" title="">2017</a>)</cite> and the test set of WebQuestions (WQ) <cite class="ltx_cite ltx_citemacro_citep">(Berant etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.12534v4#bib.bib5" title="">2013</a>)</cite>.
These test samples are non-time-sensitive questions.
The active retrieval system only needs to retrieve questions which the model does not know, and try to achieve high answer accuracy with an appropriate number of retrieval calls.</p>
</div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Oct  3 02:38:46 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
