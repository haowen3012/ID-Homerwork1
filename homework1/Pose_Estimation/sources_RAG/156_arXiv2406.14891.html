<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering</title>
<!--Generated on Mon Sep 16 17:16:06 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2406.14891v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S1" title="In Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S2" title="In Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S2.SS1" title="In 2 Related work â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Multi-hop Question Answering</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S2.SS2" title="In 2 Related work â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Retrieval-Augmented Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S3" title="In Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Generate-then-Ground with LLMs</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S3.SS1" title="In 3 Generate-then-Ground with LLMs â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Answer Deduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S3.SS2" title="In 3 Generate-then-Ground with LLMs â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Instructional Knowledge Grounding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S3.SS3" title="In 3 Generate-then-Ground with LLMs â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Batch Knowledge Ground</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S4" title="In Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Generalization with Grounding Distillation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S4.SS1" title="In 4 Generalization with Grounding Distillation â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Synthesize the Training Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S4.SS2" title="In 4 Generalization with Grounding Distillation â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Training Objective</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S5" title="In Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S5.SS1" title="In 5 Experimental Setup â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S5.SS2" title="In 5 Experimental Setup â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Baselines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S5.SS3" title="In 5 Experimental Setup â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Evaluation Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S5.SS4" title="In 5 Experimental Setup â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Implementation Details</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S6" title="In Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Experimental Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S6.SS1" title="In 6 Experimental Results â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Experimental Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S6.SS1.SSS0.Px1" title="In 6.1 Experimental Results â€£ 6 Experimental Results â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_title">Overall performance.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S6.SS1.SSS0.Px2" title="In 6.1 Experimental Results â€£ 6 Experimental Results â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_title">Results with the smaller model.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S6.SS1.SSS0.Px3" title="In 6.1 Experimental Results â€£ 6 Experimental Results â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_title">Results with different retrievers.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S6.SS2" title="In 6 Experimental Results â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Ablation Study</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S6.SS3" title="In 6 Experimental Results â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Case Study</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S7" title="In Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Analysis and Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S7.SS1" title="In 7 Analysis and Discussion â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Result Consistency and Stability</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S7.SS2" title="In 7 Analysis and Discussion â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Knowledge Incorporation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S7.SS3" title="In 7 Analysis and Discussion â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3 </span>Qualitative Analysis for Efficiency</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S8" title="In Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#Sx4.SSx3.SSS0.Px1" title="In Appendix III. Training Dataset for Grounding Distillation â€£ Appendices â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_title">Synthesize the dataset</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Generate-then-Ground in Retrieval-Augmented Generation 
<br class="ltx_break"/>for Multi-hop Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Zhengliang Shi<sup class="ltx_sup" id="id11.11.id1">1</sup>Â Â Weiwei Sun<sup class="ltx_sup" id="id12.12.id2">1</sup>Â Â Shen Gao<sup class="ltx_sup" id="id13.13.id3">2</sup>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id6.6.3">Pengjie Ren<sup class="ltx_sup" id="id6.6.3.2"><span class="ltx_text ltx_font_medium" id="id6.6.3.2.1">1</span></sup>Â Â Zhumin Chen<sup class="ltx_sup" id="id6.6.3.3"><span class="ltx_text ltx_font_medium" id="id6.6.3.3.1">1</span></sup>Â Â Zhaochun Ren<sup class="ltx_sup" id="id6.6.3.1"><math alttext="3" class="ltx_Math" display="inline" id="id6.6.3.1.m1.1"><semantics id="id6.6.3.1.m1.1a"><mn id="id6.6.3.1.m1.1.1" xref="id6.6.3.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="id6.6.3.1.m1.1b"><cn id="id6.6.3.1.m1.1.1.cmml" type="integer" xref="id6.6.3.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="id6.6.3.1.m1.1c">3</annotation><annotation encoding="application/x-llamapun" id="id6.6.3.1.m1.1d">3</annotation></semantics></math></sup></span>
<br class="ltx_break"/><sup class="ltx_sup" id="id14.14.id4">1</sup>Shandong University, Qingdao, China 
<br class="ltx_break"/><sup class="ltx_sup" id="id15.15.id5">2</sup>University of Electronic Science and Technology of China, Chengdu, China
<br class="ltx_break"/><sup class="ltx_sup" id="id16.16.id6">3</sup>Leiden University, Leiden, The Netherlands 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id17.17.id7">shizhl@mail.sdu.edu.cnÂ Â z.ren@liacs.leidenuniv.nl
<br class="ltx_break"/></span>
</span><span class="ltx_author_notes"><sup class="ltx_sup" id="id18.18.id1">âˆ—</sup><span class="ltx_text ltx_font_bold" id="id19.19.id2"> Corresponding author.</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id20.id1">Multi-Hop Question Answering (MHQA) tasks present a significant challenge for large language models (LLMs) due to the intensive knowledge required.
Current solutions, like Retrieval-Augmented Generation, typically <span class="ltx_text ltx_font_italic" id="id20.id1.1">retrieve</span> potential documents from an external corpus to <span class="ltx_text ltx_font_italic" id="id20.id1.2">read</span> an answer.
However, the performance of this <span class="ltx_text ltx_font_italic" id="id20.id1.3">retrieve-then-read</span> paradigm is constrained by the retriever and the inevitable noise in the retrieved documents.
To mitigate these challenges, we introduce a novel <span class="ltx_text ltx_font_italic" id="id20.id1.4">generate-then-ground</span> (GenGround) framework, synergizing the parametric knowledge of LLMs and external documents to solve a multi-hop question.
GenGround empowers LLMs to alternate two phases until the final answer is derived:
(1) formulate a simpler, single-hop question and directly generate the answer;
(2) ground the question-answer pair in retrieved documents, amending any wrong predictions in the answer.
We also propose an instructional grounding distillation method to generalize our method into smaller models.
Extensive experiments conducted on four datasets illustrate the superiority of our method.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.10">
<p class="ltx_p" id="p1.10.11"><span class="ltx_text ltx_font_bold" id="p1.10.11.1">Generate-then-Ground in Retrieval-Augmented Generation 
<br class="ltx_break"/>for Multi-hop Question Answering</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.10.10" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.10.10.10" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.10.10.10.10">
<span class="ltx_tr" id="p1.3.3.3.3.3">
<span class="ltx_td ltx_align_center" id="p1.3.3.3.3.3.3"><span class="ltx_text ltx_font_bold" id="p1.3.3.3.3.3.3.3">
Zhengliang Shi<sup class="ltx_sup" id="p1.3.3.3.3.3.3.3.1"><span class="ltx_text ltx_font_medium" id="p1.3.3.3.3.3.3.3.1.1">1</span></sup>Â Â Weiwei Sun<sup class="ltx_sup" id="p1.3.3.3.3.3.3.3.2"><span class="ltx_text ltx_font_medium" id="p1.3.3.3.3.3.3.3.2.1">1</span></sup>Â Â Shen Gao<sup class="ltx_sup" id="p1.3.3.3.3.3.3.3.3"><span class="ltx_text ltx_font_medium" id="p1.3.3.3.3.3.3.3.3.1">2</span></sup></span></span></span>
<span class="ltx_tr" id="p1.7.7.7.7.7">
<span class="ltx_td ltx_align_center" id="p1.7.7.7.7.7.4"><span class="ltx_text ltx_font_bold" id="p1.7.7.7.7.7.4.4">Pengjie Ren<sup class="ltx_sup" id="p1.7.7.7.7.7.4.4.3"><span class="ltx_text ltx_font_medium" id="p1.7.7.7.7.7.4.4.3.1">1</span></sup>Â Â Zhumin Chen<sup class="ltx_sup" id="p1.7.7.7.7.7.4.4.4"><span class="ltx_text ltx_font_medium" id="p1.7.7.7.7.7.4.4.4.1">1</span></sup>Â Â Zhaochun Ren<sup class="ltx_sup" id="p1.6.6.6.6.6.3.3.1"><math alttext="3" class="ltx_Math" display="inline" id="p1.6.6.6.6.6.3.3.1.m1.1"><semantics id="p1.6.6.6.6.6.3.3.1.m1.1a"><mn id="p1.6.6.6.6.6.3.3.1.m1.1.1" xref="p1.6.6.6.6.6.3.3.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="p1.6.6.6.6.6.3.3.1.m1.1b"><cn id="p1.6.6.6.6.6.3.3.1.m1.1.1.cmml" type="integer" xref="p1.6.6.6.6.6.3.3.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="p1.6.6.6.6.6.3.3.1.m1.1c">3</annotation><annotation encoding="application/x-llamapun" id="p1.6.6.6.6.6.3.3.1.m1.1d">3</annotation></semantics></math></sup><span class="ltx_note ltx_role_thanks" id="p1.7.7.7.7.7.4.4.2"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">thanks: </span><sup class="ltx_sup" id="p1.7.7.7.7.7.4.4.2.1"><span class="ltx_text ltx_font_medium" id="p1.7.7.7.7.7.4.4.2.1.1">âˆ—</span></sup> Corresponding author.</span></span></span></span></span></span>
<span class="ltx_tr" id="p1.8.8.8.8.8">
<span class="ltx_td ltx_align_center" id="p1.8.8.8.8.8.1"><sup class="ltx_sup" id="p1.8.8.8.8.8.1.1">1</sup>Shandong University, Qingdao, China</span></span>
<span class="ltx_tr" id="p1.9.9.9.9.9">
<span class="ltx_td ltx_align_center" id="p1.9.9.9.9.9.1"><sup class="ltx_sup" id="p1.9.9.9.9.9.1.1">2</sup>University of Electronic Science and Technology of China, Chengdu, China</span></span>
<span class="ltx_tr" id="p1.10.10.10.10.10">
<span class="ltx_td ltx_align_center" id="p1.10.10.10.10.10.1"><sup class="ltx_sup" id="p1.10.10.10.10.10.1.1">3</sup>Leiden University, Leiden, The Netherlands</span></span>
<span class="ltx_tr" id="p1.10.10.10.10.11">
<span class="ltx_td ltx_align_center" id="p1.10.10.10.10.11.1"><span class="ltx_text ltx_font_typewriter" id="p1.10.10.10.10.11.1.1">shizhl@mail.sdu.edu.cnÂ Â z.ren@liacs.leidenuniv.nl</span></span></span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Multi-Hop Question Answering (MHQA) tasksÂ <cite class="ltx_cite ltx_citemacro_citep">(Yang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib41" title="">2018</a>)</cite> require multi-hop reasoning using intensive knowledge to derive the answerÂ <cite class="ltx_cite ltx_citemacro_citep">(Xu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib40" title="">2024</a>)</cite>. It has been widely employed in various practical scenarios and domainsÂ <cite class="ltx_cite ltx_citemacro_citep">(Mavi etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib20" title="">2022</a>)</cite>.
To answer a multi-hop question, most prior work integrates large language models (LLMs) with information retrieval techniques, following a <span class="ltx_text ltx_font_italic" id="S1.p1.1.1">retrieve-then-read</span> paradigmÂ <cite class="ltx_cite ltx_citemacro_cite">Shao etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib27" title="">2023</a>)</cite>.
As illustrated in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_tag">1</span></a>, the initial step employs the LLMs to break down the complex question and formulate a series of simpler, single questions in a step-by-step manner.
For each step, the LLMs are guided to derive an answer from the relevant documents, which are retrieved using the formulated questionÂ <cite class="ltx_cite ltx_citemacro_citep">(Yao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib43" title="">2022</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S1.F1.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The top block depicts the comparison with the commonly-used <span class="ltx_text ltx_font_italic" id="S1.F1.2.1">retrieve-then-read</span> paradigm in MHQA task. The bottom block provides the performance of our method and baselines in four MHQA benchmarks.
</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Despite the progress of the <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">retrieve-then-read paradigm</span>, it faces <span class="ltx_text ltx_font_italic" id="S1.p2.1.2">two challenges</span> in practice.
First, its effectiveness is constrained by the performance of the retrieverÂ <cite class="ltx_cite ltx_citemacro_citep">(Yu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib44" title="">2022</a>; Xu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib40" title="">2024</a>)</cite>.
Given that the answer can only be derived from the retrieved documents, the inherent world knowledge of LLMs is overlooked.
This limitation is particularly magnified in multi-hop QA tasks that frequently require complex logical reasoningÂ <cite class="ltx_cite ltx_citemacro_citep">(Mavi etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib20" title="">2022</a>)</cite>. The retrievers may struggle to retrieve all necessary documents to answer the question, leading to a performance decline using this paradigmÂ <cite class="ltx_cite ltx_citemacro_citep">(Yu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib44" title="">2022</a>; Abdallah and Jatowt, <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib1" title="">2023</a>)</cite>.
Second, the retrieved documents inevitably contain irrelevant or plausible-looking statementsÂ <cite class="ltx_cite ltx_citemacro_citep">(Gao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib8" title="">2023b</a>; Jiang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib12" title="">2023b</a>)</cite>.
Directly incorporating them into the chain-of-reasoning of LLMs may mislead the LLMs to generate incorrect or irrelevant responsesÂ <cite class="ltx_cite ltx_citemacro_citep">(Adlakha etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib2" title="">2024</a>; Thakur etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib33" title="">2023b</a>)</cite>.
Therefore, developing an adaptive method for utilizing the retrieved documents remains an active research area.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Inspired by the extensive knowledge and powerful deduction capability of LLMs, <em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">e.g.,</em> ChatGPT or GPT-4, we propose to address these challenges with a novel <span class="ltx_text ltx_font_italic" id="S1.p3.1.2">generate-then-ground</span> (GenGround) method, as shown in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_tag">1</span></a>.
This approach diverges from the retrieve-then-read paradigm by first allowing the LLMs to generate an immediate answer, and then grounding this answer in evidence to revise it.
In this work, we focus on the following research questions:
(1) <span class="ltx_text ltx_font_bold" id="S1.p3.1.3">RQ1</span>: How does our method synergize the world knowledge of LLMs and retrieved documents to answer a multi-hop question?
(2) <span class="ltx_text ltx_font_bold" id="S1.p3.1.4">RQ2</span>: For LLMs with different scales of parameters, how do we generalize our method?</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To address <span class="ltx_text ltx_font_bold" id="S1.p4.1.1">RQ1</span>, we enable LLMs to alternate between <span class="ltx_text ltx_font_italic" id="S1.p4.1.2">answer deduction</span> and <span class="ltx_text ltx_font_italic" id="S1.p4.1.3">instructional knowledge grounding</span>. In the deduction phase, LLMs form sub-questions from the input question and context. The LLMs then produce an immediate answer for each sub-question. To prevent non-factual hallucination by LLMsÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib47" title="">2023</a>; Gao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib6" title="">2023a</a>)</cite>, we guide LLMs to revise the answer in the grounding phase, using documents retrieved from an external corpus like Wikipedia. The LLMs ground the question-answer pair in evidence by citing relevant content and correcting errors. This revised answer is used for the next iterationâ€™s sub-question, continuing till the final hop. We also introduce a batch grounding strategy for efficient document use.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To address <span class="ltx_text ltx_font_bold" id="S1.p5.1.1">RQ2</span>, we propose an instructional grounding distillation method. Despite LLMs like ChatGPT performing well with our method, smaller models may struggle with instruction-following in the grounding phase. Thus, we use 50k single-hop questions from the Natural Questions (NQ) datasetÂ <cite class="ltx_cite ltx_citemacro_citep">(Kwiatkowski etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib15" title="">2019</a>)</cite>, each with ground-truth and noise documents. We guide ChatGPT to generate and adjust an answer for each question, then distill ChatGPTâ€™s process into a student model using instruction tuning.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Extensive experiments on four commonly-used MHQA datasets demonstrate superior performance over strong baselines (<em class="ltx_emph ltx_font_italic" id="S1.p6.1.1">e.g.,</em> ReAct and DSPy), achieving the best performance overall.
We also observe that our instructional grounding distillation empowers the smaller model with strong performance.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">To sum up, our main contributions are as follows:
(1) We propose a novel generate-then-ground framework for retrieval-argument generation technique in multi-hop question tasks, which effectively synergizes the knowledge of LLMs and retrieved documents.
(2) We introduce an instructional grounding distillation method, enabling a smaller model with the generate-then-ground framework.
(3) Experiments on four datasets are conducted to demonstrate the superiority of our method.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Multi-hop Question Answering</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Multi-Hop Question Answering (MHQA) tasks focus on answering questions that require gathering information from multiple sources and conducting multi-step reasoning to arrive at a comprehensive answerÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib46" title="">2024</a>; Li and Du, <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib17" title="">2023</a>)</cite>.
Some works utilize the knowledge deduction capabilityÂ <cite class="ltx_cite ltx_citemacro_citep">(Wei etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib38" title="">2022</a>)</cite> of LLMs to decompose the input question into single-hop questions and then solve them step-by-stepÂ <cite class="ltx_cite ltx_citemacro_cite">Wang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib36" title="">2022a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib35" title="">2023</a>)</cite>.
And many techniquesÂ <cite class="ltx_cite ltx_citemacro_citep">(Yao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib42" title="">2023</a>; Besta etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib3" title="">2023</a>)</cite> are proposed to improve the reasoning ability of LLMs.
However, the LLMs suffer from generating non-factual statements.
As an intuitive solution, many recent works integrate retrieval into the chain of thought reasoning processÂ <cite class="ltx_cite ltx_citemacro_citep">(Yao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib43" title="">2022</a>; Schick etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib26" title="">2023</a>)</cite>, prompting the LLMs to generate the answer using retrieved documents.
Although promising, the inevitable noise in retrieved documents could mislead the LLMs to a wrong reasoning direction and derive a wrong answerÂ <cite class="ltx_cite ltx_citemacro_citep">(Xu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib40" title="">2024</a>)</cite>.
In this work, we propose an instructional knowledge grounding method that enables LLMs to find the most relevant evidence from the document list, thereby reducing the effect of the noise.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Retrieval-Augmented Generation</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Retrieval-augmented generation (RAG) has been proven a promising technique to improve the performance of LLMs in knowledge-intensive NLP tasksÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib49" title="">2023</a>; Yu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib45" title="">2023</a>)</cite>, which enhances LLMs with retrievers to access external knowledge.
Existing RAG methods typically follow a retrieval-then-read pipelineÂ <cite class="ltx_cite ltx_citemacro_citep">(Ma etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib18" title="">2023</a>; Feng etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib5" title="">2023</a>; Gao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib8" title="">2023b</a>)</cite>.
Given a query, a retriever is first employed to retrieve the relevant document and a reader is then used to predict the answer on the condition of retrieved documentsÂ <cite class="ltx_cite ltx_citemacro_citep">(Khattab etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib14" title="">2023</a>)</cite>.
Despite the advancement of this paradigm, it is limited by the accuracy of retrieversÂ <cite class="ltx_cite ltx_citemacro_citep">(Yu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib44" title="">2022</a>; Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib46" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Recent works mitigate this problem by incorporating the world knowledge of LLMsÂ <cite class="ltx_cite ltx_citemacro_citep">(Gao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib6" title="">2023a</a>; Chen etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib4" title="">2023</a>)</cite>, where they utilize the LLM as a knowledge base to generate contextual documents and then read the answer from both retrieved and generated documentsÂ <cite class="ltx_cite ltx_citemacro_citep">(Abdallah and Jatowt, <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib1" title="">2023</a>)</cite>.
However, the potential knowledge conflict between the two knowledge sources is ignored, which may hallucinate the LLMsÂ <cite class="ltx_cite ltx_citemacro_citep">(Xie etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib39" title="">2023</a>; Thakur etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib32" title="">2023a</a>; Mallen etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib19" title="">2023</a>)</cite>.
In our work, we propose a generate-then-ground framework, more effectively incorporating the parametric knowledge of LLMs and external documents.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Generate-then-Ground with LLMs</h2>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S3.F2.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The architecture of the proposed generate-then-ground framework.</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S3.F3.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>
The instruction for the <span class="ltx_text ltx_font_italic" id="S3.F3.3.1">answer deduction </span>(a) and <span class="ltx_text ltx_font_italic" id="S3.F3.4.2">instructional knowledge grounding</span>(b) phases in our framework.
The pink and yellow blacks indicate the input while the gray blocks indicate the output.</figcaption>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">This section provides a detailed explanation of GenGround.
As depicted in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S3.F2" title="Figure 2 â€£ 3 Generate-then-Ground with LLMs â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_tag">2</span></a>, GenGround empowers the LLMs to alternate between two phases over multiple iterations until the final answer <math alttext="a" class="ltx_Math" display="inline" id="S3.p1.1.m1.1"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">a</annotation><annotation encoding="application/x-llamapun" id="S3.p1.1.m1.1d">italic_a</annotation></semantics></math>, is derived. These phases include <span class="ltx_text ltx_font_italic" id="S3.p1.1.1">answer deduction</span> (SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S3.SS1" title="3.1 Answer Deduction â€£ 3 Generate-then-Ground with LLMs â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_tag">3.1</span></a>) and <span class="ltx_text ltx_font_italic" id="S3.p1.1.2">instructional knowledge grounding</span> (SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S3.SS2" title="3.2 Instructional Knowledge Grounding â€£ 3 Generate-then-Ground with LLMs â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_tag">3.2</span></a>).
During each iteration, the former guides the LLMs to generate a simpler, single-hop answer. The latter phase addresses LLMsâ€™ non-factual hallucinationÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib47" title="">2023</a>)</cite> by prompting them to ground the question-answer pair in evidence and correct wrong predictions. The revised answer and sub-question are then integrated into the LLMsâ€™ context for the following iterationâ€™s prediction.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Answer Deduction</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.13">The answer deduction phase aims to utilize the world knowledge of LLMs stored within their parameters <math alttext="\theta" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">Î¸</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">ğœƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">italic_Î¸</annotation></semantics></math>.
Given the complex reasoning involved in multi-hop questionsÂ <cite class="ltx_cite ltx_citemacro_citep">(Tang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib31" title="">2021</a>; Li and Du, <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib17" title="">2023</a>)</cite>, it can be challenging to generate an accurate answer directly.
As a result, we guide the LLM, denoted as <math alttext="\mathcal{M}_{\theta}" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><msub id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">â„³</mi><mi id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">Î¸</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">â„³</ci><ci id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">ğœƒ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\mathcal{M}_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">caligraphic_M start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT</annotation></semantics></math>, to break down a complex question <math alttext="\mathcal{Q}" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">ğ’¬</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">ğ’¬</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">\mathcal{Q}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">caligraphic_Q</annotation></semantics></math> into single-hop questions with a fine granularity.
Formally, for <math alttext="i" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4.1"><semantics id="S3.SS1.p1.4.m4.1a"><mi id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.1d">italic_i</annotation></semantics></math>-th iteration, we denote the current context as <math alttext="\mathcal{H}=\{(q_{j},\tilde{j}_{i})|j&lt;i\}" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m5.2"><semantics id="S3.SS1.p1.5.m5.2a"><mrow id="S3.SS1.p1.5.m5.2.2" xref="S3.SS1.p1.5.m5.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.5.m5.2.2.4" xref="S3.SS1.p1.5.m5.2.2.4.cmml">â„‹</mi><mo id="S3.SS1.p1.5.m5.2.2.3" xref="S3.SS1.p1.5.m5.2.2.3.cmml">=</mo><mrow id="S3.SS1.p1.5.m5.2.2.2.2" xref="S3.SS1.p1.5.m5.2.2.2.3.cmml"><mo id="S3.SS1.p1.5.m5.2.2.2.2.3" stretchy="false" xref="S3.SS1.p1.5.m5.2.2.2.3.1.cmml">{</mo><mrow id="S3.SS1.p1.5.m5.1.1.1.1.1.2" xref="S3.SS1.p1.5.m5.1.1.1.1.1.3.cmml"><mo id="S3.SS1.p1.5.m5.1.1.1.1.1.2.3" stretchy="false" xref="S3.SS1.p1.5.m5.1.1.1.1.1.3.cmml">(</mo><msub id="S3.SS1.p1.5.m5.1.1.1.1.1.1.1" xref="S3.SS1.p1.5.m5.1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.5.m5.1.1.1.1.1.1.1.2" xref="S3.SS1.p1.5.m5.1.1.1.1.1.1.1.2.cmml">q</mi><mi id="S3.SS1.p1.5.m5.1.1.1.1.1.1.1.3" xref="S3.SS1.p1.5.m5.1.1.1.1.1.1.1.3.cmml">j</mi></msub><mo id="S3.SS1.p1.5.m5.1.1.1.1.1.2.4" xref="S3.SS1.p1.5.m5.1.1.1.1.1.3.cmml">,</mo><msub id="S3.SS1.p1.5.m5.1.1.1.1.1.2.2" xref="S3.SS1.p1.5.m5.1.1.1.1.1.2.2.cmml"><mover accent="true" id="S3.SS1.p1.5.m5.1.1.1.1.1.2.2.2" xref="S3.SS1.p1.5.m5.1.1.1.1.1.2.2.2.cmml"><mi id="S3.SS1.p1.5.m5.1.1.1.1.1.2.2.2.2" xref="S3.SS1.p1.5.m5.1.1.1.1.1.2.2.2.2.cmml">j</mi><mo id="S3.SS1.p1.5.m5.1.1.1.1.1.2.2.2.1" xref="S3.SS1.p1.5.m5.1.1.1.1.1.2.2.2.1.cmml">~</mo></mover><mi id="S3.SS1.p1.5.m5.1.1.1.1.1.2.2.3" xref="S3.SS1.p1.5.m5.1.1.1.1.1.2.2.3.cmml">i</mi></msub><mo id="S3.SS1.p1.5.m5.1.1.1.1.1.2.5" stretchy="false" xref="S3.SS1.p1.5.m5.1.1.1.1.1.3.cmml">)</mo></mrow><mo id="S3.SS1.p1.5.m5.2.2.2.2.4" lspace="0em" rspace="0em" xref="S3.SS1.p1.5.m5.2.2.2.3.1.cmml">|</mo><mrow id="S3.SS1.p1.5.m5.2.2.2.2.2" xref="S3.SS1.p1.5.m5.2.2.2.2.2.cmml"><mi id="S3.SS1.p1.5.m5.2.2.2.2.2.2" xref="S3.SS1.p1.5.m5.2.2.2.2.2.2.cmml">j</mi><mo id="S3.SS1.p1.5.m5.2.2.2.2.2.1" xref="S3.SS1.p1.5.m5.2.2.2.2.2.1.cmml">&lt;</mo><mi id="S3.SS1.p1.5.m5.2.2.2.2.2.3" xref="S3.SS1.p1.5.m5.2.2.2.2.2.3.cmml">i</mi></mrow><mo id="S3.SS1.p1.5.m5.2.2.2.2.5" stretchy="false" xref="S3.SS1.p1.5.m5.2.2.2.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.2b"><apply id="S3.SS1.p1.5.m5.2.2.cmml" xref="S3.SS1.p1.5.m5.2.2"><eq id="S3.SS1.p1.5.m5.2.2.3.cmml" xref="S3.SS1.p1.5.m5.2.2.3"></eq><ci id="S3.SS1.p1.5.m5.2.2.4.cmml" xref="S3.SS1.p1.5.m5.2.2.4">â„‹</ci><apply id="S3.SS1.p1.5.m5.2.2.2.3.cmml" xref="S3.SS1.p1.5.m5.2.2.2.2"><csymbol cd="latexml" id="S3.SS1.p1.5.m5.2.2.2.3.1.cmml" xref="S3.SS1.p1.5.m5.2.2.2.2.3">conditional-set</csymbol><interval closure="open" id="S3.SS1.p1.5.m5.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.1.1.1.2"><apply id="S3.SS1.p1.5.m5.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.1.1.1.1.1.2">ğ‘</ci><ci id="S3.SS1.p1.5.m5.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.1.1.1.1.1.3">ğ‘—</ci></apply><apply id="S3.SS1.p1.5.m5.1.1.1.1.1.2.2.cmml" xref="S3.SS1.p1.5.m5.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.1.1.1.2.2.1.cmml" xref="S3.SS1.p1.5.m5.1.1.1.1.1.2.2">subscript</csymbol><apply id="S3.SS1.p1.5.m5.1.1.1.1.1.2.2.2.cmml" xref="S3.SS1.p1.5.m5.1.1.1.1.1.2.2.2"><ci id="S3.SS1.p1.5.m5.1.1.1.1.1.2.2.2.1.cmml" xref="S3.SS1.p1.5.m5.1.1.1.1.1.2.2.2.1">~</ci><ci id="S3.SS1.p1.5.m5.1.1.1.1.1.2.2.2.2.cmml" xref="S3.SS1.p1.5.m5.1.1.1.1.1.2.2.2.2">ğ‘—</ci></apply><ci id="S3.SS1.p1.5.m5.1.1.1.1.1.2.2.3.cmml" xref="S3.SS1.p1.5.m5.1.1.1.1.1.2.2.3">ğ‘–</ci></apply></interval><apply id="S3.SS1.p1.5.m5.2.2.2.2.2.cmml" xref="S3.SS1.p1.5.m5.2.2.2.2.2"><lt id="S3.SS1.p1.5.m5.2.2.2.2.2.1.cmml" xref="S3.SS1.p1.5.m5.2.2.2.2.2.1"></lt><ci id="S3.SS1.p1.5.m5.2.2.2.2.2.2.cmml" xref="S3.SS1.p1.5.m5.2.2.2.2.2.2">ğ‘—</ci><ci id="S3.SS1.p1.5.m5.2.2.2.2.2.3.cmml" xref="S3.SS1.p1.5.m5.2.2.2.2.2.3">ğ‘–</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.2c">\mathcal{H}=\{(q_{j},\tilde{j}_{i})|j&lt;i\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.5.m5.2d">caligraphic_H = { ( italic_q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , over~ start_ARG italic_j end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) | italic_j &lt; italic_i }</annotation></semantics></math>, which comprises the accumulation of previous deduced sub-questions, <math alttext="q_{&lt;i}" class="ltx_Math" display="inline" id="S3.SS1.p1.6.m6.1"><semantics id="S3.SS1.p1.6.m6.1a"><msub id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml"><mi id="S3.SS1.p1.6.m6.1.1.2" xref="S3.SS1.p1.6.m6.1.1.2.cmml">q</mi><mrow id="S3.SS1.p1.6.m6.1.1.3" xref="S3.SS1.p1.6.m6.1.1.3.cmml"><mi id="S3.SS1.p1.6.m6.1.1.3.2" xref="S3.SS1.p1.6.m6.1.1.3.2.cmml"></mi><mo id="S3.SS1.p1.6.m6.1.1.3.1" xref="S3.SS1.p1.6.m6.1.1.3.1.cmml">&lt;</mo><mi id="S3.SS1.p1.6.m6.1.1.3.3" xref="S3.SS1.p1.6.m6.1.1.3.3.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><apply id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.p1.6.m6.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.2">ğ‘</ci><apply id="S3.SS1.p1.6.m6.1.1.3.cmml" xref="S3.SS1.p1.6.m6.1.1.3"><lt id="S3.SS1.p1.6.m6.1.1.3.1.cmml" xref="S3.SS1.p1.6.m6.1.1.3.1"></lt><csymbol cd="latexml" id="S3.SS1.p1.6.m6.1.1.3.2.cmml" xref="S3.SS1.p1.6.m6.1.1.3.2">absent</csymbol><ci id="S3.SS1.p1.6.m6.1.1.3.3.cmml" xref="S3.SS1.p1.6.m6.1.1.3.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">q_{&lt;i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.6.m6.1d">italic_q start_POSTSUBSCRIPT &lt; italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, and revised answers, <math alttext="\tilde{a}_{&lt;i}" class="ltx_Math" display="inline" id="S3.SS1.p1.7.m7.1"><semantics id="S3.SS1.p1.7.m7.1a"><msub id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml"><mover accent="true" id="S3.SS1.p1.7.m7.1.1.2" xref="S3.SS1.p1.7.m7.1.1.2.cmml"><mi id="S3.SS1.p1.7.m7.1.1.2.2" xref="S3.SS1.p1.7.m7.1.1.2.2.cmml">a</mi><mo id="S3.SS1.p1.7.m7.1.1.2.1" xref="S3.SS1.p1.7.m7.1.1.2.1.cmml">~</mo></mover><mrow id="S3.SS1.p1.7.m7.1.1.3" xref="S3.SS1.p1.7.m7.1.1.3.cmml"><mi id="S3.SS1.p1.7.m7.1.1.3.2" xref="S3.SS1.p1.7.m7.1.1.3.2.cmml"></mi><mo id="S3.SS1.p1.7.m7.1.1.3.1" xref="S3.SS1.p1.7.m7.1.1.3.1.cmml">&lt;</mo><mi id="S3.SS1.p1.7.m7.1.1.3.3" xref="S3.SS1.p1.7.m7.1.1.3.3.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><apply id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m7.1.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">subscript</csymbol><apply id="S3.SS1.p1.7.m7.1.1.2.cmml" xref="S3.SS1.p1.7.m7.1.1.2"><ci id="S3.SS1.p1.7.m7.1.1.2.1.cmml" xref="S3.SS1.p1.7.m7.1.1.2.1">~</ci><ci id="S3.SS1.p1.7.m7.1.1.2.2.cmml" xref="S3.SS1.p1.7.m7.1.1.2.2">ğ‘</ci></apply><apply id="S3.SS1.p1.7.m7.1.1.3.cmml" xref="S3.SS1.p1.7.m7.1.1.3"><lt id="S3.SS1.p1.7.m7.1.1.3.1.cmml" xref="S3.SS1.p1.7.m7.1.1.3.1"></lt><csymbol cd="latexml" id="S3.SS1.p1.7.m7.1.1.3.2.cmml" xref="S3.SS1.p1.7.m7.1.1.3.2">absent</csymbol><ci id="S3.SS1.p1.7.m7.1.1.3.3.cmml" xref="S3.SS1.p1.7.m7.1.1.3.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">\tilde{a}_{&lt;i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.7.m7.1d">over~ start_ARG italic_a end_ARG start_POSTSUBSCRIPT &lt; italic_i end_POSTSUBSCRIPT</annotation></semantics></math>.
The context <math alttext="\mathcal{H}" class="ltx_Math" display="inline" id="S3.SS1.p1.8.m8.1"><semantics id="S3.SS1.p1.8.m8.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.8.m8.1.1" xref="S3.SS1.p1.8.m8.1.1.cmml">â„‹</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.1b"><ci id="S3.SS1.p1.8.m8.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1">â„‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">\mathcal{H}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.8.m8.1d">caligraphic_H</annotation></semantics></math>, along with the input question <math alttext="\mathcal{Q}" class="ltx_Math" display="inline" id="S3.SS1.p1.9.m9.1"><semantics id="S3.SS1.p1.9.m9.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.9.m9.1.1" xref="S3.SS1.p1.9.m9.1.1.cmml">ğ’¬</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m9.1b"><ci id="S3.SS1.p1.9.m9.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1">ğ’¬</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m9.1c">\mathcal{Q}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.9.m9.1d">caligraphic_Q</annotation></semantics></math>, is then fed into the LLMs to generate a sub-question, <math alttext="q_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.10.m10.1"><semantics id="S3.SS1.p1.10.m10.1a"><msub id="S3.SS1.p1.10.m10.1.1" xref="S3.SS1.p1.10.m10.1.1.cmml"><mi id="S3.SS1.p1.10.m10.1.1.2" xref="S3.SS1.p1.10.m10.1.1.2.cmml">q</mi><mi id="S3.SS1.p1.10.m10.1.1.3" xref="S3.SS1.p1.10.m10.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.10.m10.1b"><apply id="S3.SS1.p1.10.m10.1.1.cmml" xref="S3.SS1.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.10.m10.1.1.1.cmml" xref="S3.SS1.p1.10.m10.1.1">subscript</csymbol><ci id="S3.SS1.p1.10.m10.1.1.2.cmml" xref="S3.SS1.p1.10.m10.1.1.2">ğ‘</ci><ci id="S3.SS1.p1.10.m10.1.1.3.cmml" xref="S3.SS1.p1.10.m10.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.10.m10.1c">q_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.10.m10.1d">italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, that defines the specific information to be retrieved.
Subsequently, we prompt the LLMs, <math alttext="\mathcal{M}_{\theta}" class="ltx_Math" display="inline" id="S3.SS1.p1.11.m11.1"><semantics id="S3.SS1.p1.11.m11.1a"><msub id="S3.SS1.p1.11.m11.1.1" xref="S3.SS1.p1.11.m11.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.11.m11.1.1.2" xref="S3.SS1.p1.11.m11.1.1.2.cmml">â„³</mi><mi id="S3.SS1.p1.11.m11.1.1.3" xref="S3.SS1.p1.11.m11.1.1.3.cmml">Î¸</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.11.m11.1b"><apply id="S3.SS1.p1.11.m11.1.1.cmml" xref="S3.SS1.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.11.m11.1.1.1.cmml" xref="S3.SS1.p1.11.m11.1.1">subscript</csymbol><ci id="S3.SS1.p1.11.m11.1.1.2.cmml" xref="S3.SS1.p1.11.m11.1.1.2">â„³</ci><ci id="S3.SS1.p1.11.m11.1.1.3.cmml" xref="S3.SS1.p1.11.m11.1.1.3">ğœƒ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.11.m11.1c">\mathcal{M}_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.11.m11.1d">caligraphic_M start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT</annotation></semantics></math>, to directly generate an answer, <math alttext="a_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.12.m12.1"><semantics id="S3.SS1.p1.12.m12.1a"><msub id="S3.SS1.p1.12.m12.1.1" xref="S3.SS1.p1.12.m12.1.1.cmml"><mi id="S3.SS1.p1.12.m12.1.1.2" xref="S3.SS1.p1.12.m12.1.1.2.cmml">a</mi><mi id="S3.SS1.p1.12.m12.1.1.3" xref="S3.SS1.p1.12.m12.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.12.m12.1b"><apply id="S3.SS1.p1.12.m12.1.1.cmml" xref="S3.SS1.p1.12.m12.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.12.m12.1.1.1.cmml" xref="S3.SS1.p1.12.m12.1.1">subscript</csymbol><ci id="S3.SS1.p1.12.m12.1.1.2.cmml" xref="S3.SS1.p1.12.m12.1.1.2">ğ‘</ci><ci id="S3.SS1.p1.12.m12.1.1.3.cmml" xref="S3.SS1.p1.12.m12.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.12.m12.1c">a_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.12.m12.1d">italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, for the formulated question, <math alttext="q_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.13.m13.1"><semantics id="S3.SS1.p1.13.m13.1a"><msub id="S3.SS1.p1.13.m13.1.1" xref="S3.SS1.p1.13.m13.1.1.cmml"><mi id="S3.SS1.p1.13.m13.1.1.2" xref="S3.SS1.p1.13.m13.1.1.2.cmml">q</mi><mi id="S3.SS1.p1.13.m13.1.1.3" xref="S3.SS1.p1.13.m13.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.13.m13.1b"><apply id="S3.SS1.p1.13.m13.1.1.cmml" xref="S3.SS1.p1.13.m13.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.13.m13.1.1.1.cmml" xref="S3.SS1.p1.13.m13.1.1">subscript</csymbol><ci id="S3.SS1.p1.13.m13.1.1.2.cmml" xref="S3.SS1.p1.13.m13.1.1.2">ğ‘</ci><ci id="S3.SS1.p1.13.m13.1.1.3.cmml" xref="S3.SS1.p1.13.m13.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.13.m13.1c">q_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.13.m13.1d">italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. This can be formulated as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="q_{i},a_{i}=\mathcal{M}_{\theta}(\mathcal{I_{A}},\mathcal{Q},\mathcal{H}_{i})." class="ltx_Math" display="block" id="S3.E1.m1.2"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2.1" xref="S3.E1.m1.2.2.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1.2.2" xref="S3.E1.m1.2.2.1.1.2.3.cmml"><msub id="S3.E1.m1.2.2.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.2.cmml">q</mi><mi id="S3.E1.m1.2.2.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E1.m1.2.2.1.1.2.2.3" xref="S3.E1.m1.2.2.1.1.2.3.cmml">,</mo><msub id="S3.E1.m1.2.2.1.1.2.2.2" xref="S3.E1.m1.2.2.1.1.2.2.2.cmml"><mi id="S3.E1.m1.2.2.1.1.2.2.2.2" xref="S3.E1.m1.2.2.1.1.2.2.2.2.cmml">a</mi><mi id="S3.E1.m1.2.2.1.1.2.2.2.3" xref="S3.E1.m1.2.2.1.1.2.2.2.3.cmml">i</mi></msub></mrow><mo id="S3.E1.m1.2.2.1.1.5" xref="S3.E1.m1.2.2.1.1.5.cmml">=</mo><mrow id="S3.E1.m1.2.2.1.1.4" xref="S3.E1.m1.2.2.1.1.4.cmml"><msub id="S3.E1.m1.2.2.1.1.4.4" xref="S3.E1.m1.2.2.1.1.4.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.2.2.1.1.4.4.2" xref="S3.E1.m1.2.2.1.1.4.4.2.cmml">â„³</mi><mi id="S3.E1.m1.2.2.1.1.4.4.3" xref="S3.E1.m1.2.2.1.1.4.4.3.cmml">Î¸</mi></msub><mo id="S3.E1.m1.2.2.1.1.4.3" xref="S3.E1.m1.2.2.1.1.4.3.cmml">â¢</mo><mrow id="S3.E1.m1.2.2.1.1.4.2.2" xref="S3.E1.m1.2.2.1.1.4.2.3.cmml"><mo id="S3.E1.m1.2.2.1.1.4.2.2.3" stretchy="false" xref="S3.E1.m1.2.2.1.1.4.2.3.cmml">(</mo><msub id="S3.E1.m1.2.2.1.1.3.1.1.1" xref="S3.E1.m1.2.2.1.1.3.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.2.2.1.1.3.1.1.1.2" xref="S3.E1.m1.2.2.1.1.3.1.1.1.2.cmml">â„</mi><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.2.2.1.1.3.1.1.1.3" xref="S3.E1.m1.2.2.1.1.3.1.1.1.3.cmml">ğ’œ</mi></msub><mo id="S3.E1.m1.2.2.1.1.4.2.2.4" xref="S3.E1.m1.2.2.1.1.4.2.3.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">ğ’¬</mi><mo id="S3.E1.m1.2.2.1.1.4.2.2.5" xref="S3.E1.m1.2.2.1.1.4.2.3.cmml">,</mo><msub id="S3.E1.m1.2.2.1.1.4.2.2.2" xref="S3.E1.m1.2.2.1.1.4.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.2.2.1.1.4.2.2.2.2" xref="S3.E1.m1.2.2.1.1.4.2.2.2.2.cmml">â„‹</mi><mi id="S3.E1.m1.2.2.1.1.4.2.2.2.3" xref="S3.E1.m1.2.2.1.1.4.2.2.2.3.cmml">i</mi></msub><mo id="S3.E1.m1.2.2.1.1.4.2.2.6" stretchy="false" xref="S3.E1.m1.2.2.1.1.4.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.2.2.1.2" lspace="0em" xref="S3.E1.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1"><eq id="S3.E1.m1.2.2.1.1.5.cmml" xref="S3.E1.m1.2.2.1.1.5"></eq><list id="S3.E1.m1.2.2.1.1.2.3.cmml" xref="S3.E1.m1.2.2.1.1.2.2"><apply id="S3.E1.m1.2.2.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.2">ğ‘</ci><ci id="S3.E1.m1.2.2.1.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.E1.m1.2.2.1.1.2.2.2.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.2.2.1.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2.2">ğ‘</ci><ci id="S3.E1.m1.2.2.1.1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2.3">ğ‘–</ci></apply></list><apply id="S3.E1.m1.2.2.1.1.4.cmml" xref="S3.E1.m1.2.2.1.1.4"><times id="S3.E1.m1.2.2.1.1.4.3.cmml" xref="S3.E1.m1.2.2.1.1.4.3"></times><apply id="S3.E1.m1.2.2.1.1.4.4.cmml" xref="S3.E1.m1.2.2.1.1.4.4"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.4.4.1.cmml" xref="S3.E1.m1.2.2.1.1.4.4">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.4.4.2.cmml" xref="S3.E1.m1.2.2.1.1.4.4.2">â„³</ci><ci id="S3.E1.m1.2.2.1.1.4.4.3.cmml" xref="S3.E1.m1.2.2.1.1.4.4.3">ğœƒ</ci></apply><vector id="S3.E1.m1.2.2.1.1.4.2.3.cmml" xref="S3.E1.m1.2.2.1.1.4.2.2"><apply id="S3.E1.m1.2.2.1.1.3.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.3.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.3.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.3.1.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.3.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.3.1.1.1.2">â„</ci><ci id="S3.E1.m1.2.2.1.1.3.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.3.1.1.1.3">ğ’œ</ci></apply><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">ğ’¬</ci><apply id="S3.E1.m1.2.2.1.1.4.2.2.2.cmml" xref="S3.E1.m1.2.2.1.1.4.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.4.2.2.2.1.cmml" xref="S3.E1.m1.2.2.1.1.4.2.2.2">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.4.2.2.2.2.cmml" xref="S3.E1.m1.2.2.1.1.4.2.2.2.2">â„‹</ci><ci id="S3.E1.m1.2.2.1.1.4.2.2.2.3.cmml" xref="S3.E1.m1.2.2.1.1.4.2.2.2.3">ğ‘–</ci></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">q_{i},a_{i}=\mathcal{M}_{\theta}(\mathcal{I_{A}},\mathcal{Q},\mathcal{H}_{i}).</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.2d">italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = caligraphic_M start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( caligraphic_I start_POSTSUBSCRIPT caligraphic_A end_POSTSUBSCRIPT , caligraphic_Q , caligraphic_H start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p1.14">Here, the <math alttext="\mathcal{I_{A}}" class="ltx_Math" display="inline" id="S3.SS1.p1.14.m1.1"><semantics id="S3.SS1.p1.14.m1.1a"><msub id="S3.SS1.p1.14.m1.1.1" xref="S3.SS1.p1.14.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.14.m1.1.1.2" xref="S3.SS1.p1.14.m1.1.1.2.cmml">â„</mi><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.14.m1.1.1.3" xref="S3.SS1.p1.14.m1.1.1.3.cmml">ğ’œ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.14.m1.1b"><apply id="S3.SS1.p1.14.m1.1.1.cmml" xref="S3.SS1.p1.14.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.14.m1.1.1.1.cmml" xref="S3.SS1.p1.14.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.14.m1.1.1.2.cmml" xref="S3.SS1.p1.14.m1.1.1.2">â„</ci><ci id="S3.SS1.p1.14.m1.1.1.3.cmml" xref="S3.SS1.p1.14.m1.1.1.3">ğ’œ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.14.m1.1c">\mathcal{I_{A}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.14.m1.1d">caligraphic_I start_POSTSUBSCRIPT caligraphic_A end_POSTSUBSCRIPT</annotation></semantics></math> represents the instruction shown in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S3.F3" title="Figure 3 â€£ 3 Generate-then-Ground with LLMs â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_tag">3</span></a>, which includes the task demonstration and in-context learning examples.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S3.F4.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Demonstration of our batch grounding strategy with the batch size of 3 and retrieved documents amount of 10, where the LLMs ground the input question-answer pair into the second batch.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Instructional Knowledge Grounding </h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.4">Given that LLMs may generate non-factual statements or â€œhallucinationsâ€, we further guide the LLMs to revise the generated answer, <math alttext="a_{i}" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><msub id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">a</mi><mi id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">ğ‘</ci><ci id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">a_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, with the support of retrieved documents.
Specifically, in the <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><mi id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">italic_i</annotation></semantics></math>-th iteration, we initially utilize a retriever (<em class="ltx_emph ltx_font_italic" id="S3.SS2.p1.4.1">e.g.,</em> Google, BM25, or dense retrieval model) to retrieve relevant documents, <math alttext="\mathcal{D}_{i}" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3.1"><semantics id="S3.SS2.p1.3.m3.1a"><msub id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml">ğ’Ÿ</mi><mi id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2">ğ’Ÿ</ci><ci id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">\mathcal{D}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.3.m3.1d">caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, using the deduced sub-question <math alttext="q_{i}" class="ltx_Math" display="inline" id="S3.SS2.p1.4.m4.1"><semantics id="S3.SS2.p1.4.m4.1a"><msub id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml"><mi id="S3.SS2.p1.4.m4.1.1.2" xref="S3.SS2.p1.4.m4.1.1.2.cmml">q</mi><mi id="S3.SS2.p1.4.m4.1.1.3" xref="S3.SS2.p1.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><apply id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.p1.4.m4.1.1.2">ğ‘</ci><ci id="S3.SS2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">q_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.4.m4.1d">italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> (EquationÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S3.E1" title="In 3.1 Answer Deduction â€£ 3 Generate-then-Ground with LLMs â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_tag">1</span></a>):</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{D}=\text{Retrieval}(q_{i})." class="ltx_Math" display="block" id="S3.E2.m1.1"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.cmml">ğ’Ÿ</mi><mo id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.cmml"><mtext id="S3.E2.m1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.3a.cmml">Retrieval</mtext><mo id="S3.E2.m1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E2.m1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.cmml">q</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E2.m1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2.m1.1.1.1.2" lspace="0em" xref="S3.E2.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><eq id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2"></eq><ci id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3">ğ’Ÿ</ci><apply id="S3.E2.m1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"><times id="S3.E2.m1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.2"></times><ci id="S3.E2.m1.1.1.1.1.1.3a.cmml" xref="S3.E2.m1.1.1.1.1.1.3"><mtext id="S3.E2.m1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.3">Retrieval</mtext></ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2">ğ‘</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\mathcal{D}=\text{Retrieval}(q_{i}).</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.1d">caligraphic_D = Retrieval ( italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p1.9">We then guide the LLMs, <math alttext="\mathcal{M}_{\theta}" class="ltx_Math" display="inline" id="S3.SS2.p1.5.m1.1"><semantics id="S3.SS2.p1.5.m1.1a"><msub id="S3.SS2.p1.5.m1.1.1" xref="S3.SS2.p1.5.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.5.m1.1.1.2" xref="S3.SS2.p1.5.m1.1.1.2.cmml">â„³</mi><mi id="S3.SS2.p1.5.m1.1.1.3" xref="S3.SS2.p1.5.m1.1.1.3.cmml">Î¸</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m1.1b"><apply id="S3.SS2.p1.5.m1.1.1.cmml" xref="S3.SS2.p1.5.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m1.1.1.1.cmml" xref="S3.SS2.p1.5.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.5.m1.1.1.2.cmml" xref="S3.SS2.p1.5.m1.1.1.2">â„³</ci><ci id="S3.SS2.p1.5.m1.1.1.3.cmml" xref="S3.SS2.p1.5.m1.1.1.3">ğœƒ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m1.1c">\mathcal{M}_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.5.m1.1d">caligraphic_M start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT</annotation></semantics></math>, to ground the question-answer pair, <math alttext="(q_{i},a_{i})" class="ltx_Math" display="inline" id="S3.SS2.p1.6.m2.2"><semantics id="S3.SS2.p1.6.m2.2a"><mrow id="S3.SS2.p1.6.m2.2.2.2" xref="S3.SS2.p1.6.m2.2.2.3.cmml"><mo id="S3.SS2.p1.6.m2.2.2.2.3" stretchy="false" xref="S3.SS2.p1.6.m2.2.2.3.cmml">(</mo><msub id="S3.SS2.p1.6.m2.1.1.1.1" xref="S3.SS2.p1.6.m2.1.1.1.1.cmml"><mi id="S3.SS2.p1.6.m2.1.1.1.1.2" xref="S3.SS2.p1.6.m2.1.1.1.1.2.cmml">q</mi><mi id="S3.SS2.p1.6.m2.1.1.1.1.3" xref="S3.SS2.p1.6.m2.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS2.p1.6.m2.2.2.2.4" xref="S3.SS2.p1.6.m2.2.2.3.cmml">,</mo><msub id="S3.SS2.p1.6.m2.2.2.2.2" xref="S3.SS2.p1.6.m2.2.2.2.2.cmml"><mi id="S3.SS2.p1.6.m2.2.2.2.2.2" xref="S3.SS2.p1.6.m2.2.2.2.2.2.cmml">a</mi><mi id="S3.SS2.p1.6.m2.2.2.2.2.3" xref="S3.SS2.p1.6.m2.2.2.2.2.3.cmml">i</mi></msub><mo id="S3.SS2.p1.6.m2.2.2.2.5" stretchy="false" xref="S3.SS2.p1.6.m2.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m2.2b"><interval closure="open" id="S3.SS2.p1.6.m2.2.2.3.cmml" xref="S3.SS2.p1.6.m2.2.2.2"><apply id="S3.SS2.p1.6.m2.1.1.1.1.cmml" xref="S3.SS2.p1.6.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m2.1.1.1.1.1.cmml" xref="S3.SS2.p1.6.m2.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p1.6.m2.1.1.1.1.2.cmml" xref="S3.SS2.p1.6.m2.1.1.1.1.2">ğ‘</ci><ci id="S3.SS2.p1.6.m2.1.1.1.1.3.cmml" xref="S3.SS2.p1.6.m2.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.SS2.p1.6.m2.2.2.2.2.cmml" xref="S3.SS2.p1.6.m2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m2.2.2.2.2.1.cmml" xref="S3.SS2.p1.6.m2.2.2.2.2">subscript</csymbol><ci id="S3.SS2.p1.6.m2.2.2.2.2.2.cmml" xref="S3.SS2.p1.6.m2.2.2.2.2.2">ğ‘</ci><ci id="S3.SS2.p1.6.m2.2.2.2.2.3.cmml" xref="S3.SS2.p1.6.m2.2.2.2.2.3">ğ‘–</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m2.2c">(q_{i},a_{i})</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.6.m2.2d">( italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math>, in established evidence by citing the most relevant content, <math alttext="\tilde{d}" class="ltx_Math" display="inline" id="S3.SS2.p1.7.m3.1"><semantics id="S3.SS2.p1.7.m3.1a"><mover accent="true" id="S3.SS2.p1.7.m3.1.1" xref="S3.SS2.p1.7.m3.1.1.cmml"><mi id="S3.SS2.p1.7.m3.1.1.2" xref="S3.SS2.p1.7.m3.1.1.2.cmml">d</mi><mo id="S3.SS2.p1.7.m3.1.1.1" xref="S3.SS2.p1.7.m3.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m3.1b"><apply id="S3.SS2.p1.7.m3.1.1.cmml" xref="S3.SS2.p1.7.m3.1.1"><ci id="S3.SS2.p1.7.m3.1.1.1.cmml" xref="S3.SS2.p1.7.m3.1.1.1">~</ci><ci id="S3.SS2.p1.7.m3.1.1.2.cmml" xref="S3.SS2.p1.7.m3.1.1.2">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m3.1c">\tilde{d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.7.m3.1d">over~ start_ARG italic_d end_ARG</annotation></semantics></math>, from the retrieved documents, <math alttext="\mathcal{D}_{i}" class="ltx_Math" display="inline" id="S3.SS2.p1.8.m4.1"><semantics id="S3.SS2.p1.8.m4.1a"><msub id="S3.SS2.p1.8.m4.1.1" xref="S3.SS2.p1.8.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.8.m4.1.1.2" xref="S3.SS2.p1.8.m4.1.1.2.cmml">ğ’Ÿ</mi><mi id="S3.SS2.p1.8.m4.1.1.3" xref="S3.SS2.p1.8.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.8.m4.1b"><apply id="S3.SS2.p1.8.m4.1.1.cmml" xref="S3.SS2.p1.8.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.8.m4.1.1.1.cmml" xref="S3.SS2.p1.8.m4.1.1">subscript</csymbol><ci id="S3.SS2.p1.8.m4.1.1.2.cmml" xref="S3.SS2.p1.8.m4.1.1.2">ğ’Ÿ</ci><ci id="S3.SS2.p1.8.m4.1.1.3.cmml" xref="S3.SS2.p1.8.m4.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.8.m4.1c">\mathcal{D}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.8.m4.1d">caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. Subsequently, we revise the answer <math alttext="a_{i}" class="ltx_Math" display="inline" id="S3.SS2.p1.9.m5.1"><semantics id="S3.SS2.p1.9.m5.1a"><msub id="S3.SS2.p1.9.m5.1.1" xref="S3.SS2.p1.9.m5.1.1.cmml"><mi id="S3.SS2.p1.9.m5.1.1.2" xref="S3.SS2.p1.9.m5.1.1.2.cmml">a</mi><mi id="S3.SS2.p1.9.m5.1.1.3" xref="S3.SS2.p1.9.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.9.m5.1b"><apply id="S3.SS2.p1.9.m5.1.1.cmml" xref="S3.SS2.p1.9.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.9.m5.1.1.1.cmml" xref="S3.SS2.p1.9.m5.1.1">subscript</csymbol><ci id="S3.SS2.p1.9.m5.1.1.2.cmml" xref="S3.SS2.p1.9.m5.1.1.2">ğ‘</ci><ci id="S3.SS2.p1.9.m5.1.1.3.cmml" xref="S3.SS2.p1.9.m5.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.9.m5.1c">a_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.9.m5.1d">italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\tilde{a}_{i}=\mathcal{M}_{\theta}(\mathcal{I_{G}},Q,q_{i},a_{i})." class="ltx_Math" display="block" id="S3.E3.m1.2"><semantics id="S3.E3.m1.2a"><mrow id="S3.E3.m1.2.2.1" xref="S3.E3.m1.2.2.1.1.cmml"><mrow id="S3.E3.m1.2.2.1.1" xref="S3.E3.m1.2.2.1.1.cmml"><msub id="S3.E3.m1.2.2.1.1.5" xref="S3.E3.m1.2.2.1.1.5.cmml"><mover accent="true" id="S3.E3.m1.2.2.1.1.5.2" xref="S3.E3.m1.2.2.1.1.5.2.cmml"><mi id="S3.E3.m1.2.2.1.1.5.2.2" xref="S3.E3.m1.2.2.1.1.5.2.2.cmml">a</mi><mo id="S3.E3.m1.2.2.1.1.5.2.1" xref="S3.E3.m1.2.2.1.1.5.2.1.cmml">~</mo></mover><mi id="S3.E3.m1.2.2.1.1.5.3" xref="S3.E3.m1.2.2.1.1.5.3.cmml">i</mi></msub><mo id="S3.E3.m1.2.2.1.1.4" xref="S3.E3.m1.2.2.1.1.4.cmml">=</mo><mrow id="S3.E3.m1.2.2.1.1.3" xref="S3.E3.m1.2.2.1.1.3.cmml"><msub id="S3.E3.m1.2.2.1.1.3.5" xref="S3.E3.m1.2.2.1.1.3.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.2.2.1.1.3.5.2" xref="S3.E3.m1.2.2.1.1.3.5.2.cmml">â„³</mi><mi id="S3.E3.m1.2.2.1.1.3.5.3" xref="S3.E3.m1.2.2.1.1.3.5.3.cmml">Î¸</mi></msub><mo id="S3.E3.m1.2.2.1.1.3.4" xref="S3.E3.m1.2.2.1.1.3.4.cmml">â¢</mo><mrow id="S3.E3.m1.2.2.1.1.3.3.3" xref="S3.E3.m1.2.2.1.1.3.3.4.cmml"><mo id="S3.E3.m1.2.2.1.1.3.3.3.4" stretchy="false" xref="S3.E3.m1.2.2.1.1.3.3.4.cmml">(</mo><msub id="S3.E3.m1.2.2.1.1.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.2.2.1.1.1.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.2.cmml">â„</mi><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.2.2.1.1.1.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.cmml">ğ’¢</mi></msub><mo id="S3.E3.m1.2.2.1.1.3.3.3.5" xref="S3.E3.m1.2.2.1.1.3.3.4.cmml">,</mo><mi id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">Q</mi><mo id="S3.E3.m1.2.2.1.1.3.3.3.6" xref="S3.E3.m1.2.2.1.1.3.3.4.cmml">,</mo><msub id="S3.E3.m1.2.2.1.1.2.2.2.2" xref="S3.E3.m1.2.2.1.1.2.2.2.2.cmml"><mi id="S3.E3.m1.2.2.1.1.2.2.2.2.2" xref="S3.E3.m1.2.2.1.1.2.2.2.2.2.cmml">q</mi><mi id="S3.E3.m1.2.2.1.1.2.2.2.2.3" xref="S3.E3.m1.2.2.1.1.2.2.2.2.3.cmml">i</mi></msub><mo id="S3.E3.m1.2.2.1.1.3.3.3.7" xref="S3.E3.m1.2.2.1.1.3.3.4.cmml">,</mo><msub id="S3.E3.m1.2.2.1.1.3.3.3.3" xref="S3.E3.m1.2.2.1.1.3.3.3.3.cmml"><mi id="S3.E3.m1.2.2.1.1.3.3.3.3.2" xref="S3.E3.m1.2.2.1.1.3.3.3.3.2.cmml">a</mi><mi id="S3.E3.m1.2.2.1.1.3.3.3.3.3" xref="S3.E3.m1.2.2.1.1.3.3.3.3.3.cmml">i</mi></msub><mo id="S3.E3.m1.2.2.1.1.3.3.3.8" stretchy="false" xref="S3.E3.m1.2.2.1.1.3.3.4.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E3.m1.2.2.1.2" lspace="0em" xref="S3.E3.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.2b"><apply id="S3.E3.m1.2.2.1.1.cmml" xref="S3.E3.m1.2.2.1"><eq id="S3.E3.m1.2.2.1.1.4.cmml" xref="S3.E3.m1.2.2.1.1.4"></eq><apply id="S3.E3.m1.2.2.1.1.5.cmml" xref="S3.E3.m1.2.2.1.1.5"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.5.1.cmml" xref="S3.E3.m1.2.2.1.1.5">subscript</csymbol><apply id="S3.E3.m1.2.2.1.1.5.2.cmml" xref="S3.E3.m1.2.2.1.1.5.2"><ci id="S3.E3.m1.2.2.1.1.5.2.1.cmml" xref="S3.E3.m1.2.2.1.1.5.2.1">~</ci><ci id="S3.E3.m1.2.2.1.1.5.2.2.cmml" xref="S3.E3.m1.2.2.1.1.5.2.2">ğ‘</ci></apply><ci id="S3.E3.m1.2.2.1.1.5.3.cmml" xref="S3.E3.m1.2.2.1.1.5.3">ğ‘–</ci></apply><apply id="S3.E3.m1.2.2.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.3"><times id="S3.E3.m1.2.2.1.1.3.4.cmml" xref="S3.E3.m1.2.2.1.1.3.4"></times><apply id="S3.E3.m1.2.2.1.1.3.5.cmml" xref="S3.E3.m1.2.2.1.1.3.5"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.3.5.1.cmml" xref="S3.E3.m1.2.2.1.1.3.5">subscript</csymbol><ci id="S3.E3.m1.2.2.1.1.3.5.2.cmml" xref="S3.E3.m1.2.2.1.1.3.5.2">â„³</ci><ci id="S3.E3.m1.2.2.1.1.3.5.3.cmml" xref="S3.E3.m1.2.2.1.1.3.5.3">ğœƒ</ci></apply><vector id="S3.E3.m1.2.2.1.1.3.3.4.cmml" xref="S3.E3.m1.2.2.1.1.3.3.3"><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.2">â„</ci><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3">ğ’¢</ci></apply><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">ğ‘„</ci><apply id="S3.E3.m1.2.2.1.1.2.2.2.2.cmml" xref="S3.E3.m1.2.2.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.2.2.2.2.1.cmml" xref="S3.E3.m1.2.2.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E3.m1.2.2.1.1.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.1.1.2.2.2.2.2">ğ‘</ci><ci id="S3.E3.m1.2.2.1.1.2.2.2.2.3.cmml" xref="S3.E3.m1.2.2.1.1.2.2.2.2.3">ğ‘–</ci></apply><apply id="S3.E3.m1.2.2.1.1.3.3.3.3.cmml" xref="S3.E3.m1.2.2.1.1.3.3.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.3.3.3.3.1.cmml" xref="S3.E3.m1.2.2.1.1.3.3.3.3">subscript</csymbol><ci id="S3.E3.m1.2.2.1.1.3.3.3.3.2.cmml" xref="S3.E3.m1.2.2.1.1.3.3.3.3.2">ğ‘</ci><ci id="S3.E3.m1.2.2.1.1.3.3.3.3.3.cmml" xref="S3.E3.m1.2.2.1.1.3.3.3.3.3">ğ‘–</ci></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.2c">\tilde{a}_{i}=\mathcal{M}_{\theta}(\mathcal{I_{G}},Q,q_{i},a_{i}).</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.2d">over~ start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = caligraphic_M start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( caligraphic_I start_POSTSUBSCRIPT caligraphic_G end_POSTSUBSCRIPT , italic_Q , italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p1.14">As depicted in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S3.F3" title="Figure 3 â€£ 3 Generate-then-Ground with LLMs â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_tag">3</span></a>(b), <math alttext="\mathcal{I_{G}}" class="ltx_Math" display="inline" id="S3.SS2.p1.10.m1.1"><semantics id="S3.SS2.p1.10.m1.1a"><msub id="S3.SS2.p1.10.m1.1.1" xref="S3.SS2.p1.10.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.10.m1.1.1.2" xref="S3.SS2.p1.10.m1.1.1.2.cmml">â„</mi><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.10.m1.1.1.3" xref="S3.SS2.p1.10.m1.1.1.3.cmml">ğ’¢</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.10.m1.1b"><apply id="S3.SS2.p1.10.m1.1.1.cmml" xref="S3.SS2.p1.10.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.10.m1.1.1.1.cmml" xref="S3.SS2.p1.10.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.10.m1.1.1.2.cmml" xref="S3.SS2.p1.10.m1.1.1.2">â„</ci><ci id="S3.SS2.p1.10.m1.1.1.3.cmml" xref="S3.SS2.p1.10.m1.1.1.3">ğ’¢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.10.m1.1c">\mathcal{I_{G}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.10.m1.1d">caligraphic_I start_POSTSUBSCRIPT caligraphic_G end_POSTSUBSCRIPT</annotation></semantics></math> represents our grounding instruction in a zero-shot setting, while <math alttext="\tilde{a}_{i}" class="ltx_Math" display="inline" id="S3.SS2.p1.11.m2.1"><semantics id="S3.SS2.p1.11.m2.1a"><msub id="S3.SS2.p1.11.m2.1.1" xref="S3.SS2.p1.11.m2.1.1.cmml"><mover accent="true" id="S3.SS2.p1.11.m2.1.1.2" xref="S3.SS2.p1.11.m2.1.1.2.cmml"><mi id="S3.SS2.p1.11.m2.1.1.2.2" xref="S3.SS2.p1.11.m2.1.1.2.2.cmml">a</mi><mo id="S3.SS2.p1.11.m2.1.1.2.1" xref="S3.SS2.p1.11.m2.1.1.2.1.cmml">~</mo></mover><mi id="S3.SS2.p1.11.m2.1.1.3" xref="S3.SS2.p1.11.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.11.m2.1b"><apply id="S3.SS2.p1.11.m2.1.1.cmml" xref="S3.SS2.p1.11.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.11.m2.1.1.1.cmml" xref="S3.SS2.p1.11.m2.1.1">subscript</csymbol><apply id="S3.SS2.p1.11.m2.1.1.2.cmml" xref="S3.SS2.p1.11.m2.1.1.2"><ci id="S3.SS2.p1.11.m2.1.1.2.1.cmml" xref="S3.SS2.p1.11.m2.1.1.2.1">~</ci><ci id="S3.SS2.p1.11.m2.1.1.2.2.cmml" xref="S3.SS2.p1.11.m2.1.1.2.2">ğ‘</ci></apply><ci id="S3.SS2.p1.11.m2.1.1.3.cmml" xref="S3.SS2.p1.11.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.11.m2.1c">\tilde{a}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.11.m2.1d">over~ start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the revision trajectory, which includes the cited evidence and the revised answer.
The revision trajectory <math alttext="\tilde{a}_{i}" class="ltx_Math" display="inline" id="S3.SS2.p1.12.m3.1"><semantics id="S3.SS2.p1.12.m3.1a"><msub id="S3.SS2.p1.12.m3.1.1" xref="S3.SS2.p1.12.m3.1.1.cmml"><mover accent="true" id="S3.SS2.p1.12.m3.1.1.2" xref="S3.SS2.p1.12.m3.1.1.2.cmml"><mi id="S3.SS2.p1.12.m3.1.1.2.2" xref="S3.SS2.p1.12.m3.1.1.2.2.cmml">a</mi><mo id="S3.SS2.p1.12.m3.1.1.2.1" xref="S3.SS2.p1.12.m3.1.1.2.1.cmml">~</mo></mover><mi id="S3.SS2.p1.12.m3.1.1.3" xref="S3.SS2.p1.12.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.12.m3.1b"><apply id="S3.SS2.p1.12.m3.1.1.cmml" xref="S3.SS2.p1.12.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.12.m3.1.1.1.cmml" xref="S3.SS2.p1.12.m3.1.1">subscript</csymbol><apply id="S3.SS2.p1.12.m3.1.1.2.cmml" xref="S3.SS2.p1.12.m3.1.1.2"><ci id="S3.SS2.p1.12.m3.1.1.2.1.cmml" xref="S3.SS2.p1.12.m3.1.1.2.1">~</ci><ci id="S3.SS2.p1.12.m3.1.1.2.2.cmml" xref="S3.SS2.p1.12.m3.1.1.2.2">ğ‘</ci></apply><ci id="S3.SS2.p1.12.m3.1.1.3.cmml" xref="S3.SS2.p1.12.m3.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.12.m3.1c">\tilde{a}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.12.m3.1d">over~ start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, along with the question <math alttext="q_{i}" class="ltx_Math" display="inline" id="S3.SS2.p1.13.m4.1"><semantics id="S3.SS2.p1.13.m4.1a"><msub id="S3.SS2.p1.13.m4.1.1" xref="S3.SS2.p1.13.m4.1.1.cmml"><mi id="S3.SS2.p1.13.m4.1.1.2" xref="S3.SS2.p1.13.m4.1.1.2.cmml">q</mi><mi id="S3.SS2.p1.13.m4.1.1.3" xref="S3.SS2.p1.13.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.13.m4.1b"><apply id="S3.SS2.p1.13.m4.1.1.cmml" xref="S3.SS2.p1.13.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.13.m4.1.1.1.cmml" xref="S3.SS2.p1.13.m4.1.1">subscript</csymbol><ci id="S3.SS2.p1.13.m4.1.1.2.cmml" xref="S3.SS2.p1.13.m4.1.1.2">ğ‘</ci><ci id="S3.SS2.p1.13.m4.1.1.3.cmml" xref="S3.SS2.p1.13.m4.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.13.m4.1c">q_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.13.m4.1d">italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, are then combined to build the context, <math alttext="\mathcal{H}_{i+1}=\mathcal{H}_{i}\cup\{(q_{i},\tilde{a}_{i})\}" class="ltx_Math" display="inline" id="S3.SS2.p1.14.m5.1"><semantics id="S3.SS2.p1.14.m5.1a"><mrow id="S3.SS2.p1.14.m5.1.1" xref="S3.SS2.p1.14.m5.1.1.cmml"><msub id="S3.SS2.p1.14.m5.1.1.3" xref="S3.SS2.p1.14.m5.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.14.m5.1.1.3.2" xref="S3.SS2.p1.14.m5.1.1.3.2.cmml">â„‹</mi><mrow id="S3.SS2.p1.14.m5.1.1.3.3" xref="S3.SS2.p1.14.m5.1.1.3.3.cmml"><mi id="S3.SS2.p1.14.m5.1.1.3.3.2" xref="S3.SS2.p1.14.m5.1.1.3.3.2.cmml">i</mi><mo id="S3.SS2.p1.14.m5.1.1.3.3.1" xref="S3.SS2.p1.14.m5.1.1.3.3.1.cmml">+</mo><mn id="S3.SS2.p1.14.m5.1.1.3.3.3" xref="S3.SS2.p1.14.m5.1.1.3.3.3.cmml">1</mn></mrow></msub><mo id="S3.SS2.p1.14.m5.1.1.2" xref="S3.SS2.p1.14.m5.1.1.2.cmml">=</mo><mrow id="S3.SS2.p1.14.m5.1.1.1" xref="S3.SS2.p1.14.m5.1.1.1.cmml"><msub id="S3.SS2.p1.14.m5.1.1.1.3" xref="S3.SS2.p1.14.m5.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.14.m5.1.1.1.3.2" xref="S3.SS2.p1.14.m5.1.1.1.3.2.cmml">â„‹</mi><mi id="S3.SS2.p1.14.m5.1.1.1.3.3" xref="S3.SS2.p1.14.m5.1.1.1.3.3.cmml">i</mi></msub><mo id="S3.SS2.p1.14.m5.1.1.1.2" xref="S3.SS2.p1.14.m5.1.1.1.2.cmml">âˆª</mo><mrow id="S3.SS2.p1.14.m5.1.1.1.1.1" xref="S3.SS2.p1.14.m5.1.1.1.1.2.cmml"><mo id="S3.SS2.p1.14.m5.1.1.1.1.1.2" stretchy="false" xref="S3.SS2.p1.14.m5.1.1.1.1.2.cmml">{</mo><mrow id="S3.SS2.p1.14.m5.1.1.1.1.1.1.2" xref="S3.SS2.p1.14.m5.1.1.1.1.1.1.3.cmml"><mo id="S3.SS2.p1.14.m5.1.1.1.1.1.1.2.3" stretchy="false" xref="S3.SS2.p1.14.m5.1.1.1.1.1.1.3.cmml">(</mo><msub id="S3.SS2.p1.14.m5.1.1.1.1.1.1.1.1" xref="S3.SS2.p1.14.m5.1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p1.14.m5.1.1.1.1.1.1.1.1.2" xref="S3.SS2.p1.14.m5.1.1.1.1.1.1.1.1.2.cmml">q</mi><mi id="S3.SS2.p1.14.m5.1.1.1.1.1.1.1.1.3" xref="S3.SS2.p1.14.m5.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS2.p1.14.m5.1.1.1.1.1.1.2.4" xref="S3.SS2.p1.14.m5.1.1.1.1.1.1.3.cmml">,</mo><msub id="S3.SS2.p1.14.m5.1.1.1.1.1.1.2.2" xref="S3.SS2.p1.14.m5.1.1.1.1.1.1.2.2.cmml"><mover accent="true" id="S3.SS2.p1.14.m5.1.1.1.1.1.1.2.2.2" xref="S3.SS2.p1.14.m5.1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.SS2.p1.14.m5.1.1.1.1.1.1.2.2.2.2" xref="S3.SS2.p1.14.m5.1.1.1.1.1.1.2.2.2.2.cmml">a</mi><mo id="S3.SS2.p1.14.m5.1.1.1.1.1.1.2.2.2.1" xref="S3.SS2.p1.14.m5.1.1.1.1.1.1.2.2.2.1.cmml">~</mo></mover><mi id="S3.SS2.p1.14.m5.1.1.1.1.1.1.2.2.3" xref="S3.SS2.p1.14.m5.1.1.1.1.1.1.2.2.3.cmml">i</mi></msub><mo id="S3.SS2.p1.14.m5.1.1.1.1.1.1.2.5" stretchy="false" xref="S3.SS2.p1.14.m5.1.1.1.1.1.1.3.cmml">)</mo></mrow><mo id="S3.SS2.p1.14.m5.1.1.1.1.1.3" stretchy="false" xref="S3.SS2.p1.14.m5.1.1.1.1.2.cmml">}</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.14.m5.1b"><apply id="S3.SS2.p1.14.m5.1.1.cmml" xref="S3.SS2.p1.14.m5.1.1"><eq id="S3.SS2.p1.14.m5.1.1.2.cmml" xref="S3.SS2.p1.14.m5.1.1.2"></eq><apply id="S3.SS2.p1.14.m5.1.1.3.cmml" xref="S3.SS2.p1.14.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.14.m5.1.1.3.1.cmml" xref="S3.SS2.p1.14.m5.1.1.3">subscript</csymbol><ci id="S3.SS2.p1.14.m5.1.1.3.2.cmml" xref="S3.SS2.p1.14.m5.1.1.3.2">â„‹</ci><apply id="S3.SS2.p1.14.m5.1.1.3.3.cmml" xref="S3.SS2.p1.14.m5.1.1.3.3"><plus id="S3.SS2.p1.14.m5.1.1.3.3.1.cmml" xref="S3.SS2.p1.14.m5.1.1.3.3.1"></plus><ci id="S3.SS2.p1.14.m5.1.1.3.3.2.cmml" xref="S3.SS2.p1.14.m5.1.1.3.3.2">ğ‘–</ci><cn id="S3.SS2.p1.14.m5.1.1.3.3.3.cmml" type="integer" xref="S3.SS2.p1.14.m5.1.1.3.3.3">1</cn></apply></apply><apply id="S3.SS2.p1.14.m5.1.1.1.cmml" xref="S3.SS2.p1.14.m5.1.1.1"><union id="S3.SS2.p1.14.m5.1.1.1.2.cmml" xref="S3.SS2.p1.14.m5.1.1.1.2"></union><apply id="S3.SS2.p1.14.m5.1.1.1.3.cmml" xref="S3.SS2.p1.14.m5.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.14.m5.1.1.1.3.1.cmml" xref="S3.SS2.p1.14.m5.1.1.1.3">subscript</csymbol><ci id="S3.SS2.p1.14.m5.1.1.1.3.2.cmml" xref="S3.SS2.p1.14.m5.1.1.1.3.2">â„‹</ci><ci id="S3.SS2.p1.14.m5.1.1.1.3.3.cmml" xref="S3.SS2.p1.14.m5.1.1.1.3.3">ğ‘–</ci></apply><set id="S3.SS2.p1.14.m5.1.1.1.1.2.cmml" xref="S3.SS2.p1.14.m5.1.1.1.1.1"><interval closure="open" id="S3.SS2.p1.14.m5.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p1.14.m5.1.1.1.1.1.1.2"><apply id="S3.SS2.p1.14.m5.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.14.m5.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.14.m5.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.14.m5.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p1.14.m5.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p1.14.m5.1.1.1.1.1.1.1.1.2">ğ‘</ci><ci id="S3.SS2.p1.14.m5.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p1.14.m5.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.SS2.p1.14.m5.1.1.1.1.1.1.2.2.cmml" xref="S3.SS2.p1.14.m5.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.14.m5.1.1.1.1.1.1.2.2.1.cmml" xref="S3.SS2.p1.14.m5.1.1.1.1.1.1.2.2">subscript</csymbol><apply id="S3.SS2.p1.14.m5.1.1.1.1.1.1.2.2.2.cmml" xref="S3.SS2.p1.14.m5.1.1.1.1.1.1.2.2.2"><ci id="S3.SS2.p1.14.m5.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.SS2.p1.14.m5.1.1.1.1.1.1.2.2.2.1">~</ci><ci id="S3.SS2.p1.14.m5.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.SS2.p1.14.m5.1.1.1.1.1.1.2.2.2.2">ğ‘</ci></apply><ci id="S3.SS2.p1.14.m5.1.1.1.1.1.1.2.2.3.cmml" xref="S3.SS2.p1.14.m5.1.1.1.1.1.1.2.2.3">ğ‘–</ci></apply></interval></set></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.14.m5.1c">\mathcal{H}_{i+1}=\mathcal{H}_{i}\cup\{(q_{i},\tilde{a}_{i})\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.14.m5.1d">caligraphic_H start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT = caligraphic_H start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âˆª { ( italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , over~ start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) }</annotation></semantics></math>, for the LLMs in the subsequent iteration.
If no relevant content can be cited (<em class="ltx_emph ltx_font_italic" id="S3.SS2.p1.14.1">e.g.,</em> the citation is <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p1.14.2">Empty</span>), we keep the generated answer as the revised answer without any changes.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Batch Knowledge Ground</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.4">Since retrieved documents are typically lengthy and contain inevitable noiseÂ <cite class="ltx_cite ltx_citemacro_citep">(Xu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib40" title="">2024</a>)</cite>,
the LLMs are susceptible to being misled by plausible-looking statements during the grounding phaseÂ <cite class="ltx_cite ltx_citemacro_citep">(Sun etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib29" title="">2023a</a>; Thakur etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib32" title="">2023a</a>)</cite>.
Therefore, we propose a simple yet efficient
batch grounding strategy.
Suppose the batch size is <math alttext="b" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">b</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">italic_b</annotation></semantics></math>. We first utilize the LLMs to revise the generated answer <math alttext="a_{i}" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2.1"><semantics id="S3.SS3.p1.2.m2.1a"><msub id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml"><mi id="S3.SS3.p1.2.m2.1.1.2" xref="S3.SS3.p1.2.m2.1.1.2.cmml">a</mi><mi id="S3.SS3.p1.2.m2.1.1.3" xref="S3.SS3.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2">ğ‘</ci><ci id="S3.SS3.p1.2.m2.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">a_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m2.1d">italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> using the <math alttext="(1,b)" class="ltx_Math" display="inline" id="S3.SS3.p1.3.m3.2"><semantics id="S3.SS3.p1.3.m3.2a"><mrow id="S3.SS3.p1.3.m3.2.3.2" xref="S3.SS3.p1.3.m3.2.3.1.cmml"><mo id="S3.SS3.p1.3.m3.2.3.2.1" stretchy="false" xref="S3.SS3.p1.3.m3.2.3.1.cmml">(</mo><mn id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml">1</mn><mo id="S3.SS3.p1.3.m3.2.3.2.2" xref="S3.SS3.p1.3.m3.2.3.1.cmml">,</mo><mi id="S3.SS3.p1.3.m3.2.2" xref="S3.SS3.p1.3.m3.2.2.cmml">b</mi><mo id="S3.SS3.p1.3.m3.2.3.2.3" stretchy="false" xref="S3.SS3.p1.3.m3.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.2b"><interval closure="open" id="S3.SS3.p1.3.m3.2.3.1.cmml" xref="S3.SS3.p1.3.m3.2.3.2"><cn id="S3.SS3.p1.3.m3.1.1.cmml" type="integer" xref="S3.SS3.p1.3.m3.1.1">1</cn><ci id="S3.SS3.p1.3.m3.2.2.cmml" xref="S3.SS3.p1.3.m3.2.2">ğ‘</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.2c">(1,b)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.3.m3.2d">( 1 , italic_b )</annotation></semantics></math>-th documents.
If relevant evidence can be cited to revise the answer, we end our grounding phase for the current iteration and move to the next iteration.
Otherwise, we prompt the LLMs to generate an â€œEmptyâ€ signal and then access the <math alttext="(b+1,2b)" class="ltx_Math" display="inline" id="S3.SS3.p1.4.m4.2"><semantics id="S3.SS3.p1.4.m4.2a"><mrow id="S3.SS3.p1.4.m4.2.2.2" xref="S3.SS3.p1.4.m4.2.2.3.cmml"><mo id="S3.SS3.p1.4.m4.2.2.2.3" stretchy="false" xref="S3.SS3.p1.4.m4.2.2.3.cmml">(</mo><mrow id="S3.SS3.p1.4.m4.1.1.1.1" xref="S3.SS3.p1.4.m4.1.1.1.1.cmml"><mi id="S3.SS3.p1.4.m4.1.1.1.1.2" xref="S3.SS3.p1.4.m4.1.1.1.1.2.cmml">b</mi><mo id="S3.SS3.p1.4.m4.1.1.1.1.1" xref="S3.SS3.p1.4.m4.1.1.1.1.1.cmml">+</mo><mn id="S3.SS3.p1.4.m4.1.1.1.1.3" xref="S3.SS3.p1.4.m4.1.1.1.1.3.cmml">1</mn></mrow><mo id="S3.SS3.p1.4.m4.2.2.2.4" xref="S3.SS3.p1.4.m4.2.2.3.cmml">,</mo><mrow id="S3.SS3.p1.4.m4.2.2.2.2" xref="S3.SS3.p1.4.m4.2.2.2.2.cmml"><mn id="S3.SS3.p1.4.m4.2.2.2.2.2" xref="S3.SS3.p1.4.m4.2.2.2.2.2.cmml">2</mn><mo id="S3.SS3.p1.4.m4.2.2.2.2.1" xref="S3.SS3.p1.4.m4.2.2.2.2.1.cmml">â¢</mo><mi id="S3.SS3.p1.4.m4.2.2.2.2.3" xref="S3.SS3.p1.4.m4.2.2.2.2.3.cmml">b</mi></mrow><mo id="S3.SS3.p1.4.m4.2.2.2.5" stretchy="false" xref="S3.SS3.p1.4.m4.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.2b"><interval closure="open" id="S3.SS3.p1.4.m4.2.2.3.cmml" xref="S3.SS3.p1.4.m4.2.2.2"><apply id="S3.SS3.p1.4.m4.1.1.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1.1.1"><plus id="S3.SS3.p1.4.m4.1.1.1.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1.1.1.1"></plus><ci id="S3.SS3.p1.4.m4.1.1.1.1.2.cmml" xref="S3.SS3.p1.4.m4.1.1.1.1.2">ğ‘</ci><cn id="S3.SS3.p1.4.m4.1.1.1.1.3.cmml" type="integer" xref="S3.SS3.p1.4.m4.1.1.1.1.3">1</cn></apply><apply id="S3.SS3.p1.4.m4.2.2.2.2.cmml" xref="S3.SS3.p1.4.m4.2.2.2.2"><times id="S3.SS3.p1.4.m4.2.2.2.2.1.cmml" xref="S3.SS3.p1.4.m4.2.2.2.2.1"></times><cn id="S3.SS3.p1.4.m4.2.2.2.2.2.cmml" type="integer" xref="S3.SS3.p1.4.m4.2.2.2.2.2">2</cn><ci id="S3.SS3.p1.4.m4.2.2.2.2.3.cmml" xref="S3.SS3.p1.4.m4.2.2.2.2.3">ğ‘</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.2c">(b+1,2b)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.4.m4.2d">( italic_b + 1 , 2 italic_b )</annotation></semantics></math>-th documents sequentially.
This process continues until the relevant evidence can be found to support our grounding phase.
FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S3.F4" title="Figure 4 â€£ 3.1 Answer Deduction â€£ 3 Generate-then-Ground with LLMs â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_tag">4</span></a> shows a concrete example with ten retrieved documents.
If no relevant document can be found, we directly output the generated answer as a backup.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Generalization with Grounding Distillation</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">While LLMs like ChatGPT are skilled and adept at following instructions, they are often considered black boxesÂ <cite class="ltx_cite ltx_citemacro_citep">(Qin etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib21" title="">2023</a>; Gao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib7" title="">2024</a>)</cite> and their extensive parameters can increase latency and inference cost in real-world applicationsÂ <cite class="ltx_cite ltx_citemacro_citep">(Sun etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib30" title="">2023b</a>)</cite>. Thus, we aim to adapt our framework to smaller, open source models with fewer parameters. Initial experiments show these smaller models struggle to cite relevant evidence during the knowledge grounding phase. To overcome this, we introduce <span class="ltx_text ltx_font_bold" id="S4.p1.1.1">I</span>nstructional <span class="ltx_text ltx_font_bold" id="S4.p1.1.2">G</span>rounding <span class="ltx_text ltx_font_bold" id="S4.p1.1.3">Di</span>stillation (IDG), which distills the output trajectory of ChatGPT into a smaller student model.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Synthesize the Training Dataset</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.8">The instructional grounding distillation collects the trajectory of LLMs, <em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.8.1">i.e.,</em> ChatGPT, during the instructional knowledge grounding (SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S3.SS2" title="3.2 Instructional Knowledge Grounding â€£ 3 Generate-then-Ground with LLMs â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_tag">3.2</span></a>). This trajectory is then used as the training dataset to distill the grounding capability into a student model.
To achieve this, we first sample 50k questions from the Natural Questions (NQ) datasetÂ <cite class="ltx_cite ltx_citemacro_citep">(Kwiatkowski etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib15" title="">2019</a>)</cite>.
Each question <math alttext="q" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><mi id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">q</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">italic_q</annotation></semantics></math> is paired with a corresponding ground-truth document <math alttext="\tilde{d}" class="ltx_Math" display="inline" id="S4.SS1.p1.2.m2.1"><semantics id="S4.SS1.p1.2.m2.1a"><mover accent="true" id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml"><mi id="S4.SS1.p1.2.m2.1.1.2" xref="S4.SS1.p1.2.m2.1.1.2.cmml">d</mi><mo id="S4.SS1.p1.2.m2.1.1.1" xref="S4.SS1.p1.2.m2.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><apply id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1"><ci id="S4.SS1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1.1">~</ci><ci id="S4.SS1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.p1.2.m2.1.1.2">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">\tilde{d}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.2.m2.1d">over~ start_ARG italic_d end_ARG</annotation></semantics></math> and the noise documents <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S4.SS1.p1.3.m3.1"><semantics id="S4.SS1.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml">ğ’Ÿ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><ci id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1">ğ’Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">\mathcal{D}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.3.m3.1d">caligraphic_D</annotation></semantics></math>.
The questions in the NQ dataset are of high quality and single-hop, making them inherently similar to the setting of our instruction knowledge grounding (SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S3.SS2" title="3.2 Instructional Knowledge Grounding â€£ 3 Generate-then-Ground with LLMs â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_tag">3.2</span></a>).
Next, we supplement each question with an immediate answer <math alttext="a" class="ltx_Math" display="inline" id="S4.SS1.p1.4.m4.1"><semantics id="S4.SS1.p1.4.m4.1a"><mi id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><ci id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">a</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.4.m4.1d">italic_a</annotation></semantics></math> and a detailed revision trajectory <math alttext="\tilde{a}" class="ltx_Math" display="inline" id="S4.SS1.p1.5.m5.1"><semantics id="S4.SS1.p1.5.m5.1a"><mover accent="true" id="S4.SS1.p1.5.m5.1.1" xref="S4.SS1.p1.5.m5.1.1.cmml"><mi id="S4.SS1.p1.5.m5.1.1.2" xref="S4.SS1.p1.5.m5.1.1.2.cmml">a</mi><mo id="S4.SS1.p1.5.m5.1.1.1" xref="S4.SS1.p1.5.m5.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.5.m5.1b"><apply id="S4.SS1.p1.5.m5.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1"><ci id="S4.SS1.p1.5.m5.1.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1.1">~</ci><ci id="S4.SS1.p1.5.m5.1.1.2.cmml" xref="S4.SS1.p1.5.m5.1.1.2">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.5.m5.1c">\tilde{a}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.5.m5.1d">over~ start_ARG italic_a end_ARG</annotation></semantics></math>.
Specifically, the immediate answer <math alttext="a" class="ltx_Math" display="inline" id="S4.SS1.p1.6.m6.1"><semantics id="S4.SS1.p1.6.m6.1a"><mi id="S4.SS1.p1.6.m6.1.1" xref="S4.SS1.p1.6.m6.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.6.m6.1b"><ci id="S4.SS1.p1.6.m6.1.1.cmml" xref="S4.SS1.p1.6.m6.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.6.m6.1c">a</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.6.m6.1d">italic_a</annotation></semantics></math> is generated directly by feeding the question <math alttext="q" class="ltx_Math" display="inline" id="S4.SS1.p1.7.m7.1"><semantics id="S4.SS1.p1.7.m7.1a"><mi id="S4.SS1.p1.7.m7.1.1" xref="S4.SS1.p1.7.m7.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.7.m7.1b"><ci id="S4.SS1.p1.7.m7.1.1.cmml" xref="S4.SS1.p1.7.m7.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.7.m7.1c">q</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.7.m7.1d">italic_q</annotation></semantics></math> into a smaller model, such as Mistral-7BÂ <cite class="ltx_cite ltx_citemacro_citep">(Jiang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib11" title="">2023a</a>)</cite>.
The revision trajectory <math alttext="\tilde{a}" class="ltx_Math" display="inline" id="S4.SS1.p1.8.m8.1"><semantics id="S4.SS1.p1.8.m8.1a"><mover accent="true" id="S4.SS1.p1.8.m8.1.1" xref="S4.SS1.p1.8.m8.1.1.cmml"><mi id="S4.SS1.p1.8.m8.1.1.2" xref="S4.SS1.p1.8.m8.1.1.2.cmml">a</mi><mo id="S4.SS1.p1.8.m8.1.1.1" xref="S4.SS1.p1.8.m8.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.8.m8.1b"><apply id="S4.SS1.p1.8.m8.1.1.cmml" xref="S4.SS1.p1.8.m8.1.1"><ci id="S4.SS1.p1.8.m8.1.1.1.cmml" xref="S4.SS1.p1.8.m8.1.1.1">~</ci><ci id="S4.SS1.p1.8.m8.1.1.2.cmml" xref="S4.SS1.p1.8.m8.1.1.2">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.8.m8.1c">\tilde{a}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.8.m8.1d">over~ start_ARG italic_a end_ARG</annotation></semantics></math> is generated by ChatGPT with the assistance of the ground truth document.
Various heuristic methods are also used to filter low-quality output (see Appendix III for more details).
The statistics of out synthetic dataset is provided in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S4.T1" title="Table 1 â€£ 4.1 Synthesize the Training Dataset â€£ 4 Generalization with Grounding Distillation â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<p class="ltx_p ltx_align_center" id="S4.T1.1"><span class="ltx_text ltx_inline-block" id="S4.T1.1.1" style="width:433.6pt;">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T1.1.1.1" style="width:282.7pt;height:108pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S4.T1.1.1.1.1"><span class="ltx_text" id="S4.T1.1.1.1.1.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.1.1.1.1">
<span class="ltx_tr" id="S4.T1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S4.T1.1.1.1.1.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.1.1.1.1.1.1.1">
<span class="ltx_p" id="S4.T1.1.1.1.1.1.1.1.1.1.1" style="width:227.6pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.1.1.1.1.1.1.1">Statistic</span></span>
</span></span>
<span class="ltx_td ltx_border_tt" id="S4.T1.1.1.1.1.1.1.1.2"></span></span>
<span class="ltx_tr" id="S4.T1.1.1.1.1.1.1.2">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T1.1.1.1.1.1.1.2.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.1.1.1.1.2.1.1">
<span class="ltx_p" id="S4.T1.1.1.1.1.1.1.2.1.1.1" style="width:227.6pt;"># The data scale</span>
</span></span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.1.1.1.1.1.1.2.2">45,710</span></span>
<span class="ltx_tr" id="S4.T1.1.1.1.1.1.1.3">
<span class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.1.1.1.1.1.1.3.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.1.1.1.1.3.1.1">
<span class="ltx_p" id="S4.T1.1.1.1.1.1.1.3.1.1.1" style="width:227.6pt;"># The average length of input instruction</span>
</span></span>
<span class="ltx_td ltx_align_right" id="S4.T1.1.1.1.1.1.1.3.2">70.87</span></span>
<span class="ltx_tr" id="S4.T1.1.1.1.1.1.1.4">
<span class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.1.1.1.1.1.1.4.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.1.1.1.1.4.1.1">
<span class="ltx_p" id="S4.T1.1.1.1.1.1.1.4.1.1.1" style="width:227.6pt;"># The average length of output</span>
</span></span>
<span class="ltx_td ltx_align_right" id="S4.T1.1.1.1.1.1.1.4.2">683.21</span></span>
<span class="ltx_tr" id="S4.T1.1.1.1.1.1.1.5">
<span class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.1.1.1.1.1.1.5.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.1.1.1.1.5.1.1">
<span class="ltx_p" id="S4.T1.1.1.1.1.1.1.5.1.1.1" style="width:227.6pt;"># The average number of ground truth documents</span>
</span></span>
<span class="ltx_td ltx_align_right" id="S4.T1.1.1.1.1.1.1.5.2">1.00</span></span>
<span class="ltx_tr" id="S4.T1.1.1.1.1.1.1.6">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T1.1.1.1.1.1.1.6.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.1.1.1.1.6.1.1">
<span class="ltx_p" id="S4.T1.1.1.1.1.1.1.6.1.1.1" style="width:227.6pt;"># The average length of ground truth documents</span>
</span></span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.1.1.1.1.1.1.6.2">117.57</span></span>
</span></span></span>
</span></span></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>The statistics of our synthetic dataset in the instructional grounding distillation method.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Training Objective</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.4">Formally, for each question <math alttext="q" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">q</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">italic_q</annotation></semantics></math> in our synthetic dataset, we train the model to cite the relevant content from a document list and revise any incorrect predictions in the immediate answer <math alttext="a" class="ltx_Math" display="inline" id="S4.SS2.p1.2.m2.1"><semantics id="S4.SS2.p1.2.m2.1a"><mi id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><ci id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">a</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.2.m2.1d">italic_a</annotation></semantics></math> following the instruction <math alttext="\mathcal{I_{G}}" class="ltx_Math" display="inline" id="S4.SS2.p1.3.m3.1"><semantics id="S4.SS2.p1.3.m3.1a"><msub id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p1.3.m3.1.1.2" xref="S4.SS2.p1.3.m3.1.1.2.cmml">â„</mi><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p1.3.m3.1.1.3" xref="S4.SS2.p1.3.m3.1.1.3.cmml">ğ’¢</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><apply id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.3.m3.1.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS2.p1.3.m3.1.1.2.cmml" xref="S4.SS2.p1.3.m3.1.1.2">â„</ci><ci id="S4.SS2.p1.3.m3.1.1.3.cmml" xref="S4.SS2.p1.3.m3.1.1.3">ğ’¢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">\mathcal{I_{G}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.3.m3.1d">caligraphic_I start_POSTSUBSCRIPT caligraphic_G end_POSTSUBSCRIPT</annotation></semantics></math>.
Using the collected revision trajectory <math alttext="\tilde{a}" class="ltx_Math" display="inline" id="S4.SS2.p1.4.m4.1"><semantics id="S4.SS2.p1.4.m4.1a"><mover accent="true" id="S4.SS2.p1.4.m4.1.1" xref="S4.SS2.p1.4.m4.1.1.cmml"><mi id="S4.SS2.p1.4.m4.1.1.2" xref="S4.SS2.p1.4.m4.1.1.2.cmml">a</mi><mo id="S4.SS2.p1.4.m4.1.1.1" xref="S4.SS2.p1.4.m4.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.m4.1b"><apply id="S4.SS2.p1.4.m4.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1"><ci id="S4.SS2.p1.4.m4.1.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1.1">~</ci><ci id="S4.SS2.p1.4.m4.1.1.2.cmml" xref="S4.SS2.p1.4.m4.1.1.2">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m4.1c">\tilde{a}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.4.m4.1d">over~ start_ARG italic_a end_ARG</annotation></semantics></math>,
we apply the standard language modeling loss to optimize the student model:</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S4.E4">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E4X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{L_{G}}" class="ltx_Math" display="inline" id="S4.E4X.2.1.1.m1.1"><semantics id="S4.E4X.2.1.1.m1.1a"><msub id="S4.E4X.2.1.1.m1.1.1" xref="S4.E4X.2.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E4X.2.1.1.m1.1.1.2" xref="S4.E4X.2.1.1.m1.1.1.2.cmml">â„’</mi><mi class="ltx_font_mathcaligraphic" id="S4.E4X.2.1.1.m1.1.1.3" xref="S4.E4X.2.1.1.m1.1.1.3.cmml">ğ’¢</mi></msub><annotation-xml encoding="MathML-Content" id="S4.E4X.2.1.1.m1.1b"><apply id="S4.E4X.2.1.1.m1.1.1.cmml" xref="S4.E4X.2.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.E4X.2.1.1.m1.1.1.1.cmml" xref="S4.E4X.2.1.1.m1.1.1">subscript</csymbol><ci id="S4.E4X.2.1.1.m1.1.1.2.cmml" xref="S4.E4X.2.1.1.m1.1.1.2">â„’</ci><ci id="S4.E4X.2.1.1.m1.1.1.3.cmml" xref="S4.E4X.2.1.1.m1.1.1.3">ğ’¢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4X.2.1.1.m1.1c">\displaystyle\mathcal{L_{G}}</annotation><annotation encoding="application/x-llamapun" id="S4.E4X.2.1.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT caligraphic_G end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=-\log P_{\theta}(\tilde{a}|\mathcal{I_{G}},\{\tilde{d}\}\cup%
\mathcal{D})" class="ltx_Math" display="inline" id="S4.E4X.3.2.2.m1.2"><semantics id="S4.E4X.3.2.2.m1.2a"><mrow id="S4.E4X.3.2.2.m1.2.2" xref="S4.E4X.3.2.2.m1.2.2.cmml"><mi id="S4.E4X.3.2.2.m1.2.2.3" xref="S4.E4X.3.2.2.m1.2.2.3.cmml"></mi><mo id="S4.E4X.3.2.2.m1.2.2.2" xref="S4.E4X.3.2.2.m1.2.2.2.cmml">=</mo><mrow id="S4.E4X.3.2.2.m1.2.2.1" xref="S4.E4X.3.2.2.m1.2.2.1.cmml"><mo id="S4.E4X.3.2.2.m1.2.2.1a" rspace="0.167em" xref="S4.E4X.3.2.2.m1.2.2.1.cmml">âˆ’</mo><mrow id="S4.E4X.3.2.2.m1.2.2.1.1" xref="S4.E4X.3.2.2.m1.2.2.1.1.cmml"><mrow id="S4.E4X.3.2.2.m1.2.2.1.1.3" xref="S4.E4X.3.2.2.m1.2.2.1.1.3.cmml"><mi id="S4.E4X.3.2.2.m1.2.2.1.1.3.1" xref="S4.E4X.3.2.2.m1.2.2.1.1.3.1.cmml">log</mi><mo id="S4.E4X.3.2.2.m1.2.2.1.1.3a" lspace="0.167em" xref="S4.E4X.3.2.2.m1.2.2.1.1.3.cmml">â¡</mo><msub id="S4.E4X.3.2.2.m1.2.2.1.1.3.2" xref="S4.E4X.3.2.2.m1.2.2.1.1.3.2.cmml"><mi id="S4.E4X.3.2.2.m1.2.2.1.1.3.2.2" xref="S4.E4X.3.2.2.m1.2.2.1.1.3.2.2.cmml">P</mi><mi id="S4.E4X.3.2.2.m1.2.2.1.1.3.2.3" xref="S4.E4X.3.2.2.m1.2.2.1.1.3.2.3.cmml">Î¸</mi></msub></mrow><mo id="S4.E4X.3.2.2.m1.2.2.1.1.2" xref="S4.E4X.3.2.2.m1.2.2.1.1.2.cmml">â¢</mo><mrow id="S4.E4X.3.2.2.m1.2.2.1.1.1.1" xref="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.cmml"><mo id="S4.E4X.3.2.2.m1.2.2.1.1.1.1.2" stretchy="false" xref="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.cmml">(</mo><mrow id="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1" xref="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.cmml"><mover accent="true" id="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.4" xref="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.4.cmml"><mi id="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.4.2" xref="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.4.2.cmml">a</mi><mo id="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.4.1" xref="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.4.1.cmml">~</mo></mover><mo fence="false" id="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.3" xref="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.3.cmml">|</mo><mrow id="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.2.2" xref="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.2.3.cmml"><msub id="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.1.1.1" xref="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.2" xref="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">â„</mi><mi class="ltx_font_mathcaligraphic" id="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.3" xref="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.3.cmml">ğ’¢</mi></msub><mo id="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.2.2.3" xref="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.2.3.cmml">,</mo><mrow id="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.2.2.2" xref="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.2.2.2.cmml"><mrow id="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.2.2.2.2.2" xref="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.2.2.2.2.1.cmml"><mo id="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.2.2.2.2.2.1" stretchy="false" xref="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.2.2.2.2.1.cmml">{</mo><mover accent="true" id="S4.E4X.3.2.2.m1.1.1" xref="S4.E4X.3.2.2.m1.1.1.cmml"><mi id="S4.E4X.3.2.2.m1.1.1.2" xref="S4.E4X.3.2.2.m1.1.1.2.cmml">d</mi><mo id="S4.E4X.3.2.2.m1.1.1.1" xref="S4.E4X.3.2.2.m1.1.1.1.cmml">~</mo></mover><mo id="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.2.2.2.2.2.2" stretchy="false" xref="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.2.2.2.2.1.cmml">}</mo></mrow><mo id="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.2.2.2.1" xref="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.2.2.2.1.cmml">âˆª</mo><mi class="ltx_font_mathcaligraphic" id="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.2.2.2.3" xref="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.2.2.2.3.cmml">ğ’Ÿ</mi></mrow></mrow></mrow><mo id="S4.E4X.3.2.2.m1.2.2.1.1.1.1.3" stretchy="false" xref="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E4X.3.2.2.m1.2b"><apply id="S4.E4X.3.2.2.m1.2.2.cmml" xref="S4.E4X.3.2.2.m1.2.2"><eq id="S4.E4X.3.2.2.m1.2.2.2.cmml" xref="S4.E4X.3.2.2.m1.2.2.2"></eq><csymbol cd="latexml" id="S4.E4X.3.2.2.m1.2.2.3.cmml" xref="S4.E4X.3.2.2.m1.2.2.3">absent</csymbol><apply id="S4.E4X.3.2.2.m1.2.2.1.cmml" xref="S4.E4X.3.2.2.m1.2.2.1"><minus id="S4.E4X.3.2.2.m1.2.2.1.2.cmml" xref="S4.E4X.3.2.2.m1.2.2.1"></minus><apply id="S4.E4X.3.2.2.m1.2.2.1.1.cmml" xref="S4.E4X.3.2.2.m1.2.2.1.1"><times id="S4.E4X.3.2.2.m1.2.2.1.1.2.cmml" xref="S4.E4X.3.2.2.m1.2.2.1.1.2"></times><apply id="S4.E4X.3.2.2.m1.2.2.1.1.3.cmml" xref="S4.E4X.3.2.2.m1.2.2.1.1.3"><log id="S4.E4X.3.2.2.m1.2.2.1.1.3.1.cmml" xref="S4.E4X.3.2.2.m1.2.2.1.1.3.1"></log><apply id="S4.E4X.3.2.2.m1.2.2.1.1.3.2.cmml" xref="S4.E4X.3.2.2.m1.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S4.E4X.3.2.2.m1.2.2.1.1.3.2.1.cmml" xref="S4.E4X.3.2.2.m1.2.2.1.1.3.2">subscript</csymbol><ci id="S4.E4X.3.2.2.m1.2.2.1.1.3.2.2.cmml" xref="S4.E4X.3.2.2.m1.2.2.1.1.3.2.2">ğ‘ƒ</ci><ci id="S4.E4X.3.2.2.m1.2.2.1.1.3.2.3.cmml" xref="S4.E4X.3.2.2.m1.2.2.1.1.3.2.3">ğœƒ</ci></apply></apply><apply id="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.cmml" xref="S4.E4X.3.2.2.m1.2.2.1.1.1.1"><csymbol cd="latexml" id="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.3.cmml" xref="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.3">conditional</csymbol><apply id="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.4.cmml" xref="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.4"><ci id="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.4.1.cmml" xref="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.4.1">~</ci><ci id="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.4.2.cmml" xref="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.4.2">ğ‘</ci></apply><list id="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.2.3.cmml" xref="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.2.2"><apply id="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.2">â„</ci><ci id="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.1.1.1.3">ğ’¢</ci></apply><apply id="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.2.2.2.cmml" xref="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.2.2.2"><union id="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.2.2.2.1.cmml" xref="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.2.2.2.1"></union><set id="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.2.2.2.2.1.cmml" xref="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.2.2.2.2.2"><apply id="S4.E4X.3.2.2.m1.1.1.cmml" xref="S4.E4X.3.2.2.m1.1.1"><ci id="S4.E4X.3.2.2.m1.1.1.1.cmml" xref="S4.E4X.3.2.2.m1.1.1.1">~</ci><ci id="S4.E4X.3.2.2.m1.1.1.2.cmml" xref="S4.E4X.3.2.2.m1.1.1.2">ğ‘‘</ci></apply></set><ci id="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.2.2.2.3.cmml" xref="S4.E4X.3.2.2.m1.2.2.1.1.1.1.1.2.2.2.3">ğ’Ÿ</ci></apply></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4X.3.2.2.m1.2c">\displaystyle=-\log P_{\theta}(\tilde{a}|\mathcal{I_{G}},\{\tilde{d}\}\cup%
\mathcal{D})</annotation><annotation encoding="application/x-llamapun" id="S4.E4X.3.2.2.m1.2d">= - roman_log italic_P start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( over~ start_ARG italic_a end_ARG | caligraphic_I start_POSTSUBSCRIPT caligraphic_G end_POSTSUBSCRIPT , { over~ start_ARG italic_d end_ARG } âˆª caligraphic_D )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="2"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(4)</span></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E4Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=-\sum_{t=1}^{|\tilde{a}|}\log P_{\theta}(\tilde{a}_{t}|\tilde{a}%
_{(&lt;t)},\mathcal{I_{G}},\{\tilde{d}\}\cup\mathcal{D})." class="ltx_Math" display="inline" id="S4.E4Xa.2.1.1.m1.4"><semantics id="S4.E4Xa.2.1.1.m1.4a"><mrow id="S4.E4Xa.2.1.1.m1.4.4.1" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.cmml"><mrow id="S4.E4Xa.2.1.1.m1.4.4.1.1" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.cmml"><mi id="S4.E4Xa.2.1.1.m1.4.4.1.1.3" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.3.cmml"></mi><mo id="S4.E4Xa.2.1.1.m1.4.4.1.1.2" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.cmml">=</mo><mrow id="S4.E4Xa.2.1.1.m1.4.4.1.1.1" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.cmml"><mo id="S4.E4Xa.2.1.1.m1.4.4.1.1.1a" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.cmml">âˆ’</mo><mrow id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.cmml"><mstyle displaystyle="true" id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.2" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.2.cmml"><munderover id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.2a" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.2.cmml"><mo id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.2.2.2" movablelimits="false" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.2.2.2.cmml">âˆ‘</mo><mrow id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.2.2.3" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.2.2.3.cmml"><mi id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.2.2.3.2" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.2.2.3.2.cmml">t</mi><mo id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.2.2.3.1" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.2.2.3.3" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mrow id="S4.E4Xa.2.1.1.m1.1.1.1.3" xref="S4.E4Xa.2.1.1.m1.1.1.1.2.cmml"><mo id="S4.E4Xa.2.1.1.m1.1.1.1.3.1" stretchy="false" xref="S4.E4Xa.2.1.1.m1.1.1.1.2.1.cmml">|</mo><mover accent="true" id="S4.E4Xa.2.1.1.m1.1.1.1.1" xref="S4.E4Xa.2.1.1.m1.1.1.1.1.cmml"><mi id="S4.E4Xa.2.1.1.m1.1.1.1.1.2" xref="S4.E4Xa.2.1.1.m1.1.1.1.1.2.cmml">a</mi><mo id="S4.E4Xa.2.1.1.m1.1.1.1.1.1" xref="S4.E4Xa.2.1.1.m1.1.1.1.1.1.cmml">~</mo></mover><mo id="S4.E4Xa.2.1.1.m1.1.1.1.3.2" stretchy="false" xref="S4.E4Xa.2.1.1.m1.1.1.1.2.1.cmml">|</mo></mrow></munderover></mstyle><mrow id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.cmml"><mrow id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.3" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.3.cmml"><mi id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.3.1" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.3.1.cmml">log</mi><mo id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.3a" lspace="0.167em" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.3.cmml">â¡</mo><msub id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.3.2" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.3.2.cmml"><mi id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.3.2.2" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.3.2.2.cmml">P</mi><mi id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.3.2.3" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.3.2.3.cmml">Î¸</mi></msub></mrow><mo id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.2" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.cmml"><mo id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.2" stretchy="false" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.cmml"><msub id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.5" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.5.cmml"><mover accent="true" id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.5.2" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.5.2.cmml"><mi id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.5.2.2" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.5.2.2.cmml">a</mi><mo id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.5.2.1" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.5.2.1.cmml">~</mo></mover><mi id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.5.3" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.5.3.cmml">t</mi></msub><mo fence="false" id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.4" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.4.cmml">|</mo><mrow id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.3.3" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.3.4.cmml"><msub id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">a</mi><mo id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.1" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">~</mo></mover><mrow id="S4.E4Xa.2.1.1.m1.2.2.1.1" xref="S4.E4Xa.2.1.1.m1.2.2.1.1.1.cmml"><mo id="S4.E4Xa.2.1.1.m1.2.2.1.1.2" stretchy="false" xref="S4.E4Xa.2.1.1.m1.2.2.1.1.1.cmml">(</mo><mrow id="S4.E4Xa.2.1.1.m1.2.2.1.1.1" xref="S4.E4Xa.2.1.1.m1.2.2.1.1.1.cmml"><mi id="S4.E4Xa.2.1.1.m1.2.2.1.1.1.2" xref="S4.E4Xa.2.1.1.m1.2.2.1.1.1.2.cmml"></mi><mo id="S4.E4Xa.2.1.1.m1.2.2.1.1.1.1" xref="S4.E4Xa.2.1.1.m1.2.2.1.1.1.1.cmml">&lt;</mo><mi id="S4.E4Xa.2.1.1.m1.2.2.1.1.1.3" xref="S4.E4Xa.2.1.1.m1.2.2.1.1.1.3.cmml">t</mi></mrow><mo id="S4.E4Xa.2.1.1.m1.2.2.1.1.3" stretchy="false" xref="S4.E4Xa.2.1.1.m1.2.2.1.1.1.cmml">)</mo></mrow></msub><mo id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.3.3.4" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.3.4.cmml">,</mo><msub id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.2.2.2" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.2.2.2.2" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.2.2.2.2.cmml">â„</mi><mi class="ltx_font_mathcaligraphic" id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.2.2.2.3" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.2.2.2.3.cmml">ğ’¢</mi></msub><mo id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.3.3.5" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.3.4.cmml">,</mo><mrow id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.3.3.3" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.3.3.3.cmml"><mrow id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.3.3.3.2.2" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.3.3.3.2.1.cmml"><mo id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.3.3.3.2.2.1" stretchy="false" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.3.3.3.2.1.cmml">{</mo><mover accent="true" id="S4.E4Xa.2.1.1.m1.3.3" xref="S4.E4Xa.2.1.1.m1.3.3.cmml"><mi id="S4.E4Xa.2.1.1.m1.3.3.2" xref="S4.E4Xa.2.1.1.m1.3.3.2.cmml">d</mi><mo id="S4.E4Xa.2.1.1.m1.3.3.1" xref="S4.E4Xa.2.1.1.m1.3.3.1.cmml">~</mo></mover><mo id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.3.3.3.2.2.2" stretchy="false" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.3.3.3.2.1.cmml">}</mo></mrow><mo id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.3.3.3.1" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.3.3.3.1.cmml">âˆª</mo><mi class="ltx_font_mathcaligraphic" id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.3.3.3.3" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.3.3.3.3.cmml">ğ’Ÿ</mi></mrow></mrow></mrow><mo id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.3" stretchy="false" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S4.E4Xa.2.1.1.m1.4.4.1.2" lspace="0em" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E4Xa.2.1.1.m1.4b"><apply id="S4.E4Xa.2.1.1.m1.4.4.1.1.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1"><eq id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2"></eq><csymbol cd="latexml" id="S4.E4Xa.2.1.1.m1.4.4.1.1.3.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.3">absent</csymbol><apply id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1"><minus id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.2.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1"></minus><apply id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1"><apply id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.2.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.2.1.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.2">superscript</csymbol><apply id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.2.2.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.2.2.1.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.2">subscript</csymbol><sum id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.2.2.2.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.2.2.2"></sum><apply id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.2.2.3.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.2.2.3"><eq id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.2.2.3.1.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.2.2.3.1"></eq><ci id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.2.2.3.2.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.2.2.3.2">ğ‘¡</ci><cn id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.2.2.3.3.cmml" type="integer" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.2.2.3.3">1</cn></apply></apply><apply id="S4.E4Xa.2.1.1.m1.1.1.1.2.cmml" xref="S4.E4Xa.2.1.1.m1.1.1.1.3"><abs id="S4.E4Xa.2.1.1.m1.1.1.1.2.1.cmml" xref="S4.E4Xa.2.1.1.m1.1.1.1.3.1"></abs><apply id="S4.E4Xa.2.1.1.m1.1.1.1.1.cmml" xref="S4.E4Xa.2.1.1.m1.1.1.1.1"><ci id="S4.E4Xa.2.1.1.m1.1.1.1.1.1.cmml" xref="S4.E4Xa.2.1.1.m1.1.1.1.1.1">~</ci><ci id="S4.E4Xa.2.1.1.m1.1.1.1.1.2.cmml" xref="S4.E4Xa.2.1.1.m1.1.1.1.1.2">ğ‘</ci></apply></apply></apply><apply id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1"><times id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.2.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.2"></times><apply id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.3.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.3"><log id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.3.1.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.3.1"></log><apply id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.3.2.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.3.2.1.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.3.2">subscript</csymbol><ci id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.3.2.2.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.3.2.2">ğ‘ƒ</ci><ci id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.3.2.3.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.3.2.3">ğœƒ</ci></apply></apply><apply id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.4.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.4">conditional</csymbol><apply id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.5.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.5"><csymbol cd="ambiguous" id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.5.1.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.5">subscript</csymbol><apply id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.5.2.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.5.2"><ci id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.5.2.1.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.5.2.1">~</ci><ci id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.5.2.2.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.5.2.2">ğ‘</ci></apply><ci id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.5.3.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.5.3">ğ‘¡</ci></apply><list id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.3.4.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.3.3"><apply id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2"><ci id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.1">~</ci><ci id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.2">ğ‘</ci></apply><apply id="S4.E4Xa.2.1.1.m1.2.2.1.1.1.cmml" xref="S4.E4Xa.2.1.1.m1.2.2.1.1"><lt id="S4.E4Xa.2.1.1.m1.2.2.1.1.1.1.cmml" xref="S4.E4Xa.2.1.1.m1.2.2.1.1.1.1"></lt><csymbol cd="latexml" id="S4.E4Xa.2.1.1.m1.2.2.1.1.1.2.cmml" xref="S4.E4Xa.2.1.1.m1.2.2.1.1.1.2">absent</csymbol><ci id="S4.E4Xa.2.1.1.m1.2.2.1.1.1.3.cmml" xref="S4.E4Xa.2.1.1.m1.2.2.1.1.1.3">ğ‘¡</ci></apply></apply><apply id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.2.2.2.2">â„</ci><ci id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.2.2.2.3">ğ’¢</ci></apply><apply id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.3.3.3"><union id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.3.3.3.1.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.3.3.3.1"></union><set id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.3.3.3.2.1.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.3.3.3.2.2"><apply id="S4.E4Xa.2.1.1.m1.3.3.cmml" xref="S4.E4Xa.2.1.1.m1.3.3"><ci id="S4.E4Xa.2.1.1.m1.3.3.1.cmml" xref="S4.E4Xa.2.1.1.m1.3.3.1">~</ci><ci id="S4.E4Xa.2.1.1.m1.3.3.2.cmml" xref="S4.E4Xa.2.1.1.m1.3.3.2">ğ‘‘</ci></apply></set><ci id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.3.3.3.3.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.3.3.3.3">ğ’Ÿ</ci></apply></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4Xa.2.1.1.m1.4c">\displaystyle=-\sum_{t=1}^{|\tilde{a}|}\log P_{\theta}(\tilde{a}_{t}|\tilde{a}%
_{(&lt;t)},\mathcal{I_{G}},\{\tilde{d}\}\cup\mathcal{D}).</annotation><annotation encoding="application/x-llamapun" id="S4.E4Xa.2.1.1.m1.4d">= - âˆ‘ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | over~ start_ARG italic_a end_ARG | end_POSTSUPERSCRIPT roman_log italic_P start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( over~ start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | over~ start_ARG italic_a end_ARG start_POSTSUBSCRIPT ( &lt; italic_t ) end_POSTSUBSCRIPT , caligraphic_I start_POSTSUBSCRIPT caligraphic_G end_POSTSUBSCRIPT , { over~ start_ARG italic_d end_ARG } âˆª caligraphic_D ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p class="ltx_p" id="S4.SS2.p1.6">Here, the <math alttext="\tilde{d}" class="ltx_Math" display="inline" id="S4.SS2.p1.5.m1.1"><semantics id="S4.SS2.p1.5.m1.1a"><mover accent="true" id="S4.SS2.p1.5.m1.1.1" xref="S4.SS2.p1.5.m1.1.1.cmml"><mi id="S4.SS2.p1.5.m1.1.1.2" xref="S4.SS2.p1.5.m1.1.1.2.cmml">d</mi><mo id="S4.SS2.p1.5.m1.1.1.1" xref="S4.SS2.p1.5.m1.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.5.m1.1b"><apply id="S4.SS2.p1.5.m1.1.1.cmml" xref="S4.SS2.p1.5.m1.1.1"><ci id="S4.SS2.p1.5.m1.1.1.1.cmml" xref="S4.SS2.p1.5.m1.1.1.1">~</ci><ci id="S4.SS2.p1.5.m1.1.1.2.cmml" xref="S4.SS2.p1.5.m1.1.1.2">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.5.m1.1c">\tilde{d}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.5.m1.1d">over~ start_ARG italic_d end_ARG</annotation></semantics></math> indicates the ground-truth document, and <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S4.SS2.p1.6.m2.1"><semantics id="S4.SS2.p1.6.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p1.6.m2.1.1" xref="S4.SS2.p1.6.m2.1.1.cmml">ğ’Ÿ</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.6.m2.1b"><ci id="S4.SS2.p1.6.m2.1.1.cmml" xref="S4.SS2.p1.6.m2.1.1">ğ’Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.6.m2.1c">\mathcal{D}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.6.m2.1d">caligraphic_D</annotation></semantics></math> indicates the noise documents.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<p class="ltx_p ltx_align_center" id="S4.T2.3"><span class="ltx_text ltx_inline-block" id="S4.T2.3.3" style="width:433.6pt;">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T2.3.3.3.3" style="width:499.9pt;height:271pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S4.T2.3.3.3.3.3"><span class="ltx_text" id="S4.T2.3.3.3.3.3.3">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.3.3.3.3.3.3.3">
<span class="ltx_tr" id="S4.T2.3.3.3.3.3.3.3.4">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt ltx_rowspan ltx_rowspan_2" id="S4.T2.3.3.3.3.3.3.3.4.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.3.3.3.3.3.3.4.1.1">
<span class="ltx_p" id="S4.T2.3.3.3.3.3.3.3.4.1.1.1" style="width:148.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.3.3.3.3.3.4.1.1.1.1">Methods</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_3" id="S4.T2.3.3.3.3.3.3.3.4.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.3.3.3.3.3.4.2.1">HotpotQA</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_3" id="S4.T2.3.3.3.3.3.3.3.4.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.3.3.3.3.3.4.3.1">MuSiQue</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_3" id="S4.T2.3.3.3.3.3.3.3.4.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.3.3.3.3.3.4.4.1">2Wikimultihopqa</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.3.3.3.3.3.3.3.4.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.3.3.3.3.3.4.5.1">StrategyQA</span></span></span>
<span class="ltx_tr" id="S4.T2.3.3.3.3.3.3.3.3">
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.3.3.3.3.3.3.3.4" style="padding-left:4.0pt;padding-right:4.0pt;">F1</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.3.3.3.3.3.3.3.5" style="padding-left:4.0pt;padding-right:4.0pt;">Acc</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.1.1.1.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">Acc<math alttext="\dagger" class="ltx_Math" display="inline" id="S4.T2.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.1.1.1.1.1.1.m1.1a"><mo id="S4.T2.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.1.1.1.1.1.m1.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.1.1.1.1.1.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.1.1.1.1.1.1.m1.1d">â€ </annotation></semantics></math></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.3.3.3.3.3.3.3.6" style="padding-left:4.0pt;padding-right:4.0pt;">F1</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.3.3.3.3.3.3.3.7" style="padding-left:4.0pt;padding-right:4.0pt;">Acc</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.2.2.2.2.2.2.2" style="padding-left:4.0pt;padding-right:4.0pt;">Acc<math alttext="\dagger" class="ltx_Math" display="inline" id="S4.T2.2.2.2.2.2.2.2.2.2.m1.1"><semantics id="S4.T2.2.2.2.2.2.2.2.2.2.m1.1a"><mo id="S4.T2.2.2.2.2.2.2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.2.2.2.2.2.2.m1.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.2.2.2.2.2.2.m1.1b"><ci id="S4.T2.2.2.2.2.2.2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.2.2.2.2.2.2.m1.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.2.2.2.2.2.2.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.2.2.2.2.2.2.2.m1.1d">â€ </annotation></semantics></math></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.3.3.3.3.3.3.3.8" style="padding-left:4.0pt;padding-right:4.0pt;">F1</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.3.3.3.3.3.3.3.9" style="padding-left:4.0pt;padding-right:4.0pt;">Acc</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.3.3.3.3.3.3.3.3" style="padding-left:4.0pt;padding-right:4.0pt;">Acc<math alttext="\dagger" class="ltx_Math" display="inline" id="S4.T2.3.3.3.3.3.3.3.3.3.m1.1"><semantics id="S4.T2.3.3.3.3.3.3.3.3.3.m1.1a"><mo id="S4.T2.3.3.3.3.3.3.3.3.3.m1.1.1" xref="S4.T2.3.3.3.3.3.3.3.3.3.m1.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.3.3.3.3.3.3.m1.1b"><ci id="S4.T2.3.3.3.3.3.3.3.3.3.m1.1.1.cmml" xref="S4.T2.3.3.3.3.3.3.3.3.3.m1.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.3.3.3.3.3.3.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.3.3.3.3.3.3.3.3.m1.1d">â€ </annotation></semantics></math></span>
<span class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.3.3.3.3.3.3.3.10" style="padding-left:4.0pt;padding-right:4.0pt;">Acc</span></span>
<span class="ltx_tr" id="S4.T2.3.3.3.3.3.3.3.5" style="background-color:#DBDBDB;">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_t ltx_colspan ltx_colspan_11" id="S4.T2.3.3.3.3.3.3.3.5.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_italic" id="S4.T2.3.3.3.3.3.3.3.5.1.1" style="background-color:#DBDBDB;">Generate w/o Retrieval</span></span></span>
<span class="ltx_tr" id="S4.T2.3.3.3.3.3.3.3.6">
<span class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.3.3.3.3.3.3.3.6.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.3.3.3.3.3.3.6.1.1">
<span class="ltx_p" id="S4.T2.3.3.3.3.3.3.3.6.1.1.1" style="width:148.0pt;">CoTÂ <cite class="ltx_cite ltx_citemacro_citep">(Wei etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib38" title="">2022</a>)</cite></span>
</span></span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.6.2" style="padding-left:4.0pt;padding-right:4.0pt;">35.28</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.6.3" style="padding-left:4.0pt;padding-right:4.0pt;">30.79</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.6.4" style="padding-left:4.0pt;padding-right:4.0pt;">37.07</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.6.5" style="padding-left:4.0pt;padding-right:4.0pt;">23.35</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.6.6" style="padding-left:4.0pt;padding-right:4.0pt;">13.21</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.6.7" style="padding-left:4.0pt;padding-right:4.0pt;">17.85</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.6.8" style="padding-left:4.0pt;padding-right:4.0pt;">35.41</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.6.9" style="padding-left:4.0pt;padding-right:4.0pt;">32.46</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.6.10" style="padding-left:4.0pt;padding-right:4.0pt;">34.52</span>
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.6.11" style="padding-left:4.0pt;padding-right:4.0pt;">67.83</span></span>
<span class="ltx_tr" id="S4.T2.3.3.3.3.3.3.3.7">
<span class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.3.3.3.3.3.3.3.7.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.3.3.3.3.3.3.7.1.1">
<span class="ltx_p" id="S4.T2.3.3.3.3.3.3.3.7.1.1.1" style="width:148.0pt;">CoT-SCÂ <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib37" title="">2022b</a>)</cite></span>
</span></span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.7.2" style="padding-left:4.0pt;padding-right:4.0pt;">42.25</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.7.3" style="padding-left:4.0pt;padding-right:4.0pt;">38.68</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.7.4" style="padding-left:4.0pt;padding-right:4.0pt;">39.07</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.7.5" style="padding-left:4.0pt;padding-right:4.0pt;">15.61</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.7.6" style="padding-left:4.0pt;padding-right:4.0pt;">10.02</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.7.7" style="padding-left:4.0pt;padding-right:4.0pt;">12.42</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.7.8" style="padding-left:4.0pt;padding-right:4.0pt;">40.37</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.7.9" style="padding-left:4.0pt;padding-right:4.0pt;">36.57</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.7.10" style="padding-left:4.0pt;padding-right:4.0pt;">38.59</span>
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.7.11" style="padding-left:4.0pt;padding-right:4.0pt;">70.84</span></span>
<span class="ltx_tr" id="S4.T2.3.3.3.3.3.3.3.8">
<span class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.3.3.3.3.3.3.3.8.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.3.3.3.3.3.3.8.1.1">
<span class="ltx_p" id="S4.T2.3.3.3.3.3.3.3.8.1.1.1" style="width:148.0pt;">GenReadÂ <cite class="ltx_cite ltx_citemacro_citep">(Yu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib44" title="">2022</a>)</cite></span>
</span></span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.8.2" style="padding-left:4.0pt;padding-right:4.0pt;">35.21</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.8.3" style="padding-left:4.0pt;padding-right:4.0pt;">36.81</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.8.4" style="padding-left:4.0pt;padding-right:4.0pt;">37.54</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.8.5" style="padding-left:4.0pt;padding-right:4.0pt;">9.77</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.8.6" style="padding-left:4.0pt;padding-right:4.0pt;">9.29</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.8.7" style="padding-left:4.0pt;padding-right:4.0pt;">10.32</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.8.8" style="padding-left:4.0pt;padding-right:4.0pt;">23.13</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.8.9" style="padding-left:4.0pt;padding-right:4.0pt;">20.62</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.8.10" style="padding-left:4.0pt;padding-right:4.0pt;">28.31</span>
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.8.11" style="padding-left:4.0pt;padding-right:4.0pt;">67.13</span></span>
<span class="ltx_tr" id="S4.T2.3.3.3.3.3.3.3.9">
<span class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.3.3.3.3.3.3.3.9.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.3.3.3.3.3.3.9.1.1">
<span class="ltx_p" id="S4.T2.3.3.3.3.3.3.3.9.1.1.1" style="width:148.0pt;">GenRead <span class="ltx_text ltx_font_italic" id="S4.T2.3.3.3.3.3.3.3.9.1.1.1.1">w/ decomposition</span></span>
</span></span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.9.2" style="padding-left:4.0pt;padding-right:4.0pt;">42.28</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.9.3" style="padding-left:4.0pt;padding-right:4.0pt;">43.32</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.9.4" style="padding-left:4.0pt;padding-right:4.0pt;">45.31</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.9.5" style="padding-left:4.0pt;padding-right:4.0pt;">20.13</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.9.6" style="padding-left:4.0pt;padding-right:4.0pt;">17.58</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.9.7" style="padding-left:4.0pt;padding-right:4.0pt;">20.62</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.9.8" style="padding-left:4.0pt;padding-right:4.0pt;">41.19</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.9.9" style="padding-left:4.0pt;padding-right:4.0pt;">41.63</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.9.10" style="padding-left:4.0pt;padding-right:4.0pt;">43.24</span>
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.9.11" style="padding-left:4.0pt;padding-right:4.0pt;">68.13</span></span>
<span class="ltx_tr" id="S4.T2.3.3.3.3.3.3.3.10" style="background-color:#DBDBDB;">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_t ltx_colspan ltx_colspan_11" id="S4.T2.3.3.3.3.3.3.3.10.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_italic" id="S4.T2.3.3.3.3.3.3.3.10.1.1" style="background-color:#DBDBDB;">Generate w/ Retrieval</span></span></span>
<span class="ltx_tr" id="S4.T2.3.3.3.3.3.3.3.11">
<span class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.3.3.3.3.3.3.3.11.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.3.3.3.3.3.3.11.1.1">
<span class="ltx_p" id="S4.T2.3.3.3.3.3.3.3.11.1.1.1" style="width:148.0pt;">VEÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib48" title="">2023</a>)</cite></span>
</span></span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.11.2" style="padding-left:4.0pt;padding-right:4.0pt;">29.64</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.11.3" style="padding-left:4.0pt;padding-right:4.0pt;">22.64</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.11.4" style="padding-left:4.0pt;padding-right:4.0pt;">24.64</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.11.5" style="padding-left:4.0pt;padding-right:4.0pt;">6.5</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.11.6" style="padding-left:4.0pt;padding-right:4.0pt;">11.14</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.11.7" style="padding-left:4.0pt;padding-right:4.0pt;">15.57</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.11.8" style="padding-left:4.0pt;padding-right:4.0pt;">13.76</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.11.9" style="padding-left:4.0pt;padding-right:4.0pt;">31.57</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.11.10" style="padding-left:4.0pt;padding-right:4.0pt;">32.64</span>
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.11.11" style="padding-left:4.0pt;padding-right:4.0pt;">63.07</span></span>
<span class="ltx_tr" id="S4.T2.3.3.3.3.3.3.3.12">
<span class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.3.3.3.3.3.3.3.12.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.3.3.3.3.3.3.12.1.1">
<span class="ltx_p" id="S4.T2.3.3.3.3.3.3.3.12.1.1.1" style="width:148.0pt;">ReActÂ <cite class="ltx_cite ltx_citemacro_citep">(Yao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib43" title="">2022</a>)</cite></span>
</span></span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.12.2" style="padding-left:4.0pt;padding-right:4.0pt;">40.70</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.12.3" style="padding-left:4.0pt;padding-right:4.0pt;">33.10</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.12.4" style="padding-left:4.0pt;padding-right:4.0pt;">37.12</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.12.5" style="padding-left:4.0pt;padding-right:4.0pt;">15.34</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.12.6" style="padding-left:4.0pt;padding-right:4.0pt;">17.32</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.12.7" style="padding-left:4.0pt;padding-right:4.0pt;">19.32</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.12.8" style="padding-left:4.0pt;padding-right:4.0pt;">35.50</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.12.9" style="padding-left:4.0pt;padding-right:4.0pt;">30.10</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.12.10" style="padding-left:4.0pt;padding-right:4.0pt;">33.41</span>
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.12.11" style="padding-left:4.0pt;padding-right:4.0pt;">68.37</span></span>
<span class="ltx_tr" id="S4.T2.3.3.3.3.3.3.3.13">
<span class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.3.3.3.3.3.3.3.13.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.3.3.3.3.3.3.13.1.1">
<span class="ltx_p" id="S4.T2.3.3.3.3.3.3.3.13.1.1.1" style="width:148.0pt;">GRG <span class="ltx_text ltx_font_italic" id="S4.T2.3.3.3.3.3.3.3.13.1.1.1.1">w/ decomposition</span></span>
</span></span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.13.2" style="padding-left:4.0pt;padding-right:4.0pt;">50.21</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.13.3" style="padding-left:4.0pt;padding-right:4.0pt;">45.18</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.13.4" style="padding-left:4.0pt;padding-right:4.0pt;">50.80</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.13.5" style="padding-left:4.0pt;padding-right:4.0pt;">24.87</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.13.6" style="padding-left:4.0pt;padding-right:4.0pt;">17.91</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.13.7" style="padding-left:4.0pt;padding-right:4.0pt;">22.33</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.13.8" style="padding-left:4.0pt;padding-right:4.0pt;">40.42</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.13.9" style="padding-left:4.0pt;padding-right:4.0pt;">40.48</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.13.10" style="padding-left:4.0pt;padding-right:4.0pt;">43.05</span>
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.13.11" style="padding-left:4.0pt;padding-right:4.0pt;">75.21</span></span>
<span class="ltx_tr" id="S4.T2.3.3.3.3.3.3.3.14">
<span class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.3.3.3.3.3.3.3.14.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.3.3.3.3.3.3.14.1.1">
<span class="ltx_p" id="S4.T2.3.3.3.3.3.3.3.14.1.1.1" style="width:148.0pt;">RetGenÂ <cite class="ltx_cite ltx_citemacro_citep">(Shao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib27" title="">2023</a>)</cite></span>
</span></span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.14.2" style="padding-left:4.0pt;padding-right:4.0pt;">28.30</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.14.3" style="padding-left:4.0pt;padding-right:4.0pt;">41.04</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.14.4" style="padding-left:4.0pt;padding-right:4.0pt;">44.10</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.14.5" style="padding-left:4.0pt;padding-right:4.0pt;">21.04</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.14.6" style="padding-left:4.0pt;padding-right:4.0pt;">17.69</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.14.7" style="padding-left:4.0pt;padding-right:4.0pt;">20.19</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.14.8" style="padding-left:4.0pt;padding-right:4.0pt;">36.00</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.14.9" style="padding-left:4.0pt;padding-right:4.0pt;">42.17</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.14.10" style="padding-left:4.0pt;padding-right:4.0pt;">45.21</span>
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.14.11" style="padding-left:4.0pt;padding-right:4.0pt;">73.42</span></span>
<span class="ltx_tr" id="S4.T2.3.3.3.3.3.3.3.15">
<span class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.3.3.3.3.3.3.3.15.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.3.3.3.3.3.3.15.1.1">
<span class="ltx_p" id="S4.T2.3.3.3.3.3.3.3.15.1.1.1" style="width:148.0pt;">SearChainÂ <cite class="ltx_cite ltx_citemacro_citep">(Xu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib40" title="">2024</a>)</cite></span>
</span></span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.15.2" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.15.3" style="padding-left:4.0pt;padding-right:4.0pt;">46.76</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.15.4" style="padding-left:4.0pt;padding-right:4.0pt;">48.12</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.15.5" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.15.6" style="padding-left:4.0pt;padding-right:4.0pt;">17.07</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.15.7" style="padding-left:4.0pt;padding-right:4.0pt;">20.45</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.15.8" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.15.9" style="padding-left:4.0pt;padding-right:4.0pt;">42.14</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.15.10" style="padding-left:4.0pt;padding-right:4.0pt;">46.27</span>
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.15.11" style="padding-left:4.0pt;padding-right:4.0pt;">76.95</span></span>
<span class="ltx_tr" id="S4.T2.3.3.3.3.3.3.3.16">
<span class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.3.3.3.3.3.3.3.16.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.3.3.3.3.3.3.16.1.1">
<span class="ltx_p" id="S4.T2.3.3.3.3.3.3.3.16.1.1.1" style="width:148.0pt;">DSPyÂ <cite class="ltx_cite ltx_citemacro_citep">(Khattab etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib14" title="">2023</a>)</cite></span>
</span></span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.16.2" style="padding-left:4.0pt;padding-right:4.0pt;">47.80</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.16.3" style="padding-left:4.0pt;padding-right:4.0pt;">42.43</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.16.4" style="padding-left:4.0pt;padding-right:4.0pt;">50.07</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.16.5" style="padding-left:4.0pt;padding-right:4.0pt;">20.11</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.16.6" style="padding-left:4.0pt;padding-right:4.0pt;">13.40</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.16.7" style="padding-left:4.0pt;padding-right:4.0pt;">17.40</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.16.8" style="padding-left:4.0pt;padding-right:4.0pt;">44.77</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.16.9" style="padding-left:4.0pt;padding-right:4.0pt;">43.43</span>
<span class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.16.10" style="padding-left:4.0pt;padding-right:4.0pt;">45.43</span>
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.3.3.3.3.3.3.3.16.11" style="padding-left:4.0pt;padding-right:4.0pt;">71.78</span></span>
<span class="ltx_tr" id="S4.T2.3.3.3.3.3.3.3.17">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S4.T2.3.3.3.3.3.3.3.17.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.3.3.3.3.3.3.17.1.1">
<span class="ltx_p" id="S4.T2.3.3.3.3.3.3.3.17.1.1.1" style="width:148.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.3.3.3.3.3.17.1.1.1.1">GenGround (Ours)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.3.3.3.3.3.3.3.17.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.3.3.3.3.3.17.2.1">52.26</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.3.3.3.3.3.3.3.17.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.3.3.3.3.3.17.3.1">47.27</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.3.3.3.3.3.3.3.17.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.3.3.3.3.3.17.4.1">55.73</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.3.3.3.3.3.3.3.17.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.3.3.3.3.3.17.5.1">27.36</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.3.3.3.3.3.3.3.17.6" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.3.3.3.3.3.17.6.1">20.24</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.3.3.3.3.3.3.3.17.7" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.3.3.3.3.3.17.7.1">24.77</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.3.3.3.3.3.3.3.17.8" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.3.3.3.3.3.17.8.1">50.21</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.3.3.3.3.3.3.3.17.9" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.3.3.3.3.3.17.9.1">45.61</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.3.3.3.3.3.3.3.17.10" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.3.3.3.3.3.17.10.1">48.58</span></span>
<span class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.3.3.3.3.3.3.3.17.11" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.3.3.3.3.3.17.11.1">77.12</span></span></span>
</span></span></span>
</span></span></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>
Evaluation results on multi-hop question answering datasets. Acc<math alttext="\dagger" class="ltx_Math" display="inline" id="S4.T2.6.m1.1"><semantics id="S4.T2.6.m1.1b"><mo id="S4.T2.6.m1.1.1" xref="S4.T2.6.m1.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="S4.T2.6.m1.1c"><ci id="S4.T2.6.m1.1.1.cmml" xref="S4.T2.6.m1.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.m1.1d">\dagger</annotation><annotation encoding="application/x-llamapun" id="S4.T2.6.m1.1e">â€ </annotation></semantics></math> indicates the semantic accuracy of model outputs evaluated with <span class="ltx_text ltx_font_typewriter" id="S4.T2.11.1">gpt-3.5-turbo-instruct</span> with the same prompt.
Since the <span class="ltx_text ltx_font_italic" id="S4.T2.12.2">SearChain</span> prompts the LLM to generate a long-form answer while the ground truth answer in our dataset is short-form, we only evaluate it with the Acc and Acc<sup class="ltx_sup" id="S4.T2.13.3">â€ </sup> metrics.
</figcaption>
</figure>
<figure class="ltx_table" id="S4.T3">
<p class="ltx_p ltx_align_center" id="S4.T3.7"><span class="ltx_text ltx_inline-block" id="S4.T3.7.7" style="width:433.6pt;">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T3.7.7.7.7" style="width:248.2pt;height:144pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S4.T3.7.7.7.7.7"><span class="ltx_text" id="S4.T3.7.7.7.7.7.7">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.7.7.7.7.7.7.7">
<span class="ltx_tr" id="S4.T3.1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S4.T3.1.1.1.1.1.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.1.1.1.1.1.1.2.1">
<span class="ltx_p" id="S4.T3.1.1.1.1.1.1.1.1.2.1.1" style="width:65.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.1.1.1.1.2.1.1.1">Method</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.1.1.1.1.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.1.1.1.1.3.1">HQA</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.1.1.1.1.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.1.1.1.1.4.1">MQA</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.1.1.1.1.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.1.1.1.1.5.1">WQA</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.1.1.1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.1.1.1.1.1.1">Average <math alttext="\Delta" class="ltx_Math" display="inline" id="S4.T3.1.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.1.1.1.1.1.1.1.m1.1a"><mi id="S4.T3.1.1.1.1.1.1.1.1.1.1.m1.1.1" mathvariant="normal" xref="S4.T3.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">Î”</mi><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.1.1.1.1.1.1.m1.1.1">Î”</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.1.1.1.1.1.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.1.1.1.1.1.1.1.m1.1d">roman_Î”</annotation></semantics></math></span></span></span>
<span class="ltx_tr" id="S4.T3.2.2.2.2.2.2.2.2" style="background-color:#DBDBDB;">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_t ltx_colspan ltx_colspan_5" id="S4.T3.2.2.2.2.2.2.2.2.1"><span class="ltx_text ltx_font_italic" id="S4.T3.2.2.2.2.2.2.2.2.1.1" style="background-color:#DBDBDB;">Retrieval <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T3.2.2.2.2.2.2.2.2.1.1.m1.1"><semantics id="S4.T3.2.2.2.2.2.2.2.2.1.1.m1.1a"><mo id="S4.T3.2.2.2.2.2.2.2.2.1.1.m1.1.1" mathbackground="#DBDBDB" stretchy="false" xref="S4.T3.2.2.2.2.2.2.2.2.1.1.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.2.2.2.2.2.1.1.m1.1b"><ci id="S4.T3.2.2.2.2.2.2.2.2.1.1.m1.1.1.cmml" xref="S4.T3.2.2.2.2.2.2.2.2.1.1.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.2.2.2.2.2.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.2.2.2.2.2.2.2.1.1.m1.1d">â†’</annotation></semantics></math> ColBERTv2</span></span></span>
<span class="ltx_tr" id="S4.T3.7.7.7.7.7.7.7.8">
<span class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.7.7.7.7.7.7.7.8.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.7.7.7.7.7.7.7.8.1.1">
<span class="ltx_p" id="S4.T3.7.7.7.7.7.7.7.8.1.1.1" style="width:65.4pt;">Ours (IDG)</span>
</span></span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.7.7.7.7.7.7.8.2">42.08</span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.7.7.7.7.7.7.8.3">14.37</span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.7.7.7.7.7.7.8.4">32.69</span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.7.7.7.7.7.7.8.5">-</span></span>
<span class="ltx_tr" id="S4.T3.3.3.3.3.3.3.3.3">
<span class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.3.3.3.3.3.3.3.3.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.3.3.3.3.3.3.3.3.2.1">
<span class="ltx_p" id="S4.T3.3.3.3.3.3.3.3.3.2.1.1" style="width:65.4pt;">Ours (Vanilla)</span>
</span></span>
<span class="ltx_td ltx_align_center" id="S4.T3.3.3.3.3.3.3.3.3.3">38.31</span>
<span class="ltx_td ltx_align_center" id="S4.T3.3.3.3.3.3.3.3.3.4">11.34</span>
<span class="ltx_td ltx_align_center" id="S4.T3.3.3.3.3.3.3.3.3.5">29.45</span>
<span class="ltx_td ltx_align_center" id="S4.T3.3.3.3.3.3.3.3.3.1">3.35<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.3.3.3.3.3.3.3.3.1.m1.1"><semantics id="S4.T3.3.3.3.3.3.3.3.3.1.m1.1a"><mo id="S4.T3.3.3.3.3.3.3.3.3.1.m1.1.1" stretchy="false" xref="S4.T3.3.3.3.3.3.3.3.3.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.3.3.3.3.3.1.m1.1b"><ci id="S4.T3.3.3.3.3.3.3.3.3.1.m1.1.1.cmml" xref="S4.T3.3.3.3.3.3.3.3.3.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.3.3.3.3.3.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.3.3.3.3.3.3.3.3.1.m1.1d">â†“</annotation></semantics></math></span></span>
<span class="ltx_tr" id="S4.T3.4.4.4.4.4.4.4.4">
<span class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.4.4.4.4.4.4.4.4.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.4.4.4.4.4.4.4.2.1">
<span class="ltx_p" id="S4.T3.4.4.4.4.4.4.4.4.2.1.1" style="width:65.4pt;">DSPy</span>
</span></span>
<span class="ltx_td ltx_align_center" id="S4.T3.4.4.4.4.4.4.4.4.3">36.41</span>
<span class="ltx_td ltx_align_center" id="S4.T3.4.4.4.4.4.4.4.4.4">7.42</span>
<span class="ltx_td ltx_align_center" id="S4.T3.4.4.4.4.4.4.4.4.5">28.31</span>
<span class="ltx_td ltx_align_center" id="S4.T3.4.4.4.4.4.4.4.4.1">5.67<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.4.4.4.4.4.4.4.4.1.m1.1"><semantics id="S4.T3.4.4.4.4.4.4.4.4.1.m1.1a"><mo id="S4.T3.4.4.4.4.4.4.4.4.1.m1.1.1" stretchy="false" xref="S4.T3.4.4.4.4.4.4.4.4.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.4.4.4.4.4.4.1.m1.1b"><ci id="S4.T3.4.4.4.4.4.4.4.4.1.m1.1.1.cmml" xref="S4.T3.4.4.4.4.4.4.4.4.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.4.4.4.4.4.4.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.4.4.4.4.4.4.4.4.1.m1.1d">â†“</annotation></semantics></math></span></span>
<span class="ltx_tr" id="S4.T3.5.5.5.5.5.5.5.5">
<span class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.5.5.5.5.5.5.5.5.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.5.5.5.5.5.5.5.5.2.1">
<span class="ltx_p" id="S4.T3.5.5.5.5.5.5.5.5.2.1.1" style="width:65.4pt;">GRG <span class="ltx_text ltx_font_italic" id="S4.T3.5.5.5.5.5.5.5.5.2.1.1.1">w/ dp</span></span>
</span></span>
<span class="ltx_td ltx_align_center" id="S4.T3.5.5.5.5.5.5.5.5.3">32.56</span>
<span class="ltx_td ltx_align_center" id="S4.T3.5.5.5.5.5.5.5.5.4">9.34</span>
<span class="ltx_td ltx_align_center" id="S4.T3.5.5.5.5.5.5.5.5.5">25.63</span>
<span class="ltx_td ltx_align_center" id="S4.T3.5.5.5.5.5.5.5.5.1">7.20<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.5.5.5.5.5.5.5.5.1.m1.1"><semantics id="S4.T3.5.5.5.5.5.5.5.5.1.m1.1a"><mo id="S4.T3.5.5.5.5.5.5.5.5.1.m1.1.1" stretchy="false" xref="S4.T3.5.5.5.5.5.5.5.5.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.5.5.5.5.5.5.1.m1.1b"><ci id="S4.T3.5.5.5.5.5.5.5.5.1.m1.1.1.cmml" xref="S4.T3.5.5.5.5.5.5.5.5.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.5.5.5.5.5.5.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.5.5.5.5.5.5.5.5.1.m1.1d">â†“</annotation></semantics></math></span></span>
<span class="ltx_tr" id="S4.T3.6.6.6.6.6.6.6.6">
<span class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.6.6.6.6.6.6.6.6.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.6.6.6.6.6.6.6.6.2.1">
<span class="ltx_p" id="S4.T3.6.6.6.6.6.6.6.6.2.1.1" style="width:65.4pt;">RetGen</span>
</span></span>
<span class="ltx_td ltx_align_center" id="S4.T3.6.6.6.6.6.6.6.6.3">26.52</span>
<span class="ltx_td ltx_align_center" id="S4.T3.6.6.6.6.6.6.6.6.4">10.12</span>
<span class="ltx_td ltx_align_center" id="S4.T3.6.6.6.6.6.6.6.6.5">24.13</span>
<span class="ltx_td ltx_align_center" id="S4.T3.6.6.6.6.6.6.6.6.1">9.45<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.6.6.6.6.6.6.6.6.1.m1.1"><semantics id="S4.T3.6.6.6.6.6.6.6.6.1.m1.1a"><mo id="S4.T3.6.6.6.6.6.6.6.6.1.m1.1.1" stretchy="false" xref="S4.T3.6.6.6.6.6.6.6.6.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T3.6.6.6.6.6.6.6.6.1.m1.1b"><ci id="S4.T3.6.6.6.6.6.6.6.6.1.m1.1.1.cmml" xref="S4.T3.6.6.6.6.6.6.6.6.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.6.6.6.6.6.6.6.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.6.6.6.6.6.6.6.6.1.m1.1d">â†“</annotation></semantics></math></span></span>
<span class="ltx_tr" id="S4.T3.7.7.7.7.7.7.7.7">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T3.7.7.7.7.7.7.7.7.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.7.7.7.7.7.7.7.7.2.1">
<span class="ltx_p" id="S4.T3.7.7.7.7.7.7.7.7.2.1.1" style="width:65.4pt;">SearChain</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.7.7.7.7.7.7.7.7.3">24.62</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.7.7.7.7.7.7.7.7.4">9.46</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.7.7.7.7.7.7.7.7.5">26.53</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.7.7.7.7.7.7.7.7.1">9.51<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.7.7.7.7.7.7.7.7.1.m1.1"><semantics id="S4.T3.7.7.7.7.7.7.7.7.1.m1.1a"><mo id="S4.T3.7.7.7.7.7.7.7.7.1.m1.1.1" stretchy="false" xref="S4.T3.7.7.7.7.7.7.7.7.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T3.7.7.7.7.7.7.7.7.1.m1.1b"><ci id="S4.T3.7.7.7.7.7.7.7.7.1.m1.1.1.cmml" xref="S4.T3.7.7.7.7.7.7.7.7.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.7.7.7.7.7.7.7.7.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.7.7.7.7.7.7.7.7.1.m1.1d">â†“</annotation></semantics></math></span></span>
</span></span></span>
</span></span></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Accuracy (Acc) on three datasets with Mistral-7B as backbone.
The <span class="ltx_text ltx_font_italic" id="S4.T3.12.1">w/ dq</span> indicates <span class="ltx_text ltx_font_italic" id="S4.T3.13.2">decomposition</span>.
The <span class="ltx_text ltx_font_italic" id="S4.T3.14.3">Vanilla</span> and <span class="ltx_text ltx_font_italic" id="S4.T3.15.4">IDG</span> indicate enable our framework by prompting and our grounding distillation, respectively.
</figcaption>
</figure>
<figure class="ltx_table" id="S4.T4">
<p class="ltx_p ltx_align_center" id="S4.T4.13"><span class="ltx_text ltx_inline-block" id="S4.T4.13.13" style="width:433.6pt;">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T4.13.13.13.13" style="width:250.5pt;height:90pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S4.T4.13.13.13.13.13"><span class="ltx_text" id="S4.T4.13.13.13.13.13.13">
<span class="ltx_tabular ltx_align_middle" id="S4.T4.13.13.13.13.13.13.13">
<span class="ltx_tr" id="S4.T4.13.13.13.13.13.13.13.14">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt ltx_rowspan ltx_rowspan_2" id="S4.T4.13.13.13.13.13.13.13.14.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.13.13.13.13.13.13.13.14.1.1">
<span class="ltx_p" id="S4.T4.13.13.13.13.13.13.13.14.1.1.1" style="width:62.6pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.13.13.13.13.13.13.13.14.1.1.1.1">Ablation</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_3" id="S4.T4.13.13.13.13.13.13.13.14.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.13.13.13.13.13.13.13.14.2.1">HQA</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.13.13.13.13.13.13.13.14.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.13.13.13.13.13.13.13.14.3.1">SQA</span></span></span>
<span class="ltx_tr" id="S4.T4.1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.1.1.1.1.1.1.2" style="padding-left:4.0pt;padding-right:4.0pt;">F1</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.1.1.1.1.1.1.3" style="padding-left:4.0pt;padding-right:4.0pt;">Acc</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.1.1.1.1.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">Acc<math alttext="\dagger" class="ltx_Math" display="inline" id="S4.T4.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S4.T4.1.1.1.1.1.1.1.1.1.m1.1a"><mo id="S4.T4.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.T4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="S4.T4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.1.1.1.1.1.m1.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.1.1.1.1.1.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="S4.T4.1.1.1.1.1.1.1.1.1.m1.1d">â€ </annotation></semantics></math></span>
<span class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T4.1.1.1.1.1.1.1.1.4" style="padding-left:4.0pt;padding-right:4.0pt;">Acc</span></span>
<span class="ltx_tr" id="S4.T4.5.5.5.5.5.5.5.5">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T4.5.5.5.5.5.5.5.5.5" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.5.5.5.5.5.5.5.5.5.1">
<span class="ltx_p" id="S4.T4.5.5.5.5.5.5.5.5.5.1.1" style="width:62.6pt;"><span class="ltx_text ltx_font_italic" id="S4.T4.5.5.5.5.5.5.5.5.5.1.1.1">w/o deduction</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.2.2.2.2.2.2.2.1" style="padding-left:4.0pt;padding-right:4.0pt;">42.65 <math alttext="\downarrow_{9}" class="ltx_Math" display="inline" id="S4.T4.2.2.2.2.2.2.2.2.1.m1.1"><semantics id="S4.T4.2.2.2.2.2.2.2.2.1.m1.1a"><msub id="S4.T4.2.2.2.2.2.2.2.2.1.m1.1.1" xref="S4.T4.2.2.2.2.2.2.2.2.1.m1.1.1.cmml"><mo id="S4.T4.2.2.2.2.2.2.2.2.1.m1.1.1.2" stretchy="false" xref="S4.T4.2.2.2.2.2.2.2.2.1.m1.1.1.2.cmml">â†“</mo><mn id="S4.T4.2.2.2.2.2.2.2.2.1.m1.1.1.3" xref="S4.T4.2.2.2.2.2.2.2.2.1.m1.1.1.3.cmml">9</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.2.2.2.2.2.1.m1.1b"><apply id="S4.T4.2.2.2.2.2.2.2.2.1.m1.1.1.cmml" xref="S4.T4.2.2.2.2.2.2.2.2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.2.2.2.2.2.2.2.2.1.m1.1.1.1.cmml" xref="S4.T4.2.2.2.2.2.2.2.2.1.m1.1.1">subscript</csymbol><ci id="S4.T4.2.2.2.2.2.2.2.2.1.m1.1.1.2.cmml" xref="S4.T4.2.2.2.2.2.2.2.2.1.m1.1.1.2">â†“</ci><cn id="S4.T4.2.2.2.2.2.2.2.2.1.m1.1.1.3.cmml" type="integer" xref="S4.T4.2.2.2.2.2.2.2.2.1.m1.1.1.3">9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.2.2.2.2.2.1.m1.1c">\downarrow_{9}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.2.2.2.2.2.2.2.2.1.m1.1d">â†“ start_POSTSUBSCRIPT 9 end_POSTSUBSCRIPT</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.3.3.3.3.3.3.3.3.2" style="padding-left:4.0pt;padding-right:4.0pt;">41.08<math alttext="\downarrow_{6}" class="ltx_Math" display="inline" id="S4.T4.3.3.3.3.3.3.3.3.2.m1.1"><semantics id="S4.T4.3.3.3.3.3.3.3.3.2.m1.1a"><msub id="S4.T4.3.3.3.3.3.3.3.3.2.m1.1.1" xref="S4.T4.3.3.3.3.3.3.3.3.2.m1.1.1.cmml"><mo id="S4.T4.3.3.3.3.3.3.3.3.2.m1.1.1.2" stretchy="false" xref="S4.T4.3.3.3.3.3.3.3.3.2.m1.1.1.2.cmml">â†“</mo><mn id="S4.T4.3.3.3.3.3.3.3.3.2.m1.1.1.3" xref="S4.T4.3.3.3.3.3.3.3.3.2.m1.1.1.3.cmml">6</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T4.3.3.3.3.3.3.3.3.2.m1.1b"><apply id="S4.T4.3.3.3.3.3.3.3.3.2.m1.1.1.cmml" xref="S4.T4.3.3.3.3.3.3.3.3.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.3.3.3.3.3.3.3.3.2.m1.1.1.1.cmml" xref="S4.T4.3.3.3.3.3.3.3.3.2.m1.1.1">subscript</csymbol><ci id="S4.T4.3.3.3.3.3.3.3.3.2.m1.1.1.2.cmml" xref="S4.T4.3.3.3.3.3.3.3.3.2.m1.1.1.2">â†“</ci><cn id="S4.T4.3.3.3.3.3.3.3.3.2.m1.1.1.3.cmml" type="integer" xref="S4.T4.3.3.3.3.3.3.3.3.2.m1.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.3.3.3.3.3.3.3.2.m1.1c">\downarrow_{6}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.3.3.3.3.3.3.3.3.2.m1.1d">â†“ start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.4.4.4.4.4.4.4.4.3" style="padding-left:4.0pt;padding-right:4.0pt;">43.14 <math alttext="\downarrow_{12}" class="ltx_Math" display="inline" id="S4.T4.4.4.4.4.4.4.4.4.3.m1.1"><semantics id="S4.T4.4.4.4.4.4.4.4.4.3.m1.1a"><msub id="S4.T4.4.4.4.4.4.4.4.4.3.m1.1.1" xref="S4.T4.4.4.4.4.4.4.4.4.3.m1.1.1.cmml"><mo id="S4.T4.4.4.4.4.4.4.4.4.3.m1.1.1.2" stretchy="false" xref="S4.T4.4.4.4.4.4.4.4.4.3.m1.1.1.2.cmml">â†“</mo><mn id="S4.T4.4.4.4.4.4.4.4.4.3.m1.1.1.3" xref="S4.T4.4.4.4.4.4.4.4.4.3.m1.1.1.3.cmml">12</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T4.4.4.4.4.4.4.4.4.3.m1.1b"><apply id="S4.T4.4.4.4.4.4.4.4.4.3.m1.1.1.cmml" xref="S4.T4.4.4.4.4.4.4.4.4.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.4.4.4.4.4.4.4.4.3.m1.1.1.1.cmml" xref="S4.T4.4.4.4.4.4.4.4.4.3.m1.1.1">subscript</csymbol><ci id="S4.T4.4.4.4.4.4.4.4.4.3.m1.1.1.2.cmml" xref="S4.T4.4.4.4.4.4.4.4.4.3.m1.1.1.2">â†“</ci><cn id="S4.T4.4.4.4.4.4.4.4.4.3.m1.1.1.3.cmml" type="integer" xref="S4.T4.4.4.4.4.4.4.4.4.3.m1.1.1.3">12</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.4.4.4.4.4.4.4.3.m1.1c">\downarrow_{12}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.4.4.4.4.4.4.4.4.3.m1.1d">â†“ start_POSTSUBSCRIPT 12 end_POSTSUBSCRIPT</annotation></semantics></math></span>
<span class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T4.5.5.5.5.5.5.5.5.4" style="padding-left:4.0pt;padding-right:4.0pt;">66.51 <math alttext="\downarrow_{10}" class="ltx_Math" display="inline" id="S4.T4.5.5.5.5.5.5.5.5.4.m1.1"><semantics id="S4.T4.5.5.5.5.5.5.5.5.4.m1.1a"><msub id="S4.T4.5.5.5.5.5.5.5.5.4.m1.1.1" xref="S4.T4.5.5.5.5.5.5.5.5.4.m1.1.1.cmml"><mo id="S4.T4.5.5.5.5.5.5.5.5.4.m1.1.1.2" stretchy="false" xref="S4.T4.5.5.5.5.5.5.5.5.4.m1.1.1.2.cmml">â†“</mo><mn id="S4.T4.5.5.5.5.5.5.5.5.4.m1.1.1.3" xref="S4.T4.5.5.5.5.5.5.5.5.4.m1.1.1.3.cmml">10</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T4.5.5.5.5.5.5.5.5.4.m1.1b"><apply id="S4.T4.5.5.5.5.5.5.5.5.4.m1.1.1.cmml" xref="S4.T4.5.5.5.5.5.5.5.5.4.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.5.5.5.5.5.5.5.5.4.m1.1.1.1.cmml" xref="S4.T4.5.5.5.5.5.5.5.5.4.m1.1.1">subscript</csymbol><ci id="S4.T4.5.5.5.5.5.5.5.5.4.m1.1.1.2.cmml" xref="S4.T4.5.5.5.5.5.5.5.5.4.m1.1.1.2">â†“</ci><cn id="S4.T4.5.5.5.5.5.5.5.5.4.m1.1.1.3.cmml" type="integer" xref="S4.T4.5.5.5.5.5.5.5.5.4.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.5.5.5.5.5.5.5.5.4.m1.1c">\downarrow_{10}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.5.5.5.5.5.5.5.5.4.m1.1d">â†“ start_POSTSUBSCRIPT 10 end_POSTSUBSCRIPT</annotation></semantics></math></span></span>
<span class="ltx_tr" id="S4.T4.9.9.9.9.9.9.9.9">
<span class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.9.9.9.9.9.9.9.9.5" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.9.9.9.9.9.9.9.9.5.1">
<span class="ltx_p" id="S4.T4.9.9.9.9.9.9.9.9.5.1.1" style="width:62.6pt;"><span class="ltx_text ltx_font_italic" id="S4.T4.9.9.9.9.9.9.9.9.5.1.1.1">w/o grounding</span></span>
</span></span>
<span class="ltx_td ltx_align_center" id="S4.T4.6.6.6.6.6.6.6.6.1" style="padding-left:4.0pt;padding-right:4.0pt;">45.14<math alttext="\downarrow_{7}" class="ltx_Math" display="inline" id="S4.T4.6.6.6.6.6.6.6.6.1.m1.1"><semantics id="S4.T4.6.6.6.6.6.6.6.6.1.m1.1a"><msub id="S4.T4.6.6.6.6.6.6.6.6.1.m1.1.1" xref="S4.T4.6.6.6.6.6.6.6.6.1.m1.1.1.cmml"><mo id="S4.T4.6.6.6.6.6.6.6.6.1.m1.1.1.2" stretchy="false" xref="S4.T4.6.6.6.6.6.6.6.6.1.m1.1.1.2.cmml">â†“</mo><mn id="S4.T4.6.6.6.6.6.6.6.6.1.m1.1.1.3" xref="S4.T4.6.6.6.6.6.6.6.6.1.m1.1.1.3.cmml">7</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T4.6.6.6.6.6.6.6.6.1.m1.1b"><apply id="S4.T4.6.6.6.6.6.6.6.6.1.m1.1.1.cmml" xref="S4.T4.6.6.6.6.6.6.6.6.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.6.6.6.6.6.6.6.6.1.m1.1.1.1.cmml" xref="S4.T4.6.6.6.6.6.6.6.6.1.m1.1.1">subscript</csymbol><ci id="S4.T4.6.6.6.6.6.6.6.6.1.m1.1.1.2.cmml" xref="S4.T4.6.6.6.6.6.6.6.6.1.m1.1.1.2">â†“</ci><cn id="S4.T4.6.6.6.6.6.6.6.6.1.m1.1.1.3.cmml" type="integer" xref="S4.T4.6.6.6.6.6.6.6.6.1.m1.1.1.3">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.6.6.6.6.6.6.6.6.1.m1.1c">\downarrow_{7}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.6.6.6.6.6.6.6.6.1.m1.1d">â†“ start_POSTSUBSCRIPT 7 end_POSTSUBSCRIPT</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center" id="S4.T4.7.7.7.7.7.7.7.7.2" style="padding-left:4.0pt;padding-right:4.0pt;">41.35<math alttext="\downarrow_{4}" class="ltx_Math" display="inline" id="S4.T4.7.7.7.7.7.7.7.7.2.m1.1"><semantics id="S4.T4.7.7.7.7.7.7.7.7.2.m1.1a"><msub id="S4.T4.7.7.7.7.7.7.7.7.2.m1.1.1" xref="S4.T4.7.7.7.7.7.7.7.7.2.m1.1.1.cmml"><mo id="S4.T4.7.7.7.7.7.7.7.7.2.m1.1.1.2" stretchy="false" xref="S4.T4.7.7.7.7.7.7.7.7.2.m1.1.1.2.cmml">â†“</mo><mn id="S4.T4.7.7.7.7.7.7.7.7.2.m1.1.1.3" xref="S4.T4.7.7.7.7.7.7.7.7.2.m1.1.1.3.cmml">4</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T4.7.7.7.7.7.7.7.7.2.m1.1b"><apply id="S4.T4.7.7.7.7.7.7.7.7.2.m1.1.1.cmml" xref="S4.T4.7.7.7.7.7.7.7.7.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.7.7.7.7.7.7.7.7.2.m1.1.1.1.cmml" xref="S4.T4.7.7.7.7.7.7.7.7.2.m1.1.1">subscript</csymbol><ci id="S4.T4.7.7.7.7.7.7.7.7.2.m1.1.1.2.cmml" xref="S4.T4.7.7.7.7.7.7.7.7.2.m1.1.1.2">â†“</ci><cn id="S4.T4.7.7.7.7.7.7.7.7.2.m1.1.1.3.cmml" type="integer" xref="S4.T4.7.7.7.7.7.7.7.7.2.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.7.7.7.7.7.7.7.7.2.m1.1c">\downarrow_{4}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.7.7.7.7.7.7.7.7.2.m1.1d">â†“ start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center" id="S4.T4.8.8.8.8.8.8.8.8.3" style="padding-left:4.0pt;padding-right:4.0pt;">43.23<math alttext="\downarrow_{5}" class="ltx_Math" display="inline" id="S4.T4.8.8.8.8.8.8.8.8.3.m1.1"><semantics id="S4.T4.8.8.8.8.8.8.8.8.3.m1.1a"><msub id="S4.T4.8.8.8.8.8.8.8.8.3.m1.1.1" xref="S4.T4.8.8.8.8.8.8.8.8.3.m1.1.1.cmml"><mo id="S4.T4.8.8.8.8.8.8.8.8.3.m1.1.1.2" stretchy="false" xref="S4.T4.8.8.8.8.8.8.8.8.3.m1.1.1.2.cmml">â†“</mo><mn id="S4.T4.8.8.8.8.8.8.8.8.3.m1.1.1.3" xref="S4.T4.8.8.8.8.8.8.8.8.3.m1.1.1.3.cmml">5</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T4.8.8.8.8.8.8.8.8.3.m1.1b"><apply id="S4.T4.8.8.8.8.8.8.8.8.3.m1.1.1.cmml" xref="S4.T4.8.8.8.8.8.8.8.8.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.8.8.8.8.8.8.8.8.3.m1.1.1.1.cmml" xref="S4.T4.8.8.8.8.8.8.8.8.3.m1.1.1">subscript</csymbol><ci id="S4.T4.8.8.8.8.8.8.8.8.3.m1.1.1.2.cmml" xref="S4.T4.8.8.8.8.8.8.8.8.3.m1.1.1.2">â†“</ci><cn id="S4.T4.8.8.8.8.8.8.8.8.3.m1.1.1.3.cmml" type="integer" xref="S4.T4.8.8.8.8.8.8.8.8.3.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.8.8.8.8.8.8.8.8.3.m1.1c">\downarrow_{5}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.8.8.8.8.8.8.8.8.3.m1.1d">â†“ start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT</annotation></semantics></math></span>
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.9.9.9.9.9.9.9.9.4" style="padding-left:4.0pt;padding-right:4.0pt;">72.34<math alttext="\downarrow_{5}" class="ltx_Math" display="inline" id="S4.T4.9.9.9.9.9.9.9.9.4.m1.1"><semantics id="S4.T4.9.9.9.9.9.9.9.9.4.m1.1a"><msub id="S4.T4.9.9.9.9.9.9.9.9.4.m1.1.1" xref="S4.T4.9.9.9.9.9.9.9.9.4.m1.1.1.cmml"><mo id="S4.T4.9.9.9.9.9.9.9.9.4.m1.1.1.2" stretchy="false" xref="S4.T4.9.9.9.9.9.9.9.9.4.m1.1.1.2.cmml">â†“</mo><mn id="S4.T4.9.9.9.9.9.9.9.9.4.m1.1.1.3" xref="S4.T4.9.9.9.9.9.9.9.9.4.m1.1.1.3.cmml">5</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T4.9.9.9.9.9.9.9.9.4.m1.1b"><apply id="S4.T4.9.9.9.9.9.9.9.9.4.m1.1.1.cmml" xref="S4.T4.9.9.9.9.9.9.9.9.4.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.9.9.9.9.9.9.9.9.4.m1.1.1.1.cmml" xref="S4.T4.9.9.9.9.9.9.9.9.4.m1.1.1">subscript</csymbol><ci id="S4.T4.9.9.9.9.9.9.9.9.4.m1.1.1.2.cmml" xref="S4.T4.9.9.9.9.9.9.9.9.4.m1.1.1.2">â†“</ci><cn id="S4.T4.9.9.9.9.9.9.9.9.4.m1.1.1.3.cmml" type="integer" xref="S4.T4.9.9.9.9.9.9.9.9.4.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.9.9.9.9.9.9.9.9.4.m1.1c">\downarrow_{5}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.9.9.9.9.9.9.9.9.4.m1.1d">â†“ start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT</annotation></semantics></math></span></span>
<span class="ltx_tr" id="S4.T4.13.13.13.13.13.13.13.13">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T4.13.13.13.13.13.13.13.13.5" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.13.13.13.13.13.13.13.13.5.1">
<span class="ltx_p" id="S4.T4.13.13.13.13.13.13.13.13.5.1.1" style="width:62.6pt;"><span class="ltx_text ltx_font_italic" id="S4.T4.13.13.13.13.13.13.13.13.5.1.1.1">w/o batch</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.10.10.10.10.10.10.10.10.1" style="padding-left:4.0pt;padding-right:4.0pt;">47.27<math alttext="\downarrow_{5}" class="ltx_Math" display="inline" id="S4.T4.10.10.10.10.10.10.10.10.1.m1.1"><semantics id="S4.T4.10.10.10.10.10.10.10.10.1.m1.1a"><msub id="S4.T4.10.10.10.10.10.10.10.10.1.m1.1.1" xref="S4.T4.10.10.10.10.10.10.10.10.1.m1.1.1.cmml"><mo id="S4.T4.10.10.10.10.10.10.10.10.1.m1.1.1.2" stretchy="false" xref="S4.T4.10.10.10.10.10.10.10.10.1.m1.1.1.2.cmml">â†“</mo><mn id="S4.T4.10.10.10.10.10.10.10.10.1.m1.1.1.3" xref="S4.T4.10.10.10.10.10.10.10.10.1.m1.1.1.3.cmml">5</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T4.10.10.10.10.10.10.10.10.1.m1.1b"><apply id="S4.T4.10.10.10.10.10.10.10.10.1.m1.1.1.cmml" xref="S4.T4.10.10.10.10.10.10.10.10.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.10.10.10.10.10.10.10.10.1.m1.1.1.1.cmml" xref="S4.T4.10.10.10.10.10.10.10.10.1.m1.1.1">subscript</csymbol><ci id="S4.T4.10.10.10.10.10.10.10.10.1.m1.1.1.2.cmml" xref="S4.T4.10.10.10.10.10.10.10.10.1.m1.1.1.2">â†“</ci><cn id="S4.T4.10.10.10.10.10.10.10.10.1.m1.1.1.3.cmml" type="integer" xref="S4.T4.10.10.10.10.10.10.10.10.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.10.10.10.10.10.10.10.10.1.m1.1c">\downarrow_{5}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.10.10.10.10.10.10.10.10.1.m1.1d">â†“ start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.11.11.11.11.11.11.11.11.2" style="padding-left:4.0pt;padding-right:4.0pt;">45.03<math alttext="\downarrow_{2}" class="ltx_Math" display="inline" id="S4.T4.11.11.11.11.11.11.11.11.2.m1.1"><semantics id="S4.T4.11.11.11.11.11.11.11.11.2.m1.1a"><msub id="S4.T4.11.11.11.11.11.11.11.11.2.m1.1.1" xref="S4.T4.11.11.11.11.11.11.11.11.2.m1.1.1.cmml"><mo id="S4.T4.11.11.11.11.11.11.11.11.2.m1.1.1.2" stretchy="false" xref="S4.T4.11.11.11.11.11.11.11.11.2.m1.1.1.2.cmml">â†“</mo><mn id="S4.T4.11.11.11.11.11.11.11.11.2.m1.1.1.3" xref="S4.T4.11.11.11.11.11.11.11.11.2.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T4.11.11.11.11.11.11.11.11.2.m1.1b"><apply id="S4.T4.11.11.11.11.11.11.11.11.2.m1.1.1.cmml" xref="S4.T4.11.11.11.11.11.11.11.11.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.11.11.11.11.11.11.11.11.2.m1.1.1.1.cmml" xref="S4.T4.11.11.11.11.11.11.11.11.2.m1.1.1">subscript</csymbol><ci id="S4.T4.11.11.11.11.11.11.11.11.2.m1.1.1.2.cmml" xref="S4.T4.11.11.11.11.11.11.11.11.2.m1.1.1.2">â†“</ci><cn id="S4.T4.11.11.11.11.11.11.11.11.2.m1.1.1.3.cmml" type="integer" xref="S4.T4.11.11.11.11.11.11.11.11.2.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.11.11.11.11.11.11.11.11.2.m1.1c">\downarrow_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.11.11.11.11.11.11.11.11.2.m1.1d">â†“ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.12.12.12.12.12.12.12.12.3" style="padding-left:4.0pt;padding-right:4.0pt;">51.19<math alttext="\downarrow_{4}" class="ltx_Math" display="inline" id="S4.T4.12.12.12.12.12.12.12.12.3.m1.1"><semantics id="S4.T4.12.12.12.12.12.12.12.12.3.m1.1a"><msub id="S4.T4.12.12.12.12.12.12.12.12.3.m1.1.1" xref="S4.T4.12.12.12.12.12.12.12.12.3.m1.1.1.cmml"><mo id="S4.T4.12.12.12.12.12.12.12.12.3.m1.1.1.2" stretchy="false" xref="S4.T4.12.12.12.12.12.12.12.12.3.m1.1.1.2.cmml">â†“</mo><mn id="S4.T4.12.12.12.12.12.12.12.12.3.m1.1.1.3" xref="S4.T4.12.12.12.12.12.12.12.12.3.m1.1.1.3.cmml">4</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T4.12.12.12.12.12.12.12.12.3.m1.1b"><apply id="S4.T4.12.12.12.12.12.12.12.12.3.m1.1.1.cmml" xref="S4.T4.12.12.12.12.12.12.12.12.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.12.12.12.12.12.12.12.12.3.m1.1.1.1.cmml" xref="S4.T4.12.12.12.12.12.12.12.12.3.m1.1.1">subscript</csymbol><ci id="S4.T4.12.12.12.12.12.12.12.12.3.m1.1.1.2.cmml" xref="S4.T4.12.12.12.12.12.12.12.12.3.m1.1.1.2">â†“</ci><cn id="S4.T4.12.12.12.12.12.12.12.12.3.m1.1.1.3.cmml" type="integer" xref="S4.T4.12.12.12.12.12.12.12.12.3.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.12.12.12.12.12.12.12.12.3.m1.1c">\downarrow_{4}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.12.12.12.12.12.12.12.12.3.m1.1d">â†“ start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT</annotation></semantics></math></span>
<span class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T4.13.13.13.13.13.13.13.13.4" style="padding-left:4.0pt;padding-right:4.0pt;">71.72<math alttext="\downarrow_{5}" class="ltx_Math" display="inline" id="S4.T4.13.13.13.13.13.13.13.13.4.m1.1"><semantics id="S4.T4.13.13.13.13.13.13.13.13.4.m1.1a"><msub id="S4.T4.13.13.13.13.13.13.13.13.4.m1.1.1" xref="S4.T4.13.13.13.13.13.13.13.13.4.m1.1.1.cmml"><mo id="S4.T4.13.13.13.13.13.13.13.13.4.m1.1.1.2" stretchy="false" xref="S4.T4.13.13.13.13.13.13.13.13.4.m1.1.1.2.cmml">â†“</mo><mn id="S4.T4.13.13.13.13.13.13.13.13.4.m1.1.1.3" xref="S4.T4.13.13.13.13.13.13.13.13.4.m1.1.1.3.cmml">5</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T4.13.13.13.13.13.13.13.13.4.m1.1b"><apply id="S4.T4.13.13.13.13.13.13.13.13.4.m1.1.1.cmml" xref="S4.T4.13.13.13.13.13.13.13.13.4.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.13.13.13.13.13.13.13.13.4.m1.1.1.1.cmml" xref="S4.T4.13.13.13.13.13.13.13.13.4.m1.1.1">subscript</csymbol><ci id="S4.T4.13.13.13.13.13.13.13.13.4.m1.1.1.2.cmml" xref="S4.T4.13.13.13.13.13.13.13.13.4.m1.1.1.2">â†“</ci><cn id="S4.T4.13.13.13.13.13.13.13.13.4.m1.1.1.3.cmml" type="integer" xref="S4.T4.13.13.13.13.13.13.13.13.4.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.13.13.13.13.13.13.13.13.4.m1.1c">\downarrow_{5}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.13.13.13.13.13.13.13.13.4.m1.1d">â†“ start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT</annotation></semantics></math></span></span>
</span></span></span>
</span></span></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Evaluation results of our ablation study on two MHQA benchmarks.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental Setup</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Datasets</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">In line with previous researchÂ <cite class="ltx_cite ltx_citemacro_citep">(Lewis etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib16" title="">2020</a>; Xu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib40" title="">2024</a>)</cite>, we conduct experiments on four common-used MHQA benchmarks, namely
HotpotQA (HQA)Â <cite class="ltx_cite ltx_citemacro_citep">(Yang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib41" title="">2018</a>)</cite>, MuSiQue (MQA)Â <cite class="ltx_cite ltx_citemacro_citep">(Trivedi etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib34" title="">2022</a>)</cite>, 2Wikimultihopqa (WQA)Â <cite class="ltx_cite ltx_citemacro_citep">(Ho etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib10" title="">2020</a>)</cite>, and
StrategyQA (SQA)Â <cite class="ltx_cite ltx_citemacro_citep">(Geva etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib9" title="">2021</a>)</cite>.
StrategyQA is derived from BIG-bench<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/google/BIG-bench" title="">https://github.com/google/BIG-bench</a></span></span></span>.
For the remaining benchmarks, we randomly sample 1.4k questions adhering to RetGenÂ <cite class="ltx_cite ltx_citemacro_citep">(Shao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib27" title="">2023</a>)</cite> and Verify-and-EditÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib48" title="">2023</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Baselines</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We compare our method with <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.1">Generation w/ Retrieval</span> and <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.2">Generation w/o Retrieval</span> methods, respectively. The <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.3">w/</span> indicates <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.4">with</span> while the <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.5">w/</span> indicates <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.6">without</span>.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">The <span class="ltx_text ltx_font_italic" id="S5.SS2.p2.1.1">Generation w/o Retrieval</span> methods utilize the parametric knowledge of LLMs to answer questions. This includes
(1) <span class="ltx_text ltx_font_italic" id="S5.SS2.p2.1.2">CoT</span>Â <cite class="ltx_cite ltx_citemacro_citep">(Wei etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib38" title="">2022</a>)</cite>, which prompts the LLMs to a series of intermediate reasoning steps when answering a question;
(2) <span class="ltx_text ltx_font_italic" id="S5.SS2.p2.1.3">CoT-SC</span>Â <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib37" title="">2022b</a>)</cite>, which samples a diverse set of reasoning paths and then selects the most consistent answer;
and (3) <span class="ltx_text ltx_font_italic" id="S5.SS2.p2.1.4">GenRead</span>Â <cite class="ltx_cite ltx_citemacro_citep">(Yu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib44" title="">2022</a>)</cite>, which generates the answer by reading from the documents generated by LLMs.
The <span class="ltx_text ltx_font_italic" id="S5.SS2.p2.1.5">Generation w/ Retrieval</span> methods augment LLMs with retrievers to access external knowledge when answering questions, including:
(1) ReActÂ <cite class="ltx_cite ltx_citemacro_citep">(Yao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib43" title="">2022</a>)</cite>, which interleaves question generation, document retrieval, and knowledge incorporation to answer a question;
(2) GRGÂ <cite class="ltx_cite ltx_citemacro_citep">(Abdallah and Jatowt, <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib1" title="">2023</a>)</cite>, reading an answer from both retrieved documents and contextual documents generated by LLMs;
(3) RetGenÂ <cite class="ltx_cite ltx_citemacro_citep">(Shao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib27" title="">2023</a>)</cite>, which enhances LLMs with an iterative retrieval-generation synergy strategy to answer a multi-hop question.
(4) DSPÂ <cite class="ltx_cite ltx_citemacro_citep">(Khattab etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib13" title="">2022</a>)</cite>, a programming framework empowered by LLMs;
and (5) SearChainÂ <cite class="ltx_cite ltx_citemacro_citep">(Xu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib40" title="">2024</a>)</cite>, which dynamically interacts with the retriever to verify and correct the generated answers.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">Considering the complexity of multi-hop questions, we enhance GenRead and GRG with the chain-of-thought technique (<span class="ltx_text ltx_font_italic" id="S5.SS2.p3.1.1">w/ decomposition</span>), dividing the input question into sub-questions and using GenRead (or GRG) for each sub-question.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Evaluation Metrics</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.2">Following previous studiesÂ <cite class="ltx_cite ltx_citemacro_citep">(Xu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib40" title="">2024</a>; Ren etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib23" title="">2023</a>)</cite>, we use <span class="ltx_text ltx_font_italic" id="S5.SS3.p1.2.1">Accuracy</span> (Acc) and <span class="ltx_text ltx_font_italic" id="S5.SS3.p1.2.2">F1</span> metrics for evaluation. The <span class="ltx_text ltx_font_italic" id="S5.SS3.p1.2.3">accuracy</span> metric checks if the ground truth answer is in the generated answer, which is also named cover-EM.
The <span class="ltx_text ltx_font_italic" id="S5.SS3.p1.2.4">F1</span> score is used to measure the overlap between the generated answer and the ground truth answer, which represents the harmonic mean of precision and recall. Recall is determined by considering the number of overlaps with the correct answer tokens, while precision is determined by considering the number of overlaps with all generated tokens.
We also assess semantic accuracy (Acc<sup class="ltx_sup" id="S5.SS3.p1.2.5">â€ </sup>) using gpt-3.5-turbo-instruct<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/" title="">https://openai.com/</a></span></span></span> for a more thorough evaluation, which prompts the LLMs to evaluate the correctness of the generated answer taking the ground truth answer as reference.
In this work, we implement the Acc<sup class="ltx_sup" id="S5.SS3.p1.2.6">â€ </sup> using gpt-3.5-turbo-instruct and the full prompt for our evaluation can be found in Appendix I.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">To counter the potential bias of automatic metricsÂ <cite class="ltx_cite ltx_citemacro_citep">(Shi etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib28" title="">2023</a>)</cite>, we conduct a human evaluation, with three educated individuals assessing the <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.1">correctness</span> of 120 randomly sampled cases from four benchmarks on a three-scale rating.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Implementation Details</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">We utilize OpenAIâ€™s <span class="ltx_text ltx_font_italic" id="S5.SS4.p1.1.1">gpt-3.5-turbo</span> as the backbone for our framework and all baselines, with the decoding temperature set to 0 for deterministic generation, and batch size set to 3 in our batch grounding strategy. The open source model, Mistral-7BÂ <cite class="ltx_cite ltx_citemacro_citep">(Jiang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib11" title="">2023a</a>)</cite>, is also employed for comparison. We mainly use ColBERTv2Â <cite class="ltx_cite ltx_citemacro_citep">(Santhanam etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib25" title="">2021</a>)</cite> as the retriever, retrieving top-10 documents for each question. Alternately, BM25Â <cite class="ltx_cite ltx_citemacro_citep">(Robertson etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib24" title="">2009</a>)</cite>, Google Search<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://serper.dev/" title="">https://serper.dev/</a></span></span></span> serve as retrievers in our analysis experiment.
Following previous workÂ <cite class="ltx_cite ltx_citemacro_cite">Xu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib40" title="">2024</a>)</cite>, we use Wikipedia 2017 for HotpotQA, and a large-scale passage collection built on Wikipedia 2018 for other open-domain QA benchmarks.</p>
</div>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.1">Our instructional grounding distillation trains Mistral-7B with 50k synthetic examples.
We optimize the model using the deepspeed ZeRO strategyÂ <cite class="ltx_cite ltx_citemacro_citep">(Rasley etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib22" title="">2020</a>)</cite> with the learning rate of <math alttext="5e^{-5}" class="ltx_Math" display="inline" id="S5.SS4.p2.1.m1.1"><semantics id="S5.SS4.p2.1.m1.1a"><mrow id="S5.SS4.p2.1.m1.1.1" xref="S5.SS4.p2.1.m1.1.1.cmml"><mn id="S5.SS4.p2.1.m1.1.1.2" xref="S5.SS4.p2.1.m1.1.1.2.cmml">5</mn><mo id="S5.SS4.p2.1.m1.1.1.1" xref="S5.SS4.p2.1.m1.1.1.1.cmml">â¢</mo><msup id="S5.SS4.p2.1.m1.1.1.3" xref="S5.SS4.p2.1.m1.1.1.3.cmml"><mi id="S5.SS4.p2.1.m1.1.1.3.2" xref="S5.SS4.p2.1.m1.1.1.3.2.cmml">e</mi><mrow id="S5.SS4.p2.1.m1.1.1.3.3" xref="S5.SS4.p2.1.m1.1.1.3.3.cmml"><mo id="S5.SS4.p2.1.m1.1.1.3.3a" xref="S5.SS4.p2.1.m1.1.1.3.3.cmml">âˆ’</mo><mn id="S5.SS4.p2.1.m1.1.1.3.3.2" xref="S5.SS4.p2.1.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.1.m1.1b"><apply id="S5.SS4.p2.1.m1.1.1.cmml" xref="S5.SS4.p2.1.m1.1.1"><times id="S5.SS4.p2.1.m1.1.1.1.cmml" xref="S5.SS4.p2.1.m1.1.1.1"></times><cn id="S5.SS4.p2.1.m1.1.1.2.cmml" type="integer" xref="S5.SS4.p2.1.m1.1.1.2">5</cn><apply id="S5.SS4.p2.1.m1.1.1.3.cmml" xref="S5.SS4.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S5.SS4.p2.1.m1.1.1.3.1.cmml" xref="S5.SS4.p2.1.m1.1.1.3">superscript</csymbol><ci id="S5.SS4.p2.1.m1.1.1.3.2.cmml" xref="S5.SS4.p2.1.m1.1.1.3.2">ğ‘’</ci><apply id="S5.SS4.p2.1.m1.1.1.3.3.cmml" xref="S5.SS4.p2.1.m1.1.1.3.3"><minus id="S5.SS4.p2.1.m1.1.1.3.3.1.cmml" xref="S5.SS4.p2.1.m1.1.1.3.3"></minus><cn id="S5.SS4.p2.1.m1.1.1.3.3.2.cmml" type="integer" xref="S5.SS4.p2.1.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.1.m1.1c">5e^{-5}</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p2.1.m1.1d">5 italic_e start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math> and the weight decay coefficient of 0.01.
The training of our model can be done within 18 hours with 3 NVIDIA A100-PCIE-80GB GPUs.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Experimental Results</h2>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Experimental Results</h3>
<section class="ltx_paragraph" id="S6.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Overall performance.</h4>
<div class="ltx_para" id="S6.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S6.SS1.SSS0.Px1.p1.1">TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S4.T2" title="Table 2 â€£ 4.2 Training Objective â€£ 4 Generalization with Grounding Distillation â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_tag">2</span></a> presents the experiment results. Our framework surpasses others on all four datasets and all metrics. Specifically, on the HotpotQA dataset, our GenGround achieves Acc=47.27, F1=52.26, and Acc<math alttext="\dagger" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px1.p1.1.m1.1"><semantics id="S6.SS1.SSS0.Px1.p1.1.m1.1a"><mo id="S6.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S6.SS1.SSS0.Px1.p1.1.m1.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS0.Px1.p1.1.m1.1b"><ci id="S6.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S6.SS1.SSS0.Px1.p1.1.m1.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS0.Px1.p1.1.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.SSS0.Px1.p1.1.m1.1d">â€ </annotation></semantics></math>=55.73, considerably improving over the <span class="ltx_text ltx_font_italic" id="S6.SS1.SSS0.Px1.p1.1.1">Generate w/ Retriever</span> baselines. It also significantly outperforms retrieval-then-read baselines like DSPy and SearChain, with a 4-6 point increase in accuracy metrics across all datasets. The similar improvement is observed in our human evaluation results (see Appendix IV for more detials).
These results indicate that our method effectively utilizes world knowledge and LLMsâ€™ deductive abilities to answer questions.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Results with the smaller model.</h4>
<div class="ltx_para" id="S6.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S6.SS1.SSS0.Px2.p1.1">We further evaluate our method by swapping the backbone LLMs with the open source model, <em class="ltx_emph ltx_font_italic" id="S6.SS1.SSS0.Px2.p1.1.1">i.e.,</em> Mistral-7B, and repeating the experiment under the same conditions.
As shown in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S4.T3" title="Table 3 â€£ 4.2 Training Objective â€£ 4 Generalization with Grounding Distillation â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_tag">3</span></a>, we implement our methods in two ways with the Mistral-7B:
(1) directly prompting (vanilla);
(2) tuning with our proposed instructional grounding distillation (IGD).
We obverse that directly prompting Mistral-7B with our method yields better performance compared with baselines.
The instructional grounding distillation further improves overall performance significantly, <em class="ltx_emph ltx_font_italic" id="S6.SS1.SSS0.Px2.p1.1.2">e.g.,</em> pushing the Acc to 42.08 in the HotpotQA dataset (9.84% relative improvement) and 14.37 in the MusiqueQA dataset (26.4% relative improvement).</p>
</div>
<figure class="ltx_table" id="S6.T5">
<p class="ltx_p ltx_align_center" id="S6.T5.11"><span class="ltx_text ltx_inline-block" id="S6.T5.11.11" style="width:433.6pt;">
<span class="ltx_inline-block ltx_transformed_outer" id="S6.T5.11.11.11.11" style="width:247.5pt;height:234pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S6.T5.11.11.11.11.11"><span class="ltx_text" id="S6.T5.11.11.11.11.11.11">
<span class="ltx_tabular ltx_align_middle" id="S6.T5.11.11.11.11.11.11.11">
<span class="ltx_tr" id="S6.T5.1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S6.T5.1.1.1.1.1.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T5.1.1.1.1.1.1.1.1.2.1">
<span class="ltx_p" id="S6.T5.1.1.1.1.1.1.1.1.2.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.1.1.1.1.1.1.2.1.1.1">Method</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S6.T5.1.1.1.1.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.1.1.1.1.1.1.3.1">HQA</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S6.T5.1.1.1.1.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.1.1.1.1.1.1.4.1">MQA</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S6.T5.1.1.1.1.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.1.1.1.1.1.1.5.1">WQA</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S6.T5.1.1.1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.1.1.1.1.1.1.1.1">Average <math alttext="\Delta\downarrow" class="ltx_Math" display="inline" id="S6.T5.1.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S6.T5.1.1.1.1.1.1.1.1.1.1.m1.1a"><mrow id="S6.T5.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S6.T5.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mi id="S6.T5.1.1.1.1.1.1.1.1.1.1.m1.1.1.2" mathvariant="normal" xref="S6.T5.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml">Î”</mi><mo id="S6.T5.1.1.1.1.1.1.1.1.1.1.m1.1.1.1" stretchy="false" xref="S6.T5.1.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml">â†“</mo><mi id="S6.T5.1.1.1.1.1.1.1.1.1.1.m1.1.1.3" xref="S6.T5.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S6.T5.1.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S6.T5.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S6.T5.1.1.1.1.1.1.1.1.1.1.m1.1.1"><ci id="S6.T5.1.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S6.T5.1.1.1.1.1.1.1.1.1.1.m1.1.1.1">â†“</ci><ci id="S6.T5.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S6.T5.1.1.1.1.1.1.1.1.1.1.m1.1.1.2">Î”</ci><csymbol cd="latexml" id="S6.T5.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S6.T5.1.1.1.1.1.1.1.1.1.1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.1.1.1.1.1.1.1.1.1.1.m1.1c">\Delta\downarrow</annotation><annotation encoding="application/x-llamapun" id="S6.T5.1.1.1.1.1.1.1.1.1.1.m1.1d">roman_Î” â†“</annotation></semantics></math></span></span></span>
<span class="ltx_tr" id="S6.T5.2.2.2.2.2.2.2.2" style="background-color:#DBDBDB;">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_t ltx_colspan ltx_colspan_5" id="S6.T5.2.2.2.2.2.2.2.2.1"><span class="ltx_text ltx_font_italic" id="S6.T5.2.2.2.2.2.2.2.2.1.1" style="background-color:#DBDBDB;">Retriever <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S6.T5.2.2.2.2.2.2.2.2.1.1.m1.1"><semantics id="S6.T5.2.2.2.2.2.2.2.2.1.1.m1.1a"><mo id="S6.T5.2.2.2.2.2.2.2.2.1.1.m1.1.1" mathbackground="#DBDBDB" stretchy="false" xref="S6.T5.2.2.2.2.2.2.2.2.1.1.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S6.T5.2.2.2.2.2.2.2.2.1.1.m1.1b"><ci id="S6.T5.2.2.2.2.2.2.2.2.1.1.m1.1.1.cmml" xref="S6.T5.2.2.2.2.2.2.2.2.1.1.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.2.2.2.2.2.2.2.2.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S6.T5.2.2.2.2.2.2.2.2.1.1.m1.1d">â†’</annotation></semantics></math> BM25</span></span></span>
<span class="ltx_tr" id="S6.T5.11.11.11.11.11.11.11.12">
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T5.11.11.11.11.11.11.11.12.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T5.11.11.11.11.11.11.11.12.1.1">
<span class="ltx_p" id="S6.T5.11.11.11.11.11.11.11.12.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="S6.T5.11.11.11.11.11.11.11.12.1.1.1.1">Ours</span></span>
</span></span>
<span class="ltx_td ltx_align_center" id="S6.T5.11.11.11.11.11.11.11.12.2">42.21</span>
<span class="ltx_td ltx_align_center" id="S6.T5.11.11.11.11.11.11.11.12.3">18.32</span>
<span class="ltx_td ltx_align_center" id="S6.T5.11.11.11.11.11.11.11.12.4">40.32</span>
<span class="ltx_td ltx_align_center" id="S6.T5.11.11.11.11.11.11.11.12.5">-</span></span>
<span class="ltx_tr" id="S6.T5.3.3.3.3.3.3.3.3">
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T5.3.3.3.3.3.3.3.3.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T5.3.3.3.3.3.3.3.3.2.1">
<span class="ltx_p" id="S6.T5.3.3.3.3.3.3.3.3.2.1.1" style="width:56.9pt;">DSPy</span>
</span></span>
<span class="ltx_td ltx_align_center" id="S6.T5.3.3.3.3.3.3.3.3.3">40.86</span>
<span class="ltx_td ltx_align_center" id="S6.T5.3.3.3.3.3.3.3.3.4">15.32</span>
<span class="ltx_td ltx_align_center" id="S6.T5.3.3.3.3.3.3.3.3.5">30.85</span>
<span class="ltx_td ltx_align_center" id="S6.T5.3.3.3.3.3.3.3.3.1">5.27<math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T5.3.3.3.3.3.3.3.3.1.m1.1"><semantics id="S6.T5.3.3.3.3.3.3.3.3.1.m1.1a"><mo id="S6.T5.3.3.3.3.3.3.3.3.1.m1.1.1" stretchy="false" xref="S6.T5.3.3.3.3.3.3.3.3.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S6.T5.3.3.3.3.3.3.3.3.1.m1.1b"><ci id="S6.T5.3.3.3.3.3.3.3.3.1.m1.1.1.cmml" xref="S6.T5.3.3.3.3.3.3.3.3.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.3.3.3.3.3.3.3.3.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S6.T5.3.3.3.3.3.3.3.3.1.m1.1d">â†“</annotation></semantics></math></span></span>
<span class="ltx_tr" id="S6.T5.4.4.4.4.4.4.4.4">
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T5.4.4.4.4.4.4.4.4.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T5.4.4.4.4.4.4.4.4.2.1">
<span class="ltx_p" id="S6.T5.4.4.4.4.4.4.4.4.2.1.1" style="width:56.9pt;">GRG <span class="ltx_text ltx_font_italic" id="S6.T5.4.4.4.4.4.4.4.4.2.1.1.1">w/ dq</span></span>
</span></span>
<span class="ltx_td ltx_align_center" id="S6.T5.4.4.4.4.4.4.4.4.3">41.31</span>
<span class="ltx_td ltx_align_center" id="S6.T5.4.4.4.4.4.4.4.4.4">15.62</span>
<span class="ltx_td ltx_align_center" id="S6.T5.4.4.4.4.4.4.4.4.5">38.84</span>
<span class="ltx_td ltx_align_center" id="S6.T5.4.4.4.4.4.4.4.4.1">2.36<math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T5.4.4.4.4.4.4.4.4.1.m1.1"><semantics id="S6.T5.4.4.4.4.4.4.4.4.1.m1.1a"><mo id="S6.T5.4.4.4.4.4.4.4.4.1.m1.1.1" stretchy="false" xref="S6.T5.4.4.4.4.4.4.4.4.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S6.T5.4.4.4.4.4.4.4.4.1.m1.1b"><ci id="S6.T5.4.4.4.4.4.4.4.4.1.m1.1.1.cmml" xref="S6.T5.4.4.4.4.4.4.4.4.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.4.4.4.4.4.4.4.4.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S6.T5.4.4.4.4.4.4.4.4.1.m1.1d">â†“</annotation></semantics></math></span></span>
<span class="ltx_tr" id="S6.T5.5.5.5.5.5.5.5.5">
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T5.5.5.5.5.5.5.5.5.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T5.5.5.5.5.5.5.5.5.2.1">
<span class="ltx_p" id="S6.T5.5.5.5.5.5.5.5.5.2.1.1" style="width:56.9pt;">RetGen</span>
</span></span>
<span class="ltx_td ltx_align_center" id="S6.T5.5.5.5.5.5.5.5.5.3">39.12</span>
<span class="ltx_td ltx_align_center" id="S6.T5.5.5.5.5.5.5.5.5.4">8.41</span>
<span class="ltx_td ltx_align_center" id="S6.T5.5.5.5.5.5.5.5.5.5">35.83</span>
<span class="ltx_td ltx_align_center" id="S6.T5.5.5.5.5.5.5.5.5.1">6.50<math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T5.5.5.5.5.5.5.5.5.1.m1.1"><semantics id="S6.T5.5.5.5.5.5.5.5.5.1.m1.1a"><mo id="S6.T5.5.5.5.5.5.5.5.5.1.m1.1.1" stretchy="false" xref="S6.T5.5.5.5.5.5.5.5.5.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S6.T5.5.5.5.5.5.5.5.5.1.m1.1b"><ci id="S6.T5.5.5.5.5.5.5.5.5.1.m1.1.1.cmml" xref="S6.T5.5.5.5.5.5.5.5.5.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.5.5.5.5.5.5.5.5.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S6.T5.5.5.5.5.5.5.5.5.1.m1.1d">â†“</annotation></semantics></math></span></span>
<span class="ltx_tr" id="S6.T5.6.6.6.6.6.6.6.6">
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T5.6.6.6.6.6.6.6.6.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T5.6.6.6.6.6.6.6.6.2.1">
<span class="ltx_p" id="S6.T5.6.6.6.6.6.6.6.6.2.1.1" style="width:56.9pt;">SearChain</span>
</span></span>
<span class="ltx_td ltx_align_center" id="S6.T5.6.6.6.6.6.6.6.6.3">39.57</span>
<span class="ltx_td ltx_align_center" id="S6.T5.6.6.6.6.6.6.6.6.4">14.93</span>
<span class="ltx_td ltx_align_center" id="S6.T5.6.6.6.6.6.6.6.6.5">37.41</span>
<span class="ltx_td ltx_align_center" id="S6.T5.6.6.6.6.6.6.6.6.1">3.65<math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T5.6.6.6.6.6.6.6.6.1.m1.1"><semantics id="S6.T5.6.6.6.6.6.6.6.6.1.m1.1a"><mo id="S6.T5.6.6.6.6.6.6.6.6.1.m1.1.1" stretchy="false" xref="S6.T5.6.6.6.6.6.6.6.6.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S6.T5.6.6.6.6.6.6.6.6.1.m1.1b"><ci id="S6.T5.6.6.6.6.6.6.6.6.1.m1.1.1.cmml" xref="S6.T5.6.6.6.6.6.6.6.6.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.6.6.6.6.6.6.6.6.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S6.T5.6.6.6.6.6.6.6.6.1.m1.1d">â†“</annotation></semantics></math></span></span>
<span class="ltx_tr" id="S6.T5.7.7.7.7.7.7.7.7" style="background-color:#DBDBDB;">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_t ltx_colspan ltx_colspan_5" id="S6.T5.7.7.7.7.7.7.7.7.1"><span class="ltx_text ltx_font_italic" id="S6.T5.7.7.7.7.7.7.7.7.1.1" style="background-color:#DBDBDB;">Retriever <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S6.T5.7.7.7.7.7.7.7.7.1.1.m1.1"><semantics id="S6.T5.7.7.7.7.7.7.7.7.1.1.m1.1a"><mo id="S6.T5.7.7.7.7.7.7.7.7.1.1.m1.1.1" mathbackground="#DBDBDB" stretchy="false" xref="S6.T5.7.7.7.7.7.7.7.7.1.1.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S6.T5.7.7.7.7.7.7.7.7.1.1.m1.1b"><ci id="S6.T5.7.7.7.7.7.7.7.7.1.1.m1.1.1.cmml" xref="S6.T5.7.7.7.7.7.7.7.7.1.1.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.7.7.7.7.7.7.7.7.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S6.T5.7.7.7.7.7.7.7.7.1.1.m1.1d">â†’</annotation></semantics></math> Google Search</span></span></span>
<span class="ltx_tr" id="S6.T5.11.11.11.11.11.11.11.13">
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T5.11.11.11.11.11.11.11.13.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T5.11.11.11.11.11.11.11.13.1.1">
<span class="ltx_p" id="S6.T5.11.11.11.11.11.11.11.13.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="S6.T5.11.11.11.11.11.11.11.13.1.1.1.1">Ours</span></span>
</span></span>
<span class="ltx_td ltx_align_center" id="S6.T5.11.11.11.11.11.11.11.13.2">48.95</span>
<span class="ltx_td ltx_align_center" id="S6.T5.11.11.11.11.11.11.11.13.3">21.54</span>
<span class="ltx_td ltx_align_center" id="S6.T5.11.11.11.11.11.11.11.13.4">46.87</span>
<span class="ltx_td ltx_align_center" id="S6.T5.11.11.11.11.11.11.11.13.5">-</span></span>
<span class="ltx_tr" id="S6.T5.8.8.8.8.8.8.8.8">
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T5.8.8.8.8.8.8.8.8.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T5.8.8.8.8.8.8.8.8.2.1">
<span class="ltx_p" id="S6.T5.8.8.8.8.8.8.8.8.2.1.1" style="width:56.9pt;">DSPy</span>
</span></span>
<span class="ltx_td ltx_align_center" id="S6.T5.8.8.8.8.8.8.8.8.3">46.86</span>
<span class="ltx_td ltx_align_center" id="S6.T5.8.8.8.8.8.8.8.8.4">20.71</span>
<span class="ltx_td ltx_align_center" id="S6.T5.8.8.8.8.8.8.8.8.5">39.92</span>
<span class="ltx_td ltx_align_center" id="S6.T5.8.8.8.8.8.8.8.8.1">3.29<math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T5.8.8.8.8.8.8.8.8.1.m1.1"><semantics id="S6.T5.8.8.8.8.8.8.8.8.1.m1.1a"><mo id="S6.T5.8.8.8.8.8.8.8.8.1.m1.1.1" stretchy="false" xref="S6.T5.8.8.8.8.8.8.8.8.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S6.T5.8.8.8.8.8.8.8.8.1.m1.1b"><ci id="S6.T5.8.8.8.8.8.8.8.8.1.m1.1.1.cmml" xref="S6.T5.8.8.8.8.8.8.8.8.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.8.8.8.8.8.8.8.8.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S6.T5.8.8.8.8.8.8.8.8.1.m1.1d">â†“</annotation></semantics></math></span></span>
<span class="ltx_tr" id="S6.T5.9.9.9.9.9.9.9.9">
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T5.9.9.9.9.9.9.9.9.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T5.9.9.9.9.9.9.9.9.2.1">
<span class="ltx_p" id="S6.T5.9.9.9.9.9.9.9.9.2.1.1" style="width:56.9pt;">GRG <span class="ltx_text ltx_font_italic" id="S6.T5.9.9.9.9.9.9.9.9.2.1.1.1">w/ dq</span></span>
</span></span>
<span class="ltx_td ltx_align_center" id="S6.T5.9.9.9.9.9.9.9.9.3">42.57</span>
<span class="ltx_td ltx_align_center" id="S6.T5.9.9.9.9.9.9.9.9.4">18.41</span>
<span class="ltx_td ltx_align_center" id="S6.T5.9.9.9.9.9.9.9.9.5">43.21</span>
<span class="ltx_td ltx_align_center" id="S6.T5.9.9.9.9.9.9.9.9.1">4.39<math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T5.9.9.9.9.9.9.9.9.1.m1.1"><semantics id="S6.T5.9.9.9.9.9.9.9.9.1.m1.1a"><mo id="S6.T5.9.9.9.9.9.9.9.9.1.m1.1.1" stretchy="false" xref="S6.T5.9.9.9.9.9.9.9.9.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S6.T5.9.9.9.9.9.9.9.9.1.m1.1b"><ci id="S6.T5.9.9.9.9.9.9.9.9.1.m1.1.1.cmml" xref="S6.T5.9.9.9.9.9.9.9.9.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.9.9.9.9.9.9.9.9.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S6.T5.9.9.9.9.9.9.9.9.1.m1.1d">â†“</annotation></semantics></math></span></span>
<span class="ltx_tr" id="S6.T5.10.10.10.10.10.10.10.10">
<span class="ltx_td ltx_align_justify ltx_align_top" id="S6.T5.10.10.10.10.10.10.10.10.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T5.10.10.10.10.10.10.10.10.2.1">
<span class="ltx_p" id="S6.T5.10.10.10.10.10.10.10.10.2.1.1" style="width:56.9pt;">RetGen</span>
</span></span>
<span class="ltx_td ltx_align_center" id="S6.T5.10.10.10.10.10.10.10.10.3">42.82</span>
<span class="ltx_td ltx_align_center" id="S6.T5.10.10.10.10.10.10.10.10.4">14.27</span>
<span class="ltx_td ltx_align_center" id="S6.T5.10.10.10.10.10.10.10.10.5">44.31</span>
<span class="ltx_td ltx_align_center" id="S6.T5.10.10.10.10.10.10.10.10.1">5.32<math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T5.10.10.10.10.10.10.10.10.1.m1.1"><semantics id="S6.T5.10.10.10.10.10.10.10.10.1.m1.1a"><mo id="S6.T5.10.10.10.10.10.10.10.10.1.m1.1.1" stretchy="false" xref="S6.T5.10.10.10.10.10.10.10.10.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S6.T5.10.10.10.10.10.10.10.10.1.m1.1b"><ci id="S6.T5.10.10.10.10.10.10.10.10.1.m1.1.1.cmml" xref="S6.T5.10.10.10.10.10.10.10.10.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.10.10.10.10.10.10.10.10.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S6.T5.10.10.10.10.10.10.10.10.1.m1.1d">â†“</annotation></semantics></math></span></span>
<span class="ltx_tr" id="S6.T5.11.11.11.11.11.11.11.11">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S6.T5.11.11.11.11.11.11.11.11.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T5.11.11.11.11.11.11.11.11.2.1">
<span class="ltx_p" id="S6.T5.11.11.11.11.11.11.11.11.2.1.1" style="width:56.9pt;">SearChain</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.11.11.11.11.11.11.11.11.3">44.35</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.11.11.11.11.11.11.11.11.4">19.76</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.11.11.11.11.11.11.11.11.5">44.39</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.11.11.11.11.11.11.11.11.1">2.95<math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T5.11.11.11.11.11.11.11.11.1.m1.1"><semantics id="S6.T5.11.11.11.11.11.11.11.11.1.m1.1a"><mo id="S6.T5.11.11.11.11.11.11.11.11.1.m1.1.1" stretchy="false" xref="S6.T5.11.11.11.11.11.11.11.11.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S6.T5.11.11.11.11.11.11.11.11.1.m1.1b"><ci id="S6.T5.11.11.11.11.11.11.11.11.1.m1.1.1.cmml" xref="S6.T5.11.11.11.11.11.11.11.11.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.11.11.11.11.11.11.11.11.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S6.T5.11.11.11.11.11.11.11.11.1.m1.1d">â†“</annotation></semantics></math></span></span>
</span></span></span>
</span></span></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Accuracy (Acc) on three datasets using BM25 and Google Search as retrievers, respectively.
The <span class="ltx_text ltx_font_italic" id="S6.T5.14.1">w/ dq</span> is short for <span class="ltx_text ltx_font_italic" id="S6.T5.15.2">without decomposition</span>.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S6.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Results with different retrievers.</h4>
<div class="ltx_para" id="S6.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S6.SS1.SSS0.Px3.p1.1">We further evaluate the performance of our framework by using different retrievers in various retrieval scenarios. As shown in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S6.T5" title="Table 5 â€£ Results with the smaller model. â€£ 6.1 Experimental Results â€£ 6 Experimental Results â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_tag">5</span></a>, we replace ColBERTv2 with BM25 and Google Search, using ChatGPT as a backbone LLM in all instances. Our method demonstrates the best performance regardless of the retriever used, indicating its adaptability in both low recall (BM25) and high recall (Google Search) scenarios. This could be due to our answer deduction phase, which uses LLMsâ€™ parametric knowledge to supplement the retrieved knowledge. Moreover, our instructional knowledge grounding phase effectively incorporates the retrieved document by citing the most relevant evidence, mitigating the negative impact of noisy documents.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Ablation Study</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">We employ the following modifications and repeat the experiment in the same setting as TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S4.T2" title="Table 2 â€£ 4.2 Training Objective â€£ 4 Generalization with Grounding Distillation â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.p2.1.1">w/o deduction.</span>
We remove the <span class="ltx_text ltx_font_italic" id="S6.SS2.p2.1.2">answer deduction</span> phase mentioned in SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S3.SS1" title="3.1 Answer Deduction â€£ 3 Generate-then-Ground with LLMs â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_tag">3.1</span></a>, prompting the LLMs to directly generate an answer for a multi-hop question and revise it.
As illustrated in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S4.T4" title="Table 4 â€£ 4.2 Training Objective â€£ 4 Generalization with Grounding Distillation â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_tag">4</span></a>, we observe 6 and 10 absolute decreases in HQA and SQA datasets in terms of the Acc metric, respectively.
These results demonstrate the necessity of deducing the intricate knowledge in our answer deduction phase.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS2.p3">
<p class="ltx_p" id="S6.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.p3.1.1">w/o grounding.</span>
We replace the instructional knowledge grounding phase mentioned in SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S3.SS2" title="3.2 Instructional Knowledge Grounding â€£ 3 Generate-then-Ground with LLMs â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_tag">3.2</span></a>) with directly generating an answer using retrieved documents
As shown in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S4.T4" title="Table 4 â€£ 4.2 Training Objective â€£ 4 Generalization with Grounding Distillation â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_tag">4</span></a>, the F1 and Acc metrics have a significant decline.
A potential reason is that the LLMs may hallucinate the plausible-looking statement in the retrieved documents.
Our instructional knowledge grounding method further instructs the LLMs to find the most relevant evidence.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS2.p4">
<p class="ltx_p" id="S6.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.p4.1.1">w/o batch.</span>
We remove the <span class="ltx_text ltx_font_italic" id="S6.SS2.p4.1.2">batch grounding</span> strategy in SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S4.T2" title="Table 2 â€£ 4.2 Training Objective â€£ 4 Generalization with Grounding Distillation â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_tag">2</span></a>.
As shown in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S4.T4" title="Table 4 â€£ 4.2 Training Objective â€£ 4 Generalization with Grounding Distillation â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_tag">4</span></a>, the F1 decreases from 52.26 to 47.27 and the Acc decreases from 47.27 to 45.03.
These comparisons indicate that the LLMs struggle to generate correct answers when the reference document list is lengthy with irrelevant information.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Case Study</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">We conduct several case studies and find that GenGround is more effective at generating high quality answers to a question.
Details can be found in Appendix II.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Analysis and Discussion</h2>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>Result Consistency and Stability</h3>
<div class="ltx_para" id="S7.SS1.p1">
<p class="ltx_p" id="S7.SS1.p1.1">We further explore the consistency and stability of our framework.
Specifically, we repeat the experiment with the same setting as TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S4.T2" title="Table 2 â€£ 4.2 Training Objective â€£ 4 Generalization with Grounding Distillation â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_tag">2</span></a> in the HotpotQA dataset.
The statistical significance of differences observed between the performance of two runs is tested using a two-tailed paired t-test.
We find no significant difference between the results of two randomly conducted experiments (significance level <math alttext="\alpha" class="ltx_Math" display="inline" id="S7.SS1.p1.1.m1.1"><semantics id="S7.SS1.p1.1.m1.1a"><mi id="S7.SS1.p1.1.m1.1.1" xref="S7.SS1.p1.1.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S7.SS1.p1.1.m1.1b"><ci id="S7.SS1.p1.1.m1.1.1.cmml" xref="S7.SS1.p1.1.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p1.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p1.1.m1.1d">italic_Î±</annotation></semantics></math> = 0.05).
We further explore the <span class="ltx_text ltx_font_italic" id="S7.SS1.p1.1.1">fine-granularity stability</span> for our <span class="ltx_text ltx_font_italic" id="S7.SS1.p1.1.2">answer deduction</span> and <span class="ltx_text ltx_font_italic" id="S7.SS1.p1.1.3">instruction knowledge grounding</span> phases. Specifically, we compute the Rough score for the trajectory of our method in two repeated runs. The Rough-1, 2, and L are 81.33, 53.7, and 79.7, which shows the high lexicon similarity for the output, indicating the stability of our method.</p>
</div>
<figure class="ltx_figure" id="S7.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S7.F5.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The fine-granularity <span class="ltx_text ltx_font_italic" id="S7.F5.2.1">correctness</span> analysis of our answer deduction and knowledge grounding phases.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span>Knowledge Incorporation</h3>
<div class="ltx_para" id="S7.SS2.p1">
<p class="ltx_p" id="S7.SS2.p1.1">Our method combines the knowledge in LLMsâ€™ parameters and external documents to answer a question. We explore the synergistic integration of these two distinct knowledge sources. Specifically, we calculate the following three metrics:
(1) <span class="ltx_text ltx_font_italic" id="S7.SS2.p1.1.1">Success rate</span>: the rate at which LLMs either directly generate a correct answer or accurately revise an incorrectly generated answer;
(2) <span class="ltx_text ltx_font_italic" id="S7.SS2.p1.1.2">Failure rate</span>: LLMs generate a wrong answer and fail to correct it;
(3) <span class="ltx_text ltx_font_italic" id="S7.SS2.p1.1.3">Error rate</span>: LLMs generate a correct answer but incorrectly revise it.</p>
</div>
<div class="ltx_para" id="S7.SS2.p2">
<p class="ltx_p" id="S7.SS2.p2.1">Our method addresses multi-hop questions step-by-step. As existing datasets lack the ground truth answer for immediate answering trajectory, we invite three annotators to evaluate 100 randomly sampled cases from the Hotpot QA dataset in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S4.T2" title="Table 2 â€£ 4.2 Training Objective â€£ 4 Generalization with Grounding Distillation â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div class="ltx_para" id="S7.SS2.p3">
<p class="ltx_p" id="S7.SS2.p3.1">As illustrated in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S7.F5" title="Figure 5 â€£ 7.1 Result Consistency and Stability â€£ 7 Analysis and Discussion â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_tag">5</span></a>, the overall <span class="ltx_text ltx_font_italic" id="S7.SS2.p3.1.1">success rate</span> is 53.2%, with LLMs directly answering 28.7% of questions correctly. For 24.5% of the questions though, LLMs initially generate non-factual statements, and then use external documents for revision. These results underscore the importance of incorporating both knowledge sources.
During our grounding phase, LLMs may be misled by plausible-looking statements in the retrieved documents. Therefore, we further calculate the <span class="ltx_text ltx_font_bold" id="S7.SS2.p3.1.2">error rate</span>, which assesses how often LLMs are incorrect after revisions. We find that the error rate is only 5.6%, indicating that LLMs usually use the retrieved documents effectively.</p>
</div>
<figure class="ltx_figure" id="S7.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S7.F6.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>The efficiency analysis for different methods, where we count the number of consumed tokens and compute the average consumption <math alttext="\mu" class="ltx_Math" display="inline" id="S7.F6.2.m1.1"><semantics id="S7.F6.2.m1.1b"><mi id="S7.F6.2.m1.1.1" xref="S7.F6.2.m1.1.1.cmml">Î¼</mi><annotation-xml encoding="MathML-Content" id="S7.F6.2.m1.1c"><ci id="S7.F6.2.m1.1.1.cmml" xref="S7.F6.2.m1.1.1">ğœ‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.F6.2.m1.1d">\mu</annotation><annotation encoding="application/x-llamapun" id="S7.F6.2.m1.1e">italic_Î¼</annotation></semantics></math>.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S7.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3 </span>Qualitative Analysis for Efficiency</h3>
<div class="ltx_para" id="S7.SS3.p1">
<p class="ltx_p" id="S7.SS3.p1.1">The intensive parameters of LLMs typically raise concern about inference cost. Thus, we compare <span class="ltx_text ltx_font_bold" id="S7.SS3.p1.1.1">token consumption</span> with <span class="ltx_text ltx_font_italic" id="S7.SS3.p1.1.2">GRG w/ decomposition</span> and <span class="ltx_text ltx_font_italic" id="S7.SS3.p1.1.3">RetGen</span>, using the HotpotQA dataset in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S4.T2" title="Table 2 â€£ 4.2 Training Objective â€£ 4 Generalization with Grounding Distillation â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_tag">2</span></a>.
We show the frequency histogram for the number of consumed tokens in different methods in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S7.F6" title="Figure 6 â€£ 7.2 Knowledge Incorporation â€£ 7 Analysis and Discussion â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_tag">6</span></a>.
Though our framework achieves better performance, we observe that our method spends fewer tokens compared with strong baselines <span class="ltx_text ltx_font_italic" id="S7.SS3.p1.1.4">RetGen</span> and <span class="ltx_text ltx_font_italic" id="S7.SS3.p1.1.5">GRG w/ decomposition</span>.
The potential reason is that our framework benefits from the deduction capability of LLMs to decompose a multi-hop question into simpler sub-questions and generate an answer directly, leading to a shorter reasoning trajectory.</p>
</div>
<div class="ltx_para" id="S7.SS3.p2">
<p class="ltx_p" id="S7.SS3.p2.1">We also train Mistral-7B using different amounts of randomly sampled examples to investigate the impact of <span class="ltx_text ltx_font_bold" id="S7.SS3.p2.1.1">data scale</span> on the effectiveness of our instructional grounding distillation (IGD). We notice a slight decrease in performance as the amount of data reduces. For instance, when training with 45k examples, our method achieves Acc=42.08; but with 20k examples, it achieves Acc=40.12. All results are averaged over three runs. We also observe that our IGD allows Mistral-7B to achieve performance comparable to, and sometimes even better than, strong baselines using ChatGPT as the backbone, such as ReAct.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Conclusion</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">We present a generate-then-ground (GenGround) framework for multi-hop question answering tasks, synergizing the parametric knowledge of LLMs and external documents to solve a multi-hop question.
Given a multi-hop question, our GenGround enable LLMs to alternate two phases until the final answer is derived:
(1) formulate a simpler, single-hop question and directly generate the answer;
(2) ground the question-answer pair in retrieved documents, amending any wrong predictions in the answer.
To generalize our framework into smaller models, we also propose an instructional grounding distillation method.
Extensive experiments conducted on four datasets illustrate the superiority of our framework.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitations</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">Despite the promising results demonstrated in this paper, there are several limitations to our framework.</p>
<ul class="ltx_itemize" id="Sx1.I1">
<li class="ltx_item" id="Sx1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="Sx1.I1.i1.p1">
<p class="ltx_p" id="Sx1.I1.i1.p1.1">The first step of our framework involves generating an initial answer, which may be highly dependent on the task at hand. For different tasks, the model may struggle to generate a meaningful or useful initial answer, limiting its applicability across diverse problem domains.</p>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="Sx1.I1.i2.p1">
<p class="ltx_p" id="Sx1.I1.i2.p1.1">Our approach assumes that complex questions can be broken down into simpler ones. However, the task of decomposing complex questions is itself a challenging problem and has not been fully explored in our current framework.</p>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="Sx1.I1.i3.p1">
<p class="ltx_p" id="Sx1.I1.i3.p1.1">Our approach assumes that external documents can be used to correct initially non-factual statements generated by the model. However, if these sources do not contain the necessary information for correction, or if they contain misinformation, our frameworkâ€™s effectiveness could be compromised.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Ethics Statement</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">The research conducted in this paper centers around the development of a generate-then-ground framework for multi-hop question answering. Our framework enables Language Learning Models (LLMs) to alternately deduce answers and utilize established evidence to revise those answers across several iterations to solve multi-hop questions.</p>
</div>
<div class="ltx_para" id="Sx2.p2">
<p class="ltx_p" id="Sx2.p2.1">In the process of conducting this research, we have adhered to ethical standards to ensure the integrity and validity of our work. All the questions used in this study were obtained from existing benchmarks, thus ensuring a high level of transparency and reproducibility in our experimental procedure.</p>
</div>
<div class="ltx_para" id="Sx2.p3">
<p class="ltx_p" id="Sx2.p3.1">Furthermore, to support our retrieval system, we used an open source corpus, specifically, Wikipedia. This ensures that our research utilizes publicly accessible and freely available data, minimizing potential bias and promoting fairness.</p>
</div>
<div class="ltx_para" id="Sx2.p4">
<p class="ltx_p" id="Sx2.p4.1">We have made every effort to ensure that our research does not harm individuals or groups, nor does it involve any form of deception or potential misuse of information.</p>
</div>
</section>
<section class="ltx_section" id="Sx3">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx3.p1">
<p class="ltx_p" id="Sx3.p1.1">This work was supported by the Natural Science Foundation of China (62102234, 62372275, 62272274, 62202271, T2293773, 62072279), the National Key R&amp;D Program of China with grant No.2022YFC3303004, the Natural Science Foundation of Shandong Province (ZR2021QF129).
All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abdallah and Jatowt (2023)</span>
<span class="ltx_bibblock">
Abdelrahman Abdallah and Adam Jatowt. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/https://doi.org/https://arxiv.org/abs/2307.11278" title="">Generator-retriever-generator: A novel approach to open-domain question answering</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2307.11278</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adlakha etÂ al. (2024)</span>
<span class="ltx_bibblock">
Vaibhav Adlakha, Parishad BehnamGhader, XingÂ Han Lu, Nicholas Meade, and Siva Reddy. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/https://doi.org/10.1162/tacl_a_00667" title="">Evaluating correctness and faithfulness of instruction-following models for question answering</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Transactions of the Association for Computational Linguistics: TACL</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Besta etÂ al. (2023)</span>
<span class="ltx_bibblock">
Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, etÂ al. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/https://doi.org/10.1609/aaai.v38i16.29720" title="">Graph of thoughts: Solving elaborate problems with large language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the AAAI Conference on Artificial Intelligence: AAAI</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2023)</span>
<span class="ltx_bibblock">
Liang Chen, Yang Deng, Yatao Bian, Zeyu Qin, Bingzhe Wu, Tat-Seng Chua, and Kam-Fai Wong. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.emnlp-main.390" title="">Beyond factuality: A comprehensive evaluation of large language models as knowledge generators</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: EMNLP</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng etÂ al. (2023)</span>
<span class="ltx_bibblock">
Zhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, and Bing Qin. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.findings-emnlp.620" title="">Retrieval-generation synergy augmented large language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: EMNLP</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, ArunÂ Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, NiÂ Lao, Hongrae Lee, Da-Cheng Juan, etÂ al. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.acl-long.910" title="">Rarr: Researching and revising what language models say, using language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics: ACL</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al. (2024)</span>
<span class="ltx_bibblock">
Shen Gao, Zhengliang Shi, Minghang Zhu, Bowen Fang, Xin Xin, Pengjie Ren, Zhumin Chen, and Jun Ma. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/https://doi.org/10.1609/aaai.v38i16.29759" title="">Confucius: Iterative tool learning from introspection feedback by easy-to-difficult curriculum</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, YiÂ Dai, Jiawei Sun, and Haofen Wang. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/https://doi.org/10.48550/arXiv.2312.10997" title="">Retrieval-augmented generation for large language models: A survey</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2312.10997</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geva etÂ al. (2021)</span>
<span class="ltx_bibblock">
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00370" title="">Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Transactions of the Association for Computational Linguistics (TACL)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho etÂ al. (2020)</span>
<span class="ltx_bibblock">
Xanh Ho, Anh-Khoa DuongÂ Nguyen, Saku Sugawara, and Akiko Aizawa. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.coling-main.580" title="">Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the 28th International Conference on Computational Linguistics</em>. International Committee on Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang etÂ al. (2023a)</span>
<span class="ltx_bibblock">
AlbertÂ Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, DevendraÂ Singh Chaplot, Diego deÂ LasÂ Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lâ€™elioÂ Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, TevenÂ Le Scao, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, and WilliamÂ El Sayed. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:263830494" title="">Mistral 7b</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">ArXiv</em>, abs/2310.06825.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Zhengbao Jiang, FrankÂ F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.emnlp-main.495" title="">Active retrieval augmented generation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: EMNLP</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khattab etÂ al. (2022)</span>
<span class="ltx_bibblock">
Omar Khattab, Keshav Santhanam, XiangÂ Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/https://doi.org/10.48550/arXiv.2212.14024" title="">Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive NLP</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2212.14024</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khattab etÂ al. (2023)</span>
<span class="ltx_bibblock">
Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, ThomasÂ T. Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, and Christopher Potts. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/https://doi.org/10.48550/arXiv.2310.03714" title="">Dspy: Compiling declarative language model calls into self-improving pipelines</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2310.03714</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski etÂ al. (2019)</span>
<span class="ltx_bibblock">
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, etÂ al. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00276" title="">Natural questions: a benchmark for question answering research</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Transactions of the Association for Computational Linguistics</em>, 7:453â€“466.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis etÂ al. (2020)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen tau Yih, Tim RocktÃ¤schel, Sebastian Riedel, and Douwe Kiela. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/https://dl.acm.org/doi/abs/10.5555/3495724.3496517" title="">Retrieval-augmented generation for knowledge-intensive nlp tasks</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the 34th International Conference on Neural Information Processing Systems</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Du (2023)</span>
<span class="ltx_bibblock">
Ruosen Li and Xinya Du. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.findings-emnlp.452" title="">Leveraging structured information for explainable multi-hop question answering and reasoning</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: EMNLP</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma etÂ al. (2023)</span>
<span class="ltx_bibblock">
Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.emnlp-main.322" title="">Query rewriting for retrieval-augmented large language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: EMNLP</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mallen etÂ al. (2023)</span>
<span class="ltx_bibblock">
Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.acl-long.546" title="">When not to trust language models: Investigating effectiveness of parametric and non-parametric memories</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics: ACL</em>, pages 9802â€“9822.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mavi etÂ al. (2022)</span>
<span class="ltx_bibblock">
Vaibhav Mavi, Anubhav Jangra, and Adam Jatowt. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/https://doi.org/10.48550/arXiv.2204.09140" title="">A survey on multi-hop question answering and generation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2204.09140</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao Liang, Kunlun Zhu, Yankai Lin, XuÂ Han, Ning Ding, Huadong Wang, etÂ al. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.acl-long.499" title="">Webcpm: Interactive web search for chinese long-form question answering</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics: ACL</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rasley etÂ al. (2020)</span>
<span class="ltx_bibblock">
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/https://dl.acm.org/doi/10.1145/3394486.3406703" title="">Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">SIGKDD</em>, page 3505â€“3506.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren etÂ al. (2023)</span>
<span class="ltx_bibblock">
Ruiyang Ren, Yuhao Wang, Yingqi Qu, WayneÂ Xin Zhao, Jing Liu, Hao Tian, Hua Wu, Ji-Rong Wen, and Haifeng Wang. 2023.

</span>
<span class="ltx_bibblock">Investigating the factual knowledge boundary of large language models with retrieval augmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2307.11019</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robertson etÂ al. (2009)</span>
<span class="ltx_bibblock">
Stephen Robertson, Hugo Zaragoza, etÂ al. 2009.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/https://doi.org/10.1561/1500000019" title="">The probabilistic relevance framework: Bm25 and beyond</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Foundations and TrendsÂ® in Information Retrieval</em>, 3(4):333â€“389.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Santhanam etÂ al. (2021)</span>
<span class="ltx_bibblock">
Keshav Santhanam, O.Â Khattab, Jon Saad-Falcon, Christopher Potts, and MateiÂ A. Zaharia. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:244799249" title="">Colbertv2: Effective and efficient retrieval via lightweight late interaction</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">North American Chapter of the Association for Computational Linguistics</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick etÂ al. (2023)</span>
<span class="ltx_bibblock">
Timo Schick, Jane Dwivedi-Yu, Roberto DessÃ¬, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/https://doi.org/10.48550/arXiv.2302.04761" title="">Toolformer: Language models can teach themselves to use tools</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:2302.04761</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao etÂ al. (2023)</span>
<span class="ltx_bibblock">
Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.findings-emnlp.620" title="">Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi etÂ al. (2023)</span>
<span class="ltx_bibblock">
Zhengliang Shi, Weiwei Sun, Shuo Zhang, Zhen Zhang, Pengjie Ren, and Zhaochun Ren. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:259370539" title="">Rade: Reference-assisted dialogue evaluation for open-domain dialogue</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">ArXiv</em>, abs/2309.08156.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Weiwei Sun, Zhengliang Shi, Shen Gao, Pengjie Ren, Maarten deÂ Rijke, and Zhaochun Ren. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/https://doi.org/10.1609/aaai.v37i11.26596" title="">Contrastive learning reduces hallucination in conversations</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, volumeÂ 37, pages 13618â€“13626.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, and Zhaochun Ren. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.emnlp-main.923" title="">Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang etÂ al. (2021)</span>
<span class="ltx_bibblock">
Yixuan Tang, HweeÂ Tou Ng, and Anthony Tung. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.eacl-main.283" title="">Do multi-hop question answering systems know how to answer the single-hop sub-questions?</a>
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: ACL</em>, pages 3244â€“3249. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thakur etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Nandan Thakur, Luiz Bonifacio, Xinyu Zhang, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-Hermelo, Xiaoguang Li, Qun Liu, Boxing Chen, Mehdi Rezagholizadeh, etÂ al. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/https://doi.org/10.48550/arXiv.2312.11361" title="">Nomiracl: Knowing when you donâ€™t know for robust multilingual retrieval-augmented generation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:2312.11361</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thakur etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Nandan Thakur, Luiz Bonifacio, XinyuÂ Crystina Zhang, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-Hermelo, Xiaoguang Li, Qun Liu, Boxing Chen, Mehdi Rezagholizadeh, and JimmyÂ J. Lin. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:266359301" title="">Nomiracl: Knowing when you donâ€™t know for robust multilingual retrieval-augmented generation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">ArXiv</em>, abs/2312.11361.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trivedi etÂ al. (2022)</span>
<span class="ltx_bibblock">
Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00475" title="">MuSiQue: Multihop questions via single-hop question composition</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Transactions of the Association for Computational Linguistics: TACL</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Liang Wang, Nan Yang, and Furu Wei. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.emnlp-main.585" title="">Query2doc: Query expansion with large language models. corr abs/2303.07678 (2023)</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2022a)</span>
<span class="ltx_bibblock">
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, EdÂ Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/https://doi.org/10.48550/arXiv.2203.11171" title="">Self-consistency improves chain of thought reasoning in language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:2203.11171</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2022b)</span>
<span class="ltx_bibblock">
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, EdÂ Huai hsin Chi, and Denny Zhou. 2022b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:247595263" title="">Self-consistency improves chain of thought reasoning in language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">ArXiv</em>, abs/2203.11171.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei etÂ al. (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, EdÂ Huai hsin Chi, F.Â Xia, Quoc Le, and Denny Zhou. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:246411621" title="">Chain of thought prompting elicits reasoning in large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">ArXiv</em>, abs/2201.11903.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie etÂ al. (2023)</span>
<span class="ltx_bibblock">
Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and YuÂ Su. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/https://doi.org/10.48550/arXiv.2305.13300" title="">Adaptive chameleon or stubborn sloth: Unraveling the behavior of large language models in knowledge conflicts</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">arXiv preprint arXiv:2305.13300</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu etÂ al. (2024)</span>
<span class="ltx_bibblock">
Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, and Tat-Seng Chua. 2024.

</span>
<span class="ltx_bibblock">Search-in-the-chain: Towards accurate, credible and traceable large language models for knowledge-intensive tasks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">WWW</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al. (2018)</span>
<span class="ltx_bibblock">
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, WilliamÂ W. Cohen, Ruslan Salakhutdinov, and ChristopherÂ D. Manning. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D18-1259" title="">HotpotQA: A dataset for diverse, explainable multi-hop question answering</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao etÂ al. (2023)</span>
<span class="ltx_bibblock">
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, ThomasÂ L Griffiths, Yuan Cao, and Karthik Narasimhan. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/https://doi.org/10.48550/arXiv.2305.10601" title="">Tree of thoughts: Deliberate problem solving with large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">arXiv preprint arXiv:2305.10601</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao etÂ al. (2022)</span>
<span class="ltx_bibblock">
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/https://doi.org/10.48550/arXiv.2210.03629" title="">React: Synergizing reasoning and acting in language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">arXiv preprint arXiv:2210.03629</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al. (2022)</span>
<span class="ltx_bibblock">
Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/https://doi.org/10.48550/arXiv.2209.10063" title="">Generate rather than retrieve: Large language models are strong context generators</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">arXiv preprint arXiv:2209.10063</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al. (2023)</span>
<span class="ltx_bibblock">
Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, and Dong Yu. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/https://doi.org/10.48550/arXiv.2311.09210" title="">Chain-of-note: Enhancing robustness in retrieval-augmented language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">arXiv preprint arXiv:2311.09210</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2024)</span>
<span class="ltx_bibblock">
Jiahao Zhang, Haiyang Zhang, Dongmei Zhang, Yong Liu, and Shen Huang. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/https://doi.org/10.48550/arXiv.2308.08973" title="">End-to-end beam retrieval for multi-hop question answering</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">2024 Annual Conference of the North American Chapter of the Association for Computational Linguistics</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, YuÂ Zhang, Yulong Chen, etÂ al. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/https://doi.org/10.48550/arXiv.2309.01219" title="">Sirenâ€™s song in the ai ocean: a survey on hallucination in large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">arXiv preprint arXiv:2309.01219</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao etÂ al. (2023)</span>
<span class="ltx_bibblock">
Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin, and Lidong Bing. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.acl-long.320" title="">Verify-and-edit: A knowledge-enhanced chain-of-thought framework</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, and Ji-Rong Wen. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/https://doi.org/10.48550/arXiv.2308.07107" title="">Large language models for information retrieval: A survey</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">arXiv preprint arXiv:2308.07107</em>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_section" id="Sx4">
<h2 class="ltx_title ltx_title_section">Appendices</h2>
<section class="ltx_subsection" id="Sx4.SSx1">
<h3 class="ltx_title ltx_title_subsection">Appendix I. Evaluation Metrics Details</h3>
<div class="ltx_para" id="Sx4.SSx1.p1">
<p class="ltx_p" id="Sx4.SSx1.p1.2">Following prior researchÂ <cite class="ltx_cite ltx_citemacro_citep">(Xu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib40" title="">2024</a>; Shao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib27" title="">2023</a>; Ren etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib23" title="">2023</a>)</cite>, we evaluate our method and baselines with three metrics:
(1) <span class="ltx_text ltx_font_italic" id="Sx4.SSx1.p1.2.1">Accuracy</span> (Acc), which evaluates whether the ground truth answer is contained within the generated answer,
(2) <span class="ltx_text ltx_font_italic" id="Sx4.SSx1.p1.2.2">F1 Score</span> (F1), which computes the lexical similarity between the generated answer and the ground truth answer. As the harmonic mean of precision and recall, it is calculated using term-level exact match other than ROUGE-L, with precision being the ratio of shared terms to predicted terms, and recall being the ratio of shared terms to actual terms,
and (3) <span class="ltx_text ltx_font_italic" id="Sx4.SSx1.p1.2.3">Semantic Accuracy</span> (Acc<sup class="ltx_sup" id="Sx4.SSx1.p1.2.4">â€ </sup>), which prompts the LLMs to evaluate the correctness of the generated answer taking the ground truth answer as reference.
In this work, we implement the Acc<sup class="ltx_sup" id="Sx4.SSx1.p1.2.5">â€ </sup> using gpt-3.5-turbo-instruct<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/" title="">https://openai.com/</a></span></span></span>.
The prompt is as follows, where <span class="ltx_text ltx_font_typewriter" id="Sx4.SSx1.p1.2.6">{question}</span>, <span class="ltx_text ltx_font_typewriter" id="Sx4.SSx1.p1.2.7">{model output}</span>, and <span class="ltx_text ltx_font_typewriter" id="Sx4.SSx1.p1.2.8">{answer}</span> are placeholders.
The results are averaged over three runs.</p>
</div>
<div class="ltx_para ltx_noindent" id="Sx4.SSx1.p2">
<svg class="ltx_picture" height="277.86" id="Sx4.SSx1.p2.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,277.86) matrix(1 0 0 -1 0 0)"><g fill="#6E6E6E" fill-opacity="1.0"><path d="M 0 5.91 L 0 271.95 C 0 275.21 2.64 277.86 5.91 277.86 L 594.09 277.86 C 597.36 277.86 600 275.21 600 271.95 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FCFCFC" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 253.75 L 598.03 253.75 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 259.65)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="Sx4.SSx1.p2.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="Sx4.SSx1.p2.pic1.1.1.1.1.1.1">Prompt for Evaluating the Correctness of a Model Output</span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="228.16" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="Sx4.SSx1.p2.pic1.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="Sx4.SSx1.p2.pic1.2.2.2.1.1.1">In the following task, you are given a Question, a model Prediction for the Question, and a Ground-truth Answer to the Question. You should decide whether the model Prediction implies the Ground-truth Answer.</span>
<span class="ltx_p" id="Sx4.SSx1.p2.pic1.2.2.2.1.1.2">Question</span>
<span class="ltx_p" id="Sx4.SSx1.p2.pic1.2.2.2.1.1.3">{question}</span>
<span class="ltx_p" id="Sx4.SSx1.p2.pic1.2.2.2.1.1.4">Prediction</span>
<span class="ltx_p" id="Sx4.SSx1.p2.pic1.2.2.2.1.1.5">{model output}</span>
<span class="ltx_p" id="Sx4.SSx1.p2.pic1.2.2.2.1.1.6">Ground-truth Answer</span>
<span class="ltx_p" id="Sx4.SSx1.p2.pic1.2.2.2.1.1.7">{answer}</span>
<span class="ltx_p" id="Sx4.SSx1.p2.pic1.2.2.2.1.1.8">Does the Prediction imply the Ground-truth Answer? Output Yes or No:</span>
</span></foreignobject></g></g></svg>
</div>
</section>
<section class="ltx_subsection" id="Sx4.SSx2">
<h3 class="ltx_title ltx_title_subsection">Appendix II. Case Study</h3>
<div class="ltx_para" id="Sx4.SSx2.p1">
<p class="ltx_p" id="Sx4.SSx2.p1.1">We conduct several case studies and find that our method is more effective at generating high-quality answers to a question.
A concrete example is shown in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#Sx4.T7" title="Table 7 â€£ Appendix IV. Human Evaluation â€£ Appendices â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_tag">7</span></a>.
We find that our GenGround can derive the correct answer successfully in two hops while the other baselines fail.
In the first hop, the GenGround enables the LLMs to formulate a simpler, single-hop question and directly generate a correct answer, which intuitively demonstrates the world knowledge of LLMs.
We also observe that although a wrong prediction is generated in the second hop initially, our GenGround can instruct LLMs to revise it with the assistance of an external document.
This phenomena further illustrates the necessity to incorporate both the parametric knowledge of LLMs and external documents to answer a complex, multi-hop question.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx4.SSx3">
<h3 class="ltx_title ltx_title_subsection">Appendix III. Training Dataset for Grounding Distillation</h3>
<section class="ltx_paragraph" id="Sx4.SSx3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Synthesize the dataset </h4>
<div class="ltx_para" id="Sx4.SSx3.SSS0.Px1.p1">
<p class="ltx_p" id="Sx4.SSx3.SSS0.Px1.p1.1">Our instructional grounding distillation collects the trajectory of LLMs, <em class="ltx_emph ltx_font_italic" id="Sx4.SSx3.SSS0.Px1.p1.1.1">i.e.,</em> ChatGPT, during the instructional knowledge grounding (SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S3.SS2" title="3.2 Instructional Knowledge Grounding â€£ 3 Generate-then-Ground with LLMs â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_tag">3.2</span></a>).
To achieve this, we first sample 50k questions from the Natural Questions (NQ) datasetÂ <cite class="ltx_cite ltx_citemacro_citep">(Kwiatkowski etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#bib.bib15" title="">2019</a>)</cite>.
Each question is paired with a corresponding ground-truth document and the noise documents.
Next, we supplement each question with an immediate answer and a detailed revision trajectory.
The immediate answer is generated directly by feeding the question into a smaller model, such as Mistral-7B.
The revision trajectory is generated by ChatGPT with the assistance of the ground truth document.</p>
</div>
<div class="ltx_para" id="Sx4.SSx3.SSS0.Px1.p2">
<p class="ltx_p" id="Sx4.SSx3.SSS0.Px1.p2.1">For practical consideration, we prompt the LLMs to encapsulate the cited evidence and revised answer with special tokens, <em class="ltx_emph ltx_font_italic" id="Sx4.SSx3.SSS0.Px1.p2.1.1">i.e.,</em> â€œ&lt;ref&gt; &lt;/ref&gt;â€ and â€œ&lt;revise&gt; &lt;/revise&gt;â€ in the output.
We employ the following heuristic methods to filter the low-quality generated output:</p>
<ul class="ltx_itemize" id="Sx4.I2">
<li class="ltx_item" id="Sx4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="Sx4.I2.i1.p1">
<p class="ltx_p" id="Sx4.I2.i1.p1.1">The output contain no evidence, <em class="ltx_emph ltx_font_italic" id="Sx4.I2.i1.p1.1.1">e.g.,</em> â€œ&lt;ref&gt; Empty &lt;/ref&gt;â€.</p>
</div>
</li>
<li class="ltx_item" id="Sx4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="Sx4.I2.i2.p1">
<p class="ltx_p" id="Sx4.I2.i2.p1.1">The output contain no revised answer, <em class="ltx_emph ltx_font_italic" id="Sx4.I2.i2.p1.1.1">e.g.,</em> no â€œ&lt;revise&gt;â€ or â€œ&lt;/revise&gt;â€ can be found.</p>
</div>
</li>
<li class="ltx_item" id="Sx4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="Sx4.I2.i3.p1">
<p class="ltx_p" id="Sx4.I2.i3.p1.1">The revised answer in the output is misaligned with the ground truth evidence.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="Sx4.SSx3.SSS0.Px1.p3">
<p class="ltx_p" id="Sx4.SSx3.SSS0.Px1.p3.1">We demonstrate diversity in the length of the input instruction, and output trajectory in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#S4.T1" title="Table 1 â€£ 4.1 Synthesize the Training Dataset â€£ 4 Generalization with Grounding Distillation â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_table" id="Sx4.T6">
<p class="ltx_p ltx_align_center" id="Sx4.T6.1"><span class="ltx_text ltx_inline-block" id="Sx4.T6.1.1" style="width:433.6pt;">
<span class="ltx_inline-block ltx_transformed_outer" id="Sx4.T6.1.1.1" style="width:229.9pt;height:36pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="Sx4.T6.1.1.1.1"><span class="ltx_text" id="Sx4.T6.1.1.1.1.1">
<span class="ltx_tabular ltx_align_middle" id="Sx4.T6.1.1.1.1.1.1">
<span class="ltx_tr" id="Sx4.T6.1.1.1.1.1.1.1">
<span class="ltx_td ltx_border_tt" id="Sx4.T6.1.1.1.1.1.1.1.1"></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="Sx4.T6.1.1.1.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="Sx4.T6.1.1.1.1.1.1.1.2.1">Ours</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="Sx4.T6.1.1.1.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="Sx4.T6.1.1.1.1.1.1.1.3.1">GRG</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="Sx4.T6.1.1.1.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="Sx4.T6.1.1.1.1.1.1.1.4.1">DSPy</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="Sx4.T6.1.1.1.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="Sx4.T6.1.1.1.1.1.1.1.5.1">SearChain</span></span></span>
<span class="ltx_tr" id="Sx4.T6.1.1.1.1.1.1.2">
<span class="ltx_td ltx_align_left ltx_border_bb ltx_border_tt" id="Sx4.T6.1.1.1.1.1.1.2.1"><span class="ltx_text ltx_font_bold" id="Sx4.T6.1.1.1.1.1.1.2.1.1">Correctness</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="Sx4.T6.1.1.1.1.1.1.2.2">52.75</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="Sx4.T6.1.1.1.1.1.1.2.3">49.71</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="Sx4.T6.1.1.1.1.1.1.2.4">51.24</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="Sx4.T6.1.1.1.1.1.1.2.5">46.30</span></span>
</span></span></span>
</span></span></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>The result of our human evaluation. </figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="Sx4.SSx4">
<h3 class="ltx_title ltx_title_subsection">Appendix IV. Human Evaluation</h3>
<div class="ltx_para" id="Sx4.SSx4.p1">
<p class="ltx_p" id="Sx4.SSx4.p1.1">we conduct a human evaluation, with three educated individuals assessing the <span class="ltx_text ltx_font_italic" id="Sx4.SSx4.p1.1.1">correctness</span> of 120 randomly sampled cases from four benchmarks on a three-scale rating.
Specifically, the annotators are invited to manually evaluate the question-answering trajectory step-by-step using the ground truth document provided by existing datasets.
To guarantee annotation quality, we ask at least two annotators to evaluate the same questions repeatedly.
If there is a discrepancy between the two annotators (<em class="ltx_emph ltx_font_italic" id="Sx4.SSx4.p1.1.2">i.e.,</em> two annotators give a different correctness score), we ask a third annotator to recheck it.
As the results are shown in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.14891v2#Sx4.T6" title="Table 6 â€£ Synthesize the dataset â€£ Appendix III. Training Dataset for Grounding Distillation â€£ Appendices â€£ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering"><span class="ltx_text ltx_ref_tag">6</span></a>, our method achieves the best performance with a 3.66 point improvement on average.
This result demonstrates the superiority of our method.
The overall Kappa value of our human evaluation is 0.73, illustrating agreement among the annotators.</p>
</div>
<figure class="ltx_table" id="Sx4.T7">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="Sx4.T7.1">
<tr class="ltx_tr" id="Sx4.T7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt" id="Sx4.T7.1.1.1" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T7.1.1.1.1">
<span class="ltx_p" id="Sx4.T7.1.1.1.1.1" style="width:441.0pt;"><span class="ltx_text ltx_font_bold" id="Sx4.T7.1.1.1.1.1.1">Query</span>
In what month is the annual documentary film festival, that is presented by the fortnightly published British journal of literary essays, held? (HotpotQA dataset)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx4.T7.1.2" style="background-color:#DBDBDB;">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Sx4.T7.1.2.1" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="Sx4.T7.1.2.1.1" style="background-color:#DBDBDB;">GenGround</span></td>
</tr>
<tr class="ltx_tr" id="Sx4.T7.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="Sx4.T7.1.3.1" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T7.1.3.1.1">
<span class="ltx_p" id="Sx4.T7.1.3.1.1.1" style="width:441.0pt;">
<span class="ltx_tabular ltx_align_middle" id="Sx4.T7.1.3.1.1.1.1">
<span class="ltx_tr" id="Sx4.T7.1.3.1.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="Sx4.T7.1.3.1.1.1.1.1.1" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T7.1.3.1.1.1.1.1.1.1">
<span class="ltx_p" id="Sx4.T7.1.3.1.1.1.1.1.1.1.1" style="width:441.0pt;">Question 1: What is the name of the annual documentary film festival presented by the fortnightly published British journal of literary essays?</span>
</span></span></span>
<span class="ltx_tr" id="Sx4.T7.1.3.1.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="Sx4.T7.1.3.1.1.1.1.2.1" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T7.1.3.1.1.1.1.2.1.1">
<span class="ltx_p" id="Sx4.T7.1.3.1.1.1.1.2.1.1.1" style="width:441.0pt;">Answer 1: The <span class="ltx_text" id="Sx4.T7.1.3.1.1.1.1.2.1.1.1.1" style="color:#0000FF;">Fortnightly Review Documentary Film Festival</span>.</span>
</span></span></span>
<span class="ltx_tr" id="Sx4.T7.1.3.1.1.1.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="Sx4.T7.1.3.1.1.1.1.3.1" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T7.1.3.1.1.1.1.3.1.1">
<span class="ltx_p" id="Sx4.T7.1.3.1.1.1.1.3.1.1.1" style="width:441.0pt;">Grounding 1: The document demonstrate &lt;ref&gt; The annual documentary film festival presented by the fortnightly published British journal of literary essays is called the London International Documentary Festival (LIDF) &lt;/ref&gt;. &lt;revise&gt;the London International <span class="ltx_text" id="Sx4.T7.1.3.1.1.1.1.3.1.1.1.1" style="color:#0000FF;">Documentary Festival</span> (LIDF) &lt;/revise&gt;.</span>
</span></span></span>
<span class="ltx_tr" id="Sx4.T7.1.3.1.1.1.1.4">
<span class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="Sx4.T7.1.3.1.1.1.1.4.1" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T7.1.3.1.1.1.1.4.1.1">
<span class="ltx_p" id="Sx4.T7.1.3.1.1.1.1.4.1.1.1" style="width:441.0pt;">Question 2: The annual documentary film festival presented by the fortnightly published British journal of literary essays is called the London International Documentary Festival (LIDF). In what month is LIDF held?</span>
</span></span></span>
<span class="ltx_tr" id="Sx4.T7.1.3.1.1.1.1.5">
<span class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="Sx4.T7.1.3.1.1.1.1.5.1" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T7.1.3.1.1.1.1.5.1.1">
<span class="ltx_p" id="Sx4.T7.1.3.1.1.1.1.5.1.1.1" style="width:441.0pt;">Answer 2: LIDF is held in the months of <span class="ltx_text" id="Sx4.T7.1.3.1.1.1.1.5.1.1.1.1" style="color:#0000FF;">March and April*</span> every year.</span>
</span></span></span>
<span class="ltx_tr" id="Sx4.T7.1.3.1.1.1.1.6">
<span class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="Sx4.T7.1.3.1.1.1.1.6.1" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T7.1.3.1.1.1.1.6.1.1">
<span class="ltx_p" id="Sx4.T7.1.3.1.1.1.1.6.1.1.1" style="width:441.0pt;">Grounding 2: The document demonstrate &lt;ref&gt; The London International Documentary Festival (or LIDF) is an annual documentary film festival that takes place in the months of March and April every year &lt;/ref&gt;. The revised answer is &lt;revise&gt; LIDF is held in the months of <span class="ltx_text" id="Sx4.T7.1.3.1.1.1.1.6.1.1.1.1" style="color:#0000FF;">March and April</span> every year &lt;/revise&gt;.</span>
</span></span></span>
<span class="ltx_tr" id="Sx4.T7.1.3.1.1.1.1.7">
<span class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="Sx4.T7.1.3.1.1.1.1.7.1" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T7.1.3.1.1.1.1.7.1.1">
<span class="ltx_p" id="Sx4.T7.1.3.1.1.1.1.7.1.1.1" style="width:441.0pt;">###Finish[<span class="ltx_text" id="Sx4.T7.1.3.1.1.1.1.7.1.1.1.1" style="color:#0000FF;">March and April</span>]</span>
</span></span></span>
</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx4.T7.1.4" style="background-color:#DBDBDB;">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Sx4.T7.1.4.1" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="Sx4.T7.1.4.1.1" style="background-color:#DBDBDB;">Chain of thought</span></td>
</tr>
<tr class="ltx_tr" id="Sx4.T7.1.5">
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="Sx4.T7.1.5.1" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T7.1.5.1.1">
<span class="ltx_p" id="Sx4.T7.1.5.1.1.1" style="width:441.0pt;">
<span class="ltx_tabular ltx_align_middle" id="Sx4.T7.1.5.1.1.1.1">
<span class="ltx_tr" id="Sx4.T7.1.5.1.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="Sx4.T7.1.5.1.1.1.1.1.1" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T7.1.5.1.1.1.1.1.1.1">
<span class="ltx_p" id="Sx4.T7.1.5.1.1.1.1.1.1.1.1" style="width:441.0pt;">Step 1: The annual documentary film festival is presented by <span class="ltx_text" id="Sx4.T7.1.5.1.1.1.1.1.1.1.1.1" style="color:#0000FF;">the fortnightly</span> published British journal of literary essays.</span>
</span></span></span>
<span class="ltx_tr" id="Sx4.T7.1.5.1.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="Sx4.T7.1.5.1.1.1.1.2.1" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T7.1.5.1.1.1.1.2.1.1">
<span class="ltx_p" id="Sx4.T7.1.5.1.1.1.1.2.1.1.1" style="width:441.0pt;">Step 2: The entity presenting the festival is the British journal of literary essays.</span>
</span></span></span>
<span class="ltx_tr" id="Sx4.T7.1.5.1.1.1.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="Sx4.T7.1.5.1.1.1.1.3.1" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T7.1.5.1.1.1.1.3.1.1">
<span class="ltx_p" id="Sx4.T7.1.5.1.1.1.1.3.1.1.1" style="width:441.0pt;">Step 3: <span class="ltx_text" id="Sx4.T7.1.5.1.1.1.1.3.1.1.1.1" style="color:#0000FF;">"Fortnightly"</span> means the journal is published every two weeks.</span>
</span></span></span>
<span class="ltx_tr" id="Sx4.T7.1.5.1.1.1.1.4">
<span class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="Sx4.T7.1.5.1.1.1.1.4.1" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T7.1.5.1.1.1.1.4.1.1">
<span class="ltx_p" id="Sx4.T7.1.5.1.1.1.1.4.1.1.1" style="width:441.0pt;">Step 4: Since the journal is published every two weeks, we need to find out which months fall around the time when the festival is typically held. If the journal is published fortnightly, it means there are approximately 26 issues per year. Given that the festival is annual, it should occur once within those 26 fortnightly periods.</span>
</span></span></span>
<span class="ltx_tr" id="Sx4.T7.1.5.1.1.1.1.5">
<span class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="Sx4.T7.1.5.1.1.1.1.5.1" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T7.1.5.1.1.1.1.5.1.1">
<span class="ltx_p" id="Sx4.T7.1.5.1.1.1.1.5.1.1.1" style="width:441.0pt;">Final Answer: This film festival, presented by the British journal of literary essays, is held in the month of <span class="ltx_text" id="Sx4.T7.1.5.1.1.1.1.5.1.1.1.1" style="color:#FF0000;">May</span>.</span>
</span></span></span>
</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx4.T7.1.6" style="background-color:#DBDBDB;">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Sx4.T7.1.6.1" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="Sx4.T7.1.6.1.1" style="background-color:#DBDBDB;">GRG w/ decomposition</span></td>
</tr>
<tr class="ltx_tr" id="Sx4.T7.1.7">
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="Sx4.T7.1.7.1" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T7.1.7.1.1">
<span class="ltx_p" id="Sx4.T7.1.7.1.1.1" style="width:441.0pt;">
<span class="ltx_tabular ltx_align_middle" id="Sx4.T7.1.7.1.1.1.1">
<span class="ltx_tr" id="Sx4.T7.1.7.1.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="Sx4.T7.1.7.1.1.1.1.1.1" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T7.1.7.1.1.1.1.1.1.1">
<span class="ltx_p" id="Sx4.T7.1.7.1.1.1.1.1.1.1.1" style="width:441.0pt;">Question 1: I need to get for Name of the British journal of literary essays</span>
</span></span></span>
<span class="ltx_tr" id="Sx4.T7.1.7.1.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="Sx4.T7.1.7.1.1.1.1.2.1" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T7.1.7.1.1.1.1.2.1.1">
<span class="ltx_p" id="Sx4.T7.1.7.1.1.1.1.2.1.1.1" style="width:441.0pt;">Document 1: [1] The <span class="ltx_text" id="Sx4.T7.1.7.1.1.1.1.2.1.1.1.1" style="color:#0000FF;">London Review of Books (LRB)</span> is a British journal of literary essays. It is published fortnightlyâ€¦ ||| [2] Essays in London and Elsewhere is a book of literary criticism by Henry James published in 1893â€¦</span>
</span></span></span>
<span class="ltx_tr" id="Sx4.T7.1.7.1.1.1.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="Sx4.T7.1.7.1.1.1.1.3.1" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T7.1.7.1.1.1.1.3.1.1">
<span class="ltx_p" id="Sx4.T7.1.7.1.1.1.1.3.1.1.1" style="width:441.0pt;">Question 2: When is the <span class="ltx_text" id="Sx4.T7.1.7.1.1.1.1.3.1.1.1.1" style="color:#0000FF;">annual documentary film festival</span> held?
Document 2: The annual documentary film festival, also known as DOC NYC, is held every <span class="ltx_text" id="Sx4.T7.1.7.1.1.1.1.3.1.1.1.2" style="color:#FF0000;">November</span> in New York City. It was founded in 2010 and has since become one of the largest documentary film festivals in the United States. The festival showcases over 300 films and hosts panels, workshops. ||| The London International Documentary Festival (or LIDF) is an annual documentary film festival that takes place in the months of March and April every year. The eventâ€¦</span>
</span></span></span>
<span class="ltx_tr" id="Sx4.T7.1.7.1.1.1.1.4">
<span class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="Sx4.T7.1.7.1.1.1.1.4.1" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T7.1.7.1.1.1.1.4.1.1">
<span class="ltx_p" id="Sx4.T7.1.7.1.1.1.1.4.1.1.1" style="width:441.0pt;">###Finish[<span class="ltx_text" id="Sx4.T7.1.7.1.1.1.1.4.1.1.1.1" style="color:#FF0000;">November</span>]</span>
</span></span></span>
</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx4.T7.1.8" style="background-color:#DBDBDB;">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Sx4.T7.1.8.1" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="Sx4.T7.1.8.1.1" style="background-color:#DBDBDB;">SearChain</span></td>
</tr>
<tr class="ltx_tr" id="Sx4.T7.1.9">
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb" id="Sx4.T7.1.9.1" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T7.1.9.1.1">
<span class="ltx_p" id="Sx4.T7.1.9.1.1.1" style="width:441.0pt;">
<span class="ltx_tabular ltx_align_middle" id="Sx4.T7.1.9.1.1.1.1">
<span class="ltx_tr" id="Sx4.T7.1.9.1.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="Sx4.T7.1.9.1.1.1.1.1.1" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T7.1.9.1.1.1.1.1.1.1">
<span class="ltx_p" id="Sx4.T7.1.9.1.1.1.1.1.1.1.1" style="width:441.0pt;">Question 1: [Unsolved Query]: What is the name of the annual documentary film festival presented by the fortnightly published British journal of literary essays? [Unsolved Query]: What is the schedule or dates for the annual documentary film festival presented by the fortnightly published British journal of literary essays? [Unsolved Query]: When does the annual documentary film festival presented by the fortnightly published British journal of literary essays usually take place?
[Unsolved Query]: In what month is the annual documentary film festival usually held?</span>
</span></span></span>
<span class="ltx_tr" id="Sx4.T7.1.9.1.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="Sx4.T7.1.9.1.1.1.1.2.1" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T7.1.9.1.1.1.1.2.1.1">
<span class="ltx_p" id="Sx4.T7.1.9.1.1.1.1.2.1.1.1" style="width:441.0pt;">Document 1: [1] Essays in London and Elsewhere is a book of literary criticism by Henry James published in 1893â€¦</span>
</span></span></span>
<span class="ltx_tr" id="Sx4.T7.1.9.1.1.1.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="Sx4.T7.1.9.1.1.1.1.3.1" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T7.1.9.1.1.1.1.3.1.1">
<span class="ltx_p" id="Sx4.T7.1.9.1.1.1.1.3.1.1.1" style="width:441.0pt;">Answer 1: The annual documentary film festival presented by the <span class="ltx_text" id="Sx4.T7.1.9.1.1.1.1.3.1.1.1.1" style="color:#0000FF;">fortnightly published British</span> journal of literary essays is called the London International Documentary Festival (LIDF).
Question 2: [Query]: When does the London International Documentary Festival usually take place? Answer 5: The London International Documentary Festival usually takes place in the months of <span class="ltx_text" id="Sx4.T7.1.9.1.1.1.1.3.1.1.1.2" style="color:#FF0000;">October</span> every year.</span>
</span></span></span>
</span></span>
</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>A concrete example on the HotpotQA dataset. We highlight the <span class="ltx_text" id="Sx4.T7.4.1" style="color:#0000FF;">right</span> and <span class="ltx_text" id="Sx4.T7.5.2" style="color:#FF0000;">wrong</span> prediction used to answer the input question.
We find that our GenGround can derive the correct answer successfully in two hops while the other baselines fail.
In the first hop, the GenGround enables the LLMs to formulate a simpler, single-hop question and directly generate a correct answer.
We also observe that although a wrong prediction is generated in the second hop initially, our GenGround can instruct LLMs to revise it with the assistance of an external document.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep 16 17:16:06 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
