<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese</title>
<!--Generated on Wed Jul  3 12:45:49 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Large Language Model; Factual Consistency Evaluation;" lang="en" name="keywords"/>
<base href="/html/2407.01080v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#S1" title="In Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#S2" title="In Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#S3" title="In Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>FACE4RAG Benchmark</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#S3.SS1" title="In 3. FACE4RAG Benchmark ‣ Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Error Typology in FCE</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#S3.SS2" title="In 3. FACE4RAG Benchmark ‣ Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Synthetic Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#S3.SS3" title="In 3. FACE4RAG Benchmark ‣ Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Real-World Dataset</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#S4" title="In Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Logic-enhanced Factual Consistency Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#S4.SS1" title="In 4. Logic-enhanced Factual Consistency Evaluation ‣ Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Logic-Preserving Answer Decomposition</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#S4.SS2" title="In 4. Logic-enhanced Factual Consistency Evaluation ‣ Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Fact-Logic FCE</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#S5" title="In Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#S5.SS1" title="In 5. Experiments ‣ Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Experimental Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#S5.SS2" title="In 5. Experiments ‣ Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Performance Comparison on Face4RAG</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#S5.SS3" title="In 5. Experiments ‣ Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Performance Comparison on Existing FCE Benchmark</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#S5.SS4" title="In 5. Experiments ‣ Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Ablation Study</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#S6" title="In Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#A1" title="In Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Construction Details about Synthetic Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#A2" title="In Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Ablation Study Results on the Real-world Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#A3" title="In Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Statistic Details about Real-World Dataset</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yunqi Xu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">Ant Group</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Shanghai</span><span class="ltx_text ltx_affiliation_country" id="id3.3.id3">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tianchi Cai
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id4.1.id1">Ant Group</span><span class="ltx_text ltx_affiliation_city" id="id5.2.id2">Hangzhou</span><span class="ltx_text ltx_affiliation_country" id="id6.3.id3">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jiyan Jiang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">Tsinghua University</span><span class="ltx_text ltx_affiliation_city" id="id8.2.id2">Beijing</span><span class="ltx_text ltx_affiliation_country" id="id9.3.id3">China</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xierui Song
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id10.1.id1">Ant Group</span><span class="ltx_text ltx_affiliation_city" id="id11.2.id2">Hangzhou</span><span class="ltx_text ltx_affiliation_country" id="id12.3.id3">China</span>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id13.id1">The prevailing issue of factual inconsistency errors in conventional Retrieval Augmented Generation (RAG) motivates the study of Factual Consistency Evaluation (FCE). Despite the various FCE methods proposed earlier, these methods are evaluated on datasets generated by specific Large Language Models (LLMs). Without a comprehensive benchmark, it remains unexplored how these FCE methods perform on other LLMs with different error distributions or even unseen error types, as these methods may fail to detect the error types generated by other LLMs. To fill this gap, in this paper, we propose the first comprehensive FCE benchmark <em class="ltx_emph ltx_font_italic" id="id13.id1.1">Face4RAG</em> for RAG independent of the underlying LLM. Our benchmark consists of a synthetic dataset built upon a carefully designed typology for factuality inconsistency error and a real-world dataset constructed from six commonly used LLMs, enabling evaluation of FCE methods on specific error types or real-world error distributions. On the proposed benchmark, we discover the failure of existing FCE methods to detect the logical fallacy, which refers to a mismatch of logic structures between the answer and the retrieved reference. To fix this issue, we further propose a new method called <em class="ltx_emph ltx_font_italic" id="id13.id1.2">L-Face4RAG</em> with two novel designs of logic-preserving answer decomposition and fact-logic FCE. Extensive experiments show L-Face4RAG substantially outperforms previous methods for factual inconsistency detection on a wide range of tasks, notably beyond the RAG task from which it is originally motivated. Both the benchmark and our proposed method are publicly available.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/yq27/Face4RAG" title="">https://huggingface.co/datasets/yq27/Face4RAG</a></span></span></span></p>
</div>
<div class="ltx_keywords">Large Language Model; Factual Consistency Evaluation;
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>rightsretained</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining; August 25–29, 2024; Barcelona, Spain</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_booktitle" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">booktitle: </span>Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>10.1145/3637528.3671656</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>979-8-4007-0490-1/24/08</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id7"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Natural language processing</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id8"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>General and reference Evaluation</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="425" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>An overview of our proposed FCE benchmark and method, in comparison with prior works. The upper left plot gives an example from RAG task. The lower left plot demonstrates previous FCE method, and the lower middle plot depicts our proposed FCE method L-Face4RAG. The upper right plot shows the procedure of constructing the real-world dataset in our proposed Face4RAG benchmark, which follows the procedure of previous benchmark. The lower right plots illustrates the construction of the synthetic dataset in the Face4RAG benchmark.</figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Retrieval Augmented Generation (RAG), a technique of augmenting the context of Large Language Models (LLMs) with relevant passages retrieved from external retrievers or search engines <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib28" title="">2020</a>)</cite>, has demonstrated strong performance on various knowledge intensive tasks such as open domain conversation <cite class="ltx_cite ltx_citemacro_citep">(Thoppilan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib42" title="">2022</a>; Shuster et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib39" title="">2022</a>)</cite> and question answering <cite class="ltx_cite ltx_citemacro_citep">(Izacard and Grave, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib20" title="">2020</a>)</cite>. Despite its bright prospect, factual consistency remains a critical issue for RAG systems. Recent assessment reveals that even for the leading-edge commercial RAG systems like Bing Chat and Perplexity, barely over half of their outputs are factual consistent with the references <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib30" title="">2023</a>)</cite>. This issue urges the need of studying factual consistency evaluation (FCE) in the RAG task.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Various FCE methods have been proposed to evaluate the factual consistency of specific RAG systems, among which a two-step approach shows promising results, especially for evaluating long answers <cite class="ltx_cite ltx_citemacro_citep">(Min et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib32" title="">2023</a>; Chern et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib11" title="">2023</a>)</cite>. As shown in the bottom left of Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_tag">1</span></a>, this approach first segments the answer into shorter pieces, then evaluates the factual consistency of each segment with respect to the given reference. In this way, the evaluation of a long answer is decomposed into evaluations on several simpler pieces of information, which improves the detection of factual inconsistency.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In previous works, these FCE methods are evaluated by answers generated by the underlying LLM in the specific RAG system being studied <cite class="ltx_cite ltx_citemacro_citep">(Es et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib14" title="">2023</a>; Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib19" title="">2023</a>)</cite>. Despite their effectiveness on the specific system, it is unclear how these methods generalize to new RAG systems. As discovered in a recent study <cite class="ltx_cite ltx_citemacro_citep">(Min et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib32" title="">2023</a>)</cite>, the optimal FCE method may vary when evaluating different LLMs, hence achieving a superior performance regarding some certain LLM does not guarantee a strong performance on other LLMs. In this sense, previous benchmarks generated by a single LLM are not fair enough to evaluate the overall performance of FCE methods.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To fill this gap, in this paper, we first construct a comprehensive benchmark to enable the evaluation of FCE methods independent of the underlying LLM. Specifically, we first propose a novel error typology to cover various factual consistency errors in RAG, which includes three main categories, i.e., hallucination error, knowledge error, and logical fallacy, and is further divided into nine error types. Based on our predefined error typology, we construct a synthetic dataset in Chinese to assess the effectiveness of FCE methods across the different types of errors. Furthermore, we construct a real-world dataset in Chinese by generating answers using six distinct LLMs within RAG tasks. Empirical analysis on the real-world dataset shows that 6.96% of all factual inconsistent samples involve logical fallacies. In addition, we observe that different LLMs exhibit diverse error distributions, which echoes previous research <cite class="ltx_cite ltx_citemacro_citep">(Min et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib32" title="">2023</a>)</cite> and justifies our motivation of constructing a comprehensive benchmark independent of LLMs.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">While logical fallacy accounts for a considerable proportion of factual inconsistency errors in the real-world dataset, existing FCE methods may be incapable of detecting these sophisticated errors involving logical connections among multiple text segments, since the decomposition step may neglect the logical connections between segments in the original answer. Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_tag">1</span></a> provides a showcase where a careless decomposition may mistakenly remove the cause-effect relation, leading to a wrong evaluation result.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">To resolve this issue, we develop the Logic-Enhanced FActual Consistency Evaluation for RAG (L-Face4RAG) method to better handle the logical consistency in the RAG task. L-Face4RAG has two core designs, i.e., logic-preserving answer decomposition and fact-logic FCE. Specifically, in the answer decomposition step, we propose three principles for decomposition based on semantic linkages and logical connections. We design an elaborated prompt accordingly and construct few-shot examples to help LLM better follow the above principles. In the subsequent FCE step, we assess the factual consistency of each segment from two perspectives, i.e., the <span class="ltx_text ltx_font_italic" id="S1.p6.1.1">fact consistency</span> and <span class="ltx_text ltx_font_italic" id="S1.p6.1.2">logical consistency</span>. The former perspective aims to detect hallucination or knowledge errors, while the later is responsible for the logical fallacy errors. We further design a chain-of-thought (COT) <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib44" title="">2022</a>)</cite> prompt at each stage to instruct the LLM to better handle the inconsistency error in a step-by-step manner. Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_tag">1</span></a> gives a detailed demonstration of L-Face4RAG.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Finally, we conduct extensive experiments to verify the effectiveness of L-Face4RAG. Compared to previous FCE methods for RAG, L-Face4RAG attains substantially higher accuracy on both synthetic and real-world datasets, regardless of the error type or underlying LLM. Notably, although it is motivated by FCE in Chinese RAG, its superiority is consistent on other FCE tasks. Specifically, additional experiments on English FCE benchmarks for RAG<cite class="ltx_cite ltx_citemacro_citep">(Es et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib14" title="">2023</a>; Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib19" title="">2023</a>)</cite>, summarization<cite class="ltx_cite ltx_citemacro_citep">(Pagnoni et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib35" title="">2021</a>; Fabbri et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib15" title="">2021</a>)</cite>, dialogue<cite class="ltx_cite ltx_citemacro_citep">(Honovich et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib18" title="">2021</a>; Gupta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib17" title="">2021</a>)</cite> and fact verification<cite class="ltx_cite ltx_citemacro_citep">(Schuster et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib38" title="">2021</a>)</cite> show that L-Face4RAG achieves SOTA on most of the tasks (6 out of 7), as well as a substantially higher averaged score. We further conduct ablation studies to verify the core designs of L-Face4RAG, i.e., the logic-preserving answer decomposition approach and two-stage consistency evaluation with carefully designed COT prompts.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">The contributions of this work are summarized as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We construct the first comprehensive FCE benchmark in RAG, the Face4RAG, which includes a carefully designed error typology, a synthetic dataset, and a real-world dataset. Face4RAG allows to evaluate FCE method on specific error types or various real-world error distributions.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We propose a new FCE method called L-Face4RAG with two novel designs of logic-preserving answer decomposition and fact-logic FCE to better detect the logic fallacy in the examined answer.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Extensive experiments justify the proposed error typology, evaluate the effectiveness of L-Face4RAG on a wide range of FCE tasks, and provide further insights on the distinct error type distributions of various LLMs. All datasets and method are released for better reproducibility.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">Traditional FCE Methods</span>.
Evaluating the factuality of model generated results is widely studied across various language model generation domains like text summarization <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib16" title="">2023</a>)</cite>, dialogue summary <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib48" title="">2023</a>)</cite> and question-answering <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib5" title="">2023b</a>)</cite>. When the golden labels are given, prior methods using exact match metrics <cite class="ltx_cite ltx_citemacro_citep">(Izacard and Grave, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib20" title="">2020</a>; Lewis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib28" title="">2020</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib9" title="">2017</a>)</cite> or similarity-based metrics are proposed <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib47" title="">2019</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib8" title="">2019</a>)</cite>. However, high quality answers can vary a lot, hence these approaches using golden labels may significantly underestimate the models’ performances, especially for long answers <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib43" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">FCE for Long-form Answers.</span> To effectively evaluate factuality of long answers, recent FCE research mostly take a two step approaches <cite class="ltx_cite ltx_citemacro_citep">(Kamoi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib24" title="">2023</a>; Lattimer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib27" title="">2023</a>)</cite>, where in the first step, the long-form answer is decomposed into shorter segments, such as sentences <cite class="ltx_cite ltx_citemacro_citep">(Kryściński et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib25" title="">2019</a>; Lattimer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib27" title="">2023</a>)</cite>, sub-claims <cite class="ltx_cite ltx_citemacro_citep">(Kamoi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib24" title="">2023</a>; Chern et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib11" title="">2023</a>)</cite>, individual facts <cite class="ltx_cite ltx_citemacro_citep">(Min et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib32" title="">2023</a>)</cite> and structured triplets <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib19" title="">2023</a>)</cite>. Then the second step evaluates the verifiability of each segment with respect to the given reference text <cite class="ltx_cite ltx_citemacro_citep">(Laban et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib26" title="">2022</a>; Lattimer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib27" title="">2023</a>; Zha et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib46" title="">2023</a>)</cite>, which can be efficiently done by modern general purpose LLM <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib10" title="">2023</a>; Min et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib32" title="">2023</a>)</cite>, e.g., GPT-4. Although we follow the two-step approach, our method differs from them in the ability of leveraging logical connections via special designs of logic-preserving decomposition and fact-logic FCE.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1"><span class="ltx_text ltx_font_bold" id="S2.p3.1.1">FCE Benchmarks.</span> Prior benchmarks for FCE mostly focus on specialized tasks like summarization <cite class="ltx_cite ltx_citemacro_citep">(Kryściński et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib25" title="">2019</a>; Tang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib40" title="">2022</a>; Es et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib14" title="">2023</a>)</cite>. For FCE in RAG, existing benchmarks are derived from specific LLMs, such as Refchecker <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib19" title="">2023</a>)</cite> and FELM <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib10" title="">2023</a>)</cite>, which are constrained by the error type distribution of the underlying LLMs. Unlike these benchmarks, we construct a synthetic dataset based on our error typology, which enables evaluation independent to any underlying LLM.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>FACE4RAG Benchmark</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Recall that existing FCE benchmarks only use answers generated by some certain LLMs, which may fail to evaluate FCE methods on other LLMs with different error distributions or unseen error types.
To remedy this issue, in this section, we propose a novel approach to construct a FCE benchmark for RAG, which is independent of the underlying LLMs and called <span class="ltx_text ltx_font_italic" id="S3.p1.1.1">FActual Consistency Evaluation for RAG</span> (Face4RAG). Face4RAG contains an error-type-oriented synthetic dataset and a real-world dataset. To construct the synthetic dataset, inspired by the error typology used in an exam designed for humans, i.e. the National College Entrance Examination of China, we first propose a novel error typology to classify any factual consistency error in RAG task, which includes nine types of errors belonging to three main categories. Based on the proposed error typology, we then construct a synthetic dataset to evaluate FCE methods on each type of the error. Besides the synthetic dataset, we also collect samples from six commonly used LLMs to construct a real-world benchmark, which aims to evaluate the overall factual consistency of FCE methods in real-world scenarios. The details about Face4RAG can be seen in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#S3.T1" title="Table 1 ‣ 3. FACE4RAG Benchmark ‣ Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_tag">1</span></a> and Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#S3.F2" title="Figure 2 ‣ 3. FACE4RAG Benchmark ‣ Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1. </span> Statistics of the synthetic and real-world datasets in the Face4RAG benchmark. For each dataset, the answer-level and segment-level statistics on the number of samples, the average sample length in terms of characters and the rate of positive samples are reported.</figcaption>
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1">
<tr class="ltx_tr" id="S3.T1.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1">Statistics</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S3.T1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.2.1">Synthetic Dataset</span></td>
<td class="ltx_td ltx_border_tt" id="S3.T1.1.1.3"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S3.T1.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.4.1">Real-world Dataset</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.1">Answer</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.2">Segment</td>
<td class="ltx_td" id="S3.T1.1.2.3"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.4">Answer</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.5">Segment</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.3.1">Num. Samples</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.3.2">1299</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.3.3">6737</td>
<td class="ltx_td ltx_border_t" id="S3.T1.1.3.4"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.3.5">1200</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.3.6">6143</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4">
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.1">Avg. Length</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.2">289.3</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.3">45.4</td>
<td class="ltx_td" id="S3.T1.1.4.4"></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.5">307.7</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.6">45.2</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.5">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.5.1">Positive Rate</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.5.2">30.3%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.5.3">55.8%</td>
<td class="ltx_td ltx_border_bb" id="S3.T1.1.5.4"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.5.5">63.3%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.5.6">85.6%</td>
</tr>
</table>
</figure>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="171" id="S3.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>
Error type distribution of factually inconsistent samples in the two datasets of our Face4RAG benchmark.</figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Error Typology in FCE</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Our error typology for FCE is inspired by the questions in the National College Entrance Examination of China <cite class="ltx_cite ltx_citemacro_citep">(gao, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib2" title="">2019</a>)</cite>, which are carefully designed to test the ability of human to evaluate factual consistency. In this examination, reading comprehension is a major section to evaluate the participants’ skill of understanding a Chinese text. Factual consistency evaluation is a typical task in this section. Given the text, the participants are required to evaluate the correctness of several answers to a specific question <cite class="ltx_cite ltx_citemacro_citep">(gao, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib2" title="">2019</a>)</cite>, which is essentially a RAG task (see examples in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#A3.T11" title="Table 11 ‣ Appendix C Statistic Details about Real-World Dataset ‣ Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_tag">11</span></a> in the appendix).
As these questions are designed for a competitive entrance examination of higher education institutions at the undergraduate level, they are generally hard to answer and cover a wide range of factual inconsistency error types. Accordingly, we develop a novel error typology for RAG, which comprises three main categories and is further classified into nine error types. In the following, we give a detailed description of our proposed error typology.
</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">Hallucination Error</span> This class of error refers to the situation when the answer contains information that cannot be traced back to the reference <cite class="ltx_cite ltx_citemacro_citep">(Maynez et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib31" title="">2020</a>)</cite>. Note that there are two main usages of the term hallucination in previous literature: one refers to ”unfaithful or nonsensical” generated answers <cite class="ltx_cite ltx_citemacro_citep">(Ji et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib22" title="">2023</a>)</cite>, the other further includes ”unverifiable” answers using the given context <cite class="ltx_cite ltx_citemacro_citep">(Maynez et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib31" title="">2020</a>; Thomson and Reiter, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib41" title="">2020</a>)</cite>. Here we adopt the second usage that has a larger scope.</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I1.i1.p1.1.1">Hallucination Error (Hallu.)</span> refers to the situation when the answer is either unfaithful or unverifiable using the given context (even when it is factually correct).</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.1">Knowledge Error</span> This class of error refers to the situation when the information contained by the answer is inaccurate or incorrect regarding the reference <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib10" title="">2023</a>)</cite>. This may occur in various components of a sentence, such as the subjects, predicates, objects, adverbials of time and place, etc. We classify the knowledge error into four error types:</p>
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I2.i1.p1.1.1">Contradiction Error (KCont.)</span> refers to the situation when the statement in the answer conflicts with information from the reference.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I2.i2.p1.1.1">Entity Inversion Error (KInve.)</span> refers to the situation when entities in the answer (events, processes, or concepts) are swapped in their positions as compared to the reference.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i3.p1">
<p class="ltx_p" id="S3.I2.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I2.i3.p1.1.1">Conflation Error (KConf.)</span> refers to the situation when the entities in the reference (subjects, predicates, or objects) are inaccurately combined, altering the original meaning.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i4.p1">
<p class="ltx_p" id="S3.I2.i4.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I2.i4.p1.1.1">Conceptual Substitution Error (KConc.)</span>
refers to the situation when a term or concept in the reference is erroneously replaced by a different (though possibly related) concept.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.1">Logical Fallacy</span> This kind of error occurs when the answer contains statements that are either without logical support or have a logical relation that conflicts with the information from the reference. This incongruity undermines the logical validity of the answer with respect to the reference, which results in an unsound argument or misleading information <cite class="ltx_cite ltx_citemacro_citep">(Jin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib23" title="">2022</a>; Petric, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib36" title="">2020</a>)</cite>. We further divide the logical fallacy into four error types:</p>
<ul class="ltx_itemize" id="S3.I3">
<li class="ltx_item" id="S3.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i1.p1">
<p class="ltx_p" id="S3.I3.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I3.i1.p1.1.1">Overgeneralization Error (LOver.)</span> refers to the situation when a specific detail or attribute from the reference is incorrectly applied to a broader category or group in the answer.</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i2.p1">
<p class="ltx_p" id="S3.I3.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I3.i2.p1.1.1">Causal Confusion Error (LCaus.)</span> refer to reverse the cause and effect in the reference, or mistakenly adding a causal relationship between two noncausal segments.</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i3.p1">
<p class="ltx_p" id="S3.I3.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I3.i3.p1.1.1">Confusing Sufficient and Necessary Conditions Error (LConf.)</span> is the case when the necessary conditions in reference are misinterpreted as sufficient and necessary conditions.</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i4.p1">
<p class="ltx_p" id="S3.I3.i4.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I3.i4.p1.1.1">Inclusion Relation Error (LIncl.)</span> is the case where statements that are unrelated or have certain relationship except inclusion in the reference are misrepresented in the answer to have an inclusion relationship (e.g., hierarchical or subset).</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.1">For each of the error types defined above, we provide several examples to help better understand and distinguish it from other types. See detailed examples in Table  <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#S3.T2" title="Table 2 ‣ 3.1. Error Typology in FCE ‣ 3. FACE4RAG Benchmark ‣ Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p6">
<span class="ltx_ERROR undefined" id="S3.SS1.p6.1">{CJK*}</span>
<p class="ltx_p" id="S3.SS1.p6.2">UTF8gbsn</p>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2. </span>Examples for Knowledge Error and Logic Fallacy. For each error types, the example in Chinese and the translated version to English are presented. The colored text spans highlight the segments of factual inconsistency errors.</figcaption>
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1">
<tr class="ltx_tr" id="S3.T2.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S3.T2.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.1.1">
<span class="ltx_p" id="S3.T2.1.1.1.1.1" style="width:34.7pt;">Error Type</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S3.T2.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.2.1">
<span class="ltx_p" id="S3.T2.1.1.2.1.1" style="width:182.1pt;">Original Text</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S3.T2.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.3.1">
<span class="ltx_p" id="S3.T2.1.1.3.1.1" style="width:182.1pt;">Factual Inconsistent Text</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.1.2.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.2.1.1">
<span class="ltx_p" id="S3.T2.1.2.1.1.1" style="width:34.7pt;">KCont.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.1.2.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.2.2.1">
<span class="ltx_p" id="S3.T2.1.2.2.1.1" style="width:182.1pt;">功能饮料中的维生素、矿物质等，对于运动后快速补充身体营养，<span class="ltx_text" id="S3.T2.1.2.2.1.1.1" style="color:#0000FF;">消除</span>疲劳具有一定作用。
<br class="ltx_break"/>The vitamins and minerals in energy drinks play a certain role in quickly replenishing nutrients and <span class="ltx_text" id="S3.T2.1.2.2.1.1.2" style="color:#0000FF;">eliminating</span> fatigue after exercise.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.1.2.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.2.3.1">
<span class="ltx_p" id="S3.T2.1.2.3.1.1" style="width:182.1pt;">功能饮料中的元素、微生物等，对于运动后快速补充身体营养，<span class="ltx_text" id="S3.T2.1.2.3.1.1.1" style="color:#0000FF;">增加</span>疲劳具有一定作用。
<br class="ltx_break"/>The vitamins and minerals in energy drinks play a certain role in quickly replenishing nutrients and <span class="ltx_text" id="S3.T2.1.2.3.1.1.2" style="color:#0000FF;">inducing</span> fatigue after exercise.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.1.3.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.3.1.1">
<span class="ltx_p" id="S3.T2.1.3.1.1.1" style="width:34.7pt;">KInve.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.1.3.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.3.2.1">
<span class="ltx_p" id="S3.T2.1.3.2.1.1" style="width:182.1pt;">一般蚕可以活一个多月，其中从孵化到结茧根据季节不同大约是<span class="ltx_text" id="S3.T2.1.3.2.1.1.1" style="color:#0000FF;">25-32</span>天，变成蛹后有<span class="ltx_text" id="S3.T2.1.3.2.1.1.2" style="color:#BF8040;">15-18</span>天，最后成蛾是1-3天。
<br class="ltx_break"/>A typical silkworm can live for just over a month, during which the period from hatching to cocooning varies roughly from <span class="ltx_text" id="S3.T2.1.3.2.1.1.3" style="color:#0000FF;">25 to 32</span> days depending on the season, followed by <span class="ltx_text" id="S3.T2.1.3.2.1.1.4" style="color:#BF8040;">15 to 18</span> days as a pupa, and finally 1 to 3 days as a moth.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.1.3.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.3.3.1">
<span class="ltx_p" id="S3.T2.1.3.3.1.1" style="width:182.1pt;">一般蚕可以活一个多月，其中从孵化到结茧根据季节不同大约是<span class="ltx_text" id="S3.T2.1.3.3.1.1.1" style="color:#BF8040;">15-18</span>天，变成蛹后有<span class="ltx_text" id="S3.T2.1.3.3.1.1.2" style="color:#0000FF;">25-32</span>天，最后成蛾是1-3天。
<br class="ltx_break"/>A typical silkworm can live for just over a month, during which the period from hatching to cocooning varies roughly from <span class="ltx_text" id="S3.T2.1.3.3.1.1.3" style="color:#BF8040;">15 to 18</span> days depending on the season, followed by <span class="ltx_text" id="S3.T2.1.3.3.1.1.4" style="color:#0000FF;">25 to 32</span> days as a pupa, and finally 1 to 3 days as a moth.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.1.4.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.4.1.1">
<span class="ltx_p" id="S3.T2.1.4.1.1.1" style="width:34.7pt;">KConf.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.1.4.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.4.2.1">
<span class="ltx_p" id="S3.T2.1.4.2.1.1" style="width:182.1pt;">防晒霜中的<span class="ltx_text" id="S3.T2.1.4.2.1.1.1" style="color:#0000FF;">无机化学物质</span>可以<span class="ltx_text" id="S3.T2.1.4.2.1.1.2" style="color:#0000FF;">反射或散射皮肤上的光线</span>，而<span class="ltx_text" id="S3.T2.1.4.2.1.1.3" style="color:#BF8040;">有机(碳基)化学物质</span>可以<span class="ltx_text" id="S3.T2.1.4.2.1.1.4" style="color:#BF8040;">吸收紫外线</span>。
<br class="ltx_break"/>The <span class="ltx_text" id="S3.T2.1.4.2.1.1.5" style="color:#0000FF;">inorganic chemicals</span> in sunscreen can <span class="ltx_text" id="S3.T2.1.4.2.1.1.6" style="color:#0000FF;">reflect or scatter light on the skin</span>, while <span class="ltx_text" id="S3.T2.1.4.2.1.1.7" style="color:#BF8040;">organic (carbon-based) chemicals</span> can <span class="ltx_text" id="S3.T2.1.4.2.1.1.8" style="color:#BF8040;">absorb ultraviolet rays</span>.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.1.4.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.4.3.1">
<span class="ltx_p" id="S3.T2.1.4.3.1.1" style="width:182.1pt;">防晒霜中的<span class="ltx_text" id="S3.T2.1.4.3.1.1.1" style="color:#0000FF;">无机化学物质</span>和<span class="ltx_text" id="S3.T2.1.4.3.1.1.2" style="color:#BF8040;">有机(碳基)化学物质</span>都可以<span class="ltx_text" id="S3.T2.1.4.3.1.1.3" style="color:#0000FF;">反射或散射皮肤上的光线</span>、<span class="ltx_text" id="S3.T2.1.4.3.1.1.4" style="color:#BF8040;">吸收紫外线</span>。
<br class="ltx_break"/>Both the <span class="ltx_text" id="S3.T2.1.4.3.1.1.5" style="color:#0000FF;">inorganic chemicals</span> and <span class="ltx_text" id="S3.T2.1.4.3.1.1.6" style="color:#BF8040;">organic (carbon-based) chemicals</span> in sunscreen can <span class="ltx_text" id="S3.T2.1.4.3.1.1.7" style="color:#0000FF;">reflect or scatter light on the skin</span> and <span class="ltx_text" id="S3.T2.1.4.3.1.1.8" style="color:#BF8040;">absorb ultraviolet rays</span>.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.1.5.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.5.1.1">
<span class="ltx_p" id="S3.T2.1.5.1.1.1" style="width:34.7pt;">KConc.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.1.5.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.5.2.1">
<span class="ltx_p" id="S3.T2.1.5.2.1.1" style="width:182.1pt;">随着健康意识的增强，越来越多的人开始注重<span class="ltx_text" id="S3.T2.1.5.2.1.1.1" style="color:#0000FF;">膳食平衡</span>。
<br class="ltx_break"/>With the increasing awareness of health, more and more people are beginning to focus on <span class="ltx_text" id="S3.T2.1.5.2.1.1.2" style="color:#0000FF;">a balanced diet</span>.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.1.5.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.5.3.1">
<span class="ltx_p" id="S3.T2.1.5.3.1.1" style="width:182.1pt;">随着健康意识的增强，越来越多的人开始注重<span class="ltx_text" id="S3.T2.1.5.3.1.1.1" style="color:#0000FF;">膳食的有机质量</span>。
<br class="ltx_break"/>With the increasing awareness of health, more and more people are beginning to focus on <span class="ltx_text" id="S3.T2.1.5.3.1.1.2" style="color:#0000FF;">the organic quality of their diets</span>.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.1.6.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.6.1.1">
<span class="ltx_p" id="S3.T2.1.6.1.1.1" style="width:34.7pt;">LOver.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.1.6.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.6.2.1">
<span class="ltx_p" id="S3.T2.1.6.2.1.1" style="width:182.1pt;">一般的我们平时见到的<span class="ltx_text" id="S3.T2.1.6.2.1.1.1" style="color:#0000FF;">蜘蛛</span>都是晚上出来。
<br class="ltx_break"/>The <span class="ltx_text" id="S3.T2.1.6.2.1.1.2" style="color:#0000FF;">spiders</span> that we usually see tend to come out at night.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.1.6.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.6.3.1">
<span class="ltx_p" id="S3.T2.1.6.3.1.1" style="width:182.1pt;">一般的我们平时见到的<span class="ltx_text" id="S3.T2.1.6.3.1.1.1" style="color:#0000FF;">昆虫</span>都是晚上出来。
<br class="ltx_break"/>The <span class="ltx_text" id="S3.T2.1.6.3.1.1.2" style="color:#0000FF;">insects</span> that we usually see tend to come out at night.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.7">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.1.7.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.7.1.1">
<span class="ltx_p" id="S3.T2.1.7.1.1.1" style="width:34.7pt;">LCaus.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.1.7.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.7.2.1">
<span class="ltx_p" id="S3.T2.1.7.2.1.1" style="width:182.1pt;"><span class="ltx_text" id="S3.T2.1.7.2.1.1.1" style="color:#0000FF;">随着</span>信息技术的快速发展，大数据在各行各业中的应用越来越广泛。
<br class="ltx_break"/><span class="ltx_text" id="S3.T2.1.7.2.1.1.2" style="color:#0000FF;">With</span> the rapid development of information technology, the application of big data across various industries is becoming increasingly widespread.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.1.7.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.7.3.1">
<span class="ltx_p" id="S3.T2.1.7.3.1.1" style="width:182.1pt;">大数据在各行各业中的应用越来越广泛，这<span class="ltx_text" id="S3.T2.1.7.3.1.1.1" style="color:#0000FF;">导致</span>了信息技术的快速发展。
<br class="ltx_break"/>The application of big data across various industries is becoming increasingly widespread, <span class="ltx_text" id="S3.T2.1.7.3.1.1.2" style="color:#0000FF;">leading to</span> the rapid development of information technology.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.8">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.1.8.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.8.1.1">
<span class="ltx_p" id="S3.T2.1.8.1.1.1" style="width:34.7pt;">LConf.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.1.8.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.8.2.1">
<span class="ltx_p" id="S3.T2.1.8.2.1.1" style="width:182.1pt;">为了获得某项荣誉学生奖学金，学生<span class="ltx_text" id="S3.T2.1.8.2.1.1.1" style="color:#FF0000;">必须</span>具备以下条件：<span class="ltx_text" id="S3.T2.1.8.2.1.1.2" style="color:#0000FF;">成绩优秀</span>、<span class="ltx_text" id="S3.T2.1.8.2.1.1.3" style="color:#0000FF;">品行端正</span>、<span class="ltx_text" id="S3.T2.1.8.2.1.1.4" style="color:#BF8040;">参加社会实践活动</span>。
<br class="ltx_break"/>To receive a certain honor student scholarship, students <span class="ltx_text" id="S3.T2.1.8.2.1.1.5" style="color:#FF0000;">must</span> meet the following criteria: <span class="ltx_text" id="S3.T2.1.8.2.1.1.6" style="color:#0000FF;">excellent academic performance</span>, <span class="ltx_text" id="S3.T2.1.8.2.1.1.7" style="color:#0000FF;">good moral character</span>, and <span class="ltx_text" id="S3.T2.1.8.2.1.1.8" style="color:#BF8040;">participation in social practice activities</span>.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.1.8.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.8.3.1">
<span class="ltx_p" id="S3.T2.1.8.3.1.1" style="width:182.1pt;">学生<span class="ltx_text" id="S3.T2.1.8.3.1.1.1" style="color:#0000FF;">成绩优秀</span>、<span class="ltx_text" id="S3.T2.1.8.3.1.1.2" style="color:#0000FF;">品行端正<span class="ltx_text" id="S3.T2.1.8.3.1.1.2.1" style="color:#FF0000;">就可以</span></span>获得某项荣誉学生奖学金。
<br class="ltx_break"/>Students with <span class="ltx_text" id="S3.T2.1.8.3.1.1.3" style="color:#0000FF;">excellent academic performance</span> and <span class="ltx_text" id="S3.T2.1.8.3.1.1.4" style="color:#0000FF;">good moral character</span> <span class="ltx_text" id="S3.T2.1.8.3.1.1.5" style="color:#FF0000;">can</span> receive a certain honorary student scholarship.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.9">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S3.T2.1.9.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.9.1.1">
<span class="ltx_p" id="S3.T2.1.9.1.1.1" style="width:34.7pt;">LIncl.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S3.T2.1.9.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.9.2.1">
<span class="ltx_p" id="S3.T2.1.9.2.1.1" style="width:182.1pt;">坚持锻炼身体可以提高心肺能力，加强肌肉的耐力，提高身体的抗疲劳能力。
<br class="ltx_break"/>Regular exercise can enhance cardiorespiratory fitness, strengthen muscle endurance, and improve the body’s resistance to fatigue.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S3.T2.1.9.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.9.3.1">
<span class="ltx_p" id="S3.T2.1.9.3.1.1" style="width:182.1pt;">坚持锻炼身体可以提高心肺能力，<span class="ltx_text" id="S3.T2.1.9.3.1.1.1" style="color:#0000FF;">例如</span>加强肌肉的耐力、提高身体的抗疲劳能力。
<br class="ltx_break"/>Regular exercise can enhance cardiorespiratory fitness, <span class="ltx_text" id="S3.T2.1.9.3.1.1.2" style="color:#0000FF;">such as</span> strengthening muscle endurance and improving the body’s resistance to fatigue.</span>
</span>
</td>
</tr>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Synthetic Dataset</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Based on the above proposed error typology, we construct a synthetic dataset. In the dataset, the positive samples are factual consistent, whereas each negative sample has at least one factual inconsistency error. The dataset is constructed based on WebCPM <cite class="ltx_cite ltx_citemacro_citep">(Qin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib37" title="">2023</a>)</cite>, a web-enhanced question answering dataset in Chinese. Due to the space limit, in the following we briefly describe the process of dataset generation. Please refer to Appendix <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#A1" title="Appendix A Construction Details about Synthetic Dataset ‣ Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_tag">A</span></a> for more details of the construction of our synthetic dataset.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">Negative Samples</span> For each specific error type in the typology, we design a prompt to generate samples with this error. For the hallucination error, we setup three levels of difficulty for the evaluator to detect inconsistency and construct samples accordingly. For the remaining two categories, i.e., knowledge error and logical fallacy, we design a specific prompt for each error type except the Contradiction Error (<span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.2">KCont.</span>). Since <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.3">KCont.</span> may occur at different levels of granularity <cite class="ltx_cite ltx_citemacro_citep">(De Marneffe et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib13" title="">2008</a>)</cite>, i.e., word or sentence, we design one prompt for each. Apart from the above error types, we construct a new error type called Other Logical Fallacy (LOthe.), which accounts for potential errors in some complex logical connections uncovered by our previously defined four types of logical fallacy.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.1">Positive Samples</span> To enrich the sample diversity, we apply the augmentation technique in <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib29" title="">2022</a>)</cite>. Specifically, the original positive samples in WebCPM are augmented by synonym replacing and paraphrasing via certain prompt at either word or sentence level.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p4.1.1">Human Annotation Refinement</span> To enhance the quality of the coarse labels derived above, we further engage 12 human experts to annotate the factual consistency of each answer via a two-step approach <cite class="ltx_cite ltx_citemacro_citep">(Min et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib32" title="">2023</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="331" id="S3.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Error type distributions of the six LLMs in our real-world dataset (we omit the 50%<math alttext="\sim" class="ltx_Math" display="inline" id="S3.F3.2.m1.1"><semantics id="S3.F3.2.m1.1b"><mo id="S3.F3.2.m1.1.1" xref="S3.F3.2.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.F3.2.m1.1c"><csymbol cd="latexml" id="S3.F3.2.m1.1.1.cmml" xref="S3.F3.2.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.2.m1.1d">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.F3.2.m1.1e">∼</annotation></semantics></math>100% region in type ratio).</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Real-World Dataset</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">The synthetic dataset is generated based on various predefined error types without considering the distribution of these error types in the real world.
Consequently, there is a need for another evaluation dataset that better aligns with the actual distribution of answers in real-world RAG scenarios, thus serving as a real-world dataset. In contrast to previous studies that relied solely on GPT-based LLMs for generating responses to create their evaluation sets <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib10" title="">2023</a>; Min et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib32" title="">2023</a>)</cite>, we adopt a more comprehensive approach by utilizing six different LLMs to construct our real-world dataset.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">Specifically, we first collect 200 questions along with corresponding references. We then prompt six commonly used LLMs to generate answers for the questions based on the references, including gpt/4/turbo (GPT/4) <cite class="ltx_cite ltx_citemacro_citep">(Achiam et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib3" title="">2023</a>)</cite>, gpt/3.5/turbo (GPT/3.5) <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib34" title="">2022</a>)</cite>, Baichuan2/13B/Chat (Baichuan2) <cite class="ltx_cite ltx_citemacro_citep">(Baichuan, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib6" title="">2023</a>)</cite>, ChatGLM3 <cite class="ltx_cite ltx_citemacro_citep">(Zeng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib45" title="">2022</a>)</cite>, Qwen/14B/Chat (Qwen) <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib4" title="">2023a</a>)</cite>, and Chinese/Alpaca/2/13B/16k <cite class="ltx_cite ltx_citemacro_citep">(Cui et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib12" title="">2023</a>)</cite> (Alpaca2 (CH)) . In this way, we derive a total of 1200 data points, which constitute the real-world dataset.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">For each data point, we follow the same human annotation procedure as in our synthetic dataset to inspect if it is factually consistent. Moreover, if an answer is deemed factually inconsistent, the annotator will assign a specific error type from our proposed error typology to the answer. When the annotators notice that error of the answer does not fall into the aforementioned error types, they will mark the answer as ”Other Errors”.</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1">We now conduct empirical analyses on the error typology and the behaviors of various LLMs on the above real-world dataset.</p>
</div>
<div class="ltx_para" id="S3.SS3.p5">
<p class="ltx_p" id="S3.SS3.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p5.1.1">Overall Error Type Distribution</span>
We first justify our study on logical fallacy consistency detection by empirically showing that the logical fallacy errors are prevailing in the answers generated by various LLMs. To this end, we analyze the distribution of the error types annotated across the entire real-world dataset. As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#S3.F2" title="Figure 2 ‣ 3. FACE4RAG Benchmark ‣ Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_tag">2</span></a>, the hallucination error, knowledge error and logical fallacy account for 73.78% , 28.31%, 6.96% of all the inconsistent samples, respectively. It worth note that this 6.96% logical fallacy errors are not studied in the previous FCE methods. Besides, only 0.23% of the inconsistent samples are marked as ”Other Errors” by annotators, which suggests the comprehensiveness and completeness of our proposed error typology.</p>
</div>
<div class="ltx_para" id="S3.SS3.p6">
<p class="ltx_p" id="S3.SS3.p6.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p6.1.1">Error Distribution of Various Models </span> We then look deeper into the error types and their distributions among various LLMs in RAG. As presented in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#S3.F3" title="Figure 3 ‣ 3.2. Synthetic Dataset ‣ 3. FACE4RAG Benchmark ‣ Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_tag">3</span></a>, various LLMs exhibit distinct distributions on error types. For instance, <span class="ltx_text ltx_font_italic" id="S3.SS3.p6.1.2">LIncl.</span> emerges in three of the LLMs, and <span class="ltx_text ltx_font_italic" id="S3.SS3.p6.1.3">LCaus.</span> and <span class="ltx_text ltx_font_italic" id="S3.SS3.p6.1.4">LConf.</span> occurs in four models. In addition, while <span class="ltx_text ltx_font_italic" id="S3.SS3.p6.1.5">Hallu.</span> exists in all models, GPT-4 has a notably high percentage, with 77.91% of its errors being of this specific type; in comparison, Qwen only has 57.81% Hallucination Error and a higher proportion of logical fallacy at 9.38%. The distinct error types distributions of different LLMs suggest that FCE methods evaluated on a specific LLM may not generalize well to other LLMs, indicating the necessity for constructing a benchmark that is independent of the underlying LLM.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="314" id="S3.F4.g1" src="x4.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>A few examples for our proposed logic-preserving answer decomposition.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Logic-enhanced Factual Consistency Evaluation</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In the above statistic analysis, logical fallacy accounts for a considerable proportion of factual errors in real-world RAG scenarios. However, as we have analyzed before, existing FCE pipelines neglect the logical connections between segments in the original answer, which may result in wrong factual consistency evaluation result for samples with logical fallacy. Hence, to improve the evaluation ability of factuality consistency, a natural direction is to design an advanced FCE method that is capable of handling logical connections in long answers.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">In this section, we propose a novel pipeline called <span class="ltx_text ltx_font_italic" id="S4.p2.1.1">Logic-Enhanced FActual Consistency Evaluation for RAG (L-Face4RAG)</span>, which explicitly takes logical connections into consideration when evaluating the factual consistency. L-Face4RAG has two core modules, i.e., logic-preserving answer decomposition and fact-logic FCE, which will be described as follows.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Logic-Preserving Answer Decomposition</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Most existing studies directly decompose answers into segments, each containing only a single piece of information <cite class="ltx_cite ltx_citemacro_citep">(Min et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib32" title="">2023</a>; Kamoi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib24" title="">2023</a>)</cite>. In contrast, we propose to decompose the answers based on semantic linkages<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Semantic linkage refers to the connection or association between different pieces of information based on their meanings or semantic content <cite class="ltx_cite ltx_citemacro_citep">(Jackendoff, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib21" title="">1992</a>)</cite>.</span></span></span> and logical connections, which preserves the logical relationships and facilitates logical consistency evaluation. The core design in this module is an elaborated prompt based on the following three principles for answer decomposition.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">We prompt GPT-4 to execute the decomposition only when the two or multiple sentences do not exhibit strong semantic or logical connection.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">To ensure that each segment can be understood by GPT-4 independently without leveraging other segments, any pronoun in a segment that refers to other contextual information should be substituted with appropriate reference.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1">During the decomposition process, GPT-4 is required to maintain the sentence structure of the original answer to the best extent. This principle alleviates the risk of introducing additional hallucination to the original answer.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">In order to help GPT-4 better understand our principles for answer decomposition and deal with texts with various formats, we construct three kinds of instances to serve as the few-shot examples. The specific type of instances are as follows:</p>
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I2.i1.p1.1.1">Logical Connection</span> refers to the instances having logical connections between the sentences and thus, GPT-4 needs to learn the solution of the logical connections during the decomposition process.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I2.i2.p1.1.1">Pronoun Substitution</span> involves replacing pronoun with their referent entities during the answer decomposition to make each answer segment understandable on its own, without reliance on other segments.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i3.p1">
<p class="ltx_p" id="S4.I2.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I2.i3.p1.1.1">Unique Format</span> refers to the instances with unique format and may be difficult for GPT-4 to decompose properly.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">Examples of the answer decomposition are provided in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#S3.F4" title="Figure 4 ‣ 3.3. Real-World Dataset ‣ 3. FACE4RAG Benchmark ‣ Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_tag">4</span></a> and the detailed prompt is provided on our benchmark webpage.<sup class="ltx_sup" id="S4.SS1.p4.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#footnote1" title="footnote 1 ‣ Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_tag">1</span></a></sup></p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Fact-Logic FCE</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Previous methods directly invoke an LLM to evaluate the decomposed segments and overlook the logical fallacy. To evaluate the logical fallacy, we develop a two-stage procedure for factual consistency evaluation, which consists of a conventional stage of fact consistency evaluation and an extra stage that evaluates from both perspectives of fact and logic; we introduce the COT mechanism <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib44" title="">2022</a>)</cite> into both stages to improve LLM’s ability of evaluation. The prompts for each stage are provided at our benchmark webpage.<sup class="ltx_sup" id="S4.SS2.p1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#footnote1" title="footnote 1 ‣ Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_tag">1</span></a></sup></p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1">Fact Consistency Evaluation</span> In this stage, GPT-4 is instructed to assess the consistency of each piece of information in the segment against the reference, which mainly concerns with the hallucination error and the knowledge error. Unlike previous methods that directly instruct the model to assess the consistency with the reference <cite class="ltx_cite ltx_citemacro_citep">(Es et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib14" title="">2023</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib10" title="">2023</a>)</cite>, we use the COT technique to guide the model to evaluate the segment step-by-step, with the following steps:</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<ol class="ltx_enumerate" id="S4.I3">
<li class="ltx_item" id="S4.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S4.I3.i1.p1">
<p class="ltx_p" id="S4.I3.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I3.i1.p1.1.1">Informational Points Extraction</span>: GPT-4 extracts all informational points from the segment. This step ensures that each component of the segment will be evaluated.</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S4.I3.i2.p1">
<p class="ltx_p" id="S4.I3.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I3.i2.p1.1.1">Context Identification</span>: For each informational point, GPT-4 locates the corresponding content within the reference.</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span>
<div class="ltx_para" id="S4.I3.i3.p1">
<p class="ltx_p" id="S4.I3.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I3.i3.p1.1.1">Fact Consistency Check</span>: GPT-4 conducts a thorough fact consistency check for each informational point against its corresponding context. A segment is deemed consistent if and only if every single informational point aligns fact consistent with the reference.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">The above instruction imposes GPT-4 to evaluate consistency with each relevant content rather than the full context of the reference, reducing the probability of misjudging positive samples. We will empirically justify this point in our experiments.</p>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="723" id="S4.F5.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>The process of logic consistency evaluation.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p5">
<p class="ltx_p" id="S4.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p5.1.1">Logic Consistency Evaluation</span> In this stage, GPT-4 is instructed with a COT prompt to evaluate the logical fallacy. Since no FCE method has explicitly handled logical fallacy before, this is a novel stage for FCE, in which we elaborate a COT prompt as follows. The specific process is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#S4.F5" title="Figure 5 ‣ 4.2. Fact-Logic FCE ‣ 4. Logic-enhanced Factual Consistency Evaluation ‣ Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
<ol class="ltx_enumerate" id="S4.I4">
<li class="ltx_item" id="S4.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S4.I4.i1.p1">
<p class="ltx_p" id="S4.I4.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I4.i1.p1.1.1">Context Identification</span>: Given an answer segment, GPT-4 identifies its relevant context from the reference.</p>
</div>
</li>
<li class="ltx_item" id="S4.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S4.I4.i2.p1">
<p class="ltx_p" id="S4.I4.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I4.i2.p1.1.1">Logical Structure Analysis</span>: GPT-4 then analyzes the logical structure for both answer and relevant context.</p>
<ol class="ltx_enumerate" id="S4.I4.i2.I1">
<li class="ltx_item" id="S4.I4.i2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span>
<div class="ltx_para" id="S4.I4.i2.I1.i1.p1">
<p class="ltx_p" id="S4.I4.i2.I1.i1.p1.1">Identify the logical connections, and the sentence components connected by these logical connections.</p>
</div>
</li>
<li class="ltx_item" id="S4.I4.i2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span>
<div class="ltx_para" id="S4.I4.i2.I1.i2.p1">
<p class="ltx_p" id="S4.I4.i2.I1.i2.p1.1">Determine the type and function of logical connections to understand the logical structure between sentence components, e.g., causal, conditional, etc.</p>
</div>
</li>
<li class="ltx_item" id="S4.I4.i2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(c)</span>
<div class="ltx_para" id="S4.I4.i2.I1.i3.p1">
<p class="ltx_p" id="S4.I4.i2.I1.i3.p1.1">Map sentence components to their corresponding logical relations, e.g., cause and effect for causal relation, condition and result for conditional relation, etc.</p>
</div>
</li>
<li class="ltx_item" id="S4.I4.i2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(d)</span>
<div class="ltx_para" id="S4.I4.i2.I1.i4.p1">
<p class="ltx_p" id="S4.I4.i2.I1.i4.p1.1">Build a complete logic framework of the sentence.</p>
</div>
</li>
</ol>
</div>
</li>
<li class="ltx_item" id="S4.I4.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span>
<div class="ltx_para" id="S4.I4.i3.p1">
<p class="ltx_p" id="S4.I4.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I4.i3.p1.1.1">Logical Structure Comparison</span>: Finally, GPT-4 compares the logical structure of the answer segment with the relevant context and judge if the answer segment is logically consistent with the reference.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S4.SS2.p6">
<p class="ltx_p" id="S4.SS2.p6.1">The last step of FCE is the aggregation of answer-level factual consistency measurement. Specifically, an answer is marked factual consistent if and only if all of its decomposed segments have passed the above two consistency evaluation stages.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3. </span>Performance comparison of factual consistency evaluation on the synthetic dataset. </figcaption>
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1">
<tr class="ltx_tr" id="S4.T3.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.1.1.2" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.2.1">Total</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.1.1.3" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.3.1">Pos.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="9" id="S4.T3.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.4.1">Negative samples</span></td>
<td class="ltx_td ltx_border_tt" id="S4.T3.1.1.5"></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1">Hallu.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.2">KCont.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.3">KInve.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.4">KConf.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.5">KConc.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.6">LOver.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.7">LCaus.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.8">LConf.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.9">LIncl.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.10">LOthe.</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.3.1">FACTSCORE(GPT-3.5)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.3.2">70.36</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.3">37.31</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.4">90.45</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.3.5.1">100</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.6">94.44</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.7">55.56</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.8">94.29</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.9">78.57</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.10">64.29</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.11">68.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.12">46.34</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.13">86.07</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.4.1">FACTSCORE(GPT-4)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.4.2">71.82</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.3">33.50</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.4">93.97</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.4.5.1">100</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.6">96.30</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.7">68.25</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.8">97.14</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.9">87.5</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.10">60.71</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.11">72.00</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.12">51.22</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.13">88.37</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.5.1">FELM</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.5.2">68.05</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.3">77.67</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.4">42.21</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.5">99.27</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.6">91.98</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.7">22.22</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.8">88.57</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.9">69.64</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.10">42.86</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.11">54.00</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.12">4.88</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.13">32.56</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.6.1">RAGAS(GPT-3.5)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.6.2">69.59</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.3">70.81</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.4">76.89</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.5">98.54</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.6">71.60</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.7">49.21</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.8">87.14</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.9">54.46</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.10">39.29</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.11">48.00</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.12">34.15</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.13">44.19</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.7.1">RAGAS(GPT-4)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.7.2">76.37</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.7.3">73.60</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.7.4">93.97</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.7.5">99.27</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.7.6">79.01</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.7.7">52.38</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.7.8">90.00</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.7.9">58.93</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.7.10">50.00</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.7.11">50.00</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.7.12"><span class="ltx_text ltx_font_bold" id="S4.T3.1.7.12.1">53.66</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.7.13">72.09</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.8.1">RefChecker</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.8.2">78.52</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.8.3">76.14</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.8.4">95.48</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.8.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.8.5.1">100</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.8.6">87.65</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.8.7">63.49</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.8.8">92.86</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.8.9">55.36</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.8.10">50.00</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.8.11">52.00</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.8.12">36.59</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.8.13">67.44</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.9">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.1.9.1">L-Face4RAG (Ours)</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.1.9.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.9.2.1">93.38</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.9.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.9.3.1">96.19</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.9.4"><span class="ltx_text ltx_font_bold" id="S4.T3.1.9.4.1">96.98</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.9.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.9.5.1">100</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.9.6"><span class="ltx_text ltx_font_bold" id="S4.T3.1.9.6.1">98.77</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.9.7"><span class="ltx_text ltx_font_bold" id="S4.T3.1.9.7.1">76.19</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.9.8"><span class="ltx_text ltx_font_bold" id="S4.T3.1.9.8.1">98.57</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.9.9"><span class="ltx_text ltx_font_bold" id="S4.T3.1.9.9.1">90.18</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.9.10"><span class="ltx_text ltx_font_bold" id="S4.T3.1.9.10.1">92.86</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.9.11"><span class="ltx_text ltx_font_bold" id="S4.T3.1.9.11.1">80.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.9.12">51.22</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.9.13"><span class="ltx_text ltx_font_bold" id="S4.T3.1.9.13.1">90.70</span></td>
</tr>
</table>
</figure>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4. </span>Performance comparison of factual consistency evaluation on the real-world dataset. </figcaption>
<table class="ltx_tabular ltx_align_middle" id="S4.T4.1">
<tr class="ltx_tr" id="S4.T4.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.1.1.1">
<span class="ltx_text" id="S4.T4.1.1.1.1"></span><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.2"> <span class="ltx_text" id="S4.T4.1.1.1.2.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T4.1.1.1.2.1.1">
<span class="ltx_tr" id="S4.T4.1.1.1.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.1.1.1.2.1.1.1.1">Method</span></span>
</span></span><span class="ltx_text" id="S4.T4.1.1.1.2.2"></span></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.1.1.2">
<span class="ltx_text" id="S4.T4.1.1.2.1"></span><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.2.2"> <span class="ltx_text" id="S4.T4.1.1.2.2.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T4.1.1.2.2.1.1">
<span class="ltx_tr" id="S4.T4.1.1.2.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.1.1.2.2.1.1.1.1">Total</span></span>
</span></span><span class="ltx_text" id="S4.T4.1.1.2.2.2"></span></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.3">
<span class="ltx_text" id="S4.T4.1.1.3.1"></span><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.3.2"> <span class="ltx_text" id="S4.T4.1.1.3.2.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T4.1.1.3.2.1.1">
<span class="ltx_tr" id="S4.T4.1.1.3.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.1.1.3.2.1.1.1.1">Baichuan2</span></span>
</span></span><span class="ltx_text" id="S4.T4.1.1.3.2.2"></span></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.4">
<span class="ltx_text" id="S4.T4.1.1.4.1"></span><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.4.2"> <span class="ltx_text" id="S4.T4.1.1.4.2.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T4.1.1.4.2.1.1">
<span class="ltx_tr" id="S4.T4.1.1.4.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.1.1.4.2.1.1.1.1">ChatGLM3</span></span>
</span></span><span class="ltx_text" id="S4.T4.1.1.4.2.2"></span></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.5">
<span class="ltx_text" id="S4.T4.1.1.5.1"></span><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.5.2"> <span class="ltx_text" id="S4.T4.1.1.5.2.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T4.1.1.5.2.1.1">
<span class="ltx_tr" id="S4.T4.1.1.5.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.1.1.5.2.1.1.1.1">GPT-3.5</span></span>
</span></span><span class="ltx_text" id="S4.T4.1.1.5.2.2"></span></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.6">
<span class="ltx_text" id="S4.T4.1.1.6.1"></span><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.6.2"> <span class="ltx_text" id="S4.T4.1.1.6.2.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T4.1.1.6.2.1.1">
<span class="ltx_tr" id="S4.T4.1.1.6.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.1.1.6.2.1.1.1.1">GPT-4</span></span>
</span></span><span class="ltx_text" id="S4.T4.1.1.6.2.2"></span></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.7">
<span class="ltx_text" id="S4.T4.1.1.7.1"></span><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.7.2"> <span class="ltx_text" id="S4.T4.1.1.7.2.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T4.1.1.7.2.1.1">
<span class="ltx_tr" id="S4.T4.1.1.7.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.1.1.7.2.1.1.1.1">Alpaca2 (CH)</span></span>
</span></span><span class="ltx_text" id="S4.T4.1.1.7.2.2"></span></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.8">
<span class="ltx_text" id="S4.T4.1.1.8.1"></span><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.8.2"> <span class="ltx_text" id="S4.T4.1.1.8.2.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T4.1.1.8.2.1.1">
<span class="ltx_tr" id="S4.T4.1.1.8.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.1.1.8.2.1.1.1.1">Qwen</span></span>
</span></span><span class="ltx_text" id="S4.T4.1.1.8.2.2"></span></span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.2.1">FACTSCORE(GPT-3.5)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.2.2">53.33</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.3">54.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.4">55.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.5">47.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.6">51.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.7">59.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.8">52.5</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.3.1">FACTSCORE(GPT-4)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.3.2">54.67</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.3.3">55.0</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.3.4">59.5</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.3.5">46.5</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.3.6">52.5</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.3.7">63.0</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.3.8">51.5</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.4.1">FELM</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.4.2">55.00</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.3">49.6</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.4">56.0</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.5">56.8</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.6">52.0</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.7">55.6</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.8">60.0</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.5.1">RAGAS(GPT-3.5)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.5.2">65.92</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.3">64.5</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.4">68.5</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.5">64.5</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.6">60.0</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.7">65.0</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.8">73.0</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.6.1">RAGAS(GPT-4)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.6.2">72.92</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.6.3">72.5</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.6.4">74.0</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.6.5">71.5</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.6.6">68.5</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.6.7">76.5</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.6.8">74.5</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.7.1">RefChecker</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.7.2">68.25</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.7.3">62.0</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.7.4">72.0</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.7.5">66.5</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.7.6">63.0</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.7.7">74.5</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.7.8">71.5</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.8">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T4.1.8.1">L-Face4RAG (Ours)</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T4.1.8.2"><span class="ltx_text ltx_font_bold" id="S4.T4.1.8.2.1">87.75</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.8.3"><span class="ltx_text ltx_font_bold" id="S4.T4.1.8.3.1">90.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.8.4"><span class="ltx_text ltx_font_bold" id="S4.T4.1.8.4.1">88.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.8.5"><span class="ltx_text ltx_font_bold" id="S4.T4.1.8.5.1">81.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.8.6"><span class="ltx_text ltx_font_bold" id="S4.T4.1.8.6.1">86.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.8.7"><span class="ltx_text ltx_font_bold" id="S4.T4.1.8.7.1">93.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.8.8"><span class="ltx_text ltx_font_bold" id="S4.T4.1.8.8.1">87.5</span></td>
</tr>
</table>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Experiments</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we conduct extensive
experiments to evaluate the effectiveness of our proposed L-Face4RAG pipeline. Our experiments show that on both synthetic data and real-world data in Face4RAG benchmark, our L-Face4RAG method substantially outperforms the existing FCE methods. Notably, its superiority goes beyond the Chinese RAG task from which L-Face4RAG is originally motivated, as L-Face4RAG achieves SOTA results on 6 out of 7 of the existing English datasets and also a substantially higher average score on all tasks.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Experimental Setup </h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.1">Baselines</span> We compare L-Face4RAG with four GPT-based fine/grained FCE methods:</p>
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1">FACTSCORE <cite class="ltx_cite ltx_citemacro_citep">(Min et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib32" title="">2023</a>)</cite> first breaks the answer into a series of atomic facts and then assigns a binary label to each atomic fact individually.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1">FELM <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib10" title="">2023</a>)</cite> first segments the answer into fine-grained textual spans and then evaluates the factual consistency of all textual spans collectively. It outputs the corresponding numbers of factual inconsistent textual spans if existed.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1">Ragas <cite class="ltx_cite ltx_citemacro_citep">(Es et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib14" title="">2023</a>)</cite> first extracts a set of statements from the answer and then evaluates the factual consistency of all statements collectively, outputting a binary label for each statement along with the corresponding reason for the assessment.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i4.p1">
<p class="ltx_p" id="S5.I1.i4.p1.1">Refchecker <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib19" title="">2023</a>)</cite> extracts knowledge triplets from the answer and evaluates each knowledge triplet separately.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.1">Implementation Details</span> As the above FCE baselines are originally designed for tasks in English, we adapt them to our Chinese RAG task by translating their prompts into Chinese.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">When experimenting with FELM, we utilize the Reference-doc augmented evaluator <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib10" title="">2023</a>)</cite>, in alignment with our task which is focused on evaluating the factual consistency of answers against their references. Specifically, we input our references as the retrieved reference doc in FELM’s evaluation framework. We select the best-performing estimator in <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib10" title="">2023</a>)</cite>, i.e., decomposing the answer with segment-based method and utilizing GPT-4 as the factual error detector.</p>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1">Since the original settings of FACTSCORE and RAGAS are based on GPT-3.5, we conduct experiments with both GPT-3.5 and GPT-4 to eschew the effect of the possible performance gap between GPT-3.5 and GPT-4 on the empirical results.</p>
</div>
<div class="ltx_para" id="S5.SS1.p5">
<p class="ltx_p" id="S5.SS1.p5.1">Finally, to apply our proposed evaluation pipeline, we decompose the answer into segments and assess the factual consistency of each segment respectively. The outputs include both the label and the corresponding explanations. To derive deterministic output from GPT-4, we set its temperature to 0.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Performance Comparison on Face4RAG</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We first compare the performance of our proposed L-Face4RAG pipeline against various FCE baselines on the Face4RAG benchmark, which includes a synthetic dataset and a real-world dataset.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p2.1.1">Synthetic Dataset</span>
In Table <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#S4.T3" title="Table 3 ‣ 4.2. Fact-Logic FCE ‣ 4. Logic-enhanced Factual Consistency Evaluation ‣ Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_tag">3</span></a>, we report the predictive accuracy of different error types for examined FCE methods on the synthetic dataset. From the results, we have the two main observations. (i) Our method achieves the highest accuracy on most of the error types (except on <span class="ltx_text ltx_font_italic" id="S5.SS2.p2.1.2">LIncl.</span> where it is slightly worse than RAGAS with GPT-4), which amounts to a significant improvement on overall accuracy compared to all the baselines. (ii) In particular, the performance gap between our method and baselines on error types of logical fallacy are much larger than the gap on other error types, which indicates that our method is especially capable of handling logical fallacy owing to our specific algorithmic designs.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p3.1.1">Real-world Dataset</span> In Table <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#S4.T4" title="Table 4 ‣ 4.2. Fact-Logic FCE ‣ 4. Logic-enhanced Factual Consistency Evaluation ‣ Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_tag">4</span></a>, we compare the predictive performance of our proposed pipeline with previous FCE methods on the real-world dataset. From the results we observe that: (i) The overall accuracy of our method is substantially higher than those of the baseline FCE methods, showing superiority in real-world scenarios. (ii) Moreover, on most of the subsets generated by different LLMs, our method consistently outperforms baseline methods, which indicates the superiority of our method is universal and independent of the error distribution, which is in line with the empirical results on the synthetic dataset.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Performance Comparison on Existing FCE Benchmark</h3>
<figure class="ltx_table" id="S5.T5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5. </span>Performance comparison of factual consistency evaluation on the existing benchmark.</figcaption>
<table class="ltx_tabular ltx_align_middle" id="S5.T5.1">
<tr class="ltx_tr" id="S5.T5.1.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T5.1.2.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T5.1.2.1.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T5.1.2.2" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T5.1.2.2.1">Avg.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S5.T5.1.2.3"><span class="ltx_text ltx_font_bold" id="S5.T5.1.2.3.1">RAG</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S5.T5.1.2.4"><span class="ltx_text ltx_font_bold" id="S5.T5.1.2.4.1">Summ.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S5.T5.1.2.5"><span class="ltx_text ltx_font_bold" id="S5.T5.1.2.5.1">Dial.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.1.2.6"><span class="ltx_text ltx_font_bold" id="S5.T5.1.2.6.1">Fact Verif.</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2">RAGAS<cite class="ltx_cite ltx_citemacro_citep">(Es et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib14" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.1.1.3">RefChecker<cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib19" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.4">FRANK<cite class="ltx_cite ltx_citemacro_citep">(Pagnoni et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib35" title="">2021</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.1.1.5">SummEval<cite class="ltx_cite ltx_citemacro_citep">(Fabbri et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib15" title="">2021</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.1">
<math alttext="Q^{2}" class="ltx_Math" display="inline" id="S5.T5.1.1.1.m1.1"><semantics id="S5.T5.1.1.1.m1.1a"><msup id="S5.T5.1.1.1.m1.1.1" xref="S5.T5.1.1.1.m1.1.1.cmml"><mi id="S5.T5.1.1.1.m1.1.1.2" xref="S5.T5.1.1.1.m1.1.1.2.cmml">Q</mi><mn id="S5.T5.1.1.1.m1.1.1.3" xref="S5.T5.1.1.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S5.T5.1.1.1.m1.1b"><apply id="S5.T5.1.1.1.m1.1.1.cmml" xref="S5.T5.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T5.1.1.1.m1.1.1.1.cmml" xref="S5.T5.1.1.1.m1.1.1">superscript</csymbol><ci id="S5.T5.1.1.1.m1.1.1.2.cmml" xref="S5.T5.1.1.1.m1.1.1.2">𝑄</ci><cn id="S5.T5.1.1.1.m1.1.1.3.cmml" type="integer" xref="S5.T5.1.1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.1.1.1.m1.1c">Q^{2}</annotation><annotation encoding="application/x-llamapun" id="S5.T5.1.1.1.m1.1d">italic_Q start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math><cite class="ltx_cite ltx_citemacro_citep">(Honovich et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib18" title="">2021</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.1.1.6">DialFact<cite class="ltx_cite ltx_citemacro_citep">(Gupta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib17" title="">2021</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.7">VitaminC<cite class="ltx_cite ltx_citemacro_citep">(Schuster et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib38" title="">2021</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.1.3.1">FACTSCORE(GPT-4)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.1.3.2">70.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.3.3">70</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.1.3.4">61</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.3.5">80</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.1.3.6">65</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.3.7">74</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.1.3.8">72</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.3.9">71</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.4.1">FELM</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.4.2">74.2</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.4.3">71</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.4.4">63</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.4.5">70</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.4.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.1.4.6.1">82</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.4.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.1.4.7.1">83</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.4.8"><span class="ltx_text ltx_font_bold" id="S5.T5.1.4.8.1">79</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.4.9">72</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.5.1">RAGAS(GPT-4)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.5.2">76.9</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.5.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.1.5.3.1">88</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.5.4">69</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.5.5"><span class="ltx_text ltx_font_bold" id="S5.T5.1.5.5.1">87</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.5.6">80</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.5.7">77</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.5.8">69</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.5.9">69</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.6.1">RefChecker</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.6.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.1.6.2.1">78.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.6.3">86</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.6.4"><span class="ltx_text ltx_font_bold" id="S5.T5.1.6.4.1">73</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.6.5">85</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.6.6">80</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.6.7">80</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.6.8">72</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.6.9"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.1.6.9.1">73</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.7">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T5.1.7.1">L-Face4RAG (Ours)</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T5.1.7.2"><span class="ltx_text ltx_font_bold" id="S5.T5.1.7.2.1">84.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.7.3"><span class="ltx_text ltx_font_bold" id="S5.T5.1.7.3.1">91</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T5.1.7.4"><span class="ltx_text ltx_font_bold" id="S5.T5.1.7.4.1">73</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.7.5"><span class="ltx_text ltx_font_bold" id="S5.T5.1.7.5.1">87</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T5.1.7.6"><span class="ltx_text ltx_font_bold" id="S5.T5.1.7.6.1">90</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.7.7"><span class="ltx_text ltx_font_bold" id="S5.T5.1.7.7.1">84</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T5.1.7.8"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.1.7.8.1">77</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.7.9"><span class="ltx_text ltx_font_bold" id="S5.T5.1.7.9.1">88</span></td>
</tr>
</table>
</figure>
<figure class="ltx_table" id="S5.T6">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6. </span>Main results of the ablation studies on the synthetic dataset. We compare L-Face4RAG with the variants using conventional answer decomposition (A.D.), removing the COT (w/o COT), and removing the logic consistency evaluation (w/o logi.eval). Overall accuracy and the accuracy on positive or negative samples are reported.</figcaption>
<table class="ltx_tabular ltx_align_middle" id="S5.T6.1">
<tr class="ltx_tr" id="S5.T6.1.1">
<td class="ltx_td ltx_border_tt" id="S5.T6.1.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T6.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T6.1.1.2.1">L-Face4RAG</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T6.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T6.1.1.3.1">A.D.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T6.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T6.1.1.4.1">w/o COT</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T6.1.1.5"><span class="ltx_text ltx_font_bold" id="S5.T6.1.1.5.1">w/o logi.eval</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.2.1">Overall</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.2.2">93.38</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.2.3">76.44</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.2.4">79.60</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.2.5">88.99</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.3">
<td class="ltx_td ltx_align_center" id="S5.T6.1.3.1">-Positive</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.3.2">96.19</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.3.3">91.62</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.3.4">51.27</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.3.5">97.46</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.4">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.1.4.1">-Negative</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.1.4.2">92.15</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.1.4.3">69.83</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.1.4.4">91.93</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.1.4.5">85.30</td>
</tr>
</table>
</figure>
<figure class="ltx_table" id="S5.T7">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7. </span>Comparison of the accuracy between L-Face4RAG and the counterpart method with conventional answer decomposition (A.D.) or without logic consistency evaluation (w/o logi.eval) for detecting specific error types of negative samples on the synthetic dataset.</figcaption>
<table class="ltx_tabular ltx_align_middle" id="S5.T7.1">
<tr class="ltx_tr" id="S5.T7.1.1">
<td class="ltx_td ltx_border_tt" id="S5.T7.1.1.1"></td>
<td class="ltx_td ltx_border_tt" id="S5.T7.1.1.2"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T7.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T7.1.1.3.1">L-Face4RAG</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T7.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T7.1.1.4.1">A.D.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T7.1.1.5"><span class="ltx_text ltx_font_bold" id="S5.T7.1.1.5.1">w/o logi. eval</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.2.1">Hallucination</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.2.2">Hallu.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.2.3">96.98</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.2.4">90.45</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.2.5">96.98</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.3.1" rowspan="4"><span class="ltx_text" id="S5.T7.1.3.1.1">Knowledge</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.3.2">KCont.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.3.3">100.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.3.4">100.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.3.5">100.00</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.4">
<td class="ltx_td ltx_align_center" id="S5.T7.1.4.1">KInve.</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.4.2">98.77</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.4.3">74.07</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.4.4">97.53</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.5">
<td class="ltx_td ltx_align_center" id="S5.T7.1.5.1">KConf.</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.5.2">76.19</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.5.3">41.27</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.5.4">66.67</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.6">
<td class="ltx_td ltx_align_center" id="S5.T7.1.6.1">KConc.</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.6.2">98.57</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.6.3">90.00</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.6.4">94.29</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.7">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T7.1.7.1" rowspan="5"><span class="ltx_text" id="S5.T7.1.7.1.1">Logical</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.7.2">LOver.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.7.3">90.18</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.7.4">42.86</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.7.5">83.93</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.8">
<td class="ltx_td ltx_align_center" id="S5.T7.1.8.1">LCaus.</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.8.2">92.86</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.8.3">32.14</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.8.4">35.71</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.9">
<td class="ltx_td ltx_align_center" id="S5.T7.1.9.1">LConf.</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.9.2">80.00</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.9.3">34.00</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.9.4">64.00</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.10">
<td class="ltx_td ltx_align_center" id="S5.T7.1.10.1">LIncl.</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.10.2">51.22</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.10.3">31.71</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.10.4">29.27</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.11">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T7.1.11.1">LOth.</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T7.1.11.2">90.70</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T7.1.11.3">44.19</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T7.1.11.4">65.12</td>
</tr>
</table>
</figure>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">We then evaluate the robustness and applicability of the proposed L-Face4RAG method on other factuality detection tasks, and in English. Specifically, we consider several commonly used FCE benchmarks in English on various tasks, including RAG<cite class="ltx_cite ltx_citemacro_citep">(Es et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib14" title="">2023</a>; Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib19" title="">2023</a>)</cite>, summarization<cite class="ltx_cite ltx_citemacro_citep">(Pagnoni et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib35" title="">2021</a>; Fabbri et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib15" title="">2021</a>)</cite>, dialogue<cite class="ltx_cite ltx_citemacro_citep">(Honovich et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib18" title="">2021</a>; Gupta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib17" title="">2021</a>)</cite> and fact verification<cite class="ltx_cite ltx_citemacro_citep">(Schuster et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib38" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">In Table <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#S5.T5" title="Table 5 ‣ 5.3. Performance Comparison on Existing FCE Benchmark ‣ 5. Experiments ‣ Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_tag">5</span></a>, we report the predictive accuracy of examined FCE methods on the above tasks. The results show that our proposed L-Face4RAG achieves SOTA results on 6 out of 7 of the existing datasets and also a substantially higher average score on all tasks, indicating the effectiveness of L-Face4RAG beyond the original factuality evaluation task in RAG, and its robustness to other languages. This validates the wide-applicability of our proposed method.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1">Besides the above comparison among different methods, we also observe that the ranking of the average score of various methods on the above commonly used benchmarks is similar to the ranking of the average score on all public tasks is 0.9, and the same 0.9 between the rankings on our real-world dataset and the public datasets. This validates the strong correlation between the evaluation results of our new benchmark and the results on existing benchmarks.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4. </span>Ablation Study</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">We now verify the specific design choices of our proposed evaluation pipeline by ablation study on Face4RAG benchmark. Specifically, we examine the effectiveness of each designed module by comparing L-Face4RAG with the counterpart method without such a module.
Due to space limit, here we only present the results on the synthetic dataset. Results on the real-world dataset are qualitatively similar and deferred to Appendix <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#A2" title="Appendix B Ablation Study Results on the Real-world Dataset ‣ Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS4.p2.1.1">Evaluating the Answer Decomposition Module. (A.D.)</span> Recall that our decomposition module preserves the logic connection within one segment, which may help better identify logical fallacy while reducing extra hallucination induced by decomposition. To verify this point, we conduct an ablation study by replacing our approach by a conventional decomposition method <cite class="ltx_cite ltx_citemacro_citep">(Es et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib14" title="">2023</a>)</cite>. As presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#S5.T6" title="Table 6 ‣ 5.3. Performance Comparison on Existing FCE Benchmark ‣ 5. Experiments ‣ Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_tag">6</span></a> and Table <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#S5.T7" title="Table 7 ‣ 5.3. Performance Comparison on Existing FCE Benchmark ‣ 5. Experiments ‣ Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_tag">7</span></a>, we observe a severe decline of overall accuracy in the counterpart method, especially for negative samples related to logical fallacy. This phenomenon accords with our intuition that conventional answer decomposition method may fail to detect logical fallacy since some logical connections may be discarded during the decomposition. In addition, positive samples also have a slight decrease in accuracy, which justifies the third principle in our logic-preserving answer decomposition module, i.e., preserving the structure of the original answer may alleviate the introduction of extra hallucination.</p>
</div>
<div class="ltx_para" id="S5.SS4.p3">
<p class="ltx_p" id="S5.SS4.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS4.p3.1.1">Evaluating the Introduction of COT.(w/o COT)</span> Recall that COT is adopted in both stages of factual consistency evaluation, which instructs the model to conduct finer-grained fact consistency evaluation and sophisticated logic consistency evaluation, respectively. To validate the introduction of COT, we consider a counterpart method that removes the detailed steps in the instructions and requires GPT-4 to directly generate evaluation without outputting the underlying reasoning process. As presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#S5.T6" title="Table 6 ‣ 5.3. Performance Comparison on Existing FCE Benchmark ‣ 5. Experiments ‣ Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_tag">6</span></a>, the overall accuracy drops severely after removing COT (from 93.38% to 79.60%), especially for the positive samples (from 96.19% to 51.27%). This justifies the benefit of introducing COT into FCE.</p>
</div>
<div class="ltx_para" id="S5.SS4.p4">
<p class="ltx_p" id="S5.SS4.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS4.p4.1.1">Evaluating the Stage of Logical Consistency Evaluation. (w/o logi. eval)</span>
To evaluate the effect of our proposed logical consistency evaluation stage on error detection, we construct a counterpart method by removing the second stage from our pipeline. The results presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#S5.T7" title="Table 7 ‣ 5.3. Performance Comparison on Existing FCE Benchmark ‣ 5. Experiments ‣ Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_tag">7</span></a> show that the counterpart method incurs a decline in overall accuracy. Among all the error types, logical fallacy contributes the major part of accuracy decline, which aligns with our main motivation of the second stage design for logical fallacy evaluation. In addition, there is a slight decrease in the accuracy of knowledge error. A possible reason is that the second stage may supplement the detection of some knowledge errors that are missed in the first stage. Hence, the second stage also benefits the detection of knowledge error. Note that for the hallucination error, we have not observed any obvious change in the detection accuracy; this matches our intuition that hallucination error has no relation with logical fallacy.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this work, we give a systematic study of factual consistency evaluation in RAG. Specifically, we first propose a comprehensive benchmark termed Face4RAG, which includes the synthetic dataset and the real-world dataset. In light of the possible failure of existing FCE methods in detecting logical fallacy in RAG, we then propose a novel FCE method termed L-Face4RAG. Compared to previous method, our method has two novel designs, i.e., logic-preserving decomposition and fact-logic FCE, which can better characterize the logical relations in different pieces of information in the sentence, leading to higher ability of logical fallacy evaluation. Extensive experiments on both the synthetic and real-world datasets verify the effectiveness of the L-Face4RAG method. Notably, the superiority of L-Face4RAG is consistent on a wide range of factuality detection benchmarks beyond the Chinese RAG task. Elaborated ablation studies also justify our core algorithm designs.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">gao (2019)</span>
<span class="ltx_bibblock">
2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://gaokao.neea.edu.cn/xhtml1/report/19012/5987-1.htm" title="">https://gaokao.neea.edu.cn/xhtml1/report/19012/5987-1.htm</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Achiam et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al<span class="ltx_text" id="bib.bib3.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.4.1">arXiv preprint arXiv:2303.08774</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,
Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023a.

</span>
<span class="ltx_bibblock">Qwen Technical Report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">arXiv preprint arXiv:2309.16609</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, et al<span class="ltx_text" id="bib.bib5.3.1">.</span> 2023b.

</span>
<span class="ltx_bibblock">Benchmarking Foundation Models with Language-Model-as-an-Examiner.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.4.1">arXiv preprint arXiv:2306.04181</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baichuan (2023)</span>
<span class="ltx_bibblock">
Baichuan. 2023.

</span>
<span class="ltx_bibblock">Baichuan 2: Open Large-scale Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:2309.10305</em> (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2309.10305" title="">https://arxiv.org/abs/2309.10305</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al<span class="ltx_text" id="bib.bib7.3.1">.</span> 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.4.1">Advances in neural information processing systems</em> 33 (2020), 1877–1901.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Anthony Chen, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019.

</span>
<span class="ltx_bibblock">Evaluating question answering evaluation. In <em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">Proceedings of the 2nd workshop on machine reading for question answering</em>. 119–124.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017.

</span>
<span class="ltx_bibblock">Reading wikipedia to answer open-domain questions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">arXiv preprint arXiv:1704.00051</em> (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shiqi Chen, Yiran Zhao, Jinghan Zhang, I Chern, Siyang Gao, Pengfei Liu, Junxian He, et al<span class="ltx_text" id="bib.bib10.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Felm: Benchmarking factuality evaluation of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.4.1">arXiv preprint arXiv:2310.00741</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chern et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
I Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, Pengfei Liu, et al<span class="ltx_text" id="bib.bib11.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">FacTool: Factuality Detection in Generative AI–A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.4.1">arXiv preprint arXiv:2307.13528</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yiming Cui, Ziqing Yang, and Xin Yao. 2023.

</span>
<span class="ltx_bibblock">Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">arXiv preprint arXiv:2304.08177</em> (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2304.08177" title="">https://arxiv.org/abs/2304.08177</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">De Marneffe et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2008)</span>
<span class="ltx_bibblock">
Marie-Catherine De Marneffe, Anna N Rafferty, and Christopher D Manning. 2008.

</span>
<span class="ltx_bibblock">Finding contradictions in text. In <em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">Proceedings of acl-08: Hlt</em>. 1039–1047.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Es et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. 2023.

</span>
<span class="ltx_bibblock">Ragas: Automated evaluation of retrieval augmented generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">arXiv preprint arXiv:2309.15217</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fabbri et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Alexander R Fabbri, Wojciech Kryściński, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021.

</span>
<span class="ltx_bibblock">Summeval: Re-evaluating summarization evaluation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">Transactions of the Association for Computational Linguistics</em> 9 (2021), 391–409.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Mingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Shiping Yang, and Xiaojun Wan. 2023.

</span>
<span class="ltx_bibblock">Human-like summarization evaluation with chatgpt.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">arXiv preprint arXiv:2304.02554</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Prakhar Gupta, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. 2021.

</span>
<span class="ltx_bibblock">DialFact: A benchmark for fact-checking in dialogue.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">arXiv preprint arXiv:2110.08222</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Honovich et al<span class="ltx_text" id="bib.bib18.3.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Or Honovich, Leshem Choshen, Roee Aharoni, Ella Neeman, Idan Szpektor, and Omri Abend. 2021.

</span>
<span class="ltx_bibblock"><math alttext="Q^{2}" class="ltx_Math" display="inline" id="bib.bib18.1.m1.1"><semantics id="bib.bib18.1.m1.1a"><msup id="bib.bib18.1.m1.1.1" xref="bib.bib18.1.m1.1.1.cmml"><mi id="bib.bib18.1.m1.1.1.2" xref="bib.bib18.1.m1.1.1.2.cmml">Q</mi><mn id="bib.bib18.1.m1.1.1.3" xref="bib.bib18.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="bib.bib18.1.m1.1b"><apply id="bib.bib18.1.m1.1.1.cmml" xref="bib.bib18.1.m1.1.1"><csymbol cd="ambiguous" id="bib.bib18.1.m1.1.1.1.cmml" xref="bib.bib18.1.m1.1.1">superscript</csymbol><ci id="bib.bib18.1.m1.1.1.2.cmml" xref="bib.bib18.1.m1.1.1.2">𝑄</ci><cn id="bib.bib18.1.m1.1.1.3.cmml" type="integer" xref="bib.bib18.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib18.1.m1.1c">Q^{2}</annotation><annotation encoding="application/x-llamapun" id="bib.bib18.1.m1.1d">italic_Q start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>: Evaluating factual consistency in knowledge-grounded dialogues via question generation and question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.4.1">arXiv preprint arXiv:2104.08202</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xiangkun Hu, Dongyu Ru, Qipeng Guo, Lin Qiu, and Zheng Zhang. 2023.

</span>
<span class="ltx_bibblock">RefChecker for Fine-grained Hallucination Detection.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/amazon-science/RefChecker" title="">https://github.com/amazon-science/RefChecker</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard and Grave (2020)</span>
<span class="ltx_bibblock">
Gautier Izacard and Edouard Grave. 2020.

</span>
<span class="ltx_bibblock">Leveraging passage retrieval with generative models for open domain question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2007.01282</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jackendoff (1992)</span>
<span class="ltx_bibblock">
Ray S Jackendoff. 1992.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Semantic structures</em>. Vol. 18.

</span>
<span class="ltx_bibblock">MIT press.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023.

</span>
<span class="ltx_bibblock">Survey of hallucination in natural language generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.3.1">Comput. Surveys</em> 55, 12 (2023), 1–38.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Zhijing Jin, Abhinav Lalwani, Tejas Vaidhya, Xiaoyu Shen, Yiwen Ding, Zhiheng Lyu, Mrinmaya Sachan, Rada Mihalcea, and Bernhard Schölkopf. 2022.

</span>
<span class="ltx_bibblock">Logical fallacy detection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">arXiv preprint arXiv:2202.13758</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kamoi et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Ryo Kamoi, Tanya Goyal, Juan Diego Rodriguez, and Greg Durrett. 2023.

</span>
<span class="ltx_bibblock">Wice: Real-world entailment for claims in wikipedia.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">arXiv preprint arXiv:2303.01432</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kryściński et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Wojciech Kryściński, Bryan McCann, Caiming Xiong, and Richard Socher. 2019.

</span>
<span class="ltx_bibblock">Evaluating the factual consistency of abstractive text summarization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">arXiv preprint arXiv:1910.12840</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Laban et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Philippe Laban, Tobias Schnabel, Paul N Bennett, and Marti A Hearst. 2022.

</span>
<span class="ltx_bibblock">SummaC: Re-visiting NLI-based models for inconsistency detection in summarization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">Transactions of the Association for Computational Linguistics</em> 10 (2022), 163–177.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lattimer et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Barrett Martin Lattimer, Patrick Chen, Xinyuan Zhang, and Yi Yang. 2023.

</span>
<span class="ltx_bibblock">Fast and Accurate Factual Inconsistency Detection Over Long Documents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">arXiv preprint arXiv:2310.13189</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al<span class="ltx_text" id="bib.bib28.3.1">.</span> 2020.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.4.1">Advances in Neural Information Processing Systems</em> 33 (2020), 9459–9474.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Bohan Li, Yutai Hou, and Wanxiang Che. 2022.

</span>
<span class="ltx_bibblock">Data augmentation approaches in natural language processing: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">Ai Open</em> 3 (2022), 71–90.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Nelson F Liu, Tianyi Zhang, and Percy Liang. 2023.

</span>
<span class="ltx_bibblock">Evaluating verifiability in generative search engines.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">arXiv preprint arXiv:2304.09848</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maynez et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020.

</span>
<span class="ltx_bibblock">On faithfulness and factuality in abstractive summarization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">arXiv preprint arXiv:2005.00661</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Min et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.

</span>
<span class="ltx_bibblock">FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">arXiv preprint arXiv:2305.14251</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muhlgay et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine, Nir Ratner, Yonatan Belinkov, Omri Abend, Kevin Leyton-Brown, Amnon Shashua, and Yoav Shoham. 2023.

</span>
<span class="ltx_bibblock">Generating benchmarks for factuality evaluation of language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.3.1">arXiv preprint arXiv:2307.06908</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2022)</span>
<span class="ltx_bibblock">
OpenAI. 2022.

</span>
<span class="ltx_bibblock">Chatgpt blog post.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/blog/chatgpt" title="">https://openai.com/blog/chatgpt</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pagnoni et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021.

</span>
<span class="ltx_bibblock">Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">arXiv preprint arXiv:2104.13346</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petric (2020)</span>
<span class="ltx_bibblock">
Domina Petric. 2020.

</span>
<span class="ltx_bibblock">Logical Fallacies.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">On-line Article (preprint), doi</em> 10 (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao Liang, Kunlun Zhu, Yankai Lin, Xu Han, Ning Ding, Huadong Wang, et al<span class="ltx_text" id="bib.bib37.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">WebCPM: Interactive Web Search for Chinese Long-form Question Answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.4.1">arXiv preprint arXiv:2305.06849</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schuster et al<span class="ltx_text" id="bib.bib38.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Tal Schuster, Adam Fisch, and Regina Barzilay. 2021.

</span>
<span class="ltx_bibblock">Get your vitamin C! robust fact verification with contrastive evidence.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.3.1">arXiv preprint arXiv:2103.08541</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shuster et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, et al<span class="ltx_text" id="bib.bib39.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.4.1">arXiv preprint arXiv:2208.03188</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Liyan Tang, Tanya Goyal, Alexander R Fabbri, Philippe Laban, Jiacheng Xu, Semih Yavuz, Wojciech Kryściński, Justin F Rousseau, and Greg Durrett. 2022.

</span>
<span class="ltx_bibblock">Understanding factual errors in summarization: Errors, summarizers, datasets, error detectors.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.3.1">arXiv preprint arXiv:2205.12854</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thomson and Reiter (2020)</span>
<span class="ltx_bibblock">
Craig Thomson and Ehud Reiter. 2020.

</span>
<span class="ltx_bibblock">A gold standard methodology for evaluating accuracy in data-to-text systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">arXiv preprint arXiv:2011.03992</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thoppilan et al<span class="ltx_text" id="bib.bib42.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al<span class="ltx_text" id="bib.bib42.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Lamda: Language models for dialog applications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.4.1">arXiv preprint arXiv:2201.08239</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib43.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Cunxiang Wang, Sirui Cheng, Zhikun Xu, Bowen Ding, Yidong Wang, and Yue Zhang. 2023.

</span>
<span class="ltx_bibblock">Evaluating open question answering evaluation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.3.1">arXiv preprint arXiv:2305.12421</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span class="ltx_text" id="bib.bib44.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al<span class="ltx_text" id="bib.bib44.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.4.1">Advances in Neural Information Processing Systems</em> 35 (2022), 24824–24837.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al<span class="ltx_text" id="bib.bib45.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al<span class="ltx_text" id="bib.bib45.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Glm-130b: An open bilingual pre-trained model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.4.1">arXiv preprint arXiv:2210.02414</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zha et al<span class="ltx_text" id="bib.bib46.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yuheng Zha, Yichi Yang, Ruichen Li, and Zhiting Hu. 2023.

</span>
<span class="ltx_bibblock">AlignScore: Evaluating Factual Consistency with a Unified Alignment Function.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.3.1">arXiv preprint arXiv:2305.16739</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib47.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019.

</span>
<span class="ltx_bibblock">Bertscore: Evaluating text generation with bert.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.3.1">arXiv preprint arXiv:1904.09675</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span class="ltx_text" id="bib.bib48.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Rongxin Zhu, Jianzhong Qi, and Jey Han Lau. 2023.

</span>
<span class="ltx_bibblock">Annotating and Detecting Fine-grained Factual Errors for Dialogue Summarization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.3.1">arXiv preprint arXiv:2305.16548</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Construction Details about Synthetic Dataset</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1"><span class="ltx_text ltx_font_bold" id="A1.p1.1.1">Negative Samples</span> The negative samples are constructed based on WebCPM <cite class="ltx_cite ltx_citemacro_citep">(Qin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib37" title="">2023</a>)</cite>, a web-enhanced question answering dataset in Chinese, following the aforementioned error typology. We use GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(Achiam et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib3" title="">2023</a>)</cite> to generate the negative samples and design specific prompts corresponding to each type of error. For every sample in WebCPM, we rewrite them for every error type to collect our synthetic negative sample dataset. Detailed prompts are provided at our benchmark webpage.<sup class="ltx_sup" id="A1.p1.1.2"><a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#footnote1" title="footnote 1 ‣ Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_tag">1</span></a></sup></p>
</div>
<div class="ltx_para" id="A1.p2">
<p class="ltx_p" id="A1.p2.1">For the hallucination error, we construct corresponding data according to three levels of difficulty for the evaluator to detect inconsistency. To be more specific, the easiest samples, in the first group, are completely off-topic from the reference. The second group includes content that is on-topic but contains ungrounded information. The third group of sample mixes factually consistent information with hallucinated content, resulting in sentences where some parts are supported by the reference, while others are ungrounded. This mixture poses a challenge for evaluators, as it could mistakenly be labeled as ”consistent” due to the presence of some consistent information.</p>
</div>
<div class="ltx_para" id="A1.p3">
<p class="ltx_p" id="A1.p3.1">For the remaining two categories, i.e., knowledge error and logical fallacy, we design a specific prompt for each error type except the Contradiction Error (<span class="ltx_text ltx_font_italic" id="A1.p3.1.1">KCont.</span>). For <span class="ltx_text ltx_font_italic" id="A1.p3.1.2">KCont.</span>, since it may occurs at different levels of granularity <cite class="ltx_cite ltx_citemacro_citep">(De Marneffe et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib13" title="">2008</a>)</cite>, i.e., word or sentence, we design one prompt for each level. Specifically, the prompt of the word-level <span class="ltx_text ltx_font_italic" id="A1.p3.1.3">KCont.</span> aims to select specific words in the answer and replace them with antonyms, and the prompt of the sentence-level <span class="ltx_text ltx_font_italic" id="A1.p3.1.4">KCont.</span> is designed to construct a new answer semantically contradicting the reference. Since the types of logical connections are diverse and comprehensive, for the completeness of the dataset, we consider a new error type called <span class="ltx_text ltx_font_italic" id="A1.p3.1.5">Other Logical Fallacy (LOthe.)</span>, which accounts for potential errors in some complex logical connections uncovered by our previously defined four types of logical fallacy. The prompt of <span class="ltx_text ltx_font_italic" id="A1.p3.1.6">LOthe.</span> is designed to drive GPT-4 to insert an arbitrary logical connection error into anywhere of the original answer.</p>
</div>
<div class="ltx_para" id="A1.p4">
<p class="ltx_p" id="A1.p4.1"><span class="ltx_text ltx_font_bold" id="A1.p4.1.1">Positive Samples</span> To enrich the diversity of positive samples, we employ the commonly used data augmentation techniques <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib29" title="">2022</a>)</cite> to generate more positive samples based on the answers from WebCPM. Our data augmentation process supplements the positive samples in WebCPM by synonym replacing and paraphrasing techniques via the prompts at the word or sentence level. Specifically, at the word level, we prompt GPT-4 to randomly replace some words in the answer with their synonyms; at the sentence level, we prompt to summarize the reference or rephrase the answer without changing the meaning of the original sentence.</p>
</div>
<div class="ltx_para" id="A1.p5">
<p class="ltx_p" id="A1.p5.1"><span class="ltx_text ltx_font_bold" id="A1.p5.1.1">Construction Details</span> Following the methodology in previous research <cite class="ltx_cite ltx_citemacro_citep">(Muhlgay et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib33" title="">2023</a>)</cite>, we utilize the few-shot technique <cite class="ltx_cite ltx_citemacro_citep">(Brown et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib7" title="">2020</a>)</cite>, in conjunction with the Chain of Thought (COT) <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib44" title="">2022</a>)</cite> approach, to guide GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(Achiam et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib3" title="">2023</a>)</cite> to construct high-quality samples. We provide clear directions and relevant examples in the prompt and ask the model not only to produce the newly constructed samples, but also to show the thinking process behind the modifications it makes to the samples in the output. This ensures that the model is indeed generating new samples in the direction we desire. The construction prompts for both positive and negative examples are provided at our benchmark webpage.<sup class="ltx_sup" id="A1.p5.1.2"><a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#footnote1" title="footnote 1 ‣ Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_tag">1</span></a></sup></p>
</div>
<div class="ltx_para" id="A1.p6">
<p class="ltx_p" id="A1.p6.1"><span class="ltx_text ltx_font_bold" id="A1.p6.1.1">Human Annotation Refinement</span> The above construction process produces a coarse label of factual consistency for each sample. To enhance the quality of the labels, we further engage 12 human experts to annotate the factual consistency of each answer via a two-step approach <cite class="ltx_cite ltx_citemacro_citep">(Min et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#bib.bib32" title="">2023</a>)</cite>. Specifically, the human annotator first decompose the answer into multiple segments; for each segment, the annotator is required to judge whether it is factual consistent with the reference and give the evidence of the judgement. Then the human annotations on all segments are aggregated to yield a factual consistent label for the answer.</p>
</div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Ablation Study Results on the Real-world Dataset</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#A2.T8" title="Table 8 ‣ Appendix B Ablation Study Results on the Real-world Dataset ‣ Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_tag">8</span></a> further validates the effectiveness of our approach, particularly highlighting its importance in practical scenarios where enhancing the recall of negative samples is crucial while preserving the discriminative ability of positive samples.</p>
</div>
<figure class="ltx_table" id="A2.T8">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 8. </span>Ablation Study Results on the Real-world Dataset. Here ”ours” refers to our original pipeline, ”A.D.” refers to the ablation result of answer decomposition, ”w/o COT” refers to the ablation of COT, and ”w/o logi. eval” refers to the ablation of the logical consistency evaluation.</figcaption>
<table class="ltx_tabular ltx_align_middle" id="A2.T8.1">
<tr class="ltx_tr" id="A2.T8.1.1">
<td class="ltx_td ltx_border_tt" id="A2.T8.1.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A2.T8.1.1.2"><span class="ltx_text ltx_font_bold" id="A2.T8.1.1.2.1">L-Face4FAG</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A2.T8.1.1.3"><span class="ltx_text ltx_font_bold" id="A2.T8.1.1.3.1">A.D.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A2.T8.1.1.4"><span class="ltx_text ltx_font_bold" id="A2.T8.1.1.4.1">w/o COT</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A2.T8.1.1.5"><span class="ltx_text ltx_font_bold" id="A2.T8.1.1.5.1">w/o logi. eval</span></td>
</tr>
<tr class="ltx_tr" id="A2.T8.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T8.1.2.1">Overall</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T8.1.2.2">87.75</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T8.1.2.3">76.75</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T8.1.2.4">65.50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T8.1.2.5">86.50</td>
</tr>
<tr class="ltx_tr" id="A2.T8.1.3">
<td class="ltx_td ltx_align_center" id="A2.T8.1.3.1">-Positive</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.3.2">94.60</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.3.3">82.87</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.3.4">55.99</td>
<td class="ltx_td ltx_align_center" id="A2.T8.1.3.5">95.65</td>
</tr>
<tr class="ltx_tr" id="A2.T8.1.4">
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T8.1.4.1">-Negative</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T8.1.4.2">75.96</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T8.1.4.3">66.21</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T8.1.4.4">81.86</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T8.1.4.5">70.75</td>
</tr>
</table>
</figure>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Statistic Details about Real-World Dataset</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#A3.T9" title="Table 9 ‣ Appendix C Statistic Details about Real-World Dataset ‣ Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_tag">9</span></a> shows the statistics of 200 model-generated answers in our real-world dataset from six LLMs. Table <a class="ltx_ref" href="https://arxiv.org/html/2407.01080v2#A3.T10" title="Table 10 ‣ Appendix C Statistic Details about Real-World Dataset ‣ Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese"><span class="ltx_text ltx_ref_tag">10</span></a> shows the specific information about the error distribution about the six LLMs in the real-world dataset.</p>
</div>
<figure class="ltx_table" id="A3.T9">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 9. </span> Statistics of 200 model-generated answers in our
real-world dataset from six LLMs. ”Avg. Length” indicates the average length of the generated answer. ”Error Rate” indicates the ratio of factual inconsistent answers.</figcaption>
<table class="ltx_tabular ltx_align_middle" id="A3.T9.1">
<tr class="ltx_tr" id="A3.T9.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="A3.T9.1.1.1"><span class="ltx_text ltx_font_bold" id="A3.T9.1.1.1.1">MODEL</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A3.T9.1.1.2"><span class="ltx_text ltx_font_bold" id="A3.T9.1.1.2.1">Avg. Length</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A3.T9.1.1.3"><span class="ltx_text ltx_font_bold" id="A3.T9.1.1.3.1">Error Rate(%)</span></td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.2.1">Baichuan2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.2.2">320.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.2.3">40.5</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.3">
<td class="ltx_td ltx_align_center" id="A3.T9.1.3.1">ChatGLM3</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.3.2">158.0</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.3.3">36.5</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.4">
<td class="ltx_td ltx_align_center" id="A3.T9.1.4.1">GPT-3.5</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.4.2">160.8</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.4.3">27.5</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.5">
<td class="ltx_td ltx_align_center" id="A3.T9.1.5.1">GPT-4</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.5.2">359.2</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.5.3">40.0</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.6">
<td class="ltx_td ltx_align_center" id="A3.T9.1.6.1">Alpaca2 (CH)</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.6.2">188.8</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.6.3">47.0</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.7">
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T9.1.7.1">Qwen</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T9.1.7.2">200.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T9.1.7.3">29.0</td>
</tr>
</table>
</figure>
<figure class="ltx_table" id="A3.T10">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 10. </span>Details about the distribution of the error types in the Real-World Dataset.</figcaption>
<table class="ltx_tabular ltx_align_middle" id="A3.T10.1">
<tr class="ltx_tr" id="A3.T10.1.1">
<td class="ltx_td ltx_border_tt" id="A3.T10.1.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A3.T10.1.1.2"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.2.1">Hallu.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A3.T10.1.1.3"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.3.1">KCont.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A3.T10.1.1.4"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.4.1">KInve.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A3.T10.1.1.5"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.5.1">KConf.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A3.T10.1.1.6"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.6.1">KConc.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A3.T10.1.1.7"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.7.1">LOver.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A3.T10.1.1.8"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.8.1">LCaus.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A3.T10.1.1.9"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.9.1">LConf.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A3.T10.1.1.10"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.10.1">LIncl.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A3.T10.1.1.11"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.11.1">LOthe.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A3.T10.1.1.12"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.12.1">Other Errors</span></td>
</tr>
<tr class="ltx_tr" id="A3.T10.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.1.2.1">Qwen</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.1.2.2">57.81</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.1.2.3">21.88</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.1.2.4">3.13</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.1.2.5">1.56</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.1.2.6">6.25</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.1.2.7">1.56</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.1.2.8">3.13</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.1.2.9">3.13</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.1.2.10">1.56</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.1.2.11">0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.1.2.12">0</td>
</tr>
<tr class="ltx_tr" id="A3.T10.1.3">
<td class="ltx_td ltx_align_center" id="A3.T10.1.3.1">GPT-4</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.3.2">77.91</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.3.3">5.81</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.3.4">3.49</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.3.5">2.33</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.3.6">5.81</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.3.7">1.16</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.3.8">2.33</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.3.9">1.16</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.3.10">0</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.3.11">0</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.3.12">0</td>
</tr>
<tr class="ltx_tr" id="A3.T10.1.4">
<td class="ltx_td ltx_align_center" id="A3.T10.1.4.1">GPT-3.5</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.4.2">62.71</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.4.3">15.25</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.4.4">5.08</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.4.5">3.39</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.4.6">10.17</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.4.7">3.39</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.4.8">0</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.4.9">0</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.4.10">0</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.4.11">0</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.4.12">0</td>
</tr>
<tr class="ltx_tr" id="A3.T10.1.5">
<td class="ltx_td ltx_align_center" id="A3.T10.1.5.1">Alpaca2 (CH)</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.5.2">69.61</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.5.3">8.82</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.5.4">3.92</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.5.5">2.94</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.5.6">7.84</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.5.7">3.92</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.5.8">1.96</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.5.9">0</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.5.10">0.98</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.5.11">0</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.5.12">0</td>
</tr>
<tr class="ltx_tr" id="A3.T10.1.6">
<td class="ltx_td ltx_align_center" id="A3.T10.1.6.1">ChatGLM3</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.6.2">61.33</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.6.3">13.33</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.6.4">9.33</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.6.5">1.33</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.6.6">5.33</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.6.7">5.33</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.6.8">0</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.6.9">1.33</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.6.10">1.33</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.6.11">0</td>
<td class="ltx_td ltx_align_center" id="A3.T10.1.6.12">1.33</td>
</tr>
<tr class="ltx_tr" id="A3.T10.1.7">
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.1.7.1">Baichuan2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.1.7.2">70.59</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.1.7.3">12.94</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.1.7.4">2.35</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.1.7.5">2.35</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.1.7.6">5.33</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.1.7.7">3.53</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.1.7.8">1.18</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.1.7.9">0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.1.7.10">1.33</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.1.7.11">0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.1.7.12">0</td>
</tr>
</table>
</figure>
<figure class="ltx_table" id="A3.T11">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 11. </span>Example of the Reading Comprehension Section in the National College Entrance Examination of China</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="A3.T11.1"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_left" id="A3.T11.2"><span class="ltx_text ltx_font_bold" id="A3.T11.2.1">Translated Reference:</span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_left" id="A3.T11.3">The golden age of blue-and-white porcelain development was during the Yongle and Xuande periods of the Ming Dynasty, coinciding with Zheng He’s voyages to the Western Seas, prompting us to ponder: Is it mere historical coincidence that both seafaring and porcelain craftsmanship reached their zenith at the same time? … It was the blending of Chinese and foreign civilizations that successfully drove the transformation of Chinese porcelain from monochrome to polychrome, with blue-and-white porcelain uniquely illustrating the cultural evolution of the Ming era, serving as an example of traditional society’s progression from uniformity to diversity. (Excerpted and compiled from ”The Trajectory of the Rise of Ming Dynasty Blue-and-White Porcelain” by Wan Ming)</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_left" id="A3.T11.4"><span class="ltx_text ltx_font_bold" id="A3.T11.4.1">Task: evaluate the correctness of the following sentences:
<br class="ltx_break"/></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_left" id="A3.T11.5"><span class="ltx_text ltx_font_bold" id="A3.T11.5.1">Translated Sentence 1:</span> Zheng He’s voyages to the Western Seas stimulated the production, sales, and technological innovation of porcelain, heralding the golden age of blue-and-white porcelain development.</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_left" id="A3.T11.6"><span class="ltx_text ltx_font_bold" id="A3.T11.6.1">Label:</span> <span class="ltx_text ltx_font_italic" id="A3.T11.6.2">correct
<br class="ltx_break"/></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_left" id="A3.T11.7"><span class="ltx_text ltx_font_bold" id="A3.T11.7.1">Translated Sentence 2:</span> Factors such as the localization of raw materials ushered the development of blue-and-white porcelain into a new phase, at which point its evolution became unrelated to foreign cultures.</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_left" id="A3.T11.8"><span class="ltx_text ltx_font_bold" id="A3.T11.8.1">Label:</span> <span class="ltx_text ltx_font_italic" id="A3.T11.8.2">Incorrect</span>.</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_left" id="A3.T11.9"><span class="ltx_text ltx_font_bold" id="A3.T11.9.1">Error Type:</span> Contradiction Error</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_left" id="A3.T11.10"><span class="ltx_text ltx_font_bold" id="A3.T11.10.1">Translated Sentence 3:</span> Ming Dynasty society is often considered conservative, yet the styles of blue-and-white porcelain indicate that the society was relatively open and progressive.</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_left" id="A3.T11.11"><span class="ltx_text ltx_font_bold" id="A3.T11.11.1">Label:</span> <span class="ltx_text ltx_font_italic" id="A3.T11.11.2">Incorrect</span>.</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_left" id="A3.T11.12"><span class="ltx_text ltx_font_bold" id="A3.T11.12.1">Error Type:</span> Conceptual Substitution Error</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_left" id="A3.T11.13"><span class="ltx_text ltx_font_bold" id="A3.T11.13.1">Translated Sentence 4:</span> The blending of Chinese and foreign civilizations promoted the transformation of porcelain from monochrome to polychrome, thereby driving the society of the time towards a more diverse transition.</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_left" id="A3.T11.14"><span class="ltx_text ltx_font_bold" id="A3.T11.14.1">Label:</span> <span class="ltx_text ltx_font_italic" id="A3.T11.14.2">Incorrect</span>.</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_left" id="A3.T11.15"><span class="ltx_text ltx_font_bold" id="A3.T11.15.1">Error Type:</span> Causal Confusion Error</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="A3.T11.16"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span></p>
</div>
</div>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Jul  3 12:45:49 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
