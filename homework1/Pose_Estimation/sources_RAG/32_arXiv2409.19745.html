<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead</title>
<!--Generated on Mon Oct  7 14:16:28 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.19745v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S1" title="In PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S2" title="In PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related works</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S2.SS1" title="In 2 Related works ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Context awareness enhancement</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S2.SS2" title="In 2 Related works ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Mechanistic interpretability</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S3" title="In PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Preliminaries: Discovery of influential attention heads</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S4" title="In PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S4.SS1" title="In 4 Methodology ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Discovery of RAG-suppression heads</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S4.SS1.SSS0.Px1" title="In 4.1 Discovery of RAG-suppression heads ‣ 4 Methodology ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_title">Task Input</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S4.SS1.SSS0.Px2" title="In 4.1 Discovery of RAG-suppression heads ‣ 4 Methodology ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_title">Head discovery</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S4.SS2" title="In 4 Methodology ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Re-weighting coefficient learning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S4.SS2.SSS0.Px1" title="In 4.2 Re-weighting coefficient learning ‣ 4 Methodology ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_title">Optimization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S4.SS2.SSS0.Px2" title="In 4.2 Re-weighting coefficient learning ‣ 4 Methodology ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_title">Inference on Downstream Tasks</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S5" title="In PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S5.SS1" title="In 5 Experiments ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S5.SS1.SSS0.Px1" title="In 5.1 Setup ‣ 5 Experiments ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_title">Models and baselines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S5.SS1.SSS0.Px2" title="In 5.1 Setup ‣ 5 Experiments ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_title">Detailed setups of proxy task</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S5.SS1.SSS0.Px3" title="In 5.1 Setup ‣ 5 Experiments ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_title">Hyperparameters for re-weighting coefficient learning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S5.SS1.SSS0.Px4" title="In 5.1 Setup ‣ 5 Experiments ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_title">Difference between learning and inference</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S5.SS2" title="In 5 Experiments ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Comparison with baselines on RAG tasks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S5.SS3" title="In 5 Experiments ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Applicability to LLMs using various position embeddings</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S5.SS4" title="In 5 Experiments ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span><span class="ltx_text ltx_font_italic">PEAR</span> does not diminish knowledge capabilities in LLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S5.SS5" title="In 5 Experiments ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Analysis: The effect of <math alttext="K" class="ltx_Math" display="inline"><semantics><mi>K</mi><annotation-xml encoding="MathML-Content"><ci>𝐾</ci></annotation-xml><annotation encoding="application/x-tex">K</annotation><annotation encoding="application/x-llamapun">italic_K</annotation></semantics></math></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S5.SS6" title="In 5 Experiments ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6 </span>Analysis: The Value of <math alttext="\tau" class="ltx_Math" display="inline"><semantics><mi>τ</mi><annotation-xml encoding="MathML-Content"><ci>𝜏</ci></annotation-xml><annotation encoding="application/x-tex">\tau</annotation><annotation encoding="application/x-llamapun">italic_τ</annotation></semantics></math></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S6" title="In PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#A1" title="In PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Head discovery results</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
<span class="ltx_text ltx_font_italic" id="id14.id1">PEAR</span>: <span class="ltx_text ltx_font_italic" id="id15.id2">P</span>osition-<span class="ltx_text ltx_font_italic" id="id16.id3">E</span>mbedding-<span class="ltx_text ltx_font_italic" id="id17.id4">A</span>gnostic Attention <span class="ltx_text ltx_font_italic" id="id18.id5">R</span>e-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tao Tan<sup class="ltx_sup" id="id19.14.id1"><span class="ltx_text ltx_font_italic" id="id19.14.id1.1">1</span></sup>  Yining Qian<sup class="ltx_sup" id="id20.15.id2"><span class="ltx_text ltx_font_italic" id="id20.15.id2.1">2∗</span></sup> Ang Lv<sup class="ltx_sup" id="id21.16.id3"><span class="ltx_text ltx_font_italic" id="id21.16.id3.1">1∗</span></sup> Hongzhan Lin<sup class="ltx_sup" id="id22.17.id4">1</sup> Songhao Wu<sup class="ltx_sup" id="id23.18.id5">1</sup>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id6.6.1">Yongbo Wang<sup class="ltx_sup" id="id6.6.1.1"><span class="ltx_text ltx_font_medium" id="id6.6.1.1.1">3</span></sup></span> <span class="ltx_text ltx_font_bold" id="id7.7.2">Feng Wang<sup class="ltx_sup" id="id7.7.2.1"><span class="ltx_text ltx_font_medium" id="id7.7.2.1.1">3</span></sup></span> <span class="ltx_text ltx_font_bold" id="id8.8.3">Jingtong Wu<sup class="ltx_sup" id="id8.8.3.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id8.8.3.1.1">3</span></sup></span> <span class="ltx_text ltx_font_bold" id="id9.9.4">Xin Lu<sup class="ltx_sup" id="id9.9.4.1"><span class="ltx_text ltx_font_medium" id="id9.9.4.1.1">3</span></sup></span> <span class="ltx_text ltx_font_bold" id="id11.11.6">Rui Yan<sup class="ltx_sup" id="id11.11.6.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id11.11.6.1.1">1</span></sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id11.11.6.2"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id11.11.6.2.1">1</span></sup></span>Gaoling School of Artificial Intelligence, Renmin University of China
<br class="ltx_break"/><sup class="ltx_sup" id="id24.19.id6">2</sup>Southeast University
<br class="ltx_break"/><sup class="ltx_sup" id="id25.20.id7">3</sup>Ant Group
<br class="ltx_break"/>
<br class="ltx_break"/><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/TTArch/PEAR-RAG" title="">https://github.com/TTArch/PEAR-RAG</a>
</span><span class="ltx_author_notes">Equal Contribution.Corresponding author: Rui Yan (ruiyan@ruc.edu.cn).</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id26.id1">Large language models (LLMs) enhanced with retrieval-augmented generation (RAG) have introduced a new paradigm for web search. However, the limited context awareness of LLMs degrades their performance on RAG tasks.
Existing methods to enhance context awareness are often inefficient, incurring time or memory overhead during inference, and many are tailored to specific position embeddings.
In this paper, we propose <span class="ltx_text ltx_font_bold" id="id26.id1.1">P</span>osition-<span class="ltx_text ltx_font_bold" id="id26.id1.2">E</span>mbedding-<span class="ltx_text ltx_font_bold" id="id26.id1.3">A</span>gnostic attention <span class="ltx_text ltx_font_bold" id="id26.id1.4">R</span>e-weighting (<span class="ltx_text ltx_font_italic" id="id26.id1.5">PEAR</span>), which enhances the context awareness of LLMs with zero inference overhead.
Specifically, on a proxy task focused on context copying, we first detect heads which suppress the models’ context awareness, thereby diminishing RAG performance.
To weaken the impact of these heads, we re-weight their outputs with learnable coefficients.
The LLM (with frozen parameters) is optimized by adjusting these coefficients to minimize loss on the proxy task.
During inference, the optimized coefficients are fixed to re-weight these heads, regardless of the specific task at hand.
Our proposed <span class="ltx_text ltx_font_italic" id="id26.id1.6">PEAR</span> offers two major advantages over previous approaches:
(1) It introduces zero additional inference overhead in terms of memory usage or inference time, while outperforming competitive baselines in accuracy and efficiency across various RAG tasks.
(2) It is independent of position embedding algorithms, ensuring broader applicability.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Retrieval-augmented generation (RAG, <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib11" title="">2021</a>)</cite>) is widely utilized to enhance large language models (LLMs) on tasks like question answering.
Typically, an RAG framework retrieves documents related to users’ questions from external knowledge bases or web pages, and then arranges them in the LLMs’ context as the references to form answers.
This LLM-based question-answering paradigm has given rise to a promising web search paradigm <cite class="ltx_cite ltx_citemacro_citep">(Microsoft, <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib19" title="">2023</a>; OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib22" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Recent research demonstrated LLMs’ limitations on context awareness, especially when processing long context.
These limitations in LLMs’ context awareness challenge the effectiveness and robustness of RAG frameworks.
For instance, Liu et al. <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib13" title="">2023</a>)</cite> found that when performing in-context retrieval tasks, LLMs exhibit insensitivity to information located in the middle of the context, a phenomenon referred to as “lost in the middle.”
Chen et al. <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib2" title="">2024</a>)</cite> identified a mathematical property in rotary position embedding (RoPE, <cite class="ltx_cite ltx_citemacro_citep">(Su et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib27" title="">2023</a>)</cite>) that results in LLMs assigning less attention to specific contextual positions, leading to varying context awareness throughout the entire context.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Existing approaches to enhancing LLMs’ context awareness are inefficient in terms of memory and time cost.
Some works <cite class="ltx_cite ltx_citemacro_citep">(Peysakhovich &amp; Lerer, <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib23" title="">2023</a>)</cite> segment and re-arrange the input context, with the assumption that placing important information in positions the model attends well can improve RAG’s effectiveness.
This method incurs additional inference time costs, negatively affecting user experience, as it requires multiple forward passes to obtain attention weights for guiding segment rearrangement.
Another group of studies modifies the model’s working mechanism, specifically by employing a set of position embeddings to adjust the attention preferences of attention heads.
While these methods are input-agnostic, they also lack efficiency due to “parallelable” forward passes <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib2" title="">2024</a>)</cite> with slightly increased time cost in additional aggregation operation, or disrupt the parallelism of multi-head attention (resulting in increased time cost) to achieve low memory cost <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib12" title="">2024</a>)</cite>, alternatively, still requiring “non-parallelable” multiple forward passes <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib35" title="">2024</a>)</cite> which is similar to <cite class="ltx_cite ltx_citemacro_citep">(Peysakhovich &amp; Lerer, <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib23" title="">2023</a>)</cite>.
Moreover, these studies are mainly designed for RoPE and face challenges in generalizing to other position embedding algorithms, limiting their broader applications.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this paper, we introduce <span class="ltx_text ltx_font_bold" id="S1.p4.1.1">P</span>osition-<span class="ltx_text ltx_font_bold" id="S1.p4.1.2">E</span>mbedding-<span class="ltx_text ltx_font_bold" id="S1.p4.1.3">A</span>gnostic attention head <span class="ltx_text ltx_font_bold" id="S1.p4.1.4">R</span>e-weighting (PEAR), which unleashes the context awareness of LLMs, thereby improving their RAG performance.
<span class="ltx_text ltx_font_italic" id="S1.p4.1.5">PEAR</span> achieves zero additional overhead in memory usage and inference time.
Our motivation relies on the following facts:</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">Prior research <cite class="ltx_cite ltx_citemacro_citep">(McDougall et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib17" title="">2023</a>; Lv et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib15" title="">2024a</a>)</cite> detected some attention heads decreasing the language model’s prediction confidence by suppressing the flow of contextual information to the final position within the context, where the output is to be generated.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para ltx_noindent" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">This suppression negatively impacts LLMs’ context awareness, particularly abilities in in-context retrieval and context integration, which are crucial for effective RAG.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para ltx_noindent" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">As a result, we contend that such suppression mechanism in LLMs can be safely<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>“Safely” means that the parametric knowledge and fundamental capabilities remain unaffected.
Detailed experimental results are presented in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S5.SS4" title="5.4 PEAR does not diminish knowledge capabilities in LLMs ‣ 5 Experiments ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_tag">5.4</span></a>.</span></span></span> weakened in RAG scenarios, thereby improving the RAG performance of LLMs.
Our proposed <span class="ltx_text ltx_font_italic" id="S1.p6.1.1">PEAR</span> includes two stages:</p>
</div>
<figure class="ltx_table" id="S1.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>An example input for the proxy task, where unique letters representing distinct tokens, with <math alttext="n=4" class="ltx_Math" display="inline" id="S1.T1.6.m1.1"><semantics id="S1.T1.6.m1.1b"><mrow id="S1.T1.6.m1.1.1" xref="S1.T1.6.m1.1.1.cmml"><mi id="S1.T1.6.m1.1.1.2" xref="S1.T1.6.m1.1.1.2.cmml">n</mi><mo id="S1.T1.6.m1.1.1.1" xref="S1.T1.6.m1.1.1.1.cmml">=</mo><mn id="S1.T1.6.m1.1.1.3" xref="S1.T1.6.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.T1.6.m1.1c"><apply id="S1.T1.6.m1.1.1.cmml" xref="S1.T1.6.m1.1.1"><eq id="S1.T1.6.m1.1.1.1.cmml" xref="S1.T1.6.m1.1.1.1"></eq><ci id="S1.T1.6.m1.1.1.2.cmml" xref="S1.T1.6.m1.1.1.2">𝑛</ci><cn id="S1.T1.6.m1.1.1.3.cmml" type="integer" xref="S1.T1.6.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.6.m1.1d">n=4</annotation><annotation encoding="application/x-llamapun" id="S1.T1.6.m1.1e">italic_n = 4</annotation></semantics></math>.
For example, at position <math alttext="n+i=5" class="ltx_Math" display="inline" id="S1.T1.7.m2.1"><semantics id="S1.T1.7.m2.1b"><mrow id="S1.T1.7.m2.1.1" xref="S1.T1.7.m2.1.1.cmml"><mrow id="S1.T1.7.m2.1.1.2" xref="S1.T1.7.m2.1.1.2.cmml"><mi id="S1.T1.7.m2.1.1.2.2" xref="S1.T1.7.m2.1.1.2.2.cmml">n</mi><mo id="S1.T1.7.m2.1.1.2.1" xref="S1.T1.7.m2.1.1.2.1.cmml">+</mo><mi id="S1.T1.7.m2.1.1.2.3" xref="S1.T1.7.m2.1.1.2.3.cmml">i</mi></mrow><mo id="S1.T1.7.m2.1.1.1" xref="S1.T1.7.m2.1.1.1.cmml">=</mo><mn id="S1.T1.7.m2.1.1.3" xref="S1.T1.7.m2.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.T1.7.m2.1c"><apply id="S1.T1.7.m2.1.1.cmml" xref="S1.T1.7.m2.1.1"><eq id="S1.T1.7.m2.1.1.1.cmml" xref="S1.T1.7.m2.1.1.1"></eq><apply id="S1.T1.7.m2.1.1.2.cmml" xref="S1.T1.7.m2.1.1.2"><plus id="S1.T1.7.m2.1.1.2.1.cmml" xref="S1.T1.7.m2.1.1.2.1"></plus><ci id="S1.T1.7.m2.1.1.2.2.cmml" xref="S1.T1.7.m2.1.1.2.2">𝑛</ci><ci id="S1.T1.7.m2.1.1.2.3.cmml" xref="S1.T1.7.m2.1.1.2.3">𝑖</ci></apply><cn id="S1.T1.7.m2.1.1.3.cmml" type="integer" xref="S1.T1.7.m2.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.7.m2.1d">n+i=5</annotation><annotation encoding="application/x-llamapun" id="S1.T1.7.m2.1e">italic_n + italic_i = 5</annotation></semantics></math> (with <math alttext="i=1" class="ltx_Math" display="inline" id="S1.T1.8.m3.1"><semantics id="S1.T1.8.m3.1b"><mrow id="S1.T1.8.m3.1.1" xref="S1.T1.8.m3.1.1.cmml"><mi id="S1.T1.8.m3.1.1.2" xref="S1.T1.8.m3.1.1.2.cmml">i</mi><mo id="S1.T1.8.m3.1.1.1" xref="S1.T1.8.m3.1.1.1.cmml">=</mo><mn id="S1.T1.8.m3.1.1.3" xref="S1.T1.8.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.T1.8.m3.1c"><apply id="S1.T1.8.m3.1.1.cmml" xref="S1.T1.8.m3.1.1"><eq id="S1.T1.8.m3.1.1.1.cmml" xref="S1.T1.8.m3.1.1.1"></eq><ci id="S1.T1.8.m3.1.1.2.cmml" xref="S1.T1.8.m3.1.1.2">𝑖</ci><cn id="S1.T1.8.m3.1.1.3.cmml" type="integer" xref="S1.T1.8.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.8.m3.1d">i=1</annotation><annotation encoding="application/x-llamapun" id="S1.T1.8.m3.1e">italic_i = 1</annotation></semantics></math>), when an LLM receives “ABCDA” as input, it is likely to output “B.”
This happens because the last occurrence of “A” in the preceding context is followed by “B.”
If a head suppresses copying “B” from position <math alttext="i+1=2" class="ltx_Math" display="inline" id="S1.T1.9.m4.1"><semantics id="S1.T1.9.m4.1b"><mrow id="S1.T1.9.m4.1.1" xref="S1.T1.9.m4.1.1.cmml"><mrow id="S1.T1.9.m4.1.1.2" xref="S1.T1.9.m4.1.1.2.cmml"><mi id="S1.T1.9.m4.1.1.2.2" xref="S1.T1.9.m4.1.1.2.2.cmml">i</mi><mo id="S1.T1.9.m4.1.1.2.1" xref="S1.T1.9.m4.1.1.2.1.cmml">+</mo><mn id="S1.T1.9.m4.1.1.2.3" xref="S1.T1.9.m4.1.1.2.3.cmml">1</mn></mrow><mo id="S1.T1.9.m4.1.1.1" xref="S1.T1.9.m4.1.1.1.cmml">=</mo><mn id="S1.T1.9.m4.1.1.3" xref="S1.T1.9.m4.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.T1.9.m4.1c"><apply id="S1.T1.9.m4.1.1.cmml" xref="S1.T1.9.m4.1.1"><eq id="S1.T1.9.m4.1.1.1.cmml" xref="S1.T1.9.m4.1.1.1"></eq><apply id="S1.T1.9.m4.1.1.2.cmml" xref="S1.T1.9.m4.1.1.2"><plus id="S1.T1.9.m4.1.1.2.1.cmml" xref="S1.T1.9.m4.1.1.2.1"></plus><ci id="S1.T1.9.m4.1.1.2.2.cmml" xref="S1.T1.9.m4.1.1.2.2">𝑖</ci><cn id="S1.T1.9.m4.1.1.2.3.cmml" type="integer" xref="S1.T1.9.m4.1.1.2.3">1</cn></apply><cn id="S1.T1.9.m4.1.1.3.cmml" type="integer" xref="S1.T1.9.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.9.m4.1d">i+1=2</annotation><annotation encoding="application/x-llamapun" id="S1.T1.9.m4.1e">italic_i + 1 = 2</annotation></semantics></math> to position <math alttext="n+i=5" class="ltx_Math" display="inline" id="S1.T1.10.m5.1"><semantics id="S1.T1.10.m5.1b"><mrow id="S1.T1.10.m5.1.1" xref="S1.T1.10.m5.1.1.cmml"><mrow id="S1.T1.10.m5.1.1.2" xref="S1.T1.10.m5.1.1.2.cmml"><mi id="S1.T1.10.m5.1.1.2.2" xref="S1.T1.10.m5.1.1.2.2.cmml">n</mi><mo id="S1.T1.10.m5.1.1.2.1" xref="S1.T1.10.m5.1.1.2.1.cmml">+</mo><mi id="S1.T1.10.m5.1.1.2.3" xref="S1.T1.10.m5.1.1.2.3.cmml">i</mi></mrow><mo id="S1.T1.10.m5.1.1.1" xref="S1.T1.10.m5.1.1.1.cmml">=</mo><mn id="S1.T1.10.m5.1.1.3" xref="S1.T1.10.m5.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.T1.10.m5.1c"><apply id="S1.T1.10.m5.1.1.cmml" xref="S1.T1.10.m5.1.1"><eq id="S1.T1.10.m5.1.1.1.cmml" xref="S1.T1.10.m5.1.1.1"></eq><apply id="S1.T1.10.m5.1.1.2.cmml" xref="S1.T1.10.m5.1.1.2"><plus id="S1.T1.10.m5.1.1.2.1.cmml" xref="S1.T1.10.m5.1.1.2.1"></plus><ci id="S1.T1.10.m5.1.1.2.2.cmml" xref="S1.T1.10.m5.1.1.2.2">𝑛</ci><ci id="S1.T1.10.m5.1.1.2.3.cmml" xref="S1.T1.10.m5.1.1.2.3">𝑖</ci></apply><cn id="S1.T1.10.m5.1.1.3.cmml" type="integer" xref="S1.T1.10.m5.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.10.m5.1d">n+i=5</annotation><annotation encoding="application/x-llamapun" id="S1.T1.10.m5.1e">italic_n + italic_i = 5</annotation></semantics></math>, it could negatively impact RAG performance.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S1.T1.11">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S1.T1.11.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S1.T1.11.1.1.1">Input Sequence</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S1.T1.11.1.1.2">A</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S1.T1.11.1.1.3">B</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S1.T1.11.1.1.4">C</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S1.T1.11.1.1.5">D</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S1.T1.11.1.1.6">A</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S1.T1.11.1.1.7">B</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S1.T1.11.1.1.8">C</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S1.T1.11.1.1.9">D</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S1.T1.11.2.1">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="S1.T1.11.2.1.1">Position Index</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S1.T1.11.2.1.2">1</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S1.T1.11.2.1.3">2</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S1.T1.11.2.1.4">3</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S1.T1.11.2.1.5">4</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S1.T1.11.2.1.6">5</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S1.T1.11.2.1.7">6</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S1.T1.11.2.1.8">7</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S1.T1.11.2.1.9">8</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="S1.p7">
<p class="ltx_p" id="S1.p7.4">In the first stage, we discover attention heads negatively affect performance on a proxy task.
The proxy task involves feeding the model a random token sequence of length <math alttext="2n" class="ltx_Math" display="inline" id="S1.p7.1.m1.1"><semantics id="S1.p7.1.m1.1a"><mrow id="S1.p7.1.m1.1.1" xref="S1.p7.1.m1.1.1.cmml"><mn id="S1.p7.1.m1.1.1.2" xref="S1.p7.1.m1.1.1.2.cmml">2</mn><mo id="S1.p7.1.m1.1.1.1" xref="S1.p7.1.m1.1.1.1.cmml">⁢</mo><mi id="S1.p7.1.m1.1.1.3" xref="S1.p7.1.m1.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.p7.1.m1.1b"><apply id="S1.p7.1.m1.1.1.cmml" xref="S1.p7.1.m1.1.1"><times id="S1.p7.1.m1.1.1.1.cmml" xref="S1.p7.1.m1.1.1.1"></times><cn id="S1.p7.1.m1.1.1.2.cmml" type="integer" xref="S1.p7.1.m1.1.1.2">2</cn><ci id="S1.p7.1.m1.1.1.3.cmml" xref="S1.p7.1.m1.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p7.1.m1.1c">2n</annotation><annotation encoding="application/x-llamapun" id="S1.p7.1.m1.1d">2 italic_n</annotation></semantics></math>, which consists of a duplicated sub-sequence of length <math alttext="n" class="ltx_Math" display="inline" id="S1.p7.2.m2.1"><semantics id="S1.p7.2.m2.1a"><mi id="S1.p7.2.m2.1.1" xref="S1.p7.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S1.p7.2.m2.1b"><ci id="S1.p7.2.m2.1.1.cmml" xref="S1.p7.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p7.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S1.p7.2.m2.1d">italic_n</annotation></semantics></math>. Table <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S1.T1" title="Table 1 ‣ 1 Introduction ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates an example input.
At position <math alttext="n+i" class="ltx_Math" display="inline" id="S1.p7.3.m3.1"><semantics id="S1.p7.3.m3.1a"><mrow id="S1.p7.3.m3.1.1" xref="S1.p7.3.m3.1.1.cmml"><mi id="S1.p7.3.m3.1.1.2" xref="S1.p7.3.m3.1.1.2.cmml">n</mi><mo id="S1.p7.3.m3.1.1.1" xref="S1.p7.3.m3.1.1.1.cmml">+</mo><mi id="S1.p7.3.m3.1.1.3" xref="S1.p7.3.m3.1.1.3.cmml">i</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.p7.3.m3.1b"><apply id="S1.p7.3.m3.1.1.cmml" xref="S1.p7.3.m3.1.1"><plus id="S1.p7.3.m3.1.1.1.cmml" xref="S1.p7.3.m3.1.1.1"></plus><ci id="S1.p7.3.m3.1.1.2.cmml" xref="S1.p7.3.m3.1.1.2">𝑛</ci><ci id="S1.p7.3.m3.1.1.3.cmml" xref="S1.p7.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p7.3.m3.1c">n+i</annotation><annotation encoding="application/x-llamapun" id="S1.p7.3.m3.1d">italic_n + italic_i</annotation></semantics></math>, the model typically predicts the token from position <math alttext="i+1" class="ltx_Math" display="inline" id="S1.p7.4.m4.1"><semantics id="S1.p7.4.m4.1a"><mrow id="S1.p7.4.m4.1.1" xref="S1.p7.4.m4.1.1.cmml"><mi id="S1.p7.4.m4.1.1.2" xref="S1.p7.4.m4.1.1.2.cmml">i</mi><mo id="S1.p7.4.m4.1.1.1" xref="S1.p7.4.m4.1.1.1.cmml">+</mo><mn id="S1.p7.4.m4.1.1.3" xref="S1.p7.4.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.p7.4.m4.1b"><apply id="S1.p7.4.m4.1.1.cmml" xref="S1.p7.4.m4.1.1"><plus id="S1.p7.4.m4.1.1.1.cmml" xref="S1.p7.4.m4.1.1.1"></plus><ci id="S1.p7.4.m4.1.1.2.cmml" xref="S1.p7.4.m4.1.1.2">𝑖</ci><cn id="S1.p7.4.m4.1.1.3.cmml" type="integer" xref="S1.p7.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p7.4.m4.1c">i+1</annotation><annotation encoding="application/x-llamapun" id="S1.p7.4.m4.1d">italic_i + 1</annotation></semantics></math>, as the natural continuation for a semantically meaningless context is to copy the existing in-context token pattern <cite class="ltx_cite ltx_citemacro_citep">(Lv et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib16" title="">2024b</a>)</cite>.
The negatively impactful attention heads are discovered using the path patching technique <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib31" title="">2023</a>)</cite>.
Since this proxy task is free from semantic bias and requires both in-context retrieval and generation based on the context—fundamental capacities for RAG, we refer to discovered heads as RAG-suppression heads<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>We do not imply that these heads hinder RAG through the same mechanisms (discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S4.SS1" title="4.1 Discovery of RAG-suppression heads ‣ 4 Methodology ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_tag">4.1</span></a>).</span></span></span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">In the second stage, we weaken detected RAG-suppression heads by re-weighting their outputs using learnable coefficients.
These coefficients are optimized by minimizing the LLM’s loss on the proxy task, with the objective of next-token prediction in a supervised fine-tuning process (loss is computed only for the second half of the random-token sequence).
During the optimization, the original LLM parameters are frozen.
Intuitively, most of the learned coefficients are optimized to values less than one, reducing the relative weight of these heads compared to others in the same layer when multi-head outputs are aggregated.
Consequently, their influence during the forward pass is weakened.
Once optimized, the coefficients remain fixed and are agnostic to downstream RAG tasks.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p9">
<p class="ltx_p" id="S1.p9.1"><span class="ltx_text ltx_font_italic" id="S1.p9.1.1">PEAR</span> achieves two-fold contributions:</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p10">
<ol class="ltx_enumerate" id="S1.I2">
<li class="ltx_item" id="S1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I2.i1.p1">
<p class="ltx_p" id="S1.I2.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S1.I2.i1.p1.1.1">PEAR</span> introduces zero inference overhead in terms of both memory usage and inference time—an advantage not achieved by competitive baselines.
Across a wide range of RAG tasks, <span class="ltx_text ltx_font_italic" id="S1.I2.i1.p1.1.2">PEAR</span> surpasses previous works in both efficiency and accuracy.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para ltx_noindent" id="S1.I2.i2.p1">
<p class="ltx_p" id="S1.I2.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S1.I2.i2.p1.1.1">PEAR</span> is independent of specific position embedding algorithms, making it broadly applicable.
We demonstrate that <span class="ltx_text ltx_font_italic" id="S1.I2.i2.p1.1.2">PEAR</span> enhances the RAG performance of various LLMs using distinct position embeddings (e.g., learnable embeddings <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib34" title="">2022</a>)</cite>, RoPE <cite class="ltx_cite ltx_citemacro_citep">(Su et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib27" title="">2023</a>)</cite>, and Alibi <cite class="ltx_cite ltx_citemacro_citep">(Press et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib24" title="">2022</a>)</cite>).</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related works</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In this section, we discuss two research areas closely related to this paper: enhancements to LLMs’ context awareness and studies on mechanistic interpretability.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Context awareness enhancement</h3>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Many studies highlighted limitations in LLMs’ context awareness.
For example, Lu et al. <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib14" title="">2022</a>)</cite> found that the order of in-context learning (ICL) demonstrations significantly affects ICL accuracy.
Liu et al. <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib13" title="">2023</a>)</cite> demonstrated that LLMs exhibit stronger awareness of content at the beginning and end of context but weaker awareness in the middle, a phenomenon termed “lost in the middle.”
Chen et al. <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib2" title="">2024</a>)</cite> proposed that LLMs’ context awareness fluctuates across token positions due to mathematical properties in rotary position embedding.
These challenges impact applications like RAG that rely on robust context awareness.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Several approaches have been proposed to tackle these issues.
However, existing methods often come at the cost of increased inference time or memory overhead.
Attention Buckets (AB, <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib2" title="">2024</a>)</cite>) enhances context awareness by integrating positional information from a set of various RoPE angles, but it incurs significant inference overhead due to multiple parallel forward passes, leading to increased memory usage.
Ms-PoE <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib35" title="">2024</a>)</cite> calculates distinct re-scaling factors for each attention head, requiring multiple non-parallel forward passes that introduce noticeable time delays. MoICE <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib12" title="">2024</a>)</cite> builds on Attention Buckets by employing a Mixture-of-Experts (MoE, <cite class="ltx_cite ltx_citemacro_citep">(Shazeer et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib26" title="">2017</a>)</cite>) approach, treating each RoPE embedding as a unique in-context expert, thereby limiting extra attention computations to within each layer rather than across the entire forward pass.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">Our proposed method, <span class="ltx_text ltx_font_italic" id="S2.SS1.p3.1.1">PEAR</span>, enhances context awareness by weakening the RAG-suppression heads during the forward pass. It introduces no additional modules or extra forward passes, resulting in <span class="ltx_text ltx_font_italic" id="S2.SS1.p3.1.2">zero</span> additional overhead in memory usage and inference time.
Additionally, <span class="ltx_text ltx_font_italic" id="S2.SS1.p3.1.3">PEAR</span> operates independently of position embedding algorithms, making it applicable to more LLMs compared to existing approaches.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Mechanistic interpretability</h3>
<div class="ltx_para ltx_noindent" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Investigating the role of a specific head during a forward pass is one of key focuses in mechanistic interpretability research.
Wang et al. <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib31" title="">2023</a>)</cite> reported that in GPT-2 small <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib25" title="">2019</a>)</cite>, the 7th head in the 10th layer, termed the “negative head,” significantly hinders answer copying from context.
McDougall et al. <cite class="ltx_cite ltx_citemacro_citep">(McDougall et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib17" title="">2023</a>)</cite> comprehensively studied this head, suggesting it functions as a self-repair mechanism to prevent overconfident outputs.
Lv et al. <cite class="ltx_cite ltx_citemacro_citep">(Lv et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib15" title="">2024a</a>)</cite> found that negative heads exist across various LLMs, employing different mechanisms to mitigate overconfidence, such as generating counteracting vectors or introducing high-frequency tokens’ information.
This paper does not examine what specific mechanisms the heads employ to suppress RAG performance but instead aims to discover and suppress heads negatively impactful across general RAG tasks.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Yu et al. <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib32" title="">2023</a>)</cite> detected two types of heads in Transformer-based language models during counter-factual task execution (where counter-factual knowledge is provided in the context): memory heads, which prefer to use stored knowledge, and in-context heads, which prefer to use facts in the context.
However, when re-weighting these heads, only reducing the weight of memory heads successfully enhances the model’s preference of using contextual knowledge, while enhancing or mitigating in-context heads does not bring much influence.
Moreover, existing results only demonstrate effectiveness of these heads in the “country-capital” task (i.e., prompting the model to answer the capital city of the given country).
Additionally, there is no evidence suggesting that re-weighting these heads improves the comprehensive context awareness of LLMs.
We owe these failures to their head detection method, and the same re-weighting value applied for all the heads of the same type.
By contrast, in our proposed <span class="ltx_text ltx_font_italic" id="S2.SS2.p2.1.1">PEAR</span>, each individual head is re-weighted by a specific learnable coefficient, which is optimized through a proxy task independent of downstream tasks.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Preliminaries: Discovery of influential attention heads</h2>
<figure class="ltx_figure ltx_align_floatright" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="662" id="S3.F1.1.g1" src="x1.png" width="831"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An illustration of causal mediation methods for circuit discovery.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">For a particular task, research has shown that only a sparse sub-network is activated during the forward pass in Transformer language models <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib31" title="">2023</a>; Merullo et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib18" title="">2024</a>; Gong et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib6" title="">2024</a>)</cite>.
Such a sub-network is referred to as a circuit <cite class="ltx_cite ltx_citemacro_citep">(Olah et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib20" title="">2020</a>)</cite>.
Discovering circuits provides interpretability into the working mechanisms of language models and offers insights for model enhancement.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">The primary method for circuit discovery is based on causal mediation analysis.
The core idea is to view the forward computation graph as a causal graph, where the output of one module serves as the input for the next.
In such a case, if the output of a module is changed, the computation of subsequent modules in the causal graph is also affected, as their inputs change.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p3">
<p class="ltx_p" id="S3.p3.11">In this paper, we primarily focus on analyzing the working mechanism of attention heads in language models.
We briefly introduce a paradigm from a series of works <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib31" title="">2023</a>; Zhang &amp; Nanda, <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib33" title="">2024</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib30" title="">2024</a>)</cite> that discovers which attention heads are crucial for processing an input sequence <math alttext="X" class="ltx_Math" display="inline" id="S3.p3.1.m1.1"><semantics id="S3.p3.1.m1.1a"><mi id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><ci id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">X</annotation><annotation encoding="application/x-llamapun" id="S3.p3.1.m1.1d">italic_X</annotation></semantics></math> of length <math alttext="n" class="ltx_Math" display="inline" id="S3.p3.2.m2.1"><semantics id="S3.p3.2.m2.1a"><mi id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><ci id="S3.p3.2.m2.1.1.cmml" xref="S3.p3.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.p3.2.m2.1d">italic_n</annotation></semantics></math>.
Suppose the language model consists of <math alttext="L" class="ltx_Math" display="inline" id="S3.p3.3.m3.1"><semantics id="S3.p3.3.m3.1a"><mi id="S3.p3.3.m3.1.1" xref="S3.p3.3.m3.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.p3.3.m3.1b"><ci id="S3.p3.3.m3.1.1.cmml" xref="S3.p3.3.m3.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.3.m3.1c">L</annotation><annotation encoding="application/x-llamapun" id="S3.p3.3.m3.1d">italic_L</annotation></semantics></math> layers, with <math alttext="H" class="ltx_Math" display="inline" id="S3.p3.4.m4.1"><semantics id="S3.p3.4.m4.1a"><mi id="S3.p3.4.m4.1.1" xref="S3.p3.4.m4.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S3.p3.4.m4.1b"><ci id="S3.p3.4.m4.1.1.cmml" xref="S3.p3.4.m4.1.1">𝐻</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.4.m4.1c">H</annotation><annotation encoding="application/x-llamapun" id="S3.p3.4.m4.1d">italic_H</annotation></semantics></math> attention heads per layer.
Let <math alttext="A^{(l,h)}" class="ltx_Math" display="inline" id="S3.p3.5.m5.2"><semantics id="S3.p3.5.m5.2a"><msup id="S3.p3.5.m5.2.3" xref="S3.p3.5.m5.2.3.cmml"><mi id="S3.p3.5.m5.2.3.2" xref="S3.p3.5.m5.2.3.2.cmml">A</mi><mrow id="S3.p3.5.m5.2.2.2.4" xref="S3.p3.5.m5.2.2.2.3.cmml"><mo id="S3.p3.5.m5.2.2.2.4.1" stretchy="false" xref="S3.p3.5.m5.2.2.2.3.cmml">(</mo><mi id="S3.p3.5.m5.1.1.1.1" xref="S3.p3.5.m5.1.1.1.1.cmml">l</mi><mo id="S3.p3.5.m5.2.2.2.4.2" xref="S3.p3.5.m5.2.2.2.3.cmml">,</mo><mi id="S3.p3.5.m5.2.2.2.2" xref="S3.p3.5.m5.2.2.2.2.cmml">h</mi><mo id="S3.p3.5.m5.2.2.2.4.3" stretchy="false" xref="S3.p3.5.m5.2.2.2.3.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.p3.5.m5.2b"><apply id="S3.p3.5.m5.2.3.cmml" xref="S3.p3.5.m5.2.3"><csymbol cd="ambiguous" id="S3.p3.5.m5.2.3.1.cmml" xref="S3.p3.5.m5.2.3">superscript</csymbol><ci id="S3.p3.5.m5.2.3.2.cmml" xref="S3.p3.5.m5.2.3.2">𝐴</ci><interval closure="open" id="S3.p3.5.m5.2.2.2.3.cmml" xref="S3.p3.5.m5.2.2.2.4"><ci id="S3.p3.5.m5.1.1.1.1.cmml" xref="S3.p3.5.m5.1.1.1.1">𝑙</ci><ci id="S3.p3.5.m5.2.2.2.2.cmml" xref="S3.p3.5.m5.2.2.2.2">ℎ</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.5.m5.2c">A^{(l,h)}</annotation><annotation encoding="application/x-llamapun" id="S3.p3.5.m5.2d">italic_A start_POSTSUPERSCRIPT ( italic_l , italic_h ) end_POSTSUPERSCRIPT</annotation></semantics></math> denote the <math alttext="h" class="ltx_Math" display="inline" id="S3.p3.6.m6.1"><semantics id="S3.p3.6.m6.1a"><mi id="S3.p3.6.m6.1.1" xref="S3.p3.6.m6.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.p3.6.m6.1b"><ci id="S3.p3.6.m6.1.1.cmml" xref="S3.p3.6.m6.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.6.m6.1c">h</annotation><annotation encoding="application/x-llamapun" id="S3.p3.6.m6.1d">italic_h</annotation></semantics></math>-th attention head in the <math alttext="l" class="ltx_Math" display="inline" id="S3.p3.7.m7.1"><semantics id="S3.p3.7.m7.1a"><mi id="S3.p3.7.m7.1.1" xref="S3.p3.7.m7.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.p3.7.m7.1b"><ci id="S3.p3.7.m7.1.1.cmml" xref="S3.p3.7.m7.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.7.m7.1c">l</annotation><annotation encoding="application/x-llamapun" id="S3.p3.7.m7.1d">italic_l</annotation></semantics></math>-th layer, and let its outputs be denoted by <math alttext="\textbf{a}^{(l,h)}\in\mathbb{R}^{n\times d}" class="ltx_Math" display="inline" id="S3.p3.8.m8.2"><semantics id="S3.p3.8.m8.2a"><mrow id="S3.p3.8.m8.2.3" xref="S3.p3.8.m8.2.3.cmml"><msup id="S3.p3.8.m8.2.3.2" xref="S3.p3.8.m8.2.3.2.cmml"><mtext class="ltx_mathvariant_bold" id="S3.p3.8.m8.2.3.2.2" xref="S3.p3.8.m8.2.3.2.2a.cmml">a</mtext><mrow id="S3.p3.8.m8.2.2.2.4" xref="S3.p3.8.m8.2.2.2.3.cmml"><mo id="S3.p3.8.m8.2.2.2.4.1" stretchy="false" xref="S3.p3.8.m8.2.2.2.3.cmml">(</mo><mi id="S3.p3.8.m8.1.1.1.1" xref="S3.p3.8.m8.1.1.1.1.cmml">l</mi><mo id="S3.p3.8.m8.2.2.2.4.2" xref="S3.p3.8.m8.2.2.2.3.cmml">,</mo><mi id="S3.p3.8.m8.2.2.2.2" xref="S3.p3.8.m8.2.2.2.2.cmml">h</mi><mo id="S3.p3.8.m8.2.2.2.4.3" stretchy="false" xref="S3.p3.8.m8.2.2.2.3.cmml">)</mo></mrow></msup><mo id="S3.p3.8.m8.2.3.1" xref="S3.p3.8.m8.2.3.1.cmml">∈</mo><msup id="S3.p3.8.m8.2.3.3" xref="S3.p3.8.m8.2.3.3.cmml"><mi id="S3.p3.8.m8.2.3.3.2" xref="S3.p3.8.m8.2.3.3.2.cmml">ℝ</mi><mrow id="S3.p3.8.m8.2.3.3.3" xref="S3.p3.8.m8.2.3.3.3.cmml"><mi id="S3.p3.8.m8.2.3.3.3.2" xref="S3.p3.8.m8.2.3.3.3.2.cmml">n</mi><mo id="S3.p3.8.m8.2.3.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.p3.8.m8.2.3.3.3.1.cmml">×</mo><mi id="S3.p3.8.m8.2.3.3.3.3" xref="S3.p3.8.m8.2.3.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.8.m8.2b"><apply id="S3.p3.8.m8.2.3.cmml" xref="S3.p3.8.m8.2.3"><in id="S3.p3.8.m8.2.3.1.cmml" xref="S3.p3.8.m8.2.3.1"></in><apply id="S3.p3.8.m8.2.3.2.cmml" xref="S3.p3.8.m8.2.3.2"><csymbol cd="ambiguous" id="S3.p3.8.m8.2.3.2.1.cmml" xref="S3.p3.8.m8.2.3.2">superscript</csymbol><ci id="S3.p3.8.m8.2.3.2.2a.cmml" xref="S3.p3.8.m8.2.3.2.2"><mtext class="ltx_mathvariant_bold" id="S3.p3.8.m8.2.3.2.2.cmml" xref="S3.p3.8.m8.2.3.2.2">a</mtext></ci><interval closure="open" id="S3.p3.8.m8.2.2.2.3.cmml" xref="S3.p3.8.m8.2.2.2.4"><ci id="S3.p3.8.m8.1.1.1.1.cmml" xref="S3.p3.8.m8.1.1.1.1">𝑙</ci><ci id="S3.p3.8.m8.2.2.2.2.cmml" xref="S3.p3.8.m8.2.2.2.2">ℎ</ci></interval></apply><apply id="S3.p3.8.m8.2.3.3.cmml" xref="S3.p3.8.m8.2.3.3"><csymbol cd="ambiguous" id="S3.p3.8.m8.2.3.3.1.cmml" xref="S3.p3.8.m8.2.3.3">superscript</csymbol><ci id="S3.p3.8.m8.2.3.3.2.cmml" xref="S3.p3.8.m8.2.3.3.2">ℝ</ci><apply id="S3.p3.8.m8.2.3.3.3.cmml" xref="S3.p3.8.m8.2.3.3.3"><times id="S3.p3.8.m8.2.3.3.3.1.cmml" xref="S3.p3.8.m8.2.3.3.3.1"></times><ci id="S3.p3.8.m8.2.3.3.3.2.cmml" xref="S3.p3.8.m8.2.3.3.3.2">𝑛</ci><ci id="S3.p3.8.m8.2.3.3.3.3.cmml" xref="S3.p3.8.m8.2.3.3.3.3">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.8.m8.2c">\textbf{a}^{(l,h)}\in\mathbb{R}^{n\times d}</annotation><annotation encoding="application/x-llamapun" id="S3.p3.8.m8.2d">a start_POSTSUPERSCRIPT ( italic_l , italic_h ) end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>.
We use <math alttext="\textbf{a}^{(l,h)}_{i}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S3.p3.9.m9.2"><semantics id="S3.p3.9.m9.2a"><mrow id="S3.p3.9.m9.2.3" xref="S3.p3.9.m9.2.3.cmml"><msubsup id="S3.p3.9.m9.2.3.2" xref="S3.p3.9.m9.2.3.2.cmml"><mtext class="ltx_mathvariant_bold" id="S3.p3.9.m9.2.3.2.2.2" xref="S3.p3.9.m9.2.3.2.2.2a.cmml">a</mtext><mi id="S3.p3.9.m9.2.3.2.3" xref="S3.p3.9.m9.2.3.2.3.cmml">i</mi><mrow id="S3.p3.9.m9.2.2.2.4" xref="S3.p3.9.m9.2.2.2.3.cmml"><mo id="S3.p3.9.m9.2.2.2.4.1" stretchy="false" xref="S3.p3.9.m9.2.2.2.3.cmml">(</mo><mi id="S3.p3.9.m9.1.1.1.1" xref="S3.p3.9.m9.1.1.1.1.cmml">l</mi><mo id="S3.p3.9.m9.2.2.2.4.2" xref="S3.p3.9.m9.2.2.2.3.cmml">,</mo><mi id="S3.p3.9.m9.2.2.2.2" xref="S3.p3.9.m9.2.2.2.2.cmml">h</mi><mo id="S3.p3.9.m9.2.2.2.4.3" stretchy="false" xref="S3.p3.9.m9.2.2.2.3.cmml">)</mo></mrow></msubsup><mo id="S3.p3.9.m9.2.3.1" xref="S3.p3.9.m9.2.3.1.cmml">∈</mo><msup id="S3.p3.9.m9.2.3.3" xref="S3.p3.9.m9.2.3.3.cmml"><mi id="S3.p3.9.m9.2.3.3.2" xref="S3.p3.9.m9.2.3.3.2.cmml">ℝ</mi><mi id="S3.p3.9.m9.2.3.3.3" xref="S3.p3.9.m9.2.3.3.3.cmml">d</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.9.m9.2b"><apply id="S3.p3.9.m9.2.3.cmml" xref="S3.p3.9.m9.2.3"><in id="S3.p3.9.m9.2.3.1.cmml" xref="S3.p3.9.m9.2.3.1"></in><apply id="S3.p3.9.m9.2.3.2.cmml" xref="S3.p3.9.m9.2.3.2"><csymbol cd="ambiguous" id="S3.p3.9.m9.2.3.2.1.cmml" xref="S3.p3.9.m9.2.3.2">subscript</csymbol><apply id="S3.p3.9.m9.2.3.2.2.cmml" xref="S3.p3.9.m9.2.3.2"><csymbol cd="ambiguous" id="S3.p3.9.m9.2.3.2.2.1.cmml" xref="S3.p3.9.m9.2.3.2">superscript</csymbol><ci id="S3.p3.9.m9.2.3.2.2.2a.cmml" xref="S3.p3.9.m9.2.3.2.2.2"><mtext class="ltx_mathvariant_bold" id="S3.p3.9.m9.2.3.2.2.2.cmml" xref="S3.p3.9.m9.2.3.2.2.2">a</mtext></ci><interval closure="open" id="S3.p3.9.m9.2.2.2.3.cmml" xref="S3.p3.9.m9.2.2.2.4"><ci id="S3.p3.9.m9.1.1.1.1.cmml" xref="S3.p3.9.m9.1.1.1.1">𝑙</ci><ci id="S3.p3.9.m9.2.2.2.2.cmml" xref="S3.p3.9.m9.2.2.2.2">ℎ</ci></interval></apply><ci id="S3.p3.9.m9.2.3.2.3.cmml" xref="S3.p3.9.m9.2.3.2.3">𝑖</ci></apply><apply id="S3.p3.9.m9.2.3.3.cmml" xref="S3.p3.9.m9.2.3.3"><csymbol cd="ambiguous" id="S3.p3.9.m9.2.3.3.1.cmml" xref="S3.p3.9.m9.2.3.3">superscript</csymbol><ci id="S3.p3.9.m9.2.3.3.2.cmml" xref="S3.p3.9.m9.2.3.3.2">ℝ</ci><ci id="S3.p3.9.m9.2.3.3.3.cmml" xref="S3.p3.9.m9.2.3.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.9.m9.2c">\textbf{a}^{(l,h)}_{i}\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun" id="S3.p3.9.m9.2d">a start_POSTSUPERSCRIPT ( italic_l , italic_h ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, where <math alttext="1\leq i\leq n" class="ltx_Math" display="inline" id="S3.p3.10.m10.1"><semantics id="S3.p3.10.m10.1a"><mrow id="S3.p3.10.m10.1.1" xref="S3.p3.10.m10.1.1.cmml"><mn id="S3.p3.10.m10.1.1.2" xref="S3.p3.10.m10.1.1.2.cmml">1</mn><mo id="S3.p3.10.m10.1.1.3" xref="S3.p3.10.m10.1.1.3.cmml">≤</mo><mi id="S3.p3.10.m10.1.1.4" xref="S3.p3.10.m10.1.1.4.cmml">i</mi><mo id="S3.p3.10.m10.1.1.5" xref="S3.p3.10.m10.1.1.5.cmml">≤</mo><mi id="S3.p3.10.m10.1.1.6" xref="S3.p3.10.m10.1.1.6.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.10.m10.1b"><apply id="S3.p3.10.m10.1.1.cmml" xref="S3.p3.10.m10.1.1"><and id="S3.p3.10.m10.1.1a.cmml" xref="S3.p3.10.m10.1.1"></and><apply id="S3.p3.10.m10.1.1b.cmml" xref="S3.p3.10.m10.1.1"><leq id="S3.p3.10.m10.1.1.3.cmml" xref="S3.p3.10.m10.1.1.3"></leq><cn id="S3.p3.10.m10.1.1.2.cmml" type="integer" xref="S3.p3.10.m10.1.1.2">1</cn><ci id="S3.p3.10.m10.1.1.4.cmml" xref="S3.p3.10.m10.1.1.4">𝑖</ci></apply><apply id="S3.p3.10.m10.1.1c.cmml" xref="S3.p3.10.m10.1.1"><leq id="S3.p3.10.m10.1.1.5.cmml" xref="S3.p3.10.m10.1.1.5"></leq><share href="https://arxiv.org/html/2409.19745v2#S3.p3.10.m10.1.1.4.cmml" id="S3.p3.10.m10.1.1d.cmml" xref="S3.p3.10.m10.1.1"></share><ci id="S3.p3.10.m10.1.1.6.cmml" xref="S3.p3.10.m10.1.1.6">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.10.m10.1c">1\leq i\leq n</annotation><annotation encoding="application/x-llamapun" id="S3.p3.10.m10.1d">1 ≤ italic_i ≤ italic_n</annotation></semantics></math>, to represent the output at position <math alttext="i" class="ltx_Math" display="inline" id="S3.p3.11.m11.1"><semantics id="S3.p3.11.m11.1a"><mi id="S3.p3.11.m11.1.1" xref="S3.p3.11.m11.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.p3.11.m11.1b"><ci id="S3.p3.11.m11.1.1.cmml" xref="S3.p3.11.m11.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.11.m11.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.p3.11.m11.1d">italic_i</annotation></semantics></math>.
The discovery paradigm typically includes three steps, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S3.F1" title="Figure 1 ‣ 3 Preliminaries: Discovery of influential attention heads ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_tag">1</span></a>:</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p4">
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.3">In the <span class="ltx_text ltx_font_italic" id="S3.I1.i1.p1.3.1">normal run</span>, with an input sequence <math alttext="X" class="ltx_Math" display="inline" id="S3.I1.i1.p1.1.m1.1"><semantics id="S3.I1.i1.p1.1.m1.1a"><mi id="S3.I1.i1.p1.1.m1.1.1" xref="S3.I1.i1.p1.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.1.m1.1b"><ci id="S3.I1.i1.p1.1.m1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.1.m1.1c">X</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.1.m1.1d">italic_X</annotation></semantics></math> (e.g., <math alttext="X=" class="ltx_Math" display="inline" id="S3.I1.i1.p1.2.m2.1"><semantics id="S3.I1.i1.p1.2.m2.1a"><mrow id="S3.I1.i1.p1.2.m2.1.1" xref="S3.I1.i1.p1.2.m2.1.1.cmml"><mi id="S3.I1.i1.p1.2.m2.1.1.2" xref="S3.I1.i1.p1.2.m2.1.1.2.cmml">X</mi><mo id="S3.I1.i1.p1.2.m2.1.1.1" xref="S3.I1.i1.p1.2.m2.1.1.1.cmml">=</mo><mi id="S3.I1.i1.p1.2.m2.1.1.3" xref="S3.I1.i1.p1.2.m2.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.2.m2.1b"><apply id="S3.I1.i1.p1.2.m2.1.1.cmml" xref="S3.I1.i1.p1.2.m2.1.1"><eq id="S3.I1.i1.p1.2.m2.1.1.1.cmml" xref="S3.I1.i1.p1.2.m2.1.1.1"></eq><ci id="S3.I1.i1.p1.2.m2.1.1.2.cmml" xref="S3.I1.i1.p1.2.m2.1.1.2">𝑋</ci><csymbol cd="latexml" id="S3.I1.i1.p1.2.m2.1.1.3.cmml" xref="S3.I1.i1.p1.2.m2.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.2.m2.1c">X=</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.2.m2.1d">italic_X =</annotation></semantics></math>“The capital of France is”), <math alttext="\textbf{a}^{(l,h)}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.3.m3.2"><semantics id="S3.I1.i1.p1.3.m3.2a"><msup id="S3.I1.i1.p1.3.m3.2.3" xref="S3.I1.i1.p1.3.m3.2.3.cmml"><mtext class="ltx_mathvariant_bold" id="S3.I1.i1.p1.3.m3.2.3.2" xref="S3.I1.i1.p1.3.m3.2.3.2a.cmml">a</mtext><mrow id="S3.I1.i1.p1.3.m3.2.2.2.4" xref="S3.I1.i1.p1.3.m3.2.2.2.3.cmml"><mo id="S3.I1.i1.p1.3.m3.2.2.2.4.1" stretchy="false" xref="S3.I1.i1.p1.3.m3.2.2.2.3.cmml">(</mo><mi id="S3.I1.i1.p1.3.m3.1.1.1.1" xref="S3.I1.i1.p1.3.m3.1.1.1.1.cmml">l</mi><mo id="S3.I1.i1.p1.3.m3.2.2.2.4.2" xref="S3.I1.i1.p1.3.m3.2.2.2.3.cmml">,</mo><mi id="S3.I1.i1.p1.3.m3.2.2.2.2" xref="S3.I1.i1.p1.3.m3.2.2.2.2.cmml">h</mi><mo id="S3.I1.i1.p1.3.m3.2.2.2.4.3" stretchy="false" xref="S3.I1.i1.p1.3.m3.2.2.2.3.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.3.m3.2b"><apply id="S3.I1.i1.p1.3.m3.2.3.cmml" xref="S3.I1.i1.p1.3.m3.2.3"><csymbol cd="ambiguous" id="S3.I1.i1.p1.3.m3.2.3.1.cmml" xref="S3.I1.i1.p1.3.m3.2.3">superscript</csymbol><ci id="S3.I1.i1.p1.3.m3.2.3.2a.cmml" xref="S3.I1.i1.p1.3.m3.2.3.2"><mtext class="ltx_mathvariant_bold" id="S3.I1.i1.p1.3.m3.2.3.2.cmml" xref="S3.I1.i1.p1.3.m3.2.3.2">a</mtext></ci><interval closure="open" id="S3.I1.i1.p1.3.m3.2.2.2.3.cmml" xref="S3.I1.i1.p1.3.m3.2.2.2.4"><ci id="S3.I1.i1.p1.3.m3.1.1.1.1.cmml" xref="S3.I1.i1.p1.3.m3.1.1.1.1">𝑙</ci><ci id="S3.I1.i1.p1.3.m3.2.2.2.2.cmml" xref="S3.I1.i1.p1.3.m3.2.2.2.2">ℎ</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.3.m3.2c">\textbf{a}^{(l,h)}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.3.m3.2d">a start_POSTSUPERSCRIPT ( italic_l , italic_h ) end_POSTSUPERSCRIPT</annotation></semantics></math> for every attention head are recorded.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.3">In the <span class="ltx_text ltx_font_italic" id="S3.I1.i2.p1.3.1">perturbed run</span>, the forward computation runs using the same input sequence <math alttext="X" class="ltx_Math" display="inline" id="S3.I1.i2.p1.1.m1.1"><semantics id="S3.I1.i2.p1.1.m1.1a"><mi id="S3.I1.i2.p1.1.m1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.m1.1b"><ci id="S3.I1.i2.p1.1.m1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.1c">X</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.1.m1.1d">italic_X</annotation></semantics></math>, but with some mediation.
This mediation either changes the discrete input tokens within <math alttext="X" class="ltx_Math" display="inline" id="S3.I1.i2.p1.2.m2.1"><semantics id="S3.I1.i2.p1.2.m2.1a"><mi id="S3.I1.i2.p1.2.m2.1.1" xref="S3.I1.i2.p1.2.m2.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.2.m2.1b"><ci id="S3.I1.i2.p1.2.m2.1.1.cmml" xref="S3.I1.i2.p1.2.m2.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.2.m2.1c">X</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.2.m2.1d">italic_X</annotation></semantics></math> by substituting specific keywords (e.g., replacing “France” with “England”), or corrupts the hidden states by adding noise.
The modified <math alttext="\tilde{\textbf{a}}^{(l,h)}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.3.m3.2"><semantics id="S3.I1.i2.p1.3.m3.2a"><msup id="S3.I1.i2.p1.3.m3.2.3" xref="S3.I1.i2.p1.3.m3.2.3.cmml"><mover accent="true" id="S3.I1.i2.p1.3.m3.2.3.2" xref="S3.I1.i2.p1.3.m3.2.3.2.cmml"><mtext class="ltx_mathvariant_bold" id="S3.I1.i2.p1.3.m3.2.3.2.2" xref="S3.I1.i2.p1.3.m3.2.3.2.2a.cmml">a</mtext><mo id="S3.I1.i2.p1.3.m3.2.3.2.1" xref="S3.I1.i2.p1.3.m3.2.3.2.1.cmml">~</mo></mover><mrow id="S3.I1.i2.p1.3.m3.2.2.2.4" xref="S3.I1.i2.p1.3.m3.2.2.2.3.cmml"><mo id="S3.I1.i2.p1.3.m3.2.2.2.4.1" stretchy="false" xref="S3.I1.i2.p1.3.m3.2.2.2.3.cmml">(</mo><mi id="S3.I1.i2.p1.3.m3.1.1.1.1" xref="S3.I1.i2.p1.3.m3.1.1.1.1.cmml">l</mi><mo id="S3.I1.i2.p1.3.m3.2.2.2.4.2" xref="S3.I1.i2.p1.3.m3.2.2.2.3.cmml">,</mo><mi id="S3.I1.i2.p1.3.m3.2.2.2.2" xref="S3.I1.i2.p1.3.m3.2.2.2.2.cmml">h</mi><mo id="S3.I1.i2.p1.3.m3.2.2.2.4.3" stretchy="false" xref="S3.I1.i2.p1.3.m3.2.2.2.3.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.3.m3.2b"><apply id="S3.I1.i2.p1.3.m3.2.3.cmml" xref="S3.I1.i2.p1.3.m3.2.3"><csymbol cd="ambiguous" id="S3.I1.i2.p1.3.m3.2.3.1.cmml" xref="S3.I1.i2.p1.3.m3.2.3">superscript</csymbol><apply id="S3.I1.i2.p1.3.m3.2.3.2.cmml" xref="S3.I1.i2.p1.3.m3.2.3.2"><ci id="S3.I1.i2.p1.3.m3.2.3.2.1.cmml" xref="S3.I1.i2.p1.3.m3.2.3.2.1">~</ci><ci id="S3.I1.i2.p1.3.m3.2.3.2.2a.cmml" xref="S3.I1.i2.p1.3.m3.2.3.2.2"><mtext class="ltx_mathvariant_bold" id="S3.I1.i2.p1.3.m3.2.3.2.2.cmml" xref="S3.I1.i2.p1.3.m3.2.3.2.2">a</mtext></ci></apply><interval closure="open" id="S3.I1.i2.p1.3.m3.2.2.2.3.cmml" xref="S3.I1.i2.p1.3.m3.2.2.2.4"><ci id="S3.I1.i2.p1.3.m3.1.1.1.1.cmml" xref="S3.I1.i2.p1.3.m3.1.1.1.1">𝑙</ci><ci id="S3.I1.i2.p1.3.m3.2.2.2.2.cmml" xref="S3.I1.i2.p1.3.m3.2.2.2.2">ℎ</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.3.m3.2c">\tilde{\textbf{a}}^{(l,h)}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.3.m3.2d">over~ start_ARG a end_ARG start_POSTSUPERSCRIPT ( italic_l , italic_h ) end_POSTSUPERSCRIPT</annotation></semantics></math> for each attention head are then recorded.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para ltx_noindent" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.6">We conduct an intervention on a particular head <math alttext="A^{(l,h)}" class="ltx_Math" display="inline" id="S3.I1.i3.p1.1.m1.2"><semantics id="S3.I1.i3.p1.1.m1.2a"><msup id="S3.I1.i3.p1.1.m1.2.3" xref="S3.I1.i3.p1.1.m1.2.3.cmml"><mi id="S3.I1.i3.p1.1.m1.2.3.2" xref="S3.I1.i3.p1.1.m1.2.3.2.cmml">A</mi><mrow id="S3.I1.i3.p1.1.m1.2.2.2.4" xref="S3.I1.i3.p1.1.m1.2.2.2.3.cmml"><mo id="S3.I1.i3.p1.1.m1.2.2.2.4.1" stretchy="false" xref="S3.I1.i3.p1.1.m1.2.2.2.3.cmml">(</mo><mi id="S3.I1.i3.p1.1.m1.1.1.1.1" xref="S3.I1.i3.p1.1.m1.1.1.1.1.cmml">l</mi><mo id="S3.I1.i3.p1.1.m1.2.2.2.4.2" xref="S3.I1.i3.p1.1.m1.2.2.2.3.cmml">,</mo><mi id="S3.I1.i3.p1.1.m1.2.2.2.2" xref="S3.I1.i3.p1.1.m1.2.2.2.2.cmml">h</mi><mo id="S3.I1.i3.p1.1.m1.2.2.2.4.3" stretchy="false" xref="S3.I1.i3.p1.1.m1.2.2.2.3.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.1.m1.2b"><apply id="S3.I1.i3.p1.1.m1.2.3.cmml" xref="S3.I1.i3.p1.1.m1.2.3"><csymbol cd="ambiguous" id="S3.I1.i3.p1.1.m1.2.3.1.cmml" xref="S3.I1.i3.p1.1.m1.2.3">superscript</csymbol><ci id="S3.I1.i3.p1.1.m1.2.3.2.cmml" xref="S3.I1.i3.p1.1.m1.2.3.2">𝐴</ci><interval closure="open" id="S3.I1.i3.p1.1.m1.2.2.2.3.cmml" xref="S3.I1.i3.p1.1.m1.2.2.2.4"><ci id="S3.I1.i3.p1.1.m1.1.1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1.1.1">𝑙</ci><ci id="S3.I1.i3.p1.1.m1.2.2.2.2.cmml" xref="S3.I1.i3.p1.1.m1.2.2.2.2">ℎ</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.1.m1.2c">A^{(l,h)}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i3.p1.1.m1.2d">italic_A start_POSTSUPERSCRIPT ( italic_l , italic_h ) end_POSTSUPERSCRIPT</annotation></semantics></math> at a specific position <math alttext="i" class="ltx_Math" display="inline" id="S3.I1.i3.p1.2.m2.1"><semantics id="S3.I1.i3.p1.2.m2.1a"><mi id="S3.I1.i3.p1.2.m2.1.1" xref="S3.I1.i3.p1.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.2.m2.1b"><ci id="S3.I1.i3.p1.2.m2.1.1.cmml" xref="S3.I1.i3.p1.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.2.m2.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i3.p1.2.m2.1d">italic_i</annotation></semantics></math> (e.g., the country token position in above examples) in the normal run by substituting its outputs with <math alttext="\tilde{\textbf{a}}^{(l,h)}_{i}" class="ltx_Math" display="inline" id="S3.I1.i3.p1.3.m3.2"><semantics id="S3.I1.i3.p1.3.m3.2a"><msubsup id="S3.I1.i3.p1.3.m3.2.3" xref="S3.I1.i3.p1.3.m3.2.3.cmml"><mover accent="true" id="S3.I1.i3.p1.3.m3.2.3.2.2" xref="S3.I1.i3.p1.3.m3.2.3.2.2.cmml"><mtext class="ltx_mathvariant_bold" id="S3.I1.i3.p1.3.m3.2.3.2.2.2" xref="S3.I1.i3.p1.3.m3.2.3.2.2.2a.cmml">a</mtext><mo id="S3.I1.i3.p1.3.m3.2.3.2.2.1" xref="S3.I1.i3.p1.3.m3.2.3.2.2.1.cmml">~</mo></mover><mi id="S3.I1.i3.p1.3.m3.2.3.3" xref="S3.I1.i3.p1.3.m3.2.3.3.cmml">i</mi><mrow id="S3.I1.i3.p1.3.m3.2.2.2.4" xref="S3.I1.i3.p1.3.m3.2.2.2.3.cmml"><mo id="S3.I1.i3.p1.3.m3.2.2.2.4.1" stretchy="false" xref="S3.I1.i3.p1.3.m3.2.2.2.3.cmml">(</mo><mi id="S3.I1.i3.p1.3.m3.1.1.1.1" xref="S3.I1.i3.p1.3.m3.1.1.1.1.cmml">l</mi><mo id="S3.I1.i3.p1.3.m3.2.2.2.4.2" xref="S3.I1.i3.p1.3.m3.2.2.2.3.cmml">,</mo><mi id="S3.I1.i3.p1.3.m3.2.2.2.2" xref="S3.I1.i3.p1.3.m3.2.2.2.2.cmml">h</mi><mo id="S3.I1.i3.p1.3.m3.2.2.2.4.3" stretchy="false" xref="S3.I1.i3.p1.3.m3.2.2.2.3.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.3.m3.2b"><apply id="S3.I1.i3.p1.3.m3.2.3.cmml" xref="S3.I1.i3.p1.3.m3.2.3"><csymbol cd="ambiguous" id="S3.I1.i3.p1.3.m3.2.3.1.cmml" xref="S3.I1.i3.p1.3.m3.2.3">subscript</csymbol><apply id="S3.I1.i3.p1.3.m3.2.3.2.cmml" xref="S3.I1.i3.p1.3.m3.2.3"><csymbol cd="ambiguous" id="S3.I1.i3.p1.3.m3.2.3.2.1.cmml" xref="S3.I1.i3.p1.3.m3.2.3">superscript</csymbol><apply id="S3.I1.i3.p1.3.m3.2.3.2.2.cmml" xref="S3.I1.i3.p1.3.m3.2.3.2.2"><ci id="S3.I1.i3.p1.3.m3.2.3.2.2.1.cmml" xref="S3.I1.i3.p1.3.m3.2.3.2.2.1">~</ci><ci id="S3.I1.i3.p1.3.m3.2.3.2.2.2a.cmml" xref="S3.I1.i3.p1.3.m3.2.3.2.2.2"><mtext class="ltx_mathvariant_bold" id="S3.I1.i3.p1.3.m3.2.3.2.2.2.cmml" xref="S3.I1.i3.p1.3.m3.2.3.2.2.2">a</mtext></ci></apply><interval closure="open" id="S3.I1.i3.p1.3.m3.2.2.2.3.cmml" xref="S3.I1.i3.p1.3.m3.2.2.2.4"><ci id="S3.I1.i3.p1.3.m3.1.1.1.1.cmml" xref="S3.I1.i3.p1.3.m3.1.1.1.1">𝑙</ci><ci id="S3.I1.i3.p1.3.m3.2.2.2.2.cmml" xref="S3.I1.i3.p1.3.m3.2.2.2.2">ℎ</ci></interval></apply><ci id="S3.I1.i3.p1.3.m3.2.3.3.cmml" xref="S3.I1.i3.p1.3.m3.2.3.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.3.m3.2c">\tilde{\textbf{a}}^{(l,h)}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i3.p1.3.m3.2d">over~ start_ARG a end_ARG start_POSTSUPERSCRIPT ( italic_l , italic_h ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. The subsequent activations in the computational graph are then recomputed (these recomputed activations are denoted as <math alttext="\bar{\textbf{a}}" class="ltx_Math" display="inline" id="S3.I1.i3.p1.4.m4.1"><semantics id="S3.I1.i3.p1.4.m4.1a"><mover accent="true" id="S3.I1.i3.p1.4.m4.1.1" xref="S3.I1.i3.p1.4.m4.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S3.I1.i3.p1.4.m4.1.1.2" xref="S3.I1.i3.p1.4.m4.1.1.2a.cmml">a</mtext><mo id="S3.I1.i3.p1.4.m4.1.1.1" xref="S3.I1.i3.p1.4.m4.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.4.m4.1b"><apply id="S3.I1.i3.p1.4.m4.1.1.cmml" xref="S3.I1.i3.p1.4.m4.1.1"><ci id="S3.I1.i3.p1.4.m4.1.1.1.cmml" xref="S3.I1.i3.p1.4.m4.1.1.1">¯</ci><ci id="S3.I1.i3.p1.4.m4.1.1.2a.cmml" xref="S3.I1.i3.p1.4.m4.1.1.2"><mtext class="ltx_mathvariant_bold" id="S3.I1.i3.p1.4.m4.1.1.2.cmml" xref="S3.I1.i3.p1.4.m4.1.1.2">a</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.4.m4.1c">\bar{\textbf{a}}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i3.p1.4.m4.1d">over¯ start_ARG a end_ARG</annotation></semantics></math> in the figure).
If the language model’s final output matches the intervention’s expectation (e.g., the predicted token changes from ”Paris” to ”London”), the head <math alttext="A^{(l,h)}" class="ltx_Math" display="inline" id="S3.I1.i3.p1.5.m5.2"><semantics id="S3.I1.i3.p1.5.m5.2a"><msup id="S3.I1.i3.p1.5.m5.2.3" xref="S3.I1.i3.p1.5.m5.2.3.cmml"><mi id="S3.I1.i3.p1.5.m5.2.3.2" xref="S3.I1.i3.p1.5.m5.2.3.2.cmml">A</mi><mrow id="S3.I1.i3.p1.5.m5.2.2.2.4" xref="S3.I1.i3.p1.5.m5.2.2.2.3.cmml"><mo id="S3.I1.i3.p1.5.m5.2.2.2.4.1" stretchy="false" xref="S3.I1.i3.p1.5.m5.2.2.2.3.cmml">(</mo><mi id="S3.I1.i3.p1.5.m5.1.1.1.1" xref="S3.I1.i3.p1.5.m5.1.1.1.1.cmml">l</mi><mo id="S3.I1.i3.p1.5.m5.2.2.2.4.2" xref="S3.I1.i3.p1.5.m5.2.2.2.3.cmml">,</mo><mi id="S3.I1.i3.p1.5.m5.2.2.2.2" xref="S3.I1.i3.p1.5.m5.2.2.2.2.cmml">h</mi><mo id="S3.I1.i3.p1.5.m5.2.2.2.4.3" stretchy="false" xref="S3.I1.i3.p1.5.m5.2.2.2.3.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.5.m5.2b"><apply id="S3.I1.i3.p1.5.m5.2.3.cmml" xref="S3.I1.i3.p1.5.m5.2.3"><csymbol cd="ambiguous" id="S3.I1.i3.p1.5.m5.2.3.1.cmml" xref="S3.I1.i3.p1.5.m5.2.3">superscript</csymbol><ci id="S3.I1.i3.p1.5.m5.2.3.2.cmml" xref="S3.I1.i3.p1.5.m5.2.3.2">𝐴</ci><interval closure="open" id="S3.I1.i3.p1.5.m5.2.2.2.3.cmml" xref="S3.I1.i3.p1.5.m5.2.2.2.4"><ci id="S3.I1.i3.p1.5.m5.1.1.1.1.cmml" xref="S3.I1.i3.p1.5.m5.1.1.1.1">𝑙</ci><ci id="S3.I1.i3.p1.5.m5.2.2.2.2.cmml" xref="S3.I1.i3.p1.5.m5.2.2.2.2">ℎ</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.5.m5.2c">A^{(l,h)}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i3.p1.5.m5.2d">italic_A start_POSTSUPERSCRIPT ( italic_l , italic_h ) end_POSTSUPERSCRIPT</annotation></semantics></math> is considered to have a positive influence on the processing of sequence <math alttext="X" class="ltx_Math" display="inline" id="S3.I1.i3.p1.6.m6.1"><semantics id="S3.I1.i3.p1.6.m6.1a"><mi id="S3.I1.i3.p1.6.m6.1.1" xref="S3.I1.i3.p1.6.m6.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.6.m6.1b"><ci id="S3.I1.i3.p1.6.m6.1.1.cmml" xref="S3.I1.i3.p1.6.m6.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.6.m6.1c">X</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i3.p1.6.m6.1d">italic_X</annotation></semantics></math>.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para ltx_noindent" id="S3.p5">
<p class="ltx_p" id="S3.p5.1">This overview outlines a simplified discovery paradigm; detailed measurements of intervention impact are tailored to specific experimental needs.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Methodology</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we provide a detailed introduction to our proposed method, <span class="ltx_text ltx_font_italic" id="S4.p1.1.1">PEAR</span>, which is executed in two stages: (1) discovering RAG-suppression heads and (2) re-weighting coefficient learning.
The first stage discovers attention heads that have a negative impact on general RAG tasks based on circuit discovery for a proxy task.
In the second stage, we optimize learnable coefficients to re-weight the outputs of the discovered heads, aiming to mitigate their RAG-suppression effect.
These coefficients remain fixed during inference, irrespective of the specific input.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S4.F2" title="Figure 2 ‣ 4.2 Re-weighting coefficient learning ‣ 4 Methodology ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_tag">2</span></a> demonstrates the overview of <span class="ltx_text ltx_font_italic" id="S4.p1.1.2">PEAR</span>.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Discovery of RAG-suppression heads</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We set up a proxy task and use this task as input for circuit discovery algorithms to discover influential attention heads that hamper LLMs’ performance on general RAG tasks.</p>
</div>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Task Input</h4>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.8">For each input sample, we create a sequence of length <math alttext="n" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.1.m1.1"><semantics id="S4.SS1.SSS0.Px1.p1.1.m1.1a"><mi id="S4.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.1.m1.1b"><ci id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px1.p1.1.m1.1d">italic_n</annotation></semantics></math>, denoted as <math alttext="\{x_{1},\ldots,x_{n}\}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.2.m2.3"><semantics id="S4.SS1.SSS0.Px1.p1.2.m2.3a"><mrow id="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2" xref="S4.SS1.SSS0.Px1.p1.2.m2.3.3.3.cmml"><mo id="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.3" stretchy="false" xref="S4.SS1.SSS0.Px1.p1.2.m2.3.3.3.cmml">{</mo><msub id="S4.SS1.SSS0.Px1.p1.2.m2.2.2.1.1" xref="S4.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.cmml"><mi id="S4.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.2" xref="S4.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.2.cmml">x</mi><mn id="S4.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.3" xref="S4.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.3.cmml">1</mn></msub><mo id="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.4" xref="S4.SS1.SSS0.Px1.p1.2.m2.3.3.3.cmml">,</mo><mi id="S4.SS1.SSS0.Px1.p1.2.m2.1.1" mathvariant="normal" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.cmml">…</mi><mo id="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.5" xref="S4.SS1.SSS0.Px1.p1.2.m2.3.3.3.cmml">,</mo><msub id="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.2" xref="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.cmml"><mi id="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.2" xref="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.2.cmml">x</mi><mi id="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.3" xref="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.3.cmml">n</mi></msub><mo id="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.6" stretchy="false" xref="S4.SS1.SSS0.Px1.p1.2.m2.3.3.3.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.2.m2.3b"><set id="S4.SS1.SSS0.Px1.p1.2.m2.3.3.3.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2"><apply id="S4.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.2.2.1.1">subscript</csymbol><ci id="S4.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.2.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.2">𝑥</ci><cn id="S4.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.3.cmml" type="integer" xref="S4.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.3">1</cn></apply><ci id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1">…</ci><apply id="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.2"><csymbol cd="ambiguous" id="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.1.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.2">subscript</csymbol><ci id="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.2.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.2">𝑥</ci><ci id="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.3.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.3">𝑛</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.2.m2.3c">\{x_{1},\ldots,x_{n}\}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px1.p1.2.m2.3d">{ italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }</annotation></semantics></math>, where each <math alttext="x_{i}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.3.m3.1"><semantics id="S4.SS1.SSS0.Px1.p1.3.m3.1a"><msub id="S4.SS1.SSS0.Px1.p1.3.m3.1.1" xref="S4.SS1.SSS0.Px1.p1.3.m3.1.1.cmml"><mi id="S4.SS1.SSS0.Px1.p1.3.m3.1.1.2" xref="S4.SS1.SSS0.Px1.p1.3.m3.1.1.2.cmml">x</mi><mi id="S4.SS1.SSS0.Px1.p1.3.m3.1.1.3" xref="S4.SS1.SSS0.Px1.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.3.m3.1b"><apply id="S4.SS1.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS1.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S4.SS1.SSS0.Px1.p1.3.m3.1.1.2">𝑥</ci><ci id="S4.SS1.SSS0.Px1.p1.3.m3.1.1.3.cmml" xref="S4.SS1.SSS0.Px1.p1.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.3.m3.1c">x_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px1.p1.3.m3.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is a randomly sampled token from the vocabulary. This sequence is repeated to form an input sample <math alttext="X=\{x_{1},\ldots,x_{2n}\}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.4.m4.3"><semantics id="S4.SS1.SSS0.Px1.p1.4.m4.3a"><mrow id="S4.SS1.SSS0.Px1.p1.4.m4.3.3" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.3.cmml"><mi id="S4.SS1.SSS0.Px1.p1.4.m4.3.3.4" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.3.4.cmml">X</mi><mo id="S4.SS1.SSS0.Px1.p1.4.m4.3.3.3" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.3.3.cmml">=</mo><mrow id="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.2" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.3.cmml"><mo id="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.2.3" stretchy="false" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.3.cmml">{</mo><msub id="S4.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.1" xref="S4.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.1.cmml"><mi id="S4.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.1.2" xref="S4.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.1.2.cmml">x</mi><mn id="S4.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.1.3" xref="S4.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.2.4" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.3.cmml">,</mo><mi id="S4.SS1.SSS0.Px1.p1.4.m4.1.1" mathvariant="normal" xref="S4.SS1.SSS0.Px1.p1.4.m4.1.1.cmml">…</mi><mo id="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.2.5" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.3.cmml">,</mo><msub id="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.2.2" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.2.2.cmml"><mi id="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.2.2.2" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.2.2.2.cmml">x</mi><mrow id="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.2.2.3" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.2.2.3.cmml"><mn id="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.2.2.3.2" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.2.2.3.2.cmml">2</mn><mo id="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.2.2.3.1" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.2.2.3.1.cmml">⁢</mo><mi id="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.2.2.3.3" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.2.2.3.3.cmml">n</mi></mrow></msub><mo id="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.2.6" stretchy="false" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.4.m4.3b"><apply id="S4.SS1.SSS0.Px1.p1.4.m4.3.3.cmml" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.3"><eq id="S4.SS1.SSS0.Px1.p1.4.m4.3.3.3.cmml" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.3.3"></eq><ci id="S4.SS1.SSS0.Px1.p1.4.m4.3.3.4.cmml" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.3.4">𝑋</ci><set id="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.3.cmml" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.2"><apply id="S4.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.1">subscript</csymbol><ci id="S4.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.1.2.cmml" xref="S4.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.1.2">𝑥</ci><cn id="S4.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.1.3.cmml" type="integer" xref="S4.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.1.3">1</cn></apply><ci id="S4.SS1.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.4.m4.1.1">…</ci><apply id="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.2.2.cmml" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.2.2.1.cmml" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.2.2">subscript</csymbol><ci id="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.2.2.2.cmml" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.2.2.2">𝑥</ci><apply id="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.2.2.3.cmml" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.2.2.3"><times id="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.2.2.3.1.cmml" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.2.2.3.1"></times><cn id="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.2.2.3.2.cmml" type="integer" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.2.2.3.2">2</cn><ci id="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.2.2.3.3.cmml" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.3.2.2.2.3.3">𝑛</ci></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.4.m4.3c">X=\{x_{1},\ldots,x_{2n}\}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px1.p1.4.m4.3d">italic_X = { italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT 2 italic_n end_POSTSUBSCRIPT }</annotation></semantics></math>, with <math alttext="x_{i}=x_{i+n}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.5.m5.1"><semantics id="S4.SS1.SSS0.Px1.p1.5.m5.1a"><mrow id="S4.SS1.SSS0.Px1.p1.5.m5.1.1" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.cmml"><msub id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.2" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.2.cmml"><mi id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.2.2" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.2.2.cmml">x</mi><mi id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.2.3" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.2.3.cmml">i</mi></msub><mo id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.1" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.1.cmml">=</mo><msub id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.3" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.3.cmml"><mi id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.3.2" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.3.2.cmml">x</mi><mrow id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.3.3" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.3.3.cmml"><mi id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.3.3.2" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.3.3.2.cmml">i</mi><mo id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.3.3.1" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.3.3.1.cmml">+</mo><mi id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.3.3.3" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.3.3.3.cmml">n</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.5.m5.1b"><apply id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1"><eq id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.1"></eq><apply id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.2.cmml" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.2.1.cmml" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.2">subscript</csymbol><ci id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.2.2.cmml" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.2.2">𝑥</ci><ci id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.2.3.cmml" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.2.3">𝑖</ci></apply><apply id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.3.cmml" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.3.1.cmml" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.3">subscript</csymbol><ci id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.3.2.cmml" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.3.2">𝑥</ci><apply id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.3.3.cmml" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.3.3"><plus id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.3.3.1.cmml" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.3.3.1"></plus><ci id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.3.3.2.cmml" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.3.3.2">𝑖</ci><ci id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.3.3.3.cmml" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.3.3.3">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.5.m5.1c">x_{i}=x_{i+n}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px1.p1.5.m5.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT italic_i + italic_n end_POSTSUBSCRIPT</annotation></semantics></math> for <math alttext="i\in[1,n]" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.6.m6.2"><semantics id="S4.SS1.SSS0.Px1.p1.6.m6.2a"><mrow id="S4.SS1.SSS0.Px1.p1.6.m6.2.3" xref="S4.SS1.SSS0.Px1.p1.6.m6.2.3.cmml"><mi id="S4.SS1.SSS0.Px1.p1.6.m6.2.3.2" xref="S4.SS1.SSS0.Px1.p1.6.m6.2.3.2.cmml">i</mi><mo id="S4.SS1.SSS0.Px1.p1.6.m6.2.3.1" xref="S4.SS1.SSS0.Px1.p1.6.m6.2.3.1.cmml">∈</mo><mrow id="S4.SS1.SSS0.Px1.p1.6.m6.2.3.3.2" xref="S4.SS1.SSS0.Px1.p1.6.m6.2.3.3.1.cmml"><mo id="S4.SS1.SSS0.Px1.p1.6.m6.2.3.3.2.1" stretchy="false" xref="S4.SS1.SSS0.Px1.p1.6.m6.2.3.3.1.cmml">[</mo><mn id="S4.SS1.SSS0.Px1.p1.6.m6.1.1" xref="S4.SS1.SSS0.Px1.p1.6.m6.1.1.cmml">1</mn><mo id="S4.SS1.SSS0.Px1.p1.6.m6.2.3.3.2.2" xref="S4.SS1.SSS0.Px1.p1.6.m6.2.3.3.1.cmml">,</mo><mi id="S4.SS1.SSS0.Px1.p1.6.m6.2.2" xref="S4.SS1.SSS0.Px1.p1.6.m6.2.2.cmml">n</mi><mo id="S4.SS1.SSS0.Px1.p1.6.m6.2.3.3.2.3" stretchy="false" xref="S4.SS1.SSS0.Px1.p1.6.m6.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.6.m6.2b"><apply id="S4.SS1.SSS0.Px1.p1.6.m6.2.3.cmml" xref="S4.SS1.SSS0.Px1.p1.6.m6.2.3"><in id="S4.SS1.SSS0.Px1.p1.6.m6.2.3.1.cmml" xref="S4.SS1.SSS0.Px1.p1.6.m6.2.3.1"></in><ci id="S4.SS1.SSS0.Px1.p1.6.m6.2.3.2.cmml" xref="S4.SS1.SSS0.Px1.p1.6.m6.2.3.2">𝑖</ci><interval closure="closed" id="S4.SS1.SSS0.Px1.p1.6.m6.2.3.3.1.cmml" xref="S4.SS1.SSS0.Px1.p1.6.m6.2.3.3.2"><cn id="S4.SS1.SSS0.Px1.p1.6.m6.1.1.cmml" type="integer" xref="S4.SS1.SSS0.Px1.p1.6.m6.1.1">1</cn><ci id="S4.SS1.SSS0.Px1.p1.6.m6.2.2.cmml" xref="S4.SS1.SSS0.Px1.p1.6.m6.2.2">𝑛</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.6.m6.2c">i\in[1,n]</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px1.p1.6.m6.2d">italic_i ∈ [ 1 , italic_n ]</annotation></semantics></math>.
Research has shown that, in semantically meaningless contexts, models tend to check if the last few tokens in the sequence appeared previously and copy the suffix of their last appearance as the output <cite class="ltx_cite ltx_citemacro_citep">(Olsson et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib21" title="">2022</a>; Lv et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib16" title="">2024b</a>)</cite>.
We consider an arbitrary LLM to successfully perform the proxy task when, at position <math alttext="n+i" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.7.m7.1"><semantics id="S4.SS1.SSS0.Px1.p1.7.m7.1a"><mrow id="S4.SS1.SSS0.Px1.p1.7.m7.1.1" xref="S4.SS1.SSS0.Px1.p1.7.m7.1.1.cmml"><mi id="S4.SS1.SSS0.Px1.p1.7.m7.1.1.2" xref="S4.SS1.SSS0.Px1.p1.7.m7.1.1.2.cmml">n</mi><mo id="S4.SS1.SSS0.Px1.p1.7.m7.1.1.1" xref="S4.SS1.SSS0.Px1.p1.7.m7.1.1.1.cmml">+</mo><mi id="S4.SS1.SSS0.Px1.p1.7.m7.1.1.3" xref="S4.SS1.SSS0.Px1.p1.7.m7.1.1.3.cmml">i</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.7.m7.1b"><apply id="S4.SS1.SSS0.Px1.p1.7.m7.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.7.m7.1.1"><plus id="S4.SS1.SSS0.Px1.p1.7.m7.1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.7.m7.1.1.1"></plus><ci id="S4.SS1.SSS0.Px1.p1.7.m7.1.1.2.cmml" xref="S4.SS1.SSS0.Px1.p1.7.m7.1.1.2">𝑛</ci><ci id="S4.SS1.SSS0.Px1.p1.7.m7.1.1.3.cmml" xref="S4.SS1.SSS0.Px1.p1.7.m7.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.7.m7.1c">n+i</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px1.p1.7.m7.1d">italic_n + italic_i</annotation></semantics></math>, the token with the highest output logits is <math alttext="x_{i+1}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.8.m8.1"><semantics id="S4.SS1.SSS0.Px1.p1.8.m8.1a"><msub id="S4.SS1.SSS0.Px1.p1.8.m8.1.1" xref="S4.SS1.SSS0.Px1.p1.8.m8.1.1.cmml"><mi id="S4.SS1.SSS0.Px1.p1.8.m8.1.1.2" xref="S4.SS1.SSS0.Px1.p1.8.m8.1.1.2.cmml">x</mi><mrow id="S4.SS1.SSS0.Px1.p1.8.m8.1.1.3" xref="S4.SS1.SSS0.Px1.p1.8.m8.1.1.3.cmml"><mi id="S4.SS1.SSS0.Px1.p1.8.m8.1.1.3.2" xref="S4.SS1.SSS0.Px1.p1.8.m8.1.1.3.2.cmml">i</mi><mo id="S4.SS1.SSS0.Px1.p1.8.m8.1.1.3.1" xref="S4.SS1.SSS0.Px1.p1.8.m8.1.1.3.1.cmml">+</mo><mn id="S4.SS1.SSS0.Px1.p1.8.m8.1.1.3.3" xref="S4.SS1.SSS0.Px1.p1.8.m8.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.8.m8.1b"><apply id="S4.SS1.SSS0.Px1.p1.8.m8.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS0.Px1.p1.8.m8.1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.8.m8.1.1">subscript</csymbol><ci id="S4.SS1.SSS0.Px1.p1.8.m8.1.1.2.cmml" xref="S4.SS1.SSS0.Px1.p1.8.m8.1.1.2">𝑥</ci><apply id="S4.SS1.SSS0.Px1.p1.8.m8.1.1.3.cmml" xref="S4.SS1.SSS0.Px1.p1.8.m8.1.1.3"><plus id="S4.SS1.SSS0.Px1.p1.8.m8.1.1.3.1.cmml" xref="S4.SS1.SSS0.Px1.p1.8.m8.1.1.3.1"></plus><ci id="S4.SS1.SSS0.Px1.p1.8.m8.1.1.3.2.cmml" xref="S4.SS1.SSS0.Px1.p1.8.m8.1.1.3.2">𝑖</ci><cn id="S4.SS1.SSS0.Px1.p1.8.m8.1.1.3.3.cmml" type="integer" xref="S4.SS1.SSS0.Px1.p1.8.m8.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.8.m8.1c">x_{i+1}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px1.p1.8.m8.1d">italic_x start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT</annotation></semantics></math>.
Table <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S1.T1" title="Table 1 ‣ 1 Introduction ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_tag">1</span></a> shows an example input.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS0.Px1.p2">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p2.1">This proxy task exhibits two key characteristics that facilitate the effective discovery of RAG-related heads:</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS0.Px1.p3">
<ol class="ltx_enumerate" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">Completing the proxy task requires LLM capabilities essential for a robust RAG framework, such as in-context retrieval and generation based on context, making it suitable for discovering RAG-related attention heads.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para ltx_noindent" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">The random token composition in <math alttext="X" class="ltx_Math" display="inline" id="S4.I1.i2.p1.1.m1.1"><semantics id="S4.I1.i2.p1.1.m1.1a"><mi id="S4.I1.i2.p1.1.m1.1.1" xref="S4.I1.i2.p1.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.1.m1.1b"><ci id="S4.I1.i2.p1.1.m1.1.1.cmml" xref="S4.I1.i2.p1.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.1.m1.1c">X</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i2.p1.1.m1.1d">italic_X</annotation></semantics></math> ensures semantically meaningless input, minimizing knowledge bias and thereby ensuring the discovered attention heads to have general RAG-related functions, independent of specific downstream tasks.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Head discovery</h4>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.1">We previously outlined the head discovery algorithm in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S3" title="3 Preliminaries: Discovery of influential attention heads ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_tag">3</span></a>. Here, we provide additional practical details for the first stage of <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px2.p1.1.1">PEAR</span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS0.Px2.p2">
<ol class="ltx_enumerate" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.2">During the <span class="ltx_text ltx_font_italic" id="S4.I2.i1.p1.2.1">normal run</span>, the input sequences <math alttext="X" class="ltx_Math" display="inline" id="S4.I2.i1.p1.1.m1.1"><semantics id="S4.I2.i1.p1.1.m1.1a"><mi id="S4.I2.i1.p1.1.m1.1.1" xref="S4.I2.i1.p1.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S4.I2.i1.p1.1.m1.1b"><ci id="S4.I2.i1.p1.1.m1.1.1.cmml" xref="S4.I2.i1.p1.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i1.p1.1.m1.1c">X</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i1.p1.1.m1.1d">italic_X</annotation></semantics></math> are constructed as above described, with a length of <math alttext="2n" class="ltx_Math" display="inline" id="S4.I2.i1.p1.2.m2.1"><semantics id="S4.I2.i1.p1.2.m2.1a"><mrow id="S4.I2.i1.p1.2.m2.1.1" xref="S4.I2.i1.p1.2.m2.1.1.cmml"><mn id="S4.I2.i1.p1.2.m2.1.1.2" xref="S4.I2.i1.p1.2.m2.1.1.2.cmml">2</mn><mo id="S4.I2.i1.p1.2.m2.1.1.1" xref="S4.I2.i1.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S4.I2.i1.p1.2.m2.1.1.3" xref="S4.I2.i1.p1.2.m2.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.I2.i1.p1.2.m2.1b"><apply id="S4.I2.i1.p1.2.m2.1.1.cmml" xref="S4.I2.i1.p1.2.m2.1.1"><times id="S4.I2.i1.p1.2.m2.1.1.1.cmml" xref="S4.I2.i1.p1.2.m2.1.1.1"></times><cn id="S4.I2.i1.p1.2.m2.1.1.2.cmml" type="integer" xref="S4.I2.i1.p1.2.m2.1.1.2">2</cn><ci id="S4.I2.i1.p1.2.m2.1.1.3.cmml" xref="S4.I2.i1.p1.2.m2.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i1.p1.2.m2.1c">2n</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i1.p1.2.m2.1d">2 italic_n</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1">In the <span class="ltx_text ltx_font_italic" id="S4.I2.i2.p1.1.1">perturbed run</span>, we do not modify the input or hidden states; instead, we average the outputs of each attention head along the sequence dimension and record the resulting mean vectors.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S4.I2.i3.p1">
<p class="ltx_p" id="S4.I2.i3.p1.3">We focus on detecting changes in logits at position <math alttext="2n-1" class="ltx_Math" display="inline" id="S4.I2.i3.p1.1.m1.1"><semantics id="S4.I2.i3.p1.1.m1.1a"><mrow id="S4.I2.i3.p1.1.m1.1.1" xref="S4.I2.i3.p1.1.m1.1.1.cmml"><mrow id="S4.I2.i3.p1.1.m1.1.1.2" xref="S4.I2.i3.p1.1.m1.1.1.2.cmml"><mn id="S4.I2.i3.p1.1.m1.1.1.2.2" xref="S4.I2.i3.p1.1.m1.1.1.2.2.cmml">2</mn><mo id="S4.I2.i3.p1.1.m1.1.1.2.1" xref="S4.I2.i3.p1.1.m1.1.1.2.1.cmml">⁢</mo><mi id="S4.I2.i3.p1.1.m1.1.1.2.3" xref="S4.I2.i3.p1.1.m1.1.1.2.3.cmml">n</mi></mrow><mo id="S4.I2.i3.p1.1.m1.1.1.1" xref="S4.I2.i3.p1.1.m1.1.1.1.cmml">−</mo><mn id="S4.I2.i3.p1.1.m1.1.1.3" xref="S4.I2.i3.p1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.I2.i3.p1.1.m1.1b"><apply id="S4.I2.i3.p1.1.m1.1.1.cmml" xref="S4.I2.i3.p1.1.m1.1.1"><minus id="S4.I2.i3.p1.1.m1.1.1.1.cmml" xref="S4.I2.i3.p1.1.m1.1.1.1"></minus><apply id="S4.I2.i3.p1.1.m1.1.1.2.cmml" xref="S4.I2.i3.p1.1.m1.1.1.2"><times id="S4.I2.i3.p1.1.m1.1.1.2.1.cmml" xref="S4.I2.i3.p1.1.m1.1.1.2.1"></times><cn id="S4.I2.i3.p1.1.m1.1.1.2.2.cmml" type="integer" xref="S4.I2.i3.p1.1.m1.1.1.2.2">2</cn><ci id="S4.I2.i3.p1.1.m1.1.1.2.3.cmml" xref="S4.I2.i3.p1.1.m1.1.1.2.3">𝑛</ci></apply><cn id="S4.I2.i3.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.I2.i3.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i3.p1.1.m1.1c">2n-1</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i3.p1.1.m1.1d">2 italic_n - 1</annotation></semantics></math>, where the model is expected to copy the token from position <math alttext="n" class="ltx_Math" display="inline" id="S4.I2.i3.p1.2.m2.1"><semantics id="S4.I2.i3.p1.2.m2.1a"><mi id="S4.I2.i3.p1.2.m2.1.1" xref="S4.I2.i3.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.I2.i3.p1.2.m2.1b"><ci id="S4.I2.i3.p1.2.m2.1.1.cmml" xref="S4.I2.i3.p1.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i3.p1.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i3.p1.2.m2.1d">italic_n</annotation></semantics></math>. Consequently, we intervene at <math alttext="\textbf{a}^{(l,h)}_{2n-1}" class="ltx_Math" display="inline" id="S4.I2.i3.p1.3.m3.2"><semantics id="S4.I2.i3.p1.3.m3.2a"><msubsup id="S4.I2.i3.p1.3.m3.2.3" xref="S4.I2.i3.p1.3.m3.2.3.cmml"><mtext class="ltx_mathvariant_bold" id="S4.I2.i3.p1.3.m3.2.3.2.2" xref="S4.I2.i3.p1.3.m3.2.3.2.2a.cmml">a</mtext><mrow id="S4.I2.i3.p1.3.m3.2.3.3" xref="S4.I2.i3.p1.3.m3.2.3.3.cmml"><mrow id="S4.I2.i3.p1.3.m3.2.3.3.2" xref="S4.I2.i3.p1.3.m3.2.3.3.2.cmml"><mn id="S4.I2.i3.p1.3.m3.2.3.3.2.2" xref="S4.I2.i3.p1.3.m3.2.3.3.2.2.cmml">2</mn><mo id="S4.I2.i3.p1.3.m3.2.3.3.2.1" xref="S4.I2.i3.p1.3.m3.2.3.3.2.1.cmml">⁢</mo><mi id="S4.I2.i3.p1.3.m3.2.3.3.2.3" xref="S4.I2.i3.p1.3.m3.2.3.3.2.3.cmml">n</mi></mrow><mo id="S4.I2.i3.p1.3.m3.2.3.3.1" xref="S4.I2.i3.p1.3.m3.2.3.3.1.cmml">−</mo><mn id="S4.I2.i3.p1.3.m3.2.3.3.3" xref="S4.I2.i3.p1.3.m3.2.3.3.3.cmml">1</mn></mrow><mrow id="S4.I2.i3.p1.3.m3.2.2.2.4" xref="S4.I2.i3.p1.3.m3.2.2.2.3.cmml"><mo id="S4.I2.i3.p1.3.m3.2.2.2.4.1" stretchy="false" xref="S4.I2.i3.p1.3.m3.2.2.2.3.cmml">(</mo><mi id="S4.I2.i3.p1.3.m3.1.1.1.1" xref="S4.I2.i3.p1.3.m3.1.1.1.1.cmml">l</mi><mo id="S4.I2.i3.p1.3.m3.2.2.2.4.2" xref="S4.I2.i3.p1.3.m3.2.2.2.3.cmml">,</mo><mi id="S4.I2.i3.p1.3.m3.2.2.2.2" xref="S4.I2.i3.p1.3.m3.2.2.2.2.cmml">h</mi><mo id="S4.I2.i3.p1.3.m3.2.2.2.4.3" stretchy="false" xref="S4.I2.i3.p1.3.m3.2.2.2.3.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S4.I2.i3.p1.3.m3.2b"><apply id="S4.I2.i3.p1.3.m3.2.3.cmml" xref="S4.I2.i3.p1.3.m3.2.3"><csymbol cd="ambiguous" id="S4.I2.i3.p1.3.m3.2.3.1.cmml" xref="S4.I2.i3.p1.3.m3.2.3">subscript</csymbol><apply id="S4.I2.i3.p1.3.m3.2.3.2.cmml" xref="S4.I2.i3.p1.3.m3.2.3"><csymbol cd="ambiguous" id="S4.I2.i3.p1.3.m3.2.3.2.1.cmml" xref="S4.I2.i3.p1.3.m3.2.3">superscript</csymbol><ci id="S4.I2.i3.p1.3.m3.2.3.2.2a.cmml" xref="S4.I2.i3.p1.3.m3.2.3.2.2"><mtext class="ltx_mathvariant_bold" id="S4.I2.i3.p1.3.m3.2.3.2.2.cmml" xref="S4.I2.i3.p1.3.m3.2.3.2.2">a</mtext></ci><interval closure="open" id="S4.I2.i3.p1.3.m3.2.2.2.3.cmml" xref="S4.I2.i3.p1.3.m3.2.2.2.4"><ci id="S4.I2.i3.p1.3.m3.1.1.1.1.cmml" xref="S4.I2.i3.p1.3.m3.1.1.1.1">𝑙</ci><ci id="S4.I2.i3.p1.3.m3.2.2.2.2.cmml" xref="S4.I2.i3.p1.3.m3.2.2.2.2">ℎ</ci></interval></apply><apply id="S4.I2.i3.p1.3.m3.2.3.3.cmml" xref="S4.I2.i3.p1.3.m3.2.3.3"><minus id="S4.I2.i3.p1.3.m3.2.3.3.1.cmml" xref="S4.I2.i3.p1.3.m3.2.3.3.1"></minus><apply id="S4.I2.i3.p1.3.m3.2.3.3.2.cmml" xref="S4.I2.i3.p1.3.m3.2.3.3.2"><times id="S4.I2.i3.p1.3.m3.2.3.3.2.1.cmml" xref="S4.I2.i3.p1.3.m3.2.3.3.2.1"></times><cn id="S4.I2.i3.p1.3.m3.2.3.3.2.2.cmml" type="integer" xref="S4.I2.i3.p1.3.m3.2.3.3.2.2">2</cn><ci id="S4.I2.i3.p1.3.m3.2.3.3.2.3.cmml" xref="S4.I2.i3.p1.3.m3.2.3.3.2.3">𝑛</ci></apply><cn id="S4.I2.i3.p1.3.m3.2.3.3.3.cmml" type="integer" xref="S4.I2.i3.p1.3.m3.2.3.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i3.p1.3.m3.2c">\textbf{a}^{(l,h)}_{2n-1}</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i3.p1.3.m3.2d">a start_POSTSUPERSCRIPT ( italic_l , italic_h ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 italic_n - 1 end_POSTSUBSCRIPT</annotation></semantics></math> by replacing it with the saved mean vectors.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para ltx_noindent" id="S4.I2.i4.p1">
<p class="ltx_p" id="S4.I2.i4.p1.1">Our intervention measurements are based on the logits difference, defined as:</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.I2.i4.p2">
<table class="ltx_equation ltx_eqn_table" id="S4.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\Delta\pi^{(l,h)}=\frac{\tilde{\pi}^{(l,h)}_{2n-1}[x_{n}]}{\pi_{2n-1}[x_{n}]}-1," class="ltx_Math" display="block" id="S4.E1.m1.7"><semantics id="S4.E1.m1.7a"><mrow id="S4.E1.m1.7.7.1" xref="S4.E1.m1.7.7.1.1.cmml"><mrow id="S4.E1.m1.7.7.1.1" xref="S4.E1.m1.7.7.1.1.cmml"><mrow id="S4.E1.m1.7.7.1.1.2" xref="S4.E1.m1.7.7.1.1.2.cmml"><mi id="S4.E1.m1.7.7.1.1.2.2" mathvariant="normal" xref="S4.E1.m1.7.7.1.1.2.2.cmml">Δ</mi><mo id="S4.E1.m1.7.7.1.1.2.1" xref="S4.E1.m1.7.7.1.1.2.1.cmml">⁢</mo><msup id="S4.E1.m1.7.7.1.1.2.3" xref="S4.E1.m1.7.7.1.1.2.3.cmml"><mi id="S4.E1.m1.7.7.1.1.2.3.2" xref="S4.E1.m1.7.7.1.1.2.3.2.cmml">π</mi><mrow id="S4.E1.m1.2.2.2.4" xref="S4.E1.m1.2.2.2.3.cmml"><mo id="S4.E1.m1.2.2.2.4.1" stretchy="false" xref="S4.E1.m1.2.2.2.3.cmml">(</mo><mi id="S4.E1.m1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.cmml">l</mi><mo id="S4.E1.m1.2.2.2.4.2" xref="S4.E1.m1.2.2.2.3.cmml">,</mo><mi id="S4.E1.m1.2.2.2.2" xref="S4.E1.m1.2.2.2.2.cmml">h</mi><mo id="S4.E1.m1.2.2.2.4.3" stretchy="false" xref="S4.E1.m1.2.2.2.3.cmml">)</mo></mrow></msup></mrow><mo id="S4.E1.m1.7.7.1.1.1" xref="S4.E1.m1.7.7.1.1.1.cmml">=</mo><mrow id="S4.E1.m1.7.7.1.1.3" xref="S4.E1.m1.7.7.1.1.3.cmml"><mfrac id="S4.E1.m1.6.6" xref="S4.E1.m1.6.6.cmml"><mrow id="S4.E1.m1.5.5.3" xref="S4.E1.m1.5.5.3.cmml"><msubsup id="S4.E1.m1.5.5.3.5" xref="S4.E1.m1.5.5.3.5.cmml"><mover accent="true" id="S4.E1.m1.5.5.3.5.2.2" xref="S4.E1.m1.5.5.3.5.2.2.cmml"><mi id="S4.E1.m1.5.5.3.5.2.2.2" xref="S4.E1.m1.5.5.3.5.2.2.2.cmml">π</mi><mo id="S4.E1.m1.5.5.3.5.2.2.1" xref="S4.E1.m1.5.5.3.5.2.2.1.cmml">~</mo></mover><mrow id="S4.E1.m1.5.5.3.5.3" xref="S4.E1.m1.5.5.3.5.3.cmml"><mrow id="S4.E1.m1.5.5.3.5.3.2" xref="S4.E1.m1.5.5.3.5.3.2.cmml"><mn id="S4.E1.m1.5.5.3.5.3.2.2" xref="S4.E1.m1.5.5.3.5.3.2.2.cmml">2</mn><mo id="S4.E1.m1.5.5.3.5.3.2.1" xref="S4.E1.m1.5.5.3.5.3.2.1.cmml">⁢</mo><mi id="S4.E1.m1.5.5.3.5.3.2.3" xref="S4.E1.m1.5.5.3.5.3.2.3.cmml">n</mi></mrow><mo id="S4.E1.m1.5.5.3.5.3.1" xref="S4.E1.m1.5.5.3.5.3.1.cmml">−</mo><mn id="S4.E1.m1.5.5.3.5.3.3" xref="S4.E1.m1.5.5.3.5.3.3.cmml">1</mn></mrow><mrow id="S4.E1.m1.4.4.2.2.2.4" xref="S4.E1.m1.4.4.2.2.2.3.cmml"><mo id="S4.E1.m1.4.4.2.2.2.4.1" stretchy="false" xref="S4.E1.m1.4.4.2.2.2.3.cmml">(</mo><mi id="S4.E1.m1.3.3.1.1.1.1" xref="S4.E1.m1.3.3.1.1.1.1.cmml">l</mi><mo id="S4.E1.m1.4.4.2.2.2.4.2" xref="S4.E1.m1.4.4.2.2.2.3.cmml">,</mo><mi id="S4.E1.m1.4.4.2.2.2.2" xref="S4.E1.m1.4.4.2.2.2.2.cmml">h</mi><mo id="S4.E1.m1.4.4.2.2.2.4.3" stretchy="false" xref="S4.E1.m1.4.4.2.2.2.3.cmml">)</mo></mrow></msubsup><mo id="S4.E1.m1.5.5.3.4" xref="S4.E1.m1.5.5.3.4.cmml">⁢</mo><mrow id="S4.E1.m1.5.5.3.3.1" xref="S4.E1.m1.5.5.3.3.2.cmml"><mo id="S4.E1.m1.5.5.3.3.1.2" stretchy="false" xref="S4.E1.m1.5.5.3.3.2.1.cmml">[</mo><msub id="S4.E1.m1.5.5.3.3.1.1" xref="S4.E1.m1.5.5.3.3.1.1.cmml"><mi id="S4.E1.m1.5.5.3.3.1.1.2" xref="S4.E1.m1.5.5.3.3.1.1.2.cmml">x</mi><mi id="S4.E1.m1.5.5.3.3.1.1.3" xref="S4.E1.m1.5.5.3.3.1.1.3.cmml">n</mi></msub><mo id="S4.E1.m1.5.5.3.3.1.3" stretchy="false" xref="S4.E1.m1.5.5.3.3.2.1.cmml">]</mo></mrow></mrow><mrow id="S4.E1.m1.6.6.4" xref="S4.E1.m1.6.6.4.cmml"><msub id="S4.E1.m1.6.6.4.3" xref="S4.E1.m1.6.6.4.3.cmml"><mi id="S4.E1.m1.6.6.4.3.2" xref="S4.E1.m1.6.6.4.3.2.cmml">π</mi><mrow id="S4.E1.m1.6.6.4.3.3" xref="S4.E1.m1.6.6.4.3.3.cmml"><mrow id="S4.E1.m1.6.6.4.3.3.2" xref="S4.E1.m1.6.6.4.3.3.2.cmml"><mn id="S4.E1.m1.6.6.4.3.3.2.2" xref="S4.E1.m1.6.6.4.3.3.2.2.cmml">2</mn><mo id="S4.E1.m1.6.6.4.3.3.2.1" xref="S4.E1.m1.6.6.4.3.3.2.1.cmml">⁢</mo><mi id="S4.E1.m1.6.6.4.3.3.2.3" xref="S4.E1.m1.6.6.4.3.3.2.3.cmml">n</mi></mrow><mo id="S4.E1.m1.6.6.4.3.3.1" xref="S4.E1.m1.6.6.4.3.3.1.cmml">−</mo><mn id="S4.E1.m1.6.6.4.3.3.3" xref="S4.E1.m1.6.6.4.3.3.3.cmml">1</mn></mrow></msub><mo id="S4.E1.m1.6.6.4.2" xref="S4.E1.m1.6.6.4.2.cmml">⁢</mo><mrow id="S4.E1.m1.6.6.4.1.1" xref="S4.E1.m1.6.6.4.1.2.cmml"><mo id="S4.E1.m1.6.6.4.1.1.2" stretchy="false" xref="S4.E1.m1.6.6.4.1.2.1.cmml">[</mo><msub id="S4.E1.m1.6.6.4.1.1.1" xref="S4.E1.m1.6.6.4.1.1.1.cmml"><mi id="S4.E1.m1.6.6.4.1.1.1.2" xref="S4.E1.m1.6.6.4.1.1.1.2.cmml">x</mi><mi id="S4.E1.m1.6.6.4.1.1.1.3" xref="S4.E1.m1.6.6.4.1.1.1.3.cmml">n</mi></msub><mo id="S4.E1.m1.6.6.4.1.1.3" stretchy="false" xref="S4.E1.m1.6.6.4.1.2.1.cmml">]</mo></mrow></mrow></mfrac><mo id="S4.E1.m1.7.7.1.1.3.1" xref="S4.E1.m1.7.7.1.1.3.1.cmml">−</mo><mn id="S4.E1.m1.7.7.1.1.3.2" xref="S4.E1.m1.7.7.1.1.3.2.cmml">1</mn></mrow></mrow><mo id="S4.E1.m1.7.7.1.2" xref="S4.E1.m1.7.7.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.7b"><apply id="S4.E1.m1.7.7.1.1.cmml" xref="S4.E1.m1.7.7.1"><eq id="S4.E1.m1.7.7.1.1.1.cmml" xref="S4.E1.m1.7.7.1.1.1"></eq><apply id="S4.E1.m1.7.7.1.1.2.cmml" xref="S4.E1.m1.7.7.1.1.2"><times id="S4.E1.m1.7.7.1.1.2.1.cmml" xref="S4.E1.m1.7.7.1.1.2.1"></times><ci id="S4.E1.m1.7.7.1.1.2.2.cmml" xref="S4.E1.m1.7.7.1.1.2.2">Δ</ci><apply id="S4.E1.m1.7.7.1.1.2.3.cmml" xref="S4.E1.m1.7.7.1.1.2.3"><csymbol cd="ambiguous" id="S4.E1.m1.7.7.1.1.2.3.1.cmml" xref="S4.E1.m1.7.7.1.1.2.3">superscript</csymbol><ci id="S4.E1.m1.7.7.1.1.2.3.2.cmml" xref="S4.E1.m1.7.7.1.1.2.3.2">𝜋</ci><interval closure="open" id="S4.E1.m1.2.2.2.3.cmml" xref="S4.E1.m1.2.2.2.4"><ci id="S4.E1.m1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1">𝑙</ci><ci id="S4.E1.m1.2.2.2.2.cmml" xref="S4.E1.m1.2.2.2.2">ℎ</ci></interval></apply></apply><apply id="S4.E1.m1.7.7.1.1.3.cmml" xref="S4.E1.m1.7.7.1.1.3"><minus id="S4.E1.m1.7.7.1.1.3.1.cmml" xref="S4.E1.m1.7.7.1.1.3.1"></minus><apply id="S4.E1.m1.6.6.cmml" xref="S4.E1.m1.6.6"><divide id="S4.E1.m1.6.6.5.cmml" xref="S4.E1.m1.6.6"></divide><apply id="S4.E1.m1.5.5.3.cmml" xref="S4.E1.m1.5.5.3"><times id="S4.E1.m1.5.5.3.4.cmml" xref="S4.E1.m1.5.5.3.4"></times><apply id="S4.E1.m1.5.5.3.5.cmml" xref="S4.E1.m1.5.5.3.5"><csymbol cd="ambiguous" id="S4.E1.m1.5.5.3.5.1.cmml" xref="S4.E1.m1.5.5.3.5">subscript</csymbol><apply id="S4.E1.m1.5.5.3.5.2.cmml" xref="S4.E1.m1.5.5.3.5"><csymbol cd="ambiguous" id="S4.E1.m1.5.5.3.5.2.1.cmml" xref="S4.E1.m1.5.5.3.5">superscript</csymbol><apply id="S4.E1.m1.5.5.3.5.2.2.cmml" xref="S4.E1.m1.5.5.3.5.2.2"><ci id="S4.E1.m1.5.5.3.5.2.2.1.cmml" xref="S4.E1.m1.5.5.3.5.2.2.1">~</ci><ci id="S4.E1.m1.5.5.3.5.2.2.2.cmml" xref="S4.E1.m1.5.5.3.5.2.2.2">𝜋</ci></apply><interval closure="open" id="S4.E1.m1.4.4.2.2.2.3.cmml" xref="S4.E1.m1.4.4.2.2.2.4"><ci id="S4.E1.m1.3.3.1.1.1.1.cmml" xref="S4.E1.m1.3.3.1.1.1.1">𝑙</ci><ci id="S4.E1.m1.4.4.2.2.2.2.cmml" xref="S4.E1.m1.4.4.2.2.2.2">ℎ</ci></interval></apply><apply id="S4.E1.m1.5.5.3.5.3.cmml" xref="S4.E1.m1.5.5.3.5.3"><minus id="S4.E1.m1.5.5.3.5.3.1.cmml" xref="S4.E1.m1.5.5.3.5.3.1"></minus><apply id="S4.E1.m1.5.5.3.5.3.2.cmml" xref="S4.E1.m1.5.5.3.5.3.2"><times id="S4.E1.m1.5.5.3.5.3.2.1.cmml" xref="S4.E1.m1.5.5.3.5.3.2.1"></times><cn id="S4.E1.m1.5.5.3.5.3.2.2.cmml" type="integer" xref="S4.E1.m1.5.5.3.5.3.2.2">2</cn><ci id="S4.E1.m1.5.5.3.5.3.2.3.cmml" xref="S4.E1.m1.5.5.3.5.3.2.3">𝑛</ci></apply><cn id="S4.E1.m1.5.5.3.5.3.3.cmml" type="integer" xref="S4.E1.m1.5.5.3.5.3.3">1</cn></apply></apply><apply id="S4.E1.m1.5.5.3.3.2.cmml" xref="S4.E1.m1.5.5.3.3.1"><csymbol cd="latexml" id="S4.E1.m1.5.5.3.3.2.1.cmml" xref="S4.E1.m1.5.5.3.3.1.2">delimited-[]</csymbol><apply id="S4.E1.m1.5.5.3.3.1.1.cmml" xref="S4.E1.m1.5.5.3.3.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.5.5.3.3.1.1.1.cmml" xref="S4.E1.m1.5.5.3.3.1.1">subscript</csymbol><ci id="S4.E1.m1.5.5.3.3.1.1.2.cmml" xref="S4.E1.m1.5.5.3.3.1.1.2">𝑥</ci><ci id="S4.E1.m1.5.5.3.3.1.1.3.cmml" xref="S4.E1.m1.5.5.3.3.1.1.3">𝑛</ci></apply></apply></apply><apply id="S4.E1.m1.6.6.4.cmml" xref="S4.E1.m1.6.6.4"><times id="S4.E1.m1.6.6.4.2.cmml" xref="S4.E1.m1.6.6.4.2"></times><apply id="S4.E1.m1.6.6.4.3.cmml" xref="S4.E1.m1.6.6.4.3"><csymbol cd="ambiguous" id="S4.E1.m1.6.6.4.3.1.cmml" xref="S4.E1.m1.6.6.4.3">subscript</csymbol><ci id="S4.E1.m1.6.6.4.3.2.cmml" xref="S4.E1.m1.6.6.4.3.2">𝜋</ci><apply id="S4.E1.m1.6.6.4.3.3.cmml" xref="S4.E1.m1.6.6.4.3.3"><minus id="S4.E1.m1.6.6.4.3.3.1.cmml" xref="S4.E1.m1.6.6.4.3.3.1"></minus><apply id="S4.E1.m1.6.6.4.3.3.2.cmml" xref="S4.E1.m1.6.6.4.3.3.2"><times id="S4.E1.m1.6.6.4.3.3.2.1.cmml" xref="S4.E1.m1.6.6.4.3.3.2.1"></times><cn id="S4.E1.m1.6.6.4.3.3.2.2.cmml" type="integer" xref="S4.E1.m1.6.6.4.3.3.2.2">2</cn><ci id="S4.E1.m1.6.6.4.3.3.2.3.cmml" xref="S4.E1.m1.6.6.4.3.3.2.3">𝑛</ci></apply><cn id="S4.E1.m1.6.6.4.3.3.3.cmml" type="integer" xref="S4.E1.m1.6.6.4.3.3.3">1</cn></apply></apply><apply id="S4.E1.m1.6.6.4.1.2.cmml" xref="S4.E1.m1.6.6.4.1.1"><csymbol cd="latexml" id="S4.E1.m1.6.6.4.1.2.1.cmml" xref="S4.E1.m1.6.6.4.1.1.2">delimited-[]</csymbol><apply id="S4.E1.m1.6.6.4.1.1.1.cmml" xref="S4.E1.m1.6.6.4.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.6.6.4.1.1.1.1.cmml" xref="S4.E1.m1.6.6.4.1.1.1">subscript</csymbol><ci id="S4.E1.m1.6.6.4.1.1.1.2.cmml" xref="S4.E1.m1.6.6.4.1.1.1.2">𝑥</ci><ci id="S4.E1.m1.6.6.4.1.1.1.3.cmml" xref="S4.E1.m1.6.6.4.1.1.1.3">𝑛</ci></apply></apply></apply></apply><cn id="S4.E1.m1.7.7.1.1.3.2.cmml" type="integer" xref="S4.E1.m1.7.7.1.1.3.2">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.7c">\Delta\pi^{(l,h)}=\frac{\tilde{\pi}^{(l,h)}_{2n-1}[x_{n}]}{\pi_{2n-1}[x_{n}]}-1,</annotation><annotation encoding="application/x-llamapun" id="S4.E1.m1.7d">roman_Δ italic_π start_POSTSUPERSCRIPT ( italic_l , italic_h ) end_POSTSUPERSCRIPT = divide start_ARG over~ start_ARG italic_π end_ARG start_POSTSUPERSCRIPT ( italic_l , italic_h ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 italic_n - 1 end_POSTSUBSCRIPT [ italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ] end_ARG start_ARG italic_π start_POSTSUBSCRIPT 2 italic_n - 1 end_POSTSUBSCRIPT [ italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ] end_ARG - 1 ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.I2.i4.p3">
<p class="ltx_p" id="S4.I2.i4.p3.7">where <math alttext="\pi_{2n-1}" class="ltx_Math" display="inline" id="S4.I2.i4.p3.1.m1.1"><semantics id="S4.I2.i4.p3.1.m1.1a"><msub id="S4.I2.i4.p3.1.m1.1.1" xref="S4.I2.i4.p3.1.m1.1.1.cmml"><mi id="S4.I2.i4.p3.1.m1.1.1.2" xref="S4.I2.i4.p3.1.m1.1.1.2.cmml">π</mi><mrow id="S4.I2.i4.p3.1.m1.1.1.3" xref="S4.I2.i4.p3.1.m1.1.1.3.cmml"><mrow id="S4.I2.i4.p3.1.m1.1.1.3.2" xref="S4.I2.i4.p3.1.m1.1.1.3.2.cmml"><mn id="S4.I2.i4.p3.1.m1.1.1.3.2.2" xref="S4.I2.i4.p3.1.m1.1.1.3.2.2.cmml">2</mn><mo id="S4.I2.i4.p3.1.m1.1.1.3.2.1" xref="S4.I2.i4.p3.1.m1.1.1.3.2.1.cmml">⁢</mo><mi id="S4.I2.i4.p3.1.m1.1.1.3.2.3" xref="S4.I2.i4.p3.1.m1.1.1.3.2.3.cmml">n</mi></mrow><mo id="S4.I2.i4.p3.1.m1.1.1.3.1" xref="S4.I2.i4.p3.1.m1.1.1.3.1.cmml">−</mo><mn id="S4.I2.i4.p3.1.m1.1.1.3.3" xref="S4.I2.i4.p3.1.m1.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.I2.i4.p3.1.m1.1b"><apply id="S4.I2.i4.p3.1.m1.1.1.cmml" xref="S4.I2.i4.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.I2.i4.p3.1.m1.1.1.1.cmml" xref="S4.I2.i4.p3.1.m1.1.1">subscript</csymbol><ci id="S4.I2.i4.p3.1.m1.1.1.2.cmml" xref="S4.I2.i4.p3.1.m1.1.1.2">𝜋</ci><apply id="S4.I2.i4.p3.1.m1.1.1.3.cmml" xref="S4.I2.i4.p3.1.m1.1.1.3"><minus id="S4.I2.i4.p3.1.m1.1.1.3.1.cmml" xref="S4.I2.i4.p3.1.m1.1.1.3.1"></minus><apply id="S4.I2.i4.p3.1.m1.1.1.3.2.cmml" xref="S4.I2.i4.p3.1.m1.1.1.3.2"><times id="S4.I2.i4.p3.1.m1.1.1.3.2.1.cmml" xref="S4.I2.i4.p3.1.m1.1.1.3.2.1"></times><cn id="S4.I2.i4.p3.1.m1.1.1.3.2.2.cmml" type="integer" xref="S4.I2.i4.p3.1.m1.1.1.3.2.2">2</cn><ci id="S4.I2.i4.p3.1.m1.1.1.3.2.3.cmml" xref="S4.I2.i4.p3.1.m1.1.1.3.2.3">𝑛</ci></apply><cn id="S4.I2.i4.p3.1.m1.1.1.3.3.cmml" type="integer" xref="S4.I2.i4.p3.1.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i4.p3.1.m1.1c">\pi_{2n-1}</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i4.p3.1.m1.1d">italic_π start_POSTSUBSCRIPT 2 italic_n - 1 end_POSTSUBSCRIPT</annotation></semantics></math> represents the final logits at position <math alttext="2n-1" class="ltx_Math" display="inline" id="S4.I2.i4.p3.2.m2.1"><semantics id="S4.I2.i4.p3.2.m2.1a"><mrow id="S4.I2.i4.p3.2.m2.1.1" xref="S4.I2.i4.p3.2.m2.1.1.cmml"><mrow id="S4.I2.i4.p3.2.m2.1.1.2" xref="S4.I2.i4.p3.2.m2.1.1.2.cmml"><mn id="S4.I2.i4.p3.2.m2.1.1.2.2" xref="S4.I2.i4.p3.2.m2.1.1.2.2.cmml">2</mn><mo id="S4.I2.i4.p3.2.m2.1.1.2.1" xref="S4.I2.i4.p3.2.m2.1.1.2.1.cmml">⁢</mo><mi id="S4.I2.i4.p3.2.m2.1.1.2.3" xref="S4.I2.i4.p3.2.m2.1.1.2.3.cmml">n</mi></mrow><mo id="S4.I2.i4.p3.2.m2.1.1.1" xref="S4.I2.i4.p3.2.m2.1.1.1.cmml">−</mo><mn id="S4.I2.i4.p3.2.m2.1.1.3" xref="S4.I2.i4.p3.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.I2.i4.p3.2.m2.1b"><apply id="S4.I2.i4.p3.2.m2.1.1.cmml" xref="S4.I2.i4.p3.2.m2.1.1"><minus id="S4.I2.i4.p3.2.m2.1.1.1.cmml" xref="S4.I2.i4.p3.2.m2.1.1.1"></minus><apply id="S4.I2.i4.p3.2.m2.1.1.2.cmml" xref="S4.I2.i4.p3.2.m2.1.1.2"><times id="S4.I2.i4.p3.2.m2.1.1.2.1.cmml" xref="S4.I2.i4.p3.2.m2.1.1.2.1"></times><cn id="S4.I2.i4.p3.2.m2.1.1.2.2.cmml" type="integer" xref="S4.I2.i4.p3.2.m2.1.1.2.2">2</cn><ci id="S4.I2.i4.p3.2.m2.1.1.2.3.cmml" xref="S4.I2.i4.p3.2.m2.1.1.2.3">𝑛</ci></apply><cn id="S4.I2.i4.p3.2.m2.1.1.3.cmml" type="integer" xref="S4.I2.i4.p3.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i4.p3.2.m2.1c">2n-1</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i4.p3.2.m2.1d">2 italic_n - 1</annotation></semantics></math> during the <span class="ltx_text ltx_font_italic" id="S4.I2.i4.p3.7.1">normal run</span>, and <math alttext="[x_{n}]" class="ltx_Math" display="inline" id="S4.I2.i4.p3.3.m3.1"><semantics id="S4.I2.i4.p3.3.m3.1a"><mrow id="S4.I2.i4.p3.3.m3.1.1.1" xref="S4.I2.i4.p3.3.m3.1.1.2.cmml"><mo id="S4.I2.i4.p3.3.m3.1.1.1.2" stretchy="false" xref="S4.I2.i4.p3.3.m3.1.1.2.1.cmml">[</mo><msub id="S4.I2.i4.p3.3.m3.1.1.1.1" xref="S4.I2.i4.p3.3.m3.1.1.1.1.cmml"><mi id="S4.I2.i4.p3.3.m3.1.1.1.1.2" xref="S4.I2.i4.p3.3.m3.1.1.1.1.2.cmml">x</mi><mi id="S4.I2.i4.p3.3.m3.1.1.1.1.3" xref="S4.I2.i4.p3.3.m3.1.1.1.1.3.cmml">n</mi></msub><mo id="S4.I2.i4.p3.3.m3.1.1.1.3" stretchy="false" xref="S4.I2.i4.p3.3.m3.1.1.2.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.I2.i4.p3.3.m3.1b"><apply id="S4.I2.i4.p3.3.m3.1.1.2.cmml" xref="S4.I2.i4.p3.3.m3.1.1.1"><csymbol cd="latexml" id="S4.I2.i4.p3.3.m3.1.1.2.1.cmml" xref="S4.I2.i4.p3.3.m3.1.1.1.2">delimited-[]</csymbol><apply id="S4.I2.i4.p3.3.m3.1.1.1.1.cmml" xref="S4.I2.i4.p3.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S4.I2.i4.p3.3.m3.1.1.1.1.1.cmml" xref="S4.I2.i4.p3.3.m3.1.1.1.1">subscript</csymbol><ci id="S4.I2.i4.p3.3.m3.1.1.1.1.2.cmml" xref="S4.I2.i4.p3.3.m3.1.1.1.1.2">𝑥</ci><ci id="S4.I2.i4.p3.3.m3.1.1.1.1.3.cmml" xref="S4.I2.i4.p3.3.m3.1.1.1.1.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i4.p3.3.m3.1c">[x_{n}]</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i4.p3.3.m3.1d">[ italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ]</annotation></semantics></math> denotes selecting the value of the token <math alttext="x_{n}" class="ltx_Math" display="inline" id="S4.I2.i4.p3.4.m4.1"><semantics id="S4.I2.i4.p3.4.m4.1a"><msub id="S4.I2.i4.p3.4.m4.1.1" xref="S4.I2.i4.p3.4.m4.1.1.cmml"><mi id="S4.I2.i4.p3.4.m4.1.1.2" xref="S4.I2.i4.p3.4.m4.1.1.2.cmml">x</mi><mi id="S4.I2.i4.p3.4.m4.1.1.3" xref="S4.I2.i4.p3.4.m4.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S4.I2.i4.p3.4.m4.1b"><apply id="S4.I2.i4.p3.4.m4.1.1.cmml" xref="S4.I2.i4.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S4.I2.i4.p3.4.m4.1.1.1.cmml" xref="S4.I2.i4.p3.4.m4.1.1">subscript</csymbol><ci id="S4.I2.i4.p3.4.m4.1.1.2.cmml" xref="S4.I2.i4.p3.4.m4.1.1.2">𝑥</ci><ci id="S4.I2.i4.p3.4.m4.1.1.3.cmml" xref="S4.I2.i4.p3.4.m4.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i4.p3.4.m4.1c">x_{n}</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i4.p3.4.m4.1d">italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> from the logits.
<math alttext="\tilde{\pi}^{(l,h)}" class="ltx_Math" display="inline" id="S4.I2.i4.p3.5.m5.2"><semantics id="S4.I2.i4.p3.5.m5.2a"><msup id="S4.I2.i4.p3.5.m5.2.3" xref="S4.I2.i4.p3.5.m5.2.3.cmml"><mover accent="true" id="S4.I2.i4.p3.5.m5.2.3.2" xref="S4.I2.i4.p3.5.m5.2.3.2.cmml"><mi id="S4.I2.i4.p3.5.m5.2.3.2.2" xref="S4.I2.i4.p3.5.m5.2.3.2.2.cmml">π</mi><mo id="S4.I2.i4.p3.5.m5.2.3.2.1" xref="S4.I2.i4.p3.5.m5.2.3.2.1.cmml">~</mo></mover><mrow id="S4.I2.i4.p3.5.m5.2.2.2.4" xref="S4.I2.i4.p3.5.m5.2.2.2.3.cmml"><mo id="S4.I2.i4.p3.5.m5.2.2.2.4.1" stretchy="false" xref="S4.I2.i4.p3.5.m5.2.2.2.3.cmml">(</mo><mi id="S4.I2.i4.p3.5.m5.1.1.1.1" xref="S4.I2.i4.p3.5.m5.1.1.1.1.cmml">l</mi><mo id="S4.I2.i4.p3.5.m5.2.2.2.4.2" xref="S4.I2.i4.p3.5.m5.2.2.2.3.cmml">,</mo><mi id="S4.I2.i4.p3.5.m5.2.2.2.2" xref="S4.I2.i4.p3.5.m5.2.2.2.2.cmml">h</mi><mo id="S4.I2.i4.p3.5.m5.2.2.2.4.3" stretchy="false" xref="S4.I2.i4.p3.5.m5.2.2.2.3.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.I2.i4.p3.5.m5.2b"><apply id="S4.I2.i4.p3.5.m5.2.3.cmml" xref="S4.I2.i4.p3.5.m5.2.3"><csymbol cd="ambiguous" id="S4.I2.i4.p3.5.m5.2.3.1.cmml" xref="S4.I2.i4.p3.5.m5.2.3">superscript</csymbol><apply id="S4.I2.i4.p3.5.m5.2.3.2.cmml" xref="S4.I2.i4.p3.5.m5.2.3.2"><ci id="S4.I2.i4.p3.5.m5.2.3.2.1.cmml" xref="S4.I2.i4.p3.5.m5.2.3.2.1">~</ci><ci id="S4.I2.i4.p3.5.m5.2.3.2.2.cmml" xref="S4.I2.i4.p3.5.m5.2.3.2.2">𝜋</ci></apply><interval closure="open" id="S4.I2.i4.p3.5.m5.2.2.2.3.cmml" xref="S4.I2.i4.p3.5.m5.2.2.2.4"><ci id="S4.I2.i4.p3.5.m5.1.1.1.1.cmml" xref="S4.I2.i4.p3.5.m5.1.1.1.1">𝑙</ci><ci id="S4.I2.i4.p3.5.m5.2.2.2.2.cmml" xref="S4.I2.i4.p3.5.m5.2.2.2.2">ℎ</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i4.p3.5.m5.2c">\tilde{\pi}^{(l,h)}</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i4.p3.5.m5.2d">over~ start_ARG italic_π end_ARG start_POSTSUPERSCRIPT ( italic_l , italic_h ) end_POSTSUPERSCRIPT</annotation></semantics></math> indicates the logits after intervention on <math alttext="A^{(l,h)}" class="ltx_Math" display="inline" id="S4.I2.i4.p3.6.m6.2"><semantics id="S4.I2.i4.p3.6.m6.2a"><msup id="S4.I2.i4.p3.6.m6.2.3" xref="S4.I2.i4.p3.6.m6.2.3.cmml"><mi id="S4.I2.i4.p3.6.m6.2.3.2" xref="S4.I2.i4.p3.6.m6.2.3.2.cmml">A</mi><mrow id="S4.I2.i4.p3.6.m6.2.2.2.4" xref="S4.I2.i4.p3.6.m6.2.2.2.3.cmml"><mo id="S4.I2.i4.p3.6.m6.2.2.2.4.1" stretchy="false" xref="S4.I2.i4.p3.6.m6.2.2.2.3.cmml">(</mo><mi id="S4.I2.i4.p3.6.m6.1.1.1.1" xref="S4.I2.i4.p3.6.m6.1.1.1.1.cmml">l</mi><mo id="S4.I2.i4.p3.6.m6.2.2.2.4.2" xref="S4.I2.i4.p3.6.m6.2.2.2.3.cmml">,</mo><mi id="S4.I2.i4.p3.6.m6.2.2.2.2" xref="S4.I2.i4.p3.6.m6.2.2.2.2.cmml">h</mi><mo id="S4.I2.i4.p3.6.m6.2.2.2.4.3" stretchy="false" xref="S4.I2.i4.p3.6.m6.2.2.2.3.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.I2.i4.p3.6.m6.2b"><apply id="S4.I2.i4.p3.6.m6.2.3.cmml" xref="S4.I2.i4.p3.6.m6.2.3"><csymbol cd="ambiguous" id="S4.I2.i4.p3.6.m6.2.3.1.cmml" xref="S4.I2.i4.p3.6.m6.2.3">superscript</csymbol><ci id="S4.I2.i4.p3.6.m6.2.3.2.cmml" xref="S4.I2.i4.p3.6.m6.2.3.2">𝐴</ci><interval closure="open" id="S4.I2.i4.p3.6.m6.2.2.2.3.cmml" xref="S4.I2.i4.p3.6.m6.2.2.2.4"><ci id="S4.I2.i4.p3.6.m6.1.1.1.1.cmml" xref="S4.I2.i4.p3.6.m6.1.1.1.1">𝑙</ci><ci id="S4.I2.i4.p3.6.m6.2.2.2.2.cmml" xref="S4.I2.i4.p3.6.m6.2.2.2.2">ℎ</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i4.p3.6.m6.2c">A^{(l,h)}</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i4.p3.6.m6.2d">italic_A start_POSTSUPERSCRIPT ( italic_l , italic_h ) end_POSTSUPERSCRIPT</annotation></semantics></math>.
We contend that a higher value of this metric suggests a stronger suppression effect from <math alttext="A^{(l,h)}" class="ltx_Math" display="inline" id="S4.I2.i4.p3.7.m7.2"><semantics id="S4.I2.i4.p3.7.m7.2a"><msup id="S4.I2.i4.p3.7.m7.2.3" xref="S4.I2.i4.p3.7.m7.2.3.cmml"><mi id="S4.I2.i4.p3.7.m7.2.3.2" xref="S4.I2.i4.p3.7.m7.2.3.2.cmml">A</mi><mrow id="S4.I2.i4.p3.7.m7.2.2.2.4" xref="S4.I2.i4.p3.7.m7.2.2.2.3.cmml"><mo id="S4.I2.i4.p3.7.m7.2.2.2.4.1" stretchy="false" xref="S4.I2.i4.p3.7.m7.2.2.2.3.cmml">(</mo><mi id="S4.I2.i4.p3.7.m7.1.1.1.1" xref="S4.I2.i4.p3.7.m7.1.1.1.1.cmml">l</mi><mo id="S4.I2.i4.p3.7.m7.2.2.2.4.2" xref="S4.I2.i4.p3.7.m7.2.2.2.3.cmml">,</mo><mi id="S4.I2.i4.p3.7.m7.2.2.2.2" xref="S4.I2.i4.p3.7.m7.2.2.2.2.cmml">h</mi><mo id="S4.I2.i4.p3.7.m7.2.2.2.4.3" stretchy="false" xref="S4.I2.i4.p3.7.m7.2.2.2.3.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.I2.i4.p3.7.m7.2b"><apply id="S4.I2.i4.p3.7.m7.2.3.cmml" xref="S4.I2.i4.p3.7.m7.2.3"><csymbol cd="ambiguous" id="S4.I2.i4.p3.7.m7.2.3.1.cmml" xref="S4.I2.i4.p3.7.m7.2.3">superscript</csymbol><ci id="S4.I2.i4.p3.7.m7.2.3.2.cmml" xref="S4.I2.i4.p3.7.m7.2.3.2">𝐴</ci><interval closure="open" id="S4.I2.i4.p3.7.m7.2.2.2.3.cmml" xref="S4.I2.i4.p3.7.m7.2.2.2.4"><ci id="S4.I2.i4.p3.7.m7.1.1.1.1.cmml" xref="S4.I2.i4.p3.7.m7.1.1.1.1">𝑙</ci><ci id="S4.I2.i4.p3.7.m7.2.2.2.2.cmml" xref="S4.I2.i4.p3.7.m7.2.2.2.2">ℎ</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i4.p3.7.m7.2c">A^{(l,h)}</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i4.p3.7.m7.2d">italic_A start_POSTSUPERSCRIPT ( italic_l , italic_h ) end_POSTSUPERSCRIPT</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S4.I2.i5.p1">
<p class="ltx_p" id="S4.I2.i5.p1.1">For an arbitrary LLM, we repeat the proxy task multiple times with varying values of <math alttext="n" class="ltx_Math" display="inline" id="S4.I2.i5.p1.1.m1.1"><semantics id="S4.I2.i5.p1.1.m1.1a"><mi id="S4.I2.i5.p1.1.m1.1.1" xref="S4.I2.i5.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.I2.i5.p1.1.m1.1b"><ci id="S4.I2.i5.p1.1.m1.1.1.cmml" xref="S4.I2.i5.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i5.p1.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i5.p1.1.m1.1d">italic_n</annotation></semantics></math> to mitigate bias in context length.
The final metric score for each head is the average of the results from these repeated experiments.
The detailed setup is provided in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S5.SS1" title="5.1 Setup ‣ 5 Experiments ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_tag">5.1</span></a>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para ltx_noindent" id="S4.I2.i6.p1">
<p class="ltx_p" id="S4.I2.i6.p1.2">Based on the final metric scores, we identify the heads with the top-<math alttext="K" class="ltx_Math" display="inline" id="S4.I2.i6.p1.1.m1.1"><semantics id="S4.I2.i6.p1.1.m1.1a"><mi id="S4.I2.i6.p1.1.m1.1.1" xref="S4.I2.i6.p1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.I2.i6.p1.1.m1.1b"><ci id="S4.I2.i6.p1.1.m1.1.1.cmml" xref="S4.I2.i6.p1.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i6.p1.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i6.p1.1.m1.1d">italic_K</annotation></semantics></math> most negative influence on the proxy task as a set <math alttext="\mathcal{S}" class="ltx_Math" display="inline" id="S4.I2.i6.p1.2.m2.1"><semantics id="S4.I2.i6.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S4.I2.i6.p1.2.m2.1.1" xref="S4.I2.i6.p1.2.m2.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="S4.I2.i6.p1.2.m2.1b"><ci id="S4.I2.i6.p1.2.m2.1.1.cmml" xref="S4.I2.i6.p1.2.m2.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i6.p1.2.m2.1c">\mathcal{S}</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i6.p1.2.m2.1d">caligraphic_S</annotation></semantics></math>, defined as:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{S}=\{A^{(l,h)}|A^{(l,h)}\text{ has one of the top-$K$ values of }%
\Delta\pi^{(l,h)}\}." class="ltx_Math" display="block" id="S4.Ex1.m1.8"><semantics id="S4.Ex1.m1.8a"><mrow id="S4.Ex1.m1.8.8.1" xref="S4.Ex1.m1.8.8.1.1.cmml"><mrow id="S4.Ex1.m1.8.8.1.1" xref="S4.Ex1.m1.8.8.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.Ex1.m1.8.8.1.1.4" xref="S4.Ex1.m1.8.8.1.1.4.cmml">𝒮</mi><mo id="S4.Ex1.m1.8.8.1.1.3" xref="S4.Ex1.m1.8.8.1.1.3.cmml">=</mo><mrow id="S4.Ex1.m1.8.8.1.1.2.2" xref="S4.Ex1.m1.8.8.1.1.2.3.cmml"><mo id="S4.Ex1.m1.8.8.1.1.2.2.3" stretchy="false" xref="S4.Ex1.m1.8.8.1.1.2.3.1.cmml">{</mo><msup id="S4.Ex1.m1.8.8.1.1.1.1.1" xref="S4.Ex1.m1.8.8.1.1.1.1.1.cmml"><mi id="S4.Ex1.m1.8.8.1.1.1.1.1.2" xref="S4.Ex1.m1.8.8.1.1.1.1.1.2.cmml">A</mi><mrow id="S4.Ex1.m1.3.3.2.4" xref="S4.Ex1.m1.3.3.2.3.cmml"><mo id="S4.Ex1.m1.3.3.2.4.1" stretchy="false" xref="S4.Ex1.m1.3.3.2.3.cmml">(</mo><mi id="S4.Ex1.m1.2.2.1.1" xref="S4.Ex1.m1.2.2.1.1.cmml">l</mi><mo id="S4.Ex1.m1.3.3.2.4.2" xref="S4.Ex1.m1.3.3.2.3.cmml">,</mo><mi id="S4.Ex1.m1.3.3.2.2" xref="S4.Ex1.m1.3.3.2.2.cmml">h</mi><mo id="S4.Ex1.m1.3.3.2.4.3" stretchy="false" xref="S4.Ex1.m1.3.3.2.3.cmml">)</mo></mrow></msup><mo id="S4.Ex1.m1.8.8.1.1.2.2.4" lspace="0em" rspace="0em" xref="S4.Ex1.m1.8.8.1.1.2.3.1.cmml">|</mo><mrow id="S4.Ex1.m1.8.8.1.1.2.2.2" xref="S4.Ex1.m1.8.8.1.1.2.2.2.cmml"><msup id="S4.Ex1.m1.8.8.1.1.2.2.2.2" xref="S4.Ex1.m1.8.8.1.1.2.2.2.2.cmml"><mi id="S4.Ex1.m1.8.8.1.1.2.2.2.2.2" xref="S4.Ex1.m1.8.8.1.1.2.2.2.2.2.cmml">A</mi><mrow id="S4.Ex1.m1.5.5.2.4" xref="S4.Ex1.m1.5.5.2.3.cmml"><mo id="S4.Ex1.m1.5.5.2.4.1" stretchy="false" xref="S4.Ex1.m1.5.5.2.3.cmml">(</mo><mi id="S4.Ex1.m1.4.4.1.1" xref="S4.Ex1.m1.4.4.1.1.cmml">l</mi><mo id="S4.Ex1.m1.5.5.2.4.2" xref="S4.Ex1.m1.5.5.2.3.cmml">,</mo><mi id="S4.Ex1.m1.5.5.2.2" xref="S4.Ex1.m1.5.5.2.2.cmml">h</mi><mo id="S4.Ex1.m1.5.5.2.4.3" stretchy="false" xref="S4.Ex1.m1.5.5.2.3.cmml">)</mo></mrow></msup><mo id="S4.Ex1.m1.8.8.1.1.2.2.2.1" xref="S4.Ex1.m1.8.8.1.1.2.2.2.1.cmml">⁢</mo><mrow id="S4.Ex1.m1.1.1.1" xref="S4.Ex1.m1.1.1.1c.cmml"><mtext id="S4.Ex1.m1.1.1.1a" xref="S4.Ex1.m1.1.1.1c.cmml"> has one of the top-</mtext><mi id="S4.Ex1.m1.1.1.1.m1.1.1" xref="S4.Ex1.m1.1.1.1.m1.1.1.cmml">K</mi><mtext id="S4.Ex1.m1.1.1.1b" xref="S4.Ex1.m1.1.1.1c.cmml"> values of </mtext></mrow><mo id="S4.Ex1.m1.8.8.1.1.2.2.2.1a" xref="S4.Ex1.m1.8.8.1.1.2.2.2.1.cmml">⁢</mo><mi id="S4.Ex1.m1.8.8.1.1.2.2.2.3" mathvariant="normal" xref="S4.Ex1.m1.8.8.1.1.2.2.2.3.cmml">Δ</mi><mo id="S4.Ex1.m1.8.8.1.1.2.2.2.1b" xref="S4.Ex1.m1.8.8.1.1.2.2.2.1.cmml">⁢</mo><msup id="S4.Ex1.m1.8.8.1.1.2.2.2.4" xref="S4.Ex1.m1.8.8.1.1.2.2.2.4.cmml"><mi id="S4.Ex1.m1.8.8.1.1.2.2.2.4.2" xref="S4.Ex1.m1.8.8.1.1.2.2.2.4.2.cmml">π</mi><mrow id="S4.Ex1.m1.7.7.2.4" xref="S4.Ex1.m1.7.7.2.3.cmml"><mo id="S4.Ex1.m1.7.7.2.4.1" stretchy="false" xref="S4.Ex1.m1.7.7.2.3.cmml">(</mo><mi id="S4.Ex1.m1.6.6.1.1" xref="S4.Ex1.m1.6.6.1.1.cmml">l</mi><mo id="S4.Ex1.m1.7.7.2.4.2" xref="S4.Ex1.m1.7.7.2.3.cmml">,</mo><mi id="S4.Ex1.m1.7.7.2.2" xref="S4.Ex1.m1.7.7.2.2.cmml">h</mi><mo id="S4.Ex1.m1.7.7.2.4.3" stretchy="false" xref="S4.Ex1.m1.7.7.2.3.cmml">)</mo></mrow></msup></mrow><mo id="S4.Ex1.m1.8.8.1.1.2.2.5" stretchy="false" xref="S4.Ex1.m1.8.8.1.1.2.3.1.cmml">}</mo></mrow></mrow><mo id="S4.Ex1.m1.8.8.1.2" lspace="0em" xref="S4.Ex1.m1.8.8.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex1.m1.8b"><apply id="S4.Ex1.m1.8.8.1.1.cmml" xref="S4.Ex1.m1.8.8.1"><eq id="S4.Ex1.m1.8.8.1.1.3.cmml" xref="S4.Ex1.m1.8.8.1.1.3"></eq><ci id="S4.Ex1.m1.8.8.1.1.4.cmml" xref="S4.Ex1.m1.8.8.1.1.4">𝒮</ci><apply id="S4.Ex1.m1.8.8.1.1.2.3.cmml" xref="S4.Ex1.m1.8.8.1.1.2.2"><csymbol cd="latexml" id="S4.Ex1.m1.8.8.1.1.2.3.1.cmml" xref="S4.Ex1.m1.8.8.1.1.2.2.3">conditional-set</csymbol><apply id="S4.Ex1.m1.8.8.1.1.1.1.1.cmml" xref="S4.Ex1.m1.8.8.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.Ex1.m1.8.8.1.1.1.1.1.1.cmml" xref="S4.Ex1.m1.8.8.1.1.1.1.1">superscript</csymbol><ci id="S4.Ex1.m1.8.8.1.1.1.1.1.2.cmml" xref="S4.Ex1.m1.8.8.1.1.1.1.1.2">𝐴</ci><interval closure="open" id="S4.Ex1.m1.3.3.2.3.cmml" xref="S4.Ex1.m1.3.3.2.4"><ci id="S4.Ex1.m1.2.2.1.1.cmml" xref="S4.Ex1.m1.2.2.1.1">𝑙</ci><ci id="S4.Ex1.m1.3.3.2.2.cmml" xref="S4.Ex1.m1.3.3.2.2">ℎ</ci></interval></apply><apply id="S4.Ex1.m1.8.8.1.1.2.2.2.cmml" xref="S4.Ex1.m1.8.8.1.1.2.2.2"><times id="S4.Ex1.m1.8.8.1.1.2.2.2.1.cmml" xref="S4.Ex1.m1.8.8.1.1.2.2.2.1"></times><apply id="S4.Ex1.m1.8.8.1.1.2.2.2.2.cmml" xref="S4.Ex1.m1.8.8.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S4.Ex1.m1.8.8.1.1.2.2.2.2.1.cmml" xref="S4.Ex1.m1.8.8.1.1.2.2.2.2">superscript</csymbol><ci id="S4.Ex1.m1.8.8.1.1.2.2.2.2.2.cmml" xref="S4.Ex1.m1.8.8.1.1.2.2.2.2.2">𝐴</ci><interval closure="open" id="S4.Ex1.m1.5.5.2.3.cmml" xref="S4.Ex1.m1.5.5.2.4"><ci id="S4.Ex1.m1.4.4.1.1.cmml" xref="S4.Ex1.m1.4.4.1.1">𝑙</ci><ci id="S4.Ex1.m1.5.5.2.2.cmml" xref="S4.Ex1.m1.5.5.2.2">ℎ</ci></interval></apply><ci id="S4.Ex1.m1.1.1.1c.cmml" xref="S4.Ex1.m1.1.1.1"><mrow id="S4.Ex1.m1.1.1.1.cmml" xref="S4.Ex1.m1.1.1.1"><mtext id="S4.Ex1.m1.1.1.1a.cmml" xref="S4.Ex1.m1.1.1.1"> has one of the top-</mtext><mi id="S4.Ex1.m1.1.1.1.m1.1.1.cmml" xref="S4.Ex1.m1.1.1.1.m1.1.1">K</mi><mtext id="S4.Ex1.m1.1.1.1b.cmml" xref="S4.Ex1.m1.1.1.1"> values of </mtext></mrow></ci><ci id="S4.Ex1.m1.8.8.1.1.2.2.2.3.cmml" xref="S4.Ex1.m1.8.8.1.1.2.2.2.3">Δ</ci><apply id="S4.Ex1.m1.8.8.1.1.2.2.2.4.cmml" xref="S4.Ex1.m1.8.8.1.1.2.2.2.4"><csymbol cd="ambiguous" id="S4.Ex1.m1.8.8.1.1.2.2.2.4.1.cmml" xref="S4.Ex1.m1.8.8.1.1.2.2.2.4">superscript</csymbol><ci id="S4.Ex1.m1.8.8.1.1.2.2.2.4.2.cmml" xref="S4.Ex1.m1.8.8.1.1.2.2.2.4.2">𝜋</ci><interval closure="open" id="S4.Ex1.m1.7.7.2.3.cmml" xref="S4.Ex1.m1.7.7.2.4"><ci id="S4.Ex1.m1.6.6.1.1.cmml" xref="S4.Ex1.m1.6.6.1.1">𝑙</ci><ci id="S4.Ex1.m1.7.7.2.2.cmml" xref="S4.Ex1.m1.7.7.2.2">ℎ</ci></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex1.m1.8c">\mathcal{S}=\{A^{(l,h)}|A^{(l,h)}\text{ has one of the top-$K$ values of }%
\Delta\pi^{(l,h)}\}.</annotation><annotation encoding="application/x-llamapun" id="S4.Ex1.m1.8d">caligraphic_S = { italic_A start_POSTSUPERSCRIPT ( italic_l , italic_h ) end_POSTSUPERSCRIPT | italic_A start_POSTSUPERSCRIPT ( italic_l , italic_h ) end_POSTSUPERSCRIPT has one of the top- italic_K values of roman_Δ italic_π start_POSTSUPERSCRIPT ( italic_l , italic_h ) end_POSTSUPERSCRIPT } .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.I2.i6.p1.3">These heads are collectively referred to as RAG-suppression heads.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS0.Px2.p3">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p3.1">Notably, we do not suggest that the heads we discovered suppressing RAG tasks operate through the same mechanisms.
In Section <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S5.SS6" title="5.6 Analysis: The Value of 𝜏 ‣ 5 Experiments ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_tag">5.6</span></a>, we demonstrate that these heads may serve various functions, such as copy suppression <cite class="ltx_cite ltx_citemacro_citep">(McDougall et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib17" title="">2023</a>)</cite>, incorporating high-frequency token information <cite class="ltx_cite ltx_citemacro_citep">(Lv et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib15" title="">2024a</a>)</cite>, or influencing the behavior of other heads to indirectly affect outputs <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib31" title="">2023</a>)</cite>.
Analyzing the specific mechanism for each head is not the focus of this paper and is left for future works.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Re-weighting coefficient learning</h3>
<figure class="ltx_figure ltx_align_floatright" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="889" id="S4.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Suppose in layer <math alttext="l" class="ltx_Math" display="inline" id="S4.F2.4.m1.1"><semantics id="S4.F2.4.m1.1b"><mi id="S4.F2.4.m1.1.1" xref="S4.F2.4.m1.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S4.F2.4.m1.1c"><ci id="S4.F2.4.m1.1.1.cmml" xref="S4.F2.4.m1.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.4.m1.1d">l</annotation><annotation encoding="application/x-llamapun" id="S4.F2.4.m1.1e">italic_l</annotation></semantics></math>, <math alttext="A^{(l,h)}" class="ltx_Math" display="inline" id="S4.F2.5.m2.2"><semantics id="S4.F2.5.m2.2b"><msup id="S4.F2.5.m2.2.3" xref="S4.F2.5.m2.2.3.cmml"><mi id="S4.F2.5.m2.2.3.2" xref="S4.F2.5.m2.2.3.2.cmml">A</mi><mrow id="S4.F2.5.m2.2.2.2.4" xref="S4.F2.5.m2.2.2.2.3.cmml"><mo id="S4.F2.5.m2.2.2.2.4.1" stretchy="false" xref="S4.F2.5.m2.2.2.2.3.cmml">(</mo><mi id="S4.F2.5.m2.1.1.1.1" xref="S4.F2.5.m2.1.1.1.1.cmml">l</mi><mo id="S4.F2.5.m2.2.2.2.4.2" xref="S4.F2.5.m2.2.2.2.3.cmml">,</mo><mi id="S4.F2.5.m2.2.2.2.2" xref="S4.F2.5.m2.2.2.2.2.cmml">h</mi><mo id="S4.F2.5.m2.2.2.2.4.3" stretchy="false" xref="S4.F2.5.m2.2.2.2.3.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.F2.5.m2.2c"><apply id="S4.F2.5.m2.2.3.cmml" xref="S4.F2.5.m2.2.3"><csymbol cd="ambiguous" id="S4.F2.5.m2.2.3.1.cmml" xref="S4.F2.5.m2.2.3">superscript</csymbol><ci id="S4.F2.5.m2.2.3.2.cmml" xref="S4.F2.5.m2.2.3.2">𝐴</ci><interval closure="open" id="S4.F2.5.m2.2.2.2.3.cmml" xref="S4.F2.5.m2.2.2.2.4"><ci id="S4.F2.5.m2.1.1.1.1.cmml" xref="S4.F2.5.m2.1.1.1.1">𝑙</ci><ci id="S4.F2.5.m2.2.2.2.2.cmml" xref="S4.F2.5.m2.2.2.2.2">ℎ</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.5.m2.2d">A^{(l,h)}</annotation><annotation encoding="application/x-llamapun" id="S4.F2.5.m2.2e">italic_A start_POSTSUPERSCRIPT ( italic_l , italic_h ) end_POSTSUPERSCRIPT</annotation></semantics></math> is discovered as a RAG-suppression head. <span class="ltx_text ltx_font_italic" id="S4.F2.8.1">PEAR</span> re-weights its output with a learnable scalar <math alttext="\tau^{(l,h)}" class="ltx_Math" display="inline" id="S4.F2.6.m3.2"><semantics id="S4.F2.6.m3.2b"><msup id="S4.F2.6.m3.2.3" xref="S4.F2.6.m3.2.3.cmml"><mi id="S4.F2.6.m3.2.3.2" xref="S4.F2.6.m3.2.3.2.cmml">τ</mi><mrow id="S4.F2.6.m3.2.2.2.4" xref="S4.F2.6.m3.2.2.2.3.cmml"><mo id="S4.F2.6.m3.2.2.2.4.1" stretchy="false" xref="S4.F2.6.m3.2.2.2.3.cmml">(</mo><mi id="S4.F2.6.m3.1.1.1.1" xref="S4.F2.6.m3.1.1.1.1.cmml">l</mi><mo id="S4.F2.6.m3.2.2.2.4.2" xref="S4.F2.6.m3.2.2.2.3.cmml">,</mo><mi id="S4.F2.6.m3.2.2.2.2" xref="S4.F2.6.m3.2.2.2.2.cmml">h</mi><mo id="S4.F2.6.m3.2.2.2.4.3" stretchy="false" xref="S4.F2.6.m3.2.2.2.3.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.F2.6.m3.2c"><apply id="S4.F2.6.m3.2.3.cmml" xref="S4.F2.6.m3.2.3"><csymbol cd="ambiguous" id="S4.F2.6.m3.2.3.1.cmml" xref="S4.F2.6.m3.2.3">superscript</csymbol><ci id="S4.F2.6.m3.2.3.2.cmml" xref="S4.F2.6.m3.2.3.2">𝜏</ci><interval closure="open" id="S4.F2.6.m3.2.2.2.3.cmml" xref="S4.F2.6.m3.2.2.2.4"><ci id="S4.F2.6.m3.1.1.1.1.cmml" xref="S4.F2.6.m3.1.1.1.1">𝑙</ci><ci id="S4.F2.6.m3.2.2.2.2.cmml" xref="S4.F2.6.m3.2.2.2.2">ℎ</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.6.m3.2d">\tau^{(l,h)}</annotation><annotation encoding="application/x-llamapun" id="S4.F2.6.m3.2e">italic_τ start_POSTSUPERSCRIPT ( italic_l , italic_h ) end_POSTSUPERSCRIPT</annotation></semantics></math>.</figcaption>
</figure>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Optimization</h4>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px1.p1.3">In standard multi-head attention mechanisms, the outputs of all attention heads are aggregated with equal weighting.
We propose that re-weighting these relative aggregation weights to values less than 1 can mitigate the RAG-suppression effect from our discovered heads.
To implement this, we modify the forward computation by multiplying the output of each head, <math alttext="A^{(l,h)}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p1.1.m1.2"><semantics id="S4.SS2.SSS0.Px1.p1.1.m1.2a"><msup id="S4.SS2.SSS0.Px1.p1.1.m1.2.3" xref="S4.SS2.SSS0.Px1.p1.1.m1.2.3.cmml"><mi id="S4.SS2.SSS0.Px1.p1.1.m1.2.3.2" xref="S4.SS2.SSS0.Px1.p1.1.m1.2.3.2.cmml">A</mi><mrow id="S4.SS2.SSS0.Px1.p1.1.m1.2.2.2.4" xref="S4.SS2.SSS0.Px1.p1.1.m1.2.2.2.3.cmml"><mo id="S4.SS2.SSS0.Px1.p1.1.m1.2.2.2.4.1" stretchy="false" xref="S4.SS2.SSS0.Px1.p1.1.m1.2.2.2.3.cmml">(</mo><mi id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.1.1" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.1.1.cmml">l</mi><mo id="S4.SS2.SSS0.Px1.p1.1.m1.2.2.2.4.2" xref="S4.SS2.SSS0.Px1.p1.1.m1.2.2.2.3.cmml">,</mo><mi id="S4.SS2.SSS0.Px1.p1.1.m1.2.2.2.2" xref="S4.SS2.SSS0.Px1.p1.1.m1.2.2.2.2.cmml">h</mi><mo id="S4.SS2.SSS0.Px1.p1.1.m1.2.2.2.4.3" stretchy="false" xref="S4.SS2.SSS0.Px1.p1.1.m1.2.2.2.3.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p1.1.m1.2b"><apply id="S4.SS2.SSS0.Px1.p1.1.m1.2.3.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.2.3"><csymbol cd="ambiguous" id="S4.SS2.SSS0.Px1.p1.1.m1.2.3.1.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.2.3">superscript</csymbol><ci id="S4.SS2.SSS0.Px1.p1.1.m1.2.3.2.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.2.3.2">𝐴</ci><interval closure="open" id="S4.SS2.SSS0.Px1.p1.1.m1.2.2.2.3.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.2.2.2.4"><ci id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.1.1">𝑙</ci><ci id="S4.SS2.SSS0.Px1.p1.1.m1.2.2.2.2.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.2.2.2.2">ℎ</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p1.1.m1.2c">A^{(l,h)}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px1.p1.1.m1.2d">italic_A start_POSTSUPERSCRIPT ( italic_l , italic_h ) end_POSTSUPERSCRIPT</annotation></semantics></math>, in the set <math alttext="\mathcal{S}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p1.2.m2.1"><semantics id="S4.SS2.SSS0.Px1.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S4.SS2.SSS0.Px1.p1.2.m2.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p1.2.m2.1b"><ci id="S4.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS0.Px1.p1.2.m2.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p1.2.m2.1c">\mathcal{S}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px1.p1.2.m2.1d">caligraphic_S</annotation></semantics></math> by a learnable scalar, <math alttext="\tau^{(l,h)}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p1.3.m3.2"><semantics id="S4.SS2.SSS0.Px1.p1.3.m3.2a"><msup id="S4.SS2.SSS0.Px1.p1.3.m3.2.3" xref="S4.SS2.SSS0.Px1.p1.3.m3.2.3.cmml"><mi id="S4.SS2.SSS0.Px1.p1.3.m3.2.3.2" xref="S4.SS2.SSS0.Px1.p1.3.m3.2.3.2.cmml">τ</mi><mrow id="S4.SS2.SSS0.Px1.p1.3.m3.2.2.2.4" xref="S4.SS2.SSS0.Px1.p1.3.m3.2.2.2.3.cmml"><mo id="S4.SS2.SSS0.Px1.p1.3.m3.2.2.2.4.1" stretchy="false" xref="S4.SS2.SSS0.Px1.p1.3.m3.2.2.2.3.cmml">(</mo><mi id="S4.SS2.SSS0.Px1.p1.3.m3.1.1.1.1" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1.1.1.cmml">l</mi><mo id="S4.SS2.SSS0.Px1.p1.3.m3.2.2.2.4.2" xref="S4.SS2.SSS0.Px1.p1.3.m3.2.2.2.3.cmml">,</mo><mi id="S4.SS2.SSS0.Px1.p1.3.m3.2.2.2.2" xref="S4.SS2.SSS0.Px1.p1.3.m3.2.2.2.2.cmml">h</mi><mo id="S4.SS2.SSS0.Px1.p1.3.m3.2.2.2.4.3" stretchy="false" xref="S4.SS2.SSS0.Px1.p1.3.m3.2.2.2.3.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p1.3.m3.2b"><apply id="S4.SS2.SSS0.Px1.p1.3.m3.2.3.cmml" xref="S4.SS2.SSS0.Px1.p1.3.m3.2.3"><csymbol cd="ambiguous" id="S4.SS2.SSS0.Px1.p1.3.m3.2.3.1.cmml" xref="S4.SS2.SSS0.Px1.p1.3.m3.2.3">superscript</csymbol><ci id="S4.SS2.SSS0.Px1.p1.3.m3.2.3.2.cmml" xref="S4.SS2.SSS0.Px1.p1.3.m3.2.3.2">𝜏</ci><interval closure="open" id="S4.SS2.SSS0.Px1.p1.3.m3.2.2.2.3.cmml" xref="S4.SS2.SSS0.Px1.p1.3.m3.2.2.2.4"><ci id="S4.SS2.SSS0.Px1.p1.3.m3.1.1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1.1.1">𝑙</ci><ci id="S4.SS2.SSS0.Px1.p1.3.m3.2.2.2.2.cmml" xref="S4.SS2.SSS0.Px1.p1.3.m3.2.2.2.2">ℎ</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p1.3.m3.2c">\tau^{(l,h)}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px1.p1.3.m3.2d">italic_τ start_POSTSUPERSCRIPT ( italic_l , italic_h ) end_POSTSUPERSCRIPT</annotation></semantics></math>, referred to as the re-weighting coefficient.
The modified output for each head is:</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS0.Px1.p2">
<table class="ltx_equation ltx_eqn_table" id="S4.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\textbf{a}^{(l,h)}=\tau^{(l,h)}*\textbf{a}^{(l,h)},\ \text{for each }A^{(l,h)}%
\in\mathcal{S}." class="ltx_Math" display="block" id="S4.E2.m1.9"><semantics id="S4.E2.m1.9a"><mrow id="S4.E2.m1.9.9.1"><mrow id="S4.E2.m1.9.9.1.1.2" xref="S4.E2.m1.9.9.1.1.3.cmml"><mrow id="S4.E2.m1.9.9.1.1.1.1" xref="S4.E2.m1.9.9.1.1.1.1.cmml"><msup id="S4.E2.m1.9.9.1.1.1.1.2" xref="S4.E2.m1.9.9.1.1.1.1.2.cmml"><mtext class="ltx_mathvariant_bold" id="S4.E2.m1.9.9.1.1.1.1.2.2" xref="S4.E2.m1.9.9.1.1.1.1.2.2a.cmml">a</mtext><mrow id="S4.E2.m1.2.2.2.4" xref="S4.E2.m1.2.2.2.3.cmml"><mo id="S4.E2.m1.2.2.2.4.1" stretchy="false" xref="S4.E2.m1.2.2.2.3.cmml">(</mo><mi id="S4.E2.m1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.cmml">l</mi><mo id="S4.E2.m1.2.2.2.4.2" xref="S4.E2.m1.2.2.2.3.cmml">,</mo><mi id="S4.E2.m1.2.2.2.2" xref="S4.E2.m1.2.2.2.2.cmml">h</mi><mo id="S4.E2.m1.2.2.2.4.3" stretchy="false" xref="S4.E2.m1.2.2.2.3.cmml">)</mo></mrow></msup><mo id="S4.E2.m1.9.9.1.1.1.1.1" xref="S4.E2.m1.9.9.1.1.1.1.1.cmml">=</mo><mrow id="S4.E2.m1.9.9.1.1.1.1.3" xref="S4.E2.m1.9.9.1.1.1.1.3.cmml"><msup id="S4.E2.m1.9.9.1.1.1.1.3.2" xref="S4.E2.m1.9.9.1.1.1.1.3.2.cmml"><mi id="S4.E2.m1.9.9.1.1.1.1.3.2.2" xref="S4.E2.m1.9.9.1.1.1.1.3.2.2.cmml">τ</mi><mrow id="S4.E2.m1.4.4.2.4" xref="S4.E2.m1.4.4.2.3.cmml"><mo id="S4.E2.m1.4.4.2.4.1" stretchy="false" xref="S4.E2.m1.4.4.2.3.cmml">(</mo><mi id="S4.E2.m1.3.3.1.1" xref="S4.E2.m1.3.3.1.1.cmml">l</mi><mo id="S4.E2.m1.4.4.2.4.2" xref="S4.E2.m1.4.4.2.3.cmml">,</mo><mi id="S4.E2.m1.4.4.2.2" xref="S4.E2.m1.4.4.2.2.cmml">h</mi><mo id="S4.E2.m1.4.4.2.4.3" stretchy="false" xref="S4.E2.m1.4.4.2.3.cmml">)</mo></mrow></msup><mo id="S4.E2.m1.9.9.1.1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S4.E2.m1.9.9.1.1.1.1.3.1.cmml">∗</mo><msup id="S4.E2.m1.9.9.1.1.1.1.3.3" xref="S4.E2.m1.9.9.1.1.1.1.3.3.cmml"><mtext class="ltx_mathvariant_bold" id="S4.E2.m1.9.9.1.1.1.1.3.3.2" xref="S4.E2.m1.9.9.1.1.1.1.3.3.2a.cmml">a</mtext><mrow id="S4.E2.m1.6.6.2.4" xref="S4.E2.m1.6.6.2.3.cmml"><mo id="S4.E2.m1.6.6.2.4.1" stretchy="false" xref="S4.E2.m1.6.6.2.3.cmml">(</mo><mi id="S4.E2.m1.5.5.1.1" xref="S4.E2.m1.5.5.1.1.cmml">l</mi><mo id="S4.E2.m1.6.6.2.4.2" xref="S4.E2.m1.6.6.2.3.cmml">,</mo><mi id="S4.E2.m1.6.6.2.2" xref="S4.E2.m1.6.6.2.2.cmml">h</mi><mo id="S4.E2.m1.6.6.2.4.3" stretchy="false" xref="S4.E2.m1.6.6.2.3.cmml">)</mo></mrow></msup></mrow></mrow><mo id="S4.E2.m1.9.9.1.1.2.3" rspace="0.667em" xref="S4.E2.m1.9.9.1.1.3a.cmml">,</mo><mrow id="S4.E2.m1.9.9.1.1.2.2" xref="S4.E2.m1.9.9.1.1.2.2.cmml"><mrow id="S4.E2.m1.9.9.1.1.2.2.2" xref="S4.E2.m1.9.9.1.1.2.2.2.cmml"><mtext id="S4.E2.m1.9.9.1.1.2.2.2.2" xref="S4.E2.m1.9.9.1.1.2.2.2.2a.cmml">for each </mtext><mo id="S4.E2.m1.9.9.1.1.2.2.2.1" xref="S4.E2.m1.9.9.1.1.2.2.2.1.cmml">⁢</mo><msup id="S4.E2.m1.9.9.1.1.2.2.2.3" xref="S4.E2.m1.9.9.1.1.2.2.2.3.cmml"><mi id="S4.E2.m1.9.9.1.1.2.2.2.3.2" xref="S4.E2.m1.9.9.1.1.2.2.2.3.2.cmml">A</mi><mrow id="S4.E2.m1.8.8.2.4" xref="S4.E2.m1.8.8.2.3.cmml"><mo id="S4.E2.m1.8.8.2.4.1" stretchy="false" xref="S4.E2.m1.8.8.2.3.cmml">(</mo><mi id="S4.E2.m1.7.7.1.1" xref="S4.E2.m1.7.7.1.1.cmml">l</mi><mo id="S4.E2.m1.8.8.2.4.2" xref="S4.E2.m1.8.8.2.3.cmml">,</mo><mi id="S4.E2.m1.8.8.2.2" xref="S4.E2.m1.8.8.2.2.cmml">h</mi><mo id="S4.E2.m1.8.8.2.4.3" stretchy="false" xref="S4.E2.m1.8.8.2.3.cmml">)</mo></mrow></msup></mrow><mo id="S4.E2.m1.9.9.1.1.2.2.1" xref="S4.E2.m1.9.9.1.1.2.2.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S4.E2.m1.9.9.1.1.2.2.3" xref="S4.E2.m1.9.9.1.1.2.2.3.cmml">𝒮</mi></mrow></mrow><mo id="S4.E2.m1.9.9.1.2" lspace="0em">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.9b"><apply id="S4.E2.m1.9.9.1.1.3.cmml" xref="S4.E2.m1.9.9.1.1.2"><csymbol cd="ambiguous" id="S4.E2.m1.9.9.1.1.3a.cmml" xref="S4.E2.m1.9.9.1.1.2.3">formulae-sequence</csymbol><apply id="S4.E2.m1.9.9.1.1.1.1.cmml" xref="S4.E2.m1.9.9.1.1.1.1"><eq id="S4.E2.m1.9.9.1.1.1.1.1.cmml" xref="S4.E2.m1.9.9.1.1.1.1.1"></eq><apply id="S4.E2.m1.9.9.1.1.1.1.2.cmml" xref="S4.E2.m1.9.9.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E2.m1.9.9.1.1.1.1.2.1.cmml" xref="S4.E2.m1.9.9.1.1.1.1.2">superscript</csymbol><ci id="S4.E2.m1.9.9.1.1.1.1.2.2a.cmml" xref="S4.E2.m1.9.9.1.1.1.1.2.2"><mtext class="ltx_mathvariant_bold" id="S4.E2.m1.9.9.1.1.1.1.2.2.cmml" xref="S4.E2.m1.9.9.1.1.1.1.2.2">a</mtext></ci><interval closure="open" id="S4.E2.m1.2.2.2.3.cmml" xref="S4.E2.m1.2.2.2.4"><ci id="S4.E2.m1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1">𝑙</ci><ci id="S4.E2.m1.2.2.2.2.cmml" xref="S4.E2.m1.2.2.2.2">ℎ</ci></interval></apply><apply id="S4.E2.m1.9.9.1.1.1.1.3.cmml" xref="S4.E2.m1.9.9.1.1.1.1.3"><times id="S4.E2.m1.9.9.1.1.1.1.3.1.cmml" xref="S4.E2.m1.9.9.1.1.1.1.3.1"></times><apply id="S4.E2.m1.9.9.1.1.1.1.3.2.cmml" xref="S4.E2.m1.9.9.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E2.m1.9.9.1.1.1.1.3.2.1.cmml" xref="S4.E2.m1.9.9.1.1.1.1.3.2">superscript</csymbol><ci id="S4.E2.m1.9.9.1.1.1.1.3.2.2.cmml" xref="S4.E2.m1.9.9.1.1.1.1.3.2.2">𝜏</ci><interval closure="open" id="S4.E2.m1.4.4.2.3.cmml" xref="S4.E2.m1.4.4.2.4"><ci id="S4.E2.m1.3.3.1.1.cmml" xref="S4.E2.m1.3.3.1.1">𝑙</ci><ci id="S4.E2.m1.4.4.2.2.cmml" xref="S4.E2.m1.4.4.2.2">ℎ</ci></interval></apply><apply id="S4.E2.m1.9.9.1.1.1.1.3.3.cmml" xref="S4.E2.m1.9.9.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S4.E2.m1.9.9.1.1.1.1.3.3.1.cmml" xref="S4.E2.m1.9.9.1.1.1.1.3.3">superscript</csymbol><ci id="S4.E2.m1.9.9.1.1.1.1.3.3.2a.cmml" xref="S4.E2.m1.9.9.1.1.1.1.3.3.2"><mtext class="ltx_mathvariant_bold" id="S4.E2.m1.9.9.1.1.1.1.3.3.2.cmml" xref="S4.E2.m1.9.9.1.1.1.1.3.3.2">a</mtext></ci><interval closure="open" id="S4.E2.m1.6.6.2.3.cmml" xref="S4.E2.m1.6.6.2.4"><ci id="S4.E2.m1.5.5.1.1.cmml" xref="S4.E2.m1.5.5.1.1">𝑙</ci><ci id="S4.E2.m1.6.6.2.2.cmml" xref="S4.E2.m1.6.6.2.2">ℎ</ci></interval></apply></apply></apply><apply id="S4.E2.m1.9.9.1.1.2.2.cmml" xref="S4.E2.m1.9.9.1.1.2.2"><in id="S4.E2.m1.9.9.1.1.2.2.1.cmml" xref="S4.E2.m1.9.9.1.1.2.2.1"></in><apply id="S4.E2.m1.9.9.1.1.2.2.2.cmml" xref="S4.E2.m1.9.9.1.1.2.2.2"><times id="S4.E2.m1.9.9.1.1.2.2.2.1.cmml" xref="S4.E2.m1.9.9.1.1.2.2.2.1"></times><ci id="S4.E2.m1.9.9.1.1.2.2.2.2a.cmml" xref="S4.E2.m1.9.9.1.1.2.2.2.2"><mtext id="S4.E2.m1.9.9.1.1.2.2.2.2.cmml" xref="S4.E2.m1.9.9.1.1.2.2.2.2">for each </mtext></ci><apply id="S4.E2.m1.9.9.1.1.2.2.2.3.cmml" xref="S4.E2.m1.9.9.1.1.2.2.2.3"><csymbol cd="ambiguous" id="S4.E2.m1.9.9.1.1.2.2.2.3.1.cmml" xref="S4.E2.m1.9.9.1.1.2.2.2.3">superscript</csymbol><ci id="S4.E2.m1.9.9.1.1.2.2.2.3.2.cmml" xref="S4.E2.m1.9.9.1.1.2.2.2.3.2">𝐴</ci><interval closure="open" id="S4.E2.m1.8.8.2.3.cmml" xref="S4.E2.m1.8.8.2.4"><ci id="S4.E2.m1.7.7.1.1.cmml" xref="S4.E2.m1.7.7.1.1">𝑙</ci><ci id="S4.E2.m1.8.8.2.2.cmml" xref="S4.E2.m1.8.8.2.2">ℎ</ci></interval></apply></apply><ci id="S4.E2.m1.9.9.1.1.2.2.3.cmml" xref="S4.E2.m1.9.9.1.1.2.2.3">𝒮</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.9c">\textbf{a}^{(l,h)}=\tau^{(l,h)}*\textbf{a}^{(l,h)},\ \text{for each }A^{(l,h)}%
\in\mathcal{S}.</annotation><annotation encoding="application/x-llamapun" id="S4.E2.m1.9d">a start_POSTSUPERSCRIPT ( italic_l , italic_h ) end_POSTSUPERSCRIPT = italic_τ start_POSTSUPERSCRIPT ( italic_l , italic_h ) end_POSTSUPERSCRIPT ∗ a start_POSTSUPERSCRIPT ( italic_l , italic_h ) end_POSTSUPERSCRIPT , for each italic_A start_POSTSUPERSCRIPT ( italic_l , italic_h ) end_POSTSUPERSCRIPT ∈ caligraphic_S .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS0.Px1.p3">
<p class="ltx_p" id="S4.SS2.SSS0.Px1.p3.1">To optimize these re-weighting coefficients for RAG-suppression heads, we freeze the original parameters of the LLM and train only the re-weighting coefficients to minimize the loss on a proxy task.
Importantly, the loss is calculated only over the latter half of the sequence, optimizing the coefficients to enhance in-context retrieval capacities rather than predicting the next token.
Formally, our adopted loss can be written as:</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS0.Px1.p4">
<table class="ltx_equation ltx_eqn_table" id="S4.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}=-\sum^{2n-1}_{i=n}\log p(x_{i+1}|x_{1:i})" class="ltx_Math" display="block" id="S4.E3.m1.1"><semantics id="S4.E3.m1.1a"><mrow id="S4.E3.m1.1.1" xref="S4.E3.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E3.m1.1.1.3" xref="S4.E3.m1.1.1.3.cmml">ℒ</mi><mo id="S4.E3.m1.1.1.2" xref="S4.E3.m1.1.1.2.cmml">=</mo><mrow id="S4.E3.m1.1.1.1" xref="S4.E3.m1.1.1.1.cmml"><mo id="S4.E3.m1.1.1.1a" xref="S4.E3.m1.1.1.1.cmml">−</mo><mrow id="S4.E3.m1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.cmml"><munderover id="S4.E3.m1.1.1.1.1.2" xref="S4.E3.m1.1.1.1.1.2.cmml"><mo id="S4.E3.m1.1.1.1.1.2.2.2" movablelimits="false" xref="S4.E3.m1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S4.E3.m1.1.1.1.1.2.3" xref="S4.E3.m1.1.1.1.1.2.3.cmml"><mi id="S4.E3.m1.1.1.1.1.2.3.2" xref="S4.E3.m1.1.1.1.1.2.3.2.cmml">i</mi><mo id="S4.E3.m1.1.1.1.1.2.3.1" xref="S4.E3.m1.1.1.1.1.2.3.1.cmml">=</mo><mi id="S4.E3.m1.1.1.1.1.2.3.3" xref="S4.E3.m1.1.1.1.1.2.3.3.cmml">n</mi></mrow><mrow id="S4.E3.m1.1.1.1.1.2.2.3" xref="S4.E3.m1.1.1.1.1.2.2.3.cmml"><mrow id="S4.E3.m1.1.1.1.1.2.2.3.2" xref="S4.E3.m1.1.1.1.1.2.2.3.2.cmml"><mn id="S4.E3.m1.1.1.1.1.2.2.3.2.2" xref="S4.E3.m1.1.1.1.1.2.2.3.2.2.cmml">2</mn><mo id="S4.E3.m1.1.1.1.1.2.2.3.2.1" xref="S4.E3.m1.1.1.1.1.2.2.3.2.1.cmml">⁢</mo><mi id="S4.E3.m1.1.1.1.1.2.2.3.2.3" xref="S4.E3.m1.1.1.1.1.2.2.3.2.3.cmml">n</mi></mrow><mo id="S4.E3.m1.1.1.1.1.2.2.3.1" xref="S4.E3.m1.1.1.1.1.2.2.3.1.cmml">−</mo><mn id="S4.E3.m1.1.1.1.1.2.2.3.3" xref="S4.E3.m1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow></munderover><mrow id="S4.E3.m1.1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.1.cmml"><mrow id="S4.E3.m1.1.1.1.1.1.3" xref="S4.E3.m1.1.1.1.1.1.3.cmml"><mi id="S4.E3.m1.1.1.1.1.1.3.1" xref="S4.E3.m1.1.1.1.1.1.3.1.cmml">log</mi><mo id="S4.E3.m1.1.1.1.1.1.3a" lspace="0.167em" xref="S4.E3.m1.1.1.1.1.1.3.cmml">⁡</mo><mi id="S4.E3.m1.1.1.1.1.1.3.2" xref="S4.E3.m1.1.1.1.1.1.3.2.cmml">p</mi></mrow><mo id="S4.E3.m1.1.1.1.1.1.2" xref="S4.E3.m1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S4.E3.m1.1.1.1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S4.E3.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="S4.E3.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E3.m1.1.1.1.1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.1.1.1.1.cmml"><msub id="S4.E3.m1.1.1.1.1.1.1.1.1.2" xref="S4.E3.m1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S4.E3.m1.1.1.1.1.1.1.1.1.2.2" xref="S4.E3.m1.1.1.1.1.1.1.1.1.2.2.cmml">x</mi><mrow id="S4.E3.m1.1.1.1.1.1.1.1.1.2.3" xref="S4.E3.m1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S4.E3.m1.1.1.1.1.1.1.1.1.2.3.2" xref="S4.E3.m1.1.1.1.1.1.1.1.1.2.3.2.cmml">i</mi><mo id="S4.E3.m1.1.1.1.1.1.1.1.1.2.3.1" xref="S4.E3.m1.1.1.1.1.1.1.1.1.2.3.1.cmml">+</mo><mn id="S4.E3.m1.1.1.1.1.1.1.1.1.2.3.3" xref="S4.E3.m1.1.1.1.1.1.1.1.1.2.3.3.cmml">1</mn></mrow></msub><mo fence="false" id="S4.E3.m1.1.1.1.1.1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1.cmml">|</mo><msub id="S4.E3.m1.1.1.1.1.1.1.1.1.3" xref="S4.E3.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E3.m1.1.1.1.1.1.1.1.1.3.2" xref="S4.E3.m1.1.1.1.1.1.1.1.1.3.2.cmml">x</mi><mrow id="S4.E3.m1.1.1.1.1.1.1.1.1.3.3" xref="S4.E3.m1.1.1.1.1.1.1.1.1.3.3.cmml"><mn id="S4.E3.m1.1.1.1.1.1.1.1.1.3.3.2" xref="S4.E3.m1.1.1.1.1.1.1.1.1.3.3.2.cmml">1</mn><mo id="S4.E3.m1.1.1.1.1.1.1.1.1.3.3.1" lspace="0.278em" rspace="0.278em" xref="S4.E3.m1.1.1.1.1.1.1.1.1.3.3.1.cmml">:</mo><mi id="S4.E3.m1.1.1.1.1.1.1.1.1.3.3.3" xref="S4.E3.m1.1.1.1.1.1.1.1.1.3.3.3.cmml">i</mi></mrow></msub></mrow><mo id="S4.E3.m1.1.1.1.1.1.1.1.3" stretchy="false" xref="S4.E3.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.1b"><apply id="S4.E3.m1.1.1.cmml" xref="S4.E3.m1.1.1"><eq id="S4.E3.m1.1.1.2.cmml" xref="S4.E3.m1.1.1.2"></eq><ci id="S4.E3.m1.1.1.3.cmml" xref="S4.E3.m1.1.1.3">ℒ</ci><apply id="S4.E3.m1.1.1.1.cmml" xref="S4.E3.m1.1.1.1"><minus id="S4.E3.m1.1.1.1.2.cmml" xref="S4.E3.m1.1.1.1"></minus><apply id="S4.E3.m1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1"><apply id="S4.E3.m1.1.1.1.1.2.cmml" xref="S4.E3.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.2.1.cmml" xref="S4.E3.m1.1.1.1.1.2">subscript</csymbol><apply id="S4.E3.m1.1.1.1.1.2.2.cmml" xref="S4.E3.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.2.2.1.cmml" xref="S4.E3.m1.1.1.1.1.2">superscript</csymbol><sum id="S4.E3.m1.1.1.1.1.2.2.2.cmml" xref="S4.E3.m1.1.1.1.1.2.2.2"></sum><apply id="S4.E3.m1.1.1.1.1.2.2.3.cmml" xref="S4.E3.m1.1.1.1.1.2.2.3"><minus id="S4.E3.m1.1.1.1.1.2.2.3.1.cmml" xref="S4.E3.m1.1.1.1.1.2.2.3.1"></minus><apply id="S4.E3.m1.1.1.1.1.2.2.3.2.cmml" xref="S4.E3.m1.1.1.1.1.2.2.3.2"><times id="S4.E3.m1.1.1.1.1.2.2.3.2.1.cmml" xref="S4.E3.m1.1.1.1.1.2.2.3.2.1"></times><cn id="S4.E3.m1.1.1.1.1.2.2.3.2.2.cmml" type="integer" xref="S4.E3.m1.1.1.1.1.2.2.3.2.2">2</cn><ci id="S4.E3.m1.1.1.1.1.2.2.3.2.3.cmml" xref="S4.E3.m1.1.1.1.1.2.2.3.2.3">𝑛</ci></apply><cn id="S4.E3.m1.1.1.1.1.2.2.3.3.cmml" type="integer" xref="S4.E3.m1.1.1.1.1.2.2.3.3">1</cn></apply></apply><apply id="S4.E3.m1.1.1.1.1.2.3.cmml" xref="S4.E3.m1.1.1.1.1.2.3"><eq id="S4.E3.m1.1.1.1.1.2.3.1.cmml" xref="S4.E3.m1.1.1.1.1.2.3.1"></eq><ci id="S4.E3.m1.1.1.1.1.2.3.2.cmml" xref="S4.E3.m1.1.1.1.1.2.3.2">𝑖</ci><ci id="S4.E3.m1.1.1.1.1.2.3.3.cmml" xref="S4.E3.m1.1.1.1.1.2.3.3">𝑛</ci></apply></apply><apply id="S4.E3.m1.1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1.1"><times id="S4.E3.m1.1.1.1.1.1.2.cmml" xref="S4.E3.m1.1.1.1.1.1.2"></times><apply id="S4.E3.m1.1.1.1.1.1.3.cmml" xref="S4.E3.m1.1.1.1.1.1.3"><log id="S4.E3.m1.1.1.1.1.1.3.1.cmml" xref="S4.E3.m1.1.1.1.1.1.3.1"></log><ci id="S4.E3.m1.1.1.1.1.1.3.2.cmml" xref="S4.E3.m1.1.1.1.1.1.3.2">𝑝</ci></apply><apply id="S4.E3.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E3.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.1">conditional</csymbol><apply id="S4.E3.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E3.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.2.2">𝑥</ci><apply id="S4.E3.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.2.3"><plus id="S4.E3.m1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.2.3.1"></plus><ci id="S4.E3.m1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.2.3.2">𝑖</ci><cn id="S4.E3.m1.1.1.1.1.1.1.1.1.2.3.3.cmml" type="integer" xref="S4.E3.m1.1.1.1.1.1.1.1.1.2.3.3">1</cn></apply></apply><apply id="S4.E3.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E3.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.3.2">𝑥</ci><apply id="S4.E3.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.3.3"><ci id="S4.E3.m1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.3.3.1">:</ci><cn id="S4.E3.m1.1.1.1.1.1.1.1.1.3.3.2.cmml" type="integer" xref="S4.E3.m1.1.1.1.1.1.1.1.1.3.3.2">1</cn><ci id="S4.E3.m1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1.1.3.3.3">𝑖</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.1c">\mathcal{L}=-\sum^{2n-1}_{i=n}\log p(x_{i+1}|x_{1:i})</annotation><annotation encoding="application/x-llamapun" id="S4.E3.m1.1d">caligraphic_L = - ∑ start_POSTSUPERSCRIPT 2 italic_n - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i = italic_n end_POSTSUBSCRIPT roman_log italic_p ( italic_x start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT 1 : italic_i end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS0.Px1.p5">
<p class="ltx_p" id="S4.SS2.SSS0.Px1.p5.3">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S4.F2" title="Figure 2 ‣ 4.2 Re-weighting coefficient learning ‣ 4 Methodology ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates a re-weighting process during optimization.
Notably, the re-weighting process shown in this figure adds extra multiplication operations in a forward pass.
In practice, when coefficient learning ends, we re-scale <math alttext="W^{(l,h)}_{O}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p5.1.m1.2"><semantics id="S4.SS2.SSS0.Px1.p5.1.m1.2a"><msubsup id="S4.SS2.SSS0.Px1.p5.1.m1.2.3" xref="S4.SS2.SSS0.Px1.p5.1.m1.2.3.cmml"><mi id="S4.SS2.SSS0.Px1.p5.1.m1.2.3.2.2" xref="S4.SS2.SSS0.Px1.p5.1.m1.2.3.2.2.cmml">W</mi><mi id="S4.SS2.SSS0.Px1.p5.1.m1.2.3.3" xref="S4.SS2.SSS0.Px1.p5.1.m1.2.3.3.cmml">O</mi><mrow id="S4.SS2.SSS0.Px1.p5.1.m1.2.2.2.4" xref="S4.SS2.SSS0.Px1.p5.1.m1.2.2.2.3.cmml"><mo id="S4.SS2.SSS0.Px1.p5.1.m1.2.2.2.4.1" stretchy="false" xref="S4.SS2.SSS0.Px1.p5.1.m1.2.2.2.3.cmml">(</mo><mi id="S4.SS2.SSS0.Px1.p5.1.m1.1.1.1.1" xref="S4.SS2.SSS0.Px1.p5.1.m1.1.1.1.1.cmml">l</mi><mo id="S4.SS2.SSS0.Px1.p5.1.m1.2.2.2.4.2" xref="S4.SS2.SSS0.Px1.p5.1.m1.2.2.2.3.cmml">,</mo><mi id="S4.SS2.SSS0.Px1.p5.1.m1.2.2.2.2" xref="S4.SS2.SSS0.Px1.p5.1.m1.2.2.2.2.cmml">h</mi><mo id="S4.SS2.SSS0.Px1.p5.1.m1.2.2.2.4.3" stretchy="false" xref="S4.SS2.SSS0.Px1.p5.1.m1.2.2.2.3.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p5.1.m1.2b"><apply id="S4.SS2.SSS0.Px1.p5.1.m1.2.3.cmml" xref="S4.SS2.SSS0.Px1.p5.1.m1.2.3"><csymbol cd="ambiguous" id="S4.SS2.SSS0.Px1.p5.1.m1.2.3.1.cmml" xref="S4.SS2.SSS0.Px1.p5.1.m1.2.3">subscript</csymbol><apply id="S4.SS2.SSS0.Px1.p5.1.m1.2.3.2.cmml" xref="S4.SS2.SSS0.Px1.p5.1.m1.2.3"><csymbol cd="ambiguous" id="S4.SS2.SSS0.Px1.p5.1.m1.2.3.2.1.cmml" xref="S4.SS2.SSS0.Px1.p5.1.m1.2.3">superscript</csymbol><ci id="S4.SS2.SSS0.Px1.p5.1.m1.2.3.2.2.cmml" xref="S4.SS2.SSS0.Px1.p5.1.m1.2.3.2.2">𝑊</ci><interval closure="open" id="S4.SS2.SSS0.Px1.p5.1.m1.2.2.2.3.cmml" xref="S4.SS2.SSS0.Px1.p5.1.m1.2.2.2.4"><ci id="S4.SS2.SSS0.Px1.p5.1.m1.1.1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p5.1.m1.1.1.1.1">𝑙</ci><ci id="S4.SS2.SSS0.Px1.p5.1.m1.2.2.2.2.cmml" xref="S4.SS2.SSS0.Px1.p5.1.m1.2.2.2.2">ℎ</ci></interval></apply><ci id="S4.SS2.SSS0.Px1.p5.1.m1.2.3.3.cmml" xref="S4.SS2.SSS0.Px1.p5.1.m1.2.3.3">𝑂</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p5.1.m1.2c">W^{(l,h)}_{O}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px1.p5.1.m1.2d">italic_W start_POSTSUPERSCRIPT ( italic_l , italic_h ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_O end_POSTSUBSCRIPT</annotation></semantics></math> (the output projection matrix in head <math alttext="A^{(l,h)}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p5.2.m2.2"><semantics id="S4.SS2.SSS0.Px1.p5.2.m2.2a"><msup id="S4.SS2.SSS0.Px1.p5.2.m2.2.3" xref="S4.SS2.SSS0.Px1.p5.2.m2.2.3.cmml"><mi id="S4.SS2.SSS0.Px1.p5.2.m2.2.3.2" xref="S4.SS2.SSS0.Px1.p5.2.m2.2.3.2.cmml">A</mi><mrow id="S4.SS2.SSS0.Px1.p5.2.m2.2.2.2.4" xref="S4.SS2.SSS0.Px1.p5.2.m2.2.2.2.3.cmml"><mo id="S4.SS2.SSS0.Px1.p5.2.m2.2.2.2.4.1" stretchy="false" xref="S4.SS2.SSS0.Px1.p5.2.m2.2.2.2.3.cmml">(</mo><mi id="S4.SS2.SSS0.Px1.p5.2.m2.1.1.1.1" xref="S4.SS2.SSS0.Px1.p5.2.m2.1.1.1.1.cmml">l</mi><mo id="S4.SS2.SSS0.Px1.p5.2.m2.2.2.2.4.2" xref="S4.SS2.SSS0.Px1.p5.2.m2.2.2.2.3.cmml">,</mo><mi id="S4.SS2.SSS0.Px1.p5.2.m2.2.2.2.2" xref="S4.SS2.SSS0.Px1.p5.2.m2.2.2.2.2.cmml">h</mi><mo id="S4.SS2.SSS0.Px1.p5.2.m2.2.2.2.4.3" stretchy="false" xref="S4.SS2.SSS0.Px1.p5.2.m2.2.2.2.3.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p5.2.m2.2b"><apply id="S4.SS2.SSS0.Px1.p5.2.m2.2.3.cmml" xref="S4.SS2.SSS0.Px1.p5.2.m2.2.3"><csymbol cd="ambiguous" id="S4.SS2.SSS0.Px1.p5.2.m2.2.3.1.cmml" xref="S4.SS2.SSS0.Px1.p5.2.m2.2.3">superscript</csymbol><ci id="S4.SS2.SSS0.Px1.p5.2.m2.2.3.2.cmml" xref="S4.SS2.SSS0.Px1.p5.2.m2.2.3.2">𝐴</ci><interval closure="open" id="S4.SS2.SSS0.Px1.p5.2.m2.2.2.2.3.cmml" xref="S4.SS2.SSS0.Px1.p5.2.m2.2.2.2.4"><ci id="S4.SS2.SSS0.Px1.p5.2.m2.1.1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p5.2.m2.1.1.1.1">𝑙</ci><ci id="S4.SS2.SSS0.Px1.p5.2.m2.2.2.2.2.cmml" xref="S4.SS2.SSS0.Px1.p5.2.m2.2.2.2.2">ℎ</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p5.2.m2.2c">A^{(l,h)}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px1.p5.2.m2.2d">italic_A start_POSTSUPERSCRIPT ( italic_l , italic_h ) end_POSTSUPERSCRIPT</annotation></semantics></math>) by <math alttext="\tau^{(l,h)}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p5.3.m3.2"><semantics id="S4.SS2.SSS0.Px1.p5.3.m3.2a"><msup id="S4.SS2.SSS0.Px1.p5.3.m3.2.3" xref="S4.SS2.SSS0.Px1.p5.3.m3.2.3.cmml"><mi id="S4.SS2.SSS0.Px1.p5.3.m3.2.3.2" xref="S4.SS2.SSS0.Px1.p5.3.m3.2.3.2.cmml">τ</mi><mrow id="S4.SS2.SSS0.Px1.p5.3.m3.2.2.2.4" xref="S4.SS2.SSS0.Px1.p5.3.m3.2.2.2.3.cmml"><mo id="S4.SS2.SSS0.Px1.p5.3.m3.2.2.2.4.1" stretchy="false" xref="S4.SS2.SSS0.Px1.p5.3.m3.2.2.2.3.cmml">(</mo><mi id="S4.SS2.SSS0.Px1.p5.3.m3.1.1.1.1" xref="S4.SS2.SSS0.Px1.p5.3.m3.1.1.1.1.cmml">l</mi><mo id="S4.SS2.SSS0.Px1.p5.3.m3.2.2.2.4.2" xref="S4.SS2.SSS0.Px1.p5.3.m3.2.2.2.3.cmml">,</mo><mi id="S4.SS2.SSS0.Px1.p5.3.m3.2.2.2.2" xref="S4.SS2.SSS0.Px1.p5.3.m3.2.2.2.2.cmml">h</mi><mo id="S4.SS2.SSS0.Px1.p5.3.m3.2.2.2.4.3" stretchy="false" xref="S4.SS2.SSS0.Px1.p5.3.m3.2.2.2.3.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p5.3.m3.2b"><apply id="S4.SS2.SSS0.Px1.p5.3.m3.2.3.cmml" xref="S4.SS2.SSS0.Px1.p5.3.m3.2.3"><csymbol cd="ambiguous" id="S4.SS2.SSS0.Px1.p5.3.m3.2.3.1.cmml" xref="S4.SS2.SSS0.Px1.p5.3.m3.2.3">superscript</csymbol><ci id="S4.SS2.SSS0.Px1.p5.3.m3.2.3.2.cmml" xref="S4.SS2.SSS0.Px1.p5.3.m3.2.3.2">𝜏</ci><interval closure="open" id="S4.SS2.SSS0.Px1.p5.3.m3.2.2.2.3.cmml" xref="S4.SS2.SSS0.Px1.p5.3.m3.2.2.2.4"><ci id="S4.SS2.SSS0.Px1.p5.3.m3.1.1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p5.3.m3.1.1.1.1">𝑙</ci><ci id="S4.SS2.SSS0.Px1.p5.3.m3.2.2.2.2.cmml" xref="S4.SS2.SSS0.Px1.p5.3.m3.2.2.2.2">ℎ</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p5.3.m3.2c">\tau^{(l,h)}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px1.p5.3.m3.2d">italic_τ start_POSTSUPERSCRIPT ( italic_l , italic_h ) end_POSTSUPERSCRIPT</annotation></semantics></math>, which is equivalent to Eq. <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S4.E2" title="In Optimization ‣ 4.2 Re-weighting coefficient learning ‣ 4 Methodology ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_tag">2</span></a> and does not add any extra computation during inference.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Inference on Downstream Tasks</h4>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p1.1">We highlight several points regarding the inference process of our proposed <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS0.Px2.p1.1.1">PEAR</span> on downstream RAG tasks:</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS0.Px2.p2">
<ol class="ltx_enumerate" id="S4.I3">
<li class="ltx_item" id="S4.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I3.i1.p1">
<p class="ltx_p" id="S4.I3.i1.p1.1">In downstream RAG tasks, the re-weighting coefficients are task-independent and remain fixed.</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para ltx_noindent" id="S4.I3.i2.p1">
<p class="ltx_p" id="S4.I3.i2.p1.1">RAG-suppression heads are optimized once for each LLM via the proxy task.
For a new RAG task, head discovery and coefficient learning do not need to be repeated.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS0.Px2.p3">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p3.1">In theory, our approach, <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS0.Px2.p3.1.1">PEAR</span>, introduces <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS0.Px2.p3.1.2">zero</span> additional overhead during inference on downstream RAG tasks, as it does not incorporate extra computational modules; instead, it only adjusts the aggregation weights of specific heads.
Additionally, the learning of re-weighting coefficients is independent of the LLM architecture, thus making our method compatible with various position embedding algorithms.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Setup</h3>
<div class="ltx_para ltx_noindent" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">In this section, we introduce the LLMs we used for experiments, the baseline methods for enhancing context awareness, the setups for the proxy tasks, hyperparameters for learning re-weighting coefficients.</p>
</div>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Models and baselines</h4>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px1.p1.1">We conducted experiments with three LLMs, each employing a different position embedding algorithm: Llama2-7B-chat-4k <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib28" title="">2023</a>)</cite> using RoPE <cite class="ltx_cite ltx_citemacro_citep">(Su et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib27" title="">2023</a>)</cite>, OPT-6.7B-2k <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib34" title="">2022</a>)</cite> using learnable position embeddings, and Baichuan-13B-4k <cite class="ltx_cite ltx_citemacro_citep">(Baichuan, <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib1" title="">2023</a>)</cite> using Alibi position embeddings <cite class="ltx_cite ltx_citemacro_citep">(Press et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib24" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS0.Px1.p2">
<p class="ltx_p" id="S5.SS1.SSS0.Px1.p2.1">We also compared several competitive baseline methods for enhancing LLMs’ context awareness, including Attention Buckets (AB, <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib2" title="">2024</a>)</cite>), Ms-PoE <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib35" title="">2024</a>)</cite>, and MoICE <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib12" title="">2024</a>)</cite>.
Details on these methods can be found in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S2.SS1" title="2.1 Context awareness enhancement ‣ 2 Related works ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_tag">2.1</span></a>.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Discovered RAG-suppression heads in Llama2-Chat-7B-4k, OPT-6.7B-2k and Baichuan-13B-chat-4k, respectively.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T2.2" style="width:424.9pt;height:131.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-48.8pt,15.2pt) scale(0.813118631097333,0.813118631097333) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T2.2.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T2.2.2.2.3">Model Name</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.2.2.2.2">
<math alttext="(l,h)" class="ltx_Math" display="inline" id="S5.T2.1.1.1.1.m1.2"><semantics id="S5.T2.1.1.1.1.m1.2a"><mrow id="S5.T2.1.1.1.1.m1.2.3.2" xref="S5.T2.1.1.1.1.m1.2.3.1.cmml"><mo id="S5.T2.1.1.1.1.m1.2.3.2.1" stretchy="false" xref="S5.T2.1.1.1.1.m1.2.3.1.cmml">(</mo><mi id="S5.T2.1.1.1.1.m1.1.1" xref="S5.T2.1.1.1.1.m1.1.1.cmml">l</mi><mo id="S5.T2.1.1.1.1.m1.2.3.2.2" xref="S5.T2.1.1.1.1.m1.2.3.1.cmml">,</mo><mi id="S5.T2.1.1.1.1.m1.2.2" xref="S5.T2.1.1.1.1.m1.2.2.cmml">h</mi><mo id="S5.T2.1.1.1.1.m1.2.3.2.3" stretchy="false" xref="S5.T2.1.1.1.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.1.m1.2b"><interval closure="open" id="S5.T2.1.1.1.1.m1.2.3.1.cmml" xref="S5.T2.1.1.1.1.m1.2.3.2"><ci id="S5.T2.1.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.1.m1.1.1">𝑙</ci><ci id="S5.T2.1.1.1.1.m1.2.2.cmml" xref="S5.T2.1.1.1.1.m1.2.2">ℎ</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.1.m1.2c">(l,h)</annotation><annotation encoding="application/x-llamapun" id="S5.T2.1.1.1.1.m1.2d">( italic_l , italic_h )</annotation></semantics></math> for discovered <math alttext="A^{(l,h)}" class="ltx_Math" display="inline" id="S5.T2.2.2.2.2.m2.2"><semantics id="S5.T2.2.2.2.2.m2.2a"><msup id="S5.T2.2.2.2.2.m2.2.3" xref="S5.T2.2.2.2.2.m2.2.3.cmml"><mi id="S5.T2.2.2.2.2.m2.2.3.2" xref="S5.T2.2.2.2.2.m2.2.3.2.cmml">A</mi><mrow id="S5.T2.2.2.2.2.m2.2.2.2.4" xref="S5.T2.2.2.2.2.m2.2.2.2.3.cmml"><mo id="S5.T2.2.2.2.2.m2.2.2.2.4.1" stretchy="false" xref="S5.T2.2.2.2.2.m2.2.2.2.3.cmml">(</mo><mi id="S5.T2.2.2.2.2.m2.1.1.1.1" xref="S5.T2.2.2.2.2.m2.1.1.1.1.cmml">l</mi><mo id="S5.T2.2.2.2.2.m2.2.2.2.4.2" xref="S5.T2.2.2.2.2.m2.2.2.2.3.cmml">,</mo><mi id="S5.T2.2.2.2.2.m2.2.2.2.2" xref="S5.T2.2.2.2.2.m2.2.2.2.2.cmml">h</mi><mo id="S5.T2.2.2.2.2.m2.2.2.2.4.3" stretchy="false" xref="S5.T2.2.2.2.2.m2.2.2.2.3.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.2.m2.2b"><apply id="S5.T2.2.2.2.2.m2.2.3.cmml" xref="S5.T2.2.2.2.2.m2.2.3"><csymbol cd="ambiguous" id="S5.T2.2.2.2.2.m2.2.3.1.cmml" xref="S5.T2.2.2.2.2.m2.2.3">superscript</csymbol><ci id="S5.T2.2.2.2.2.m2.2.3.2.cmml" xref="S5.T2.2.2.2.2.m2.2.3.2">𝐴</ci><interval closure="open" id="S5.T2.2.2.2.2.m2.2.2.2.3.cmml" xref="S5.T2.2.2.2.2.m2.2.2.2.4"><ci id="S5.T2.2.2.2.2.m2.1.1.1.1.cmml" xref="S5.T2.2.2.2.2.m2.1.1.1.1">𝑙</ci><ci id="S5.T2.2.2.2.2.m2.2.2.2.2.cmml" xref="S5.T2.2.2.2.2.m2.2.2.2.2">ℎ</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.2.m2.2c">A^{(l,h)}</annotation><annotation encoding="application/x-llamapun" id="S5.T2.2.2.2.2.m2.2d">italic_A start_POSTSUPERSCRIPT ( italic_l , italic_h ) end_POSTSUPERSCRIPT</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.2.2.3.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T2.2.2.3.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.3.1.2">(26, 28), (11, 6), (14, 15), (30, 9), (18, 9), (15, 10), (13, 9), (12, 10), (15, 14), (10, 18),</td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.2.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T2.2.2.4.2.1">Llama2-7B-chat-4k</th>
<td class="ltx_td ltx_align_center" id="S5.T2.2.2.4.2.2">(15, 25), (19, 15), (29, 15), (14, 0), (10, 2),
(31, 17), (8, 22), (17, 0), (20, 26), (9, 13),</td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.2.5.3">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S5.T2.2.2.5.3.1"></th>
<td class="ltx_td ltx_align_center" id="S5.T2.2.2.5.3.2">(13, 14), (7, 9), (10, 1), (15, 12), (11, 9), (15, 7), (9, 16), (26, 9), (28, 22), (15, 2)</td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.2.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T2.2.2.6.4.1" rowspan="2"><span class="ltx_text" id="S5.T2.2.2.6.4.1.1">OPT-6.7B-2k</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.6.4.2">(29, 19), (0, 22), (0, 6), (26, 16), (26, 15), (30, 19), (0, 18), (23, 30), (0, 10), (31, 31),</td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.2.7.5">
<td class="ltx_td ltx_align_center" id="S5.T2.2.2.7.5.1">(28, 6), (30, 30), (21, 27), (0, 17), (31, 25), (12, 23), (22, 16), (0, 0), (23, 0), (0, 1), (24, 31), (23, 8)</td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.2.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S5.T2.2.2.8.6.1" rowspan="2"><span class="ltx_text" id="S5.T2.2.2.8.6.1.1">Baichuan-13B-chat-4k</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.8.6.2">(26, 22), (33, 25), (28, 26), (32, 13), (23, 20), (25, 24), (19, 20), (38, 16), (22, 21), (21, 12),</td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.2.9.7">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.2.2.9.7.1">(3, 24), (39, 39), (20, 27), (37, 21), (0, 32), (24, 39), (39, 28), (39, 20), (27, 24), (2, 20), (36, 10)</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="238" id="S5.F3.g1" src="x3.png" width="813"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Heatmaps of <math alttext="\Delta\pi" class="ltx_Math" display="inline" id="S5.F3.3.m1.1"><semantics id="S5.F3.3.m1.1b"><mrow id="S5.F3.3.m1.1.1" xref="S5.F3.3.m1.1.1.cmml"><mi id="S5.F3.3.m1.1.1.2" mathvariant="normal" xref="S5.F3.3.m1.1.1.2.cmml">Δ</mi><mo id="S5.F3.3.m1.1.1.1" xref="S5.F3.3.m1.1.1.1.cmml">⁢</mo><mi id="S5.F3.3.m1.1.1.3" xref="S5.F3.3.m1.1.1.3.cmml">π</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.F3.3.m1.1c"><apply id="S5.F3.3.m1.1.1.cmml" xref="S5.F3.3.m1.1.1"><times id="S5.F3.3.m1.1.1.1.cmml" xref="S5.F3.3.m1.1.1.1"></times><ci id="S5.F3.3.m1.1.1.2.cmml" xref="S5.F3.3.m1.1.1.2">Δ</ci><ci id="S5.F3.3.m1.1.1.3.cmml" xref="S5.F3.3.m1.1.1.3">𝜋</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F3.3.m1.1d">\Delta\pi</annotation><annotation encoding="application/x-llamapun" id="S5.F3.3.m1.1e">roman_Δ italic_π</annotation></semantics></math> scores for each head across three LLMs (<math alttext="n=10" class="ltx_Math" display="inline" id="S5.F3.4.m2.1"><semantics id="S5.F3.4.m2.1b"><mrow id="S5.F3.4.m2.1.1" xref="S5.F3.4.m2.1.1.cmml"><mi id="S5.F3.4.m2.1.1.2" xref="S5.F3.4.m2.1.1.2.cmml">n</mi><mo id="S5.F3.4.m2.1.1.1" xref="S5.F3.4.m2.1.1.1.cmml">=</mo><mn id="S5.F3.4.m2.1.1.3" xref="S5.F3.4.m2.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.F3.4.m2.1c"><apply id="S5.F3.4.m2.1.1.cmml" xref="S5.F3.4.m2.1.1"><eq id="S5.F3.4.m2.1.1.1.cmml" xref="S5.F3.4.m2.1.1.1"></eq><ci id="S5.F3.4.m2.1.1.2.cmml" xref="S5.F3.4.m2.1.1.2">𝑛</ci><cn id="S5.F3.4.m2.1.1.3.cmml" type="integer" xref="S5.F3.4.m2.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F3.4.m2.1d">n=10</annotation><annotation encoding="application/x-llamapun" id="S5.F3.4.m2.1e">italic_n = 10</annotation></semantics></math>).</figcaption>
</figure>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Performance comparison of Llama2-7B-chat-4k and its enhancements across three RAG tasks.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T3.4.4.5"><span class="ltx_text ltx_font_bold" id="S5.T3.4.4.5.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.1">2WikiMultiHopQA<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.1.1.1.1.m1.1"><semantics id="S5.T3.1.1.1.1.m1.1a"><mo id="S5.T3.1.1.1.1.m1.1.1" stretchy="false" xref="S5.T3.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.1.m1.1b"><ci id="S5.T3.1.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.1.1.1.1.m1.1d">↑</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.2.2.2"><span class="ltx_text ltx_font_bold" id="S5.T3.2.2.2.1">MuSiQue<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.2.2.2.1.m1.1"><semantics id="S5.T3.2.2.2.1.m1.1a"><mo id="S5.T3.2.2.2.1.m1.1.1" stretchy="false" xref="S5.T3.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.2.1.m1.1b"><ci id="S5.T3.2.2.2.1.m1.1.1.cmml" xref="S5.T3.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.2.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.2.2.2.1.m1.1d">↑</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.3.3.3"><span class="ltx_text ltx_font_bold" id="S5.T3.3.3.3.1">Qasper<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.3.3.3.1.m1.1"><semantics id="S5.T3.3.3.3.1.m1.1a"><mo id="S5.T3.3.3.3.1.m1.1.1" stretchy="false" xref="S5.T3.3.3.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T3.3.3.3.1.m1.1b"><ci id="S5.T3.3.3.3.1.m1.1.1.cmml" xref="S5.T3.3.3.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.3.3.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.3.3.3.1.m1.1d">↑</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.4.4.4">
<span class="ltx_text ltx_font_bold" id="S5.T3.4.4.4.1">Avg.</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.4.4.4.m1.1"><semantics id="S5.T3.4.4.4.m1.1a"><mo id="S5.T3.4.4.4.m1.1.1" stretchy="false" xref="S5.T3.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T3.4.4.4.m1.1b"><ci id="S5.T3.4.4.4.m1.1.1.cmml" xref="S5.T3.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.4.4.4.m1.1d">↑</annotation></semantics></math>
</th>
</tr>
<tr class="ltx_tr" id="S5.T3.4.5.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T3.4.5.1.1">Llama2-7B-chat-4k</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.5.1.2">29.50</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.5.1.3">6.50</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.5.1.4">17.00</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.5.1.5">17.67</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.4.6.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T3.4.6.1.1">+ Ms-PoE <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib35" title="">2024</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.6.1.2">27.50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.6.1.3">9.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.6.1.4"><span class="ltx_text ltx_font_bold" id="S5.T3.4.6.1.4.1">18.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.6.1.5">18.17</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.7.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.4.7.2.1">+ AB <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib2" title="">2024</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T3.4.7.2.2">31.00</td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.7.2.3"><span class="ltx_text ltx_font_bold" id="S5.T3.4.7.2.3.1">11.00</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.7.2.4">16.50</td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.7.2.5">19.50</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.8.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.4.8.3.1">+ MoICE <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib12" title="">2024</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T3.4.8.3.2">30.00</td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.8.3.3">10.00</td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.8.3.4">15.50</td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.8.3.5">18.50</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.9.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T3.4.9.4.1">+ <span class="ltx_text ltx_font_italic" id="S5.T3.4.9.4.1.1">PEAR</span> (Ours)</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.4.9.4.2"><span class="ltx_text ltx_font_bold" id="S5.T3.4.9.4.2.1">35.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.4.9.4.3">8.50</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.4.9.4.4"><span class="ltx_text ltx_font_bold" id="S5.T3.4.9.4.4.1">18.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.4.9.4.5"><span class="ltx_text ltx_font_bold" id="S5.T3.4.9.4.5.1">20.50</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S5.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Practical inference time (in seconds) and GPU memory cost (in GB) per test sample for different methods.
For a fair comparison, Flash-Attention <cite class="ltx_cite ltx_citemacro_citep">(Dao, <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib3" title="">2024</a>)</cite> was not applied.
The experiments were conducted on a single H800-80G GPU.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T4.4" style="width:433.6pt;height:52.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-242.9pt,29.3pt) scale(0.471597339725356,0.471597339725356) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T4.4.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.4.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T4.4.4.4.5" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.4.4.4.5.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.1.1.1.1" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.1.1">2WikiMultiHopQA<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T4.1.1.1.1.1.m1.1"><semantics id="S5.T4.1.1.1.1.1.m1.1a"><mo id="S5.T4.1.1.1.1.1.m1.1.1" stretchy="false" xref="S5.T4.1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.1.1.m1.1b"><ci id="S5.T4.1.1.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.1.1.1.1.1.m1.1d">↓</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.2.2.2.2" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.2.2.2.2.1">MuSiQue<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T4.2.2.2.2.1.m1.1"><semantics id="S5.T4.2.2.2.2.1.m1.1a"><mo id="S5.T4.2.2.2.2.1.m1.1.1" stretchy="false" xref="S5.T4.2.2.2.2.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T4.2.2.2.2.1.m1.1b"><ci id="S5.T4.2.2.2.2.1.m1.1.1.cmml" xref="S5.T4.2.2.2.2.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.2.2.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.2.2.2.2.1.m1.1d">↓</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.3.3.3.3" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.3.3.3.3.1">Qasper<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T4.3.3.3.3.1.m1.1"><semantics id="S5.T4.3.3.3.3.1.m1.1a"><mo id="S5.T4.3.3.3.3.1.m1.1.1" stretchy="false" xref="S5.T4.3.3.3.3.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T4.3.3.3.3.1.m1.1b"><ci id="S5.T4.3.3.3.3.1.m1.1.1.cmml" xref="S5.T4.3.3.3.3.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.3.3.3.3.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.3.3.3.3.1.m1.1d">↓</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.4.4.4.4" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.4.4.4.4.1">Avg.<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T4.4.4.4.4.1.m1.1"><semantics id="S5.T4.4.4.4.4.1.m1.1a"><mo id="S5.T4.4.4.4.4.1.m1.1.1" stretchy="false" xref="S5.T4.4.4.4.4.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T4.4.4.4.4.1.m1.1b"><ci id="S5.T4.4.4.4.4.1.m1.1.1.cmml" xref="S5.T4.4.4.4.4.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.4.4.4.4.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.4.4.4.4.1.m1.1d">↓</annotation></semantics></math></span></th>
</tr>
<tr class="ltx_tr" id="S5.T4.4.4.5.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T4.4.4.5.1.1" style="padding-top:0.25pt;padding-bottom:0.25pt;">Llama2-7B-chat-4k</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.4.4.5.1.2" style="padding-top:0.25pt;padding-bottom:0.25pt;">0.63/31.33</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.4.4.5.1.3" style="padding-top:0.25pt;padding-bottom:0.25pt;">0.70/31.33</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.4.4.5.1.4" style="padding-top:0.25pt;padding-bottom:0.25pt;">1.23/31.33</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.4.4.5.1.5" style="padding-top:0.25pt;padding-bottom:0.25pt;">0.88/31.33</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.4.4.6.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T4.4.4.6.1.1" style="padding-top:0.25pt;padding-bottom:0.25pt;">+ Ms-PoE <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib35" title="">2024</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.4.6.1.2" style="padding-top:0.25pt;padding-bottom:0.25pt;">0.95 (<span class="ltx_text" id="S5.T4.4.4.6.1.2.1" style="color:#FF0000;">+0.32</span>) / 39.21 (<span class="ltx_text" id="S5.T4.4.4.6.1.2.2" style="color:#FF0000;">+7.88</span>)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.4.6.1.3" style="padding-top:0.25pt;padding-bottom:0.25pt;">1.11(<span class="ltx_text" id="S5.T4.4.4.6.1.3.1" style="color:#FF0000;">+0.41</span>) / 39.21 (<span class="ltx_text" id="S5.T4.4.4.6.1.3.2" style="color:#FF0000;">+7.88</span>)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.4.6.1.4" style="padding-top:0.25pt;padding-bottom:0.25pt;">1.84 (<span class="ltx_text" id="S5.T4.4.4.6.1.4.1" style="color:#FF0000;">+0.61</span>) / 34.59 (<span class="ltx_text" id="S5.T4.4.4.6.1.4.2" style="color:#FF0000;">+3.26</span>)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.4.6.1.5" style="padding-top:0.25pt;padding-bottom:0.25pt;">1.30 (<span class="ltx_text" id="S5.T4.4.4.6.1.5.1" style="color:#FF0000;">+0.42</span>) /37.67 (<span class="ltx_text" id="S5.T4.4.4.6.1.5.2" style="color:#FF0000;">+6.34</span>)</td>
</tr>
<tr class="ltx_tr" id="S5.T4.4.4.7.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.4.4.7.2.1" style="padding-top:0.25pt;padding-bottom:0.25pt;">+ AB <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib2" title="">2024</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T4.4.4.7.2.2" style="padding-top:0.25pt;padding-bottom:0.25pt;">2.50(<span class="ltx_text" id="S5.T4.4.4.7.2.2.1" style="color:#FF0000;">+1.87</span>) /66.19(<span class="ltx_text" id="S5.T4.4.4.7.2.2.2" style="color:#FF0000;">+34.86</span>)</td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.4.7.2.3" style="padding-top:0.25pt;padding-bottom:0.25pt;">2.70(<span class="ltx_text" id="S5.T4.4.4.7.2.3.1" style="color:#FF0000;">+2.00</span>) / 66.19 (<span class="ltx_text" id="S5.T4.4.4.7.2.3.2" style="color:#FF0000;">+34.86</span>)</td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.4.7.2.4" style="padding-top:0.25pt;padding-bottom:0.25pt;">5.67(<span class="ltx_text" id="S5.T4.4.4.7.2.4.1" style="color:#FF0000;">+4.44</span>) /66.34(<span class="ltx_text" id="S5.T4.4.4.7.2.4.2" style="color:#FF0000;">+35.01</span>)</td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.4.7.2.5" style="padding-top:0.25pt;padding-bottom:0.25pt;">3.62(<span class="ltx_text" id="S5.T4.4.4.7.2.5.1" style="color:#FF0000;">+2.74</span>)/66.24(<span class="ltx_text" id="S5.T4.4.4.7.2.5.2" style="color:#FF0000;">+34.91</span>)</td>
</tr>
<tr class="ltx_tr" id="S5.T4.4.4.8.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.4.4.8.3.1" style="padding-top:0.25pt;padding-bottom:0.25pt;">+ MoICE <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib12" title="">2024</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T4.4.4.8.3.2" style="padding-top:0.25pt;padding-bottom:0.25pt;">2.91(<span class="ltx_text" id="S5.T4.4.4.8.3.2.1" style="color:#FF0000;">+2.28</span>) / 79.13(<span class="ltx_text" id="S5.T4.4.4.8.3.2.2" style="color:#FF0000;">+47.80</span>)</td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.4.8.3.3" style="padding-top:0.25pt;padding-bottom:0.25pt;">3.06(<span class="ltx_text" id="S5.T4.4.4.8.3.3.1" style="color:#FF0000;">+2.36</span>) / 79.12(<span class="ltx_text" id="S5.T4.4.4.8.3.3.2" style="color:#FF0000;">+47.79</span>)</td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.4.8.3.4" style="padding-top:0.25pt;padding-bottom:0.25pt;">5.86(<span class="ltx_text" id="S5.T4.4.4.8.3.4.1" style="color:#FF0000;">+4.63</span>) / 79.10(<span class="ltx_text" id="S5.T4.4.4.8.3.4.2" style="color:#FF0000;">+47.77</span>)</td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.4.8.3.5" style="padding-top:0.25pt;padding-bottom:0.25pt;">3.94(<span class="ltx_text" id="S5.T4.4.4.8.3.5.1" style="color:#FF0000;">+3.06</span>) / 79.12(<span class="ltx_text" id="S5.T4.4.4.8.3.5.2" style="color:#FF0000;">+47.79</span>)</td>
</tr>
<tr class="ltx_tr" id="S5.T4.4.4.9.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T4.4.4.9.4.1" style="padding-top:0.25pt;padding-bottom:0.25pt;">+ <span class="ltx_text ltx_font_italic" id="S5.T4.4.4.9.4.1.1">PEAR</span> (Ours)</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.4.4.9.4.2" style="padding-top:0.25pt;padding-bottom:0.25pt;">0.63 (<span class="ltx_text ltx_font_bold" id="S5.T4.4.4.9.4.2.1" style="color:#0000FF;">+0.00</span>) /31.33 (<span class="ltx_text ltx_font_bold" id="S5.T4.4.4.9.4.2.2" style="color:#0000FF;">+0.00</span>)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.4.4.9.4.3" style="padding-top:0.25pt;padding-bottom:0.25pt;">0.70 (<span class="ltx_text ltx_font_bold" id="S5.T4.4.4.9.4.3.1" style="color:#0000FF;">+0.00</span>) / 31.33 (<span class="ltx_text ltx_font_bold" id="S5.T4.4.4.9.4.3.2" style="color:#0000FF;">+0.00</span>)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.4.4.9.4.4" style="padding-top:0.25pt;padding-bottom:0.25pt;">1.23 (<span class="ltx_text ltx_font_bold" id="S5.T4.4.4.9.4.4.1" style="color:#0000FF;">+0.00</span>) / 31.33 (<span class="ltx_text ltx_font_bold" id="S5.T4.4.4.9.4.4.2" style="color:#0000FF;">+0.00</span>)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.4.4.9.4.5" style="padding-top:0.25pt;padding-bottom:0.25pt;">0.88 (<span class="ltx_text ltx_font_bold" id="S5.T4.4.4.9.4.5.1" style="color:#0000FF;">+0.00</span>) / 31.33 (<span class="ltx_text ltx_font_bold" id="S5.T4.4.4.9.4.5.2" style="color:#0000FF;">+0.00</span>)</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Detailed setups of proxy task</h4>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px2.p1.8">For head discovery, we constructed 200 task samples.
In the case of the Llama and OPT models, we repeat the discovery process four times with varying values of <math alttext="n" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px2.p1.1.m1.1"><semantics id="S5.SS1.SSS0.Px2.p1.1.m1.1a"><mi id="S5.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S5.SS1.SSS0.Px2.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px2.p1.1.m1.1b"><ci id="S5.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S5.SS1.SSS0.Px2.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px2.p1.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px2.p1.1.m1.1d">italic_n</annotation></semantics></math>: <math alttext="10,15,25,50" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px2.p1.2.m2.4"><semantics id="S5.SS1.SSS0.Px2.p1.2.m2.4a"><mrow id="S5.SS1.SSS0.Px2.p1.2.m2.4.5.2" xref="S5.SS1.SSS0.Px2.p1.2.m2.4.5.1.cmml"><mn id="S5.SS1.SSS0.Px2.p1.2.m2.1.1" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1.cmml">10</mn><mo id="S5.SS1.SSS0.Px2.p1.2.m2.4.5.2.1" xref="S5.SS1.SSS0.Px2.p1.2.m2.4.5.1.cmml">,</mo><mn id="S5.SS1.SSS0.Px2.p1.2.m2.2.2" xref="S5.SS1.SSS0.Px2.p1.2.m2.2.2.cmml">15</mn><mo id="S5.SS1.SSS0.Px2.p1.2.m2.4.5.2.2" xref="S5.SS1.SSS0.Px2.p1.2.m2.4.5.1.cmml">,</mo><mn id="S5.SS1.SSS0.Px2.p1.2.m2.3.3" xref="S5.SS1.SSS0.Px2.p1.2.m2.3.3.cmml">25</mn><mo id="S5.SS1.SSS0.Px2.p1.2.m2.4.5.2.3" xref="S5.SS1.SSS0.Px2.p1.2.m2.4.5.1.cmml">,</mo><mn id="S5.SS1.SSS0.Px2.p1.2.m2.4.4" xref="S5.SS1.SSS0.Px2.p1.2.m2.4.4.cmml">50</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px2.p1.2.m2.4b"><list id="S5.SS1.SSS0.Px2.p1.2.m2.4.5.1.cmml" xref="S5.SS1.SSS0.Px2.p1.2.m2.4.5.2"><cn id="S5.SS1.SSS0.Px2.p1.2.m2.1.1.cmml" type="integer" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1">10</cn><cn id="S5.SS1.SSS0.Px2.p1.2.m2.2.2.cmml" type="integer" xref="S5.SS1.SSS0.Px2.p1.2.m2.2.2">15</cn><cn id="S5.SS1.SSS0.Px2.p1.2.m2.3.3.cmml" type="integer" xref="S5.SS1.SSS0.Px2.p1.2.m2.3.3">25</cn><cn id="S5.SS1.SSS0.Px2.p1.2.m2.4.4.cmml" type="integer" xref="S5.SS1.SSS0.Px2.p1.2.m2.4.4">50</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px2.p1.2.m2.4c">10,15,25,50</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px2.p1.2.m2.4d">10 , 15 , 25 , 50</annotation></semantics></math>.
For the Baichuan model, the <math alttext="n" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px2.p1.3.m3.1"><semantics id="S5.SS1.SSS0.Px2.p1.3.m3.1a"><mi id="S5.SS1.SSS0.Px2.p1.3.m3.1.1" xref="S5.SS1.SSS0.Px2.p1.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px2.p1.3.m3.1b"><ci id="S5.SS1.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S5.SS1.SSS0.Px2.p1.3.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px2.p1.3.m3.1c">n</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px2.p1.3.m3.1d">italic_n</annotation></semantics></math> values are <math alttext="10,20,50,80" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px2.p1.4.m4.4"><semantics id="S5.SS1.SSS0.Px2.p1.4.m4.4a"><mrow id="S5.SS1.SSS0.Px2.p1.4.m4.4.5.2" xref="S5.SS1.SSS0.Px2.p1.4.m4.4.5.1.cmml"><mn id="S5.SS1.SSS0.Px2.p1.4.m4.1.1" xref="S5.SS1.SSS0.Px2.p1.4.m4.1.1.cmml">10</mn><mo id="S5.SS1.SSS0.Px2.p1.4.m4.4.5.2.1" xref="S5.SS1.SSS0.Px2.p1.4.m4.4.5.1.cmml">,</mo><mn id="S5.SS1.SSS0.Px2.p1.4.m4.2.2" xref="S5.SS1.SSS0.Px2.p1.4.m4.2.2.cmml">20</mn><mo id="S5.SS1.SSS0.Px2.p1.4.m4.4.5.2.2" xref="S5.SS1.SSS0.Px2.p1.4.m4.4.5.1.cmml">,</mo><mn id="S5.SS1.SSS0.Px2.p1.4.m4.3.3" xref="S5.SS1.SSS0.Px2.p1.4.m4.3.3.cmml">50</mn><mo id="S5.SS1.SSS0.Px2.p1.4.m4.4.5.2.3" xref="S5.SS1.SSS0.Px2.p1.4.m4.4.5.1.cmml">,</mo><mn id="S5.SS1.SSS0.Px2.p1.4.m4.4.4" xref="S5.SS1.SSS0.Px2.p1.4.m4.4.4.cmml">80</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px2.p1.4.m4.4b"><list id="S5.SS1.SSS0.Px2.p1.4.m4.4.5.1.cmml" xref="S5.SS1.SSS0.Px2.p1.4.m4.4.5.2"><cn id="S5.SS1.SSS0.Px2.p1.4.m4.1.1.cmml" type="integer" xref="S5.SS1.SSS0.Px2.p1.4.m4.1.1">10</cn><cn id="S5.SS1.SSS0.Px2.p1.4.m4.2.2.cmml" type="integer" xref="S5.SS1.SSS0.Px2.p1.4.m4.2.2">20</cn><cn id="S5.SS1.SSS0.Px2.p1.4.m4.3.3.cmml" type="integer" xref="S5.SS1.SSS0.Px2.p1.4.m4.3.3">50</cn><cn id="S5.SS1.SSS0.Px2.p1.4.m4.4.4.cmml" type="integer" xref="S5.SS1.SSS0.Px2.p1.4.m4.4.4">80</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px2.p1.4.m4.4c">10,20,50,80</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px2.p1.4.m4.4d">10 , 20 , 50 , 80</annotation></semantics></math>.
We found that each model has a group of heads with significantly large <math alttext="\Delta\pi" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px2.p1.5.m5.1"><semantics id="S5.SS1.SSS0.Px2.p1.5.m5.1a"><mrow id="S5.SS1.SSS0.Px2.p1.5.m5.1.1" xref="S5.SS1.SSS0.Px2.p1.5.m5.1.1.cmml"><mi id="S5.SS1.SSS0.Px2.p1.5.m5.1.1.2" mathvariant="normal" xref="S5.SS1.SSS0.Px2.p1.5.m5.1.1.2.cmml">Δ</mi><mo id="S5.SS1.SSS0.Px2.p1.5.m5.1.1.1" xref="S5.SS1.SSS0.Px2.p1.5.m5.1.1.1.cmml">⁢</mo><mi id="S5.SS1.SSS0.Px2.p1.5.m5.1.1.3" xref="S5.SS1.SSS0.Px2.p1.5.m5.1.1.3.cmml">π</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px2.p1.5.m5.1b"><apply id="S5.SS1.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S5.SS1.SSS0.Px2.p1.5.m5.1.1"><times id="S5.SS1.SSS0.Px2.p1.5.m5.1.1.1.cmml" xref="S5.SS1.SSS0.Px2.p1.5.m5.1.1.1"></times><ci id="S5.SS1.SSS0.Px2.p1.5.m5.1.1.2.cmml" xref="S5.SS1.SSS0.Px2.p1.5.m5.1.1.2">Δ</ci><ci id="S5.SS1.SSS0.Px2.p1.5.m5.1.1.3.cmml" xref="S5.SS1.SSS0.Px2.p1.5.m5.1.1.3">𝜋</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px2.p1.5.m5.1c">\Delta\pi</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px2.p1.5.m5.1d">roman_Δ italic_π</annotation></semantics></math> values, leading us to select the <math alttext="K" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px2.p1.6.m6.1"><semantics id="S5.SS1.SSS0.Px2.p1.6.m6.1a"><mi id="S5.SS1.SSS0.Px2.p1.6.m6.1.1" xref="S5.SS1.SSS0.Px2.p1.6.m6.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px2.p1.6.m6.1b"><ci id="S5.SS1.SSS0.Px2.p1.6.m6.1.1.cmml" xref="S5.SS1.SSS0.Px2.p1.6.m6.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px2.p1.6.m6.1c">K</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px2.p1.6.m6.1d">italic_K</annotation></semantics></math> values based on the observed group sizes: 30, 22, and 21, respectively.
Table <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S5.T2" title="Table 2 ‣ Models and baselines ‣ 5.1 Setup ‣ 5 Experiments ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_tag">2</span></a> presents the discovered RAG-suppression heads for the LLMs under study.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S5.F3" title="Figure 3 ‣ Models and baselines ‣ 5.1 Setup ‣ 5 Experiments ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates <math alttext="\Delta\pi" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px2.p1.7.m7.1"><semantics id="S5.SS1.SSS0.Px2.p1.7.m7.1a"><mrow id="S5.SS1.SSS0.Px2.p1.7.m7.1.1" xref="S5.SS1.SSS0.Px2.p1.7.m7.1.1.cmml"><mi id="S5.SS1.SSS0.Px2.p1.7.m7.1.1.2" mathvariant="normal" xref="S5.SS1.SSS0.Px2.p1.7.m7.1.1.2.cmml">Δ</mi><mo id="S5.SS1.SSS0.Px2.p1.7.m7.1.1.1" xref="S5.SS1.SSS0.Px2.p1.7.m7.1.1.1.cmml">⁢</mo><mi id="S5.SS1.SSS0.Px2.p1.7.m7.1.1.3" xref="S5.SS1.SSS0.Px2.p1.7.m7.1.1.3.cmml">π</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px2.p1.7.m7.1b"><apply id="S5.SS1.SSS0.Px2.p1.7.m7.1.1.cmml" xref="S5.SS1.SSS0.Px2.p1.7.m7.1.1"><times id="S5.SS1.SSS0.Px2.p1.7.m7.1.1.1.cmml" xref="S5.SS1.SSS0.Px2.p1.7.m7.1.1.1"></times><ci id="S5.SS1.SSS0.Px2.p1.7.m7.1.1.2.cmml" xref="S5.SS1.SSS0.Px2.p1.7.m7.1.1.2">Δ</ci><ci id="S5.SS1.SSS0.Px2.p1.7.m7.1.1.3.cmml" xref="S5.SS1.SSS0.Px2.p1.7.m7.1.1.3">𝜋</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px2.p1.7.m7.1c">\Delta\pi</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px2.p1.7.m7.1d">roman_Δ italic_π</annotation></semantics></math> values when <math alttext="n=10" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px2.p1.8.m8.1"><semantics id="S5.SS1.SSS0.Px2.p1.8.m8.1a"><mrow id="S5.SS1.SSS0.Px2.p1.8.m8.1.1" xref="S5.SS1.SSS0.Px2.p1.8.m8.1.1.cmml"><mi id="S5.SS1.SSS0.Px2.p1.8.m8.1.1.2" xref="S5.SS1.SSS0.Px2.p1.8.m8.1.1.2.cmml">n</mi><mo id="S5.SS1.SSS0.Px2.p1.8.m8.1.1.1" xref="S5.SS1.SSS0.Px2.p1.8.m8.1.1.1.cmml">=</mo><mn id="S5.SS1.SSS0.Px2.p1.8.m8.1.1.3" xref="S5.SS1.SSS0.Px2.p1.8.m8.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px2.p1.8.m8.1b"><apply id="S5.SS1.SSS0.Px2.p1.8.m8.1.1.cmml" xref="S5.SS1.SSS0.Px2.p1.8.m8.1.1"><eq id="S5.SS1.SSS0.Px2.p1.8.m8.1.1.1.cmml" xref="S5.SS1.SSS0.Px2.p1.8.m8.1.1.1"></eq><ci id="S5.SS1.SSS0.Px2.p1.8.m8.1.1.2.cmml" xref="S5.SS1.SSS0.Px2.p1.8.m8.1.1.2">𝑛</ci><cn id="S5.SS1.SSS0.Px2.p1.8.m8.1.1.3.cmml" type="integer" xref="S5.SS1.SSS0.Px2.p1.8.m8.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px2.p1.8.m8.1c">n=10</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px2.p1.8.m8.1d">italic_n = 10</annotation></semantics></math> for each model.
Further detailed results can be found in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#A1" title="Appendix A Head discovery results ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS0.Px2.p2">
<p class="ltx_p" id="S5.SS1.SSS0.Px2.p2.1">For the re-weighting coefficient learning stage, we constructed 500 task samples, setting <math alttext="n" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px2.p2.1.m1.1"><semantics id="S5.SS1.SSS0.Px2.p2.1.m1.1a"><mi id="S5.SS1.SSS0.Px2.p2.1.m1.1.1" xref="S5.SS1.SSS0.Px2.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px2.p2.1.m1.1b"><ci id="S5.SS1.SSS0.Px2.p2.1.m1.1.1.cmml" xref="S5.SS1.SSS0.Px2.p2.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px2.p2.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px2.p2.1.m1.1d">italic_n</annotation></semantics></math> to 50 for all models.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Hyperparameters for re-weighting coefficient learning</h4>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px3.p1.2">We employed the AdamW optimizer with a learning rate of 0.005 and parameters <math alttext="(\beta_{1},\beta_{2})=(0.9,0.999)" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px3.p1.1.m1.4"><semantics id="S5.SS1.SSS0.Px3.p1.1.m1.4a"><mrow id="S5.SS1.SSS0.Px3.p1.1.m1.4.4" xref="S5.SS1.SSS0.Px3.p1.1.m1.4.4.cmml"><mrow id="S5.SS1.SSS0.Px3.p1.1.m1.4.4.2.2" xref="S5.SS1.SSS0.Px3.p1.1.m1.4.4.2.3.cmml"><mo id="S5.SS1.SSS0.Px3.p1.1.m1.4.4.2.2.3" stretchy="false" xref="S5.SS1.SSS0.Px3.p1.1.m1.4.4.2.3.cmml">(</mo><msub id="S5.SS1.SSS0.Px3.p1.1.m1.3.3.1.1.1" xref="S5.SS1.SSS0.Px3.p1.1.m1.3.3.1.1.1.cmml"><mi id="S5.SS1.SSS0.Px3.p1.1.m1.3.3.1.1.1.2" xref="S5.SS1.SSS0.Px3.p1.1.m1.3.3.1.1.1.2.cmml">β</mi><mn id="S5.SS1.SSS0.Px3.p1.1.m1.3.3.1.1.1.3" xref="S5.SS1.SSS0.Px3.p1.1.m1.3.3.1.1.1.3.cmml">1</mn></msub><mo id="S5.SS1.SSS0.Px3.p1.1.m1.4.4.2.2.4" xref="S5.SS1.SSS0.Px3.p1.1.m1.4.4.2.3.cmml">,</mo><msub id="S5.SS1.SSS0.Px3.p1.1.m1.4.4.2.2.2" xref="S5.SS1.SSS0.Px3.p1.1.m1.4.4.2.2.2.cmml"><mi id="S5.SS1.SSS0.Px3.p1.1.m1.4.4.2.2.2.2" xref="S5.SS1.SSS0.Px3.p1.1.m1.4.4.2.2.2.2.cmml">β</mi><mn id="S5.SS1.SSS0.Px3.p1.1.m1.4.4.2.2.2.3" xref="S5.SS1.SSS0.Px3.p1.1.m1.4.4.2.2.2.3.cmml">2</mn></msub><mo id="S5.SS1.SSS0.Px3.p1.1.m1.4.4.2.2.5" stretchy="false" xref="S5.SS1.SSS0.Px3.p1.1.m1.4.4.2.3.cmml">)</mo></mrow><mo id="S5.SS1.SSS0.Px3.p1.1.m1.4.4.3" xref="S5.SS1.SSS0.Px3.p1.1.m1.4.4.3.cmml">=</mo><mrow id="S5.SS1.SSS0.Px3.p1.1.m1.4.4.4.2" xref="S5.SS1.SSS0.Px3.p1.1.m1.4.4.4.1.cmml"><mo id="S5.SS1.SSS0.Px3.p1.1.m1.4.4.4.2.1" stretchy="false" xref="S5.SS1.SSS0.Px3.p1.1.m1.4.4.4.1.cmml">(</mo><mn id="S5.SS1.SSS0.Px3.p1.1.m1.1.1" xref="S5.SS1.SSS0.Px3.p1.1.m1.1.1.cmml">0.9</mn><mo id="S5.SS1.SSS0.Px3.p1.1.m1.4.4.4.2.2" xref="S5.SS1.SSS0.Px3.p1.1.m1.4.4.4.1.cmml">,</mo><mn id="S5.SS1.SSS0.Px3.p1.1.m1.2.2" xref="S5.SS1.SSS0.Px3.p1.1.m1.2.2.cmml">0.999</mn><mo id="S5.SS1.SSS0.Px3.p1.1.m1.4.4.4.2.3" stretchy="false" xref="S5.SS1.SSS0.Px3.p1.1.m1.4.4.4.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px3.p1.1.m1.4b"><apply id="S5.SS1.SSS0.Px3.p1.1.m1.4.4.cmml" xref="S5.SS1.SSS0.Px3.p1.1.m1.4.4"><eq id="S5.SS1.SSS0.Px3.p1.1.m1.4.4.3.cmml" xref="S5.SS1.SSS0.Px3.p1.1.m1.4.4.3"></eq><interval closure="open" id="S5.SS1.SSS0.Px3.p1.1.m1.4.4.2.3.cmml" xref="S5.SS1.SSS0.Px3.p1.1.m1.4.4.2.2"><apply id="S5.SS1.SSS0.Px3.p1.1.m1.3.3.1.1.1.cmml" xref="S5.SS1.SSS0.Px3.p1.1.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS0.Px3.p1.1.m1.3.3.1.1.1.1.cmml" xref="S5.SS1.SSS0.Px3.p1.1.m1.3.3.1.1.1">subscript</csymbol><ci id="S5.SS1.SSS0.Px3.p1.1.m1.3.3.1.1.1.2.cmml" xref="S5.SS1.SSS0.Px3.p1.1.m1.3.3.1.1.1.2">𝛽</ci><cn id="S5.SS1.SSS0.Px3.p1.1.m1.3.3.1.1.1.3.cmml" type="integer" xref="S5.SS1.SSS0.Px3.p1.1.m1.3.3.1.1.1.3">1</cn></apply><apply id="S5.SS1.SSS0.Px3.p1.1.m1.4.4.2.2.2.cmml" xref="S5.SS1.SSS0.Px3.p1.1.m1.4.4.2.2.2"><csymbol cd="ambiguous" id="S5.SS1.SSS0.Px3.p1.1.m1.4.4.2.2.2.1.cmml" xref="S5.SS1.SSS0.Px3.p1.1.m1.4.4.2.2.2">subscript</csymbol><ci id="S5.SS1.SSS0.Px3.p1.1.m1.4.4.2.2.2.2.cmml" xref="S5.SS1.SSS0.Px3.p1.1.m1.4.4.2.2.2.2">𝛽</ci><cn id="S5.SS1.SSS0.Px3.p1.1.m1.4.4.2.2.2.3.cmml" type="integer" xref="S5.SS1.SSS0.Px3.p1.1.m1.4.4.2.2.2.3">2</cn></apply></interval><interval closure="open" id="S5.SS1.SSS0.Px3.p1.1.m1.4.4.4.1.cmml" xref="S5.SS1.SSS0.Px3.p1.1.m1.4.4.4.2"><cn id="S5.SS1.SSS0.Px3.p1.1.m1.1.1.cmml" type="float" xref="S5.SS1.SSS0.Px3.p1.1.m1.1.1">0.9</cn><cn id="S5.SS1.SSS0.Px3.p1.1.m1.2.2.cmml" type="float" xref="S5.SS1.SSS0.Px3.p1.1.m1.2.2">0.999</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px3.p1.1.m1.4c">(\beta_{1},\beta_{2})=(0.9,0.999)</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px3.p1.1.m1.4d">( italic_β start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_β start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) = ( 0.9 , 0.999 )</annotation></semantics></math>.
<math alttext="\tau" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px3.p1.2.m2.1"><semantics id="S5.SS1.SSS0.Px3.p1.2.m2.1a"><mi id="S5.SS1.SSS0.Px3.p1.2.m2.1.1" xref="S5.SS1.SSS0.Px3.p1.2.m2.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px3.p1.2.m2.1b"><ci id="S5.SS1.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S5.SS1.SSS0.Px3.p1.2.m2.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px3.p1.2.m2.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px3.p1.2.m2.1d">italic_τ</annotation></semantics></math> are initialized at 1.0.
Training was performed for a single epoch using BF16 precision on an A100-PCIE-40GB GPU.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px4">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Difference between learning and inference</h4>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS0.Px4.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px4.p1.1">We conducted internal loading training on the re-weighting coefficients <math alttext="\tau^{(l,h)}" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px4.p1.1.m1.2"><semantics id="S5.SS1.SSS0.Px4.p1.1.m1.2a"><msup id="S5.SS1.SSS0.Px4.p1.1.m1.2.3" xref="S5.SS1.SSS0.Px4.p1.1.m1.2.3.cmml"><mi id="S5.SS1.SSS0.Px4.p1.1.m1.2.3.2" xref="S5.SS1.SSS0.Px4.p1.1.m1.2.3.2.cmml">τ</mi><mrow id="S5.SS1.SSS0.Px4.p1.1.m1.2.2.2.4" xref="S5.SS1.SSS0.Px4.p1.1.m1.2.2.2.3.cmml"><mo id="S5.SS1.SSS0.Px4.p1.1.m1.2.2.2.4.1" stretchy="false" xref="S5.SS1.SSS0.Px4.p1.1.m1.2.2.2.3.cmml">(</mo><mi id="S5.SS1.SSS0.Px4.p1.1.m1.1.1.1.1" xref="S5.SS1.SSS0.Px4.p1.1.m1.1.1.1.1.cmml">l</mi><mo id="S5.SS1.SSS0.Px4.p1.1.m1.2.2.2.4.2" xref="S5.SS1.SSS0.Px4.p1.1.m1.2.2.2.3.cmml">,</mo><mi id="S5.SS1.SSS0.Px4.p1.1.m1.2.2.2.2" xref="S5.SS1.SSS0.Px4.p1.1.m1.2.2.2.2.cmml">h</mi><mo id="S5.SS1.SSS0.Px4.p1.1.m1.2.2.2.4.3" stretchy="false" xref="S5.SS1.SSS0.Px4.p1.1.m1.2.2.2.3.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px4.p1.1.m1.2b"><apply id="S5.SS1.SSS0.Px4.p1.1.m1.2.3.cmml" xref="S5.SS1.SSS0.Px4.p1.1.m1.2.3"><csymbol cd="ambiguous" id="S5.SS1.SSS0.Px4.p1.1.m1.2.3.1.cmml" xref="S5.SS1.SSS0.Px4.p1.1.m1.2.3">superscript</csymbol><ci id="S5.SS1.SSS0.Px4.p1.1.m1.2.3.2.cmml" xref="S5.SS1.SSS0.Px4.p1.1.m1.2.3.2">𝜏</ci><interval closure="open" id="S5.SS1.SSS0.Px4.p1.1.m1.2.2.2.3.cmml" xref="S5.SS1.SSS0.Px4.p1.1.m1.2.2.2.4"><ci id="S5.SS1.SSS0.Px4.p1.1.m1.1.1.1.1.cmml" xref="S5.SS1.SSS0.Px4.p1.1.m1.1.1.1.1">𝑙</ci><ci id="S5.SS1.SSS0.Px4.p1.1.m1.2.2.2.2.cmml" xref="S5.SS1.SSS0.Px4.p1.1.m1.2.2.2.2">ℎ</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px4.p1.1.m1.2c">\tau^{(l,h)}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px4.p1.1.m1.2d">italic_τ start_POSTSUPERSCRIPT ( italic_l , italic_h ) end_POSTSUPERSCRIPT</annotation></semantics></math> during the training process. However, during inference, we externally re-weighting the output matrix weights of specific heads before loading the model. Therefore, our method is simple to train and has <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px4.p1.1.1">zero</span> inference overhead.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Comparison with baselines on RAG tasks</h3>
<div class="ltx_para ltx_noindent" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We compare <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.1">PEAR</span> against various baselines on RAG tasks we constructed using three datasets: 2WikiMultihopQA <cite class="ltx_cite ltx_citemacro_citep">(Ho et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib8" title="">2020</a>)</cite>, MuSiQue <cite class="ltx_cite ltx_citemacro_citep">(Trivedi et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib29" title="">2022</a>)</cite>, and Qasper <cite class="ltx_cite ltx_citemacro_citep">(Dasigi et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib4" title="">2021</a>)</cite>. The first two datasets require the model to answer questions based on multiple documents, while the third focuses on questions related to NLP research papers, formulated and answered by NLP researchers. We truncate the context to 4,000 tokens for the first two datasets; the third dataset has an average context length of 3,619 tokens.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">Our experiments are conducted with Llama2-7B-chat-4k, as the baselines are tailored specifically for RoPE.
We evaluate the models’ performance using exact match scores, with results reported in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S5.T3" title="Table 3 ‣ Models and baselines ‣ 5.1 Setup ‣ 5 Experiments ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_tag">3</span></a>.
Notably, our method achieves the highest average improvement across all three tasks.
Although <span class="ltx_text ltx_font_italic" id="S5.SS2.p2.1.1">PEAR</span> does not achieve the top performance on the MuSiQue task, it outperforms the original model by a large margin.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">Additionally, we present inference time and memory costs for these datasets in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S5.T4" title="Table 4 ‣ Models and baselines ‣ 5.1 Setup ‣ 5 Experiments ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_tag">4</span></a>.
<span class="ltx_text ltx_font_italic" id="S5.SS2.p3.1.1">PEAR</span> does not increase GPU memory usage and inference time costs.
This makes it significantly more efficient than other enhancement methods.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1">These experiments underscore the effectiveness and efficiency of <span class="ltx_text ltx_font_italic" id="S5.SS2.p4.1.1">PEAR</span> in enhancing LLMs for RAG tasks.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Applicability to LLMs using various position embeddings</h3>
<div class="ltx_para ltx_noindent" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">In this section, we demonstrate the applicability of <span class="ltx_text ltx_font_italic" id="S5.SS3.p1.1.1">PEAR</span> to LLMs utilizing different position embeddings.
We conduct a multi-document question-answering (MDQA) experiment based on data from <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib13" title="">2023</a>)</cite>, which leverage a subset of NaturalQuestions-Open  <cite class="ltx_cite ltx_citemacro_citep">(Lee et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib10" title="">2019</a>; Kwiatkowski et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib9" title="">2019</a>)</cite>, consisting of 2,655 queries. Each query is paired with a context consisting of 10 documents with an average of 1,722 tokens.
Following <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib13" title="">2023</a>; Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib2" title="">2024</a>)</cite>, we position the gold document (i.e., the document contains the ground truth answer) at various contextual positions to evaluate the robustness of a context-awareness enhancement method.
In our experiments, we set the maximum document count to 10 and assess the question-answering accuracy when the gold document is placed as the 1st, 3rd, 5th, 7th, and 10th document, respectively.
Since baseline methods are not compatible with the OPT and Baichuan models, we compare <span class="ltx_text ltx_font_italic" id="S5.SS3.p1.1.2">PEAR</span> only with the original models.
The results are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S5.T5" title="Table 5 ‣ 5.3 Applicability to LLMs using various position embeddings ‣ 5 Experiments ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure class="ltx_table" id="S5.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Experimental results on the MDQA task show that <span class="ltx_text ltx_font_italic" id="S5.T5.2.1">PEAR</span> achieves the highest accuracy in 24 out of 25 comparisons across three LLMs, demonstrating its broad applicability to various position embeddings and its robustness in enhancing awareness to different contextual positions.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T5.3" style="width:433.6pt;height:126.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-122.6pt,35.8pt) scale(0.638878914503949,0.638878914503949) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T5.3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.3.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" id="S5.T5.3.1.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T5.3.1.1.1.1.1">Position Embedding</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S5.T5.3.1.1.1.2" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T5.3.1.1.1.2.1">Method</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="5" id="S5.T5.3.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T5.3.1.1.1.3.1">Gold Document Position</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="S5.T5.3.1.1.1.4" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T5.3.1.1.1.4.1">Avg.</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.3.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.2.2.1"><span class="ltx_text ltx_font_bold" id="S5.T5.3.1.2.2.1.1">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.2.2.2"><span class="ltx_text ltx_font_bold" id="S5.T5.3.1.2.2.2.1">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.2.2.3"><span class="ltx_text ltx_font_bold" id="S5.T5.3.1.2.2.3.1">5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.2.2.4"><span class="ltx_text ltx_font_bold" id="S5.T5.3.1.2.2.4.1">7</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.2.2.5"><span class="ltx_text ltx_font_bold" id="S5.T5.3.1.2.2.5.1">10</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.3.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T5.3.1.3.3.1" rowspan="5"><span class="ltx_text" id="S5.T5.3.1.3.3.1.1">RoPE</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T5.3.1.3.3.2">Llama2-7B-chat-4k</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.3.3.3">64.14</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.3.3.4">65.95</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.3.3.5">64.97</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.3.3.6">62.67</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.3.3.7">67.53</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T5.3.1.3.3.8">65.05</td>
</tr>
<tr class="ltx_tr" id="S5.T5.3.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.3.1.4.4.1">+ Ms-PoE <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib35" title="">2024</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.4.4.2">66.06</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.4.4.3">64.29</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.4.4.4">63.99</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.4.4.5">62.22</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.4.4.6">64.75</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.3.1.4.4.7">64.34</td>
</tr>
<tr class="ltx_tr" id="S5.T5.3.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.3.1.5.5.1">+ AB <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib2" title="">2024</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.5.5.2"><span class="ltx_text ltx_font_bold" id="S5.T5.3.1.5.5.2.1">66.36</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.5.5.3">66.14</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.5.5.4">65.25</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.5.5.5">63.20</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.5.5.6">64.93</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.3.1.5.5.7">65.18</td>
</tr>
<tr class="ltx_tr" id="S5.T5.3.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.3.1.6.6.1">+ MoICE <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib12" title="">2024</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.6.6.2">65.50</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.6.6.3">66.33</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.6.6.4">65.61</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.6.6.5">64.11</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.6.6.6">65.84</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.3.1.6.6.7">65.48</td>
</tr>
<tr class="ltx_tr" id="S5.T5.3.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.3.1.7.7.1">+ <span class="ltx_text ltx_font_italic" id="S5.T5.3.1.7.7.1.1">PEAR</span> (Ours)</th>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.7.7.2">62.71</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.7.7.3"><span class="ltx_text ltx_font_bold" id="S5.T5.3.1.7.7.3.1">67.01</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.7.7.4"><span class="ltx_text ltx_font_bold" id="S5.T5.3.1.7.7.4.1">68.32</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.7.7.5"><span class="ltx_text ltx_font_bold" id="S5.T5.3.1.7.7.5.1">66.44</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.7.7.6"><span class="ltx_text ltx_font_bold" id="S5.T5.3.1.7.7.6.1">69.57</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.3.1.7.7.7"><span class="ltx_text ltx_font_bold" id="S5.T5.3.1.7.7.7.1">66.81</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.3.1.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T5.3.1.8.8.1" rowspan="2"><span class="ltx_text" id="S5.T5.3.1.8.8.1.1">Learnable Embeddings</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T5.3.1.8.8.2">OPT-6.7B-2k</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.8.8.3">19.07</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.8.8.4">15.45</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.8.8.5">17.03</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.8.8.6">16.54</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.8.8.7">22.61</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T5.3.1.8.8.8">18.14</td>
</tr>
<tr class="ltx_tr" id="S5.T5.3.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.3.1.9.9.1">+ <span class="ltx_text ltx_font_italic" id="S5.T5.3.1.9.9.1.1">PEAR</span> (Ours)</th>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.9.9.2"><span class="ltx_text ltx_font_bold" id="S5.T5.3.1.9.9.2.1">20.23</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.9.9.3"><span class="ltx_text ltx_font_bold" id="S5.T5.3.1.9.9.3.1">17.18</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.9.9.4"><span class="ltx_text ltx_font_bold" id="S5.T5.3.1.9.9.4.1">17.60</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.9.9.5"><span class="ltx_text ltx_font_bold" id="S5.T5.3.1.9.9.5.1">17.22</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.9.9.6"><span class="ltx_text ltx_font_bold" id="S5.T5.3.1.9.9.6.1">22.87</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.3.1.9.9.7"><span class="ltx_text ltx_font_bold" id="S5.T5.3.1.9.9.7.1">19.02</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.3.1.10.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S5.T5.3.1.10.10.1" rowspan="2"><span class="ltx_text" id="S5.T5.3.1.10.10.1.1">Alibi</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T5.3.1.10.10.2">Baichuan-13B-chat-4k</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.10.10.3">12.28</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.10.10.4">13.45</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.10.10.5">11.98</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.10.10.6">11.04</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.10.10.7">12.96</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T5.3.1.10.10.8">12.34</td>
</tr>
<tr class="ltx_tr" id="S5.T5.3.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T5.3.1.11.11.1">+ <span class="ltx_text ltx_font_italic" id="S5.T5.3.1.11.11.1.1">PEAR</span> (Ours)</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.3.1.11.11.2"><span class="ltx_text ltx_font_bold" id="S5.T5.3.1.11.11.2.1">14.16</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.3.1.11.11.3"><span class="ltx_text ltx_font_bold" id="S5.T5.3.1.11.11.3.1">14.84</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.3.1.11.11.4"><span class="ltx_text ltx_font_bold" id="S5.T5.3.1.11.11.4.1">13.67</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.3.1.11.11.5"><span class="ltx_text ltx_font_bold" id="S5.T5.3.1.11.11.5.1">12.77</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.3.1.11.11.6"><span class="ltx_text ltx_font_bold" id="S5.T5.3.1.11.11.6.1">13.94</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S5.T5.3.1.11.11.7"><span class="ltx_text ltx_font_bold" id="S5.T5.3.1.11.11.7.1">13.88</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span><span class="ltx_text ltx_font_italic" id="S5.SS4.1.1">PEAR</span> does not diminish knowledge capabilities in LLMs</h3>
<div class="ltx_para ltx_noindent" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">Previous research <cite class="ltx_cite ltx_citemacro_citep">(Geva et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib5" title="">2023</a>; Lv et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib15" title="">2024a</a>)</cite> has shown that certain attention heads store or play a crucial role in eliciting parametric knowledge.
This raises the question of whether <span class="ltx_text ltx_font_italic" id="S5.SS4.p1.1.1">PEAR</span> enhances context awareness of LLMs at the expense of their ability to utilize this parametric knowledge.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.1">To investigate this, we evaluated a <span class="ltx_text ltx_font_italic" id="S5.SS4.p2.1.1">PEAR</span>-enhanced Llama2-7B-chat model using the MMLU benchmark <cite class="ltx_cite ltx_citemacro_citep">(Hendrycks et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#bib.bib7" title="">2021</a>)</cite>, and the results are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S5.T6" title="Table 6 ‣ 5.4 PEAR does not diminish knowledge capabilities in LLMs ‣ 5 Experiments ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_tag">6</span></a>.
The performance of the enhanced Llama2-7B-chat and the original Llama2-7B-chat did not show a significant difference.
Consequently, we argue that <span class="ltx_text ltx_font_italic" id="S5.SS4.p2.1.2">PEAR</span>, through its effective head discovery and re-weighting learning approaches, does not compromise the knowledge capabilities of LLMs.</p>
</div>
<figure class="ltx_table" id="S5.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Results on the MMLU benchmark showing that <span class="ltx_text ltx_font_italic" id="S5.T6.2.1">PEAR</span> does not enhance LLMs’ context capacities at the expense of knowledge ability.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T6.3" style="width:346.9pt;height:54.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.8pt,-0.1pt) scale(1.00436558680424,1.00436558680424) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T6.3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T6.3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T6.3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T6.3.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T6.3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T6.3.1.1.1.2.1">Humanities</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T6.3.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T6.3.1.1.1.3.1">Social Science</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T6.3.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T6.3.1.1.1.4.1">STEM</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T6.3.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S5.T6.3.1.1.1.5.1">Other</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T6.3.1.1.1.6"><span class="ltx_text ltx_font_bold" id="S5.T6.3.1.1.1.6.1">Avg.</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T6.3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T6.3.1.2.1.1">Llama2-7B-chat-4k</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.3.1.2.1.2">42.55</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.3.1.2.1.3">52.29</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.3.1.2.1.4">37.14</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.3.1.2.1.5">52.47</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T6.3.1.2.1.6">45.81</td>
</tr>
<tr class="ltx_tr" id="S5.T6.3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T6.3.1.3.2.1">+ <span class="ltx_text ltx_font_italic" id="S5.T6.3.1.3.2.1.1">PEAR</span> (Ours)</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.3.1.3.2.2">42.06</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.3.1.3.2.3">52.03</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.3.1.3.2.4">36.61</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.3.1.3.2.5">52.19</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S5.T6.3.1.3.2.6">45.41</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_table" id="S5.T7">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>The experiment results on the question answering task with ablation settings, which show that our control over the number of suppression heads is effective.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T7.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T7.4.5.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T7.4.5.1.1" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span class="ltx_text ltx_font_bold" id="S5.T7.4.5.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T7.4.5.1.2" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span class="ltx_text ltx_font_bold" id="S5.T7.4.5.1.2.1">2WikiMultiHopQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T7.4.5.1.3" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span class="ltx_text ltx_font_bold" id="S5.T7.4.5.1.3.1">MuSiQue</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T7.4.5.1.4" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span class="ltx_text ltx_font_bold" id="S5.T7.4.5.1.4.1">Qasper</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T7.4.5.1.5" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span class="ltx_text ltx_font_bold" id="S5.T7.4.5.1.5.1">Avg.</span></th>
</tr>
<tr class="ltx_tr" id="S5.T7.4.6.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T7.4.6.2.1" style="padding-top:0.25pt;padding-bottom:0.25pt;">Llama2-7B-chat-4k</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T7.4.6.2.2" style="padding-top:0.25pt;padding-bottom:0.25pt;">29.50</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T7.4.6.2.3" style="padding-top:0.25pt;padding-bottom:0.25pt;">6.50</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T7.4.6.2.4" style="padding-top:0.25pt;padding-bottom:0.25pt;">17.00</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T7.4.6.2.5" style="padding-top:0.25pt;padding-bottom:0.25pt;">17.67</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T7.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T7.1.1.1" style="padding-top:0.25pt;padding-bottom:0.25pt;">+ <span class="ltx_text ltx_font_italic" id="S5.T7.1.1.1.1">PEAR</span> (<math alttext="K" class="ltx_Math" display="inline" id="S5.T7.1.1.1.m1.1"><semantics id="S5.T7.1.1.1.m1.1a"><mi id="S5.T7.1.1.1.m1.1.1" xref="S5.T7.1.1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S5.T7.1.1.1.m1.1b"><ci id="S5.T7.1.1.1.m1.1.1.cmml" xref="S5.T7.1.1.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.1.1.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S5.T7.1.1.1.m1.1d">italic_K</annotation></semantics></math>=10)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.2" style="padding-top:0.25pt;padding-bottom:0.25pt;">33.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.3" style="padding-top:0.25pt;padding-bottom:0.25pt;">8.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.4" style="padding-top:0.25pt;padding-bottom:0.25pt;">16.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.5" style="padding-top:0.25pt;padding-bottom:0.25pt;">19.00</td>
</tr>
<tr class="ltx_tr" id="S5.T7.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T7.2.2.1" style="padding-top:0.25pt;padding-bottom:0.25pt;">+ <span class="ltx_text ltx_font_italic" id="S5.T7.2.2.1.1">PEAR</span> (<math alttext="K" class="ltx_Math" display="inline" id="S5.T7.2.2.1.m1.1"><semantics id="S5.T7.2.2.1.m1.1a"><mi id="S5.T7.2.2.1.m1.1.1" xref="S5.T7.2.2.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S5.T7.2.2.1.m1.1b"><ci id="S5.T7.2.2.1.m1.1.1.cmml" xref="S5.T7.2.2.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.2.2.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S5.T7.2.2.1.m1.1d">italic_K</annotation></semantics></math>=20)</th>
<td class="ltx_td ltx_align_center" id="S5.T7.2.2.2" style="padding-top:0.25pt;padding-bottom:0.25pt;">33.50</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.2.3" style="padding-top:0.25pt;padding-bottom:0.25pt;">8.50</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.2.4" style="padding-top:0.25pt;padding-bottom:0.25pt;">16.50</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.2.5" style="padding-top:0.25pt;padding-bottom:0.25pt;">19.50</td>
</tr>
<tr class="ltx_tr" id="S5.T7.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T7.3.3.1" style="padding-top:0.25pt;padding-bottom:0.25pt;">+ <span class="ltx_text ltx_font_italic" id="S5.T7.3.3.1.1">PEAR</span> (<math alttext="K" class="ltx_Math" display="inline" id="S5.T7.3.3.1.m1.1"><semantics id="S5.T7.3.3.1.m1.1a"><mi id="S5.T7.3.3.1.m1.1.1" xref="S5.T7.3.3.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S5.T7.3.3.1.m1.1b"><ci id="S5.T7.3.3.1.m1.1.1.cmml" xref="S5.T7.3.3.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.3.3.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S5.T7.3.3.1.m1.1d">italic_K</annotation></semantics></math>=30)</th>
<td class="ltx_td ltx_align_center" id="S5.T7.3.3.2" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span class="ltx_text ltx_font_bold" id="S5.T7.3.3.2.1">35.00</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.3.3.3" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span class="ltx_text ltx_font_bold" id="S5.T7.3.3.3.1">8.50</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.3.3.4" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span class="ltx_text ltx_font_bold" id="S5.T7.3.3.4.1">18.00</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.3.3.5" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span class="ltx_text ltx_font_bold" id="S5.T7.3.3.5.1">20.50</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T7.4.4.1" style="padding-top:0.25pt;padding-bottom:0.25pt;">+ <span class="ltx_text ltx_font_italic" id="S5.T7.4.4.1.1">PEAR</span> (<math alttext="K" class="ltx_Math" display="inline" id="S5.T7.4.4.1.m1.1"><semantics id="S5.T7.4.4.1.m1.1a"><mi id="S5.T7.4.4.1.m1.1.1" xref="S5.T7.4.4.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S5.T7.4.4.1.m1.1b"><ci id="S5.T7.4.4.1.m1.1.1.cmml" xref="S5.T7.4.4.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.4.4.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S5.T7.4.4.1.m1.1d">italic_K</annotation></semantics></math>=40)</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T7.4.4.2" style="padding-top:0.25pt;padding-bottom:0.25pt;">32.50</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T7.4.4.3" style="padding-top:0.25pt;padding-bottom:0.25pt;">8.00</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T7.4.4.4" style="padding-top:0.25pt;padding-bottom:0.25pt;">17.00</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T7.4.4.5" style="padding-top:0.25pt;padding-bottom:0.25pt;">19.17</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S5.T8">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>The experiment results on the MDQA task with ablation settings, which show that our control over the number of suppression heads is effective.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T8.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T8.4.5.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T8.4.5.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T8.4.5.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="5" id="S5.T8.4.5.1.2"><span class="ltx_text ltx_font_bold" id="S5.T8.4.5.1.2.1">Gold Document Position</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T8.4.5.1.3" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T8.4.5.1.3.1">Avg.</span></th>
</tr>
<tr class="ltx_tr" id="S5.T8.4.6.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T8.4.6.2.1"><span class="ltx_text ltx_font_bold" id="S5.T8.4.6.2.1.1">1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T8.4.6.2.2"><span class="ltx_text ltx_font_bold" id="S5.T8.4.6.2.2.1">3</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T8.4.6.2.3"><span class="ltx_text ltx_font_bold" id="S5.T8.4.6.2.3.1">5</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T8.4.6.2.4"><span class="ltx_text ltx_font_bold" id="S5.T8.4.6.2.4.1">7</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T8.4.6.2.5"><span class="ltx_text ltx_font_bold" id="S5.T8.4.6.2.5.1">10</span></th>
</tr>
<tr class="ltx_tr" id="S5.T8.4.7.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S5.T8.4.7.3.1">Llama2-7B-chat-4k</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T8.4.7.3.2"><span class="ltx_text ltx_font_bold" id="S5.T8.4.7.3.2.1">64.14</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T8.4.7.3.3">65.95</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T8.4.7.3.4">64.97</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T8.4.7.3.5">62.67</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T8.4.7.3.6">67.53</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T8.4.7.3.7">65.05</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T8.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T8.1.1.1">+ <span class="ltx_text ltx_font_italic" id="S5.T8.1.1.1.1">PEAR</span> (<math alttext="K" class="ltx_Math" display="inline" id="S5.T8.1.1.1.m1.1"><semantics id="S5.T8.1.1.1.m1.1a"><mi id="S5.T8.1.1.1.m1.1.1" xref="S5.T8.1.1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S5.T8.1.1.1.m1.1b"><ci id="S5.T8.1.1.1.m1.1.1.cmml" xref="S5.T8.1.1.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.1.1.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S5.T8.1.1.1.m1.1d">italic_K</annotation></semantics></math>=10)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.1.1.2">63.43</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.1.1.3">66.26</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.1.1.4">66.82</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.1.1.5">64.67</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.1.1.6">67.76</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T8.1.1.7">65.79</td>
</tr>
<tr class="ltx_tr" id="S5.T8.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T8.2.2.1">+ <span class="ltx_text ltx_font_italic" id="S5.T8.2.2.1.1">PEAR</span> (<math alttext="K" class="ltx_Math" display="inline" id="S5.T8.2.2.1.m1.1"><semantics id="S5.T8.2.2.1.m1.1a"><mi id="S5.T8.2.2.1.m1.1.1" xref="S5.T8.2.2.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S5.T8.2.2.1.m1.1b"><ci id="S5.T8.2.2.1.m1.1.1.cmml" xref="S5.T8.2.2.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.2.2.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S5.T8.2.2.1.m1.1d">italic_K</annotation></semantics></math>=20)</th>
<td class="ltx_td ltx_align_center" id="S5.T8.2.2.2">63.88</td>
<td class="ltx_td ltx_align_center" id="S5.T8.2.2.3">66.29</td>
<td class="ltx_td ltx_align_center" id="S5.T8.2.2.4">66.44</td>
<td class="ltx_td ltx_align_center" id="S5.T8.2.2.5">65.65</td>
<td class="ltx_td ltx_align_center" id="S5.T8.2.2.6">68.51</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T8.2.2.7">66.15</td>
</tr>
<tr class="ltx_tr" id="S5.T8.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T8.3.3.1">+ <span class="ltx_text ltx_font_italic" id="S5.T8.3.3.1.1">PEAR</span> (<math alttext="K" class="ltx_Math" display="inline" id="S5.T8.3.3.1.m1.1"><semantics id="S5.T8.3.3.1.m1.1a"><mi id="S5.T8.3.3.1.m1.1.1" xref="S5.T8.3.3.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S5.T8.3.3.1.m1.1b"><ci id="S5.T8.3.3.1.m1.1.1.cmml" xref="S5.T8.3.3.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.3.3.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S5.T8.3.3.1.m1.1d">italic_K</annotation></semantics></math>=30)</th>
<td class="ltx_td ltx_align_center" id="S5.T8.3.3.2">62.71</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.3.3"><span class="ltx_text ltx_font_bold" id="S5.T8.3.3.3.1">67.01</span></td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.3.4"><span class="ltx_text ltx_font_bold" id="S5.T8.3.3.4.1">68.32</span></td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.3.5">66.44</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.3.6"><span class="ltx_text ltx_font_bold" id="S5.T8.3.3.6.1">69.57</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T8.3.3.7"><span class="ltx_text ltx_font_bold" id="S5.T8.3.3.7.1">66.81</span></td>
</tr>
<tr class="ltx_tr" id="S5.T8.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T8.4.4.1">+ <span class="ltx_text ltx_font_italic" id="S5.T8.4.4.1.1">PEAR</span> (<math alttext="K" class="ltx_Math" display="inline" id="S5.T8.4.4.1.m1.1"><semantics id="S5.T8.4.4.1.m1.1a"><mi id="S5.T8.4.4.1.m1.1.1" xref="S5.T8.4.4.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S5.T8.4.4.1.m1.1b"><ci id="S5.T8.4.4.1.m1.1.1.cmml" xref="S5.T8.4.4.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.4.4.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S5.T8.4.4.1.m1.1d">italic_K</annotation></semantics></math>=40)</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T8.4.4.2">62.90</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T8.4.4.3">66.00</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T8.4.4.4">67.16</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T8.4.4.5"><span class="ltx_text ltx_font_bold" id="S5.T8.4.4.5.1">66.59</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T8.4.4.6">68.40</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S5.T8.4.4.7">66.21</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Analysis: The effect of <math alttext="K" class="ltx_Math" display="inline" id="S5.SS5.1.m1.1"><semantics id="S5.SS5.1.m1.1b"><mi id="S5.SS5.1.m1.1.1" xref="S5.SS5.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S5.SS5.1.m1.1c"><ci id="S5.SS5.1.m1.1.1.cmml" xref="S5.SS5.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.1.m1.1d">K</annotation><annotation encoding="application/x-llamapun" id="S5.SS5.1.m1.1e">italic_K</annotation></semantics></math>
</h3>
<div class="ltx_para ltx_noindent" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1">While we have demonstrated the effectiveness of <span class="ltx_text ltx_font_italic" id="S5.SS5.p1.1.1">PEAR</span> from various angles, a key point for discussion is the role of <math alttext="K" class="ltx_Math" display="inline" id="S5.SS5.p1.1.m1.1"><semantics id="S5.SS5.p1.1.m1.1a"><mi id="S5.SS5.p1.1.m1.1.1" xref="S5.SS5.p1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S5.SS5.p1.1.m1.1b"><ci id="S5.SS5.p1.1.m1.1.1.cmml" xref="S5.SS5.p1.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p1.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S5.SS5.p1.1.m1.1d">italic_K</annotation></semantics></math>, representing the number of heads to re-weight.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS5.p2">
<p class="ltx_p" id="S5.SS5.p2.4">Using Llama2-7B-chat as a case study, we vary <math alttext="K" class="ltx_Math" display="inline" id="S5.SS5.p2.1.m1.1"><semantics id="S5.SS5.p2.1.m1.1a"><mi id="S5.SS5.p2.1.m1.1.1" xref="S5.SS5.p2.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S5.SS5.p2.1.m1.1b"><ci id="S5.SS5.p2.1.m1.1.1.cmml" xref="S5.SS5.p2.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p2.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S5.SS5.p2.1.m1.1d">italic_K</annotation></semantics></math> and observe its impact on <span class="ltx_text ltx_font_italic" id="S5.SS5.p2.4.1">PEAR</span>’s performance.
Table <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S5.T7" title="Table 7 ‣ 5.4 PEAR does not diminish knowledge capabilities in LLMs ‣ 5 Experiments ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_tag">7</span></a> presents results on RAG tasks, while Table <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S5.T8" title="Table 8 ‣ 5.4 PEAR does not diminish knowledge capabilities in LLMs ‣ 5 Experiments ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_tag">8</span></a> details analysis for MDQA tasks.
The findings indicate that <span class="ltx_text ltx_font_italic" id="S5.SS5.p2.4.2">PEAR</span> performs optimally when <math alttext="K" class="ltx_Math" display="inline" id="S5.SS5.p2.2.m2.1"><semantics id="S5.SS5.p2.2.m2.1a"><mi id="S5.SS5.p2.2.m2.1.1" xref="S5.SS5.p2.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S5.SS5.p2.2.m2.1b"><ci id="S5.SS5.p2.2.m2.1.1.cmml" xref="S5.SS5.p2.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p2.2.m2.1c">K</annotation><annotation encoding="application/x-llamapun" id="S5.SS5.p2.2.m2.1d">italic_K</annotation></semantics></math> matches the inherent threshold of the model (i.e., <math alttext="K=30" class="ltx_Math" display="inline" id="S5.SS5.p2.3.m3.1"><semantics id="S5.SS5.p2.3.m3.1a"><mrow id="S5.SS5.p2.3.m3.1.1" xref="S5.SS5.p2.3.m3.1.1.cmml"><mi id="S5.SS5.p2.3.m3.1.1.2" xref="S5.SS5.p2.3.m3.1.1.2.cmml">K</mi><mo id="S5.SS5.p2.3.m3.1.1.1" xref="S5.SS5.p2.3.m3.1.1.1.cmml">=</mo><mn id="S5.SS5.p2.3.m3.1.1.3" xref="S5.SS5.p2.3.m3.1.1.3.cmml">30</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS5.p2.3.m3.1b"><apply id="S5.SS5.p2.3.m3.1.1.cmml" xref="S5.SS5.p2.3.m3.1.1"><eq id="S5.SS5.p2.3.m3.1.1.1.cmml" xref="S5.SS5.p2.3.m3.1.1.1"></eq><ci id="S5.SS5.p2.3.m3.1.1.2.cmml" xref="S5.SS5.p2.3.m3.1.1.2">𝐾</ci><cn id="S5.SS5.p2.3.m3.1.1.3.cmml" type="integer" xref="S5.SS5.p2.3.m3.1.1.3">30</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p2.3.m3.1c">K=30</annotation><annotation encoding="application/x-llamapun" id="S5.SS5.p2.3.m3.1d">italic_K = 30</annotation></semantics></math> for Llama2-7B-chat), i.e., the number of heads with a significantly higher <math alttext="\Delta\pi" class="ltx_Math" display="inline" id="S5.SS5.p2.4.m4.1"><semantics id="S5.SS5.p2.4.m4.1a"><mrow id="S5.SS5.p2.4.m4.1.1" xref="S5.SS5.p2.4.m4.1.1.cmml"><mi id="S5.SS5.p2.4.m4.1.1.2" mathvariant="normal" xref="S5.SS5.p2.4.m4.1.1.2.cmml">Δ</mi><mo id="S5.SS5.p2.4.m4.1.1.1" xref="S5.SS5.p2.4.m4.1.1.1.cmml">⁢</mo><mi id="S5.SS5.p2.4.m4.1.1.3" xref="S5.SS5.p2.4.m4.1.1.3.cmml">π</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS5.p2.4.m4.1b"><apply id="S5.SS5.p2.4.m4.1.1.cmml" xref="S5.SS5.p2.4.m4.1.1"><times id="S5.SS5.p2.4.m4.1.1.1.cmml" xref="S5.SS5.p2.4.m4.1.1.1"></times><ci id="S5.SS5.p2.4.m4.1.1.2.cmml" xref="S5.SS5.p2.4.m4.1.1.2">Δ</ci><ci id="S5.SS5.p2.4.m4.1.1.3.cmml" xref="S5.SS5.p2.4.m4.1.1.3">𝜋</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p2.4.m4.1c">\Delta\pi</annotation><annotation encoding="application/x-llamapun" id="S5.SS5.p2.4.m4.1d">roman_Δ italic_π</annotation></semantics></math> than others.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS5.p3">
<p class="ltx_p" id="S5.SS5.p3.1">Re-weighting fewer heads fails to fully alleviate the suppression from RAG-suppression heads, while exceeding this optimal number can harm the performance of non-RAG-suppression heads, ultimately diminishing overall effectiveness.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.6 </span>Analysis: The Value of <math alttext="\tau" class="ltx_Math" display="inline" id="S5.SS6.1.m1.1"><semantics id="S5.SS6.1.m1.1b"><mi id="S5.SS6.1.m1.1.1" xref="S5.SS6.1.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S5.SS6.1.m1.1c"><ci id="S5.SS6.1.m1.1.1.cmml" xref="S5.SS6.1.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.1.m1.1d">\tau</annotation><annotation encoding="application/x-llamapun" id="S5.SS6.1.m1.1e">italic_τ</annotation></semantics></math>
</h3>
<figure class="ltx_figure ltx_align_floatright" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="718" id="S5.F4.g1" src="extracted/5907299/bar_plot.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The learned coefficients of <span class="ltx_text ltx_font_italic" id="S5.F4.4.1">PEAR</span> (on Llama2-7B-chat and <math alttext="K=40" class="ltx_Math" display="inline" id="S5.F4.2.m1.1"><semantics id="S5.F4.2.m1.1b"><mrow id="S5.F4.2.m1.1.1" xref="S5.F4.2.m1.1.1.cmml"><mi id="S5.F4.2.m1.1.1.2" xref="S5.F4.2.m1.1.1.2.cmml">K</mi><mo id="S5.F4.2.m1.1.1.1" xref="S5.F4.2.m1.1.1.1.cmml">=</mo><mn id="S5.F4.2.m1.1.1.3" xref="S5.F4.2.m1.1.1.3.cmml">40</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.F4.2.m1.1c"><apply id="S5.F4.2.m1.1.1.cmml" xref="S5.F4.2.m1.1.1"><eq id="S5.F4.2.m1.1.1.1.cmml" xref="S5.F4.2.m1.1.1.1"></eq><ci id="S5.F4.2.m1.1.1.2.cmml" xref="S5.F4.2.m1.1.1.2">𝐾</ci><cn id="S5.F4.2.m1.1.1.3.cmml" type="integer" xref="S5.F4.2.m1.1.1.3">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.2.m1.1d">K=40</annotation><annotation encoding="application/x-llamapun" id="S5.F4.2.m1.1e">italic_K = 40</annotation></semantics></math>).</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS6.p1">
<p class="ltx_p" id="S5.SS6.p1.1">Using Llama2-7B-chat as an example, we present the learned coefficients of PEAR in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S5.F4" title="Figure 4 ‣ 5.6 Analysis: The Value of 𝜏 ‣ 5 Experiments ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_tag">4</span></a>, with heads ranked by their <math alttext="\Delta\pi" class="ltx_Math" display="inline" id="S5.SS6.p1.1.m1.1"><semantics id="S5.SS6.p1.1.m1.1a"><mrow id="S5.SS6.p1.1.m1.1.1" xref="S5.SS6.p1.1.m1.1.1.cmml"><mi id="S5.SS6.p1.1.m1.1.1.2" mathvariant="normal" xref="S5.SS6.p1.1.m1.1.1.2.cmml">Δ</mi><mo id="S5.SS6.p1.1.m1.1.1.1" xref="S5.SS6.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS6.p1.1.m1.1.1.3" xref="S5.SS6.p1.1.m1.1.1.3.cmml">π</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS6.p1.1.m1.1b"><apply id="S5.SS6.p1.1.m1.1.1.cmml" xref="S5.SS6.p1.1.m1.1.1"><times id="S5.SS6.p1.1.m1.1.1.1.cmml" xref="S5.SS6.p1.1.m1.1.1.1"></times><ci id="S5.SS6.p1.1.m1.1.1.2.cmml" xref="S5.SS6.p1.1.m1.1.1.2">Δ</ci><ci id="S5.SS6.p1.1.m1.1.1.3.cmml" xref="S5.SS6.p1.1.m1.1.1.3">𝜋</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.p1.1.m1.1c">\Delta\pi</annotation><annotation encoding="application/x-llamapun" id="S5.SS6.p1.1.m1.1d">roman_Δ italic_π</annotation></semantics></math> scores.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS6.p2">
<p class="ltx_p" id="S5.SS6.p2.3">Intuitively, most heads are optimized to values less than one, which reduces their relative weight compared to other heads within the same layer when multi-head outputs are aggregated.
Due to BF16 training precision, many <math alttext="\tau" class="ltx_Math" display="inline" id="S5.SS6.p2.1.m1.1"><semantics id="S5.SS6.p2.1.m1.1a"><mi id="S5.SS6.p2.1.m1.1.1" xref="S5.SS6.p2.1.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S5.SS6.p2.1.m1.1b"><ci id="S5.SS6.p2.1.m1.1.1.cmml" xref="S5.SS6.p2.1.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.p2.1.m1.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S5.SS6.p2.1.m1.1d">italic_τ</annotation></semantics></math>s are optimized to the same value.
However, using FP32 precision for training did not significantly impact the results.
Notably, <math alttext="\tau" class="ltx_Math" display="inline" id="S5.SS6.p2.2.m2.1"><semantics id="S5.SS6.p2.2.m2.1a"><mi id="S5.SS6.p2.2.m2.1.1" xref="S5.SS6.p2.2.m2.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S5.SS6.p2.2.m2.1b"><ci id="S5.SS6.p2.2.m2.1.1.cmml" xref="S5.SS6.p2.2.m2.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.p2.2.m2.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S5.SS6.p2.2.m2.1d">italic_τ</annotation></semantics></math>s for 9 heads, which have relatively low <math alttext="\Delta\pi" class="ltx_Math" display="inline" id="S5.SS6.p2.3.m3.1"><semantics id="S5.SS6.p2.3.m3.1a"><mrow id="S5.SS6.p2.3.m3.1.1" xref="S5.SS6.p2.3.m3.1.1.cmml"><mi id="S5.SS6.p2.3.m3.1.1.2" mathvariant="normal" xref="S5.SS6.p2.3.m3.1.1.2.cmml">Δ</mi><mo id="S5.SS6.p2.3.m3.1.1.1" xref="S5.SS6.p2.3.m3.1.1.1.cmml">⁢</mo><mi id="S5.SS6.p2.3.m3.1.1.3" xref="S5.SS6.p2.3.m3.1.1.3.cmml">π</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS6.p2.3.m3.1b"><apply id="S5.SS6.p2.3.m3.1.1.cmml" xref="S5.SS6.p2.3.m3.1.1"><times id="S5.SS6.p2.3.m3.1.1.1.cmml" xref="S5.SS6.p2.3.m3.1.1.1"></times><ci id="S5.SS6.p2.3.m3.1.1.2.cmml" xref="S5.SS6.p2.3.m3.1.1.2">Δ</ci><ci id="S5.SS6.p2.3.m3.1.1.3.cmml" xref="S5.SS6.p2.3.m3.1.1.3">𝜋</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.p2.3.m3.1c">\Delta\pi</annotation><annotation encoding="application/x-llamapun" id="S5.SS6.p2.3.m3.1d">roman_Δ italic_π</annotation></semantics></math>, are greater than one.
We do not attribute this to the precision of the discovery process, as constraining the re-weighting coefficients to be less than one led to suboptimal performance. Thus, a plausible explanation is that RAG suppression is a complex, cooperative effect involving multiple heads, each with distinct working mechanisms, as discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.19745v2#S4.SS1" title="4.1 Discovery of RAG-suppression heads ‣ 4 Methodology ‣ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead"><span class="ltx_text ltx_ref_tag">4.1</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we introduce <span class="ltx_text ltx_font_italic" id="S6.p1.1.1">PEAR</span>, a position-embedding-agnostic method designed to enhance the performance of LLMs on RAG tasks with zero inference overhead.
Our method not only outperforms competitive baselines in both effectiveness and efficiency but also demonstrates broad applicability across various LLMs.
We also presented that <span class="ltx_text ltx_font_italic" id="S6.p1.1.2">PEAR</span> improves context awareness in LLMs without compromising their inherent knowledge capabilities.
These benefits make <span class="ltx_text ltx_font_italic" id="S6.p1.1.3">PEAR</span> a promising approach for a wide range of applications that require robust context abilities, such as in-context learning and strict instruction following, which we leave for future research.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baichuan (2023)</span>
<span class="ltx_bibblock">
Baichuan.

</span>
<span class="ltx_bibblock">A 13b large language model developed by baichuan intelligent technology, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/baichuan-inc/Baichuan-13B/blob/main/README_EN.md" title="">https://github.com/baichuan-inc/Baichuan-13B/blob/main/README_EN.md</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024)</span>
<span class="ltx_bibblock">
Yuhan Chen, Ang Lv, Ting-En Lin, Changyu Chen, Yuchuan Wu, Fei Huang, Yongbin Li, and Rui Yan.

</span>
<span class="ltx_bibblock">Fortify the shortest stave in attention: Enhancing context awareness of large language models for effective tool use.

</span>
<span class="ltx_bibblock">In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pp.  11160–11174, Bangkok, Thailand, August 2024. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2024.acl-long.601" title="">https://aclanthology.org/2024.acl-long.601</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dao (2024)</span>
<span class="ltx_bibblock">
Tri Dao.

</span>
<span class="ltx_bibblock">FlashAttention-2: Faster attention with better parallelism and work partitioning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">International Conference on Learning Representations (ICLR)</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dasigi et al. (2021)</span>
<span class="ltx_bibblock">
Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner.

</span>
<span class="ltx_bibblock">A dataset of information-seeking questions and answers anchored in research papers, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2105.03011" title="">https://arxiv.org/abs/2105.03011</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geva et al. (2023)</span>
<span class="ltx_bibblock">
Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson.

</span>
<span class="ltx_bibblock">Dissecting recall of factual associations in auto-regressive language models.

</span>
<span class="ltx_bibblock">In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pp.  12216–12235, Singapore, December 2023. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2023.emnlp-main.751</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.emnlp-main.751" title="">https://aclanthology.org/2023.emnlp-main.751</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong et al. (2024)</span>
<span class="ltx_bibblock">
Zhuocheng Gong, Ang Lv, Jian Guan, Junxi Yan, Wei Wu, Huishuai Zhang, Minlie Huang, Dongyan Zhao, and Rui Yan.

</span>
<span class="ltx_bibblock">Mixture-of-modules: Reinventing transformers as dynamic assemblies of modules, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2407.06677" title="">https://arxiv.org/abs/2407.06677</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et al. (2021)</span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Measuring massive multitask language understanding, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2009.03300" title="">https://arxiv.org/abs/2009.03300</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho et al. (2020)</span>
<span class="ltx_bibblock">
Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa.

</span>
<span class="ltx_bibblock">Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the 28th International Conference on Computational Linguistics</em>, pp.  6609–6625, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.aclweb.org/anthology/2020.coling-main.580" title="">https://www.aclweb.org/anthology/2020.coling-main.580</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski et al. (2019)</span>
<span class="ltx_bibblock">
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al.

</span>
<span class="ltx_bibblock">Natural questions: a benchmark for question answering research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Transactions of the Association for Computational Linguistics</em>, 7:453–466, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2019)</span>
<span class="ltx_bibblock">
Kenton Lee, Ming-Wei Chang, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">Latent retrieval for weakly supervised open domain question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:1906.00300</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2021)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2005.11401" title="">https://arxiv.org/abs/2005.11401</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2024)</span>
<span class="ltx_bibblock">
Hongzhan Lin, Ang Lv, Yuhan Chen, Chen Zhu, Yang Song, Hengshu Zhu, and Rui Yan.

</span>
<span class="ltx_bibblock">Mixture of in-context experts enhance llms’ long context awareness, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2406.19598" title="">https://arxiv.org/abs/2406.19598</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.

</span>
<span class="ltx_bibblock">Lost in the middle: How language models use long contexts, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2022)</span>
<span class="ltx_bibblock">
Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp.

</span>
<span class="ltx_bibblock">Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity.

</span>
<span class="ltx_bibblock">In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pp.  8086–8098, Dublin, Ireland, May 2022. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2022.acl-long.556</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.acl-long.556" title="">https://aclanthology.org/2022.acl-long.556</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lv et al. (2024a)</span>
<span class="ltx_bibblock">
Ang Lv, Yuhan Chen, Kaiyi Zhang, Yulong Wang, Lifeng Liu, Ji-Rong Wen, Jian Xie, and Rui Yan.

</span>
<span class="ltx_bibblock">Interpreting key mechanisms of factual recall in transformer-based language models, 2024a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2403.19521" title="">https://arxiv.org/abs/2403.19521</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lv et al. (2024b)</span>
<span class="ltx_bibblock">
Ang Lv, Ruobing Xie, Xingwu Sun, Zhanhui Kang, and Rui Yan.

</span>
<span class="ltx_bibblock">Language models ”grok” to copy, 2024b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2409.09281" title="">https://arxiv.org/abs/2409.09281</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McDougall et al. (2023)</span>
<span class="ltx_bibblock">
Callum McDougall, Arthur Conmy, Cody Rushing, Thomas McGrath, and Neel Nanda.

</span>
<span class="ltx_bibblock">Copy suppression: Comprehensively understanding an attention head, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2310.04625" title="">https://arxiv.org/abs/2310.04625</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Merullo et al. (2024)</span>
<span class="ltx_bibblock">
Jack Merullo, Carsten Eickhoff, and Ellie Pavlick.

</span>
<span class="ltx_bibblock">Circuit component reuse across tasks in transformer language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">The Twelfth International Conference on Learning Representations</em>, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=fpoAYV6Wsk" title="">https://openreview.net/forum?id=fpoAYV6Wsk</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Microsoft (2023)</span>
<span class="ltx_bibblock">
Microsoft.

</span>
<span class="ltx_bibblock">Reinventing search with a new ai-powered microsoft bing and edge, your copilot for the web, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Olah et al. (2020)</span>
<span class="ltx_bibblock">
Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter.

</span>
<span class="ltx_bibblock">Zoom in: An introduction to circuits.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Distill</em>, 2020.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.23915/distill.00024.001</span>.

</span>
<span class="ltx_bibblock">https://distill.pub/2020/circuits/zoom-in.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Olsson et al. (2022)</span>
<span class="ltx_bibblock">
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah.

</span>
<span class="ltx_bibblock">In-context learning and induction heads.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Transformer Circuits Thread</em>, 2022.

</span>
<span class="ltx_bibblock">https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2024)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Gpt-4 technical report, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2303.08774" title="">https://arxiv.org/abs/2303.08774</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peysakhovich &amp; Lerer (2023)</span>
<span class="ltx_bibblock">
Alexander Peysakhovich and Adam Lerer.

</span>
<span class="ltx_bibblock">Attention sorting combats recency bias in long context language models, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Press et al. (2022)</span>
<span class="ltx_bibblock">
Ofir Press, Noah A. Smith, and Mike Lewis.

</span>
<span class="ltx_bibblock">Train short, test long: Attention with linear biases enables input length extrapolation, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2108.12409" title="">https://arxiv.org/abs/2108.12409</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2019)</span>
<span class="ltx_bibblock">
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shazeer et al. (2017)</span>
<span class="ltx_bibblock">
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton, and Jeff Dean.

</span>
<span class="ltx_bibblock">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">CoRR</em>, abs/1701.06538, 2017.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1701.06538" title="">http://arxiv.org/abs/1701.06538</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al. (2023)</span>
<span class="ltx_bibblock">
Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu.

</span>
<span class="ltx_bibblock">Roformer: Enhanced transformer with rotary position embedding, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2104.09864" title="">https://arxiv.org/abs/2104.09864</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2307.09288" title="">https://arxiv.org/abs/2307.09288</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trivedi et al. (2022)</span>
<span class="ltx_bibblock">
Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal.

</span>
<span class="ltx_bibblock">MuSiQue: Multihop questions via single-hop question composition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Transactions of the Association for Computational Linguistics</em>, 10:539–554, 2022.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1162/tacl˙a˙00475</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.tacl-1.31" title="">https://aclanthology.org/2022.tacl-1.31</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024)</span>
<span class="ltx_bibblock">
Boshi Wang, Xiang Yue, Yu Su, and Huan Sun.

</span>
<span class="ltx_bibblock">Grokked transformers are implicit reasoners: A mechanistic journey to the edge of generalization, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2405.15071" title="">https://arxiv.org/abs/2405.15071</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023)</span>
<span class="ltx_bibblock">
Kevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Interpretability in the wild: a circuit for indirect object identification in GPT-2 small.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">The Eleventh International Conference on Learning Representations</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=NpsVSN6o4ul" title="">https://openreview.net/forum?id=NpsVSN6o4ul</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2023)</span>
<span class="ltx_bibblock">
Qinan Yu, Jack Merullo, and Ellie Pavlick.

</span>
<span class="ltx_bibblock">Characterizing mechanisms for factual recall in language models.

</span>
<span class="ltx_bibblock">In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pp.  9924–9959, Singapore, December 2023. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2023.emnlp-main.615</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.emnlp-main.615" title="">https://aclanthology.org/2023.emnlp-main.615</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang &amp; Nanda (2024)</span>
<span class="ltx_bibblock">
Fred Zhang and Neel Nanda.

</span>
<span class="ltx_bibblock">Towards best practices of activation patching in language models: Metrics and methods.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">The Twelfth International Conference on Learning Representations</em>, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=Hf17y6u9BC" title="">https://openreview.net/forum?id=Hf17y6u9BC</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2022)</span>
<span class="ltx_bibblock">
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Opt: Open pre-trained transformer language models, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2205.01068" title="">https://arxiv.org/abs/2205.01068</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024)</span>
<span class="ltx_bibblock">
Zhenyu Zhang, Runjin Chen, Shiwei Liu, Zhewei Yao, Olatunji Ruwase, Beidi Chen, Xiaoxia Wu, and Zhangyang Wang.

</span>
<span class="ltx_bibblock">Found in the middle: How language models use long contexts better via plug-and-play positional encoding, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2403.04797" title="">https://arxiv.org/abs/2403.04797</a>.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Head discovery results</h2>
<figure class="ltx_figure" id="A1.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="242" id="A1.F5.g1" src="x4.png" width="813"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Heatmaps of <math alttext="\Delta\pi" class="ltx_Math" display="inline" id="A1.F5.3.m1.1"><semantics id="A1.F5.3.m1.1b"><mrow id="A1.F5.3.m1.1.1" xref="A1.F5.3.m1.1.1.cmml"><mi id="A1.F5.3.m1.1.1.2" mathvariant="normal" xref="A1.F5.3.m1.1.1.2.cmml">Δ</mi><mo id="A1.F5.3.m1.1.1.1" xref="A1.F5.3.m1.1.1.1.cmml">⁢</mo><mi id="A1.F5.3.m1.1.1.3" xref="A1.F5.3.m1.1.1.3.cmml">π</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.F5.3.m1.1c"><apply id="A1.F5.3.m1.1.1.cmml" xref="A1.F5.3.m1.1.1"><times id="A1.F5.3.m1.1.1.1.cmml" xref="A1.F5.3.m1.1.1.1"></times><ci id="A1.F5.3.m1.1.1.2.cmml" xref="A1.F5.3.m1.1.1.2">Δ</ci><ci id="A1.F5.3.m1.1.1.3.cmml" xref="A1.F5.3.m1.1.1.3">𝜋</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.F5.3.m1.1d">\Delta\pi</annotation><annotation encoding="application/x-llamapun" id="A1.F5.3.m1.1e">roman_Δ italic_π</annotation></semantics></math> scores for each head of llama2-7B-chat (<math alttext="n=15,n=25,n=50" class="ltx_Math" display="inline" id="A1.F5.4.m2.2"><semantics id="A1.F5.4.m2.2b"><mrow id="A1.F5.4.m2.2.2.2" xref="A1.F5.4.m2.2.2.3.cmml"><mrow id="A1.F5.4.m2.1.1.1.1" xref="A1.F5.4.m2.1.1.1.1.cmml"><mi id="A1.F5.4.m2.1.1.1.1.2" xref="A1.F5.4.m2.1.1.1.1.2.cmml">n</mi><mo id="A1.F5.4.m2.1.1.1.1.1" xref="A1.F5.4.m2.1.1.1.1.1.cmml">=</mo><mn id="A1.F5.4.m2.1.1.1.1.3" xref="A1.F5.4.m2.1.1.1.1.3.cmml">15</mn></mrow><mo id="A1.F5.4.m2.2.2.2.3" xref="A1.F5.4.m2.2.2.3a.cmml">,</mo><mrow id="A1.F5.4.m2.2.2.2.2.2" xref="A1.F5.4.m2.2.2.2.2.3.cmml"><mrow id="A1.F5.4.m2.2.2.2.2.1.1" xref="A1.F5.4.m2.2.2.2.2.1.1.cmml"><mi id="A1.F5.4.m2.2.2.2.2.1.1.2" xref="A1.F5.4.m2.2.2.2.2.1.1.2.cmml">n</mi><mo id="A1.F5.4.m2.2.2.2.2.1.1.1" xref="A1.F5.4.m2.2.2.2.2.1.1.1.cmml">=</mo><mn id="A1.F5.4.m2.2.2.2.2.1.1.3" xref="A1.F5.4.m2.2.2.2.2.1.1.3.cmml">25</mn></mrow><mo id="A1.F5.4.m2.2.2.2.2.2.3" xref="A1.F5.4.m2.2.2.2.2.3a.cmml">,</mo><mrow id="A1.F5.4.m2.2.2.2.2.2.2" xref="A1.F5.4.m2.2.2.2.2.2.2.cmml"><mi id="A1.F5.4.m2.2.2.2.2.2.2.2" xref="A1.F5.4.m2.2.2.2.2.2.2.2.cmml">n</mi><mo id="A1.F5.4.m2.2.2.2.2.2.2.1" xref="A1.F5.4.m2.2.2.2.2.2.2.1.cmml">=</mo><mn id="A1.F5.4.m2.2.2.2.2.2.2.3" xref="A1.F5.4.m2.2.2.2.2.2.2.3.cmml">50</mn></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A1.F5.4.m2.2c"><apply id="A1.F5.4.m2.2.2.3.cmml" xref="A1.F5.4.m2.2.2.2"><csymbol cd="ambiguous" id="A1.F5.4.m2.2.2.3a.cmml" xref="A1.F5.4.m2.2.2.2.3">formulae-sequence</csymbol><apply id="A1.F5.4.m2.1.1.1.1.cmml" xref="A1.F5.4.m2.1.1.1.1"><eq id="A1.F5.4.m2.1.1.1.1.1.cmml" xref="A1.F5.4.m2.1.1.1.1.1"></eq><ci id="A1.F5.4.m2.1.1.1.1.2.cmml" xref="A1.F5.4.m2.1.1.1.1.2">𝑛</ci><cn id="A1.F5.4.m2.1.1.1.1.3.cmml" type="integer" xref="A1.F5.4.m2.1.1.1.1.3">15</cn></apply><apply id="A1.F5.4.m2.2.2.2.2.3.cmml" xref="A1.F5.4.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="A1.F5.4.m2.2.2.2.2.3a.cmml" xref="A1.F5.4.m2.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="A1.F5.4.m2.2.2.2.2.1.1.cmml" xref="A1.F5.4.m2.2.2.2.2.1.1"><eq id="A1.F5.4.m2.2.2.2.2.1.1.1.cmml" xref="A1.F5.4.m2.2.2.2.2.1.1.1"></eq><ci id="A1.F5.4.m2.2.2.2.2.1.1.2.cmml" xref="A1.F5.4.m2.2.2.2.2.1.1.2">𝑛</ci><cn id="A1.F5.4.m2.2.2.2.2.1.1.3.cmml" type="integer" xref="A1.F5.4.m2.2.2.2.2.1.1.3">25</cn></apply><apply id="A1.F5.4.m2.2.2.2.2.2.2.cmml" xref="A1.F5.4.m2.2.2.2.2.2.2"><eq id="A1.F5.4.m2.2.2.2.2.2.2.1.cmml" xref="A1.F5.4.m2.2.2.2.2.2.2.1"></eq><ci id="A1.F5.4.m2.2.2.2.2.2.2.2.cmml" xref="A1.F5.4.m2.2.2.2.2.2.2.2">𝑛</ci><cn id="A1.F5.4.m2.2.2.2.2.2.2.3.cmml" type="integer" xref="A1.F5.4.m2.2.2.2.2.2.2.3">50</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.F5.4.m2.2d">n=15,n=25,n=50</annotation><annotation encoding="application/x-llamapun" id="A1.F5.4.m2.2e">italic_n = 15 , italic_n = 25 , italic_n = 50</annotation></semantics></math>).</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="240" id="A1.F6.g1" src="x5.png" width="813"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Heatmaps of <math alttext="\Delta\pi" class="ltx_Math" display="inline" id="A1.F6.3.m1.1"><semantics id="A1.F6.3.m1.1b"><mrow id="A1.F6.3.m1.1.1" xref="A1.F6.3.m1.1.1.cmml"><mi id="A1.F6.3.m1.1.1.2" mathvariant="normal" xref="A1.F6.3.m1.1.1.2.cmml">Δ</mi><mo id="A1.F6.3.m1.1.1.1" xref="A1.F6.3.m1.1.1.1.cmml">⁢</mo><mi id="A1.F6.3.m1.1.1.3" xref="A1.F6.3.m1.1.1.3.cmml">π</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.F6.3.m1.1c"><apply id="A1.F6.3.m1.1.1.cmml" xref="A1.F6.3.m1.1.1"><times id="A1.F6.3.m1.1.1.1.cmml" xref="A1.F6.3.m1.1.1.1"></times><ci id="A1.F6.3.m1.1.1.2.cmml" xref="A1.F6.3.m1.1.1.2">Δ</ci><ci id="A1.F6.3.m1.1.1.3.cmml" xref="A1.F6.3.m1.1.1.3">𝜋</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.F6.3.m1.1d">\Delta\pi</annotation><annotation encoding="application/x-llamapun" id="A1.F6.3.m1.1e">roman_Δ italic_π</annotation></semantics></math> scores for each head of OPT-6.7B (<math alttext="n=15,n=25,n=50" class="ltx_Math" display="inline" id="A1.F6.4.m2.2"><semantics id="A1.F6.4.m2.2b"><mrow id="A1.F6.4.m2.2.2.2" xref="A1.F6.4.m2.2.2.3.cmml"><mrow id="A1.F6.4.m2.1.1.1.1" xref="A1.F6.4.m2.1.1.1.1.cmml"><mi id="A1.F6.4.m2.1.1.1.1.2" xref="A1.F6.4.m2.1.1.1.1.2.cmml">n</mi><mo id="A1.F6.4.m2.1.1.1.1.1" xref="A1.F6.4.m2.1.1.1.1.1.cmml">=</mo><mn id="A1.F6.4.m2.1.1.1.1.3" xref="A1.F6.4.m2.1.1.1.1.3.cmml">15</mn></mrow><mo id="A1.F6.4.m2.2.2.2.3" xref="A1.F6.4.m2.2.2.3a.cmml">,</mo><mrow id="A1.F6.4.m2.2.2.2.2.2" xref="A1.F6.4.m2.2.2.2.2.3.cmml"><mrow id="A1.F6.4.m2.2.2.2.2.1.1" xref="A1.F6.4.m2.2.2.2.2.1.1.cmml"><mi id="A1.F6.4.m2.2.2.2.2.1.1.2" xref="A1.F6.4.m2.2.2.2.2.1.1.2.cmml">n</mi><mo id="A1.F6.4.m2.2.2.2.2.1.1.1" xref="A1.F6.4.m2.2.2.2.2.1.1.1.cmml">=</mo><mn id="A1.F6.4.m2.2.2.2.2.1.1.3" xref="A1.F6.4.m2.2.2.2.2.1.1.3.cmml">25</mn></mrow><mo id="A1.F6.4.m2.2.2.2.2.2.3" xref="A1.F6.4.m2.2.2.2.2.3a.cmml">,</mo><mrow id="A1.F6.4.m2.2.2.2.2.2.2" xref="A1.F6.4.m2.2.2.2.2.2.2.cmml"><mi id="A1.F6.4.m2.2.2.2.2.2.2.2" xref="A1.F6.4.m2.2.2.2.2.2.2.2.cmml">n</mi><mo id="A1.F6.4.m2.2.2.2.2.2.2.1" xref="A1.F6.4.m2.2.2.2.2.2.2.1.cmml">=</mo><mn id="A1.F6.4.m2.2.2.2.2.2.2.3" xref="A1.F6.4.m2.2.2.2.2.2.2.3.cmml">50</mn></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A1.F6.4.m2.2c"><apply id="A1.F6.4.m2.2.2.3.cmml" xref="A1.F6.4.m2.2.2.2"><csymbol cd="ambiguous" id="A1.F6.4.m2.2.2.3a.cmml" xref="A1.F6.4.m2.2.2.2.3">formulae-sequence</csymbol><apply id="A1.F6.4.m2.1.1.1.1.cmml" xref="A1.F6.4.m2.1.1.1.1"><eq id="A1.F6.4.m2.1.1.1.1.1.cmml" xref="A1.F6.4.m2.1.1.1.1.1"></eq><ci id="A1.F6.4.m2.1.1.1.1.2.cmml" xref="A1.F6.4.m2.1.1.1.1.2">𝑛</ci><cn id="A1.F6.4.m2.1.1.1.1.3.cmml" type="integer" xref="A1.F6.4.m2.1.1.1.1.3">15</cn></apply><apply id="A1.F6.4.m2.2.2.2.2.3.cmml" xref="A1.F6.4.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="A1.F6.4.m2.2.2.2.2.3a.cmml" xref="A1.F6.4.m2.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="A1.F6.4.m2.2.2.2.2.1.1.cmml" xref="A1.F6.4.m2.2.2.2.2.1.1"><eq id="A1.F6.4.m2.2.2.2.2.1.1.1.cmml" xref="A1.F6.4.m2.2.2.2.2.1.1.1"></eq><ci id="A1.F6.4.m2.2.2.2.2.1.1.2.cmml" xref="A1.F6.4.m2.2.2.2.2.1.1.2">𝑛</ci><cn id="A1.F6.4.m2.2.2.2.2.1.1.3.cmml" type="integer" xref="A1.F6.4.m2.2.2.2.2.1.1.3">25</cn></apply><apply id="A1.F6.4.m2.2.2.2.2.2.2.cmml" xref="A1.F6.4.m2.2.2.2.2.2.2"><eq id="A1.F6.4.m2.2.2.2.2.2.2.1.cmml" xref="A1.F6.4.m2.2.2.2.2.2.2.1"></eq><ci id="A1.F6.4.m2.2.2.2.2.2.2.2.cmml" xref="A1.F6.4.m2.2.2.2.2.2.2.2">𝑛</ci><cn id="A1.F6.4.m2.2.2.2.2.2.2.3.cmml" type="integer" xref="A1.F6.4.m2.2.2.2.2.2.2.3">50</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.F6.4.m2.2d">n=15,n=25,n=50</annotation><annotation encoding="application/x-llamapun" id="A1.F6.4.m2.2e">italic_n = 15 , italic_n = 25 , italic_n = 50</annotation></semantics></math>).</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="234" id="A1.F7.g1" src="x6.png" width="813"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Heatmaps of <math alttext="\Delta\pi" class="ltx_Math" display="inline" id="A1.F7.3.m1.1"><semantics id="A1.F7.3.m1.1b"><mrow id="A1.F7.3.m1.1.1" xref="A1.F7.3.m1.1.1.cmml"><mi id="A1.F7.3.m1.1.1.2" mathvariant="normal" xref="A1.F7.3.m1.1.1.2.cmml">Δ</mi><mo id="A1.F7.3.m1.1.1.1" xref="A1.F7.3.m1.1.1.1.cmml">⁢</mo><mi id="A1.F7.3.m1.1.1.3" xref="A1.F7.3.m1.1.1.3.cmml">π</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.F7.3.m1.1c"><apply id="A1.F7.3.m1.1.1.cmml" xref="A1.F7.3.m1.1.1"><times id="A1.F7.3.m1.1.1.1.cmml" xref="A1.F7.3.m1.1.1.1"></times><ci id="A1.F7.3.m1.1.1.2.cmml" xref="A1.F7.3.m1.1.1.2">Δ</ci><ci id="A1.F7.3.m1.1.1.3.cmml" xref="A1.F7.3.m1.1.1.3">𝜋</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.F7.3.m1.1d">\Delta\pi</annotation><annotation encoding="application/x-llamapun" id="A1.F7.3.m1.1e">roman_Δ italic_π</annotation></semantics></math> scores for each head of Baichuan-13B-chat (<math alttext="n=20,n=50,n=80" class="ltx_Math" display="inline" id="A1.F7.4.m2.2"><semantics id="A1.F7.4.m2.2b"><mrow id="A1.F7.4.m2.2.2.2" xref="A1.F7.4.m2.2.2.3.cmml"><mrow id="A1.F7.4.m2.1.1.1.1" xref="A1.F7.4.m2.1.1.1.1.cmml"><mi id="A1.F7.4.m2.1.1.1.1.2" xref="A1.F7.4.m2.1.1.1.1.2.cmml">n</mi><mo id="A1.F7.4.m2.1.1.1.1.1" xref="A1.F7.4.m2.1.1.1.1.1.cmml">=</mo><mn id="A1.F7.4.m2.1.1.1.1.3" xref="A1.F7.4.m2.1.1.1.1.3.cmml">20</mn></mrow><mo id="A1.F7.4.m2.2.2.2.3" xref="A1.F7.4.m2.2.2.3a.cmml">,</mo><mrow id="A1.F7.4.m2.2.2.2.2.2" xref="A1.F7.4.m2.2.2.2.2.3.cmml"><mrow id="A1.F7.4.m2.2.2.2.2.1.1" xref="A1.F7.4.m2.2.2.2.2.1.1.cmml"><mi id="A1.F7.4.m2.2.2.2.2.1.1.2" xref="A1.F7.4.m2.2.2.2.2.1.1.2.cmml">n</mi><mo id="A1.F7.4.m2.2.2.2.2.1.1.1" xref="A1.F7.4.m2.2.2.2.2.1.1.1.cmml">=</mo><mn id="A1.F7.4.m2.2.2.2.2.1.1.3" xref="A1.F7.4.m2.2.2.2.2.1.1.3.cmml">50</mn></mrow><mo id="A1.F7.4.m2.2.2.2.2.2.3" xref="A1.F7.4.m2.2.2.2.2.3a.cmml">,</mo><mrow id="A1.F7.4.m2.2.2.2.2.2.2" xref="A1.F7.4.m2.2.2.2.2.2.2.cmml"><mi id="A1.F7.4.m2.2.2.2.2.2.2.2" xref="A1.F7.4.m2.2.2.2.2.2.2.2.cmml">n</mi><mo id="A1.F7.4.m2.2.2.2.2.2.2.1" xref="A1.F7.4.m2.2.2.2.2.2.2.1.cmml">=</mo><mn id="A1.F7.4.m2.2.2.2.2.2.2.3" xref="A1.F7.4.m2.2.2.2.2.2.2.3.cmml">80</mn></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A1.F7.4.m2.2c"><apply id="A1.F7.4.m2.2.2.3.cmml" xref="A1.F7.4.m2.2.2.2"><csymbol cd="ambiguous" id="A1.F7.4.m2.2.2.3a.cmml" xref="A1.F7.4.m2.2.2.2.3">formulae-sequence</csymbol><apply id="A1.F7.4.m2.1.1.1.1.cmml" xref="A1.F7.4.m2.1.1.1.1"><eq id="A1.F7.4.m2.1.1.1.1.1.cmml" xref="A1.F7.4.m2.1.1.1.1.1"></eq><ci id="A1.F7.4.m2.1.1.1.1.2.cmml" xref="A1.F7.4.m2.1.1.1.1.2">𝑛</ci><cn id="A1.F7.4.m2.1.1.1.1.3.cmml" type="integer" xref="A1.F7.4.m2.1.1.1.1.3">20</cn></apply><apply id="A1.F7.4.m2.2.2.2.2.3.cmml" xref="A1.F7.4.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="A1.F7.4.m2.2.2.2.2.3a.cmml" xref="A1.F7.4.m2.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="A1.F7.4.m2.2.2.2.2.1.1.cmml" xref="A1.F7.4.m2.2.2.2.2.1.1"><eq id="A1.F7.4.m2.2.2.2.2.1.1.1.cmml" xref="A1.F7.4.m2.2.2.2.2.1.1.1"></eq><ci id="A1.F7.4.m2.2.2.2.2.1.1.2.cmml" xref="A1.F7.4.m2.2.2.2.2.1.1.2">𝑛</ci><cn id="A1.F7.4.m2.2.2.2.2.1.1.3.cmml" type="integer" xref="A1.F7.4.m2.2.2.2.2.1.1.3">50</cn></apply><apply id="A1.F7.4.m2.2.2.2.2.2.2.cmml" xref="A1.F7.4.m2.2.2.2.2.2.2"><eq id="A1.F7.4.m2.2.2.2.2.2.2.1.cmml" xref="A1.F7.4.m2.2.2.2.2.2.2.1"></eq><ci id="A1.F7.4.m2.2.2.2.2.2.2.2.cmml" xref="A1.F7.4.m2.2.2.2.2.2.2.2">𝑛</ci><cn id="A1.F7.4.m2.2.2.2.2.2.2.3.cmml" type="integer" xref="A1.F7.4.m2.2.2.2.2.2.2.3">80</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.F7.4.m2.2d">n=20,n=50,n=80</annotation><annotation encoding="application/x-llamapun" id="A1.F7.4.m2.2e">italic_n = 20 , italic_n = 50 , italic_n = 80</annotation></semantics></math>).</figcaption>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Oct  7 14:16:28 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
