<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Retrieval-Augmented Generation for AI-Generated Content: A Survey</title>
<!--Generated on Fri Jun 21 08:17:51 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Retrieval-augmented generation,  AI-generated content,  generative models,  information retrieval.
" lang="en" name="keywords"/>
<base href="/html/2402.19473v6/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S1" title="In Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_bold">Introduction</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S1.SS1" title="In I Introduction ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">I-A</span> </span><span class="ltx_text ltx_font_bold">Background</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S1.SS2" title="In I Introduction ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">I-B</span> </span><span class="ltx_text ltx_font_bold">Contribution</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S1.SS3" title="In I Introduction ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">I-C</span> </span><span class="ltx_text ltx_font_bold">Related Work</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S1.SS4" title="In I Introduction ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">I-D</span> </span><span class="ltx_text ltx_font_bold">Roadmap</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S2" title="In Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_bold">Preliminary</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S2.SS1" title="In II Preliminary ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_bold">Overview</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S2.SS2" title="In II Preliminary ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_bold">Generator</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S2.SS2.SSS1" title="In II-B Generator ‣ II Preliminary ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span>1 </span><span class="ltx_text ltx_font_bold">Transformer Model</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S2.SS2.SSS2" title="In II-B Generator ‣ II Preliminary ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span>2 </span><span class="ltx_text ltx_font_bold">LSTM</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S2.SS2.SSS3" title="In II-B Generator ‣ II Preliminary ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span>3 </span><span class="ltx_text ltx_font_bold">Diffusion Model</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S2.SS2.SSS4" title="In II-B Generator ‣ II Preliminary ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span>4 </span><span class="ltx_text ltx_font_bold">GAN</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S2.SS3" title="In II Preliminary ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_bold">Retriever</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S2.SS3.SSS1" title="In II-C Retriever ‣ II Preliminary ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span>1 </span><span class="ltx_text ltx_font_bold">Sparse Retriever</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S2.SS3.SSS2" title="In II-C Retriever ‣ II Preliminary ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span>2 </span><span class="ltx_text ltx_font_bold">Dense Retriever</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S2.SS3.SSS3" title="In II-C Retriever ‣ II Preliminary ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span>3 </span><span class="ltx_text ltx_font_bold">Others</span></span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S3" title="In Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_bold">Methodologies</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S3.SS1" title="In III Methodologies ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_bold">RAG Foundations</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S3.SS1.SSS1" title="In III-A RAG Foundations ‣ III Methodologies ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span>1 </span><span class="ltx_text ltx_font_bold">Query-based RAG</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S3.SS1.SSS2" title="In III-A RAG Foundations ‣ III Methodologies ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span>2 </span><span class="ltx_text ltx_font_bold">Latent Representation-based RAG</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S3.SS1.SSS3" title="In III-A RAG Foundations ‣ III Methodologies ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span>3 </span><span class="ltx_text ltx_font_bold">Logit-based RAG</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S3.SS1.SSS4" title="In III-A RAG Foundations ‣ III Methodologies ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span>4 </span><span class="ltx_text ltx_font_bold">Speculative RAG</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S3.SS2" title="In III Methodologies ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_bold">RAG Enhancements</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S3.SS2.SSS1" title="In III-B RAG Enhancements ‣ III Methodologies ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span>1 </span><span class="ltx_text ltx_font_bold">Input Enhancement</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S3.SS2.SSS2" title="In III-B RAG Enhancements ‣ III Methodologies ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span>2 </span><span class="ltx_text ltx_font_bold">Retriever Enhancement</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S3.SS2.SSS3" title="In III-B RAG Enhancements ‣ III Methodologies ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span>3 </span><span class="ltx_text ltx_font_bold">Generator Enhancement</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S3.SS2.SSS4" title="In III-B RAG Enhancements ‣ III Methodologies ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span>4 </span><span class="ltx_text ltx_font_bold">Result Enhancement</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S3.SS2.SSS5" title="In III-B RAG Enhancements ‣ III Methodologies ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span>5 </span><span class="ltx_text ltx_font_bold">RAG Pipeline Enhancement</span></span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4" title="In Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_bold">Applications</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.SS1" title="In IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_bold">RAG for Text</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.SS1.SSS1" title="In IV-A RAG for Text ‣ IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>1 </span><span class="ltx_text ltx_font_bold">Question Answering</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.SS1.SSS2" title="In IV-A RAG for Text ‣ IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>2 </span><span class="ltx_text ltx_font_bold">Fact Verification</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.SS1.SSS3" title="In IV-A RAG for Text ‣ IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>3 </span><span class="ltx_text ltx_font_bold">Commonsense Reasoning</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.SS1.SSS4" title="In IV-A RAG for Text ‣ IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>4 </span><span class="ltx_text ltx_font_bold">Human-Machine Conversation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.SS1.SSS5" title="In IV-A RAG for Text ‣ IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>5 </span><span class="ltx_text ltx_font_bold">Neural Machine Translation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.SS1.SSS6" title="In IV-A RAG for Text ‣ IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>6 </span><span class="ltx_text ltx_font_bold">Event Extraction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.SS1.SSS7" title="In IV-A RAG for Text ‣ IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>7 </span><span class="ltx_text ltx_font_bold">Summarization</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.SS2" title="In IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_bold">RAG for Code</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.SS2.SSS1" title="In IV-B RAG for Code ‣ IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span>1 </span><span class="ltx_text ltx_font_bold">Code Generation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.SS2.SSS2" title="In IV-B RAG for Code ‣ IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span>2 </span><span class="ltx_text ltx_font_bold">Code Summarization</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.SS2.SSS3" title="In IV-B RAG for Code ‣ IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span>3 </span><span class="ltx_text ltx_font_bold">Code Completion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.SS2.SSS4" title="In IV-B RAG for Code ‣ IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span>4 </span><span class="ltx_text ltx_font_bold">Automatic Program Repair</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.SS2.SSS5" title="In IV-B RAG for Code ‣ IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span>5 </span><span class="ltx_text ltx_font_bold">Text-to-SQL and Code-based Semantic Parsing</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.SS2.SSS6" title="In IV-B RAG for Code ‣ IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span>6 </span><span class="ltx_text ltx_font_bold">Others</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.SS3" title="In IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_bold">RAG for Knowledge</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.SS3.SSS1" title="In IV-C RAG for Knowledge ‣ IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span>1 </span><span class="ltx_text ltx_font_bold">Knowledge Base Question Answering</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.SS3.SSS2" title="In IV-C RAG for Knowledge ‣ IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span>2 </span><span class="ltx_text ltx_font_bold">Knowledge-augmented Open-domain Question Answering</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.SS3.SSS3" title="In IV-C RAG for Knowledge ‣ IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span>3 </span><span class="ltx_text ltx_font_bold">Table for Question Answering</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.SS3.SSS4" title="In IV-C RAG for Knowledge ‣ IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span>4 </span><span class="ltx_text ltx_font_bold">Others</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.SS4" title="In IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span> </span><span class="ltx_text ltx_font_bold">RAG for Image</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.SS4.SSS1" title="In IV-D RAG for Image ‣ IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span>1 </span><span class="ltx_text ltx_font_bold">Image Generation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.SS4.SSS2" title="In IV-D RAG for Image ‣ IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span>2 </span><span class="ltx_text ltx_font_bold">Image Captioning</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.SS4.SSS3" title="In IV-D RAG for Image ‣ IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span>3 </span><span class="ltx_text ltx_font_bold">Others</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.SS5" title="In IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-E</span> </span><span class="ltx_text ltx_font_bold">RAG for Video</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.SS5.SSS1" title="In IV-E RAG for Video ‣ IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-E</span>1 </span><span class="ltx_text ltx_font_bold">Video Captioning</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.SS5.SSS2" title="In IV-E RAG for Video ‣ IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-E</span>2 </span><span class="ltx_text ltx_font_bold">Video QA&amp;Dialogue</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.SS5.SSS3" title="In IV-E RAG for Video ‣ IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-E</span>3 </span><span class="ltx_text ltx_font_bold">Others</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.SS6" title="In IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-F</span> </span><span class="ltx_text ltx_font_bold">RAG for Audio</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.SS6.SSS1" title="In IV-F RAG for Audio ‣ IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-F</span>1 </span><span class="ltx_text ltx_font_bold">Audio Generation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.SS6.SSS2" title="In IV-F RAG for Audio ‣ IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-F</span>2 </span><span class="ltx_text ltx_font_bold">Audio Captioning</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.SS7" title="In IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-G</span> </span><span class="ltx_text ltx_font_bold">RAG for 3D</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.SS7.SSS1" title="In IV-G RAG for 3D ‣ IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-G</span>1 </span><span class="ltx_text ltx_font_bold">Text-to-3D</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.SS8" title="In IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-H</span> </span><span class="ltx_text ltx_font_bold">RAG for Science</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.SS8.SSS1" title="In IV-H RAG for Science ‣ IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-H</span>1 </span><span class="ltx_text ltx_font_bold">Drug Discovery</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.SS8.SSS2" title="In IV-H RAG for Science ‣ IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-H</span>2 </span><span class="ltx_text ltx_font_bold">Biomedical Informatics Enhancement</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.SS8.SSS3" title="In IV-H RAG for Science ‣ IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-H</span>3 </span><span class="ltx_text ltx_font_bold">Math Applications</span></span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S5" title="In Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Benchmark</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S6" title="In Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Discussion</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S6.SS1" title="In VI Discussion ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-A</span> </span><span class="ltx_text ltx_font_bold">Limitations</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S6.SS1.SSS1" title="In VI-A Limitations ‣ VI Discussion ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-A</span>1 </span><span class="ltx_text ltx_font_bold">Noises in Retrieval Results</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S6.SS1.SSS2" title="In VI-A Limitations ‣ VI Discussion ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-A</span>2 </span><span class="ltx_text ltx_font_bold">Extra Overhead</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S6.SS1.SSS3" title="In VI-A Limitations ‣ VI Discussion ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-A</span>3 </span><span class="ltx_text ltx_font_bold">The Gap between Retrievers and Generators</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S6.SS1.SSS4" title="In VI-A Limitations ‣ VI Discussion ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-A</span>4 </span><span class="ltx_text ltx_font_bold">Increased System Complexity</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S6.SS1.SSS5" title="In VI-A Limitations ‣ VI Discussion ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-A</span>5 </span><span class="ltx_text ltx_font_bold">Lengthy Context</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S6.SS2" title="In VI Discussion ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-B</span> </span><span class="ltx_text ltx_font_bold">Potential Future Directions</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S6.SS2.SSS1" title="In VI-B Potential Future Directions ‣ VI Discussion ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-B</span>1 </span><span class="ltx_text ltx_font_bold">Novel Design of Augmentation Methodologies</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S6.SS2.SSS2" title="In VI-B Potential Future Directions ‣ VI Discussion ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-B</span>2 </span><span class="ltx_text ltx_font_bold">Flexible RAG Pipelines</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S6.SS2.SSS3" title="In VI-B Potential Future Directions ‣ VI Discussion ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-B</span>3 </span><span class="ltx_text ltx_font_bold">Broader Applications</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S6.SS2.SSS4" title="In VI-B Potential Future Directions ‣ VI Discussion ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-B</span>4 </span><span class="ltx_text ltx_font_bold">Efficient Deployment and Processing</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S6.SS2.SSS5" title="In VI-B Potential Future Directions ‣ VI Discussion ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-B</span>5 </span><span class="ltx_text ltx_font_bold">Incorporating Long-tail and Real-time Knowledge</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S6.SS2.SSS6" title="In VI-B Potential Future Directions ‣ VI Discussion ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-B</span>6 </span><span class="ltx_text ltx_font_bold">Combined with Other Techniques</span></span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S7" title="In Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Retrieval-Augmented Generation for
<br class="ltx_break"/>AI-Generated Content: A Survey</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Penghao Zhao<sup class="ltx_sup" id="id10.10.id1"><span class="ltx_text ltx_font_italic" id="id10.10.id1.1">∗</span></sup>, Hailin Zhang<sup class="ltx_sup" id="id11.11.id2"><span class="ltx_text ltx_font_italic" id="id11.11.id2.1">∗</span></sup>, Qinhan Yu, Zhengren Wang, Yunteng Geng, 
<br class="ltx_break"/>Fangcheng Fu<sup class="ltx_sup" id="id12.12.id3">†</sup>, Ling Yang, Wentao Zhang<sup class="ltx_sup" id="id13.13.id4">†</sup>, Jie Jiang, Bin Cui<sup class="ltx_sup" id="id14.14.id5">†</sup>
</span><span class="ltx_author_notes"><sup class="ltx_sup" id="id15.15.id1"><span class="ltx_text ltx_font_italic" id="id15.15.id1.1">∗</span></sup> Both authors contributed equally to this research.<sup class="ltx_sup" id="id16.16.id1">†</sup> Corresponding authors.<math alttext="\bullet" class="ltx_Math" display="inline" id="id8.8.m1.1"><semantics id="id8.8.m1.1a"><mo id="id8.8.m1.1.1" xref="id8.8.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="id8.8.m1.1b"><ci id="id8.8.m1.1.1.cmml" xref="id8.8.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="id8.8.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="id8.8.m1.1d">∙</annotation></semantics></math> Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang and Bin Cui are with Peking University (e-mail: penghao.zhao@stu.pku.edu.cn, z.hl@pku.edu.cn, yuqinhan@stu.pku.edu.cn, wzr@stu.pku.edu.cn, 1800012997@pku.edu.cn, ccchengff@pku.edu.cn, yangling0818@163.com, wentao.zhang@pku.edu.cn, bin.cui@pku.edu.cn).<math alttext="\bullet" class="ltx_Math" display="inline" id="id9.9.m1.1"><semantics id="id9.9.m1.1a"><mo id="id9.9.m1.1.1" xref="id9.9.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="id9.9.m1.1b"><ci id="id9.9.m1.1.1.cmml" xref="id9.9.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="id9.9.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="id9.9.m1.1d">∙</annotation></semantics></math> Jie Jiang is with Tencent Inc. (email: zeus@tencent.com)</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id17.id1">Advancements in model algorithms, the growth of foundational models, and access to high-quality datasets have propelled the evolution of Artificial Intelligence Generated Content (AIGC).
Despite its notable successes, AIGC still faces hurdles such as updating knowledge, handling long-tail data, mitigating data leakage, and managing high training and inference costs.
Retrieval-Augmented Generation (RAG) has recently emerged as a paradigm to address such challenges.
In particular, RAG introduces the information retrieval process, which enhances the generation process by retrieving relevant objects from available data stores, leading to higher accuracy and better robustness.
In this paper, we comprehensively review existing efforts that integrate RAG techniques into AIGC scenarios.
We first classify RAG foundations according to how the retriever augments the generator,
distilling the fundamental abstractions of the augmentation methodologies for various retrievers and generators.
This unified perspective encompasses all RAG scenarios, illuminating advancements and pivotal technologies that help with potential future progress.
We also summarize additional enhancements methods for RAG, facilitating effective engineering and implementation of RAG systems.
Then from another view, we survey on practical applications of RAG across different modalities and tasks, offering valuable references for researchers and practitioners.
Furthermore, we introduce the benchmarks for RAG, discuss the limitations of current RAG systems, and suggest potential directions for future research.
Github: <a class="ltx_ref ltx_href" href="https://github.com/PKU-DAIR/RAG-Survey" title="">https://github.com/PKU-DAIR/RAG-Survey</a>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Retrieval-augmented generation, AI-generated content, generative models, information retrieval.

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_bold" id="S1.1.1">Introduction</span>
</h2>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="385" id="S1.F1.g1" src="x1.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">A generic RAG architecture. The user queries, spanning different modalities, serve as input to both the retriever and the generator. The retriever extracts relevant information from data sources. The generator interacts with the retrieval results and ultimately produces outcomes of various modalities.</span></figcaption>
</figure>
<section class="ltx_subsection" id="S1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S1.SS1.5.1.1">I-A</span> </span><span class="ltx_text ltx_font_bold" id="S1.SS1.6.2">Background</span>
</h3>
<div class="ltx_para" id="S1.SS1.p1">
<p class="ltx_p" id="S1.SS1.p1.1">Recent years have witnessed the surge in interests surrounding Artificial Intelligence Generated Content (AIGC).
Various content generation tools have been meticulously crafted to produce diverse outputs across various modalities, such as Large Language Models (LLMs) including the GPT series <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib3" title="">3</a>]</cite> and the LLAMA series <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib6" title="">6</a>]</cite> for texts and codes, DALL-E <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib9" title="">9</a>]</cite> and Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib10" title="">10</a>]</cite> for images, and Sora <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib11" title="">11</a>]</cite> for videos.
The word “AIGC” emphasizes that the contents are produced by advanced generative models other than human beings or rule-based approaches.
These generative models have achieved remarkable performance due to the utilization of novel model algorithms, explosive scale of foundation models, and massive high-quality datasets.
Specifically, sequence-to-sequence tasks have transitioned from utilizing Long Short-Term Memory (LSTM) networks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib12" title="">12</a>]</cite> to Transformer-based models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib13" title="">13</a>]</cite>, and image-generation tasks have shifted from Generative Adversarial Networks (GANs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib14" title="">14</a>]</cite> to Latent Diffusion Models (LDMs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib10" title="">10</a>]</cite> as well.
Notably, the architecture of foundation models, initially constituted by millions of parameters <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib16" title="">16</a>]</cite>, has now grown to billions or even trillions of parameters <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib17" title="">17</a>]</cite>.
These advancements are further bolstered by the availability of rich, high-quality datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib18" title="">18</a>]</cite>, which provide ample training samples to fully optimize model parameters.</p>
</div>
<div class="ltx_para" id="S1.SS1.p2">
<p class="ltx_p" id="S1.SS1.p2.1">Information retrieval is another pivotal application within the field of computer science.
Different from generation, retrieval aims to locate relevant existing objects from a vast pool of resources.
The most prevalent application of retrieval lies in web search engines, which primarily focus on the task of document retrieval <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib20" title="">20</a>]</cite>.
In the present era, efficient information retrieval systems can handle document collections on the order of billions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib22" title="">22</a>]</cite>.
Besides documents, retrieval has also been applied for many other modalities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib26" title="">26</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.SS1.p3">
<p class="ltx_p" id="S1.SS1.p3.1">Despite significant advancements in generative models, AIGC still grapples with challenges like outdated knowledge, lack of long-tail knowledge <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib27" title="">27</a>]</cite>, and risks of leaking private training data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib28" title="">28</a>]</cite>.
Retrieval-Augmented Generation (RAG) aims to mitigate these issues with its flexible data repository <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib29" title="">29</a>]</cite>. The retrievable knowledge acts as non-parametric memory, which is easily updatable, accommodates extensive long-tail knowledge, and can encode confidential data.
Moreover, retrieval can lower generation costs. RAG can reduce the size of large models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib30" title="">30</a>]</cite>, support long contexts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib31" title="">31</a>]</cite>, and eliminate certain generation steps <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib32" title="">32</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.SS1.p4">
<p class="ltx_p" id="S1.SS1.p4.1">A typical RAG process is depicted in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S1.F1" title="Figure 1 ‣ I Introduction ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_tag">1</span></a>.
Given an input query, the retriever identifies relevant data sources, and the retrieved information interacts with the generator to improve the generation process.
There are several <span class="ltx_text ltx_font_italic ltx_framed ltx_framed_underline" id="S1.SS1.p4.1.1">foundational paradigms</span> (<span class="ltx_text ltx_font_italic ltx_framed ltx_framed_underline" id="S1.SS1.p4.1.2">foundations</span> in short) according to how the retrieved results augment the generation:
they can serve as augmented input to the generator <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib34" title="">34</a>]</cite>; they can join at the middle stage of generation as latent representations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib36" title="">36</a>]</cite>; they can contribute to the final generation results in the form of logits <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib38" title="">38</a>]</cite>; they can even influence or omit certain generation steps <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib39" title="">39</a>]</cite>.
Additionally, researchers have proposed various <span class="ltx_text ltx_font_italic ltx_framed ltx_framed_underline" id="S1.SS1.p4.1.3">enhancements</span> to improve the foundational RAG process.
These methods encompass specific optimizations for individual components as well as holistic enhancements aimed at the entire pipeline.</p>
</div>
<div class="ltx_para" id="S1.SS1.p5">
<p class="ltx_p" id="S1.SS1.p5.1">In addition, while the concept of RAG initially emerged in text-to-text generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib34" title="">34</a>]</cite>, this technique has also found <span class="ltx_text ltx_font_italic ltx_framed ltx_framed_underline" id="S1.SS1.p5.1.1">applications</span> across various domains, including codes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib42" title="">42</a>]</cite>, audios <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib44" title="">44</a>]</cite>, images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib47" title="">47</a>]</cite>, videos <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib49" title="">49</a>]</cite>, 3D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib50" title="">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib51" title="">51</a>]</cite>, knowledge <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib52" title="">52</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib54" title="">54</a>]</cite>, and AI for science <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib56" title="">56</a>]</cite>.
In particular, the essential idea and process of RAG are largely consistent across modalities.
However, it necessitates minor adjustments in augmentation techniques, and the selection of retrievers and generators varies depending on the specific modalities and applications.</p>
</div>
<div class="ltx_para" id="S1.SS1.p6">
<p class="ltx_p" id="S1.SS1.p6.1">Despite the rapid growth in recent research on RAG and the booming applications, a systematic review encompassing all foundations, enhancements, and applications is notably absent, hindering the development of this field.
For one thing, the absence of discussion on RAG foundations significantly undermines the practical value of the research in this domain, leaving the potential of RAG not fully explored.
While the majority of research interest, particularly among LLM researchers, centers on query-based RAG in text-generation tasks, it is essential to acknowledge that other RAG foundations are also effective and with significant potential for usage and further development.
For another, the lack of an overview on RAG applications causes researchers and practitioners to overlook RAG’s progress across multiple modalities and remain unaware of how RAG can be effectively applied.
Although text generation is typically considered as the main application of RAG, we emphasize that the development of RAG in other modalities has also begun to catch on and has yielded promising advancements.
Certain modalities have a rich historical connection to retrieval techniques, infusing RAG with distinctive characteristics.
Inspired by this, in this paper, our objective is to present a comprehensive survey to provide a systematic overview of RAG.</p>
</div>
</section>
<section class="ltx_subsection" id="S1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S1.SS2.5.1.1">I-B</span> </span><span class="ltx_text ltx_font_bold" id="S1.SS2.6.2">Contribution</span>
</h3>
<div class="ltx_para" id="S1.SS2.p1">
<p class="ltx_p" id="S1.SS2.p1.1">This survey offers a comprehensive overview of RAG, covering foundations, enhancements, applications, benchmarks, limitations, and potential future directions.
Despite variations in retrievers and generators across modalities and tasks, we distill the core principles of RAG foundations, viewing applications as adaptations of these principles.
We aim to offer references and guidelines to researchers and practitioners, providing valuable insights for advancing RAG methodologies and related applications.
In summary, we list our contributions as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We conduct a comprehensive review of RAG, and distill the abstractions of RAG foundations for various retrievers and generators.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We investigate the enhancements in the literature of RAG, elaborating the techniques leveraged to enable more effective RAG systems.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">For various modalities and tasks, we survey existing AIGC methods that incorporate RAG techniques, exhibiting how RAG contributes to current generative models.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">We discuss the limitations and promising research directions of RAG, shedding light on its potential future development.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S1.SS3.5.1.1">I-C</span> </span><span class="ltx_text ltx_font_bold" id="S1.SS3.6.2">Related Work</span>
</h3>
<div class="ltx_para" id="S1.SS3.p1">
<p class="ltx_p" id="S1.SS3.p1.1">As the field of RAG advances, several surveys have emerged; yet they address only specific facets of the area. In particular, they either exclusively focus on a single RAG foundation or provide only a brief overview of RAG augmentation methodologies for limited scenarios.</p>
</div>
<div class="ltx_para" id="S1.SS3.p2">
<p class="ltx_p" id="S1.SS3.p2.1">Most of the existing works focus on text-related RAG tasks that are facilitated by LLMs, without in-depth investigation in other modalities.
The survey by Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib57" title="">57</a>]</cite> offers a basic overview of RAG and discusses specific applications within the scope of text generation tasks.
In a similar vein, the tutorial crafted by Asai et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib58" title="">58</a>]</cite> centers on retrieval-based language models, detailing their structures and training strategies.
Meanwhile, a recent survey by Gao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib59" title="">59</a>]</cite> explores RAG in the context of LLMs, with a particular emphasis on enhancement approaches for query-based RAG.
Recognizing that RAG has extended beyond the text domain, our work broadens its reach to the entire AIGC landscape, facilitating a more comprehensive coverage of RAG research.</p>
</div>
<div class="ltx_para" id="S1.SS3.p3">
<p class="ltx_p" id="S1.SS3.p3.1">In addition, another survey proposed by Zhao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib60" title="">60</a>]</cite> introduces RAG applications across multiple modalities, but ignoring the discussion on RAG foundations.
Another work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib61" title="">61</a>]</cite> covers only part works of other modalities.
While existing research has explored various aspects of RAG, there remains a need for a comprehensive overview that covers RAG foundations, enhancements, and its applicability across different domains.
In this paper, we aim to address the gap by presenting a systematic survey of RAG.</p>
</div>
</section>
<section class="ltx_subsection" id="S1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S1.SS4.5.1.1">I-D</span> </span><span class="ltx_text ltx_font_bold" id="S1.SS4.6.2">Roadmap</span>
</h3>
<div class="ltx_para" id="S1.SS4.p1">
<p class="ltx_p" id="S1.SS4.p1.1">The rest of the paper is organized as follows.
Section <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S2" title="II Preliminary ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_tag">II</span></a> elaborates on the preliminary of RAG, introducing retrievers and generators.
Section <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S3" title="III Methodologies ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_tag">III</span></a> presents RAG foundations and further enhancements on RAG.
Section <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4" title="IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_tag">IV</span></a> reviews existing research on RAG across various applications.
Section <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S5" title="V Benchmark ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_tag">V</span></a> investigates the benchmark frameworks for RAG.
Section <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S6" title="VI Discussion ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_tag">VI</span></a> discusses current limitations of RAG and potential future directions.
Finally, Section <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S7" title="VII Conclusion ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_tag">VII</span></a> concludes this paper.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_bold" id="S2.1.1">Preliminary</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In this section, we provide an overview of the general RAG architecture and explore the generators and the retrievers in today’s RAG-based AIGC.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.5.1.1">II-A</span> </span><span class="ltx_text ltx_font_bold" id="S2.SS1.6.2">Overview</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S1.F1" title="Figure 1 ‣ I Introduction ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_tag">1</span></a>, the entire RAG system consists of two core modules: the retriever and the generator,
where the retriever searches for relevant information from the data store and the generator produces the required contents.
The RAG process unfolds as follows: (i) the retriever initially receives the input query and searches for relevant information; (ii) then, the original query and the retrieval results are fed into the generator through a specific augmentation methodology; (iii) finally, the generator produces the desired outcomes.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.5.1.1">II-B</span> </span><span class="ltx_text ltx_font_bold" id="S2.SS2.6.2">Generator</span>
</h3>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="351" id="S2.F2.g1" src="x2.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S2.F2.3.2" style="font-size:90%;">General architectures of several generators.</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The remarkable performance of generative AI across diverse tasks has ushered in the era of AIGC.
The generation module plays a crucial role within the RAG system.
Different generative models are applied for different scenarios, such as transformer models for text-to-text tasks, VisualGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib62" title="">62</a>]</cite> for image-to-text tasks, Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib10" title="">10</a>]</cite> for text-to-image tasks, Codex <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib2" title="">2</a>]</cite> for text-to-code tasks, etc.
Here we introduce 4 typical generators that are frequently used in RAG: transformer model, LSTM, diffusion model, and GAN.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS2.SSS1.5.1.1">II-B</span>1 </span><span class="ltx_text ltx_font_bold" id="S2.SS2.SSS1.6.2">Transformer Model</span>
</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">Transformer models are one of the best performing models in the field of Natural Language Processing (NLP), consisting of self-attention mechanisms, feed-forward networks, layer normalization modules, and residual networks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib63" title="">63</a>]</cite>.
As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S2.F2" title="Figure 2 ‣ II-B Generator ‣ II Preliminary ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_tag">2</span></a>, the final output sequence is produced by executing vocabulary classification at each generative step on a sequence of latent representations derived from tokenization and embedding.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS2.SSS2.5.1.1">II-B</span>2 </span><span class="ltx_text ltx_font_bold" id="S2.SS2.SSS2.6.2">LSTM</span>
</h4>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S2.F2" title="Figure 2 ‣ II-B Generator ‣ II Preliminary ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_tag">2</span></a>, Long Short-Term Memory (LSTM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib64" title="">64</a>]</cite> is a special form of Recurrent Neural Network (RNN) model.
It tackles the issues of exploding/vanishing gradients in long-term dependency processing by incorporating cell states and gating mechanisms.
The model comprises three gates (Input, Forget, and Output) that filter information, and a central Cell State module that retains and manages information.
It uses the same vocabulary classification method as transformer models to autoregressively generate outputs.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS2.SSS3.5.1.1">II-B</span>3 </span><span class="ltx_text ltx_font_bold" id="S2.SS2.SSS3.6.2">Diffusion Model</span>
</h4>
<div class="ltx_para" id="S2.SS2.SSS3.p1">
<p class="ltx_p" id="S2.SS2.SSS3.p1.1">Diffusion models are a family of deep generative models that can create realistic and diverse samples of data (including images, texts, videos, molecules, etc.) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib65" title="">65</a>]</cite>.
As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S2.F2" title="Figure 2 ‣ II-B Generator ‣ II Preliminary ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_tag">2</span></a>, diffusion models work by gradually adding noise to data until it becomes random, then reversing the process to generate new data from noise.
This process is based on probabilistic modeling and neural networks.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS2.SSS4.5.1.1">II-B</span>4 </span><span class="ltx_text ltx_font_bold" id="S2.SS2.SSS4.6.2">GAN</span>
</h4>
<div class="ltx_para" id="S2.SS2.SSS4.p1">
<p class="ltx_p" id="S2.SS2.SSS4.p1.1">Generative Adversarial Networks (GANs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib14" title="">14</a>]</cite> are highly anticipated deep learning models which can simulate and generate realistic images, audio, and other data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib66" title="">66</a>]</cite>.
As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S2.F2" title="Figure 2 ‣ II-B Generator ‣ II Preliminary ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_tag">2</span></a>, a typical GAN consists of two main components: a generator and a discriminator.
These two parts compete with each other through adversarial learning, allowing the generator to continuously improve its ability to generate realistic samples, while the discriminator continuously improves its ability to distinguish between true and false samples.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.5.1.1">II-C</span> </span><span class="ltx_text ltx_font_bold" id="S2.SS3.6.2">Retriever</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Retrieval is to identify and obtain relevant information given an information need.
Specifically, let’s consider information resources that can be conceptualized as a key-value store, where each key corresponds to a value (keys and values can be identical).
Given a query, the objective is to search for the top-<math alttext="k" class="ltx_Math" display="inline" id="S2.SS3.p1.1.m1.1"><semantics id="S2.SS3.p1.1.m1.1a"><mi id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.1b"><ci id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.1.m1.1d">italic_k</annotation></semantics></math> most similar keys using a similarity function, and obtain the paired values.
Based on different similarity functions, existing retrieval methods can be categorized into sparse retrieval, dense retrieval, and others.
In widely used sparse and dense retrieval, the entire process can be divided into two distinct phases: (i) each object is first encoded into a specific representation; and then (ii) an index is constructed to organize the data source for efficient search.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS3.SSS1.5.1.1">II-C</span>1 </span><span class="ltx_text ltx_font_bold" id="S2.SS3.SSS1.6.2">Sparse Retriever</span>
</h4>
<div class="ltx_para" id="S2.SS3.SSS1.p1">
<p class="ltx_p" id="S2.SS3.SSS1.p1.1">Sparse retrieval methods are commonly used in document retrieval, where the keys/values represent the documents to be searched.
These methods leverage term matching metrics such as TF-IDF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib67" title="">67</a>]</cite>, query likelihood <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib68" title="">68</a>]</cite>, and BM25 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib19" title="">19</a>]</cite>, which analyze word statistics from texts and construct inverted indices for efficient searching.
Essentially, BM25 is a strong baseline in large-scale web search, integrating inverse document frequency weights, query token occurrences, and other pertinent metrics.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS1.p2">
<p class="ltx_p" id="S2.SS3.SSS1.p2.1">To enable efficient search, sparse retrieval typically leverages an inverted index to organize documents.
Concretely, each term from the query performs a lookup to obtain a list of candidate documents, which are subsequently ranked based on their statistical scores.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS3.SSS2.5.1.1">II-C</span>2 </span><span class="ltx_text ltx_font_bold" id="S2.SS3.SSS2.6.2">Dense Retriever</span>
</h4>
<div class="ltx_para" id="S2.SS3.SSS2.p1">
<p class="ltx_p" id="S2.SS3.SSS2.p1.1">Unlike sparse retrieval, dense retrieval methods represent queries and keys using dense embedding vectors, and build Approximate Nearest Neighbor (ANN) index to speed up the search.
This can be applied to all modalities.
For text data, recent advancements in pre-trained models (such as BERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib15" title="">15</a>]</cite>) have been employed encode queries and keys individually <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib20" title="">20</a>]</cite>. This approach is often referred to as Dense Passage Retrieval (DPR).
Similar to text, models have been proposed to encode code data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib25" title="">25</a>]</cite>, audio data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib69" title="">69</a>]</cite>, image data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib24" title="">24</a>]</cite>, video data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib70" title="">70</a>]</cite>, etc.
The similarity score between dense representations are usually computed with metrics such as cosine, inner product, L2-distance.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS2.p2">
<p class="ltx_p" id="S2.SS3.SSS2.p2.1">During training, dense retrieval uses contrastive learning to increase the similarity of positive samples and decrease that of negative ones. Several hard negative techniques <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib71" title="">71</a>]</cite> have been proposed to further enhance model quality. For efficient searching during inference, ANN methods are employed. Various indices are developed to serve ANN search, including tree <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib72" title="">72</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib73" title="">73</a>]</cite>, locality sensitive hashing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib74" title="">74</a>]</cite>, neighbor graph indices (e.g., HNSW <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib75" title="">75</a>]</cite>, DiskANN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib76" title="">76</a>]</cite>), and combined graph and inverted indices (e.g., SPANN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib22" title="">22</a>]</cite>).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS3.SSS3.5.1.1">II-C</span>3 </span><span class="ltx_text ltx_font_bold" id="S2.SS3.SSS3.6.2">Others</span>
</h4>
<div class="ltx_para" id="S2.SS3.SSS3.p1">
<p class="ltx_p" id="S2.SS3.SSS3.p1.1">In addition to sparse retrieval and dense retrieval, there are alternative methods for retrieving relevant objects <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib77" title="">77</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib78" title="">78</a>]</cite>.
Instead of calculating representations, some research works directly use the edit distance between natural language texts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib79" title="">79</a>]</cite> or abstract syntax trees (AST) of code snippets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib80" title="">80</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib81" title="">81</a>]</cite>.
In knowledge graphs, entities are connected by relations, serving as a pre-built index for retrieval. Thus, RAG methods utilizing knowledge graphs can employ <math alttext="k" class="ltx_Math" display="inline" id="S2.SS3.SSS3.p1.1.m1.1"><semantics id="S2.SS3.SSS3.p1.1.m1.1a"><mi id="S2.SS3.SSS3.p1.1.m1.1.1" xref="S2.SS3.SSS3.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS3.p1.1.m1.1b"><ci id="S2.SS3.SSS3.p1.1.m1.1.1.cmml" xref="S2.SS3.SSS3.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS3.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS3.p1.1.m1.1d">italic_k</annotation></semantics></math>-hop neighbor searches for retrieval <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib82" title="">82</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib83" title="">83</a>]</cite>. Another retrieval method is Named Entity Recognition (NER) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib84" title="">84</a>]</cite>, where the query is the input and the entities act as keys.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_bold" id="S3.1.1">Methodologies</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we first introduce foundational paradigms of RAG, and then outline enhancement methods that further improve the effectiveness.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_bold" id="S3.SS1.6.2">RAG Foundations</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Based on how the retriever augments the generator, we categorize RAG foundations into 4 classes, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S3.F3" title="Figure 3 ‣ III-A RAG Foundations ‣ III Methodologies ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="120" id="S3.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.3.2" style="font-size:90%;">Taxonomy of RAG foundations.</span></figcaption>
</figure>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS1.SSS1.5.1.1">III-A</span>1 </span><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS1.6.2">Query-based RAG</span>
</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">Stemming from the idea of prompt augmentation, query-based RAG seamlessly integrates the user’s query with insights from retrieved information, feeding it directly into the initial stage of the generator’s input.
This method is prevalent in RAG applications. Post-retrieval, the obtained content is merged with the user’s original query to form a composite input, which is then processed by the generator to create a response.
Query-based RAG is widely employed across various modalities.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p2">
<p class="ltx_p" id="S3.SS1.SSS1.p2.1">For text generation, REALM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib33" title="">33</a>]</cite> employs a dual-BERT framework to streamline knowledge retrieval and integration, marrying pre-trained models with knowledge extractors.
Lewis et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib34" title="">34</a>]</cite> leveraged DPR for information retrieval and employs BART as the generator
to effectively enhance the generation.
SELF-RAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib85" title="">85</a>]</cite> utilizes a critique module to determine whether the retrieval is required.
In addition to being compatible with local generators, query-based RAG is also applicable to scenarios that use LLM through API calls.
REPLUG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib86" title="">86</a>]</cite> follows this methodology by treating the language model as a “black box”, and effectively integrates relevant external documents into the query.
In-Context RALM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib87" title="">87</a>]</cite> uses BM25 for document retrieval and trains a predictive reranker to reorder and integrate the top-ranked documents.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p3">
<p class="ltx_p" id="S3.SS1.SSS1.p3.1">In the field of code, several works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib88" title="">88</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib89" title="">89</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib90" title="">90</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib91" title="">91</a>]</cite> have utilized the query-based paradigm to incorporate contextual information from text or code into the prompt, resulting in improved effectiveness of downstream tasks.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p4">
<p class="ltx_p" id="S3.SS1.SSS1.p4.1">Recent researches in Knowledge Base Question Answering (KBQA) has also shown significant effects of combining retrieval and language models. For instance, Uni-Parser <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib92" title="">92</a>]</cite>, RNG-KBQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib82" title="">82</a>]</cite>, and ECBRF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib93" title="">93</a>]</cite> effectively improve the performance and accuracy of QA systems by merging queries and retrieved information into prompts.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p5">
<p class="ltx_p" id="S3.SS1.SSS1.p5.1">In the AI-for-Science field, Chat-Orthopedist <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib94" title="">94</a>]</cite> aids shared decision-making for adolescents with idiopathic scoliosis, improving LLMs’ effectiveness and information precision by incorporating retrieved data into model prompts.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p6">
<p class="ltx_p" id="S3.SS1.SSS1.p6.1">In the image generation task, RetrieveGAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib45" title="">45</a>]</cite> boosts the relevance and precision of generated images by incorporating retrieved data, such as selected image patches and their bounding boxes, into the generator’s input stage.
IC-GAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib95" title="">95</a>]</cite> modulates the specific conditions and details of the generated images by concatenating noise vectors with instance features.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p7">
<p class="ltx_p" id="S3.SS1.SSS1.p7.1">For 3D generation, RetDream <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib50" title="">50</a>]</cite> initially utilizes CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib24" title="">24</a>]</cite> to retrieve relevant 3D assets, then merges the retrieved contents with the user input during the input phase.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p8">
<p class="ltx_p" id="S3.SS1.SSS1.p8.1">Query-based RAG, often paired with LLM generators, offers modular flexibility, allowing swift integration of pre-trained components for quick deployment. Prompt design is crucial for utilizing retrieved data within this setup.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS1.SSS2.5.1.1">III-A</span>2 </span><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS2.6.2">Latent Representation-based RAG</span>
</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">In latent representation-based RAG framework, retrieved objects are incorporated into generative models as latent representations. This enhances the model’s comprehension abilities and improves the quality of the generated content.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p2">
<p class="ltx_p" id="S3.SS1.SSS2.p2.1">In the text field, FiD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib35" title="">35</a>]</cite> and RETRO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib36" title="">36</a>]</cite> are two classic structures of latent representation-based RAG, with many subsequent works conducting modifications based on them.
FiD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib35" title="">35</a>]</cite> processes each retrieved paragraph and its title alongside the query through distinct encoders, then amalgamates the resulting latent representations for decoding by a single decoder to produce the final output.
RETRO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib36" title="">36</a>]</cite> retrieves relevant information for each segmented sub-query, then applies a novel module termed Chunked Cross-Attention (CCA) to integrate the retrieved contents with each sub-query tokens.
In addition, there are other noteworthy novel structures within the scope of latent representation-based RAG.
Several studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib96" title="">96</a>]</cite> have integrated k Nearest Neighbor (kNN) search within transformer blocks, allowing for input chunking and, in theory, addressing the long-criticized context length constraints of Transformer models.
Kuratov et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib97" title="">97</a>]</cite> integrated Transformer with RNN, utilizing the model’s intermediate output as the content for retrieval.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p3">
<p class="ltx_p" id="S3.SS1.SSS2.p3.1">In the realms of code and science, FiD has gained widespread adoption, with applications spanning various code-related fields <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib98" title="">98</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib99" title="">99</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib100" title="">100</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib101" title="">101</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib102" title="">102</a>]</cite>, and AI-for-Science <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib55" title="">55</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p4">
<p class="ltx_p" id="S3.SS1.SSS2.p4.1">In the image domain, several studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib103" title="">103</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib104" title="">104</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib105" title="">105</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib106" title="">106</a>]</cite> employ cross-attention mechanisms to fuse retrieval results by integrating their latent representations.
Conversely, Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib107" title="">107</a>]</cite> implement a text-image Affine Combination Module (ACM) that directly concatenates hidden features.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p5">
<p class="ltx_p" id="S3.SS1.SSS2.p5.1">Within the knowledge domain, several studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib108" title="">108</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib109" title="">109</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib110" title="">110</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib111" title="">111</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib112" title="">112</a>]</cite> have adopted FiD and its derivatives for downstream tasks. EaE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib113" title="">113</a>]</cite> enhances the generator’s understanding through entity-specific parameterization, while TOME <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib114" title="">114</a>]</cite> pivots to a nuanced encoding of mentions, prioritizing the granularity of mentions over entity representations alone.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p6">
<p class="ltx_p" id="S3.SS1.SSS2.p6.1">In the field of 3D generation, ReMoDiffuse <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib51" title="">51</a>]</cite> introduces a semantics-modulated attention mechanism which enhances the accuracy of generating corresponding 3D motions based on textual descriptions. AMD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib115" title="">115</a>]</cite> achieves efficient conversion from text to 3D motion by fusing the original diffusion process with the reference diffusion process.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p7">
<p class="ltx_p" id="S3.SS1.SSS2.p7.1">In the audio domain, Koizumi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib43" title="">43</a>]</cite> utilized an LLM, incorporating encoded dense features in the attention module to guide the generation of audio captions. Re-AudioLDM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib116" title="">116</a>]</cite> utilizes distinct encoders to extract deep features from text and audio, which are then integrated into the attention mechanism of its Latent Diffusion Model (LDM).</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p8">
<p class="ltx_p" id="S3.SS1.SSS2.p8.1">For video captioning, R-ConvED <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib48" title="">48</a>]</cite> uses a convolutional encoder-decoder network to process retrieved video-sentence pairs with an attention mechanism, generating hidden states to produce captions. CARE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib117" title="">117</a>]</cite> introduces a concept detector to produce concept probabilities, and incorporates concept representations into a hybrid attention mechanism.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p9">
<p class="ltx_p" id="S3.SS1.SSS2.p9.1">EgoInstructor <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib49" title="">49</a>]</cite> uses gated-cross attention to merge text and video features, improving the relevance and coherence of captions for egocentric videos.
Latent representation-based RAG, adaptable across modalities and tasks, blends retriever and generator hidden states but requires additional training for aligning latent spaces. It enables the development of sophisticated algorithms that seamlessly incorporate retrieved information.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS1.SSS3.5.1.1">III-A</span>3 </span><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS3.6.2">Logit-based RAG</span>
</h4>
<div class="ltx_para" id="S3.SS1.SSS3.p1">
<p class="ltx_p" id="S3.SS1.SSS3.p1.1">In logit-based RAG, generative models integrate retrieval information through logits during the decoding process.
Typically, the logits are combined through simple summation or models to compute the probabilities for step-wise generation.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS3.p2">
<p class="ltx_p" id="S3.SS1.SSS3.p2.1">In the text domain, kNN-LM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib37" title="">37</a>]</cite> and its variant <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib38" title="">38</a>]</cite> blend language model probabilities with those from retrieval distances of similar prefixes at each decoding step.
TRIME <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib118" title="">118</a>]</cite> and NPM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib119" title="">119</a>]</cite> are radical evolutions of traditional kNN-LM approaches, using closely aligned tokens from a local database as output, particularly boosting performance in long-tail distribution scenarios.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS3.p3">
<p class="ltx_p" id="S3.SS1.SSS3.p3.1">Beyond text, other modalities, such as code and image, also leverage logit-based RAG.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS3.p4">
<p class="ltx_p" id="S3.SS1.SSS3.p4.1">In the domain of code, several studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib80" title="">80</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib120" title="">120</a>]</cite> have also adopted the concept kNN to enhance final output control, thereby achieving superior performance.
Furthermore, EDITSUM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib98" title="">98</a>]</cite> improves the quality of code summarization by integrating prototype summaries at the logit level.
For image captioning, MA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib121" title="">121</a>]</cite> directly applies the kNN-LM framework to address the image caption problem, achieving favorable results.
In summary, logit-based RAG utilizes historical data to deduce current states and merges information at the logit level, ideal for sequence generation. It focuses on generator training and allows for novel methods that capitalize on probability distributions for future tasks.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS1.SSS4.5.1.1">III-A</span>4 </span><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS4.6.2">Speculative RAG</span>
</h4>
<div class="ltx_para" id="S3.SS1.SSS4.p1">
<p class="ltx_p" id="S3.SS1.SSS4.p1.1">Speculative RAG seeks opportunities to use retrieval instead of pure generation, aiming to save resources and accelerate response speed.
REST <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib32" title="">32</a>]</cite> replaces the small models in speculative decoding <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib122" title="">122</a>]</cite> with retrieval, enabling the generation of drafts.
GPTCache <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib39" title="">39</a>]</cite> addresses the issue of high latency when using the LLM APIs by building a semantic cache for storing LLM responses.
COG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib123" title="">123</a>]</cite> decomposes the text generation process into a series of copy-and-paste operations, retrieving words or phrases from the documents instead of generation.
Cao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib124" title="">124</a>]</cite> proposed a new paradigm to eliminate the dependence of the final result on the quality of the first-stage retrieved content, replacing generation with directly retrieved phrase level content.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS4.p2">
<p class="ltx_p" id="S3.SS1.SSS4.p2.1">In conclusion, speculative RAG is currently primarily applicable to sequential data.
It decouples the generator and the retriever, enabling the direct use of pre-trained models as components.
Within this paradigm, we can explore a wider range of strategies to effectively utilize the retrieved content.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_bold" id="S3.SS2.6.2">RAG Enhancements</span>
</h3>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="230" id="S3.F4.g1" src="x4.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S3.F4.3.2" style="font-size:90%;">Taxonomy of RAG Enhancements.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">In this section, we introduce methods which enhance the performance of a constructed RAG system.
We categorize existing methods into 5 groups based on their enhancement targets: input, retriever, generator, result, and the entire pipeline.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS2.SSS1.5.1.1">III-B</span>1 </span><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.6.2">Input Enhancement</span>
</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">The input, initially fed into the retriever, significantly impacts the final outcome of the retrieval stage. In this section, we introduce two methods for input enhancement: query transformation and data augmentation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p2.1.1">Query Transformation:</span>
Query transformation can enhance the result of retrieval by modifying the input query.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p3">
<p class="ltx_p" id="S3.SS2.SSS1.p3.1">Query2doc <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib125" title="">125</a>]</cite> and HyDE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib126" title="">126</a>]</cite> use the original query to generate a pseudo document, which is later used as the query for retrieval. The pseudo document contains richer relevant information, which helps to retrieve more accurate results.
TOC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib127" title="">127</a>]</cite> leverages retrieved contents to decompose the ambiguous query into multiple clear sub-queries, which are sent to the generator and aggregated to produce the final result.
For complex or ambiguous queries, RQ-RAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib128" title="">128</a>]</cite> breaks them down into clear subqueries for fine-grained retrieval and synthesizes the responses to deliver a cohesive answer to the original query.
Tayal et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib129" title="">129</a>]</cite> refined the initial query using dynamic few-shot examples and context retrieval, enhancing the generator’s grasp of user intent.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS1.p4">
<p class="ltx_p" id="S3.SS2.SSS1.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p4.1.1">Data Augmentation:</span>
Data augmentation improves data before retrieval, including techniques such as removing irrelevant information, eliminating ambiguity, updating outdated documents, synthesize new data, etc.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p5">
<p class="ltx_p" id="S3.SS2.SSS1.p5.1">Make-An-Audio <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib44" title="">44</a>]</cite> uses captioning and audio-text retrieval to generate captions for language-free audio to mitigate data sparsity, and adds random concept audio to improve the original audio.
LESS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib130" title="">130</a>]</cite> optimizes dataset selection for downstream tasks by analyzing gradient information, aiming to enhance model performance in response to instructional prompts.
ReACC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib91" title="">91</a>]</cite> employs data augmentation (including renaming and dead code insertion) to pre-train the code retrieval model.
Telco-RAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib131" title="">131</a>]</cite> enhances the retrieve accuracy by appling a “Vocabulary for 3GPP Specifications”, and match them to user queries with a router module.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS2.SSS2.5.1.1">III-B</span>2 </span><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.6.2">Retriever Enhancement</span>
</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">In RAG systems, the quality of retrieved content determines the information fed into the generators. Lower content quality increases the risk of model hallucinations or other degradation. In this section, we introduce efficient ways to enhance retrieval effectiveness.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.p2">
<p class="ltx_p" id="S3.SS2.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.p2.1.1">Recursive Retrieval:</span>
Recursive retrieval is to perform multiple searches to retrieve richer and higher-quality contents.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p3">
<p class="ltx_p" id="S3.SS2.SSS2.p3.1">ReACT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib132" title="">132</a>]</cite> uses Chain-of-Thought (CoT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib133" title="">133</a>]</cite> to break queries down for recursive retrieval and provide richer information.
RATP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib134" title="">134</a>]</cite> uses the Monte-Carlo Tree Search for simulations to select optimal retrieval content, which is then templated and forwarded to the generator for output.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.p4">
<p class="ltx_p" id="S3.SS2.SSS2.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.p4.1.1">Chunk Optimization:</span>
Chunk optimization refers to adjusting chunk size for improved retrieval results.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p5">
<p class="ltx_p" id="S3.SS2.SSS2.p5.1">LlamaIndex <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib135" title="">135</a>]</cite> incorporates a series of chunk optimization methods, one of which operates on a ‘small to big’ principle. The core concept here is to pinpoint finer-grained content but return richer information.
For instance, Sentence-window retrieval fetches small text chunks and returns a window of relevant sentences surrounding the retrieved segment.
In auto-merge retrieval, documents are arranged in a tree structure. The process retrieves the parent node, which encapsulates the content of its child nodes, by fetching the child node first.
To address the lack of contextual information, RAPTOR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib136" title="">136</a>]</cite> employs recursive embedding, clustering, and summarization of text chunks until further clustering becomes infeasible, thereby constructing a multi-level tree structure.
Prompt-RAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib137" title="">137</a>]</cite> enhances retrieval accuracy by pre-generating a table of contents, enabling the model to autonomously select relevant chapters based on the query.
Raina et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib138" title="">138</a>]</cite> break text chunks into finer atomic statements to achieve higher recall and improved results.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.p6">
<p class="ltx_p" id="S3.SS2.SSS2.p6.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.p6.1.1">Retriever Finetuning:</span>
The retriever, central to the RAG system, relies on a proficient embedding model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib139" title="">139</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib140" title="">140</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib141" title="">141</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib142" title="">142</a>]</cite> to represent related content and feed the generator, enhancing system performance.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p7">
<p class="ltx_p" id="S3.SS2.SSS2.p7.1">Additionally, embedding models with strong expressive power can be fine-tuned with domain-specific or task-related data to boost performance in targeted areas.
REPLUG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib86" title="">86</a>]</cite> treats LM as a black box and update the retriever model based on the final results. APICoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib88" title="">88</a>]</cite> finetunes the retriever with python files and api names, signature, description.
EDITSUM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib98" title="">98</a>]</cite> finetunes the retriever to decrease the jaccard distance between summaries after retrieval.
SYNCHROMESH <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib81" title="">81</a>]</cite> adds tree distance os ASTs in the loss and uses Target Similarity Tuning (TST) to finetune the retriever.
R-ConvED <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib48" title="">48</a>]</cite> finetunes the retriever with the same data as generator.
Kulkarni et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib143" title="">143</a>]</cite> applied infoNCE loss to finetune the retriever.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.p8">
<p class="ltx_p" id="S3.SS2.SSS2.p8.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.p8.1.1">Hybrid Retrieval:</span>
Hybrid retrieve denotes the concurrent employment of a diverse array of retrieval methodologies or the extraction of information from multiple distinct sources.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p9">
<p class="ltx_p" id="S3.SS2.SSS2.p9.1">RAP-Gen <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib144" title="">144</a>]</cite>, BlendedRAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib145" title="">145</a>]</cite>and ReACC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib91" title="">91</a>]</cite> use both dense retriever and sparse retriever to improve the quality of retrieval. Rencos <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib80" title="">80</a>]</cite>
uses sparse retriever to retrieve similar code snippets on syntactic-level and uses dense retriever to retrieve similar code snippets on semantic-level.
BASHEXPLAINER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib99" title="">99</a>]</cite> first uses dense retriever to capture semantic information and then uses sparse retriever to acquire lexical information.
RetDream <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib50" title="">50</a>]</cite> first retrieves with text and then retrieves with the image embedding.
CRAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib146" title="">146</a>]</cite> features a retrieval evaluator that gauges document relevance to queries, prompting three retrieval responses based on confidence: direct use of results for Knowledge Refinement if accurate, Web Search if incorrect, and a hybrid approach for ambiguous cases.
Huang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib147" title="">147</a>]</cite> improved question-answering by introducing DKS (Dense Knowledge Similarity) and RAC (Retriever as Answer Classifier) in the retrieval phase, evaluating answer relevance and knowledge applicability.
UniMS-RAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib148" title="">148</a>]</cite> introduces a novel kind of token, termed as the “acting token”, which determines the source from which to retrieve information.
Koley et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib149" title="">149</a>]</cite> enhance image retrieval by integrating sketch and text for fine-grained retrieval, yielding improved results.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.p10">
<p class="ltx_p" id="S3.SS2.SSS2.p10.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.p10.1.1">Re-ranking:</span>
The Rerank technique refers to reordering the retrieved content in order to achieve greater diversity and better results.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p11">
<p class="ltx_p" id="S3.SS2.SSS2.p11.1">Re2G <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib150" title="">150</a>]</cite> applies a re-ranker <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib151" title="">151</a>]</cite> model after the traditional retriever to reduce the impact of information loss caused by compressing text into vectors.
AceCoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib152" title="">152</a>]</cite> reranks the retrieved programs with a selector to reduce redundant programs and obtain diverse retrieved programs.
XRICL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib153" title="">153</a>]</cite> uses a distillation-based exemplar reranker after retrieval.
Rangan <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib154" title="">154</a>]</cite> employs the Quantized Influence Measure, assessing statistical biases between a query and a reference to evaluate the similarity of data subsets and rerank retrieval results.
UDAPDR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib155" title="">155</a>]</cite> uses LLMs to cost-effectively generate synthetic queries that train domain-specific rerankers, which then apply multi-teacher knowledge distillation to develop a cohesive retriever.
LLM-R <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib156" title="">156</a>]</cite> refines its retriever iteratively by employing a static LLM for document ranking and reward model training, complemented by knowledge distillation. Each training cycle incrementally improves the retriever, enabling progressive optimization.
Finardi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib157" title="">157</a>]</cite> integrated reciprocal rank into the retrieval process for enhanced text chunk relevance, and utilized monoT5 as a reranker to optimize the result quality.
Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib158" title="">158</a>]</cite> integrate a reranking module into their end-to-end RAG system, enhancing the retrieval quality and factual accuracy of LLMs.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.p12">
<p class="ltx_p" id="S3.SS2.SSS2.p12.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.p12.1.1">Retrieval Transformation:</span>
Retrieval Transformation involves rephrasing retrieved content to better activate the generator’s potential, resulting in improved output.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p13">
<p class="ltx_p" id="S3.SS2.SSS2.p13.1">FILCO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib159" title="">159</a>]</cite> efficiently purges extraneous material from retrieved text, isolating only the pertinent supporting content to streamline the generator’s task and facilitate accurate answer prediction.
FiD-Light <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib160" title="">160</a>]</cite> initially employs an encoder to convert the retrieved content into a vector, which it then compresses, resulting in a substantial reduction of latency time.
RRR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib161" title="">161</a>]</cite> integrates the current query with the top-k document in each round through a template, and subsequently restructures it via a pre-trained LLMs (GPT-3.5-Turbo etc.).</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.p14">
<p class="ltx_p" id="S3.SS2.SSS2.p14.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.p14.1.1">Others:</span>
In addition to the above optimization methods, there are also some other optimization methods for the retrieve process.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p15">
<p class="ltx_p" id="S3.SS2.SSS2.p15.1">For example, meta-data filtering  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib162" title="">162</a>]</cite> is a method to help processing retrieved documents which uses metadata (such as time, purpose, etc.) to filter the retrieved documents for better results. GENREAD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib163" title="">163</a>]</cite> and GRG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib164" title="">164</a>]</cite> introduce a novel approach where the retrieval process is supplanted or improved by prompting a LLM to generate documents in response to a given question. Multi-Head-RAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib165" title="">165</a>]</cite> employs multiple embedding models to project the same text chunk into various vector spaces and utilizes a multi-head attention layer to capture different informational aspects, thereby increasing the accuracy of the retrieval process.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS2.SSS3.5.1.1">III-B</span>3 </span><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS3.6.2">Generator Enhancement</span>
</h4>
<div class="ltx_para" id="S3.SS2.SSS3.p1">
<p class="ltx_p" id="S3.SS2.SSS3.p1.1">In RAG systems, the quality of the generator often determines the quality of the final output results. Therefore, the ability of the generator determines the upper limit of the entire RAG system’s effectiveness.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS3.p2">
<p class="ltx_p" id="S3.SS2.SSS3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS3.p2.1.1">Prompt Engineering:</span>
Technologies in prompt engineering <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib166" title="">166</a>]</cite> that focus on improving the quality of LLMs’ output, such as prompt compression, Stepback Prompt <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib167" title="">167</a>]</cite>, Active Prompt <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib168" title="">168</a>]</cite>, Chain of Thought Prompt <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib133" title="">133</a>]</cite>, etc., are all applicable to LLM generators in RAG systems.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS3.p3">
<p class="ltx_p" id="S3.SS2.SSS3.p3.1">LLMLingua <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib169" title="">169</a>]</cite> applies a small model to compresses the overall length of the query to accelerate model inference, relieving the negative impact of irrelevant information on the model and alleviating the phenomenon of “Lost in the Middle”<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib170" title="">170</a>]</cite>.
ReMoDiffuse <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib51" title="">51</a>]</cite> decomposes complex descriptions into anatomical text scripts by using ChatGPT.
ASAP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib171" title="">171</a>]</cite> incorporates exemplar tuples, consisting of input code, function definitions, analysis results, and corresponding comments, into prompts to yield better results.
CEDAR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib89" title="">89</a>]</cite> uses a designed prompt template to organize code demonstration, query, and natural language instructions into a prompt.
XRICL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib153" title="">153</a>]</cite> utilizes COT technology to add translation pairs as an intermediate step in cross linguistic semantic parsing and inference.
ACTIVERAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib172" title="">172</a>]</cite> employs the Cognition Nexus mechanism to calibrate the intrinsic cognition of LLMs and applies COT prompt in answer generation.
Make-An-Audio <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib44" title="">44</a>]</cite> is able to use other modalities as input which can provide much richer information for the following process.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS3.p4">
<p class="ltx_p" id="S3.SS2.SSS3.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS3.p4.1.1">Decoding Tuning:</span>
Decoding tuning involves enhancing generator control by fine-tuning hyperparameters for increased diversity and constraining the output vocabulary, among other adjustments.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS3.p5">
<p class="ltx_p" id="S3.SS2.SSS3.p5.1">InferFix <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib90" title="">90</a>]</cite> balances the diversity and quality of results by adjusting the temperature in decoder.
SYNCHROMESH <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib81" title="">81</a>]</cite> limits the output vocabulary of the decoder by implementing a completion engine to eliminate implementation errors.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS3.p6">
<p class="ltx_p" id="S3.SS2.SSS3.p6.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS3.p6.1.1">Generator Finetuning:</span>
The finetuning of the generator can enhance the model’s ability to have more precise domain knowledge or better fit with the retriever.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS3.p7">
<p class="ltx_p" id="S3.SS2.SSS3.p7.1">RETRO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib36" title="">36</a>]</cite> fixes the parameters of the retriever and uses the chunked cross attention mechanism in the generator to combine the content of the query and retriever.
APICoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib88" title="">88</a>]</cite> finetunes the generator CODEGEN-MONO 350M <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib173" title="">173</a>]</cite> with a shuffled new file combined with API information and code blocks.
CARE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib117" title="">117</a>]</cite> trains encoders with image, audio, and video-text pairs, then fine-tunes the decoder (generator) to simultaneously reduce caption and concept detection loss, while keeping the encoders and retriever fixed.
Animate-A-Story <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib174" title="">174</a>]</cite> optimizes the video generator with image data, and then finetunes a LoRA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib175" title="">175</a>]</cite> adapter to capture the appearance details of the given character.
RetDream <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib50" title="">50</a>]</cite> finetunes a LoRA adapter <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib175" title="">175</a>]</cite> with the rendered images.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS2.SSS4.5.1.1">III-B</span>4 </span><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS4.6.2">Result Enhancement</span>
</h4>
<div class="ltx_para" id="S3.SS2.SSS4.p1">
<p class="ltx_p" id="S3.SS2.SSS4.p1.1">In many scenarios, the result of RAG may not achieve the expected effect, and some techniques of Result Enhancement can help alleviate this problem.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS4.p2">
<p class="ltx_p" id="S3.SS2.SSS4.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS4.p2.1.1">Output Rewrite:</span>
Output Rewrite refers to rewriting the content generated by the generator in certain scenarios to meet the needs of downstream tasks.
SARGAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib176" title="">176</a>]</cite> refines outputs in code-related tasks by employing a special Transformer alongside Deletion, Placeholder, and Insertion Classifiers to better align with the real-world code context.
Ring <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib177" title="">177</a>]</cite> obtains diversity results by reranking candidates based on the average of per token log probabilities produced by the generator.
CBR-KBQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib54" title="">54</a>]</cite> revises the result by aligning generated relations with those presented in the local neighborhood of the query entity in knowledge graph.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS2.SSS5.5.1.1">III-B</span>5 </span><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS5.6.2">RAG Pipeline Enhancement</span>
</h4>
<div class="ltx_para" id="S3.SS2.SSS5.p1">
<p class="ltx_p" id="S3.SS2.SSS5.p1.1">RAG pipeline enhancement refers to optimizing the overall process of RAG in order to achieve better performance results.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS5.p2">
<p class="ltx_p" id="S3.SS2.SSS5.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS5.p2.1.1">Adaptive Retrieval:</span>
Some studies on RAG suggest that retrieval doesn’t always enhance the final results. Over-retrieval can lead to resource wastage and potential confusion when the model’s inherent parameterized knowledge suffices for answering relevant questions. Consequently, this chapter will delve into two methods for determining retrieval necessity: rule-based and model-based approaches.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS5.p3">
<p class="ltx_p" id="S3.SS2.SSS5.p3.1"><span class="ltx_text ltx_font_italic" id="S3.SS2.SSS5.p3.1.1">Rule-based:</span> FLARE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib178" title="">178</a>]</cite> actively decides whether and when to search through the probability in the generation process. Efficient-KNNLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib38" title="">38</a>]</cite> combines the generation probability of KNN-LM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib37" title="">37</a>]</cite> and NPM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib119" title="">119</a>]</cite> with a hyperparameter <math alttext="\lambda" class="ltx_Math" display="inline" id="S3.SS2.SSS5.p3.1.m1.1"><semantics id="S3.SS2.SSS5.p3.1.m1.1a"><mi id="S3.SS2.SSS5.p3.1.m1.1.1" xref="S3.SS2.SSS5.p3.1.m1.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS5.p3.1.m1.1b"><ci id="S3.SS2.SSS5.p3.1.m1.1.1.cmml" xref="S3.SS2.SSS5.p3.1.m1.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS5.p3.1.m1.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS5.p3.1.m1.1d">italic_λ</annotation></semantics></math> to determine the proportion of generation and retrieval. Mallen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib179" title="">179</a>]</cite> used statistical analysis on questions to enable direct answers for high-frequency ones and applied RAG for low-frequency ones.
Jiang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib180" title="">180</a>]</cite> evaluated model confidence based on Model Uncertainty, Input Uncertainty, and Input Statistics to guide retrieval decisions.
Kandpal et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib181" title="">181</a>]</cite> studied the correlation between the number of relevant documents and the model’s knowledge mastery to assess the need for retrieval.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS5.p4">
<p class="ltx_p" id="S3.SS2.SSS5.p4.1"><span class="ltx_text ltx_font_italic" id="S3.SS2.SSS5.p4.1.1">Model-based:</span>
Self-RAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib85" title="">85</a>]</cite> uses a trained generator to determine whether to perform a retrieval based on the retrieve token under different user querys.
Ren et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib182" title="">182</a>]</cite> used “Judgment Prompting” to determine whether LLMs can answer relevant questions and whether their answers are correct or not, thereby assisting in determining the necessity of a retrieval.
SKR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib183" title="">183</a>]</cite> uses the ability of LLMs themselves to judge in advance whether they can answer the question, and if they can answer, no retrieval is performed.
Rowen <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib184" title="">184</a>]</cite> translates a question into multiple languages and checks for answer consistency across these languages, using the results to determine the need for information retrieval.
AdaptiveRAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib185" title="">185</a>]</cite> dynamically decides whether to retrieve based on the query complexity by a classifier, which is a smaller LM.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS5.p5">
<p class="ltx_p" id="S3.SS2.SSS5.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS5.p5.1.1">Iterative RAG:</span>
Iterative RAG progressively refines results by repeatedly cycling through retrieval and generation phases, rather than a single round.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS5.p6">
<p class="ltx_p" id="S3.SS2.SSS5.p6.1">RepoCoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib186" title="">186</a>]</cite> uses an iterative retrieval-generation approach for code completion, refining queries with previously generated code to better utilize dispersed information and improve outcomes.
ITER-RETGEN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib187" title="">187</a>]</cite> iteratively enhances content quality by using the generator’s output to pinpoint knowledge gaps, retrieving necessary information, and informing future generation cycles.
SelfMemory <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib188" title="">188</a>]</cite> utilizes a retrieval-augmented generator iteratively to form an expansive memory pool, from which a memory selector picks an output to inform the next generation cycle.
RAT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib189" title="">189</a>]</cite> initially generates content by an LLM with a zero-shot CoT prompt, then revises each thought step by retrieving knowledge from external knowledge base.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_bold" id="S4.1.1">Applications</span>
</h2>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.189.1.1" style="font-size:90%;">TABLE I</span>: </span><span class="ltx_text" id="S4.T1.190.2" style="font-size:90%;">Taxonomy of RAG applications across various modalities.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.T1.48"><span class="ltx_text ltx_inline-block" id="S4.T1.48.48" style="font-size:90%;width:433.6pt;">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T1.48.48.48.48" style="width:681.8pt;height:96.4pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S4.T1.48.48.48.48.48"><span class="ltx_text" id="S4.T1.48.48.48.48.48.48">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.48.48.48.48.48.48.48">
<span class="ltx_tr" id="S4.T1.48.48.48.48.48.48.48.49">
<span class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t ltx_colspan ltx_colspan_5" id="S4.T1.48.48.48.48.48.48.48.49.1" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.48.48.48.48.48.48.48.49.1.1">RAG for Text</span></span></span>
<span class="ltx_tr" id="S4.T1.48.48.48.48.48.48.48.50">
<span class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.48.48.48.48.48.48.48.50.1" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.48.48.48.48.48.48.48.50.1.1">Question Answering</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.48.48.48.48.48.48.48.50.2" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.48.48.48.48.48.48.48.50.2.1">Human-Machine Conversation</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.48.48.48.48.48.48.48.50.3" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.48.48.48.48.48.48.48.50.3.1">Neural Machine Translation</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.48.48.48.48.48.48.48.50.4" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.48.48.48.48.48.48.48.50.4.1">Summarization</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.48.48.48.48.48.48.48.50.5" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.48.48.48.48.48.48.48.50.5.1">Others</span></span></span>
<span class="ltx_tr" id="S4.T1.48.48.48.48.48.48.48.48">
<span class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.14.14.14.14.14.14.14.14.14" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.14.14.14.14.14.14.14.14.14.14">
<span class="ltx_tr" id="S4.T1.4.4.4.4.4.4.4.4.4.4.4">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.4.4.4.4.4.4.4.4.4.4.4.4" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span class="ltx_text" id="S4.T1.2.2.2.2.2.2.2.2.2.2.2.2.2" style="background-color:#F9E5E5;">REALM<sup class="ltx_sup" id="S4.T1.2.2.2.2.2.2.2.2.2.2.2.2.2.3">‡</sup><sup class="ltx_sup" id="S4.T1.2.2.2.2.2.2.2.2.2.2.2.2.2.4">§</sup></span> <span class="ltx_text" id="S4.T1.3.3.3.3.3.3.3.3.3.3.3.3.3" style="background-color:#F9E5E5;"> TKEGEN<sup class="ltx_sup" id="S4.T1.3.3.3.3.3.3.3.3.3.3.3.3.3.2">§</sup></span> <span class="ltx_text" id="S4.T1.4.4.4.4.4.4.4.4.4.4.4.4.4" style="background-color:#FFE1BB;"> RIAG<sup class="ltx_sup" id="S4.T1.4.4.4.4.4.4.4.4.4.4.4.4.4.2">‡</sup></span></span></span>
<span class="ltx_tr" id="S4.T1.9.9.9.9.9.9.9.9.9.9.9">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.9.9.9.9.9.9.9.9.9.9.9.5" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span class="ltx_text" id="S4.T1.6.6.6.6.6.6.6.6.6.6.6.2.2" style="background-color:#D1E9B8;">Fid<sup class="ltx_sup" id="S4.T1.6.6.6.6.6.6.6.6.6.6.6.2.2.3">‡</sup><sup class="ltx_sup" id="S4.T1.6.6.6.6.6.6.6.6.6.6.6.2.2.4">§</sup></span> <span class="ltx_text" id="S4.T1.7.7.7.7.7.7.7.7.7.7.7.3.3" style="background-color:#D1E9B8;"> RETRO<sup class="ltx_sup" id="S4.T1.7.7.7.7.7.7.7.7.7.7.7.3.3.2">§</sup></span> <span class="ltx_text" id="S4.T1.9.9.9.9.9.9.9.9.9.9.9.5.5" style="background-color:#DAF3F6;"> NPM<sup class="ltx_sup" id="S4.T1.9.9.9.9.9.9.9.9.9.9.9.5.5.3">‡</sup><sup class="ltx_sup" id="S4.T1.9.9.9.9.9.9.9.9.9.9.9.5.5.4">§</sup></span></span></span>
<span class="ltx_tr" id="S4.T1.14.14.14.14.14.14.14.14.14.14.14">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.14.14.14.14.14.14.14.14.14.14.14.5" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span class="ltx_text" id="S4.T1.11.11.11.11.11.11.11.11.11.11.11.2.2" style="background-color:#F9E5E5;">SKR<sup class="ltx_sup" id="S4.T1.11.11.11.11.11.11.11.11.11.11.11.2.2.3">§</sup><sup class="ltx_sup" id="S4.T1.11.11.11.11.11.11.11.11.11.11.11.2.2.4">¶</sup></span> <span class="ltx_text" id="S4.T1.13.13.13.13.13.13.13.13.13.13.13.4.4" style="background-color:#F9E5E5;"> Self-RAG<sup class="ltx_sup" id="S4.T1.13.13.13.13.13.13.13.13.13.13.13.4.4.3">§</sup><sup class="ltx_sup" id="S4.T1.13.13.13.13.13.13.13.13.13.13.13.4.4.4">¶</sup></span> <span class="ltx_text" id="S4.T1.14.14.14.14.14.14.14.14.14.14.14.5.5" style="background-color:#F9E5E5;"> TOG<sup class="ltx_sup" id="S4.T1.14.14.14.14.14.14.14.14.14.14.14.5.5.2">‡</sup></span></span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.26.26.26.26.26.26.26.26.26" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.26.26.26.26.26.26.26.26.26.12">
<span class="ltx_tr" id="S4.T1.18.18.18.18.18.18.18.18.18.4.4">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.18.18.18.18.18.18.18.18.18.4.4.4" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span class="ltx_text" id="S4.T1.16.16.16.16.16.16.16.16.16.2.2.2.2" style="background-color:#D1E9B8;">ConceptFlow<sup class="ltx_sup" id="S4.T1.16.16.16.16.16.16.16.16.16.2.2.2.2.3">‡</sup><sup class="ltx_sup" id="S4.T1.16.16.16.16.16.16.16.16.16.2.2.2.2.4">§</sup></span> <span class="ltx_text" id="S4.T1.18.18.18.18.18.18.18.18.18.4.4.4.4" style="background-color:#D1E9B8;"> Skeleton-to-Response<sup class="ltx_sup" id="S4.T1.18.18.18.18.18.18.18.18.18.4.4.4.4.3">‡</sup><sup class="ltx_sup" id="S4.T1.18.18.18.18.18.18.18.18.18.4.4.4.4.4">§</sup></span></span></span>
<span class="ltx_tr" id="S4.T1.22.22.22.22.22.22.22.22.22.8.8">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.22.22.22.22.22.22.22.22.22.8.8.4" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span class="ltx_text" id="S4.T1.20.20.20.20.20.20.20.20.20.6.6.2.2" style="background-color:#F9E5E5;">CREA-ICL<sup class="ltx_sup" id="S4.T1.20.20.20.20.20.20.20.20.20.6.6.2.2.3">†</sup><sup class="ltx_sup" id="S4.T1.20.20.20.20.20.20.20.20.20.6.6.2.2.4">‡</sup></span><span class="ltx_text" id="S4.T1.22.22.22.22.22.22.22.22.22.8.8.4.4" style="background-color:#D1E9B8;"> Internet-Augmented-DG<sup class="ltx_sup" id="S4.T1.22.22.22.22.22.22.22.22.22.8.8.4.4.3">‡</sup><sup class="ltx_sup" id="S4.T1.22.22.22.22.22.22.22.22.22.8.8.4.4.4">§</sup></span></span></span>
<span class="ltx_tr" id="S4.T1.26.26.26.26.26.26.26.26.26.12.12">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.26.26.26.26.26.26.26.26.26.12.12.4" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span class="ltx_text" id="S4.T1.24.24.24.24.24.24.24.24.24.10.10.2.2" style="background-color:#F9E5E5;">BlenderBot3<sup class="ltx_sup" id="S4.T1.24.24.24.24.24.24.24.24.24.10.10.2.2.3">‡</sup><sup class="ltx_sup" id="S4.T1.24.24.24.24.24.24.24.24.24.10.10.2.2.4">§</sup></span> <span class="ltx_text" id="S4.T1.26.26.26.26.26.26.26.26.26.12.12.4.4" style="background-color:#F9E5E5;"> CEG<sup class="ltx_sup" id="S4.T1.26.26.26.26.26.26.26.26.26.12.12.4.4.3">‡</sup><sup class="ltx_sup" id="S4.T1.26.26.26.26.26.26.26.26.26.12.12.4.4.4">∥</sup></span></span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.34.34.34.34.34.34.34.34.34" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.34.34.34.34.34.34.34.34.34.8">
<span class="ltx_tr" id="S4.T1.29.29.29.29.29.29.29.29.29.3.3">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.29.29.29.29.29.29.29.29.29.3.3.3" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span class="ltx_text" id="S4.T1.29.29.29.29.29.29.29.29.29.3.3.3.3" style="background-color:#D1E9B8;">NMT-with-Monolingual-TM<sup class="ltx_sup" id="S4.T1.29.29.29.29.29.29.29.29.29.3.3.3.3.4">†</sup><sup class="ltx_sup" id="S4.T1.29.29.29.29.29.29.29.29.29.3.3.3.3.5">‡</sup><sup class="ltx_sup" id="S4.T1.29.29.29.29.29.29.29.29.29.3.3.3.3.6">§</sup></span></span></span>
<span class="ltx_tr" id="S4.T1.34.34.34.34.34.34.34.34.34.8.8">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.34.34.34.34.34.34.34.34.34.8.8.5" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span class="ltx_text" id="S4.T1.31.31.31.31.31.31.31.31.31.5.5.2.2" style="background-color:#DAF3F6;">KNN-MT<sup class="ltx_sup" id="S4.T1.31.31.31.31.31.31.31.31.31.5.5.2.2.3">‡</sup><sup class="ltx_sup" id="S4.T1.31.31.31.31.31.31.31.31.31.5.5.2.2.4">§</sup></span> <span class="ltx_text" id="S4.T1.32.32.32.32.32.32.32.32.32.6.6.3.3" style="background-color:#FFE1BB;"> COG<sup class="ltx_sup" id="S4.T1.32.32.32.32.32.32.32.32.32.6.6.3.3.2">‡</sup></span> <span class="ltx_text" id="S4.T1.34.34.34.34.34.34.34.34.34.8.8.5.5" style="background-color:#DAF3F6;"> TRIME<sup class="ltx_sup" id="S4.T1.34.34.34.34.34.34.34.34.34.8.8.5.5.3">‡</sup><sup class="ltx_sup" id="S4.T1.34.34.34.34.34.34.34.34.34.8.8.5.5.4">§</sup></span></span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.40.40.40.40.40.40.40.40.40" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.40.40.40.40.40.40.40.40.40.6">
<span class="ltx_tr" id="S4.T1.37.37.37.37.37.37.37.37.37.3.3">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.37.37.37.37.37.37.37.37.37.3.3.3" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span class="ltx_text" id="S4.T1.36.36.36.36.36.36.36.36.36.2.2.2.2" style="background-color:#F9E5E5;">RAMKG<sup class="ltx_sup" id="S4.T1.36.36.36.36.36.36.36.36.36.2.2.2.2.3">‡</sup><sup class="ltx_sup" id="S4.T1.36.36.36.36.36.36.36.36.36.2.2.2.2.4">§</sup></span> <span class="ltx_text" id="S4.T1.37.37.37.37.37.37.37.37.37.3.3.3.3" style="background-color:#D1E9B8;"> Unlimiformer<sup class="ltx_sup" id="S4.T1.37.37.37.37.37.37.37.37.37.3.3.3.3.2">§</sup></span></span></span>
<span class="ltx_tr" id="S4.T1.40.40.40.40.40.40.40.40.40.6.6">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.40.40.40.40.40.40.40.40.40.6.6.3" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span class="ltx_text" id="S4.T1.38.38.38.38.38.38.38.38.38.4.4.1.1" style="background-color:#F9E5E5;">RPRR<sup class="ltx_sup" id="S4.T1.38.38.38.38.38.38.38.38.38.4.4.1.1.2">‡</sup></span> <span class="ltx_text" id="S4.T1.40.40.40.40.40.40.40.40.40.6.6.3.3" style="background-color:#F9E5E5;"> RIGHT<sup class="ltx_sup" id="S4.T1.40.40.40.40.40.40.40.40.40.6.6.3.3.3">‡</sup><sup class="ltx_sup" id="S4.T1.40.40.40.40.40.40.40.40.40.6.6.3.3.4">§</sup></span></span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.48.48.48.48.48.48.48.48.48" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.48.48.48.48.48.48.48.48.48.8">
<span class="ltx_tr" id="S4.T1.44.44.44.44.44.44.44.44.44.4.4">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.44.44.44.44.44.44.44.44.44.4.4.4" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span class="ltx_text" id="S4.T1.42.42.42.42.42.42.42.42.42.2.2.2.2" style="background-color:#F9E5E5;">CONCRETE<sup class="ltx_sup" id="S4.T1.42.42.42.42.42.42.42.42.42.2.2.2.2.3">‡</sup><sup class="ltx_sup" id="S4.T1.42.42.42.42.42.42.42.42.42.2.2.2.2.4">§</sup></span> <span class="ltx_text" id="S4.T1.44.44.44.44.44.44.44.44.44.4.4.4.4" style="background-color:#F9E5E5;"> Atlas<sup class="ltx_sup" id="S4.T1.44.44.44.44.44.44.44.44.44.4.4.4.4.3">‡</sup><sup class="ltx_sup" id="S4.T1.44.44.44.44.44.44.44.44.44.4.4.4.4.4">§</sup></span></span></span>
<span class="ltx_tr" id="S4.T1.48.48.48.48.48.48.48.48.48.8.8">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.48.48.48.48.48.48.48.48.48.8.8.4" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span class="ltx_text" id="S4.T1.46.46.46.46.46.46.46.46.46.6.6.2.2" style="background-color:#D1E9B8;">KG-BART<sup class="ltx_sup" id="S4.T1.46.46.46.46.46.46.46.46.46.6.6.2.2.3">‡</sup><sup class="ltx_sup" id="S4.T1.46.46.46.46.46.46.46.46.46.6.6.2.2.4">§</sup></span> <span class="ltx_text" id="S4.T1.48.48.48.48.48.48.48.48.48.8.8.4.4" style="background-color:#F9E5E5;"> R-GQA<sup class="ltx_sup" id="S4.T1.48.48.48.48.48.48.48.48.48.8.8.4.4.3">‡</sup><sup class="ltx_sup" id="S4.T1.48.48.48.48.48.48.48.48.48.8.8.4.4.4">§</sup></span></span></span>
</span></span></span>
</span></span></span>
</span></span></span><span class="ltx_text" id="S4.T1.48.49" style="font-size:90%;"></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.T1.95"><span class="ltx_text ltx_inline-block" id="S4.T1.95.47" style="font-size:90%;width:433.6pt;">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T1.95.47.47.47" style="width:985.3pt;height:122.9pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S4.T1.95.47.47.47.47"><span class="ltx_text" id="S4.T1.95.47.47.47.47.47">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.95.47.47.47.47.47.47">
<span class="ltx_tr" id="S4.T1.95.47.47.47.47.47.47.48">
<span class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t ltx_colspan ltx_colspan_6" id="S4.T1.95.47.47.47.47.47.47.48.1" style="padding-top:3pt;padding-bottom:3pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.95.47.47.47.47.47.47.48.1.1" style="font-size:133%;">RAG for Code</span></span></span>
<span class="ltx_tr" id="S4.T1.95.47.47.47.47.47.47.49">
<span class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.95.47.47.47.47.47.47.49.1" style="padding-top:3pt;padding-bottom:3pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.95.47.47.47.47.47.47.49.1.1" style="font-size:133%;">Code Generation</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.95.47.47.47.47.47.47.49.2" style="padding-top:3pt;padding-bottom:3pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.95.47.47.47.47.47.47.49.2.1" style="font-size:133%;">Code Summary</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.95.47.47.47.47.47.47.49.3" style="padding-top:3pt;padding-bottom:3pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.95.47.47.47.47.47.47.49.3.1" style="font-size:133%;"> Code Completion</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.95.47.47.47.47.47.47.49.4" style="padding-top:3pt;padding-bottom:3pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.95.47.47.47.47.47.47.49.4.1" style="font-size:133%;">Automatic Program Repair</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.95.47.47.47.47.47.47.49.5" style="padding-top:3pt;padding-bottom:3pt;"><span class="ltx_text" id="S4.T1.95.47.47.47.47.47.47.49.5.1"></span><span class="ltx_text" id="S4.T1.95.47.47.47.47.47.47.49.5.2" style="font-size:133%;"> </span><span class="ltx_text" id="S4.T1.95.47.47.47.47.47.47.49.5.3" style="font-size:133%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.95.47.47.47.47.47.47.49.5.3.1">
<span class="ltx_tr" id="S4.T1.95.47.47.47.47.47.47.49.5.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.95.47.47.47.47.47.47.49.5.3.1.1.1" style="padding-top:3pt;padding-bottom:3pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.95.47.47.47.47.47.47.49.5.3.1.1.1.1">Text-to-SQL and</span></span></span>
<span class="ltx_tr" id="S4.T1.95.47.47.47.47.47.47.49.5.3.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.95.47.47.47.47.47.47.49.5.3.1.2.1" style="padding-top:3pt;padding-bottom:3pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.95.47.47.47.47.47.47.49.5.3.1.2.1.1">Code-based Semantic Parsing</span></span></span>
</span></span><span class="ltx_text" id="S4.T1.95.47.47.47.47.47.47.49.5.4"></span><span class="ltx_text" id="S4.T1.95.47.47.47.47.47.47.49.5.5" style="font-size:133%;"></span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.95.47.47.47.47.47.47.49.6" style="padding-top:3pt;padding-bottom:3pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.95.47.47.47.47.47.47.49.6.1" style="font-size:133%;"> Others</span></span></span>
<span class="ltx_tr" id="S4.T1.95.47.47.47.47.47.47.47">
<span class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.55.7.7.7.7.7.7.7.7" style="padding-top:3pt;padding-bottom:3pt;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.55.7.7.7.7.7.7.7.7.7">
<span class="ltx_tr" id="S4.T1.50.2.2.2.2.2.2.2.2.2.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.50.2.2.2.2.2.2.2.2.2.2.2" style="padding-top:3pt;padding-bottom:3pt;"><span class="ltx_text" id="S4.T1.49.1.1.1.1.1.1.1.1.1.1.1.1" style="font-size:133%;background-color:#F9E5E5;">SKCODER<sup class="ltx_sup" id="S4.T1.49.1.1.1.1.1.1.1.1.1.1.1.1.2">§</sup></span><span class="ltx_text" id="S4.T1.50.2.2.2.2.2.2.2.2.2.2.2.3" style="font-size:133%;"> </span><span class="ltx_text" id="S4.T1.50.2.2.2.2.2.2.2.2.2.2.2.2" style="font-size:133%;background-color:#F9E5E5;"> RRGCode<sup class="ltx_sup" id="S4.T1.50.2.2.2.2.2.2.2.2.2.2.2.2.2">‡</sup></span></span></span>
<span class="ltx_tr" id="S4.T1.53.5.5.5.5.5.5.5.5.5.5">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.53.5.5.5.5.5.5.5.5.5.5.3" style="padding-top:3pt;padding-bottom:3pt;"><span class="ltx_text" id="S4.T1.52.4.4.4.4.4.4.4.4.4.4.2.2" style="font-size:133%;background-color:#F9E5E5;">ARKS<sup class="ltx_sup" id="S4.T1.52.4.4.4.4.4.4.4.4.4.4.2.2.3">†</sup><sup class="ltx_sup" id="S4.T1.52.4.4.4.4.4.4.4.4.4.4.2.2.4">¶</sup></span><span class="ltx_text" id="S4.T1.53.5.5.5.5.5.5.5.5.5.5.3.4" style="font-size:133%;"> </span><span class="ltx_text" id="S4.T1.53.5.5.5.5.5.5.5.5.5.5.3.3" style="font-size:133%;background-color:#DAF3F6;"> KNN-TRANX<sup class="ltx_sup" id="S4.T1.53.5.5.5.5.5.5.5.5.5.5.3.3.2">∥</sup></span></span></span>
<span class="ltx_tr" id="S4.T1.55.7.7.7.7.7.7.7.7.7.7">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.55.7.7.7.7.7.7.7.7.7.7.2" style="padding-top:3pt;padding-bottom:3pt;"><span class="ltx_text" id="S4.T1.55.7.7.7.7.7.7.7.7.7.7.2.3" style="font-size:133%;background-color:#DAF3F6;">RECODE</span><span class="ltx_text" id="S4.T1.55.7.7.7.7.7.7.7.7.7.7.2.4" style="font-size:133%;"> </span><span class="ltx_text" id="S4.T1.55.7.7.7.7.7.7.7.7.7.7.2.2" style="font-size:133%;background-color:#FFE1BB;"> Toolcoder<sup class="ltx_sup" id="S4.T1.55.7.7.7.7.7.7.7.7.7.7.2.2.3">§</sup><sup class="ltx_sup" id="S4.T1.55.7.7.7.7.7.7.7.7.7.7.2.2.4">∥</sup></span></span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.62.14.14.14.14.14.14.14.14" style="padding-top:3pt;padding-bottom:3pt;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.62.14.14.14.14.14.14.14.14.7">
<span class="ltx_tr" id="S4.T1.57.9.9.9.9.9.9.9.9.2.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.57.9.9.9.9.9.9.9.9.2.2.2" style="padding-top:3pt;padding-bottom:3pt;"><span class="ltx_text" id="S4.T1.56.8.8.8.8.8.8.8.8.1.1.1.1" style="font-size:133%;background-color:#D1E9B8;">RACE<sup class="ltx_sup" id="S4.T1.56.8.8.8.8.8.8.8.8.1.1.1.1.2">†</sup></span><span class="ltx_text" id="S4.T1.57.9.9.9.9.9.9.9.9.2.2.2.3" style="font-size:133%;"> </span><span class="ltx_text" id="S4.T1.57.9.9.9.9.9.9.9.9.2.2.2.2" style="font-size:133%;background-color:#D1E9B8;"> BASHEXPLAINER<sup class="ltx_sup" id="S4.T1.57.9.9.9.9.9.9.9.9.2.2.2.2.2">‡</sup></span></span></span>
<span class="ltx_tr" id="S4.T1.59.11.11.11.11.11.11.11.11.4.4">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.59.11.11.11.11.11.11.11.11.4.4.2" style="padding-top:3pt;padding-bottom:3pt;"><span class="ltx_text" id="S4.T1.58.10.10.10.10.10.10.10.10.3.3.1.1" style="font-size:133%;background-color:#D1E9B8;">READSUM<sup class="ltx_sup" id="S4.T1.58.10.10.10.10.10.10.10.10.3.3.1.1.2">∥</sup></span><span class="ltx_text" id="S4.T1.59.11.11.11.11.11.11.11.11.4.4.2.3" style="font-size:133%;"> </span><span class="ltx_text" id="S4.T1.59.11.11.11.11.11.11.11.11.4.4.2.2" style="font-size:133%;background-color:#DAF3F6;"> Rencos<sup class="ltx_sup" id="S4.T1.59.11.11.11.11.11.11.11.11.4.4.2.2.2">‡</sup></span></span></span>
<span class="ltx_tr" id="S4.T1.62.14.14.14.14.14.14.14.14.7.7">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.62.14.14.14.14.14.14.14.14.7.7.3" style="padding-top:3pt;padding-bottom:3pt;"><span class="ltx_text" id="S4.T1.60.12.12.12.12.12.12.12.12.5.5.1.1" style="font-size:133%;background-color:#DAF3F6;">CoRec<sup class="ltx_sup" id="S4.T1.60.12.12.12.12.12.12.12.12.5.5.1.1.2">‡</sup></span><span class="ltx_text" id="S4.T1.62.14.14.14.14.14.14.14.14.7.7.3.3" style="font-size:133%;"> </span><span class="ltx_text" id="S4.T1.61.13.13.13.13.13.13.13.13.6.6.2.2" style="font-size:133%;background-color:#DAF3F6;">Tram<sup class="ltx_sup" id="S4.T1.61.13.13.13.13.13.13.13.13.6.6.2.2.2">§</sup></span><span class="ltx_text" id="S4.T1.62.14.14.14.14.14.14.14.14.7.7.3.4" style="font-size:133%;">
</span><svg class="ltx_picture" height="21.77" id="S4.T1.62.14.14.14.14.14.14.14.14.7.7.3.pic1" overflow="visible" version="1.1" width="88.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,21.77) matrix(1 0 0 -1 0 0) translate(44.27,0) translate(0,4.15)"><clippath id="pgfcp1"><path d="M -44.27 -4.15 h 88.54 v 21.77 h -88.54 Z"></path></clippath><g clip-path="url(#pgfcp1)"><g transform="matrix(1.0 0.0 0.0 1.0 0 6.73) matrix(0.0 1.0 -1.0 0.0 0 0) matrix(0.31352 0.0 0.0 1.27512 0 0)"><defs><lineargradient gradienttransform="rotate(90)" id="pgfsh2"><stop offset="0.0" stop-color="#DAF3F6"></stop><stop offset="0.24998" stop-color="#DAF3F6"></stop><stop offset="0.49996" stop-color="#D6EED7"></stop><stop offset="0.75002" stop-color="#D1E9B8"></stop><stop offset="1.0" stop-color="#D1E9B8"></stop></lineargradient></defs><g transform="translate(-69.44 -69.44)"><rect height="138.89" style="fill:url(#pgfsh2);stroke:none" width="138.89"></rect></g></g></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -41.5 0)"><foreignobject height="14.85" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="83"><span class="ltx_text" id="S4.T1.62.14.14.14.14.14.14.14.14.7.7.3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="font-size:133%;">EDITSUM<sup class="ltx_sup" id="S4.T1.62.14.14.14.14.14.14.14.14.7.7.3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">‡</sup></span></foreignobject></g></g></svg></span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.71.23.23.23.23.23.23.23.23" style="padding-top:3pt;padding-bottom:3pt;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.71.23.23.23.23.23.23.23.23.9">
<span class="ltx_tr" id="S4.T1.67.19.19.19.19.19.19.19.19.5.5">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.67.19.19.19.19.19.19.19.19.5.5.5" style="padding-top:3pt;padding-bottom:3pt;"><span class="ltx_text" id="S4.T1.64.16.16.16.16.16.16.16.16.2.2.2.2" style="font-size:133%;background-color:#F9E5E5;">ReACC<sup class="ltx_sup" id="S4.T1.64.16.16.16.16.16.16.16.16.2.2.2.2.3">†</sup><sup class="ltx_sup" id="S4.T1.64.16.16.16.16.16.16.16.16.2.2.2.2.4">‡</sup></span><span class="ltx_text" id="S4.T1.67.19.19.19.19.19.19.19.19.5.5.5.6" style="font-size:133%;"> </span><span class="ltx_text" id="S4.T1.67.19.19.19.19.19.19.19.19.5.5.5.5" style="font-size:133%;background-color:#F9E5E5;"> RepoCoder<sup class="ltx_sup" id="S4.T1.67.19.19.19.19.19.19.19.19.5.5.5.5.4">†</sup><sup class="ltx_sup" id="S4.T1.67.19.19.19.19.19.19.19.19.5.5.5.5.5">§</sup><sup class="ltx_sup" id="S4.T1.67.19.19.19.19.19.19.19.19.5.5.5.5.6">¶</sup></span></span></span>
<span class="ltx_tr" id="S4.T1.69.21.21.21.21.21.21.21.21.7.7">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.69.21.21.21.21.21.21.21.21.7.7.2" style="padding-top:3pt;padding-bottom:3pt;"><span class="ltx_text" id="S4.T1.68.20.20.20.20.20.20.20.20.6.6.1.1" style="font-size:133%;background-color:#F9E5E5;">De-Hallucinator<sup class="ltx_sup" id="S4.T1.68.20.20.20.20.20.20.20.20.6.6.1.1.2">¶</sup></span><span class="ltx_text" id="S4.T1.69.21.21.21.21.21.21.21.21.7.7.2.3" style="font-size:133%;"> </span><span class="ltx_text" id="S4.T1.69.21.21.21.21.21.21.21.21.7.7.2.2" style="font-size:133%;background-color:#F9E5E5;">REPOFUSE<sup class="ltx_sup" id="S4.T1.69.21.21.21.21.21.21.21.21.7.7.2.2.2">§</sup></span></span></span>
<span class="ltx_tr" id="S4.T1.71.23.23.23.23.23.23.23.23.9.9">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.71.23.23.23.23.23.23.23.23.9.9.2" style="padding-top:3pt;padding-bottom:3pt;"><span class="ltx_text" id="S4.T1.70.22.22.22.22.22.22.22.22.8.8.1.1" style="font-size:133%;background-color:#D1E9B8;">RepoFusion<sup class="ltx_sup" id="S4.T1.70.22.22.22.22.22.22.22.22.8.8.1.1.2">§</sup></span><span class="ltx_text" id="S4.T1.71.23.23.23.23.23.23.23.23.9.9.2.3" style="font-size:133%;"> </span><span class="ltx_text" id="S4.T1.71.23.23.23.23.23.23.23.23.9.9.2.2" style="font-size:133%;background-color:#D1E9B8;">EDITAS<sup class="ltx_sup" id="S4.T1.71.23.23.23.23.23.23.23.23.9.9.2.2.2">§</sup></span></span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.79.31.31.31.31.31.31.31.31" style="padding-top:3pt;padding-bottom:3pt;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.79.31.31.31.31.31.31.31.31.8">
<span class="ltx_tr" id="S4.T1.73.25.25.25.25.25.25.25.25.2.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.73.25.25.25.25.25.25.25.25.2.2.2" style="padding-top:3pt;padding-bottom:3pt;"><span class="ltx_text" id="S4.T1.72.24.24.24.24.24.24.24.24.1.1.1.1" style="font-size:133%;background-color:#F9E5E5;">RING<sup class="ltx_sup" id="S4.T1.72.24.24.24.24.24.24.24.24.1.1.1.1.2">∥</sup></span><span class="ltx_text" id="S4.T1.73.25.25.25.25.25.25.25.25.2.2.2.3" style="font-size:133%;"> </span><span class="ltx_text" id="S4.T1.73.25.25.25.25.25.25.25.25.2.2.2.2" style="font-size:133%;background-color:#F9E5E5;"> CEDAR<sup class="ltx_sup" id="S4.T1.73.25.25.25.25.25.25.25.25.2.2.2.2.2">§</sup></span></span></span>
<span class="ltx_tr" id="S4.T1.76.28.28.28.28.28.28.28.28.5.5">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.76.28.28.28.28.28.28.28.28.5.5.3" style="padding-top:3pt;padding-bottom:3pt;"><span class="ltx_text" id="S4.T1.75.27.27.27.27.27.27.27.27.4.4.2.2" style="font-size:133%;background-color:#F9E5E5;">RAP-Gen<sup class="ltx_sup" id="S4.T1.75.27.27.27.27.27.27.27.27.4.4.2.2.3">‡</sup><sup class="ltx_sup" id="S4.T1.75.27.27.27.27.27.27.27.27.4.4.2.2.4">§</sup></span><span class="ltx_text" id="S4.T1.76.28.28.28.28.28.28.28.28.5.5.3.4" style="font-size:133%;"> </span><span class="ltx_text" id="S4.T1.76.28.28.28.28.28.28.28.28.5.5.3.3" style="font-size:133%;background-color:#F9E5E5;"> InferFix<sup class="ltx_sup" id="S4.T1.76.28.28.28.28.28.28.28.28.5.5.3.3.2">§</sup></span></span></span>
<span class="ltx_tr" id="S4.T1.79.31.31.31.31.31.31.31.31.8.8">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.79.31.31.31.31.31.31.31.31.8.8.3" style="padding-top:3pt;padding-bottom:3pt;"><span class="ltx_text" id="S4.T1.77.29.29.29.29.29.29.29.29.6.6.1.1" style="font-size:133%;background-color:#F9E5E5;">SARGAM<sup class="ltx_sup" id="S4.T1.77.29.29.29.29.29.29.29.29.6.6.1.1.2">§</sup></span><span class="ltx_text" id="S4.T1.79.31.31.31.31.31.31.31.31.8.8.3.4" style="font-size:133%;"> </span><span class="ltx_text" id="S4.T1.79.31.31.31.31.31.31.31.31.8.8.3.3" style="font-size:133%;background-color:#F9E5E5;"> RTLFixer<sup class="ltx_sup" id="S4.T1.79.31.31.31.31.31.31.31.31.8.8.3.3.3">‡</sup><sup class="ltx_sup" id="S4.T1.79.31.31.31.31.31.31.31.31.8.8.3.3.4">§</sup></span></span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.89.41.41.41.41.41.41.41.41" style="padding-top:3pt;padding-bottom:3pt;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.89.41.41.41.41.41.41.41.41.10">
<span class="ltx_tr" id="S4.T1.83.35.35.35.35.35.35.35.35.4.4">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.83.35.35.35.35.35.35.35.35.4.4.4" style="padding-top:3pt;padding-bottom:3pt;"><span class="ltx_text" id="S4.T1.81.33.33.33.33.33.33.33.33.2.2.2.2" style="font-size:133%;background-color:#F9E5E5;">XRICL<sup class="ltx_sup" id="S4.T1.81.33.33.33.33.33.33.33.33.2.2.2.2.3">‡</sup><sup class="ltx_sup" id="S4.T1.81.33.33.33.33.33.33.33.33.2.2.2.2.4">§</sup></span><span class="ltx_text" id="S4.T1.83.35.35.35.35.35.35.35.35.4.4.4.5" style="font-size:133%;"> </span><span class="ltx_text" id="S4.T1.83.35.35.35.35.35.35.35.35.4.4.4.4" style="font-size:133%;background-color:#F9E5E5;"> SYNCHROMESH<sup class="ltx_sup" id="S4.T1.83.35.35.35.35.35.35.35.35.4.4.4.4.3">‡</sup><sup class="ltx_sup" id="S4.T1.83.35.35.35.35.35.35.35.35.4.4.4.4.4">§</sup></span></span></span>
<span class="ltx_tr" id="S4.T1.86.38.38.38.38.38.38.38.38.7.7">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.86.38.38.38.38.38.38.38.38.7.7.3" style="padding-top:3pt;padding-bottom:3pt;"><span class="ltx_text" id="S4.T1.84.36.36.36.36.36.36.36.36.5.5.1.1" style="font-size:133%;background-color:#F9E5E5;">RESDSQL<sup class="ltx_sup" id="S4.T1.84.36.36.36.36.36.36.36.36.5.5.1.1.2">§</sup></span><span class="ltx_text" id="S4.T1.86.38.38.38.38.38.38.38.38.7.7.3.4" style="font-size:133%;"> </span><span class="ltx_text" id="S4.T1.86.38.38.38.38.38.38.38.38.7.7.3.3" style="font-size:133%;background-color:#F9E5E5;"> REFSQL<sup class="ltx_sup" id="S4.T1.86.38.38.38.38.38.38.38.38.7.7.3.3.3">‡</sup><sup class="ltx_sup" id="S4.T1.86.38.38.38.38.38.38.38.38.7.7.3.3.4">§</sup></span></span></span>
<span class="ltx_tr" id="S4.T1.89.41.41.41.41.41.41.41.41.10.10">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.89.41.41.41.41.41.41.41.41.10.10.3" style="padding-top:3pt;padding-bottom:3pt;"><span class="ltx_text" id="S4.T1.87.39.39.39.39.39.39.39.39.8.8.1.1" style="font-size:133%;background-color:#F9E5E5;">CodeICL<sup class="ltx_sup" id="S4.T1.87.39.39.39.39.39.39.39.39.8.8.1.1.2">§</sup></span><span class="ltx_text" id="S4.T1.89.41.41.41.41.41.41.41.41.10.10.3.4" style="font-size:133%;"> </span><span class="ltx_text" id="S4.T1.89.41.41.41.41.41.41.41.41.10.10.3.3" style="font-size:133%;background-color:#F9E5E5;"> MURRE<sup class="ltx_sup" id="S4.T1.89.41.41.41.41.41.41.41.41.10.10.3.3.3">∥</sup><sup class="ltx_sup" id="S4.T1.89.41.41.41.41.41.41.41.41.10.10.3.3.4">¶</sup></span></span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.95.47.47.47.47.47.47.47.47" style="padding-top:3pt;padding-bottom:3pt;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.95.47.47.47.47.47.47.47.47.6">
<span class="ltx_tr" id="S4.T1.91.43.43.43.43.43.43.43.43.2.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.91.43.43.43.43.43.43.43.43.2.2.2" style="padding-top:3pt;padding-bottom:3pt;"><span class="ltx_text" id="S4.T1.91.43.43.43.43.43.43.43.43.2.2.2.2" style="font-size:133%;background-color:#F9E5E5;">StackSpotAI<sup class="ltx_sup" id="S4.T1.91.43.43.43.43.43.43.43.43.2.2.2.2.3">‡</sup><sup class="ltx_sup" id="S4.T1.91.43.43.43.43.43.43.43.43.2.2.2.2.4">§</sup></span><span class="ltx_text" id="S4.T1.91.43.43.43.43.43.43.43.43.2.2.2.3" style="font-size:133%;"> </span><span class="ltx_text" id="S4.T1.91.43.43.43.43.43.43.43.43.2.2.2.4" style="font-size:133%;background-color:#F9E5E5;"> E&amp;V</span></span></span>
<span class="ltx_tr" id="S4.T1.94.46.46.46.46.46.46.46.46.5.5">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.94.46.46.46.46.46.46.46.46.5.5.3" style="padding-top:3pt;padding-bottom:3pt;"><span class="ltx_text" id="S4.T1.92.44.44.44.44.44.44.44.44.3.3.1.1" style="font-size:133%;background-color:#F9E5E5;">Code4UIE<sup class="ltx_sup" id="S4.T1.92.44.44.44.44.44.44.44.44.3.3.1.1.2">§</sup></span><span class="ltx_text" id="S4.T1.94.46.46.46.46.46.46.46.46.5.5.3.4" style="font-size:133%;"> </span><span class="ltx_text" id="S4.T1.94.46.46.46.46.46.46.46.46.5.5.3.3" style="font-size:133%;background-color:#F9E5E5;"> De-fine<sup class="ltx_sup" id="S4.T1.94.46.46.46.46.46.46.46.46.5.5.3.3.3">‡</sup><sup class="ltx_sup" id="S4.T1.94.46.46.46.46.46.46.46.46.5.5.3.3.4">∥</sup></span></span></span>
<span class="ltx_tr" id="S4.T1.95.47.47.47.47.47.47.47.47.6.6">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.95.47.47.47.47.47.47.47.47.6.6.1" style="padding-top:3pt;padding-bottom:3pt;"><span class="ltx_text" id="S4.T1.95.47.47.47.47.47.47.47.47.6.6.1.1" style="font-size:133%;background-color:#F9E5E5;">ImputBlaster<sup class="ltx_sup" id="S4.T1.95.47.47.47.47.47.47.47.47.6.6.1.1.2">¶</sup></span></span></span>
</span></span></span>
</span><span class="ltx_text" id="S4.T1.95.47.47.47.47.47.48" style="font-size:133%;"></span></span></span>
</span></span></span><span class="ltx_text" id="S4.T1.95.48" style="font-size:90%;"></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.T1.133"><span class="ltx_text ltx_inline-block" id="S4.T1.133.38" style="font-size:90%;width:433.6pt;">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T1.133.38.38.38" style="width:420.5pt;height:82.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S4.T1.133.38.38.38.38"><span class="ltx_text" id="S4.T1.133.38.38.38.38.38">
<span class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T1.130.35.35.35.35.35.35">
<span class="ltx_tr" id="S4.T1.130.35.35.35.35.35.35.36">
<span class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t ltx_colspan ltx_colspan_4" id="S4.T1.130.35.35.35.35.35.35.36.1" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.130.35.35.35.35.35.35.36.1.1" style="font-size:56%;">RAG for Knowledge</span></span></span>
<span class="ltx_tr" id="S4.T1.130.35.35.35.35.35.35.37">
<span class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.130.35.35.35.35.35.35.37.1" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text" id="S4.T1.130.35.35.35.35.35.35.37.1.1"></span><span class="ltx_text ltx_font_bold" id="S4.T1.130.35.35.35.35.35.35.37.1.2" style="font-size:56%;"> <span class="ltx_text" id="S4.T1.130.35.35.35.35.35.35.37.1.2.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.130.35.35.35.35.35.35.37.1.2.1.1">
<span class="ltx_tr" id="S4.T1.130.35.35.35.35.35.35.37.1.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.130.35.35.35.35.35.35.37.1.2.1.1.1.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">Knowledge Base QA</span></span>
</span></span><span class="ltx_text" id="S4.T1.130.35.35.35.35.35.35.37.1.2.2"></span></span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.130.35.35.35.35.35.35.37.2" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text" id="S4.T1.130.35.35.35.35.35.35.37.2.1"></span><span class="ltx_text ltx_font_bold" id="S4.T1.130.35.35.35.35.35.35.37.2.2" style="font-size:56%;"> <span class="ltx_text" id="S4.T1.130.35.35.35.35.35.35.37.2.2.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.130.35.35.35.35.35.35.37.2.2.1.1">
<span class="ltx_tr" id="S4.T1.130.35.35.35.35.35.35.37.2.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.130.35.35.35.35.35.35.37.2.2.1.1.1.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">Knowledge-augmented Open-domain QA</span></span>
</span></span><span class="ltx_text" id="S4.T1.130.35.35.35.35.35.35.37.2.2.2"></span></span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.130.35.35.35.35.35.35.37.3" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text" id="S4.T1.130.35.35.35.35.35.35.37.3.1"></span><span class="ltx_text ltx_font_bold" id="S4.T1.130.35.35.35.35.35.35.37.3.2" style="font-size:56%;"> <span class="ltx_text" id="S4.T1.130.35.35.35.35.35.35.37.3.2.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.130.35.35.35.35.35.35.37.3.2.1.1">
<span class="ltx_tr" id="S4.T1.130.35.35.35.35.35.35.37.3.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.130.35.35.35.35.35.35.37.3.2.1.1.1.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">Table for QA</span></span>
</span></span><span class="ltx_text" id="S4.T1.130.35.35.35.35.35.35.37.3.2.2"></span></span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.130.35.35.35.35.35.35.37.4" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.130.35.35.35.35.35.35.37.4.1" style="font-size:56%;">Others</span></span></span>
<span class="ltx_tr" id="S4.T1.130.35.35.35.35.35.35.35">
<span class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.110.15.15.15.15.15.15.15.15" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.110.15.15.15.15.15.15.15.15.15">
<span class="ltx_tr" id="S4.T1.104.9.9.9.9.9.9.9.9.9.9">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.104.9.9.9.9.9.9.9.9.9.9.9" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text" id="S4.T1.98.3.3.3.3.3.3.3.3.3.3.3.3" style="font-size:56%;background-color:#F9E5E5;">CBR-KBQA<sup class="ltx_sup" id="S4.T1.98.3.3.3.3.3.3.3.3.3.3.3.3.4">‡</sup><sup class="ltx_sup" id="S4.T1.98.3.3.3.3.3.3.3.3.3.3.3.3.5">§</sup><sup class="ltx_sup" id="S4.T1.98.3.3.3.3.3.3.3.3.3.3.3.3.6">∥</sup></span><span class="ltx_text" id="S4.T1.104.9.9.9.9.9.9.9.9.9.9.9.10" style="font-size:56%;"> </span><span class="ltx_text" id="S4.T1.101.6.6.6.6.6.6.6.6.6.6.6.6" style="font-size:56%;background-color:#F9E5E5;"> TIARA<sup class="ltx_sup" id="S4.T1.101.6.6.6.6.6.6.6.6.6.6.6.6.4">†</sup><sup class="ltx_sup" id="S4.T1.101.6.6.6.6.6.6.6.6.6.6.6.6.5">‡</sup><sup class="ltx_sup" id="S4.T1.101.6.6.6.6.6.6.6.6.6.6.6.6.6">§</sup></span><span class="ltx_text" id="S4.T1.104.9.9.9.9.9.9.9.9.9.9.9.11" style="font-size:56%;"> </span><span class="ltx_text" id="S4.T1.104.9.9.9.9.9.9.9.9.9.9.9.9" style="font-size:56%;background-color:#F9E5E5;"> Keqing<sup class="ltx_sup" id="S4.T1.104.9.9.9.9.9.9.9.9.9.9.9.9.4">†</sup><sup class="ltx_sup" id="S4.T1.104.9.9.9.9.9.9.9.9.9.9.9.9.5">‡</sup><sup class="ltx_sup" id="S4.T1.104.9.9.9.9.9.9.9.9.9.9.9.9.6">§</sup></span></span></span>
<span class="ltx_tr" id="S4.T1.110.15.15.15.15.15.15.15.15.15.15">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.110.15.15.15.15.15.15.15.15.15.15.6" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text" id="S4.T1.106.11.11.11.11.11.11.11.11.11.11.2.2" style="font-size:56%;background-color:#F9E5E5;">RNG-KBQA<sup class="ltx_sup" id="S4.T1.106.11.11.11.11.11.11.11.11.11.11.2.2.3">‡</sup><sup class="ltx_sup" id="S4.T1.106.11.11.11.11.11.11.11.11.11.11.2.2.4">∥</sup></span><span class="ltx_text" id="S4.T1.110.15.15.15.15.15.15.15.15.15.15.6.7" style="font-size:56%;"> </span><span class="ltx_text" id="S4.T1.107.12.12.12.12.12.12.12.12.12.12.3.3" style="font-size:56%;background-color:#D1E9B8;"> ReTraCk<sup class="ltx_sup" id="S4.T1.107.12.12.12.12.12.12.12.12.12.12.3.3.2">§</sup></span><span class="ltx_text" id="S4.T1.110.15.15.15.15.15.15.15.15.15.15.6.8" style="font-size:56%;"> </span><span class="ltx_text" id="S4.T1.110.15.15.15.15.15.15.15.15.15.15.6.6" style="font-size:56%;background-color:#D1E9B8;"> SKP<sup class="ltx_sup" id="S4.T1.110.15.15.15.15.15.15.15.15.15.15.6.6.4">†</sup><sup class="ltx_sup" id="S4.T1.110.15.15.15.15.15.15.15.15.15.15.6.6.5">‡</sup><sup class="ltx_sup" id="S4.T1.110.15.15.15.15.15.15.15.15.15.15.6.6.6">§</sup></span></span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.118.23.23.23.23.23.23.23.23" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.118.23.23.23.23.23.23.23.23.8">
<span class="ltx_tr" id="S4.T1.114.19.19.19.19.19.19.19.19.4.4">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.114.19.19.19.19.19.19.19.19.4.4.4" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text" id="S4.T1.112.17.17.17.17.17.17.17.17.2.2.2.2" style="font-size:56%;background-color:#D1E9B8;">UniK-QA<sup class="ltx_sup" id="S4.T1.112.17.17.17.17.17.17.17.17.2.2.2.2.3">†</sup><sup class="ltx_sup" id="S4.T1.112.17.17.17.17.17.17.17.17.2.2.2.2.4">‡</sup></span><span class="ltx_text" id="S4.T1.114.19.19.19.19.19.19.19.19.4.4.4.5" style="font-size:56%;"> </span><span class="ltx_text" id="S4.T1.113.18.18.18.18.18.18.18.18.3.3.3.3" style="font-size:56%;background-color:#D1E9B8;"> KG-FiD<sup class="ltx_sup" id="S4.T1.113.18.18.18.18.18.18.18.18.3.3.3.3.2">‡</sup></span><span class="ltx_text" id="S4.T1.114.19.19.19.19.19.19.19.19.4.4.4.6" style="font-size:56%;"> </span><span class="ltx_text" id="S4.T1.114.19.19.19.19.19.19.19.19.4.4.4.4" style="font-size:56%;background-color:#D1E9B8;"> GRAPE<sup class="ltx_sup" id="S4.T1.114.19.19.19.19.19.19.19.19.4.4.4.4.2">‡</sup></span></span></span>
<span class="ltx_tr" id="S4.T1.118.23.23.23.23.23.23.23.23.8.8">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.118.23.23.23.23.23.23.23.23.8.8.4" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text" id="S4.T1.116.21.21.21.21.21.21.21.21.6.6.2.2" style="font-size:56%;background-color:#D1E9B8;">SKURG<sup class="ltx_sup" id="S4.T1.116.21.21.21.21.21.21.21.21.6.6.2.2.3">†</sup><sup class="ltx_sup" id="S4.T1.116.21.21.21.21.21.21.21.21.6.6.2.2.4">‡</sup></span><span class="ltx_text" id="S4.T1.118.23.23.23.23.23.23.23.23.8.8.4.5" style="font-size:56%;"> </span><span class="ltx_text" id="S4.T1.117.22.22.22.22.22.22.22.22.7.7.3.3" style="font-size:56%;background-color:#F9E5E5;"> KnowledGPT<sup class="ltx_sup" id="S4.T1.117.22.22.22.22.22.22.22.22.7.7.3.3.2">‡</sup></span><span class="ltx_text" id="S4.T1.118.23.23.23.23.23.23.23.23.8.8.4.6" style="font-size:56%;"> </span><span class="ltx_text" id="S4.T1.118.23.23.23.23.23.23.23.23.8.8.4.4" style="font-size:56%;background-color:#F9E5E5;"> EFSUM<sup class="ltx_sup" id="S4.T1.118.23.23.23.23.23.23.23.23.8.8.4.4.2">§</sup></span></span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.127.32.32.32.32.32.32.32.32" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.127.32.32.32.32.32.32.32.32.9">
<span class="ltx_tr" id="S4.T1.122.27.27.27.27.27.27.27.27.4.4">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.122.27.27.27.27.27.27.27.27.4.4.4" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text" id="S4.T1.119.24.24.24.24.24.24.24.24.1.1.1.1" style="font-size:56%;background-color:#D1E9B8;">EfficientQA<sup class="ltx_sup" id="S4.T1.119.24.24.24.24.24.24.24.24.1.1.1.1.2">‡</sup></span><span class="ltx_text" id="S4.T1.122.27.27.27.27.27.27.27.27.4.4.4.5" style="font-size:56%;"> </span><span class="ltx_text" id="S4.T1.120.25.25.25.25.25.25.25.25.2.2.2.2" style="font-size:56%;background-color:#D1E9B8;"> CORE<sup class="ltx_sup" id="S4.T1.120.25.25.25.25.25.25.25.25.2.2.2.2.2">§</sup></span><span class="ltx_text" id="S4.T1.122.27.27.27.27.27.27.27.27.4.4.4.6" style="font-size:56%;"> </span><span class="ltx_text" id="S4.T1.122.27.27.27.27.27.27.27.27.4.4.4.4" style="font-size:56%;background-color:#D1E9B8;"> Convinse<sup class="ltx_sup" id="S4.T1.122.27.27.27.27.27.27.27.27.4.4.4.4.3">†</sup><sup class="ltx_sup" id="S4.T1.122.27.27.27.27.27.27.27.27.4.4.4.4.4">‡</sup></span></span></span>
<span class="ltx_tr" id="S4.T1.127.32.32.32.32.32.32.32.32.9.9">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.127.32.32.32.32.32.32.32.32.9.9.5" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text" id="S4.T1.124.29.29.29.29.29.29.29.29.6.6.2.2" style="font-size:56%;background-color:#D1E9B8;">RINK<sup class="ltx_sup" id="S4.T1.124.29.29.29.29.29.29.29.29.6.6.2.2.3">‡</sup><sup class="ltx_sup" id="S4.T1.124.29.29.29.29.29.29.29.29.6.6.2.2.4">§</sup></span><span class="ltx_text" id="S4.T1.127.32.32.32.32.32.32.32.32.9.9.5.6" style="font-size:56%;"> </span><span class="ltx_text" id="S4.T1.126.31.31.31.31.31.31.31.31.8.8.4.4" style="font-size:56%;background-color:#F9E5E5;"> T-RAG<sup class="ltx_sup" id="S4.T1.126.31.31.31.31.31.31.31.31.8.8.4.4.3">‡</sup><sup class="ltx_sup" id="S4.T1.126.31.31.31.31.31.31.31.31.8.8.4.4.4">§</sup></span><span class="ltx_text" id="S4.T1.127.32.32.32.32.32.32.32.32.9.9.5.7" style="font-size:56%;"> </span><span class="ltx_text" id="S4.T1.127.32.32.32.32.32.32.32.32.9.9.5.5" style="font-size:56%;background-color:#F9E5E5;"> StructGPT<sup class="ltx_sup" id="S4.T1.127.32.32.32.32.32.32.32.32.9.9.5.5.2">‡</sup></span></span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.130.35.35.35.35.35.35.35.35" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.130.35.35.35.35.35.35.35.35.3">
<span class="ltx_tr" id="S4.T1.129.34.34.34.34.34.34.34.34.2.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.129.34.34.34.34.34.34.34.34.2.2.2" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text" id="S4.T1.128.33.33.33.33.33.33.33.33.1.1.1.1" style="font-size:56%;background-color:#F9E5E5;">GRetriever<sup class="ltx_sup" id="S4.T1.128.33.33.33.33.33.33.33.33.1.1.1.1.2">§</sup></span><span class="ltx_text" id="S4.T1.129.34.34.34.34.34.34.34.34.2.2.2.3" style="font-size:56%;"> </span><span class="ltx_text" id="S4.T1.129.34.34.34.34.34.34.34.34.2.2.2.2" style="font-size:56%;background-color:#F9E5E5;"> SURGE<sup class="ltx_sup" id="S4.T1.129.34.34.34.34.34.34.34.34.2.2.2.2.2">§</sup></span></span></span>
<span class="ltx_tr" id="S4.T1.130.35.35.35.35.35.35.35.35.3.3">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.130.35.35.35.35.35.35.35.35.3.3.1" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text" id="S4.T1.130.35.35.35.35.35.35.35.35.3.3.1.2" style="font-size:56%;background-color:#F9E5E5;">K-LaMP</span><span class="ltx_text" id="S4.T1.130.35.35.35.35.35.35.35.35.3.3.1.3" style="font-size:56%;"> </span><span class="ltx_text" id="S4.T1.130.35.35.35.35.35.35.35.35.3.3.1.1" style="font-size:56%;background-color:#D1E9B8;"> RHO<sup class="ltx_sup" id="S4.T1.130.35.35.35.35.35.35.35.35.3.3.1.1.2">∥</sup></span></span></span>
</span></span></span>
</span><span class="ltx_text ltx_align_center" id="S4.T1.133.38.38.38.38.38.38" style="font-size:56%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.133.38.38.38.38.38.38.3">
<span class="ltx_tr" id="S4.T1.133.38.38.38.38.38.38.3.4">
<span class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.133.38.38.38.38.38.38.3.4.1" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.133.38.38.38.38.38.38.3.4.1.1">RAG for 3D</span></span></span>
<span class="ltx_tr" id="S4.T1.133.38.38.38.38.38.38.3.5">
<span class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.133.38.38.38.38.38.38.3.5.1" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text" id="S4.T1.133.38.38.38.38.38.38.3.5.1.1"></span><span class="ltx_text ltx_font_bold" id="S4.T1.133.38.38.38.38.38.38.3.5.1.2"> <span class="ltx_text" id="S4.T1.133.38.38.38.38.38.38.3.5.1.2.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.133.38.38.38.38.38.38.3.5.1.2.1.1">
<span class="ltx_tr" id="S4.T1.133.38.38.38.38.38.38.3.5.1.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.133.38.38.38.38.38.38.3.5.1.2.1.1.1.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">Text-to-3D</span></span>
</span></span><span class="ltx_text" id="S4.T1.133.38.38.38.38.38.38.3.5.1.2.2"></span></span></span></span>
<span class="ltx_tr" id="S4.T1.133.38.38.38.38.38.38.3.3">
<span class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.133.38.38.38.38.38.38.3.3.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.133.38.38.38.38.38.38.3.3.3.3">
<span class="ltx_tr" id="S4.T1.132.37.37.37.37.37.37.2.2.2.2.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.132.37.37.37.37.37.37.2.2.2.2.2.2" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text" id="S4.T1.132.37.37.37.37.37.37.2.2.2.2.2.2.2" style="background-color:#D1E9B8;">ReMoDiffuse<sup class="ltx_sup" id="S4.T1.132.37.37.37.37.37.37.2.2.2.2.2.2.2.3">†</sup><sup class="ltx_sup" id="S4.T1.132.37.37.37.37.37.37.2.2.2.2.2.2.2.4">‡</sup></span></span></span>
<span class="ltx_tr" id="S4.T1.133.38.38.38.38.38.38.3.3.3.3.3">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.133.38.38.38.38.38.38.3.3.3.3.3.1" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text" id="S4.T1.133.38.38.38.38.38.38.3.3.3.3.3.1.1" style="background-color:#F9E5E5;">AMD<sup class="ltx_sup" id="S4.T1.133.38.38.38.38.38.38.3.3.3.3.3.1.1.2">†</sup></span></span></span>
</span></span></span>
</span></span></span></span>
</span></span></span><span class="ltx_text" id="S4.T1.133.39" style="font-size:90%;"></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.T1.161"><span class="ltx_text ltx_inline-block" id="S4.T1.161.28" style="font-size:90%;width:433.6pt;">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T1.161.28.28.28" style="width:655.2pt;height:78.5pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S4.T1.161.28.28.28.28"><span class="ltx_text" id="S4.T1.161.28.28.28.28.28">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.145.12.12.12.12.12.12">
<span class="ltx_tr" id="S4.T1.145.12.12.12.12.12.12.13">
<span class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t ltx_colspan ltx_colspan_3" id="S4.T1.145.12.12.12.12.12.12.13.1" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.145.12.12.12.12.12.12.13.1.1" style="font-size:89%;">RAG for Image</span></span></span>
<span class="ltx_tr" id="S4.T1.145.12.12.12.12.12.12.14">
<span class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.145.12.12.12.12.12.12.14.1" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.145.12.12.12.12.12.12.14.1.1" style="font-size:89%;">Image Generation</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.145.12.12.12.12.12.12.14.2" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.145.12.12.12.12.12.12.14.2.1" style="font-size:89%;">Image Captioning</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.145.12.12.12.12.12.12.14.3" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.145.12.12.12.12.12.12.14.3.1" style="font-size:89%;">Others</span></span></span>
<span class="ltx_tr" id="S4.T1.145.12.12.12.12.12.12.12">
<span class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.137.4.4.4.4.4.4.4.4" style="padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.137.4.4.4.4.4.4.4.4.4">
<span class="ltx_tr" id="S4.T1.136.3.3.3.3.3.3.3.3.3.3">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.136.3.3.3.3.3.3.3.3.3.3.3" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text" id="S4.T1.134.1.1.1.1.1.1.1.1.1.1.1.1" style="font-size:89%;background-color:#F9E5E5;">RetrieveGAN<sup class="ltx_sup" id="S4.T1.134.1.1.1.1.1.1.1.1.1.1.1.1.2">‡</sup></span><span class="ltx_text" id="S4.T1.136.3.3.3.3.3.3.3.3.3.3.3.4" style="font-size:89%;"> </span><span class="ltx_text" id="S4.T1.135.2.2.2.2.2.2.2.2.2.2.2.2" style="font-size:89%;background-color:#F9E5E5;"> IC-GAN<sup class="ltx_sup" id="S4.T1.135.2.2.2.2.2.2.2.2.2.2.2.2.2">§</sup></span><span class="ltx_text" id="S4.T1.136.3.3.3.3.3.3.3.3.3.3.3.5" style="font-size:89%;"> </span><span class="ltx_text" id="S4.T1.136.3.3.3.3.3.3.3.3.3.3.3.3" style="font-size:89%;background-color:#D1E9B8;"> Re-imagen<sup class="ltx_sup" id="S4.T1.136.3.3.3.3.3.3.3.3.3.3.3.3.2">§</sup></span></span></span>
<span class="ltx_tr" id="S4.T1.137.4.4.4.4.4.4.4.4.4.4">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.137.4.4.4.4.4.4.4.4.4.4.1" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text" id="S4.T1.137.4.4.4.4.4.4.4.4.4.4.1.2" style="font-size:89%;background-color:#D1E9B8;">RDM</span><span class="ltx_text" id="S4.T1.137.4.4.4.4.4.4.4.4.4.4.1.3" style="font-size:89%;"> </span><span class="ltx_text" id="S4.T1.137.4.4.4.4.4.4.4.4.4.4.1.1" style="font-size:89%;background-color:#D1E9B8;"> Retrieve&amp;Fuse<sup class="ltx_sup" id="S4.T1.137.4.4.4.4.4.4.4.4.4.4.1.1.2">§</sup></span><span class="ltx_text" id="S4.T1.137.4.4.4.4.4.4.4.4.4.4.1.4" style="font-size:89%;"> </span><span class="ltx_text" id="S4.T1.137.4.4.4.4.4.4.4.4.4.4.1.5" style="font-size:89%;background-color:#D1E9B8;"> KNN-Diffusion</span></span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.141.8.8.8.8.8.8.8.8" style="padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.141.8.8.8.8.8.8.8.8.4">
<span class="ltx_tr" id="S4.T1.140.7.7.7.7.7.7.7.7.3.3">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.140.7.7.7.7.7.7.7.7.3.3.3" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text" id="S4.T1.138.5.5.5.5.5.5.5.5.1.1.1.1" style="font-size:89%;background-color:#DAF3F6;">MA<sup class="ltx_sup" id="S4.T1.138.5.5.5.5.5.5.5.5.1.1.1.1.2">∥</sup></span><span class="ltx_text" id="S4.T1.140.7.7.7.7.7.7.7.7.3.3.3.4" style="font-size:89%;"> </span><span class="ltx_text" id="S4.T1.139.6.6.6.6.6.6.6.6.2.2.2.2" style="font-size:89%;background-color:#F9E5E5;"> REVEAL<sup class="ltx_sup" id="S4.T1.139.6.6.6.6.6.6.6.6.2.2.2.2.2">‡</sup></span><span class="ltx_text" id="S4.T1.140.7.7.7.7.7.7.7.7.3.3.3.5" style="font-size:89%;"> </span><span class="ltx_text" id="S4.T1.140.7.7.7.7.7.7.7.7.3.3.3.3" style="font-size:89%;background-color:#F9E5E5;"> SMALLCAP<sup class="ltx_sup" id="S4.T1.140.7.7.7.7.7.7.7.7.3.3.3.3.2">†</sup></span></span></span>
<span class="ltx_tr" id="S4.T1.141.8.8.8.8.8.8.8.8.4.4">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.141.8.8.8.8.8.8.8.8.4.4.1" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text" id="S4.T1.141.8.8.8.8.8.8.8.8.4.4.1.1" style="font-size:89%;background-color:#F9E5E5;">CRSR<sup class="ltx_sup" id="S4.T1.141.8.8.8.8.8.8.8.8.4.4.1.1.2">†</sup></span><span class="ltx_text" id="S4.T1.141.8.8.8.8.8.8.8.8.4.4.1.2" style="font-size:89%;"> </span><span class="ltx_text" id="S4.T1.141.8.8.8.8.8.8.8.8.4.4.1.3" style="font-size:89%;background-color:#D1E9B8;"> RA-Transformer</span></span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.145.12.12.12.12.12.12.12.12" style="padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.145.12.12.12.12.12.12.12.12.4">
<span class="ltx_tr" id="S4.T1.143.10.10.10.10.10.10.10.10.2.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.143.10.10.10.10.10.10.10.10.2.2.2" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text" id="S4.T1.142.9.9.9.9.9.9.9.9.1.1.1.1" style="font-size:89%;background-color:#F9E5E5;">PICa<sup class="ltx_sup" id="S4.T1.142.9.9.9.9.9.9.9.9.1.1.1.1.2">∥</sup></span><span class="ltx_text" id="S4.T1.143.10.10.10.10.10.10.10.10.2.2.2.3" style="font-size:89%;"> </span><span class="ltx_text" id="S4.T1.143.10.10.10.10.10.10.10.10.2.2.2.2" style="font-size:89%;background-color:#F9E5E5;"> Maira<sup class="ltx_sup" id="S4.T1.143.10.10.10.10.10.10.10.10.2.2.2.2.2">‡</sup></span></span></span>
<span class="ltx_tr" id="S4.T1.145.12.12.12.12.12.12.12.12.4.4">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.145.12.12.12.12.12.12.12.12.4.4.2" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text" id="S4.T1.144.11.11.11.11.11.11.11.11.3.3.1.1" style="font-size:89%;background-color:#F9E5E5;">KIF<sup class="ltx_sup" id="S4.T1.144.11.11.11.11.11.11.11.11.3.3.1.1.2">‡</sup></span><span class="ltx_text" id="S4.T1.145.12.12.12.12.12.12.12.12.4.4.2.3" style="font-size:89%;"> </span><span class="ltx_text" id="S4.T1.145.12.12.12.12.12.12.12.12.4.4.2.2" style="font-size:89%;background-color:#F9E5E5;"> RA-VQA<sup class="ltx_sup" id="S4.T1.145.12.12.12.12.12.12.12.12.4.4.2.2.2">‡</sup></span></span></span>
</span></span></span>
</span><span class="ltx_text" id="S4.T1.161.28.28.28.28.28.28" style="font-size:89%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.161.28.28.28.28.28.28.16">
<span class="ltx_tr" id="S4.T1.161.28.28.28.28.28.28.16.17">
<span class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t ltx_colspan ltx_colspan_3" id="S4.T1.161.28.28.28.28.28.28.16.17.1" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.161.28.28.28.28.28.28.16.17.1.1">RAG for Video</span></span></span>
<span class="ltx_tr" id="S4.T1.161.28.28.28.28.28.28.16.18">
<span class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.161.28.28.28.28.28.28.16.18.1" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.161.28.28.28.28.28.28.16.18.1.1">Video Captioning</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.161.28.28.28.28.28.28.16.18.2" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.161.28.28.28.28.28.28.16.18.2.1">Video QA&amp;Dialogue</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.161.28.28.28.28.28.28.16.18.3" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.161.28.28.28.28.28.28.16.18.3.1">Others</span></span></span>
<span class="ltx_tr" id="S4.T1.161.28.28.28.28.28.28.16.16">
<span class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.151.18.18.18.18.18.18.6.6.6" style="padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.151.18.18.18.18.18.18.6.6.6.6">
<span class="ltx_tr" id="S4.T1.149.16.16.16.16.16.16.4.4.4.4.4">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.149.16.16.16.16.16.16.4.4.4.4.4.4" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text" id="S4.T1.147.14.14.14.14.14.14.2.2.2.2.2.2.2" style="background-color:#DAF3F6;">KaVD<sup class="ltx_sup" id="S4.T1.147.14.14.14.14.14.14.2.2.2.2.2.2.2.3">‡</sup><sup class="ltx_sup" id="S4.T1.147.14.14.14.14.14.14.2.2.2.2.2.2.2.4">§</sup></span> <span class="ltx_text" id="S4.T1.149.16.16.16.16.16.16.4.4.4.4.4.4.4" style="background-color:#D1E9B8;"> R-ConvED<sup class="ltx_sup" id="S4.T1.149.16.16.16.16.16.16.4.4.4.4.4.4.4.3">‡</sup><sup class="ltx_sup" id="S4.T1.149.16.16.16.16.16.16.4.4.4.4.4.4.4.4">§</sup></span></span></span>
<span class="ltx_tr" id="S4.T1.151.18.18.18.18.18.18.6.6.6.6.6">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.151.18.18.18.18.18.18.6.6.6.6.6.2" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text" id="S4.T1.150.17.17.17.17.17.17.5.5.5.5.5.1.1" style="background-color:#D1E9B8;">CARE<sup class="ltx_sup" id="S4.T1.150.17.17.17.17.17.17.5.5.5.5.5.1.1.2">§</sup></span>
<svg class="ltx_picture" height="17.59" id="S4.T1.151.18.18.18.18.18.18.6.6.6.6.6.2.pic1" overflow="visible" version="1.1" width="80.8"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,17.59) matrix(1 0 0 -1 0 0) translate(40.4,0) translate(0,4.92)"><clippath id="pgfcp3"><path d="M -40.4 -4.92 h 80.8 v 17.59 h -80.8 Z"></path></clippath><g clip-path="url(#pgfcp3)"><g transform="matrix(1.0 0.0 0.0 1.0 0 3.87) matrix(0.0 1.0 -1.0 0.0 0 0) matrix(0.2533 0.0 0.0 1.1637 0 0)"><defs><lineargradient gradienttransform="rotate(90)" id="pgfsh4"><stop offset="0.0" stop-color="#D1E9B8"></stop><stop offset="0.24998" stop-color="#D1E9B8"></stop><stop offset="0.49996" stop-color="#E5E7CF"></stop><stop offset="0.75002" stop-color="#F9E5E5"></stop><stop offset="1.0" stop-color="#F9E5E5"></stop></lineargradient></defs><g transform="translate(-69.44 -69.44)"><rect height="138.89" style="fill:url(#pgfsh4);stroke:none" width="138.89"></rect></g></g></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -37.63 0)"><foreignobject height="12.05" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="75.27"><span class="ltx_text" id="S4.T1.151.18.18.18.18.18.18.6.6.6.6.6.2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3">EgoInstructor<sup class="ltx_sup" id="S4.T1.151.18.18.18.18.18.18.6.6.6.6.6.2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1">†</sup><sup class="ltx_sup" id="S4.T1.151.18.18.18.18.18.18.6.6.6.6.6.2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.2">‡</sup><sup class="ltx_sup" id="S4.T1.151.18.18.18.18.18.18.6.6.6.6.6.2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3">§</sup></span></foreignobject></g></g></svg></span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.156.23.23.23.23.23.23.11.11.11" style="padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.156.23.23.23.23.23.23.11.11.11.5">
<span class="ltx_tr" id="S4.T1.154.21.21.21.21.21.21.9.9.9.3.3">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.154.21.21.21.21.21.21.9.9.9.3.3.3" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text" id="S4.T1.153.20.20.20.20.20.20.8.8.8.2.2.2.2" style="background-color:#D1E9B8;">MA-DRNN<sup class="ltx_sup" id="S4.T1.153.20.20.20.20.20.20.8.8.8.2.2.2.2.3">†</sup><sup class="ltx_sup" id="S4.T1.153.20.20.20.20.20.20.8.8.8.2.2.2.2.4">‡</sup></span> <span class="ltx_text" id="S4.T1.154.21.21.21.21.21.21.9.9.9.3.3.3.3" style="background-color:#F9E5E5;"> R2A<sup class="ltx_sup" id="S4.T1.154.21.21.21.21.21.21.9.9.9.3.3.3.3.2">‡</sup></span></span></span>
<span class="ltx_tr" id="S4.T1.156.23.23.23.23.23.23.11.11.11.5.5">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.156.23.23.23.23.23.23.11.11.11.5.5.2" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text" id="S4.T1.155.22.22.22.22.22.22.10.10.10.4.4.1.1" style="background-color:#D1E9B8;">Tvqa+<sup class="ltx_sup" id="S4.T1.155.22.22.22.22.22.22.10.10.10.4.4.1.1.2">§</sup></span> <span class="ltx_text" id="S4.T1.156.23.23.23.23.23.23.11.11.11.5.5.2.2" style="background-color:#F9E5E5;"> VGNMN<sup class="ltx_sup" id="S4.T1.156.23.23.23.23.23.23.11.11.11.5.5.2.2.2">‡</sup></span></span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.161.28.28.28.28.28.28.16.16.16" style="padding-top:2pt;padding-bottom:2pt;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.161.28.28.28.28.28.28.16.16.16.5">
<span class="ltx_tr" id="S4.T1.159.26.26.26.26.26.26.14.14.14.3.3">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.159.26.26.26.26.26.26.14.14.14.3.3.3" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text" id="S4.T1.158.25.25.25.25.25.25.13.13.13.2.2.2.2" style="background-color:#F9E5E5;">VidIL<sup class="ltx_sup" id="S4.T1.158.25.25.25.25.25.25.13.13.13.2.2.2.2.3">†</sup><sup class="ltx_sup" id="S4.T1.158.25.25.25.25.25.25.13.13.13.2.2.2.2.4">‡</sup></span> <span class="ltx_text" id="S4.T1.159.26.26.26.26.26.26.14.14.14.3.3.3.3" style="background-color:#F9E5E5;"> RAG-Driver<sup class="ltx_sup" id="S4.T1.159.26.26.26.26.26.26.14.14.14.3.3.3.3.2">‡</sup></span></span></span>
<span class="ltx_tr" id="S4.T1.161.28.28.28.28.28.28.16.16.16.5.5">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.161.28.28.28.28.28.28.16.16.16.5.5.2" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text" id="S4.T1.161.28.28.28.28.28.28.16.16.16.5.5.2.2" style="background-color:#D1E9B8;">Animate-A-Story<sup class="ltx_sup" id="S4.T1.161.28.28.28.28.28.28.16.16.16.5.5.2.2.3">†</sup><sup class="ltx_sup" id="S4.T1.161.28.28.28.28.28.28.16.16.16.5.5.2.2.4">§</sup></span></span></span>
</span></span></span>
</span></span></span></span>
</span></span></span><span class="ltx_text" id="S4.T1.161.29" style="font-size:90%;"></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.T1.180"><span class="ltx_text ltx_inline-block" id="S4.T1.180.19" style="font-size:90%;width:433.6pt;">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T1.180.19.19.19" style="width:470.3pt;height:114.4pt;vertical-align:-60.4pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S4.T1.180.19.19.19.19"><span class="ltx_text" id="S4.T1.180.19.19.19.19.19">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.175.14.14.14.14.14.14">
<span class="ltx_tr" id="S4.T1.175.14.14.14.14.14.14.15">
<span class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t ltx_colspan ltx_colspan_3" id="S4.T1.175.14.14.14.14.14.14.15.1" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.175.14.14.14.14.14.14.15.1.1" style="font-size:89%;">RAG for Science</span></span></span>
<span class="ltx_tr" id="S4.T1.175.14.14.14.14.14.14.16">
<span class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.175.14.14.14.14.14.14.16.1" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.175.14.14.14.14.14.14.16.1.1" style="font-size:89%;">Drug Discovery</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.175.14.14.14.14.14.14.16.2" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.175.14.14.14.14.14.14.16.2.1" style="font-size:89%;">Biomedical Informatics Enhancement</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.175.14.14.14.14.14.14.16.3" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.175.14.14.14.14.14.14.16.3.1" style="font-size:89%;">Math Applications</span></span></span>
<span class="ltx_tr" id="S4.T1.175.14.14.14.14.14.14.14">
<span class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.165.4.4.4.4.4.4.4.4" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text" id="S4.T1.163.2.2.2.2.2.2.2.2.2" style="font-size:89%;background-color:#F9E5E5;">RetMol<sup class="ltx_sup" id="S4.T1.163.2.2.2.2.2.2.2.2.2.3">†</sup><sup class="ltx_sup" id="S4.T1.163.2.2.2.2.2.2.2.2.2.4">§</sup></span><span class="ltx_text" id="S4.T1.165.4.4.4.4.4.4.4.4.5" style="font-size:89%;"> </span><span class="ltx_text" id="S4.T1.165.4.4.4.4.4.4.4.4.4" style="font-size:89%;background-color:#D1E9B8;"> PromptDiff<sup class="ltx_sup" id="S4.T1.165.4.4.4.4.4.4.4.4.4.3">†</sup><sup class="ltx_sup" id="S4.T1.165.4.4.4.4.4.4.4.4.4.4">‡</sup></span></span>
<span class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.172.11.11.11.11.11.11.11.11" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text" id="S4.T1.166.5.5.5.5.5.5.5.5.1" style="font-size:89%;background-color:#FFE1BB;">PoET<sup class="ltx_sup" id="S4.T1.166.5.5.5.5.5.5.5.5.1.2">‡</sup></span><span class="ltx_text" id="S4.T1.172.11.11.11.11.11.11.11.11.8" style="font-size:89%;"> </span><span class="ltx_text" id="S4.T1.168.7.7.7.7.7.7.7.7.3" style="font-size:89%;background-color:#F9E5E5;"> Chat-Orthopedist<sup class="ltx_sup" id="S4.T1.168.7.7.7.7.7.7.7.7.3.3">†</sup><sup class="ltx_sup" id="S4.T1.168.7.7.7.7.7.7.7.7.3.4">§</sup></span><span class="ltx_text" id="S4.T1.172.11.11.11.11.11.11.11.11.9" style="font-size:89%;"> </span><span class="ltx_text" id="S4.T1.169.8.8.8.8.8.8.8.8.4" style="font-size:89%;background-color:#F9E5E5;"> BIOREADER<sup class="ltx_sup" id="S4.T1.169.8.8.8.8.8.8.8.8.4.2">†</sup></span><span class="ltx_text" id="S4.T1.172.11.11.11.11.11.11.11.11.10" style="font-size:89%;"> </span><span class="ltx_text" id="S4.T1.170.9.9.9.9.9.9.9.9.5" style="font-size:89%;background-color:#F9E5E5;"> MedWriter<sup class="ltx_sup" id="S4.T1.170.9.9.9.9.9.9.9.9.5.2">‡</sup></span><span class="ltx_text" id="S4.T1.172.11.11.11.11.11.11.11.11.11" style="font-size:89%;"> </span><span class="ltx_text" id="S4.T1.172.11.11.11.11.11.11.11.11.7" style="font-size:89%;background-color:#F9E5E5;"> QARAG<sup class="ltx_sup" id="S4.T1.172.11.11.11.11.11.11.11.11.7.3">†</sup><sup class="ltx_sup" id="S4.T1.172.11.11.11.11.11.11.11.11.7.4">‡</sup></span></span>
<span class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.175.14.14.14.14.14.14.14.14" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text" id="S4.T1.173.12.12.12.12.12.12.12.12.1" style="font-size:89%;background-color:#FFE1BB;">LeanDojo<sup class="ltx_sup" id="S4.T1.173.12.12.12.12.12.12.12.12.1.2">‡</sup></span><span class="ltx_text" id="S4.T1.175.14.14.14.14.14.14.14.14.4" style="font-size:89%;"> </span><span class="ltx_text" id="S4.T1.175.14.14.14.14.14.14.14.14.3" style="font-size:89%;background-color:#F9E5E5;"> RAG-for-math-QA<sup class="ltx_sup" id="S4.T1.175.14.14.14.14.14.14.14.14.3.3">†</sup><sup class="ltx_sup" id="S4.T1.175.14.14.14.14.14.14.14.14.3.4">‡</sup></span></span></span>
</span><span class="ltx_text" id="S4.T1.180.19.19.19.19.19.19" style="font-size:89%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.180.19.19.19.19.19.19.5">
<span class="ltx_tr" id="S4.T1.180.19.19.19.19.19.19.5.6">
<span class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t ltx_colspan ltx_colspan_2" id="S4.T1.180.19.19.19.19.19.19.5.6.1" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.180.19.19.19.19.19.19.5.6.1.1">RAG for Audio</span></span></span>
<span class="ltx_tr" id="S4.T1.180.19.19.19.19.19.19.5.7">
<span class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.180.19.19.19.19.19.19.5.7.1" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text" id="S4.T1.180.19.19.19.19.19.19.5.7.1.1"></span><span class="ltx_text ltx_font_bold" id="S4.T1.180.19.19.19.19.19.19.5.7.1.2"> <span class="ltx_text" id="S4.T1.180.19.19.19.19.19.19.5.7.1.2.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.180.19.19.19.19.19.19.5.7.1.2.1.1">
<span class="ltx_tr" id="S4.T1.180.19.19.19.19.19.19.5.7.1.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.180.19.19.19.19.19.19.5.7.1.2.1.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">Audio Generation</span></span>
</span></span><span class="ltx_text" id="S4.T1.180.19.19.19.19.19.19.5.7.1.2.2"></span></span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.180.19.19.19.19.19.19.5.7.2" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text" id="S4.T1.180.19.19.19.19.19.19.5.7.2.1"></span><span class="ltx_text ltx_font_bold" id="S4.T1.180.19.19.19.19.19.19.5.7.2.2"> <span class="ltx_text" id="S4.T1.180.19.19.19.19.19.19.5.7.2.2.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.180.19.19.19.19.19.19.5.7.2.2.1.1">
<span class="ltx_tr" id="S4.T1.180.19.19.19.19.19.19.5.7.2.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.180.19.19.19.19.19.19.5.7.2.2.1.1.1.1" style="padding-top:2pt;padding-bottom:2pt;">Audio Captioning</span></span>
</span></span><span class="ltx_text" id="S4.T1.180.19.19.19.19.19.19.5.7.2.2.2"></span></span></span></span>
<span class="ltx_tr" id="S4.T1.180.19.19.19.19.19.19.5.5">
<span class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.178.17.17.17.17.17.17.3.3.3" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text" id="S4.T1.176.15.15.15.15.15.15.1.1.1.1" style="background-color:#D1E9B8;">Re-AudioLDM<sup class="ltx_sup" id="S4.T1.176.15.15.15.15.15.15.1.1.1.1.2">§</sup></span> <span class="ltx_text" id="S4.T1.178.17.17.17.17.17.17.3.3.3.3" style="background-color:#F9E5E5;"> Make-An-Audio<sup class="ltx_sup" id="S4.T1.178.17.17.17.17.17.17.3.3.3.3.3">†</sup><sup class="ltx_sup" id="S4.T1.178.17.17.17.17.17.17.3.3.3.3.4">§</sup></span></span>
<span class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.180.19.19.19.19.19.19.5.5.5" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text" id="S4.T1.180.19.19.19.19.19.19.5.5.5.2" style="background-color:#F9E5E5;">RECAP<sup class="ltx_sup" id="S4.T1.180.19.19.19.19.19.19.5.5.5.2.3">‡</sup><sup class="ltx_sup" id="S4.T1.180.19.19.19.19.19.19.5.5.5.2.4">§</sup></span></span></span>
</span></span></span></span>
</span></span></span><span class="ltx_text" id="S4.T1.180.20" style="font-size:90%;"></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_top" id="S4.T1.182" style="width:173.4pt;">
<p class="ltx_p" id="S4.T1.182.1"><span class="ltx_text" id="S4.T1.182.1.1" style="font-size:70%;background-color:#F9E5E5;">Query-based</span></p>
<p class="ltx_p ltx_align_center" id="S4.T1.182.2"><span class="ltx_text" id="S4.T1.182.2.1" style="font-size:70%;background-color:#D1E9B8;">Latent-based</span></p>
<p class="ltx_p ltx_align_center" id="S4.T1.182.3"><span class="ltx_text" id="S4.T1.182.3.1" style="font-size:70%;background-color:#DAF3F6;">Logit-based</span></p>
<p class="ltx_p ltx_align_center" id="S4.T1.182.4"><span class="ltx_text" id="S4.T1.182.4.1" style="font-size:70%;background-color:#FFE1BB;">Speculative</span></p>
<svg class="ltx_picture ltx_centering" height="14.04" id="S4.T1.181.pic1" overflow="visible" version="1.1" width="67.04"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.04) matrix(1 0 0 -1 0 0) translate(33.52,0) translate(0,4.65)"><clippath id="pgfcp5"><path d="M -33.52 -4.65 h 67.04 v 14.04 h -67.04 Z"></path></clippath><g clip-path="url(#pgfcp5)"><g transform="matrix(1.0 0.0 0.0 1.0 0 2.37) matrix(0.0 1.0 -1.0 0.0 0 0) matrix(0.20215 0.0 0.0 0.9655 0 0)"><defs><lineargradient gradienttransform="rotate(90)" id="pgfsh6"><stop offset="0.0" stop-color="#D1E9B8"></stop><stop offset="0.24998" stop-color="#D1E9B8"></stop><stop offset="0.49996" stop-color="#E5E7CF"></stop><stop offset="0.75002" stop-color="#F9E5E5"></stop><stop offset="1.0" stop-color="#F9E5E5"></stop></lineargradient></defs><g transform="translate(-69.44 -69.44)"><rect height="138.89" style="fill:url(#pgfsh6);stroke:none" width="138.89"></rect></g></g></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -30.75 0)"><foreignobject height="8.5" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="61.51"><span class="ltx_text" id="S4.T1.181.pic1.1.1.1.1.1" style="font-size:70%;">Query+Latent</span></foreignobject></g></g></svg><svg class="ltx_picture ltx_centering" height="14.04" id="S4.T1.182.pic2" overflow="visible" version="1.1" width="63.11"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.04) matrix(1 0 0 -1 0 0) translate(31.56,0) translate(0,4.65)"><clippath id="pgfcp7"><path d="M -31.56 -4.65 h 63.11 v 14.04 h -63.11 Z"></path></clippath><g clip-path="url(#pgfcp7)"><g transform="matrix(1.0 0.0 0.0 1.0 0 2.37) matrix(0.0 1.0 -1.0 0.0 0 0) matrix(0.20215 0.0 0.0 0.90894 0 0)"><defs><lineargradient gradienttransform="rotate(90)" id="pgfsh8"><stop offset="0.0" stop-color="#DAF3F6"></stop><stop offset="0.24998" stop-color="#DAF3F6"></stop><stop offset="0.49996" stop-color="#D6EED7"></stop><stop offset="0.75002" stop-color="#D1E9B8"></stop><stop offset="1.0" stop-color="#D1E9B8"></stop></lineargradient></defs><g transform="translate(-69.44 -69.44)"><rect height="138.89" style="fill:url(#pgfsh8);stroke:none" width="138.89"></rect></g></g></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -28.79 0)"><foreignobject height="8.5" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="57.58"><span class="ltx_text" id="S4.T1.182.pic2.1.1.1.1.1" style="font-size:70%;">Latent+Logit</span></foreignobject></g></g></svg>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_top" id="S4.T1.187" style="width:173.4pt;">
<p class="ltx_p" id="S4.T1.183.1"><math alttext="\dagger" class="ltx_Math" display="inline" id="S4.T1.183.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S4.T1.183.1.m1.1a"><mo id="S4.T1.183.1.m1.1.1" mathbackground="#FFFFFF" mathsize="70%" xref="S4.T1.183.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S4.T1.183.1.m1.1b"><ci id="S4.T1.183.1.m1.1.1.cmml" xref="S4.T1.183.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.183.1.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="S4.T1.183.1.m1.1d">†</annotation></semantics></math><span class="ltx_text" id="S4.T1.183.1.1" style="font-size:70%;background-color:#FFFFFF;"> Input</span></p>
<p class="ltx_p ltx_align_center" id="S4.T1.184.2"><math alttext="\ddagger" class="ltx_Math" display="inline" id="S4.T1.184.2.m1.1" style="background-color:#FFFFFF;"><semantics id="S4.T1.184.2.m1.1a"><mo id="S4.T1.184.2.m1.1.1" mathbackground="#FFFFFF" mathsize="70%" xref="S4.T1.184.2.m1.1.1.cmml">‡</mo><annotation-xml encoding="MathML-Content" id="S4.T1.184.2.m1.1b"><ci id="S4.T1.184.2.m1.1.1.cmml" xref="S4.T1.184.2.m1.1.1">‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.184.2.m1.1c">\ddagger</annotation><annotation encoding="application/x-llamapun" id="S4.T1.184.2.m1.1d">‡</annotation></semantics></math><span class="ltx_text" id="S4.T1.184.2.1" style="font-size:70%;background-color:#FFFFFF;"> Retriever</span></p>
<p class="ltx_p ltx_align_center" id="S4.T1.185.3"><span class="ltx_text" id="S4.T1.185.3.1" style="font-size:70%;background-color:#FFFFFF;"> <math alttext="\mathsection" class="ltx_Math" display="inline" id="S4.T1.185.3.1.m1.1"><semantics id="S4.T1.185.3.1.m1.1a"><mi id="S4.T1.185.3.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S4.T1.185.3.1.m1.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="S4.T1.185.3.1.m1.1b"><ci id="S4.T1.185.3.1.m1.1.1.cmml" xref="S4.T1.185.3.1.m1.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.185.3.1.m1.1c">\mathsection</annotation><annotation encoding="application/x-llamapun" id="S4.T1.185.3.1.m1.1d">§</annotation></semantics></math> Generator</span></p>
<p class="ltx_p ltx_align_center" id="S4.T1.186.4"><math alttext="\|" class="ltx_Math" display="inline" id="S4.T1.186.4.m1.1" style="background-color:#FFFFFF;"><semantics id="S4.T1.186.4.m1.1a"><mo id="S4.T1.186.4.m1.1.1" mathbackground="#FFFFFF" mathsize="70%" xref="S4.T1.186.4.m1.1.1.cmml">∥</mo><annotation-xml encoding="MathML-Content" id="S4.T1.186.4.m1.1b"><ci id="S4.T1.186.4.m1.1.1.cmml" xref="S4.T1.186.4.m1.1.1">∥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.186.4.m1.1c">\|</annotation><annotation encoding="application/x-llamapun" id="S4.T1.186.4.m1.1d">∥</annotation></semantics></math><span class="ltx_text" id="S4.T1.186.4.1" style="font-size:70%;background-color:#FFFFFF;"> Output</span></p>
<p class="ltx_p ltx_align_center" id="S4.T1.187.5"><math alttext="\mathparagraph" class="ltx_Math" display="inline" id="S4.T1.187.5.m1.1" style="background-color:#FFFFFF;"><semantics id="S4.T1.187.5.m1.1a"><mi id="S4.T1.187.5.m1.1.1" mathbackground="#FFFFFF" mathsize="70%" mathvariant="normal" xref="S4.T1.187.5.m1.1.1.cmml">¶</mi><annotation-xml encoding="MathML-Content" id="S4.T1.187.5.m1.1b"><ci id="S4.T1.187.5.m1.1.1.cmml" xref="S4.T1.187.5.m1.1.1">¶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.187.5.m1.1c">\mathparagraph</annotation><annotation encoding="application/x-llamapun" id="S4.T1.187.5.m1.1d">¶</annotation></semantics></math><span class="ltx_text" id="S4.T1.187.5.1" style="font-size:70%;background-color:#FFFFFF;"> Pipeline</span></p>
</div>
</div>
</div>
</figure>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we focus on RAG applications spanning various modalities. To echo with the taxonomy of RAG foundations and enhancements, we also demonstrate their utilization across different tasks in Table <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.T1" title="TABLE I ‣ IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_tag">I</span></a>.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.5.1.1">IV-A</span> </span><span class="ltx_text ltx_font_bold" id="S4.SS1.6.2">RAG for Text</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">To begin with, text generation is among the most important and widely deployed applications for RAG. Here we introduce popular works for seven tasks, respectively.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS1.5.1.1">IV-A</span>1 </span><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.6.2">Question Answering</span>
</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">Question answering involves the process of providing responses to posed questions by drawing from a vast and comprehensive collection of textual sources.
FiD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib35" title="">35</a>]</cite> and REALM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib33" title="">33</a>]</cite> identify the top-k most pertinent article snippets based on the query and forward each snippet along with the question to LLMs to generate k responses. These responses are then synthesized into a final answer.
Toutanova et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib190" title="">190</a>]</cite> substituted the text corpus in REALM with subgraphs from a knowledge graph, yielding impressive results.
As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#S4.F5" title="Figure 5 ‣ IV-A1 Question Answering ‣ IV-A RAG for Text ‣ IV Applications ‣ Retrieval-Augmented Generation for AI-Generated Content: A Survey"><span class="ltx_text ltx_ref_tag">5</span></a>, RETRO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib36" title="">36</a>]</cite> employs attention mechanisms to integrate the question with relevant retrieved documents within the model to produce the final answer.
SKR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib183" title="">183</a>]</cite> observes that using RAG does not invariably benefit question answering and thus explored guiding the model to evaluate its grasp of pertinent knowledge, subsequently adapting its use of external resources for retrieval enhancement.
TOG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib191" title="">191</a>]</cite> introduces an innovative knowledge graph-augmented LLM framework, which excels by fostering interactions between LLMs and the knowledge graph and by expanding the inference path space with beam search.
NPM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib119" title="">119</a>]</cite> pioneers the use of nonparametric data distributions in lieu of the softmax layer, enabling models with fewer parameters to perform effectively.
CL-ReLKT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib192" title="">192</a>]</cite> employs a language-generalized encoder to bridge the gap between question-document pairs across languages, thus better leveraging multilingual data. CORE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib193" title="">193</a>]</cite> mitigates language resource disparities by introducing a novel dense passage retrieval algorithm and a multilingual autoregressive generation model. Lastly, EAE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib113" title="">113</a>]</cite> enhances answer quality by retrieving entity embeddings for query entities and integrating these with hidden states for further processing.</p>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="336" id="S4.F5.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S4.F5.3.2" style="font-size:90%;">Architecture of RETRO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib36" title="">36</a>]</cite> model.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.SSS1.p2">
<p class="ltx_p" id="S4.SS1.SSS1.p2.1">UR-QA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib194" title="">194</a>]</cite> proposes to simultaneously retrieve QA pairs and text chunks, selecting the final answer by comparing their calibrated confidences.
DISC-LawLLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib195" title="">195</a>]</cite> constructs a supervised fine-tuning dataset through a legal syllogism prompting strategy, enabling the model to receive support from the latest legal information.
RAG-end2end <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib196" title="">196</a>]</cite> conducts simultaneous training of the retriever (DPR) and the generator (BART) to optimize performance for the end-to-end question-answering task and to facilitate domain adaptation.
MultiHop-RAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib197" title="">197</a>]</cite> extracts and aggregates information from distinct documents, providing the generator with the necessary context for definitive query answers.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS2.5.1.1">IV-A</span>2 </span><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.6.2">Fact Verification</span>
</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">Fact verification typically refers to determining whether a given natural language text and a related claim or assertion match the facts in the text.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p2">
<p class="ltx_p" id="S4.SS1.SSS2.p2.1">CONCRETE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib198" title="">198</a>]</cite> leverages cross-lingual retrieval mechanisms to tap into a wealth of multilingual evidence, effectively bridging the gap in resources for languages that are underrepresented in fact-checking datasets.
Atlas <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib30" title="">30</a>]</cite> shows that using RAG to support LLMs in knowledge-intensive tasks markedly improves their few-shot learning performance.
Hagström et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib199" title="">199</a>]</cite> proved on LLaMA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib4" title="">4</a>]</cite> and Atlas <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib30" title="">30</a>]</cite> that search augmentation is more beneficial for solving inconsistency problems than increasing model size.
Stochastic RAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib200" title="">200</a>]</cite> employs stochastic sampling without replacement to address the non-differentiable topk selection process in RAG retrieval, enabling end-to-end optimization and achieving excellent results in fact verification scenarios.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS3.5.1.1">IV-A</span>3 </span><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.6.2">Commonsense Reasoning</span>
</h4>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">Commonsense reasoning entails the capability of machines to infer or make decisions on problems or tasks in a human-like manner, drawing upon their acquired external knowledge and its application.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS3.p2">
<p class="ltx_p" id="S4.SS1.SSS3.p2.1">KG-BART <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib201" title="">201</a>]</cite> expands the conceptual landscape by incorporating intricate interrelations among diverse concepts within a knowledge graph. It employs graph attention mechanisms to aid LLMs in crafting more nuanced and logically coherent sentences.
Wan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib202" title="">202</a>]</cite> constructed the CONFLICTINGQA dataset with contentious questions and conflicting answers to study how textual features affect LMs’ handling of controversial issues.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS4.5.1.1">IV-A</span>4 </span><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.6.2">Human-Machine Conversation</span>
</h4>
<div class="ltx_para" id="S4.SS1.SSS4.p1">
<p class="ltx_p" id="S4.SS1.SSS4.p1.1">Human-machine conversation encompasses the ability of machines to comprehend natural language and adeptly employ this skill to engage with humans seamlessly.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS4.p2">
<p class="ltx_p" id="S4.SS1.SSS4.p2.1">ConceptFlow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib203" title="">203</a>]</cite> leverages a commonsense knowledge graph to structure conversations, directing the flow of dialogue based on attention scores, and propelling the conversation forward.
Cai et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib204" title="">204</a>]</cite> reimagined the text generation task as a cloze test by retrieving and distilling the essence of past conversational history, leading to notable outcomes. Komeili et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib205" title="">205</a>]</cite> augmented dialogue generation quality by harnessing advanced search engine technologies to source pertinent content from the internet. BlenderBot3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib206" title="">206</a>]</cite> broadens its search horizon, not only mining relevant internet content but also local dialogue history, and employs entity extraction among other techniques to refine the quality of the resulting dialogue. Kim et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib207" title="">207</a>]</cite>, PARC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib208" title="">208</a>]</cite>, and CREA-ICL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib209" title="">209</a>]</cite> improve the caliber of non-English conversations by incorporating cross-lingual knowledge, effectively addressing the scarcity of non-English datasets and enhancing the quality of the generated dialogue.
CEG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib210" title="">210</a>]</cite> addresses hallucination issues through a post-processing mechanism, verifying LLM-generated answers through retrieval.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS5.5.1.1">IV-A</span>5 </span><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS5.6.2">Neural Machine Translation</span>
</h4>
<div class="ltx_para" id="S4.SS1.SSS5.p1">
<p class="ltx_p" id="S4.SS1.SSS5.p1.1">Neural Machine Translation (NMT) is the automated process of translating text from a source language to a target language <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib118" title="">118</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib211" title="">211</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib212" title="">212</a>]</cite>. It is a pivotal task in the domain of NLP and represents a significant objective in the pursuit of AI, boasting considerable scientific and practical significance.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS5.p2">
<p class="ltx_p" id="S4.SS1.SSS5.p2.1">Cai et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib211" title="">211</a>]</cite> proposed an innovative approach that utilizes monolingual corpora alongside multilingual learning techniques, challenging the traditional dependency on bilingual corpora in Neural Machine Translation.
kNN-MT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib212" title="">212</a>]</cite> executes translation tasks at the token level by computing vector space distances.
TRIME <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib118" title="">118</a>]</cite> effectively minimizes the discrepancy between training and inference phases by jointly training the retrieval system and the generation model, thereby enhancing the precision of translations.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS6.5.1.1">IV-A</span>6 </span><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS6.6.2">Event Extraction</span>
</h4>
<div class="ltx_para" id="S4.SS1.SSS6.p1">
<p class="ltx_p" id="S4.SS1.SSS6.p1.1">Event extraction is a process in NLP that involves identifying and categorizing specific events within a text and associating them with relevant entities. These events are usually represented by verbs and the entities are the participants involved in the event.
R-GQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib213" title="">213</a>]</cite> enhances the context of a given issue by identifying and utilizing the most closely aligned Question-Answer pair from a repository, thereby enriching the information available for processing the current query.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS7">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS7.5.1.1">IV-A</span>7 </span><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS7.6.2">Summarization</span>
</h4>
<div class="ltx_para" id="S4.SS1.SSS7.p1">
<p class="ltx_p" id="S4.SS1.SSS7.p1.1">Summarization is a task aimed at distilling the essential information from lengthy texts and producing a concise, coherent summary that encapsulates the primary themes. There are two main approaches to summarization: extractive and abstractive.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS7.p2">
<p class="ltx_p" id="S4.SS1.SSS7.p2.1">Extractive summarization involves the automatic selection and compilation of key phrases directly from the source text, which refrains from creating new sentences, instead repurposing segments from the original text.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS7.p3">
<p class="ltx_p" id="S4.SS1.SSS7.p3.1">Abstractive summarization, on the other hand, entails comprehending the original text’s meaning and reformulating it into new sentences <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib214" title="">214</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib96" title="">96</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib215" title="">215</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib216" title="">216</a>]</cite>, which can convey the source’s intent more fluidly but poses greater challenges in terms of implementation due to its complexity.
RAMKG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib214" title="">214</a>]</cite> effectively leverages a comprehensive English corpus to bolster the performance of keyphrase generation in non-English contexts.
Unlimiformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib96" title="">96</a>]</cite> addresses the issue of input length constraints in transformer-based models by retrieving and utilizing the top-k most relevant hidden states, thereby extending the model’s capacity to handle longer inputs.
RPRR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib215" title="">215</a>]</cite> employs a Retrieve-Plan-Retrieve-Read approach to overcome the limited context window constraints faced by LLMs, utilizing retrieved information to generate high-quality Wikipedia documents for emerging events.
RIGHT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib216" title="">216</a>]</cite> chooses to use different types of retrievers in different datasets to enhance the generator.
M-RAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib217" title="">217</a>]</cite> significantly enhances text summarization by segmenting documents into various databases and incorporating multi-agent reinforcement learning techniques.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.5.1.1">IV-B</span> </span><span class="ltx_text ltx_font_bold" id="S4.SS2.6.2">RAG for Code</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Separate retrieval and generation approaches have historically been employed for code-related tasks.
For retrieval, similar code snippets can be identified using Abstract Syntax Trees (AST) or text edit distance.
For generation, sequence-to-sequence models are employed to generate code or natural language.
Recent RAG research combines both retrieval and generation techniques to enhance the overall performance.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS2.SSS1.5.1.1">IV-B</span>1 </span><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.6.2">Code Generation</span>
</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">Code generation aims to convert Natural Language (NL) descriptions into code implementations.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p2">
<p class="ltx_p" id="S4.SS2.SSS1.p2.1">Query-based RAG is a common method for code generation.
It builds prompts for transformer-based generative models with retrieved information, including similar examples <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib218" title="">218</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib219" title="">219</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib220" title="">220</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib152" title="">152</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib221" title="">221</a>]</cite>, relevant API details <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib88" title="">88</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib222" title="">222</a>]</cite>, documentations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib42" title="">42</a>]</cite>, imports <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib223" title="">223</a>]</cite>, and global functions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib224" title="">224</a>]</cite>.
SKCODER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib225" title="">225</a>]</cite> retrieves relevant code snippets to produce sketch template for final code generation.
RRGCode <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib226" title="">226</a>]</cite> employs a cross-encoder to rank the retrieval results.
CODEAGENT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib227" title="">227</a>]</cite> designs agents for web search, documentation retrieval, program generation, and correctness testing.
ARKS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib228" title="">228</a>]</cite> incorporates iterative RAG to re-formulate queries and update retrieval sources.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p3">
<p class="ltx_p" id="S4.SS2.SSS1.p3.1">Logit-based RAG is also applicable for code generation.
RECODE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib79" title="">79</a>]</cite> retrieves NL descriptions and paired codes using edit distance, then extracts n-gram action subtrees from ASTs. During LSTM-based generation, the processed subtrees are leveraged through logits at each decoding step.
kNN-TRANX <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib120" title="">120</a>]</cite> uses a seq2tree model to convert NL to code AST.
During each decoding step, hidden states are searched in the AST prefix datastore to create new probabilities, later merged with the seq2tree model’s output via a confidence network.
</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p4">
<p class="ltx_p" id="S4.SS2.SSS1.p4.1">ToolCoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib229" title="">229</a>]</cite> generates codes containing special tokens. When it encounters these tokens, ToolCoder performs online search or offline retrievals to fill in the blanks with API calls, which is a specialized form of speculative RAG.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS2.SSS2.5.1.1">IV-B</span>2 </span><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS2.6.2">Code Summarization</span>
</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">Code summarization tasks in turn convert the code into NL descriptions.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p2">
<p class="ltx_p" id="S4.SS2.SSS2.p2.1">Many research works process retrieval results using additional encoders and then combine them for subsequent decoder, which is similar to the Fusion-in-Decoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib35" title="">35</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="271" id="S4.F6.g1" src="x6.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.2.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S4.F6.3.2" style="font-size:90%;">Architecture of Re2Com <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib101" title="">101</a>]</cite> model.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.SSS2.p3">
<p class="ltx_p" id="S4.SS2.SSS2.p3.1">Re2Com <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib101" title="">101</a>]</cite> and EditSum <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib98" title="">98</a>]</cite> retrieve similar codes using BM25 and generate summary using LSTM.
They separately encode the input, the retrieved code, and the corresponding summary, then combine the hidden states or logits in the decoder.
HGNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib230" title="">230</a>]</cite> instead uses code edit distance for retrieval, and substitutes the code encoder with hybrid GNN on their Code Property Graphs (CPG) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib231" title="">231</a>]</cite>.
RACE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib102" title="">102</a>]</cite> employs separate encoders for the input code difference, the retrieved code differences through dense retrieval, and corresponding commit message to generate the final commit messages.
BASHEXPLAINER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib99" title="">99</a>]</cite> applies dense retrieval, and fuses the embeddings for subsequent transformer-based decoder.
READSUM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib232" title="">232</a>]</cite> uses Levenshtein distance for retrieval, and employs a fusion network to combine the representations of retrieved codes and summaries.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p4">
<p class="ltx_p" id="S4.SS2.SSS2.p4.1">Query-based RAG is prevalent for code summary generation.
REDCODER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib40" title="">40</a>]</cite>, ASAP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib171" title="">171</a>]</cite>, and SCCLLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib233" title="">233</a>]</cite> all form prompts with retrieved contents for summarization. They employ dense retrieval, sparse retrieval, and hybrid retrieval (including semantic, syntactic, and lexical-based retrieval), respectively.
The paradigm is also leveraged for pseudocode generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib234" title="">234</a>]</cite> and log statement generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib235" title="">235</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p5">
<p class="ltx_p" id="S4.SS2.SSS2.p5.1">Logit-based RAG also prevails in code summarization.
Rencos <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib80" title="">80</a>]</cite> and CoRec <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib236" title="">236</a>]</cite> retrieve similar code snippets or code differences through AST or dense representations. They both adopt multiple LSTMs for the input and the retrieved results, and the probabilities are combined for final generation.
kNN-Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib237" title="">237</a>]</cite> uses a transformer-based generator to obtain context vectors of input codes, then combines three parts of logits from vector search, the generator, and the copy mechanism for rare tokens in the input.
Tram <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib238" title="">238</a>]</cite> also combines three sets of logits from the original generator, the generator for sentence-level retrieved results, and the search logits of the token-level vectors (which represent the source codes and their ASTs).
CMR-Sum <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib239" title="">239</a>]</cite> incorporates the cross-attention probabilities between the retrieved summary and the generated summary, to the original generation logits.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS2.SSS3.5.1.1">IV-B</span>3 </span><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS3.6.2">Code Completion</span>
</h4>
<div class="ltx_para" id="S4.SS2.SSS3.p1">
<p class="ltx_p" id="S4.SS2.SSS3.p1.1">Code completion is akin to the code version of the “next sentence prediction” task.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p2">
<p class="ltx_p" id="S4.SS2.SSS3.p2.1">Query-based RAG is the mainstream paradigm for code completion.
Drain et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib240" title="">240</a>]</cite> retrieved template functions for function completion.
ReACC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib91" title="">91</a>]</cite> uses both sparse and dense retrieval.
RepoCoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib186" title="">186</a>]</cite> performs iterative RAG by augmenting the retrieval input with previously generated code.
De-Hallucinator <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib241" title="">241</a>]</cite> retrieves API references using first-time generated contents, then conducts query-based RAG for improved code completion.
REPOFUSE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib242" title="">242</a>]</cite> includes rationale context and retrieved codes to form prompt, and ranks the contexts to fit in the length limit.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p3">
<p class="ltx_p" id="S4.SS2.SSS3.p3.1">Many works leverage latent representation-based RAG.
Retrieve-and-edit <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib100" title="">100</a>]</cite>, RepoFusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib243" title="">243</a>]</cite>, and EDITAS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib244" title="">244</a>]</cite> employ multiple encoders for retrieved contents or edit sequences, then fuse the information for subsequent decoder.
CoCoMic <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib245" title="">245</a>]</cite> retrieves codes on the project context graph of the whole code project. It jointly processes the representations of source codes and retrieved contexts in the generator.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p4">
<p class="ltx_p" id="S4.SS2.SSS3.p4.1">kNM-LM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib246" title="">246</a>]</cite> performs logit-based RAG, combining the logits of retrieval and generation using bayes inference.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS2.SSS4.5.1.1">IV-B</span>4 </span><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS4.6.2">Automatic Program Repair</span>
</h4>
<div class="ltx_para" id="S4.SS2.SSS4.p1">
<p class="ltx_p" id="S4.SS2.SSS4.p1.1">Query-based RAG is often used in automatic program repair to help generative models fix buggy codes.
RING <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib177" title="">177</a>]</cite>, CEDAR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib89" title="">89</a>]</cite>, and RAP-Gen <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib144" title="">144</a>]</cite> all use hybrid retrieval (including both sparse and dense retrieval) for similar error messages, buggy codes, or fixes to build prompts.
InferFix <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib90" title="">90</a>]</cite> includes the bug type, the location, relevant syntax hierarchies, and similar fixes into the prompt.
SARGAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib176" title="">176</a>]</cite> utilizes prompts with similar buggy codes to generate patches; then another model is employed to refine the final result.
RTLFixer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib247" title="">247</a>]</cite> leverages ReAct <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib132" title="">132</a>]</cite> to implement an agent fixing errors in Verilog codes. It iteratively retrieves errors and paired solutions, and combines reasoning and action planning into prompts for LLMs.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS2.SSS5.5.1.1">IV-B</span>5 </span><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS5.6.2">Text-to-SQL and Code-based Semantic Parsing</span>
</h4>
<div class="ltx_para" id="S4.SS2.SSS5.p1">
<p class="ltx_p" id="S4.SS2.SSS5.p1.1">Semantic parsing converts NL into clear, structured representations, like SQL or other domain-specific languages, often with the assistance of codes.
All related works that employ RAG specifically utilize its query-based variant.
XRICL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib153" title="">153</a>]</cite> searches and reranks English utterance using non-English ones, then builds prompt to generate SQL queries.
SYNCHROMESH <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib81" title="">81</a>]</cite> retrieves similar NL and SQL to build prompts, then conducts constrained semantic decoding to enforce rich syntactic and semantic constraints during SQL generation.
CodeICL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib248" title="">248</a>]</cite> uses Python for semantic parsing, leveraging BM25 to incorporate similar training examples into prompts.
RESDSQL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib249" title="">249</a>]</cite> includes ranked schemas into prompts to generate SQL skeleton and SQL query.
ReFSQL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib250" title="">250</a>]</cite> uses a structure-enhanced retriever with schema linking and Mahalanobis contrastive learning, which helps to make better text-to-SQL generation.
To build prompts for SQL generation, ODIS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib251" title="">251</a>]</cite> retrieves both in-domain and out-of-domain demonstrations, while Nan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib252" title="">252</a>]</cite> retrieved both similar and diverse demonstrations.
MURRE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib253" title="">253</a>]</cite> conducts multi-hop retrieve-rewrite on tables to generate tabularized question, then ranks the results for prompt construction.
CodeS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib254" title="">254</a>]</cite> retrieves relevant information from table databases in a coarse-to-fine manner to generate SQL.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS2.SSS6.5.1.1">IV-B</span>6 </span><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS6.6.2">Others</span>
</h4>
<div class="ltx_para" id="S4.SS2.SSS6.p1">
<p class="ltx_p" id="S4.SS2.SSS6.p1.1">There are several other code-related tasks that adopt query-based RAG paradigm, incorporating similar examples to construct prompts.
Jie et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib255" title="">255</a>]</cite> used programs as the intermediate step in numerical reasoning.
De-fine <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib256" title="">256</a>]</cite> uses programs to solve complex tasks. It refines the answer generated by query-based RAG, then adds the refined programs back to the retrieval source.
For program static analysis, E&amp;V <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib257" title="">257</a>]</cite> leverages an LLM agent to form intermediate results with AST-based source code retrieval, pseudo-code execution, execution specifications verification, and other tools.
Code4UIE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib258" title="">258</a>]</cite> performs information extraction through code representation.
StackSpotAI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib259" title="">259</a>]</cite> builds an AI coding assistant with an RAG component.
InputBlaster <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib260" title="">260</a>]</cite> generates unusual text input that could cause mobile app crash.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.5.1.1">IV-C</span> </span><span class="ltx_text ltx_font_bold" id="S4.SS3.6.2">RAG for Knowledge</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Structured knowledge, including KGs (Knowledge Graph) and tables, is widely used in language-related tasks. It usually serves as the retrieval source to augment generation.
In addition to regular sparse and dense retrieval, NER (Named-Entity Recognition) technique and graph-aware neighbor retrieval are applied to identify and extract relevant entities and relations.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS3.SSS1.5.1.1">IV-C</span>1 </span><span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.6.2">Knowledge Base Question Answering</span>
</h4>
<div class="ltx_para" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.1">KBQA (knowledge base question answering) typically utilizes a knowledge base to determine the correct answer to a question. Many semantic parsing methods have been proposed, generating logical forms (e.g. SPARQL) based on the question.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS1.p2">
<p class="ltx_p" id="S4.SS3.SSS1.p2.1">Query-based RAG is the mainstream approach.
Unseen Entity Handling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib53" title="">53</a>]</cite> uses FreeBase <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib261" title="">261</a>]</cite> to retrieve topic entities, which are combined with query to generate SPARQL output.
CBR-KBQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib54" title="">54</a>]</cite> combines the query and the retrieved (query, logical form) pairs for generation. It also revises the final result to align with the relations present in the knowledge graph.
GMT-KBQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib52" title="">52</a>]</cite> re-ranks the retrieved entities and relations, and conducts relation classification and entity disambiguation before generation.
RNG-KBQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib82" title="">82</a>]</cite>, TIARA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib83" title="">83</a>]</cite>, BLLM augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib262" title="">262</a>]</cite>, and Shu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib263" title="">263</a>]</cite> re-rank the candidate logical forms or entities from the knowledge graph for prompt construction.
Uni-Parser <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib92" title="">92</a>]</cite> includes entities from mention detection, 2-hop paths extraction, and tables from databases into generator input.
ECBRF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib93" title="">93</a>]</cite> follows the case-based reasoning paradigm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib264" title="">264</a>]</cite>, retrieving similar triplet to build prompt input.
FC-KBQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib265" title="">265</a>]</cite> extracts relevant classes, relations, and entities from BM25 or mention detection, StructGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib266" title="">266</a>]</cite> extracts relevant triplets and nearest entities, and KAPING <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib267" title="">267</a>]</cite> extracts relevant facts through entity matching.
Sen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib268" title="">268</a>]</cite> replaced the retrieval with a relation distribution generation model for weighted triplets.
Retrieve-Rewrite-Answer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib269" title="">269</a>]</cite> retrieves subgraphs into prompts using hop prediction, relation path prediction, and triplet sampling.
Keqing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib270" title="">270</a>]</cite> decomposes a complex question into simple sub-questions through LLM, then retrieves sub-question templates and extract candidate entities from knowledge graph, and finally generates the answer through ChatGPT.
Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib271" title="">271</a>]</cite> leveraged retrieved pairs to explore the capability of formal language understanding and generation.
Interactive-KBQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib272" title="">272</a>]</cite> employs the LLM as an agent, which conducts entity-linking on KG and generates current thought and action until obtaining the final answer.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS1.p3">
<p class="ltx_p" id="S4.SS3.SSS1.p3.1">Latent representation-based RAG is also employed for KBQA.
ReTraCk <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib273" title="">273</a>]</cite> retrieves entities and schemas through mention detection and dense retrieval. It generates logical forms using LSTM, using retrieved items through knowledge-specific rules.
SKP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib110" title="">110</a>]</cite>, DECAF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib109" title="">109</a>]</cite>, and KD-CoT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib111" title="">111</a>]</cite> all retrieve triplets and conduct fusion-in-decoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib35" title="">35</a>]</cite> RAG.
KD-CoT also follows a chain-of-thought paradigm, iteratively performing retrieval, generation, and verification.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS3.SSS2.5.1.1">IV-C</span>2 </span><span class="ltx_text ltx_font_bold" id="S4.SS3.SSS2.6.2">Knowledge-augmented Open-domain Question Answering</span>
</h4>
<div class="ltx_para" id="S4.SS3.SSS2.p1">
<p class="ltx_p" id="S4.SS3.SSS2.p1.1">Structured knowledge is often leveraged to augment ODQA (open-domain question answering).</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS2.p2">
<p class="ltx_p" id="S4.SS3.SSS2.p2.1">Latent representation-based RAG, especially the fusion-in-decoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib35" title="">35</a>]</cite> technique, is prevalent for knowledge-augmented ODQA.
UniK-QA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib108" title="">108</a>]</cite>, KG-FiD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib274" title="">274</a>]</cite>, GRAPE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib275" title="">275</a>]</cite> all apply the fusion-in-decoder technique. They incorporate triplet-based documents, re-ranked documents through KG, and bipartite graph for pairs of question and passage, respectively.</p>
</div>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="488" id="S4.F7.g1" src="x7.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F7.2.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text" id="S4.F7.3.2" style="font-size:90%;">Architecture of KG-FiD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib274" title="">274</a>]</cite> model.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.SSS2.p3">
<p class="ltx_p" id="S4.SS3.SSS2.p3.1">OREOLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib276" title="">276</a>]</cite> empowers LLM with knowledge reasoning paths, integrating the entity value memory derived from contextualized random walk paths on KG into the hidden states of the LLM.
SKURG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib277" title="">277</a>]</cite> performs iterative retrieval and generation, using cross-attention to incorporate data sources into the input embedding. It uses a gate score to determine whether to re-start retrieval or to generate the real answer.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS2.p4">
<p class="ltx_p" id="S4.SS3.SSS2.p4.1">With the rapid development of LLMs, query-based RAG is emerging as a new standard.
DIVKNOWQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib278" title="">278</a>]</cite> retrieves from multiple sources using different techniques. It iteratively retrieves and re-ranks the data before generating the final answer.
KnowledGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib279" title="">279</a>]</cite> uses generated code to retrieve from both public and personal knowledge bases.
EFSUM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib280" title="">280</a>]</cite> optimizes the evidence-focused summary after facts-augmented generation, so as to align the QA-specific preference for helpfulness and faithfulness.
GenTKGQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib281" title="">281</a>]</cite> employs GNN (graph neural network) to integrate structural and temporal information from subgraph retrieval into virtual token representations.
KnowledgeNavigator <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib282" title="">282</a>]</cite> performs retrieval on KG through iterative filtering of relations with respect to core entities, so as to obtain relevant triplets.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS2.p5">
<p class="ltx_p" id="S4.SS3.SSS2.p5.1">GNN-RAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib283" title="">283</a>]</cite> fuses LLMs’ language understanding with GNN’s reasoning prowess and employs a retrieval augmentation strategy to enhance KGQA performance.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS3.SSS3.5.1.1">IV-C</span>3 </span><span class="ltx_text ltx_font_bold" id="S4.SS3.SSS3.6.2">Table for Question Answering</span>
</h4>
<div class="ltx_para" id="S4.SS3.SSS3.p1">
<p class="ltx_p" id="S4.SS3.SSS3.p1.1">Tables, as another form of structured knowledge, also facilitates question answering.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS3.p2">
<p class="ltx_p" id="S4.SS3.SSS3.p2.1">Fusion-in-decoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib35" title="">35</a>]</cite> style RAG is often used for table QA.
EfficientQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib284" title="">284</a>]</cite>, a competition held in NeurIPS 2020, witnessed the proposal of numerous retrieval-reader systems that rely on textual and tabular data.
Dual Reader-Parser <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib285" title="">285</a>]</cite> and CORE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib286" title="">286</a>]</cite> both re-rank the retrieved textual and tabular data for generation.
Convinse <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib287" title="">287</a>]</cite> retrieves information from knowledge bases, tables, and texts after question understanding.
RINK <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib288" title="">288</a>]</cite> designs a set-level reader-inherited re-ranker to get the relevance score of table segments.
TAG-QA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib289" title="">289</a>]</cite> retrieves tables and texts through GNN (after table-to-graph conversion) and BM25, respectively.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS3.p3">
<p class="ltx_p" id="S4.SS3.SSS3.p3.1">Tables can be integrated into prompts for query-based RAG.
Both T-RAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib290" title="">290</a>]</cite> and OmniTab <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib291" title="">291</a>]</cite> concatenates the retrieved tables with the query to generate the answer.
CARP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib292" title="">292</a>]</cite> extracts hybrid chain of retrieved tables and passages for prompt construction.
StructGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib266" title="">266</a>]</cite> retrieves from multiple sources including KGs, tables, and databases.
cTBLS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib293" title="">293</a>]</cite> forms prompts with ranked tables after retrieval.
Min et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib294" title="">294</a>]</cite> integrated tabular data through table-to-text techniques, then experiments on both finetuning and RAG.
ERATTA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib295" title="">295</a>]</cite> generates SQL code to extract table information, integrating it into the prompt to minimize model hallucination.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS3.SSS4.5.1.1">IV-C</span>4 </span><span class="ltx_text ltx_font_bold" id="S4.SS3.SSS4.6.2">Others</span>
</h4>
<div class="ltx_para" id="S4.SS3.SSS4.p1">
<p class="ltx_p" id="S4.SS3.SSS4.p1.1">Prototype-KRG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib296" title="">296</a>]</cite> integrates retrieved knowledge facts and dialogue prototypes into a GRU model through both hidden states and logits.
SURGE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib297" title="">297</a>]</cite> combines relevant subgraphs into the input for dialogue generation.
RHO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib298" title="">298</a>]</cite> fuses KG embedding of relevant entities and relations into textual embeddings during dialogue generation.
K-LaMP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib299" title="">299</a>]</cite> retrieves entities in history queries to construct prompt for query suggestion.
ReSKGC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib112" title="">112</a>]</cite> retrieves relevant triplets to complete triplet using Fid.
G-Retriever <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib300" title="">300</a>]</cite> retrieves nodes and edges from textual graphs to construct subgraph and perform graph prompt tuning for QA.
Hussien et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib301" title="">301</a>]</cite> fuse the reasoning power of KG with the expressiveness of LLMs through RAG techniques.
HippoRAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib302" title="">302</a>]</cite> excels in multi-hop question answering by emulating mammalian brain knowledge storage with KG triples and employing a personalized PageRank algorithm for retrieval.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS4.5.1.1">IV-D</span> </span><span class="ltx_text ltx_font_bold" id="S4.SS4.6.2">RAG for Image</span>
</h3>
<section class="ltx_subsubsection" id="S4.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS4.SSS1.5.1.1">IV-D</span>1 </span><span class="ltx_text ltx_font_bold" id="S4.SS4.SSS1.6.2">Image Generation</span>
</h4>
<div class="ltx_para" id="S4.SS4.SSS1.p1">
<p class="ltx_p" id="S4.SS4.SSS1.p1.1">Image generation refers to the process of creating new images, typically using algorithms in the field of artificial intelligence and machine learning.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS1.p2">
<p class="ltx_p" id="S4.SS4.SSS1.p2.1">The retrieval process can not only help yield high-quality images even for rare or unseen subjects, but also reduces the parameter count and computational expense <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib95" title="">95</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib104" title="">104</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib105" title="">105</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib103" title="">103</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib303" title="">303</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib106" title="">106</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib107" title="">107</a>]</cite>.
For GAN-based model,
RetrieveGAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib45" title="">45</a>]</cite> uses a differentiable retriever for image patch selection, facilitating end-to-end training. IC-GAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib95" title="">95</a>]</cite> models data as conditional distributions around each training instance, conditioning both the generator and discriminator on these instances.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS1.p3">
<p class="ltx_p" id="S4.SS4.SSS1.p3.1">Recently, diffusion models beat GANs on image generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib304" title="">304</a>]</cite>.
KNN-Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib104" title="">104</a>]</cite> and RDM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib105" title="">105</a>]</cite> train diffusion models conditioned on CLIP embeddings and image neighbors, enabling post-hoc conditioning on labels, prompts, and zero-shot stylization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib106" title="">106</a>]</cite>.
Beyond only images, Re-imagen <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib103" title="">103</a>]</cite> extends retrieval to image-text pairs for text-to-image generation, with interleaved guidance to balance the alignment between prompts and retrieval conditions.
Retrieve&amp;Fuse <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib303" title="">303</a>]</cite> prevents information loss of CLIP embeddings by concatenating retrieved and noised images before each U-Net attention block, allowing fully interaction via self-attention.
RPG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib305" title="">305</a>]</cite> retrieves representative images to construct in-context examples, and utilizes chain-of-thought reasoning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib306" title="">306</a>]</cite> to plan out complementary subregions for compositional text-to-image diffusion.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS4.SSS2.5.1.1">IV-D</span>2 </span><span class="ltx_text ltx_font_bold" id="S4.SS4.SSS2.6.2">Image Captioning</span>
</h4>
<div class="ltx_para" id="S4.SS4.SSS2.p1">
<p class="ltx_p" id="S4.SS4.SSS2.p1.1">Image captioning is the process of generating a textual description of an image.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS2.p2">
<p class="ltx_p" id="S4.SS4.SSS2.p2.1">Retrieval-augmented image captioning typically synthesises description with a collection of retrieved captions.
MA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib121" title="">121</a>]</cite> augments via a memory bank, built with historical context and target word of image-text training set, and queried with inference context.
In adversarial training, RAMP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib307" title="">307</a>]</cite> takes retrieved captions as discriminator reference, and employs memory-augmented attention and copying mechanisms for better utilization of retrieved captions.</p>
</div>
<figure class="ltx_figure" id="S4.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="604" id="S4.F8.g1" src="x8.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F8.2.1.1" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text" id="S4.F8.3.2" style="font-size:90%;">Architecture of EXTRA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib308" title="">308</a>]</cite> model.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS4.SSS2.p3">
<p class="ltx_p" id="S4.SS4.SSS2.p3.1">The RA-Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib46" title="">46</a>]</cite> and EXTRA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib308" title="">308</a>]</cite>, both retrieval-augmented transformer-based captioning models, utilize cross-attention over encoded retrieved captions.
Beyond caption retrieval, REVEAL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib309" title="">309</a>]</cite> uniformly encodes and retrieves multi-modal world knowledge, integrated with retrieval score-aware attention.
Directly, SMALLCAP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib47" title="">47</a>]</cite> employs a CLIP vision encoder and a LLM decoder, with retrieved captions serving as input-specific in-context examples.
For remote sensing images, CRSR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib310" title="">310</a>]</cite> refines retrieved captions, filtering out misleading details and emphasizing visually salient content.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS4.SSS3.5.1.1">IV-D</span>3 </span><span class="ltx_text ltx_font_bold" id="S4.SS4.SSS3.6.2">Others</span>
</h4>
<div class="ltx_para" id="S4.SS4.SSS3.p1">
<p class="ltx_p" id="S4.SS4.SSS3.p1.1">There also exist many retrieval augmented works for other image-related tasks. For Visual Question Answering (VQA), PICa <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib311" title="">311</a>]</cite> converts images into textual descriptions, prompts GPT-3 and ensembles multi-query results. RA-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib312" title="">312</a>]</cite> enables an end-to-end training with differentiable retrieval for answer generation. For visually grounded dialogue, KIF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib313" title="">313</a>]</cite> and Maria <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib314" title="">314</a>]</cite> enhances dialog generation with external knowledge like visual experiences. In multi-modal machine translation, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib315" title="">315</a>]</cite> incorporates visual information at the phrase level to improve NMT with multi-modal information.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS5.5.1.1">IV-E</span> </span><span class="ltx_text ltx_font_bold" id="S4.SS5.6.2">RAG for Video</span>
</h3>
<section class="ltx_subsubsection" id="S4.SS5.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS5.SSS1.5.1.1">IV-E</span>1 </span><span class="ltx_text ltx_font_bold" id="S4.SS5.SSS1.6.2">Video Captioning</span>
</h4>
<div class="ltx_para" id="S4.SS5.SSS1.p1">
<p class="ltx_p" id="S4.SS5.SSS1.p1.1">Video captioning translates the visual content into descriptive utterances.
KaVD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib316" title="">316</a>]</cite> generates news video caption with background knowledge in related documents like named entities and events.
R-ConvED <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib48" title="">48</a>]</cite> retrieves relevant sentences and videos via Dual Encoding <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib70" title="">70</a>]</cite>, and predicts the target word with a convolutional encoder-decoder network.
CARE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib117" title="">117</a>]</cite> combines three modalities data, i.e. frame, audio, and retrieved texts, to provide both global and local semantic guidance as augmentation.
EgoInstructor <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib49" title="">49</a>]</cite> focuses on first-person videos, retrieves relevant exocentric videos and texts, and generates captions through LLM via cross-attention with encoded videos</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS5.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS5.SSS2.5.1.1">IV-E</span>2 </span><span class="ltx_text ltx_font_bold" id="S4.SS5.SSS2.6.2">Video QA&amp;Dialogue</span>
</h4>
<div class="ltx_para" id="S4.SS5.SSS2.p1">
<p class="ltx_p" id="S4.SS5.SSS2.p1.1">Video QA&amp;Dialogue generates single or multiple-round responses in alignment with video content.
For VideoQA, MA-DRNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib317" title="">317</a>]</cite> stores and retrieves useful information in queries and videos with external memory, therefore models the long-term visual-textual dependence. R2A <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib318" title="">318</a>]</cite> retrieves semantically similar texts by CLIP, and prompts LLM with both the query and the retrieved texts. For video dialogue, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib319" title="">319</a>]</cite> proposes TVQA+ dataset to enable relevant moments and visual concepts retrieval, and designs corresponding spatio-temporal-aware generator. VGNMN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib320" title="">320</a>]</cite> extracts visual cues from videos, while the retrieval process is parameterized by entities and actions in previous dialogues.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS5.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS5.SSS3.5.1.1">IV-E</span>3 </span><span class="ltx_text ltx_font_bold" id="S4.SS5.SSS3.6.2">Others</span>
</h4>
<div class="ltx_para" id="S4.SS5.SSS3.p1">
<p class="ltx_p" id="S4.SS5.SSS3.p1.1">RAG also works for other video-related tasks.
VidIL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib321" title="">321</a>]</cite> converts video content into temporal-aware LLM prompts for tasks like video captioning, question answering, and future event prediction.</p>
</div>
<figure class="ltx_figure" id="S4.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="968" id="S4.F9.g1" src="x9.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F9.2.1.1" style="font-size:90%;">Figure 9</span>: </span><span class="ltx_text" id="S4.F9.3.2" style="font-size:90%;">Architecture of Animate-A-Story <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib174" title="">174</a>]</cite> model.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS5.SSS3.p2">
<p class="ltx_p" id="S4.SS5.SSS3.p2.1">For trustworthy autonomous driving, RAG-Driver <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib322" title="">322</a>]</cite> grounds the MLLM in retrieved expert demonstrations, to produce driving action explanations.
Animate-A-Story <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib174" title="">174</a>]</cite> simplifies text-to-video generation by dividing it into plot-based video augmentation and video-diffusion generation conditioned on text and video inputs.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS6.5.1.1">IV-F</span> </span><span class="ltx_text ltx_font_bold" id="S4.SS6.6.2">RAG for Audio</span>
</h3>
<section class="ltx_subsubsection" id="S4.SS6.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS6.SSS1.5.1.1">IV-F</span>1 </span><span class="ltx_text ltx_font_bold" id="S4.SS6.SSS1.6.2">Audio Generation</span>
</h4>
<div class="ltx_para" id="S4.SS6.SSS1.p1">
<p class="ltx_p" id="S4.SS6.SSS1.p1.1">Audio generation usually synthesises audio with natural language prompt.</p>
</div>
<figure class="ltx_figure" id="S4.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="245" id="S4.F10.g1" src="x10.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F10.2.1.1" style="font-size:90%;">Figure 10</span>: </span><span class="ltx_text" id="S4.F10.3.2" style="font-size:90%;">Architecture of Re-AudioLDM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib116" title="">116</a>]</cite> model.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS6.SSS1.p2">
<p class="ltx_p" id="S4.SS6.SSS1.p2.1">Given input prompt, Re-AudioLDM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib116" title="">116</a>]</cite> retrieves relevant caption-audio pairs with dense retriever CLAP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib26" title="">26</a>]</cite> for generation.
Make-An-Audio <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib44" title="">44</a>]</cite> retrieves audios given text prompt, then constructs pseudo prompts for text-to-audio diffusion model training.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS6.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS6.SSS2.5.1.1">IV-F</span>2 </span><span class="ltx_text ltx_font_bold" id="S4.SS6.SSS2.6.2">Audio Captioning</span>
</h4>
<div class="ltx_para" id="S4.SS6.SSS2.p1">
<p class="ltx_p" id="S4.SS6.SSS2.p1.1">Audio captioning, basically a sequence-to-sequence task, generates natural language data for audio data.
RECAP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib323" title="">323</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib43" title="">43</a>]</cite> leverages dense retrievers, CLAP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib26" title="">26</a>]</cite> and VGGish <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib69" title="">69</a>]</cite> respectively, to retrieve related captions given audio data. For RECAP, captions are included into LLM prompts, while  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib43" title="">43</a>]</cite> uses both audio and retrieved captions in attention module.
Other research studies align audio modality with text to leverage advancements in LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib324" title="">324</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib325" title="">325</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib326" title="">326</a>]</cite> for various downstream text generation.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS7.5.1.1">IV-G</span> </span><span class="ltx_text ltx_font_bold" id="S4.SS7.6.2">RAG for 3D</span>
</h3>
<section class="ltx_subsubsection" id="S4.SS7.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS7.SSS1.5.1.1">IV-G</span>1 </span><span class="ltx_text ltx_font_bold" id="S4.SS7.SSS1.6.2">Text-to-3D</span>
</h4>
<div class="ltx_para" id="S4.SS7.SSS1.p1">
<p class="ltx_p" id="S4.SS7.SSS1.p1.1">Retrieval can be applied to augment 3D asset generation.</p>
</div>
<figure class="ltx_figure" id="S4.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="374" id="S4.F11.g1" src="x11.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F11.2.1.1" style="font-size:90%;">Figure 11</span>: </span><span class="ltx_text" id="S4.F11.3.2" style="font-size:90%;">Architecture of AMD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib115" title="">115</a>]</cite> model.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS7.SSS1.p2">
<p class="ltx_p" id="S4.SS7.SSS1.p2.1">ReMoDiffuse <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib51" title="">51</a>]</cite> retrieves relevant motion entities and generates motions using diffusion models, with the semantic-modulated attention and condition mixture guidance.
AMD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib115" title="">115</a>]</cite> designs and fuses two motion diffusion models. One branch conditions on the original prompt, while the other decomposes the prompt into anatomical scripts and retrieves similar motions.
RetDream <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib50" title="">50</a>]</cite> retrieves 3D assets to augment the variational score distillation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib327" title="">327</a>]</cite> of 2D diffusion models. These assets offer geometric and adapted 2D priors, which not only impose additional velocity on particles for initialization but also help optimize 2D diffusion models by LoRA.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS8">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS8.5.1.1">IV-H</span> </span><span class="ltx_text ltx_font_bold" id="S4.SS8.6.2">RAG for Science</span>
</h3>
<div class="ltx_para" id="S4.SS8.p1">
<p class="ltx_p" id="S4.SS8.p1.1">RAG has also emerged as a promising research direction for many interdisciplinary applications, such as molecular generation, medical tasks and computational research.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS8.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS8.SSS1.5.1.1">IV-H</span>1 </span><span class="ltx_text ltx_font_bold" id="S4.SS8.SSS1.6.2">Drug Discovery</span>
</h4>
<div class="ltx_para" id="S4.SS8.SSS1.p1">
<p class="ltx_p" id="S4.SS8.SSS1.p1.1">The goal of drug discovery is to generate molecules that concurrently fulfill diverse properties.</p>
</div>
<figure class="ltx_figure" id="S4.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="304" id="S4.F12.g1" src="x12.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F12.2.1.1" style="font-size:90%;">Figure 12</span>: </span><span class="ltx_text" id="S4.F12.3.2" style="font-size:90%;">Architecture of RetMol <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib55" title="">55</a>]</cite> model.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS8.SSS1.p2">
<p class="ltx_p" id="S4.SS8.SSS1.p2.1">RetMol <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib55" title="">55</a>]</cite> integrates a lightweight retrieval mechanism and molecular strings into a pre-trained encoder-decoder generative model to retrieve and fuse exemplar molecules with the input.
PromptDiff <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib328" title="">328</a>]</cite> introduces an interaction-based, retrieval-augmented 3D molecular diffusion model that retrieves a curated set of ligand references to guide the synthesis of ligands meeting specific design criteria.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS8.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS8.SSS2.5.1.1">IV-H</span>2 </span><span class="ltx_text ltx_font_bold" id="S4.SS8.SSS2.6.2">Biomedical Informatics Enhancement</span>
</h4>
<div class="ltx_para" id="S4.SS8.SSS2.p1">
<p class="ltx_p" id="S4.SS8.SSS2.p1.1">Several recent studies have improved the expressiveness of LLM by retrieving information from biomedical domain-specific databases, thereby augmenting the model’s capabilities to provide valuable guidance for tasks in the medical field.</p>
</div>
<div class="ltx_para" id="S4.SS8.SSS2.p2">
<p class="ltx_p" id="S4.SS8.SSS2.p2.1">PoET <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib329" title="">329</a>]</cite> is an autoregressive model using a transformer variant with a retrieval mechanism for prompt augmentation, speeding up the prediction of protein variant fitness properties.
Chat-Orthopedist <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib94" title="">94</a>]</cite> enhances ChatGPT with a retrieval-augmented mechanism focused on adolescent idiopathic scoliosis (AIS), utilizing an external knowledge base for precise responses.
BIOREADER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib330" title="">330</a>]</cite> is the first retrieval-enhanced text-to-text transformer-based model for biomedical natural language processing, incorporating the retrieved literature evidence into the model using a chunked-cross attention mechanism.
MedWriter <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib331" title="">331</a>]</cite> employs a hierarchical retrieval-augmented generation method that combines report-level and sentence-level templates to produce coherent and clinically accurate medical reports from images.
QA-RAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib332" title="">332</a>]</cite> employs a dual-track RAG strategy to enhance pharmaceutical compliance by effectively retrieving and integrating regulatory guidelines based on language model responses and user queries. RAG-RLRC-LaySum <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib333" title="">333</a>]</cite> leverages biomedical text knowledge for llms, employing reinforcement learning and re-ranking techniques to enhance content relevance and readability of the output.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS8.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS8.SSS3.5.1.1">IV-H</span>3 </span><span class="ltx_text ltx_font_bold" id="S4.SS8.SSS3.6.2">Math Applications</span>
</h4>
<div class="ltx_para" id="S4.SS8.SSS3.p1">
<p class="ltx_p" id="S4.SS8.SSS3.p1.1">Retrieval-augmented generation technology in mathematics streamlines problem-solving, boosts research innovation, and refines educational strategies.</p>
</div>
<div class="ltx_para" id="S4.SS8.SSS3.p2">
<p class="ltx_p" id="S4.SS8.SSS3.p2.1">LeanDojo <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib334" title="">334</a>]</cite> boosts theorem proving by using retrieval-augmented methods to choose relevant premises from extensive mathematical libraries, improving automation and theorem generalization.
RAG-for-math-QA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib335" title="">335</a>]</cite> improves math question-answering by integrating a high-quality math textbook with RAG, enhancing LLM-generated responses for middle-school algebra and geometry.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Benchmark</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Given the increasing research interests and applications of RAG, there have also been several benchmarks assessing RAG from certain aspects.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib336" title="">336</a>]</cite> proposed an RAG benchmark that evaluates across four dimensions:
(1) Noise Robustness, testing if LLMs can extract necessary information from noisy documents;
(2) Negative Rejection, assessing if LLMs can reject to respond when retrieved content is insufficient;
(3) Information Integration,checking if LLMs can acquire knowledge and respond by integrating multiple retrieved contents;
(4) Counterfactual Robustness, determining if LLMs can identify counterfactual errors in retrieved content.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">Three other benchmarks, RAGAS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib337" title="">337</a>]</cite>, ARES <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib338" title="">338</a>]</cite>, and TruLens <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib339" title="">339</a>]</cite>, evaluate three aspects using a separate evaluator LLM: (1) Faithfulness, assessing factual accuracy based on retrieved content; (2) Answer Relevance, determining if results address the queries; (3) Context Relevance, evaluating the relevance of retrieved content and its conciseness.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">CRUD-RAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib340" title="">340</a>]</cite> divides RAG tasks into four types: Create, Read, Update, and Delete, assessing them through text continuation, question answering, hallucination correction, and open-domain multi-document summary.
MIRAGE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib341" title="">341</a>]</cite> assesses RAG in the medical domain, focusing on the performance of medical question-answering systems.
KILT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib342" title="">342</a>]</cite> aligns Wikipedia snapshots to verify information accuracy, using BLEU scores to pinpoint relevant texts and filtering to uphold quality, thus providing diverse retrieval systems for evidence-backed predictions or citations.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Discussion</span>
</h2>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS1.5.1.1">VI-A</span> </span><span class="ltx_text ltx_font_bold" id="S6.SS1.6.2">Limitations</span>
</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">Despite the widespread adoption of RAG,
it suffers from several limitations by nature.</p>
</div>
<section class="ltx_subsubsection" id="S6.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S6.SS1.SSS1.5.1.1">VI-A</span>1 </span><span class="ltx_text ltx_font_bold" id="S6.SS1.SSS1.6.2">Noises in Retrieval Results</span>
</h4>
<div class="ltx_para" id="S6.SS1.SSS1.p1">
<p class="ltx_p" id="S6.SS1.SSS1.p1.1">Information retrieval is inherently flawed due to information loss in item representations and ANN search.
The inevitable noise, manifesting as irrelevant content or misleading information, can create failure points in RAG systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib343" title="">343</a>]</cite>.
However, although improving retrieval accuracy seems intuitive for RAG effectiveness, recent research surprisingly finds that noisy retrieval results might enhance generation quality <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib344" title="">344</a>]</cite>.
A possible explanation is that diverse retrieval outcomes could contribute to prompt construction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib345" title="">345</a>]</cite>.
Thus, the impact of retrieval noise remains unclear, leading to confusion about metric selection and retriever-generator interaction in practical uses.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S6.SS1.SSS2.5.1.1">VI-A</span>2 </span><span class="ltx_text ltx_font_bold" id="S6.SS1.SSS2.6.2">Extra Overhead</span>
</h4>
<div class="ltx_para" id="S6.SS1.SSS2.p1">
<p class="ltx_p" id="S6.SS1.SSS2.p1.1">While retrieval can reduce generation costs in certain cases <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib32" title="">32</a>]</cite>, it incurs non-negligible overhead in most cases.
In other words, the retrieval and interaction processes increase latency inevitably.
This is amplified when RAG is combined with complex enhancement methods, such as recursive retrieval <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib346" title="">346</a>]</cite> and iterative RAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib186" title="">186</a>]</cite>.
Furthermore, as the scale of retrieval sources expands, the storage and access complexity will also increase <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib347" title="">347</a>]</cite>.
Such overhead hampers the practicality of RAG in real-time services that are sensitive to latency.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S6.SS1.SSS3.5.1.1">VI-A</span>3 </span><span class="ltx_text ltx_font_bold" id="S6.SS1.SSS3.6.2">The Gap between Retrievers and Generators</span>
</h4>
<div class="ltx_para" id="S6.SS1.SSS3.p1">
<p class="ltx_p" id="S6.SS1.SSS3.p1.1">Since the objectives of retrievers and generators may not align, and their latent spaces might differ, designing their interaction requires meticulous design and optimization.
Current approaches either disentangle retrieval and generation or integrate them at an intermediate stage.
While the former is more modular, the latter could benefit from joint training but hamper
generality.
Selecting a cost-effective interaction method to bridge the gap poses a challenge and necessities deliberation in practice.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S6.SS1.SSS4.5.1.1">VI-A</span>4 </span><span class="ltx_text ltx_font_bold" id="S6.SS1.SSS4.6.2">Increased System Complexity</span>
</h4>
<div class="ltx_para" id="S6.SS1.SSS4.p1">
<p class="ltx_p" id="S6.SS1.SSS4.p1.1">The introduction of retrieval unavoidably increases the system complexity and the number of hyper-parameters to tune.
For instance, a recent study found that using top-<math alttext="k" class="ltx_Math" display="inline" id="S6.SS1.SSS4.p1.1.m1.1"><semantics id="S6.SS1.SSS4.p1.1.m1.1a"><mi id="S6.SS1.SSS4.p1.1.m1.1.1" xref="S6.SS1.SSS4.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS4.p1.1.m1.1b"><ci id="S6.SS1.SSS4.p1.1.m1.1.1.cmml" xref="S6.SS1.SSS4.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS4.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.SSS4.p1.1.m1.1d">italic_k</annotation></semantics></math> rather than a single retrieval improves attribution but harms fluency in query-based RAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib348" title="">348</a>]</cite>,
while other aspects such as metric selection are still under explored.
Thus, it requires more expertise to tune the generation service when RAG is involved.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S6.SS1.SSS5.5.1.1">VI-A</span>5 </span><span class="ltx_text ltx_font_bold" id="S6.SS1.SSS5.6.2">Lengthy Context</span>
</h4>
<div class="ltx_para" id="S6.SS1.SSS5.p1">
<p class="ltx_p" id="S6.SS1.SSS5.p1.1">One of the primary shortcomings of RAG, in particular the query-based RAG, is that it lengthens the context tremendously, making it infeasible for generators with limited context length.
In addition, the lengthened context also slows down the generation process generally.
The research advancements in prompt compression <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib169" title="">169</a>]</cite> and long-context support <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib349" title="">349</a>]</cite> have partially mitigated these challenges, albeit with a slight trade-off in accuracy or costs.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS2.5.1.1">VI-B</span> </span><span class="ltx_text ltx_font_bold" id="S6.SS2.6.2">Potential Future Directions</span>
</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">Lastly, we wish to outline several potential directions for future RAG research and applications.</p>
</div>
<section class="ltx_subsubsection" id="S6.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S6.SS2.SSS1.5.1.1">VI-B</span>1 </span><span class="ltx_text ltx_font_bold" id="S6.SS2.SSS1.6.2">Novel Design of Augmentation Methodologies</span>
</h4>
<div class="ltx_para" id="S6.SS2.SSS1.p1">
<p class="ltx_p" id="S6.SS2.SSS1.p1.1">Existing research has explored various interaction patterns between retrievers and generators.
However, due to distinct objectives in these two components, the practical augmentation process has a significant impact on the final generation results.
Investigation of more advanced foundations for augmentation holds promise for fully unleashing the potential of RAG.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S6.SS2.SSS2.5.1.1">VI-B</span>2 </span><span class="ltx_text ltx_font_bold" id="S6.SS2.SSS2.6.2">Flexible RAG Pipelines</span>
</h4>
<div class="ltx_para" id="S6.SS2.SSS2.p1">
<p class="ltx_p" id="S6.SS2.SSS2.p1.1">RAG systems are progressively embracing flexible pipelines, such as recursive, adaptive, and iterative RAG.
With precise tuning and meticulous engineering, the unique blend of retrieval sources, retrievers, generators, and RAG subsystems promises to tackle complex tasks and boost overall performance.
We eagerly anticipate pioneering exploration that will drive the evolution of even more innovative RAG systems.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S6.SS2.SSS3.5.1.1">VI-B</span>3 </span><span class="ltx_text ltx_font_bold" id="S6.SS2.SSS3.6.2">Broader Applications</span>
</h4>
<div class="ltx_para" id="S6.SS2.SSS3.p1">
<p class="ltx_p" id="S6.SS2.SSS3.p1.1">RAG is a general technique applied in various applications.
However, some generative tasks have not yet explored RAG, and in many domains, RAG is applied naively without considering the domain’s unique characteristics.
We believe designing domain-specific RAG techniques will significantly benefit broader applications.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S6.SS2.SSS4.5.1.1">VI-B</span>4 </span><span class="ltx_text ltx_font_bold" id="S6.SS2.SSS4.6.2">Efficient Deployment and Processing</span>
</h4>
<div class="ltx_para" id="S6.SS2.SSS4.p1">
<p class="ltx_p" id="S6.SS2.SSS4.p1.1">There exist several deployment solutions for query-based RAG with LLMs, such as LangChain <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib350" title="">350</a>]</cite>, LLAMA-Index <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib135" title="">135</a>]</cite>, and PipeRAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib351" title="">351</a>]</cite>.
However, for other RAG foundations and/or generation tasks, there lacks a plug-and-play solution.
Besides, due to retrieval overhead and increasing complexities in retrievers and generators, achieving efficient RAG is still challenging and necessitates further system-level optimizations.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S6.SS2.SSS5.5.1.1">VI-B</span>5 </span><span class="ltx_text ltx_font_bold" id="S6.SS2.SSS5.6.2">Incorporating Long-tail and Real-time Knowledge</span>
</h4>
<div class="ltx_para" id="S6.SS2.SSS5.p1">
<p class="ltx_p" id="S6.SS2.SSS5.p1.1">While a key motivation of RAG is to harness real-time and long-tail knowledge, few studies have explored the pipeline for knowledge updating and expansion.
Many existing works use merely the generators’ training data as retrieval sources, neglecting the dynamic and flexible information that retrieval could offer.
As a consequence, there is a growing research on designing RAG systems with continuously updated knowledge and flexible sources.
We also expect RAG to step further, adapting to personalized information in today’s web service.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S6.SS2.SSS6.5.1.1">VI-B</span>6 </span><span class="ltx_text ltx_font_bold" id="S6.SS2.SSS6.6.2">Combined with Other Techniques</span>
</h4>
<div class="ltx_para" id="S6.SS2.SSS6.p1">
<p class="ltx_p" id="S6.SS2.SSS6.p1.1">RAG is orthogonal to other techniques that also aim to improve AIGC effectiveness, such as fine-tuning, reinforcement learning, chain-of-thought, and agent-based generation.
The combining of these methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib352" title="">352</a>]</cite> is still in its early stages, calling for further research to fully exploit their potential through novel algorithm designs.
It is worthy to note that a recent notion appears “long-context models like Gemini 1.5 will replace RAG”.
Nevertheless, this assertion overlooks RAG’s flexibility in managing dynamic information, encompassing both up-to-date and long-tail knowledge <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.19473v6#bib.bib353" title="">353</a>]</cite>.
We expect RAG to benefit from long context generation, rather than being replaced by it.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this paper, we conducted a thorough and comprehensive survey on RAG within the context of AIGC, with a particular focus on augmentation foundations, enhancements, and applications.
We first systematically organized and summarized the foundation paradigms in RAG, providing insights into the interaction between retrievers and generators.
Then, we reviewed the enhancements that further improve the effectiveness of RAG, including the enhancements on each component or the entire pipeline.
To facilitate researchers across diverse domains, we showcased practical applications of RAG in a range of modalities and tasks.
Finally, we also presented existing benchmarks for RAG, discussed current limitations of RAG, and shed light on promising future directions.
</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
T. B. Brown, B. Mann <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">et al.</em>, “Language models are few-shot learners,” in <em class="ltx_emph ltx_font_italic" id="bib.bib1.2.2">NeurIPS</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
M. Chen, J. Tworek <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">et al.</em>, “Evaluating large language models trained on code,” <em class="ltx_emph ltx_font_italic" id="bib.bib2.2.2">arXiv:2107.03374</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
OpenAI, “GPT-4 technical report,” <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">arXiv:2303.08774</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
H. Touvron, T. Lavril <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">et al.</em>, “Llama: Open and efficient foundation language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib4.2.2">arXiv:2302.13971</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
H. Touvron, L. Martin <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">et al.</em>, “Llama 2: Open foundation and fine-tuned chat models,” <em class="ltx_emph ltx_font_italic" id="bib.bib5.2.2">arXiv:2307.09288</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
B. Rozière, J. Gehring <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">et al.</em>, “Code llama: Open foundation models for code,” <em class="ltx_emph ltx_font_italic" id="bib.bib6.2.2">arXiv:2308.12950</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
A. Ramesh, M. Pavlov, G. Goh <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">et al.</em>, “Zero-shot text-to-image generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib7.2.2">ICML</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
A. Ramesh, P. Dhariwal, A. Nichol <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">et al.</em>, “Hierarchical text-conditional image generation with CLIP latents,” <em class="ltx_emph ltx_font_italic" id="bib.bib8.2.2">arXiv:2204.06125</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
J. Betker, G. Goh, L. Jing <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">et al.</em>, “Improving image generation with better captions,” <em class="ltx_emph ltx_font_italic" id="bib.bib9.2.2">Computer Science</em>, vol. 2, no. 3, p. 8, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
R. Rombach, A. Blattmann, D. Lorenz <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">et al.</em>, “High-resolution image synthesis with latent diffusion models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib10.2.2">IEEE/CVF</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
OpenAI, “Video generation models as world simulators,” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/research/video-generation-models-as-world-simulators" title="">https://openai.com/research/video-generation-models-as-world-simulators</a>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
S. Hochreiter and J. Schmidhuber, “Long short-term memory,” <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Neural Comput.</em>, vol. 9, no. 8, pp. 1735–1780, 1997.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">et al.</em>, “Attention is all you need,” in <em class="ltx_emph ltx_font_italic" id="bib.bib13.2.2">NeurIPS</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
I. Goodfellow, J. Pouget-Abadie, M. Mirza <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">et al.</em>, “Generative adversarial networks,” <em class="ltx_emph ltx_font_italic" id="bib.bib14.2.2">CACM</em>, vol. 63, no. 11, pp. 139–144, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
J. Devlin, M. Chang <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">et al.</em>, “BERT: pre-training of deep bidirectional transformers for language understanding,” in <em class="ltx_emph ltx_font_italic" id="bib.bib15.2.2">NAACL-HLT</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
C. Raffel, N. Shazeer, A. Roberts <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">et al.</em>, “Exploring the limits of transfer learning with a unified text-to-text transformer,” <em class="ltx_emph ltx_font_italic" id="bib.bib16.2.2">JMLR</em>, vol. 21, pp. 140:1–140:67, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity,” <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">JMLR</em>, vol. 23, no. 120, pp. 1–39, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
J. Kaplan, S. McCandlish, T. Henighan <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">et al.</em>, “Scaling laws for neural language models,” 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
S. E. Robertson and H. Zaragoza, “The probabilistic relevance framework: BM25 and beyond,” <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">FTIR</em>, vol. 3, no. 4, pp. 333–389, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
V. Karpukhin, B. Oguz, S. Min <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">et al.</em>, “Dense passage retrieval for open-domain question answering,” in <em class="ltx_emph ltx_font_italic" id="bib.bib20.2.2">EMNLP</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
J. Johnson, M. Douze, and H. Jégou, “Billion-scale similarity search with gpus,” <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">IEEE Trans. Big Data</em>, vol. 7, no. 3, pp. 535–547, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Q. Chen, B. Zhao, H. Wang <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">et al.</em>, “SPANN: highly-efficient billion-scale approximate nearest neighborhood search,” in <em class="ltx_emph ltx_font_italic" id="bib.bib22.2.2">NeurIPS</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
R. Datta, D. Joshi, J. Li <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">et al.</em>, “Image retrieval: Ideas, influences, and trends of the new age,” <em class="ltx_emph ltx_font_italic" id="bib.bib23.2.2">CSUR</em>, vol. 40, no. 2, pp. 5:1–5:60, 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
A. Radford, J. W. Kim, C. Hallacy <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">et al.</em>, “Learning transferable visual models from natural language supervision,” in <em class="ltx_emph ltx_font_italic" id="bib.bib24.2.2">ICML</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Z. Feng, D. Guo <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">et al.</em>, “Codebert: A pre-trained model for programming and natural languages,” in <em class="ltx_emph ltx_font_italic" id="bib.bib25.2.2">EMNLP Findings</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Y. Wu, K. Chen, T. Zhang <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">et al.</em>, “Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib26.2.2">ICASSP</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
A. Mallen, A. Asai, V. Zhong <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">et al.</em>, “When not to trust language models: Investigating effectiveness of parametric and non-parametric memories,” in <em class="ltx_emph ltx_font_italic" id="bib.bib27.2.2">ACL</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
N. Carlini, F. Tramèr <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">et al.</em>, “Extracting training data from large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib28.2.2">USENIX</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
M. Kang, N. M. Gürel <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">et al.</em>, “C-RAG: certified generation risks for retrieval-augmented language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib29.2.2">arXiv:2402.03181</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
G. Izacard, P. Lewis, M. Lomeli <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">et al.</em>, “Atlas: Few-shot learning with retrieval augmented language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib30.2.2">arXiv:2208.03299</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Y. Wu, M. N. Rabe, D. Hutchins, and C. Szegedy, “Memorizing transformers,” in <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">ICLR</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Z. He, Z. Zhong, T. Cai <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">et al.</em>, “REST: retrieval-based speculative decoding,” <em class="ltx_emph ltx_font_italic" id="bib.bib32.2.2">arxiv:2311.08252</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
K. Guu, K. Lee, Z. Tung <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">et al.</em>, “REALM: retrieval-augmented language model pre-training,” <em class="ltx_emph ltx_font_italic" id="bib.bib33.2.2">ICML</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
P. S. H. Lewis, E. Perez, A. Piktus <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">et al.</em>, “Retrieval-augmented generation for knowledge-intensive NLP tasks,” in <em class="ltx_emph ltx_font_italic" id="bib.bib34.2.2">NeurIPS</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
G. Izacard and E. Grave, “Leveraging passage retrieval with generative models for open domain question answering,” in <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">EACL</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
S. Borgeaud, A. Mensch <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">et al.</em>, “Improving language models by retrieving from trillions of tokens,” in <em class="ltx_emph ltx_font_italic" id="bib.bib36.2.2">ICML</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
U. Khandelwal, O. Levy, D. Jurafsky <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">et al.</em>, “Generalization through memorization: Nearest neighbor language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib37.2.2">ICLR</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
J. He, G. Neubig, and T. Berg-Kirkpatrick, “Efficient nearest neighbor language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">EMNLP</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
zilliztech. (2023) Gptcache. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/zilliztech/GPTCache" title="">https://github.com/zilliztech/GPTCache</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
M. R. Parvez, W. U. Ahmad <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">et al.</em>, “Retrieval augmented code generation and summarization,” in <em class="ltx_emph ltx_font_italic" id="bib.bib40.2.2">EMNLP Findings</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
W. U. Ahmad, S. Chakraborty, B. Ray <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">et al.</em>, “Unified pre-training for program understanding and generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib41.2.2">NAACL-HLT</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
S. Zhou, U. Alon, F. F. Xu <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">et al.</em>, “Docprompting: Generating code by retrieving the docs,” in <em class="ltx_emph ltx_font_italic" id="bib.bib42.2.2">ICLR</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Y. Koizumi, Y. Ohishi <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">et al.</em>, “Audio captioning using pre-trained large-scale language model guided by audio-based similar caption retrieval,” <em class="ltx_emph ltx_font_italic" id="bib.bib43.2.2">arXiv:2012.07331</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
R. Huang, J. Huang, D. Yang <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">et al.</em>, “Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib44.2.2">ICML</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
H.-Y. Tseng, H.-Y. Lee <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">et al.</em>, “Retrievegan: Image synthesis via differentiable patch retrieval,” in <em class="ltx_emph ltx_font_italic" id="bib.bib45.2.2">ECCV</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
S. Sarto, M. Cornia, L. Baraldi, and R. Cucchiara, “Retrieval-augmented transformer for image captioning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">CBMI</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
R. Ramos, B. Martins <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">et al.</em>, “Smallcap: lightweight image captioning prompted with retrieval augmentation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib47.2.2">CVPR</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
J. Chen, Y. Pan, Y. Li <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">et al.</em>, “Retrieval augmented convolutional encoder-decoder networks for video captioning,” <em class="ltx_emph ltx_font_italic" id="bib.bib48.2.2">TOMCCAP</em>, vol. 19, no. 1s, pp. 48:1–48:24, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
J. Xu, Y. Huang, J. Hou <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">et al.</em>, “Retrieval-augmented egocentric video captioning,” <em class="ltx_emph ltx_font_italic" id="bib.bib49.2.2">arXiv:2401.00789</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
J. Seo, S. Hong <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">et al.</em>, “Retrieval-augmented score distillation for text-to-3d generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib50.2.2">arXiv:2402.02972</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
M. Zhang, X. Guo, L. Pan <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">et al.</em>, “Remodiffuse: Retrieval-augmented motion diffusion model,” in <em class="ltx_emph ltx_font_italic" id="bib.bib51.2.2">ICCV</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
X. Hu, X. Wu, Y. Shu, and Y. Qu, “Logical form generation via multi-task learning for complex question answering over knowledge bases,” in <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">COLING</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
X. Huang, J. Kim, and B. Zou, “Unseen entity handling in complex question answering over knowledge base via language generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">EMNLP Findings</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
R. Das, M. Zaheer, D. Thai <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">et al.</em>, “Case-based reasoning for natural language queries over knowledge bases,” in <em class="ltx_emph ltx_font_italic" id="bib.bib54.2.2">EMNLP</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Z. Wang, W. Nie, Z. Qiao <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">et al.</em>, “Retrieval-based controllable molecule generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib55.2.2">ICLR</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Q. Jin, Y. Yang, Q. Chen, and Z. Lu, “Genegpt: Augmenting large language models with domain tools for improved access to biomedical information,” <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">Bioinformatics</em>, vol. 40, no. 2, p. btae075, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
H. Li, Y. Su, D. Cai <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">et al.</em>, “A survey on retrieval-augmented text generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib57.2.2">arxiv:2202.01110</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
A. Asai, S. Min, Z. Zhong, and D. Chen, “Acl 2023 tutorial: Retrieval-based language models and applications,” <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">ACL 2023</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Y. Gao, Y. Xiong <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">et al.</em>, “Retrieval-augmented generation for large language models: A survey,” <em class="ltx_emph ltx_font_italic" id="bib.bib59.2.2">arxiv:2312.10997</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
R. Zhao, H. Chen <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">et al.</em>, “Retrieving multimodal information for augmented generation: A survey,” in <em class="ltx_emph ltx_font_italic" id="bib.bib60.2.2">EMNLP</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Y. Ding, W. Fan <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">et al.</em>, “A survey on rag meets llms: Towards retrieval-augmented large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib61.2.2">arXiv:2405.06211</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
J. Chen, H. Guo, K. Yi <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">et al.</em>, “Visualgpt: Data-efficient adaptation of pretrained language models for image captioning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib62.2.2">CVPR</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Y. Tay, M. Dehghani, D. Bahri, and D. Metzler, “Efficient transformers: A survey,” <em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">CSUR</em>, vol. 55, no. 6, pp. 109:1–109:28, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
G. V. Houdt <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">et al.</em>, “A review on the long short-term memory model,” <em class="ltx_emph ltx_font_italic" id="bib.bib64.2.2">Artif. Intell. Rev.</em>, vol. 53, no. 8, pp. 5929–5955, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
L. Yang, Z. Zhang <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">et al.</em>, “Diffusion models: A comprehensive survey of methods and applications,” <em class="ltx_emph ltx_font_italic" id="bib.bib65.2.2">CSUR</em>, vol. 56, no. 4, pp. 1–39, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
J. Gui, Z. Sun, Y. Wen <em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">et al.</em>, “A review on generative adversarial networks: Algorithms, theory, and applications,” <em class="ltx_emph ltx_font_italic" id="bib.bib66.2.2">TKDE</em>, vol. 35, no. 4, pp. 3313–3332, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
S. E. Robertson and S. Walker, “On relevance weights with little relevance information,” in <em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">SIGIR</em>, 1997.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
J. D. Lafferty and C. Zhai, “Document language models, query models, and risk minimization for information retrieval,” in <em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">SIGIR</em>, 2001.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
S. Hershey, S. Chaudhuri <em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">et al.</em>, “CNN architectures for large-scale audio classification,” in <em class="ltx_emph ltx_font_italic" id="bib.bib69.2.2">ICASSP</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
J. Dong, X. Li, C. Xu <em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">et al.</em>, “Dual encoding for zero-example video retrieval,” in <em class="ltx_emph ltx_font_italic" id="bib.bib70.2.2">CVPR</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
L. Xiong, C. Xiong, Y. Li <em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">et al.</em>, “Approximate nearest neighbor negative contrastive learning for dense text retrieval,” in <em class="ltx_emph ltx_font_italic" id="bib.bib71.2.2">ICLR</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
J. L. Bentley, “Multidimensional binary search trees used for associative searching,” <em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">CACM</em>, vol. 18, no. 9, pp. 509–517, 1975.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
W. Li, C. Feng, D. Lian <em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">et al.</em>, “Learning balanced tree indexes for large-scale vector retrieval,” in <em class="ltx_emph ltx_font_italic" id="bib.bib73.2.2">SIGKDDg</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
M. Datar, N. Immorlica, P. Indyk <em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">et al.</em>, “Locality-sensitive hashing scheme based on p-stable distributions,” in <em class="ltx_emph ltx_font_italic" id="bib.bib74.2.2">SCG</em>, 2004.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Y. A. Malkov and D. A. Yashunin, “Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs,” <em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">TPAMI</em>, vol. 42, no. 4, pp. 824–836, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
S. Jayaram Subramanya, F. Devvrit <em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">et al.</em>, “Diskann: Fast accurate billion-point nearest neighbor search on a single node,” <em class="ltx_emph ltx_font_italic" id="bib.bib76.2.2">NeurIPS</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
Y. Wang, Y. Hou, H. Wang <em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">et al.</em>, “A neural corpus indexer for document retrieval,” in <em class="ltx_emph ltx_font_italic" id="bib.bib77.2.2">NeurIPS</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
H. Zhang, Y. Wang, Q. Chen <em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">et al.</em>, “Model-enhanced vector index,” in <em class="ltx_emph ltx_font_italic" id="bib.bib78.2.2">NeurIPS</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
S. A. Hayati, R. Olivier, P. Avvaru <em class="ltx_emph ltx_font_italic" id="bib.bib79.1.1">et al.</em>, “Retrieval-based neural code generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib79.2.2">EMNLP</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
J. Zhang, X. Wang, H. Zhang <em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">et al.</em>, “Retrieval-based neural source code summarization,” in <em class="ltx_emph ltx_font_italic" id="bib.bib80.2.2">ICSE</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
G. Poesia, A. Polozov, V. Le <em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">et al.</em>, “Synchromesh: Reliable code generation from pre-trained language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib81.2.2">ICLR</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
X. Ye, S. Yavuz <em class="ltx_emph ltx_font_italic" id="bib.bib82.1.1">et al.</em>, “RNG-KBQA: generation augmented iterative ranking for knowledge base question answering,” in <em class="ltx_emph ltx_font_italic" id="bib.bib82.2.2">ACL</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
Y. Shu <em class="ltx_emph ltx_font_italic" id="bib.bib83.1.1">et al.</em>, “TIARA: multi-grained retrieval for robust question answering over large knowledge bases,” <em class="ltx_emph ltx_font_italic" id="bib.bib83.2.2">arXiv:2210.12925</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
X. V. Lin, R. Socher <em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">et al.</em>, “Bridging textual and tabular data for cross-domain text-to-sql semantic parsing,” <em class="ltx_emph ltx_font_italic" id="bib.bib84.2.2">arXiv:2012.12627</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
A. Asai, Z. Wu, Y. Wang <em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">et al.</em>, “Self-rag: Learning to retrieve, generate, and critique through self-reflection,” <em class="ltx_emph ltx_font_italic" id="bib.bib85.2.2">arxiv:2310.11511</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
W. Shi, S. Min, M. Yasunaga <em class="ltx_emph ltx_font_italic" id="bib.bib86.1.1">et al.</em>, “Replug: Retrieval-augmented black-box language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib86.2.2">arXiv:2301.12652</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
O. Ram, Y. Levine, I. Dalmedigos <em class="ltx_emph ltx_font_italic" id="bib.bib87.1.1">et al.</em>, “In-context retrieval-augmented language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib87.2.2">arXiv:2302.00083</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
D. Zan, B. Chen, Z. Lin <em class="ltx_emph ltx_font_italic" id="bib.bib88.1.1">et al.</em>, “When language model meets private library,” in <em class="ltx_emph ltx_font_italic" id="bib.bib88.2.2">EMNLP Findings</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
N. Nashid, M. Sintaha, and A. Mesbah, “Retrieval-based prompt selection for code-related few-shot learning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib89.1.1">ICSE</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
M. Jin, S. Shahriar, M. Tufano <em class="ltx_emph ltx_font_italic" id="bib.bib90.1.1">et al.</em>, “Inferfix: End-to-end program repair with llms,” in <em class="ltx_emph ltx_font_italic" id="bib.bib90.2.2">ESEC/FSE</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
S. Lu, N. Duan, H. Han <em class="ltx_emph ltx_font_italic" id="bib.bib91.1.1">et al.</em>, “Reacc: A retrieval-augmented code completion framework,” in <em class="ltx_emph ltx_font_italic" id="bib.bib91.2.2">ACL</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
Y. Liu <em class="ltx_emph ltx_font_italic" id="bib.bib92.1.1">et al.</em>, “Uni-parser: Unified semantic parser for question answering on knowledge base and database,” in <em class="ltx_emph ltx_font_italic" id="bib.bib92.2.2">EMNLP</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
Z. Yang, X. Du, E. Cambria <em class="ltx_emph ltx_font_italic" id="bib.bib93.1.1">et al.</em>, “End-to-end case-based reasoning for commonsense knowledge base completion,” in <em class="ltx_emph ltx_font_italic" id="bib.bib93.2.2">EACL</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
W. Shi, Y. Zhuang, Y. Zhu <em class="ltx_emph ltx_font_italic" id="bib.bib94.1.1">et al.</em>, “Retrieval-augmented large language models for adolescent idiopathic scoliosis patients in shared decision-making,” in <em class="ltx_emph ltx_font_italic" id="bib.bib94.2.2">ACM-BCB</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
A. Casanova, M. Careil, J. Verbeek <em class="ltx_emph ltx_font_italic" id="bib.bib95.1.1">et al.</em>, “Instance-conditioned gan,” in <em class="ltx_emph ltx_font_italic" id="bib.bib95.2.2">NeurIPS</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
A. Bertsch, U. Alon, G. Neubig, and M. R. Gormley, “Unlimiformer: Long-range transformers with unlimited length input,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
Y. Kuratov, A. Bulatov <em class="ltx_emph ltx_font_italic" id="bib.bib97.1.1">et al.</em>, “In search of needles in a 10m haystack: Recurrent memory finds what llms miss,” <em class="ltx_emph ltx_font_italic" id="bib.bib97.2.2">arXiv:2402.10790</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
J. Li, Y. Li, G. Li <em class="ltx_emph ltx_font_italic" id="bib.bib98.1.1">et al.</em>, “Editsum: A retrieve-and-edit framework for source code summarization,” in <em class="ltx_emph ltx_font_italic" id="bib.bib98.2.2">ASE</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
C. Yu, G. Yang, X. Chen <em class="ltx_emph ltx_font_italic" id="bib.bib99.1.1">et al.</em>, “Bashexplainer: Retrieval-augmented bash code comment generation based on fine-tuned codebert,” in <em class="ltx_emph ltx_font_italic" id="bib.bib99.2.2">ICSME</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
T. B. Hashimoto, K. Guu, Y. Oren, and P. Liang, “A retrieve-and-edit framework for predicting structured outputs,” in <em class="ltx_emph ltx_font_italic" id="bib.bib100.1.1">NeurIPS</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
B. Wei, Y. Li, G. Li <em class="ltx_emph ltx_font_italic" id="bib.bib101.1.1">et al.</em>, “Retrieve and refine: Exemplar-based neural comment generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib101.2.2">ASE</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
E. Shi, Y. Wang, W. Tao <em class="ltx_emph ltx_font_italic" id="bib.bib102.1.1">et al.</em>, “RACE: retrieval-augmented commit message generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib102.2.2">EMNLP</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
W. Chen, H. Hu, C. Saharia, and W. W. Cohen, “Re-imagen: Retrieval-augmented text-to-image generator,” in <em class="ltx_emph ltx_font_italic" id="bib.bib103.1.1">ICLR</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
S. Sheynin, O. Ashual, A. Polyak <em class="ltx_emph ltx_font_italic" id="bib.bib104.1.1">et al.</em>, “Knn-diffusion: Image generation via large-scale retrieval,” in <em class="ltx_emph ltx_font_italic" id="bib.bib104.2.2">ICLR</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
A. Blattmann, R. Rombach, K. Oktay <em class="ltx_emph ltx_font_italic" id="bib.bib105.1.1">et al.</em>, “Retrieval-augmented diffusion models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib105.2.2">NeurIPS</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
R. Rombach, A. Blattmann, and B. Ommer, “Text-guided synthesis of artistic images with retrieval-augmented diffusion models,” <em class="ltx_emph ltx_font_italic" id="bib.bib106.1.1">arXiv:2207.13038</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
B. Li, P. H. Torr, and T. Lukasiewicz, “Memory-driven text-to-image generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib107.1.1">arXiv:2208.07022</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
B. Oguz, X. Chen, V. Karpukhin <em class="ltx_emph ltx_font_italic" id="bib.bib108.1.1">et al.</em>, “Unik-qa: Unified representations of structured and unstructured knowledge for open-domain question answering,” in <em class="ltx_emph ltx_font_italic" id="bib.bib108.2.2">NAACL Findings</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
D. Yu, S. Zhang <em class="ltx_emph ltx_font_italic" id="bib.bib109.1.1">et al.</em>, “Decaf: Joint decoding of answers and logical forms for question answering over knowledge bases,” in <em class="ltx_emph ltx_font_italic" id="bib.bib109.2.2">ICLR</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
G. Dong, R. Li, S. Wang <em class="ltx_emph ltx_font_italic" id="bib.bib110.1.1">et al.</em>, “Bridging the kb-text gap: Leveraging structured knowledge-aware pre-training for KBQA,” in <em class="ltx_emph ltx_font_italic" id="bib.bib110.2.2">CIKM</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
K. Wang, F. Duan, S. Wang <em class="ltx_emph ltx_font_italic" id="bib.bib111.1.1">et al.</em>, “Knowledge-driven cot: Exploring faithful reasoning in llms for knowledge-intensive question answering,” <em class="ltx_emph ltx_font_italic" id="bib.bib111.2.2">arXiv:2308.13259</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">
D. Yu and Y. Yang, “Retrieval-enhanced generative model for large-scale knowledge graph completion,” in <em class="ltx_emph ltx_font_italic" id="bib.bib112.1.1">SIGIR</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">
T. Févry, L. B. Soares <em class="ltx_emph ltx_font_italic" id="bib.bib113.1.1">et al.</em>, “Entities as experts: Sparse memory access with entity supervision,” in <em class="ltx_emph ltx_font_italic" id="bib.bib113.2.2">EMNLP</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">
M. de Jong, Y. Zemlyanskiy, N. FitzGerald <em class="ltx_emph ltx_font_italic" id="bib.bib114.1.1">et al.</em>, “Mention memory: incorporating textual knowledge into transformers through entity mention attention,” in <em class="ltx_emph ltx_font_italic" id="bib.bib114.2.2">ICLR</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">
B. Jing, Y. Zhang, Z. Song <em class="ltx_emph ltx_font_italic" id="bib.bib115.1.1">et al.</em>, “Amd: Anatomical motion diffusion with interpretable motion decomposition and fusion,” in <em class="ltx_emph ltx_font_italic" id="bib.bib115.2.2">AAAI</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">
Y. Yuan, H. Liu, X. Liu <em class="ltx_emph ltx_font_italic" id="bib.bib116.1.1">et al.</em>, “Retrieval-augmented text-to-audio generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib116.2.2">ICASSP</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">
B. Yang, M. Cao, and Y. Zou, “Concept-aware video captioning: Describing videos with effective prior information,” <em class="ltx_emph ltx_font_italic" id="bib.bib117.1.1">TIP</em>, vol. 32, pp. 5366–5378, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">
Z. Zhong, T. Lei, and D. Chen, “Training language models with memory augmentation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib118.1.1">EMNLP</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">
S. Min, W. Shi, M. Lewis <em class="ltx_emph ltx_font_italic" id="bib.bib119.1.1">et al.</em>, “Nonparametric masked language modeling,” in <em class="ltx_emph ltx_font_italic" id="bib.bib119.2.2">ACL Findings</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock">
X. Zhang, Y. Zhou, G. Yang, and T. Chen, “Syntax-aware retrieval augmented code generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib120.1.1">EMNLP Findings</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock">
Z. Fei, “Memory-augmented image captioning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib121.1.1">AAAI</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_tag_bibitem">[122]</span>
<span class="ltx_bibblock">
Y. Leviathan, M. Kalman, and Y. Matias, “Fast inference from transformers via speculative decoding,” in <em class="ltx_emph ltx_font_italic" id="bib.bib122.1.1">ICML</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_tag_bibitem">[123]</span>
<span class="ltx_bibblock">
T. Lan, D. Cai, Y. Wang <em class="ltx_emph ltx_font_italic" id="bib.bib123.1.1">et al.</em>, “Copy is all you need,” in <em class="ltx_emph ltx_font_italic" id="bib.bib123.2.2">ICLR</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_tag_bibitem">[124]</span>
<span class="ltx_bibblock">
B. Cao, D. Cai, L. Cui <em class="ltx_emph ltx_font_italic" id="bib.bib124.1.1">et al.</em>, “Retrieval is accurate generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib124.2.2">arXiv:2402.17532</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_tag_bibitem">[125]</span>
<span class="ltx_bibblock">
L. Wang, N. Yang, and F. Wei, “Query2doc: Query expansion with large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib125.1.1">EMNLP</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_tag_bibitem">[126]</span>
<span class="ltx_bibblock">
L. Gao, X. Ma, J. Lin, and J. Callan, “Precise zero-shot dense retrieval without relevance labels,” in <em class="ltx_emph ltx_font_italic" id="bib.bib126.1.1">ACL</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_tag_bibitem">[127]</span>
<span class="ltx_bibblock">
G. Kim, S. Kim, B. Jeon <em class="ltx_emph ltx_font_italic" id="bib.bib127.1.1">et al.</em>, “Tree of clarifications: Answering ambiguous questions with retrieval-augmented large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib127.2.2">EMNLP</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_tag_bibitem">[128]</span>
<span class="ltx_bibblock">
C.-M. Chan, C. Xu <em class="ltx_emph ltx_font_italic" id="bib.bib128.1.1">et al.</em>, “Rq-rag: Learning to refine queries for retrieval augmented generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib128.2.2">arXiv:2404.00610</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_tag_bibitem">[129]</span>
<span class="ltx_bibblock">
A. Tayal and A. Tyagi, “Dynamic contexts for generating suggestion questions in rag based conversational systems,” in <em class="ltx_emph ltx_font_italic" id="bib.bib129.1.1">WWW’24 Companion</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_tag_bibitem">[130]</span>
<span class="ltx_bibblock">
M. Xia, S. Malladi, S. Gururangan <em class="ltx_emph ltx_font_italic" id="bib.bib130.1.1">et al.</em>, “LESS: selecting influential data for targeted instruction tuning,” <em class="ltx_emph ltx_font_italic" id="bib.bib130.2.2">arXiv:2402.04333</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_tag_bibitem">[131]</span>
<span class="ltx_bibblock">
A.-L. Bornea, F. Ayed <em class="ltx_emph ltx_font_italic" id="bib.bib131.1.1">et al.</em>, “Telco-rag: Navigating the challenges of retrieval-augmented language models for telecommunications,” <em class="ltx_emph ltx_font_italic" id="bib.bib131.2.2">arXiv:2404.15939</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_tag_bibitem">[132]</span>
<span class="ltx_bibblock">
S. Yao, J. Zhao, D. Yu <em class="ltx_emph ltx_font_italic" id="bib.bib132.1.1">et al.</em>, “React: Synergizing reasoning and acting in language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib132.2.2">ICLR</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_tag_bibitem">[133]</span>
<span class="ltx_bibblock">
J. Wei, X. Wang, D. Schuurmans <em class="ltx_emph ltx_font_italic" id="bib.bib133.1.1">et al.</em>, “Chain-of-thought prompting elicits reasoning in large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib133.2.2">NeurIPS</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_tag_bibitem">[134]</span>
<span class="ltx_bibblock">
T. Pouplin, H. Sun, S. Holt, and M. Van der Schaar, “Retrieval-augmented thought process as sequential decision making,” <em class="ltx_emph ltx_font_italic" id="bib.bib134.1.1">arXiv:2402.07812</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_tag_bibitem">[135]</span>
<span class="ltx_bibblock">
J. Liu, “LlamaIndex,” 11 2022. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/jerryjliu/llama_index" title="">https://github.com/jerryjliu/llama_index</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_tag_bibitem">[136]</span>
<span class="ltx_bibblock">
P. Sarthi, S. Abdullah, A. Tuli <em class="ltx_emph ltx_font_italic" id="bib.bib136.1.1">et al.</em>, “Raptor: Recursive abstractive processing for tree-organized retrieval,” in <em class="ltx_emph ltx_font_italic" id="bib.bib136.2.2">ICLR</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_tag_bibitem">[137]</span>
<span class="ltx_bibblock">
B. Kang, J. Kim <em class="ltx_emph ltx_font_italic" id="bib.bib137.1.1">et al.</em>, “Prompt-rag: Pioneering vector embedding-free retrieval-augmented generation in niche domains, exemplified by korean medicine,” <em class="ltx_emph ltx_font_italic" id="bib.bib137.2.2">arXiv:2401.11246</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_tag_bibitem">[138]</span>
<span class="ltx_bibblock">
V. Raina <em class="ltx_emph ltx_font_italic" id="bib.bib138.1.1">et al.</em>, “Question-based retrieval using atomic units for enterprise rag,” <em class="ltx_emph ltx_font_italic" id="bib.bib138.2.2">arXiv:2405.12363</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_tag_bibitem">[139]</span>
<span class="ltx_bibblock">
S. Xiao, Z. Liu, P. Zhang <em class="ltx_emph ltx_font_italic" id="bib.bib139.1.1">et al.</em>, “C-pack: Packaged resources to advance general chinese embedding,” <em class="ltx_emph ltx_font_italic" id="bib.bib139.2.2">arxiv:2309.07597</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_tag_bibitem">[140]</span>
<span class="ltx_bibblock">
J. Chen, S. Xiao, P. Zhang <em class="ltx_emph ltx_font_italic" id="bib.bib140.1.1">et al.</em>, “Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation,” <em class="ltx_emph ltx_font_italic" id="bib.bib140.2.2">arxiv:2309.07597</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_tag_bibitem">[141]</span>
<span class="ltx_bibblock">
S. Xiao, Z. Liu, P. Zhang, and X. Xing, “Lm-cocktail: Resilient tuning of language models via model merging,” <em class="ltx_emph ltx_font_italic" id="bib.bib141.1.1">arxiv:2311.13534</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_tag_bibitem">[142]</span>
<span class="ltx_bibblock">
P. Zhang, S. Xiao, Z. Liu, Z. Dou, and J.-Y. Nie, “Retrieve anything to augment large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib142.1.1">arxiv:2310.07554</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_tag_bibitem">[143]</span>
<span class="ltx_bibblock">
M. Kulkarni, P. Tangarajan, K. Kim <em class="ltx_emph ltx_font_italic" id="bib.bib143.1.1">et al.</em>, “Reinforcement learning for optimizing RAG for domain chatbots,” <em class="ltx_emph ltx_font_italic" id="bib.bib143.2.2">arXiv:2401.06800</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib144">
<span class="ltx_tag ltx_tag_bibitem">[144]</span>
<span class="ltx_bibblock">
W. Wang, Y. Wang <em class="ltx_emph ltx_font_italic" id="bib.bib144.1.1">et al.</em>, “Rap-gen: Retrieval-augmented patch generation with codet5 for automatic program repair,” in <em class="ltx_emph ltx_font_italic" id="bib.bib144.2.2">ESEC/FSE</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib145">
<span class="ltx_tag ltx_tag_bibitem">[145]</span>
<span class="ltx_bibblock">
K. Sawarkar, A. Mangal <em class="ltx_emph ltx_font_italic" id="bib.bib145.1.1">et al.</em>, “Blended rag: Improving rag (retriever-augmented generation) accuracy with semantic search and hybrid query-based retrievers,” <em class="ltx_emph ltx_font_italic" id="bib.bib145.2.2">arXiv:2404.07220</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib146">
<span class="ltx_tag ltx_tag_bibitem">[146]</span>
<span class="ltx_bibblock">
S.-Q. Yan, J.-C. Gu, Y. Zhu, and Z.-H. Ling, “Corrective retrieval augmented generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib146.1.1">arXiv:2401.15884</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib147">
<span class="ltx_tag ltx_tag_bibitem">[147]</span>
<span class="ltx_bibblock">
W. Huang, M. Lapata, P. Vougiouklis <em class="ltx_emph ltx_font_italic" id="bib.bib147.1.1">et al.</em>, “Retrieval augmented generation with rich answer encoding,” in <em class="ltx_emph ltx_font_italic" id="bib.bib147.2.2">IJCNLP-AACL</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib148">
<span class="ltx_tag ltx_tag_bibitem">[148]</span>
<span class="ltx_bibblock">
H. Wang, W. Huang, Y. Deng <em class="ltx_emph ltx_font_italic" id="bib.bib148.1.1">et al.</em>, “Unims-rag: A unified multi-source retrieval-augmented generation for personalized dialogue systems,” <em class="ltx_emph ltx_font_italic" id="bib.bib148.2.2">arXiv:2401.13256</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib149">
<span class="ltx_tag ltx_tag_bibitem">[149]</span>
<span class="ltx_bibblock">
S. Koley, A. K. Bhunia <em class="ltx_emph ltx_font_italic" id="bib.bib149.1.1">et al.</em>, “You’ll never walk alone: A sketch and text duet for fine-grained image retrieval,” in <em class="ltx_emph ltx_font_italic" id="bib.bib149.2.2">CVPR</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib150">
<span class="ltx_tag ltx_tag_bibitem">[150]</span>
<span class="ltx_bibblock">
M. R. Glass, G. Rossiello, M. F. M. Chowdhury <em class="ltx_emph ltx_font_italic" id="bib.bib150.1.1">et al.</em>, “Re2g: Retrieve, rerank, generate,” in <em class="ltx_emph ltx_font_italic" id="bib.bib150.2.2">NAACL</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib151">
<span class="ltx_tag ltx_tag_bibitem">[151]</span>
<span class="ltx_bibblock">
R. F. Nogueira and K. Cho, “Passage re-ranking with BERT,” <em class="ltx_emph ltx_font_italic" id="bib.bib151.1.1">arxiv:1901.04085</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib152">
<span class="ltx_tag ltx_tag_bibitem">[152]</span>
<span class="ltx_bibblock">
J. Li, Y. Zhao, Y. Li <em class="ltx_emph ltx_font_italic" id="bib.bib152.1.1">et al.</em>, “Acecoder: Utilizing existing code to enhance code generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib152.2.2">arXiv:2303.17780</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib153">
<span class="ltx_tag ltx_tag_bibitem">[153]</span>
<span class="ltx_bibblock">
P. Shi, R. Zhang, H. Bai, and J. Lin, “XRICL: cross-lingual retrieval-augmented in-context learning for cross-lingual text-to-sql semantic parsing,” in <em class="ltx_emph ltx_font_italic" id="bib.bib153.1.1">EMNLP Findings</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib154">
<span class="ltx_tag ltx_tag_bibitem">[154]</span>
<span class="ltx_bibblock">
K. Rangan and Y. Yin, “A fine-tuning enhanced rag system with quantized influence measure as ai judge,” <em class="ltx_emph ltx_font_italic" id="bib.bib154.1.1">arXiv:2402.17081</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib155">
<span class="ltx_tag ltx_tag_bibitem">[155]</span>
<span class="ltx_bibblock">
J. Saad-Falcon, O. Khattab, K. Santhanam <em class="ltx_emph ltx_font_italic" id="bib.bib155.1.1">et al.</em>, “Udapdr: Unsupervised domain adaptation via llm prompting and distillation of rerankers,” in <em class="ltx_emph ltx_font_italic" id="bib.bib155.2.2">EMNLP</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib156">
<span class="ltx_tag ltx_tag_bibitem">[156]</span>
<span class="ltx_bibblock">
L. Wang, N. Yang, and F. Wei, “Learning to retrieve in-context examples for large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib156.1.1">arXiv:2307.07164</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib157">
<span class="ltx_tag ltx_tag_bibitem">[157]</span>
<span class="ltx_bibblock">
P. Finardi, L. Avila <em class="ltx_emph ltx_font_italic" id="bib.bib157.1.1">et al.</em>, “The chronicles of rag: The retriever, the chunk and the generator,” <em class="ltx_emph ltx_font_italic" id="bib.bib157.2.2">arXiv:2401.07883</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib158">
<span class="ltx_tag ltx_tag_bibitem">[158]</span>
<span class="ltx_bibblock">
J. Li, Y. Yuan, and Z. Zhang, “Enhancing llm factual accuracy with rag to counter hallucinations: A case study on domain-specific queries in private knowledge-bases,” <em class="ltx_emph ltx_font_italic" id="bib.bib158.1.1">arXiv:2403.10446</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib159">
<span class="ltx_tag ltx_tag_bibitem">[159]</span>
<span class="ltx_bibblock">
Z. Wang, J. Araki, Z. Jiang <em class="ltx_emph ltx_font_italic" id="bib.bib159.1.1">et al.</em>, “Learning to filter context for retrieval-augmented generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib159.2.2">arxiv:2311.08377</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib160">
<span class="ltx_tag ltx_tag_bibitem">[160]</span>
<span class="ltx_bibblock">
S. Hofstätter, J. Chen, K. Raman, and H. Zamani, “Fid-light: Efficient and effective retrieval-augmented text generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib160.1.1">SIGIR</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib161">
<span class="ltx_tag ltx_tag_bibitem">[161]</span>
<span class="ltx_bibblock">
D. Arora, A. Kini, S. R. Chowdhury <em class="ltx_emph ltx_font_italic" id="bib.bib161.1.1">et al.</em>, “Gar-meets-rag paradigm for zero-shot information retrieval,” <em class="ltx_emph ltx_font_italic" id="bib.bib161.2.2">arXiv:2310.20158</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib162">
<span class="ltx_tag ltx_tag_bibitem">[162]</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.pinecone.io" title="">https://www.pinecone.io</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib163">
<span class="ltx_tag ltx_tag_bibitem">[163]</span>
<span class="ltx_bibblock">
W. Yu, D. Iter <em class="ltx_emph ltx_font_italic" id="bib.bib163.1.1">et al.</em>, “Generate rather than retrieve: Large language models are strong context generators,” <em class="ltx_emph ltx_font_italic" id="bib.bib163.2.2">arXiv:2209.10063</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib164">
<span class="ltx_tag ltx_tag_bibitem">[164]</span>
<span class="ltx_bibblock">
A. Abdallah and A. Jatowt, “Generator-retriever-generator: A novel approach to open-domain question answering,” <em class="ltx_emph ltx_font_italic" id="bib.bib164.1.1">arXiv:2307.11278</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib165">
<span class="ltx_tag ltx_tag_bibitem">[165]</span>
<span class="ltx_bibblock">
M. Besta, A. Kubicek <em class="ltx_emph ltx_font_italic" id="bib.bib165.1.1">et al.</em>, “Multi-head rag: Solving multi-aspect problems with llms,” <em class="ltx_emph ltx_font_italic" id="bib.bib165.2.2">arXiv:2406.05085</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib166">
<span class="ltx_tag ltx_tag_bibitem">[166]</span>
<span class="ltx_bibblock">
E. Saravia, “Prompt Engineering Guide,” <em class="ltx_emph ltx_font_italic" id="bib.bib166.1.1">https://github.com/dair-ai/Prompt-Engineering-Guide</em>, 12 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib167">
<span class="ltx_tag ltx_tag_bibitem">[167]</span>
<span class="ltx_bibblock">
H. S. Zheng, S. Mishra <em class="ltx_emph ltx_font_italic" id="bib.bib167.1.1">et al.</em>, “Take a step back: Evoking reasoning via abstraction in large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib167.2.2">arxiv:2310.06117</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib168">
<span class="ltx_tag ltx_tag_bibitem">[168]</span>
<span class="ltx_bibblock">
S. Diao, P. Wang, Y. Lin, and T. Zhang, “Active prompting with chain-of-thought for large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib168.1.1">arxiv:2302.12246</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib169">
<span class="ltx_tag ltx_tag_bibitem">[169]</span>
<span class="ltx_bibblock">
H. Jiang, Q. Wu, C. Lin <em class="ltx_emph ltx_font_italic" id="bib.bib169.1.1">et al.</em>, “Llmlingua: Compressing prompts for accelerated inference of large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib169.2.2">EMNLP</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib170">
<span class="ltx_tag ltx_tag_bibitem">[170]</span>
<span class="ltx_bibblock">
N. F. Liu, K. Lin, J. Hewitt <em class="ltx_emph ltx_font_italic" id="bib.bib170.1.1">et al.</em>, “Lost in the middle: How language models use long contexts,” <em class="ltx_emph ltx_font_italic" id="bib.bib170.2.2">arxiv:2307.03172</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib171">
<span class="ltx_tag ltx_tag_bibitem">[171]</span>
<span class="ltx_bibblock">
T. Ahmed, K. S. Pai, P. Devanbu, and E. T. Barr, “Automatic semantic augmentation of language model prompts (for code summarization),” <em class="ltx_emph ltx_font_italic" id="bib.bib171.1.1">arXiv:2304.06815</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib172">
<span class="ltx_tag ltx_tag_bibitem">[172]</span>
<span class="ltx_bibblock">
Z. Xu, Z. Liu, Y. Liu <em class="ltx_emph ltx_font_italic" id="bib.bib172.1.1">et al.</em>, “Activerag: Revealing the treasures of knowledge via active learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib172.2.2">arXiv:2402.13547</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib173">
<span class="ltx_tag ltx_tag_bibitem">[173]</span>
<span class="ltx_bibblock">
E. Nijkamp, B. Pang, H. Hayashi <em class="ltx_emph ltx_font_italic" id="bib.bib173.1.1">et al.</em>, “A conversational paradigm for program synthesis,” <em class="ltx_emph ltx_font_italic" id="bib.bib173.2.2">arxiv:2203.13474</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib174">
<span class="ltx_tag ltx_tag_bibitem">[174]</span>
<span class="ltx_bibblock">
Y. He, M. Xia, H. Chen <em class="ltx_emph ltx_font_italic" id="bib.bib174.1.1">et al.</em>, “Animate-a-story: Storytelling with retrieval-augmented video generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib174.2.2">arXiv:2307.06940</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib175">
<span class="ltx_tag ltx_tag_bibitem">[175]</span>
<span class="ltx_bibblock">
E. J. Hu, Y. Shen, P. Wallis <em class="ltx_emph ltx_font_italic" id="bib.bib175.1.1">et al.</em>, “Lora: Low-rank adaptation of large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib175.2.2">ICLR</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib176">
<span class="ltx_tag ltx_tag_bibitem">[176]</span>
<span class="ltx_bibblock">
C. Liu, P. Çetin, Y. Patodia <em class="ltx_emph ltx_font_italic" id="bib.bib176.1.1">et al.</em>, “Automated code editing with search-generate-modify,” <em class="ltx_emph ltx_font_italic" id="bib.bib176.2.2">arXiv:2306.06490</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib177">
<span class="ltx_tag ltx_tag_bibitem">[177]</span>
<span class="ltx_bibblock">
H. Joshi, J. P. C. Sánchez, S. Gulwani <em class="ltx_emph ltx_font_italic" id="bib.bib177.1.1">et al.</em>, “Repair is nearly generation: Multilingual program repair with llms,” in <em class="ltx_emph ltx_font_italic" id="bib.bib177.2.2">AAAI</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib178">
<span class="ltx_tag ltx_tag_bibitem">[178]</span>
<span class="ltx_bibblock">
Z. Jiang, F. F. Xu, L. Gao <em class="ltx_emph ltx_font_italic" id="bib.bib178.1.1">et al.</em>, “Active retrieval augmented generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib178.2.2">arXiv:2305.06983</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib179">
<span class="ltx_tag ltx_tag_bibitem">[179]</span>
<span class="ltx_bibblock">
A. Mallen, A. Asai, V. Zhong <em class="ltx_emph ltx_font_italic" id="bib.bib179.1.1">et al.</em>, “When not to trust language models: Investigating effectiveness of parametric and non-parametric memories,” in <em class="ltx_emph ltx_font_italic" id="bib.bib179.2.2">ACL</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib180">
<span class="ltx_tag ltx_tag_bibitem">[180]</span>
<span class="ltx_bibblock">
Z. Jiang, J. Araki, H. Ding, and G. Neubig, “How can we know <em class="ltx_emph ltx_font_italic" id="bib.bib180.1.1">When</em> language models know? on the calibration of language models for question answering,” <em class="ltx_emph ltx_font_italic" id="bib.bib180.2.2">TACL</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib181">
<span class="ltx_tag ltx_tag_bibitem">[181]</span>
<span class="ltx_bibblock">
N. Kandpal, H. Deng, A. Roberts <em class="ltx_emph ltx_font_italic" id="bib.bib181.1.1">et al.</em>, “Large language models struggle to learn long-tail knowledge,” in <em class="ltx_emph ltx_font_italic" id="bib.bib181.2.2">ICML</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib182">
<span class="ltx_tag ltx_tag_bibitem">[182]</span>
<span class="ltx_bibblock">
R. Ren, Y. Wang, Y. Qu <em class="ltx_emph ltx_font_italic" id="bib.bib182.1.1">et al.</em>, “Investigating the factual knowledge boundary of large language models with retrieval augmentation,” <em class="ltx_emph ltx_font_italic" id="bib.bib182.2.2">arxiv:2307.11019</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib183">
<span class="ltx_tag ltx_tag_bibitem">[183]</span>
<span class="ltx_bibblock">
Y. Wang, P. Li, M. Sun, and Y. Liu, “Self-knowledge guided retrieval augmentation for large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib183.1.1">EMNLP Findings</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib184">
<span class="ltx_tag ltx_tag_bibitem">[184]</span>
<span class="ltx_bibblock">
H. Ding, L. Pang, Z. Wei <em class="ltx_emph ltx_font_italic" id="bib.bib184.1.1">et al.</em>, “Retrieve only when it needs: Adaptive retrieval augmentation for hallucination mitigation in large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib184.2.2">arXiv:2402.10612</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib185">
<span class="ltx_tag ltx_tag_bibitem">[185]</span>
<span class="ltx_bibblock">
S. Jeong, J. Baek, S. Cho <em class="ltx_emph ltx_font_italic" id="bib.bib185.1.1">et al.</em>, “Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity,” <em class="ltx_emph ltx_font_italic" id="bib.bib185.2.2">arXiv:2403.14403</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib186">
<span class="ltx_tag ltx_tag_bibitem">[186]</span>
<span class="ltx_bibblock">
F. Zhang, B. Chen <em class="ltx_emph ltx_font_italic" id="bib.bib186.1.1">et al.</em>, “Repocoder: Repository-level code completion through iterative retrieval and generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib186.2.2">EMNLP</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib187">
<span class="ltx_tag ltx_tag_bibitem">[187]</span>
<span class="ltx_bibblock">
Z. Shao, Y. Gong, Y. Shen <em class="ltx_emph ltx_font_italic" id="bib.bib187.1.1">et al.</em>, “Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy,” in <em class="ltx_emph ltx_font_italic" id="bib.bib187.2.2">EMNLP Findings</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib188">
<span class="ltx_tag ltx_tag_bibitem">[188]</span>
<span class="ltx_bibblock">
X. Cheng, D. Luo, X. Chen <em class="ltx_emph ltx_font_italic" id="bib.bib188.1.1">et al.</em>, “Lift yourself up: Retrieval-augmented text generation with self-memory,” in <em class="ltx_emph ltx_font_italic" id="bib.bib188.2.2">NeurIPS</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib189">
<span class="ltx_tag ltx_tag_bibitem">[189]</span>
<span class="ltx_bibblock">
Z. Wang, A. Liu, H. Lin <em class="ltx_emph ltx_font_italic" id="bib.bib189.1.1">et al.</em>, “Rat: Retrieval augmented thoughts elicit context-aware reasoning in long-horizon generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib189.2.2">arXiv:2403.05313</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib190">
<span class="ltx_tag ltx_tag_bibitem">[190]</span>
<span class="ltx_bibblock">
O. Agarwal, H. Ge, S. Shakeri, and R. Al-Rfou, “Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training,” in <em class="ltx_emph ltx_font_italic" id="bib.bib190.1.1">NAACL-HLT</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib191">
<span class="ltx_tag ltx_tag_bibitem">[191]</span>
<span class="ltx_bibblock">
J. Sun, C. Xu, L. Tang <em class="ltx_emph ltx_font_italic" id="bib.bib191.1.1">et al.</em>, “Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph,” <em class="ltx_emph ltx_font_italic" id="bib.bib191.2.2">arXiv:2307.07697</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib192">
<span class="ltx_tag ltx_tag_bibitem">[192]</span>
<span class="ltx_bibblock">
P. Limkonchotiwat, W. Ponwitayarat, C. Udomcharoenchaikit <em class="ltx_emph ltx_font_italic" id="bib.bib192.1.1">et al.</em>, “Cl-relkt: Cross-lingual language knowledge transfer for multilingual retrieval question answering,” in <em class="ltx_emph ltx_font_italic" id="bib.bib192.2.2">NAACL Findings</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib193">
<span class="ltx_tag ltx_tag_bibitem">[193]</span>
<span class="ltx_bibblock">
A. Asai, X. Yu, J. Kasai, and H. Hajishirzi, “One question answering model for many languages with cross-lingual dense passage retrieval,” in <em class="ltx_emph ltx_font_italic" id="bib.bib193.1.1">NeurIPS</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib194">
<span class="ltx_tag ltx_tag_bibitem">[194]</span>
<span class="ltx_bibblock">
K. Lee, S. Han <em class="ltx_emph ltx_font_italic" id="bib.bib194.1.1">et al.</em>, “When to read documents or QA history: On unified and selective open-domain QA,” in <em class="ltx_emph ltx_font_italic" id="bib.bib194.2.2">ACL Findings</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib195">
<span class="ltx_tag ltx_tag_bibitem">[195]</span>
<span class="ltx_bibblock">
S. Yue, W. Chen <em class="ltx_emph ltx_font_italic" id="bib.bib195.1.1">et al.</em>, “Disc-lawllm: Fine-tuning large language models for intelligent legal services,” <em class="ltx_emph ltx_font_italic" id="bib.bib195.2.2">arXiv:2309.11325</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib196">
<span class="ltx_tag ltx_tag_bibitem">[196]</span>
<span class="ltx_bibblock">
S. Siriwardhana, R. Weerasekera, T. Kaluarachchi <em class="ltx_emph ltx_font_italic" id="bib.bib196.1.1">et al.</em>, “Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering,” <em class="ltx_emph ltx_font_italic" id="bib.bib196.2.2">TACL</em>, vol. 11, pp. 1–17, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib197">
<span class="ltx_tag ltx_tag_bibitem">[197]</span>
<span class="ltx_bibblock">
Y. Tang and Y. Yang, “Multihop-rag: Benchmarking retrieval-augmented generation for multi-hop queries,” <em class="ltx_emph ltx_font_italic" id="bib.bib197.1.1">arXiv:2401.15391</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib198">
<span class="ltx_tag ltx_tag_bibitem">[198]</span>
<span class="ltx_bibblock">
K. Huang, C. Zhai, and H. Ji, “CONCRETE: improving cross-lingual fact-checking with cross-lingual retrieval,” in <em class="ltx_emph ltx_font_italic" id="bib.bib198.1.1">COLING</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib199">
<span class="ltx_tag ltx_tag_bibitem">[199]</span>
<span class="ltx_bibblock">
L. Hagström, D. Saynova, T. Norlund <em class="ltx_emph ltx_font_italic" id="bib.bib199.1.1">et al.</em>, “The effect of scaling, retrieval augmentation and form on the factual consistency of language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib199.2.2">arXiv:2311.01307</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib200">
<span class="ltx_tag ltx_tag_bibitem">[200]</span>
<span class="ltx_bibblock">
H. Zamani and M. Bendersky, “Stochastic rag: End-to-end retrieval-augmented generation through expected utility maximization,” <em class="ltx_emph ltx_font_italic" id="bib.bib200.1.1">arXiv:2405.02816</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib201">
<span class="ltx_tag ltx_tag_bibitem">[201]</span>
<span class="ltx_bibblock">
Y. Liu, Y. Wan <em class="ltx_emph ltx_font_italic" id="bib.bib201.1.1">et al.</em>, “KG-BART: knowledge graph-augmented BART for generative commonsense reasoning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib201.2.2">AAAI</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib202">
<span class="ltx_tag ltx_tag_bibitem">[202]</span>
<span class="ltx_bibblock">
A. Wan, E. Wallace, and D. Klein, “What evidence do language models find convincing?” <em class="ltx_emph ltx_font_italic" id="bib.bib202.1.1">arXiv:2402.11782</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib203">
<span class="ltx_tag ltx_tag_bibitem">[203]</span>
<span class="ltx_bibblock">
H. Zhang, Z. Liu <em class="ltx_emph ltx_font_italic" id="bib.bib203.1.1">et al.</em>, “Grounded conversation generation as guided traverses in commonsense knowledge graphs,” in <em class="ltx_emph ltx_font_italic" id="bib.bib203.2.2">ACL</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib204">
<span class="ltx_tag ltx_tag_bibitem">[204]</span>
<span class="ltx_bibblock">
D. Cai, Y. Wang, W. Bi <em class="ltx_emph ltx_font_italic" id="bib.bib204.1.1">et al.</em>, “Skeleton-to-response: Dialogue generation guided by retrieval memory,” in <em class="ltx_emph ltx_font_italic" id="bib.bib204.2.2">NAACL-HLT</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib205">
<span class="ltx_tag ltx_tag_bibitem">[205]</span>
<span class="ltx_bibblock">
M. Komeili, K. Shuster, and J. Weston, “Internet-augmented dialogue generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib205.1.1">ACL</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib206">
<span class="ltx_tag ltx_tag_bibitem">[206]</span>
<span class="ltx_bibblock">
K. Shuster, J. Xu <em class="ltx_emph ltx_font_italic" id="bib.bib206.1.1">et al.</em>, “Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage,” <em class="ltx_emph ltx_font_italic" id="bib.bib206.2.2">arXiv:2208.03188</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib207">
<span class="ltx_tag ltx_tag_bibitem">[207]</span>
<span class="ltx_bibblock">
S. Kim, J. Y. Jang, M. Jung, and S. Shin, “A model of cross-lingual knowledge-grounded response generation for open-domain dialogue systems,” in <em class="ltx_emph ltx_font_italic" id="bib.bib207.1.1">EMNLP Findings</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib208">
<span class="ltx_tag ltx_tag_bibitem">[208]</span>
<span class="ltx_bibblock">
E. Nie, S. Liang, H. Schmid, and H. Schütze, “Cross-lingual retrieval augmented prompt for low-resource languages,” in <em class="ltx_emph ltx_font_italic" id="bib.bib208.1.1">ACL</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib209">
<span class="ltx_tag ltx_tag_bibitem">[209]</span>
<span class="ltx_bibblock">
X. Li, E. Nie, and S. Liang, “From classification to generation: Insights into crosslingual retrieval augmented icl,” in <em class="ltx_emph ltx_font_italic" id="bib.bib209.1.1">NeurIPS</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib210">
<span class="ltx_tag ltx_tag_bibitem">[210]</span>
<span class="ltx_bibblock">
W. Li, J. Li, W. Ma, and Y. Liu, “Citation-enhanced generation for llm-based chatbot,” <em class="ltx_emph ltx_font_italic" id="bib.bib210.1.1">arXiv:2402.16063</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib211">
<span class="ltx_tag ltx_tag_bibitem">[211]</span>
<span class="ltx_bibblock">
D. Cai, Y. Wang, H. Li <em class="ltx_emph ltx_font_italic" id="bib.bib211.1.1">et al.</em>, “Neural machine translation with monolingual translation memory,” in <em class="ltx_emph ltx_font_italic" id="bib.bib211.2.2">ACL/IJCNLP</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib212">
<span class="ltx_tag ltx_tag_bibitem">[212]</span>
<span class="ltx_bibblock">
U. Khandelwal, A. Fan, D. Jurafsky <em class="ltx_emph ltx_font_italic" id="bib.bib212.1.1">et al.</em>, “Nearest neighbor machine translation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib212.2.2">ICLR</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib213">
<span class="ltx_tag ltx_tag_bibitem">[213]</span>
<span class="ltx_bibblock">
X. Du and H. Ji, “Retrieval-augmented generative question answering for event argument extraction,” in <em class="ltx_emph ltx_font_italic" id="bib.bib213.1.1">EMNLP</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib214">
<span class="ltx_tag ltx_tag_bibitem">[214]</span>
<span class="ltx_bibblock">
Y. Gao, Q. Yin, Z. Li <em class="ltx_emph ltx_font_italic" id="bib.bib214.1.1">et al.</em>, “Retrieval-augmented multilingual keyphrase generation with retriever-generator iterative training,” in <em class="ltx_emph ltx_font_italic" id="bib.bib214.2.2">NAACL Findings</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib215">
<span class="ltx_tag ltx_tag_bibitem">[215]</span>
<span class="ltx_bibblock">
J. Zhang, E. J. Yu, Q. Chen <em class="ltx_emph ltx_font_italic" id="bib.bib215.1.1">et al.</em>, “Retrieval-based full-length wikipedia generation for emergent events,” <em class="ltx_emph ltx_font_italic" id="bib.bib215.2.2">arXiv:2402.18264</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib216">
<span class="ltx_tag ltx_tag_bibitem">[216]</span>
<span class="ltx_bibblock">
R. Fan, Y. Fan, J. Chen <em class="ltx_emph ltx_font_italic" id="bib.bib216.1.1">et al.</em>, “RIGHT: retrieval-augmented generation for mainstream hashtag recommendation,” <em class="ltx_emph ltx_font_italic" id="bib.bib216.2.2">arxiv:2312.10466</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib217">
<span class="ltx_tag ltx_tag_bibitem">[217]</span>
<span class="ltx_bibblock">
Z. Wang, S. X. Teo <em class="ltx_emph ltx_font_italic" id="bib.bib217.1.1">et al.</em>, “M-rag: Reinforcing large language model performance through retrieval-augmented generation with multiple partitions,” <em class="ltx_emph ltx_font_italic" id="bib.bib217.2.2">arXiv:2405.16420</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib218">
<span class="ltx_tag ltx_tag_bibitem">[218]</span>
<span class="ltx_bibblock">
Y. Wang, H. Le, A. D. Gotmare <em class="ltx_emph ltx_font_italic" id="bib.bib218.1.1">et al.</em>, “Codet5mix: A pretrained mixture of encoder-decoder transformers for code understanding and generation,” 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib219">
<span class="ltx_tag ltx_tag_bibitem">[219]</span>
<span class="ltx_bibblock">
A. Madaan, S. Zhou, U. Alon <em class="ltx_emph ltx_font_italic" id="bib.bib219.1.1">et al.</em>, “Language models of code are few-shot commonsense learners,” in <em class="ltx_emph ltx_font_italic" id="bib.bib219.2.2">EMNLP</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib220">
<span class="ltx_tag ltx_tag_bibitem">[220]</span>
<span class="ltx_bibblock">
Y. Wang, H. Le, A. Gotmare <em class="ltx_emph ltx_font_italic" id="bib.bib220.1.1">et al.</em>, “Codet5+: Open code large language models for code understanding and generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib220.2.2">EMNLP</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib221">
<span class="ltx_tag ltx_tag_bibitem">[221]</span>
<span class="ltx_bibblock">
J. Chen, X. Hu, Z. Li <em class="ltx_emph ltx_font_italic" id="bib.bib221.1.1">et al.</em>, “Code search is all you need? improving code suggestions with code search,” in <em class="ltx_emph ltx_font_italic" id="bib.bib221.2.2">ICSE</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib222">
<span class="ltx_tag ltx_tag_bibitem">[222]</span>
<span class="ltx_bibblock">
D. Zan, B. Chen, Y. Gong <em class="ltx_emph ltx_font_italic" id="bib.bib222.1.1">et al.</em>, “Private-library-oriented code generation with large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib222.2.2">arXiv:2307.15370</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib223">
<span class="ltx_tag ltx_tag_bibitem">[223]</span>
<span class="ltx_bibblock">
M. Liu, T. Yang, Y. Lou <em class="ltx_emph ltx_font_italic" id="bib.bib223.1.1">et al.</em>, “Codegen4libs: A two-stage approach for library-oriented code generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib223.2.2">ASE</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib224">
<span class="ltx_tag ltx_tag_bibitem">[224]</span>
<span class="ltx_bibblock">
D. Liao, S. Pan, Q. Huang <em class="ltx_emph ltx_font_italic" id="bib.bib224.1.1">et al.</em>, “Context-aware code generation framework for code repositories: Local, global, and third-party library awareness,” <em class="ltx_emph ltx_font_italic" id="bib.bib224.2.2">arXiv:2312.05772</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib225">
<span class="ltx_tag ltx_tag_bibitem">[225]</span>
<span class="ltx_bibblock">
J. Li, Y. Li, G. Li <em class="ltx_emph ltx_font_italic" id="bib.bib225.1.1">et al.</em>, “Skcoder: A sketch-based approach for automatic code generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib225.2.2">ICSE</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib226">
<span class="ltx_tag ltx_tag_bibitem">[226]</span>
<span class="ltx_bibblock">
Q. Gou, Y. Dong, Y. Wu, and Q. Ke, “Rrgcode: Deep hierarchical search-based code generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib226.1.1">Journal of Systems and Software</em>, vol. 211, p. 111982, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib227">
<span class="ltx_tag ltx_tag_bibitem">[227]</span>
<span class="ltx_bibblock">
K. Zhang, J. Li, G. Li <em class="ltx_emph ltx_font_italic" id="bib.bib227.1.1">et al.</em>, “Codeagent: Enhancing code generation with tool-integrated agent systems for real-world repo-level coding challenges,” <em class="ltx_emph ltx_font_italic" id="bib.bib227.2.2">arXiv:2401.07339</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib228">
<span class="ltx_tag ltx_tag_bibitem">[228]</span>
<span class="ltx_bibblock">
H. Su, S. Jiang, Y. Lai <em class="ltx_emph ltx_font_italic" id="bib.bib228.1.1">et al.</em>, “Arks: Active retrieval in knowledge soup for code generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib228.2.2">arXiv:2402.12317</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib229">
<span class="ltx_tag ltx_tag_bibitem">[229]</span>
<span class="ltx_bibblock">
K. Zhang, G. Li, J. Li <em class="ltx_emph ltx_font_italic" id="bib.bib229.1.1">et al.</em>, “Toolcoder: Teach code generation models to use API search tools,” <em class="ltx_emph ltx_font_italic" id="bib.bib229.2.2">arXiv:2305.04032</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib230">
<span class="ltx_tag ltx_tag_bibitem">[230]</span>
<span class="ltx_bibblock">
S. Liu, Y. Chen, X. Xie <em class="ltx_emph ltx_font_italic" id="bib.bib230.1.1">et al.</em>, “Retrieval-augmented generation for code summarization via hybrid GNN,” in <em class="ltx_emph ltx_font_italic" id="bib.bib230.2.2">ICLR</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib231">
<span class="ltx_tag ltx_tag_bibitem">[231]</span>
<span class="ltx_bibblock">
F. Yamaguchi, N. Golde, D. Arp, and K. Rieck, “Modeling and discovering vulnerabilities with code property graphs,” in <em class="ltx_emph ltx_font_italic" id="bib.bib231.1.1">S&amp;P</em>, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib232">
<span class="ltx_tag ltx_tag_bibitem">[232]</span>
<span class="ltx_bibblock">
Y. Choi, C. Na <em class="ltx_emph ltx_font_italic" id="bib.bib232.1.1">et al.</em>, “Readsum: Retrieval-augmented adaptive transformer for source code summarization,” <em class="ltx_emph ltx_font_italic" id="bib.bib232.2.2">IEEE Access</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib233">
<span class="ltx_tag ltx_tag_bibitem">[233]</span>
<span class="ltx_bibblock">
J. Zhao, X. Chen, G. Yang, and Y. Shen, “Automatic smart contract comment generation via large language models and in-context learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib233.1.1">IST</em>, vol. 168, p. 107405, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib234">
<span class="ltx_tag ltx_tag_bibitem">[234]</span>
<span class="ltx_bibblock">
A. Alokla, W. Gad, W. Nazih <em class="ltx_emph ltx_font_italic" id="bib.bib234.1.1">et al.</em>, “Retrieval-based transformer pseudocode generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib234.2.2">Mathematics</em>, vol. 10, no. 4, p. 604, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib235">
<span class="ltx_tag ltx_tag_bibitem">[235]</span>
<span class="ltx_bibblock">
J. Xu, Z. Cui <em class="ltx_emph ltx_font_italic" id="bib.bib235.1.1">et al.</em>, “Unilog: Automatic logging via LLM and in-context learning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib235.2.2">ICSE</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib236">
<span class="ltx_tag ltx_tag_bibitem">[236]</span>
<span class="ltx_bibblock">
H. Wang, X. Xia <em class="ltx_emph ltx_font_italic" id="bib.bib236.1.1">et al.</em>, “Context-aware retrieval-based deep commit message generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib236.2.2">TOSEM</em>, vol. 30, no. 4, pp. 56:1–56:30, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib237">
<span class="ltx_tag ltx_tag_bibitem">[237]</span>
<span class="ltx_bibblock">
X. Zhu, C. Sha, and J. Niu, “A simple retrieval-based method for code comment generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib237.1.1">SANER</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib238">
<span class="ltx_tag ltx_tag_bibitem">[238]</span>
<span class="ltx_bibblock">
T. Ye, L. Wu, T. Ma <em class="ltx_emph ltx_font_italic" id="bib.bib238.1.1">et al.</em>, “Tram: A token-level retrieval-augmented mechanism for source code summarization,” <em class="ltx_emph ltx_font_italic" id="bib.bib238.2.2">arXiv:2305.11074</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib239">
<span class="ltx_tag ltx_tag_bibitem">[239]</span>
<span class="ltx_bibblock">
L. Li, B. Liang, L. Chen, and X. Zhang, “Cross-modal retrieval-enhanced code summarization based on joint learning for retrieval and generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib239.1.1">Available at SSRN 4724884</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib240">
<span class="ltx_tag ltx_tag_bibitem">[240]</span>
<span class="ltx_bibblock">
D. Drain, C. Hu, C. Wu <em class="ltx_emph ltx_font_italic" id="bib.bib240.1.1">et al.</em>, “Generating code with the help of retrieved template functions and stack overflow answers,” <em class="ltx_emph ltx_font_italic" id="bib.bib240.2.2">arXiv:2104.05310</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib241">
<span class="ltx_tag ltx_tag_bibitem">[241]</span>
<span class="ltx_bibblock">
A. Eghbali and M. Pradel, “De-hallucinator: Iterative grounding for llm-based code completion,” <em class="ltx_emph ltx_font_italic" id="bib.bib241.1.1">arXiv:2401.01701</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib242">
<span class="ltx_tag ltx_tag_bibitem">[242]</span>
<span class="ltx_bibblock">
M. Liang, X. Xie, G. Zhang <em class="ltx_emph ltx_font_italic" id="bib.bib242.1.1">et al.</em>, “Repofuse: Repository-level code completion with fused dual context,” <em class="ltx_emph ltx_font_italic" id="bib.bib242.2.2">arXiv:2402.14323</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib243">
<span class="ltx_tag ltx_tag_bibitem">[243]</span>
<span class="ltx_bibblock">
D. Shrivastava, D. Kocetkov <em class="ltx_emph ltx_font_italic" id="bib.bib243.1.1">et al.</em>, “Repofusion: Training code models to understand your repository,” <em class="ltx_emph ltx_font_italic" id="bib.bib243.2.2">arXiv:2306.10998</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib244">
<span class="ltx_tag ltx_tag_bibitem">[244]</span>
<span class="ltx_bibblock">
W. Sun, H. Li, M. Yan <em class="ltx_emph ltx_font_italic" id="bib.bib244.1.1">et al.</em>, “Revisiting and improving retrieval-augmented deep assertion generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib244.2.2">ASE</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib245">
<span class="ltx_tag ltx_tag_bibitem">[245]</span>
<span class="ltx_bibblock">
Y. Ding, Z. Wang <em class="ltx_emph ltx_font_italic" id="bib.bib245.1.1">et al.</em>, “Cocomic: Code completion by jointly modeling in-file and cross-file context,” <em class="ltx_emph ltx_font_italic" id="bib.bib245.2.2">arXiv:2212.10007</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib246">
<span class="ltx_tag ltx_tag_bibitem">[246]</span>
<span class="ltx_bibblock">
Z. Tang, J. Ge, S. Liu <em class="ltx_emph ltx_font_italic" id="bib.bib246.1.1">et al.</em>, “Domain adaptive code completion via language models and decoupled domain databases,” in <em class="ltx_emph ltx_font_italic" id="bib.bib246.2.2">ASE</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib247">
<span class="ltx_tag ltx_tag_bibitem">[247]</span>
<span class="ltx_bibblock">
Y. Tsai, M. Liu, and H. Ren, “Rtlfixer: Automatically fixing RTL syntax errors with large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib247.1.1">arXiv:2311.16543</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib248">
<span class="ltx_tag ltx_tag_bibitem">[248]</span>
<span class="ltx_bibblock">
B. Bogin, S. Gupta, P. Clark <em class="ltx_emph ltx_font_italic" id="bib.bib248.1.1">et al.</em>, “Leveraging code to improve in-context learning for semantic parsing,” <em class="ltx_emph ltx_font_italic" id="bib.bib248.2.2">arXiv:2311.09519</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib249">
<span class="ltx_tag ltx_tag_bibitem">[249]</span>
<span class="ltx_bibblock">
H. Li, J. Zhang, C. Li, and H. Chen, “Resdsql: Decoupling schema linking and skeleton parsing for text-to-sql,” in <em class="ltx_emph ltx_font_italic" id="bib.bib249.1.1">AAAI</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib250">
<span class="ltx_tag ltx_tag_bibitem">[250]</span>
<span class="ltx_bibblock">
K. Zhang, X. Lin, Y. Wang <em class="ltx_emph ltx_font_italic" id="bib.bib250.1.1">et al.</em>, “Refsql: A retrieval-augmentation framework for text-to-sql generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib250.2.2">EMNLP Findings</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib251">
<span class="ltx_tag ltx_tag_bibitem">[251]</span>
<span class="ltx_bibblock">
S. Chang and E. Fosler-Lussier, “Selective demonstrations for cross-domain text-to-sql,” <em class="ltx_emph ltx_font_italic" id="bib.bib251.1.1">arXiv:2310.06302</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib252">
<span class="ltx_tag ltx_tag_bibitem">[252]</span>
<span class="ltx_bibblock">
L. Nan, Y. Zhao, W. Zou <em class="ltx_emph ltx_font_italic" id="bib.bib252.1.1">et al.</em>, “Enhancing text-to-sql capabilities of large language models: A study on prompt design strategies,” in <em class="ltx_emph ltx_font_italic" id="bib.bib252.2.2">EMNLP Findings</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib253">
<span class="ltx_tag ltx_tag_bibitem">[253]</span>
<span class="ltx_bibblock">
X. Zhang, D. Wang, L. Dou <em class="ltx_emph ltx_font_italic" id="bib.bib253.1.1">et al.</em>, “Multi-hop table retrieval for open-domain text-to-sql,” <em class="ltx_emph ltx_font_italic" id="bib.bib253.2.2">arXiv:2402.10666</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib254">
<span class="ltx_tag ltx_tag_bibitem">[254]</span>
<span class="ltx_bibblock">
H. Li, J. Zhang, H. Liu <em class="ltx_emph ltx_font_italic" id="bib.bib254.1.1">et al.</em>, “Codes: Towards building open-source language models for text-to-sql,” <em class="ltx_emph ltx_font_italic" id="bib.bib254.2.2">arXiv:2402.16347</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib255">
<span class="ltx_tag ltx_tag_bibitem">[255]</span>
<span class="ltx_bibblock">
Z. Jie and W. Lu, “Leveraging training data in few-shot prompting for numerical reasoning,” <em class="ltx_emph ltx_font_italic" id="bib.bib255.1.1">arXiv:2305.18170</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib256">
<span class="ltx_tag ltx_tag_bibitem">[256]</span>
<span class="ltx_bibblock">
M. Gao, J. Li, H. Fei <em class="ltx_emph ltx_font_italic" id="bib.bib256.1.1">et al.</em>, “De-fine: Decomposing and refining visual programs with auto-feedback,” <em class="ltx_emph ltx_font_italic" id="bib.bib256.2.2">arXiv:2311.12890</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib257">
<span class="ltx_tag ltx_tag_bibitem">[257]</span>
<span class="ltx_bibblock">
Y. Hao, W. Chen, Z. Zhou, and W. Cui, “E&amp;v: Prompting large language models to perform static analysis by pseudo-code execution and verification,” <em class="ltx_emph ltx_font_italic" id="bib.bib257.1.1">arXiv:2312.08477</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib258">
<span class="ltx_tag ltx_tag_bibitem">[258]</span>
<span class="ltx_bibblock">
Y. Guo, Z. Li <em class="ltx_emph ltx_font_italic" id="bib.bib258.1.1">et al.</em>, “Retrieval-augmented code generation for universal information extraction,” <em class="ltx_emph ltx_font_italic" id="bib.bib258.2.2">arXiv:2311.02962</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib259">
<span class="ltx_tag ltx_tag_bibitem">[259]</span>
<span class="ltx_bibblock">
G. Pinto, C. de Souza <em class="ltx_emph ltx_font_italic" id="bib.bib259.1.1">et al.</em>, “Lessons from building stackspot ai: A contextualized ai coding assistant,” <em class="ltx_emph ltx_font_italic" id="bib.bib259.2.2">arXiv:2311.18450</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib260">
<span class="ltx_tag ltx_tag_bibitem">[260]</span>
<span class="ltx_bibblock">
Z. Liu, C. Chen, J. Wang <em class="ltx_emph ltx_font_italic" id="bib.bib260.1.1">et al.</em>, “Testing the limits: Unusual text inputs generation for mobile app crash detection with large language model,” <em class="ltx_emph ltx_font_italic" id="bib.bib260.2.2">arXiv:2310.15657</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib261">
<span class="ltx_tag ltx_tag_bibitem">[261]</span>
<span class="ltx_bibblock">
K. D. Bollacker, C. Evans <em class="ltx_emph ltx_font_italic" id="bib.bib261.1.1">et al.</em>, “Freebase: a collaboratively created graph database for structuring human knowledge,” in <em class="ltx_emph ltx_font_italic" id="bib.bib261.2.2">SIGMOD</em>, 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib262">
<span class="ltx_tag ltx_tag_bibitem">[262]</span>
<span class="ltx_bibblock">
M. Patidar, A. K. Singh, R. Sawhney <em class="ltx_emph ltx_font_italic" id="bib.bib262.1.1">et al.</em>, “Combining transfer learning with in-context learning using blackbox llms for zero-shot knowledge base question answering,” <em class="ltx_emph ltx_font_italic" id="bib.bib262.2.2">arXiv:2311.08894</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib263">
<span class="ltx_tag ltx_tag_bibitem">[263]</span>
<span class="ltx_bibblock">
Y. Shu and Z. Yu, “Data distribution bottlenecks in grounding language models to knowledge bases,” <em class="ltx_emph ltx_font_italic" id="bib.bib263.1.1">arXiv:2309.08345</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib264">
<span class="ltx_tag ltx_tag_bibitem">[264]</span>
<span class="ltx_bibblock">
D. Leake and D. J. Crandall, “On bringing case-based reasoning methodology to deep learning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib264.1.1">ICCBR</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib265">
<span class="ltx_tag ltx_tag_bibitem">[265]</span>
<span class="ltx_bibblock">
L. Zhang, J. Zhang <em class="ltx_emph ltx_font_italic" id="bib.bib265.1.1">et al.</em>, “FC-KBQA: A fine-to-coarse composition framework for knowledge base question answering,” in <em class="ltx_emph ltx_font_italic" id="bib.bib265.2.2">ACL</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib266">
<span class="ltx_tag ltx_tag_bibitem">[266]</span>
<span class="ltx_bibblock">
J. Jiang, K. Zhou <em class="ltx_emph ltx_font_italic" id="bib.bib266.1.1">et al.</em>, “Structgpt: A general framework for large language model to reason over structured data,” in <em class="ltx_emph ltx_font_italic" id="bib.bib266.2.2">EMNLP</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib267">
<span class="ltx_tag ltx_tag_bibitem">[267]</span>
<span class="ltx_bibblock">
J. Baek, A. F. Aji, and A. Saffari, “Knowledge-augmented language model prompting for zero-shot knowledge graph question answering,” <em class="ltx_emph ltx_font_italic" id="bib.bib267.1.1">arXiv:2306.04136</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib268">
<span class="ltx_tag ltx_tag_bibitem">[268]</span>
<span class="ltx_bibblock">
P. Sen, S. Mavadia, and A. Saffari, “Knowledge graph-augmented language models for complex question answering,” in <em class="ltx_emph ltx_font_italic" id="bib.bib268.1.1">NLRSE</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib269">
<span class="ltx_tag ltx_tag_bibitem">[269]</span>
<span class="ltx_bibblock">
Y. Wu, N. Hu, S. Bi <em class="ltx_emph ltx_font_italic" id="bib.bib269.1.1">et al.</em>, “Retrieve-rewrite-answer: A kg-to-text enhanced llms framework for knowledge graph question answering,” <em class="ltx_emph ltx_font_italic" id="bib.bib269.2.2">arXiv:2309.11206</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib270">
<span class="ltx_tag ltx_tag_bibitem">[270]</span>
<span class="ltx_bibblock">
C. Wang, Y. Xu, Z. Peng <em class="ltx_emph ltx_font_italic" id="bib.bib270.1.1">et al.</em>, “keqing: knowledge-based question answering is a nature chain-of-thought mentor of LLM,” <em class="ltx_emph ltx_font_italic" id="bib.bib270.2.2">arXiv:2401.00426</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib271">
<span class="ltx_tag ltx_tag_bibitem">[271]</span>
<span class="ltx_bibblock">
J. Liu, S. Cao, J. Shi <em class="ltx_emph ltx_font_italic" id="bib.bib271.1.1">et al.</em>, “Probing structured semantics understanding and generation of language models via question answering,” <em class="ltx_emph ltx_font_italic" id="bib.bib271.2.2">arXiv:2401.05777</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib272">
<span class="ltx_tag ltx_tag_bibitem">[272]</span>
<span class="ltx_bibblock">
G. Xiong, J. Bao, and W. Zhao, “Interactive-kbqa: Multi-turn interactions for knowledge base question answering with large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib272.1.1">arXiv:2402.15131</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib273">
<span class="ltx_tag ltx_tag_bibitem">[273]</span>
<span class="ltx_bibblock">
S. Chen, Q. Liu, Z. Yu <em class="ltx_emph ltx_font_italic" id="bib.bib273.1.1">et al.</em>, “Retrack: A flexible and efficient framework for knowledge base question answering,” in <em class="ltx_emph ltx_font_italic" id="bib.bib273.2.2">ACL</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib274">
<span class="ltx_tag ltx_tag_bibitem">[274]</span>
<span class="ltx_bibblock">
D. Yu, C. Zhu, Y. Fang <em class="ltx_emph ltx_font_italic" id="bib.bib274.1.1">et al.</em>, “Kg-fid: Infusing knowledge graph in fusion-in-decoder for open-domain question answering,” in <em class="ltx_emph ltx_font_italic" id="bib.bib274.2.2">ACL</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib275">
<span class="ltx_tag ltx_tag_bibitem">[275]</span>
<span class="ltx_bibblock">
M. Ju, W. Yu, T. Zhao <em class="ltx_emph ltx_font_italic" id="bib.bib275.1.1">et al.</em>, “Grape: Knowledge graph enhanced passage reader for open-domain question answering,” in <em class="ltx_emph ltx_font_italic" id="bib.bib275.2.2">EMNLP Findings</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib276">
<span class="ltx_tag ltx_tag_bibitem">[276]</span>
<span class="ltx_bibblock">
Z. Hu, Y. Xu, W. Yu <em class="ltx_emph ltx_font_italic" id="bib.bib276.1.1">et al.</em>, “Empowering language models with knowledge graph reasoning for open-domain question answering,” in <em class="ltx_emph ltx_font_italic" id="bib.bib276.2.2">EMNLP</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib277">
<span class="ltx_tag ltx_tag_bibitem">[277]</span>
<span class="ltx_bibblock">
Q. Yang, Q. Chen, W. Wang <em class="ltx_emph ltx_font_italic" id="bib.bib277.1.1">et al.</em>, “Enhancing multi-modal multi-hop question answering via structured knowledge and unified retrieval-generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib277.2.2">MM</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib278">
<span class="ltx_tag ltx_tag_bibitem">[278]</span>
<span class="ltx_bibblock">
W. Zhao, Y. Liu, T. Niu <em class="ltx_emph ltx_font_italic" id="bib.bib278.1.1">et al.</em>, “DIVKNOWQA: assessing the reasoning ability of llms via open-domain question answering over knowledge base and text,” <em class="ltx_emph ltx_font_italic" id="bib.bib278.2.2">arXiv:2310.20170</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib279">
<span class="ltx_tag ltx_tag_bibitem">[279]</span>
<span class="ltx_bibblock">
X. Wang, Q. Yang, Y. Qiu <em class="ltx_emph ltx_font_italic" id="bib.bib279.1.1">et al.</em>, “Knowledgpt: Enhancing large language models with retrieval and storage access on knowledge bases,” <em class="ltx_emph ltx_font_italic" id="bib.bib279.2.2">arXiv:2308.11761</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib280">
<span class="ltx_tag ltx_tag_bibitem">[280]</span>
<span class="ltx_bibblock">
S. Ko, H. Cho, H. Chae <em class="ltx_emph ltx_font_italic" id="bib.bib280.1.1">et al.</em>, “Evidence-focused fact summarization for knowledge-augmented zero-shot question answering,” <em class="ltx_emph ltx_font_italic" id="bib.bib280.2.2">arXiv:2403.02966</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib281">
<span class="ltx_tag ltx_tag_bibitem">[281]</span>
<span class="ltx_bibblock">
Y. Gao, L. Qiao, Z. Kan <em class="ltx_emph ltx_font_italic" id="bib.bib281.1.1">et al.</em>, “Two-stage generative question answering on temporal knowledge graph using large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib281.2.2">arXiv:2402.16568</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib282">
<span class="ltx_tag ltx_tag_bibitem">[282]</span>
<span class="ltx_bibblock">
T. Guo, Q. Yang, C. Wang <em class="ltx_emph ltx_font_italic" id="bib.bib282.1.1">et al.</em>, “Knowledgenavigator: Leveraging large language models for enhanced reasoning over knowledge graph,” <em class="ltx_emph ltx_font_italic" id="bib.bib282.2.2">arXiv:2312.15880</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib283">
<span class="ltx_tag ltx_tag_bibitem">[283]</span>
<span class="ltx_bibblock">
C. Mavromatis and G. Karypis, “Gnn-rag: Graph neural retrieval for large language model reasoning,” <em class="ltx_emph ltx_font_italic" id="bib.bib283.1.1">arXiv:2405.20139</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib284">
<span class="ltx_tag ltx_tag_bibitem">[284]</span>
<span class="ltx_bibblock">
S. Min, J. Boyd-Graber, C. Alberti <em class="ltx_emph ltx_font_italic" id="bib.bib284.1.1">et al.</em>, “Neurips 2020 efficientqa competition: Systems, analyses and lessons learned,” in <em class="ltx_emph ltx_font_italic" id="bib.bib284.2.2">NeurIPS 2020 Competition and Demonstration Track</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib285">
<span class="ltx_tag ltx_tag_bibitem">[285]</span>
<span class="ltx_bibblock">
A. H. Li, P. Ng, P. Xu <em class="ltx_emph ltx_font_italic" id="bib.bib285.1.1">et al.</em>, “Dual reader-parser on hybrid textual and tabular evidence for open domain question answering,” in <em class="ltx_emph ltx_font_italic" id="bib.bib285.2.2">ACL/IJCNLP</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib286">
<span class="ltx_tag ltx_tag_bibitem">[286]</span>
<span class="ltx_bibblock">
K. Ma, H. Cheng, X. Liu <em class="ltx_emph ltx_font_italic" id="bib.bib286.1.1">et al.</em>, “Open-domain question answering via chain of reasoning over heterogeneous knowledge,” in <em class="ltx_emph ltx_font_italic" id="bib.bib286.2.2">EMNLP Findings</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib287">
<span class="ltx_tag ltx_tag_bibitem">[287]</span>
<span class="ltx_bibblock">
P. Christmann, R. S. Roy, and G. Weikum, “Conversational question answering on heterogeneous sources,” in <em class="ltx_emph ltx_font_italic" id="bib.bib287.1.1">SIGIR</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib288">
<span class="ltx_tag ltx_tag_bibitem">[288]</span>
<span class="ltx_bibblock">
E. Park, S.-M. Lee <em class="ltx_emph ltx_font_italic" id="bib.bib288.1.1">et al.</em>, “Rink: reader-inherited evidence reranker for table-and-text open domain question answering,” in <em class="ltx_emph ltx_font_italic" id="bib.bib288.2.2">AAAI</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib289">
<span class="ltx_tag ltx_tag_bibitem">[289]</span>
<span class="ltx_bibblock">
W. Zhao, Y. Liu, Y. Wan <em class="ltx_emph ltx_font_italic" id="bib.bib289.1.1">et al.</em>, “Localize, retrieve and fuse: A generalized framework for free-form question answering over tables,” <em class="ltx_emph ltx_font_italic" id="bib.bib289.2.2">arXiv:2309.11049</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib290">
<span class="ltx_tag ltx_tag_bibitem">[290]</span>
<span class="ltx_bibblock">
F. Pan, M. Canim <em class="ltx_emph ltx_font_italic" id="bib.bib290.1.1">et al.</em>, “End-to-end table question answering via retrieval-augmented generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib290.2.2">arXiv:2203.16714</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib291">
<span class="ltx_tag ltx_tag_bibitem">[291]</span>
<span class="ltx_bibblock">
Z. Jiang, Y. Mao, P. He <em class="ltx_emph ltx_font_italic" id="bib.bib291.1.1">et al.</em>, “Omnitab: Pretraining with natural and synthetic data for few-shot table-based question answering,” in <em class="ltx_emph ltx_font_italic" id="bib.bib291.2.2">NAACL</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib292">
<span class="ltx_tag ltx_tag_bibitem">[292]</span>
<span class="ltx_bibblock">
W. Zhong, J. Huang, Q. Liu <em class="ltx_emph ltx_font_italic" id="bib.bib292.1.1">et al.</em>, “Reasoning over hybrid chain for table-and-text open domain question answering,” in <em class="ltx_emph ltx_font_italic" id="bib.bib292.2.2">IJCAI</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib293">
<span class="ltx_tag ltx_tag_bibitem">[293]</span>
<span class="ltx_bibblock">
A. S. Sundar and L. Heck, “ctbl: Augmenting large language models for conversational tables,” <em class="ltx_emph ltx_font_italic" id="bib.bib293.1.1">arXiv:2303.12024</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib294">
<span class="ltx_tag ltx_tag_bibitem">[294]</span>
<span class="ltx_bibblock">
D. Min, N. Hu, R. Jin <em class="ltx_emph ltx_font_italic" id="bib.bib294.1.1">et al.</em>, “Exploring the impact of table-to-text methods on augmenting llm-based question answering with domain hybrid data,” <em class="ltx_emph ltx_font_italic" id="bib.bib294.2.2">arXiv:2402.12869</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib295">
<span class="ltx_tag ltx_tag_bibitem">[295]</span>
<span class="ltx_bibblock">
S. Roychowdhury, M. Krema <em class="ltx_emph ltx_font_italic" id="bib.bib295.1.1">et al.</em>, “Eratta: Extreme rag for table to answers with large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib295.2.2">arXiv:2405.03963</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib296">
<span class="ltx_tag ltx_tag_bibitem">[296]</span>
<span class="ltx_bibblock">
S. Wu, Y. Li, D. Zhang, and Z. Wu, “Improving knowledge-aware dialogue response generation by using human-written prototype dialogues,” in <em class="ltx_emph ltx_font_italic" id="bib.bib296.1.1">EMNLP Findings</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib297">
<span class="ltx_tag ltx_tag_bibitem">[297]</span>
<span class="ltx_bibblock">
M. Kang, J. M. Kwak <em class="ltx_emph ltx_font_italic" id="bib.bib297.1.1">et al.</em>, “Knowledge-consistent dialogue generation with knowledge graphs,” in <em class="ltx_emph ltx_font_italic" id="bib.bib297.2.2">ICML Workshop</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib298">
<span class="ltx_tag ltx_tag_bibitem">[298]</span>
<span class="ltx_bibblock">
Z. Ji, Z. Liu, N. Lee <em class="ltx_emph ltx_font_italic" id="bib.bib298.1.1">et al.</em>, “RHO: reducing hallucination in open-domain dialogues with knowledge grounding,” in <em class="ltx_emph ltx_font_italic" id="bib.bib298.2.2">ACL Findings</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib299">
<span class="ltx_tag ltx_tag_bibitem">[299]</span>
<span class="ltx_bibblock">
J. Baek, N. Chandrasekaran, S. Cucerzan <em class="ltx_emph ltx_font_italic" id="bib.bib299.1.1">et al.</em>, “Knowledge-augmented large language models for personalized contextual query suggestion,” <em class="ltx_emph ltx_font_italic" id="bib.bib299.2.2">arXiv:2311.06318</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib300">
<span class="ltx_tag ltx_tag_bibitem">[300]</span>
<span class="ltx_bibblock">
X. He, Y. Tian, Y. Sun <em class="ltx_emph ltx_font_italic" id="bib.bib300.1.1">et al.</em>, “G-retriever: Retrieval-augmented generation for textual graph understanding and question answering,” <em class="ltx_emph ltx_font_italic" id="bib.bib300.2.2">arXiv:2402.07630</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib301">
<span class="ltx_tag ltx_tag_bibitem">[301]</span>
<span class="ltx_bibblock">
M. M. Hussien, A. N. Melo <em class="ltx_emph ltx_font_italic" id="bib.bib301.1.1">et al.</em>, “Rag-based explainable prediction of road users behaviors for automated driving using knowledge graphs and large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib301.2.2">arXiv:2405.00449</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib302">
<span class="ltx_tag ltx_tag_bibitem">[302]</span>
<span class="ltx_bibblock">
B. J. Gutiérrez, Y. Shu <em class="ltx_emph ltx_font_italic" id="bib.bib302.1.1">et al.</em>, “Hipporag: Neurobiologically inspired long-term memory for large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib302.2.2">arXiv:2405.14831</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib303">
<span class="ltx_tag ltx_tag_bibitem">[303]</span>
<span class="ltx_bibblock">
Y. Kirstain, O. Levy, and A. Polyak, “X&amp;fuse: Fusing visual information in text-to-image generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib303.1.1">arXiv:2303.01000</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib304">
<span class="ltx_tag ltx_tag_bibitem">[304]</span>
<span class="ltx_bibblock">
P. Dhariwal and A. Nichol, “Diffusion models beat gans on image synthesis,” <em class="ltx_emph ltx_font_italic" id="bib.bib304.1.1">NeurIPS</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib305">
<span class="ltx_tag ltx_tag_bibitem">[305]</span>
<span class="ltx_bibblock">
L. Yang, Z. Yu, C. Meng <em class="ltx_emph ltx_font_italic" id="bib.bib305.1.1">et al.</em>, “Mastering text-to-image diffusion: Recaptioning, planning, and generating with multimodal llms,” <em class="ltx_emph ltx_font_italic" id="bib.bib305.2.2">arXiv:2401.11708</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib306">
<span class="ltx_tag ltx_tag_bibitem">[306]</span>
<span class="ltx_bibblock">
Z. Zhang, A. Zhang, M. Li <em class="ltx_emph ltx_font_italic" id="bib.bib306.1.1">et al.</em>, “Multimodal chain-of-thought reasoning in language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib306.2.2">arXiv:2302.00923</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib307">
<span class="ltx_tag ltx_tag_bibitem">[307]</span>
<span class="ltx_bibblock">
C. Xu, M. Yang, X. Ao <em class="ltx_emph ltx_font_italic" id="bib.bib307.1.1">et al.</em>, “Retrieval-enhanced adversarial training with dynamic memory-augmented attention for image paragraph captioning,” <em class="ltx_emph ltx_font_italic" id="bib.bib307.2.2">Knowledge-Based Systems</em>, vol. 214, p. 106730, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib308">
<span class="ltx_tag ltx_tag_bibitem">[308]</span>
<span class="ltx_bibblock">
R. Ramos, D. Elliott, and B. Martins, “Retrieval-augmented image captioning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib308.1.1">EACL</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib309">
<span class="ltx_tag ltx_tag_bibitem">[309]</span>
<span class="ltx_bibblock">
Z. Hu, A. Iscen, C. Sun <em class="ltx_emph ltx_font_italic" id="bib.bib309.1.1">et al.</em>, “Reveal: Retrieval-augmented visual-language pre-training with multi-source multimodal knowledge memory,” in <em class="ltx_emph ltx_font_italic" id="bib.bib309.2.2">CVPR</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib310">
<span class="ltx_tag ltx_tag_bibitem">[310]</span>
<span class="ltx_bibblock">
Z. Li, W. Zhao, X. Du <em class="ltx_emph ltx_font_italic" id="bib.bib310.1.1">et al.</em>, “Cross-modal retrieval and semantic refinement for remote sensing image captioning,” <em class="ltx_emph ltx_font_italic" id="bib.bib310.2.2">Remote Sensing</em>, vol. 16, no. 1, p. 196, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib311">
<span class="ltx_tag ltx_tag_bibitem">[311]</span>
<span class="ltx_bibblock">
Z. Yang, Z. Gan, J. Wang <em class="ltx_emph ltx_font_italic" id="bib.bib311.1.1">et al.</em>, “An empirical study of gpt-3 for few-shot knowledge-based vqa,” in <em class="ltx_emph ltx_font_italic" id="bib.bib311.2.2">AAAI</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib312">
<span class="ltx_tag ltx_tag_bibitem">[312]</span>
<span class="ltx_bibblock">
W. Lin and B. Byrne, “Retrieval augmented visual question answering with outside knowledge,” in <em class="ltx_emph ltx_font_italic" id="bib.bib312.1.1">EMNLP</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib313">
<span class="ltx_tag ltx_tag_bibitem">[313]</span>
<span class="ltx_bibblock">
A. Fan, C. Gardent, C. Braud, and A. Bordes, “Augmenting transformers with knn-based composite memory for dialog,” <em class="ltx_emph ltx_font_italic" id="bib.bib313.1.1">TACL</em>, vol. 9, pp. 82–99, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib314">
<span class="ltx_tag ltx_tag_bibitem">[314]</span>
<span class="ltx_bibblock">
Z. Liang, H. Hu, C. Xu <em class="ltx_emph ltx_font_italic" id="bib.bib314.1.1">et al.</em>, “Maria: A visual experience powered conversational agent,” in <em class="ltx_emph ltx_font_italic" id="bib.bib314.2.2">ACL-IJCNLP</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib315">
<span class="ltx_tag ltx_tag_bibitem">[315]</span>
<span class="ltx_bibblock">
Q. Fang and Y. Feng, “Neural machine translation with phrase-level universal visual representations,” in <em class="ltx_emph ltx_font_italic" id="bib.bib315.1.1">ACL</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib316">
<span class="ltx_tag ltx_tag_bibitem">[316]</span>
<span class="ltx_bibblock">
S. Whitehead, H. Ji, M. Bansal <em class="ltx_emph ltx_font_italic" id="bib.bib316.1.1">et al.</em>, “Incorporating background knowledge into video description generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib316.2.2">EMNLP</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib317">
<span class="ltx_tag ltx_tag_bibitem">[317]</span>
<span class="ltx_bibblock">
C. Yin, J. Tang, Z. Xu, and Y. Wang, “Memory augmented deep recurrent neural network for video question answering,” <em class="ltx_emph ltx_font_italic" id="bib.bib317.1.1">TNNLS</em>, vol. 31, no. 9, pp. 3159–3167, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib318">
<span class="ltx_tag ltx_tag_bibitem">[318]</span>
<span class="ltx_bibblock">
J. Pan, Z. Lin, Y. Ge <em class="ltx_emph ltx_font_italic" id="bib.bib318.1.1">et al.</em>, “Retrieving-to-answer: Zero-shot video question answering with frozen large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib318.2.2">ICCV</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib319">
<span class="ltx_tag ltx_tag_bibitem">[319]</span>
<span class="ltx_bibblock">
J. Lei, L. Yu, T. L. Berg, and M. Bansal, “Tvqa+: Spatio-temporal grounding for video question answering,” in <em class="ltx_emph ltx_font_italic" id="bib.bib319.1.1">ACL</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib320">
<span class="ltx_tag ltx_tag_bibitem">[320]</span>
<span class="ltx_bibblock">
H. Le, N. Chen, and S. Hoi, “Vgnmn: Video-grounded neural module networks for video-grounded dialogue systems,” in <em class="ltx_emph ltx_font_italic" id="bib.bib320.1.1">NAACL</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib321">
<span class="ltx_tag ltx_tag_bibitem">[321]</span>
<span class="ltx_bibblock">
Z. Wang, M. Li, R. Xu <em class="ltx_emph ltx_font_italic" id="bib.bib321.1.1">et al.</em>, “Language models with image descriptors are strong few-shot video-language learners,” in <em class="ltx_emph ltx_font_italic" id="bib.bib321.2.2">NeurIPS</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib322">
<span class="ltx_tag ltx_tag_bibitem">[322]</span>
<span class="ltx_bibblock">
J. Yuan, S. Sun, D. Omeiza <em class="ltx_emph ltx_font_italic" id="bib.bib322.1.1">et al.</em>, “Rag-driver: Generalisable driving explanations with retrieval-augmented in-context learning in multi-modal large language model,” <em class="ltx_emph ltx_font_italic" id="bib.bib322.2.2">arXiv:2402.10828</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib323">
<span class="ltx_tag ltx_tag_bibitem">[323]</span>
<span class="ltx_bibblock">
S. Ghosh, S. Kumar, C. K. R. Evuru <em class="ltx_emph ltx_font_italic" id="bib.bib323.1.1">et al.</em>, “Recap: retrieval-augmented audio captioning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib323.2.2">ICASSP</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib324">
<span class="ltx_tag ltx_tag_bibitem">[324]</span>
<span class="ltx_bibblock">
B. Elizalde, S. Deshmukh, and H. Wang, “Natural language supervision for general-purpose audio representations,” in <em class="ltx_emph ltx_font_italic" id="bib.bib324.1.1">ICASSP</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib325">
<span class="ltx_tag ltx_tag_bibitem">[325]</span>
<span class="ltx_bibblock">
T. Kouzelis and V. Katsouros, “Weakly-supervised automated audio captioning via text only training,” in <em class="ltx_emph ltx_font_italic" id="bib.bib325.1.1">DCASE Workshop</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib326">
<span class="ltx_tag ltx_tag_bibitem">[326]</span>
<span class="ltx_bibblock">
S. Deshmukh, B. Elizalde, D. Emmanouilidou <em class="ltx_emph ltx_font_italic" id="bib.bib326.1.1">et al.</em>, “Training audio captioning models without audio,” in <em class="ltx_emph ltx_font_italic" id="bib.bib326.2.2">ICASSP</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib327">
<span class="ltx_tag ltx_tag_bibitem">[327]</span>
<span class="ltx_bibblock">
Z. Wang, C. Lu, Y. Wang <em class="ltx_emph ltx_font_italic" id="bib.bib327.1.1">et al.</em>, “Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib327.2.2">NeurIPS</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib328">
<span class="ltx_tag ltx_tag_bibitem">[328]</span>
<span class="ltx_bibblock">
L. Yang, Z. Huang, X. Zhou <em class="ltx_emph ltx_font_italic" id="bib.bib328.1.1">et al.</em>, “Prompt-based 3d molecular diffusion models for structure-based drug design,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib329">
<span class="ltx_tag ltx_tag_bibitem">[329]</span>
<span class="ltx_bibblock">
T. Truong Jr and T. Bepler, “Poet: A generative model of protein families as sequences-of-sequences,” <em class="ltx_emph ltx_font_italic" id="bib.bib329.1.1">NeurIPS</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib330">
<span class="ltx_tag ltx_tag_bibitem">[330]</span>
<span class="ltx_bibblock">
G. Frisoni, M. Mizutani, G. Moro, and L. Valgimigli, “Bioreader: a retrieval-enhanced text-to-text transformer for biomedical literature,” in <em class="ltx_emph ltx_font_italic" id="bib.bib330.1.1">EMNLP</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib331">
<span class="ltx_tag ltx_tag_bibitem">[331]</span>
<span class="ltx_bibblock">
X. Yang, M. Ye, Q. You <em class="ltx_emph ltx_font_italic" id="bib.bib331.1.1">et al.</em>, “Writing by memorizing: Hierarchical retrieval-based medical report generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib331.2.2">arXiv:2106.06471</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib332">
<span class="ltx_tag ltx_tag_bibitem">[332]</span>
<span class="ltx_bibblock">
J. Kim and M. Min, “From rag to qa-rag: Integrating generative ai for pharmaceutical regulatory compliance process,” <em class="ltx_emph ltx_font_italic" id="bib.bib332.1.1">arXiv:2402.01717</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib333">
<span class="ltx_tag ltx_tag_bibitem">[333]</span>
<span class="ltx_bibblock">
Y. Ji, Z. Li <em class="ltx_emph ltx_font_italic" id="bib.bib333.1.1">et al.</em>, “Rag-rlrc-laysum at biolaysumm: Integrating retrieval-augmented generation and readability control for layman summarization of biomedical texts,” <em class="ltx_emph ltx_font_italic" id="bib.bib333.2.2">arXiv:2405.13179</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib334">
<span class="ltx_tag ltx_tag_bibitem">[334]</span>
<span class="ltx_bibblock">
K. Yang, A. Swope <em class="ltx_emph ltx_font_italic" id="bib.bib334.1.1">et al.</em>, “Leandojo: Theorem proving with retrieval-augmented language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib334.2.2">NeurIPS</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib335">
<span class="ltx_tag ltx_tag_bibitem">[335]</span>
<span class="ltx_bibblock">
Z. Levonian, C. Li, W. Zhu <em class="ltx_emph ltx_font_italic" id="bib.bib335.1.1">et al.</em>, “Retrieval-augmented generation to improve math question-answering: Trade-offs between groundedness and human preference,” <em class="ltx_emph ltx_font_italic" id="bib.bib335.2.2">arXiv:2310.03184</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib336">
<span class="ltx_tag ltx_tag_bibitem">[336]</span>
<span class="ltx_bibblock">
J. Chen, H. Lin, X. Han, and L. Sun, “Benchmarking large language models in retrieval-augmented generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib336.1.1">arxiv:2309.01431</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib337">
<span class="ltx_tag ltx_tag_bibitem">[337]</span>
<span class="ltx_bibblock">
S. ES, J. James, L. E. Anke, and S. Schockaert, “RAGAS: automated evaluation of retrieval augmented generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib337.1.1">arxiv:2309.15217</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib338">
<span class="ltx_tag ltx_tag_bibitem">[338]</span>
<span class="ltx_bibblock">
J. Saad-Falcon, O. Khattab, C. Potts <em class="ltx_emph ltx_font_italic" id="bib.bib338.1.1">et al.</em>, “ARES: an automated evaluation framework for retrieval-augmented generation systems,” <em class="ltx_emph ltx_font_italic" id="bib.bib338.2.2">arxiv:2311.09476</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib339">
<span class="ltx_tag ltx_tag_bibitem">[339]</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/truera/trulens" title="">https://github.com/truera/trulens</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib340">
<span class="ltx_tag ltx_tag_bibitem">[340]</span>
<span class="ltx_bibblock">
Y. Lyu, Z. Li, S. Niu <em class="ltx_emph ltx_font_italic" id="bib.bib340.1.1">et al.</em>, “CRUD-RAG: A comprehensive chinese benchmark for retrieval-augmented generation of large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib340.2.2">arxiv:2401.17043</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib341">
<span class="ltx_tag ltx_tag_bibitem">[341]</span>
<span class="ltx_bibblock">
G. Xiong, Q. Jin, Z. Lu, and A. Zhang, “Benchmarking retrieval-augmented generation for medicine,” <em class="ltx_emph ltx_font_italic" id="bib.bib341.1.1">arXiv:2402.13178</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib342">
<span class="ltx_tag ltx_tag_bibitem">[342]</span>
<span class="ltx_bibblock">
F. Petroni, A. Piktus <em class="ltx_emph ltx_font_italic" id="bib.bib342.1.1">et al.</em>, “Kilt: a benchmark for knowledge intensive language tasks,” in <em class="ltx_emph ltx_font_italic" id="bib.bib342.2.2">NAACL-HLT</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib343">
<span class="ltx_tag ltx_tag_bibitem">[343]</span>
<span class="ltx_bibblock">
S. Barnett, S. Kurniawan, S. Thudumu <em class="ltx_emph ltx_font_italic" id="bib.bib343.1.1">et al.</em>, “Seven failure points when engineering a retrieval augmented generation system,” <em class="ltx_emph ltx_font_italic" id="bib.bib343.2.2">arXiv:2401.05856</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib344">
<span class="ltx_tag ltx_tag_bibitem">[344]</span>
<span class="ltx_bibblock">
F. Cuconasu, G. Trappolini, F. Siciliano <em class="ltx_emph ltx_font_italic" id="bib.bib344.1.1">et al.</em>, “The power of noise: Redefining retrieval for RAG systems,” <em class="ltx_emph ltx_font_italic" id="bib.bib344.2.2">arXiv:2401.14887</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib345">
<span class="ltx_tag ltx_tag_bibitem">[345]</span>
<span class="ltx_bibblock">
L. Qiu, P. Shaw, P. Pasupat <em class="ltx_emph ltx_font_italic" id="bib.bib345.1.1">et al.</em>, “Evaluating the impact of model scale for compositional generalization in semantic parsing,” <em class="ltx_emph ltx_font_italic" id="bib.bib345.2.2">arXiv:2205.12253</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib346">
<span class="ltx_tag ltx_tag_bibitem">[346]</span>
<span class="ltx_bibblock">
R. Jagerman, H. Zhuang, Z. Qin <em class="ltx_emph ltx_font_italic" id="bib.bib346.1.1">et al.</em>, “Query expansion by prompting large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib346.2.2">arxiv:2305.03653</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib347">
<span class="ltx_tag ltx_tag_bibitem">[347]</span>
<span class="ltx_bibblock">
H. Zhang, P. Zhao, X. Miao <em class="ltx_emph ltx_font_italic" id="bib.bib347.1.1">et al.</em>, “Experimental analysis of large-scale learnable vector storage compression,” <em class="ltx_emph ltx_font_italic" id="bib.bib347.2.2">VLDB</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib348">
<span class="ltx_tag ltx_tag_bibitem">[348]</span>
<span class="ltx_bibblock">
R. Aksitov, C. Chang, D. Reitter <em class="ltx_emph ltx_font_italic" id="bib.bib348.1.1">et al.</em>, “Characterizing attribution and fluency tradeoffs for retrieval-augmented large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib348.2.2">arXiv:2302.05578</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib349">
<span class="ltx_tag ltx_tag_bibitem">[349]</span>
<span class="ltx_bibblock">
C. Han, Q. Wang, W. Xiong <em class="ltx_emph ltx_font_italic" id="bib.bib349.1.1">et al.</em>, “Lm-infinite: Simple on-the-fly length generalization for large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib349.2.2">arXiv:2308.16137</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib350">
<span class="ltx_tag ltx_tag_bibitem">[350]</span>
<span class="ltx_bibblock">
H. Chase, “Langchain,” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/langchain-ai/langchain" title="">https://github.com/langchain-ai/langchain</a>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib351">
<span class="ltx_tag ltx_tag_bibitem">[351]</span>
<span class="ltx_bibblock">
W. Jiang, S. Zhang, B. Han <em class="ltx_emph ltx_font_italic" id="bib.bib351.1.1">et al.</em>, “Piperag: Fast retrieval-augmented generation via algorithm-system co-design,” <em class="ltx_emph ltx_font_italic" id="bib.bib351.2.2">arXiv:2403.05676</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib352">
<span class="ltx_tag ltx_tag_bibitem">[352]</span>
<span class="ltx_bibblock">
K. Meduri <em class="ltx_emph ltx_font_italic" id="bib.bib352.1.1">et al.</em>, “Efficient rag framework for large-scale knowledge bases,” 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib353">
<span class="ltx_tag ltx_tag_bibitem">[353]</span>
<span class="ltx_bibblock">
S. Jindal, “Did google gemini 1.5 really kill rag?” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://analyticsindiamag.com/did-google-gemini-1-5-really-kill-rag/" title="">https://analyticsindiamag.com/did-google-gemini-1-5-really-kill-rag/</a>, 2024.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Jun 21 08:17:51 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
