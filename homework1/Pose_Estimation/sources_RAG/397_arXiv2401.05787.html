<!DOCTYPE html>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning</title>
<!--Generated on Thu Jan 11 09:47:40 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2401.05787v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S1" title="1 Introduction ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S2" title="2 Preliminaries ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Preliminaries</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S2.SS1" title="2.1 Chain-of-Thought (CoT) Prompting ‣ 2 Preliminaries ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Chain-of-Thought (CoT) Prompting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S2.SS2" title="2.2 Bottlenecks in Context-reasoning &amp; RAG ‣ 2 Preliminaries ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Bottlenecks in Context-reasoning &amp; RAG</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S3" title="3 Evidence to Generate (E2G) Prompting ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Evidence to Generate (<span class="ltx_text ltx_font_smallcaps">E2G</span>) Prompting</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS1" title="3.1 Single-agent ‣ 3 Evidence to Generate (E2G) Prompting ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Single-agent</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS2" title="3.2 E-step ‣ 3 Evidence to Generate (E2G) Prompting ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>E-step</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS3" title="3.3 G-Step ‣ 3 Evidence to Generate (E2G) Prompting ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>G-Step</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS4" title="3.4 Features, Adaptation and Best Practices ‣ 3 Evidence to Generate (E2G) Prompting ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Features, Adaptation and Best Practices</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S4" title="4 Experimental Setup ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experimental Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S5" title="5 Arithmetic/Logical Context Reasoning ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Arithmetic/Logical Context Reasoning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S6" title="6 Multi-hop QA w/ Distracting Contexts ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Multi-hop QA w/ Distracting Contexts</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S7" title="7 Retrieval Augmented Generation ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Retrieval Augmented Generation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S7.SS0.SSS0.Px1" title="Fact Verification ‣ 7 Retrieval Augmented Generation ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_title">Fact Verification</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S7.SS0.SSS0.Px2" title="Open-Domain Question Answering ‣ 7 Retrieval Augmented Generation ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_title">Open-Domain Question Answering</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S7.SS0.SSS0.Px3" title="Open-ended Long Form Generation ‣ 7 Retrieval Augmented Generation ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_title">Open-ended Long Form Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S8" title="8 Ablation on Robustness &amp; Limitations ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Ablation on Robustness &amp; Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S9" title="9 Related Work ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S10" title="10 Conclusion ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S11" title="11 Appendix ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">11 </span>Appendix</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S12" title="12 Limitations ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">12 </span>Limitations</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div aria-label="”Conversion" been="" class="package-alerts ltx_document" errors="" found”="" have="" role="“status”">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewbox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: inconsolata</li>
<li>failed: fdsymbol</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p>
</div><div class="section" id="target-section"><div id="license-tr">License: CC BY 4.0</div><div id="watermark-tr">arXiv:2401.05787v1 [cs.CL] 11 Jan 2024</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Md Rizwan Parvez

<br class="ltx_break"/>Qatar Computing Research Institute (QCRI)
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">mparvez@hbku.edu.qa, rizwan@cs.ucla.edu</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">While chain-of-thought (CoT) prompting has revolutionized how LLMs perform reasoning tasks, its current methods and variations (e.g, Self-consistency, ReACT, Reflexion, Tree-of-Thoughts (ToT), Cumulative Reasoning (CR)) suffer from limitations like slowness, limited context grounding, hallucination and inconsistent outputs. To overcome these challenges, we introduce <span class="ltx_text ltx_font_bold" id="id2.id1.1">Evidence to Generate (<span class="ltx_text ltx_font_smallcaps" id="id2.id1.1.1">E2G</span>)</span>, a novel single-agent, two-step prompting framework. Instead of unverified reasoning claims, this innovative approach leverages the power of "evidence for decision making" by first focusing exclusively on the thought sequences (the series of intermediate steps) explicitly mentioned in the context which then serve as extracted evidence, guiding the LLM’s output generation process with greater precision and efficiency. This simple yet powerful approach unlocks the true potential of chain-of-thought like prompting, paving the way for faster, more reliable, and more contextually aware reasoning in LLMs. <span class="ltx_text ltx_font_smallcaps" id="id2.id1.2">E2G</span> achieves remarkable results robustly across a wide range of knowledge-intensive reasoning and generation tasks, surpassing baseline approaches with state-of-the-art LLMs.
For example, (i) on LogiQA benchmark using GPT-4 as backbone model, <span class="ltx_text ltx_font_smallcaps" id="id2.id1.3">E2G</span> achieves a new state-of-the Accuracy of 53.8% exceeding CoT by 18%, ToT by 11%, CR by 9% (ii) a variant of <span class="ltx_text ltx_font_smallcaps" id="id2.id1.4">E2G</span> with PaLM2 outperforms the variable-shot performance of Gemini Ultra by 0.9 F1 points, reaching an F1 score of 83.3 on a subset of DROP.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<p class="ltx_p ltx_align_center ltx_align_bottom" id="p1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1">Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning</span></p>
</div>
<div class="ltx_para" id="p2">
<br class="ltx_break"/>
<p class="ltx_p" id="p2.1"><span class="ltx_text" id="p2.1.1" style="width:433.6pt;"><span class="ltx_text" id="p2.1.1.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p2.1.1.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p2.1.1.1.1.1.1">
<span class="ltx_td ltx_align_center" id="p2.1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p2.1.1.1.1.1.1.1.1">Md Rizwan Parvez</span></span></span>
<span class="ltx_tr" id="p2.1.1.1.1.2.2">
<span class="ltx_td ltx_align_center" id="p2.1.1.1.1.2.2.1">Qatar Computing Research Institute (QCRI)</span></span>
<span class="ltx_tr" id="p2.1.1.1.1.3.3">
<span class="ltx_td ltx_align_center" id="p2.1.1.1.1.3.3.1"><span class="ltx_text ltx_font_typewriter" id="p2.1.1.1.1.3.3.1.1">mparvez@hbku.edu.qa, rizwan@cs.ucla.edu</span></span></span>
</span>
</span></span> </span></p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="591" id="S1.F1.g1" src="x1.png" width="407"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>CoT &amp; variants falter in context-aware reasoning. 1. Overwhelming task complexity leads models’ failure even with partially/fully correct reasoning. 2. Ungrounded internal reasoning fails to grasp context, confusing "DoD" (ungroundeded company) vs Walmart. </figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Retrieval-augmented and context-based generation have revolutionized knowledge-intensive tasks, empowering large language models (LLMs) with access to vast repositories of domain-specific information and up-to-date knowledge <cite class="ltx_cite ltx_citemacro_citep">(Khattab et al., <a class="ltx_ref" href="#bib.bib14" title="">2022</a>; Lewis et al., <a class="ltx_ref" href="#bib.bib20" title="">2020</a>)</cite>. This access enables the generation of outputs that are factually accurate, interpretable, and tailored to specific domains <cite class="ltx_cite ltx_citemacro_citep">(Shuster et al., <a class="ltx_ref" href="#bib.bib38" title="">2021</a>; Guu et al., <a class="ltx_ref" href="#bib.bib7" title="">2020</a>; Parvez et al., <a class="ltx_ref" href="#bib.bib32" title="">2023</a>)</cite>. However, despite excelling at text understanding and generation, LLMs still face a critical challenge: the ability to reason effectively over context-based or retrieved information. This significantly hinders their full potential and necessitates innovative solutions to unlock the true power of contextual reasoning.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The Chain-of-Thought (CoT;<cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="#bib.bib44" title="">2022</a>)</cite>) prompting paradigm has emerged as a powerful tool for enhancing LLM reasoning capabilities. By simulating step-by-step thinking, it enables LLMs to break down complex problems into smaller, manageable chunks, leading to improved performance on various tasks. Several follow-up methods have been proposed to enhance CoT by generalizing it with various multi-agent (e.g, actor-critic), ensemble based, external tool-augmented, and trial &amp; error schema, including Self-consistency (SC;<cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="#bib.bib41" title="">2022</a>)</cite>), ReACT <cite class="ltx_cite ltx_citemacro_citep">(Yao et al., <a class="ltx_ref" href="#bib.bib48" title="">2022</a>)</cite>, Reflexion <cite class="ltx_cite ltx_citemacro_citep">(Shinn et al., <a class="ltx_ref" href="#bib.bib37" title="">2023</a>)</cite>, Tree of Thoughts (ToT; <cite class="ltx_cite ltx_citemacro_citep">(Yao et al., <a class="ltx_ref" href="#bib.bib47" title="">2023</a>)</cite>), and Cumulative Reasoning (CR; <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="#bib.bib52" title="">2023b</a>)</cite>). However, these methods’ hurdles in finding suitable prompts for each agent, requirement of external evaluation tools, and dependence on iterative prompting limits their applicability in real-time document reasoning tasks, making CoT the de facto approach in practical or time-sensitive applications <cite class="ltx_cite ltx_citemacro_citep">(Yasunaga et al., <a class="ltx_ref" href="#bib.bib49" title="">2023</a>)</cite>.
</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Nonetheless, while these approaches enhances the reasoning capabilities in various domains including arithmetic, commonsense and neuro-symbolic reasoning tasks, they do not address the problem of context-grounded or retrieval augmented (that are often long and imperfect text with distractive information) generation (RAG) (See Section <a class="ltx_ref" href="#S2.SS2" title="2.2 Bottlenecks in Context-reasoning &amp; RAG ‣ 2 Preliminaries ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_tag">2.2</span></a> and Figure <a class="ltx_ref" href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_tag">1</span></a>). Therefore, unlocking the true potentials of CoT like prompting for context-aware reasoning remains an open question. This paper delves into this critical challenge, exploring the limitations of existing approaches and proposing a novel framework, Evidence to Generate (<span class="ltx_text ltx_font_smallcaps" id="S1.p3.1.1">E2G</span>).</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1"><span class="ltx_text ltx_font_smallcaps" id="S1.p4.1.1">E2G</span> is a single-agent two-step framework particularly tailored for context-aware reasoning. In the first (E) step, our designed prompting explicitly asks for the reasoning paths ("evidences") that are mentioned in the given context–series of intermediate reasoning steps with directly extracted relevant rationales from the given context and then in G step: we pass only the "evidence" (or prepending them with the original context) for a second round to the LLM. Our technical novelty in this work is to consider the grounded reasoning path and solve the complex task via breaking into two sub steps. While this is a simple extension to the CoT setup, it is non-obvious why this works and how to setup the prompt to function correctly and robustly, and we analyze different alternatives. Our key distinction from existing CoT approaches is that instead of mere "thinking step-by-step" <cite class="ltx_cite ltx_citemacro_citep">(Kojima et al., <a class="ltx_ref" href="#bib.bib17" title="">2022</a>)</cite> our prompt instruction asks for the "evidence and explanation". By harnessing the power of thought sequences explicitly mentioned in the context, which we call as <em class="ltx_emph ltx_font_italic" id="S1.p4.1.2">Evidence</em>, <span class="ltx_text ltx_font_smallcaps" id="S1.p4.1.3">E2G</span> empowers LLMs for robust, accurate reasoning, even amidst distractions in a long context. Furthermore, our single-agent approach leverages the same prompt to instruct both E &amp; G steps with minimal adaptation, streamlining usage and removing the need for diverse prompt design.
</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Finally, in experiment, on a diverse set of eight context-driven tasks w/ multiple LLMs, we show that <span class="ltx_text ltx_font_smallcaps" id="S1.p5.1.1">E2G</span> outperforms existing approaches including natural QA, complex multi-hop &amp; long-form QA, fact verification, dialog generation, and reading comprehension tasks. We will release the collection of prompted outputs of these benchmarks as for community uses like distillation for open source LLMs via instruction tuning.</p>
</div>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="517" id="S1.F2.g1" src="x2.png" width="838"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>(left) CoT and generic view of its (iterative) variants, (right) The <span class="ltx_text ltx_font_smallcaps" id="S1.F2.2.1">E2G</span> pipeline: In E step our "generate ans/output with evidence and explanation" instruction first extracts the rationales, coupled with the ans, grounded in the original context, then in G step we use the same instruction to derive the final answer from the evidence only.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Preliminaries</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Chain-of-Thought (CoT) Prompting</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Chain-of-thought (CoT; <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="#bib.bib44" title="">2022</a>)</cite>) is a prompting framework that guides LLMs to produce intermediate reasoning steps towards the final answer, enhancing its reasoning. Original version of CoT employs a few-shot version by providing multiple exemplars of the reasoning
process (question–reasoning–answer), leveraging LLMs’ in-context learning abilities. However, due to the requirement of
labeled exemplars, it quickly evolved with a zero-shot instance <cite class="ltx_cite ltx_citemacro_citep">(Kojima et al., <a class="ltx_ref" href="#bib.bib17" title="">2022</a>)</cite>. Zero-shot CoT prompts LLMs with a general instruction like “think step by step” to produce intermediate reasoning steps (See Figure <a class="ltx_ref" href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Bottlenecks in Context-reasoning &amp; RAG</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Designing an inference framework for reasoning over given context document or for RAG is challenging that requires beyond context-free arithmetic or commonsense. We identify two main complexities: First, it needs to process the long input document, understand the query/question properly, reason over multiple distant relevant sentences/phrases in the imperfect retrieved text and then answer it exactly in the way the question was asked which are overwhelmingly complex. To further understand the bottleneck of existing approaches and mitigating them, we take a data driven approach by conducting an in-depth error analyses on the complex multihop QA on a set of 50 examples from the popular HotpotQA <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="#bib.bib46" title="">2018</a>)</cite> benchmark. We observe that in addition to the diminishing reasoning capability of LLMs when the text relevant to the user query lies in the middle of the retrieved context <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="#bib.bib24" title="">2023b</a>)</cite>, when the question is (linguistically, or in reasoning steps) hard or with long contexts, even with a valid reasoning made by CoT, LLMs fail to answer the question (See Figure <a class="ltx_ref" href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_tag">1</span></a>: top). In particular, among the erroneous <em class="ltx_emph ltx_font_italic" id="S2.SS2.p1.1.1">wh</em> questions, in 23% of them, the gold answer span is actually present in the reasoning, and for the erroneous <em class="ltx_emph ltx_font_italic" id="S2.SS2.p1.1.2">yes/no</em> questions, 75% of their reasoning actually hypotheses opposite of the predicted answer (e.g., "yes" should be derived from reasoning but the predicted answer is "no"). This suggests that exploring some post-processing can raise the upper bound of CoT’s effectiveness.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Second, the fundamental problem of not-grounding the reasoning in the context. We observe as CoT prompting does not aim to ground the path of decision making in the context and hence makes internal assumptions or imaginary hypothesis (a shortcut approach to bridge the reasoning paths a.k.a hallucination) that are often partially or fully incorrect. Specifically, in our analyses, 23% of erroneous <em class="ltx_emph ltx_font_italic" id="S2.SS2.p2.1.1">wh</em> and 25% of <em class="ltx_emph ltx_font_italic" id="S2.SS2.p2.1.2">yes/no</em> questions are of this category. (See Figure <a class="ltx_ref" href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_tag">1</span></a>: bottom)). This suggests a root change in the prompting strategy to focus on verification of the reasoning rationales.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Evidence to Generate (<span class="ltx_text ltx_font_smallcaps" id="S3.1.1">E2G</span>) Prompting</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In order to tailor a (prompting) inference framework for context grounded and retrieval augmented reasoning, we develop a new single-agent two-step prompting approach <span class="ltx_text ltx_font_smallcaps" id="S3.p1.1.1">E2G</span>. Below we discuss it.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Single-agent</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Our proposed framework is a single-agent one, has only one target task to solve. Given a target task <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p1.1.1">T</span> (e.g., QA; input: a context and a query <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p1.1.2">x</span>; output: a short/long answer <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p1.1.3">y</span>), we use the same generic objective/agent instruction (below) in both our steps in Section <a class="ltx_ref" href="#S3.SS2" title="3.2 E-step ‣ 3 Evidence to Generate (E2G) Prompting ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_tag">3.2</span></a> and <a class="ltx_ref" href="#S3.SS3" title="3.3 G-Step ‣ 3 Evidence to Generate (E2G) Prompting ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_tag">3.3</span></a>:
</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.ix1" style="list-style-type:none;">
<div class="ltx_para" id="S3.I1.ix1.p1">
<p class="ltx_p" id="S3.I1.ix1.p1.3"><span class="ltx_text ltx_font_typewriter" id="S3.I1.ix1.p1.3.3"># You are a/an [<math alttext="T" class="ltx_Math" display="inline" id="S3.I1.ix1.p1.1.1.m1.1"><semantics id="S3.I1.ix1.p1.1.1.m1.1a"><mi id="S3.I1.ix1.p1.1.1.m1.1.1" xref="S3.I1.ix1.p1.1.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.I1.ix1.p1.1.1.m1.1b"><ci id="S3.I1.ix1.p1.1.1.m1.1.1.cmml" xref="S3.I1.ix1.p1.1.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.ix1.p1.1.1.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.I1.ix1.p1.1.1.m1.1d">italic_T</annotation></semantics></math>] agent. Given a context and a [<math alttext="T[x]" class="ltx_Math" display="inline" id="S3.I1.ix1.p1.2.2.m2.1"><semantics id="S3.I1.ix1.p1.2.2.m2.1a"><mrow id="S3.I1.ix1.p1.2.2.m2.1.2" xref="S3.I1.ix1.p1.2.2.m2.1.2.cmml"><mi id="S3.I1.ix1.p1.2.2.m2.1.2.2" xref="S3.I1.ix1.p1.2.2.m2.1.2.2.cmml">T</mi><mo id="S3.I1.ix1.p1.2.2.m2.1.2.1" mathvariant="monospace" xref="S3.I1.ix1.p1.2.2.m2.1.2.1.cmml">⁢</mo><mrow id="S3.I1.ix1.p1.2.2.m2.1.2.3.2" xref="S3.I1.ix1.p1.2.2.m2.1.2.3.1.cmml"><mo id="S3.I1.ix1.p1.2.2.m2.1.2.3.2.1" mathvariant="normal" stretchy="false" xref="S3.I1.ix1.p1.2.2.m2.1.2.3.1.1.cmml">[</mo><mi id="S3.I1.ix1.p1.2.2.m2.1.1" xref="S3.I1.ix1.p1.2.2.m2.1.1.cmml">x</mi><mo id="S3.I1.ix1.p1.2.2.m2.1.2.3.2.2" mathvariant="normal" stretchy="false" xref="S3.I1.ix1.p1.2.2.m2.1.2.3.1.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.ix1.p1.2.2.m2.1b"><apply id="S3.I1.ix1.p1.2.2.m2.1.2.cmml" xref="S3.I1.ix1.p1.2.2.m2.1.2"><times id="S3.I1.ix1.p1.2.2.m2.1.2.1.cmml" xref="S3.I1.ix1.p1.2.2.m2.1.2.1"></times><ci id="S3.I1.ix1.p1.2.2.m2.1.2.2.cmml" xref="S3.I1.ix1.p1.2.2.m2.1.2.2">𝑇</ci><apply id="S3.I1.ix1.p1.2.2.m2.1.2.3.1.cmml" xref="S3.I1.ix1.p1.2.2.m2.1.2.3.2"><csymbol cd="latexml" id="S3.I1.ix1.p1.2.2.m2.1.2.3.1.1.cmml" xref="S3.I1.ix1.p1.2.2.m2.1.2.3.2.1">delimited-[]</csymbol><ci id="S3.I1.ix1.p1.2.2.m2.1.1.cmml" xref="S3.I1.ix1.p1.2.2.m2.1.1">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.ix1.p1.2.2.m2.1c">T[x]</annotation><annotation encoding="application/x-llamapun" id="S3.I1.ix1.p1.2.2.m2.1d">italic_T [ italic_x ]</annotation></semantics></math>] (e.g., question, claim, previous dialogue) as input, please give a [<math alttext="T[y]" class="ltx_Math" display="inline" id="S3.I1.ix1.p1.3.3.m3.1"><semantics id="S3.I1.ix1.p1.3.3.m3.1a"><mrow id="S3.I1.ix1.p1.3.3.m3.1.2" xref="S3.I1.ix1.p1.3.3.m3.1.2.cmml"><mi id="S3.I1.ix1.p1.3.3.m3.1.2.2" xref="S3.I1.ix1.p1.3.3.m3.1.2.2.cmml">T</mi><mo id="S3.I1.ix1.p1.3.3.m3.1.2.1" mathvariant="monospace" xref="S3.I1.ix1.p1.3.3.m3.1.2.1.cmml">⁢</mo><mrow id="S3.I1.ix1.p1.3.3.m3.1.2.3.2" xref="S3.I1.ix1.p1.3.3.m3.1.2.3.1.cmml"><mo id="S3.I1.ix1.p1.3.3.m3.1.2.3.2.1" mathvariant="normal" stretchy="false" xref="S3.I1.ix1.p1.3.3.m3.1.2.3.1.1.cmml">[</mo><mi id="S3.I1.ix1.p1.3.3.m3.1.1" xref="S3.I1.ix1.p1.3.3.m3.1.1.cmml">y</mi><mo id="S3.I1.ix1.p1.3.3.m3.1.2.3.2.2" mathvariant="normal" stretchy="false" xref="S3.I1.ix1.p1.3.3.m3.1.2.3.1.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.ix1.p1.3.3.m3.1b"><apply id="S3.I1.ix1.p1.3.3.m3.1.2.cmml" xref="S3.I1.ix1.p1.3.3.m3.1.2"><times id="S3.I1.ix1.p1.3.3.m3.1.2.1.cmml" xref="S3.I1.ix1.p1.3.3.m3.1.2.1"></times><ci id="S3.I1.ix1.p1.3.3.m3.1.2.2.cmml" xref="S3.I1.ix1.p1.3.3.m3.1.2.2">𝑇</ci><apply id="S3.I1.ix1.p1.3.3.m3.1.2.3.1.cmml" xref="S3.I1.ix1.p1.3.3.m3.1.2.3.2"><csymbol cd="latexml" id="S3.I1.ix1.p1.3.3.m3.1.2.3.1.1.cmml" xref="S3.I1.ix1.p1.3.3.m3.1.2.3.2.1">delimited-[]</csymbol><ci id="S3.I1.ix1.p1.3.3.m3.1.1.cmml" xref="S3.I1.ix1.p1.3.3.m3.1.1">𝑦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.ix1.p1.3.3.m3.1c">T[y]</annotation><annotation encoding="application/x-llamapun" id="S3.I1.ix1.p1.3.3.m3.1d">italic_T [ italic_y ]</annotation></semantics></math>] (e.g., answer, judgement, next turn dialogue) output based on the context. </span></p>
</div>
</li>
<li class="ltx_item" id="S3.I1.ix2" style="list-style-type:none;">
<div class="ltx_para" id="S3.I1.ix2.p1">
<p class="ltx_p" id="S3.I1.ix2.p1.1"><span class="ltx_text ltx_font_typewriter" id="S3.I1.ix2.p1.1.1">E.g., <span class="ltx_text" id="S3.I1.ix2.p1.1.1.1" style="background-color:#CCCCFF;"># You are a text classification </span>
<br class="ltx_break"/><span class="ltx_text" id="S3.I1.ix2.p1.1.1.2" style="background-color:#CCCCFF;"> agent. Given a context and a claim,</span>
<br class="ltx_break"/><span class="ltx_text" id="S3.I1.ix2.p1.1.1.3" style="background-color:#CCCCFF;"> please give a judgement to the </span>
<br class="ltx_break"/><span class="ltx_text" id="S3.I1.ix2.p1.1.1.4" style="background-color:#CCCCFF;"> claim (’SUPPORTS’ or ’REFUTES’)</span>
<br class="ltx_break"/><span class="ltx_text" id="S3.I1.ix2.p1.1.1.5" style="background-color:#CCCCFF;"> based on the context.</span></span></p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>E-step</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.3">In our first step, along with the agent objective (above), we instruct the model to generate the rationales or the evidence from the context towards the output answer. We note that there are multiple ways to prompt for generating the relevant chunks in the context. For example, asking LLM to act as context retrieval/ranking agent or to perform sentence level ranking of the context. However, we find all of these approaches are sub-optimal as rationales are objective to the final target output. Prompting to perform intermediate goals hinders the model to realize how they will actually be used for generating the final answer. Therefore, our decision choice considers to prompt the same way as CoT (answer-reasoning). To do so (i.e., "finding evidence for output"), instead of CoT’s instruction of <span class="ltx_text" id="S3.SS2.p1.3.1" style="background-color:#FFCCCC;">Think step-by-step</span>, we prompt <span class="ltx_text" id="S3.SS2.p1.3.2" style="background-color:#CCFFCC;">Generate the answer w/ evidence and explanation</span> (we note it as <span class="ltx_text ltx_font_typewriter ltx_font_smallcaps" id="S3.SS2.p1.3.3">E2G<span class="ltx_text ltx_font_upright" id="S3.SS2.p1.3.3.1">-base</span></span>). This guides LLMs to produce intermediate reasoning steps towards the final answer that are explicitly evident from the context, empirically reducing the overall error rate mentioned in Second problem Section <a class="ltx_ref" href="#S2.SS2" title="2.2 Bottlenecks in Context-reasoning &amp; RAG ‣ 2 Preliminaries ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_tag">2.2</span></a> to 8% where it produces a few new errors for numerical/arithmetic reasoning problems that are implicit and do not require grounding (e.g., person w/ former birth date is older than w/ later birth date). Below is our E-step model input and we discuss further adaption addressing the limitations (i.e., new/additional errors) in Section <a class="ltx_ref" href="#S3.SS4" title="3.4 Features, Adaptation and Best Practices ‣ 3 Evidence to Generate (E2G) Prompting ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_tag">3.4</span></a>.</p>
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.ix1" style="list-style-type:none;">
<div class="ltx_para" id="S3.I2.ix1.p1">
<p class="ltx_p" id="S3.I2.ix1.p1.1"><span class="ltx_text ltx_font_typewriter" id="S3.I2.ix1.p1.1.1"># Agent Instruction (as in Section <a class="ltx_ref" href="#S3.SS1" title="3.1 Single-agent ‣ 3 Evidence to Generate (E2G) Prompting ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_tag">3.1</span></a>)</span></p>
</div>
</li>
<li class="ltx_item" id="S3.I2.ix2" style="list-style-type:none;">
<div class="ltx_para" id="S3.I2.ix2.p1">
<p class="ltx_p" id="S3.I2.ix2.p1.1"><span class="ltx_text ltx_font_typewriter" id="S3.I2.ix2.p1.1.1"># Generate the answer with evidence and explanation</span></p>
</div>
</li>
<li class="ltx_item" id="S3.I2.ix3" style="list-style-type:none;">
<div class="ltx_para" id="S3.I2.ix3.p1">
<p class="ltx_p" id="S3.I2.ix3.p1.1"><span class="ltx_text ltx_font_typewriter" id="S3.I2.ix3.p1.1.1"># Context: [Context(<math alttext="T[x_{i}]" class="ltx_Math" display="inline" id="S3.I2.ix3.p1.1.1.m1.1"><semantics id="S3.I2.ix3.p1.1.1.m1.1a"><mrow id="S3.I2.ix3.p1.1.1.m1.1.1" xref="S3.I2.ix3.p1.1.1.m1.1.1.cmml"><mi id="S3.I2.ix3.p1.1.1.m1.1.1.3" xref="S3.I2.ix3.p1.1.1.m1.1.1.3.cmml">T</mi><mo id="S3.I2.ix3.p1.1.1.m1.1.1.2" mathvariant="monospace" xref="S3.I2.ix3.p1.1.1.m1.1.1.2.cmml">⁢</mo><mrow id="S3.I2.ix3.p1.1.1.m1.1.1.1.1" xref="S3.I2.ix3.p1.1.1.m1.1.1.1.2.cmml"><mo id="S3.I2.ix3.p1.1.1.m1.1.1.1.1.2" mathvariant="normal" stretchy="false" xref="S3.I2.ix3.p1.1.1.m1.1.1.1.2.1.cmml">[</mo><msub id="S3.I2.ix3.p1.1.1.m1.1.1.1.1.1" xref="S3.I2.ix3.p1.1.1.m1.1.1.1.1.1.cmml"><mi id="S3.I2.ix3.p1.1.1.m1.1.1.1.1.1.2" xref="S3.I2.ix3.p1.1.1.m1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.I2.ix3.p1.1.1.m1.1.1.1.1.1.3" xref="S3.I2.ix3.p1.1.1.m1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.I2.ix3.p1.1.1.m1.1.1.1.1.3" mathvariant="normal" stretchy="false" xref="S3.I2.ix3.p1.1.1.m1.1.1.1.2.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I2.ix3.p1.1.1.m1.1b"><apply id="S3.I2.ix3.p1.1.1.m1.1.1.cmml" xref="S3.I2.ix3.p1.1.1.m1.1.1"><times id="S3.I2.ix3.p1.1.1.m1.1.1.2.cmml" xref="S3.I2.ix3.p1.1.1.m1.1.1.2"></times><ci id="S3.I2.ix3.p1.1.1.m1.1.1.3.cmml" xref="S3.I2.ix3.p1.1.1.m1.1.1.3">𝑇</ci><apply id="S3.I2.ix3.p1.1.1.m1.1.1.1.2.cmml" xref="S3.I2.ix3.p1.1.1.m1.1.1.1.1"><csymbol cd="latexml" id="S3.I2.ix3.p1.1.1.m1.1.1.1.2.1.cmml" xref="S3.I2.ix3.p1.1.1.m1.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.I2.ix3.p1.1.1.m1.1.1.1.1.1.cmml" xref="S3.I2.ix3.p1.1.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.I2.ix3.p1.1.1.m1.1.1.1.1.1.1.cmml" xref="S3.I2.ix3.p1.1.1.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.I2.ix3.p1.1.1.m1.1.1.1.1.1.2.cmml" xref="S3.I2.ix3.p1.1.1.m1.1.1.1.1.1.2">𝑥</ci><ci id="S3.I2.ix3.p1.1.1.m1.1.1.1.1.1.3.cmml" xref="S3.I2.ix3.p1.1.1.m1.1.1.1.1.1.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.ix3.p1.1.1.m1.1c">T[x_{i}]</annotation><annotation encoding="application/x-llamapun" id="S3.I2.ix3.p1.1.1.m1.1d">italic_T [ italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ]</annotation></semantics></math>])] </span></p>
</div>
</li>
<li class="ltx_item" id="S3.I2.ix4" style="list-style-type:none;">
<div class="ltx_para" id="S3.I2.ix4.p1">
<p class="ltx_p" id="S3.I2.ix4.p1.1"><span class="ltx_text ltx_font_typewriter" id="S3.I2.ix4.p1.1.1"># Question: [<math alttext="T[x_{i}]" class="ltx_Math" display="inline" id="S3.I2.ix4.p1.1.1.m1.1"><semantics id="S3.I2.ix4.p1.1.1.m1.1a"><mrow id="S3.I2.ix4.p1.1.1.m1.1.1" xref="S3.I2.ix4.p1.1.1.m1.1.1.cmml"><mi id="S3.I2.ix4.p1.1.1.m1.1.1.3" xref="S3.I2.ix4.p1.1.1.m1.1.1.3.cmml">T</mi><mo id="S3.I2.ix4.p1.1.1.m1.1.1.2" mathvariant="monospace" xref="S3.I2.ix4.p1.1.1.m1.1.1.2.cmml">⁢</mo><mrow id="S3.I2.ix4.p1.1.1.m1.1.1.1.1" xref="S3.I2.ix4.p1.1.1.m1.1.1.1.2.cmml"><mo id="S3.I2.ix4.p1.1.1.m1.1.1.1.1.2" mathvariant="normal" stretchy="false" xref="S3.I2.ix4.p1.1.1.m1.1.1.1.2.1.cmml">[</mo><msub id="S3.I2.ix4.p1.1.1.m1.1.1.1.1.1" xref="S3.I2.ix4.p1.1.1.m1.1.1.1.1.1.cmml"><mi id="S3.I2.ix4.p1.1.1.m1.1.1.1.1.1.2" xref="S3.I2.ix4.p1.1.1.m1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.I2.ix4.p1.1.1.m1.1.1.1.1.1.3" xref="S3.I2.ix4.p1.1.1.m1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.I2.ix4.p1.1.1.m1.1.1.1.1.3" mathvariant="normal" stretchy="false" xref="S3.I2.ix4.p1.1.1.m1.1.1.1.2.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I2.ix4.p1.1.1.m1.1b"><apply id="S3.I2.ix4.p1.1.1.m1.1.1.cmml" xref="S3.I2.ix4.p1.1.1.m1.1.1"><times id="S3.I2.ix4.p1.1.1.m1.1.1.2.cmml" xref="S3.I2.ix4.p1.1.1.m1.1.1.2"></times><ci id="S3.I2.ix4.p1.1.1.m1.1.1.3.cmml" xref="S3.I2.ix4.p1.1.1.m1.1.1.3">𝑇</ci><apply id="S3.I2.ix4.p1.1.1.m1.1.1.1.2.cmml" xref="S3.I2.ix4.p1.1.1.m1.1.1.1.1"><csymbol cd="latexml" id="S3.I2.ix4.p1.1.1.m1.1.1.1.2.1.cmml" xref="S3.I2.ix4.p1.1.1.m1.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.I2.ix4.p1.1.1.m1.1.1.1.1.1.cmml" xref="S3.I2.ix4.p1.1.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.I2.ix4.p1.1.1.m1.1.1.1.1.1.1.cmml" xref="S3.I2.ix4.p1.1.1.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.I2.ix4.p1.1.1.m1.1.1.1.1.1.2.cmml" xref="S3.I2.ix4.p1.1.1.m1.1.1.1.1.1.2">𝑥</ci><ci id="S3.I2.ix4.p1.1.1.m1.1.1.1.1.1.3.cmml" xref="S3.I2.ix4.p1.1.1.m1.1.1.1.1.1.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.ix4.p1.1.1.m1.1c">T[x_{i}]</annotation><annotation encoding="application/x-llamapun" id="S3.I2.ix4.p1.1.1.m1.1d">italic_T [ italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ]</annotation></semantics></math>] </span></p>
</div>
</li>
</ul>
<p class="ltx_p" id="S3.SS2.p1.2">Figure <a class="ltx_ref" href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_tag">2</span></a> (top) shows a full example (w/o the agent instruction) for an example QA task. Being prompted, the model outputs a temporary answer (<math alttext="A_{temp}" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><msub id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">A</mi><mrow id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.p1.1.m1.1.1.3.2" xref="S3.SS2.p1.1.m1.1.1.3.2.cmml">t</mi><mo id="S3.SS2.p1.1.m1.1.1.3.1" xref="S3.SS2.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p1.1.m1.1.1.3.3" xref="S3.SS2.p1.1.m1.1.1.3.3.cmml">e</mi><mo id="S3.SS2.p1.1.m1.1.1.3.1a" xref="S3.SS2.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p1.1.m1.1.1.3.4" xref="S3.SS2.p1.1.m1.1.1.3.4.cmml">m</mi><mo id="S3.SS2.p1.1.m1.1.1.3.1b" xref="S3.SS2.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p1.1.m1.1.1.3.5" xref="S3.SS2.p1.1.m1.1.1.3.5.cmml">p</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">𝐴</ci><apply id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3"><times id="S3.SS2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.p1.1.m1.1.1.3.1"></times><ci id="S3.SS2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.p1.1.m1.1.1.3.2">𝑡</ci><ci id="S3.SS2.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3">𝑒</ci><ci id="S3.SS2.p1.1.m1.1.1.3.4.cmml" xref="S3.SS2.p1.1.m1.1.1.3.4">𝑚</ci><ci id="S3.SS2.p1.1.m1.1.1.3.5.cmml" xref="S3.SS2.p1.1.m1.1.1.3.5">𝑝</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">A_{temp}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_A start_POSTSUBSCRIPT italic_t italic_e italic_m italic_p end_POSTSUBSCRIPT</annotation></semantics></math>) and the "evidence and explanation" (<math alttext="Evidence" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><mrow id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">E</mi><mo id="S3.SS2.p1.2.m2.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">v</mi><mo id="S3.SS2.p1.2.m2.1.1.1a" xref="S3.SS2.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p1.2.m2.1.1.4" xref="S3.SS2.p1.2.m2.1.1.4.cmml">i</mi><mo id="S3.SS2.p1.2.m2.1.1.1b" xref="S3.SS2.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p1.2.m2.1.1.5" xref="S3.SS2.p1.2.m2.1.1.5.cmml">d</mi><mo id="S3.SS2.p1.2.m2.1.1.1c" xref="S3.SS2.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p1.2.m2.1.1.6" xref="S3.SS2.p1.2.m2.1.1.6.cmml">e</mi><mo id="S3.SS2.p1.2.m2.1.1.1d" xref="S3.SS2.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p1.2.m2.1.1.7" xref="S3.SS2.p1.2.m2.1.1.7.cmml">n</mi><mo id="S3.SS2.p1.2.m2.1.1.1e" xref="S3.SS2.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p1.2.m2.1.1.8" xref="S3.SS2.p1.2.m2.1.1.8.cmml">c</mi><mo id="S3.SS2.p1.2.m2.1.1.1f" xref="S3.SS2.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p1.2.m2.1.1.9" xref="S3.SS2.p1.2.m2.1.1.9.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><times id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1"></times><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">𝐸</ci><ci id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3">𝑣</ci><ci id="S3.SS2.p1.2.m2.1.1.4.cmml" xref="S3.SS2.p1.2.m2.1.1.4">𝑖</ci><ci id="S3.SS2.p1.2.m2.1.1.5.cmml" xref="S3.SS2.p1.2.m2.1.1.5">𝑑</ci><ci id="S3.SS2.p1.2.m2.1.1.6.cmml" xref="S3.SS2.p1.2.m2.1.1.6">𝑒</ci><ci id="S3.SS2.p1.2.m2.1.1.7.cmml" xref="S3.SS2.p1.2.m2.1.1.7">𝑛</ci><ci id="S3.SS2.p1.2.m2.1.1.8.cmml" xref="S3.SS2.p1.2.m2.1.1.8">𝑐</ci><ci id="S3.SS2.p1.2.m2.1.1.9.cmml" xref="S3.SS2.p1.2.m2.1.1.9">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">Evidence</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">italic_E italic_v italic_i italic_d italic_e italic_n italic_c italic_e</annotation></semantics></math>).
Along with statistical motivation, to further understand why it works, we deliberately ask two different State-of-the-art (SOTA) LLMs (ChapGPT and Gemini Pro) the internal advantages of our designed instruction over CoT’s. Below we summarize it.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<ul class="ltx_itemize" id="S3.I3">
<li class="ltx_item" id="S3.I3.ix1" style="list-style-type:none;">
<div class="ltx_para" id="S3.I3.ix1.p1">
<p class="ltx_p" id="S3.I3.ix1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I3.ix1.p1.1.1">Logical Reasoning:</span> By prompting for explanations along with evidence, this promotes a more structured and logical thought process, reducing unsupported or illogical statements.
</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.ix2" style="list-style-type:none;">
<div class="ltx_para" id="S3.I3.ix2.p1">
<p class="ltx_p" id="S3.I3.ix2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I3.ix2.p1.1.1">Factual Basis:</span> Explicitly asking to focus on justifying its answer by providing evidence &amp; explanation encourages the LLM to ground its reasoning in the context and relevant facts, making it less likely to resort to imaginary or unsupported claims.</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.ix3" style="list-style-type:none;">
<div class="ltx_para" id="S3.I3.ix3.p1">
<p class="ltx_p" id="S3.I3.ix3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I3.ix3.p1.1.1">Reduced Speculation:</span> Prompting for evidence encourages to rely on what is known or can be reasonably inferred from existing information.</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.ix4" style="list-style-type:none;">
<div class="ltx_para" id="S3.I3.ix4.p1">
<p class="ltx_p" id="S3.I3.ix4.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I3.ix4.p1.1.1">Accountability:</span> When prompted to provide evidence, models are held accountable for the accuracy and reliability of their responses.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>G-Step</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">As suggested in Section <a class="ltx_ref" href="#S2.SS2" title="2.2 Bottlenecks in Context-reasoning &amp; RAG ‣ 2 Preliminaries ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_tag">2.2</span></a>, our second step (’G’-step) is a query aware post-processing where we seek to re-prompt the LLM but with simplification of task complexity by providing very short relevant-only context (i.e., strightforward answer deriving statements w/o no further multi-hop or distant sentence reasoning). We employ the same instruction prompt <span class="ltx_text ltx_font_typewriter ltx_font_smallcaps" id="S3.SS3.p1.1.1">E2G<span class="ltx_text ltx_font_upright" id="S3.SS3.p1.1.1.1">-base</span></span> but using the model output <math alttext="Evidence" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mrow id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">E</mi><mo id="S3.SS3.p1.1.m1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">v</mi><mo id="S3.SS3.p1.1.m1.1.1.1a" xref="S3.SS3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS3.p1.1.m1.1.1.4" xref="S3.SS3.p1.1.m1.1.1.4.cmml">i</mi><mo id="S3.SS3.p1.1.m1.1.1.1b" xref="S3.SS3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS3.p1.1.m1.1.1.5" xref="S3.SS3.p1.1.m1.1.1.5.cmml">d</mi><mo id="S3.SS3.p1.1.m1.1.1.1c" xref="S3.SS3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS3.p1.1.m1.1.1.6" xref="S3.SS3.p1.1.m1.1.1.6.cmml">e</mi><mo id="S3.SS3.p1.1.m1.1.1.1d" xref="S3.SS3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS3.p1.1.m1.1.1.7" xref="S3.SS3.p1.1.m1.1.1.7.cmml">n</mi><mo id="S3.SS3.p1.1.m1.1.1.1e" xref="S3.SS3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS3.p1.1.m1.1.1.8" xref="S3.SS3.p1.1.m1.1.1.8.cmml">c</mi><mo id="S3.SS3.p1.1.m1.1.1.1f" xref="S3.SS3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS3.p1.1.m1.1.1.9" xref="S3.SS3.p1.1.m1.1.1.9.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><times id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1"></times><ci id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">𝐸</ci><ci id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">𝑣</ci><ci id="S3.SS3.p1.1.m1.1.1.4.cmml" xref="S3.SS3.p1.1.m1.1.1.4">𝑖</ci><ci id="S3.SS3.p1.1.m1.1.1.5.cmml" xref="S3.SS3.p1.1.m1.1.1.5">𝑑</ci><ci id="S3.SS3.p1.1.m1.1.1.6.cmml" xref="S3.SS3.p1.1.m1.1.1.6">𝑒</ci><ci id="S3.SS3.p1.1.m1.1.1.7.cmml" xref="S3.SS3.p1.1.m1.1.1.7">𝑛</ci><ci id="S3.SS3.p1.1.m1.1.1.8.cmml" xref="S3.SS3.p1.1.m1.1.1.8">𝑐</ci><ci id="S3.SS3.p1.1.m1.1.1.9.cmml" xref="S3.SS3.p1.1.m1.1.1.9">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">Evidence</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">italic_E italic_v italic_i italic_d italic_e italic_n italic_c italic_e</annotation></semantics></math> of previous step as the new "Context" (Figure <a class="ltx_ref" href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_tag">2</span></a>: bottom). Model output from this prompt is used as the final answer.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T1.6" style="width:433.6pt;height:184.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(26.8pt,-11.4pt) scale(1.14089886517223,1.14089886517223) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.6">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.2.2.2">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.2.2.2.3">Dataset</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.2.2.4">Size</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.2.2.5">Reasoning</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.2.2.2">
<math alttext="|" class="ltx_Math" display="inline" id="S3.T1.1.1.1.1.m1.1"><semantics id="S3.T1.1.1.1.1.m1.1a"><mo fence="false" id="S3.T1.1.1.1.1.m1.1.1" stretchy="false" xref="S3.T1.1.1.1.1.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.1.m1.1b"><ci id="S3.T1.1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.1.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.1.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S3.T1.1.1.1.1.m1.1d">|</annotation></semantics></math>Context<math alttext="|" class="ltx_Math" display="inline" id="S3.T1.2.2.2.2.m2.1"><semantics id="S3.T1.2.2.2.2.m2.1a"><mo fence="false" id="S3.T1.2.2.2.2.m2.1.1" stretchy="false" xref="S3.T1.2.2.2.2.m2.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.2.m2.1b"><ci id="S3.T1.2.2.2.2.m2.1.1.cmml" xref="S3.T1.2.2.2.2.m2.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.2.m2.1c">|</annotation><annotation encoding="application/x-llamapun" id="S3.T1.2.2.2.2.m2.1d">|</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.2.2.6">Task</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.2.2.7">Metric</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.6.7.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.6.6.7.1.1">LogiQA</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.7.1.2">651</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.7.1.3" rowspan="2"><span class="ltx_text" id="S3.T1.6.6.7.1.3.1">MRC</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.7.1.4">77</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.7.1.5">Logical Reasoning</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.7.1.6">Acc</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.6.8.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.6.6.8.2.1">DROP</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.6.8.2.2">500</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.6.8.2.3">196</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.6.8.2.4">Arithmetic Reasoning</td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.6.8.2.5">F1</td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.4.4">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.4.4.4.3">HotpotQA</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.4.4.4.2">7.41K<math alttext="{}^{CG}" class="ltx_Math" display="inline" id="S3.T1.3.3.3.1.m1.1"><semantics id="S3.T1.3.3.3.1.m1.1a"><msup id="S3.T1.3.3.3.1.m1.1.1" xref="S3.T1.3.3.3.1.m1.1.1.cmml"><mi id="S3.T1.3.3.3.1.m1.1.1a" xref="S3.T1.3.3.3.1.m1.1.1.cmml"></mi><mrow id="S3.T1.3.3.3.1.m1.1.1.1" xref="S3.T1.3.3.3.1.m1.1.1.1.cmml"><mi id="S3.T1.3.3.3.1.m1.1.1.1.2" xref="S3.T1.3.3.3.1.m1.1.1.1.2.cmml">C</mi><mo id="S3.T1.3.3.3.1.m1.1.1.1.1" xref="S3.T1.3.3.3.1.m1.1.1.1.1.cmml">⁢</mo><mi id="S3.T1.3.3.3.1.m1.1.1.1.3" xref="S3.T1.3.3.3.1.m1.1.1.1.3.cmml">G</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.1.m1.1b"><apply id="S3.T1.3.3.3.1.m1.1.1.cmml" xref="S3.T1.3.3.3.1.m1.1.1"><apply id="S3.T1.3.3.3.1.m1.1.1.1.cmml" xref="S3.T1.3.3.3.1.m1.1.1.1"><times id="S3.T1.3.3.3.1.m1.1.1.1.1.cmml" xref="S3.T1.3.3.3.1.m1.1.1.1.1"></times><ci id="S3.T1.3.3.3.1.m1.1.1.1.2.cmml" xref="S3.T1.3.3.3.1.m1.1.1.1.2">𝐶</ci><ci id="S3.T1.3.3.3.1.m1.1.1.1.3.cmml" xref="S3.T1.3.3.3.1.m1.1.1.1.3">𝐺</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.1.m1.1c">{}^{CG}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.3.3.3.1.m1.1d">start_FLOATSUPERSCRIPT italic_C italic_G end_FLOATSUPERSCRIPT</annotation></semantics></math>/1.5K<math alttext="{}^{P}" class="ltx_Math" display="inline" id="S3.T1.4.4.4.2.m2.1"><semantics id="S3.T1.4.4.4.2.m2.1a"><msup id="S3.T1.4.4.4.2.m2.1.1" xref="S3.T1.4.4.4.2.m2.1.1.cmml"><mi id="S3.T1.4.4.4.2.m2.1.1a" xref="S3.T1.4.4.4.2.m2.1.1.cmml"></mi><mi id="S3.T1.4.4.4.2.m2.1.1.1" xref="S3.T1.4.4.4.2.m2.1.1.1.cmml">P</mi></msup><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.4.2.m2.1b"><apply id="S3.T1.4.4.4.2.m2.1.1.cmml" xref="S3.T1.4.4.4.2.m2.1.1"><ci id="S3.T1.4.4.4.2.m2.1.1.1.cmml" xref="S3.T1.4.4.4.2.m2.1.1.1">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.4.2.m2.1c">{}^{P}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.4.4.4.2.m2.1d">start_FLOATSUPERSCRIPT italic_P end_FLOATSUPERSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.4.4.4.4">Distarctor</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.4.4.4.5">1106</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.4.4.4.6">Multi-hop QA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.4.4.4.7" rowspan="3"><span class="ltx_text" id="S3.T1.4.4.4.7.1">EM, F1</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.6.9.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.6.6.9.3.1">NQ</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.9.3.2">500</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.6.6.9.3.3" rowspan="5"><span class="ltx_text" id="S3.T1.6.6.9.3.3.1">RAG</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.6.6.9.3.4" rowspan="5"><span class="ltx_text" id="S3.T1.6.6.9.3.4.1">650-675</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.6.9.3.5" rowspan="2"><span class="ltx_text" id="S3.T1.6.6.9.3.5.1">Open-domain QA</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.6.10.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.6.6.10.4.1">TQA</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.6.10.4.2">1.5K</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.6.11.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.6.6.11.5.1">WOW</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.6.11.5.2">500</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.11.5.3">Know. Grounded Dialouge Gen.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.11.5.4" rowspan="2"><span class="ltx_text" id="S3.T1.6.6.11.5.4.1">F1</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.6.12.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.6.6.12.6.1">ELI5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.6.12.6.2">300</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.6.12.6.3">Long Form QA</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.6.6">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S3.T1.6.6.6.3">FEVER</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S3.T1.6.6.6.2">10.1K<math alttext="{}^{CG}" class="ltx_Math" display="inline" id="S3.T1.5.5.5.1.m1.1"><semantics id="S3.T1.5.5.5.1.m1.1a"><msup id="S3.T1.5.5.5.1.m1.1.1" xref="S3.T1.5.5.5.1.m1.1.1.cmml"><mi id="S3.T1.5.5.5.1.m1.1.1a" xref="S3.T1.5.5.5.1.m1.1.1.cmml"></mi><mrow id="S3.T1.5.5.5.1.m1.1.1.1" xref="S3.T1.5.5.5.1.m1.1.1.1.cmml"><mi id="S3.T1.5.5.5.1.m1.1.1.1.2" xref="S3.T1.5.5.5.1.m1.1.1.1.2.cmml">C</mi><mo id="S3.T1.5.5.5.1.m1.1.1.1.1" xref="S3.T1.5.5.5.1.m1.1.1.1.1.cmml">⁢</mo><mi id="S3.T1.5.5.5.1.m1.1.1.1.3" xref="S3.T1.5.5.5.1.m1.1.1.1.3.cmml">G</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.5.1.m1.1b"><apply id="S3.T1.5.5.5.1.m1.1.1.cmml" xref="S3.T1.5.5.5.1.m1.1.1"><apply id="S3.T1.5.5.5.1.m1.1.1.1.cmml" xref="S3.T1.5.5.5.1.m1.1.1.1"><times id="S3.T1.5.5.5.1.m1.1.1.1.1.cmml" xref="S3.T1.5.5.5.1.m1.1.1.1.1"></times><ci id="S3.T1.5.5.5.1.m1.1.1.1.2.cmml" xref="S3.T1.5.5.5.1.m1.1.1.1.2">𝐶</ci><ci id="S3.T1.5.5.5.1.m1.1.1.1.3.cmml" xref="S3.T1.5.5.5.1.m1.1.1.1.3">𝐺</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.5.1.m1.1c">{}^{CG}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.5.5.5.1.m1.1d">start_FLOATSUPERSCRIPT italic_C italic_G end_FLOATSUPERSCRIPT</annotation></semantics></math>/1.5K<math alttext="{}^{P}" class="ltx_Math" display="inline" id="S3.T1.6.6.6.2.m2.1"><semantics id="S3.T1.6.6.6.2.m2.1a"><msup id="S3.T1.6.6.6.2.m2.1.1" xref="S3.T1.6.6.6.2.m2.1.1.cmml"><mi id="S3.T1.6.6.6.2.m2.1.1a" xref="S3.T1.6.6.6.2.m2.1.1.cmml"></mi><mi id="S3.T1.6.6.6.2.m2.1.1.1" xref="S3.T1.6.6.6.2.m2.1.1.1.cmml">P</mi></msup><annotation-xml encoding="MathML-Content" id="S3.T1.6.6.6.2.m2.1b"><apply id="S3.T1.6.6.6.2.m2.1.1.cmml" xref="S3.T1.6.6.6.2.m2.1.1"><ci id="S3.T1.6.6.6.2.m2.1.1.1.cmml" xref="S3.T1.6.6.6.2.m2.1.1.1">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.6.6.2.m2.1c">{}^{P}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.6.6.6.2.m2.1d">start_FLOATSUPERSCRIPT italic_P end_FLOATSUPERSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.6.6.6.4">Fact Verification</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.T1.6.6.6.5">Acc</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Evaluation Datasets. MRC, and distractor denote machine reading comprehension, and context with distracting documents. |Context| denotes avg token length. <math alttext="{}^{CG/P}" class="ltx_Math" display="inline" id="S3.T1.8.m1.1"><semantics id="S3.T1.8.m1.1b"><msup id="S3.T1.8.m1.1.1" xref="S3.T1.8.m1.1.1.cmml"><mi id="S3.T1.8.m1.1.1b" xref="S3.T1.8.m1.1.1.cmml"></mi><mrow id="S3.T1.8.m1.1.1.1" xref="S3.T1.8.m1.1.1.1.cmml"><mrow id="S3.T1.8.m1.1.1.1.2" xref="S3.T1.8.m1.1.1.1.2.cmml"><mi id="S3.T1.8.m1.1.1.1.2.2" xref="S3.T1.8.m1.1.1.1.2.2.cmml">C</mi><mo id="S3.T1.8.m1.1.1.1.2.1" xref="S3.T1.8.m1.1.1.1.2.1.cmml">⁢</mo><mi id="S3.T1.8.m1.1.1.1.2.3" xref="S3.T1.8.m1.1.1.1.2.3.cmml">G</mi></mrow><mo id="S3.T1.8.m1.1.1.1.1" xref="S3.T1.8.m1.1.1.1.1.cmml">/</mo><mi id="S3.T1.8.m1.1.1.1.3" xref="S3.T1.8.m1.1.1.1.3.cmml">P</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.T1.8.m1.1c"><apply id="S3.T1.8.m1.1.1.cmml" xref="S3.T1.8.m1.1.1"><apply id="S3.T1.8.m1.1.1.1.cmml" xref="S3.T1.8.m1.1.1.1"><divide id="S3.T1.8.m1.1.1.1.1.cmml" xref="S3.T1.8.m1.1.1.1.1"></divide><apply id="S3.T1.8.m1.1.1.1.2.cmml" xref="S3.T1.8.m1.1.1.1.2"><times id="S3.T1.8.m1.1.1.1.2.1.cmml" xref="S3.T1.8.m1.1.1.1.2.1"></times><ci id="S3.T1.8.m1.1.1.1.2.2.cmml" xref="S3.T1.8.m1.1.1.1.2.2">𝐶</ci><ci id="S3.T1.8.m1.1.1.1.2.3.cmml" xref="S3.T1.8.m1.1.1.1.2.3">𝐺</ci></apply><ci id="S3.T1.8.m1.1.1.1.3.cmml" xref="S3.T1.8.m1.1.1.1.3">𝑃</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.8.m1.1d">{}^{CG/P}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.8.m1.1e">start_FLOATSUPERSCRIPT italic_C italic_G / italic_P end_FLOATSUPERSCRIPT</annotation></semantics></math> denotes w/ ChatGPT and PALM-2 respectively. </figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Features, Adaptation and Best Practices</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.7">Generating grounded rationales is important: To achieve this, we explicitly include an instruction in the prompt. Therefore, our approach is distinctive from a 2 round of CoT, which slightly improves the performance due to post-processing, however as it does not address the grounding reasoning problem, and the hallucination effect propagates. Our prompting instruction exhibits robustness in general w/ different LLMs, tasks and benchmarks. As for the limitation (in Section <a class="ltx_ref" href="#S3.SS2" title="3.2 E-step ‣ 3 Evidence to Generate (E2G) Prompting ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_tag">3.2</span></a>) that <span class="ltx_text ltx_font_smallcaps" id="S3.SS4.p1.7.4">E2G</span> overemphasizing on grounding even for the context-less reasoning (arithmetic, commonsense), we combine the benefits of both CoT and <span class="ltx_text ltx_font_typewriter ltx_font_smallcaps" id="S3.SS4.p1.7.5">E2G<span class="ltx_text ltx_font_upright" id="S3.SS4.p1.7.5.1">-base</span></span>, and design a new prompt, <span class="ltx_text ltx_font_typewriter ltx_font_smallcaps" id="S3.SS4.p1.7.6">E2G<span class="ltx_text ltx_font_upright" id="S3.SS4.p1.7.6.1">-Pro</span></span>, <span class="ltx_text" id="S3.SS4.p1.7.7" style="background-color:#CCFFCC;">Think step by step with evidence and explanation.</span> <span class="ltx_text" id="S3.SS4.p1.7.8" style="background-color:#CCFFCC;">Generate both answer and step-by-step-reasoning</span> <span class="ltx_text" id="S3.SS4.p1.7.9" style="background-color:#CCFFCC;">-with-evidence-and-explanation</span>.
Our recommended best practices for context-aware prompting include (i) <span class="ltx_text ltx_font_typewriter ltx_font_smallcaps" id="S3.SS4.p1.7.10">E2G<span class="ltx_text ltx_font_upright" id="S3.SS4.p1.7.10.1">-Pro</span></span> for heavy arithmetic, commonsense, symbolic/logical etc., reasoning tasks and <span class="ltx_text ltx_font_typewriter ltx_font_smallcaps" id="S3.SS4.p1.7.11">E2G<span class="ltx_text ltx_font_upright" id="S3.SS4.p1.7.11.1">-base</span></span> for others (iii) Both (E &amp; G) steps if context is large (&gt;200 words or so). For smaller context, only E-step prompting may be sufficient. (iii) as for the "Context" in G-step: (a) <span class="ltx_text" id="S3.SS4.p1.1.1"><math alttext="Evidence" class="ltx_Math" display="inline" id="S3.SS4.p1.1.1.m1.1" style="background-color:#CCFFCC;"><semantics id="S3.SS4.p1.1.1.m1.1a"><mrow id="S3.SS4.p1.1.1.m1.1.1" xref="S3.SS4.p1.1.1.m1.1.1.cmml"><mi id="S3.SS4.p1.1.1.m1.1.1.2" mathbackground="#CCFFCC" xref="S3.SS4.p1.1.1.m1.1.1.2.cmml">E</mi><mo id="S3.SS4.p1.1.1.m1.1.1.1" mathbackground="#CCFFCC" xref="S3.SS4.p1.1.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS4.p1.1.1.m1.1.1.3" mathbackground="#CCFFCC" xref="S3.SS4.p1.1.1.m1.1.1.3.cmml">v</mi><mo id="S3.SS4.p1.1.1.m1.1.1.1a" mathbackground="#CCFFCC" xref="S3.SS4.p1.1.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS4.p1.1.1.m1.1.1.4" mathbackground="#CCFFCC" xref="S3.SS4.p1.1.1.m1.1.1.4.cmml">i</mi><mo id="S3.SS4.p1.1.1.m1.1.1.1b" mathbackground="#CCFFCC" xref="S3.SS4.p1.1.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS4.p1.1.1.m1.1.1.5" mathbackground="#CCFFCC" xref="S3.SS4.p1.1.1.m1.1.1.5.cmml">d</mi><mo id="S3.SS4.p1.1.1.m1.1.1.1c" mathbackground="#CCFFCC" xref="S3.SS4.p1.1.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS4.p1.1.1.m1.1.1.6" mathbackground="#CCFFCC" xref="S3.SS4.p1.1.1.m1.1.1.6.cmml">e</mi><mo id="S3.SS4.p1.1.1.m1.1.1.1d" mathbackground="#CCFFCC" xref="S3.SS4.p1.1.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS4.p1.1.1.m1.1.1.7" mathbackground="#CCFFCC" xref="S3.SS4.p1.1.1.m1.1.1.7.cmml">n</mi><mo id="S3.SS4.p1.1.1.m1.1.1.1e" mathbackground="#CCFFCC" xref="S3.SS4.p1.1.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS4.p1.1.1.m1.1.1.8" mathbackground="#CCFFCC" xref="S3.SS4.p1.1.1.m1.1.1.8.cmml">c</mi><mo id="S3.SS4.p1.1.1.m1.1.1.1f" mathbackground="#CCFFCC" xref="S3.SS4.p1.1.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS4.p1.1.1.m1.1.1.9" mathbackground="#CCFFCC" xref="S3.SS4.p1.1.1.m1.1.1.9.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.1.m1.1b"><apply id="S3.SS4.p1.1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.1.m1.1.1"><times id="S3.SS4.p1.1.1.m1.1.1.1.cmml" xref="S3.SS4.p1.1.1.m1.1.1.1"></times><ci id="S3.SS4.p1.1.1.m1.1.1.2.cmml" xref="S3.SS4.p1.1.1.m1.1.1.2">𝐸</ci><ci id="S3.SS4.p1.1.1.m1.1.1.3.cmml" xref="S3.SS4.p1.1.1.m1.1.1.3">𝑣</ci><ci id="S3.SS4.p1.1.1.m1.1.1.4.cmml" xref="S3.SS4.p1.1.1.m1.1.1.4">𝑖</ci><ci id="S3.SS4.p1.1.1.m1.1.1.5.cmml" xref="S3.SS4.p1.1.1.m1.1.1.5">𝑑</ci><ci id="S3.SS4.p1.1.1.m1.1.1.6.cmml" xref="S3.SS4.p1.1.1.m1.1.1.6">𝑒</ci><ci id="S3.SS4.p1.1.1.m1.1.1.7.cmml" xref="S3.SS4.p1.1.1.m1.1.1.7">𝑛</ci><ci id="S3.SS4.p1.1.1.m1.1.1.8.cmml" xref="S3.SS4.p1.1.1.m1.1.1.8">𝑐</ci><ci id="S3.SS4.p1.1.1.m1.1.1.9.cmml" xref="S3.SS4.p1.1.1.m1.1.1.9">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.1.m1.1c">Evidence</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.1.1.m1.1d">italic_E italic_v italic_i italic_d italic_e italic_n italic_c italic_e</annotation></semantics></math></span> in general (b) if the query has multiple joint questions (e.g., where are the winter olympics and when do they start?), the <math alttext="Evidence" class="ltx_Math" display="inline" id="S3.SS4.p1.2.m1.1"><semantics id="S3.SS4.p1.2.m1.1a"><mrow id="S3.SS4.p1.2.m1.1.1" xref="S3.SS4.p1.2.m1.1.1.cmml"><mi id="S3.SS4.p1.2.m1.1.1.2" xref="S3.SS4.p1.2.m1.1.1.2.cmml">E</mi><mo id="S3.SS4.p1.2.m1.1.1.1" xref="S3.SS4.p1.2.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS4.p1.2.m1.1.1.3" xref="S3.SS4.p1.2.m1.1.1.3.cmml">v</mi><mo id="S3.SS4.p1.2.m1.1.1.1a" xref="S3.SS4.p1.2.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS4.p1.2.m1.1.1.4" xref="S3.SS4.p1.2.m1.1.1.4.cmml">i</mi><mo id="S3.SS4.p1.2.m1.1.1.1b" xref="S3.SS4.p1.2.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS4.p1.2.m1.1.1.5" xref="S3.SS4.p1.2.m1.1.1.5.cmml">d</mi><mo id="S3.SS4.p1.2.m1.1.1.1c" xref="S3.SS4.p1.2.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS4.p1.2.m1.1.1.6" xref="S3.SS4.p1.2.m1.1.1.6.cmml">e</mi><mo id="S3.SS4.p1.2.m1.1.1.1d" xref="S3.SS4.p1.2.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS4.p1.2.m1.1.1.7" xref="S3.SS4.p1.2.m1.1.1.7.cmml">n</mi><mo id="S3.SS4.p1.2.m1.1.1.1e" xref="S3.SS4.p1.2.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS4.p1.2.m1.1.1.8" xref="S3.SS4.p1.2.m1.1.1.8.cmml">c</mi><mo id="S3.SS4.p1.2.m1.1.1.1f" xref="S3.SS4.p1.2.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS4.p1.2.m1.1.1.9" xref="S3.SS4.p1.2.m1.1.1.9.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m1.1b"><apply id="S3.SS4.p1.2.m1.1.1.cmml" xref="S3.SS4.p1.2.m1.1.1"><times id="S3.SS4.p1.2.m1.1.1.1.cmml" xref="S3.SS4.p1.2.m1.1.1.1"></times><ci id="S3.SS4.p1.2.m1.1.1.2.cmml" xref="S3.SS4.p1.2.m1.1.1.2">𝐸</ci><ci id="S3.SS4.p1.2.m1.1.1.3.cmml" xref="S3.SS4.p1.2.m1.1.1.3">𝑣</ci><ci id="S3.SS4.p1.2.m1.1.1.4.cmml" xref="S3.SS4.p1.2.m1.1.1.4">𝑖</ci><ci id="S3.SS4.p1.2.m1.1.1.5.cmml" xref="S3.SS4.p1.2.m1.1.1.5">𝑑</ci><ci id="S3.SS4.p1.2.m1.1.1.6.cmml" xref="S3.SS4.p1.2.m1.1.1.6">𝑒</ci><ci id="S3.SS4.p1.2.m1.1.1.7.cmml" xref="S3.SS4.p1.2.m1.1.1.7">𝑛</ci><ci id="S3.SS4.p1.2.m1.1.1.8.cmml" xref="S3.SS4.p1.2.m1.1.1.8">𝑐</ci><ci id="S3.SS4.p1.2.m1.1.1.9.cmml" xref="S3.SS4.p1.2.m1.1.1.9">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m1.1c">Evidence</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.2.m1.1d">italic_E italic_v italic_i italic_d italic_e italic_n italic_c italic_e</annotation></semantics></math> can focus on partial answer and hence to answer the full question finally, the preferred G-step "Context" is <span class="ltx_text" id="S3.SS4.p1.3.2" style="background-color:#CCCCFF;"> <math alttext="Evidence" class="ltx_Math" display="inline" id="S3.SS4.p1.3.2.1.m1.1" style="background-color:#CCCCFF;"><semantics id="S3.SS4.p1.3.2.1.m1.1a"><mrow id="S3.SS4.p1.3.2.1.m1.1.1" xref="S3.SS4.p1.3.2.1.m1.1.1.cmml"><mi id="S3.SS4.p1.3.2.1.m1.1.1.2" mathbackground="#CCCCFF" xref="S3.SS4.p1.3.2.1.m1.1.1.2.cmml">E</mi><mo id="S3.SS4.p1.3.2.1.m1.1.1.1" mathbackground="#CCCCFF" xref="S3.SS4.p1.3.2.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS4.p1.3.2.1.m1.1.1.3" mathbackground="#CCCCFF" xref="S3.SS4.p1.3.2.1.m1.1.1.3.cmml">v</mi><mo id="S3.SS4.p1.3.2.1.m1.1.1.1a" mathbackground="#CCCCFF" xref="S3.SS4.p1.3.2.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS4.p1.3.2.1.m1.1.1.4" mathbackground="#CCCCFF" xref="S3.SS4.p1.3.2.1.m1.1.1.4.cmml">i</mi><mo id="S3.SS4.p1.3.2.1.m1.1.1.1b" mathbackground="#CCCCFF" xref="S3.SS4.p1.3.2.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS4.p1.3.2.1.m1.1.1.5" mathbackground="#CCCCFF" xref="S3.SS4.p1.3.2.1.m1.1.1.5.cmml">d</mi><mo id="S3.SS4.p1.3.2.1.m1.1.1.1c" mathbackground="#CCCCFF" xref="S3.SS4.p1.3.2.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS4.p1.3.2.1.m1.1.1.6" mathbackground="#CCCCFF" xref="S3.SS4.p1.3.2.1.m1.1.1.6.cmml">e</mi><mo id="S3.SS4.p1.3.2.1.m1.1.1.1d" mathbackground="#CCCCFF" xref="S3.SS4.p1.3.2.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS4.p1.3.2.1.m1.1.1.7" mathbackground="#CCCCFF" xref="S3.SS4.p1.3.2.1.m1.1.1.7.cmml">n</mi><mo id="S3.SS4.p1.3.2.1.m1.1.1.1e" mathbackground="#CCCCFF" xref="S3.SS4.p1.3.2.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS4.p1.3.2.1.m1.1.1.8" mathbackground="#CCCCFF" xref="S3.SS4.p1.3.2.1.m1.1.1.8.cmml">c</mi><mo id="S3.SS4.p1.3.2.1.m1.1.1.1f" mathbackground="#CCCCFF" xref="S3.SS4.p1.3.2.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS4.p1.3.2.1.m1.1.1.9" mathbackground="#CCCCFF" xref="S3.SS4.p1.3.2.1.m1.1.1.9.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.3.2.1.m1.1b"><apply id="S3.SS4.p1.3.2.1.m1.1.1.cmml" xref="S3.SS4.p1.3.2.1.m1.1.1"><times id="S3.SS4.p1.3.2.1.m1.1.1.1.cmml" xref="S3.SS4.p1.3.2.1.m1.1.1.1"></times><ci id="S3.SS4.p1.3.2.1.m1.1.1.2.cmml" xref="S3.SS4.p1.3.2.1.m1.1.1.2">𝐸</ci><ci id="S3.SS4.p1.3.2.1.m1.1.1.3.cmml" xref="S3.SS4.p1.3.2.1.m1.1.1.3">𝑣</ci><ci id="S3.SS4.p1.3.2.1.m1.1.1.4.cmml" xref="S3.SS4.p1.3.2.1.m1.1.1.4">𝑖</ci><ci id="S3.SS4.p1.3.2.1.m1.1.1.5.cmml" xref="S3.SS4.p1.3.2.1.m1.1.1.5">𝑑</ci><ci id="S3.SS4.p1.3.2.1.m1.1.1.6.cmml" xref="S3.SS4.p1.3.2.1.m1.1.1.6">𝑒</ci><ci id="S3.SS4.p1.3.2.1.m1.1.1.7.cmml" xref="S3.SS4.p1.3.2.1.m1.1.1.7">𝑛</ci><ci id="S3.SS4.p1.3.2.1.m1.1.1.8.cmml" xref="S3.SS4.p1.3.2.1.m1.1.1.8">𝑐</ci><ci id="S3.SS4.p1.3.2.1.m1.1.1.9.cmml" xref="S3.SS4.p1.3.2.1.m1.1.1.9">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.3.2.1.m1.1c">Evidence</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.3.2.1.m1.1d">italic_E italic_v italic_i italic_d italic_e italic_n italic_c italic_e</annotation></semantics></math><span class="ltx_text ltx_font_typewriter" id="S3.SS4.p1.3.2.2"> + Original Context</span></span> (c) if the target task is to generate long answer, then <math alttext="Evidence" class="ltx_Math" display="inline" id="S3.SS4.p1.4.m2.1"><semantics id="S3.SS4.p1.4.m2.1a"><mrow id="S3.SS4.p1.4.m2.1.1" xref="S3.SS4.p1.4.m2.1.1.cmml"><mi id="S3.SS4.p1.4.m2.1.1.2" xref="S3.SS4.p1.4.m2.1.1.2.cmml">E</mi><mo id="S3.SS4.p1.4.m2.1.1.1" xref="S3.SS4.p1.4.m2.1.1.1.cmml">⁢</mo><mi id="S3.SS4.p1.4.m2.1.1.3" xref="S3.SS4.p1.4.m2.1.1.3.cmml">v</mi><mo id="S3.SS4.p1.4.m2.1.1.1a" xref="S3.SS4.p1.4.m2.1.1.1.cmml">⁢</mo><mi id="S3.SS4.p1.4.m2.1.1.4" xref="S3.SS4.p1.4.m2.1.1.4.cmml">i</mi><mo id="S3.SS4.p1.4.m2.1.1.1b" xref="S3.SS4.p1.4.m2.1.1.1.cmml">⁢</mo><mi id="S3.SS4.p1.4.m2.1.1.5" xref="S3.SS4.p1.4.m2.1.1.5.cmml">d</mi><mo id="S3.SS4.p1.4.m2.1.1.1c" xref="S3.SS4.p1.4.m2.1.1.1.cmml">⁢</mo><mi id="S3.SS4.p1.4.m2.1.1.6" xref="S3.SS4.p1.4.m2.1.1.6.cmml">e</mi><mo id="S3.SS4.p1.4.m2.1.1.1d" xref="S3.SS4.p1.4.m2.1.1.1.cmml">⁢</mo><mi id="S3.SS4.p1.4.m2.1.1.7" xref="S3.SS4.p1.4.m2.1.1.7.cmml">n</mi><mo id="S3.SS4.p1.4.m2.1.1.1e" xref="S3.SS4.p1.4.m2.1.1.1.cmml">⁢</mo><mi id="S3.SS4.p1.4.m2.1.1.8" xref="S3.SS4.p1.4.m2.1.1.8.cmml">c</mi><mo id="S3.SS4.p1.4.m2.1.1.1f" xref="S3.SS4.p1.4.m2.1.1.1.cmml">⁢</mo><mi id="S3.SS4.p1.4.m2.1.1.9" xref="S3.SS4.p1.4.m2.1.1.9.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.4.m2.1b"><apply id="S3.SS4.p1.4.m2.1.1.cmml" xref="S3.SS4.p1.4.m2.1.1"><times id="S3.SS4.p1.4.m2.1.1.1.cmml" xref="S3.SS4.p1.4.m2.1.1.1"></times><ci id="S3.SS4.p1.4.m2.1.1.2.cmml" xref="S3.SS4.p1.4.m2.1.1.2">𝐸</ci><ci id="S3.SS4.p1.4.m2.1.1.3.cmml" xref="S3.SS4.p1.4.m2.1.1.3">𝑣</ci><ci id="S3.SS4.p1.4.m2.1.1.4.cmml" xref="S3.SS4.p1.4.m2.1.1.4">𝑖</ci><ci id="S3.SS4.p1.4.m2.1.1.5.cmml" xref="S3.SS4.p1.4.m2.1.1.5">𝑑</ci><ci id="S3.SS4.p1.4.m2.1.1.6.cmml" xref="S3.SS4.p1.4.m2.1.1.6">𝑒</ci><ci id="S3.SS4.p1.4.m2.1.1.7.cmml" xref="S3.SS4.p1.4.m2.1.1.7">𝑛</ci><ci id="S3.SS4.p1.4.m2.1.1.8.cmml" xref="S3.SS4.p1.4.m2.1.1.8">𝑐</ci><ci id="S3.SS4.p1.4.m2.1.1.9.cmml" xref="S3.SS4.p1.4.m2.1.1.9">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.4.m2.1c">Evidence</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.4.m2.1d">italic_E italic_v italic_i italic_d italic_e italic_n italic_c italic_e</annotation></semantics></math>, which is typically a short precious rationale lacks verbose, so the preferred G-step "Context" is <span class="ltx_text" id="S3.SS4.p1.5.3"><math alttext="A_{temp}" class="ltx_Math" display="inline" id="S3.SS4.p1.5.3.m1.1" style="background-color:#CCCCFF;"><semantics id="S3.SS4.p1.5.3.m1.1a"><msub id="S3.SS4.p1.5.3.m1.1.1" xref="S3.SS4.p1.5.3.m1.1.1.cmml"><mi id="S3.SS4.p1.5.3.m1.1.1.2" mathbackground="#CCCCFF" xref="S3.SS4.p1.5.3.m1.1.1.2.cmml">A</mi><mrow id="S3.SS4.p1.5.3.m1.1.1.3" xref="S3.SS4.p1.5.3.m1.1.1.3.cmml"><mi id="S3.SS4.p1.5.3.m1.1.1.3.2" mathbackground="#CCCCFF" xref="S3.SS4.p1.5.3.m1.1.1.3.2.cmml">t</mi><mo id="S3.SS4.p1.5.3.m1.1.1.3.1" mathbackground="#CCCCFF" xref="S3.SS4.p1.5.3.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS4.p1.5.3.m1.1.1.3.3" mathbackground="#CCCCFF" xref="S3.SS4.p1.5.3.m1.1.1.3.3.cmml">e</mi><mo id="S3.SS4.p1.5.3.m1.1.1.3.1a" mathbackground="#CCCCFF" xref="S3.SS4.p1.5.3.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS4.p1.5.3.m1.1.1.3.4" mathbackground="#CCCCFF" xref="S3.SS4.p1.5.3.m1.1.1.3.4.cmml">m</mi><mo id="S3.SS4.p1.5.3.m1.1.1.3.1b" mathbackground="#CCCCFF" xref="S3.SS4.p1.5.3.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS4.p1.5.3.m1.1.1.3.5" mathbackground="#CCCCFF" xref="S3.SS4.p1.5.3.m1.1.1.3.5.cmml">p</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.5.3.m1.1b"><apply id="S3.SS4.p1.5.3.m1.1.1.cmml" xref="S3.SS4.p1.5.3.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.5.3.m1.1.1.1.cmml" xref="S3.SS4.p1.5.3.m1.1.1">subscript</csymbol><ci id="S3.SS4.p1.5.3.m1.1.1.2.cmml" xref="S3.SS4.p1.5.3.m1.1.1.2">𝐴</ci><apply id="S3.SS4.p1.5.3.m1.1.1.3.cmml" xref="S3.SS4.p1.5.3.m1.1.1.3"><times id="S3.SS4.p1.5.3.m1.1.1.3.1.cmml" xref="S3.SS4.p1.5.3.m1.1.1.3.1"></times><ci id="S3.SS4.p1.5.3.m1.1.1.3.2.cmml" xref="S3.SS4.p1.5.3.m1.1.1.3.2">𝑡</ci><ci id="S3.SS4.p1.5.3.m1.1.1.3.3.cmml" xref="S3.SS4.p1.5.3.m1.1.1.3.3">𝑒</ci><ci id="S3.SS4.p1.5.3.m1.1.1.3.4.cmml" xref="S3.SS4.p1.5.3.m1.1.1.3.4">𝑚</ci><ci id="S3.SS4.p1.5.3.m1.1.1.3.5.cmml" xref="S3.SS4.p1.5.3.m1.1.1.3.5">𝑝</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.5.3.m1.1c">A_{temp}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.5.3.m1.1d">italic_A start_POSTSUBSCRIPT italic_t italic_e italic_m italic_p end_POSTSUBSCRIPT</annotation></semantics></math></span>. Shorter <math alttext="Evidence" class="ltx_Math" display="inline" id="S3.SS4.p1.6.m3.1"><semantics id="S3.SS4.p1.6.m3.1a"><mrow id="S3.SS4.p1.6.m3.1.1" xref="S3.SS4.p1.6.m3.1.1.cmml"><mi id="S3.SS4.p1.6.m3.1.1.2" xref="S3.SS4.p1.6.m3.1.1.2.cmml">E</mi><mo id="S3.SS4.p1.6.m3.1.1.1" xref="S3.SS4.p1.6.m3.1.1.1.cmml">⁢</mo><mi id="S3.SS4.p1.6.m3.1.1.3" xref="S3.SS4.p1.6.m3.1.1.3.cmml">v</mi><mo id="S3.SS4.p1.6.m3.1.1.1a" xref="S3.SS4.p1.6.m3.1.1.1.cmml">⁢</mo><mi id="S3.SS4.p1.6.m3.1.1.4" xref="S3.SS4.p1.6.m3.1.1.4.cmml">i</mi><mo id="S3.SS4.p1.6.m3.1.1.1b" xref="S3.SS4.p1.6.m3.1.1.1.cmml">⁢</mo><mi id="S3.SS4.p1.6.m3.1.1.5" xref="S3.SS4.p1.6.m3.1.1.5.cmml">d</mi><mo id="S3.SS4.p1.6.m3.1.1.1c" xref="S3.SS4.p1.6.m3.1.1.1.cmml">⁢</mo><mi id="S3.SS4.p1.6.m3.1.1.6" xref="S3.SS4.p1.6.m3.1.1.6.cmml">e</mi><mo id="S3.SS4.p1.6.m3.1.1.1d" xref="S3.SS4.p1.6.m3.1.1.1.cmml">⁢</mo><mi id="S3.SS4.p1.6.m3.1.1.7" xref="S3.SS4.p1.6.m3.1.1.7.cmml">n</mi><mo id="S3.SS4.p1.6.m3.1.1.1e" xref="S3.SS4.p1.6.m3.1.1.1.cmml">⁢</mo><mi id="S3.SS4.p1.6.m3.1.1.8" xref="S3.SS4.p1.6.m3.1.1.8.cmml">c</mi><mo id="S3.SS4.p1.6.m3.1.1.1f" xref="S3.SS4.p1.6.m3.1.1.1.cmml">⁢</mo><mi id="S3.SS4.p1.6.m3.1.1.9" xref="S3.SS4.p1.6.m3.1.1.9.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.6.m3.1b"><apply id="S3.SS4.p1.6.m3.1.1.cmml" xref="S3.SS4.p1.6.m3.1.1"><times id="S3.SS4.p1.6.m3.1.1.1.cmml" xref="S3.SS4.p1.6.m3.1.1.1"></times><ci id="S3.SS4.p1.6.m3.1.1.2.cmml" xref="S3.SS4.p1.6.m3.1.1.2">𝐸</ci><ci id="S3.SS4.p1.6.m3.1.1.3.cmml" xref="S3.SS4.p1.6.m3.1.1.3">𝑣</ci><ci id="S3.SS4.p1.6.m3.1.1.4.cmml" xref="S3.SS4.p1.6.m3.1.1.4">𝑖</ci><ci id="S3.SS4.p1.6.m3.1.1.5.cmml" xref="S3.SS4.p1.6.m3.1.1.5">𝑑</ci><ci id="S3.SS4.p1.6.m3.1.1.6.cmml" xref="S3.SS4.p1.6.m3.1.1.6">𝑒</ci><ci id="S3.SS4.p1.6.m3.1.1.7.cmml" xref="S3.SS4.p1.6.m3.1.1.7">𝑛</ci><ci id="S3.SS4.p1.6.m3.1.1.8.cmml" xref="S3.SS4.p1.6.m3.1.1.8">𝑐</ci><ci id="S3.SS4.p1.6.m3.1.1.9.cmml" xref="S3.SS4.p1.6.m3.1.1.9">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.6.m3.1c">Evidence</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.6.m3.1d">italic_E italic_v italic_i italic_d italic_e italic_n italic_c italic_e</annotation></semantics></math> reduces our run-times (e.g., 1.5s vs CoT’s 1.5s; worst case <math alttext="2*CoT" class="ltx_Math" display="inline" id="S3.SS4.p1.7.m4.1"><semantics id="S3.SS4.p1.7.m4.1a"><mrow id="S3.SS4.p1.7.m4.1.1" xref="S3.SS4.p1.7.m4.1.1.cmml"><mrow id="S3.SS4.p1.7.m4.1.1.2" xref="S3.SS4.p1.7.m4.1.1.2.cmml"><mn id="S3.SS4.p1.7.m4.1.1.2.2" xref="S3.SS4.p1.7.m4.1.1.2.2.cmml">2</mn><mo id="S3.SS4.p1.7.m4.1.1.2.1" lspace="0.222em" rspace="0.222em" xref="S3.SS4.p1.7.m4.1.1.2.1.cmml">*</mo><mi id="S3.SS4.p1.7.m4.1.1.2.3" xref="S3.SS4.p1.7.m4.1.1.2.3.cmml">C</mi></mrow><mo id="S3.SS4.p1.7.m4.1.1.1" xref="S3.SS4.p1.7.m4.1.1.1.cmml">⁢</mo><mi id="S3.SS4.p1.7.m4.1.1.3" xref="S3.SS4.p1.7.m4.1.1.3.cmml">o</mi><mo id="S3.SS4.p1.7.m4.1.1.1a" xref="S3.SS4.p1.7.m4.1.1.1.cmml">⁢</mo><mi id="S3.SS4.p1.7.m4.1.1.4" xref="S3.SS4.p1.7.m4.1.1.4.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.7.m4.1b"><apply id="S3.SS4.p1.7.m4.1.1.cmml" xref="S3.SS4.p1.7.m4.1.1"><times id="S3.SS4.p1.7.m4.1.1.1.cmml" xref="S3.SS4.p1.7.m4.1.1.1"></times><apply id="S3.SS4.p1.7.m4.1.1.2.cmml" xref="S3.SS4.p1.7.m4.1.1.2"><times id="S3.SS4.p1.7.m4.1.1.2.1.cmml" xref="S3.SS4.p1.7.m4.1.1.2.1"></times><cn id="S3.SS4.p1.7.m4.1.1.2.2.cmml" type="integer" xref="S3.SS4.p1.7.m4.1.1.2.2">2</cn><ci id="S3.SS4.p1.7.m4.1.1.2.3.cmml" xref="S3.SS4.p1.7.m4.1.1.2.3">𝐶</ci></apply><ci id="S3.SS4.p1.7.m4.1.1.3.cmml" xref="S3.SS4.p1.7.m4.1.1.3">𝑜</ci><ci id="S3.SS4.p1.7.m4.1.1.4.cmml" xref="S3.SS4.p1.7.m4.1.1.4">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.7.m4.1c">2*CoT</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.7.m4.1d">2 * italic_C italic_o italic_T</annotation></semantics></math>)–applicable to practical use cases.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Setup</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We evaluate <span class="ltx_text ltx_font_smallcaps" id="S4.p1.1.1">E2G</span> on eight context-intensive language tasks that necessitate reasoning over given contexts, contexts with distracting documents and retrieval augmentation for generation. We use with three different LLMs (ChatGPT, GPT-4, PALM (540B)) via APIs as backbone model for comprehensive experiments.
As the full eight datasets are too large to run on LLMs, following all previous works <cite class="ltx_cite ltx_citemacro_citep">(Yao et al., <a class="ltx_ref" href="#bib.bib48" title="">2022</a>; Shinn et al., <a class="ltx_ref" href="#bib.bib37" title="">2023</a>)</cite>: (i) for some datasets we use a random sampled subset (ii) we use the dev splits for evaluation when the test set data/answers are not publicly available. (iii) in addition to CoT baseline, we compare ours with other prompting frameworks reported in the literature for the corresponding task. However, if the CoT results are not reported, we reproduce 0-shot CoT. (iv) for the retrieval tasks, we use the DPR <cite class="ltx_cite ltx_citemacro_citep">(Karpukhin et al., <a class="ltx_ref" href="#bib.bib13" title="">2020</a>)</cite> retrieved top-5 context documents from Wikipedia released by <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a class="ltx_ref" href="#bib.bib42" title="">2023</a>)</cite>. We summarize the benchmarks in Table <a class="ltx_ref" href="#S3.T1" title="Table 1 ‣ 3.3 G-Step ‣ 3 Evidence to Generate (E2G) Prompting ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_tag">1</span></a>. By default, we adopt <span class="ltx_text ltx_font_typewriter ltx_font_smallcaps" id="S4.p1.1.2">E2G<span class="ltx_text ltx_font_upright" id="S4.p1.1.2.1">-base</span></span> w/ both (E &amp; G) steps. By large, for DROP and LogiQA benchmarks, we adopt <span class="ltx_text ltx_font_typewriter ltx_font_smallcaps" id="S4.p1.1.3">E2G<span class="ltx_text ltx_font_upright" id="S4.p1.1.3.1">-Pro</span></span> w/ E-step only as the tasks are mainly arithmetic or logical reasoning and the context length is small. G-Step "Context" is <math alttext="Evidence" class="ltx_Math" display="inline" id="S4.p1.1.m1.1"><semantics id="S4.p1.1.m1.1a"><mrow id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml"><mi id="S4.p1.1.m1.1.1.2" xref="S4.p1.1.m1.1.1.2.cmml">E</mi><mo id="S4.p1.1.m1.1.1.1" xref="S4.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.p1.1.m1.1.1.3" xref="S4.p1.1.m1.1.1.3.cmml">v</mi><mo id="S4.p1.1.m1.1.1.1a" xref="S4.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.p1.1.m1.1.1.4" xref="S4.p1.1.m1.1.1.4.cmml">i</mi><mo id="S4.p1.1.m1.1.1.1b" xref="S4.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.p1.1.m1.1.1.5" xref="S4.p1.1.m1.1.1.5.cmml">d</mi><mo id="S4.p1.1.m1.1.1.1c" xref="S4.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.p1.1.m1.1.1.6" xref="S4.p1.1.m1.1.1.6.cmml">e</mi><mo id="S4.p1.1.m1.1.1.1d" xref="S4.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.p1.1.m1.1.1.7" xref="S4.p1.1.m1.1.1.7.cmml">n</mi><mo id="S4.p1.1.m1.1.1.1e" xref="S4.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.p1.1.m1.1.1.8" xref="S4.p1.1.m1.1.1.8.cmml">c</mi><mo id="S4.p1.1.m1.1.1.1f" xref="S4.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.p1.1.m1.1.1.9" xref="S4.p1.1.m1.1.1.9.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><apply id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"><times id="S4.p1.1.m1.1.1.1.cmml" xref="S4.p1.1.m1.1.1.1"></times><ci id="S4.p1.1.m1.1.1.2.cmml" xref="S4.p1.1.m1.1.1.2">𝐸</ci><ci id="S4.p1.1.m1.1.1.3.cmml" xref="S4.p1.1.m1.1.1.3">𝑣</ci><ci id="S4.p1.1.m1.1.1.4.cmml" xref="S4.p1.1.m1.1.1.4">𝑖</ci><ci id="S4.p1.1.m1.1.1.5.cmml" xref="S4.p1.1.m1.1.1.5">𝑑</ci><ci id="S4.p1.1.m1.1.1.6.cmml" xref="S4.p1.1.m1.1.1.6">𝑒</ci><ci id="S4.p1.1.m1.1.1.7.cmml" xref="S4.p1.1.m1.1.1.7">𝑛</ci><ci id="S4.p1.1.m1.1.1.8.cmml" xref="S4.p1.1.m1.1.1.8">𝑐</ci><ci id="S4.p1.1.m1.1.1.9.cmml" xref="S4.p1.1.m1.1.1.9">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">Evidence</annotation><annotation encoding="application/x-llamapun" id="S4.p1.1.m1.1d">italic_E italic_v italic_i italic_d italic_e italic_n italic_c italic_e</annotation></semantics></math> if not stated otherwise. For implementations, we use the <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">dalvi2023llmebench</span></cite> codebase.
</p>
</div>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T2.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.4.5.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T2.4.5.1.1">Backbone</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.4.5.1.2">Method</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.4.5.1.3">Acc</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.4.5.1.4">Steps</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.1.2" rowspan="4"><span class="ltx_text" id="S4.T2.1.1.2.1">GPT-4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.1">CoT<math alttext="{}^{a}" class="ltx_Math" display="inline" id="S4.T2.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.m1.1a"><msup id="S4.T2.1.1.1.m1.1.1" xref="S4.T2.1.1.1.m1.1.1.cmml"><mi id="S4.T2.1.1.1.m1.1.1a" xref="S4.T2.1.1.1.m1.1.1.cmml"></mi><mi id="S4.T2.1.1.1.m1.1.1.1" xref="S4.T2.1.1.1.m1.1.1.1.cmml">a</mi></msup><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><apply id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1"><ci id="S4.T2.1.1.1.m1.1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1.1">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">{}^{a}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.m1.1d">start_FLOATSUPERSCRIPT italic_a end_FLOATSUPERSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.3">38.55%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.4">1</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.2.1">ToT<math alttext="{}^{a}" class="ltx_Math" display="inline" id="S4.T2.2.2.1.m1.1"><semantics id="S4.T2.2.2.1.m1.1a"><msup id="S4.T2.2.2.1.m1.1.1" xref="S4.T2.2.2.1.m1.1.1.cmml"><mi id="S4.T2.2.2.1.m1.1.1a" xref="S4.T2.2.2.1.m1.1.1.cmml"></mi><mi id="S4.T2.2.2.1.m1.1.1.1" xref="S4.T2.2.2.1.m1.1.1.1.cmml">a</mi></msup><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.1.m1.1b"><apply id="S4.T2.2.2.1.m1.1.1.cmml" xref="S4.T2.2.2.1.m1.1.1"><ci id="S4.T2.2.2.1.m1.1.1.1.cmml" xref="S4.T2.2.2.1.m1.1.1.1">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.1.m1.1c">{}^{a}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.1.m1.1d">start_FLOATSUPERSCRIPT italic_a end_FLOATSUPERSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.2.2">43.02%</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.3">19.87</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.3.3.1">CR<math alttext="{}^{a}" class="ltx_Math" display="inline" id="S4.T2.3.3.1.m1.1"><semantics id="S4.T2.3.3.1.m1.1a"><msup id="S4.T2.3.3.1.m1.1.1" xref="S4.T2.3.3.1.m1.1.1.cmml"><mi id="S4.T2.3.3.1.m1.1.1a" xref="S4.T2.3.3.1.m1.1.1.cmml"></mi><mi id="S4.T2.3.3.1.m1.1.1.1" xref="S4.T2.3.3.1.m1.1.1.1.cmml">a</mi></msup><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.1.m1.1b"><apply id="S4.T2.3.3.1.m1.1.1.cmml" xref="S4.T2.3.3.1.m1.1.1"><ci id="S4.T2.3.3.1.m1.1.1.1.cmml" xref="S4.T2.3.3.1.m1.1.1.1">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.1.m1.1c">{}^{a}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.3.1.m1.1d">start_FLOATSUPERSCRIPT italic_a end_FLOATSUPERSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.3.3.2">45.25%</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.3.3">17</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.6.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.4.6.2.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T2.4.6.2.1.1">E2G</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.4.6.2.2"><span class="ltx_text ltx_font_bold" id="S4.T2.4.6.2.2.1">53.76%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.6.2.3">1</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.7.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.4.7.3.1" rowspan="2"><span class="ltx_text" id="S4.T2.4.7.3.1.1">PALM2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.4.7.3.2">CoT</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.4.7.3.3">35.0%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.7.3.4">1</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.8.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.4.8.4.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T2.4.8.4.1.1">E2G</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.4.8.4.2">37.0%</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.8.4.3">2</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="S4.T2.4.4.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T2.4.4.1.1">SOTA<math alttext="{}^{b}" class="ltx_Math" display="inline" id="S4.T2.4.4.1.1.m1.1"><semantics id="S4.T2.4.4.1.1.m1.1a"><msup id="S4.T2.4.4.1.1.m1.1.1" xref="S4.T2.4.4.1.1.m1.1.1.cmml"><mi id="S4.T2.4.4.1.1.m1.1.1a" xref="S4.T2.4.4.1.1.m1.1.1.cmml"></mi><mi id="S4.T2.4.4.1.1.m1.1.1.1" xref="S4.T2.4.4.1.1.m1.1.1.1.cmml">b</mi></msup><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.1.1.m1.1b"><apply id="S4.T2.4.4.1.1.m1.1.1.cmml" xref="S4.T2.4.4.1.1.m1.1.1"><ci id="S4.T2.4.4.1.1.m1.1.1.1.cmml" xref="S4.T2.4.4.1.1.m1.1.1.1">𝑏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.1.1.m1.1c">{}^{b}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.4.4.1.1.m1.1d">start_FLOATSUPERSCRIPT italic_b end_FLOATSUPERSCRIPT</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S4.T2.4.4.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S4.T2.4.4.3">45.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.4.4.4">-</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Performance on LogiQA. <math alttext="{}^{a-b}" class="ltx_Math" display="inline" id="S4.T2.6.m1.1"><semantics id="S4.T2.6.m1.1b"><msup id="S4.T2.6.m1.1.1" xref="S4.T2.6.m1.1.1.cmml"><mi id="S4.T2.6.m1.1.1b" xref="S4.T2.6.m1.1.1.cmml"></mi><mrow id="S4.T2.6.m1.1.1.1" xref="S4.T2.6.m1.1.1.1.cmml"><mi id="S4.T2.6.m1.1.1.1.2" xref="S4.T2.6.m1.1.1.1.2.cmml">a</mi><mo id="S4.T2.6.m1.1.1.1.1" xref="S4.T2.6.m1.1.1.1.1.cmml">−</mo><mi id="S4.T2.6.m1.1.1.1.3" xref="S4.T2.6.m1.1.1.1.3.cmml">b</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.T2.6.m1.1c"><apply id="S4.T2.6.m1.1.1.cmml" xref="S4.T2.6.m1.1.1"><apply id="S4.T2.6.m1.1.1.1.cmml" xref="S4.T2.6.m1.1.1.1"><minus id="S4.T2.6.m1.1.1.1.1.cmml" xref="S4.T2.6.m1.1.1.1.1"></minus><ci id="S4.T2.6.m1.1.1.1.2.cmml" xref="S4.T2.6.m1.1.1.1.2">𝑎</ci><ci id="S4.T2.6.m1.1.1.1.3.cmml" xref="S4.T2.6.m1.1.1.1.3">𝑏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.m1.1d">{}^{a-b}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.6.m1.1e">start_FLOATSUPERSCRIPT italic_a - italic_b end_FLOATSUPERSCRIPT</annotation></semantics></math> refer to <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a class="ltx_ref" href="#bib.bib52" title="">2023b</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Ouyang et al. (<a class="ltx_ref" href="#bib.bib30" title="">2021</a>)</cite> respectively. </figcaption>
</figure>
<figure class="ltx_table" id="S4.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T3.3.4.1.1">Backbone</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.3.4.1.2">Method</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.3.4.1.3">EM</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.3.4.1.4">F1</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.3.5.2.1" rowspan="2"><span class="ltx_text" id="S4.T3.3.5.2.1.1">GPT-4</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.3.5.2.2">CoT</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.3.5.2.3">56.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.5.2.4">71.3</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.6.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.3.6.3.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T3.3.6.3.1.1">E2G</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.3.6.3.2">56.4</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.6.3.3">73.7</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.1.1.2" rowspan="2"><span class="ltx_text" id="S4.T3.1.1.2.1">PALM2</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.1.3">CoT</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.1.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.1">82.0<math alttext="{}^{a}" class="ltx_Math" display="inline" id="S4.T3.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.m1.1a"><msup id="S4.T3.1.1.1.m1.1.1" xref="S4.T3.1.1.1.m1.1.1.cmml"><mi id="S4.T3.1.1.1.m1.1.1a" xref="S4.T3.1.1.1.m1.1.1.cmml"></mi><mi id="S4.T3.1.1.1.m1.1.1.1" xref="S4.T3.1.1.1.m1.1.1.1.cmml">a</mi></msup><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.m1.1b"><apply id="S4.T3.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1"><ci id="S4.T3.1.1.1.m1.1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1.1">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.m1.1c">{}^{a}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.m1.1d">start_FLOATSUPERSCRIPT italic_a end_FLOATSUPERSCRIPT</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.7.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.3.7.4.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T3.3.7.4.1.1">E2G</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.3.7.4.2"><span class="ltx_text ltx_font_bold" id="S4.T3.3.7.4.2.1">79.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.7.4.3"><span class="ltx_text ltx_font_bold" id="S4.T3.3.7.4.3.1">83.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S4.T3.3.3.3"><span class="ltx_text ltx_font_smallcaps" id="S4.T3.3.3.3.1">few-shot SOTA</span></th>
<td class="ltx_td ltx_border_bb ltx_border_r ltx_border_t" id="S4.T3.3.3.4"></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S4.T3.3.3.5">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T3.3.3.2">82.4<math alttext="{}^{a}" class="ltx_Math" display="inline" id="S4.T3.2.2.1.m1.1"><semantics id="S4.T3.2.2.1.m1.1a"><msup id="S4.T3.2.2.1.m1.1.1" xref="S4.T3.2.2.1.m1.1.1.cmml"><mi id="S4.T3.2.2.1.m1.1.1a" xref="S4.T3.2.2.1.m1.1.1.cmml"></mi><mi id="S4.T3.2.2.1.m1.1.1.1" xref="S4.T3.2.2.1.m1.1.1.1.cmml">a</mi></msup><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.1.m1.1b"><apply id="S4.T3.2.2.1.m1.1.1.cmml" xref="S4.T3.2.2.1.m1.1.1"><ci id="S4.T3.2.2.1.m1.1.1.1.cmml" xref="S4.T3.2.2.1.m1.1.1.1">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.1.m1.1c">{}^{a}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.2.1.m1.1d">start_FLOATSUPERSCRIPT italic_a end_FLOATSUPERSCRIPT</annotation></semantics></math>/83.0<math alttext="{}^{b}" class="ltx_Math" display="inline" id="S4.T3.3.3.2.m2.1"><semantics id="S4.T3.3.3.2.m2.1a"><msup id="S4.T3.3.3.2.m2.1.1" xref="S4.T3.3.3.2.m2.1.1.cmml"><mi id="S4.T3.3.3.2.m2.1.1a" xref="S4.T3.3.3.2.m2.1.1.cmml"></mi><mi id="S4.T3.3.3.2.m2.1.1.1" xref="S4.T3.3.3.2.m2.1.1.1.cmml">b</mi></msup><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.2.m2.1b"><apply id="S4.T3.3.3.2.m2.1.1.cmml" xref="S4.T3.3.3.2.m2.1.1"><ci id="S4.T3.3.3.2.m2.1.1.1.cmml" xref="S4.T3.3.3.2.m2.1.1.1">𝑏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.2.m2.1c">{}^{b}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.3.3.2.m2.1d">start_FLOATSUPERSCRIPT italic_b end_FLOATSUPERSCRIPT</annotation></semantics></math>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Performance on DROP. <math alttext="{}^{a-b}" class="ltx_Math" display="inline" id="S4.T3.5.m1.1"><semantics id="S4.T3.5.m1.1b"><msup id="S4.T3.5.m1.1.1" xref="S4.T3.5.m1.1.1.cmml"><mi id="S4.T3.5.m1.1.1b" xref="S4.T3.5.m1.1.1.cmml"></mi><mrow id="S4.T3.5.m1.1.1.1" xref="S4.T3.5.m1.1.1.1.cmml"><mi id="S4.T3.5.m1.1.1.1.2" xref="S4.T3.5.m1.1.1.1.2.cmml">a</mi><mo id="S4.T3.5.m1.1.1.1.1" xref="S4.T3.5.m1.1.1.1.1.cmml">−</mo><mi id="S4.T3.5.m1.1.1.1.3" xref="S4.T3.5.m1.1.1.1.3.cmml">b</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.T3.5.m1.1c"><apply id="S4.T3.5.m1.1.1.cmml" xref="S4.T3.5.m1.1.1"><apply id="S4.T3.5.m1.1.1.1.cmml" xref="S4.T3.5.m1.1.1.1"><minus id="S4.T3.5.m1.1.1.1.1.cmml" xref="S4.T3.5.m1.1.1.1.1"></minus><ci id="S4.T3.5.m1.1.1.1.2.cmml" xref="S4.T3.5.m1.1.1.1.2">𝑎</ci><ci id="S4.T3.5.m1.1.1.1.3.cmml" xref="S4.T3.5.m1.1.1.1.3">𝑏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.m1.1d">{}^{a-b}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.5.m1.1e">start_FLOATSUPERSCRIPT italic_a - italic_b end_FLOATSUPERSCRIPT</annotation></semantics></math> refers to Gemini Technical Report and <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a class="ltx_ref" href="#bib.bib9" title="">2022</a>)</cite>. </figcaption>
</figure>
<figure class="ltx_table" id="S4.T4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T4.7" style="width:212.5pt;height:155.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-16.4pt,12.1pt) scale(0.866030355510063,0.866030355510063) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T4.7.7">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.7.7.8.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T4.7.7.8.1.1" rowspan="2"><span class="ltx_text" id="S4.T4.7.7.8.1.1.1">Backbone</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.7.7.8.1.2" rowspan="2"><span class="ltx_text" id="S4.T4.7.7.8.1.2.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S4.T4.7.7.8.1.3">HotpotQA</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.7.7.8.1.4">FEVER</td>
</tr>
<tr class="ltx_tr" id="S4.T4.7.7.9.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.7.7.9.2.1">EM</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.7.7.9.2.2">F1</td>
<td class="ltx_td ltx_align_center" id="S4.T4.7.7.9.2.3">Acc</td>
</tr>
<tr class="ltx_tr" id="S4.T4.7.7.10.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T4.7.7.10.3.1" rowspan="3"><span class="ltx_text" id="S4.T4.7.7.10.3.1.1">ChatGPT</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.7.7.10.3.2">CoT</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.7.7.10.3.3">43.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.7.7.10.3.4">55.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.7.7.10.3.5">76.7</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.1.1">ReACT+Reflexion<math alttext="{}^{6}" class="ltx_Math" display="inline" id="S4.T4.1.1.1.1.m1.1"><semantics id="S4.T4.1.1.1.1.m1.1a"><msup id="S4.T4.1.1.1.1.m1.1.1" xref="S4.T4.1.1.1.1.m1.1.1.cmml"><mi id="S4.T4.1.1.1.1.m1.1.1a" xref="S4.T4.1.1.1.1.m1.1.1.cmml"></mi><mn id="S4.T4.1.1.1.1.m1.1.1.1" xref="S4.T4.1.1.1.1.m1.1.1.1.cmml">6</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.m1.1b"><apply id="S4.T4.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.m1.1.1"><cn id="S4.T4.1.1.1.1.m1.1.1.1.cmml" type="integer" xref="S4.T4.1.1.1.1.m1.1.1.1">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.m1.1c">{}^{6}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT 6 end_FLOATSUPERSCRIPT</annotation></semantics></math> (t=2)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.1.2">42</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.1.3">-</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.1.4">-</td>
</tr>
<tr class="ltx_tr" id="S4.T4.7.7.11.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.7.7.11.4.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T4.7.7.11.4.1.1">E2G</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.7.7.11.4.2"><span class="ltx_text ltx_font_bold" id="S4.T4.7.7.11.4.2.1">47.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.7.7.11.4.3"><span class="ltx_text ltx_font_bold" id="S4.T4.7.7.11.4.3.1">59.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.7.7.11.4.4">80.7</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T4.2.2.2.2" rowspan="4"><span class="ltx_text" id="S4.T4.2.2.2.2.1">PALM2</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.2.2.1">CoT<math alttext="{}^{1}" class="ltx_Math" display="inline" id="S4.T4.2.2.2.1.m1.1"><semantics id="S4.T4.2.2.2.1.m1.1a"><msup id="S4.T4.2.2.2.1.m1.1.1" xref="S4.T4.2.2.2.1.m1.1.1.cmml"><mi id="S4.T4.2.2.2.1.m1.1.1a" xref="S4.T4.2.2.2.1.m1.1.1.cmml"></mi><mn id="S4.T4.2.2.2.1.m1.1.1.1" xref="S4.T4.2.2.2.1.m1.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.1.m1.1b"><apply id="S4.T4.2.2.2.1.m1.1.1.cmml" xref="S4.T4.2.2.2.1.m1.1.1"><cn id="S4.T4.2.2.2.1.m1.1.1.1.cmml" type="integer" xref="S4.T4.2.2.2.1.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.1.m1.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.2.2.2.1.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.2.2.3">29.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.2.2.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.2.2.5">56.3</td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.3.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.3.3.3.1">CoT-SC<math alttext="{}^{2}" class="ltx_Math" display="inline" id="S4.T4.3.3.3.1.m1.1"><semantics id="S4.T4.3.3.3.1.m1.1a"><msup id="S4.T4.3.3.3.1.m1.1.1" xref="S4.T4.3.3.3.1.m1.1.1.cmml"><mi id="S4.T4.3.3.3.1.m1.1.1a" xref="S4.T4.3.3.3.1.m1.1.1.cmml"></mi><mn id="S4.T4.3.3.3.1.m1.1.1.1" xref="S4.T4.3.3.3.1.m1.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T4.3.3.3.1.m1.1b"><apply id="S4.T4.3.3.3.1.m1.1.1.cmml" xref="S4.T4.3.3.3.1.m1.1.1"><cn id="S4.T4.3.3.3.1.m1.1.1.1.cmml" type="integer" xref="S4.T4.3.3.3.1.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.3.3.1.m1.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.3.3.3.1.m1.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.3.3.3.2">33.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.3.3.3.3">-</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.3.3.4">60.4</td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.4.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.4.4.4.1">ReACT+CoT-SC<math alttext="{}^{3}" class="ltx_Math" display="inline" id="S4.T4.4.4.4.1.m1.1"><semantics id="S4.T4.4.4.4.1.m1.1a"><msup id="S4.T4.4.4.4.1.m1.1.1" xref="S4.T4.4.4.4.1.m1.1.1.cmml"><mi id="S4.T4.4.4.4.1.m1.1.1a" xref="S4.T4.4.4.4.1.m1.1.1.cmml"></mi><mn id="S4.T4.4.4.4.1.m1.1.1.1" xref="S4.T4.4.4.4.1.m1.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T4.4.4.4.1.m1.1b"><apply id="S4.T4.4.4.4.1.m1.1.1.cmml" xref="S4.T4.4.4.4.1.m1.1.1"><cn id="S4.T4.4.4.4.1.m1.1.1.1.cmml" type="integer" xref="S4.T4.4.4.4.1.m1.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.4.4.1.m1.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.4.4.4.1.m1.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.4.4.4.2">35.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.4.4.4.3">-</td>
<td class="ltx_td ltx_align_center" id="S4.T4.4.4.4.4">62.0</td>
</tr>
<tr class="ltx_tr" id="S4.T4.7.7.12.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.7.7.12.5.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T4.7.7.12.5.1.1">E2G</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.7.7.12.5.2">46.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.7.7.12.5.3">57.9</td>
<td class="ltx_td ltx_align_center" id="S4.T4.7.7.12.5.4"><span class="ltx_text ltx_font_bold" id="S4.T4.7.7.12.5.4.1">81.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.7.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S4.T4.7.7.7.4"><span class="ltx_text ltx_font_smallcaps" id="S4.T4.7.7.7.4.1">Sup. SOTA</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S4.T4.7.7.7.5">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S4.T4.5.5.5.1">72.7<math alttext="{}^{4}" class="ltx_Math" display="inline" id="S4.T4.5.5.5.1.m1.1"><semantics id="S4.T4.5.5.5.1.m1.1a"><msup id="S4.T4.5.5.5.1.m1.1.1" xref="S4.T4.5.5.5.1.m1.1.1.cmml"><mi id="S4.T4.5.5.5.1.m1.1.1a" xref="S4.T4.5.5.5.1.m1.1.1.cmml"></mi><mn id="S4.T4.5.5.5.1.m1.1.1.1" xref="S4.T4.5.5.5.1.m1.1.1.1.cmml">4</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T4.5.5.5.1.m1.1b"><apply id="S4.T4.5.5.5.1.m1.1.1.cmml" xref="S4.T4.5.5.5.1.m1.1.1"><cn id="S4.T4.5.5.5.1.m1.1.1.1.cmml" type="integer" xref="S4.T4.5.5.5.1.m1.1.1.1">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.5.5.5.1.m1.1c">{}^{4}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.5.5.5.1.m1.1d">start_FLOATSUPERSCRIPT 4 end_FLOATSUPERSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S4.T4.6.6.6.2">85.0<math alttext="{}^{4}" class="ltx_Math" display="inline" id="S4.T4.6.6.6.2.m1.1"><semantics id="S4.T4.6.6.6.2.m1.1a"><msup id="S4.T4.6.6.6.2.m1.1.1" xref="S4.T4.6.6.6.2.m1.1.1.cmml"><mi id="S4.T4.6.6.6.2.m1.1.1a" xref="S4.T4.6.6.6.2.m1.1.1.cmml"></mi><mn id="S4.T4.6.6.6.2.m1.1.1.1" xref="S4.T4.6.6.6.2.m1.1.1.1.cmml">4</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T4.6.6.6.2.m1.1b"><apply id="S4.T4.6.6.6.2.m1.1.1.cmml" xref="S4.T4.6.6.6.2.m1.1.1"><cn id="S4.T4.6.6.6.2.m1.1.1.1.cmml" type="integer" xref="S4.T4.6.6.6.2.m1.1.1.1">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.6.6.6.2.m1.1c">{}^{4}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.6.6.6.2.m1.1d">start_FLOATSUPERSCRIPT 4 end_FLOATSUPERSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T4.7.7.7.3">94.2 <math alttext="{}^{5}" class="ltx_Math" display="inline" id="S4.T4.7.7.7.3.m1.1"><semantics id="S4.T4.7.7.7.3.m1.1a"><msup id="S4.T4.7.7.7.3.m1.1.1" xref="S4.T4.7.7.7.3.m1.1.1.cmml"><mi id="S4.T4.7.7.7.3.m1.1.1a" xref="S4.T4.7.7.7.3.m1.1.1.cmml"></mi><mn id="S4.T4.7.7.7.3.m1.1.1.1" xref="S4.T4.7.7.7.3.m1.1.1.1.cmml">5</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T4.7.7.7.3.m1.1b"><apply id="S4.T4.7.7.7.3.m1.1.1.cmml" xref="S4.T4.7.7.7.3.m1.1.1"><cn id="S4.T4.7.7.7.3.m1.1.1.1.cmml" type="integer" xref="S4.T4.7.7.7.3.m1.1.1.1">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.7.7.7.3.m1.1c">{}^{5}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.7.7.7.3.m1.1d">start_FLOATSUPERSCRIPT 5 end_FLOATSUPERSCRIPT</annotation></semantics></math>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Performance on HotpotQA. <math alttext="{}^{1-6}" class="ltx_Math" display="inline" id="S4.T4.9.m1.1"><semantics id="S4.T4.9.m1.1b"><msup id="S4.T4.9.m1.1.1" xref="S4.T4.9.m1.1.1.cmml"><mi id="S4.T4.9.m1.1.1b" xref="S4.T4.9.m1.1.1.cmml"></mi><mrow id="S4.T4.9.m1.1.1.1" xref="S4.T4.9.m1.1.1.1.cmml"><mn id="S4.T4.9.m1.1.1.1.2" xref="S4.T4.9.m1.1.1.1.2.cmml">1</mn><mo id="S4.T4.9.m1.1.1.1.1" xref="S4.T4.9.m1.1.1.1.1.cmml">−</mo><mn id="S4.T4.9.m1.1.1.1.3" xref="S4.T4.9.m1.1.1.1.3.cmml">6</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.T4.9.m1.1c"><apply id="S4.T4.9.m1.1.1.cmml" xref="S4.T4.9.m1.1.1"><apply id="S4.T4.9.m1.1.1.1.cmml" xref="S4.T4.9.m1.1.1.1"><minus id="S4.T4.9.m1.1.1.1.1.cmml" xref="S4.T4.9.m1.1.1.1.1"></minus><cn id="S4.T4.9.m1.1.1.1.2.cmml" type="integer" xref="S4.T4.9.m1.1.1.1.2">1</cn><cn id="S4.T4.9.m1.1.1.1.3.cmml" type="integer" xref="S4.T4.9.m1.1.1.1.3">6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.9.m1.1d">{}^{1-6}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.9.m1.1e">start_FLOATSUPERSCRIPT 1 - 6 end_FLOATSUPERSCRIPT</annotation></semantics></math> refers to <cite class="ltx_cite ltx_citemacro_citet">Wei et al. (<a class="ltx_ref" href="#bib.bib44" title="">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a class="ltx_ref" href="#bib.bib41" title="">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Yao et al. (<a class="ltx_ref" href="#bib.bib48" title="">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a class="ltx_ref" href="#bib.bib51" title="">2023a</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a class="ltx_ref" href="#bib.bib42" title="">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Shinn et al. (<a class="ltx_ref" href="#bib.bib37" title="">2023</a>)</cite> respectively. <span class="ltx_text ltx_font_typewriter" id="S4.T4.11.1">t=2</span> refers to our equivalent 2 trials. </figcaption>
</figure>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Arithmetic/Logical Context Reasoning</h2>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="281" id="S5.F3.g1" src="x3.png" width="407"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>CoT distracted by focusing on numerical precision only. <span class="ltx_text ltx_font_typewriter ltx_font_smallcaps" id="S5.F3.2.1">E2G<span class="ltx_text ltx_font_upright" id="S5.F3.2.1.1">-Pro</span></span> provides superior reasoning by considering both arithmetics and validity of rationales. </figcaption>
</figure>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We first evaluate our approach on MRC task with heavy arithmetic and logical reasoning complexities. We adopt the popular LogiQA <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="#bib.bib23" title="">2020</a>)</cite> and DROP <cite class="ltx_cite ltx_citemacro_citep">(Dua et al., <a class="ltx_ref" href="#bib.bib4" title="">2019</a>)</cite>. Given a small context, LogiQA questions are asked to choose among four options that can be inferred/derived from the context and in DROP, in general, the task to answer questions with complex arithmetical computations. Although reasoning in both tasks are mainly independent, however as they are connected to the context, LLMs reasoning still needs to be aligned. We compare the model performances using various prompting frameworks in Table <a class="ltx_ref" href="#S4.T2" title="Table 2 ‣ 4 Experimental Setup ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_tag">2</span></a> and Table <a class="ltx_ref" href="#S4.T3" title="Table 3 ‣ 4 Experimental Setup ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_tag">3</span></a>. Overall, <span class="ltx_text ltx_font_smallcaps" id="S5.p1.1.1">E2G</span> robustly boosts the real-time contextual reasoning in both benchmarks securing new state-of-the art 0-shot results. In LogiQA, <span class="ltx_text ltx_font_typewriter ltx_font_smallcaps" id="S5.p1.1.2">E2G<span class="ltx_text ltx_font_upright" id="S5.p1.1.2.1">-Pro</span></span> significantly outperformed existing approaches including ToT (11%), CR (9%) using GPT-4 as backbone. This reveals that variants built on CoT also suffer from generating inconsistent outputs, and guiding their reasoning paths w/ grounding precision can enhance a broad spectrum of CoT approaches. On DROP, PALM-2 achieve higher performances than GPT-4 in general, and w/ <span class="ltx_text ltx_font_typewriter ltx_font_smallcaps" id="S5.p1.1.3">E2G<span class="ltx_text ltx_font_upright" id="S5.p1.1.3.1">-Pro</span></span> it outperforms the few-shot F1 scores of recent performer LLM Gemini Ultra. In compare to the best performances of <span class="ltx_text ltx_font_typewriter ltx_font_smallcaps" id="S5.p1.1.4">E2G<span class="ltx_text ltx_font_upright" id="S5.p1.1.4.1">-Pro</span></span> in these two tasks, F1 performances of <span class="ltx_text ltx_font_typewriter ltx_font_smallcaps" id="S5.p1.1.5">E2G<span class="ltx_text ltx_font_upright" id="S5.p1.1.5.1">-base</span></span> are (LogiQA 53.76 vs 51.77) and (83.3 vs 82.68) which validates our intuition that <span class="ltx_text ltx_font_typewriter ltx_font_smallcaps" id="S5.p1.1.6">E2G<span class="ltx_text ltx_font_upright" id="S5.p1.1.6.1">-Pro</span></span> excels more when the task is based on arithmetic and logical reasoning. Figure <a class="ltx_ref" href="#S5.F3" title="Figure 3 ‣ 5 Arithmetic/Logical Context Reasoning ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_tag">3</span></a> shows an example how <span class="ltx_text ltx_font_smallcaps" id="S5.p1.1.7">E2G</span> provides superior reasoning w.r.t CoT (more in Appendix).</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Multi-hop QA w/ Distracting Contexts</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We also consider more complex QA problems. For this task, we evaluate on the distractor split of HotpotQA <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="#bib.bib46" title="">2018</a>)</cite> benchmark, where for each inference query, two relevant and 8 irrelevant documents in the context and in fact only 2-5 far-apart sentences from this large context is actually the rationales. We report our results in Table <a class="ltx_ref" href="#S4.T4" title="Table 4 ‣ 4 Experimental Setup ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_tag">4</span></a>. W/ both ChatGPT and PALM2, <span class="ltx_text ltx_font_smallcaps" id="S6.p1.1.1">E2G</span> shows a large margin in performance gains w.r.t CoT and other variants.</p>
</div>
<figure class="ltx_figure" id="S6.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="141" id="S6.F4.g1" src="x4.png" width="407"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>CoT reasoning does not account for grounding–even overrides the contextual fact w/ its pre-trained knowledge (or hallucinates). Finding <span class="ltx_text ltx_font_typewriter" id="S6.F4.3.1">Evidence</span>, <span class="ltx_text ltx_font_smallcaps" id="S6.F4.4.2">E2G</span> enables more factually verified reasoning. </figcaption>
</figure>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Using ChatGPT, <span class="ltx_text ltx_font_smallcaps" id="S6.p2.1.1">E2G</span> achieves 4% gain over CoT in both EM and F1 score and w/ PALM2 the gains are more (17EM).
In addition, other iterative baselines such as ReACT, Reflexion find the hops (relevant texts) one by one, and hence take either longer number of trials or fail to answer leveraging the local hops altogether. In contrast, E and G both steps solve the full problem in each–showcasing more efficiency. As connecting the relevant texts distant apart is one of the key challenges in multi-hop QA, to understand the advantages of extracting <span class="ltx_text ltx_font_typewriter" id="S6.p2.1.2">Evidence</span> along w/ the answer (i.e., single agent), we perform an additional experiment towards the multi-agentic approach.
We take the ChatGPT model to create one agent as a co-reference resolutioner to make the sentences independent, then another agent as a retriever model that ranks the sentences and then another QA agent which takes the top few or all of the ranked sentences to generate the final answer. We observe this approach falls behind in 10EM points than ours. We identify that the problems are mutli-folds such as each agent contributes in propagating its own error like imperfect co-reference resolution and ranking. We also find they are too sensitive to input Agent Instruction. But the key problem is as these questions are multi-hop w/ actually connecting multiple sentences together (i.e., w/ each individual sentences) toward the answer, other agents do not know how they will be utilized by the QA agent, and hence their ranking becomes even worse. For instance regarding a question about two persons, it may rank all the sentences that are only speaking about person-1 first and then all about person-2–making the problem even harder. Figure <a class="ltx_ref" href="#S6.F4" title="Figure 4 ‣ 6 Multi-hop QA w/ Distracting Contexts ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_tag">4</span></a> depicts an example of reasoning w/ distractors.</p>
</div>
<figure class="ltx_table" id="S6.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T5.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T5.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S6.T5.1.2.1.1" rowspan="2"><span class="ltx_text" id="S6.T5.1.2.1.1.1">Backbone</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S6.T5.1.2.1.2" rowspan="2"><span class="ltx_text" id="S6.T5.1.2.1.2.1">Method</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S6.T5.1.2.1.3">NQ</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S6.T5.1.2.1.4">TQA</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S6.T5.1.2.1.5">WOW</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T5.1.2.1.6">ELI5</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T5.1.3.2.1">EM</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T5.1.3.2.2">F1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T5.1.3.2.3">EM</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T5.1.3.2.4">F1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T5.1.3.2.5">F1</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.3.2.6">F1</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T5.1.4.3.1" rowspan="2"><span class="ltx_text" id="S6.T5.1.4.3.1.1">ChatGPT</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T5.1.4.3.2">CoT</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T5.1.4.3.3">41.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T5.1.4.3.4">51.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T5.1.4.3.5">68.3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T5.1.4.3.6">75.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T5.1.4.3.7">13.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.4.3.8"><span class="ltx_text ltx_font_bold" id="S6.T5.1.4.3.8.1">27.0</span></td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T5.1.5.4.1">
<span class="ltx_text ltx_font_smallcaps" id="S6.T5.1.5.4.1.1">E2G</span>-base</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T5.1.5.4.2"><span class="ltx_text ltx_font_bold" id="S6.T5.1.5.4.2.1">42.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T5.1.5.4.3"><span class="ltx_text ltx_font_bold" id="S6.T5.1.5.4.3.1">53.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T5.1.5.4.4"><span class="ltx_text ltx_font_bold" id="S6.T5.1.5.4.4.1">69.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T5.1.5.4.5"><span class="ltx_text ltx_font_bold" id="S6.T5.1.5.4.5.1">76.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T5.1.5.4.6"><span class="ltx_text ltx_font_bold" id="S6.T5.1.5.4.6.1">15.0</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.5.4.7"><span class="ltx_text" id="S6.T5.1.5.4.7.1" style="color:#FF0000;">25.1</span></td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T5.1.6.5.1" rowspan="2"><span class="ltx_text" id="S6.T5.1.6.5.1.1">PALM2</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T5.1.6.5.2">CoT</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T5.1.6.5.3">28.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T5.1.6.5.4">36.6.</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T5.1.6.5.5">46.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T5.1.6.5.6">51.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T5.1.6.5.7">12.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.6.5.8">15.3</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.7.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T5.1.7.6.1">
<span class="ltx_text ltx_font_smallcaps" id="S6.T5.1.7.6.1.1">E2G</span>-base</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T5.1.7.6.2">31.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T5.1.7.6.3">39.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T5.1.7.6.4">46.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T5.1.7.6.5">52.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T5.1.7.6.6">12.4</td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.7.6.7">17.4</td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S6.T5.1.1.1"><span class="ltx_text ltx_font_smallcaps" id="S6.T5.1.1.1.1">Sup. SOTA<math alttext="{}^{1}" class="ltx_Math" display="inline" id="S6.T5.1.1.1.1.m1.1"><semantics id="S6.T5.1.1.1.1.m1.1a"><msup id="S6.T5.1.1.1.1.m1.1.1" xref="S6.T5.1.1.1.1.m1.1.1.cmml"><mi id="S6.T5.1.1.1.1.m1.1.1a" xref="S6.T5.1.1.1.1.m1.1.1.cmml"></mi><mn id="S6.T5.1.1.1.1.m1.1.1.1" mathvariant="normal" xref="S6.T5.1.1.1.1.m1.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="S6.T5.1.1.1.1.m1.1b"><apply id="S6.T5.1.1.1.1.m1.1.1.cmml" xref="S6.T5.1.1.1.1.m1.1.1"><cn id="S6.T5.1.1.1.1.m1.1.1.1.cmml" type="integer" xref="S6.T5.1.1.1.1.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.1.1.1.1.m1.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="S6.T5.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math></span></th>
<th class="ltx_td ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S6.T5.1.1.2"></th>
<td class="ltx_td ltx_border_bb ltx_border_r ltx_border_t" id="S6.T5.1.1.3"></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S6.T5.1.1.4">61.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S6.T5.1.1.5">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S6.T5.1.1.6">71.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S6.T5.1.1.7">68.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S6.T5.1.1.8">73.9</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Performance on NQ, TQA, WOW, and ELI5. <math alttext="{}^{1}" class="ltx_Math" display="inline" id="S6.T5.3.m1.1"><semantics id="S6.T5.3.m1.1b"><msup id="S6.T5.3.m1.1.1" xref="S6.T5.3.m1.1.1.cmml"><mi id="S6.T5.3.m1.1.1b" xref="S6.T5.3.m1.1.1.cmml"></mi><mn id="S6.T5.3.m1.1.1.1" xref="S6.T5.3.m1.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="S6.T5.3.m1.1c"><apply id="S6.T5.3.m1.1.1.cmml" xref="S6.T5.3.m1.1.1"><cn id="S6.T5.3.m1.1.1.1.cmml" type="integer" xref="S6.T5.3.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.3.m1.1d">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="S6.T5.3.m1.1e">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math> &amp; <span class="ltx_text" id="S6.T5.5.1" style="color:#FF0000;">Red</span> refer to <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a class="ltx_ref" href="#bib.bib42" title="">2023</a>)</cite> &amp; an inferior performance.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Retrieval Augmented Generation </h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In addition to the MRC and Distractor, we evaluate our framework on the following five RAG tasks in the KILT benchmark <cite class="ltx_cite ltx_citemacro_citep">(Petroni et al., <a class="ltx_ref" href="#bib.bib34" title="">2021</a>)</cite>.</p>
</div>
<section class="ltx_paragraph" id="S7.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Fact Verification</h4>
<div class="ltx_para" id="S7.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S7.SS0.SSS0.Px1.p1.1">We adopt the Fact Extraction and VERification (FEVER) dataset <cite class="ltx_cite ltx_citemacro_cite">Thorne et al. (<a class="ltx_ref" href="#bib.bib40" title="">2018</a>)</cite>. Given a claims, the task is to determine whether it preserves the fact in the Wikipedia reference (“SUPPORTS”), or contradicts (“REFUTES”). As Table <a class="ltx_ref" href="#S4.T4" title="Table 4 ‣ 4 Experimental Setup ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_tag">4</span></a> shows on FEVER, <span class="ltx_text ltx_font_smallcaps" id="S7.SS0.SSS0.Px1.p1.1.1">E2G</span> significantly outperforms strong baselines like CoT-SC, ReACT and achieves a new 0-shot SOTA accuracy of 81.
Our <span class="ltx_text ltx_font_typewriter" id="S7.SS0.SSS0.Px1.p1.1.2">Evidence</span> captures necessary rationales to judge the claim, and similar to HotpotQA, solving the problem globally gives us advanatges over iterative CoT variants (examples in Appendix).</p>
</div>
</section>
<section class="ltx_paragraph" id="S7.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Open-Domain Question Answering</h4>
<div class="ltx_para" id="S7.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S7.SS0.SSS0.Px2.p1.1">We adopt the Natural Questions (NQ) <cite class="ltx_cite ltx_citemacro_cite">Kwiatkowski et al. (<a class="ltx_ref" href="#bib.bib18" title="">2019</a>)</cite> and TriviaQA (TQA) <cite class="ltx_cite ltx_citemacro_cite">Joshi et al. (<a class="ltx_ref" href="#bib.bib11" title="">2017</a>)</cite> benchmark to analyze our prompting framework. For each example, there is a short associated answers (less than five tokens) to generate. We present model performances w/ <span class="ltx_text ltx_font_smallcaps" id="S7.SS0.SSS0.Px2.p1.1.1">E2G</span> in Table <a class="ltx_ref" href="#S6.T5" title="Table 5 ‣ 6 Multi-hop QA w/ Distracting Contexts ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_tag">5</span></a>. Initially, while we achieve significant gains in TQA (obtaining a new SOTA score) we found exact same score as CoT (41.6 EM) on NQ.</p>
</div>
<figure class="ltx_figure" id="S7.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="122" id="S7.F5.g1" src="x5.png" width="407"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>E-step may focus on answering partially when asked joint questions or multiple named entity answers. Hence, to increase our chances, in second step (G) Context we use the <span class="ltx_text ltx_font_typewriter" id="S7.F5.2.1">Evidence + Original Context</span>. </figcaption>
</figure>
<div class="ltx_para" id="S7.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="S7.SS0.SSS0.Px2.p2.1">Upon investing, we realize that questions in NQ are often joint or has multiple named entity answers, which the model may focus to answer partially (as our Agent Instruction says to generate very short responses (&lt;5 tokens).
Hence, we use <span class="ltx_text ltx_font_typewriter" id="S7.SS0.SSS0.Px2.p2.1.1">Evidence + Original Context</span> as the G-step "Context" to provide another chance to combine parts from it (See Figure <a class="ltx_ref" href="#S7.F5" title="Figure 5 ‣ Open-Domain Question Answering ‣ 7 Retrieval Augmented Generation ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_tag">5</span></a>). To grasp more on improving <span class="ltx_text ltx_font_smallcaps" id="S7.SS0.SSS0.Px2.p2.1.2">E2G</span> , we further develop an adaptive version with a heuristic that for the well formed questions (starts with <em class="ltx_emph ltx_font_italic" id="S7.SS0.SSS0.Px2.p2.1.3">wh</em> words or "be/do/have/modal" words) are often from this type. Hence using <span class="ltx_text ltx_font_typewriter" id="S7.SS0.SSS0.Px2.p2.1.4">Evidence + Original Context</span> for them and using only <span class="ltx_text ltx_font_typewriter" id="S7.SS0.SSS0.Px2.p2.1.5">Evidence)</span> for ill-formed others (e.g., my age is what?), we find even better performance (See Figure <a class="ltx_ref" href="#S7.F6" title="Figure 6 ‣ Open-Domain Question Answering ‣ 7 Retrieval Augmented Generation ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_tag">6</span></a>).</p>
</div>
<figure class="ltx_figure" id="S7.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="199" id="S7.F6.g1" src="x6.png" width="332"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Reasoning w/ different "Context" in G-step. Adaptive means selecting them dynamically on the fly.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S7.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Open-ended Long Form Generation</h4>
<figure class="ltx_figure" id="S7.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="139" id="S7.F7.g1" src="x7.png" width="407"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Dialogue by ChatGPT using CoT and <span class="ltx_text ltx_font_smallcaps" id="S7.F7.2.1">E2G</span>.</figcaption>
</figure>
<div class="ltx_para" id="S7.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S7.SS0.SSS0.Px3.p1.1">Although, our focus is to develop a context-aware reasoning prompt, to shed more lights on its possible usage in open-ended generation, we consider two verbose QA tasks that require generating longer outputs. We adopt (a) the Knowledge-Grounded Dialog Generation task using the WoW dataset <cite class="ltx_cite ltx_citemacro_cite">Dinan et al. (<a class="ltx_ref" href="#bib.bib3" title="">2019</a>)</cite> from KILT a long, where a short history of previous dialogues is given as context, and the task is generate one/few sentences of next-turn chat response. (ii) another challenging task is Long Form QA on “Explain Like I’m Five” ELI5 <cite class="ltx_cite ltx_citemacro_citep">(Fan et al., <a class="ltx_ref" href="#bib.bib5" title="">2019</a>)</cite> dataset, which requires more elaborate and in-depth answers to open-ended questions. Table <a class="ltx_ref" href="#S6.T5" title="Table 5 ‣ 6 Multi-hop QA w/ Distracting Contexts ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_tag">5</span></a> compares <span class="ltx_text ltx_font_smallcaps" id="S7.SS0.SSS0.Px3.p1.1.1">E2G</span> w/ CoT baseline.
As expected, being verbose the performance difference is less than other tasks where only particular paths are valid. Among them on WoW, <span class="ltx_text ltx_font_smallcaps" id="S7.SS0.SSS0.Px3.p1.1.2">E2G</span> achieves a slight gain over CoT, to validate we perform a
voluntary small-scale human evaluation on generated dialogues on 4% of the test instances. We found a clear human preference of 71% in compare to CoT responses by ChatGPT considering factual correctness, similarity to gold response and naturalness. Figure <a class="ltx_ref" href="#S7.F7" title="Figure 7 ‣ Open-ended Long Form Generation ‣ 7 Retrieval Augmented Generation ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_tag">7</span></a> shows a hand-picked example dialogue but being verbose often the overall responses are not much different. In fact this phenomena gets more amplified with the length of expected output in ELI5. While in both benchmarks our results are as par the existing performer models like RAG <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al., <a class="ltx_ref" href="#bib.bib20" title="">2020</a>)</cite>, FiD <cite class="ltx_cite ltx_citemacro_citep">(Izacard and Grave, <a class="ltx_ref" href="#bib.bib10" title="">2021</a>)</cite>, and evidentiality-guided (<span class="ltx_text ltx_font_smallcaps" id="S7.SS0.SSS0.Px3.p1.1.3">Evi</span>;<cite class="ltx_cite ltx_citemacro_citep">(Asai et al., <a class="ltx_ref" href="#bib.bib1" title="">2022</a>)</cite>, they are far behind than recent Supervised SOTA performance in <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a class="ltx_ref" href="#bib.bib42" title="">2023</a>)</cite> which uses filtered contexts. We found that the retrieval recalls in WoW and ELI5 are lower than others (See Figure <a class="ltx_ref" href="#S7.F8" title="Figure 8 ‣ Open-ended Long Form Generation ‣ 7 Retrieval Augmented Generation ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_tag">8</span></a>) which may cause this. Upon investigating more on a performance drop in ELI5: while the task is to generate verbose answers, ours are still short (Word length 130 vs &lt;100) and may actually not fulfilling the target requirements–suggesting a future work of model fine-tuning/domain adaptation.</p>
</div>
<figure class="ltx_figure" id="S7.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="206" id="S7.F8.g1" src="x8.png" width="332"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>F1 scores w/ <span class="ltx_text ltx_font_smallcaps" id="S7.F8.2.1">E2G</span> &amp; CoT vs retrieval recall. </figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Ablation on Robustness &amp; Limitations</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">While <span class="ltx_text ltx_font_smallcaps" id="S8.p1.1.1">E2G</span> effectively enhances CoT-like prompting, featuring a robustly improved context-aware and retrieval-augmented reasoning across various tasks, benchmarks, language models (LLMs), and settings, as demonstrated in Section <a class="ltx_ref" href="#S5" title="5 Arithmetic/Logical Context Reasoning ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_tag">5</span></a>-<a class="ltx_ref" href="#S7" title="7 Retrieval Augmented Generation ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_tag">7</span></a>, our comprehensive experiments and ablations also uncover potential shortcomings related to context-free reasoning and the generation of very long-form outputs, as highlighted in Section <a class="ltx_ref" href="#S3.SS2" title="3.2 E-step ‣ 3 Evidence to Generate (E2G) Prompting ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_tag">3.2</span></a>, <a class="ltx_ref" href="#S3.SS4" title="3.4 Features, Adaptation and Best Practices ‣ 3 Evidence to Generate (E2G) Prompting ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_tag">3.4</span></a>, <a class="ltx_ref" href="#S7" title="7 Retrieval Augmented Generation ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_tag">7</span></a>. Further insights are gained through additional ablations from various perspectives.
<span class="ltx_text ltx_font_bold" id="S8.p1.1.2">Cost:</span> While <span class="ltx_text ltx_font_smallcaps" id="S8.p1.1.3">E2G</span>-pro demonstrates greater expressiveness and generality, when two-stepped prompting is applied (e.g., for long context), <span class="ltx_text ltx_font_smallcaps" id="S8.p1.1.4">E2G</span>-base can still be more cost-effectiveness. This is evident due to the fact that (i) prompts requiring step-by-step demonstrations are often more detailed and output token costs typically exceed input token costs. (ii) the G-step input context is a concise <em class="ltx_emph ltx_font_italic" id="S8.p1.1.5">Evidence</em>.
<span class="ltx_text ltx_font_bold" id="S8.p1.1.6">Failures:</span> Ablations reveal certain limitations of <span class="ltx_text ltx_font_smallcaps" id="S8.p1.1.7">E2G</span> , including (i) occasional hallucinations, (ii) an overemphasis on grounding leading to the model’s failure to infer common sense, leverage generic world knowledge, logic, and principles, and (iii) a drawback associated with overemphasizing grounding, causing the model to generate responses such as "unknown," "cannot be determined," or "not mentioned in the context" more frequently than the baseline CoT. Specific examples of categorical mistakes are provided in the Appendix.
<span class="ltx_text ltx_font_bold" id="S8.p1.1.8">Drawbacks w.r.t CoT variants:</span> In comparison to iterative CoT variants (e.g., Reflexion), <span class="ltx_text ltx_font_smallcaps" id="S8.p1.1.9">E2G</span> is two-step only method and does not extend to additional turns. In addition, it does not explore diverse paths like ToT and CR, and consequently, it may lag behind in scenarios where context-grounding is not a crucial requirement.</p>
</div>
<figure class="ltx_figure" id="S8.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="96" id="S8.F9.g1" src="x9.png" width="407"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Overemphasising on grounding can hinder model from leveraging world knowledge, common sense, etc., (e.g., Man. U. is a team in premier league)</figcaption>
</figure>
</section>
<section class="ltx_section" id="S9">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span>Related Work</h2>
<div class="ltx_para" id="S9.p1">
<p class="ltx_p" id="S9.p1.1">Various prompting paradigms have been studied in literature toward enhancing reasoning in LLMs. In Section <a class="ltx_ref" href="#S1" title="1 Introduction ‣ Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning"><span class="ltx_text ltx_ref_tag">1</span></a>, we provide
a (non-exhaustive) list of examples that employ the widely applied chain-of-thought techniques. Among others, search-based <cite class="ltx_cite ltx_citemacro_citep">(Pryzant et al., <a class="ltx_ref" href="#bib.bib35" title="">2023</a>; Lu et al., <a class="ltx_ref" href="#bib.bib25" title="">2021</a>)</cite>, Program-aided LLM generation <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="#bib.bib22" title="">2023a</a>; Gao et al., <a class="ltx_ref" href="#bib.bib6" title="">2023</a>; Jung et al., <a class="ltx_ref" href="#bib.bib12" title="">2022</a>; Zhu et al., <a class="ltx_ref" href="#bib.bib53" title="">2022</a>)</cite>, self generation of prompts <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a class="ltx_ref" href="#bib.bib8" title="">2023</a>; Yasunaga et al., <a class="ltx_ref" href="#bib.bib49" title="">2023</a>; Sun et al., <a class="ltx_ref" href="#bib.bib39" title="">2022</a>; Kim et al., <a class="ltx_ref" href="#bib.bib16" title="">2022</a>; Li et al., <a class="ltx_ref" href="#bib.bib21" title="">2022</a>)</cite>, self evaluation based approaches <cite class="ltx_cite ltx_citemacro_citep">(Madaan et al., <a class="ltx_ref" href="#bib.bib26" title="">2023</a>; Xie et al., <a class="ltx_ref" href="#bib.bib45" title="">2023</a>; Kim et al., <a class="ltx_ref" href="#bib.bib15" title="">2023</a>; Paul et al., <a class="ltx_ref" href="#bib.bib33" title="">2023</a>)</cite> have been studied.
Other works have also been extended w/ more complex multi-step reasoning procedure (e.g., using a different fine-tuned model <cite class="ltx_cite ltx_citemacro_citep">(Zelikman et al., <a class="ltx_ref" href="#bib.bib50" title="">2022</a>; Nye et al., <a class="ltx_ref" href="#bib.bib28" title="">2021</a>; Lester et al., <a class="ltx_ref" href="#bib.bib19" title="">2021</a>)</cite>) or for domain specific applications <cite class="ltx_cite ltx_citemacro_citep">(Parvez et al., <a class="ltx_ref" href="#bib.bib32" title="">2023</a>; Ouyang et al., <a class="ltx_ref" href="#bib.bib29" title="">2022</a>; Sanh et al., <a class="ltx_ref" href="#bib.bib36" title="">2021</a>; Wei et al., <a class="ltx_ref" href="#bib.bib43" title="">2021</a>)</cite>. Closest to ours, <cite class="ltx_cite ltx_citemacro_citet">Creswell et al. (<a class="ltx_ref" href="#bib.bib2" title="">2022</a>)</cite> adopts techniques that improves multi-stage reasoning where for each reasoning stage, it employs a two-agent (e.g., rationale selection and inference/premise derivation using different prompts) method instantiated with k-shot annotated examples. Although this is beyond simple prompting, our approach constructively is a single-agent (same task/prompt) two-step 0-shot instruction-only prompting technique. Nonetheless, while majority of existing works focus on context-free, arithmetic or commonsense reasoning, <span class="ltx_text ltx_font_smallcaps" id="S9.p1.1.1">E2G</span> uniquely tackles the context-aware reasoning reason more accurately and can be robustly applicable to tasks beyond reasoning (e.g. dialogue generation).</p>
</div>
</section>
<section class="ltx_section" id="S10">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">10 </span>Conclusion </h2>
<div class="ltx_para" id="S10.p1">
<p class="ltx_p" id="S10.p1.1">In this paper we explore the limitations of existing prompting frameworks for context-aware and retrieval augmented reasoning. We identify a fundamental bottleneck of ungrounded reasoning rationales within context that leads to potential hallucinations in LLMs when generating outputs. We propose a novel framework, Evidence to Generate (<span class="ltx_text ltx_font_smallcaps" id="S10.p1.1.1">E2G</span>) that uses the power of evidence – the "thought" sequences explicitly mentioned within the context necessary for the model output. Our prompt instruction dictates the LLM to first identify the evidence in the context and then use only that evidence to generate the target answer. On a wide range of context-aware and retrieval augmented generation tasks, our approach demonstrate its empowering LLMs to perform robust, accurate more reliable and trustworthy reasoning. Our future works include collecting annotation on context-reasoning datasets from performant black-box LLMs using this prompting techniques and distill it to open-source white-box LLMs via instruction fine-tuning on those annotated examples.</p>
</div>
</section>
<section class="ltx_section" id="S11">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">11 </span>Appendix</h2>
<figure class="ltx_figure" id="S11.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1155" id="S11.F10.g1" src="x10.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Example Model I/O (E2G vs CoT)</figcaption>
</figure>
<figure class="ltx_figure" id="S11.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1155" id="S11.F11.g1" src="x11.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Example Model I/O (correct and incorrect outcome)</figcaption>
</figure>
<figure class="ltx_figure" id="S11.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="796" id="S11.F12.g1" src="x12.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Example-1 w/ different prompting for LogiQA benchmarks</figcaption>
</figure>
<figure class="ltx_figure" id="S11.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="597" id="S11.F13.g1" src="x13.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Example-2 w/ different prompting for LogiQA benchmarks</figcaption>
</figure>
</section>
<section class="ltx_section" id="S12">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">12 </span>Limitations</h2>
<div class="ltx_para" id="S12.p1">
<p class="ltx_p" id="S12.p1.1">Our proposed inference framework has achieved significant gains over baseline approaches across various tasks, and in English. However, in certain data domains (e.g., bio-medical domain <cite class="ltx_cite ltx_citemacro_citep">(Nentidis et al., <a class="ltx_ref" href="#bib.bib27" title="">2023</a>)</cite>), or language (e.g., low-resource languages <cite class="ltx_cite ltx_citemacro_cite">Parvez and Chang (<a class="ltx_ref" href="#bib.bib31" title="">2021</a>)</cite>), under automatic evaluation metrics, and with sufficient computational resources or LLMs, it may not exhibit such trends. Another thing the performance scale in RAG tasksm may also vary if the retrieval accuracy is quite different than ours. Our evaluation considers the EM, F1, Accuracy, and such matrices for method comparisons, and a different comparison outcomes may be found while using different sets of matrices. For RAG tasks, we use top-5 retrieved documents w/o any context filtering and for all tasks, we did not adopt any model fine-tuning. Under these change in settings, a different kind of results may be obtained regarding which we do not conduct any experiments on.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Ethics</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">In this paper, we conduct a small scale human evaluation. All our participants were pre-informed about the voluntary nature of our survey, approximated required time, criteria of the feedback. An example human evaluation screen-shot can be found: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://forms.gle/h6WJtC7TrDj9LUNc6" title="">https://forms.gle/h6WJtC7TrDj9LUNc6</a>.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asai et al. (2022)</span>
<span class="ltx_bibblock">
Akari Asai, Matt Gardner, and Hannaneh Hajishirzi. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.naacl-main.162" title="">Evidentiality-guided generation for knowledge-intensive NLP tasks</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 2226–2243, Seattle, United States. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Creswell et al. (2022)</span>
<span class="ltx_bibblock">
Antonia Creswell, Murray Shanahan, and Irina Higgins. 2022.

</span>
<span class="ltx_bibblock">Selection-inference: Exploiting large language models for interpretable logical reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2205.09712</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dinan et al. (2019)</span>
<span class="ltx_bibblock">
Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=r1l73iRqKm" title="">Wizard of Wikipedia: Knowledge-powered conversational agents</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dua et al. (2019)</span>
<span class="ltx_bibblock">
Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N19-1246" title="">DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pages 2368–2378, Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al. (2019)</span>
<span class="ltx_bibblock">
Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P19-1346" title="">ELI5: Long form question answering</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, pages 3558–3567, Florence, Italy. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2023)</span>
<span class="ltx_bibblock">
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023.

</span>
<span class="ltx_bibblock">Pal: Program-aided language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">International Conference on Machine Learning</em>, pages 10764–10799. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guu et al. (2020)</span>
<span class="ltx_bibblock">
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://dl.acm.org/doi/abs/10.5555/3524938.3525306" title="">REALM: Retrieval-augmented language model pre-training</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">International Conference on Machine Learning</em>. JMLR.org.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2023)</span>
<span class="ltx_bibblock">
Zhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, Yujiu Yang, Rui Wang, Zhaopeng Tu, Shuming Shi, and Xing Wang. 2023.

</span>
<span class="ltx_bibblock">Exploring human-like translation strategy with large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2305.04118</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2022)</span>
<span class="ltx_bibblock">
Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022.

</span>
<span class="ltx_bibblock">Large language models can self-improve.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:2210.11610</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard and Grave (2021)</span>
<span class="ltx_bibblock">
Gautier Izacard and Edouard Grave. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.eacl-main.74" title="">Leveraging passage retrieval with generative models for open domain question answering</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</em>, pages 874–880, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi et al. (2017)</span>
<span class="ltx_bibblock">
Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P17-1147" title="">TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1601–1611, Vancouver, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jung et al. (2022)</span>
<span class="ltx_bibblock">
Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, and Yejin Choi. 2022.

</span>
<span class="ltx_bibblock">Maieutic prompting: Logically consistent reasoning with recursive explanations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2205.11822</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpukhin et al. (2020)</span>
<span class="ltx_bibblock">
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.emnlp-main.550" title="">Dense passage retrieval for open-domain question answering</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 6769–6781, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khattab et al. (2022)</span>
<span class="ltx_bibblock">
Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. 2022.

</span>
<span class="ltx_bibblock">Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2212.14024</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2023)</span>
<span class="ltx_bibblock">
Geunwoo Kim, Pierre Baldi, and Stephen McAleer. 2023.

</span>
<span class="ltx_bibblock">Language models can solve computer tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">arXiv preprint arXiv:2303.17491</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2022)</span>
<span class="ltx_bibblock">
Hyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim, Taeuk Kim, Kang Min Yoo, and Sang-goo Lee. 2022.

</span>
<span class="ltx_bibblock">Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:2206.08082</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kojima et al. (2022)</span>
<span class="ltx_bibblock">
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022.

</span>
<span class="ltx_bibblock">Large language models are zero-shot reasoners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Advances in neural information processing systems</em>, 35:22199–22213.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski et al. (2019)</span>
<span class="ltx_bibblock">
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00276" title="">Natural questions: A benchmark for question answering research</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Transactions of the Association for Computational Linguistics</em>, 7:452–466.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lester et al. (2021)</span>
<span class="ltx_bibblock">
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.emnlp-main.243" title="">The power of scale for parameter-efficient prompt tuning</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, pages 3045–3059, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2020)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf" title="">Retrieval-Augmented Generation for knowledge-intensive NLP tasks</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Advances in Neural Information Processing Systems</em>, volume 33, pages 9459–9474.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2022)</span>
<span class="ltx_bibblock">
Junlong Li, Zhuosheng Zhang, and Hai Zhao. 2022.

</span>
<span class="ltx_bibblock">Self-prompting large language models for open-domain qa.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">arXiv preprint arXiv:2212.08635</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023a)</span>
<span class="ltx_bibblock">
Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. 2023a.

</span>
<span class="ltx_bibblock">Llm+ p: Empowering large language models with optimal planning proficiency.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2304.11477</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2020)</span>
<span class="ltx_bibblock">
Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. 2020.

</span>
<span class="ltx_bibblock">Logiqa: A challenge dataset for machine reading comprehension with logical reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2007.08124</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023b)</span>
<span class="ltx_bibblock">
Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023b.

</span>
<span class="ltx_bibblock">Lost in the middle: How language models use long contexts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2307.03172</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2021)</span>
<span class="ltx_bibblock">
Ximing Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai, Daniel Khashabi, Ronan Le Bras, Lianhui Qin, Youngjae Yu, Rowan Zellers, et al. 2021.

</span>
<span class="ltx_bibblock">Neurologic a* esque decoding: Constrained text generation with lookahead heuristics.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:2112.08726</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Madaan et al. (2023)</span>
<span class="ltx_bibblock">
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2023.

</span>
<span class="ltx_bibblock">Self-refine: Iterative refinement with self-feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:2303.17651</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nentidis et al. (2023)</span>
<span class="ltx_bibblock">
Anastasios Nentidis, Anastasia Krithara, Georgios Paliouras, Eulàlia Farré-Maduell, Salvador Lima-López, and Martin Krallinger. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://link.springer.com/chapter/10.1007/978-3-031-28241-6_66" title="">Bioasq at clef2023: The eleventh edition of the large-scale biomedical semantic indexing and question answering challenge</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Advances in Information Retrieval</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nye et al. (2021)</span>
<span class="ltx_bibblock">
Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021.

</span>
<span class="ltx_bibblock">Show your work: Scratchpads for intermediate computation with language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">arXiv preprint arXiv:2112.00114</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al. (2022)</span>
<span class="ltx_bibblock">
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Advances in Neural Information Processing Systems</em>, 35:27730–27744.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al. (2021)</span>
<span class="ltx_bibblock">
Siru Ouyang, Zhuosheng Zhang, and Hai Zhao. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2105.10334" title="">Fact-driven logical reasoning</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">CoRR</em>, abs/2105.10334.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Parvez and Chang (2021)</span>
<span class="ltx_bibblock">
Md Rizwan Parvez and Kai-Wei Chang. 2021.

</span>
<span class="ltx_bibblock">Evaluating the values of sources in transfer learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 5084–5116.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Parvez et al. (2023)</span>
<span class="ltx_bibblock">
Md Rizwan Parvez, Jianfeng Chi, Wasi Uddin Ahmad, Yuan Tian, and Kai-Wei Chang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.eacl-main.16" title="">Retrieval enhanced data augmentation for question answering on privacy policies</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics</em>, pages 201–210, Dubrovnik, Croatia. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paul et al. (2023)</span>
<span class="ltx_bibblock">
Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings. 2023.

</span>
<span class="ltx_bibblock">Refiner: Reasoning feedback on intermediate representations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:2304.01904</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petroni et al. (2021)</span>
<span class="ltx_bibblock">
Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.naacl-main.200" title="">KILT: a benchmark for knowledge intensive language tasks</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 2523–2544, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pryzant et al. (2023)</span>
<span class="ltx_bibblock">
Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. 2023.

</span>
<span class="ltx_bibblock">Automatic prompt optimization with" gradient descent" and beam search.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">arXiv preprint arXiv:2305.03495</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanh et al. (2021)</span>
<span class="ltx_bibblock">
Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2021.

</span>
<span class="ltx_bibblock">Multitask prompted training enables zero-shot task generalization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:2110.08207</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shinn et al. (2023)</span>
<span class="ltx_bibblock">
Noah Shinn, Beck Labash, and Ashwin Gopinath. 2023.

</span>
<span class="ltx_bibblock">Reflexion: an autonomous agent with dynamic memory and self-reflection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:2303.11366</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shuster et al. (2021)</span>
<span class="ltx_bibblock">
Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.findings-emnlp.320" title="">Retrieval augmentation reduces hallucination in conversation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Findings of the Association for Computational Linguistics: EMNLP 2021</em>, pages 3784–3803, Punta Cana, Dominican Republic. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2022)</span>
<span class="ltx_bibblock">
Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. 2022.

</span>
<span class="ltx_bibblock">Recitation-augmented language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">arXiv preprint arXiv:2210.01296</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thorne et al. (2018)</span>
<span class="ltx_bibblock">
James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N18-1074" title="">FEVER: a large-scale dataset for fact extraction and VERification</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</em>, pages 809–819, New Orleans, Louisiana. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022)</span>
<span class="ltx_bibblock">
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022.

</span>
<span class="ltx_bibblock">Self-consistency improves chain of thought reasoning in language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">arXiv preprint arXiv:2203.11171</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023)</span>
<span class="ltx_bibblock">
Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, and Graham Neubig. 2023.

</span>
<span class="ltx_bibblock">Learning to filter context for retrieval-augmented generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">arXiv preprint arXiv:2311.08377</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2021)</span>
<span class="ltx_bibblock">
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021.

</span>
<span class="ltx_bibblock">Finetuned language models are zero-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">arXiv preprint arXiv:2109.01652</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Advances in Neural Information Processing Systems</em>, 35:24824–24837.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. (2023)</span>
<span class="ltx_bibblock">
Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie. 2023.

</span>
<span class="ltx_bibblock">Decomposition enhances reasoning via self-evaluation guided decoding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">arXiv preprint arXiv:2305.00633</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2018)</span>
<span class="ltx_bibblock">
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D18-1259" title="">HotpotQA: A dataset for diverse, explainable multi-hop question answering</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, pages 2369–2380, Brussels, Belgium. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al. (2023)</span>
<span class="ltx_bibblock">
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. 2023.

</span>
<span class="ltx_bibblock">Tree of thoughts: Deliberate problem solving with large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">arXiv preprint arXiv:2305.10601</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al. (2022)</span>
<span class="ltx_bibblock">
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022.

</span>
<span class="ltx_bibblock">React: Synergizing reasoning and acting in language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">arXiv preprint arXiv:2210.03629</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yasunaga et al. (2023)</span>
<span class="ltx_bibblock">
Michihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong Pasupat, Jure Leskovec, Percy Liang, Ed H Chi, and Denny Zhou. 2023.

</span>
<span class="ltx_bibblock">Large language models as analogical reasoners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">arXiv preprint arXiv:2310.01714</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zelikman et al. (2022)</span>
<span class="ltx_bibblock">
Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022.

</span>
<span class="ltx_bibblock">Star: Bootstrapping reasoning with reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Advances in Neural Information Processing Systems</em>, 35:15476–15488.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023a)</span>
<span class="ltx_bibblock">
Jiahao Zhang, Haiyang Zhang, Dongmei Zhang, Yong Liu, and Shen Huang. 2023a.

</span>
<span class="ltx_bibblock">Beam retrieval: General end-to-end retrieval for multi-hop question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">arXiv preprint arXiv:2308.08973</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023b)</span>
<span class="ltx_bibblock">
Yifan Zhang, Jingqin Yang, Yang Yuan, and Andrew Chi-Chih Yao. 2023b.

</span>
<span class="ltx_bibblock">Cumulative reasoning with large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">arXiv preprint arXiv:2308.04371</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2022)</span>
<span class="ltx_bibblock">
Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Ruyi Gan, Jiaxing Zhang, and Yujiu Yang. 2022.

</span>
<span class="ltx_bibblock">Solving math word problem via cooperative reasoning induced language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">arXiv preprint arXiv:2210.16257</em>.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Jan 11 09:47:40 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
