<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation</title>
<!--Generated on Tue Jun 18 20:47:41 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2406.13050v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S1" title="In Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S2" title="In Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S2.SS1" title="In 2 Related Work â€£ Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Query Optimization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S2.SS2" title="In 2 Related Work â€£ Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Adaptive Retrieval</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S3" title="In Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S3.SS1" title="In 3 Methodology â€£ Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Initial Query Assessment</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S3.SS1.SSS1" title="In 3.1 Initial Query Assessment â€£ 3 Methodology â€£ Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S3.SS1.SSS2" title="In 3.1 Initial Query Assessment â€£ 3 Methodology â€£ Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>Rewriting</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S3.SS2" title="In 3 Methodology â€£ Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Model Capability Check</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S3.SS2.SSS1" title="In 3.2 Model Capability Check â€£ 3 Methodology â€£ Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Information Retrieval</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S4" title="In Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S4.SS1" title="In 4 Experimental Setup â€£ Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Task Settings</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S4.SS1.SSS1" title="In 4.1 Task Settings â€£ 4 Experimental Setup â€£ Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>MultihopQA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S4.SS1.SSS2" title="In 4.1 Task Settings â€£ 4 Experimental Setup â€£ Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Commonsense Reasoning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S4.SS1.SSS3" title="In 4.1 Task Settings â€£ 4 Experimental Setup â€£ Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.3 </span>Fact Checking</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S4.SS1.SSS4" title="In 4.1 Task Settings â€£ 4 Experimental Setup â€£ Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.4 </span>Domain QA</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S4.SS2" title="In 4 Experimental Setup â€£ Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Retriever Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S4.SS3" title="In 4 Experimental Setup â€£ Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Baseline</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S5" title="In Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experimental Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S5.SS1" title="In 5 Experimental Results â€£ Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Comparison with Baselines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S5.SS2" title="In 5 Experimental Results â€£ Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Ablation Study</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S6" title="In Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S7" title="In Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#A1" title="In Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yige Shen 
<br class="ltx_break"/>Xiâ€™an Jiaotong University 
<br class="ltx_break"/>shenyige@stu.xjtu.edu.cn
&amp;Hao Jiang 
<br class="ltx_break"/>Xiâ€™an Jiaotong University 
<br class="ltx_break"/>haojiang@stu.xjtu.edu.cn
<span class="ltx_ERROR undefined" id="id1.1.id1">\AND</span>Hua Qu 
<br class="ltx_break"/>Xiâ€™an Jiaotong University 
<br class="ltx_break"/>qh@mail.xjtu.edu.cn
&amp;Jihong Zhao 
<br class="ltx_break"/>Xiâ€™an Jiaotong University 
<br class="ltx_break"/>zhaojihong@mail.xjtu.edu.cn
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">Despite their impressive capabilities, large language models (LLMs) often face challenges such as temporal misalignment and generating hallucinatory content. Enhancing LLMs with retrieval mechanisms to fetch relevant information from external sources offers a promising solution.
Inspired by the proverb "Think twice before you act," we propose a dual-angle evaluated retrieval-augmented generation framework <span class="ltx_text ltx_font_italic" id="id2.id1.1">Think-then-Act</span>. Unlike previous approaches that indiscriminately rewrite queries or perform retrieval regardless of necessity, or generate temporary responses before deciding on additional retrieval, which increases model generation costs, our framework employs a two-phase process: (i) assessing the input query for clarity and completeness to determine if rewriting is necessary; and (ii) evaluating the modelâ€™s capability to answer the query and deciding if additional retrieval is needed.
Experimental results on five datasets show that the <span class="ltx_text ltx_font_italic" id="id2.id1.2">Think-then-Act</span> framework significantly improves performance. Our framework demonstrates notable improvements in accuracy and efficiency compared to existing baselines and performs well in both English and non-English contexts. Ablation studies validate the optimal model confidence threshold, highlighting the resource optimization benefits of our approach.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.1">
<p class="ltx_p" id="p1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1.1">Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.1.2" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.1.2.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.2.1.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.1.1.1.1">Yige Shen</span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.2.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.2.2.1">Xiâ€™an Jiaotong University</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.3.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.3.3.1">shenyige@stu.xjtu.edu.cn</span></span>
</span>
</span></span>Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â <span class="ltx_text ltx_inline-block" id="p1.1.2.2" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.2.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.2.2.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.2.1.1.1.1.1">Hao Jiang</span></span></span>
<span class="ltx_tr" id="p1.1.2.2.1.2.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.2.1.2.2.1">Xiâ€™an Jiaotong University</span></span>
<span class="ltx_tr" id="p1.1.2.2.1.3.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.2.1.3.3.1">haojiang@stu.xjtu.edu.cn</span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.1.3" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.1.3.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.3.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.3.1.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.3.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.3.1.1.1.1.1.1">Hua Qu</span></span></span>
<span class="ltx_tr" id="p1.1.3.1.1.2.2">
<span class="ltx_td ltx_align_center" id="p1.1.3.1.1.2.2.1">Xiâ€™an Jiaotong University</span></span>
<span class="ltx_tr" id="p1.1.3.1.1.3.3">
<span class="ltx_td ltx_align_center" id="p1.1.3.1.1.3.3.1">qh@mail.xjtu.edu.cn</span></span>
</span>
</span></span>Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â <span class="ltx_text ltx_inline-block" id="p1.1.3.2" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.3.2.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.3.2.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.3.2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.3.2.1.1.1.1.1">Jihong Zhao</span></span></span>
<span class="ltx_tr" id="p1.1.3.2.1.2.2">
<span class="ltx_td ltx_align_center" id="p1.1.3.2.1.2.2.1">Xiâ€™an Jiaotong University</span></span>
<span class="ltx_tr" id="p1.1.3.2.1.3.3">
<span class="ltx_td ltx_align_center" id="p1.1.3.2.1.3.3.1">zhaojihong@mail.xjtu.edu.cn</span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<figure class="ltx_figure" id="S0.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="467" id="S0.F1.g1" src="x1.png" width="739"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Think-then-Act: (i) assessing the input query for clarity and completeness to determine if rewriting is necessary; (ii) evaluating the modelâ€™s capability to answer the query and deciding if additional retrieval is needed.</figcaption>
</figure>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large language models (LLMs) have become a cornerstone of natural language processing (NLP) systems due to their impressive capabilities in understanding and generating human language <cite class="ltx_cite ltx_citemacro_citep">(Brown etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib3" title="">2020</a>; Ouyang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib27" title="">2022</a>; OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib26" title="">2023</a>)</cite>.
Despite their success, LLMs often suffer from temporal misalignment <cite class="ltx_cite ltx_citemacro_cite">RÃ¶ttger and Pierrehumbert (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib31" title="">2021</a>); Luu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib23" title="">2022</a>)</cite>or generating hallucinatory content <cite class="ltx_cite ltx_citemacro_cite">Ji etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib14" title="">2023</a>); Shi etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib35" title="">2023</a>); Bang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib2" title="">2023</a>)</cite>. This impacts the dependability of LLMs and limits their broader practical use, as the alignment between LLM outputs and real-world information still requires further validation.
Augmenting LLMs with retrieval mechanisms to fetch relevant information from external sources has emerged as a promising approach to mitigate these issues <cite class="ltx_cite ltx_citemacro_cite">Khandelwal etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib18" title="">2019</a>); Izacard etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib12" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Retrieval-augmented language models (LMs) typically operate using a retrieve-and-generate framework. This process begins by retrieving relevant documents based on the userâ€™s input. Subsequently, the model generates a comprehensive response that is conditioned on the information contained within these retrieved documents. This approach leverages the synergy between information retrieval and natural language generation, enhancing the modelâ€™s ability to provide accurate and contextually relevant answers. <cite class="ltx_cite ltx_citemacro_cite">Chen etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib4" title="">2017</a>); Guu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib9" title="">2020</a>); Lewis etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib22" title="">2021</a>); Izacard and Grave (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib11" title="">2021</a>); Sachan etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib32" title="">2021</a>); Lee etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib21" title="">2022</a>); Jiang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib15" title="">2022</a>); Izacard etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib12" title="">2023</a>); Nakano etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib25" title="">2022</a>); Qian etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib29" title="">2023</a>); Lazaridou etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib20" title="">2022</a>); Shi etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib35" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Standard RAG methods often involve a single retrieval step, which can be insufficient for complex problems requiring multi-step reasoning. <cite class="ltx_cite ltx_citemacro_cite">Yoran etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib41" title="">2024</a>)</cite>.
To address these limitations, various retrieval strategies such as Iterative Retrieval <cite class="ltx_cite ltx_citemacro_cite">Shao etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib34" title="">2023</a>)</cite>, Recursive Retrieval <cite class="ltx_cite ltx_citemacro_cite">Trivedi etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib37" title="">2023</a>); Kim etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib19" title="">2023</a>)</cite>, and Adaptive Retrieval <cite class="ltx_cite ltx_citemacro_cite">Jiang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib16" title="">2023</a>); Asai etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib1" title="">2023</a>); Yang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib39" title="">2023</a>); Schick etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib33" title="">2023</a>); Zhang (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib42" title="">2023</a>)</cite> have been proposed. Among these, adaptive retrieval refines the RAG framework by enabling LLMs to actively determine the optimal moments and content for retrieval, thereby enhancing the efficiency and relevance of the sourced information. For example, Flare automates temporal retrieval by monitoring the confidence levels during the generation process, such as the probability of generated terms <cite class="ltx_cite ltx_citemacro_cite">Jiang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib16" title="">2023</a>)</cite>. When this probability falls below a certain threshold, the retrieval system is activated to gather relevant information, thereby optimizing the retrieval cycle.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">However, another significant challenge with naive RAG is its reliance on the userâ€™s original query for retrieval. Formulating precise and clear queries is difficult, leading to suboptimal retrieval effectiveness. Moreover, language complexity and ambiguity further complicate the process, as models may struggle with specialized vocabulary or ambiguous abbreviations.
To enhance retrieval effectiveness, query optimization strategies such as query expansion and query transformation have been developed. Query expansion enriches the content of the query by breaking down complex questions into simpler sub-queries or creating multiple parallel queries <cite class="ltx_cite ltx_citemacro_cite">Zhou etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib44" title="">2023</a>); Dhuliawala etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib5" title="">2023</a>)</cite>. Query transformation involves rewriting or rephrasing the original query to improve retrieval effectiveness, using techniques like prompt engineering and hypothetical document generation <cite class="ltx_cite ltx_citemacro_cite">Ma etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib24" title="">2023</a>); Peng etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib28" title="">2024</a>); Gao etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib6" title="">2022</a>); Zheng etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib43" title="">2024</a>)</cite>.
These query optimization strategies are crucial for improving the effectiveness of RAG systems, ensuring they provide accurate and contextually appropriate responses.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">While these existing methods are effective in many applications, they tend to focus on either query rewriting or retriever adaptation. Even when both aspects are considered, they are often addressed implicitly during the generation process. Moreover, in adaptive retrieval methods, the LM typically generates a response first and then decides whether additional retrieval is necessary based on the generated output. For instance, Flare automates temporal retrieval by evaluating the confidence in the generated terms <cite class="ltx_cite ltx_citemacro_cite">Jiang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib16" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Given the robust semantic understanding capabilities of large language models (LLMs), we propose a hypothesis: can we assess the necessity of document retrieval before generating a response? This concept is inspired by the behavior of students during open-book exams. Faced with a question, students first understand the question, then evaluate their ability to answer it. If they can, they respond directly; if not, they consult their textbooks to gather the necessary information before crafting their final response. This two-step approach ensures that answers are both accurate and comprehensive. Applying this strategy to LLMs could potentially reduce the costs associated with calling APIs of black-box models, while maintaining or even enhancing response accuracy and relevance.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Building on this concept, this paper introduces <span class="ltx_text ltx_font_italic" id="S1.p7.1.1">Think-then-Act</span>, an accurate and efficient framework for retrieval augmentation, as illustrated in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S0.F1" title="Figure 1 â€£ Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">1</span></a>. This framework incorporates a dual-phase evaluation and response process: (i) assessing the input query to determine if it is clear and complete and if it needs rewriting; (ii) evaluating the language modelâ€™s capability to answer the query and whether additional information retrieval is necessary.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">To validate the effectiveness of our proposed framework, we examine the performance of <span class="ltx_text ltx_font_italic" id="S1.p8.1.1">Think-then-Act</span> with gpt-3.5-turbo across five diverse datasets: HotPotQA <cite class="ltx_cite ltx_citemacro_cite">Yang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib40" title="">2018</a>)</cite>, 2WikiMultihopQA <cite class="ltx_cite ltx_citemacro_cite">Ho etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib10" title="">2020</a>)</cite>, StrategyQA <cite class="ltx_cite ltx_citemacro_cite">Geva etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib8" title="">2021</a>)</cite>, FEVER <cite class="ltx_cite ltx_citemacro_cite">Thorne etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib36" title="">2018</a>)</cite>, and a custom-built Chinese Poetry dataset. These datasets are chosen to comprehensively test various aspects of our approach, including multi-hop reasoning, commonsense reasoning, fact-checking, and domain-specific question answering. Our experimental results demonstrate that the <span class="ltx_text ltx_font_italic" id="S1.p8.1.2">Think-then-Act</span> framework significantly improves retrieval-augmented generationâ€™s performance, achieving higher accuracy and efficiency compared to existing baselines. Notably, the framework shows robust performance in both English and non-English contexts, highlighting its versatility and potential for broader applications.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Our framework involves two modules of RAG: (i) query optimization within the context of RAG; and (ii) adaptive retrieval within the augmentation process of RAG.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Query Optimization</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">A key issue with Naive RAG is its dependence on the userâ€™s initial query <cite class="ltx_cite ltx_citemacro_citep">(Gao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib7" title="">2024</a>)</cite>, often resulting in ineffective retrieval due to challenges in crafting clear questions and managing intricate or ambiguous language.
Query transformation is an effective method for optimizing initial queries, which focuses on retrieving information using a modified query instead of the userâ€™s original query.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Some studies use prompt engineering to enable LLM to generate a query based on the original one for subsequent retrieval <cite class="ltx_cite ltx_citemacro_citep">(Jagerman etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib13" title="">2023</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Gao etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib6" title="">2022</a>)</cite> generates hypothetical documents, which are presumed answers to the initial query. This approach emphasizes the similarity of embeddings between these generated answers rather than focusing on the similarity of embeddings related to the original problem or query. <cite class="ltx_cite ltx_citemacro_citet">Zheng etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib43" title="">2024</a>)</cite> using the Step-back Prompting method abstracts the initial query to formulate a broader, high-level conceptual question (step-back question).
In addition to using LLM for rewriting, <cite class="ltx_cite ltx_citemacro_citet">Ma etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib24" title="">2023</a>)</cite> also specifically trained a smaller model to handle query rewriting tasks.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">These methods enhance retrieval effectiveness; however, they assume that the input query always requires rewriting. Our approach introduces an evaluation step before rewriting, ensuring that the query is only modified if it is deemed incomplete or ambiguous.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Adaptive Retrieval</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">To improve factual accuracy, language models often rely on external knowledge via retrieval augmentation <cite class="ltx_cite ltx_citemacro_citep">(Lewis etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib22" title="">2021</a>)</cite>. Conventional retrieval-augmented generation (RAG) methods use a single retrieval step followed by generation, which can be insufficient for complex, multi-step reasoning tasks. Adaptive retrieval techniques optimize this process by allowing models to dynamically decide when and what to retrieve, enhancing both efficiency and relevance.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">One strategy is to add retrieval capabilities through the fine-tuning of a white-box generation model.
<cite class="ltx_cite ltx_citemacro_citet">Nakano etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib25" title="">2022</a>)</cite> uses a reinforcement learning framework to train the GPT-3 model to autonomously use a search engine during text generation. It employs specific tokens to perform tasks such as making search queries, reviewing search results, and adding references, thus enhancing GPT-3â€™s abilities with the help of external search engines.
<cite class="ltx_cite ltx_citemacro_citet">Asai etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib1" title="">2023</a>)</cite> trained a flexible language model (LM) that can dynamically retrieve passages as required. This model uses special tokens, called â€™reflection tokens,â€™ classified into two types: â€™retrieveâ€™ and â€™critic,â€™ to generate and review both the retrieved passages and its own outputs. By using these reflection tokens, the LM can be directed during the inference phase, allowing it to adapt its behavior to suit various task needs.
Additionally, some researchers use prompt engineering methods. Graph-Toolformer <cite class="ltx_cite ltx_citemacro_citep">(Schick etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib33" title="">2023</a>)</cite>, for instance, separates the retrieval process into specific stages, where LLMs actively use retrievers, utilize Self-Ask techniques, and apply few-shot prompts to start search queries.
Others <cite class="ltx_cite ltx_citemacro_citep">(Jiang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib16" title="">2023</a>)</cite> generates a preliminary answer first, then, based on whether the probability of the generated terms falls below a certain threshold, decides if additional information is needed before regenerating the response based on the initial result.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">The generate-then-retrieve approach, while effective, is inefficient for queries that definitely need retrieval, as it introduces an extra generation step. We propose an approach where the modelâ€™s capabilities are evaluated prior to generation, which achieves a balance between precision and efficiency in situations where absolute accuracy is not required.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We present a dual-angle evaluated retrieval-augmented generation framework <span class="ltx_text ltx_font_italic" id="S3.p1.1.1">Think-then-Act</span>. This approach enhances both the query assessment and model capability evaluation processes. FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S0.F1" title="Figure 1 â€£ Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">1</span></a> provides an overview. This section first introduces the query assessment and rewriting process in SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S3.SS1" title="3.1 Initial Query Assessment â€£ 3 Methodology â€£ Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">3.1</span></a>, followed by the model capability check and information retrieval in SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S3.SS2" title="3.2 Model Capability Check â€£ 3 Methodology â€£ Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">3.2</span></a>.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Initial Query Assessment</h3>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Evaluation</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">Accurate responses require clear and precise questions. Therefore, our first step involves evaluating the input query to determine whether it is clear and complete, incomplete, or ambiguous. Leveraging the inherent semantic understanding capabilities of large language models, we avoid the need for additional models for this evaluation. Instead, we use a prompting method that enables the model to self-assess the clarity and completeness of the input query, and the model categorizes the query as CLEAR AND COMPLETE, INCOMPLETE, or AMBIGUOUS.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Rewriting</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">If the evaluation categorizes the query as INCOMPLETE or AMBIGUOUS, the query requires rewriting. Utilizing the powerful generation capabilities of large language models, we employ a prompting method that enables the model to generate the revised queries itself.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS2.p2">
<p class="ltx_p" id="S3.SS1.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS2.p2.1.1">INCOMPELTE</span> The model generates a more complete version of the query by filling in any missing information, ensuring it is clear and comprehensive.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS2.p3">
<p class="ltx_p" id="S3.SS1.SSS2.p3.2"><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS2.p3.2.1">AMBIGUOUS</span> The model resolves ambiguity by breaking down the query into multiple, straightforward sub-queries, each addressing a specific aspect of the original query.
Formally, the overall process of initial query <math alttext="q_{init}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p3.1.m1.1"><semantics id="S3.SS1.SSS2.p3.1.m1.1a"><msub id="S3.SS1.SSS2.p3.1.m1.1.1" xref="S3.SS1.SSS2.p3.1.m1.1.1.cmml"><mi id="S3.SS1.SSS2.p3.1.m1.1.1.2" xref="S3.SS1.SSS2.p3.1.m1.1.1.2.cmml">q</mi><mrow id="S3.SS1.SSS2.p3.1.m1.1.1.3" xref="S3.SS1.SSS2.p3.1.m1.1.1.3.cmml"><mi id="S3.SS1.SSS2.p3.1.m1.1.1.3.2" xref="S3.SS1.SSS2.p3.1.m1.1.1.3.2.cmml">i</mi><mo id="S3.SS1.SSS2.p3.1.m1.1.1.3.1" xref="S3.SS1.SSS2.p3.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS2.p3.1.m1.1.1.3.3" xref="S3.SS1.SSS2.p3.1.m1.1.1.3.3.cmml">n</mi><mo id="S3.SS1.SSS2.p3.1.m1.1.1.3.1a" xref="S3.SS1.SSS2.p3.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS2.p3.1.m1.1.1.3.4" xref="S3.SS1.SSS2.p3.1.m1.1.1.3.4.cmml">i</mi><mo id="S3.SS1.SSS2.p3.1.m1.1.1.3.1b" xref="S3.SS1.SSS2.p3.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS2.p3.1.m1.1.1.3.5" xref="S3.SS1.SSS2.p3.1.m1.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p3.1.m1.1b"><apply id="S3.SS1.SSS2.p3.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p3.1.m1.1.1.1.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p3.1.m1.1.1.2.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.1.2">ğ‘</ci><apply id="S3.SS1.SSS2.p3.1.m1.1.1.3.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.1.3"><times id="S3.SS1.SSS2.p3.1.m1.1.1.3.1.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.1.3.1"></times><ci id="S3.SS1.SSS2.p3.1.m1.1.1.3.2.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.1.3.2">ğ‘–</ci><ci id="S3.SS1.SSS2.p3.1.m1.1.1.3.3.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.1.3.3">ğ‘›</ci><ci id="S3.SS1.SSS2.p3.1.m1.1.1.3.4.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.1.3.4">ğ‘–</ci><ci id="S3.SS1.SSS2.p3.1.m1.1.1.3.5.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.1.3.5">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p3.1.m1.1c">q_{init}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p3.1.m1.1d">italic_q start_POSTSUBSCRIPT italic_i italic_n italic_i italic_t end_POSTSUBSCRIPT</annotation></semantics></math> assessment and rewriting, resulting in the modelâ€™s final input <math alttext="q_{final}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p3.2.m2.1"><semantics id="S3.SS1.SSS2.p3.2.m2.1a"><msub id="S3.SS1.SSS2.p3.2.m2.1.1" xref="S3.SS1.SSS2.p3.2.m2.1.1.cmml"><mi id="S3.SS1.SSS2.p3.2.m2.1.1.2" xref="S3.SS1.SSS2.p3.2.m2.1.1.2.cmml">q</mi><mrow id="S3.SS1.SSS2.p3.2.m2.1.1.3" xref="S3.SS1.SSS2.p3.2.m2.1.1.3.cmml"><mi id="S3.SS1.SSS2.p3.2.m2.1.1.3.2" xref="S3.SS1.SSS2.p3.2.m2.1.1.3.2.cmml">f</mi><mo id="S3.SS1.SSS2.p3.2.m2.1.1.3.1" xref="S3.SS1.SSS2.p3.2.m2.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS2.p3.2.m2.1.1.3.3" xref="S3.SS1.SSS2.p3.2.m2.1.1.3.3.cmml">i</mi><mo id="S3.SS1.SSS2.p3.2.m2.1.1.3.1a" xref="S3.SS1.SSS2.p3.2.m2.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS2.p3.2.m2.1.1.3.4" xref="S3.SS1.SSS2.p3.2.m2.1.1.3.4.cmml">n</mi><mo id="S3.SS1.SSS2.p3.2.m2.1.1.3.1b" xref="S3.SS1.SSS2.p3.2.m2.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS2.p3.2.m2.1.1.3.5" xref="S3.SS1.SSS2.p3.2.m2.1.1.3.5.cmml">a</mi><mo id="S3.SS1.SSS2.p3.2.m2.1.1.3.1c" xref="S3.SS1.SSS2.p3.2.m2.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.SSS2.p3.2.m2.1.1.3.6" xref="S3.SS1.SSS2.p3.2.m2.1.1.3.6.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p3.2.m2.1b"><apply id="S3.SS1.SSS2.p3.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p3.2.m2.1.1.1.cmml" xref="S3.SS1.SSS2.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p3.2.m2.1.1.2.cmml" xref="S3.SS1.SSS2.p3.2.m2.1.1.2">ğ‘</ci><apply id="S3.SS1.SSS2.p3.2.m2.1.1.3.cmml" xref="S3.SS1.SSS2.p3.2.m2.1.1.3"><times id="S3.SS1.SSS2.p3.2.m2.1.1.3.1.cmml" xref="S3.SS1.SSS2.p3.2.m2.1.1.3.1"></times><ci id="S3.SS1.SSS2.p3.2.m2.1.1.3.2.cmml" xref="S3.SS1.SSS2.p3.2.m2.1.1.3.2">ğ‘“</ci><ci id="S3.SS1.SSS2.p3.2.m2.1.1.3.3.cmml" xref="S3.SS1.SSS2.p3.2.m2.1.1.3.3">ğ‘–</ci><ci id="S3.SS1.SSS2.p3.2.m2.1.1.3.4.cmml" xref="S3.SS1.SSS2.p3.2.m2.1.1.3.4">ğ‘›</ci><ci id="S3.SS1.SSS2.p3.2.m2.1.1.3.5.cmml" xref="S3.SS1.SSS2.p3.2.m2.1.1.3.5">ğ‘</ci><ci id="S3.SS1.SSS2.p3.2.m2.1.1.3.6.cmml" xref="S3.SS1.SSS2.p3.2.m2.1.1.3.6">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p3.2.m2.1c">q_{final}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p3.2.m2.1d">italic_q start_POSTSUBSCRIPT italic_f italic_i italic_n italic_a italic_l end_POSTSUBSCRIPT</annotation></semantics></math>, can be summarized as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="q_{\text{final}}=\left\{\begin{array}[]{lr}q_{\text{init}}&amp;\quad\text{if}\;%
\mbox{CLEAR AND COMPLETE}\\
q_{\text{comp}}&amp;\quad\text{if}\;\mbox{INCOMPLETE}\\
q_{\text{group}}=&amp;\{q^{1},q^{2},\ldots,q^{k}\}\quad\text{if}\;\mbox{AMBIGUOUS}%
\end{array}\right." class="ltx_Math" display="block" id="S3.E1.m1.3"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.4" xref="S3.E1.m1.3.4.cmml"><msub id="S3.E1.m1.3.4.2" xref="S3.E1.m1.3.4.2.cmml"><mi id="S3.E1.m1.3.4.2.2" xref="S3.E1.m1.3.4.2.2.cmml">q</mi><mtext id="S3.E1.m1.3.4.2.3" xref="S3.E1.m1.3.4.2.3a.cmml">final</mtext></msub><mo id="S3.E1.m1.3.4.1" xref="S3.E1.m1.3.4.1.cmml">=</mo><mrow id="S3.E1.m1.3.4.3.2" xref="S3.E1.m1.3.4.3.1.cmml"><mo id="S3.E1.m1.3.4.3.2.1" xref="S3.E1.m1.3.4.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" id="S3.E1.m1.3.3" rowspacing="0pt" xref="S3.E1.m1.3.3.cmml"><mtr id="S3.E1.m1.3.3a" xref="S3.E1.m1.3.3.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.3.3b" xref="S3.E1.m1.3.3.cmml"><msub id="S3.E1.m1.3.3.4.1.1" xref="S3.E1.m1.3.3.4.1.1.cmml"><mi id="S3.E1.m1.3.3.4.1.1.2" xref="S3.E1.m1.3.3.4.1.1.2.cmml">q</mi><mtext id="S3.E1.m1.3.3.4.1.1.3" xref="S3.E1.m1.3.3.4.1.1.3a.cmml">init</mtext></msub></mtd><mtd class="ltx_align_right" columnalign="right" id="S3.E1.m1.3.3c" xref="S3.E1.m1.3.3.cmml"><mrow id="S3.E1.m1.3.3.4.2.1" xref="S3.E1.m1.3.3.4.2.1.cmml"><mtext id="S3.E1.m1.3.3.4.2.1.2" xref="S3.E1.m1.3.3.4.2.1.2a.cmml">if</mtext><mo id="S3.E1.m1.3.3.4.2.1.1" lspace="0.280em" xref="S3.E1.m1.3.3.4.2.1.1.cmml">â¢</mo><mtext id="S3.E1.m1.3.3.4.2.1.3" xref="S3.E1.m1.3.3.4.2.1.3a.cmml">CLEAR AND COMPLETE</mtext></mrow></mtd></mtr><mtr id="S3.E1.m1.3.3d" xref="S3.E1.m1.3.3.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.3.3e" xref="S3.E1.m1.3.3.cmml"><msub id="S3.E1.m1.3.3.5.1.1" xref="S3.E1.m1.3.3.5.1.1.cmml"><mi id="S3.E1.m1.3.3.5.1.1.2" xref="S3.E1.m1.3.3.5.1.1.2.cmml">q</mi><mtext id="S3.E1.m1.3.3.5.1.1.3" xref="S3.E1.m1.3.3.5.1.1.3a.cmml">comp</mtext></msub></mtd><mtd class="ltx_align_right" columnalign="right" id="S3.E1.m1.3.3f" xref="S3.E1.m1.3.3.cmml"><mrow id="S3.E1.m1.3.3.5.2.1" xref="S3.E1.m1.3.3.5.2.1.cmml"><mtext id="S3.E1.m1.3.3.5.2.1.2" xref="S3.E1.m1.3.3.5.2.1.2a.cmml">if</mtext><mo id="S3.E1.m1.3.3.5.2.1.1" lspace="0.280em" xref="S3.E1.m1.3.3.5.2.1.1.cmml">â¢</mo><mtext id="S3.E1.m1.3.3.5.2.1.3" xref="S3.E1.m1.3.3.5.2.1.3a.cmml">INCOMPLETE</mtext></mrow></mtd></mtr><mtr id="S3.E1.m1.3.3g" xref="S3.E1.m1.3.3.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.3.3h" xref="S3.E1.m1.3.3.cmml"><mrow id="S3.E1.m1.3.3.3.4.1" xref="S3.E1.m1.3.3.3.4.1.cmml"><msub id="S3.E1.m1.3.3.3.4.1.2" xref="S3.E1.m1.3.3.3.4.1.2.cmml"><mi id="S3.E1.m1.3.3.3.4.1.2.2" xref="S3.E1.m1.3.3.3.4.1.2.2.cmml">q</mi><mtext id="S3.E1.m1.3.3.3.4.1.2.3" xref="S3.E1.m1.3.3.3.4.1.2.3a.cmml">group</mtext></msub><mo id="S3.E1.m1.3.3.3.4.1.1" xref="S3.E1.m1.3.3.3.4.1.1.cmml">=</mo><mi id="S3.E1.m1.3.3.3.4.1.3" xref="S3.E1.m1.3.3.3.4.1.3.cmml"></mi></mrow></mtd><mtd class="ltx_align_right" columnalign="right" id="S3.E1.m1.3.3i" xref="S3.E1.m1.3.3.cmml"><mrow id="S3.E1.m1.3.3.3.3.3.3" xref="S3.E1.m1.3.3.3.3.3.4.cmml"><mrow id="S3.E1.m1.2.2.2.2.2.2.1.3" xref="S3.E1.m1.2.2.2.2.2.2.1.4.cmml"><mo id="S3.E1.m1.2.2.2.2.2.2.1.3.4" stretchy="false" xref="S3.E1.m1.2.2.2.2.2.2.1.4.cmml">{</mo><msup id="S3.E1.m1.2.2.2.2.2.2.1.1.1" xref="S3.E1.m1.2.2.2.2.2.2.1.1.1.cmml"><mi id="S3.E1.m1.2.2.2.2.2.2.1.1.1.2" xref="S3.E1.m1.2.2.2.2.2.2.1.1.1.2.cmml">q</mi><mn id="S3.E1.m1.2.2.2.2.2.2.1.1.1.3" xref="S3.E1.m1.2.2.2.2.2.2.1.1.1.3.cmml">1</mn></msup><mo id="S3.E1.m1.2.2.2.2.2.2.1.3.5" xref="S3.E1.m1.2.2.2.2.2.2.1.4.cmml">,</mo><msup id="S3.E1.m1.2.2.2.2.2.2.1.2.2" xref="S3.E1.m1.2.2.2.2.2.2.1.2.2.cmml"><mi id="S3.E1.m1.2.2.2.2.2.2.1.2.2.2" xref="S3.E1.m1.2.2.2.2.2.2.1.2.2.2.cmml">q</mi><mn id="S3.E1.m1.2.2.2.2.2.2.1.2.2.3" xref="S3.E1.m1.2.2.2.2.2.2.1.2.2.3.cmml">2</mn></msup><mo id="S3.E1.m1.2.2.2.2.2.2.1.3.6" xref="S3.E1.m1.2.2.2.2.2.2.1.4.cmml">,</mo><mi id="S3.E1.m1.1.1.1.1.1.1" mathvariant="normal" xref="S3.E1.m1.1.1.1.1.1.1.cmml">â€¦</mi><mo id="S3.E1.m1.2.2.2.2.2.2.1.3.7" xref="S3.E1.m1.2.2.2.2.2.2.1.4.cmml">,</mo><msup id="S3.E1.m1.2.2.2.2.2.2.1.3.3" xref="S3.E1.m1.2.2.2.2.2.2.1.3.3.cmml"><mi id="S3.E1.m1.2.2.2.2.2.2.1.3.3.2" xref="S3.E1.m1.2.2.2.2.2.2.1.3.3.2.cmml">q</mi><mi id="S3.E1.m1.2.2.2.2.2.2.1.3.3.3" xref="S3.E1.m1.2.2.2.2.2.2.1.3.3.3.cmml">k</mi></msup><mo id="S3.E1.m1.2.2.2.2.2.2.1.3.8" stretchy="false" xref="S3.E1.m1.2.2.2.2.2.2.1.4.cmml">}</mo></mrow><mspace id="S3.E1.m1.3.3.3.3.3.3.3" width="1em" xref="S3.E1.m1.3.3.3.3.3.4.cmml"></mspace><mrow id="S3.E1.m1.3.3.3.3.3.3.2" xref="S3.E1.m1.3.3.3.3.3.3.2.cmml"><mtext id="S3.E1.m1.3.3.3.3.3.3.2.2" xref="S3.E1.m1.3.3.3.3.3.3.2.2a.cmml">if</mtext><mo id="S3.E1.m1.3.3.3.3.3.3.2.1" lspace="0.280em" xref="S3.E1.m1.3.3.3.3.3.3.2.1.cmml">â¢</mo><mtext id="S3.E1.m1.3.3.3.3.3.3.2.3" xref="S3.E1.m1.3.3.3.3.3.3.2.3a.cmml">AMBIGUOUS</mtext></mrow></mrow></mtd></mtr></mtable><mi id="S3.E1.m1.3.4.3.2.2" xref="S3.E1.m1.3.4.3.1.1.cmml"></mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.4.cmml" xref="S3.E1.m1.3.4"><eq id="S3.E1.m1.3.4.1.cmml" xref="S3.E1.m1.3.4.1"></eq><apply id="S3.E1.m1.3.4.2.cmml" xref="S3.E1.m1.3.4.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.4.2.1.cmml" xref="S3.E1.m1.3.4.2">subscript</csymbol><ci id="S3.E1.m1.3.4.2.2.cmml" xref="S3.E1.m1.3.4.2.2">ğ‘</ci><ci id="S3.E1.m1.3.4.2.3a.cmml" xref="S3.E1.m1.3.4.2.3"><mtext id="S3.E1.m1.3.4.2.3.cmml" mathsize="70%" xref="S3.E1.m1.3.4.2.3">final</mtext></ci></apply><apply id="S3.E1.m1.3.4.3.1.cmml" xref="S3.E1.m1.3.4.3.2"><csymbol cd="latexml" id="S3.E1.m1.3.4.3.1.1.cmml" xref="S3.E1.m1.3.4.3.2.1">cases</csymbol><matrix id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3"><matrixrow id="S3.E1.m1.3.3a.cmml" xref="S3.E1.m1.3.3"><apply id="S3.E1.m1.3.3.4.1.1.cmml" xref="S3.E1.m1.3.3.4.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.4.1.1.1.cmml" xref="S3.E1.m1.3.3.4.1.1">subscript</csymbol><ci id="S3.E1.m1.3.3.4.1.1.2.cmml" xref="S3.E1.m1.3.3.4.1.1.2">ğ‘</ci><ci id="S3.E1.m1.3.3.4.1.1.3a.cmml" xref="S3.E1.m1.3.3.4.1.1.3"><mtext id="S3.E1.m1.3.3.4.1.1.3.cmml" mathsize="70%" xref="S3.E1.m1.3.3.4.1.1.3">init</mtext></ci></apply><apply id="S3.E1.m1.3.3.4.2.1.cmml" xref="S3.E1.m1.3.3.4.2.1"><times id="S3.E1.m1.3.3.4.2.1.1.cmml" xref="S3.E1.m1.3.3.4.2.1.1"></times><ci id="S3.E1.m1.3.3.4.2.1.2a.cmml" xref="S3.E1.m1.3.3.4.2.1.2"><mtext id="S3.E1.m1.3.3.4.2.1.2.cmml" xref="S3.E1.m1.3.3.4.2.1.2">if</mtext></ci><ci id="S3.E1.m1.3.3.4.2.1.3a.cmml" xref="S3.E1.m1.3.3.4.2.1.3"><mtext id="S3.E1.m1.3.3.4.2.1.3.cmml" xref="S3.E1.m1.3.3.4.2.1.3">CLEAR AND COMPLETE</mtext></ci></apply></matrixrow><matrixrow id="S3.E1.m1.3.3b.cmml" xref="S3.E1.m1.3.3"><apply id="S3.E1.m1.3.3.5.1.1.cmml" xref="S3.E1.m1.3.3.5.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.5.1.1.1.cmml" xref="S3.E1.m1.3.3.5.1.1">subscript</csymbol><ci id="S3.E1.m1.3.3.5.1.1.2.cmml" xref="S3.E1.m1.3.3.5.1.1.2">ğ‘</ci><ci id="S3.E1.m1.3.3.5.1.1.3a.cmml" xref="S3.E1.m1.3.3.5.1.1.3"><mtext id="S3.E1.m1.3.3.5.1.1.3.cmml" mathsize="70%" xref="S3.E1.m1.3.3.5.1.1.3">comp</mtext></ci></apply><apply id="S3.E1.m1.3.3.5.2.1.cmml" xref="S3.E1.m1.3.3.5.2.1"><times id="S3.E1.m1.3.3.5.2.1.1.cmml" xref="S3.E1.m1.3.3.5.2.1.1"></times><ci id="S3.E1.m1.3.3.5.2.1.2a.cmml" xref="S3.E1.m1.3.3.5.2.1.2"><mtext id="S3.E1.m1.3.3.5.2.1.2.cmml" xref="S3.E1.m1.3.3.5.2.1.2">if</mtext></ci><ci id="S3.E1.m1.3.3.5.2.1.3a.cmml" xref="S3.E1.m1.3.3.5.2.1.3"><mtext id="S3.E1.m1.3.3.5.2.1.3.cmml" xref="S3.E1.m1.3.3.5.2.1.3">INCOMPLETE</mtext></ci></apply></matrixrow><matrixrow id="S3.E1.m1.3.3c.cmml" xref="S3.E1.m1.3.3"><apply id="S3.E1.m1.3.3.3.4.1.cmml" xref="S3.E1.m1.3.3.3.4.1"><eq id="S3.E1.m1.3.3.3.4.1.1.cmml" xref="S3.E1.m1.3.3.3.4.1.1"></eq><apply id="S3.E1.m1.3.3.3.4.1.2.cmml" xref="S3.E1.m1.3.3.3.4.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.3.4.1.2.1.cmml" xref="S3.E1.m1.3.3.3.4.1.2">subscript</csymbol><ci id="S3.E1.m1.3.3.3.4.1.2.2.cmml" xref="S3.E1.m1.3.3.3.4.1.2.2">ğ‘</ci><ci id="S3.E1.m1.3.3.3.4.1.2.3a.cmml" xref="S3.E1.m1.3.3.3.4.1.2.3"><mtext id="S3.E1.m1.3.3.3.4.1.2.3.cmml" mathsize="70%" xref="S3.E1.m1.3.3.3.4.1.2.3">group</mtext></ci></apply><csymbol cd="latexml" id="S3.E1.m1.3.3.3.4.1.3.cmml" xref="S3.E1.m1.3.3.3.4.1.3">absent</csymbol></apply><list id="S3.E1.m1.3.3.3.3.3.4.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><set id="S3.E1.m1.2.2.2.2.2.2.1.4.cmml" xref="S3.E1.m1.2.2.2.2.2.2.1.3"><apply id="S3.E1.m1.2.2.2.2.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.2.2.1.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.2.2.1.1.1">superscript</csymbol><ci id="S3.E1.m1.2.2.2.2.2.2.1.1.1.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2.1.1.1.2">ğ‘</ci><cn id="S3.E1.m1.2.2.2.2.2.2.1.1.1.3.cmml" type="integer" xref="S3.E1.m1.2.2.2.2.2.2.1.1.1.3">1</cn></apply><apply id="S3.E1.m1.2.2.2.2.2.2.1.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2.1.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.2.2.1.2.2.1.cmml" xref="S3.E1.m1.2.2.2.2.2.2.1.2.2">superscript</csymbol><ci id="S3.E1.m1.2.2.2.2.2.2.1.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2.1.2.2.2">ğ‘</ci><cn id="S3.E1.m1.2.2.2.2.2.2.1.2.2.3.cmml" type="integer" xref="S3.E1.m1.2.2.2.2.2.2.1.2.2.3">2</cn></apply><ci id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1">â€¦</ci><apply id="S3.E1.m1.2.2.2.2.2.2.1.3.3.cmml" xref="S3.E1.m1.2.2.2.2.2.2.1.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.2.2.1.3.3.1.cmml" xref="S3.E1.m1.2.2.2.2.2.2.1.3.3">superscript</csymbol><ci id="S3.E1.m1.2.2.2.2.2.2.1.3.3.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2.1.3.3.2">ğ‘</ci><ci id="S3.E1.m1.2.2.2.2.2.2.1.3.3.3.cmml" xref="S3.E1.m1.2.2.2.2.2.2.1.3.3.3">ğ‘˜</ci></apply></set><apply id="S3.E1.m1.3.3.3.3.3.3.2.cmml" xref="S3.E1.m1.3.3.3.3.3.3.2"><times id="S3.E1.m1.3.3.3.3.3.3.2.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3.2.1"></times><ci id="S3.E1.m1.3.3.3.3.3.3.2.2a.cmml" xref="S3.E1.m1.3.3.3.3.3.3.2.2"><mtext id="S3.E1.m1.3.3.3.3.3.3.2.2.cmml" xref="S3.E1.m1.3.3.3.3.3.3.2.2">if</mtext></ci><ci id="S3.E1.m1.3.3.3.3.3.3.2.3a.cmml" xref="S3.E1.m1.3.3.3.3.3.3.2.3"><mtext id="S3.E1.m1.3.3.3.3.3.3.2.3.cmml" xref="S3.E1.m1.3.3.3.3.3.3.2.3">AMBIGUOUS</mtext></ci></apply></list></matrixrow></matrix></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">q_{\text{final}}=\left\{\begin{array}[]{lr}q_{\text{init}}&amp;\quad\text{if}\;%
\mbox{CLEAR AND COMPLETE}\\
q_{\text{comp}}&amp;\quad\text{if}\;\mbox{INCOMPLETE}\\
q_{\text{group}}=&amp;\{q^{1},q^{2},\ldots,q^{k}\}\quad\text{if}\;\mbox{AMBIGUOUS}%
\end{array}\right.</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.3d">italic_q start_POSTSUBSCRIPT final end_POSTSUBSCRIPT = { start_ARRAY start_ROW start_CELL italic_q start_POSTSUBSCRIPT init end_POSTSUBSCRIPT end_CELL start_CELL if CLEAR AND COMPLETE end_CELL end_ROW start_ROW start_CELL italic_q start_POSTSUBSCRIPT comp end_POSTSUBSCRIPT end_CELL start_CELL if INCOMPLETE end_CELL end_ROW start_ROW start_CELL italic_q start_POSTSUBSCRIPT group end_POSTSUBSCRIPT = end_CELL start_CELL { italic_q start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , italic_q start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , â€¦ , italic_q start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT } if AMBIGUOUS end_CELL end_ROW end_ARRAY</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Model Capability Check</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.2">After completing the initial query assessment and obtaining the final input <math alttext="q_{final}" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><msub id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">q</mi><mrow id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.p1.1.m1.1.1.3.2" xref="S3.SS2.p1.1.m1.1.1.3.2.cmml">f</mi><mo id="S3.SS2.p1.1.m1.1.1.3.1" xref="S3.SS2.p1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.SS2.p1.1.m1.1.1.3.3" xref="S3.SS2.p1.1.m1.1.1.3.3.cmml">i</mi><mo id="S3.SS2.p1.1.m1.1.1.3.1a" xref="S3.SS2.p1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.SS2.p1.1.m1.1.1.3.4" xref="S3.SS2.p1.1.m1.1.1.3.4.cmml">n</mi><mo id="S3.SS2.p1.1.m1.1.1.3.1b" xref="S3.SS2.p1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.SS2.p1.1.m1.1.1.3.5" xref="S3.SS2.p1.1.m1.1.1.3.5.cmml">a</mi><mo id="S3.SS2.p1.1.m1.1.1.3.1c" xref="S3.SS2.p1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.SS2.p1.1.m1.1.1.3.6" xref="S3.SS2.p1.1.m1.1.1.3.6.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">ğ‘</ci><apply id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3"><times id="S3.SS2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.p1.1.m1.1.1.3.1"></times><ci id="S3.SS2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.p1.1.m1.1.1.3.2">ğ‘“</ci><ci id="S3.SS2.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3">ğ‘–</ci><ci id="S3.SS2.p1.1.m1.1.1.3.4.cmml" xref="S3.SS2.p1.1.m1.1.1.3.4">ğ‘›</ci><ci id="S3.SS2.p1.1.m1.1.1.3.5.cmml" xref="S3.SS2.p1.1.m1.1.1.3.5">ğ‘</ci><ci id="S3.SS2.p1.1.m1.1.1.3.6.cmml" xref="S3.SS2.p1.1.m1.1.1.3.6">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">q_{final}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_q start_POSTSUBSCRIPT italic_f italic_i italic_n italic_a italic_l end_POSTSUBSCRIPT</annotation></semantics></math>, the next step involves evaluating the LMâ€™s capability to answer <math alttext="q_{final}" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><msub id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">q</mi><mrow id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml"><mi id="S3.SS2.p1.2.m2.1.1.3.2" xref="S3.SS2.p1.2.m2.1.1.3.2.cmml">f</mi><mo id="S3.SS2.p1.2.m2.1.1.3.1" xref="S3.SS2.p1.2.m2.1.1.3.1.cmml">â¢</mo><mi id="S3.SS2.p1.2.m2.1.1.3.3" xref="S3.SS2.p1.2.m2.1.1.3.3.cmml">i</mi><mo id="S3.SS2.p1.2.m2.1.1.3.1a" xref="S3.SS2.p1.2.m2.1.1.3.1.cmml">â¢</mo><mi id="S3.SS2.p1.2.m2.1.1.3.4" xref="S3.SS2.p1.2.m2.1.1.3.4.cmml">n</mi><mo id="S3.SS2.p1.2.m2.1.1.3.1b" xref="S3.SS2.p1.2.m2.1.1.3.1.cmml">â¢</mo><mi id="S3.SS2.p1.2.m2.1.1.3.5" xref="S3.SS2.p1.2.m2.1.1.3.5.cmml">a</mi><mo id="S3.SS2.p1.2.m2.1.1.3.1c" xref="S3.SS2.p1.2.m2.1.1.3.1.cmml">â¢</mo><mi id="S3.SS2.p1.2.m2.1.1.3.6" xref="S3.SS2.p1.2.m2.1.1.3.6.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">ğ‘</ci><apply id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3"><times id="S3.SS2.p1.2.m2.1.1.3.1.cmml" xref="S3.SS2.p1.2.m2.1.1.3.1"></times><ci id="S3.SS2.p1.2.m2.1.1.3.2.cmml" xref="S3.SS2.p1.2.m2.1.1.3.2">ğ‘“</ci><ci id="S3.SS2.p1.2.m2.1.1.3.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3">ğ‘–</ci><ci id="S3.SS2.p1.2.m2.1.1.3.4.cmml" xref="S3.SS2.p1.2.m2.1.1.3.4">ğ‘›</ci><ci id="S3.SS2.p1.2.m2.1.1.3.5.cmml" xref="S3.SS2.p1.2.m2.1.1.3.5">ğ‘</ci><ci id="S3.SS2.p1.2.m2.1.1.3.6.cmml" xref="S3.SS2.p1.2.m2.1.1.3.6">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">q_{final}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">italic_q start_POSTSUBSCRIPT italic_f italic_i italic_n italic_a italic_l end_POSTSUBSCRIPT</annotation></semantics></math>. We propose two methods for this evaluation:</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">Direct Decision:</span> In this straightforward approach, the LM directly outputs either RETRIEVAL or NO RETRIEVAL. This binary decision indicates whether the LM needs additional information to answer the query effectively.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.8"><span class="ltx_text ltx_font_bold" id="S3.SS2.p3.8.1">Confidence Score:</span> This method involves the LM generating a confidence score, denoted as <math alttext="\beta" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.1"><semantics id="S3.SS2.p3.1.m1.1a"><mi id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">Î²</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><ci id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">ğ›½</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.1d">italic_Î²</annotation></semantics></math>, which represents its confidence level in answering the question. By comparing this score to a predefined threshold <math alttext="\beta^{\prime}" class="ltx_Math" display="inline" id="S3.SS2.p3.2.m2.1"><semantics id="S3.SS2.p3.2.m2.1a"><msup id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml"><mi id="S3.SS2.p3.2.m2.1.1.2" xref="S3.SS2.p3.2.m2.1.1.2.cmml">Î²</mi><mo id="S3.SS2.p3.2.m2.1.1.3" xref="S3.SS2.p3.2.m2.1.1.3.cmml">â€²</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">superscript</csymbol><ci id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2">ğ›½</ci><ci id="S3.SS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">\beta^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.2.m2.1d">italic_Î² start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT</annotation></semantics></math>, we can dynamically decide whether retrieval is necessary. <math alttext="\beta&lt;\beta^{\prime}" class="ltx_Math" display="inline" id="S3.SS2.p3.3.m3.1"><semantics id="S3.SS2.p3.3.m3.1a"><mrow id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml"><mi id="S3.SS2.p3.3.m3.1.1.2" xref="S3.SS2.p3.3.m3.1.1.2.cmml">Î²</mi><mo id="S3.SS2.p3.3.m3.1.1.1" xref="S3.SS2.p3.3.m3.1.1.1.cmml">&lt;</mo><msup id="S3.SS2.p3.3.m3.1.1.3" xref="S3.SS2.p3.3.m3.1.1.3.cmml"><mi id="S3.SS2.p3.3.m3.1.1.3.2" xref="S3.SS2.p3.3.m3.1.1.3.2.cmml">Î²</mi><mo id="S3.SS2.p3.3.m3.1.1.3.3" xref="S3.SS2.p3.3.m3.1.1.3.3.cmml">â€²</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><apply id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1"><lt id="S3.SS2.p3.3.m3.1.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1.1"></lt><ci id="S3.SS2.p3.3.m3.1.1.2.cmml" xref="S3.SS2.p3.3.m3.1.1.2">ğ›½</ci><apply id="S3.SS2.p3.3.m3.1.1.3.cmml" xref="S3.SS2.p3.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m3.1.1.3.1.cmml" xref="S3.SS2.p3.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS2.p3.3.m3.1.1.3.2.cmml" xref="S3.SS2.p3.3.m3.1.1.3.2">ğ›½</ci><ci id="S3.SS2.p3.3.m3.1.1.3.3.cmml" xref="S3.SS2.p3.3.m3.1.1.3.3">â€²</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">\beta&lt;\beta^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.3.m3.1d">italic_Î² &lt; italic_Î² start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT</annotation></semantics></math>, indicating that the model lacks sufficient confidence to answer the query on its own, so retrieval is required. <math alttext="\beta&gt;=\beta^{\prime}" class="ltx_Math" display="inline" id="S3.SS2.p3.4.m4.1"><semantics id="S3.SS2.p3.4.m4.1a"><mrow id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml"><mi id="S3.SS2.p3.4.m4.1.1.2" xref="S3.SS2.p3.4.m4.1.1.2.cmml">Î²</mi><mo id="S3.SS2.p3.4.m4.1.1.1" lspace="0.278em" rspace="0.278em" xref="S3.SS2.p3.4.m4.1.1.1.cmml">&gt;=</mo><msup id="S3.SS2.p3.4.m4.1.1.3" xref="S3.SS2.p3.4.m4.1.1.3.cmml"><mi id="S3.SS2.p3.4.m4.1.1.3.2" xref="S3.SS2.p3.4.m4.1.1.3.2.cmml">Î²</mi><mo id="S3.SS2.p3.4.m4.1.1.3.3" xref="S3.SS2.p3.4.m4.1.1.3.3.cmml">â€²</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b"><apply id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1"><geq id="S3.SS2.p3.4.m4.1.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1.1"></geq><ci id="S3.SS2.p3.4.m4.1.1.2.cmml" xref="S3.SS2.p3.4.m4.1.1.2">ğ›½</ci><apply id="S3.SS2.p3.4.m4.1.1.3.cmml" xref="S3.SS2.p3.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p3.4.m4.1.1.3.1.cmml" xref="S3.SS2.p3.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS2.p3.4.m4.1.1.3.2.cmml" xref="S3.SS2.p3.4.m4.1.1.3.2">ğ›½</ci><ci id="S3.SS2.p3.4.m4.1.1.3.3.cmml" xref="S3.SS2.p3.4.m4.1.1.3.3">â€²</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">\beta&gt;=\beta^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.4.m4.1d">italic_Î² &gt; = italic_Î² start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT</annotation></semantics></math>, suggesting that the model is confident in its ability to provide an accurate response without additional information, so retrieval is not needed. <math alttext="\beta^{\prime}\in[0,1]" class="ltx_Math" display="inline" id="S3.SS2.p3.5.m5.2"><semantics id="S3.SS2.p3.5.m5.2a"><mrow id="S3.SS2.p3.5.m5.2.3" xref="S3.SS2.p3.5.m5.2.3.cmml"><msup id="S3.SS2.p3.5.m5.2.3.2" xref="S3.SS2.p3.5.m5.2.3.2.cmml"><mi id="S3.SS2.p3.5.m5.2.3.2.2" xref="S3.SS2.p3.5.m5.2.3.2.2.cmml">Î²</mi><mo id="S3.SS2.p3.5.m5.2.3.2.3" xref="S3.SS2.p3.5.m5.2.3.2.3.cmml">â€²</mo></msup><mo id="S3.SS2.p3.5.m5.2.3.1" xref="S3.SS2.p3.5.m5.2.3.1.cmml">âˆˆ</mo><mrow id="S3.SS2.p3.5.m5.2.3.3.2" xref="S3.SS2.p3.5.m5.2.3.3.1.cmml"><mo id="S3.SS2.p3.5.m5.2.3.3.2.1" stretchy="false" xref="S3.SS2.p3.5.m5.2.3.3.1.cmml">[</mo><mn id="S3.SS2.p3.5.m5.1.1" xref="S3.SS2.p3.5.m5.1.1.cmml">0</mn><mo id="S3.SS2.p3.5.m5.2.3.3.2.2" xref="S3.SS2.p3.5.m5.2.3.3.1.cmml">,</mo><mn id="S3.SS2.p3.5.m5.2.2" xref="S3.SS2.p3.5.m5.2.2.cmml">1</mn><mo id="S3.SS2.p3.5.m5.2.3.3.2.3" stretchy="false" xref="S3.SS2.p3.5.m5.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m5.2b"><apply id="S3.SS2.p3.5.m5.2.3.cmml" xref="S3.SS2.p3.5.m5.2.3"><in id="S3.SS2.p3.5.m5.2.3.1.cmml" xref="S3.SS2.p3.5.m5.2.3.1"></in><apply id="S3.SS2.p3.5.m5.2.3.2.cmml" xref="S3.SS2.p3.5.m5.2.3.2"><csymbol cd="ambiguous" id="S3.SS2.p3.5.m5.2.3.2.1.cmml" xref="S3.SS2.p3.5.m5.2.3.2">superscript</csymbol><ci id="S3.SS2.p3.5.m5.2.3.2.2.cmml" xref="S3.SS2.p3.5.m5.2.3.2.2">ğ›½</ci><ci id="S3.SS2.p3.5.m5.2.3.2.3.cmml" xref="S3.SS2.p3.5.m5.2.3.2.3">â€²</ci></apply><interval closure="closed" id="S3.SS2.p3.5.m5.2.3.3.1.cmml" xref="S3.SS2.p3.5.m5.2.3.3.2"><cn id="S3.SS2.p3.5.m5.1.1.cmml" type="integer" xref="S3.SS2.p3.5.m5.1.1">0</cn><cn id="S3.SS2.p3.5.m5.2.2.cmml" type="integer" xref="S3.SS2.p3.5.m5.2.2">1</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.5.m5.2c">\beta^{\prime}\in[0,1]</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.5.m5.2d">italic_Î² start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT âˆˆ [ 0 , 1 ]</annotation></semantics></math>. When <math alttext="\beta^{\prime}=0" class="ltx_Math" display="inline" id="S3.SS2.p3.6.m6.1"><semantics id="S3.SS2.p3.6.m6.1a"><mrow id="S3.SS2.p3.6.m6.1.1" xref="S3.SS2.p3.6.m6.1.1.cmml"><msup id="S3.SS2.p3.6.m6.1.1.2" xref="S3.SS2.p3.6.m6.1.1.2.cmml"><mi id="S3.SS2.p3.6.m6.1.1.2.2" xref="S3.SS2.p3.6.m6.1.1.2.2.cmml">Î²</mi><mo id="S3.SS2.p3.6.m6.1.1.2.3" xref="S3.SS2.p3.6.m6.1.1.2.3.cmml">â€²</mo></msup><mo id="S3.SS2.p3.6.m6.1.1.1" xref="S3.SS2.p3.6.m6.1.1.1.cmml">=</mo><mn id="S3.SS2.p3.6.m6.1.1.3" xref="S3.SS2.p3.6.m6.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.6.m6.1b"><apply id="S3.SS2.p3.6.m6.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1"><eq id="S3.SS2.p3.6.m6.1.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1.1"></eq><apply id="S3.SS2.p3.6.m6.1.1.2.cmml" xref="S3.SS2.p3.6.m6.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p3.6.m6.1.1.2.1.cmml" xref="S3.SS2.p3.6.m6.1.1.2">superscript</csymbol><ci id="S3.SS2.p3.6.m6.1.1.2.2.cmml" xref="S3.SS2.p3.6.m6.1.1.2.2">ğ›½</ci><ci id="S3.SS2.p3.6.m6.1.1.2.3.cmml" xref="S3.SS2.p3.6.m6.1.1.2.3">â€²</ci></apply><cn id="S3.SS2.p3.6.m6.1.1.3.cmml" type="integer" xref="S3.SS2.p3.6.m6.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.6.m6.1c">\beta^{\prime}=0</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.6.m6.1d">italic_Î² start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT = 0</annotation></semantics></math> it means that retrieval is never performed. When <math alttext="\beta^{\prime}=1" class="ltx_Math" display="inline" id="S3.SS2.p3.7.m7.1"><semantics id="S3.SS2.p3.7.m7.1a"><mrow id="S3.SS2.p3.7.m7.1.1" xref="S3.SS2.p3.7.m7.1.1.cmml"><msup id="S3.SS2.p3.7.m7.1.1.2" xref="S3.SS2.p3.7.m7.1.1.2.cmml"><mi id="S3.SS2.p3.7.m7.1.1.2.2" xref="S3.SS2.p3.7.m7.1.1.2.2.cmml">Î²</mi><mo id="S3.SS2.p3.7.m7.1.1.2.3" xref="S3.SS2.p3.7.m7.1.1.2.3.cmml">â€²</mo></msup><mo id="S3.SS2.p3.7.m7.1.1.1" xref="S3.SS2.p3.7.m7.1.1.1.cmml">=</mo><mn id="S3.SS2.p3.7.m7.1.1.3" xref="S3.SS2.p3.7.m7.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.7.m7.1b"><apply id="S3.SS2.p3.7.m7.1.1.cmml" xref="S3.SS2.p3.7.m7.1.1"><eq id="S3.SS2.p3.7.m7.1.1.1.cmml" xref="S3.SS2.p3.7.m7.1.1.1"></eq><apply id="S3.SS2.p3.7.m7.1.1.2.cmml" xref="S3.SS2.p3.7.m7.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p3.7.m7.1.1.2.1.cmml" xref="S3.SS2.p3.7.m7.1.1.2">superscript</csymbol><ci id="S3.SS2.p3.7.m7.1.1.2.2.cmml" xref="S3.SS2.p3.7.m7.1.1.2.2">ğ›½</ci><ci id="S3.SS2.p3.7.m7.1.1.2.3.cmml" xref="S3.SS2.p3.7.m7.1.1.2.3">â€²</ci></apply><cn id="S3.SS2.p3.7.m7.1.1.3.cmml" type="integer" xref="S3.SS2.p3.7.m7.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.7.m7.1c">\beta^{\prime}=1</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.7.m7.1d">italic_Î² start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT = 1</annotation></semantics></math> it means that retrieval is performed for every <math alttext="q_{final}" class="ltx_Math" display="inline" id="S3.SS2.p3.8.m8.1"><semantics id="S3.SS2.p3.8.m8.1a"><msub id="S3.SS2.p3.8.m8.1.1" xref="S3.SS2.p3.8.m8.1.1.cmml"><mi id="S3.SS2.p3.8.m8.1.1.2" xref="S3.SS2.p3.8.m8.1.1.2.cmml">q</mi><mrow id="S3.SS2.p3.8.m8.1.1.3" xref="S3.SS2.p3.8.m8.1.1.3.cmml"><mi id="S3.SS2.p3.8.m8.1.1.3.2" xref="S3.SS2.p3.8.m8.1.1.3.2.cmml">f</mi><mo id="S3.SS2.p3.8.m8.1.1.3.1" xref="S3.SS2.p3.8.m8.1.1.3.1.cmml">â¢</mo><mi id="S3.SS2.p3.8.m8.1.1.3.3" xref="S3.SS2.p3.8.m8.1.1.3.3.cmml">i</mi><mo id="S3.SS2.p3.8.m8.1.1.3.1a" xref="S3.SS2.p3.8.m8.1.1.3.1.cmml">â¢</mo><mi id="S3.SS2.p3.8.m8.1.1.3.4" xref="S3.SS2.p3.8.m8.1.1.3.4.cmml">n</mi><mo id="S3.SS2.p3.8.m8.1.1.3.1b" xref="S3.SS2.p3.8.m8.1.1.3.1.cmml">â¢</mo><mi id="S3.SS2.p3.8.m8.1.1.3.5" xref="S3.SS2.p3.8.m8.1.1.3.5.cmml">a</mi><mo id="S3.SS2.p3.8.m8.1.1.3.1c" xref="S3.SS2.p3.8.m8.1.1.3.1.cmml">â¢</mo><mi id="S3.SS2.p3.8.m8.1.1.3.6" xref="S3.SS2.p3.8.m8.1.1.3.6.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.8.m8.1b"><apply id="S3.SS2.p3.8.m8.1.1.cmml" xref="S3.SS2.p3.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.8.m8.1.1.1.cmml" xref="S3.SS2.p3.8.m8.1.1">subscript</csymbol><ci id="S3.SS2.p3.8.m8.1.1.2.cmml" xref="S3.SS2.p3.8.m8.1.1.2">ğ‘</ci><apply id="S3.SS2.p3.8.m8.1.1.3.cmml" xref="S3.SS2.p3.8.m8.1.1.3"><times id="S3.SS2.p3.8.m8.1.1.3.1.cmml" xref="S3.SS2.p3.8.m8.1.1.3.1"></times><ci id="S3.SS2.p3.8.m8.1.1.3.2.cmml" xref="S3.SS2.p3.8.m8.1.1.3.2">ğ‘“</ci><ci id="S3.SS2.p3.8.m8.1.1.3.3.cmml" xref="S3.SS2.p3.8.m8.1.1.3.3">ğ‘–</ci><ci id="S3.SS2.p3.8.m8.1.1.3.4.cmml" xref="S3.SS2.p3.8.m8.1.1.3.4">ğ‘›</ci><ci id="S3.SS2.p3.8.m8.1.1.3.5.cmml" xref="S3.SS2.p3.8.m8.1.1.3.5">ğ‘</ci><ci id="S3.SS2.p3.8.m8.1.1.3.6.cmml" xref="S3.SS2.p3.8.m8.1.1.3.6">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.8.m8.1c">q_{final}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.8.m8.1d">italic_q start_POSTSUBSCRIPT italic_f italic_i italic_n italic_a italic_l end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Information Retrieval</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">If the model determines that additional information is necessary, we proceed with the information retrieval step. Search engines possess features that large language models (LLMs) lack, such as the ability to be easily and quickly updated <cite class="ltx_cite ltx_citemacro_citep">(Kasai etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib17" title="">2024</a>)</cite>. We use the Google search engine and Wikipedia-API(wiki) as the retriever to obtain relevant documents <math alttext="D" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.1.m1.1"><semantics id="S3.SS2.SSS1.p1.1.m1.1a"><mi id="S3.SS2.SSS1.p1.1.m1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.1.m1.1b"><ci id="S3.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1">ğ·</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.1.m1.1c">D</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.1.m1.1d">italic_D</annotation></semantics></math>, 2 example in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S3.T1" title="Table 1 â€£ 3.2.1 Information Retrieval â€£ 3.2 Model Capability Check â€£ 3 Methodology â€£ Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">1</span></a>. Detailed settings for the retrieval process are described in SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S4.SS2" title="4.2 Retriever Details â€£ 4 Experimental Setup â€£ Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">4.2</span></a>.</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="y_{\text{output}}=\left\{\begin{array}[]{ll}LM(\&gt;q_{\text{final}}\&gt;)&amp;\text{if %
}\&gt;\beta\geq\beta^{\prime}\\
LM(\&gt;[\&gt;D,\&gt;q_{\text{final}}\&gt;]\&gt;)&amp;\text{if }\&gt;\beta&lt;\beta^{\prime}\end{array}\right." class="ltx_Math" display="block" id="S3.E2.m1.3"><semantics id="S3.E2.m1.3a"><mrow id="S3.E2.m1.3.4" xref="S3.E2.m1.3.4.cmml"><msub id="S3.E2.m1.3.4.2" xref="S3.E2.m1.3.4.2.cmml"><mi id="S3.E2.m1.3.4.2.2" xref="S3.E2.m1.3.4.2.2.cmml">y</mi><mtext id="S3.E2.m1.3.4.2.3" xref="S3.E2.m1.3.4.2.3a.cmml">output</mtext></msub><mo id="S3.E2.m1.3.4.1" xref="S3.E2.m1.3.4.1.cmml">=</mo><mrow id="S3.E2.m1.3.4.3.2" xref="S3.E2.m1.3.4.3.1.cmml"><mo id="S3.E2.m1.3.4.3.2.1" xref="S3.E2.m1.3.4.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" id="S3.E2.m1.3.3" rowspacing="0pt" xref="S3.E2.m1.3.3.cmml"><mtr id="S3.E2.m1.3.3a" xref="S3.E2.m1.3.3.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E2.m1.3.3b" xref="S3.E2.m1.3.3.cmml"><mrow id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.3.cmml">L</mi><mo id="S3.E2.m1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.2.cmml">â¢</mo><mi id="S3.E2.m1.1.1.1.1.1.4" xref="S3.E2.m1.1.1.1.1.1.4.cmml">M</mi><mo id="S3.E2.m1.1.1.1.1.1.2a" xref="S3.E2.m1.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.1.1.1.1.1.1.1.2" rspace="0.220em" stretchy="false" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E2.m1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.cmml">q</mi><mtext id="S3.E2.m1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3a.cmml">final</mtext></msub><mo id="S3.E2.m1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E2.m1.3.3c" xref="S3.E2.m1.3.3.cmml"><mrow id="S3.E2.m1.1.1.1.2.1" xref="S3.E2.m1.1.1.1.2.1.cmml"><mrow id="S3.E2.m1.1.1.1.2.1.2" xref="S3.E2.m1.1.1.1.2.1.2.cmml"><mtext id="S3.E2.m1.1.1.1.2.1.2.2" xref="S3.E2.m1.1.1.1.2.1.2.2a.cmml">ifÂ </mtext><mo id="S3.E2.m1.1.1.1.2.1.2.1" lspace="0.220em" xref="S3.E2.m1.1.1.1.2.1.2.1.cmml">â¢</mo><mi id="S3.E2.m1.1.1.1.2.1.2.3" xref="S3.E2.m1.1.1.1.2.1.2.3.cmml">Î²</mi></mrow><mo id="S3.E2.m1.1.1.1.2.1.1" xref="S3.E2.m1.1.1.1.2.1.1.cmml">â‰¥</mo><msup id="S3.E2.m1.1.1.1.2.1.3" xref="S3.E2.m1.1.1.1.2.1.3.cmml"><mi id="S3.E2.m1.1.1.1.2.1.3.2" xref="S3.E2.m1.1.1.1.2.1.3.2.cmml">Î²</mi><mo id="S3.E2.m1.1.1.1.2.1.3.3" xref="S3.E2.m1.1.1.1.2.1.3.3.cmml">â€²</mo></msup></mrow></mtd></mtr><mtr id="S3.E2.m1.3.3d" xref="S3.E2.m1.3.3.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E2.m1.3.3e" xref="S3.E2.m1.3.3.cmml"><mrow id="S3.E2.m1.3.3.3.2.2" xref="S3.E2.m1.3.3.3.2.2.cmml"><mi id="S3.E2.m1.3.3.3.2.2.4" xref="S3.E2.m1.3.3.3.2.2.4.cmml">L</mi><mo id="S3.E2.m1.3.3.3.2.2.3" xref="S3.E2.m1.3.3.3.2.2.3.cmml">â¢</mo><mi id="S3.E2.m1.3.3.3.2.2.5" xref="S3.E2.m1.3.3.3.2.2.5.cmml">M</mi><mo id="S3.E2.m1.3.3.3.2.2.3a" xref="S3.E2.m1.3.3.3.2.2.3.cmml">â¢</mo><mrow id="S3.E2.m1.3.3.3.2.2.2.1" xref="S3.E2.m1.3.3.3.2.2.cmml"><mo id="S3.E2.m1.3.3.3.2.2.2.1.2" rspace="0.220em" stretchy="false" xref="S3.E2.m1.3.3.3.2.2.cmml">(</mo><mrow id="S3.E2.m1.3.3.3.2.2.2.1.1.1" xref="S3.E2.m1.3.3.3.2.2.2.1.1.2.cmml"><mo id="S3.E2.m1.3.3.3.2.2.2.1.1.1.2" rspace="0.220em" stretchy="false" xref="S3.E2.m1.3.3.3.2.2.2.1.1.2.cmml">[</mo><mi id="S3.E2.m1.2.2.2.1.1.1" xref="S3.E2.m1.2.2.2.1.1.1.cmml">D</mi><mo id="S3.E2.m1.3.3.3.2.2.2.1.1.1.3" rspace="0.387em" xref="S3.E2.m1.3.3.3.2.2.2.1.1.2.cmml">,</mo><msub id="S3.E2.m1.3.3.3.2.2.2.1.1.1.1" xref="S3.E2.m1.3.3.3.2.2.2.1.1.1.1.cmml"><mi id="S3.E2.m1.3.3.3.2.2.2.1.1.1.1.2" xref="S3.E2.m1.3.3.3.2.2.2.1.1.1.1.2.cmml">q</mi><mtext id="S3.E2.m1.3.3.3.2.2.2.1.1.1.1.3" xref="S3.E2.m1.3.3.3.2.2.2.1.1.1.1.3a.cmml">final</mtext></msub><mo id="S3.E2.m1.3.3.3.2.2.2.1.1.1.4" rspace="0.220em" stretchy="false" xref="S3.E2.m1.3.3.3.2.2.2.1.1.2.cmml">]</mo></mrow><mo id="S3.E2.m1.3.3.3.2.2.2.1.3" stretchy="false" xref="S3.E2.m1.3.3.3.2.2.cmml">)</mo></mrow></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E2.m1.3.3f" xref="S3.E2.m1.3.3.cmml"><mrow id="S3.E2.m1.3.3.3.3.1" xref="S3.E2.m1.3.3.3.3.1.cmml"><mrow id="S3.E2.m1.3.3.3.3.1.2" xref="S3.E2.m1.3.3.3.3.1.2.cmml"><mtext id="S3.E2.m1.3.3.3.3.1.2.2" xref="S3.E2.m1.3.3.3.3.1.2.2a.cmml">ifÂ </mtext><mo id="S3.E2.m1.3.3.3.3.1.2.1" lspace="0.220em" xref="S3.E2.m1.3.3.3.3.1.2.1.cmml">â¢</mo><mi id="S3.E2.m1.3.3.3.3.1.2.3" xref="S3.E2.m1.3.3.3.3.1.2.3.cmml">Î²</mi></mrow><mo id="S3.E2.m1.3.3.3.3.1.1" xref="S3.E2.m1.3.3.3.3.1.1.cmml">&lt;</mo><msup id="S3.E2.m1.3.3.3.3.1.3" xref="S3.E2.m1.3.3.3.3.1.3.cmml"><mi id="S3.E2.m1.3.3.3.3.1.3.2" xref="S3.E2.m1.3.3.3.3.1.3.2.cmml">Î²</mi><mo id="S3.E2.m1.3.3.3.3.1.3.3" xref="S3.E2.m1.3.3.3.3.1.3.3.cmml">â€²</mo></msup></mrow></mtd></mtr></mtable><mi id="S3.E2.m1.3.4.3.2.2" xref="S3.E2.m1.3.4.3.1.1.cmml"></mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.3b"><apply id="S3.E2.m1.3.4.cmml" xref="S3.E2.m1.3.4"><eq id="S3.E2.m1.3.4.1.cmml" xref="S3.E2.m1.3.4.1"></eq><apply id="S3.E2.m1.3.4.2.cmml" xref="S3.E2.m1.3.4.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.4.2.1.cmml" xref="S3.E2.m1.3.4.2">subscript</csymbol><ci id="S3.E2.m1.3.4.2.2.cmml" xref="S3.E2.m1.3.4.2.2">ğ‘¦</ci><ci id="S3.E2.m1.3.4.2.3a.cmml" xref="S3.E2.m1.3.4.2.3"><mtext id="S3.E2.m1.3.4.2.3.cmml" mathsize="70%" xref="S3.E2.m1.3.4.2.3">output</mtext></ci></apply><apply id="S3.E2.m1.3.4.3.1.cmml" xref="S3.E2.m1.3.4.3.2"><csymbol cd="latexml" id="S3.E2.m1.3.4.3.1.1.cmml" xref="S3.E2.m1.3.4.3.2.1">cases</csymbol><matrix id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3"><matrixrow id="S3.E2.m1.3.3a.cmml" xref="S3.E2.m1.3.3"><apply id="S3.E2.m1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"><times id="S3.E2.m1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.2"></times><ci id="S3.E2.m1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.3">ğ¿</ci><ci id="S3.E2.m1.1.1.1.1.1.4.cmml" xref="S3.E2.m1.1.1.1.1.1.4">ğ‘€</ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2">ğ‘</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.3a.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3"><mtext id="S3.E2.m1.1.1.1.1.1.1.1.1.3.cmml" mathsize="70%" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3">final</mtext></ci></apply></apply><apply id="S3.E2.m1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.2.1"><geq id="S3.E2.m1.1.1.1.2.1.1.cmml" xref="S3.E2.m1.1.1.1.2.1.1"></geq><apply id="S3.E2.m1.1.1.1.2.1.2.cmml" xref="S3.E2.m1.1.1.1.2.1.2"><times id="S3.E2.m1.1.1.1.2.1.2.1.cmml" xref="S3.E2.m1.1.1.1.2.1.2.1"></times><ci id="S3.E2.m1.1.1.1.2.1.2.2a.cmml" xref="S3.E2.m1.1.1.1.2.1.2.2"><mtext id="S3.E2.m1.1.1.1.2.1.2.2.cmml" xref="S3.E2.m1.1.1.1.2.1.2.2">ifÂ </mtext></ci><ci id="S3.E2.m1.1.1.1.2.1.2.3.cmml" xref="S3.E2.m1.1.1.1.2.1.2.3">ğ›½</ci></apply><apply id="S3.E2.m1.1.1.1.2.1.3.cmml" xref="S3.E2.m1.1.1.1.2.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.2.1.3.1.cmml" xref="S3.E2.m1.1.1.1.2.1.3">superscript</csymbol><ci id="S3.E2.m1.1.1.1.2.1.3.2.cmml" xref="S3.E2.m1.1.1.1.2.1.3.2">ğ›½</ci><ci id="S3.E2.m1.1.1.1.2.1.3.3.cmml" xref="S3.E2.m1.1.1.1.2.1.3.3">â€²</ci></apply></apply></matrixrow><matrixrow id="S3.E2.m1.3.3b.cmml" xref="S3.E2.m1.3.3"><apply id="S3.E2.m1.3.3.3.2.2.cmml" xref="S3.E2.m1.3.3.3.2.2"><times id="S3.E2.m1.3.3.3.2.2.3.cmml" xref="S3.E2.m1.3.3.3.2.2.3"></times><ci id="S3.E2.m1.3.3.3.2.2.4.cmml" xref="S3.E2.m1.3.3.3.2.2.4">ğ¿</ci><ci id="S3.E2.m1.3.3.3.2.2.5.cmml" xref="S3.E2.m1.3.3.3.2.2.5">ğ‘€</ci><interval closure="closed" id="S3.E2.m1.3.3.3.2.2.2.1.1.2.cmml" xref="S3.E2.m1.3.3.3.2.2.2.1.1.1"><ci id="S3.E2.m1.2.2.2.1.1.1.cmml" xref="S3.E2.m1.2.2.2.1.1.1">ğ·</ci><apply id="S3.E2.m1.3.3.3.2.2.2.1.1.1.1.cmml" xref="S3.E2.m1.3.3.3.2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.3.2.2.2.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.3.2.2.2.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.3.3.3.2.2.2.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.3.2.2.2.1.1.1.1.2">ğ‘</ci><ci id="S3.E2.m1.3.3.3.2.2.2.1.1.1.1.3a.cmml" xref="S3.E2.m1.3.3.3.2.2.2.1.1.1.1.3"><mtext id="S3.E2.m1.3.3.3.2.2.2.1.1.1.1.3.cmml" mathsize="70%" xref="S3.E2.m1.3.3.3.2.2.2.1.1.1.1.3">final</mtext></ci></apply></interval></apply><apply id="S3.E2.m1.3.3.3.3.1.cmml" xref="S3.E2.m1.3.3.3.3.1"><lt id="S3.E2.m1.3.3.3.3.1.1.cmml" xref="S3.E2.m1.3.3.3.3.1.1"></lt><apply id="S3.E2.m1.3.3.3.3.1.2.cmml" xref="S3.E2.m1.3.3.3.3.1.2"><times id="S3.E2.m1.3.3.3.3.1.2.1.cmml" xref="S3.E2.m1.3.3.3.3.1.2.1"></times><ci id="S3.E2.m1.3.3.3.3.1.2.2a.cmml" xref="S3.E2.m1.3.3.3.3.1.2.2"><mtext id="S3.E2.m1.3.3.3.3.1.2.2.cmml" xref="S3.E2.m1.3.3.3.3.1.2.2">ifÂ </mtext></ci><ci id="S3.E2.m1.3.3.3.3.1.2.3.cmml" xref="S3.E2.m1.3.3.3.3.1.2.3">ğ›½</ci></apply><apply id="S3.E2.m1.3.3.3.3.1.3.cmml" xref="S3.E2.m1.3.3.3.3.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.3.3.1.3.1.cmml" xref="S3.E2.m1.3.3.3.3.1.3">superscript</csymbol><ci id="S3.E2.m1.3.3.3.3.1.3.2.cmml" xref="S3.E2.m1.3.3.3.3.1.3.2">ğ›½</ci><ci id="S3.E2.m1.3.3.3.3.1.3.3.cmml" xref="S3.E2.m1.3.3.3.3.1.3.3">â€²</ci></apply></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.3c">y_{\text{output}}=\left\{\begin{array}[]{ll}LM(\&gt;q_{\text{final}}\&gt;)&amp;\text{if %
}\&gt;\beta\geq\beta^{\prime}\\
LM(\&gt;[\&gt;D,\&gt;q_{\text{final}}\&gt;]\&gt;)&amp;\text{if }\&gt;\beta&lt;\beta^{\prime}\end{array}\right.</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.3d">italic_y start_POSTSUBSCRIPT output end_POSTSUBSCRIPT = { start_ARRAY start_ROW start_CELL italic_L italic_M ( italic_q start_POSTSUBSCRIPT final end_POSTSUBSCRIPT ) end_CELL start_CELL if italic_Î² â‰¥ italic_Î² start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT end_CELL end_ROW start_ROW start_CELL italic_L italic_M ( [ italic_D , italic_q start_POSTSUBSCRIPT final end_POSTSUBSCRIPT ] ) end_CELL start_CELL if italic_Î² &lt; italic_Î² start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT end_CELL end_ROW end_ARRAY</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<figure class="ltx_table" id="S3.T1">
<div class="ltx_inline-block ltx_transformed_outer" id="S3.T1.1" style="width:433.6pt;height:283.9pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-37.9pt,24.8pt) scale(0.851068188250304,0.851068188250304) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.1.1">Question: Is popular science used to peer review papers?</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.2.2">
<td class="ltx_td ltx_align_left" id="S3.T1.1.1.2.2.1">The question is: ambiguous.</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.3.3">
<td class="ltx_td ltx_align_left" id="S3.T1.1.1.3.3.1">Sub-Question: What is popular science?</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.4.4">
<td class="ltx_td ltx_align_left" id="S3.T1.1.1.4.4.1">Probability of correct answer is: 0.5.</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.5.5">
<td class="ltx_td ltx_align_left" id="S3.T1.1.1.5.5.1">Sub-Answer: Popular science is a simplified version of scientific work.</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.6.6">
<td class="ltx_td ltx_align_left" id="S3.T1.1.1.6.6.1">Sub-Question: What types of documents does peer review use to verify papers?</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.7.7">
<td class="ltx_td ltx_align_left" id="S3.T1.1.1.7.7.1">Probability of correct answer is: 0.6.</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.8.8">
<td class="ltx_td ltx_align_left" id="S3.T1.1.1.8.8.1">Sub-Answer: Peer review uses detailed scientific information to verify papers.</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.9.9">
<td class="ltx_td ltx_align_left" id="S3.T1.1.1.9.9.1">So the final answer is: False.</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.10.10">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.1.10.10.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.10.10.1.1">Question: Does a lapidary work with items that are studied by geologists?</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.11.11">
<td class="ltx_td ltx_align_left" id="S3.T1.1.1.11.11.1">The question is: ambiguous.</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.12.12">
<td class="ltx_td ltx_align_left" id="S3.T1.1.1.12.12.1">Sub-Question: What are the materials a lapidary works with?</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.13.13">
<td class="ltx_td ltx_align_left" id="S3.T1.1.1.13.13.1">Probability of correct answer is: <span class="ltx_text" id="S3.T1.1.1.13.13.1.1" style="background-color:#FFFF00;">0.4.</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.14.14">
<td class="ltx_td ltx_align_left" id="S3.T1.1.1.14.14.1">Sub-Answer: <span class="ltx_text" id="S3.T1.1.1.14.14.1.1" style="background-color:#00FF00;">Quartz is a popular material for lapidary because it is relatively easy to work with and comes in various colours and patterns. As it is â€¦.</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.15.15">
<td class="ltx_td ltx_align_left" id="S3.T1.1.1.15.15.1">Sub-Question: What do geologists study?</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.16.16">
<td class="ltx_td ltx_align_left" id="S3.T1.1.1.16.16.1">Probability of correct answer is: 0.5.</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.17.17">
<td class="ltx_td ltx_align_left" id="S3.T1.1.1.17.17.1">Sub-Answer: Lapidarists work with stone, minerals and gemstones.</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.18.18">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.1.1.18.18.1">So the final answer is: True.</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Think-then-Act on StrategyQA: 2 samples</figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Setup</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Task Settings</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">To thoroughly evaluate the capabilities of the <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.1">Think-then-Act</span> framework, we selected tasks and datasets designed to comprehensively test various aspects of our approach. Specifically, we chose three sub-tasks: Multihop QA, Commonsense Reasoning, Fact Checking and Domain QA.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>MultihopQA</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">Two multihop QA datasets are used for evaluation. (i) HotPotQA <cite class="ltx_cite ltx_citemacro_citep">Yang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib40" title="">2018</a></cite>: consists of complex questions that require multi-hop reasoning, where the answer to a question requires synthesizing information from multiple documents. For our evaluation, we use the full test set, ensuring a comprehensive assessment of our framework capabilitues of multihop reasoning. (ii) 2WikiMultihopQA <cite class="ltx_cite ltx_citemacro_citep">Ho etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib10" title="">2020</a></cite> is a multi-hop question-answering dataset that exploits the structured format in Wikidata and uses logical rules to create questions. By evaluating on this dataset, we aim to test the proficiency of our framework in handling structured information and executing logical inference.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Commonsense Reasoning</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">Commonsense reasoning requires a blend of world knowledge and commonsense understanding to generate accurate answers. For this purpose, we utilize the StrategyQA dataset <cite class="ltx_cite ltx_citemacro_cite">Geva etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib8" title="">2021</a>)</cite>, which consists of crowd sourced yes/no questions, such as â€œWould a pear sink in water?â€. The final answers are extracted and matched against the gold standard answers using exact match to evaluate the performance of our framework in commonsense reasoning tasks.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3 </span>Fact Checking</h4>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">We also employ the FEVER dataset <cite class="ltx_cite ltx_citemacro_cite">Thorne etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib36" title="">2018</a>)</cite> for fact verification tasks. This dataset categorizes claims as "SUPPORTS", "REFUTES", or "NOT ENOUGH INFO" based on evidence paragraphs extracted from Wikipedia. To ensure a challenging evaluation, we sample a balanced set of instances where GPT-3â€™s chain-of-thought (CoT) method makes both correct and incorrect predictions. This approach allows us to rigorously test the modelâ€™s ability to verify facts using evidence-based reasoning.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.4 </span>Domain QA</h4>
<div class="ltx_para" id="S4.SS1.SSS4.p1">
<p class="ltx_p" id="S4.SS1.SSS4.p1.1">For the domain-specific question answering task, we utilize a custom-built dataset focused on <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS4.p1.1.1">Chinese Poetry</span>. This dataset was developed in response to recurring issues with existing QA models, such as ChatGPT, which often incorrectly match poetry verses with their titles and authors. Our analysis revealed two primary reasons for these errors: firstly, the models may possess accurate parametric knowledge but still generate incorrect answers; secondly, they may lack the requisite information for obscure poetry verses. Consequently, this dataset is ideally suited to evaluate the effectiveness of our framework. Additionally, this allows us to test the effectiveness of our framework in the Chinese language context, extending its applicability beyond just English.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS4.p2">
<p class="ltx_p" id="S4.SS1.SSS4.p2.1">Our custom dataset comprises 9,791 poetry verses from 60 different poets, providing a comprehensive basis for testing. This dataset enables us to thoroughly assess the ability of our framework to handle both common and obscure queries in the domain of classical Chinese poetry.</p>
</div>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="250" id="S4.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overall results using the EM metric.Think-then-Act parameter <math alttext="\beta^{\prime}=0.5" class="ltx_Math" display="inline" id="S4.F2.2.m1.1"><semantics id="S4.F2.2.m1.1b"><mrow id="S4.F2.2.m1.1.1" xref="S4.F2.2.m1.1.1.cmml"><msup id="S4.F2.2.m1.1.1.2" xref="S4.F2.2.m1.1.1.2.cmml"><mi id="S4.F2.2.m1.1.1.2.2" xref="S4.F2.2.m1.1.1.2.2.cmml">Î²</mi><mo id="S4.F2.2.m1.1.1.2.3" xref="S4.F2.2.m1.1.1.2.3.cmml">â€²</mo></msup><mo id="S4.F2.2.m1.1.1.1" xref="S4.F2.2.m1.1.1.1.cmml">=</mo><mn id="S4.F2.2.m1.1.1.3" xref="S4.F2.2.m1.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F2.2.m1.1c"><apply id="S4.F2.2.m1.1.1.cmml" xref="S4.F2.2.m1.1.1"><eq id="S4.F2.2.m1.1.1.1.cmml" xref="S4.F2.2.m1.1.1.1"></eq><apply id="S4.F2.2.m1.1.1.2.cmml" xref="S4.F2.2.m1.1.1.2"><csymbol cd="ambiguous" id="S4.F2.2.m1.1.1.2.1.cmml" xref="S4.F2.2.m1.1.1.2">superscript</csymbol><ci id="S4.F2.2.m1.1.1.2.2.cmml" xref="S4.F2.2.m1.1.1.2.2">ğ›½</ci><ci id="S4.F2.2.m1.1.1.2.3.cmml" xref="S4.F2.2.m1.1.1.2.3">â€²</ci></apply><cn id="S4.F2.2.m1.1.1.3.cmml" type="float" xref="S4.F2.2.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.2.m1.1d">\beta^{\prime}=0.5</annotation><annotation encoding="application/x-llamapun" id="S4.F2.2.m1.1e">italic_Î² start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT = 0.5</annotation></semantics></math>.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Retriever Details</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">In the information retrieval step, we use two systems to obtain relevant information:</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1">Wikipedia-API</span>(wiki): For the final query <math alttext="q_{final}" class="ltx_Math" display="inline" id="S4.SS2.p2.1.m1.1"><semantics id="S4.SS2.p2.1.m1.1a"><msub id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><mi id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2.cmml">q</mi><mrow id="S4.SS2.p2.1.m1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.3.cmml"><mi id="S4.SS2.p2.1.m1.1.1.3.2" xref="S4.SS2.p2.1.m1.1.1.3.2.cmml">f</mi><mo id="S4.SS2.p2.1.m1.1.1.3.1" xref="S4.SS2.p2.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S4.SS2.p2.1.m1.1.1.3.3" xref="S4.SS2.p2.1.m1.1.1.3.3.cmml">i</mi><mo id="S4.SS2.p2.1.m1.1.1.3.1a" xref="S4.SS2.p2.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S4.SS2.p2.1.m1.1.1.3.4" xref="S4.SS2.p2.1.m1.1.1.3.4.cmml">n</mi><mo id="S4.SS2.p2.1.m1.1.1.3.1b" xref="S4.SS2.p2.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S4.SS2.p2.1.m1.1.1.3.5" xref="S4.SS2.p2.1.m1.1.1.3.5.cmml">a</mi><mo id="S4.SS2.p2.1.m1.1.1.3.1c" xref="S4.SS2.p2.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S4.SS2.p2.1.m1.1.1.3.6" xref="S4.SS2.p2.1.m1.1.1.3.6.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2">ğ‘</ci><apply id="S4.SS2.p2.1.m1.1.1.3.cmml" xref="S4.SS2.p2.1.m1.1.1.3"><times id="S4.SS2.p2.1.m1.1.1.3.1.cmml" xref="S4.SS2.p2.1.m1.1.1.3.1"></times><ci id="S4.SS2.p2.1.m1.1.1.3.2.cmml" xref="S4.SS2.p2.1.m1.1.1.3.2">ğ‘“</ci><ci id="S4.SS2.p2.1.m1.1.1.3.3.cmml" xref="S4.SS2.p2.1.m1.1.1.3.3">ğ‘–</ci><ci id="S4.SS2.p2.1.m1.1.1.3.4.cmml" xref="S4.SS2.p2.1.m1.1.1.3.4">ğ‘›</ci><ci id="S4.SS2.p2.1.m1.1.1.3.5.cmml" xref="S4.SS2.p2.1.m1.1.1.3.5">ğ‘</ci><ci id="S4.SS2.p2.1.m1.1.1.3.6.cmml" xref="S4.SS2.p2.1.m1.1.1.3.6">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">q_{final}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.1.m1.1d">italic_q start_POSTSUBSCRIPT italic_f italic_i italic_n italic_a italic_l end_POSTSUBSCRIPT</annotation></semantics></math>, we search through Wikipedia and select the top sentences from the relevant Wikipedia pages. This approach leverages the structured and comprehensive nature of Wikipedia to provide accurate and detailed information.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.2"><span class="ltx_text ltx_font_bold" id="S4.SS2.p3.2.1">Google Search Engine</span>: For queries that can be directly answered, such as "Where is the capital of China?", Google searches often present direct "<math alttext="answer\;boxes" class="ltx_Math" display="inline" id="S4.SS2.p3.1.m1.1"><semantics id="S4.SS2.p3.1.m1.1a"><mrow id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml"><mi id="S4.SS2.p3.1.m1.1.1.2" xref="S4.SS2.p3.1.m1.1.1.2.cmml">a</mi><mo id="S4.SS2.p3.1.m1.1.1.1" xref="S4.SS2.p3.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.SS2.p3.1.m1.1.1.3" xref="S4.SS2.p3.1.m1.1.1.3.cmml">n</mi><mo id="S4.SS2.p3.1.m1.1.1.1a" xref="S4.SS2.p3.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.SS2.p3.1.m1.1.1.4" xref="S4.SS2.p3.1.m1.1.1.4.cmml">s</mi><mo id="S4.SS2.p3.1.m1.1.1.1b" xref="S4.SS2.p3.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.SS2.p3.1.m1.1.1.5" xref="S4.SS2.p3.1.m1.1.1.5.cmml">w</mi><mo id="S4.SS2.p3.1.m1.1.1.1c" xref="S4.SS2.p3.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.SS2.p3.1.m1.1.1.6" xref="S4.SS2.p3.1.m1.1.1.6.cmml">e</mi><mo id="S4.SS2.p3.1.m1.1.1.1d" xref="S4.SS2.p3.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.SS2.p3.1.m1.1.1.7" xref="S4.SS2.p3.1.m1.1.1.7.cmml">r</mi><mo id="S4.SS2.p3.1.m1.1.1.1e" lspace="0.280em" xref="S4.SS2.p3.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.SS2.p3.1.m1.1.1.8" xref="S4.SS2.p3.1.m1.1.1.8.cmml">b</mi><mo id="S4.SS2.p3.1.m1.1.1.1f" xref="S4.SS2.p3.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.SS2.p3.1.m1.1.1.9" xref="S4.SS2.p3.1.m1.1.1.9.cmml">o</mi><mo id="S4.SS2.p3.1.m1.1.1.1g" xref="S4.SS2.p3.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.SS2.p3.1.m1.1.1.10" xref="S4.SS2.p3.1.m1.1.1.10.cmml">x</mi><mo id="S4.SS2.p3.1.m1.1.1.1h" xref="S4.SS2.p3.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.SS2.p3.1.m1.1.1.11" xref="S4.SS2.p3.1.m1.1.1.11.cmml">e</mi><mo id="S4.SS2.p3.1.m1.1.1.1i" xref="S4.SS2.p3.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.SS2.p3.1.m1.1.1.12" xref="S4.SS2.p3.1.m1.1.1.12.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><apply id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1"><times id="S4.SS2.p3.1.m1.1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1.1"></times><ci id="S4.SS2.p3.1.m1.1.1.2.cmml" xref="S4.SS2.p3.1.m1.1.1.2">ğ‘</ci><ci id="S4.SS2.p3.1.m1.1.1.3.cmml" xref="S4.SS2.p3.1.m1.1.1.3">ğ‘›</ci><ci id="S4.SS2.p3.1.m1.1.1.4.cmml" xref="S4.SS2.p3.1.m1.1.1.4">ğ‘ </ci><ci id="S4.SS2.p3.1.m1.1.1.5.cmml" xref="S4.SS2.p3.1.m1.1.1.5">ğ‘¤</ci><ci id="S4.SS2.p3.1.m1.1.1.6.cmml" xref="S4.SS2.p3.1.m1.1.1.6">ğ‘’</ci><ci id="S4.SS2.p3.1.m1.1.1.7.cmml" xref="S4.SS2.p3.1.m1.1.1.7">ğ‘Ÿ</ci><ci id="S4.SS2.p3.1.m1.1.1.8.cmml" xref="S4.SS2.p3.1.m1.1.1.8">ğ‘</ci><ci id="S4.SS2.p3.1.m1.1.1.9.cmml" xref="S4.SS2.p3.1.m1.1.1.9">ğ‘œ</ci><ci id="S4.SS2.p3.1.m1.1.1.10.cmml" xref="S4.SS2.p3.1.m1.1.1.10">ğ‘¥</ci><ci id="S4.SS2.p3.1.m1.1.1.11.cmml" xref="S4.SS2.p3.1.m1.1.1.11">ğ‘’</ci><ci id="S4.SS2.p3.1.m1.1.1.12.cmml" xref="S4.SS2.p3.1.m1.1.1.12">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">answer\;boxes</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.1.m1.1d">italic_a italic_n italic_s italic_w italic_e italic_r italic_b italic_o italic_x italic_e italic_s</annotation></semantics></math>". We utilize these explicit answers for straightforward questions. For more complex queries, Google provides "<math alttext="organic\;results" class="ltx_Math" display="inline" id="S4.SS2.p3.2.m2.1"><semantics id="S4.SS2.p3.2.m2.1a"><mrow id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml"><mi id="S4.SS2.p3.2.m2.1.1.2" xref="S4.SS2.p3.2.m2.1.1.2.cmml">o</mi><mo id="S4.SS2.p3.2.m2.1.1.1" xref="S4.SS2.p3.2.m2.1.1.1.cmml">â¢</mo><mi id="S4.SS2.p3.2.m2.1.1.3" xref="S4.SS2.p3.2.m2.1.1.3.cmml">r</mi><mo id="S4.SS2.p3.2.m2.1.1.1a" xref="S4.SS2.p3.2.m2.1.1.1.cmml">â¢</mo><mi id="S4.SS2.p3.2.m2.1.1.4" xref="S4.SS2.p3.2.m2.1.1.4.cmml">g</mi><mo id="S4.SS2.p3.2.m2.1.1.1b" xref="S4.SS2.p3.2.m2.1.1.1.cmml">â¢</mo><mi id="S4.SS2.p3.2.m2.1.1.5" xref="S4.SS2.p3.2.m2.1.1.5.cmml">a</mi><mo id="S4.SS2.p3.2.m2.1.1.1c" xref="S4.SS2.p3.2.m2.1.1.1.cmml">â¢</mo><mi id="S4.SS2.p3.2.m2.1.1.6" xref="S4.SS2.p3.2.m2.1.1.6.cmml">n</mi><mo id="S4.SS2.p3.2.m2.1.1.1d" xref="S4.SS2.p3.2.m2.1.1.1.cmml">â¢</mo><mi id="S4.SS2.p3.2.m2.1.1.7" xref="S4.SS2.p3.2.m2.1.1.7.cmml">i</mi><mo id="S4.SS2.p3.2.m2.1.1.1e" xref="S4.SS2.p3.2.m2.1.1.1.cmml">â¢</mo><mi id="S4.SS2.p3.2.m2.1.1.8" xref="S4.SS2.p3.2.m2.1.1.8.cmml">c</mi><mo id="S4.SS2.p3.2.m2.1.1.1f" lspace="0.280em" xref="S4.SS2.p3.2.m2.1.1.1.cmml">â¢</mo><mi id="S4.SS2.p3.2.m2.1.1.9" xref="S4.SS2.p3.2.m2.1.1.9.cmml">r</mi><mo id="S4.SS2.p3.2.m2.1.1.1g" xref="S4.SS2.p3.2.m2.1.1.1.cmml">â¢</mo><mi id="S4.SS2.p3.2.m2.1.1.10" xref="S4.SS2.p3.2.m2.1.1.10.cmml">e</mi><mo id="S4.SS2.p3.2.m2.1.1.1h" xref="S4.SS2.p3.2.m2.1.1.1.cmml">â¢</mo><mi id="S4.SS2.p3.2.m2.1.1.11" xref="S4.SS2.p3.2.m2.1.1.11.cmml">s</mi><mo id="S4.SS2.p3.2.m2.1.1.1i" xref="S4.SS2.p3.2.m2.1.1.1.cmml">â¢</mo><mi id="S4.SS2.p3.2.m2.1.1.12" xref="S4.SS2.p3.2.m2.1.1.12.cmml">u</mi><mo id="S4.SS2.p3.2.m2.1.1.1j" xref="S4.SS2.p3.2.m2.1.1.1.cmml">â¢</mo><mi id="S4.SS2.p3.2.m2.1.1.13" xref="S4.SS2.p3.2.m2.1.1.13.cmml">l</mi><mo id="S4.SS2.p3.2.m2.1.1.1k" xref="S4.SS2.p3.2.m2.1.1.1.cmml">â¢</mo><mi id="S4.SS2.p3.2.m2.1.1.14" xref="S4.SS2.p3.2.m2.1.1.14.cmml">t</mi><mo id="S4.SS2.p3.2.m2.1.1.1l" xref="S4.SS2.p3.2.m2.1.1.1.cmml">â¢</mo><mi id="S4.SS2.p3.2.m2.1.1.15" xref="S4.SS2.p3.2.m2.1.1.15.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><apply id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1"><times id="S4.SS2.p3.2.m2.1.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1.1"></times><ci id="S4.SS2.p3.2.m2.1.1.2.cmml" xref="S4.SS2.p3.2.m2.1.1.2">ğ‘œ</ci><ci id="S4.SS2.p3.2.m2.1.1.3.cmml" xref="S4.SS2.p3.2.m2.1.1.3">ğ‘Ÿ</ci><ci id="S4.SS2.p3.2.m2.1.1.4.cmml" xref="S4.SS2.p3.2.m2.1.1.4">ğ‘”</ci><ci id="S4.SS2.p3.2.m2.1.1.5.cmml" xref="S4.SS2.p3.2.m2.1.1.5">ğ‘</ci><ci id="S4.SS2.p3.2.m2.1.1.6.cmml" xref="S4.SS2.p3.2.m2.1.1.6">ğ‘›</ci><ci id="S4.SS2.p3.2.m2.1.1.7.cmml" xref="S4.SS2.p3.2.m2.1.1.7">ğ‘–</ci><ci id="S4.SS2.p3.2.m2.1.1.8.cmml" xref="S4.SS2.p3.2.m2.1.1.8">ğ‘</ci><ci id="S4.SS2.p3.2.m2.1.1.9.cmml" xref="S4.SS2.p3.2.m2.1.1.9">ğ‘Ÿ</ci><ci id="S4.SS2.p3.2.m2.1.1.10.cmml" xref="S4.SS2.p3.2.m2.1.1.10">ğ‘’</ci><ci id="S4.SS2.p3.2.m2.1.1.11.cmml" xref="S4.SS2.p3.2.m2.1.1.11">ğ‘ </ci><ci id="S4.SS2.p3.2.m2.1.1.12.cmml" xref="S4.SS2.p3.2.m2.1.1.12">ğ‘¢</ci><ci id="S4.SS2.p3.2.m2.1.1.13.cmml" xref="S4.SS2.p3.2.m2.1.1.13">ğ‘™</ci><ci id="S4.SS2.p3.2.m2.1.1.14.cmml" xref="S4.SS2.p3.2.m2.1.1.14">ğ‘¡</ci><ci id="S4.SS2.p3.2.m2.1.1.15.cmml" xref="S4.SS2.p3.2.m2.1.1.15">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">organic\;results</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.2.m2.1d">italic_o italic_r italic_g italic_a italic_n italic_i italic_c italic_r italic_e italic_s italic_u italic_l italic_t italic_s</annotation></semantics></math>" as the main search output.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">For wiki and the second case of Google, we select the top 3 most similar to the query ranked by the pre-trained Sentence BERT model<cite class="ltx_cite ltx_citemacro_cite">Reimers and Gurevych (<a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib30" title="">2019</a>)</cite> as context.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Baseline</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">To provide a comprehensive evaluation of our framework, we compare it against the following baselines. (i) <span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.1">Standard Prediction (Standard)</span>: This baseline involves directly predicting the label based on the input, utilizing the same number of in-context learning examples as our framework. (ii) <span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.2">Original Chain-of-Thought (CoT)<cite class="ltx_cite ltx_citemacro_cite">Wei etÂ al. <span class="ltx_text ltx_font_medium" id="S4.SS3.p1.1.2.1.1.1.1">(</span><a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib38" title="">2023</a><span class="ltx_text ltx_font_medium" id="S4.SS3.p1.1.2.2.2.2.1">)</span></cite></span>: This approach predicts the label after generating an explanatory chain-of-thought. It helps in understanding the modelâ€™s reasoning process and its impact on the final prediction. (iii) <span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.3">Retrieve-then-Read</span>: This is the standard retrieval-augmented method where retrieved documents are concatenated with the question to form the input. This baseline allows us to measure the performance gains from our dual-focus approach compared to traditional retrieval-augmented methods.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.3.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T2.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.3.1.1.1.1">Datasets</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="2" id="S4.T2.3.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T2.3.1.1.2.1">HotPotQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="2" id="S4.T2.3.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T2.3.1.1.3.1">2Wiki.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="2" id="S4.T2.3.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T2.3.1.1.4.1">StrategyQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="2" id="S4.T2.3.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T2.3.1.1.5.1">FEVER</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T2.3.1.1.6"><span class="ltx_text ltx_font_bold" id="S4.T2.3.1.1.6.1">ChinesePoetry</span></th>
</tr>
<tr class="ltx_tr" id="S4.T2.3.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" id="S4.T2.3.2.2.1"><span class="ltx_text ltx_font_bold" id="S4.T2.3.2.2.1.1">Metrics</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.3.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.T2.3.2.2.2.1">EM</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T2.3.2.2.3"><span class="ltx_text ltx_font_bold" id="S4.T2.3.2.2.3.1">F1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.3.2.2.4"><span class="ltx_text ltx_font_bold" id="S4.T2.3.2.2.4.1">EM</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T2.3.2.2.5"><span class="ltx_text ltx_font_bold" id="S4.T2.3.2.2.5.1">F1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.3.2.2.6"><span class="ltx_text ltx_font_bold" id="S4.T2.3.2.2.6.1">EM</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T2.3.2.2.7"><span class="ltx_text ltx_font_bold" id="S4.T2.3.2.2.7.1">F1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.3.2.2.8"><span class="ltx_text ltx_font_bold" id="S4.T2.3.2.2.8.1">EM</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T2.3.2.2.9"><span class="ltx_text ltx_font_bold" id="S4.T2.3.2.2.9.1">F1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.3.2.2.10"><span class="ltx_text ltx_font_bold" id="S4.T2.3.2.2.10.1">EM</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.3.2.2.11"><span class="ltx_text ltx_font_bold" id="S4.T2.3.2.2.11.1">F1</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.3.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.3.3.1.1">Standard</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.3.1.2">42.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.3.3.1.3">51.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.3.1.4">36.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.3.3.1.5">54.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.3.1.6">40.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.3.3.1.7">57.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.3.1.8">37.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.3.3.1.9">50.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.3.1.10">36.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.3.1.11">39.3</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.3.4.2.1">CoT</th>
<td class="ltx_td ltx_align_center" id="S4.T2.3.4.2.2">47.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.3.4.2.3">59.7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.4.2.4">40.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.3.4.2.5">60.2</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.4.2.6">44.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.3.4.2.7">60.1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.4.2.8">42.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.3.4.2.9">57.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.4.2.10">40.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.4.2.11">49.2</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.3.5.3.1">Retrieve-then-Read</th>
<td class="ltx_td ltx_align_center" id="S4.T2.3.5.3.2">52.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.3.5.3.3"><span class="ltx_text ltx_font_bold" id="S4.T2.3.5.3.3.1">66.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.5.3.4">42.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.3.5.3.5">69.3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.5.3.6">37.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.3.5.3.7">50.7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.5.3.8">40.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.3.5.3.9"><span class="ltx_text ltx_font_bold" id="S4.T2.3.5.3.9.1">59.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.5.3.10"><span class="ltx_text ltx_font_bold" id="S4.T2.3.5.3.10.1">69.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.5.3.11"><span class="ltx_text ltx_font_bold" id="S4.T2.3.5.3.11.1">76.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T2.3.6.4.1"><span class="ltx_text" id="S4.T2.3.6.4.1.1">Think-then-Act(ours)</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.3.6.4.2"><span class="ltx_text ltx_font_bold" id="S4.T2.3.6.4.2.1">56.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.3.6.4.3">65.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.3.6.4.4"><span class="ltx_text ltx_font_bold" id="S4.T2.3.6.4.4.1">52.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.3.6.4.5"><span class="ltx_text ltx_font_bold" id="S4.T2.3.6.4.5.1">69.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.3.6.4.6"><span class="ltx_text ltx_font_bold" id="S4.T2.3.6.4.6.1">62.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.3.6.4.7"><span class="ltx_text ltx_font_bold" id="S4.T2.3.6.4.7.1">71.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.3.6.4.8"><span class="ltx_text ltx_font_bold" id="S4.T2.3.6.4.8.1">53.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.3.6.4.9">55.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.3.6.4.10">68.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.3.6.4.11">70.1</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span> Comparison between Think-then-Act (<math alttext="\beta^{\prime}=0.5" class="ltx_Math" display="inline" id="S4.T2.2.m1.1"><semantics id="S4.T2.2.m1.1b"><mrow id="S4.T2.2.m1.1.1" xref="S4.T2.2.m1.1.1.cmml"><msup id="S4.T2.2.m1.1.1.2" xref="S4.T2.2.m1.1.1.2.cmml"><mi id="S4.T2.2.m1.1.1.2.2" xref="S4.T2.2.m1.1.1.2.2.cmml">Î²</mi><mo id="S4.T2.2.m1.1.1.2.3" xref="S4.T2.2.m1.1.1.2.3.cmml">â€²</mo></msup><mo id="S4.T2.2.m1.1.1.1" xref="S4.T2.2.m1.1.1.1.cmml">=</mo><mn id="S4.T2.2.m1.1.1.3" xref="S4.T2.2.m1.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.2.m1.1c"><apply id="S4.T2.2.m1.1.1.cmml" xref="S4.T2.2.m1.1.1"><eq id="S4.T2.2.m1.1.1.1.cmml" xref="S4.T2.2.m1.1.1.1"></eq><apply id="S4.T2.2.m1.1.1.2.cmml" xref="S4.T2.2.m1.1.1.2"><csymbol cd="ambiguous" id="S4.T2.2.m1.1.1.2.1.cmml" xref="S4.T2.2.m1.1.1.2">superscript</csymbol><ci id="S4.T2.2.m1.1.1.2.2.cmml" xref="S4.T2.2.m1.1.1.2.2">ğ›½</ci><ci id="S4.T2.2.m1.1.1.2.3.cmml" xref="S4.T2.2.m1.1.1.2.3">â€²</ci></apply><cn id="S4.T2.2.m1.1.1.3.cmml" type="float" xref="S4.T2.2.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.m1.1d">\beta^{\prime}=0.5</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.m1.1e">italic_Î² start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT = 0.5</annotation></semantics></math>) and baselines on all datastes using EM &amp; F1. </figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental Results</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We first report the overall results across the selected tasks and datasets, comparing the performance of the <span class="ltx_text ltx_font_italic" id="S5.p1.1.1">Think-then-Act</span> framework with all the baselines introduced in SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S4.SS3" title="4.3 Baseline â€£ 4 Experimental Setup â€£ Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">4.3</span></a>. We then conduct ablation experiments to study the efficacy of various design choices within our method. This structured analysis allows us to thoroughly evaluate the strengths and areas for improvement in our approach.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Comparison with Baselines</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S4.F2" title="Figure 2 â€£ 4.1.4 Domain QA â€£ 4.1 Task Settings â€£ 4 Experimental Setup â€£ Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">2</span></a> displays the performance comparison of the <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.1">Think-then-Act</span> framework against the baselines across various tasks and datasets. Our framework generally outperforms the baselines, indicating its superior capability in enhancing retrieval-augmented generation.</p>
</div>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="464" id="S5.F3.g1" src="x3.png" width="832"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Comparison of Think-then-Act and Retrieval-then-Read(<math alttext="\beta^{\prime}=0.5" class="ltx_Math" display="inline" id="S5.F3.2.m1.1"><semantics id="S5.F3.2.m1.1b"><mrow id="S5.F3.2.m1.1.1" xref="S5.F3.2.m1.1.1.cmml"><msup id="S5.F3.2.m1.1.1.2" xref="S5.F3.2.m1.1.1.2.cmml"><mi id="S5.F3.2.m1.1.1.2.2" xref="S5.F3.2.m1.1.1.2.2.cmml">Î²</mi><mo id="S5.F3.2.m1.1.1.2.3" xref="S5.F3.2.m1.1.1.2.3.cmml">â€²</mo></msup><mo id="S5.F3.2.m1.1.1.1" xref="S5.F3.2.m1.1.1.1.cmml">=</mo><mn id="S5.F3.2.m1.1.1.3" xref="S5.F3.2.m1.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.F3.2.m1.1c"><apply id="S5.F3.2.m1.1.1.cmml" xref="S5.F3.2.m1.1.1"><eq id="S5.F3.2.m1.1.1.1.cmml" xref="S5.F3.2.m1.1.1.1"></eq><apply id="S5.F3.2.m1.1.1.2.cmml" xref="S5.F3.2.m1.1.1.2"><csymbol cd="ambiguous" id="S5.F3.2.m1.1.1.2.1.cmml" xref="S5.F3.2.m1.1.1.2">superscript</csymbol><ci id="S5.F3.2.m1.1.1.2.2.cmml" xref="S5.F3.2.m1.1.1.2.2">ğ›½</ci><ci id="S5.F3.2.m1.1.1.2.3.cmml" xref="S5.F3.2.m1.1.1.2.3">â€²</ci></apply><cn id="S5.F3.2.m1.1.1.3.cmml" type="float" xref="S5.F3.2.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F3.2.m1.1d">\beta^{\prime}=0.5</annotation><annotation encoding="application/x-llamapun" id="S5.F3.2.m1.1e">italic_Î² start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT = 0.5</annotation></semantics></math>) on the Chinese Poetry dataset: generation accuracy(blue) and retrieval ratio(red).</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.1">Comparisons on StrategyQA:</span> The most notable improvement is observed in StrategyQA, shown in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S5.T3" title="Table 3 â€£ 5.1 Comparison with Baselines â€£ 5 Experimental Results â€£ Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">3</span></a>. The Chain of Thought(CoT)method <cite class="ltx_cite ltx_citemacro_citep">(Wei etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib38" title="">2023</a>)</cite>, which involves deeper question analysis, outperforms direct retrieval methods. This is directly related to the characteristics of the StrategyQA dataset. For commonsense reasoning tasks, deeply analyzing and understanding the question is more crucial than acquiring additional information, few example in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S3.T1" title="Table 1 â€£ 3.2.1 Information Retrieval â€£ 3.2 Model Capability Check â€£ 3 Methodology â€£ Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">1</span></a>. This trend is similarly observed in the FEVER dataset, where accurate fact verification benefits more from a thorough understanding of the query rather than from additional data.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<div class="ltx_inline-block ltx_transformed_outer" id="S5.T3.3" style="width:433.6pt;height:136.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(102.3pt,-32.2pt) scale(1.89376347382205,1.89376347382205) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T3.3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T3.3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T3.3.1.1.1.1.1">Datasets</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2" id="S5.T3.3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T3.3.1.1.1.2.1">StrategyQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2" id="S5.T3.3.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T3.3.1.1.1.3.1">FEVER</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.3.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S5.T3.3.1.2.1.1.1">Metrics</span></th>
<td class="ltx_td ltx_align_center" id="S5.T3.3.1.2.1.2"><span class="ltx_text ltx_font_bold" id="S5.T3.3.1.2.1.2.1">EM</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.3.1.2.1.3"><span class="ltx_text ltx_font_bold" id="S5.T3.3.1.2.1.3.1">F1</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.1.2.1.4"><span class="ltx_text ltx_font_bold" id="S5.T3.3.1.2.1.4.1">EM</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.1.2.1.5"><span class="ltx_text ltx_font_bold" id="S5.T3.3.1.2.1.5.1">F1</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T3.3.1.3.2.1">Retrieve-then-Read</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.3.1.3.2.2">37.5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T3.3.1.3.2.3">50.7</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.3.1.3.2.4">40.3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.3.1.3.2.5"><span class="ltx_text ltx_font_bold" id="S5.T3.3.1.3.2.5.1">59.2</span></th>
</tr>
<tr class="ltx_tr" id="S5.T3.3.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S5.T3.3.1.4.3.1"><span class="ltx_text" id="S5.T3.3.1.4.3.1.1">Think-then-Act(ours)</span></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.3.1.4.3.2"><span class="ltx_text ltx_font_bold" id="S5.T3.3.1.4.3.2.1">62.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T3.3.1.4.3.3"><span class="ltx_text ltx_font_bold" id="S5.T3.3.1.4.3.3.1">71.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.3.1.4.3.4"><span class="ltx_text ltx_font_bold" id="S5.T3.3.1.4.3.4.1">53.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.3.1.4.3.5">55.7</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison between Think-then-Act (<math alttext="\beta^{\prime}=0.5" class="ltx_Math" display="inline" id="S5.T3.2.m1.1"><semantics id="S5.T3.2.m1.1b"><mrow id="S5.T3.2.m1.1.1" xref="S5.T3.2.m1.1.1.cmml"><msup id="S5.T3.2.m1.1.1.2" xref="S5.T3.2.m1.1.1.2.cmml"><mi id="S5.T3.2.m1.1.1.2.2" xref="S5.T3.2.m1.1.1.2.2.cmml">Î²</mi><mo id="S5.T3.2.m1.1.1.2.3" xref="S5.T3.2.m1.1.1.2.3.cmml">â€²</mo></msup><mo id="S5.T3.2.m1.1.1.1" xref="S5.T3.2.m1.1.1.1.cmml">=</mo><mn id="S5.T3.2.m1.1.1.3" xref="S5.T3.2.m1.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.2.m1.1c"><apply id="S5.T3.2.m1.1.1.cmml" xref="S5.T3.2.m1.1.1"><eq id="S5.T3.2.m1.1.1.1.cmml" xref="S5.T3.2.m1.1.1.1"></eq><apply id="S5.T3.2.m1.1.1.2.cmml" xref="S5.T3.2.m1.1.1.2"><csymbol cd="ambiguous" id="S5.T3.2.m1.1.1.2.1.cmml" xref="S5.T3.2.m1.1.1.2">superscript</csymbol><ci id="S5.T3.2.m1.1.1.2.2.cmml" xref="S5.T3.2.m1.1.1.2.2">ğ›½</ci><ci id="S5.T3.2.m1.1.1.2.3.cmml" xref="S5.T3.2.m1.1.1.2.3">â€²</ci></apply><cn id="S5.T3.2.m1.1.1.3.cmml" type="float" xref="S5.T3.2.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.m1.1d">\beta^{\prime}=0.5</annotation><annotation encoding="application/x-llamapun" id="S5.T3.2.m1.1e">italic_Î² start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT = 0.5</annotation></semantics></math>) and Retrieve-then-Read on all StrategyQA and FEVER</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p3.1.1">Comparisons on ChinesePoetry:</span> On our custom ChinesePoetry dataset, our method performs comparably to the Retrieve-then-Read baseline. This can be attributed to the clarity and completeness of the questions in this dataset, where additional information retrieval significantly enhances accuracy. However, unlike the baseline method that retrieves information for all queries, our approach first assesses the modelâ€™s capability before deciding whether retrieval is necessary. As shown in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S5.F3" title="Figure 3 â€£ 5.1 Comparison with Baselines â€£ 5 Experimental Results â€£ Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">3</span></a>, our method retrieves information for only 36.8% of the questions, achieving the same effectiveness as retrieving for 100% of the queries. This selective retrieval significantly reduces computational costs.</p>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1">We report all metrics for the every baselines in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S4.T2" title="Table 2 â€£ 4.3 Baseline â€£ 4 Experimental Setup â€£ Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">2</span></a>, highlight the performance metrics (EM and F1 scores) for different methods across various datasets. Our <span class="ltx_text ltx_font_italic" id="S5.SS1.p4.1.1">Think-then-Act</span> framework consistently demonstrates superior performance, particularly in tasks requiring complex reasoning and fact verification. Notably, it achieves the highest EM scores in HotPotQA (56.9), 2WikiMultihopQA (52.6), StrategyQA (62.9), and FEVER (53.9), showcasing its robustness and adaptability. The frameworkâ€™s comparable performance in the ChinesePoetry dataset (EM: 68.6) against the Retrieve-then-Read baseline (EM: 69.9) further illustrates its efficiency in handling domain-specific tasks with reduced computational overhead.</p>
</div>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="387" id="S5.F4.g1" src="x4.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>the Exact Match (EM) scores across various datasets with different <math alttext="\beta^{\prime}" class="ltx_Math" display="inline" id="S5.F4.2.m1.1"><semantics id="S5.F4.2.m1.1b"><msup id="S5.F4.2.m1.1.1" xref="S5.F4.2.m1.1.1.cmml"><mi id="S5.F4.2.m1.1.1.2" xref="S5.F4.2.m1.1.1.2.cmml">Î²</mi><mo id="S5.F4.2.m1.1.1.3" xref="S5.F4.2.m1.1.1.3.cmml">â€²</mo></msup><annotation-xml encoding="MathML-Content" id="S5.F4.2.m1.1c"><apply id="S5.F4.2.m1.1.1.cmml" xref="S5.F4.2.m1.1.1"><csymbol cd="ambiguous" id="S5.F4.2.m1.1.1.1.cmml" xref="S5.F4.2.m1.1.1">superscript</csymbol><ci id="S5.F4.2.m1.1.1.2.cmml" xref="S5.F4.2.m1.1.1.2">ğ›½</ci><ci id="S5.F4.2.m1.1.1.3.cmml" xref="S5.F4.2.m1.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.2.m1.1d">\beta^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S5.F4.2.m1.1e">italic_Î² start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT</annotation></semantics></math> values</figcaption>
</figure>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="464" id="S5.F5.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Comparison of Think-then-Act(<math alttext="\beta^{\prime}=0.5" class="ltx_Math" display="inline" id="S5.F5.2.m1.1"><semantics id="S5.F5.2.m1.1b"><mrow id="S5.F5.2.m1.1.1" xref="S5.F5.2.m1.1.1.cmml"><msup id="S5.F5.2.m1.1.1.2" xref="S5.F5.2.m1.1.1.2.cmml"><mi id="S5.F5.2.m1.1.1.2.2" xref="S5.F5.2.m1.1.1.2.2.cmml">Î²</mi><mo id="S5.F5.2.m1.1.1.2.3" xref="S5.F5.2.m1.1.1.2.3.cmml">â€²</mo></msup><mo id="S5.F5.2.m1.1.1.1" xref="S5.F5.2.m1.1.1.1.cmml">=</mo><mn id="S5.F5.2.m1.1.1.3" xref="S5.F5.2.m1.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.F5.2.m1.1c"><apply id="S5.F5.2.m1.1.1.cmml" xref="S5.F5.2.m1.1.1"><eq id="S5.F5.2.m1.1.1.1.cmml" xref="S5.F5.2.m1.1.1.1"></eq><apply id="S5.F5.2.m1.1.1.2.cmml" xref="S5.F5.2.m1.1.1.2"><csymbol cd="ambiguous" id="S5.F5.2.m1.1.1.2.1.cmml" xref="S5.F5.2.m1.1.1.2">superscript</csymbol><ci id="S5.F5.2.m1.1.1.2.2.cmml" xref="S5.F5.2.m1.1.1.2.2">ğ›½</ci><ci id="S5.F5.2.m1.1.1.2.3.cmml" xref="S5.F5.2.m1.1.1.2.3">â€²</ci></apply><cn id="S5.F5.2.m1.1.1.3.cmml" type="float" xref="S5.F5.2.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.2.m1.1d">\beta^{\prime}=0.5</annotation><annotation encoding="application/x-llamapun" id="S5.F5.2.m1.1e">italic_Î² start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT = 0.5</annotation></semantics></math>, LM postprocessing) and FLARE(LM preprocessing) on the Chinese Poetry dataset: generation accuracy(blue) and retrieval ratio(red).</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Ablation Study</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Our framework <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.1">Think-then-Act</span>, primarily consists of two components: query assessment and model capability check. Unlike traditional approaches, we do not omit these parts separately to demonstrate their effectiveness, as previous studies have already established their importance. Instead, our ablation study focuses on two main experiments to validate the design choices and their impact on performance.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.2"><span class="ltx_text ltx_font_bold" id="S5.SS2.p2.1.1">Impact of Different <math alttext="\boldsymbol{\beta^{\prime}}" class="ltx_Math" display="inline" id="S5.SS2.p2.1.1.m1.1"><semantics id="S5.SS2.p2.1.1.m1.1a"><msup id="S5.SS2.p2.1.1.m1.1.1" xref="S5.SS2.p2.1.1.m1.1.1.cmml"><mi id="S5.SS2.p2.1.1.m1.1.1.2" xref="S5.SS2.p2.1.1.m1.1.1.2.cmml">Î²</mi><mo class="ltx_mathvariant_bold" id="S5.SS2.p2.1.1.m1.1.1.3" mathvariant="bold" xref="S5.SS2.p2.1.1.m1.1.1.3.cmml">â€²</mo></msup><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.1.m1.1b"><apply id="S5.SS2.p2.1.1.m1.1.1.cmml" xref="S5.SS2.p2.1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.1.1.m1.1.1.1.cmml" xref="S5.SS2.p2.1.1.m1.1.1">superscript</csymbol><ci id="S5.SS2.p2.1.1.m1.1.1.2.cmml" xref="S5.SS2.p2.1.1.m1.1.1.2">ğ›½</ci><ci id="S5.SS2.p2.1.1.m1.1.1.3.cmml" xref="S5.SS2.p2.1.1.m1.1.1.3">bold-â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.1.m1.1c">\boldsymbol{\beta^{\prime}}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.1.1.m1.1d">bold_italic_Î² start_POSTSUPERSCRIPT bold_â€² end_POSTSUPERSCRIPT</annotation></semantics></math>:</span> As mentioned in SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S3.SS2" title="3.2 Model Capability Check â€£ 3 Methodology â€£ Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">3.2</span></a>, we examine how varying the threshold <math alttext="\beta^{\prime}" class="ltx_Math" display="inline" id="S5.SS2.p2.2.m1.1"><semantics id="S5.SS2.p2.2.m1.1a"><msup id="S5.SS2.p2.2.m1.1.1" xref="S5.SS2.p2.2.m1.1.1.cmml"><mi id="S5.SS2.p2.2.m1.1.1.2" xref="S5.SS2.p2.2.m1.1.1.2.cmml">Î²</mi><mo id="S5.SS2.p2.2.m1.1.1.3" xref="S5.SS2.p2.2.m1.1.1.3.cmml">â€²</mo></msup><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.2.m1.1b"><apply id="S5.SS2.p2.2.m1.1.1.cmml" xref="S5.SS2.p2.2.m1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.2.m1.1.1.1.cmml" xref="S5.SS2.p2.2.m1.1.1">superscript</csymbol><ci id="S5.SS2.p2.2.m1.1.1.2.cmml" xref="S5.SS2.p2.2.m1.1.1.2">ğ›½</ci><ci id="S5.SS2.p2.2.m1.1.1.3.cmml" xref="S5.SS2.p2.2.m1.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.2.m1.1c">\beta^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.2.m1.1d">italic_Î² start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT</annotation></semantics></math> for model confidence affects the results. This helps us understand the optimal threshold for balancing retrieval necessity and model confidence.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.7">FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S5.F4" title="Figure 4 â€£ 5.1 Comparison with Baselines â€£ 5 Experimental Results â€£ Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">4</span></a> displays the Exact Match (EM) scores across various datasets with different <math alttext="\beta^{\prime}" class="ltx_Math" display="inline" id="S5.SS2.p3.1.m1.1"><semantics id="S5.SS2.p3.1.m1.1a"><msup id="S5.SS2.p3.1.m1.1.1" xref="S5.SS2.p3.1.m1.1.1.cmml"><mi id="S5.SS2.p3.1.m1.1.1.2" xref="S5.SS2.p3.1.m1.1.1.2.cmml">Î²</mi><mo id="S5.SS2.p3.1.m1.1.1.3" xref="S5.SS2.p3.1.m1.1.1.3.cmml">â€²</mo></msup><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.1.m1.1b"><apply id="S5.SS2.p3.1.m1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p3.1.m1.1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1">superscript</csymbol><ci id="S5.SS2.p3.1.m1.1.1.2.cmml" xref="S5.SS2.p3.1.m1.1.1.2">ğ›½</ci><ci id="S5.SS2.p3.1.m1.1.1.3.cmml" xref="S5.SS2.p3.1.m1.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.1.m1.1c">\beta^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p3.1.m1.1d">italic_Î² start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT</annotation></semantics></math> values. As observed, the performance improves significantly when <math alttext="\beta^{\prime}" class="ltx_Math" display="inline" id="S5.SS2.p3.2.m2.1"><semantics id="S5.SS2.p3.2.m2.1a"><msup id="S5.SS2.p3.2.m2.1.1" xref="S5.SS2.p3.2.m2.1.1.cmml"><mi id="S5.SS2.p3.2.m2.1.1.2" xref="S5.SS2.p3.2.m2.1.1.2.cmml">Î²</mi><mo id="S5.SS2.p3.2.m2.1.1.3" xref="S5.SS2.p3.2.m2.1.1.3.cmml">â€²</mo></msup><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.2.m2.1b"><apply id="S5.SS2.p3.2.m2.1.1.cmml" xref="S5.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS2.p3.2.m2.1.1.1.cmml" xref="S5.SS2.p3.2.m2.1.1">superscript</csymbol><ci id="S5.SS2.p3.2.m2.1.1.2.cmml" xref="S5.SS2.p3.2.m2.1.1.2">ğ›½</ci><ci id="S5.SS2.p3.2.m2.1.1.3.cmml" xref="S5.SS2.p3.2.m2.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.2.m2.1c">\beta^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p3.2.m2.1d">italic_Î² start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT</annotation></semantics></math> is increased from <math alttext="0.0" class="ltx_Math" display="inline" id="S5.SS2.p3.3.m3.1"><semantics id="S5.SS2.p3.3.m3.1a"><mn id="S5.SS2.p3.3.m3.1.1" xref="S5.SS2.p3.3.m3.1.1.cmml">0.0</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.3.m3.1b"><cn id="S5.SS2.p3.3.m3.1.1.cmml" type="float" xref="S5.SS2.p3.3.m3.1.1">0.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.3.m3.1c">0.0</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p3.3.m3.1d">0.0</annotation></semantics></math> to <math alttext="0.5" class="ltx_Math" display="inline" id="S5.SS2.p3.4.m4.1"><semantics id="S5.SS2.p3.4.m4.1a"><mn id="S5.SS2.p3.4.m4.1.1" xref="S5.SS2.p3.4.m4.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.4.m4.1b"><cn id="S5.SS2.p3.4.m4.1.1.cmml" type="float" xref="S5.SS2.p3.4.m4.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.4.m4.1c">0.5</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p3.4.m4.1d">0.5</annotation></semantics></math>. Beyond <math alttext="0.5" class="ltx_Math" display="inline" id="S5.SS2.p3.5.m5.1"><semantics id="S5.SS2.p3.5.m5.1a"><mn id="S5.SS2.p3.5.m5.1.1" xref="S5.SS2.p3.5.m5.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.5.m5.1b"><cn id="S5.SS2.p3.5.m5.1.1.cmml" type="float" xref="S5.SS2.p3.5.m5.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.5.m5.1c">0.5</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p3.5.m5.1d">0.5</annotation></semantics></math>, the improvement plateaus, indicating diminishing returns. Therefore, we set <math alttext="\beta^{\prime}" class="ltx_Math" display="inline" id="S5.SS2.p3.6.m6.1"><semantics id="S5.SS2.p3.6.m6.1a"><msup id="S5.SS2.p3.6.m6.1.1" xref="S5.SS2.p3.6.m6.1.1.cmml"><mi id="S5.SS2.p3.6.m6.1.1.2" xref="S5.SS2.p3.6.m6.1.1.2.cmml">Î²</mi><mo id="S5.SS2.p3.6.m6.1.1.3" xref="S5.SS2.p3.6.m6.1.1.3.cmml">â€²</mo></msup><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.6.m6.1b"><apply id="S5.SS2.p3.6.m6.1.1.cmml" xref="S5.SS2.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S5.SS2.p3.6.m6.1.1.1.cmml" xref="S5.SS2.p3.6.m6.1.1">superscript</csymbol><ci id="S5.SS2.p3.6.m6.1.1.2.cmml" xref="S5.SS2.p3.6.m6.1.1.2">ğ›½</ci><ci id="S5.SS2.p3.6.m6.1.1.3.cmml" xref="S5.SS2.p3.6.m6.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.6.m6.1c">\beta^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p3.6.m6.1d">italic_Î² start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT</annotation></semantics></math> to <math alttext="0.5" class="ltx_Math" display="inline" id="S5.SS2.p3.7.m7.1"><semantics id="S5.SS2.p3.7.m7.1a"><mn id="S5.SS2.p3.7.m7.1.1" xref="S5.SS2.p3.7.m7.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.7.m7.1b"><cn id="S5.SS2.p3.7.m7.1.1.cmml" type="float" xref="S5.SS2.p3.7.m7.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.7.m7.1c">0.5</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p3.7.m7.1d">0.5</annotation></semantics></math> for optimal performance, balancing the trade-off between retrieval and self-reliance of the model.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p4.1.1">Preprocess and Postprocess:</span> We compare our framework with a method inspired by FLARE <cite class="ltx_cite ltx_citemacro_citep">(Jiang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib16" title="">2023</a>)</cite>, where the LM generates a temporary next sentence and checks the token probabilities before deciding on retrieval. We refer to this as the <span class="ltx_text ltx_font_italic" id="S5.SS2.p4.1.2">LM preprocess</span> approach. In contrast, our framework first assesses whether retrieval is needed and then generates the response, which we term as the <span class="ltx_text ltx_font_italic" id="S5.SS2.p4.1.3">LM postprocess</span> approach.</p>
</div>
<div class="ltx_para" id="S5.SS2.p5">
<p class="ltx_p" id="S5.SS2.p5.1">FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#S5.F5" title="Figure 5 â€£ 5.1 Comparison with Baselines â€£ 5 Experimental Results â€£ Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">5</span></a> illustrates the comparison between LM preprocessing and LM postprocessing. The results show that our postprocessing approach achieves a comparable EM score (68.6%) to the preprocessing approach (69.9%), but with a significantly lower retrieval ratio (36.8% vs. 77.3%). This indicates that our method is more efficient, reducing the number of retrievals required while maintaining similar performance. Consequently, this leads to faster processing times and enhanced resource efficiency.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">This paper presents the <span class="ltx_text ltx_font_italic" id="S6.p1.1.1">Think-then-Act</span> framework, enhancing retrieval-augmented generation by combining query transformation and model capability assessment. Our approach evaluates query clarity and model confidence, triggering retrieval only when necessary, improving accuracy, and optimizing resources. Experiments on datasets including MultihopQA, Commonsense Reasoning, FEVER, and a custom Chinese poetry dataset show significant improvements over baselines. The framework proves effective in both English and non-English contexts. Ablation studies confirmed the optimal model confidence threshold and highlighted efficiency gains from our approach. The <span class="ltx_text ltx_font_italic" id="S6.p1.1.2">Think-then-Act</span> framework offers a robust solution for enhancing retrieval-augmented generation, paving the way for more accurate and efficient LLM applications. Future work will refine query assessment and extend the framework to additional languages and domains.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Limitations</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">While the <span class="ltx_text ltx_font_italic" id="S7.p1.1.1">Think-then-Act</span> framework has demonstrated promising results, several limitations need to be addressed in future research.
Firstly, our study exclusively utilized black-box models, such as GPT-3.5, which necessitate API calls for each interaction. This approach incurs significant costs and poses potential security risks due to the transmission of data over external servers. Inspired by <cite class="ltx_cite ltx_citemacro_citep">(Asai etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.13050v1#bib.bib1" title="">2023</a>)</cite>, future work could focus on fine-tuning a white-box model based on our framework. This would enable local deployment, thereby reducing costs and enhancing data security by processing all information internally.
Secondly, although we conducted experiments on five distinct datasets, each experiment was isolated to a single type of dataset. This approach does not fully capture the versatility and robustness of our framework across mixed-type scenarios. Future research should investigate the performance of our framework on more diverse and comprehensive datasets that incorporate various types of questions and contexts within a single dataset. This would provide a more rigorous validation of the frameworkâ€™s effectiveness and adaptability in real-world applications.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asai etÂ al. (2023)</span>
<span class="ltx_bibblock">
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2310.11511" title="">Self-rag: Learning to retrieve, generate, and critique through self-reflection</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Preprint</em>, arXiv:2310.11511.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, QuyetÂ V. Do, Yan Xu, and Pascale Fung. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.ijcnlp-main.45" title="">A multitask, multilingual, multimodal evaluation of ChatGPT on reasoning, hallucination, and interactivity</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 675â€“718, Nusa Dua, Bali. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown etÂ al. (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, JaredÂ D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, etÂ al. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Advances in neural information processing systems</em>, 33:1877â€“1901.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2017)</span>
<span class="ltx_bibblock">
Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P17-1171" title="">Reading Wikipedia to answer open-domain questions</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1870â€“1879, Vancouver, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhuliawala etÂ al. (2023)</span>
<span class="ltx_bibblock">
Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2309.11495" title="">Chain-of-verification reduces hallucination in large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Preprint</em>, arXiv:2309.11495.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al. (2022)</span>
<span class="ltx_bibblock">
Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2212.10496" title="">Precise zero-shot dense retrieval without relevance labels</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Preprint</em>, arXiv:2212.10496.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al. (2024)</span>
<span class="ltx_bibblock">
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, YiÂ Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2312.10997" title="">Retrieval-augmented generation for large language models: A survey</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Preprint</em>, arXiv:2312.10997.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geva etÂ al. (2021)</span>
<span class="ltx_bibblock">
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021.

</span>
<span class="ltx_bibblock">Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Transactions of the Association for Computational Linguistics</em>, 9:346â€“361.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guu etÂ al. (2020)</span>
<span class="ltx_bibblock">
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2002.08909" title="">Realm: Retrieval-augmented language model pre-training</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Preprint</em>, arXiv:2002.08909.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho etÂ al. (2020)</span>
<span class="ltx_bibblock">
Xanh Ho, Anh-Khoa DuongÂ Nguyen, Saku Sugawara, and Akiko Aizawa. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.coling-main.580" title="">Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the 28th International Conference on Computational Linguistics</em>, pages 6609â€“6625, Barcelona, Spain (Online). International Committee on Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard and Grave (2021)</span>
<span class="ltx_bibblock">
Gautier Izacard and Edouard Grave. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.eacl-main.74" title="">Leveraging passage retrieval with generative models for open domain question answering</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</em>, pages 874â€“880, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard etÂ al. (2023)</span>
<span class="ltx_bibblock">
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2023.

</span>
<span class="ltx_bibblock">Atlas: Few-shot learning with retrieval augmented language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Journal of Machine Learning Research</em>, 24(251):1â€“43.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jagerman etÂ al. (2023)</span>
<span class="ltx_bibblock">
Rolf Jagerman, Honglei Zhuang, Zhen Qin, Xuanhui Wang, and Michael Bendersky. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2305.03653" title="">Query expansion by prompting large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Preprint</em>, arXiv:2305.03653.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji etÂ al. (2023)</span>
<span class="ltx_bibblock">
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, YeÂ Jin Bang, Andrea Madotto, and Pascale Fung. 2023.

</span>
<span class="ltx_bibblock">Survey of hallucination in natural language generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">ACM Computing Surveys</em>, 55(12):1â€“38.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Zhengbao Jiang, Luyu Gao, Jun Araki, Haibo Ding, Zhiruo Wang, Jamie Callan, and Graham Neubig. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2212.02027" title="">Retrieval as attention: End-to-end learning of retrieval and reading within a single transformer</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Preprint</em>, arXiv:2212.02027.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Zhengbao Jiang, FrankÂ F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2305.06983" title="">Active retrieval augmented generation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Preprint</em>, arXiv:2305.06983.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kasai etÂ al. (2024)</span>
<span class="ltx_bibblock">
Jungo Kasai, Keisuke Sakaguchi, Ronan LeÂ Bras, Akari Asai, Xinyan Yu, Dragomir Radev, NoahÂ A Smith, Yejin Choi, Kentaro Inui, etÂ al. 2024.

</span>
<span class="ltx_bibblock">Realtime qa: Whatâ€™s the answer right now?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Advances in Neural Information Processing Systems</em>, 36.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khandelwal etÂ al. (2019)</span>
<span class="ltx_bibblock">
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2019.

</span>
<span class="ltx_bibblock">Generalization through memorization: Nearest neighbor language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:1911.00172</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim etÂ al. (2023)</span>
<span class="ltx_bibblock">
Gangwoo Kim, Sungdong Kim, Byeongguk Jeon, Joonsuk Park, and Jaewoo Kang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2310.14696" title="">Tree of clarifications: Answering ambiguous questions with retrieval-augmented large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Preprint</em>, arXiv:2310.14696.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lazaridou etÂ al. (2022)</span>
<span class="ltx_bibblock">
Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2203.05115" title="">Internet-augmented language models through few-shot prompting for open-domain question answering</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Preprint</em>, arXiv:2203.05115.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee etÂ al. (2022)</span>
<span class="ltx_bibblock">
Haejun Lee, Akhil Kedia, Jongwon Lee, Ashwin Paranjape, ChristopherÂ D. Manning, and Kyoung-Gu Woo. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2112.07381" title="">You only need one model for open-domain question answering</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Preprint</em>, arXiv:2112.07381.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis etÂ al. (2021)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen tau Yih, Tim RocktÃ¤schel, Sebastian Riedel, and Douwe Kiela. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2005.11401" title="">Retrieval-augmented generation for knowledge-intensive nlp tasks</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Preprint</em>, arXiv:2005.11401.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luu etÂ al. (2022)</span>
<span class="ltx_bibblock">
Kelvin Luu, Daniel Khashabi, Suchin Gururangan, Karishma Mandyam, and NoahÂ A. Smith. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.naacl-main.435" title="">Time waits for no one! analysis and challenges of temporal misalignment</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 5944â€“5958, Seattle, United States. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma etÂ al. (2023)</span>
<span class="ltx_bibblock">
Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2305.14283" title="">Query rewriting for retrieval-augmented large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Preprint</em>, arXiv:2305.14283.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nakano etÂ al. (2022)</span>
<span class="ltx_bibblock">
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, XuÂ Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2112.09332" title="">Webgpt: Browser-assisted question-answering with human feedback</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Preprint</em>, arXiv:2112.09332.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
RÂ OpenAI. 2023.

</span>
<span class="ltx_bibblock">Gpt-4 technical report. arxiv 2303.08774.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">View in Article</em>, 2(5).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Long Ouyang, Jeffrey Wu, XuÂ Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, etÂ al. 2022.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Advances in neural information processing systems</em>, 35:27730â€“27744.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng etÂ al. (2024)</span>
<span class="ltx_bibblock">
Wenjun Peng, Guiyang Li, Yue Jiang, Zilong Wang, Dan Ou, Xiaoyi Zeng, Derong Xu, Tong Xu, and Enhong Chen. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2311.03758" title="">Large language model based long-tail query rewriting in taobao search</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Preprint</em>, arXiv:2311.03758.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qian etÂ al. (2023)</span>
<span class="ltx_bibblock">
Hongjing Qian, Yutao Zhu, Zhicheng Dou, Haoqi Gu, Xinyu Zhang, Zheng Liu, Ruofei Lai, Zhao Cao, Jian-Yun Nie, and Ji-Rong Wen. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2304.04358" title="">Webbrain: Learning to generate factually correct articles for queries by grounding on large web corpus</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Preprint</em>, arXiv:2304.04358.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers and Gurevych (2019)</span>
<span class="ltx_bibblock">
Nils Reimers and Iryna Gurevych. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/1908.10084" title="">Sentence-bert: Sentence embeddings using siamese bert-networks</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Preprint</em>, arXiv:1908.10084.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">RÃ¶ttger and Pierrehumbert (2021)</span>
<span class="ltx_bibblock">
Paul RÃ¶ttger and Janet Pierrehumbert. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.findings-emnlp.206" title="">Temporal adaptation of BERT and performance on downstream document classification: Insights from social media</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Findings of the Association for Computational Linguistics: EMNLP 2021</em>, pages 2400â€“2412, Punta Cana, Dominican Republic. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sachan etÂ al. (2021)</span>
<span class="ltx_bibblock">
DevendraÂ Singh Sachan, Siva Reddy, William Hamilton, Chris Dyer, and Dani Yogatama. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2106.05346" title="">End-to-end training of multi-document reader and retriever for open-domain question answering</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Preprint</em>, arXiv:2106.05346.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick etÂ al. (2023)</span>
<span class="ltx_bibblock">
Timo Schick, Jane Dwivedi-Yu, Roberto DessÃ¬, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2302.04761" title="">Toolformer: Language models can teach themselves to use tools</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Preprint</em>, arXiv:2302.04761.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao etÂ al. (2023)</span>
<span class="ltx_bibblock">
Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2305.15294" title="">Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Preprint</em>, arXiv:2305.15294.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi etÂ al. (2023)</span>
<span class="ltx_bibblock">
Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023.

</span>
<span class="ltx_bibblock">Replug: Retrieval-augmented black-box language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">arXiv preprint arXiv:2301.12652</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thorne etÂ al. (2018)</span>
<span class="ltx_bibblock">
James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N18-1074" title="">FEVER: a large-scale dataset for fact extraction and VERification</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</em>, pages 809â€“819, New Orleans, Louisiana. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trivedi etÂ al. (2023)</span>
<span class="ltx_bibblock">
Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2212.10509" title="">Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Preprint</em>, arXiv:2212.10509.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei etÂ al. (2023)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, EdÂ Chi, Quoc Le, and Denny Zhou. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2201.11903" title="">Chain-of-thought prompting elicits reasoning in large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Preprint</em>, arXiv:2201.11903.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Hui Yang, Sifu Yue, and Yunzhong He. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2306.02224" title="">Auto-gpt for online decision making: Benchmarks and additional opinions</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Preprint</em>, arXiv:2306.02224.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al. (2018)</span>
<span class="ltx_bibblock">
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and ChristopherÂ D. Manning. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D18-1259" title="">HotpotQA: A dataset for diverse, explainable multi-hop question answering</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, pages 2369â€“2380, Brussels, Belgium. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yoran etÂ al. (2024)</span>
<span class="ltx_bibblock">
Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2310.01558" title="">Making retrieval-augmented language models robust to irrelevant context</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Preprint</em>, arXiv:2310.01558.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang (2023)</span>
<span class="ltx_bibblock">
Jiawei Zhang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2304.11116" title="">Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Preprint</em>, arXiv:2304.11116.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng etÂ al. (2024)</span>
<span class="ltx_bibblock">
HuaixiuÂ Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, EdÂ H. Chi, QuocÂ V Le, and Denny Zhou. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2310.06117" title="">Take a step back: Evoking reasoning via abstraction in large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Preprint</em>, arXiv:2310.06117.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou etÂ al. (2023)</span>
<span class="ltx_bibblock">
Denny Zhou, Nathanael SchÃ¤rli, LeÂ Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and EdÂ Chi. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2205.10625" title="">Least-to-most prompting enables complex reasoning in large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Preprint</em>, arXiv:2205.10625.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>
<figure class="ltx_table" id="A1.T4">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.T4.1" style="width:433.6pt;height:1199.5pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-37.9pt,104.9pt) scale(0.851068188250304,0.851068188250304) ;">
<table class="ltx_tabular ltx_align_middle" id="A1.T4.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T4.1.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T4.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.1.1.1">Question: Could Carl Friedrich Gauss speak to someone 100 miles away?</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.2.2">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.2.2.1">The question is: ambiguous.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.3.3">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.3.3.1">Sub-Question: What device allows people to speak to each other even if they are 100 miles apart?</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.4.4">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.4.4.1">Probability of correct answer is: 0.8.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.5.5">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.5.5.1">Sub-Answer: Carl Friedrich Gauss was born in 1777.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.6.6">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.6.6.1">Sub-Question: When was #1 invented?</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.7.7">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.7.7.1">Probability of correct answer is: 0.8.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.8.8">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.8.8.1">Sub-Answer: Speaking to someone 100 miles away requires a telephone.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.9.9">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.9.9.1">Sub-Question: When did Carl Friedrich Gauss die?</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.10.10">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.10.10.1">Probability of correct answer is: 0.8.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.11.11">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.11.11.1">Sub-Answer: The telephone was invented in 1876.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.12.12">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.12.12.1">So the final answer is: False.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.13.13">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T4.1.1.13.13.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.13.13.1.1">Question: Did Jackson 5 members exceed number in The Osmonds?</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.14.14">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.14.14.1">The question is: ambiguous.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.15.15">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.15.15.1">Sub-Question: How many members did The Jackson 5 have?</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.16.16">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.16.16.1">Probability of correct answer is: <span class="ltx_text" id="A1.T4.1.1.16.16.1.1" style="background-color:#FFFF00;">0.3</span>.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.17.17">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.17.17.1">Sub-Answer: The group was founded in 1964 in Gary, Indiana, and originally consisted of brothers Jackie, Tito, Jermaine, Marlon and Michael. They were managed by their â€¦.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.18.18">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.18.18.1">Sub-Question: How many members did The Osmonds have?</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.19.19">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.19.19.1">Probability of correct answer is: <span class="ltx_text" id="A1.T4.1.1.19.19.1.1" style="background-color:#FFFF00;">0.3</span>.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.20.20">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.20.20.1">Sub-Answer: The Osmonds then and now Â· Donny Osmond Â· Marie Osmond Â· Jay Osmond Â· Merrill Osmond. Get info without leaving the page. Â· Wayne Osmond Â· Alan Osmond.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.21.21">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.21.21.1">So the final answer is: False.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.22.22">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T4.1.1.22.22.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.22.22.1.1">Question: Is popular science used to peer review papers?</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.23.23">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.23.23.1">The question is: ambiguous.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.24.24">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.24.24.1">Sub-Question: What is popular science?</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.25.25">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.25.25.1">Probability of correct answer is: 0.5.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.26.26">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.26.26.1">Sub-Answer: Popular science is a simplified version of scientific work.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.27.27">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.27.27.1">Sub-Question: What types of documents does peer review use to verify papers?</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.28.28">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.28.28.1">Probability of correct answer is: 0.6.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.29.29">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.29.29.1">Sub-Answer: Peer review uses detailed scientific information to verify papers.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.30.30">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.30.30.1">So the final answer is: False.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.31.31">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T4.1.1.31.31.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.31.31.1.1">Question: Does a lapidary work with items that are studied by geologists?</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.32.32">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.32.32.1">The question is: ambiguous.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.33.33">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.33.33.1">Sub-Question: What are the materials a lapidary works with?</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.34.34">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.34.34.1">Probability of correct answer is: <span class="ltx_text" id="A1.T4.1.1.34.34.1.1" style="background-color:#FFFF00;">0.4.</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.35.35">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.35.35.1">Sub-Answer: <span class="ltx_text" id="A1.T4.1.1.35.35.1.1" style="background-color:#00FF00;">Quartz is a popular material for lapidary because it is relatively easy to work with and comes in various colours and patterns. As it is â€¦.</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.36.36">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.36.36.1">Sub-Question: What do geologists study?</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.37.37">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.37.37.1">Probability of correct answer is: 0.5.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.38.38">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.38.38.1">Sub-Answer: Lapidarists work with stone, minerals and gemstones.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.39.39">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.39.39.1">So the final answer is: True.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.40.40">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T4.1.1.40.40.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.40.40.1.1">Question: Could Carl Friedrich Gauss speak to someone 100 miles away?</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.41.41">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.41.41.1">The question is: ambiguous.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.42.42">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.42.42.1">Sub-Question: What device allows people to speak to each other even if they are 100 miles apart?</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.43.43">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.43.43.1">Probability of correct answer is: <span class="ltx_text" id="A1.T4.1.1.43.43.1.1" style="background-color:#FFFF00;">0.1.</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.44.44">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.44.44.1">Sub-Answer: Start with a handheld. I like Yaesus.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.45.45">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.45.45.1">Sub-Question: When was #1 invented?</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.46.46">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.46.46.1">Probability of correct answer is: <span class="ltx_text" id="A1.T4.1.1.46.46.1.1" style="background-color:#FFFF00;">0.4.</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.47.47">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.47.47.1">Sub-Answer: 1 (one, unit, unity) is a number representing a single or the only entity. 1 is also a numerical digit and represents a single unit of counting or â€¦.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.48.48">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.48.48.1">Sub-Question: When did Carl Friedrich Gauss die?</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.49.49">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.49.49.1">Probability of correct answer is: <span class="ltx_text" id="A1.T4.1.1.49.49.1.1" style="background-color:#FFFF00;">0.3.</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.50.50">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.50.50.1">Sub-Answer: Carl Friedrich Gauss (born April 30, 1777, Brunswick [Germany]â€”died February 23, 1855, Â· Gauss was the only child of poor parents. Â· Gaussâ€™s first â€¦.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.51.51">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.51.51.1">So the final answer is: False.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.52.52">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T4.1.1.52.52.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.52.52.1.1">Question: Can you listen to the entire iTunes song catalog in one year?</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.53.53">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.53.53.1">The question is: ambiguous.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.54.54">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.54.54.1">Sub-Question: How many songs are on iTunes?</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.55.55">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.55.55.1">Probability of correct answer is: 0.8.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.56.56">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.56.56.1">Sub-Answer: iTunes has around 43 million songs as of 2017.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.57.57">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.57.57.1">Sub-Question: What is the average song length?</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.58.58">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.58.58.1">Probability of correct answer is: 0.9.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.59.59">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.59.59.1">Sub-Answer: The average length of a song is 3 minutes.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.60.60">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.60.60.1">Sub-Question: What is #1 multiplies by #2?</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.61.61">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.61.61.1">Probability of correct answer is: 0.6.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.62.62">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.62.62.1">Sub-Answer: There are 525,600 minutes in a year.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.63.63">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.63.63.1">So the final answer is: False.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.64.64">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T4.1.1.64.64.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.64.64.1.1">Question: Can you listen to the entire iTunes song catalog in one year?</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.65.65">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.65.65.1">The question is: ambiguous.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.66.66">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.66.66.1">Sub-Question: How many songs are on iTunes?</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.67.67">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.67.67.1">Probability of correct answer is: <span class="ltx_text" id="A1.T4.1.1.67.67.1.1" style="background-color:#FFFF00;">0.0.</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.68.68">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.68.68.1">Sub-Answer: If you go to the "Songs" window and reenable the status bar by selecting "View" then "Show Status Bar" you will get the total count at the â€¦.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.69.69">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.69.69.1">Sub-Question: What is the average song length?</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.70.70">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.70.70.1">Probability of correct answer is: <span class="ltx_text" id="A1.T4.1.1.70.70.1.1" style="background-color:#FFFF00;">0.2.</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.71.71">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.71.71.1">Sub-Answer: Nowadays, songs average around 3:15/3:30 which shows a decrease in length by up to 60 seconds. On top of that, weâ€™re constantly seeing way more â€¦.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.72.72">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.72.72.1">Sub-Question: What is #1 multiplies by #2?</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.73.73">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.73.73.1">Probability of correct answer is: 0.7.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.74.74">
<td class="ltx_td ltx_align_left" id="A1.T4.1.1.74.74.1">Sub-Answer: 1/2.</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.75.75">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T4.1.1.75.75.1">So the final answer is: False.</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Think-then-Act on StrategyQA: 10 samples</figcaption>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jun 18 20:47:41 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
