<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track</title>
<!--Generated on Mon Jun 24 17:27:50 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2406.16828v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#S1" title="In RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#S2" title="In RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#S2.SS0.SSS0.Px1" title="In 2. Related Work â€£ RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_title">RAG Frameworks.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#S2.SS0.SSS0.Px2" title="In 2. Related Work â€£ RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_title">Collection selection.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#S2.SS0.SSS0.Px3" title="In 2. Related Work â€£ RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_title">Topic selection.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#S3" title="In RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Our Framework</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#S3.SS0.SSS0.Px1" title="In 3. Our Framework â€£ RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_title">Retrieval Module</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#S3.SS0.SSS0.Px2" title="In 3. Our Framework â€£ RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_title">Augmented Generation Module</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#S3.SS1" title="In 3. Our Framework â€£ RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>RAG Input/Output Definitions</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#S3.SS1.SSS0.Px1" title="In 3.1. RAG Input/Output Definitions â€£ 3. Our Framework â€£ RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_title">RAG Input</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#S3.SS1.SSS0.Px2" title="In 3.1. RAG Input/Output Definitions â€£ 3. Our Framework â€£ RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_title">RAG Output</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#S4" title="In RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Document Collection</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#S4.SS0.SSS0.Px1" title="In 4. Document Collection â€£ RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_title">MS MARCO V2.1 Document Collection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#S4.SS0.SSS0.Px2" title="In 4. Document Collection â€£ RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_title">MS MARCO V2.1 Segment Collection.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#S5" title="In RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Topic Collection</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#S5.SS0.SSS0.Px1" title="In 5. Topic Collection â€£ RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_title">TREC-RAGgy 2024</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#S5.SS0.SSS0.Px2" title="In 5. Topic Collection â€£ RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_title">TREC-Researchy 2024</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#S6" title="In RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>TREC 2024 RAG Baselines</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#S6.SS0.SSS0.Px1" title="In 6. TREC 2024 RAG Baselines â€£ RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_title">Retrieval.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#S6.SS0.SSS0.Px2" title="In 6. TREC 2024 RAG Baselines â€£ RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_title">Augmented Generation.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#S6.SS0.SSS0.Px3" title="In 6. TREC 2024 RAG Baselines â€£ RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_title">RAG-Bench Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#S6.SS1" title="In 6. TREC 2024 RAG Baselines â€£ RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>RagnarÃ¶k System Arena</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#S7" title="In RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Ongoing Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#S8" title="In RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#A1" title="In RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>TREC-RAGgy 2024: Additional Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#A2" title="In RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>TREC-Researchy 2024: Additional Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#A3" title="In RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>RagnarÃ¶k System Arena</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">RagnarÃ¶k: A Reusable RAG Framework and Baselines for 
<br class="ltx_break"/>TREC 2024 Retrieval-Augmented Generation Track</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ronak Pradeep
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">University of Waterloo 
<br class="ltx_break"/><span class="ltx_text ltx_affiliation_city" id="id2.1.id1">Waterloo</span><span class="ltx_text ltx_affiliation_country" id="id3.2.id2">Canada</span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Nandan Thakur
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">University of Waterloo 
<br class="ltx_break"/><span class="ltx_text ltx_affiliation_city" id="id4.1.id1">Waterloo</span><span class="ltx_text ltx_affiliation_country" id="id5.2.id2">Canada</span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sahel Sharifymoghaddam
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">University of Waterloo 
<br class="ltx_break"/><span class="ltx_text ltx_affiliation_city" id="id6.1.id1">Waterloo</span><span class="ltx_text ltx_affiliation_country" id="id7.2.id2">Canada</span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Eric Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">University of Waterloo 
<br class="ltx_break"/><span class="ltx_text ltx_affiliation_city" id="id8.1.id1">Waterloo</span><span class="ltx_text ltx_affiliation_country" id="id9.2.id2">Canada</span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ryan Nguyen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">University of Waterloo 
<br class="ltx_break"/><span class="ltx_text ltx_affiliation_city" id="id10.1.id1">Waterloo</span><span class="ltx_text ltx_affiliation_country" id="id11.2.id2">Canada</span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Daniel Campos
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Snowflake Inc. 
<br class="ltx_break"/><span class="ltx_text ltx_affiliation_city" id="id12.1.id1">New York</span><span class="ltx_text ltx_affiliation_country" id="id13.2.id2">USA</span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Nick Craswell
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Microsoft 
<br class="ltx_break"/><span class="ltx_text ltx_affiliation_city" id="id14.1.id1">Seattle</span><span class="ltx_text ltx_affiliation_country" id="id15.2.id2">USA</span>
</span></span></span>
<span class="ltx_author_before">Â andÂ </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jimmy Lin
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">University of Waterloo 
<br class="ltx_break"/><span class="ltx_text ltx_affiliation_city" id="id16.1.id1">Waterloo</span><span class="ltx_text ltx_affiliation_country" id="id17.2.id2">Canada</span>
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id18.id1">Did you try out the new Bing Search?
Or maybe you fiddled around with Google AIÂ Overviews?
These might sound familiar because the modern-day search stack has recently evolved to include retrieval-augmented generation (RAG) systems.
They allow searching and incorporating real-time data into large language models (LLMs) to provide a well-informed, attributed, concise summary in contrast to the traditional search paradigm that relies on displaying a ranked list of documents.
Therefore, given these recent advancements, it is crucial to have an arena to build, test, visualize, and systematically evaluate RAG-based search systems.
With this in mind, we propose the TREC 2024 RAG Track to foster innovation in evaluating RAG systems.
In our work, we lay out the steps weâ€™ve made towards making this track a reality â€” we describe the details of our reusable framework, RagnarÃ¶k, explain the curation of the new MS MARCO V2.1 collection choice, release the development topics for the track, and standardize the I/O definitions which assist the end user.
Next, using RagnarÃ¶k, we identify and provide key industrial baselines such as OpenAIâ€™s GPT-4o or Cohereâ€™s Command R+.
Further, we introduce a web-based user interface for an interactive arena allowing benchmarking pairwise RAG systems by crowdsourcing.
We open-source our RagnarÃ¶k framework and baselines to achieve a unified standard for future RAG systems.</p>
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="18" id="id1.g1" src="extracted/5688564/github.png" width="18"/>
<p class="ltx_p ltx_parbox ltx_align_middle" id="id19.id2" style="width:433.6pt;"><a class="ltx_ref ltx_href" href="https://github.com/castorini/ragnarok" title=""><span class="ltx_ref ltx_nolink">https://github.com/castorini/ragnarok</span></a></p>
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">copyright: </span>none</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Retrieval Augmented Generation (RAG)Â <cite class="ltx_cite ltx_citemacro_citep">(Guu etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib22" title="">2020</a>; Lewis etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib30" title="">2020</a>; Izacard and Grave, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib23" title="">2021</a>; Borgeaud etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib11" title="">2022</a>)</cite> has emerged as a popular technique to augment large language model (LLM) generation for knowledge-intensive tasks such as open-domain question answering or fact verification <cite class="ltx_cite ltx_citemacro_citep">(Petroni etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib45" title="">2021</a>)</cite>.
Using the top-<math alttext="k" class="ltx_Math" display="inline" id="S1.p1.1.m1.1"><semantics id="S1.p1.1.m1.1a"><mi id="S1.p1.1.m1.1.1" xref="S1.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S1.p1.1.m1.1b"><ci id="S1.p1.1.m1.1.1.cmml" xref="S1.p1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S1.p1.1.m1.1d">italic_k</annotation></semantics></math> retrieved segments from a suitable retrieval system, RAG systems output an answer summary grounded on the relevant context.
RAG systems mitigate factual inconsistencies in LLM outputs <cite class="ltx_cite ltx_citemacro_citep">(Khandelwal etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib27" title="">2020</a>; Lewis etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib30" title="">2020</a>; Gao etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib20" title="">2023b</a>; Liu etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib35" title="">2024a</a>)</cite>, and enhance interpretabilityÂ <cite class="ltx_cite ltx_citemacro_citep">(Guu etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib22" title="">2020</a>)</cite> and generalization <cite class="ltx_cite ltx_citemacro_citep">(Gao etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib21" title="">2023a</a>)</cite>, thus facilitating a wider adoption across several domains like MedicineÂ <cite class="ltx_cite ltx_citemacro_citep">(Xiong etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib56" title="">2024</a>)</cite> and FinanceÂ <cite class="ltx_cite ltx_citemacro_citep">(Jimeno-Yepes etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib24" title="">2024</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="148" id="S1.F1.g1" src="x1.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Schematic diagram of the RagnarÃ¶k framework.
Given a user topic (left), the process consists of two steps: (1)
(R) retrieval (+ rerank), where the topic yields the top-<math alttext="k" class="ltx_Math" display="inline" id="S1.F1.2.m1.1"><semantics id="S1.F1.2.m1.1b"><mi id="S1.F1.2.m1.1.1" xref="S1.F1.2.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S1.F1.2.m1.1c"><ci id="S1.F1.2.m1.1.1.cmml" xref="S1.F1.2.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.2.m1.1d">k</annotation><annotation encoding="application/x-llamapun" id="S1.F1.2.m1.1e">italic_k</annotation></semantics></math> relevant segments from our document collection (e.g., potty training articles); and (2) (AG) augmented-generation, where the retrieved segments with a suitable prompt template is fed to the large language model (LLM) to generate the post-processed answer response (JSON) containing individual sentence-level citations.
</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Several companies provide end-to-end RAG frameworks such as Bing SearchÂ <cite class="ltx_cite ltx_citemacro_citep">(Microsoft, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib40" title="">2023</a>)</cite>, or Google GeminiÂ <cite class="ltx_cite ltx_citemacro_citep">(Anil etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib6" title="">2023</a>)</cite>.
Most of these systems are either proprietary or offer limited user customization.
Likewise, the absence of a standardized RAG framework makes implementing RAG at a large scale challenging.
Implementing atop existing frameworks requires custom code for multiple steps including retrieval, reranking, and generation.
To promote wider adoption of RAG in academia, we develop RagnarÃ¶k, a user-friendly, reusable, end-to-end RAG framework offering code for customizable retrievers, rerankers, and generation models.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.2">RagnarÃ¶k comprises two key modules: (R) Retrieval and (AG) Augmented Generation.
The retrieval module incorporates both the retrieval and re-ranking stages to yield the top-<math alttext="k" class="ltx_Math" display="inline" id="S1.p3.1.m1.1"><semantics id="S1.p3.1.m1.1a"><mi id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><ci id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S1.p3.1.m1.1d">italic_k</annotation></semantics></math> retrieved segments for an input user topic.
Next, the augmented generation module uses the user-provided topic and retrieved segments to produce an RAG answer, formatted into individual sentences, citing the relevant information from the top-<math alttext="k" class="ltx_Math" display="inline" id="S1.p3.2.m2.1"><semantics id="S1.p3.2.m2.1a"><mi id="S1.p3.2.m2.1.1" xref="S1.p3.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S1.p3.2.m2.1b"><ci id="S1.p3.2.m2.1.1.cmml" xref="S1.p3.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S1.p3.2.m2.1d">italic_k</annotation></semantics></math> retrieved segments.
RagnarÃ¶k is deeply integrated with existing Python frameworks, such asÂ <span class="ltx_text ltx_font_typewriter" id="S1.p3.2.1">Pyserini</span>Â <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib32" title="">2021</a>)</cite> andÂ <span class="ltx_text ltx_font_typewriter" id="S1.p3.2.2">rank_llm</span>Â <cite class="ltx_cite ltx_citemacro_citep">(Pradeep etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib47" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib48" title="">b</a>)</cite> and can be easily installed via PyPI usingÂ â€œ<span class="ltx_text ltx_font_typewriter" id="S1.p3.2.3">pip install pyragnarok</span>â€.
The framework offers easy-to-use REST APIs and an integrated WebUI to enhance user-friendliness and improve the human evaluation experience.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">The RagnarÃ¶k framework will be used for providing baselines in the upcoming TREC 2024 Retrieval Augmented Generation (RAG) Track.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>TREC 2024 Retrieval Augmented Generation (RAG) Track: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://trec-rag.github.io" title="">https://trec-rag.github.io</a>.</span></span></span> An ideal framework should include a sufficiently large document collection covering diverse information and non-factoid, decompositional topics requiring long-form answers.
In our framework, we deduplicate the existing MS MARCO V2 document collection. In addition, we provide a â€œsegmentâ€ collection using a sliding-window chunking technique (discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#S4" title="4. Document Collection â€£ RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_tag">4</span></a>). Further, we release two sets of development topics: (i) TREC-RAGgy 2024: a filtered subset of topics with long-form answers from TREC Deep Learning 2021-23 <cite class="ltx_cite ltx_citemacro_citep">(Craswell etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib16" title="">2021</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib17" title="">2022</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib18" title="">2024</a>)</cite>; and (ii) TREC-Researchy 2024: a subset of the Researchy Questions introduced in <cite class="ltx_cite ltx_citemacro_citet">Rosset etÂ al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib52" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Our RagnarÃ¶k framework supports a head-to-head RAG battle arena for the answer evaluation, heavily inspired by recent work such as the Chatbot ArenaÂ <cite class="ltx_cite ltx_citemacro_citep">(Chiang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib14" title="">2024</a>; Zheng etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib59" title="">2023</a>)</cite>.
We include key industrial baselines such as Cohere Command R+ <cite class="ltx_cite ltx_citemacro_citep">(Cohere, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib15" title="">2024</a>)</cite> and OpenAI GPT-4oÂ <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib42" title="">2024</a>)</cite> and evaluate both the baselines using the retrieval setup involving BM25Â <cite class="ltx_cite ltx_citemacro_citep">(Robertson and Zaragoza, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib50" title="">2009</a>)</cite> and RankZephyrÂ <cite class="ltx_cite ltx_citemacro_citep">(Pradeep etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib48" title="">2023b</a>)</cite> with human preferences. Overall, we observe GPT-4o to provide more detailed answers over Command R+ on the development set of topics (discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#S6" title="6. TREC 2024 RAG Baselines â€£ RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_tag">6</span></a>).
Finally, we open-source RagnarÃ¶k and make it publicly available at the following URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/castorini/ragnarok" title="">https://github.com/castorini/ragnarok</a>. In the future, we will include a wider variety of LLMs as baselines and continue to improve our framework.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">RAG Frameworks.</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">Existing RAG systems are primarily closed-source and difficult to reproduce.
Open-source frameworks such as LangChain <cite class="ltx_cite ltx_citemacro_citep">(Chase, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib13" title="">2022</a>)</cite> and LlamaIndex <cite class="ltx_cite ltx_citemacro_citep">(Liu, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib34" title="">2022</a>)</cite>, while available, are not research-friendly and lack proper evaluation and benchmarking.
FlashRAGÂ <cite class="ltx_cite ltx_citemacro_citep">(Jin etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib25" title="">2024</a>)</cite>, a concurrent work, is a similarly motivated toolkit to improve the RAG experience for researchers.
While the framework is extensive and designed for pipeline flexibility, RagnarÃ¶k offers a few additional capabilities â€” a WebUI serving a RAG battle arena, easy-to-use REST APIs, a standardized I/O definition working with sentence-level citations, and a tight integration with popular retrieval (+ reranking) frameworks like PyseriniÂ <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib32" title="">2021</a>)</cite> and RankLLM.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Collection selection.</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">Current RAG datasets are constructed using the English Wikipedia as the document collection, However, their scale is limited to provide rich and comprehensive information to support RAG systems.
In addition, ClueWeb22Â <cite class="ltx_cite ltx_citemacro_citep">(Overwijk etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib43" title="">2022</a>)</cite> offers an extensive collection of 22 billion curated web pages, previously utilized in TREC tracks such as the TREC Conversational Assistance Track (CAsT) <cite class="ltx_cite ltx_citemacro_citep">(Owoicho etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib44" title="">2022</a>)</cite> and the forthcoming TREC Interactive Knowledge Assistance Track (iKAT) <cite class="ltx_cite ltx_citemacro_citep">(Aliannejadi etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib4" title="">2024</a>)</cite>.
Another alternative is the MS MARCO V2 document collection, used in the TREC Deep Learning (DL) track.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Topic selection.</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p1.1">Recently, there has been a surge in datasets providing topics with long-form answers for evaluating RAG systems. ASQA <cite class="ltx_cite ltx_citemacro_citep">(Stelmakh etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib53" title="">2022</a>)</cite>, ELI5 <cite class="ltx_cite ltx_citemacro_citep">(Fan etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib19" title="">2019</a>)</cite>, and QAMPARIÂ <cite class="ltx_cite ltx_citemacro_citep">(Amouyal etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib5" title="">2022</a>)</cite> were utilized for evaluation in the Automatic LLMsâ€™ Citation Evaluation (ALCE) framework <cite class="ltx_cite ltx_citemacro_citep">(Gao etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib20" title="">2023b</a>)</cite>. Similarly, related long-form QA datasets include AquaMuseÂ <cite class="ltx_cite ltx_citemacro_citep">(Kulkarni etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib28" title="">2020</a>)</cite>, ExpertQAÂ <cite class="ltx_cite ltx_citemacro_citep">(Malaviya etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib37" title="">2023</a>)</cite>, and TruthfulQAÂ <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib33" title="">2022</a>)</cite>.
Another recently introduced dataset is ClapNQÂ <cite class="ltx_cite ltx_citemacro_citep">(Rosenthal etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib51" title="">2024</a>)</cite>, created from the subset of Natural Questions (NQ)Â <cite class="ltx_cite ltx_citemacro_citep">(Kwiatkowski etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib29" title="">2019</a>)</cite> and HAGRIDÂ <cite class="ltx_cite ltx_citemacro_citep">(Kamalloo etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib26" title="">2023</a>)</cite> built on a subset of MS MARCO Dev <cite class="ltx_cite ltx_citemacro_citep">(Bajaj etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib9" title="">2016</a>)</cite>.
Almost all previous datasets are built on English Wikipedia. In contrast, our work deliberately avoids English Wikipedia to prevent the overfitting commonly seen in existing benchmarksÂ <cite class="ltx_cite ltx_citemacro_citep">(Thakur etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib55" title="">2021</a>; Muennighoff etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib41" title="">2023</a>)</cite>.
In our work, we re-utilize topics from previous TREC tracks such as the Deep Learning (DL) track, because human judgments are available on the MS MARCO V2 corpora and Researchy Questions <cite class="ltx_cite ltx_citemacro_citep">(Rosset etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib52" title="">2024</a>)</cite> as it covers a wide range of topics based on ClueWeb22Â <cite class="ltx_cite ltx_citemacro_citep">(Overwijk etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib43" title="">2022</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Our Framework</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">RagnarÃ¶k is an open-source, reproducible, and reusable framework implementing an end-to-end retrieval-augmented generation (RAG) pipeline, comprising two modules applied sequentially: (1) (R) retrieval and (2) (AG) augmented generation.
Through the RagnarÃ¶k framework, we will provide several baselines to all participants in the upcoming TREC 2024 RAG track.
An overview of the framework is provided in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.16828v1#S1.F1" title="Figure 1 â€£ 1. Introduction â€£ RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_tag">FigureÂ 1</span></a>.
We first describe both modules and expand on the I/O specifications in our framework.</p>
</div>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Retrieval Module</h4>
<div class="ltx_para" id="S3.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px1.p1.2">This module retrieves the relevant segments for a user topic as the input. It supports (i) first-stage lexical retrieval models such as BM25 <cite class="ltx_cite ltx_citemacro_citep">(Robertson and Zaragoza, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib50" title="">2009</a>)</cite> and (ii) reranking models such as RankZephyrÂ <cite class="ltx_cite ltx_citemacro_citep">(Pradeep etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib48" title="">2023b</a>)</cite>. The retrieval system searches for relevant segments in the document collection and retrieves the top-<math alttext="100" class="ltx_Math" display="inline" id="S3.SS0.SSS0.Px1.p1.1.m1.1"><semantics id="S3.SS0.SSS0.Px1.p1.1.m1.1a"><mn id="S3.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.1.m1.1b"><cn id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" type="integer" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.1.m1.1c">100</annotation><annotation encoding="application/x-llamapun" id="S3.SS0.SSS0.Px1.p1.1.m1.1d">100</annotation></semantics></math> segments further reranked by the reranker model to filter out the top-<math alttext="20" class="ltx_Math" display="inline" id="S3.SS0.SSS0.Px1.p1.2.m2.1"><semantics id="S3.SS0.SSS0.Px1.p1.2.m2.1a"><mn id="S3.SS0.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.2.m2.1b"><cn id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" type="integer" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.2.m2.1c">20</annotation><annotation encoding="application/x-llamapun" id="S3.SS0.SSS0.Px1.p1.2.m2.1d">20</annotation></semantics></math> relevant segments for the next stage.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Augmented Generation Module</h4>
<div class="ltx_para" id="S3.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px2.p1.1">This module takes in the user topic and the top-<math alttext="20" class="ltx_Math" display="inline" id="S3.SS0.SSS0.Px2.p1.1.m1.1"><semantics id="S3.SS0.SSS0.Px2.p1.1.m1.1a"><mn id="S3.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p1.1.m1.1b"><cn id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" type="integer" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p1.1.m1.1c">20</annotation><annotation encoding="application/x-llamapun" id="S3.SS0.SSS0.Px2.p1.1.m1.1d">20</annotation></semantics></math> retrieved segments (from the retrieval module) as the input and a prompting strategy to the large language model (LLM) to generate the answer response with in-context citations for the topic.
The answer response is divided into individual sentences, each sentence within the answer contains text and is grounded on retrieved documents provided as references.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>RAG Input/Output Definitions</h3>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">RAG Input</h4>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.1">The input specifications are straightforward as the user can formulate any question they wish to ask, provide the user topic, and call our RagnarÃ¶k REST-API framework.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">RAG Output</h4>
<div class="ltx_para" id="S3.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p1.1">The user receives a JSON output in response to their topic from the RagnarÃ¶k framework. The first key in the output JSON schema, <span class="ltx_text ltx_font_typewriter" id="S3.SS1.SSS0.Px2.p1.1.1">references</span>, provides an ordered list of the top-<math alttext="20" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p1.1.m1.1"><semantics id="S3.SS1.SSS0.Px2.p1.1.m1.1a"><mn id="S3.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.1.m1.1b"><cn id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" type="integer" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.1.m1.1c">20</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p1.1.m1.1d">20</annotation></semantics></math> ranked segment IDs from our retrieval module. Next,<span class="ltx_text ltx_font_typewriter" id="S3.SS1.SSS0.Px2.p1.1.2">answer</span>, provides the LLM-generated RAG answer to the user topic, presented as a top-to-bottom list of sentence-level texts with corresponding segment citations. All citations are zero-based indexed indicating the exact position of the segment ID from the <span class="ltx_text ltx_font_typewriter" id="S3.SS1.SSS0.Px2.p1.1.3">references</span> list. Finally, <span class="ltx_text ltx_font_typewriter" id="S3.SS1.SSS0.Px2.p1.1.4">response_length</span>, provides the total count of the text characters present in the output RAG answer.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1. </span>Comparison of document and segment counts between versions V2 and V2.1 (our version after removing near-duplicates) of the MS MARCO collection.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.1" style="font-size:90%;">Collection</span><span class="ltx_text" id="S3.T1.1.1.1.1.2" style="font-size:90%;"></span>
</th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.2.1" style="font-size:90%;">Version V2</span><span class="ltx_text" id="S3.T1.1.1.1.2.2" style="font-size:90%;"></span>
</th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.3.1" style="font-size:90%;">Version V2.1 (Ours)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.1.2.1.1.1" style="font-size:90%;">MS MARCO Document</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S3.T1.1.2.1.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.1.2.1.2.1" style="font-size:90%;">11,959,635</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S3.T1.1.2.1.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.1.2.1.3.1" style="font-size:90%;">10,960,555</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.3.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.1.3.2.1.1" style="font-size:90%;">MS MARCO Segment</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S3.T1.1.3.2.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.1.3.2.2.1" style="font-size:90%;">124,131,414</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S3.T1.1.3.2.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.1.3.2.3.1" style="font-size:90%;">113,520,750</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Document Collection</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">The MS MARCO V2 document collection, earlier used in the TREC-DL tracks, contains a substantial overlap of near-duplicates (documents with sufficiently similar text information) within the collection <cite class="ltx_cite ltx_citemacro_citep">(Craswell etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib17" title="">2022</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib18" title="">2024</a>)</cite>.
When left intact, these near-duplicates degrade the downstream retrieval accuracy and reduce the diversity of the collected documents, potentially impacting the effectiveness of RAG systems.
In addition, chunking, which breaks down a long verbose document into smaller compact representations is a key challenge in RAG, as the retrieved chunk representations correlate with the RAG answer qualityÂ <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib35" title="">2024a</a>)</cite>.</p>
</div>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">MS MARCO V2.1 Document Collection</h4>
<div class="ltx_para" id="S4.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px1.p1.1">We conduct a deduplication strategy in the MS MARCO V2 document collection to avoid near-duplicates in two stages. In the first stage, we establish an equivalence class of the documents using Locality Sensitive Hashing (LSH) with MinHash <cite class="ltx_cite ltx_citemacro_citep">(Broder, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib12" title="">1997</a>)</cite> and 9-gram shingles.
Next, we selected a representative document for each equivalence class for our refined MS MARCO V2.1 document collection,<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>MS MARCO V2.1 document collection: <a class="ltx_ref ltx_href" href="https://msmarco.z22.web.core.windows.net/msmarcoranking/msmarco_v2.1_doc.tar" title="">msmarco_v2.1_doc.tar</a>.</span></span></span> reducing the duplicates in the original MS MARCO V2 document collection by 8.35% as shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.16828v1#S3.T1" title="Table 1 â€£ RAG Output â€£ 3.1. RAG Input/Output Definitions â€£ 3. Our Framework â€£ RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_tag">TableÂ 1</span></a>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">MS MARCO V2.1 Segment Collection.</h4>
<div class="ltx_para" id="S4.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p1.1">We segment the MS MARCO V2.1 document collection into overlapping segments (or chunks) and develop the MS MARCO V2.1 segment collection<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>MS MARCO V2.1 segment collection: <a class="ltx_ref ltx_href" href="https://msmarco.z22.web.core.windows.net/msmarcoranking/msmarco_v2.1_doc_segmented.tar" title="">msmarco_v2.1_doc_segmented.tar</a>.</span></span></span> with more than 113 million text segments (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.16828v1#S3.T1" title="Table 1 â€£ RAG Output â€£ 3.1. RAG Input/Output Definitions â€£ 3. Our Framework â€£ RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_tag">TableÂ 1</span></a>). We utilize a sliding window technique to generate the segments, by fixing the sliding window size of 10 sentences and a stride of 5 sentences to create each segment, roughly on average, between 500â€“1000 characters long. To easily map each segment back to the document, every segment contains the document ID within the segment ID. Further, two new fields: <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px2.p1.1.1">start_char</span> and <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px2.p1.1.2">end_char</span> indicate the start and the end position character of where the segment begins and ends in the mapped MS MARCO V2.1 document collection.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Topic Collection</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Topics, i.e., user queries, are crucial for robust evaluation of RAG systems. Traditionally, popular retrieval and traditional QA benchmarks primarily consist of factoid queries, where answers are typically found within a single sentence or paragraph. However, these topics lack complexity, leading to short answers that can be easily memorized by LLMs. For instance, MS MARCO <cite class="ltx_cite ltx_citemacro_citep">(Bajaj etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib9" title="">2016</a>)</cite> surprisingly contains up to 55% factoid queries <cite class="ltx_cite ltx_citemacro_citep">(Bolotova etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib10" title="">2022</a>; Rosset etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib52" title="">2024</a>)</cite>.
To avoid short-form answers in RAG, we utilize two collections containing non-factoid topics covering information about diverse topics and requiring long-form answers. We describe these collections below:</p>
</div>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2. </span>TREC-RAGgy and TREC-Researchy 2024 topic distribution. The table shows the top-<math alttext="5" class="ltx_Math" display="inline" id="S5.T2.2.m1.1"><semantics id="S5.T2.2.m1.1b"><mn id="S5.T2.2.m1.1.1" xref="S5.T2.2.m1.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S5.T2.2.m1.1c"><cn id="S5.T2.2.m1.1.1.cmml" type="integer" xref="S5.T2.2.m1.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.m1.1d">5</annotation><annotation encoding="application/x-llamapun" id="S5.T2.2.m1.1e">5</annotation></semantics></math> categories in topic classification for TREC-RAGgy, intrinsic attributes for TREC-Researchy, and the first word in all topics. Definitions in more detail can be found in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.16828v1#A1" title="Appendix A TREC-RAGgy 2024: Additional Details â€£ RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_tag">AppendixÂ A</span></a> &amp; <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#A2" title="Appendix B TREC-Researchy 2024: Additional Details â€£ RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_tag">B</span></a>.</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S5.T2.3" style="width:208.1pt;height:78.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-62.7pt,23.7pt) scale(0.62393816855334,0.62393816855334) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T2.3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.3.1.1.1">
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T2.3.1.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.3.1.1.1.1.1" style="font-size:90%;">TREC-RAGgy 2024</span></th>
<th class="ltx_td ltx_nopad_l ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T2.3.1.1.1.2" style="padding-left:1.0pt;padding-right:1.0pt;"></th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T2.3.1.1.1.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.3.1.1.1.3.1" style="font-size:90%;">TREC-Researchy 2024</span></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S5.T2.3.1.1.1.4" style="padding-left:1.0pt;padding-right:1.0pt;"></th>
<td class="ltx_td ltx_border_tt" id="S5.T2.3.1.1.1.5" style="padding-left:1.0pt;padding-right:1.0pt;"></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.1.2.2">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T2.3.1.2.2.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.3.1.2.2.1.1" style="font-size:90%;">Topic Category</span></th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.3.1.2.2.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.2.2.2.1" style="font-size:90%;">%</span></th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T2.3.1.2.2.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.3.1.2.2.3.1" style="font-size:90%;">First Word</span></th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.3.1.2.2.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.2.2.4.1" style="font-size:90%;">%</span></th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T2.3.1.2.2.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.3.1.2.2.5.1" style="font-size:90%;">Intrinsic Attributes</span></th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.3.1.2.2.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.2.2.6.1" style="font-size:90%;">%</span></th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T2.3.1.2.2.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.3.1.2.2.7.1" style="font-size:90%;">First Word</span></th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.3.1.2.2.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.2.2.8.1" style="font-size:90%;">%</span></th>
</tr>
<tr class="ltx_tr" id="S5.T2.3.1.3.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="S5.T2.3.1.3.3.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.3.3.1.1" style="font-size:90%;">Aggregation</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S5.T2.3.1.3.3.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.3.3.2.1" style="font-size:90%;">24.2</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="S5.T2.3.1.3.3.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.3.3.3.1" style="font-size:90%;">What</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S5.T2.3.1.3.3.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.3.3.4.1" style="font-size:90%;">37.5</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="S5.T2.3.1.3.3.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.3.3.5.1" style="font-size:90%;">Knowledge-Intensive</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S5.T2.3.1.3.3.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.3.3.6.1" style="font-size:90%;">79.8</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="S5.T2.3.1.3.3.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.3.3.7.1" style="font-size:90%;">How</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S5.T2.3.1.3.3.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.3.3.8.1" style="font-size:90%;">41.0</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.1.4.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T2.3.1.4.4.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.4.4.1.1" style="font-size:90%;">Simple w/ cond.</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S5.T2.3.1.4.4.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.4.4.2.1" style="font-size:90%;">23.3</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T2.3.1.4.4.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.4.4.3.1" style="font-size:90%;">How</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S5.T2.3.1.4.4.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.4.4.4.1" style="font-size:90%;">27.5</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T2.3.1.4.4.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.4.4.5.1" style="font-size:90%;">Multi-Faceted</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S5.T2.3.1.4.4.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.4.4.6.1" style="font-size:90%;">75.7</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T2.3.1.4.4.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.4.4.7.1" style="font-size:90%;">Why</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S5.T2.3.1.4.4.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.4.4.8.1" style="font-size:90%;">25.5</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.1.5.5">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T2.3.1.5.5.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.5.5.1.1" style="font-size:90%;">Set</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S5.T2.3.1.5.5.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.5.5.2.1" style="font-size:90%;">20.8</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T2.3.1.5.5.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.5.5.3.1" style="font-size:90%;">Why</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S5.T2.3.1.5.5.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.5.5.4.1" style="font-size:90%;">3.3</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T2.3.1.5.5.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.5.5.5.1" style="font-size:90%;">Reasoning-Intensive</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S5.T2.3.1.5.5.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.5.5.6.1" style="font-size:90%;">75.5</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T2.3.1.5.5.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.5.5.7.1" style="font-size:90%;">What</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S5.T2.3.1.5.5.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.5.5.8.1" style="font-size:90%;">15.0</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.1.6.6">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T2.3.1.6.6.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.6.6.1.1" style="font-size:90%;">Simple</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S5.T2.3.1.6.6.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.6.6.2.1" style="font-size:90%;">10.0</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T2.3.1.6.6.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.6.6.3.1" style="font-size:90%;">Is</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S5.T2.3.1.6.6.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.6.6.4.1" style="font-size:90%;">2.5</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T2.3.1.6.6.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.6.6.5.1" style="font-size:90%;">Subjective</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S5.T2.3.1.6.6.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.6.6.6.1" style="font-size:90%;">48.5</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T2.3.1.6.6.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.6.6.7.1" style="font-size:90%;">Is</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S5.T2.3.1.6.6.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.6.6.8.1" style="font-size:90%;">5.2</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.1.7.7">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" id="S5.T2.3.1.7.7.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.7.7.1.1" style="font-size:90%;">Comparison</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S5.T2.3.1.7.7.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.7.7.2.1" style="font-size:90%;">6.7</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" id="S5.T2.3.1.7.7.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.7.7.3.1" style="font-size:90%;">When</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S5.T2.3.1.7.7.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.7.7.4.1" style="font-size:90%;">1.7</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" id="S5.T2.3.1.7.7.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.7.7.5.1" style="font-size:90%;">Assumptive</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S5.T2.3.1.7.7.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.7.7.6.1" style="font-size:90%;">25.7</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" id="S5.T2.3.1.7.7.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.7.7.7.1" style="font-size:90%;">Should</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S5.T2.3.1.7.7.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T2.3.1.7.7.8.1" style="font-size:90%;">2.2</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">TREC-RAGgy 2024</h4>
<div class="ltx_para" id="S5.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px1.p1.1">We develop TREC-RAGgy 2024, a collection with topics filtered from TREC Deep Learning 2021-2023 tracks <cite class="ltx_cite ltx_citemacro_citep">(Craswell etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib16" title="">2021</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib17" title="">2022</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib18" title="">2024</a>)</cite>, based on topic category and generated-answer classification. We classify each available topic into seven categories described in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.16828v1#A1" title="Appendix A TREC-RAGgy 2024: Additional Details â€£ RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_tag">AppendixÂ A</span></a> and filter out a subset of topics that either have a long-form answer or require information aggregation across multiple sources of information.
Out of the 210 original topics available, we filter and include 120 topics (57.1%) in the TREC-RAGgy 2024 topic collection.<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>TREC-RAGgy 2024 topic collection: <a class="ltx_ref ltx_href" href="https://github.com/castorini/anserini-tools/blob/master/topics-and-qrels/topics.rag24.raggy-dev.txt" title="">topics.rag24.raggy-dev.txt</a>.</span></span></span> From Â <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.16828v1#S5.T2" title="Table 2 â€£ 5. Topic Collection â€£ RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_tag">TableÂ 2</span></a>, we observe 24.2% of the topics included are â€œaggregationâ€, indicating RAG systems require to aggregate information from multiple retrieved segments to generate an accurate long-form answer. Similarly, 65% of the topics start with â€œwhatâ€ or â€œhowâ€. Overall, a majority of the topics are useful for evaluation containing diverse topic categories requiring a long-form answer.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">TREC-Researchy 2024</h4>
<div class="ltx_para" id="S5.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px2.p1.3">Researchy Questions, introduced in <cite class="ltx_cite ltx_citemacro_citet">Rosset etÂ al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib52" title="">2024</a>)</cite>, contains 102K non-factoid topics with long-form answers. These topics were curated from Bing Search logs and evaluated by GPT-4 on a scale of 0â€“10 based on eight intrinsic attributes, such as subjectivity and multifacetedness (definitions provided in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.16828v1#A2" title="Appendix B TREC-Researchy 2024: Additional Details â€£ RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_tag">AppendixÂ B</span></a>). Notably, unlike TREC-RAGgy 2024, these queries lack relevance judgments.
To curate a smaller development subset for a faster evaluation of RAG systems, we employ a sampler designed to maximize diversity based on the eight intrinsic attributes.
This is achieved by iteratively selecting the query with the highest <math alttext="l_{1}" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px2.p1.1.m1.1"><semantics id="S5.SS0.SSS0.Px2.p1.1.m1.1a"><msub id="S5.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S5.SS0.SSS0.Px2.p1.1.m1.1.1.cmml"><mi id="S5.SS0.SSS0.Px2.p1.1.m1.1.1.2" xref="S5.SS0.SSS0.Px2.p1.1.m1.1.1.2.cmml">l</mi><mn id="S5.SS0.SSS0.Px2.p1.1.m1.1.1.3" xref="S5.SS0.SSS0.Px2.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.1.m1.1b"><apply id="S5.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS0.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.1.m1.1.1">subscript</csymbol><ci id="S5.SS0.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S5.SS0.SSS0.Px2.p1.1.m1.1.1.2">ğ‘™</ci><cn id="S5.SS0.SSS0.Px2.p1.1.m1.1.1.3.cmml" type="integer" xref="S5.SS0.SSS0.Px2.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.1.m1.1c">l_{1}</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px2.p1.1.m1.1d">italic_l start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> norm in the intrinsic attribute space (of all eight dimensions) relative to the already-sampled set.
The resultant topic set we dub as TREC-Researchy 2024.<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>TREC-Researchy 2024 topic collection: <a class="ltx_ref ltx_href" href="https://github.com/castorini/anserini-tools/blob/master/topics-and-qrels/topics.rag24.researchy-dev.txt" title="">topics.rag24.researchy-dev.txt</a></span></span></span>
From <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.16828v1#S5.T2" title="Table 2 â€£ 5. Topic Collection â€£ RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_tag">TableÂ 2</span></a>, about <math alttext="80" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px2.p1.2.m2.1"><semantics id="S5.SS0.SSS0.Px2.p1.2.m2.1a"><mn id="S5.SS0.SSS0.Px2.p1.2.m2.1.1" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1.cmml">80</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.2.m2.1b"><cn id="S5.SS0.SSS0.Px2.p1.2.m2.1.1.cmml" type="integer" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1">80</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.2.m2.1c">80</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px2.p1.2.m2.1d">80</annotation></semantics></math>% of the topics are Knowledge-Intensive and about <math alttext="76" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px2.p1.3.m3.1"><semantics id="S5.SS0.SSS0.Px2.p1.3.m3.1a"><mn id="S5.SS0.SSS0.Px2.p1.3.m3.1.1" xref="S5.SS0.SSS0.Px2.p1.3.m3.1.1.cmml">76</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.3.m3.1b"><cn id="S5.SS0.SSS0.Px2.p1.3.m3.1.1.cmml" type="integer" xref="S5.SS0.SSS0.Px2.p1.3.m3.1.1">76</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.3.m3.1c">76</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px2.p1.3.m3.1d">76</annotation></semantics></math>% are Multi-Faceted highlighting the need for effective RAG systems.
Additionally, 66.5% of topics start with â€œhowâ€ or â€œwhyâ€, emphasizing explanatory questions.
These distributions suggest that TREC-Researchy 2024 prioritizes complex and multi-dimensional topics.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>TREC 2024 RAG Baselines</h2>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Retrieval.</h4>
<div class="ltx_para" id="S6.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px1.p1.6">Our retrieval module integrates both first-stage retrievers and rerankers.
We use BM25 available in AnseriniÂ <cite class="ltx_cite ltx_citemacro_citep">(Yang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib58" title="">2017</a>)</cite> with the following default parameters (<math alttext="k_{1}=0.9" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px1.p1.1.m1.1"><semantics id="S6.SS0.SSS0.Px1.p1.1.m1.1a"><mrow id="S6.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S6.SS0.SSS0.Px1.p1.1.m1.1.1.cmml"><msub id="S6.SS0.SSS0.Px1.p1.1.m1.1.1.2" xref="S6.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml"><mi id="S6.SS0.SSS0.Px1.p1.1.m1.1.1.2.2" xref="S6.SS0.SSS0.Px1.p1.1.m1.1.1.2.2.cmml">k</mi><mn id="S6.SS0.SSS0.Px1.p1.1.m1.1.1.2.3" xref="S6.SS0.SSS0.Px1.p1.1.m1.1.1.2.3.cmml">1</mn></msub><mo id="S6.SS0.SSS0.Px1.p1.1.m1.1.1.1" xref="S6.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml">=</mo><mn id="S6.SS0.SSS0.Px1.p1.1.m1.1.1.3" xref="S6.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px1.p1.1.m1.1b"><apply id="S6.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S6.SS0.SSS0.Px1.p1.1.m1.1.1"><eq id="S6.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S6.SS0.SSS0.Px1.p1.1.m1.1.1.1"></eq><apply id="S6.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S6.SS0.SSS0.Px1.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S6.SS0.SSS0.Px1.p1.1.m1.1.1.2.1.cmml" xref="S6.SS0.SSS0.Px1.p1.1.m1.1.1.2">subscript</csymbol><ci id="S6.SS0.SSS0.Px1.p1.1.m1.1.1.2.2.cmml" xref="S6.SS0.SSS0.Px1.p1.1.m1.1.1.2.2">ğ‘˜</ci><cn id="S6.SS0.SSS0.Px1.p1.1.m1.1.1.2.3.cmml" type="integer" xref="S6.SS0.SSS0.Px1.p1.1.m1.1.1.2.3">1</cn></apply><cn id="S6.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml" type="float" xref="S6.SS0.SSS0.Px1.p1.1.m1.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px1.p1.1.m1.1c">k_{1}=0.9</annotation><annotation encoding="application/x-llamapun" id="S6.SS0.SSS0.Px1.p1.1.m1.1d">italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.9</annotation></semantics></math> and <math alttext="b=0.4" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px1.p1.2.m2.1"><semantics id="S6.SS0.SSS0.Px1.p1.2.m2.1a"><mrow id="S6.SS0.SSS0.Px1.p1.2.m2.1.1" xref="S6.SS0.SSS0.Px1.p1.2.m2.1.1.cmml"><mi id="S6.SS0.SSS0.Px1.p1.2.m2.1.1.2" xref="S6.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml">b</mi><mo id="S6.SS0.SSS0.Px1.p1.2.m2.1.1.1" xref="S6.SS0.SSS0.Px1.p1.2.m2.1.1.1.cmml">=</mo><mn id="S6.SS0.SSS0.Px1.p1.2.m2.1.1.3" xref="S6.SS0.SSS0.Px1.p1.2.m2.1.1.3.cmml">0.4</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px1.p1.2.m2.1b"><apply id="S6.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S6.SS0.SSS0.Px1.p1.2.m2.1.1"><eq id="S6.SS0.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S6.SS0.SSS0.Px1.p1.2.m2.1.1.1"></eq><ci id="S6.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S6.SS0.SSS0.Px1.p1.2.m2.1.1.2">ğ‘</ci><cn id="S6.SS0.SSS0.Px1.p1.2.m2.1.1.3.cmml" type="float" xref="S6.SS0.SSS0.Px1.p1.2.m2.1.1.3">0.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px1.p1.2.m2.1c">b=0.4</annotation><annotation encoding="application/x-llamapun" id="S6.SS0.SSS0.Px1.p1.2.m2.1d">italic_b = 0.4</annotation></semantics></math>), to retrieve the top-<math alttext="100" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px1.p1.3.m3.1"><semantics id="S6.SS0.SSS0.Px1.p1.3.m3.1a"><mn id="S6.SS0.SSS0.Px1.p1.3.m3.1.1" xref="S6.SS0.SSS0.Px1.p1.3.m3.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px1.p1.3.m3.1b"><cn id="S6.SS0.SSS0.Px1.p1.3.m3.1.1.cmml" type="integer" xref="S6.SS0.SSS0.Px1.p1.3.m3.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px1.p1.3.m3.1c">100</annotation><annotation encoding="application/x-llamapun" id="S6.SS0.SSS0.Px1.p1.3.m3.1d">100</annotation></semantics></math> segments for a given topic.
Next, RankZephyrÂ <cite class="ltx_cite ltx_citemacro_citep">(Pradeep etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib48" title="">2023b</a>)</cite>, a state-of-the-art listwise reranker, is used to rerank the top-<math alttext="100" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px1.p1.4.m4.1"><semantics id="S6.SS0.SSS0.Px1.p1.4.m4.1a"><mn id="S6.SS0.SSS0.Px1.p1.4.m4.1.1" xref="S6.SS0.SSS0.Px1.p1.4.m4.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px1.p1.4.m4.1b"><cn id="S6.SS0.SSS0.Px1.p1.4.m4.1.1.cmml" type="integer" xref="S6.SS0.SSS0.Px1.p1.4.m4.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px1.p1.4.m4.1c">100</annotation><annotation encoding="application/x-llamapun" id="S6.SS0.SSS0.Px1.p1.4.m4.1d">100</annotation></semantics></math> candidates.
We use RankZephyr<sub class="ltx_sub" id="S6.SS0.SSS0.Px1.p1.5.1"><math alttext="\rho" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px1.p1.5.1.m1.1"><semantics id="S6.SS0.SSS0.Px1.p1.5.1.m1.1a"><mi id="S6.SS0.SSS0.Px1.p1.5.1.m1.1.1" xref="S6.SS0.SSS0.Px1.p1.5.1.m1.1.1.cmml">Ï</mi><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px1.p1.5.1.m1.1b"><ci id="S6.SS0.SSS0.Px1.p1.5.1.m1.1.1.cmml" xref="S6.SS0.SSS0.Px1.p1.5.1.m1.1.1">ğœŒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px1.p1.5.1.m1.1c">\rho</annotation><annotation encoding="application/x-llamapun" id="S6.SS0.SSS0.Px1.p1.5.1.m1.1d">italic_Ï</annotation></semantics></math></sub>, a variant, that reranks the candidates progressively, i.e., makes three passes iteratively, refining the final ranked candidate list to achieve better precision.
An easy-to-use implementation of RankZephyr is available via the <span class="ltx_text ltx_font_typewriter" id="S6.SS0.SSS0.Px1.p1.6.2">rank_llm</span> package, along with various other rerankers like RankGPTÂ <cite class="ltx_cite ltx_citemacro_citep">(Sun etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib54" title="">2023</a>)</cite>, which we provide as secondary baselines.
Finally, the top-<math alttext="20" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px1.p1.6.m5.1"><semantics id="S6.SS0.SSS0.Px1.p1.6.m5.1a"><mn id="S6.SS0.SSS0.Px1.p1.6.m5.1.1" xref="S6.SS0.SSS0.Px1.p1.6.m5.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px1.p1.6.m5.1b"><cn id="S6.SS0.SSS0.Px1.p1.6.m5.1.1.cmml" type="integer" xref="S6.SS0.SSS0.Px1.p1.6.m5.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px1.p1.6.m5.1c">20</annotation><annotation encoding="application/x-llamapun" id="S6.SS0.SSS0.Px1.p1.6.m5.1d">20</annotation></semantics></math> reranked documents from the document collection are passed onto the next stage for RAG generation.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Augmented Generation.</h4>
<div class="ltx_para" id="S6.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px2.p1.1">Our generation module integrates two popular and commercially available LLMs: (i) Command R+ is Cohereâ€™s instruction following LLM developed for complex RAG pipelinesÂ <cite class="ltx_cite ltx_citemacro_citep">(Cohere, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib15" title="">2024</a>)</cite>; (ii) GPT-4o is the latest GPT version from OpenAIÂ <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib42" title="">2024</a>)</cite>. Given that Command R+ cites in a span level, we map the citations to their parent sentences.
For GPT-4o, we follow the zero-shot ChatQA prompt templateÂ <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib36" title="">2024b</a>)</cite> and cite relevant segments within the text (in-line) using the IEEE format. An example of the prompt template is shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.16828v1#A3.F2" title="Figure 2 â€£ Appendix C RagnarÃ¶k System Arena â€£ RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_tag">FigureÂ 2</span></a>, in the Appendix.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">RAG-Bench Evaluation</h4>
<div class="ltx_para" id="S6.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px3.p1.1">Evaluating different RAG answers is challenging as multiple factors within the output response are crucial for effectiveness evaluation. To combat this, recent works rely on an LLM-as-a-judge setup <cite class="ltx_cite ltx_citemacro_citep">(Zheng etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib59" title="">2023</a>)</cite>, where strong LLM assessors judge the RAG-generated output in a pairwise evaluation style (side-by-side) in a head-on tournament. In our work, we briefly overview our baseline techniques using human evaluators. A complete illustration can be found in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.16828v1#A3" title="Appendix C RagnarÃ¶k System Arena â€£ RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_tag">AppendixÂ C</span></a>, in the Appendix. The Command R+ baseline outputs shorter answers and cites more relevant segments, whereas, the GPT-4o baseline outputs longer and more detailed answers and cites fewer segments. Therefore, for topics in both TREC-Raggy and TREC-Researchy 2024, GPT-4o intuitively is the better choice for RAG answer generation. We leave it for future work, to empirically compute the win rates (in %) between our baselines in the RAG-bench evaluation.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>RagnarÃ¶k System Arena</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">Heavily inspired by the success of Chatbot ArenaÂ <cite class="ltx_cite ltx_citemacro_citep">(Chiang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib14" title="">2024</a>; Zheng etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib59" title="">2023</a>)</cite>, a crowdsourcing benchmark WebUI featuring anonymous battles, we extend the concept to multi-stage configurable RAG pipelines with RagnarÃ¶k.
In the arena, users interact with two unblinded/blinded RAG systems simultaneously, issuing the same topic to both.
The participants evaluate and select the pipeline that delivers their most preferred response, with the identities of the modules in the end-to-end pipeline revealed after the voting process in the blinded case.
We leverage GradioÂ <cite class="ltx_cite ltx_citemacro_citep">(Abid etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib2" title="">2019</a>)</cite> to build the WebUI for RagnarÃ¶k.
Each step of the pipeline uses REST APIs for intercommunication, enabling easy module switching within the pipeline.
This modular design simplifies the integration of different retrieval and LLM configurations, enhancing scalability and maintainability.</p>
</div>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.4"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.16828v1#A3.F3" title="Figure 3 â€£ Appendix C RagnarÃ¶k System Arena â€£ RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_tag">FigureÂ 3</span></a> in the Appendix illustrates an example topic â€œwhat inspired pink floydâ€™s the wall?â€ processed by two different pipelines: Pipeline A, comprising BM25 <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S6.SS1.p2.1.m1.1"><semantics id="S6.SS1.p2.1.m1.1a"><mo id="S6.SS1.p2.1.m1.1.1" stretchy="false" xref="S6.SS1.p2.1.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.1.m1.1b"><ci id="S6.SS1.p2.1.m1.1.1.cmml" xref="S6.SS1.p2.1.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p2.1.m1.1d">â†’</annotation></semantics></math> RankZephyr <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S6.SS1.p2.2.m2.1"><semantics id="S6.SS1.p2.2.m2.1a"><mo id="S6.SS1.p2.2.m2.1.1" stretchy="false" xref="S6.SS1.p2.2.m2.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.2.m2.1b"><ci id="S6.SS1.p2.2.m2.1.1.cmml" xref="S6.SS1.p2.2.m2.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p2.2.m2.1d">â†’</annotation></semantics></math> GPT-4o (left), and Pipeline B, comprising BM25 <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S6.SS1.p2.3.m3.1"><semantics id="S6.SS1.p2.3.m3.1a"><mo id="S6.SS1.p2.3.m3.1.1" stretchy="false" xref="S6.SS1.p2.3.m3.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.3.m3.1b"><ci id="S6.SS1.p2.3.m3.1.1.cmml" xref="S6.SS1.p2.3.m3.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.3.m3.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p2.3.m3.1d">â†’</annotation></semantics></math> RankGPT-4o <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S6.SS1.p2.4.m4.1"><semantics id="S6.SS1.p2.4.m4.1a"><mo id="S6.SS1.p2.4.m4.1.1" stretchy="false" xref="S6.SS1.p2.4.m4.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.4.m4.1b"><ci id="S6.SS1.p2.4.m4.1.1.cmml" xref="S6.SS1.p2.4.m4.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.4.m4.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p2.4.m4.1d">â†’</annotation></semantics></math> Command R+ (right) in the unblinded tab.
The outputs generated by each pipeline are compared, allowing users to discern which system provided a more satisfactory answer.
Note that when the user hovers the mouse over a citation, they can preview the cited segment.
Further, in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#A3" title="Appendix C RagnarÃ¶k System Arena â€£ RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_tag">C</span></a>, we discuss the blinded pairwise evaluation and the responses (JSON output) tab, available in the WebUI for RagnarÃ¶k.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Ongoing Work</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">RagnarÃ¶k is the first step for the ongoing work in the TREC 2024 RAG track,
by releasing the document collections, development topics, and baseline strategies for participants.
We will continue to update the pipelines to include more diverse retrieval models including state-of-the-art dual encoders such as Artic-Embed <cite class="ltx_cite ltx_citemacro_citep">(Merrick etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib39" title="">2024</a>)</cite> and effective pointwise/pairwise rerankersÂ <cite class="ltx_cite ltx_citemacro_citep">(Pradeep etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib46" title="">2021</a>)</cite>.
We plan to add additional support for more advanced RAG techniques like SelfRAGÂ <cite class="ltx_cite ltx_citemacro_citep">(Asai etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib8" title="">2023</a>)</cite> and CRAGÂ <cite class="ltx_cite ltx_citemacro_citep">(Yan etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib57" title="">2024</a>)</cite>.
For the TREC 2024 RAG track test topics, we plan to conduct a new and fresh scrape of the Bing search logs closer to the submission period. This approach will compile a fresh and recent set of topics, similar to <cite class="ltx_cite ltx_citemacro_citet">Rosset etÂ al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib52" title="">2024</a>)</cite>, thereby minimizing the risk of data leakage and ensuring a fair evaluation with existing commercially available LLMs.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">The next phase of our efforts will focus on finalizing the evaluation methodology using an automatic nugget-based evaluation, following earlier work in <cite class="ltx_cite ltx_citemacro_citet">Lin and Demner-Fushman (<a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib31" title="">2006</a>)</cite> and first discussed in the TREC RAG 2024 presentation deck.<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_href" href="https://cs.uwaterloo.ca/~jimmylin/publications/Lin_etal_TREC2023-planning.pdf" title="">https://cs.uwaterloo.ca/jÌƒimmylin/publications/Lin_etal_TREC2023-planning.pdf</a></span></span></span>
The nugget-based evaluation is recently gaining popularity <cite class="ltx_cite ltx_citemacro_citep">(Alaofi etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib3" title="">2024</a>; Raina and Gales, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib49" title="">2024</a>; Arabzadeh and Clarke, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib7" title="">2024</a>; Mayfield etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib38" title="">2024</a>)</cite>, and is becoming the de facto strategy for RAG evaluation.</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8. </span>Conclusion</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">The emergence of retrieval-augmented generation (RAG) has revolutionized modern search systems by allowing real-time data incorporation into large language models (LLMs).
In our work, we develop a reusable end-to-end framework, RagnarÃ¶k, providing reproducible baselines and a WebUI serving a RAG battle arena for retriever, reranker, and generation models.
We also introduce the MS MARCO V2.1 collection, carefully curated topics from the TREC-DL 2021-2023 queries and Researchy Questions, and I/O definitions to assist users in the RAG paradigm.
Additionally, the paper identifies key industrial baselines (such as Cohereâ€™s Command R+ and OpenAIâ€™s GPT-4o) and includes a qualitative analysis of the baselines on the development topics.
By open-sourcing this framework, we aim to standardize RAG applications in preparation for the upcoming TREC 2024 RAG challenge.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
We thank Ian Soboroff for the MS MARCO V2 document collection deduplication for our TREC 2024 RAG track, Cohere for providing us with the necessary credits to evaluate Command-R+, and Microsoft for providing Azure credits to evaluate GPT-4o. Additionally, we thank Corby Rosset for the discussions surrounding Researchy QuestionsÂ <cite class="ltx_cite ltx_citemacro_citep">(Rosset etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib52" title="">2024</a>)</cite>.

</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abid etÂ al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Abubakar Abid, Ali Abdalla, Ali Abid, Dawood Khan, Abdulrahman Alfozan, and JamesÂ Y. Zou. 2019.

</span>
<span class="ltx_bibblock">Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.3.1">CoRR</em> abs/1906.02569 (2019).

</span>
<span class="ltx_bibblock">arXiv:1906.02569

<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1906.02569" title="">http://arxiv.org/abs/1906.02569</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alaofi etÂ al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Marwah Alaofi, Negar Arabzadeh, Charles L.Â A. Clarke, and Mark Sanderson. 2024.

</span>
<span class="ltx_bibblock">Generative Information Retrieval Evaluation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">CoRR</em> abs/2404.08137 (2024).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2404.08137" title="">https://doi.org/10.48550/ARXIV.2404.08137</a>
arXiv:2404.08137

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aliannejadi etÂ al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Mohammad Aliannejadi, Zahra Abbasiantaeb, Shubham Chatterjee, Jeffery Dalton, and Leif Azzopardi. 2024.

</span>
<span class="ltx_bibblock">TREC iKAT 2023: The Interactive Knowledge Assistance Track Overview.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">CoRR</em> abs/2401.01330 (2024).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2401.01330" title="">https://doi.org/10.48550/ARXIV.2401.01330</a>
arXiv:2401.01330

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amouyal etÂ al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
SamuelÂ Joseph Amouyal, Ohad Rubin, Ori Yoran, Tomer Wolfson, Jonathan Herzig, and Jonathan Berant. 2022.

</span>
<span class="ltx_bibblock">QAMPARI: : An Open-domain Question Answering Benchmark for Questions with Many Answers from Multiple Paragraphs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">CoRR</em> abs/2205.12665 (2022).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2205.12665" title="">https://doi.org/10.48550/ARXIV.2205.12665</a>
arXiv:2205.12665

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anil etÂ al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, AndrewÂ M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, TimothyÂ P. Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, PaulÂ Ronald Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan
Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, AnaÃ¯s White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, and et al. 2023.

</span>
<span class="ltx_bibblock">Gemini: A Family of Highly Capable Multimodal Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">CoRR</em> abs/2312.11805 (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2312.11805" title="">https://doi.org/10.48550/ARXIV.2312.11805</a>
arXiv:2312.11805

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arabzadeh and Clarke (2024)</span>
<span class="ltx_bibblock">
Negar Arabzadeh and Charles L.Â A. Clarke. 2024.

</span>
<span class="ltx_bibblock">A Comparison of Methods for Evaluating Generative IR.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">CoRR</em> abs/2404.04044 (2024).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2404.04044" title="">https://doi.org/10.48550/ARXIV.2404.04044</a>
arXiv:2404.04044

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asai etÂ al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023.

</span>
<span class="ltx_bibblock">Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">CoRR</em> abs/2310.11511 (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2310.11511" title="">https://doi.org/10.48550/ARXIV.2310.11511</a>
arXiv:2310.11511

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bajaj etÂ al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. 2016.

</span>
<span class="ltx_bibblock">MS MARCO: A Human Generated MAchine Reading COmprehension Dataset.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">CoRR</em> abs/1611.09268 (2016).

</span>
<span class="ltx_bibblock">arXiv:1611.09268

<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1611.09268" title="">http://arxiv.org/abs/1611.09268</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bolotova etÂ al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Valeria Bolotova, Vladislav Blinov, Falk Scholer, W.Â Bruce Croft, and Mark Sanderson. 2022.

</span>
<span class="ltx_bibblock">A Non-Factoid Question-Answering Taxonomy. In <em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">SIGIR â€™22: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022</em>, Enrique AmigÃ³, Pablo Castells, Julio Gonzalo, Ben Carterette, J.Â Shane Culpepper, and Gabriella Kazai (Eds.). ACM, 1196â€“1207.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3477495.3531926" title="">https://doi.org/10.1145/3477495.3531926</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borgeaud etÂ al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George vanÂ den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de LasÂ Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, JackÂ W. Rae, Erich Elsen, and Laurent Sifre. 2022.

</span>
<span class="ltx_bibblock">Improving Language Models by Retrieving from Trillions of Tokens. In <em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA</em> <em class="ltx_emph ltx_font_italic" id="bib.bib11.4.2">(Proceedings of Machine Learning Research, Vol.Â 162)</em>, Kamalika Chaudhuri, Stefanie Jegelka, LeÂ Song, Csaba SzepesvÃ¡ri, Gang Niu, and Sivan Sabato (Eds.). PMLR, 2206â€“2240.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.mlr.press/v162/borgeaud22a.html" title="">https://proceedings.mlr.press/v162/borgeaud22a.html</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Broder (1997)</span>
<span class="ltx_bibblock">
AndreiÂ Z. Broder. 1997.

</span>
<span class="ltx_bibblock">On the resemblance and containment of documents. In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Compression and Complexity of SEQUENCES 1997, Positano, Amalfitan Coast, Salerno, Italy, June 11-13, 1997, Proceedings</em>, Bruno Carpentieri, AlfredoÂ De Santis, Ugo Vaccaro, and JamesÂ A. Storer (Eds.). IEEE, 21â€“29.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/SEQUEN.1997.666900" title="">https://doi.org/10.1109/SEQUEN.1997.666900</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chase (2022)</span>
<span class="ltx_bibblock">
Harrison Chase. 2022.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">LangChain</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/langchain-ai/langchain" title="">https://github.com/langchain-ai/langchain</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang etÂ al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, AnastasiosÂ Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, MichaelÂ I. Jordan, JosephÂ E. Gonzalez, and Ion Stoica. 2024.

</span>
<span class="ltx_bibblock">Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">CoRR</em> abs/2403.04132 (2024).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2403.04132" title="">https://doi.org/10.48550/ARXIV.2403.04132</a>
arXiv:2403.04132

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cohere (2024)</span>
<span class="ltx_bibblock">
Cohere. 2024.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Introducing Command R+: A Scalable LLM Built for Business</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://cohere.com/blog/command-r-plus-microsoft-azure" title="">https://cohere.com/blog/command-r-plus-microsoft-azure</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Craswell etÂ al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Jimmy Lin. 2021.

</span>
<span class="ltx_bibblock">Overview of the TREC 2021 Deep Learning Track. In <em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">Proceedings of the Thirtieth Text REtrieval Conference, TREC 2021, online, November 15-19, 2021</em> <em class="ltx_emph ltx_font_italic" id="bib.bib16.4.2">(NIST Special Publication, Vol.Â 500-335)</em>, Ian Soboroff and Angela Ellis (Eds.). National Institute of Standards and Technology (NIST).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://trec.nist.gov/pubs/trec30/papers/Overview-DL.pdf" title="">https://trec.nist.gov/pubs/trec30/papers/Overview-DL.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Craswell etÂ al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, Jimmy Lin, EllenÂ M. Voorhees, and Ian Soboroff. 2022.

</span>
<span class="ltx_bibblock">Overview of the TREC 2022 Deep Learning Track. In <em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">Proceedings of the Thirty-First Text REtrieval Conference, TREC 2022, online, November 15-19, 2022</em> <em class="ltx_emph ltx_font_italic" id="bib.bib17.4.2">(NIST Special Publication, Vol.Â 500-338)</em>, Ian Soboroff and Angela Ellis (Eds.). National Institute of Standards and Technology (NIST).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://trec.nist.gov/pubs/trec31/papers/Overview_deep.pdf" title="">https://trec.nist.gov/pubs/trec31/papers/Overview_deep.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Craswell etÂ al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Nick Craswell, Bhaskar Mitra, Emine Yilmaz, HosseinÂ A. Rahmani, Daniel Campos, Jimmy Lin, EllenÂ M. Voorhees, and Ian Soboroff. 2024.

</span>
<span class="ltx_bibblock">Overview of the TREC 2023 Deep Learning Track. In <em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">Text REtrieval Conference (TREC)</em>. NIST, TREC.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.microsoft.com/en-us/research/publication/overview-of-the-trec-2023-deep-learning-track/" title="">https://www.microsoft.com/en-us/research/publication/overview-of-the-trec-2023-deep-learning-track/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan etÂ al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019.

</span>
<span class="ltx_bibblock">ELI5: Long Form Question Answering. In <em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers</em>, Anna Korhonen, DavidÂ R. Traum, and LluÃ­s MÃ rquez (Eds.). Association for Computational Linguistics, 3558â€“3567.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/V1/P19-1346" title="">https://doi.org/10.18653/V1/P19-1346</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023b.

</span>
<span class="ltx_bibblock">Enabling Large Language Models to Generate Text with Citations. In <em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023</em>, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 6465â€“6488.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/V1/2023.EMNLP-MAIN.398" title="">https://doi.org/10.18653/V1/2023.EMNLP-MAIN.398</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. 2023a.

</span>
<span class="ltx_bibblock">Retrieval-Augmented Generation for Large Language Models: A Survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.3.1">CoRR</em> abs/2312.10997 (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2312.10997" title="">https://doi.org/10.48550/ARXIV.2312.10997</a>
arXiv:2312.10997

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guu etÂ al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020.

</span>
<span class="ltx_bibblock">Retrieval Augmented Language Model Pre-Training. In <em class="ltx_emph ltx_font_italic" id="bib.bib22.3.1">Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event</em> <em class="ltx_emph ltx_font_italic" id="bib.bib22.4.2">(Proceedings of Machine Learning Research, Vol.Â 119)</em>. PMLR, 3929â€“3938.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://proceedings.mlr.press/v119/guu20a.html" title="">http://proceedings.mlr.press/v119/guu20a.html</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard and Grave (2021)</span>
<span class="ltx_bibblock">
Gautier Izacard and Edouard Grave. 2021.

</span>
<span class="ltx_bibblock">Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering. In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021</em>, Paola Merlo, JÃ¶rg Tiedemann, and Reut Tsarfaty (Eds.). Association for Computational Linguistics, 874â€“880.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/V1/2021.EACL-MAIN.74" title="">https://doi.org/10.18653/V1/2021.EACL-MAIN.74</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jimeno-Yepes etÂ al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Antonio Jimeno-Yepes, Yao You, Jan Milczek, Sebastian Laverde, and Renyu Li. 2024.

</span>
<span class="ltx_bibblock">Financial Report Chunking for Effective Retrieval Augmented Generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">CoRR</em> abs/2402.05131 (2024).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2402.05131" title="">https://doi.org/10.48550/ARXIV.2402.05131</a>
arXiv:2402.05131

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin etÂ al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, and Zhicheng Dou. 2024.

</span>
<span class="ltx_bibblock">FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">CoRR</em> abs/2405.13576 (2024).

</span>
<span class="ltx_bibblock">arXiv:2405.13576

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2405.13576" title="">https://arxiv.org/abs/2405.13576</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kamalloo etÂ al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Ehsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan Thakur, and Jimmy Lin. 2023.

</span>
<span class="ltx_bibblock">HAGRID: A Human-LLM Collaborative Dataset for Generative Information-Seeking with Attribution.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">CoRR</em> abs/2307.16883 (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2307.16883" title="">https://doi.org/10.48550/ARXIV.2307.16883</a>
arXiv:2307.16883

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khandelwal etÂ al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020.

</span>
<span class="ltx_bibblock">Generalization through Memorization: Nearest Neighbor Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020</em>. OpenReview.net.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=HklBjCEKvH" title="">https://openreview.net/forum?id=HklBjCEKvH</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kulkarni etÂ al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Sayali Kulkarni, Sheide Chammas, Wan Zhu, Fei Sha, and Eugene Ie. 2020.

</span>
<span class="ltx_bibblock">AQuaMuSe: Automatically Generating Datasets for Query-Based Multi-Document Summarization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">CoRR</em> abs/2010.12694 (2020).

</span>
<span class="ltx_bibblock">arXiv:2010.12694

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2010.12694" title="">https://arxiv.org/abs/2010.12694</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski etÂ al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, AnkurÂ P. Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, AndrewÂ M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.

</span>
<span class="ltx_bibblock">Natural Questions: a Benchmark for Question Answering Research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">Trans. Assoc. Comput. Linguistics</em> 7 (2019), 452â€“466.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1162/TACL_A_00276" title="">https://doi.org/10.1162/TACL_A_00276</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis etÂ al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Patrick S.Â H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel, Sebastian Riedel, and Douwe Kiela. 2020.

</span>
<span class="ltx_bibblock">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. In <em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>, Hugo Larochelle, Marcâ€™Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html" title="">https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin and Demner-Fushman (2006)</span>
<span class="ltx_bibblock">
Jimmy Lin and Dina Demner-Fushman. 2006.

</span>
<span class="ltx_bibblock">Methods for automatically evaluating answers to complex questions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Inf. Retr.</em> 9, 5 (2006), 565â€“587.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/S10791-006-9003-7" title="">https://doi.org/10.1007/S10791-006-9003-7</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep, and Rodrigo Nogueira. 2021.

</span>
<span class="ltx_bibblock">Pyserini: A Python Toolkit for Reproducible Information Retrieval Research with Sparse and Dense Representations. In <em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">Proceedings of the 44th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2021)</em>. 2356â€“2362.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Stephanie Lin, Jacob Hilton, and Owain Evans. 2022.

</span>
<span class="ltx_bibblock">TruthfulQA: Measuring How Models Mimic Human Falsehoods. In <em class="ltx_emph ltx_font_italic" id="bib.bib33.3.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022</em>, Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). Association for Computational Linguistics, 3214â€“3252.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/V1/2022.ACL-LONG.229" title="">https://doi.org/10.18653/V1/2022.ACL-LONG.229</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu (2022)</span>
<span class="ltx_bibblock">
Jerry Liu. 2022.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">LlamaIndex</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.llamaindex.ai/" title="">https://www.llamaindex.ai/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
NelsonÂ F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024a.

</span>
<span class="ltx_bibblock">Lost in the Middle: How Language Models Use Long Contexts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">Transactions of the Association for Computational Linguistics</em> 12 (02 2024), 157â€“173.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1162/tacl_a_00638" title="">https://doi.org/10.1162/tacl_a_00638</a>
arXiv:https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00638/2336043/tacl_a_00638.pdf

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Zihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Chankyu Lee, Mohammad Shoeybi, and Bryan Catanzaro. 2024b.

</span>
<span class="ltx_bibblock">ChatQA: Building GPT-4 Level Conversational QA Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">CoRR</em> abs/2401.10225 (2024).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2401.10225" title="">https://doi.org/10.48550/ARXIV.2401.10225</a>
arXiv:2401.10225

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malaviya etÂ al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Chaitanya Malaviya, Subin Lee, Sihao Chen, Elizabeth Sieber, Mark Yatskar, and Dan Roth. 2023.

</span>
<span class="ltx_bibblock">ExpertQA: Expert-Curated Questions and Attributed Answers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.3.1">CoRR</em> abs/2309.07852 (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2309.07852" title="">https://doi.org/10.48550/ARXIV.2309.07852</a>
arXiv:2309.07852

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mayfield etÂ al<span class="ltx_text" id="bib.bib38.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
James Mayfield, Eugene Yang, Dawn Lawrie, Sean MacAvaney, Paul McNamee, DouglasÂ W. Oard, Luca Soldaini, Ian Soboroff, Orion Weller, Efsun Kayi, Kate Sanders, Marc Mason, and Noah Hibbler. 2024.

</span>
<span class="ltx_bibblock">On the Evaluation of Machine-Generated Reports. In <em class="ltx_emph ltx_font_italic" id="bib.bib38.3.1">Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Merrick etÂ al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Luke Merrick, Danmei Xu, Gaurav Nuti, and Daniel Campos. 2024.

</span>
<span class="ltx_bibblock">Arctic-Embed: Scalable, Efficient, and Accurate Text Embedding Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2405.05374Â [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Microsoft (2023)</span>
<span class="ltx_bibblock">
Microsoft. 2023.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Reinventing search with a new AI-powered Microsoft Bing and Edge, your copilot for the web</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/" title="">https://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muennighoff etÂ al<span class="ltx_text" id="bib.bib41.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Niklas Muennighoff, Nouamane Tazi, LoÃ¯c Magne, and Nils Reimers. 2023.

</span>
<span class="ltx_bibblock">MTEB: Massive Text Embedding Benchmark. In <em class="ltx_emph ltx_font_italic" id="bib.bib41.3.1">Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2023, Dubrovnik, Croatia, May 2-6, 2023</em>, Andreas Vlachos and Isabelle Augenstein (Eds.). Association for Computational Linguistics, 2006â€“2029.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/V1/2023.EACL-MAIN.148" title="">https://doi.org/10.18653/V1/2023.EACL-MAIN.148</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2024)</span>
<span class="ltx_bibblock">
OpenAI. 2024.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Hello GPT-4o</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/index/hello-gpt-4o/" title="">https://openai.com/index/hello-gpt-4o/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Overwijk etÂ al<span class="ltx_text" id="bib.bib43.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Arnold Overwijk, Chenyan Xiong, and Jamie Callan. 2022.

</span>
<span class="ltx_bibblock">ClueWeb22: 10 Billion Web Documents with Rich Information. In <em class="ltx_emph ltx_font_italic" id="bib.bib43.3.1">SIGIR â€™22: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022</em>, Enrique AmigÃ³, Pablo Castells, Julio Gonzalo, Ben Carterette, J.Â Shane Culpepper, and Gabriella Kazai (Eds.). ACM, 3360â€“3362.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3477495.3536321" title="">https://doi.org/10.1145/3477495.3536321</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Owoicho etÂ al<span class="ltx_text" id="bib.bib44.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Paul Owoicho, Jeff Dalton, Mohammad Aliannejadi, Leif Azzopardi, JohanneÂ R. Trippas, and Svitlana Vakulenko. 2022.

</span>
<span class="ltx_bibblock">TREC CAsT 2022: Going Beyond User Ask and System Retrieve with Initiative and Response Generation. In <em class="ltx_emph ltx_font_italic" id="bib.bib44.3.1">Proceedings of the Thirty-First Text REtrieval Conference, TREC 2022, online, November 15-19, 2022</em> <em class="ltx_emph ltx_font_italic" id="bib.bib44.4.2">(NIST Special Publication, Vol.Â 500-338)</em>, Ian Soboroff and Angela Ellis (Eds.). National Institute of Standards and Technology (NIST).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://trec.nist.gov/pubs/trec31/papers/Overview_cast.pdf" title="">https://trec.nist.gov/pubs/trec31/papers/Overview_cast.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petroni etÂ al<span class="ltx_text" id="bib.bib45.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick S.Â H. Lewis, Majid Yazdani, NicolaÂ De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim RocktÃ¤schel, and Sebastian Riedel. 2021.

</span>
<span class="ltx_bibblock">KILT: a Benchmark for Knowledge Intensive Language Tasks. In <em class="ltx_emph ltx_font_italic" id="bib.bib45.3.1">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021</em>, Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-TÃ¼r, IzÂ Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (Eds.). Association for Computational Linguistics, 2523â€“2544.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/V1/2021.NAACL-MAIN.200" title="">https://doi.org/10.18653/V1/2021.NAACL-MAIN.200</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pradeep etÂ al<span class="ltx_text" id="bib.bib46.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Ronak Pradeep, Rodrigo Nogueira, and Jimmy Lin. 2021.

</span>
<span class="ltx_bibblock">The Expando-Mono-Duo Design Pattern for Text Ranking with Pretrained Sequence-to-Sequence Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.3.1">arXiv:2101.05667</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pradeep etÂ al<span class="ltx_text" id="bib.bib47.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Ronak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin. 2023a.

</span>
<span class="ltx_bibblock">RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.3.1">CoRR</em> abs/2309.15088 (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2309.15088" title="">https://doi.org/10.48550/ARXIV.2309.15088</a>
arXiv:2309.15088

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pradeep etÂ al<span class="ltx_text" id="bib.bib48.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Ronak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin. 2023b.

</span>
<span class="ltx_bibblock">RankZephyr: Effective and Robust Zero-Shot Listwise Reranking is a Breeze!

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.3.1">CoRR</em> abs/2312.02724 (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2312.02724" title="">https://doi.org/10.48550/ARXIV.2312.02724</a>
arXiv:2312.02724

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raina and Gales (2024)</span>
<span class="ltx_bibblock">
Vatsal Raina and Mark Gales. 2024.

</span>
<span class="ltx_bibblock">Question-Based Retrieval using Atomic Units for Enterprise RAG.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2405.12363Â [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robertson and Zaragoza (2009)</span>
<span class="ltx_bibblock">
StephenÂ E. Robertson and Hugo Zaragoza. 2009.

</span>
<span class="ltx_bibblock">The Probabilistic Relevance Framework: BM25 and Beyond.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Found. Trends Inf. Retr.</em> 3, 4 (2009), 333â€“389.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1561/1500000019" title="">https://doi.org/10.1561/1500000019</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rosenthal etÂ al<span class="ltx_text" id="bib.bib51.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Sara Rosenthal, Avirup Sil, Radu Florian, and Salim Roukos. 2024.

</span>
<span class="ltx_bibblock">CLAPNQ: Cohesive Long-form Answers from Passages in Natural Questions for RAG systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.3.1">CoRR</em> abs/2404.02103 (2024).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2404.02103" title="">https://doi.org/10.48550/ARXIV.2404.02103</a>
arXiv:2404.02103

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rosset etÂ al<span class="ltx_text" id="bib.bib52.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Corby Rosset, Ho-Lam Chung, Guanghui Qin, EthanÂ C. Chau, Zhuo Feng, Ahmed Awadallah, Jennifer Neville, and Nikhil Rao. 2024.

</span>
<span class="ltx_bibblock">Researchy Questions: A Dataset of Multi-Perspective, Decompositional Questions for LLM Web Agents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.3.1">CoRR</em> abs/2402.17896 (2024).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2402.17896" title="">https://doi.org/10.48550/ARXIV.2402.17896</a>
arXiv:2402.17896

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stelmakh etÂ al<span class="ltx_text" id="bib.bib53.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. 2022.

</span>
<span class="ltx_bibblock">ASQA: Factoid Questions Meet Long-Form Answers. In <em class="ltx_emph ltx_font_italic" id="bib.bib53.3.1">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022</em>, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, 8273â€“8288.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/V1/2022.EMNLP-MAIN.566" title="">https://doi.org/10.18653/V1/2022.EMNLP-MAIN.566</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun etÂ al<span class="ltx_text" id="bib.bib54.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, and Zhaochun Ren. 2023.

</span>
<span class="ltx_bibblock">Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.3.1">arXiv:2304.09542</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thakur etÂ al<span class="ltx_text" id="bib.bib55.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Nandan Thakur, Nils Reimers, Andreas RÃ¼cklÃ©, Abhishek Srivastava, and Iryna Gurevych. 2021.

</span>
<span class="ltx_bibblock">BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib55.3.1">Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual</em>, Joaquin Vanschoren and Sai-Kit Yeung (Eds.).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/65b9eea6e1cc6bb9f0cd2a47751a186f-Abstract-round2.html" title="">https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/65b9eea6e1cc6bb9f0cd2a47751a186f-Abstract-round2.html</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiong etÂ al<span class="ltx_text" id="bib.bib56.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang. 2024.

</span>
<span class="ltx_bibblock">Benchmarking Retrieval-Augmented Generation for Medicine.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.3.1">CoRR</em> abs/2402.13178 (2024).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2402.13178" title="">https://doi.org/10.48550/ARXIV.2402.13178</a>
arXiv:2402.13178

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan etÂ al<span class="ltx_text" id="bib.bib57.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. 2024.

</span>
<span class="ltx_bibblock">Corrective Retrieval Augmented Generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.3.1">CoRR</em> abs/2401.15884 (2024).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2401.15884" title="">https://doi.org/10.48550/ARXIV.2401.15884</a>
arXiv:2401.15884

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al<span class="ltx_text" id="bib.bib58.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Peilin Yang, Hui Fang, and Jimmy Lin. 2017.

</span>
<span class="ltx_bibblock">Anserini: Enabling the Use of Lucene for Information Retrieval Research. In <em class="ltx_emph ltx_font_italic" id="bib.bib58.3.1">International Conference on Research and Development in Information Retrieval (SIGIR)</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3077136.3080721" title="">https://doi.org/10.1145/3077136.3080721</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng etÂ al<span class="ltx_text" id="bib.bib59.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, EricÂ P. Xing, Hao Zhang, JosephÂ E. Gonzalez, and Ion Stoica. 2023.

</span>
<span class="ltx_bibblock">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. In <em class="ltx_emph ltx_font_italic" id="bib.bib59.3.1">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023</em>, Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (Eds.).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://papers.nips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html" title="">http://papers.nips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html</a>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>TREC-RAGgy 2024: Additional Details</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">We manually classify each available topic in TREC Deep Learning Tracks 2021-2023 <cite class="ltx_cite ltx_citemacro_citep">(Craswell etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib16" title="">2021</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib17" title="">2022</a>; Chiang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib14" title="">2024</a>)</cite> into one of the seven different topic categories. We manually labeled each topic following the guidelines<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>Guidelines have been inspired from the <a class="ltx_ref ltx_href" href="https://www.aicrowd.com/challenges/meta-comprehensive-rag-benchmark-kdd-cup-2024" title="">2024 Meta Comprehensive RAG benchmark</a>.</span></span></span> mentioned below:</p>
<ul class="ltx_itemize" id="A1.I1">
<li class="ltx_item" id="A1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I1.i1.p1">
<p class="ltx_p" id="A1.I1.i1.p1.1"><em class="ltx_emph ltx_font_italic" id="A1.I1.i1.p1.1.1">Simple</em>: topic asking for information about a simple fact, e.g., â€œhow to emulsion a house?â€</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I1.i2.p1">
<p class="ltx_p" id="A1.I1.i2.p1.1"><em class="ltx_emph ltx_font_italic" id="A1.I1.i2.p1.1.1">Simple with condition</em>: topic asking for information about a topic with an imposed condition, e.g., â€œhow to cook thinly sliced home fries?â€</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I1.i3.p1">
<p class="ltx_p" id="A1.I1.i3.p1.1"><em class="ltx_emph ltx_font_italic" id="A1.I1.i3.p1.1.1">Set</em>: a topic containing multiple short entities in the answer, e.g., â€œwhat themes are in action movies?â€</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I1.i4.p1">
<p class="ltx_p" id="A1.I1.i4.p1.1"><em class="ltx_emph ltx_font_italic" id="A1.I1.i4.p1.1.1">Aggregation</em>: a topic that requires aggregation of multiple retrieved segments, e.g., â€œhow to put together a scuba regulator?â€</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I1.i5.p1">
<p class="ltx_p" id="A1.I1.i5.p1.1"><em class="ltx_emph ltx_font_italic" id="A1.I1.i5.p1.1.1">Comparison</em>: a topic that requires comparison of the retrieved segments, e.g. â€œdoes light intensity or concentration of carbon dioxide have a higher rate of photosynthesis?â€</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I1.i6.p1">
<p class="ltx_p" id="A1.I1.i6.p1.1"><em class="ltx_emph ltx_font_italic" id="A1.I1.i6.p1.1.1">Multi-hop</em>: a topic that requires to chain multiple information from different retrieved segments, e.g., â€œthe population of kings grant fayetteville prior to liberty hills?â€</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I1.i7.p1">
<p class="ltx_p" id="A1.I1.i7.p1.1"><em class="ltx_emph ltx_font_italic" id="A1.I1.i7.p1.1.1">False premise</em>: a topic that has a false preposition or assumption, e.g., â€œDo larger lobsters become tougher when cooked?â€</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>TREC-Researchy 2024: Additional Details</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.2">Note that for Researchy QuestionsÂ <cite class="ltx_cite ltx_citemacro_citep">(Rosset etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib52" title="">2024</a>)</cite>, the following eight intrinsic attributes were measured by GPT-4 on a scale of 0-10:</p>
<ul class="ltx_itemize" id="A2.I1">
<li class="ltx_item" id="A2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A2.I1.i1.p1">
<p class="ltx_p" id="A2.I1.i1.p1.1"><em class="ltx_emph ltx_font_italic" id="A2.I1.i1.p1.1.1">Ambiguity:</em> Checks if the questionâ€™s intent is moderately ambiguous, suggesting multiple interpretations.</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A2.I1.i2.p1">
<p class="ltx_p" id="A2.I1.i2.p1.1"><em class="ltx_emph ltx_font_italic" id="A2.I1.i2.p1.1.1">Incompleteness:</em> Checks if the question is difficult to answer due to missing crucial context or details.</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A2.I1.i3.p1">
<p class="ltx_p" id="A2.I1.i3.p1.1"><em class="ltx_emph ltx_font_italic" id="A2.I1.i3.p1.1.1">Assumptive:</em> Checks if the question has some built-in assumptions that may influence the answer.</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A2.I1.i4.p1">
<p class="ltx_p" id="A2.I1.i4.p1.1"><em class="ltx_emph ltx_font_italic" id="A2.I1.i4.p1.1.1">Multi-faceted:</em> Checks if the question requires considering multiple perspectives to provide a comprehensive answer.</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A2.I1.i5.p1">
<p class="ltx_p" id="A2.I1.i5.p1.1"><em class="ltx_emph ltx_font_italic" id="A2.I1.i5.p1.1.1">Knowledge-intensive:</em> Checks if the question demands specialized knowledge and extensive research to answer thoroughly.</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A2.I1.i6.p1">
<p class="ltx_p" id="A2.I1.i6.p1.1"><em class="ltx_emph ltx_font_italic" id="A2.I1.i6.p1.1.1">Subjective:</em> Measures if the question contains some level of subjectivity, with potential for varying opinions.</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A2.I1.i7.p1">
<p class="ltx_p" id="A2.I1.i7.p1.1"><em class="ltx_emph ltx_font_italic" id="A2.I1.i7.p1.1.1">Reasoning-intensive:</em> Checks if the question requires significant reasoning and synthesis of information to answer.</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A2.I1.i8.p1">
<p class="ltx_p" id="A2.I1.i8.p1.1"><em class="ltx_emph ltx_font_italic" id="A2.I1.i8.p1.1.1">Harmful:</em> Checks to what extent the question is harmful or inappropriate.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="A2.p1.1">It is worth noting that all the questions provided scored 0 in harmfulness and a tiny fraction scored highly on ambiguity.
We used a score of <math alttext="5" class="ltx_Math" display="inline" id="A2.p1.1.m1.1"><semantics id="A2.p1.1.m1.1a"><mn id="A2.p1.1.m1.1.1" xref="A2.p1.1.m1.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="A2.p1.1.m1.1b"><cn id="A2.p1.1.m1.1.1.cmml" type="integer" xref="A2.p1.1.m1.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.1.m1.1c">5</annotation><annotation encoding="application/x-llamapun" id="A2.p1.1.m1.1d">5</annotation></semantics></math> as the threshold to label the query for that intrinsic attribute.</p>
</div>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>RagnarÃ¶k System Arena</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.16828v1#A3.F4" title="Figure 4 â€£ Appendix C RagnarÃ¶k System Arena â€£ RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_tag">FigureÂ 4</span></a> showcases the RagnarÃ¶k WebUI (dark mode) and the user query, â€œwhy have used car prices increasedâ€, from TREC-2024 Researchy issued to two blinded systems.
This blind setup enables fair leaderboards, especially when incentives to game leaderboards are huge in this competitive proprietary LLM space.
The output displays the answers in human-readable form, allowing users to assess the quality of responses without bias.</p>
</div>
<div class="ltx_para" id="A3.p2">
<p class="ltx_p" id="A3.p2.1"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.16828v1#A3.F5" title="Figure 5 â€£ Appendix C RagnarÃ¶k System Arena â€£ RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_tag">FigureÂ 5</span></a> demonstrates the responses tab for the example in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.16828v1#A3.F4" title="Figure 4 â€£ Appendix C RagnarÃ¶k System Arena â€£ RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_tag">FigureÂ 4</span></a>.
The responses tab reformats the final answers into the JSON output expected by the I/O definitions of the TREC 2024 RAG Track.
This feature is particularly useful for developers and researchers who need to ensure that their systemsâ€™ outputs conform to specific standards and formats required by evaluation frameworks.</p>
</div>
<div class="ltx_para" id="A3.p3">
<p class="ltx_p" id="A3.p3.1">By incorporating both human-readable and JSON-formatted outputs, RagnarÃ¶k provides a comprehensive evaluation platform that caters to a wide range of needs in the research and development community.
The ability to toggle between different views and formats ensures that users can efficiently analyze and interpret the effectiveness of various RAG systems.</p>
</div>
<figure class="ltx_figure" id="A3.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="A3.F2.1">{mdframed}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="A3.F2.2">[backgroundcolor=gray!5]
<span class="ltx_text ltx_font_typewriter" id="A3.F2.2.1" style="font-size:90%;">System: This is a chat between a user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the userâ€™s questions based on the context. The assistant should also indicate when the answer cannot be found in the context. 
<br class="ltx_break"/>
<br class="ltx_break"/>INSTRUCTION: Please give a complete answer to the question. Cite each context document that supports your answer within brackets [] using the IEEE format. 
<br class="ltx_break"/>
<br class="ltx_break"/>QUESTION: {query} 
<br class="ltx_break"/>
<br class="ltx_break"/>CONTEXTS:
<br class="ltx_break"/>[1] {Passage title}: {Passage text}
<br class="ltx_break"/>[2] {Passage title}: {Passage text}
<br class="ltx_break"/>... 
<br class="ltx_break"/>[20] {Passage title}: {Passage text}
<br class="ltx_break"/>
<br class="ltx_break"/>INSTRUCTION: Please give a complete answer to the question. Cite each context document that supports your answer within brackets [] using the IEEE format.
<span class="ltx_text ltx_font_serif" id="A3.F2.2.1.1">
</span></span></p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>ChatQA prompt templateÂ <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.16828v1#bib.bib36" title="">2024b</a>)</cite> used for RAG generation with in-text citations with GPT-4o in our RagnarÃ¶k framework.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="283" id="A3.F3.g1" src="extracted/5688564/plots/WebUIPF.png" width="401"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>WebUI showcasing the RagnarÃ¶k System Arena and the user query, â€œwhat inspired pink floydâ€™s the wall?â€, with answers from two pipelines side-by-side comparing GPT-4o answer (left) and Command R+ answer (right).
</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="343" id="A3.F4.g1" src="extracted/5688564/plots/WebUIBlind.png" width="401"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>
WebUI (dark mode) showcasing the RagnarÃ¶k system arena for the user query on â€œwhy have used car prices increasedâ€ from TREC-2024 Researchy with two different blinded pipelines.
The output tab displays the answers in human-readable form.
</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="290" id="A3.F5.g1" src="extracted/5688564/plots/WebUIBlindResponse.png" width="401"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>
The responses tab for the example in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.16828v1#A3.F4" title="Figure 4 â€£ Appendix C RagnarÃ¶k System Arena â€£ RagnarÃ¶k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track"><span class="ltx_text ltx_ref_tag">FigureÂ 4</span></a>. Note that the responses tab reformats the final answers into the JSON format expected by the I/O definitions of the TREC 2024 RAG Track.
</figcaption>
</figure>
<figure class="ltx_table" id="A3.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3. </span>An example of the first segment of two near-duplicate documents present in the MS MARCO V2 segment collection. During the deduplication procedure, the segments of one of the documents is kept in the MS MARCO V2.1 segment collection (msmarco_doc_00_995170174#0), whereas the other segment is discarded as a duplicate (msmarco_doc_00_995171191#0).</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A3.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A3.T3.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="A3.T3.1.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T3.1.1.1.1.1">
<span class="ltx_p" id="A3.T3.1.1.1.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="A3.T3.1.1.1.1.1.1.1" style="font-size:90%;">Segment ID</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="A3.T3.1.1.1.2" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T3.1.1.1.2.1">
<span class="ltx_p" id="A3.T3.1.1.1.2.1.1" style="width:91.0pt;"><span class="ltx_text ltx_font_bold" id="A3.T3.1.1.1.2.1.1.1" style="font-size:90%;">URL</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="A3.T3.1.1.1.3" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T3.1.1.1.3.1">
<span class="ltx_p" id="A3.T3.1.1.1.3.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="A3.T3.1.1.1.3.1.1.1" style="font-size:90%;">Title</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="A3.T3.1.1.1.4" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T3.1.1.1.4.1">
<span class="ltx_p" id="A3.T3.1.1.1.4.1.1" style="width:284.5pt;"><span class="ltx_text ltx_font_bold" id="A3.T3.1.1.1.4.1.1.1" style="font-size:90%;">Segment</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.T3.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" colspan="4" id="A3.T3.1.2.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="A3.T3.1.2.1.1.1" style="font-size:90%;">MS MARCO V2.1 segment (discarded)</span></td>
</tr>
<tr class="ltx_tr" id="A3.T3.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A3.T3.1.3.2.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T3.1.3.2.1.1">
<span class="ltx_p" id="A3.T3.1.3.2.1.1.1" style="width:56.9pt;"><span class="ltx_text" id="A3.T3.1.3.2.1.1.1.1" style="font-size:90%;">msmarco_doc_00 _995171191#0</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A3.T3.1.3.2.2" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T3.1.3.2.2.1">
<span class="ltx_p" id="A3.T3.1.3.2.2.1.1" style="width:91.0pt;"><a class="ltx_ref ltx_href" href="http://center.serve.org/TT/fp_tips.html" style="font-size:90%;" title="">http://center.serve.org/TT /fp_tips.html</a></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A3.T3.1.3.2.3" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T3.1.3.2.3.1">
<span class="ltx_p" id="A3.T3.1.3.2.3.1.1" style="width:56.9pt;"><span class="ltx_text" id="A3.T3.1.3.2.3.1.1.1" style="font-size:90%;">SERVE Center Resources</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A3.T3.1.3.2.4" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T3.1.3.2.4.1">
<span class="ltx_p" id="A3.T3.1.3.2.4.1.1" style="width:284.5pt;"><span class="ltx_text" id="A3.T3.1.3.2.4.1.1.1" style="font-size:90%;">SERVE Center Get to know us About Us We believe that educational improvement requires a partnership with our clients, and we focus on doing work that is important and directly relevant to both policymakers and practitioners. Services Program evaluation, capacity building, technical assistance, strategic planning, and much more. Customized services designed to meet the specific needs of our partnering organizations. Projects During its history, SERVE has been awarded over $200 million in contracts and grants and has successfully managed 14 major awards, including multiple contracts with the US Department of Education. Resources Forging connections with a variety of individuals and organizations, the SERVE Center offers an array of resources developed to inform and aide both the policymaker and the practioner.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T3.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" colspan="4" id="A3.T3.1.4.3.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="A3.T3.1.4.3.1.1" style="font-size:90%;">MS MARCO V2.1 segment (kept)</span></td>
</tr>
<tr class="ltx_tr" id="A3.T3.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" id="A3.T3.1.5.4.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T3.1.5.4.1.1">
<span class="ltx_p" id="A3.T3.1.5.4.1.1.1" style="width:56.9pt;"><span class="ltx_text" id="A3.T3.1.5.4.1.1.1.1" style="font-size:90%;">msmarco_doc_00 _995170174#0</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" id="A3.T3.1.5.4.2" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T3.1.5.4.2.1">
<span class="ltx_p" id="A3.T3.1.5.4.2.1.1" style="width:91.0pt;"><a class="ltx_ref ltx_href" href="http://center.serve.org/TT/" style="font-size:90%;" title="">http://center.serve.org/TT/</a></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" id="A3.T3.1.5.4.3" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T3.1.5.4.3.1">
<span class="ltx_p" id="A3.T3.1.5.4.3.1.1" style="width:56.9pt;"><span class="ltx_text" id="A3.T3.1.5.4.3.1.1.1" style="font-size:90%;">SERVE Center Resources</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" id="A3.T3.1.5.4.4" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T3.1.5.4.4.1">
<span class="ltx_p" id="A3.T3.1.5.4.4.1.1" style="width:284.5pt;"><span class="ltx_text" id="A3.T3.1.5.4.4.1.1.1" style="font-size:90%;">SERVE Center Get to know us About Us We believe that educational improvement requires a partnership with our clients, and we focus on doing work that is important and directly relevant to both policymakers and practitioners. Services Program evaluation, capacity building, technical assistance, strategic planning, and much more. Customized services designed to meet the specific needs of our partnering organizations. Projects During its history, SERVE has been awarded over $200 million in contracts and grants and has successfully managed 14 major awards, including multiple contracts with the US Department of Education. Resources Forging connections with a variety of individuals and organizations, the SERVE Center offers an array of resources developed to inform and aide both the policymaker and the practioner.</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="A3.tab1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4. </span>An end-to-end RAG example for a randomly sampled topic (topic ID: 2027497) in the TREC-RAGgy 2024 collection: â€œhow often should you take your toddler to the potty when potty training?â€</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A3.tab1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A3.tab1.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_tt" colspan="3" id="A3.tab1.1.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="A3.tab1.1.1.1.1.1" style="font-size:90%;">1. Top-10 retrieved segments from the (R) retrieval stage (ordered from top to bottom in terms of relevancy):</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.tab1.1.2.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_t" id="A3.tab1.1.2.1.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.2.1.1.1">
<span class="ltx_p" id="A3.tab1.1.2.1.1.1.1" style="width:128.0pt;"><span class="ltx_text ltx_font_bold" id="A3.tab1.1.2.1.1.1.1.1" style="font-size:90%;">Segment ID</span></span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A3.tab1.1.2.1.2" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.2.1.2.1">
<span class="ltx_p" id="A3.tab1.1.2.1.2.1.1" style="width:113.8pt;"><span class="ltx_text ltx_font_bold" id="A3.tab1.1.2.1.2.1.1.1" style="font-size:90%;">Title</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A3.tab1.1.2.1.3" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.2.1.3.1">
<span class="ltx_p" id="A3.tab1.1.2.1.3.1.1" style="width:270.3pt;"><span class="ltx_text ltx_font_bold" id="A3.tab1.1.2.1.3.1.1.1" style="font-size:90%;">Segment</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.tab1.1.3.2">
<th class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_th ltx_th_row" id="A3.tab1.1.3.2.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.3.2.1.1">
<span class="ltx_p" id="A3.tab1.1.3.2.1.1.1" style="width:128.0pt;"><span class="ltx_ERROR undefined" id="A3.tab1.1.3.2.1.1.1.1">\hdashline</span><span class="ltx_text" id="A3.tab1.1.3.2.1.1.1.2" style="font-size:90%;">
(1) msmarco_v2.1_doc_51_766815931 #2_1606878413</span></span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A3.tab1.1.3.2.2" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.3.2.2.1">
<span class="ltx_p" id="A3.tab1.1.3.2.2.1.1" style="width:113.8pt;"><span class="ltx_text" id="A3.tab1.1.3.2.2.1.1.1" style="font-size:90%;">How Often Should I Take My Toddler To The Potty? There Are Some Guidelines To Follow</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A3.tab1.1.3.2.3" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.3.2.3.1">
<span class="ltx_p" id="A3.tab1.1.3.2.3.1.1" style="width:270.3pt;"><span class="ltx_text" id="A3.tab1.1.3.2.3.1.1.1" style="font-size:90%;">Self-esteem can be fragile at this time, so itâ€™s important to toilet train gently, letting your child lead the way. Additionally, itâ€™s unwise to begin potty training unless your child is truly ready. Take this potty training readiness quiz featured in Parents before you begin. Elizabeth Pantley [ â€¦ ] and you still must be on the lookout for signs your toddler has to go ahead of schedule.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.tab1.1.4.3">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row" id="A3.tab1.1.4.3.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.4.3.1.1">
<span class="ltx_p" id="A3.tab1.1.4.3.1.1.1" style="width:128.0pt;"><span class="ltx_text" id="A3.tab1.1.4.3.1.1.1.1" style="font-size:90%;">(2) msmarco_v2.1_doc_51_766815931 #6_1606884302</span></span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A3.tab1.1.4.3.2" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.4.3.2.1">
<span class="ltx_p" id="A3.tab1.1.4.3.2.1.1" style="width:113.8pt;"><span class="ltx_text" id="A3.tab1.1.4.3.2.1.1.1" style="font-size:90%;">How Often Should I Take My Toddler To The Potty? There Are Some Guidelines To Follow</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A3.tab1.1.4.3.3" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.4.3.3.1">
<span class="ltx_p" id="A3.tab1.1.4.3.3.1.1" style="width:270.3pt;"><span class="ltx_text" id="A3.tab1.1.4.3.3.1.1.1" style="font-size:90%;">Davis and Keyser stressed the importance of keeping your own emotions in check throughout the toilet training process. The bottom line: when it comes to potty time, thereâ€™s no magic number, [ â€¦ ] Youâ€™ll just have that much longer to plan a good one.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.tab1.1.5.4">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row" id="A3.tab1.1.5.4.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.5.4.1.1">
<span class="ltx_p" id="A3.tab1.1.5.4.1.1.1" style="width:128.0pt;"><span class="ltx_text" id="A3.tab1.1.5.4.1.1.1.1" style="font-size:90%;">(3) msmarco_v2.1_doc_08_935420812 #8_1683502976</span></span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A3.tab1.1.5.4.2" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.5.4.2.1">
<span class="ltx_p" id="A3.tab1.1.5.4.2.1.1" style="width:113.8pt;"><span class="ltx_text" id="A3.tab1.1.5.4.2.1.1.1" style="font-size:90%;">Q&amp;A: Know Your Childâ€™s Level of Readiness for Potty Training â€” Parents</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A3.tab1.1.5.4.3" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.5.4.3.1">
<span class="ltx_p" id="A3.tab1.1.5.4.3.1.1" style="width:270.3pt;"><span class="ltx_text" id="A3.tab1.1.5.4.3.1.1.1" style="font-size:90%;">Start by sitting on the toilet the first thing in the morning after taking off the overnight diaper, make it funâ€”sing songs, read a book, drink some juice, and see what happens. [ â€¦ ] â€” Dr. Carrie M. Brown My 3-year-old is adamant about not using the potty. How can I motivate her?</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.tab1.1.6.5">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row" id="A3.tab1.1.6.5.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.6.5.1.1">
<span class="ltx_p" id="A3.tab1.1.6.5.1.1.1" style="width:128.0pt;"><span class="ltx_text" id="A3.tab1.1.6.5.1.1.1.1" style="font-size:90%;">(4) msmarco_v2.1_doc_08_935420812 #0_1683481876</span></span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A3.tab1.1.6.5.2" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.6.5.2.1">
<span class="ltx_p" id="A3.tab1.1.6.5.2.1.1" style="width:113.8pt;"><span class="ltx_text" id="A3.tab1.1.6.5.2.1.1.1" style="font-size:90%;">Q&amp;A: Know Your Childâ€™s Level of Readiness for Potty Training â€” Parents</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A3.tab1.1.6.5.3" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.6.5.3.1">
<span class="ltx_p" id="A3.tab1.1.6.5.3.1.1" style="width:270.3pt;"><span class="ltx_text" id="A3.tab1.1.6.5.3.1.1.1" style="font-size:90%;">Q&amp;A: Know Your Childâ€™s Level of Readiness for Potty Training â€” Parents Home Toddlers &amp; Preschoolers Potty Training Potty Training Tips Q&amp;A: [ â€¦ ] Have your child sit on the potty soon after sheâ€™s finished breakfast and again after dinner.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.tab1.1.7.6">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row" id="A3.tab1.1.7.6.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.7.6.1.1">
<span class="ltx_p" id="A3.tab1.1.7.6.1.1.1" style="width:128.0pt;"><span class="ltx_text" id="A3.tab1.1.7.6.1.1.1.1" style="font-size:90%;">(5) msmarco_v2.1_doc_28_472446307 #23_1012991039</span></span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A3.tab1.1.7.6.2" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.7.6.2.1">
<span class="ltx_p" id="A3.tab1.1.7.6.2.1.1" style="width:113.8pt;"><span class="ltx_text" id="A3.tab1.1.7.6.2.1.1.1" style="font-size:90%;">Potty Trained Toddler Having Accidents on Purpose?</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A3.tab1.1.7.6.3" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.7.6.3.1">
<span class="ltx_p" id="A3.tab1.1.7.6.3.1.1" style="width:270.3pt;"><span class="ltx_text" id="A3.tab1.1.7.6.3.1.1.1" style="font-size:90%;">But donâ€™t feel disappointed if most of these tries end up with no pee in the pottyâ€”you wouldnâ€™t be able to pee [ â€¦ ] Get more tips on how to ease your childâ€™s potty training poop anxiety.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.tab1.1.8.7">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row" id="A3.tab1.1.8.7.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.8.7.1.1">
<span class="ltx_p" id="A3.tab1.1.8.7.1.1.1" style="width:128.0pt;"><span class="ltx_text" id="A3.tab1.1.8.7.1.1.1.1" style="font-size:90%;">(6) msmarco_v2.1_doc_28_472446307 #22_1012988885</span></span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A3.tab1.1.8.7.2" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.8.7.2.1">
<span class="ltx_p" id="A3.tab1.1.8.7.2.1.1" style="width:113.8pt;"><span class="ltx_text" id="A3.tab1.1.8.7.2.1.1.1" style="font-size:90%;">Potty Trained Toddler Having Accidents on Purpose?</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A3.tab1.1.8.7.3" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.8.7.3.1">
<span class="ltx_p" id="A3.tab1.1.8.7.3.1.1" style="width:270.3pt;"><span class="ltx_text" id="A3.tab1.1.8.7.3.1.1.1" style="font-size:90%;">Get tips on what to do when your 4 year old wonâ€™t poop on potty. [ â€¦ ] Routines give him the predictability he needs so he knows exactly what to do and when. You might use the potty after waking up, before leaving the house or after eating meals.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.tab1.1.9.8">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row" id="A3.tab1.1.9.8.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.9.8.1.1">
<span class="ltx_p" id="A3.tab1.1.9.8.1.1.1" style="width:128.0pt;"><span class="ltx_text" id="A3.tab1.1.9.8.1.1.1.1" style="font-size:90%;">(7) msmarco_v2.1_doc_57_1222573163 #16_2241797192</span></span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A3.tab1.1.9.8.2" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.9.8.2.1">
<span class="ltx_p" id="A3.tab1.1.9.8.2.1.1" style="width:113.8pt;"><span class="ltx_text" id="A3.tab1.1.9.8.2.1.1.1" style="font-size:90%;">Tips on Starting Potty Training: How to Potty Train Your Toddler</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A3.tab1.1.9.8.3" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.9.8.3.1">
<span class="ltx_p" id="A3.tab1.1.9.8.3.1.1" style="width:270.3pt;"><span class="ltx_text" id="A3.tab1.1.9.8.3.1.1.1" style="font-size:90%;">Teach him to check for dryness. This offers him an added sense of control. If heâ€™s dry, give him a pat on the back (or a big hug), but donâ€™t criticize him if heâ€™s wet. [ â€¦ ] If you overreact, you might discourage your toddlerâ€™s future attempts.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.tab1.1.10.9">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row" id="A3.tab1.1.10.9.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.10.9.1.1">
<span class="ltx_p" id="A3.tab1.1.10.9.1.1.1" style="width:128.0pt;"><span class="ltx_text" id="A3.tab1.1.10.9.1.1.1.1" style="font-size:90%;">(8) msmarco_v2.1_doc_57_1222573163 #15_2241795642</span></span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A3.tab1.1.10.9.2" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.10.9.2.1">
<span class="ltx_p" id="A3.tab1.1.10.9.2.1.1" style="width:113.8pt;"><span class="ltx_text" id="A3.tab1.1.10.9.2.1.1.1" style="font-size:90%;">Tips on Starting Potty Training: How to Potty Train Your Toddler</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A3.tab1.1.10.9.3" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.10.9.3.1">
<span class="ltx_p" id="A3.tab1.1.10.9.3.1.1" style="width:270.3pt;"><span class="ltx_text" id="A3.tab1.1.10.9.3.1.1.1" style="font-size:90%;">Even if youâ€™re too late and heâ€™s already done the deed, have him sit on the potty anyway to reinforce the connection. Keep her motivated. [ â€¦ ] Even the most enthusiastic toddler can take several weeks to master potty training proficiency â€” often with as many steps backward as forward.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.tab1.1.11.10">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row" id="A3.tab1.1.11.10.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.11.10.1.1">
<span class="ltx_p" id="A3.tab1.1.11.10.1.1.1" style="width:128.0pt;"><span class="ltx_text" id="A3.tab1.1.11.10.1.1.1.1" style="font-size:90%;">(9) msmarco_v2.1_doc_51_766815931 #5_1606882767</span></span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A3.tab1.1.11.10.2" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.11.10.2.1">
<span class="ltx_p" id="A3.tab1.1.11.10.2.1.1" style="width:113.8pt;"><span class="ltx_text" id="A3.tab1.1.11.10.2.1.1.1" style="font-size:90%;">How Often Should I Take My Toddler To The Potty? There Are Some Guidelines To Follow</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A3.tab1.1.11.10.3" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.11.10.3.1">
<span class="ltx_p" id="A3.tab1.1.11.10.3.1.1" style="width:270.3pt;"><span class="ltx_text" id="A3.tab1.1.11.10.3.1.1.1" style="font-size:90%;">According to Becoming the Parent You Want To Be by Laura Davis and Janis Keyser, itâ€™s common for children to go back and forth â€” somedays preferring diapers, and other days selecting to use the toilet. [ â€¦ ] You can only take your child to the bathroom too much if it begins to feel stressful or punitive.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.tab1.1.12.11">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row" id="A3.tab1.1.12.11.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.12.11.1.1">
<span class="ltx_p" id="A3.tab1.1.12.11.1.1.1" style="width:128.0pt;"><span class="ltx_text" id="A3.tab1.1.12.11.1.1.1.1" style="font-size:90%;">(10) msmarco_v2.1_doc_51_766815931 #0_1606874600</span></span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A3.tab1.1.12.11.2" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.12.11.2.1">
<span class="ltx_p" id="A3.tab1.1.12.11.2.1.1" style="width:113.8pt;"><span class="ltx_text" id="A3.tab1.1.12.11.2.1.1.1" style="font-size:90%;">How Often Should I Take My Toddler To The Potty? There Are Some Guidelines To Follow</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A3.tab1.1.12.11.3" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.12.11.3.1">
<span class="ltx_p" id="A3.tab1.1.12.11.3.1.1" style="width:270.3pt;"><span class="ltx_text" id="A3.tab1.1.12.11.3.1.1.1" style="font-size:90%;">How Often Should I Take My Toddler To The Potty? There Are Some Guidelines To Follow Life Ashley Batz/Romper How Often Should I Take My Toddler To The Potty? There Are Some Guidelines To Follow by Kelly Mullen-McWilliams July 22, 2017 I donâ€™t know about you, but Iâ€™m personally super pumped for that perfect day [ â€¦ ] he article noted that toilet training is also one of the most difficult milestones for children and their parents, and that it can become highly emotional.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.tab1.1.13.12">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_t" colspan="3" id="A3.tab1.1.13.12.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="A3.tab1.1.13.12.1.1" style="font-size:90%;">2. Results from the (AG) augmented generation stage using top-10 retrieved segments:</span></th>
</tr>
<tr class="ltx_tr" id="A3.tab1.1.14.13">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_t" id="A3.tab1.1.14.13.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.14.13.1.1">
<span class="ltx_p" id="A3.tab1.1.14.13.1.1.1" style="width:128.0pt;"><span class="ltx_text ltx_font_bold" id="A3.tab1.1.14.13.1.1.1.1" style="font-size:90%;">Model</span></span>
</span>
</th>
<td class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2" id="A3.tab1.1.14.13.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="A3.tab1.1.14.13.2.1" style="font-size:90%;">RAG Answer</span></td>
</tr>
<tr class="ltx_tr" id="A3.tab1.1.15.14">
<th class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_th ltx_th_row" id="A3.tab1.1.15.14.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.15.14.1.1">
<span class="ltx_p" id="A3.tab1.1.15.14.1.1.1" style="width:128.0pt;"><span class="ltx_ERROR undefined" id="A3.tab1.1.15.14.1.1.1.1">\hdashline</span><span class="ltx_text" id="A3.tab1.1.15.14.1.1.1.2" style="font-size:90%;">
Command R+</span></span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_top" colspan="2" id="A3.tab1.1.15.14.2" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.15.14.2.1">
<span class="ltx_p" id="A3.tab1.1.15.14.2.1.1" style="width:398.3pt;"><span class="ltx_text" id="A3.tab1.1.15.14.2.1.1.1" style="font-size:90%;">[{â€™textâ€™: â€™There is no magic number for how often you should take your toddler to the potty when potty training.â€™, â€™citationsâ€™: [1, 8]}, {â€™textâ€™: â€™However, it is recommended to take them frequently, especially after waking up, before leaving the house, after eating meals, or before bath time.â€™, â€™citationsâ€™: [0, 1, 2, 3, 4, 5, 8]}, {â€™textâ€™: â€™You can also use a timer and set it to every 30 minutes to an hour for a potty break.â€™, â€™citationsâ€™: [5]}, {â€™textâ€™: â€™It is important to be patient and gentle throughout the process, as it can be emotionally challenging for both the child and the parent.â€™, â€™citationsâ€™: [0, 1, 3, 6, 7, 8, 9]}]</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.tab1.1.16.15">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_bb" id="A3.tab1.1.16.15.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.16.15.1.1">
<span class="ltx_p" id="A3.tab1.1.16.15.1.1.1" style="width:128.0pt;"><span class="ltx_text" id="A3.tab1.1.16.15.1.1.1.1" style="font-size:90%;">GPT-4o</span></span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" colspan="2" id="A3.tab1.1.16.15.2" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.tab1.1.16.15.2.1">
<span class="ltx_p" id="A3.tab1.1.16.15.2.1.1" style="width:398.3pt;"><span class="ltx_text" id="A3.tab1.1.16.15.2.1.1.1" style="font-size:90%;">[{â€™textâ€™: â€™When potty training your toddler, it is recommended to take them to the potty frequently to increase the chances of success.â€™, â€™citationsâ€™: []}, {â€™textâ€™: â€™According to Elizabeth Pantley, author of â€The No-Cry Potty Training Solution,â€ most toddlers pee four to eight times per day and have one or two bowel movements daily.â€™, â€™citationsâ€™: []}, {â€™textâ€™: â€™She suggests setting up a potty routine, such as taking your toddler to the potty first thing in the morning, after eating, and before activities like riding in the car or going to sleep.â€™, â€™citationsâ€™: [0]}, {â€™textâ€™: â€™Additionally, Davis and Keyser emphasize the importance of following a schedule, such as morning pee and before nap time, while also being attentive to signs that your toddler needs to go.â€™, â€™citationsâ€™: []}, {â€™textâ€™: â€™They caution against making the process stressful or punitive, as negative emotions can hinder progress.â€™, â€™citationsâ€™: [1]}, {â€™textâ€™: â€™Dr. Carrie M. Brown recommends planning bathroom trips every 90-120 minutes during the day and remaining calm and consistent, even if accidents occur.â€™, â€™citationsâ€™: []}, {â€™textâ€™: â€™This approach helps toddlers learn the routine and increases the likelihood of successful potty use.â€™, â€™citationsâ€™: [2]}]</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Jun 24 17:27:50 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
