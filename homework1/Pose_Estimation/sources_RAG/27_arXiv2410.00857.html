<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Quantifying reliance on external information over parametric knowledge during Retrieval Augmented Generation (RAG) using mechanistic analysis</title>
<!--Generated on Tue Oct  1 16:33:31 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.00857v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00857v1#S1" title="In Quantifying reliance on external information over parametric knowledge during Retrieval Augmented Generation (RAG) using mechanistic analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00857v1#S2" title="In Quantifying reliance on external information over parametric knowledge during Retrieval Augmented Generation (RAG) using mechanistic analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Probing Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00857v1#S3" title="In Quantifying reliance on external information over parametric knowledge during Retrieval Augmented Generation (RAG) using mechanistic analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Data and Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00857v1#S4" title="In Quantifying reliance on external information over parametric knowledge during Retrieval Augmented Generation (RAG) using mechanistic analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00857v1#S5" title="In Quantifying reliance on external information over parametric knowledge during Retrieval Augmented Generation (RAG) using mechanistic analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusions and Future Work</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Quantifying reliance on external information over parametric knowledge during Retrieval Augmented Generation (RAG) using mechanistic analysis</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span class="ltx_text ltx_font_bold" id="id1.1.id1">Reshmi Ghosh<sup class="ltx_sup" id="id1.1.id1.1">1</sup></span>,
<span class="ltx_text ltx_font_bold" id="id2.2.id2">Rahul Seetharaman<sup class="ltx_sup" id="id2.2.id2.1">2</sup></span>,
<span class="ltx_text ltx_font_bold" id="id3.3.id3">Hitesh Wadhwa<sup class="ltx_sup" id="id3.3.id3.1">2</sup></span>,
<span class="ltx_text ltx_font_bold" id="id4.4.id4"> Somyaa Aggarwal<sup class="ltx_sup" id="id4.4.id4.1">2</sup></span>,

<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id5.5.id5">Samyadeep Basu<sup class="ltx_sup" id="id5.5.id5.1">1,3</sup></span>,
<span class="ltx_text ltx_font_bold" id="id6.6.id6">Soundararajan Srinivasan<sup class="ltx_sup" id="id6.6.id6.1">1</sup></span>,
<span class="ltx_text ltx_font_bold" id="id7.7.id7">Wenlong Zhao<sup class="ltx_sup" id="id7.7.id7.1">2</sup></span>,
<span class="ltx_text ltx_font_bold" id="id8.8.id8">Shreyas Chaudhar<sup class="ltx_sup" id="id8.8.id8.1">2</sup></span>,

<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id9.9.id9">Ehsan Aghazadeh<sup class="ltx_sup" id="id9.9.id9.1">2</sup></span>
<br class="ltx_break"/><sup class="ltx_sup" id="id10.10.id10">1</sup>Microsoft,
<sup class="ltx_sup" id="id11.11.id11">2</sup>University of Massachusetts, Amherst,
<sup class="ltx_sup" id="id12.12.id12">3</sup>University of Maryland, College Park
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id13.id1">Retrieval Augmented Generation (RAG) is a widely used approach for leveraging external context in several natural language applications such as question answering and information retrieval. Yet, the exact nature in which a Language Model (LM) leverages this non-parametric memory or retrieved context isn‚Äôt clearly understood. This paper <span class="ltx_text ltx_font_bold ltx_font_italic" id="id13.id1.1">mechanistically</span> examines the RAG pipeline to highlight that LMs demonstrate a ‚Äúshortcut‚Äù effect and have a strong bias towards utilizing the retrieved context to answer questions, while relying minimally on model priors. We propose (a) Causal Mediation Analysis; for proving that parametric memory is minimally utilized when answering a question and (b) Attention Contributions and Knockouts for showing the last token residual stream do not get enriched from the subject token in the question, but gets enriched from tokens of RAG-context. We find this pronounced ‚Äúshortcut‚Äù behaviour to be true across both LLMs (e.g.,LlaMa) and SLMs (e.g., Phi).</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.1">
<p class="ltx_p" id="p1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1.1">Quantifying reliance on external information over parametric knowledge during Retrieval Augmented Generation (RAG) using mechanistic analysis</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.1.2" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.1.2.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.2.1.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.1.1.1.1">
Reshmi Ghosh<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.1">1</sup>,
Rahul Seetharaman<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.2">2</sup>,
Hitesh Wadhwa<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.3">2</sup>,
 Somyaa Aggarwal<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.4">2</sup>,</span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.2.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.2.2.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.2.2.1.1">Samyadeep Basu<sup class="ltx_sup" id="p1.1.2.1.1.2.2.1.1.1">1,3</sup></span>,
<span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.2.2.1.2">Soundararajan Srinivasan<sup class="ltx_sup" id="p1.1.2.1.1.2.2.1.2.1">1</sup></span>,
<span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.2.2.1.3">Wenlong Zhao<sup class="ltx_sup" id="p1.1.2.1.1.2.2.1.3.1">2</sup></span>,
<span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.2.2.1.4">Shreyas Chaudhar<sup class="ltx_sup" id="p1.1.2.1.1.2.2.1.4.1">2</sup></span>,</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.3.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.3.3.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.3.3.1.1">Ehsan Aghazadeh<sup class="ltx_sup" id="p1.1.2.1.1.3.3.1.1.1">2</sup></span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.4.4">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.4.4.1"><sup class="ltx_sup" id="p1.1.2.1.1.4.4.1.1">1</sup>Microsoft,
<sup class="ltx_sup" id="p1.1.2.1.1.4.4.1.2">2</sup>University of Massachusetts, Amherst,
<sup class="ltx_sup" id="p1.1.2.1.1.4.4.1.3">3</sup>University of Maryland, College Park,</span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Retrieval Augmented Generation (RAG) <cite class="ltx_cite ltx_citemacro_cite">Lewis et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.00857v1#bib.bib3" title="">2021</a>)</cite>is a popular method to enhance a Language Model‚Äôs (LLM) capability to reason and execute tasks by leveraging additional context provided during inference time <cite class="ltx_cite ltx_citemacro_cite">Shao et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.00857v1#bib.bib7" title="">2023</a>)</cite><cite class="ltx_cite ltx_citemacro_cite">Singh et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.00857v1#bib.bib8" title="">2023</a>)</cite><cite class="ltx_cite ltx_citemacro_cite">IngestAI (<a class="ltx_ref" href="https://arxiv.org/html/2410.00857v1#bib.bib2" title="">2023</a>)</cite>. Additionally, researchers have also explored shortcomings of RAG systems, such as inconsistent responses<cite class="ltx_cite ltx_citemacro_cite">Liu et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.00857v1#bib.bib4" title="">2023</a>)</cite> and only¬†<cite class="ltx_cite ltx_citemacro_cite">Wu et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.00857v1#bib.bib9" title="">2024</a>)</cite> delved into the balance between a model‚Äôs internal knowledge and externally retrieved information, examining their practical value.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Several research papers have proposed the approaches for editing knowledge in language model, including techniques like ROME <cite class="ltx_cite ltx_citemacro_cite">Meng et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.00857v1#bib.bib5" title="">2022a</a>)</cite>, MEMIT <cite class="ltx_cite ltx_citemacro_cite">Meng et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.00857v1#bib.bib6" title="">2022b</a>)</cite> to update or correct facts. On the flip side, with the popularity of LLM integration for various tasks leveraging properitary, enterprise, and private data, the use of RAG framework has increased to tackle <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">hallucinations</span> while reasoning on <span class="ltx_text ltx_font_bold" id="S1.p2.1.2">new never seen before</span> (out of distribution) knowledge tasks. However, a comprehensive study mechanistically probing of Langauge Model‚Äôs behavior of choosing between information from RAG-generated context over intrinsic parametric knowledge has not been conducted to the best of our knowledge.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Probing Methods</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">To mechanistically interpret the knowledge contributions towards factual reasoning by LLMs and SLMs, we use three methods for causal mediation, described as follows:
<span class="ltx_text ltx_font_bold" id="S2.p1.1.1">Causal Tracing</span> <cite class="ltx_cite ltx_citemacro_cite">Meng et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.00857v1#bib.bib5" title="">2022a</a>)</cite> identifies specific hidden states that significantly influence factual predictions. The approach involves a clean run, corrupted run and a corrupted-with-restoration run. The corrupted run involves corrupting a certain span of the text, and running the forward pass of the model. In the restoration run, activations from the clean run are patched one by one into the corrupted run, and the increase in answer probability is observed; the most crucial activations are thus causally determined. The causal importance of a certain activation is quantified using Indirect Effect, which is defined as the difference between the corrupted run and the corrupted-with-restoration run probabilities: <math alttext="\text{IE}(h^{(l)}_{i})=P^{*}_{\text{clean}}(h^{(l)}_{i})[y]-P^{*}[y]" class="ltx_Math" display="inline" id="S2.p1.1.m1.6"><semantics id="S2.p1.1.m1.6a"><mrow id="S2.p1.1.m1.6.6" xref="S2.p1.1.m1.6.6.cmml"><mrow id="S2.p1.1.m1.5.5.1" xref="S2.p1.1.m1.5.5.1.cmml"><mtext id="S2.p1.1.m1.5.5.1.3" xref="S2.p1.1.m1.5.5.1.3a.cmml">IE</mtext><mo id="S2.p1.1.m1.5.5.1.2" xref="S2.p1.1.m1.5.5.1.2.cmml">‚Å¢</mo><mrow id="S2.p1.1.m1.5.5.1.1.1" xref="S2.p1.1.m1.5.5.1.1.1.1.cmml"><mo id="S2.p1.1.m1.5.5.1.1.1.2" stretchy="false" xref="S2.p1.1.m1.5.5.1.1.1.1.cmml">(</mo><msubsup id="S2.p1.1.m1.5.5.1.1.1.1" xref="S2.p1.1.m1.5.5.1.1.1.1.cmml"><mi id="S2.p1.1.m1.5.5.1.1.1.1.2.2" xref="S2.p1.1.m1.5.5.1.1.1.1.2.2.cmml">h</mi><mi id="S2.p1.1.m1.5.5.1.1.1.1.3" xref="S2.p1.1.m1.5.5.1.1.1.1.3.cmml">i</mi><mrow id="S2.p1.1.m1.1.1.1.3" xref="S2.p1.1.m1.5.5.1.1.1.1.cmml"><mo id="S2.p1.1.m1.1.1.1.3.1" stretchy="false" xref="S2.p1.1.m1.5.5.1.1.1.1.cmml">(</mo><mi id="S2.p1.1.m1.1.1.1.1" xref="S2.p1.1.m1.1.1.1.1.cmml">l</mi><mo id="S2.p1.1.m1.1.1.1.3.2" stretchy="false" xref="S2.p1.1.m1.5.5.1.1.1.1.cmml">)</mo></mrow></msubsup><mo id="S2.p1.1.m1.5.5.1.1.1.3" stretchy="false" xref="S2.p1.1.m1.5.5.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.p1.1.m1.6.6.3" xref="S2.p1.1.m1.6.6.3.cmml">=</mo><mrow id="S2.p1.1.m1.6.6.2" xref="S2.p1.1.m1.6.6.2.cmml"><mrow id="S2.p1.1.m1.6.6.2.1" xref="S2.p1.1.m1.6.6.2.1.cmml"><msubsup id="S2.p1.1.m1.6.6.2.1.3" xref="S2.p1.1.m1.6.6.2.1.3.cmml"><mi id="S2.p1.1.m1.6.6.2.1.3.2.2" xref="S2.p1.1.m1.6.6.2.1.3.2.2.cmml">P</mi><mtext id="S2.p1.1.m1.6.6.2.1.3.3" xref="S2.p1.1.m1.6.6.2.1.3.3a.cmml">clean</mtext><mo id="S2.p1.1.m1.6.6.2.1.3.2.3" xref="S2.p1.1.m1.6.6.2.1.3.2.3.cmml">‚àó</mo></msubsup><mo id="S2.p1.1.m1.6.6.2.1.2" xref="S2.p1.1.m1.6.6.2.1.2.cmml">‚Å¢</mo><mrow id="S2.p1.1.m1.6.6.2.1.1.1" xref="S2.p1.1.m1.6.6.2.1.1.1.1.cmml"><mo id="S2.p1.1.m1.6.6.2.1.1.1.2" stretchy="false" xref="S2.p1.1.m1.6.6.2.1.1.1.1.cmml">(</mo><msubsup id="S2.p1.1.m1.6.6.2.1.1.1.1" xref="S2.p1.1.m1.6.6.2.1.1.1.1.cmml"><mi id="S2.p1.1.m1.6.6.2.1.1.1.1.2.2" xref="S2.p1.1.m1.6.6.2.1.1.1.1.2.2.cmml">h</mi><mi id="S2.p1.1.m1.6.6.2.1.1.1.1.3" xref="S2.p1.1.m1.6.6.2.1.1.1.1.3.cmml">i</mi><mrow id="S2.p1.1.m1.2.2.1.3" xref="S2.p1.1.m1.6.6.2.1.1.1.1.cmml"><mo id="S2.p1.1.m1.2.2.1.3.1" stretchy="false" xref="S2.p1.1.m1.6.6.2.1.1.1.1.cmml">(</mo><mi id="S2.p1.1.m1.2.2.1.1" xref="S2.p1.1.m1.2.2.1.1.cmml">l</mi><mo id="S2.p1.1.m1.2.2.1.3.2" stretchy="false" xref="S2.p1.1.m1.6.6.2.1.1.1.1.cmml">)</mo></mrow></msubsup><mo id="S2.p1.1.m1.6.6.2.1.1.1.3" stretchy="false" xref="S2.p1.1.m1.6.6.2.1.1.1.1.cmml">)</mo></mrow><mo id="S2.p1.1.m1.6.6.2.1.2a" xref="S2.p1.1.m1.6.6.2.1.2.cmml">‚Å¢</mo><mrow id="S2.p1.1.m1.6.6.2.1.4.2" xref="S2.p1.1.m1.6.6.2.1.4.1.cmml"><mo id="S2.p1.1.m1.6.6.2.1.4.2.1" stretchy="false" xref="S2.p1.1.m1.6.6.2.1.4.1.1.cmml">[</mo><mi id="S2.p1.1.m1.3.3" xref="S2.p1.1.m1.3.3.cmml">y</mi><mo id="S2.p1.1.m1.6.6.2.1.4.2.2" stretchy="false" xref="S2.p1.1.m1.6.6.2.1.4.1.1.cmml">]</mo></mrow></mrow><mo id="S2.p1.1.m1.6.6.2.2" xref="S2.p1.1.m1.6.6.2.2.cmml">‚àí</mo><mrow id="S2.p1.1.m1.6.6.2.3" xref="S2.p1.1.m1.6.6.2.3.cmml"><msup id="S2.p1.1.m1.6.6.2.3.2" xref="S2.p1.1.m1.6.6.2.3.2.cmml"><mi id="S2.p1.1.m1.6.6.2.3.2.2" xref="S2.p1.1.m1.6.6.2.3.2.2.cmml">P</mi><mo id="S2.p1.1.m1.6.6.2.3.2.3" xref="S2.p1.1.m1.6.6.2.3.2.3.cmml">‚àó</mo></msup><mo id="S2.p1.1.m1.6.6.2.3.1" xref="S2.p1.1.m1.6.6.2.3.1.cmml">‚Å¢</mo><mrow id="S2.p1.1.m1.6.6.2.3.3.2" xref="S2.p1.1.m1.6.6.2.3.3.1.cmml"><mo id="S2.p1.1.m1.6.6.2.3.3.2.1" stretchy="false" xref="S2.p1.1.m1.6.6.2.3.3.1.1.cmml">[</mo><mi id="S2.p1.1.m1.4.4" xref="S2.p1.1.m1.4.4.cmml">y</mi><mo id="S2.p1.1.m1.6.6.2.3.3.2.2" stretchy="false" xref="S2.p1.1.m1.6.6.2.3.3.1.1.cmml">]</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.6b"><apply id="S2.p1.1.m1.6.6.cmml" xref="S2.p1.1.m1.6.6"><eq id="S2.p1.1.m1.6.6.3.cmml" xref="S2.p1.1.m1.6.6.3"></eq><apply id="S2.p1.1.m1.5.5.1.cmml" xref="S2.p1.1.m1.5.5.1"><times id="S2.p1.1.m1.5.5.1.2.cmml" xref="S2.p1.1.m1.5.5.1.2"></times><ci id="S2.p1.1.m1.5.5.1.3a.cmml" xref="S2.p1.1.m1.5.5.1.3"><mtext id="S2.p1.1.m1.5.5.1.3.cmml" xref="S2.p1.1.m1.5.5.1.3">IE</mtext></ci><apply id="S2.p1.1.m1.5.5.1.1.1.1.cmml" xref="S2.p1.1.m1.5.5.1.1.1"><csymbol cd="ambiguous" id="S2.p1.1.m1.5.5.1.1.1.1.1.cmml" xref="S2.p1.1.m1.5.5.1.1.1">subscript</csymbol><apply id="S2.p1.1.m1.5.5.1.1.1.1.2.cmml" xref="S2.p1.1.m1.5.5.1.1.1"><csymbol cd="ambiguous" id="S2.p1.1.m1.5.5.1.1.1.1.2.1.cmml" xref="S2.p1.1.m1.5.5.1.1.1">superscript</csymbol><ci id="S2.p1.1.m1.5.5.1.1.1.1.2.2.cmml" xref="S2.p1.1.m1.5.5.1.1.1.1.2.2">‚Ñé</ci><ci id="S2.p1.1.m1.1.1.1.1.cmml" xref="S2.p1.1.m1.1.1.1.1">ùëô</ci></apply><ci id="S2.p1.1.m1.5.5.1.1.1.1.3.cmml" xref="S2.p1.1.m1.5.5.1.1.1.1.3">ùëñ</ci></apply></apply><apply id="S2.p1.1.m1.6.6.2.cmml" xref="S2.p1.1.m1.6.6.2"><minus id="S2.p1.1.m1.6.6.2.2.cmml" xref="S2.p1.1.m1.6.6.2.2"></minus><apply id="S2.p1.1.m1.6.6.2.1.cmml" xref="S2.p1.1.m1.6.6.2.1"><times id="S2.p1.1.m1.6.6.2.1.2.cmml" xref="S2.p1.1.m1.6.6.2.1.2"></times><apply id="S2.p1.1.m1.6.6.2.1.3.cmml" xref="S2.p1.1.m1.6.6.2.1.3"><csymbol cd="ambiguous" id="S2.p1.1.m1.6.6.2.1.3.1.cmml" xref="S2.p1.1.m1.6.6.2.1.3">subscript</csymbol><apply id="S2.p1.1.m1.6.6.2.1.3.2.cmml" xref="S2.p1.1.m1.6.6.2.1.3"><csymbol cd="ambiguous" id="S2.p1.1.m1.6.6.2.1.3.2.1.cmml" xref="S2.p1.1.m1.6.6.2.1.3">superscript</csymbol><ci id="S2.p1.1.m1.6.6.2.1.3.2.2.cmml" xref="S2.p1.1.m1.6.6.2.1.3.2.2">ùëÉ</ci><times id="S2.p1.1.m1.6.6.2.1.3.2.3.cmml" xref="S2.p1.1.m1.6.6.2.1.3.2.3"></times></apply><ci id="S2.p1.1.m1.6.6.2.1.3.3a.cmml" xref="S2.p1.1.m1.6.6.2.1.3.3"><mtext id="S2.p1.1.m1.6.6.2.1.3.3.cmml" mathsize="70%" xref="S2.p1.1.m1.6.6.2.1.3.3">clean</mtext></ci></apply><apply id="S2.p1.1.m1.6.6.2.1.1.1.1.cmml" xref="S2.p1.1.m1.6.6.2.1.1.1"><csymbol cd="ambiguous" id="S2.p1.1.m1.6.6.2.1.1.1.1.1.cmml" xref="S2.p1.1.m1.6.6.2.1.1.1">subscript</csymbol><apply id="S2.p1.1.m1.6.6.2.1.1.1.1.2.cmml" xref="S2.p1.1.m1.6.6.2.1.1.1"><csymbol cd="ambiguous" id="S2.p1.1.m1.6.6.2.1.1.1.1.2.1.cmml" xref="S2.p1.1.m1.6.6.2.1.1.1">superscript</csymbol><ci id="S2.p1.1.m1.6.6.2.1.1.1.1.2.2.cmml" xref="S2.p1.1.m1.6.6.2.1.1.1.1.2.2">‚Ñé</ci><ci id="S2.p1.1.m1.2.2.1.1.cmml" xref="S2.p1.1.m1.2.2.1.1">ùëô</ci></apply><ci id="S2.p1.1.m1.6.6.2.1.1.1.1.3.cmml" xref="S2.p1.1.m1.6.6.2.1.1.1.1.3">ùëñ</ci></apply><apply id="S2.p1.1.m1.6.6.2.1.4.1.cmml" xref="S2.p1.1.m1.6.6.2.1.4.2"><csymbol cd="latexml" id="S2.p1.1.m1.6.6.2.1.4.1.1.cmml" xref="S2.p1.1.m1.6.6.2.1.4.2.1">delimited-[]</csymbol><ci id="S2.p1.1.m1.3.3.cmml" xref="S2.p1.1.m1.3.3">ùë¶</ci></apply></apply><apply id="S2.p1.1.m1.6.6.2.3.cmml" xref="S2.p1.1.m1.6.6.2.3"><times id="S2.p1.1.m1.6.6.2.3.1.cmml" xref="S2.p1.1.m1.6.6.2.3.1"></times><apply id="S2.p1.1.m1.6.6.2.3.2.cmml" xref="S2.p1.1.m1.6.6.2.3.2"><csymbol cd="ambiguous" id="S2.p1.1.m1.6.6.2.3.2.1.cmml" xref="S2.p1.1.m1.6.6.2.3.2">superscript</csymbol><ci id="S2.p1.1.m1.6.6.2.3.2.2.cmml" xref="S2.p1.1.m1.6.6.2.3.2.2">ùëÉ</ci><times id="S2.p1.1.m1.6.6.2.3.2.3.cmml" xref="S2.p1.1.m1.6.6.2.3.2.3"></times></apply><apply id="S2.p1.1.m1.6.6.2.3.3.1.cmml" xref="S2.p1.1.m1.6.6.2.3.3.2"><csymbol cd="latexml" id="S2.p1.1.m1.6.6.2.3.3.1.1.cmml" xref="S2.p1.1.m1.6.6.2.3.3.2.1">delimited-[]</csymbol><ci id="S2.p1.1.m1.4.4.cmml" xref="S2.p1.1.m1.4.4">ùë¶</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.6c">\text{IE}(h^{(l)}_{i})=P^{*}_{\text{clean}}(h^{(l)}_{i})[y]-P^{*}[y]</annotation><annotation encoding="application/x-llamapun" id="S2.p1.1.m1.6d">IE ( italic_h start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = italic_P start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT start_POSTSUBSCRIPT clean end_POSTSUBSCRIPT ( italic_h start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) [ italic_y ] - italic_P start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT [ italic_y ]</annotation></semantics></math>. The Average Indirect Effect of a hidden node is an average of IE over all the prompts in the dataset.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.4">The <span class="ltx_text ltx_font_bold" id="S2.p2.4.1">Attention Contribution</span> <cite class="ltx_cite ltx_citemacro_cite">Yuksekgonul et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.00857v1#bib.bib10" title="">2024</a>)</cite>, focuses on the role of attention mechanisms in shaping the output of language models. This approach investigates how attention weights, particularly from the subject token in a query to the last token position, contribute to the model‚Äôs predictions. By examining the norm of these attention weights <math alttext="\|a^{(\ell)}_{i,T}\|" class="ltx_Math" display="inline" id="S2.p2.1.m1.4"><semantics id="S2.p2.1.m1.4a"><mrow id="S2.p2.1.m1.4.4.1" xref="S2.p2.1.m1.4.4.2.cmml"><mo id="S2.p2.1.m1.4.4.1.2" stretchy="false" xref="S2.p2.1.m1.4.4.2.1.cmml">‚Äñ</mo><msubsup id="S2.p2.1.m1.4.4.1.1" xref="S2.p2.1.m1.4.4.1.1.cmml"><mi id="S2.p2.1.m1.4.4.1.1.2.2" xref="S2.p2.1.m1.4.4.1.1.2.2.cmml">a</mi><mrow id="S2.p2.1.m1.3.3.2.4" xref="S2.p2.1.m1.3.3.2.3.cmml"><mi id="S2.p2.1.m1.2.2.1.1" xref="S2.p2.1.m1.2.2.1.1.cmml">i</mi><mo id="S2.p2.1.m1.3.3.2.4.1" xref="S2.p2.1.m1.3.3.2.3.cmml">,</mo><mi id="S2.p2.1.m1.3.3.2.2" xref="S2.p2.1.m1.3.3.2.2.cmml">T</mi></mrow><mrow id="S2.p2.1.m1.1.1.1.3" xref="S2.p2.1.m1.4.4.1.1.cmml"><mo id="S2.p2.1.m1.1.1.1.3.1" stretchy="false" xref="S2.p2.1.m1.4.4.1.1.cmml">(</mo><mi id="S2.p2.1.m1.1.1.1.1" mathvariant="normal" xref="S2.p2.1.m1.1.1.1.1.cmml">‚Ñì</mi><mo id="S2.p2.1.m1.1.1.1.3.2" stretchy="false" xref="S2.p2.1.m1.4.4.1.1.cmml">)</mo></mrow></msubsup><mo id="S2.p2.1.m1.4.4.1.3" stretchy="false" xref="S2.p2.1.m1.4.4.2.1.cmml">‚Äñ</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.4b"><apply id="S2.p2.1.m1.4.4.2.cmml" xref="S2.p2.1.m1.4.4.1"><csymbol cd="latexml" id="S2.p2.1.m1.4.4.2.1.cmml" xref="S2.p2.1.m1.4.4.1.2">norm</csymbol><apply id="S2.p2.1.m1.4.4.1.1.cmml" xref="S2.p2.1.m1.4.4.1.1"><csymbol cd="ambiguous" id="S2.p2.1.m1.4.4.1.1.1.cmml" xref="S2.p2.1.m1.4.4.1.1">subscript</csymbol><apply id="S2.p2.1.m1.4.4.1.1.2.cmml" xref="S2.p2.1.m1.4.4.1.1"><csymbol cd="ambiguous" id="S2.p2.1.m1.4.4.1.1.2.1.cmml" xref="S2.p2.1.m1.4.4.1.1">superscript</csymbol><ci id="S2.p2.1.m1.4.4.1.1.2.2.cmml" xref="S2.p2.1.m1.4.4.1.1.2.2">ùëé</ci><ci id="S2.p2.1.m1.1.1.1.1.cmml" xref="S2.p2.1.m1.1.1.1.1">‚Ñì</ci></apply><list id="S2.p2.1.m1.3.3.2.3.cmml" xref="S2.p2.1.m1.3.3.2.4"><ci id="S2.p2.1.m1.2.2.1.1.cmml" xref="S2.p2.1.m1.2.2.1.1">ùëñ</ci><ci id="S2.p2.1.m1.3.3.2.2.cmml" xref="S2.p2.1.m1.3.3.2.2">ùëá</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.4c">\|a^{(\ell)}_{i,T}\|</annotation><annotation encoding="application/x-llamapun" id="S2.p2.1.m1.4d">‚à• italic_a start_POSTSUPERSCRIPT ( roman_‚Ñì ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i , italic_T end_POSTSUBSCRIPT ‚à•</annotation></semantics></math>, we observe what tokens the last token pays the most attention to, during the generation process. The <span class="ltx_text ltx_font_bold" id="S2.p2.4.2">Attention Knockout</span> mechanism <cite class="ltx_cite ltx_citemacro_cite">Geva et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.00857v1#bib.bib1" title="">2023</a>)</cite> identifies critical attention edges in transformer-based models that are essential for maintaining prediction quality. The process involves identifying critical edges whose removal significantly degrades the model‚Äôs prediction quality, by means of setting the attention from position <math alttext="i" class="ltx_Math" display="inline" id="S2.p2.2.m2.1"><semantics id="S2.p2.2.m2.1a"><mi id="S2.p2.2.m2.1.1" xref="S2.p2.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.1b"><ci id="S2.p2.2.m2.1.1.cmml" xref="S2.p2.2.m2.1.1">ùëñ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m2.1c">i</annotation><annotation encoding="application/x-llamapun" id="S2.p2.2.m2.1d">italic_i</annotation></semantics></math> to <math alttext="j" class="ltx_Math" display="inline" id="S2.p2.3.m3.1"><semantics id="S2.p2.3.m3.1a"><mi id="S2.p2.3.m3.1.1" xref="S2.p2.3.m3.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S2.p2.3.m3.1b"><ci id="S2.p2.3.m3.1.1.cmml" xref="S2.p2.3.m3.1.1">ùëó</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.3.m3.1c">j</annotation><annotation encoding="application/x-llamapun" id="S2.p2.3.m3.1d">italic_j</annotation></semantics></math> in the attention matrix to <math alttext="-\infty" class="ltx_Math" display="inline" id="S2.p2.4.m4.1"><semantics id="S2.p2.4.m4.1a"><mrow id="S2.p2.4.m4.1.1" xref="S2.p2.4.m4.1.1.cmml"><mo id="S2.p2.4.m4.1.1a" xref="S2.p2.4.m4.1.1.cmml">‚àí</mo><mi id="S2.p2.4.m4.1.1.2" mathvariant="normal" xref="S2.p2.4.m4.1.1.2.cmml">‚àû</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.4.m4.1b"><apply id="S2.p2.4.m4.1.1.cmml" xref="S2.p2.4.m4.1.1"><minus id="S2.p2.4.m4.1.1.1.cmml" xref="S2.p2.4.m4.1.1"></minus><infinity id="S2.p2.4.m4.1.1.2.cmml" xref="S2.p2.4.m4.1.1.2"></infinity></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.4.m4.1c">-\infty</annotation><annotation encoding="application/x-llamapun" id="S2.p2.4.m4.1d">- ‚àû</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Data and Models</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">For examining model activations for causal tracing, patching and inspecting AIE, whilesystematically analyzing attention contributions we choose open source LMs like LlaMa-2 (7B) and Phi-2 (2.7B) models. And for understanding the behavior in the non-RAG setting, we leverage the <span class="ltx_text ltx_font_italic" id="S3.p1.1.1">Knowns 1000 dataset</span>, a dataset of 1209 prompts <cite class="ltx_cite ltx_citemacro_cite">Meng et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.00857v1#bib.bib5" title="">2022a</a>)</cite>. For the RAG setting, we augment the <span class="ltx_text ltx_font_italic" id="S3.p1.1.2">Knowns 1000 dataset</span> with added context generated synthetically using GPT-4. We use GPT-4 generated context to control the length of each segment within the RAg-context and also the presence of <span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.p1.1.3">attribute</span> or <span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.p1.1.4">object</span>.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Experimenting with LLaMa and Phi-2 family of models on 1209 samples from the knowns fact dataset for vanilla-case and RAG-scenario, demonstrate that both models exhibit a strong bias towards utilizing external knowledge provided by RAG.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.2">Utilizing <span class="ltx_text ltx_font_italic" id="S4.p2.2.1">Causal Tracing</span> method and measuring Average Indirect Effect (AIE) at different positions of the prompt, such as Last Subject Token (LST), Last Token (LT) , it is found that for the vanilla-case (non-RAG) LST had high AIE, but it substantially lowered when RAG-generated context was added. As concluded in <cite class="ltx_cite ltx_citemacro_cite">Meng et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.00857v1#bib.bib5" title="">2022a</a>)</cite>, LST has the largest influence from model priors and lowering AIE of LST demonstrates reduced influence of parametric memory. We specifically observe <math alttext="\sim" class="ltx_Math" display="inline" id="S4.p2.1.m1.1"><semantics id="S4.p2.1.m1.1a"><mo id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">‚àº</mo><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><csymbol cd="latexml" id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S4.p2.1.m1.1d">‚àº</annotation></semantics></math>10X decrease in AIE of LST for LLaMA-2 and <math alttext="\sim" class="ltx_Math" display="inline" id="S4.p2.2.m2.1"><semantics id="S4.p2.2.m2.1a"><mo id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml">‚àº</mo><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><csymbol cd="latexml" id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S4.p2.2.m2.1d">‚àº</annotation></semantics></math>35X decrease in AIE of LST for Phi-2, when RAG-generated context is added.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">This finding was further corroborated by utilizing two other probing methods - Attention Knockouts and Attention Contributions. The LT is a crucial component in the LLM decoding process, as it is projected onto the vocabulary during decoding time. Thus any information that has to be decoded, will be propagated by the MLP and attention layers to the LT residual stream. We measured the attention contributions from the Subject Token (ST) to the LT and observe a substantial decrease in ST contributions for the RAG-scenario as compared to the vanilla non-RAG case where no external context is provided. For LlaMa-2, the mean attention contribution decreased by <math alttext="\sim" class="ltx_Math" display="inline" id="S4.p3.1.m1.1"><semantics id="S4.p3.1.m1.1a"><mo id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml">‚àº</mo><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><csymbol cd="latexml" id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S4.p3.1.m1.1d">‚àº</annotation></semantics></math>1.6X for RAG case, in comparison to non-RAG vanilla case, and for Phi-2 a reduction of 7x was observed for ST contribution. Conversely, the <span class="ltx_text ltx_font_italic" id="S4.p3.1.1">answer token</span> contribution for RAG setting, increases significantly for LlaMa-2 and Phi-2 in comparison to ST contribution in the RAG setting. This further confirms our hypothesis of the LLM being less reliant on its parametric memory and exhibiting a "shortcut" behavior.</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1">Using Attention knockouts <cite class="ltx_cite ltx_citemacro_cite">Geva et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.00857v1#bib.bib1" title="">2023</a>)</cite> approach, it is observed that "knocking out" attention from ST to the LT, reduces the probability of the answer in the LM‚Äôs last token predictions by 20% in LlaMa-2 and 25% in Phi-2. This is in sharp contrast to the RAG setting, where knocking off attention at ST positions leads to &lt;5% drop in the answer probabilities. This finding further reinforces the finding that the model takes a "shortcut" while relying minimally on its parametric memory.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions and Future Work</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Using Causal Tracing <cite class="ltx_cite ltx_citemacro_cite">Meng et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.00857v1#bib.bib5" title="">2022a</a>)</cite> in over 1200 samples of the <span class="ltx_text ltx_font_italic" id="S5.p1.1.1">known facts dataset</span>, in RAG-scenario for LLaMa-2 and Phi-2, we observe a reduced AIE on the last subject token, and potentially reduced dependence on parametric memory. This is further corroborated by our experiments with attention contributions and attention knockouts. Using three mechanistic probing techniques, we observe 1) reduced reliance on parametric memory 2) reduced information flow from the subject token to the last token residual stream 3) a shortcut behavior where information from the attribute token flows to the last token residual stream during factual predictions in the RAG setting.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Future work will address the extension to larger LMs (&gt; 13B parameters). We also plan to study the impact of LM behavior in longer context, and in settings where language models are known to exhibit primacy and recency bias <cite class="ltx_cite ltx_citemacro_cite">Liu et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.00857v1#bib.bib4" title="">2023</a>)</cite> in a future work. Additionally, we aim to replicate our findings using a conventional RAG pipeline to automatically create context rather than synthetically generating it using GPT4.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geva et¬†al. (2023)</span>
<span class="ltx_bibblock">
Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2304.14767" title="">Dissecting recall of factual associations in auto-regressive language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Preprint</em>, arXiv:2304.14767.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">IngestAI (2023)</span>
<span class="ltx_bibblock">
IngestAI. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://ingestai.io/research/rag-enhancing-llms" title="">Retrieval-augmented generation (rag): Enhancing llms with external knowledge</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et¬†al. (2021)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K√ºttler, Mike Lewis, Wen tau Yih, Tim Rockt√§schel, Sebastian Riedel, and Douwe Kiela. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2005.11401" title="">Retrieval-augmented generation for knowledge-intensive nlp tasks</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Preprint</em>, arXiv:2005.11401.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et¬†al. (2023)</span>
<span class="ltx_bibblock">
Nelson¬†F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023.

</span>
<span class="ltx_bibblock">Lost in the middle: How language models use long contexts.

</span>
<span class="ltx_bibblock">ArXiv:2307.03172.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meng et¬†al. (2022a)</span>
<span class="ltx_bibblock">
Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022a.

</span>
<span class="ltx_bibblock">Locating and editing factual associations in gpt.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Advances in Neural Information Processing Systems</em>, 35:17359‚Äì17372.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meng et¬†al. (2022b)</span>
<span class="ltx_bibblock">
Kevin Meng, Arnab¬†Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. 2022b.

</span>
<span class="ltx_bibblock">Mass-editing memory in a transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:2210.07229</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et¬†al. (2023)</span>
<span class="ltx_bibblock">
C.¬†Shao, T.¬†Kim, and Z.¬†Gao. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://ar5iv.org/2405.06683" title="">Eragent: Enhancing retrieval-augmented language models with improved accuracy, efficiency, and personalization</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">arXiv preprint arXiv:2405.06683</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et¬†al. (2023)</span>
<span class="ltx_bibblock">
A.¬†Singh, M.¬†Sachan, and K.¬†Guu. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00411/106233/Improving-the-Domain-Adaptation-of-Retrieval" title="">Improving the domain adaptation of retrieval augmented generation (rag) models for open domain question answering</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Transactions of the Association for Computational Linguistics</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et¬†al. (2024)</span>
<span class="ltx_bibblock">
Kevin Wu, Eric Wu, and James Zou. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2404.10198" title="">How faithful are rag models? quantifying the tug-of-war between rag and llms‚Äô internal prior</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Preprint</em>, arXiv:2404.10198.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuksekgonul et¬†al. (2024)</span>
<span class="ltx_bibblock">
Mert Yuksekgonul, Varun Chandrasekaran, Erik Jones, Suriya Gunasekar, Ranjita Naik, Hamid Palangi, Ece Kamar, and Besmira Nushi. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2309.15098" title="">Attention satisfies: A constraint-satisfaction lens on factual errors of language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Preprint</em>, arXiv:2309.15098.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Oct  1 16:33:31 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
