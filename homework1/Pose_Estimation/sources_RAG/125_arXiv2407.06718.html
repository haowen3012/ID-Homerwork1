<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>A Simple Architecture for Enterprise Large Language Model Applications based on Role based security and Clearance Levels using Retrieval-Augmented Generation or Mixture of Experts</title>
<!--Generated on Tue Jul  9 09:43:09 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on 
.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2407.06718v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#S1" title="In A Simple Architecture for Enterprise Large Language Model Applications based on Role based security and Clearance Levels using Retrieval-Augmented Generation or Mixture of Experts"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#S2" title="In A Simple Architecture for Enterprise Large Language Model Applications based on Role based security and Clearance Levels using Retrieval-Augmented Generation or Mixture of Experts"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Background Information</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#S2.SS1" title="In 2 Background Information ‣ A Simple Architecture for Enterprise Large Language Model Applications based on Role based security and Clearance Levels using Retrieval-Augmented Generation or Mixture of Experts"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>NATO Clearance Levels</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#S2.SS2" title="In 2 Background Information ‣ A Simple Architecture for Enterprise Large Language Model Applications based on Role based security and Clearance Levels using Retrieval-Augmented Generation or Mixture of Experts"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Role-Based Access Control (RBAC)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#S2.SS3" title="In 2 Background Information ‣ A Simple Architecture for Enterprise Large Language Model Applications based on Role based security and Clearance Levels using Retrieval-Augmented Generation or Mixture of Experts"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Introduction to large language models</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#S2.SS3.SSS1" title="In 2.3 Introduction to large language models ‣ 2 Background Information ‣ A Simple Architecture for Enterprise Large Language Model Applications based on Role based security and Clearance Levels using Retrieval-Augmented Generation or Mixture of Experts"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.1 </span>Training of large language models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#S2.SS4" title="In 2 Background Information ‣ A Simple Architecture for Enterprise Large Language Model Applications based on Role based security and Clearance Levels using Retrieval-Augmented Generation or Mixture of Experts"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Retrieval-Augmented Generation (RAG)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#S2.SS5" title="In 2 Background Information ‣ A Simple Architecture for Enterprise Large Language Model Applications based on Role based security and Clearance Levels using Retrieval-Augmented Generation or Mixture of Experts"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5 </span>Mixture of experts</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#S3" title="In A Simple Architecture for Enterprise Large Language Model Applications based on Role based security and Clearance Levels using Retrieval-Augmented Generation or Mixture of Experts"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Proposed Architecture based on Role
based security and Clearance Levels</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#S3.SS1" title="In 3 Proposed Architecture based on Role based security and Clearance Levels ‣ A Simple Architecture for Enterprise Large Language Model Applications based on Role based security and Clearance Levels using Retrieval-Augmented Generation or Mixture of Experts"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>User, Role, Security Clearance to Documents Mapping</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#S3.SS2" title="In 3 Proposed Architecture based on Role based security and Clearance Levels ‣ A Simple Architecture for Enterprise Large Language Model Applications based on Role based security and Clearance Levels using Retrieval-Augmented Generation or Mixture of Experts"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Training LLM using Roles and Security Clearance Levels</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#S3.SS3" title="In 3 Proposed Architecture based on Role based security and Clearance Levels ‣ A Simple Architecture for Enterprise Large Language Model Applications based on Role based security and Clearance Levels using Retrieval-Augmented Generation or Mixture of Experts"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Inference using Retrieval-Augmented Generation (RAG)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#S3.SS4" title="In 3 Proposed Architecture based on Role based security and Clearance Levels ‣ A Simple Architecture for Enterprise Large Language Model Applications based on Role based security and Clearance Levels using Retrieval-Augmented Generation or Mixture of Experts"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Inference using Mixture of experts models (MoE)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#S3.SS5" title="In 3 Proposed Architecture based on Role based security and Clearance Levels ‣ A Simple Architecture for Enterprise Large Language Model Applications based on Role based security and Clearance Levels using Retrieval-Augmented Generation or Mixture of Experts"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Inference using both RAG and MoE</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#S4" title="In A Simple Architecture for Enterprise Large Language Model Applications based on Role based security and Clearance Levels using Retrieval-Augmented Generation or Mixture of Experts"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">A Simple Architecture for Enterprise Large Language Model Applications based on Role based security and Clearance Levels using Retrieval-Augmented Generation or Mixture of Experts</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Atilla Özgür<sup class="ltx_sup" id="id4.2.id1">1</sup>
</span><span class="ltx_author_notes">Atilla Özgür is partially supported by Grant DFG UY-56/5-1</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yılmaz Uygun <sup class="ltx_sup" id="id5.2.id1">1</sup>
</span></span>
</div>
<div class="ltx_dates">(
<sup class="ltx_sup" id="id6.id1">1</sup>School of Business, Social and Decision Sciences, Constructor University Bremen, Bremen, Germany 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id7.id2">{aoezguer, yuygun}@constructor.university</span>)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id8.id1">This study proposes a simple architecture for Enterprise application for Large Language Models (LLMs) for role based security and NATO clearance levels.
Our proposal aims to address the limitations of current LLMs in handling security and information access.
The proposed architecture could be used while utilizing Retrieval-Augmented Generation (RAG) and fine tuning of Mixture of experts models (MoE).
It could be used only with RAG, or only with MoE or with both of them.
Using roles and security clearance level of the user, documents in RAG and experts in MoE are filtered.
This way information leakage is prevented.</p>
<p class="ltx_p" id="id9.id2"><span class="ltx_text ltx_font_bold" id="id9.id2.1">Keywords:</span> Large language models, Mixture of Experts, role-based security, clearance levels</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">According to World Economic Forum <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#bib.bib9" title="">9</a>]</cite>, Venture Capital investments in the area of artificial intelligence are about $290 billion in the last 5 years.
With the introduction of ChatGPT by OpenAI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#bib.bib21" title="">21</a>]</cite>, interest in the large language models (LLMs) has exploded.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Even though ChatGPT provides an application programming interface (API) to the developers, it is a closed model.
Developers could only utilize provided API.
To answer ChatGPT from OpenAI, large language model called LLaMA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#bib.bib27" title="">27</a>]</cite> is introduced by Meta (Facebook).
With the advent of LLaMA, the era of open source Large Language Models started.
Open source developers trained their own models in different domains using LLaMA and published these models.
Due to success of this, other companies followed and published their own models in open source fashion.
For example, Microsoft published Phi <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#bib.bib1" title="">1</a>]</cite> and Google published gemini <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#bib.bib10" title="">10</a>]</cite> open source LLMs.
A more complete list could be found in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#bib.bib30" title="">30</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Additionally, a lot of different client libraries for these open source models are introduced.
With these client libraries, it is easier than ever to produce applications which utilizes LLMs in their portfolio.
Security of the these applications are very important.
But in academia, instead of general security, most of the time security for LLM itself are the focus.
Security of LLMs are reviewed in a lot of different articles <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#bib.bib17" title="">17</a>]</cite>.
But these articles are mostly about the attack on LLMs itself.
Even articles about privacy is mostly concerned from general usage of LLMs.
According to the best of our knowledge, how to secure LLMs in enterprises from the view point of security clearance levels and role based security is not investigated.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">A recent lawsuit by New York Times <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#bib.bib12" title="">12</a>]</cite> shows that is is possible to reproduce exact trained documents using appropriate prompts.
Normally, this is only a copyright and privacy problem but in the context of military or enterprise applications, such a problem becomes a big security problem.
Think about the following example: A custom LLM application developed for NATO military documents.
If an user of this NATO LLM application could reproduce a Secret NATO document when his security clearance is only for Confidential documents, this reproduction of document would be a very big security problem.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">This study proposes a simple role based security architecture for custom LLM applications.
This approach could be used while utilizing Retrieval-Augmented Generation (RAG) and fine tuning of Mixture of experts models (MoE).
It could be used only with RAG, or only with MoE or with both of them.
While using RAG, only documents which user’s roles have access to will be returned from RAG.
While using MoE, only experts which user’s roles have access will be consulted.
This way, information leakage is prevented and security of the application will be ensured.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background Information</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">To be able to understand proposed model more easily, some background information is needed.
In this section, this background information about NATO clearance levels, role based security, introduction to large language models, Retrieval-Augmented Generation (RAG) and Mixture of experts are given.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>NATO Clearance Levels</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The North Atlantic Treaty Organization (NATO) is a military alliance.
NATO consists of 32 member states and is established after World War II.
NATO like many international organizations deals with sensitive information.
This information can range from battle plans to diplomatic communications and intelligence reports.
Leaking this information could have serious consequences <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#bib.bib4" title="">4</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">NATO has following four security classifications:</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<ol class="ltx_enumerate" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1">Cosmic top secret</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1">Secret</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1">Confidential</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S2.I1.i4.p1">
<p class="ltx_p" id="S2.I1.i4.p1.1">Restricted</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">Even though it is not counted among the classifications, not classified category is also used to show that a document or an information could be shared outside NATO but their rights belong to NATO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#bib.bib24" title="">24</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p5">
<p class="ltx_p" id="S2.SS1.p5.1">Clearance levels or security classifications are used for following purposes: Protect Classified Information, Minimize Risks and Ensure Trust Between Allies
Classified information is categorized according to its sensitivity.
For example size information of F-35 fighter jet could be not classified while radar sensitive painting information of F-35 could be Cosmic top secret.
In short, NATO clearance levels are a security categorization designed to prevent sensitive information.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Role-Based Access Control (RBAC)</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Role-Based Access Control (RBAC) is a security approach that manages access to resources within a system.
Idea of RBAC has been around since the start of multi user computers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#bib.bib26" title="">26</a>]</cite>.
RBAC resolves around permissions, roles and user.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="382" id="S2.F1.g1" src="extracted/5719817/role-based-example1.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S2.F1.3.2" style="font-size:90%;">Example Role Based Access</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Permissions control access to resources or abilities of users.
An example for permission could be log on to system.
Roles are used as containers for permissions.
For example a human resources (HR) role could be used to contain permission related to personnel management tasks.
In most systems, a role could also contain other roles.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#S2.F1" title="Figure 1 ‣ 2.2 Role-Based Access Control (RBAC) ‣ 2 Background Information ‣ A Simple Architecture for Enterprise Large Language Model Applications based on Role based security and Clearance Levels using Retrieval-Augmented Generation or Mixture of Experts"><span class="ltx_text ltx_ref_tag">1</span></a> shows that Normal User role belongs to other roles.
Similar to Roles, Users also could belong to more than one role.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">When security clearance levels are used, most of the time, these clearance levels names are extended together with normal roles.
For example, Operator role will be extended as Operator not classified, Operator restricted and so on.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Introduction to large language models</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Large Language Models are supervised learning models.
That is they are trained with using known input and outputs.
LLMs are trained using a lot of text from internet to repeatedly predict next words using previous words.
See Table <a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#S2.T1" title="Table 1 ‣ 2.3 Introduction to large language models ‣ 2 Background Information ‣ A Simple Architecture for Enterprise Large Language Model Applications based on Role based security and Clearance Levels using Retrieval-Augmented Generation or Mixture of Experts"><span class="ltx_text ltx_ref_tag">1</span></a> for an example sentence: My favorite food is a Döner with spicy sauce.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T1.2.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S2.T1.3.2" style="font-size:90%;">LLM Text Generation Example</span></figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.T1.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.4.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S2.T1.4.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.1.1.1.1">
<span class="ltx_p" id="S2.T1.4.1.1.1.1.1" style="width:346.9pt;">Input A</span>
</span>
</th>
<th class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S2.T1.4.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.1.1.2.1">
<span class="ltx_p" id="S2.T1.4.1.1.2.1.1" style="width:86.7pt;">Output</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.4.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.4.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.2.1.1.1">
<span class="ltx_p" id="S2.T1.4.2.1.1.1.1" style="width:346.9pt;">My favorite food is a</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.4.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.2.1.2.1">
<span class="ltx_p" id="S2.T1.4.2.1.2.1.1" style="width:86.7pt;">Döner</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.4.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.3.2.1.1">
<span class="ltx_p" id="S2.T1.4.3.2.1.1.1" style="width:346.9pt;">My favorite food is a Döner</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S2.T1.4.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.3.2.2.1">
<span class="ltx_p" id="S2.T1.4.3.2.2.1.1" style="width:86.7pt;">with</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.4.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.4.3.1.1">
<span class="ltx_p" id="S2.T1.4.4.3.1.1.1" style="width:346.9pt;">My favorite food is a Döner with</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S2.T1.4.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.4.3.2.1">
<span class="ltx_p" id="S2.T1.4.4.3.2.1.1" style="width:86.7pt;">spicy</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S2.T1.4.5.4.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.5.4.1.1">
<span class="ltx_p" id="S2.T1.4.5.4.1.1.1" style="width:346.9pt;">My favorite food is a Döner with spicy</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb" id="S2.T1.4.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.5.4.2.1">
<span class="ltx_p" id="S2.T1.4.5.4.2.1.1" style="width:86.7pt;">sauce</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<section class="ltx_subsubsection" id="S2.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.1 </span>Training of large language models</h4>
<div class="ltx_para" id="S2.SS3.SSS1.p1">
<p class="ltx_p" id="S2.SS3.SSS1.p1.1">For training LLMs, terabytes of text from internet are used.
Basic workflow for training LLM are something like below:</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS1.p2">
<ol class="ltx_enumerate" id="S2.I2">
<li class="ltx_item" id="S2.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S2.I2.i1.p1">
<p class="ltx_p" id="S2.I2.i1.p1.1">Download  10+TB of text</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S2.I2.i2.p1">
<p class="ltx_p" id="S2.I2.i2.p1.1">Get a cluster of 6k+ GPUs</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S2.I2.i3.p1">
<p class="ltx_p" id="S2.I2.i3.p1.1">Train your neural network, pay  $2M, wait for  12 days</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S2.I2.i4.p1">
<p class="ltx_p" id="S2.I2.i4.p1.1">Obtain <span class="ltx_text ltx_font_bold" id="S2.I2.i4.p1.1.1">base model</span></p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S2.SS3.SSS1.p3">
<p class="ltx_p" id="S2.SS3.SSS1.p3.1">This training could be seen as something like compressing the training dataset.
Since LLMs could only produce values that are in their training data, for specialized tasks fine tuning of LLMs should be done.
Fine tuning should be done with quality data specially prepared for the task.
Basic workflow for fine tuning LLM is something like below:</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS1.p4">
<ol class="ltx_enumerate" id="S2.I3">
<li class="ltx_item" id="S2.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S2.I3.i1.p1">
<p class="ltx_p" id="S2.I3.i1.p1.1">create curated dataset of quality instructions</p>
</div>
</li>
<li class="ltx_item" id="S2.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S2.I3.i2.p1">
<p class="ltx_p" id="S2.I3.i2.p1.1">fine tune base model on this data, wait  1 day</p>
</div>
</li>
<li class="ltx_item" id="S2.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S2.I3.i3.p1">
<p class="ltx_p" id="S2.I3.i3.p1.1">Obtain <span class="ltx_text ltx_font_bold" id="S2.I3.i3.p1.1.1">assistant model</span></p>
</div>
</li>
<li class="ltx_item" id="S2.I3.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S2.I3.i4.p1">
<p class="ltx_p" id="S2.I3.i4.p1.1">Test your model if necessary go to step 1</p>
</div>
</li>
<li class="ltx_item" id="S2.I3.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S2.I3.i5.p1">
<p class="ltx_p" id="S2.I3.i5.p1.1">Deploy on your servers</p>
</div>
</li>
<li class="ltx_item" id="S2.I3.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para" id="S2.I3.i6.p1">
<p class="ltx_p" id="S2.I3.i6.p1.1">Monitor, go to step 1</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S2.SS3.SSS1.p5">
<p class="ltx_p" id="S2.SS3.SSS1.p5.1">Since fine tuning uses less data, it costs less and could be repeated more.
When using Mixture of experts, first fine tuning would be slower since we will need to fine tune multiple expert LLMs.
But subsequent fine tuning for experts will be faster since whenever new documents come only the relevant experts will be re-fine tuned.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Retrieval-Augmented Generation (RAG)</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">As we have explained in section <a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#S2.SS3" title="2.3 Introduction to large language models ‣ 2 Background Information ‣ A Simple Architecture for Enterprise Large Language Model Applications based on Role based security and Clearance Levels using Retrieval-Augmented Generation or Mixture of Experts"><span class="ltx_text ltx_ref_tag">2.3</span></a>, LLMs are trained with massive amounts of data.
As all machine learning models, LLMs are also depended on statistical patterns in their training data.
For example, an LLM model, which is trained in 2023, will not be able to answer questions about Euro 2024 (European Football Championship).</p>
</div>
<div class="ltx_para" id="S2.SS4.p2">
<p class="ltx_p" id="S2.SS4.p2.1">Retrieval-Augmented Generation (RAG) introduced by Facebook researchers<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#bib.bib16" title="">16</a>]</cite> address these limitation by connecting LLMs to update data sources.
These sources could be news articles, company internal knowledge base or databases like wikipedia.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="84" id="S2.F2.g1" src="extracted/5719817/RAG-workflow.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S2.F2.3.2" style="font-size:90%;">Retrieval-Augmented Generation (RAG) workflow</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS4.p3">
<p class="ltx_p" id="S2.SS4.p3.1">RAG workflow could be seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#S2.F2" title="Figure 2 ‣ 2.4 Retrieval-Augmented Generation (RAG) ‣ 2 Background Information ‣ A Simple Architecture for Enterprise Large Language Model Applications based on Role based security and Clearance Levels using Retrieval-Augmented Generation or Mixture of Experts"><span class="ltx_text ltx_ref_tag">2</span></a></p>
</div>
<div class="ltx_para" id="S2.SS4.p4">
<p class="ltx_p" id="S2.SS4.p4.1">When a new prompt comes to LLM system, similar documents to this new prompt are searched in databases.
Most of the time a vector database is used for fast response times.
Then, LLM uses this context enriched prompt to give answers.</p>
</div>
<div class="ltx_para" id="S2.SS4.p5">
<p class="ltx_p" id="S2.SS4.p5.1">RAG makes LLM outputs more reliable using factual databases.
Like previous example of Euro 2024, RAG enables LLMs to use latest information if their training data is older.
RAG could also be adapted to specific domains using relevant databases.</p>
</div>
<div class="ltx_para" id="S2.SS4.p6">
<p class="ltx_p" id="S2.SS4.p6.1">In our proposed role-based access control architecture, every document in RAG databases will have allowed Roles information.
When similar documents are searched for RAG prompts, only documents which asking user have access to will be searched.
This way, information leakage will be prevented.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span>Mixture of experts</h3>
<div class="ltx_para" id="S2.SS5.p1">
<p class="ltx_p" id="S2.SS5.p1.1">Jacobs et al <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#bib.bib15" title="">15</a>]</cite> introduced mixture-of-experts (MoE) model in 1991.
The MoE models utilizes individual neural networks as experts instead of a single neural network.
The idea of mixture of experts are very similar to ensemble learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#bib.bib23" title="">23</a>]</cite>.
Ensemble learning combines multiple classifiers in different fashion.
According to Polikar <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#bib.bib23" title="">23</a>]</cite>, among the first examples of Ensemble Learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#bib.bib6" title="">6</a>]</cite> was in 1979 by Dasarathy and Sheela.
Mixture of experts name is more popular in deep learning literature while ensemble learning is more popular in general machine learning literature.</p>
</div>
<div class="ltx_para" id="S2.SS5.p2">
<p class="ltx_p" id="S2.SS5.p2.1">In mixture of experts, different neural networks to solve the problem are called experts.
Mixture of expert model uses a router or coordinator neural network to decide which experts to utilize <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#bib.bib20" title="">20</a>]</cite>.
Router assigns a ”gating score” to each network to indicate how relevant the input to the expert is.
A softmax function is used to transform the gating scores to a probability distribution.
When doing the predictions only experts with highest gating scores are activated.
This router idea is very similar to voting in the ensemble classifiers.
Voting classifiers are used in a lot of different domains like intrusion detection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#bib.bib34" title="">34</a>]</cite></p>
</div>
<div class="ltx_para" id="S2.SS5.p3">
<p class="ltx_p" id="S2.SS5.p3.1">In our role based access control architecture, gating scores of some experts will be zero according to the roles of the caller.
For example, a normal user is calling the large language model with a human resource prompt.
Only the experts which normal user has access to will be activated.
Other experts like human resources expert will not be called at all.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Proposed Architecture based on Role
based security and Clearance Levels</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>User, Role, Security Clearance to Documents Mapping</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Most enterprises map their users to one or more roles.
This mapping could be stored in directory services like active directory (Microsoft) or Enterprise Resource planning databases.
For our LLM application, this user to role and user to security clearance levels mappings should be accessible from its programming interface.
Additionally, role to documents and security clearance levels to documents should also be accessible.
This mapping information could be stored in LLM application’s own database or a web service could be provided to the application.
Basic mapping could be seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#S3.F3" title="Figure 3 ‣ 3.1 User, Role, Security Clearance to Documents Mapping ‣ 3 Proposed Architecture based on Role based security and Clearance Levels ‣ A Simple Architecture for Enterprise Large Language Model Applications based on Role based security and Clearance Levels using Retrieval-Augmented Generation or Mixture of Experts"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="390" id="S3.F3.g1" src="extracted/5719817/role-document-db-diagram.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.3.2" style="font-size:90%;">Entity Relationship Diagram for User to Role and Clearance Level to Document Mapping</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">Every User has more than one role, every User has exactly one security clearance.
Every Document has exactly one security clearance.
Every Role has access to zero or more documents.
It is implied that if a user does not have access to necessary clearance level, he will not be able to access the document.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Training LLM using Roles and Security Clearance Levels</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Proposed architecture presumes using local open source LLMs due to security requirements.
But if strict local security is not required then commercial LLMs like OpenAI ChatGPT with Retrieval-Augmented Generation (RAG) could be used.
When commercial LLMs are used, training step may not be necessary according to requirements or only not confidential documents could be used for training.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">For local open source LLMs, Mixture of Experts model should be used.
For this case, it is advised to train multiple experts for every role multiplied by five, that is four clearance levels plus not classified.
For example, let’s assume that the LLM application has four roles: HR, Accounting, Normal User, IT as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#S2.F1" title="Figure 1 ‣ 2.2 Role-Based Access Control (RBAC) ‣ 2 Background Information ‣ A Simple Architecture for Enterprise Large Language Model Applications based on Role based security and Clearance Levels using Retrieval-Augmented Generation or Mixture of Experts"><span class="ltx_text ltx_ref_tag">1</span></a>.
Then, we should train <math alttext="4*5=20" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mrow id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml"><mn id="S3.SS2.p2.1.m1.1.1.2.2" xref="S3.SS2.p2.1.m1.1.1.2.2.cmml">4</mn><mo id="S3.SS2.p2.1.m1.1.1.2.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p2.1.m1.1.1.2.1.cmml">∗</mo><mn id="S3.SS2.p2.1.m1.1.1.2.3" xref="S3.SS2.p2.1.m1.1.1.2.3.cmml">5</mn></mrow><mo id="S3.SS2.p2.1.m1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><eq id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1"></eq><apply id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2"><times id="S3.SS2.p2.1.m1.1.1.2.1.cmml" xref="S3.SS2.p2.1.m1.1.1.2.1"></times><cn id="S3.SS2.p2.1.m1.1.1.2.2.cmml" type="integer" xref="S3.SS2.p2.1.m1.1.1.2.2">4</cn><cn id="S3.SS2.p2.1.m1.1.1.2.3.cmml" type="integer" xref="S3.SS2.p2.1.m1.1.1.2.3">5</cn></apply><cn id="S3.SS2.p2.1.m1.1.1.3.cmml" type="integer" xref="S3.SS2.p2.1.m1.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">4*5=20</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">4 ∗ 5 = 20</annotation></semantics></math> experts.
This experts will be named like HR Not Classified, HR Restricted, HR Confidential, HR Secret, HR Cosmic top secret.
See full names in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#S3.T2" title="Table 2 ‣ 3.2 Training LLM using Roles and Security Clearance Levels ‣ 3 Proposed Architecture based on Role based security and Clearance Levels ‣ A Simple Architecture for Enterprise Large Language Model Applications based on Role based security and Clearance Levels using Retrieval-Augmented Generation or Mixture of Experts"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T2.2.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S3.T2.3.2" style="font-size:90%;">Roles and Clearance Levels for Experts</span></figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T2.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.4.1.1">
<td class="ltx_td ltx_align_top ltx_border_tt" id="S3.T2.4.1.1.1"></td>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S3.T2.4.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.4.1.1.2.1">
<span class="ltx_p" id="S3.T2.4.1.1.2.1.1" style="width:86.7pt;">HR</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S3.T2.4.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.4.1.1.3.1">
<span class="ltx_p" id="S3.T2.4.1.1.3.1.1" style="width:86.7pt;">Accounting</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S3.T2.4.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.4.1.1.4.1">
<span class="ltx_p" id="S3.T2.4.1.1.4.1.1" style="width:86.7pt;">Normal User</span>
</span>
</th>
<th class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S3.T2.4.1.1.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.4.1.1.5.1">
<span class="ltx_p" id="S3.T2.4.1.1.5.1.1" style="width:86.7pt;">IT</span>
</span>
</th>
</tr>
<tr class="ltx_tr" id="S3.T2.4.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.4.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.4.2.2.1.1">
<span class="ltx_p" id="S3.T2.4.2.2.1.1.1" style="width:86.7pt;">Not classified</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.4.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.4.2.2.2.1">
<span class="ltx_p" id="S3.T2.4.2.2.2.1.1" style="width:86.7pt;">HR Not classified</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.4.2.2.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.4.2.2.3.1">
<span class="ltx_p" id="S3.T2.4.2.2.3.1.1" style="width:86.7pt;">Accounting Not classified</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.4.2.2.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.4.2.2.4.1">
<span class="ltx_p" id="S3.T2.4.2.2.4.1.1" style="width:86.7pt;">Normal User Not classified</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.4.2.2.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.4.2.2.5.1">
<span class="ltx_p" id="S3.T2.4.2.2.5.1.1" style="width:86.7pt;">IT Not classified</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.3.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.4.3.3.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.4.3.3.1.1">
<span class="ltx_p" id="S3.T2.4.3.3.1.1.1" style="width:86.7pt;">Restricted</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.4.3.3.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.4.3.3.2.1">
<span class="ltx_p" id="S3.T2.4.3.3.2.1.1" style="width:86.7pt;">HR Restricted</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.4.3.3.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.4.3.3.3.1">
<span class="ltx_p" id="S3.T2.4.3.3.3.1.1" style="width:86.7pt;">Accounting Restricted</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.4.3.3.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.4.3.3.4.1">
<span class="ltx_p" id="S3.T2.4.3.3.4.1.1" style="width:86.7pt;">Restricted</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S3.T2.4.3.3.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.4.3.3.5.1">
<span class="ltx_p" id="S3.T2.4.3.3.5.1.1" style="width:86.7pt;">IT Restricted</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.4.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.4.4.4.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.4.4.4.1.1">
<span class="ltx_p" id="S3.T2.4.4.4.1.1.1" style="width:86.7pt;">Confidential</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.4.4.4.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.4.4.4.2.1">
<span class="ltx_p" id="S3.T2.4.4.4.2.1.1" style="width:86.7pt;">HR Confidential</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.4.4.4.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.4.4.4.3.1">
<span class="ltx_p" id="S3.T2.4.4.4.3.1.1" style="width:86.7pt;">Accounting Confidential</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.4.4.4.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.4.4.4.4.1">
<span class="ltx_p" id="S3.T2.4.4.4.4.1.1" style="width:86.7pt;">Normal User Confidential</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S3.T2.4.4.4.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.4.4.4.5.1">
<span class="ltx_p" id="S3.T2.4.4.4.5.1.1" style="width:86.7pt;">IT Confidential</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.5.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.4.5.5.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.4.5.5.1.1">
<span class="ltx_p" id="S3.T2.4.5.5.1.1.1" style="width:86.7pt;">Secret</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.4.5.5.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.4.5.5.2.1">
<span class="ltx_p" id="S3.T2.4.5.5.2.1.1" style="width:86.7pt;">HR Secret</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.4.5.5.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.4.5.5.3.1">
<span class="ltx_p" id="S3.T2.4.5.5.3.1.1" style="width:86.7pt;">Accounting Secret</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.4.5.5.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.4.5.5.4.1">
<span class="ltx_p" id="S3.T2.4.5.5.4.1.1" style="width:86.7pt;">Normal User Secret</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S3.T2.4.5.5.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.4.5.5.5.1">
<span class="ltx_p" id="S3.T2.4.5.5.5.1.1" style="width:86.7pt;">IT Secret</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.6.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S3.T2.4.6.6.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.4.6.6.1.1">
<span class="ltx_p" id="S3.T2.4.6.6.1.1.1" style="width:86.7pt;">Cosmic top secret</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S3.T2.4.6.6.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.4.6.6.2.1">
<span class="ltx_p" id="S3.T2.4.6.6.2.1.1" style="width:86.7pt;">HR Cosmic top secret</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S3.T2.4.6.6.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.4.6.6.3.1">
<span class="ltx_p" id="S3.T2.4.6.6.3.1.1" style="width:86.7pt;">Accounting Cosmic top secret</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S3.T2.4.6.6.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.4.6.6.4.1">
<span class="ltx_p" id="S3.T2.4.6.6.4.1.1" style="width:86.7pt;">Normal User Cosmic top secret</span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb" id="S3.T2.4.6.6.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.4.6.6.5.1">
<span class="ltx_p" id="S3.T2.4.6.6.5.1.1" style="width:86.7pt;">IT Cosmic top secret</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Inference using Retrieval-Augmented Generation (RAG)</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">While doing the inference using Retrieval-Augmented Generation (RAG), role information and security clearance levels should be used.
Most of the vector databases allow use of filters.
Using filters, only documents a user has access to should be returned.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="390" id="S3.F4.g1" src="extracted/5719817/role-based-access-to-LLM-only-RAG-sequence-diagram.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S3.F4.3.2" style="font-size:90%;"> Sequence Diagram for Role/Clearance level based access to LLM only using RAG</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">As could be seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#S3.F4" title="Figure 4 ‣ 3.3 Inference using Retrieval-Augmented Generation (RAG) ‣ 3 Proposed Architecture based on Role based security and Clearance Levels ‣ A Simple Architecture for Enterprise Large Language Model Applications based on Role based security and Clearance Levels using Retrieval-Augmented Generation or Mixture of Experts"><span class="ltx_text ltx_ref_tag">4</span></a>, this architecture does not presume mixture of experts or local open source model; therefore, this role based model could also be used with commercial LLMs like ChatGPT.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Inference using Mixture of experts models (MoE)</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">In this inference model, local open source LLM should be used.
Experts in MoE are trained using only documents for which role and security clearance level has an access to.
For performance reasons, filtering in the router part of MoE would be useful.
If router does not ask answers from experts for whom user has no security clearance or necessary role, the application will perform faster.
Full workflow could be seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#S3.F5" title="Figure 5 ‣ 3.4 Inference using Mixture of experts models (MoE) ‣ 3 Proposed Architecture based on Role based security and Clearance Levels ‣ A Simple Architecture for Enterprise Large Language Model Applications based on Role based security and Clearance Levels using Retrieval-Augmented Generation or Mixture of Experts"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="390" id="S3.F5.g1" src="extracted/5719817/role-based-access-to-LLM-only-MoE-sequence-diagram.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S3.F5.3.2" style="font-size:90%;"> Sequence Diagram for Role/Clearance level based access to LLM only using MoE</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Inference using both RAG and MoE</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">Doing inference using both Retrieval-Augmented Generation (RAG) and Mixture of experts models (MoE) could be seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.06718v1#S3.F6" title="Figure 6 ‣ 3.5 Inference using both RAG and MoE ‣ 3 Proposed Architecture based on Role based security and Clearance Levels ‣ A Simple Architecture for Enterprise Large Language Model Applications based on Role based security and Clearance Levels using Retrieval-Augmented Generation or Mixture of Experts"><span class="ltx_text ltx_ref_tag">6</span></a>.
Basically, this workflow is combination of previous two workflows.</p>
</div>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="303" id="S3.F6.g1" src="extracted/5719817/role-based-access-to-LLM-sequence-diagram.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F6.2.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S3.F6.3.2" style="font-size:90%;">Role based access to LLM Sequence Diagram</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">A simple role based security architecture for Large Language Model (LLM) applications are proposed.
Background information necessary for the understanding of the architecture given in multiple sections.
Proposed approach is usable with Retrieval-Augmented Generation (RAG) and/or Mixture of experts models (MoE).
Normally usage of local open source LLM models are assumed but RAG version is also suitable for commercial LLMs.
Sequence diagrams for workflows using RAG, MoE and RAG+MoE are given.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Marah Abdin and et al.

</span>
<span class="ltx_bibblock">Phi-3 technical report: A highly capable language model locally on
your phone, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Muhammad Ali, Mansoor-ul Haque, Muhammad Hanif Durad, Anila Usman,
Syed Muhammad Mohsin, Hana Mujlid, and Carsten Maple.

</span>
<span class="ltx_bibblock">Effective network intrusion detection using stacking-based ensemble
approach.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">International Journal of Information Security</span>,
22(6):1781–1798, July 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Arijit Ghosh Chowdhury, Md Mofijul Islam, Vaibhav Kumar, Faysal Hossain Shezan,
Vaibhav Kumar, Vinija Jain, and Aman Chadha.

</span>
<span class="ltx_bibblock">Breaking down the defenses: A comparative survey of attacks on large
language models, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
NATO Security Comittee.

</span>
<span class="ltx_bibblock">A short guide to the handling of nato classified information.

</span>
<span class="ltx_bibblock">Technical report, NATO, 1958.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Badhan Chandra Das, M. Hadi Amini, and Yanzhao Wu.

</span>
<span class="ltx_bibblock">Security and privacy challenges of large language models: A survey,
2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
B.V. Dasarathy and B.V. Sheela.

</span>
<span class="ltx_bibblock">A composite classifier system design: Concepts and methodology.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">Proceedings of the IEEE</span>, 67(5):708–713, 1979.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Erik Derner, Kristina Batistič, Jan Zahálka, and Robert Babuška.

</span>
<span class="ltx_bibblock">A security risk taxonomy for large language models, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Mohamed Amine Ferrag, Fatima Alwahedi, Ammar Battah, Bilel Cherif, Abdechakour
Mechri, and Norbert Tihanyi.

</span>
<span class="ltx_bibblock">Generative ai and large language models for cyber security: All
insights you need, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
World Economic Forum.

</span>
<span class="ltx_bibblock">How venture capital is investing in ai in the top five global
economies — and shaping the ai ecosystem, May 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Gemma Team and et al.

</span>
<span class="ltx_bibblock">Gemma: Open models based on gemini research and technology, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Takeshi Goto, Kensuke Ono, and Akira Morita.

</span>
<span class="ltx_bibblock">Evaluating the cybersecurity robustness of commercial llms against
adversarial prompts: A promptbench analysis.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">TechRxiv</span>, March 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Michael M. Grynbaum and Ryan Mac.

</span>
<span class="ltx_bibblock">The times sues openai and microsoft over a.i. use of copyrighted
work, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Mohammed Hassanin and Nour Moustafa.

</span>
<span class="ltx_bibblock">A comprehensive overview of large language models (llms) for cyber
defences: Opportunities and directions, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Umar Iqbal, Tadayoshi Kohno, and Franziska Roesner.

</span>
<span class="ltx_bibblock">Llm platform security: Applying a systematic evaluation framework to
openai’s chatgpt plugins, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton.

</span>
<span class="ltx_bibblock">Adaptive mixtures of local experts.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">Neural Computation</span>, 3(1):79–87, February 1991.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
Rocktäschel, Sebastian Riedel, and Douwe Kiela.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks,
2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Jiaju Lin, Haoran Zhao, Aochi Zhang, Yiting Wu, Huqiuyue Ping, and Qin Chen.

</span>
<span class="ltx_bibblock">Agentsims: An open-source sandbox for large language model
evaluation, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Zefang Liu.

</span>
<span class="ltx_bibblock">A review of advancements and applications of pre-trained language
models in cybersecurity.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">2024 12th International Symposium on Digital Forensics and
Security (ISDFS)</span>. IEEE, April 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Sichun Luo, Wei Shao, Yuxuan Yao, Jian Xu, Mingyang Liu, Qintong Li, Bowei He,
Maolin Wang, Guanzhi Deng, Hanxu Hou, Xinyi Zhang, and Linqi Song.

</span>
<span class="ltx_bibblock">Privacy in llm-based recommendation: Recent advances and future
directions, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Saeed Masoudnia and Reza Ebrahimpour.

</span>
<span class="ltx_bibblock">Mixture of experts: a literature survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">Artificial Intelligence Review</span>, 42(2):275–293, May 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Chatgpt (mar 14 version) [large language model], 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Rahul Pankajakshan, Sumitra Biswal, Yuvaraj Govindarajulu, and Gilad Gressel.

</span>
<span class="ltx_bibblock">Mapping llm security landscapes: A comprehensive stakeholder risk
assessment proposal, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
R. Polikar.

</span>
<span class="ltx_bibblock">Ensemble learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">Scholarpedia</span>, 4(1):2776, 2009.

</span>
<span class="ltx_bibblock">revision #186077.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Alasdair Roberts.

</span>
<span class="ltx_bibblock">Nato, secrecy, and the right to information.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">East EuropeanConstitutional Review</span>, 12 2002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Yakub Kayode Saheed and Sanjay Misra.

</span>
<span class="ltx_bibblock">A voting gray wolf optimizer-based ensemble learning models for
intrusion detection in the internet of things.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">International Journal of Information Security</span>,
23(3):1557–1581, January 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Ravi S. Sandhu.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">Role-based Access Control</span>, pages 237–286.

</span>
<span class="ltx_bibblock">Elsevier, 1998.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume
Lample.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Shang Wang, Tianqing Zhu, Bo Liu, Ming Ding, Xu Guo, Dayong Ye, Wanlei Zhou,
and Philip S. Yu.

</span>
<span class="ltx_bibblock">Unique security and privacy threats of large language model: A
comprehensive survey, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Fangzhou Wu, Ning Zhang, Somesh Jha, Patrick McDaniel, and Chaowei Xiao.

</span>
<span class="ltx_bibblock">A new era in llm security: Exploring security concerns in real-world
llm-based systems, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Eugene Yan.

</span>
<span class="ltx_bibblock">Open large language models list, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo Sun, and Yue Zhang.

</span>
<span class="ltx_bibblock">A survey on large language model (llm) security and privacy: The
good, the bad, and the ugly.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">High-Confidence Computing</span>, 4(2):100211, June 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Hayette Zeghida, Mehdi Boulaiche, and Ramdane Chikh.

</span>
<span class="ltx_bibblock">Securing mqtt protocol for iot environment using ids based on
ensemble learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">International Journal of Information Security</span>,
22(4):1075–1086, March 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Jie Zhang, Haoyu Bu, Hui Wen, Yu Chen, Lun Li, and Hongsong Zhu.

</span>
<span class="ltx_bibblock">When llms meet cybersecurity: A systematic literature review, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Atilla Özgür and Hamit Erdem.

</span>
<span class="ltx_bibblock">Feature selection and multiple classifier fusion using genetic
algorithms in intrusion detection systems.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">Journal of the Faculty of Engineering and Architecture of Gazi
University</span>, 33:0 – 0, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Atilla Özgür, Fatih Nar, and Hamit Erdem.

</span>
<span class="ltx_bibblock">Sparsity-driven weighted ensemble classifier.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">International Journal of Computational Intelligence Systems</span>,
11:962–978, 2018.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jul  9 09:43:09 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
