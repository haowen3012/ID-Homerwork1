<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation</title>
<!--Generated on Tue Sep  3 03:10:03 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.13694v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx1" title="In A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx2" title="In A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Preliminaries</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx2.SSx1" title="In Preliminaries ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Problem Definition</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx2.SSx2" title="In Preliminaries ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Dataset and Metrics</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx3" title="In A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Benchmark Evaluation of RAG</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx4" title="In A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Extensive Empirical Studies</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx4.SSx1" title="In Extensive Empirical Studies ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Advancing Knowledge Source Selection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx4.SSx2" title="In Extensive Empirical Studies ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Enhancing Retrieval for Knowledge Extraction</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx4.SSx2.SSSx1" title="In Enhancing Retrieval for Knowledge Extraction ‣ Extensive Empirical Studies ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Advancing Efficiency with Broad Retrieval.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx4.SSx2.SSSx2" title="In Enhancing Retrieval for Knowledge Extraction ‣ Extensive Empirical Studies ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Evaluating Focused Retrieval Methodologies in RAG.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx4.SSx2.SSSx3" title="In Enhancing Retrieval for Knowledge Extraction ‣ Extensive Empirical Studies ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Reranking for Enhanced Knowledge Utilization.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx4.SSx3" title="In Extensive Empirical Studies ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Evaluating Knowledge Reasoning on RAG Performance</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx4.SSx3.SSSx1" title="In Evaluating Knowledge Reasoning on RAG Performance ‣ Extensive Empirical Studies ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Impact of Chain of Thought (CoT) .</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx4.SSx3.SSSx2" title="In Evaluating Knowledge Reasoning on RAG Performance ‣ Extensive Empirical Studies ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Analysis of Few-Shot Learning.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx4.SSx3.SSSx3" title="In Evaluating Knowledge Reasoning on RAG Performance ‣ Extensive Empirical Studies ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Impact of Query Position in Prompt.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx4.SSx3.SSSx4" title="In Evaluating Knowledge Reasoning on RAG Performance ‣ Extensive Empirical Studies ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Impact of Noise Chunks.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx5" title="In A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Impact of Hyperparameter Configurations</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx5.SSx1" title="In Impact of Hyperparameter Configurations ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Effect of Hyperparameters on Retrieval</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx5.SSx2" title="In Impact of Hyperparameter Configurations ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Effect of Hyperparameters on LLM Reasoning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx6" title="In A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Conclusion: Challenges and Future Direction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx6.SSx2.SSSx4.Px1" title="In Conclusion: Challenges and Future Direction ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Knowledge Selection.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx6.SSx2.SSSx4.Px2" title="In Conclusion: Challenges and Future Direction ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Knowledge Retrieval.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx6.SSx2.SSSx4.Px3" title="In Conclusion: Challenges and Future Direction ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Knowledge Reasoning.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx6.SSx2.SSSx4.Px4" title="In Conclusion: Challenges and Future Direction ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Performance Evaluation.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#A1" title="In A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix A Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#A1.SSx1" title="In Appendix A Appendix A Related Work ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Retrieval-Augmented Generation (RAG)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#A1.SSx2" title="In Appendix A Appendix A Related Work ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Existing Benchmarks for RAG Systems</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#A1.SSx3" title="In Appendix A Appendix A Related Work ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Chain of Thought (CoT) and In-Context Learning (ICL)</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#A2" title="In A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Appendix B Reproducibility</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#A2.SSx1" title="In Appendix B Appendix B Reproducibility ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Dataset Processing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#A2.SSx2" title="In Appendix B Appendix B Reproducibility ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Experimental Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#A2.SSx3" title="In Appendix B Appendix B Reproducibility ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Evaluation Metrics</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#A2.SSx3.SSS0.Px1" title="In Evaluation Metrics ‣ Appendix B Appendix B Reproducibility ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Accuracy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#A2.SSx3.SSS0.Px2" title="In Evaluation Metrics ‣ Appendix B Appendix B Reproducibility ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Hallucination</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#A2.SSx3.SSS0.Px3" title="In Evaluation Metrics ‣ Appendix B Appendix B Reproducibility ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Missing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#A2.SSx3.SSS0.Px4" title="In Evaluation Metrics ‣ Appendix B Appendix B Reproducibility ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Score</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#A2.SSx3.SSS0.Px5" title="In Evaluation Metrics ‣ Appendix B Appendix B Reproducibility ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Evaluation Process</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#A2.SSx4" title="In Appendix B Appendix B Reproducibility ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Explanation of Knowledge Source Selection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#A2.SSx5" title="In Appendix B Appendix B Reproducibility ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Automatic Selection of External Knowledge Sources</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#A2.SSx6" title="In Appendix B Appendix B Reproducibility ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Noise Chunk Selection Strategy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#A2.SSx7" title="In Appendix B Appendix B Reproducibility ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Guidelines for Extracting Information from the Mock API Responses</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#A2.SSx7.SSS0.Px1" title="In Guidelines for Extracting Information from the Mock API Responses ‣ Appendix B Appendix B Reproducibility ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Time-Specific Guidelines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#A2.SSx7.SSS0.Px2" title="In Guidelines for Extracting Information from the Mock API Responses ‣ Appendix B Appendix B Reproducibility ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Movie Domain</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#A2.SSx7.SSS0.Px3" title="In Guidelines for Extracting Information from the Mock API Responses ‣ Appendix B Appendix B Reproducibility ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Finance Domain</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#A2.SSx7.SSS0.Px4" title="In Guidelines for Extracting Information from the Mock API Responses ‣ Appendix B Appendix B Reproducibility ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Music Domain</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#A2.SSx7.SSS0.Px5" title="In Guidelines for Extracting Information from the Mock API Responses ‣ Appendix B Appendix B Reproducibility ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Sports Domain</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#A3" title="In A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Appendix C Computing Infrastructure</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#A4" title="In A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Appendix D Case Study</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#A4.SSx4" title="In Appendix D Appendix D Case Study ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Prompt Without Chain of Thought (CoT) and No Internal Knowledge</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#A4.SSx5" title="In Appendix D Appendix D Case Study ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title">Prompt with Chain of Thought (CoT) and Use of Internal Knowledge</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Shuo Yu,
Mingyue Cheng<sup class="ltx_sup" id="id2.1.id1">*</sup>,
Jiqian Yang,
Jie Ouyang
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id3.id1">Retrieval-Augmented Generation (RAG) enhances generative models by integrating retrieval mechanisms, which allow these models to access and utilize external knowledge sources. Despite its advantages, RAG encounters significant challenges, particularly in effectively handling real-world queries and mitigating hallucinations. The KDD Cup 2024 CRAG competition brings these issues to the forefront by incorporating both web pages and a mock API as knowledge sources, adding the complexity of parsing HTML before large language models (LLMs) can process the information. In this paper, we propose a novel RAG benchmark designed to address these challenges. Our work provides a comprehensive set of experimental results, offering valuable insights for the study of RAG. We thoroughly examine the entire RAG process, including knowledge source selection, retrieval, organization, and reasoning. Key findings from our study include the impact of automated knowledge source selection using agents and the influence of noise chunks on RAG reasoning. Additionally, we conduct detailed experiments to analyze the effects of various hyperparameters on RAG performance. To support further research, we have made our results, the associated code, and a parsed version of the CRAG dataset publicly available<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/USTCAGI/RAG-X</span></span></span>, contributing to the advancement of RAG methodologies and establishing a solid foundation for future work in this domain.</p>
</div>
<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><sup class="ltx_sup" id="footnote2.1">*</sup>Mingyue Cheng is the corresponding author.</span></span></span>
<figure class="ltx_figure" id="S0.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="336" id="S0.F1.g1" src="x1.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span> RAG-X: A knowledge-centric RAG framework with LLM router-driven source selection, multi-step retrieval, and enhanced reasoning using noise chunks.</figcaption>
</figure>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Introduction</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">Retrieval-Augmented Generation (RAG) enhances generative models by integrating retrieval mechanisms <cite class="ltx_cite ltx_citemacro_citep">(Guu et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib15" title="">2020</a>; Lewis et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib20" title="">2020</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib21" title="">2021</a>)</cite>, enabling these models to dynamically access and utilize external knowledge <cite class="ltx_cite ltx_citemacro_citep">(Gao et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib12" title="">2024</a>; Sun et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib30" title="">2024</a>)</cite>. This hybrid approach not only significantly improves the accuracy and relevance of generated content but also effectively mitigates the problem of hallucinations in generative models <cite class="ltx_cite ltx_citemacro_citep">(Xu et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib35" title="">2024</a>)</cite>. By grounding responses in real-time, contextually relevant data, RAG systems become robust tools for producing reliable, fact-based content across a wide array of applications.</p>
</div>
<div class="ltx_para" id="Sx1.p2">
<p class="ltx_p" id="Sx1.p2.1">Despite significant advancements, RAG systems continue to face challenges in handling real-world, long-tail queries <cite class="ltx_cite ltx_citemacro_citep">(Yang et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib36" title="">2024</a>; Cheng et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib5" title="">2024</a>)</cite> and mitigating hallucinations <cite class="ltx_cite ltx_citemacro_citep">(Kandpal et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib18" title="">2023</a>; Ding et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib10" title="">2024</a>)</cite>. The KDD Cup 2024 CRAG competition underscores these difficulties by centering on complex, realistic Question Answering (QA) tasks that leverage two primary knowledge sources: web pages and a structured mock API <cite class="ltx_cite ltx_citemacro_citep">(Gaur et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib13" title="">2021</a>; Wang et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib32" title="">2023a</a>)</cite>. While these sources provide valuable opportunities for integrating diverse external knowledge, they also expose limitations in current RAG frameworks <cite class="ltx_cite ltx_citemacro_citep">(Chen et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib3" title="">2023</a>)</cite>, particularly in the added complexity of parsing HTML-formatted web pages for effective utilization by large language models (LLMs).</p>
</div>
<div class="ltx_para" id="Sx1.p3">
<p class="ltx_p" id="Sx1.p3.1">To address the challenges in evaluating and improving RAG systems, this paper introduces a novel RAG benchmark. In our work, we have converted the HTML-formatted web pages from the KDD Cup CRAG competition dataset into markdown (MD) format, enabling LLMs to more effectively utilize the information embedded within these web pages. Additionally, we present our self-implemented RAG framework, accompanied by a comprehensive set of experimental results, demonstrating the robustness and effectiveness of our approach. These findings provide valuable insights and practical guidelines for the research community, laying a solid foundation for further advancements and innovations in RAG methodologies.</p>
</div>
<div class="ltx_para" id="Sx1.p4">
<p class="ltx_p" id="Sx1.p4.1">Our study provides an in-depth examination of the entire RAG process, including knowledge source selection, retrieval, organization, and reasoning. We investigate the impact of various factors on system performance, such as the use of agents for automatic knowledge source selection <cite class="ltx_cite ltx_citemacro_citep">(Asai et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib1" title="">2023</a>)</cite> and the effects of noise chunks on the reasoning capabilities of RAG systems <cite class="ltx_cite ltx_citemacro_citep">(Cuconasu et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib8" title="">2024</a>)</cite>. Furthermore, we conduct detailed experiments to analyze how different hyperparameter configurations can influence RAG performance, offering a understanding of how specific settings can either enhance or impair the system’s accuracy and reliability. This approach ensures a thorough evaluation, providing key insights for optimizing RAG systems to achieve more accurate and dependable outcomes.</p>
</div>
<div class="ltx_para" id="Sx1.p5">
<p class="ltx_p" id="Sx1.p5.1">Beyond our experimental findings, we contribute to the research community by openly sharing our results, codebase, and a parsed version of the CRAG dataset. This transparency is intended to encourage further exploration and innovation in the field, allowing other researchers to build on our work and advance the state of the art in RAG methodologies. By making these resources publicly available, we aim to support the continued development and refinement of RAG systems, fostering collaboration and progress within the research community.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Preliminaries</h2>
<section class="ltx_subsection" id="Sx2.SSx1">
<h3 class="ltx_title ltx_title_subsection">Problem Definition</h3>
<div class="ltx_para" id="Sx2.SSx1.p1">
<p class="ltx_p" id="Sx2.SSx1.p1.3">The central challenge addressed by the Retrieval-Augmented Generation (RAG) model is the effective fusion of internal LLM knowledge with diverse external knowledge sources to enhance generative performance in knowledge-intensive tasks <cite class="ltx_cite ltx_citemacro_citep">(Guu et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib15" title="">2020</a>)</cite>. The RAG model aims to generate the optimal output <math alttext="y^{*}" class="ltx_Math" display="inline" id="Sx2.SSx1.p1.1.m1.1"><semantics id="Sx2.SSx1.p1.1.m1.1a"><msup id="Sx2.SSx1.p1.1.m1.1.1" xref="Sx2.SSx1.p1.1.m1.1.1.cmml"><mi id="Sx2.SSx1.p1.1.m1.1.1.2" xref="Sx2.SSx1.p1.1.m1.1.1.2.cmml">y</mi><mo id="Sx2.SSx1.p1.1.m1.1.1.3" xref="Sx2.SSx1.p1.1.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="Sx2.SSx1.p1.1.m1.1b"><apply id="Sx2.SSx1.p1.1.m1.1.1.cmml" xref="Sx2.SSx1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="Sx2.SSx1.p1.1.m1.1.1.1.cmml" xref="Sx2.SSx1.p1.1.m1.1.1">superscript</csymbol><ci id="Sx2.SSx1.p1.1.m1.1.1.2.cmml" xref="Sx2.SSx1.p1.1.m1.1.1.2">𝑦</ci><times id="Sx2.SSx1.p1.1.m1.1.1.3.cmml" xref="Sx2.SSx1.p1.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx1.p1.1.m1.1c">y^{*}</annotation><annotation encoding="application/x-llamapun" id="Sx2.SSx1.p1.1.m1.1d">italic_y start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math> by conditioning on both the input <math alttext="x" class="ltx_Math" display="inline" id="Sx2.SSx1.p1.2.m2.1"><semantics id="Sx2.SSx1.p1.2.m2.1a"><mi id="Sx2.SSx1.p1.2.m2.1.1" xref="Sx2.SSx1.p1.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx1.p1.2.m2.1b"><ci id="Sx2.SSx1.p1.2.m2.1.1.cmml" xref="Sx2.SSx1.p1.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx1.p1.2.m2.1c">x</annotation><annotation encoding="application/x-llamapun" id="Sx2.SSx1.p1.2.m2.1d">italic_x</annotation></semantics></math> and a set of retrieved documents <math alttext="z^{*}" class="ltx_Math" display="inline" id="Sx2.SSx1.p1.3.m3.1"><semantics id="Sx2.SSx1.p1.3.m3.1a"><msup id="Sx2.SSx1.p1.3.m3.1.1" xref="Sx2.SSx1.p1.3.m3.1.1.cmml"><mi id="Sx2.SSx1.p1.3.m3.1.1.2" xref="Sx2.SSx1.p1.3.m3.1.1.2.cmml">z</mi><mo id="Sx2.SSx1.p1.3.m3.1.1.3" xref="Sx2.SSx1.p1.3.m3.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="Sx2.SSx1.p1.3.m3.1b"><apply id="Sx2.SSx1.p1.3.m3.1.1.cmml" xref="Sx2.SSx1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="Sx2.SSx1.p1.3.m3.1.1.1.cmml" xref="Sx2.SSx1.p1.3.m3.1.1">superscript</csymbol><ci id="Sx2.SSx1.p1.3.m3.1.1.2.cmml" xref="Sx2.SSx1.p1.3.m3.1.1.2">𝑧</ci><times id="Sx2.SSx1.p1.3.m3.1.1.3.cmml" xref="Sx2.SSx1.p1.3.m3.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx1.p1.3.m3.1c">z^{*}</annotation><annotation encoding="application/x-llamapun" id="Sx2.SSx1.p1.3.m3.1d">italic_z start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math> from external sources, with the objective defined as:</p>
<table class="ltx_equation ltx_eqn_table" id="Sx2.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="y^{*}=\\
arg\\
max_{y}P(y|x,z^{*})," class="ltx_Math" display="block" id="Sx2.Ex1.m1.2"><semantics id="Sx2.Ex1.m1.2a"><mrow id="Sx2.Ex1.m1.2.2.1" xref="Sx2.Ex1.m1.2.2.1.1.cmml"><mrow id="Sx2.Ex1.m1.2.2.1.1" xref="Sx2.Ex1.m1.2.2.1.1.cmml"><msup id="Sx2.Ex1.m1.2.2.1.1.3" xref="Sx2.Ex1.m1.2.2.1.1.3.cmml"><mi id="Sx2.Ex1.m1.2.2.1.1.3.2" xref="Sx2.Ex1.m1.2.2.1.1.3.2.cmml">y</mi><mo id="Sx2.Ex1.m1.2.2.1.1.3.3" xref="Sx2.Ex1.m1.2.2.1.1.3.3.cmml">∗</mo></msup><mo id="Sx2.Ex1.m1.2.2.1.1.2" xref="Sx2.Ex1.m1.2.2.1.1.2.cmml">=</mo><mrow id="Sx2.Ex1.m1.2.2.1.1.1" xref="Sx2.Ex1.m1.2.2.1.1.1.cmml"><mi id="Sx2.Ex1.m1.2.2.1.1.1.3" xref="Sx2.Ex1.m1.2.2.1.1.1.3.cmml">a</mi><mo id="Sx2.Ex1.m1.2.2.1.1.1.2" xref="Sx2.Ex1.m1.2.2.1.1.1.2.cmml">⁢</mo><mi id="Sx2.Ex1.m1.2.2.1.1.1.4" xref="Sx2.Ex1.m1.2.2.1.1.1.4.cmml">r</mi><mo id="Sx2.Ex1.m1.2.2.1.1.1.2a" xref="Sx2.Ex1.m1.2.2.1.1.1.2.cmml">⁢</mo><mi id="Sx2.Ex1.m1.2.2.1.1.1.5" xref="Sx2.Ex1.m1.2.2.1.1.1.5.cmml">g</mi><mo id="Sx2.Ex1.m1.2.2.1.1.1.2b" xref="Sx2.Ex1.m1.2.2.1.1.1.2.cmml">⁢</mo><mi id="Sx2.Ex1.m1.2.2.1.1.1.6" xref="Sx2.Ex1.m1.2.2.1.1.1.6.cmml">m</mi><mo id="Sx2.Ex1.m1.2.2.1.1.1.2c" xref="Sx2.Ex1.m1.2.2.1.1.1.2.cmml">⁢</mo><mi id="Sx2.Ex1.m1.2.2.1.1.1.7" xref="Sx2.Ex1.m1.2.2.1.1.1.7.cmml">a</mi><mo id="Sx2.Ex1.m1.2.2.1.1.1.2d" xref="Sx2.Ex1.m1.2.2.1.1.1.2.cmml">⁢</mo><msub id="Sx2.Ex1.m1.2.2.1.1.1.8" xref="Sx2.Ex1.m1.2.2.1.1.1.8.cmml"><mi id="Sx2.Ex1.m1.2.2.1.1.1.8.2" xref="Sx2.Ex1.m1.2.2.1.1.1.8.2.cmml">x</mi><mi id="Sx2.Ex1.m1.2.2.1.1.1.8.3" xref="Sx2.Ex1.m1.2.2.1.1.1.8.3.cmml">y</mi></msub><mo id="Sx2.Ex1.m1.2.2.1.1.1.2e" xref="Sx2.Ex1.m1.2.2.1.1.1.2.cmml">⁢</mo><mi id="Sx2.Ex1.m1.2.2.1.1.1.9" xref="Sx2.Ex1.m1.2.2.1.1.1.9.cmml">P</mi><mo id="Sx2.Ex1.m1.2.2.1.1.1.2f" xref="Sx2.Ex1.m1.2.2.1.1.1.2.cmml">⁢</mo><mrow id="Sx2.Ex1.m1.2.2.1.1.1.1.1" xref="Sx2.Ex1.m1.2.2.1.1.1.1.1.1.cmml"><mo id="Sx2.Ex1.m1.2.2.1.1.1.1.1.2" stretchy="false" xref="Sx2.Ex1.m1.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="Sx2.Ex1.m1.2.2.1.1.1.1.1.1" xref="Sx2.Ex1.m1.2.2.1.1.1.1.1.1.cmml"><mi id="Sx2.Ex1.m1.2.2.1.1.1.1.1.1.3" xref="Sx2.Ex1.m1.2.2.1.1.1.1.1.1.3.cmml">y</mi><mo fence="false" id="Sx2.Ex1.m1.2.2.1.1.1.1.1.1.2" xref="Sx2.Ex1.m1.2.2.1.1.1.1.1.1.2.cmml">|</mo><mrow id="Sx2.Ex1.m1.2.2.1.1.1.1.1.1.1.1" xref="Sx2.Ex1.m1.2.2.1.1.1.1.1.1.1.2.cmml"><mi id="Sx2.Ex1.m1.1.1" xref="Sx2.Ex1.m1.1.1.cmml">x</mi><mo id="Sx2.Ex1.m1.2.2.1.1.1.1.1.1.1.1.2" xref="Sx2.Ex1.m1.2.2.1.1.1.1.1.1.1.2.cmml">,</mo><msup id="Sx2.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1" xref="Sx2.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><mi id="Sx2.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.2" xref="Sx2.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml">z</mi><mo id="Sx2.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.3" xref="Sx2.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml">∗</mo></msup></mrow></mrow><mo id="Sx2.Ex1.m1.2.2.1.1.1.1.1.3" stretchy="false" xref="Sx2.Ex1.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="Sx2.Ex1.m1.2.2.1.2" xref="Sx2.Ex1.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx2.Ex1.m1.2b"><apply id="Sx2.Ex1.m1.2.2.1.1.cmml" xref="Sx2.Ex1.m1.2.2.1"><eq id="Sx2.Ex1.m1.2.2.1.1.2.cmml" xref="Sx2.Ex1.m1.2.2.1.1.2"></eq><apply id="Sx2.Ex1.m1.2.2.1.1.3.cmml" xref="Sx2.Ex1.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="Sx2.Ex1.m1.2.2.1.1.3.1.cmml" xref="Sx2.Ex1.m1.2.2.1.1.3">superscript</csymbol><ci id="Sx2.Ex1.m1.2.2.1.1.3.2.cmml" xref="Sx2.Ex1.m1.2.2.1.1.3.2">𝑦</ci><times id="Sx2.Ex1.m1.2.2.1.1.3.3.cmml" xref="Sx2.Ex1.m1.2.2.1.1.3.3"></times></apply><apply id="Sx2.Ex1.m1.2.2.1.1.1.cmml" xref="Sx2.Ex1.m1.2.2.1.1.1"><times id="Sx2.Ex1.m1.2.2.1.1.1.2.cmml" xref="Sx2.Ex1.m1.2.2.1.1.1.2"></times><ci id="Sx2.Ex1.m1.2.2.1.1.1.3.cmml" xref="Sx2.Ex1.m1.2.2.1.1.1.3">𝑎</ci><ci id="Sx2.Ex1.m1.2.2.1.1.1.4.cmml" xref="Sx2.Ex1.m1.2.2.1.1.1.4">𝑟</ci><ci id="Sx2.Ex1.m1.2.2.1.1.1.5.cmml" xref="Sx2.Ex1.m1.2.2.1.1.1.5">𝑔</ci><ci id="Sx2.Ex1.m1.2.2.1.1.1.6.cmml" xref="Sx2.Ex1.m1.2.2.1.1.1.6">𝑚</ci><ci id="Sx2.Ex1.m1.2.2.1.1.1.7.cmml" xref="Sx2.Ex1.m1.2.2.1.1.1.7">𝑎</ci><apply id="Sx2.Ex1.m1.2.2.1.1.1.8.cmml" xref="Sx2.Ex1.m1.2.2.1.1.1.8"><csymbol cd="ambiguous" id="Sx2.Ex1.m1.2.2.1.1.1.8.1.cmml" xref="Sx2.Ex1.m1.2.2.1.1.1.8">subscript</csymbol><ci id="Sx2.Ex1.m1.2.2.1.1.1.8.2.cmml" xref="Sx2.Ex1.m1.2.2.1.1.1.8.2">𝑥</ci><ci id="Sx2.Ex1.m1.2.2.1.1.1.8.3.cmml" xref="Sx2.Ex1.m1.2.2.1.1.1.8.3">𝑦</ci></apply><ci id="Sx2.Ex1.m1.2.2.1.1.1.9.cmml" xref="Sx2.Ex1.m1.2.2.1.1.1.9">𝑃</ci><apply id="Sx2.Ex1.m1.2.2.1.1.1.1.1.1.cmml" xref="Sx2.Ex1.m1.2.2.1.1.1.1.1"><csymbol cd="latexml" id="Sx2.Ex1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="Sx2.Ex1.m1.2.2.1.1.1.1.1.1.2">conditional</csymbol><ci id="Sx2.Ex1.m1.2.2.1.1.1.1.1.1.3.cmml" xref="Sx2.Ex1.m1.2.2.1.1.1.1.1.1.3">𝑦</ci><list id="Sx2.Ex1.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="Sx2.Ex1.m1.2.2.1.1.1.1.1.1.1.1"><ci id="Sx2.Ex1.m1.1.1.cmml" xref="Sx2.Ex1.m1.1.1">𝑥</ci><apply id="Sx2.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="Sx2.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="Sx2.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx2.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="Sx2.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="Sx2.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.2">𝑧</ci><times id="Sx2.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml" xref="Sx2.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.3"></times></apply></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.Ex1.m1.2c">y^{*}=\\
arg\\
max_{y}P(y|x,z^{*}),</annotation><annotation encoding="application/x-llamapun" id="Sx2.Ex1.m1.2d">italic_y start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT = italic_a italic_r italic_g italic_m italic_a italic_x start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT italic_P ( italic_y | italic_x , italic_z start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="Sx2.SSx1.p1.4">where <math alttext="z^{*}" class="ltx_Math" display="inline" id="Sx2.SSx1.p1.4.m1.1"><semantics id="Sx2.SSx1.p1.4.m1.1a"><msup id="Sx2.SSx1.p1.4.m1.1.1" xref="Sx2.SSx1.p1.4.m1.1.1.cmml"><mi id="Sx2.SSx1.p1.4.m1.1.1.2" xref="Sx2.SSx1.p1.4.m1.1.1.2.cmml">z</mi><mo id="Sx2.SSx1.p1.4.m1.1.1.3" xref="Sx2.SSx1.p1.4.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="Sx2.SSx1.p1.4.m1.1b"><apply id="Sx2.SSx1.p1.4.m1.1.1.cmml" xref="Sx2.SSx1.p1.4.m1.1.1"><csymbol cd="ambiguous" id="Sx2.SSx1.p1.4.m1.1.1.1.cmml" xref="Sx2.SSx1.p1.4.m1.1.1">superscript</csymbol><ci id="Sx2.SSx1.p1.4.m1.1.1.2.cmml" xref="Sx2.SSx1.p1.4.m1.1.1.2">𝑧</ci><times id="Sx2.SSx1.p1.4.m1.1.1.3.cmml" xref="Sx2.SSx1.p1.4.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx1.p1.4.m1.1c">z^{*}</annotation><annotation encoding="application/x-llamapun" id="Sx2.SSx1.p1.4.m1.1d">italic_z start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math> represents the set of documents that are retrieved to maximize the probability:</p>
<table class="ltx_equation ltx_eqn_table" id="Sx2.Ex2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="z^{*}=\\
arg\\
max_{z}P(z|x)." class="ltx_Math" display="block" id="Sx2.Ex2.m1.1"><semantics id="Sx2.Ex2.m1.1a"><mrow id="Sx2.Ex2.m1.1.1.1" xref="Sx2.Ex2.m1.1.1.1.1.cmml"><mrow id="Sx2.Ex2.m1.1.1.1.1" xref="Sx2.Ex2.m1.1.1.1.1.cmml"><msup id="Sx2.Ex2.m1.1.1.1.1.3" xref="Sx2.Ex2.m1.1.1.1.1.3.cmml"><mi id="Sx2.Ex2.m1.1.1.1.1.3.2" xref="Sx2.Ex2.m1.1.1.1.1.3.2.cmml">z</mi><mo id="Sx2.Ex2.m1.1.1.1.1.3.3" xref="Sx2.Ex2.m1.1.1.1.1.3.3.cmml">∗</mo></msup><mo id="Sx2.Ex2.m1.1.1.1.1.2" xref="Sx2.Ex2.m1.1.1.1.1.2.cmml">=</mo><mrow id="Sx2.Ex2.m1.1.1.1.1.1" xref="Sx2.Ex2.m1.1.1.1.1.1.cmml"><mi id="Sx2.Ex2.m1.1.1.1.1.1.3" xref="Sx2.Ex2.m1.1.1.1.1.1.3.cmml">a</mi><mo id="Sx2.Ex2.m1.1.1.1.1.1.2" xref="Sx2.Ex2.m1.1.1.1.1.1.2.cmml">⁢</mo><mi id="Sx2.Ex2.m1.1.1.1.1.1.4" xref="Sx2.Ex2.m1.1.1.1.1.1.4.cmml">r</mi><mo id="Sx2.Ex2.m1.1.1.1.1.1.2a" xref="Sx2.Ex2.m1.1.1.1.1.1.2.cmml">⁢</mo><mi id="Sx2.Ex2.m1.1.1.1.1.1.5" xref="Sx2.Ex2.m1.1.1.1.1.1.5.cmml">g</mi><mo id="Sx2.Ex2.m1.1.1.1.1.1.2b" xref="Sx2.Ex2.m1.1.1.1.1.1.2.cmml">⁢</mo><mi id="Sx2.Ex2.m1.1.1.1.1.1.6" xref="Sx2.Ex2.m1.1.1.1.1.1.6.cmml">m</mi><mo id="Sx2.Ex2.m1.1.1.1.1.1.2c" xref="Sx2.Ex2.m1.1.1.1.1.1.2.cmml">⁢</mo><mi id="Sx2.Ex2.m1.1.1.1.1.1.7" xref="Sx2.Ex2.m1.1.1.1.1.1.7.cmml">a</mi><mo id="Sx2.Ex2.m1.1.1.1.1.1.2d" xref="Sx2.Ex2.m1.1.1.1.1.1.2.cmml">⁢</mo><msub id="Sx2.Ex2.m1.1.1.1.1.1.8" xref="Sx2.Ex2.m1.1.1.1.1.1.8.cmml"><mi id="Sx2.Ex2.m1.1.1.1.1.1.8.2" xref="Sx2.Ex2.m1.1.1.1.1.1.8.2.cmml">x</mi><mi id="Sx2.Ex2.m1.1.1.1.1.1.8.3" xref="Sx2.Ex2.m1.1.1.1.1.1.8.3.cmml">z</mi></msub><mo id="Sx2.Ex2.m1.1.1.1.1.1.2e" xref="Sx2.Ex2.m1.1.1.1.1.1.2.cmml">⁢</mo><mi id="Sx2.Ex2.m1.1.1.1.1.1.9" xref="Sx2.Ex2.m1.1.1.1.1.1.9.cmml">P</mi><mo id="Sx2.Ex2.m1.1.1.1.1.1.2f" xref="Sx2.Ex2.m1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="Sx2.Ex2.m1.1.1.1.1.1.1.1" xref="Sx2.Ex2.m1.1.1.1.1.1.1.1.1.cmml"><mo id="Sx2.Ex2.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="Sx2.Ex2.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="Sx2.Ex2.m1.1.1.1.1.1.1.1.1" xref="Sx2.Ex2.m1.1.1.1.1.1.1.1.1.cmml"><mi id="Sx2.Ex2.m1.1.1.1.1.1.1.1.1.2" xref="Sx2.Ex2.m1.1.1.1.1.1.1.1.1.2.cmml">z</mi><mo fence="false" id="Sx2.Ex2.m1.1.1.1.1.1.1.1.1.1" xref="Sx2.Ex2.m1.1.1.1.1.1.1.1.1.1.cmml">|</mo><mi id="Sx2.Ex2.m1.1.1.1.1.1.1.1.1.3" xref="Sx2.Ex2.m1.1.1.1.1.1.1.1.1.3.cmml">x</mi></mrow><mo id="Sx2.Ex2.m1.1.1.1.1.1.1.1.3" stretchy="false" xref="Sx2.Ex2.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="Sx2.Ex2.m1.1.1.1.2" lspace="0em" xref="Sx2.Ex2.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx2.Ex2.m1.1b"><apply id="Sx2.Ex2.m1.1.1.1.1.cmml" xref="Sx2.Ex2.m1.1.1.1"><eq id="Sx2.Ex2.m1.1.1.1.1.2.cmml" xref="Sx2.Ex2.m1.1.1.1.1.2"></eq><apply id="Sx2.Ex2.m1.1.1.1.1.3.cmml" xref="Sx2.Ex2.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="Sx2.Ex2.m1.1.1.1.1.3.1.cmml" xref="Sx2.Ex2.m1.1.1.1.1.3">superscript</csymbol><ci id="Sx2.Ex2.m1.1.1.1.1.3.2.cmml" xref="Sx2.Ex2.m1.1.1.1.1.3.2">𝑧</ci><times id="Sx2.Ex2.m1.1.1.1.1.3.3.cmml" xref="Sx2.Ex2.m1.1.1.1.1.3.3"></times></apply><apply id="Sx2.Ex2.m1.1.1.1.1.1.cmml" xref="Sx2.Ex2.m1.1.1.1.1.1"><times id="Sx2.Ex2.m1.1.1.1.1.1.2.cmml" xref="Sx2.Ex2.m1.1.1.1.1.1.2"></times><ci id="Sx2.Ex2.m1.1.1.1.1.1.3.cmml" xref="Sx2.Ex2.m1.1.1.1.1.1.3">𝑎</ci><ci id="Sx2.Ex2.m1.1.1.1.1.1.4.cmml" xref="Sx2.Ex2.m1.1.1.1.1.1.4">𝑟</ci><ci id="Sx2.Ex2.m1.1.1.1.1.1.5.cmml" xref="Sx2.Ex2.m1.1.1.1.1.1.5">𝑔</ci><ci id="Sx2.Ex2.m1.1.1.1.1.1.6.cmml" xref="Sx2.Ex2.m1.1.1.1.1.1.6">𝑚</ci><ci id="Sx2.Ex2.m1.1.1.1.1.1.7.cmml" xref="Sx2.Ex2.m1.1.1.1.1.1.7">𝑎</ci><apply id="Sx2.Ex2.m1.1.1.1.1.1.8.cmml" xref="Sx2.Ex2.m1.1.1.1.1.1.8"><csymbol cd="ambiguous" id="Sx2.Ex2.m1.1.1.1.1.1.8.1.cmml" xref="Sx2.Ex2.m1.1.1.1.1.1.8">subscript</csymbol><ci id="Sx2.Ex2.m1.1.1.1.1.1.8.2.cmml" xref="Sx2.Ex2.m1.1.1.1.1.1.8.2">𝑥</ci><ci id="Sx2.Ex2.m1.1.1.1.1.1.8.3.cmml" xref="Sx2.Ex2.m1.1.1.1.1.1.8.3">𝑧</ci></apply><ci id="Sx2.Ex2.m1.1.1.1.1.1.9.cmml" xref="Sx2.Ex2.m1.1.1.1.1.1.9">𝑃</ci><apply id="Sx2.Ex2.m1.1.1.1.1.1.1.1.1.cmml" xref="Sx2.Ex2.m1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="Sx2.Ex2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx2.Ex2.m1.1.1.1.1.1.1.1.1.1">conditional</csymbol><ci id="Sx2.Ex2.m1.1.1.1.1.1.1.1.1.2.cmml" xref="Sx2.Ex2.m1.1.1.1.1.1.1.1.1.2">𝑧</ci><ci id="Sx2.Ex2.m1.1.1.1.1.1.1.1.1.3.cmml" xref="Sx2.Ex2.m1.1.1.1.1.1.1.1.1.3">𝑥</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.Ex2.m1.1c">z^{*}=\\
arg\\
max_{z}P(z|x).</annotation><annotation encoding="application/x-llamapun" id="Sx2.Ex2.m1.1d">italic_z start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT = italic_a italic_r italic_g italic_m italic_a italic_x start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT italic_P ( italic_z | italic_x ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="Sx2.SSx1.p1.8">In this formulation, external knowledge sources <math alttext="z^{*}" class="ltx_Math" display="inline" id="Sx2.SSx1.p1.5.m1.1"><semantics id="Sx2.SSx1.p1.5.m1.1a"><msup id="Sx2.SSx1.p1.5.m1.1.1" xref="Sx2.SSx1.p1.5.m1.1.1.cmml"><mi id="Sx2.SSx1.p1.5.m1.1.1.2" xref="Sx2.SSx1.p1.5.m1.1.1.2.cmml">z</mi><mo id="Sx2.SSx1.p1.5.m1.1.1.3" xref="Sx2.SSx1.p1.5.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="Sx2.SSx1.p1.5.m1.1b"><apply id="Sx2.SSx1.p1.5.m1.1.1.cmml" xref="Sx2.SSx1.p1.5.m1.1.1"><csymbol cd="ambiguous" id="Sx2.SSx1.p1.5.m1.1.1.1.cmml" xref="Sx2.SSx1.p1.5.m1.1.1">superscript</csymbol><ci id="Sx2.SSx1.p1.5.m1.1.1.2.cmml" xref="Sx2.SSx1.p1.5.m1.1.1.2">𝑧</ci><times id="Sx2.SSx1.p1.5.m1.1.1.3.cmml" xref="Sx2.SSx1.p1.5.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx1.p1.5.m1.1c">z^{*}</annotation><annotation encoding="application/x-llamapun" id="Sx2.SSx1.p1.5.m1.1d">italic_z start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math> are not homogeneous but include structured databases, web-based knowledge, and API-driven resources, each contributing uniquely to the inference process. The model’s two-step approach first retrieves the most relevant <math alttext="z^{*}" class="ltx_Math" display="inline" id="Sx2.SSx1.p1.6.m2.1"><semantics id="Sx2.SSx1.p1.6.m2.1a"><msup id="Sx2.SSx1.p1.6.m2.1.1" xref="Sx2.SSx1.p1.6.m2.1.1.cmml"><mi id="Sx2.SSx1.p1.6.m2.1.1.2" xref="Sx2.SSx1.p1.6.m2.1.1.2.cmml">z</mi><mo id="Sx2.SSx1.p1.6.m2.1.1.3" xref="Sx2.SSx1.p1.6.m2.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="Sx2.SSx1.p1.6.m2.1b"><apply id="Sx2.SSx1.p1.6.m2.1.1.cmml" xref="Sx2.SSx1.p1.6.m2.1.1"><csymbol cd="ambiguous" id="Sx2.SSx1.p1.6.m2.1.1.1.cmml" xref="Sx2.SSx1.p1.6.m2.1.1">superscript</csymbol><ci id="Sx2.SSx1.p1.6.m2.1.1.2.cmml" xref="Sx2.SSx1.p1.6.m2.1.1.2">𝑧</ci><times id="Sx2.SSx1.p1.6.m2.1.1.3.cmml" xref="Sx2.SSx1.p1.6.m2.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx1.p1.6.m2.1c">z^{*}</annotation><annotation encoding="application/x-llamapun" id="Sx2.SSx1.p1.6.m2.1d">italic_z start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math> given the input <math alttext="x" class="ltx_Math" display="inline" id="Sx2.SSx1.p1.7.m3.1"><semantics id="Sx2.SSx1.p1.7.m3.1a"><mi id="Sx2.SSx1.p1.7.m3.1.1" xref="Sx2.SSx1.p1.7.m3.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx1.p1.7.m3.1b"><ci id="Sx2.SSx1.p1.7.m3.1.1.cmml" xref="Sx2.SSx1.p1.7.m3.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx1.p1.7.m3.1c">x</annotation><annotation encoding="application/x-llamapun" id="Sx2.SSx1.p1.7.m3.1d">italic_x</annotation></semantics></math>, and then generates <math alttext="y^{*}" class="ltx_Math" display="inline" id="Sx2.SSx1.p1.8.m4.1"><semantics id="Sx2.SSx1.p1.8.m4.1a"><msup id="Sx2.SSx1.p1.8.m4.1.1" xref="Sx2.SSx1.p1.8.m4.1.1.cmml"><mi id="Sx2.SSx1.p1.8.m4.1.1.2" xref="Sx2.SSx1.p1.8.m4.1.1.2.cmml">y</mi><mo id="Sx2.SSx1.p1.8.m4.1.1.3" xref="Sx2.SSx1.p1.8.m4.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="Sx2.SSx1.p1.8.m4.1b"><apply id="Sx2.SSx1.p1.8.m4.1.1.cmml" xref="Sx2.SSx1.p1.8.m4.1.1"><csymbol cd="ambiguous" id="Sx2.SSx1.p1.8.m4.1.1.1.cmml" xref="Sx2.SSx1.p1.8.m4.1.1">superscript</csymbol><ci id="Sx2.SSx1.p1.8.m4.1.1.2.cmml" xref="Sx2.SSx1.p1.8.m4.1.1.2">𝑦</ci><times id="Sx2.SSx1.p1.8.m4.1.1.3.cmml" xref="Sx2.SSx1.p1.8.m4.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx1.p1.8.m4.1c">y^{*}</annotation><annotation encoding="application/x-llamapun" id="Sx2.SSx1.p1.8.m4.1d">italic_y start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math> by integrating this external knowledge with the LLM’s internal knowledge. This framework highlights the dynamic fusion of internal and external knowledge, crucially allowing for adaptive reasoning that leverages the strengths of both sources, thereby optimizing relevance and accuracy in generating responses for knowledge-intensive tasks.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx2.SSx2">
<h3 class="ltx_title ltx_title_subsection">Dataset and Metrics</h3>
<div class="ltx_para" id="Sx2.SSx2.p1">
<p class="ltx_p" id="Sx2.SSx2.p1.1">The CRAG dataset, central to the KDD Cup 2024, is particularly well-suited for real-world retrieval-based Question Answering (QA) tasks due to its diverse query types and document formats <cite class="ltx_cite ltx_citemacro_citep">(Yang et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib36" title="">2024</a>; Kwiatkowski et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib19" title="">2019</a>)</cite>. Reflecting practical scenarios where correct answers may not be entirely present within external knowledge sources, the dataset challenges models to engage in sophisticated integration of both internal and external knowledge for accurate inference. Additionally, certain queries may lack direct answers in either knowledge domain <cite class="ltx_cite ltx_citemacro_citep">(Bajaj et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib2" title="">2018</a>)</cite>, necessitating deeper collaborative reasoning between different knowledge types—making the dataset an ideal benchmark for evaluating the effectiveness of Retrieval-Augmented Generation (RAG) systems.</p>
</div>
<div class="ltx_para" id="Sx2.SSx2.p2">
<p class="ltx_p" id="Sx2.SSx2.p2.1">Our work significantly enhances the utility of this dataset, which we have named RM3QA, standing for ”A Real-time Multi-domain, Multi-format, Multi-source Question Answer Dataset for RAG.” This improved dataset facilitates more accurate integration of external knowledge, laying a stronger foundation for LLM inference. RM3QA is designed to handle real-time questions across multiple domains, utilizing both structured and unstructured knowledge sources, and integrating information from various origins such as web pages and a mock API. Alongside the public release of this dataset in markdown (md) format, our RAG system is evaluated on four key metrics: accuracy (Acc.), hallucination (Halluc.), missing (Miss.), and overall score. These metrics provide a comprehensive assessment of the system’s ability to balance retrieval accuracy, avoid incorrect inferences, and maintain relevance in complex, knowledge-intensive tasks. The evaluation includes both string matching and GPT-based assessments <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib26" title="">2022</a>)</cite>. For detailed explanations of these metrics and dataset processing, please refer to Appendix B.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Sx3">
<h2 class="ltx_title ltx_title_section">Benchmark Evaluation of RAG</h2>
<figure class="ltx_table" id="Sx3.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span> Comparative analysis of RAG performance across different external knowledge configurations. </figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="Sx3.T1.1" style="width:404.7pt;height:213.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(31.4pt,-16.5pt) scale(1.18385144034266,1.18385144034266) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Sx3.T1.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Sx3.T1.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="Sx3.T1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="Sx3.T1.1.1.1.1.1.1">External Knowledge</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="Sx3.T1.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="Sx3.T1.1.1.1.1.2.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx3.T1.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="Sx3.T1.1.1.1.1.3.1">Acc.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx3.T1.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="Sx3.T1.1.1.1.1.4.1">Halluc.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx3.T1.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="Sx3.T1.1.1.1.1.5.1">Miss.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx3.T1.1.1.1.1.6"><span class="ltx_text ltx_font_bold" id="Sx3.T1.1.1.1.1.6.1">Score</span></th>
</tr>
<tr class="ltx_tr" id="Sx3.T1.1.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="Sx3.T1.1.1.2.2.1">None</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="Sx3.T1.1.1.2.2.2">LLM-Only</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Sx3.T1.1.1.2.2.3">15.61%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Sx3.T1.1.1.2.2.4">20.42%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Sx3.T1.1.1.2.2.5">63.97%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Sx3.T1.1.1.2.2.6">-4.81%</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Sx3.T1.1.1.3.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="Sx3.T1.1.1.3.1.1" rowspan="2"><span class="ltx_text" id="Sx3.T1.1.1.3.1.1.1">5 Web</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Sx3.T1.1.1.3.1.2">CRAG</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx3.T1.1.1.3.1.3">7.51%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx3.T1.1.1.3.1.4">8.68%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx3.T1.1.1.3.1.5">83.81%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx3.T1.1.1.3.1.6">-1.16%</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.1.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Sx3.T1.1.1.4.2.1">RAG-X</th>
<td class="ltx_td ltx_align_center" id="Sx3.T1.1.1.4.2.2">24.87%</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.1.1.4.2.3">22.10%</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.1.1.4.2.4">53.03%</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.1.1.4.2.5">2.77%</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.1.1.5.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="Sx3.T1.1.1.5.3.1" rowspan="2"><span class="ltx_text" id="Sx3.T1.1.1.5.3.1.1">Mock API</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Sx3.T1.1.1.5.3.2">CRAG</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx3.T1.1.1.5.3.3">8.53%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx3.T1.1.1.5.3.4"><span class="ltx_text ltx_font_bold" id="Sx3.T1.1.1.5.3.4.1">1.60%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx3.T1.1.1.5.3.5">89.86%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx3.T1.1.1.5.3.6">6.93%</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.1.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Sx3.T1.1.1.6.4.1">RAG-X</th>
<td class="ltx_td ltx_align_center" id="Sx3.T1.1.1.6.4.2">29.03%</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.1.1.6.4.3">10.86%</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.1.1.6.4.4">60.10%</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.1.1.6.4.5"><span class="ltx_text ltx_font_bold" id="Sx3.T1.1.1.6.4.5.1">18.16%</span></td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.1.1.7.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="Sx3.T1.1.1.7.5.1" rowspan="2"><span class="ltx_text" id="Sx3.T1.1.1.7.5.1.1">5 Web + Mock API</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Sx3.T1.1.1.7.5.2">CRAG</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx3.T1.1.1.7.5.3">15.10%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx3.T1.1.1.7.5.4">7.95%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx3.T1.1.1.7.5.5">76.95%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx3.T1.1.1.7.5.6">7.15%</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.1.1.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Sx3.T1.1.1.8.6.1">RAG-X</th>
<td class="ltx_td ltx_align_center" id="Sx3.T1.1.1.8.6.2">32.53%</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.1.1.8.6.3">23.12%</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.1.1.8.6.4">44.35%</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.1.1.8.6.5">9.41%</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.1.1.9.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="Sx3.T1.1.1.9.7.1" rowspan="2"><span class="ltx_text" id="Sx3.T1.1.1.9.7.1.1">50 Web + Mock API</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Sx3.T1.1.1.9.7.2">CRAG</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx3.T1.1.1.9.7.3">14.22%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx3.T1.1.1.9.7.4">8.90%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx3.T1.1.1.9.7.5">76.88%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx3.T1.1.1.9.7.6">5.32%</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.1.1.10.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="Sx3.T1.1.1.10.8.1">RAG-X</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx3.T1.1.1.10.8.2"><span class="ltx_text ltx_font_bold" id="Sx3.T1.1.1.10.8.2.1">35.16%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx3.T1.1.1.10.8.3">26.26%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx3.T1.1.1.10.8.4">38.58%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx3.T1.1.1.10.8.5">8.90%</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="Sx3.p1">
<p class="ltx_p" id="Sx3.p1.1">The CRAG benchmark <cite class="ltx_cite ltx_citemacro_citep">(Yang et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib36" title="">2024</a>)</cite> currently lacks comprehensive evaluation results that researchers can reference. To address this gap, we propose a novel RAG benchmark and evaluate the performance of our RAG implementation across a series of benchmark tests.</p>
</div>
<div class="ltx_para" id="Sx3.p2">
<p class="ltx_p" id="Sx3.p2.1">Our RAG framework, RAG-X, depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#S0.F1" title="Figure 1 ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">1</span></a>, employs an LLM Agent-based Router to intelligently select relevant knowledge sources from a curated mix of structured and unstructured data, including web pages and a mock API. Our retrieval process avoids web searches, focusing exclusively on the provided information sources. The retrieval process is divided into three steps: broad retrieval narrows down vast external knowledge, focused retrieval uses sparse, dense, or hybrid methods to identify key information, and rank refinement ensures the final output is accurate and prioritized. After retrieval, we enhance the LLM’s reasoning with noise chunks, Chain of Thought (CoT), and In-Context Learning (ICL), leading to more relevant responses.</p>
</div>
<div class="ltx_para" id="Sx3.p3">
<p class="ltx_p" id="Sx3.p3.1">The experimental results in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx3.T1" title="Table 1 ‣ Benchmark Evaluation of RAG ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">1</span></a> show that RAG-X consistently outperforms both the CRAG baseline and the LLM-only model. Structured knowledge from the mock API notably enhances accuracy and reduces hallucinations compared to unstructured web sources. However, increasing knowledge inputs, like combining 50 web pages with mock API data, improves accuracy but slightly raises hallucination rates. This highlights the need to carefully balance external knowledge sources to optimize overall performance <cite class="ltx_cite ltx_citemacro_citep">(Wang et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib33" title="">2023b</a>; Yu et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib38" title="">2023</a>; Shi et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib28" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="Sx3.p4">
<p class="ltx_p" id="Sx3.p4.1">To further support the research community, we are making both our code and dataset fully open-source. This commitment ensures that our work is not only robust and reproducible but also applicable to real-world scenarios. For detailed information on our implementation and dataset, please refer to Appendix B.</p>
</div>
</section>
<section class="ltx_section" id="Sx4">
<h2 class="ltx_title ltx_title_section">Extensive Empirical Studies</h2>
<div class="ltx_para" id="Sx4.p1">
<p class="ltx_p" id="Sx4.p1.1">In this section, we conduct an in-depth analysis of the Retrieval-Augmented Generation (RAG) process. Our empirical studies rigorously evaluate each critical component of the RAG pipeline—namely, knowledge source selection, retrieval, organization, and reasoning. Through this systematic examination, the insights gained from these analyses are intended to guide future optimizations in RAG methodologies. In the presentation of our experimental results, we have <span class="ltx_text ltx_font_bold" id="Sx4.p1.1.1">bolded</span> the optimal values to highlight the best-performing configurations. Specific details of the experimental setup are provided in Appendix B.</p>
</div>
<section class="ltx_subsection" id="Sx4.SSx1">
<h3 class="ltx_title ltx_title_subsection">Advancing Knowledge Source Selection</h3>
<div class="ltx_para" id="Sx4.SSx1.p1">
<p class="ltx_p" id="Sx4.SSx1.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx4.T2" title="Table 2 ‣ Advancing Knowledge Source Selection ‣ Extensive Empirical Studies ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">2</span></a> offers a comparative analysis of performance across different knowledge sources, including internal, external, and combined scenarios. Integrating structured knowledge from the mock API significantly improves accuracy and reduces hallucination rates. In contrast, aggregating all sources can lead to conflicts and suboptimal results <cite class="ltx_cite ltx_citemacro_citep">(Liu et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib24" title="">2023b</a>)</cite>. Predefined fallback strategies, where the LLM prioritizes internal knowledge before querying external sources, boost accuracy but also raise hallucination rates, especially with multiple sources. While the agent-based dynamic selection strategy <cite class="ltx_cite ltx_citemacro_citep">(Li, Nie, and Liang <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib22" title="">2023</a>)</cite> adds flexibility, it doesn’t consistently outperform predefined methods, underscoring the challenges of real-time decision-making. These findings highlight the importance of structured external knowledge for performance enhancement and the need for advanced selection mechanisms to balance knowledge coverage and minimize hallucinations in retrieval processes <cite class="ltx_cite ltx_citemacro_citep">(Rackauckas <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib27" title="">2024</a>; Yoran et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib37" title="">2024</a>)</cite>.</p>
</div>
<figure class="ltx_table" id="Sx4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Performance comparison across different knowledge source selection methods.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="Sx4.T2.1" style="width:404.7pt;height:237.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(18.2pt,-10.7pt) scale(1.09876764458054,1.09876764458054) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Sx4.T2.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Sx4.T2.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="Sx4.T2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="Sx4.T2.1.1.1.1.1.1">Phase</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="Sx4.T2.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="Sx4.T2.1.1.1.1.2.1">Experiment Setting</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Sx4.T2.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="Sx4.T2.1.1.1.1.3.1">Acc.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Sx4.T2.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="Sx4.T2.1.1.1.1.4.1">Halluc.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Sx4.T2.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="Sx4.T2.1.1.1.1.5.1">Miss.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Sx4.T2.1.1.1.1.6"><span class="ltx_text ltx_font_bold" id="Sx4.T2.1.1.1.1.6.1">Score</span></td>
</tr>
<tr class="ltx_tr" id="Sx4.T2.1.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Sx4.T2.1.1.2.2.1" rowspan="7"><span class="ltx_text" id="Sx4.T2.1.1.2.2.1.1">Manual Setting</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Sx4.T2.1.1.2.2.2">LLM</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T2.1.1.2.2.3">13.86%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T2.1.1.2.2.4">16.70%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T2.1.1.2.2.5">69.44%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T2.1.1.2.2.6">-2.84%</td>
</tr>
<tr class="ltx_tr" id="Sx4.T2.1.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Sx4.T2.1.1.3.3.1">WEB</th>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.3.3.2">16.63%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.3.3.3">12.40%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.3.3.4">70.97%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.3.3.5">4.23%</td>
</tr>
<tr class="ltx_tr" id="Sx4.T2.1.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Sx4.T2.1.1.4.4.1">Mock API</th>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.4.4.2">29.18%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.4.4.3">10.14%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.4.4.4">55.26%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.4.4.5"><span class="ltx_text ltx_font_bold" id="Sx4.T2.1.1.4.4.5.1">19.04%</span></td>
</tr>
<tr class="ltx_tr" id="Sx4.T2.1.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Sx4.T2.1.1.5.5.1">ALL</th>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.5.5.2">17.07%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.5.5.3"><span class="ltx_text ltx_font_bold" id="Sx4.T2.1.1.5.5.3.1">8.83%</span></td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.5.5.4">74.11%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.5.5.5">8.24%</td>
</tr>
<tr class="ltx_tr" id="Sx4.T2.1.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Sx4.T2.1.1.6.6.1">LLM+WEB</th>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.6.6.2">13.20%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.6.6.3">10.50%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.6.6.4">76.29%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.6.6.5">2.70%</td>
</tr>
<tr class="ltx_tr" id="Sx4.T2.1.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Sx4.T2.1.1.7.7.1">LLM+Mock API</th>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.7.7.2">21.88%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.7.7.3">10.21%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.7.7.4">67.91%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.7.7.5">11.67%</td>
</tr>
<tr class="ltx_tr" id="Sx4.T2.1.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Sx4.T2.1.1.8.8.1">LLM+ALL</th>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.8.8.2">17.72%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.8.8.3"><span class="ltx_text ltx_font_bold" id="Sx4.T2.1.1.8.8.3.1">8.83%</span></td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.8.8.4">73.45%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.8.8.5">8.90%</td>
</tr>
<tr class="ltx_tr" id="Sx4.T2.1.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Sx4.T2.1.1.9.9.1" rowspan="3"><span class="ltx_text" id="Sx4.T2.1.1.9.9.1.1">Predefined Path</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Sx4.T2.1.1.9.9.2">LLM+WEB</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T2.1.1.9.9.3">24.00%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T2.1.1.9.9.4">22.61%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T2.1.1.9.9.5">53.39%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T2.1.1.9.9.6">1.39%</td>
</tr>
<tr class="ltx_tr" id="Sx4.T2.1.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Sx4.T2.1.1.10.10.1">LLM+Mock API</th>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.10.10.2"><span class="ltx_text ltx_font_bold" id="Sx4.T2.1.1.10.10.2.1">36.62%</span></td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.10.10.3">25.97%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.10.10.4">37.42%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.10.10.5">10.65%</td>
</tr>
<tr class="ltx_tr" id="Sx4.T2.1.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Sx4.T2.1.1.11.11.1">LLM+ALL</th>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.11.11.2"><span class="ltx_text ltx_font_bold" id="Sx4.T2.1.1.11.11.2.1">36.62%</span></td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.11.11.3">39.82%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.11.11.4">23.56%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T2.1.1.11.11.5">-3.20%</td>
</tr>
<tr class="ltx_tr" id="Sx4.T2.1.1.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="Sx4.T2.1.1.12.12.1">Agent-Based Path</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="Sx4.T2.1.1.12.12.2">Agent</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Sx4.T2.1.1.12.12.3">25.31%</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Sx4.T2.1.1.12.12.4">21.59%</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Sx4.T2.1.1.12.12.5">53.10%</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Sx4.T2.1.1.12.12.6">3.72%</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_table" id="Sx4.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison of performance with and without broad retrieval.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="Sx4.T3.1" style="width:404.7pt;height:58.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(14.2pt,-2.0pt) scale(1.07572806193864,1.07572806193864) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Sx4.T3.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Sx4.T3.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="Sx4.T3.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="Sx4.T3.1.1.1.1.1.1">Setting</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx4.T3.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="Sx4.T3.1.1.1.1.2.1">Acc.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx4.T3.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="Sx4.T3.1.1.1.1.3.1">Halluc.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx4.T3.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="Sx4.T3.1.1.1.1.4.1">Miss.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx4.T3.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="Sx4.T3.1.1.1.1.5.1">Score</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx4.T3.1.1.1.1.6"><span class="ltx_text ltx_font_bold" id="Sx4.T3.1.1.1.1.6.1">Processing Time</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Sx4.T3.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Sx4.T3.1.1.2.1.1">Broad Retrieval (w/)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T3.1.1.2.1.2"><span class="ltx_text ltx_font_bold" id="Sx4.T3.1.1.2.1.2.1">31.71%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T3.1.1.2.1.3"><span class="ltx_text ltx_font_bold" id="Sx4.T3.1.1.2.1.3.1">41.50%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T3.1.1.2.1.4">26.79%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T3.1.1.2.1.5"><span class="ltx_text ltx_font_bold" id="Sx4.T3.1.1.2.1.5.1">-9.79%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T3.1.1.2.1.6"><span class="ltx_text ltx_font_bold" id="Sx4.T3.1.1.2.1.6.1">8166.59s</span></td>
</tr>
<tr class="ltx_tr" id="Sx4.T3.1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="Sx4.T3.1.1.3.2.1">Broad Retrieval (w/o)</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx4.T3.1.1.3.2.2">28.94%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx4.T3.1.1.3.2.3">42.65%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx4.T3.1.1.3.2.4">28.42%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx4.T3.1.1.3.2.5">-13.71%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx4.T3.1.1.3.2.6">93459.87s</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="Sx4.SSx2">
<h3 class="ltx_title ltx_title_subsection">Enhancing Retrieval for Knowledge Extraction </h3>
<div class="ltx_para" id="Sx4.SSx2.p1">
<p class="ltx_p" id="Sx4.SSx2.p1.1">This subsection examines strategies for enhancing knowledge retrieval and utilization in a RAG system, focusing on broad and focused retrieval methods and refining reranking to improve overall performance.</p>
</div>
<section class="ltx_subsubsection" id="Sx4.SSx2.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Advancing Efficiency with Broad Retrieval.</h4>
<div class="ltx_para" id="Sx4.SSx2.SSSx1.p1">
<p class="ltx_p" id="Sx4.SSx2.SSSx1.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx4.T3" title="Table 3 ‣ Advancing Knowledge Source Selection ‣ Extensive Empirical Studies ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">3</span></a> compares the performance of the RAG system with and without a broad retrieval step, where each query is supplemented with 50 web pages—far more than the 5 used in other experiments. The results demonstrate that this broad retrieval phase significantly enhances system efficiency, notably reducing processing time. Sparse retrieval methods like BM25 <cite class="ltx_cite ltx_citemacro_citep">(Izacard et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib16" title="">2022</a>; Cheng et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib7" title="">2021</a>)</cite> filter out irrelevant data, allowing dense retrieval to focus on a more targeted subset of external knowledge <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib41" title="">2024</a>)</cite>. These findings underscore the effectiveness of a two-tiered approach, where sparse retrieval narrows the search space, enabling dense retrieval to operate with greater precision and speed. This strategy is especially beneficial in scenarios involving extensive external knowledge, optimizing both processing time and relevance, and offering a more efficient design for RAG systems <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib17" title="">2024</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="Sx4.SSx2.SSSx2">
<h4 class="ltx_title ltx_title_subsubsection">Evaluating Focused Retrieval Methodologies in RAG.</h4>
<div class="ltx_para" id="Sx4.SSx2.SSSx2.p1">
<p class="ltx_p" id="Sx4.SSx2.SSSx2.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx4.F2" title="Figure 2 ‣ Evaluating Focused Retrieval Methodologies in RAG. ‣ Enhancing Retrieval for Knowledge Extraction ‣ Extensive Empirical Studies ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates the trade-offs between dense and sparse retrieval configurations in a RAG system. While higher dense retrieval ratios boost accuracy due to richer semantic understanding, they also elevate hallucination rates, suggesting a risk of contextual misalignment. Conversely, increasing sparse retrieval may lower accuracy and fails to consistently reduce hallucinations, likely because of its dependence on surface-level keyword matching. These findings underscore the need for a dynamic, hybrid strategy that balances the strengths and weaknesses of both approaches, optimizing performance by mitigating the inherent limitations of each method <cite class="ltx_cite ltx_citemacro_citep">(Gu et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib14" title="">2018</a>; Zhang et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib40" title="">2018</a>; Cheng et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib6" title="">2022</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="Sx4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="560" id="Sx4.F2.g1" src="x2.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Performance of varying retrieval methods in RAG.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="Sx4.SSx2.SSSx3">
<h4 class="ltx_title ltx_title_subsubsection">Reranking for Enhanced Knowledge Utilization.</h4>
<div class="ltx_para" id="Sx4.SSx2.SSSx3.p1">
<p class="ltx_p" id="Sx4.SSx2.SSSx3.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx4.T4" title="Table 4 ‣ Reranking for Enhanced Knowledge Utilization. ‣ Enhancing Retrieval for Knowledge Extraction ‣ Extensive Empirical Studies ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">4</span></a> summarizes the impact of various reranker configurations on RAG system performance, focusing on different retrieval chunk sizes. The results show that as the number of retrieved chunks increases, hallucination rates rise, while accuracy remains stable, suggesting that the reranker performs better with smaller, more focused sets. Bypassing retrieval and directly passing all chunks to the reranker, as in configuration (3, All), results in similar performance to configurations with a large number of chunks, indicating that increasing chunk size beyond a certain point does not enhance effectiveness. These findings emphasize the need to balance the number of retrieved chunks and optimize the reranker’s selection process <cite class="ltx_cite ltx_citemacro_citep">(Ma et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib25" title="">2023</a>; Vu et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib31" title="">2023</a>; Yu et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib39" title="">2024</a>)</cite>, especially in large-scale retrieval scenarios.</p>
</div>
<figure class="ltx_table" id="Sx4.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Performance of reranker configurations in RAG.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Sx4.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Sx4.T4.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx4.T4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="Sx4.T4.1.1.1.1.1">Config.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx4.T4.1.1.1.2"><span class="ltx_text ltx_font_bold" id="Sx4.T4.1.1.1.2.1">Acc.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx4.T4.1.1.1.3"><span class="ltx_text ltx_font_bold" id="Sx4.T4.1.1.1.3.1">Halluc.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx4.T4.1.1.1.4"><span class="ltx_text ltx_font_bold" id="Sx4.T4.1.1.1.4.1">Miss.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx4.T4.1.1.1.5"><span class="ltx_text ltx_font_bold" id="Sx4.T4.1.1.1.5.1">Score</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Sx4.T4.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T4.1.2.1.1">(3, 3)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T4.1.2.1.2">24.07%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T4.1.2.1.3"><span class="ltx_text ltx_font_bold" id="Sx4.T4.1.2.1.3.1">43.18%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T4.1.2.1.4">32.75%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T4.1.2.1.5"><span class="ltx_text ltx_font_bold" id="Sx4.T4.1.2.1.5.1">-19.11%</span></td>
</tr>
<tr class="ltx_tr" id="Sx4.T4.1.3.2">
<td class="ltx_td ltx_align_center" id="Sx4.T4.1.3.2.1">(3, 5)</td>
<td class="ltx_td ltx_align_center" id="Sx4.T4.1.3.2.2">24.87%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T4.1.3.2.3">44.06%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T4.1.3.2.4">31.07%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T4.1.3.2.5">-19.18%</td>
</tr>
<tr class="ltx_tr" id="Sx4.T4.1.4.3">
<td class="ltx_td ltx_align_center" id="Sx4.T4.1.4.3.1">(3, 10)</td>
<td class="ltx_td ltx_align_center" id="Sx4.T4.1.4.3.2">24.28%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T4.1.4.3.3">45.88%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T4.1.4.3.4">29.83%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T4.1.4.3.5">-21.59%</td>
</tr>
<tr class="ltx_tr" id="Sx4.T4.1.5.4">
<td class="ltx_td ltx_align_center" id="Sx4.T4.1.5.4.1">(3, 20)</td>
<td class="ltx_td ltx_align_center" id="Sx4.T4.1.5.4.2">24.80%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T4.1.5.4.3">46.32%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T4.1.5.4.4">28.88%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T4.1.5.4.5">-21.52%</td>
</tr>
<tr class="ltx_tr" id="Sx4.T4.1.6.5">
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx4.T4.1.6.5.1">(3, All)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx4.T4.1.6.5.2"><span class="ltx_text ltx_font_bold" id="Sx4.T4.1.6.5.2.1">24.95%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx4.T4.1.6.5.3">46.32%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx4.T4.1.6.5.4">28.74%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx4.T4.1.6.5.5">-21.37%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_text ltx_font_italic" id="Sx4.T4.2.1">Note:</span> (3, X) denotes retrieval of X chunks, with (3, ALL) indicating all chunks passed directly.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="Sx4.SSx3">
<h3 class="ltx_title ltx_title_subsection">Evaluating Knowledge Reasoning on RAG Performance</h3>
<div class="ltx_para" id="Sx4.SSx3.p1">
<p class="ltx_p" id="Sx4.SSx3.p1.1">This section evaluates the impact of various knowledge reasoning techniques on RAG system performance, including Chain of Thought (CoT), few-shot learning, query positioning within prompts, and the introduction of noise chunks.</p>
</div>
<section class="ltx_subsubsection" id="Sx4.SSx3.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Impact of Chain of Thought (CoT) .</h4>
<div class="ltx_para" id="Sx4.SSx3.SSSx1.p1">
<p class="ltx_p" id="Sx4.SSx3.SSSx1.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx4.F3" title="Figure 3 ‣ Impact of Query Position in Prompt. ‣ Evaluating Knowledge Reasoning on RAG Performance ‣ Extensive Empirical Studies ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the context-dependent impact of incorporating Chain of Thought (CoT) into RAG systems. While CoT aims to enhance logical reasoning, the results indicate that it does not consistently improve performance <cite class="ltx_cite ltx_citemacro_citep">(Wei et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib34" title="">2023</a>; Zhou et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib42" title="">2023</a>)</cite> and can sometimes reduce accuracy, especially when dealing with multiple conflicting knowledge sources. This suggests that CoT may complicate the integration process, leading to higher missing rates and decreased effectiveness. Therefore, CoT should be carefully applied, proving beneficial in straightforward scenarios but potentially counterproductive in more complex contexts.</p>
</div>
</section>
<section class="ltx_subsubsection" id="Sx4.SSx3.SSSx2">
<h4 class="ltx_title ltx_title_subsubsection">Analysis of Few-Shot Learning. </h4>
<div class="ltx_para" id="Sx4.SSx3.SSSx2.p1">
<p class="ltx_p" id="Sx4.SSx3.SSSx2.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx4.T5" title="Table 5 ‣ Analysis of Few-Shot Learning. ‣ Evaluating Knowledge Reasoning on RAG Performance ‣ Extensive Empirical Studies ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">5</span></a> presents the impact of few-shot learning  <cite class="ltx_cite ltx_citemacro_citep">(Dai et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib9" title="">2022</a>; Dong et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib11" title="">2024</a>)</cite> on RAG systems, particularly in identifying false premises across various domains. The results show that the model performs best under 0-shot conditions. However, as more examples are introduced, performance on these questions declines due to overfitting and noise. Despite this, overall accuracy improves with few-shot examples, which provide task-specific guidance. Cross-domain examples also enhance generalization and reduce hallucination rates, demonstrating the value of diverse examples in broadening the model’s adaptability across different query types.</p>
</div>
<figure class="ltx_table" id="Sx4.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Impact of few-shot learning on LLM reasoning. </figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="Sx4.T5.1" style="width:252.9pt;height:165.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-11.3pt,7.4pt) scale(0.917758866140323,0.917758866140323) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Sx4.T5.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Sx4.T5.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="Sx4.T5.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.1.1.1.1">Category</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" id="Sx4.T5.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.1.1.2.1">N</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Sx4.T5.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.1.1.3.1">Acc.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Sx4.T5.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.1.1.4.1">Halluc.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Sx4.T5.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.1.1.5.1">Miss.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Sx4.T5.1.1.1.1.6"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.1.1.6.1">Score</span></td>
</tr>
<tr class="ltx_tr" id="Sx4.T5.1.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Sx4.T5.1.1.2.2.1" rowspan="4"><span class="ltx_text" id="Sx4.T5.1.1.2.2.1.1">Overall</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="Sx4.T5.1.1.2.2.2">0</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T5.1.1.2.2.3">13.20%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T5.1.1.2.2.4"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.2.2.4.1">10.50%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T5.1.1.2.2.5">76.29%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T5.1.1.2.2.6">2.70%</td>
</tr>
<tr class="ltx_tr" id="Sx4.T5.1.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="Sx4.T5.1.1.3.3.1">1</th>
<td class="ltx_td ltx_align_center" id="Sx4.T5.1.1.3.3.2">16.05%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T5.1.1.3.3.3">12.62%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T5.1.1.3.3.4">71.33%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T5.1.1.3.3.5"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.3.3.5.1">3.43%</span></td>
</tr>
<tr class="ltx_tr" id="Sx4.T5.1.1.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="Sx4.T5.1.1.4.4.1">2</th>
<td class="ltx_td ltx_align_center" id="Sx4.T5.1.1.4.4.2"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.4.4.2.1">16.12%</span></td>
<td class="ltx_td ltx_align_center" id="Sx4.T5.1.1.4.4.3">12.98%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T5.1.1.4.4.4">70.90%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T5.1.1.4.4.5">3.14%</td>
</tr>
<tr class="ltx_tr" id="Sx4.T5.1.1.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="Sx4.T5.1.1.5.5.1">3</th>
<td class="ltx_td ltx_align_center" id="Sx4.T5.1.1.5.5.2">15.17%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T5.1.1.5.5.3">12.69%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T5.1.1.5.5.4">72.14%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T5.1.1.5.5.5">2.48%</td>
</tr>
<tr class="ltx_tr" id="Sx4.T5.1.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Sx4.T5.1.1.6.6.1" rowspan="4"><span class="ltx_text" id="Sx4.T5.1.1.6.6.1.1">False Premise</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="Sx4.T5.1.1.6.6.2">0</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T5.1.1.6.6.3"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.6.6.3.1">25.00%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T5.1.1.6.6.4"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.6.6.4.1">5.77%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T5.1.1.6.6.5">69.23%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T5.1.1.6.6.6"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.6.6.6.1">19.23%</span></td>
</tr>
<tr class="ltx_tr" id="Sx4.T5.1.1.7.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="Sx4.T5.1.1.7.7.1">1</th>
<td class="ltx_td ltx_align_center" id="Sx4.T5.1.1.7.7.2">16.03%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T5.1.1.7.7.3">14.10%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T5.1.1.7.7.4">69.87%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T5.1.1.7.7.5">1.93%</td>
</tr>
<tr class="ltx_tr" id="Sx4.T5.1.1.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="Sx4.T5.1.1.8.8.1">2</th>
<td class="ltx_td ltx_align_center" id="Sx4.T5.1.1.8.8.2">16.57%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T5.1.1.8.8.3">13.46%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T5.1.1.8.8.4">69.87%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T5.1.1.8.8.5">3.11%</td>
</tr>
<tr class="ltx_tr" id="Sx4.T5.1.1.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="Sx4.T5.1.1.9.9.1">3</th>
<td class="ltx_td ltx_align_center" id="Sx4.T5.1.1.9.9.2">17.31%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T5.1.1.9.9.3">12.82%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T5.1.1.9.9.4">69.87%</td>
<td class="ltx_td ltx_align_center" id="Sx4.T5.1.1.9.9.5">4.49%</td>
</tr>
<tr class="ltx_tr" id="Sx4.T5.1.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="Sx4.T5.1.1.10.10.1"><span class="ltx_text" id="Sx4.T5.1.1.10.10.1.1">Cross-Domain</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="Sx4.T5.1.1.10.10.2">2</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Sx4.T5.1.1.10.10.3">18.02%</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Sx4.T5.1.1.10.10.4">11.23%</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Sx4.T5.1.1.10.10.5">70.75%</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Sx4.T5.1.1.10.10.6">6.78%</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsubsection" id="Sx4.SSx3.SSSx3">
<h4 class="ltx_title ltx_title_subsubsection">Impact of Query Position in Prompt.</h4>
<div class="ltx_para" id="Sx4.SSx3.SSSx3.p1">
<p class="ltx_p" id="Sx4.SSx3.SSSx3.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx4.F4" title="Figure 4 ‣ Impact of Query Position in Prompt. ‣ Evaluating Knowledge Reasoning on RAG Performance ‣ Extensive Empirical Studies ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">4</span></a> illustrates the effect of query position within the prompt on RAG system performance. The results show that placing the query after the reference information not only increases accuracy but also reduces hallucination rates, suggesting that the model benefits from having more context before addressing the query. Conversely, positioning the query before the reference leads to a higher hallucination rate, likely because the model lacks sufficient context, making it more prone to misinterpret the query, overlook critical details, and generate responses based on incomplete information. This experiment highlights the need for careful prompt structuring to balance accuracy, hallucination, and missing information in RAG systems <cite class="ltx_cite ltx_citemacro_citep">(Liu et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib23" title="">2023a</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="Sx4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="560" id="Sx4.F3.g1" src="x3.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span> Impact of CoT across knowledge sources.</figcaption>
</figure>
<figure class="ltx_figure" id="Sx4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="560" id="Sx4.F4.g1" src="x4.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Impact of query position within prompt.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="Sx4.SSx3.SSSx4">
<h4 class="ltx_title ltx_title_subsubsection">Impact of Noise Chunks.</h4>
<figure class="ltx_figure" id="Sx4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="560" id="Sx4.F5.g1" src="x5.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Performance comparison based on noise chunk quantity in RAG.</figcaption>
</figure>
<div class="ltx_para" id="Sx4.SSx3.SSSx4.p1">
<p class="ltx_p" id="Sx4.SSx3.SSSx4.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx4.F5" title="Figure 5 ‣ Impact of Noise Chunks. ‣ Evaluating Knowledge Reasoning on RAG Performance ‣ Extensive Empirical Studies ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">5</span></a> illustrates the impact of varying noise chunks on RAG system performance. The results show that as the number of noise chunks increases, accuracy initially dips but then improves, reaching its peak at moderate noise levels. Interestingly, while hallucination rates rise with the introduction of noise, they tend to stabilize and slightly decrease at higher noise levels. This suggests that a certain degree of noise may prompt the model to better filter out irrelevant information <cite class="ltx_cite ltx_citemacro_citep">(Cuconasu et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib8" title="">2024</a>)</cite>. However, the consistent decrease in the missing rate as noise increases indicates that the model becomes more decisive, though not always accurately guided. The overall score declines with increasing noise but improves at specific levels, highlighting the complex interplay between noise and performance. These findings highlight the complex impact of noise in RAG systems, emphasizing the importance of carefully optimizing noise management strategies to maintain performance.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="Sx5">
<h2 class="ltx_title ltx_title_section">Impact of Hyperparameter Configurations</h2>
<div class="ltx_para" id="Sx5.p1">
<p class="ltx_p" id="Sx5.p1.1">In this section, we systematically examine how different hyperparameter configurations influence the performance of our Retrieval-Augmented Generation (RAG) system. Our analysis spans both the Retrieval and Large Language Model (LLM) Reasoning components, assessing how adjustments to these parameters affect the performance. For comprehensive details on the experimental setup, including variations in configurations, refer to Appendix B.</p>
</div>
<section class="ltx_subsection" id="Sx5.SSx1">
<h3 class="ltx_title ltx_title_subsection">Effect of Hyperparameters on Retrieval</h3>
<div class="ltx_para" id="Sx5.SSx1.p1">
<p class="ltx_p" id="Sx5.SSx1.p1.1"><span class="ltx_text ltx_font_bold" id="Sx5.SSx1.p1.1.1">Impact of Retrieval Embedding.</span>
As summarized in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx5.T6" title="Table 6 ‣ Effect of Hyperparameters on Retrieval ‣ Impact of Hyperparameter Configurations ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">6</span></a>, the experiment evaluates the performance of three embedding models—BGE-small-en-v1.5, BGE-large-en-v1.5, and BGE-M3—within a RAG system. The BGE-M3 model <cite class="ltx_cite ltx_citemacro_citep">(Chen et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib4" title="">2024</a>)</cite> achieves the highest accuracy but also the highest hallucination rate, highlighting a trade-off between precision and error. BGE-large-en-v1.5 minimizes hallucinations but at the expense of accuracy, indicating a more conservative approach. BGE-small-en-v1.5 offers a balanced performance between these two extremes. The performance differences among the models stem from a trade-off between capturing rich semantics and overfitting.</p>
</div>
<figure class="ltx_table" id="Sx5.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Performance comparison of embedding models.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Sx5.T6.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Sx5.T6.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="Sx5.T6.1.1.1.1"><span class="ltx_text ltx_font_bold" id="Sx5.T6.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx5.T6.1.1.1.2"><span class="ltx_text ltx_font_bold" id="Sx5.T6.1.1.1.2.1">Acc.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx5.T6.1.1.1.3"><span class="ltx_text ltx_font_bold" id="Sx5.T6.1.1.1.3.1">Halluc.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx5.T6.1.1.1.4"><span class="ltx_text ltx_font_bold" id="Sx5.T6.1.1.1.4.1">Miss.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx5.T6.1.1.1.5"><span class="ltx_text ltx_font_bold" id="Sx5.T6.1.1.1.5.1">Score</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Sx5.T6.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Sx5.T6.1.2.1.1">BGE-small</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T6.1.2.1.2">24.65%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T6.1.2.1.3">38.95%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T6.1.2.1.4">36.40%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T6.1.2.1.5">-14.30%</td>
</tr>
<tr class="ltx_tr" id="Sx5.T6.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Sx5.T6.1.3.2.1">BGE-large</th>
<td class="ltx_td ltx_align_center" id="Sx5.T6.1.3.2.2">22.83%</td>
<td class="ltx_td ltx_align_center" id="Sx5.T6.1.3.2.3"><span class="ltx_text ltx_font_bold" id="Sx5.T6.1.3.2.3.1">34.35%</span></td>
<td class="ltx_td ltx_align_center" id="Sx5.T6.1.3.2.4">42.82%</td>
<td class="ltx_td ltx_align_center" id="Sx5.T6.1.3.2.5"><span class="ltx_text ltx_font_bold" id="Sx5.T6.1.3.2.5.1">-11.52%</span></td>
</tr>
<tr class="ltx_tr" id="Sx5.T6.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="Sx5.T6.1.4.3.1">BGE-M3</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T6.1.4.3.2"><span class="ltx_text ltx_font_bold" id="Sx5.T6.1.4.3.2.1">25.09%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T6.1.4.3.3">41.21%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T6.1.4.3.4">33.70%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T6.1.4.3.5">-16.12%</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="Sx5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="560" id="Sx5.F6.g1" src="x6.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Effect of chunk size on RAG performance.</figcaption>
</figure>
<div class="ltx_para" id="Sx5.SSx1.p2">
<p class="ltx_p" id="Sx5.SSx1.p2.1"><span class="ltx_text ltx_font_bold" id="Sx5.SSx1.p2.1.1">Impact of Chunk Size and Overlap on Retrieval Performance.</span> As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx5.F6" title="Figure 6 ‣ Effect of Hyperparameters on Retrieval ‣ Impact of Hyperparameter Configurations ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">6</span></a> and Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx5.F7" title="Figure 7 ‣ Effect of Hyperparameters on Retrieval ‣ Impact of Hyperparameter Configurations ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">7</span></a>, increasing chunk size generally enhances accuracy by providing more context. However, it also raises hallucination rates due to the retrieval of excessive or less relevant content. Chunk overlap further exacerbates this issue by introducing redundancy, which increases the risk of errors. Balancing chunk size and overlap is therefore crucial for optimizing accuracy while minimizing hallucinations in RAG systems.</p>
</div>
<figure class="ltx_figure" id="Sx5.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="560" id="Sx5.F7.g1" src="x7.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Impact of chunk overlap on RAG performance.</figcaption>
</figure>
<figure class="ltx_figure" id="Sx5.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="560" id="Sx5.F8.g1" src="x8.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Performance across different temperature settings.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="Sx5.SSx2">
<h3 class="ltx_title ltx_title_subsection">Effect of Hyperparameters on LLM Reasoning</h3>
<div class="ltx_para" id="Sx5.SSx2.p1">
<p class="ltx_p" id="Sx5.SSx2.p1.1"><span class="ltx_text ltx_font_bold" id="Sx5.SSx2.p1.1.1">Impact of Large Language Model Selection. </span>
In Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx5.T7" title="Table 7 ‣ Effect of Hyperparameters on LLM Reasoning ‣ Impact of Hyperparameter Configurations ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">7</span></a>, we compare the performance of different LLM backbones within a RAG system, particularly when using techniques like Chain of Thought (CoT) prompting. This experiment highlights the impact of the LLM backbone on handling complex reasoning tasks. The LLaMA 3.1 8B model demonstrates a balanced approach, achieving a lower hallucination rate and a slightly positive overall score. In contrast, the Qwen2-7B model, while delivering higher accuracy, exhibits a much higher hallucination rate, leading to a negative overall score. This emphasizes the importance of selecting an LLM that balances accuracy and reliability, especially when tackling complex queries with CoT prompting.</p>
</div>
<figure class="ltx_table" id="Sx5.T7">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Performance comparison between LLaMA 3.1 8B and Qwen2-7B with CoT prompting.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Sx5.T7.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Sx5.T7.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="Sx5.T7.1.1.1.1"><span class="ltx_text ltx_font_bold" id="Sx5.T7.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx5.T7.1.1.1.2"><span class="ltx_text ltx_font_bold" id="Sx5.T7.1.1.1.2.1">Acc.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx5.T7.1.1.1.3"><span class="ltx_text ltx_font_bold" id="Sx5.T7.1.1.1.3.1">Halluc.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx5.T7.1.1.1.4"><span class="ltx_text ltx_font_bold" id="Sx5.T7.1.1.1.4.1">Miss.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx5.T7.1.1.1.5"><span class="ltx_text ltx_font_bold" id="Sx5.T7.1.1.1.5.1">Score</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Sx5.T7.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Sx5.T7.1.2.1.1">LLaMA 3.1 8B</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T7.1.2.1.2">13.20%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T7.1.2.1.3"><span class="ltx_text ltx_font_bold" id="Sx5.T7.1.2.1.3.1">10.50%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T7.1.2.1.4">76.29%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T7.1.2.1.5"><span class="ltx_text ltx_font_bold" id="Sx5.T7.1.2.1.5.1">2.70%</span></td>
</tr>
<tr class="ltx_tr" id="Sx5.T7.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="Sx5.T7.1.3.2.1">Qwen2-7B</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T7.1.3.2.2"><span class="ltx_text ltx_font_bold" id="Sx5.T7.1.3.2.2.1">26.19%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T7.1.3.2.3">27.43%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T7.1.3.2.4">46.39%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T7.1.3.2.5">-1.23%</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="Sx5.T8">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Comparison of top-p settings on RAG performance.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Sx5.T8.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Sx5.T8.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx5.T8.1.1.1.1"><span class="ltx_text ltx_font_bold" id="Sx5.T8.1.1.1.1.1">Top-p</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx5.T8.1.1.1.2"><span class="ltx_text ltx_font_bold" id="Sx5.T8.1.1.1.2.1">Acc.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx5.T8.1.1.1.3"><span class="ltx_text ltx_font_bold" id="Sx5.T8.1.1.1.3.1">Halluc.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx5.T8.1.1.1.4"><span class="ltx_text ltx_font_bold" id="Sx5.T8.1.1.1.4.1">Miss.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx5.T8.1.1.1.5"><span class="ltx_text ltx_font_bold" id="Sx5.T8.1.1.1.5.1">Score</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Sx5.T8.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T8.1.2.1.1">0.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T8.1.2.1.2">22.17%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T8.1.2.1.3">42.23%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T8.1.2.1.4">35.59%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T8.1.2.1.5">-20.06%</td>
</tr>
<tr class="ltx_tr" id="Sx5.T8.1.3.2">
<td class="ltx_td ltx_align_center" id="Sx5.T8.1.3.2.1">0.7</td>
<td class="ltx_td ltx_align_center" id="Sx5.T8.1.3.2.2">25.53%</td>
<td class="ltx_td ltx_align_center" id="Sx5.T8.1.3.2.3">38.88%</td>
<td class="ltx_td ltx_align_center" id="Sx5.T8.1.3.2.4">35.59%</td>
<td class="ltx_td ltx_align_center" id="Sx5.T8.1.3.2.5">-13.35%</td>
</tr>
<tr class="ltx_tr" id="Sx5.T8.1.4.3">
<td class="ltx_td ltx_align_center" id="Sx5.T8.1.4.3.1">0.8</td>
<td class="ltx_td ltx_align_center" id="Sx5.T8.1.4.3.2">25.75%</td>
<td class="ltx_td ltx_align_center" id="Sx5.T8.1.4.3.3">38.73%</td>
<td class="ltx_td ltx_align_center" id="Sx5.T8.1.4.3.4">35.52%</td>
<td class="ltx_td ltx_align_center" id="Sx5.T8.1.4.3.5">-12.98%</td>
</tr>
<tr class="ltx_tr" id="Sx5.T8.1.5.4">
<td class="ltx_td ltx_align_center" id="Sx5.T8.1.5.4.1">0.9</td>
<td class="ltx_td ltx_align_center" id="Sx5.T8.1.5.4.2">25.89%</td>
<td class="ltx_td ltx_align_center" id="Sx5.T8.1.5.4.3">38.37%</td>
<td class="ltx_td ltx_align_center" id="Sx5.T8.1.5.4.4">35.74%</td>
<td class="ltx_td ltx_align_center" id="Sx5.T8.1.5.4.5">-12.47%</td>
</tr>
<tr class="ltx_tr" id="Sx5.T8.1.6.5">
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T8.1.6.5.1">1.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T8.1.6.5.2"><span class="ltx_text ltx_font_bold" id="Sx5.T8.1.6.5.2.1">26.11%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T8.1.6.5.3"><span class="ltx_text ltx_font_bold" id="Sx5.T8.1.6.5.3.1">38.29%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T8.1.6.5.4">35.59%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T8.1.6.5.5"><span class="ltx_text ltx_font_bold" id="Sx5.T8.1.6.5.5.1">-12.18%</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="Sx5.SSx2.p2">
<p class="ltx_p" id="Sx5.SSx2.p2.1"><span class="ltx_text ltx_font_bold" id="Sx5.SSx2.p2.1.1">Effect of Sampling parameters. </span>
In Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx5.F8" title="Figure 8 ‣ Effect of Hyperparameters on Retrieval ‣ Impact of Hyperparameter Configurations ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">8</span></a> and Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#Sx5.T8" title="Table 8 ‣ Effect of Hyperparameters on LLM Reasoning ‣ Impact of Hyperparameter Configurations ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">8</span></a>, the impact of varying temperature and top-p settings on RAG performance is analyzed. The figure illustrates that increasing the temperature slightly improves accuracy, but it also raises the hallucination rate and decreases the overall score. Higher temperatures encourage more diverse responses, reducing the missing rate, but this comes at the cost of introducing inaccuracies. This suggests that while higher temperatures might help generate more comprehensive answers, they also increase the risk of unreliable outputs, necessitating careful tuning to balance accuracy and creativity.</p>
</div>
<div class="ltx_para" id="Sx5.SSx2.p3">
<p class="ltx_p" id="Sx5.SSx2.p3.1">Similarly, the table shows that as the top-p value increases, accuracy improves while hallucination rates decrease, indicating that a broader range of token options leads to more accurate and reliable responses. The missing rate remains stable across different top-p settings, suggesting that top-p primarily affects the quality rather than the completeness of the generated content. Overall, higher top-p values appear to enhance the model’s performance by achieving a better balance between accuracy and diversity in the output.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Sx6">
<h2 class="ltx_title ltx_title_section">Conclusion: Challenges and Future Direction</h2>
<div class="ltx_para" id="Sx6.p1">
<p class="ltx_p" id="Sx6.p1.1">In summary, we review our findings on key aspects of the RAG framework—knowledge selection, retrieval, reasoning, and performance evaluation—highlighting challenges and identifying future research opportunities to further enhance system effectiveness.</p>
</div>
<section class="ltx_paragraph" id="Sx6.SSx2.SSSx4.Px1">
<h5 class="ltx_title ltx_title_paragraph">Knowledge Selection.</h5>
<div class="ltx_para" id="Sx6.SSx2.SSSx4.Px1.p1">
<p class="ltx_p" id="Sx6.SSx2.SSSx4.Px1.p1.1">We empirically evaluated strategies for selecting knowledge from multi-source and multi-format external databases. Our results show that the selection mechanism significantly affects the accuracy and relevance of generated answers. Specifically, differences in filtering and prioritization lead to varying alignment with query intent. This highlights the need for more adaptive selection methods that can dynamically cater to diverse and complex knowledge sources. Future research should focus on developing these adaptive frameworks to better align knowledge selection with RAG system objectives.</p>
</div>
</section>
<section class="ltx_paragraph" id="Sx6.SSx2.SSSx4.Px2">
<h5 class="ltx_title ltx_title_paragraph">Knowledge Retrieval.</h5>
<div class="ltx_para" id="Sx6.SSx2.SSSx4.Px2.p1">
<p class="ltx_p" id="Sx6.SSx2.SSSx4.Px2.p1.1">Our exploration of the multi-step knowledge retrieval process—encompassing broad retrieval, focused retrieval, and rank refinement—revealed the importance of coordinating these stages for optimal RAG performance. The interaction among these phases significantly enhances the relevance and precision of retrieved knowledge. Furthermore, initial processing steps like Named Entity Recognition (NER), matching, and reformatting retrieved data, particularly from sources like Mock APIs, were found to improve content coherence. Future work should refine these retrieval pipelines, incorporating advanced preprocessing techniques such as contextual re-ranking and dynamic filtering to achieve more aligned and context-aware knowledge retrieval.</p>
</div>
</section>
<section class="ltx_paragraph" id="Sx6.SSx2.SSSx4.Px3">
<h5 class="ltx_title ltx_title_paragraph">Knowledge Reasoning.</h5>
<div class="ltx_para" id="Sx6.SSx2.SSSx4.Px3.p1">
<p class="ltx_p" id="Sx6.SSx2.SSSx4.Px3.p1.1">In the reasoning phase using large language models (LLMs), we explored advanced techniques like In-Context Learning (ICL) and Chain-of-Thought (CoT), yielding insights into noise chunking and the sensitivity of query positioning. These findings emphasize the need for refining reasoning methods within RAG frameworks to enhance both accuracy and interpretability. Future research should investigate the interplay between reasoning strategies and retrieval contexts, aiming to improve the robustness and consistency of LLM-based reasoning across various applications, which could significantly advance the scalability and reliability of RAG systems.</p>
</div>
</section>
<section class="ltx_paragraph" id="Sx6.SSx2.SSSx4.Px4">
<h5 class="ltx_title ltx_title_paragraph">Performance Evaluation.</h5>
<div class="ltx_para" id="Sx6.SSx2.SSSx4.Px4.p1">
<p class="ltx_p" id="Sx6.SSx2.SSSx4.Px4.p1.1">We adopted the KDD Cup2024 evaluation strategy, which starts with strict string matching followed by GPT-based automated assessments for mismatches. While effective, this method has limitations, particularly the inconsistency of GPT-based evaluations when model performances are closely matched, which can lead to unreliable results. Additionally, the evaluation criteria are narrow, neglecting the quality of intermediate retrieval contexts and the reasoning processes within LLMs. Future research should aim to develop more comprehensive evaluation methodologies, including human-in-the-loop <cite class="ltx_cite ltx_citemacro_citep">(Stiennon et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#bib.bib29" title="">2022</a>)</cite> assessments and metrics that evaluate the quality of retrieval and reasoning processes. These improvements would provide a more nuanced and reliable understanding of RAG performance, driving further progress in the field.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asai et al. (2023)</span>
<span class="ltx_bibblock">
Asai, A.; Wu, Z.; Wang, Y.; Sil, A.; and Hajishirzi, H. 2023.

</span>
<span class="ltx_bibblock">Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection.

</span>
<span class="ltx_bibblock">arXiv:2310.11511.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bajaj et al. (2018)</span>
<span class="ltx_bibblock">
Bajaj, P.; Campos, D.; Craswell, N.; Deng, L.; Gao, J.; Liu, X.; Majumder, R.; McNamara, A.; Mitra, B.; Nguyen, T.; Rosenberg, M.; Song, X.; Stoica, A.; Tiwary, S.; and Wang, T. 2018.

</span>
<span class="ltx_bibblock">MS MARCO: A Human Generated MAchine Reading COmprehension Dataset.

</span>
<span class="ltx_bibblock">arXiv:1611.09268.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023)</span>
<span class="ltx_bibblock">
Chen, J.; Lin, H.; Han, X.; and Sun, L. 2023.

</span>
<span class="ltx_bibblock">Benchmarking Large Language Models in Retrieval-Augmented Generation.

</span>
<span class="ltx_bibblock">arXiv:2309.01431.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024)</span>
<span class="ltx_bibblock">
Chen, J.; Xiao, S.; Zhang, P.; Luo, K.; Lian, D.; and Liu, Z. 2024.

</span>
<span class="ltx_bibblock">BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation.

</span>
<span class="ltx_bibblock">arXiv:2402.03216.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al. (2024)</span>
<span class="ltx_bibblock">
Cheng, M.; Liu, Q.; Zhang, W.; Liu, Z.; Zhao, H.; and Chen, E. 2024.

</span>
<span class="ltx_bibblock">A general tail item representation enhancement framework for sequential recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Frontiers of Computer Science</em>, 18(6): 186333.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al. (2022)</span>
<span class="ltx_bibblock">
Cheng, M.; Liu, Z.; Liu, Q.; Ge, S.; and Chen, E. 2022.

</span>
<span class="ltx_bibblock">Towards automatic discovering of deep hybrid network architecture for sequential recommendation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the ACM Web Conference 2022</em>, 1923–1932.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al. (2021)</span>
<span class="ltx_bibblock">
Cheng, M.; Yuan, F.; Liu, Q.; Xin, X.; and Chen, E. 2021.

</span>
<span class="ltx_bibblock">Learning transferable user representations with sequential behaviors via contrastive pre-training.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">2021 IEEE International Conference on Data Mining (ICDM)</em>, 51–60. IEEE.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cuconasu et al. (2024)</span>
<span class="ltx_bibblock">
Cuconasu, F.; Trappolini, G.; Siciliano, F.; Filice, S.; Campagnano, C.; Maarek, Y.; Tonellotto, N.; and Silvestri, F. 2024.

</span>
<span class="ltx_bibblock">The Power of Noise: Redefining Retrieval for RAG Systems.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, volume 17 of <em class="ltx_emph ltx_font_italic" id="bib.bib8.2.2">SIGIR 2024</em>, 719–729. ACM.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al. (2022)</span>
<span class="ltx_bibblock">
Dai, Z.; Zhao, V. Y.; Ma, J.; Luan, Y.; Ni, J.; Lu, J.; Bakalov, A.; Guu, K.; Hall, K. B.; and Chang, M.-W. 2022.

</span>
<span class="ltx_bibblock">Promptagator: Few-shot Dense Retrieval From 8 Examples.

</span>
<span class="ltx_bibblock">arXiv:2209.11755.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al. (2024)</span>
<span class="ltx_bibblock">
Ding, H.; Pang, L.; Wei, Z.; Shen, H.; and Cheng, X. 2024.

</span>
<span class="ltx_bibblock">Retrieve Only When It Needs: Adaptive Retrieval Augmentation for Hallucination Mitigation in Large Language Models.

</span>
<span class="ltx_bibblock">arXiv:2402.10612.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al. (2024)</span>
<span class="ltx_bibblock">
Dong, Q.; Li, L.; Dai, D.; Zheng, C.; Ma, J.; Li, R.; Xia, H.; Xu, J.; Wu, Z.; Chang, B.; Sun, X.; Li, L.; and Sui, Z. 2024.

</span>
<span class="ltx_bibblock">A Survey on In-context Learning.

</span>
<span class="ltx_bibblock">arXiv:2301.00234.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2024)</span>
<span class="ltx_bibblock">
Gao, Y.; Xiong, Y.; Gao, X.; Jia, K.; Pan, J.; Bi, Y.; Dai, Y.; Sun, J.; Wang, M.; and Wang, H. 2024.

</span>
<span class="ltx_bibblock">Retrieval-Augmented Generation for Large Language Models: A Survey.

</span>
<span class="ltx_bibblock">arXiv:2312.10997.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gaur et al. (2021)</span>
<span class="ltx_bibblock">
Gaur, M.; Gunaratna, K.; Srinivasan, V.; and Jin, H. 2021.

</span>
<span class="ltx_bibblock">ISEEQ: Information Seeking Question Generation using Dynamic Meta-Information Retrieval and Knowledge Graphs.

</span>
<span class="ltx_bibblock">arXiv:2112.07622.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al. (2018)</span>
<span class="ltx_bibblock">
Gu, J.; Wang, Y.; Cho, K.; and Li, V. O. K. 2018.

</span>
<span class="ltx_bibblock">Search Engine Guided Non-Parametric Neural Machine Translation.

</span>
<span class="ltx_bibblock">arXiv:1705.07267.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guu et al. (2020)</span>
<span class="ltx_bibblock">
Guu, K.; Lee, K.; Tung, Z.; Pasupat, P.; and Chang, M. 2020.

</span>
<span class="ltx_bibblock">Retrieval Augmented Language Model Pre-Training.

</span>
<span class="ltx_bibblock">In III, H. D.; and Singh, A., eds., <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the 37th International Conference on Machine Learning</em>, volume 119 of <em class="ltx_emph ltx_font_italic" id="bib.bib15.2.2">Proceedings of Machine Learning Research</em>, 3929–3938. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard et al. (2022)</span>
<span class="ltx_bibblock">
Izacard, G.; Caron, M.; Hosseini, L.; Riedel, S.; Bojanowski, P.; Joulin, A.; and Grave, E. 2022.

</span>
<span class="ltx_bibblock">Unsupervised Dense Information Retrieval with Contrastive Learning.

</span>
<span class="ltx_bibblock">arXiv:2112.09118.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2024)</span>
<span class="ltx_bibblock">
Jiang, H.; Wu, Q.; Luo, X.; Li, D.; Lin, C.-Y.; Yang, Y.; and Qiu, L. 2024.

</span>
<span class="ltx_bibblock">LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression.

</span>
<span class="ltx_bibblock">arXiv:2310.06839.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kandpal et al. (2023)</span>
<span class="ltx_bibblock">
Kandpal, N.; Deng, H.; Roberts, A.; Wallace, E.; and Raffel, C. 2023.

</span>
<span class="ltx_bibblock">Large Language Models Struggle to Learn Long-Tail Knowledge.

</span>
<span class="ltx_bibblock">arXiv:2211.08411.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski et al. (2019)</span>
<span class="ltx_bibblock">
Kwiatkowski, T.; Palomaki, J.; Redfield, O.; Collins, M.; Parikh, A.; Alberti, C.; Epstein, D.; Polosukhin, I.; Devlin, J.; Lee, K.; Toutanova, K.; Jones, L.; Kelcey, M.; Chang, M.-W.; Dai, A. M.; Uszkoreit, J.; Le, Q.; and Petrov, S. 2019.

</span>
<span class="ltx_bibblock">Natural Questions: A Benchmark for Question Answering Research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Transactions of the Association for Computational Linguistics</em>, 7: 452–466.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2020)</span>
<span class="ltx_bibblock">
Lewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V.; Goyal, N.; Küttler, H.; Lewis, M.; Yih, W.-t.; Rocktäschel, T.; Riedel, S.; and Kiela, D. 2020.

</span>
<span class="ltx_bibblock">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.

</span>
<span class="ltx_bibblock">In Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.; and Lin, H., eds., <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Advances in Neural Information Processing Systems</em>, volume 33, 9459–9474. Curran Associates, Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2021)</span>
<span class="ltx_bibblock">
Lewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V.; Goyal, N.; Küttler, H.; Lewis, M.; tau Yih, W.; Rocktäschel, T.; Riedel, S.; and Kiela, D. 2021.

</span>
<span class="ltx_bibblock">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.

</span>
<span class="ltx_bibblock">arXiv:2005.11401.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li, Nie, and Liang (2023)</span>
<span class="ltx_bibblock">
Li, X.; Nie, E.; and Liang, S. 2023.

</span>
<span class="ltx_bibblock">From Classification to Generation: Insights into Crosslingual Retrieval Augmented ICL.

</span>
<span class="ltx_bibblock">arXiv:2311.06595.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023a)</span>
<span class="ltx_bibblock">
Liu, N. F.; Lin, K.; Hewitt, J.; Paranjape, A.; Bevilacqua, M.; Petroni, F.; and Liang, P. 2023a.

</span>
<span class="ltx_bibblock">Lost in the Middle: How Language Models Use Long Contexts.

</span>
<span class="ltx_bibblock">arXiv:2307.03172.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023b)</span>
<span class="ltx_bibblock">
Liu, Y.; Huang, L.; Li, S.; Chen, S.; Zhou, H.; Meng, F.; Zhou, J.; and Sun, X. 2023b.

</span>
<span class="ltx_bibblock">RECALL: A Benchmark for LLMs Robustness against External Counterfactual Knowledge.

</span>
<span class="ltx_bibblock">arXiv:2311.08147.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. (2023)</span>
<span class="ltx_bibblock">
Ma, Y.; Cao, Y.; Hong, Y.; and Sun, A. 2023.

</span>
<span class="ltx_bibblock">Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023</em>. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al. (2022)</span>
<span class="ltx_bibblock">
Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C. L.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; Schulman, J.; Hilton, J.; Kelton, F.; Miller, L.; Simens, M.; Askell, A.; Welinder, P.; Christiano, P.; Leike, J.; and Lowe, R. 2022.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock">arXiv:2203.02155.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rackauckas (2024)</span>
<span class="ltx_bibblock">
Rackauckas, Z. 2024.

</span>
<span class="ltx_bibblock">Rag-Fusion: A New Take on Retrieval Augmented Generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">International Journal on Natural Language Computing</em>, 13(1): 37–47.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. (2023)</span>
<span class="ltx_bibblock">
Shi, F.; Chen, X.; Misra, K.; Scales, N.; Dohan, D.; Chi, E.; Schärli, N.; and Zhou, D. 2023.

</span>
<span class="ltx_bibblock">Large Language Models Can Be Easily Distracted by Irrelevant Context.

</span>
<span class="ltx_bibblock">arXiv:2302.00093.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stiennon et al. (2022)</span>
<span class="ltx_bibblock">
Stiennon, N.; Ouyang, L.; Wu, J.; Ziegler, D. M.; Lowe, R.; Voss, C.; Radford, A.; Amodei, D.; and Christiano, P. 2022.

</span>
<span class="ltx_bibblock">Learning to summarize from human feedback.

</span>
<span class="ltx_bibblock">arXiv:2009.01325.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2024)</span>
<span class="ltx_bibblock">
Sun, K.; Xu, Y. E.; Zha, H.; Liu, Y.; and Dong, X. L. 2024.

</span>
<span class="ltx_bibblock">Head-to-Tail: How Knowledgeable are Large Language Models (LLMs)? A.K.A. Will LLMs Replace Knowledge Graphs?

</span>
<span class="ltx_bibblock">arXiv:2308.10168.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vu et al. (2023)</span>
<span class="ltx_bibblock">
Vu, T.; Iyyer, M.; Wang, X.; Constant, N.; Wei, J.; Wei, J.; Tar, C.; Sung, Y.-H.; Zhou, D.; Le, Q.; and Luong, T. 2023.

</span>
<span class="ltx_bibblock">FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation.

</span>
<span class="ltx_bibblock">arXiv:2310.03214.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023a)</span>
<span class="ltx_bibblock">
Wang, X.; Yang, Q.; Qiu, Y.; Liang, J.; He, Q.; Gu, Z.; Xiao, Y.; and Wang, W. 2023a.

</span>
<span class="ltx_bibblock">KnowledGPT: Enhancing Large Language Models with Retrieval and Storage Access on Knowledge Bases.

</span>
<span class="ltx_bibblock">arXiv:2308.11761.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023b)</span>
<span class="ltx_bibblock">
Wang, Y.; Li, P.; Sun, M.; and Liu, Y. 2023b.

</span>
<span class="ltx_bibblock">Self-Knowledge Guided Retrieval Augmentation for Large Language Models.

</span>
<span class="ltx_bibblock">arXiv:2310.05002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2023)</span>
<span class="ltx_bibblock">
Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Ichter, B.; Xia, F.; Chi, E.; Le, Q.; and Zhou, D. 2023.

</span>
<span class="ltx_bibblock">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.

</span>
<span class="ltx_bibblock">arXiv:2201.11903.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2024)</span>
<span class="ltx_bibblock">
Xu, P.; Ping, W.; Wu, X.; McAfee, L.; Zhu, C.; Liu, Z.; Subramanian, S.; Bakhturina, E.; Shoeybi, M.; and Catanzaro, B. 2024.

</span>
<span class="ltx_bibblock">Retrieval meets Long Context Large Language Models.

</span>
<span class="ltx_bibblock">arXiv:2310.03025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2024)</span>
<span class="ltx_bibblock">
Yang, X.; Sun, K.; Xin, H.; Sun, Y.; Bhalla, N.; Chen, X.; Choudhary, S.; Gui, R. D.; Jiang, Z. W.; Jiang, Z.; Kong, L.; Moran, B.; Wang, J.; Xu, Y. E.; Yan, A.; Yang, C.; Yuan, E.; Zha, H.; Tang, N.; Chen, L.; Scheffer, N.; Liu, Y.; Shah, N.; Wanga, R.; Kumar, A.; tau Yih, W.; and Dong, X. L. 2024.

</span>
<span class="ltx_bibblock">CRAG – Comprehensive RAG Benchmark.

</span>
<span class="ltx_bibblock">arXiv:2406.04744.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yoran et al. (2024)</span>
<span class="ltx_bibblock">
Yoran, O.; Wolfson, T.; Ram, O.; and Berant, J. 2024.

</span>
<span class="ltx_bibblock">Making Retrieval-Augmented Language Models Robust to Irrelevant Context.

</span>
<span class="ltx_bibblock">arXiv:2310.01558.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2023)</span>
<span class="ltx_bibblock">
Yu, W.; Zhang, H.; Pan, X.; Ma, K.; Wang, H.; and Yu, D. 2023.

</span>
<span class="ltx_bibblock">Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models.

</span>
<span class="ltx_bibblock">arXiv:2311.09210.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2024)</span>
<span class="ltx_bibblock">
Yu, Y.; Ping, W.; Liu, Z.; Wang, B.; You, J.; Zhang, C.; Shoeybi, M.; and Catanzaro, B. 2024.

</span>
<span class="ltx_bibblock">RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs.

</span>
<span class="ltx_bibblock">arXiv:2407.02485.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2018)</span>
<span class="ltx_bibblock">
Zhang, J.; Utiyama, M.; Sumita, E.; Neubig, G.; and Nakamura, S. 2018.

</span>
<span class="ltx_bibblock">Guiding Neural Machine Translation with Retrieved Translation Pieces.

</span>
<span class="ltx_bibblock">arXiv:1804.02559.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024)</span>
<span class="ltx_bibblock">
Zhang, T.; Patil, S. G.; Jain, N.; Shen, S.; Zaharia, M.; Stoica, I.; and Gonzalez, J. E. 2024.

</span>
<span class="ltx_bibblock">RAFT: Adapting Language Model to Domain Specific RAG.

</span>
<span class="ltx_bibblock">arXiv:2403.10131.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2023)</span>
<span class="ltx_bibblock">
Zhou, D.; Schärli, N.; Hou, L.; Wei, J.; Scales, N.; Wang, X.; Schuurmans, D.; Cui, C.; Bousquet, O.; Le, Q.; and Chi, E. 2023.

</span>
<span class="ltx_bibblock">Least-to-Most Prompting Enables Complex Reasoning in Large Language Models.

</span>
<span class="ltx_bibblock">arXiv:2205.10625.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_logical-block" id="id1">
<div class="ltx_para" id="id1.p1">
<p class="ltx_p ltx_align_left" id="id1.p1.1"><span class="ltx_text ltx_font_bold" id="id1.p1.1.1">Appendices</span></p>
</div>
</div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix A Related Work</h2>
<section class="ltx_subsection" id="A1.SSx1">
<h3 class="ltx_title ltx_title_subsection">Retrieval-Augmented Generation (RAG)</h3>
<div class="ltx_para" id="A1.SSx1.p1">
<p class="ltx_p" id="A1.SSx1.p1.1">Retrieval-Augmented Generation (RAG) have been developed to enhance generative models by integrating retrieval mechanisms, allowing these models to dynamically access and utilize external knowledge. This approach significantly improves the accuracy and relevance of generated content. However, RAG systems still face difficulties in handling long-tail queries and mitigating hallucinations, especially in complex scenarios. To address these challenges, we propose the RAG-X framework, which incorporates well-designed strategies for knowledge source selection, knowledge retrieval, and knowledge reasoning. These improvements are specifically aimed at enhancing the system’s ability to manage complex queries and reduce the hallucinations.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SSx2">
<h3 class="ltx_title ltx_title_subsection">Existing Benchmarks for RAG Systems</h3>
<div class="ltx_para" id="A1.SSx2.p1">
<p class="ltx_p" id="A1.SSx2.p1.1">RAG benchmarks have been instrumental in the development of RAG systems, but they often fall short in capturing real-world complexity due to limited query types and document formats. The dataset for KDD Cup 2024 CRAG addresses these gaps by offering diverse query types and document formats, yet it presents external knowledge in HTML format, which can hinder effective model integration. Our work improves this by parsing the HTML knowledge into Markdown (MD) format, making it more accessible and structured for models. We will release this enhanced dataset, named RM3QA (A Real-time Multi-domain, Multi-format, Multi-source Question Answer Dataset for RAG), thereby providing a stronger benchmark for future RAG research.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SSx3">
<h3 class="ltx_title ltx_title_subsection">Chain of Thought (CoT) and In-Context Learning (ICL)</h3>
<div class="ltx_para" id="A1.SSx3.p1">
<p class="ltx_p" id="A1.SSx3.p1.1">Chain of Thought (CoT) allows Large Language Models (LLMs) to break down complex problems into sequential steps, improving response accuracy in tasks requiring multi-step reasoning. In-Context Learning (ICL), on the other hand, enables LLMs to adapt to new tasks using only a few examples provided within the context, without the need for additional fine-tuning. However, we found that the performance of CoT and ICL in RAG systems may require further investigation. To explore this, we conducted experiments on the application of CoT and ICL in RAG and discovered some notable findings, such as the influence of CoT on RAG potentially being closely tied to the choice of external knowledge sources.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Appendix B Reproducibility </h2>
<section class="ltx_subsection" id="A2.SSx1">
<h3 class="ltx_title ltx_title_subsection">Dataset Processing</h3>
<div class="ltx_para" id="A2.SSx1.p1">
<p class="ltx_p" id="A2.SSx1.p1.1">For our experiments, we utilized the official validation set provided by the dataset of the KDD Cup 2024 CRAG competition. To enhance the usability of the web-based knowledge within the dataset, we converted HTML-formatted web pages into markdown format using the Jina framework. This conversion was essential to improve the compatibility of the data with Large Language Models (LLMs), enabling more effective inference and retrieval of relevant information.</p>
</div>
<div class="ltx_para" id="A2.SSx1.p2">
<p class="ltx_p" id="A2.SSx1.p2.1">This processing step was crucial for ensuring that the external knowledge sources were optimally formatted for our Retrieval-Augmented Generation (RAG) framework. The parsed markdown dataset, which will be made publicly available, supports further research and underscores the practical improvements brought by our approach in handling complex QA scenarios.</p>
</div>
</section>
<section class="ltx_subsection" id="A2.SSx2">
<h3 class="ltx_title ltx_title_subsection">Experimental Setup</h3>
<div class="ltx_para" id="A2.SSx2.p1">
<p class="ltx_p" id="A2.SSx2.p1.1">To ensure the reproducibility and consistency of our experiments, we establish a base configuration for our Retrieval-Augmented Generation (RAG) model, detailed in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#A2.T9" title="Table 9 ‣ Experimental Setup ‣ Appendix B Appendix B Reproducibility ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">9</span></a>. The base hyperparameters were carefully selected to balance retrieval relevance and computational efficiency while maintaining deterministic outputs from the LLM.</p>
</div>
<div class="ltx_para" id="A2.SSx2.p2">
<p class="ltx_p" id="A2.SSx2.p2.1">The model configuration uses the BGE-M3 retriever with a chunk size of 200 tokens and no chunk overlap (<span class="ltx_text ltx_font_typewriter" id="A2.SSx2.p2.1.1">chunk_overlap=0</span>). Dense retrieval methods are exclusively employed, with no sparse retrieval or reranking applied. For each query, 3 chunks are retrieved.</p>
</div>
<div class="ltx_para" id="A2.SSx2.p3">
<p class="ltx_p" id="A2.SSx2.p3.1">The generative component relies on the Meta-LLaMA-3-1-8B-Instruct model, which is utilized without any fine-tuning or additional training. Prompts are constructed by default without employing Chain of Thought (CoT) reasoning or In-Context Learning (ICL).</p>
</div>
<div class="ltx_para" id="A2.SSx2.p4">
<p class="ltx_p" id="A2.SSx2.p4.1">During experimentation, any analysis of specific hyperparameters involves altering only those particular settings, while all other configurations remain consistent with the base setup. The results section presents the specific values and settings explored in our experiments, and any configuration not explicitly mentioned should be assumed to adhere to this base configuration.</p>
</div>
<div class="ltx_para" id="A2.SSx2.p5">
<p class="ltx_p" id="A2.SSx2.p5.1">This standardized approach ensures a controlled environment, minimizes noise, and allows for accurate comparisons and reliable reproduction of our results.</p>
</div>
<figure class="ltx_table" id="A2.T9">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>Base hyperparameter configuration.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A2.T9.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A2.T9.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A2.T9.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A2.T9.1.1.1.1.1">Hyperparameter</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A2.T9.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A2.T9.1.1.1.2.1">Value</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T9.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="A2.T9.1.2.1.1">Chunk Size</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A2.T9.1.2.1.2">200 tokens</td>
</tr>
<tr class="ltx_tr" id="A2.T9.1.3.2">
<td class="ltx_td ltx_align_left" id="A2.T9.1.3.2.1">Chunk Overlap</td>
<td class="ltx_td ltx_align_left" id="A2.T9.1.3.2.2">0 (no overlap)</td>
</tr>
<tr class="ltx_tr" id="A2.T9.1.4.3">
<td class="ltx_td ltx_align_left" id="A2.T9.1.4.3.1">Embedding Model</td>
<td class="ltx_td ltx_align_left" id="A2.T9.1.4.3.2">Hybrid retrieval disabled</td>
</tr>
<tr class="ltx_tr" id="A2.T9.1.5.4">
<td class="ltx_td ltx_align_left" id="A2.T9.1.5.4.1">Rerank</td>
<td class="ltx_td ltx_align_left" id="A2.T9.1.5.4.2">False (no reranking)</td>
</tr>
<tr class="ltx_tr" id="A2.T9.1.6.5">
<td class="ltx_td ltx_align_left" id="A2.T9.1.6.5.1">Temperature</td>
<td class="ltx_td ltx_align_left" id="A2.T9.1.6.5.2">0 (deterministic)</td>
</tr>
<tr class="ltx_tr" id="A2.T9.1.7.6">
<td class="ltx_td ltx_align_left" id="A2.T9.1.7.6.1">Top-p</td>
<td class="ltx_td ltx_align_left" id="A2.T9.1.7.6.2">1 (all tokens considered)</td>
</tr>
<tr class="ltx_tr" id="A2.T9.1.8.7">
<td class="ltx_td ltx_align_left" id="A2.T9.1.8.7.1">Noise Blocks</td>
<td class="ltx_td ltx_align_left" id="A2.T9.1.8.7.2">0 (no noise blocks)</td>
</tr>
<tr class="ltx_tr" id="A2.T9.1.9.8">
<td class="ltx_td ltx_align_left" id="A2.T9.1.9.8.1">COT (Chain of Thought)</td>
<td class="ltx_td ltx_align_left" id="A2.T9.1.9.8.2">False (not used)</td>
</tr>
<tr class="ltx_tr" id="A2.T9.1.10.9">
<td class="ltx_td ltx_align_left" id="A2.T9.1.10.9.1">ICL (In-Context Learning)</td>
<td class="ltx_td ltx_align_left" id="A2.T9.1.10.9.2">False (not used)</td>
</tr>
<tr class="ltx_tr" id="A2.T9.1.11.10">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A2.T9.1.11.10.1">LLM Backbone</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A2.T9.1.11.10.2">LLaMA 3.1 8B</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="A2.SSx3">
<h3 class="ltx_title ltx_title_subsection">Evaluation Metrics</h3>
<div class="ltx_para" id="A2.SSx3.p1">
<p class="ltx_p" id="A2.SSx3.p1.1">The performance of our Retrieval-Augmented Generation (RAG) model is evaluated using four key metrics: <span class="ltx_text ltx_font_italic" id="A2.SSx3.p1.1.1">accuracy</span>, <span class="ltx_text ltx_font_italic" id="A2.SSx3.p1.1.2">hallucination</span>, <span class="ltx_text ltx_font_italic" id="A2.SSx3.p1.1.3">missing</span>, and <span class="ltx_text ltx_font_italic" id="A2.SSx3.p1.1.4">score</span>.</p>
</div>
<section class="ltx_paragraph" id="A2.SSx3.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Accuracy</h5>
<div class="ltx_para" id="A2.SSx3.SSS0.Px1.p1">
<p class="ltx_p" id="A2.SSx3.SSS0.Px1.p1.1">The accuracy of a prediction is first determined through string matching—if the prediction exactly matches any of the ground truth answers, it is deemed correct and earns 1 point. If the prediction does not match exactly, GPT-3.5 Turbo is used to evaluate the prediction. If GPT-3.5 Turbo assesses the prediction as correct, it is also considered accurate and awarded 1 point.</p>
</div>
</section>
<section class="ltx_paragraph" id="A2.SSx3.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Hallucination</h5>
<div class="ltx_para" id="A2.SSx3.SSS0.Px2.p1">
<p class="ltx_p" id="A2.SSx3.SSS0.Px2.p1.1">If a prediction is incorrect or misleading, GPT-3.5 Turbo classifies it as a hallucination, scoring -1.</p>
</div>
</section>
<section class="ltx_paragraph" id="A2.SSx3.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Missing</h5>
<div class="ltx_para" id="A2.SSx3.SSS0.Px3.p1">
<p class="ltx_p" id="A2.SSx3.SSS0.Px3.p1.1">Responses like “I don’t know” are classified as <span class="ltx_text ltx_font_italic" id="A2.SSx3.SSS0.Px3.p1.1.1">missing</span> and score 0, contributing to the overall error rate.</p>
</div>
</section>
<section class="ltx_paragraph" id="A2.SSx3.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Score</h5>
<div class="ltx_para" id="A2.SSx3.SSS0.Px4.p1">
<p class="ltx_p" id="A2.SSx3.SSS0.Px4.p1.1">The <span class="ltx_text ltx_font_italic" id="A2.SSx3.SSS0.Px4.p1.1.1">score</span> is calculated as the difference between accurate predictions and hallucinations.</p>
</div>
</section>
<section class="ltx_paragraph" id="A2.SSx3.SSS0.Px5">
<h5 class="ltx_title ltx_title_paragraph">Evaluation Process</h5>
<div class="ltx_para" id="A2.SSx3.SSS0.Px5.p1">
<p class="ltx_p" id="A2.SSx3.SSS0.Px5.p1.1">GPT-3.5 Turbo uses the following prompt to evaluate predictions:</p>
</div>
<div class="ltx_para" id="A2.SSx3.SSS0.Px5.p2">
<blockquote class="ltx_quote" id="A2.SSx3.SSS0.Px5.p2.1">
<p class="ltx_p" id="A2.SSx3.SSS0.Px5.p2.1.1"><span class="ltx_text ltx_font_bold" id="A2.SSx3.SSS0.Px5.p2.1.1.1"># Task:</span> Judge if the model prediction matches any ground truth. If matched,“Accuracy” is “True”; otherwise, “False”. If the model cannot answer or lacks information, “Accuracy” is “False”. If the ground truth is “invalid question”, “Accuracy” is “True” only if the prediction is exactly “invalid question”.</p>
<p class="ltx_p" id="A2.SSx3.SSS0.Px5.p2.1.2"><span class="ltx_text ltx_font_bold" id="A2.SSx3.SSS0.Px5.p2.1.2.1"># Output:</span> A JSON string with an “Accuracy” field as “True” or “False”.</p>
</blockquote>
</div>
<div class="ltx_para" id="A2.SSx3.SSS0.Px5.p3">
<p class="ltx_p" id="A2.SSx3.SSS0.Px5.p3.1">The final score is determined by averaging the scores across all evaluated questions.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A2.SSx4">
<h3 class="ltx_title ltx_title_subsection">Explanation of Knowledge Source Selection</h3>
<div class="ltx_para" id="A2.SSx4.p1">
<p class="ltx_p" id="A2.SSx4.p1.1">The specific meanings of external knowledge in each setting are as follows:</p>
</div>
<div class="ltx_para" id="A2.SSx4.p2">
<ul class="ltx_itemize" id="A2.I1">
<li class="ltx_item" id="A2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I1.i1.p1">
<p class="ltx_p" id="A2.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I1.i1.p1.1.1">LLM:</span> This setting indicates that the model relies solely on its internal knowledge for inference, without using any external knowledge sources. The model depends entirely on the knowledge and capabilities acquired during its pre-training phase.</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I1.i2.p1">
<p class="ltx_p" id="A2.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I1.i2.p1.1.1">WEB:</span> In this setting, the model exclusively relies on external knowledge retrieved from web pages for inference, without utilizing its internal knowledge base. The model generates answers by retrieving and leveraging information obtained from the web.</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I1.i3.p1">
<p class="ltx_p" id="A2.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I1.i3.p1.1.1">Mock API:</span> This setting indicates that the model uses only the structured external data provided by the mock API for inference, without depending on any internal knowledge. The model directly retrieves relevant information from the mock API to generate answers.</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I1.i4.p1">
<p class="ltx_p" id="A2.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I1.i4.p1.1.1">ALL:</span> This setting indicates that the model relies solely on external knowledge sources, including information from the web and structured data from the mock API, without using its internal knowledge base. The model integrates external resources to complete the task.</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I1.i5.p1">
<p class="ltx_p" id="A2.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I1.i5.p1.1.1">LLM+WEB:</span> In this setting, the model utilizes both its internal knowledge and external knowledge from the web for inference. The model combines its internal knowledge base with information retrieved from the web to perform a more comprehensive inference.</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I1.i6.p1">
<p class="ltx_p" id="A2.I1.i6.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I1.i6.p1.1.1">LLM+Mock API:</span> This setting indicates that the model combines its internal knowledge with the external structured data from the mock API for inference. The model synthesizes these two knowledge sources to generate more accurate answers.</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I1.i7.p1">
<p class="ltx_p" id="A2.I1.i7.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I1.i7.p1.1.1">LLM+ALL:</span> This setting indicates that the model leverages both internal knowledge and external information from the web and the mock API for inference. By accessing the broadest range of knowledge sources, the model aims to enhance the accuracy of its answers through collaborative reasoning across multiple inputs.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="A2.SSx4.p2.1">These settings represent manually selected knowledge sources, allowing us to assess the LLM’s performance across various combinations of internal and external knowledge. However, in addition to these manual settings, we have implemented Predefined Fallback Strategies. In these strategies, the LLM initially attempts to answer the query using only its internal knowledge. If the LLM is unable to generate a satisfactory answer, it then follows a manually predefined path to access external knowledge. This predefined path is categorized into three specific combinations: LLM+WEB, LLM+Mock API, and LLM+ALL. Each path specifies the order and combination of knowledge sources to be used after the initial internal knowledge attempt.</p>
</div>
<div class="ltx_para" id="A2.SSx4.p3">
<p class="ltx_p" id="A2.SSx4.p3.1">Moreover, we have developed an Agent-Based Dynamic Selection Strategy. In this approach, the LLM first tries to answer using its internal knowledge. If it fails, an LLM Agent-based Router dynamically selects the most appropriate external knowledge source from the available options—LLM+WEB, LLM+Mock API, or LLM+ALL—based on the query’s requirements. This strategy allows for more flexible and adaptive knowledge integration, potentially improving the LLM’s response accuracy in diverse and complex scenarios.</p>
</div>
<div class="ltx_para" id="A2.SSx4.p4">
<p class="ltx_p" id="A2.SSx4.p4.1">These different settings allow us to evaluate the LLM’s performance under various combinations of knowledge sources and explore how to optimize knowledge integration strategies to improve inference accuracy and reliability in practical applications.</p>
</div>
</section>
<section class="ltx_subsection" id="A2.SSx5">
<h3 class="ltx_title ltx_title_subsection">Automatic Selection of External Knowledge Sources</h3>
<div class="ltx_para" id="A2.SSx5.p1">
<p class="ltx_p" id="A2.SSx5.p1.1">In our Retrieval-Augmented Generation (RAG) system, we’ve implemented a method that allows the large language model (LLM) to autonomously select the most appropriate external knowledge source when it encounters a query that cannot be answered using its internal knowledge. The process is as follows:</p>
</div>
<div class="ltx_para" id="A2.SSx5.p2">
<p class="ltx_p" id="A2.SSx5.p2.1">Initially, the LLM attempts to answer the query using its internal knowledge. If the LLM responds with “I don’t know,” the query is then passed back to the LLM with a specialized prompt designed to guide the model in selecting the appropriate external data source.</p>
</div>
<div class="ltx_para" id="A2.SSx5.p3">
<p class="ltx_p" id="A2.SSx5.p3.1">The prompt provided to the LLM is as follows:</p>
</div>
<div class="ltx_para" id="A2.SSx5.p4">
<blockquote class="ltx_quote" id="A2.SSx5.p4.1">
<p class="ltx_p" id="A2.SSx5.p4.1.1"><span class="ltx_text ltx_font_bold" id="A2.SSx5.p4.1.1.1">You are an intelligent assistant tasked with selecting the most appropriate data source(s) for answering user queries. You have access to two types of data sources:</span></p>
<p class="ltx_p" id="A2.SSx5.p4.1.2">1. <span class="ltx_text ltx_font_bold" id="A2.SSx5.p4.1.2.1">Web Pages</span>: Obtained through search engines, providing rich and comprehensive information but may contain outdated or misleading information for time-sensitive queries.</p>
<p class="ltx_p" id="A2.SSx5.p4.1.3">2. <span class="ltx_text ltx_font_bold" id="A2.SSx5.p4.1.3.1">Mock APIs</span>: Real-time APIs offering current information, less comprehensive than web pages but highly reliable for time-sensitive data.</p>
<p class="ltx_p" id="A2.SSx5.p4.1.4"><span class="ltx_text ltx_font_bold" id="A2.SSx5.p4.1.4.1">Based on the user’s query, you must choose the most suitable data source(s) from these options:</span></p>
<ul class="ltx_itemize" id="A2.I2">
<li class="ltx_item" id="A2.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I2.i1.p1">
<p class="ltx_p" id="A2.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I2.i1.p1.1.1">a) Web Pages only</span></p>
</div>
</li>
<li class="ltx_item" id="A2.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I2.i2.p1">
<p class="ltx_p" id="A2.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I2.i2.p1.1.1">b) Mock APIs only</span></p>
</div>
</li>
<li class="ltx_item" id="A2.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I2.i3.p1">
<p class="ltx_p" id="A2.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I2.i3.p1.1.1">c) Both Web Pages and Mock APIs</span></p>
</div>
</li>
</ul>
<p class="ltx_p" id="A2.SSx5.p4.1.5"><span class="ltx_text ltx_font_bold" id="A2.SSx5.p4.1.5.1">Consider the following factors when making your decision:</span></p>
<ul class="ltx_itemize" id="A2.I3">
<li class="ltx_item" id="A2.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I3.i1.p1">
<p class="ltx_p" id="A2.I3.i1.p1.1">The nature of the query (static information vs. time-sensitive data)</p>
</div>
</li>
<li class="ltx_item" id="A2.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I3.i2.p1">
<p class="ltx_p" id="A2.I3.i2.p1.1">The need for comprehensive information</p>
</div>
</li>
<li class="ltx_item" id="A2.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I3.i3.p1">
<p class="ltx_p" id="A2.I3.i3.p1.1">The importance of up-to-date information</p>
</div>
</li>
<li class="ltx_item" id="A2.I3.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I3.i4.p1">
<p class="ltx_p" id="A2.I3.i4.p1.1">The potential for complementary information from both sources</p>
</div>
</li>
</ul>
<p class="ltx_p" id="A2.SSx5.p4.1.6"><span class="ltx_text ltx_font_bold" id="A2.SSx5.p4.1.6.1">Your response should be only one of the following options: a, b, or c. Do not provide any explanation or additional information.</span></p>
<p class="ltx_p" id="A2.SSx5.p4.1.7"><span class="ltx_text ltx_font_bold" id="A2.SSx5.p4.1.7.1"># Query:</span> <span class="ltx_text ltx_font_typewriter" id="A2.SSx5.p4.1.7.2">{query}</span></p>
<p class="ltx_p" id="A2.SSx5.p4.1.8"><span class="ltx_text ltx_font_bold" id="A2.SSx5.p4.1.8.1"># Option:</span></p>
</blockquote>
</div>
<div class="ltx_para" id="A2.SSx5.p5">
<p class="ltx_p" id="A2.SSx5.p5.1">Based on the response from the LLM—whether it selects option <span class="ltx_text ltx_font_italic" id="A2.SSx5.p5.1.1">a</span>, <span class="ltx_text ltx_font_italic" id="A2.SSx5.p5.1.2">b</span>, or <span class="ltx_text ltx_font_italic" id="A2.SSx5.p5.1.3">c</span>—the corresponding knowledge source(s) are then retrieved and provided to the LLM. The model uses this external information to attempt answering the query again.</p>
</div>
<div class="ltx_para" id="A2.SSx5.p6">
<p class="ltx_p" id="A2.SSx5.p6.1">This approach leverages the strengths of different data sources, allowing the LLM to dynamically adapt to the requirements of each query, ensuring that it provides the most accurate and contextually appropriate responses possible.</p>
</div>
</section>
<section class="ltx_subsection" id="A2.SSx6">
<h3 class="ltx_title ltx_title_subsection">Noise Chunk Selection Strategy</h3>
<div class="ltx_para" id="A2.SSx6.p1">
<p class="ltx_p" id="A2.SSx6.p1.1">In our experiments, noise chunks were introduced into the Retrieval-Augmented Generation (RAG) system to study their impact on performance. The strategy for selecting noise chunks involved randomly selecting a few chunks from the entire set of available chunks before the retrieval process. These randomly chosen chunks were then deliberately inserted into the set of chunks retrieved after the retrieval process, introducing irrelevant or misleading information into the system.</p>
</div>
</section>
<section class="ltx_subsection" id="A2.SSx7">
<h3 class="ltx_title ltx_title_subsection">Guidelines for Extracting Information from the Mock API Responses</h3>
<div class="ltx_para" id="A2.SSx7.p1">
<p class="ltx_p" id="A2.SSx7.p1.1">The mock API utilized in our Retrieval-Augmented Generation (RAG) system is provided by the CRAG benchmark. We designed rules to extract valid and relevant information from the mock API responses. Below is a summary of the key rules for each domain.</p>
</div>
<section class="ltx_paragraph" id="A2.SSx7.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Time-Specific Guidelines</h5>
<div class="ltx_para" id="A2.SSx7.SSS0.Px1.p1">
<p class="ltx_p" id="A2.SSx7.SSS0.Px1.p1.1">The knowledge graph used by the mock API covers data from 1958 to 2019. All time-related information processed adheres to this range.</p>
</div>
</section>
<section class="ltx_paragraph" id="A2.SSx7.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Movie Domain</h5>
<div class="ltx_para" id="A2.SSx7.SSS0.Px2.p1">
<p class="ltx_p" id="A2.SSx7.SSS0.Px2.p1.1">We extract movie titles, director names, and actor names from the API responses. For each movie, we retrieve details such as title, original title, director(s), genres, language, revenue, budget, length, and Oscar Awards information (category, year, and status). Missing data is marked as <span class="ltx_text ltx_font_italic" id="A2.SSx7.SSS0.Px2.p1.1.1">Unknown</span>. For individuals, we list movies they acted in or directed, check title accuracy, and retrieve Oscar Awards information for specified years.</p>
</div>
</section>
<section class="ltx_paragraph" id="A2.SSx7.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Finance Domain</h5>
<div class="ltx_para" id="A2.SSx7.SSS0.Px3.p1">
<p class="ltx_p" id="A2.SSx7.SSS0.Px3.p1.1">Stock information is categorized into <span class="ltx_text ltx_font_italic" id="A2.SSx7.SSS0.Px3.p1.1.1">stock prices</span> and <span class="ltx_text ltx_font_italic" id="A2.SSx7.SSS0.Px3.p1.1.2">other stock-related information</span>. We retrieve single-day and multi-day stock prices, including opening, closing, highest, lowest prices, and trading volume. Other stock information includes financial metrics like market capitalization, EPS, P/E ratio, and dividend yield. Relevant dates are filtered, and prices are compared chronologically.</p>
</div>
</section>
<section class="ltx_paragraph" id="A2.SSx7.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Music Domain</h5>
<div class="ltx_para" id="A2.SSx7.SSS0.Px4.p1">
<p class="ltx_p" id="A2.SSx7.SSS0.Px4.p1.1">We extract song names, artist names, band names, and dates from the API. For each song, we retrieve the author, release date, release country, and Grammy Award count. For artists, we retrieve biography details and lists of works, sorted by year. Band information includes current members and historical data when relevant. Grammy Awards information is retrieved for specified years.</p>
</div>
</section>
<section class="ltx_paragraph" id="A2.SSx7.SSS0.Px5">
<h5 class="ltx_title ltx_title_paragraph">Sports Domain</h5>
<div class="ltx_para" id="A2.SSx7.SSS0.Px5.p1">
<p class="ltx_p" id="A2.SSx7.SSS0.Px5.p1.1">We extract NBA and soccer team names, check for exact or alias matches, and process dates. Game information is retrieved and formatted based on specified dates, including win-loss records, total points, and relevant statistics. All relevant game details are presented concisely.</p>
</div>
<div class="ltx_para" id="A2.SSx7.SSS0.Px5.p2">
<p class="ltx_p" id="A2.SSx7.SSS0.Px5.p2.1">This approach ensures efficient and accurate extraction of information from the mock API, providing comprehensive responses across domains.</p>
</div>
</section>
</section>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Appendix C Computing Infrastructure</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.3">All the experiments are conducted on 2 <math alttext="\times" class="ltx_Math" display="inline" id="A3.p1.1.m1.1"><semantics id="A3.p1.1.m1.1a"><mo id="A3.p1.1.m1.1.1" xref="A3.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A3.p1.1.m1.1b"><times id="A3.p1.1.m1.1.1.cmml" xref="A3.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="A3.p1.1.m1.1d">×</annotation></semantics></math> Nvidia GeForce RTX 4090 GPUs (24GB memory each). Other configuration includes 2 <math alttext="\times" class="ltx_Math" display="inline" id="A3.p1.2.m2.1"><semantics id="A3.p1.2.m2.1a"><mo id="A3.p1.2.m2.1.1" xref="A3.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A3.p1.2.m2.1b"><times id="A3.p1.2.m2.1.1.cmml" xref="A3.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="A3.p1.2.m2.1d">×</annotation></semantics></math> Intel Xeon Gold 6426Y CPUs, 503GB DDR4 RAM, and 1 <math alttext="\times" class="ltx_Math" display="inline" id="A3.p1.3.m3.1"><semantics id="A3.p1.3.m3.1a"><mo id="A3.p1.3.m3.1.1" xref="A3.p1.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A3.p1.3.m3.1b"><times id="A3.p1.3.m3.1.1.cmml" xref="A3.p1.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="A3.p1.3.m3.1d">×</annotation></semantics></math> 893.8GB SATA SSD, which is sufficient for all the baselines.</p>
</div>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Appendix D Case Study</h2>
<section class="ltx_subsection" id="A4.SSx1">
<h3 class="ltx_title ltx_title_subsection"> Correct Answer</h3>
<div class="ltx_para" id="A4.SSx1.p1">
<p class="ltx_p" id="A4.SSx1.p1.1">In this example, our RAG system was queried about the birth date of Randall Wallace. The system successfully retrieved relevant contexts that directly mentioned his birth date. By leveraging these accurate contexts, the system provided the correct answer, “July 28, 1949” which matches the ground truth date “1949-07-28”. This demonstrates the system’s capability to correctly identify and use precise information when the relevant data is available in the retrieved context.</p>
</div>
<div class="ltx_para" id="A4.SSx1.p2">
<p class="ltx_p" id="A4.SSx1.p2.1">As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#A4.F9" title="Figure 9 ‣ Correct Answer ‣ Appendix D Appendix D Case Study ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">9</span></a>, the effectiveness of the RAG approach in this instance highlights the potential of retrieval-augmented methods to enhance the accuracy of answers by supplementing the model’s internal knowledge with precise external information.</p>
</div>
<figure class="ltx_figure" id="A4.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="623" id="A4.F9.g1" src="x9.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>An example where our RAG successfully retrieved and utilized the correct context to answer the query.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A4.SSx2">
<h3 class="ltx_title ltx_title_subsection"> Missing Answer</h3>
<div class="ltx_para" id="A4.SSx2.p1">
<p class="ltx_p" id="A4.SSx2.p1.1">In this case, the RAG system was asked to determine the number of number-one hit songs by Lady Gaga. Despite retrieving relevant contexts that discussed Lady Gaga’s albums and songs, the system failed to extract the specific information needed to answer the query. Consequently, the system responded with “I don’t know.” even though the correct answer was 7.</p>
</div>
<div class="ltx_para" id="A4.SSx2.p2">
<p class="ltx_p" id="A4.SSx2.p2.1">As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#A4.F10" title="Figure 10 ‣ Missing Answer ‣ Appendix D Appendix D Case Study ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">10</span></a>, this example underscores a limitation in the system’s ability to synthesize information when the retrieved contexts do not directly address the query. This highlights the challenge of ensuring that retrieved data is both relevant and specific enough to enable accurate responses.</p>
</div>
<figure class="ltx_figure" id="A4.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="623" id="A4.F10.g1" src="x10.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>An example of a missed response by our RAG. </figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A4.SSx3">
<h3 class="ltx_title ltx_title_subsection"> Incorrect Answer</h3>
<div class="ltx_para" id="A4.SSx3.p1">
<p class="ltx_p" id="A4.SSx3.p1.1">In this example, the RAG system was asked to identify the year Emma Watson was born in England. The retrieved contexts provided her birth date, but they did not address the location of her birth. Consequently, the system incorrectly inferred that the answer was 1990, based solely on the birth year provided in the context. However, the ground truth indicates that the question is invalid because Emma Watson was actually born in France, not England.</p>
</div>
<div class="ltx_para" id="A4.SSx3.p2">
<p class="ltx_p" id="A4.SSx3.p2.1">As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13694v1#A4.F11" title="Figure 11 ‣ Incorrect Answer ‣ Appendix D Appendix D Case Study ‣ A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">11</span></a>, this case highlights a significant challenge in the RAG system: the ability to discern and validate the context’s relevance to the specific query, particularly when the query contains geographical or contextual nuances that are not directly addressed by the retrieved information.</p>
</div>
<figure class="ltx_figure" id="A4.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="623" id="A4.F11.g1" src="x11.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>An example of an incorrect response generated by our RAG. </figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A4.SSx4">
<h3 class="ltx_title ltx_title_subsection">Prompt Without Chain of Thought (CoT) and No Internal Knowledge</h3>
<div class="ltx_para" id="A4.SSx4.p1">
<p class="ltx_p" id="A4.SSx4.p1.1">In this experiment, we designed a prompt that intentionally avoids the use of Chain of Thought (CoT) reasoning and strictly limits the model from using any internal knowledge. The purpose of this prompt is to ensure that the model’s answers are derived solely from the external references provided, which include both web and knowledge graph (KG) information. This approach emphasizes reliance on external data, testing the model’s ability to produce concise and accurate responses without additional reasoning steps.</p>
</div>
<div class="ltx_para" id="A4.SSx4.p2">
<blockquote class="ltx_quote" id="A4.SSx4.p2.1">
<p class="ltx_p" id="A4.SSx4.p2.1.1">You are given a <span class="ltx_text ltx_font_bold" id="A4.SSx4.p2.1.1.1">Question</span>, <span class="ltx_text ltx_font_bold" id="A4.SSx4.p2.1.1.2">References</span>, and the time when it was asked in the Pacific Time Zone (PT), referred to as ‘Query Time’. The query time is formatted as “mm/dd/yyyy, hh:mm:ss PT”. The references may or may not help answer the question. Your task is to answer the question in as few words as possible.</p>
<p class="ltx_p" id="A4.SSx4.p2.1.2">Please follow these guidelines when formulating your answer:</p>
<ol class="ltx_enumerate" id="A4.I1">
<li class="ltx_item" id="A4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="A4.I1.i1.p1">
<p class="ltx_p" id="A4.I1.i1.p1.1">If the question contains a false premise or assumption, answer <span class="ltx_text ltx_font_bold" id="A4.I1.i1.p1.1.1">“invalid question”</span>.</p>
</div>
</li>
<li class="ltx_item" id="A4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="A4.I1.i2.p1">
<p class="ltx_p" id="A4.I1.i2.p1.1">If you are uncertain or don’t know the answer, respond with <span class="ltx_text ltx_font_bold" id="A4.I1.i2.p1.1.1">“I don’t know”</span>.</p>
</div>
</li>
<li class="ltx_item" id="A4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="A4.I1.i3.p1">
<p class="ltx_p" id="A4.I1.i3.p1.1">If the references do not contain the necessary information to answer the question, respond with <span class="ltx_text ltx_font_typewriter" id="A4.I1.i3.p1.1.1">I don’t know</span>.</p>
</div>
</li>
<li class="ltx_item" id="A4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="A4.I1.i4.p1">
<p class="ltx_p" id="A4.I1.i4.p1.1">Using <span class="ltx_text ltx_font_bold" id="A4.I1.i4.p1.1.1">only the references below</span> and not prior knowledge, if there is no reference, respond with <span class="ltx_text ltx_font_typewriter" id="A4.I1.i4.p1.1.2">I don’t know</span>.</p>
</div>
</li>
<li class="ltx_item" id="A4.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="A4.I1.i5.p1">
<p class="ltx_p" id="A4.I1.i5.p1.1">Your answer should be <span class="ltx_text ltx_font_bold" id="A4.I1.i5.p1.1.1">short and concise</span>, using as few words as possible.</p>
</div>
</li>
</ol>
<p class="ltx_p" id="A4.SSx4.p2.1.3"><span class="ltx_text ltx_font_bold" id="A4.SSx4.p2.1.3.1"># Question:</span> <span class="ltx_text ltx_font_typewriter" id="A4.SSx4.p2.1.3.2">{query}</span>
<br class="ltx_break"/>
<span class="ltx_text ltx_font_bold" id="A4.SSx4.p2.1.3.3"># Query Time:</span> <span class="ltx_text ltx_font_typewriter" id="A4.SSx4.p2.1.3.4">{query_time}</span>
<br class="ltx_break"/>
<span class="ltx_text ltx_font_bold" id="A4.SSx4.p2.1.3.5"># References:</span></p>
<ul class="ltx_itemize" id="A4.I2">
<li class="ltx_item" id="A4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A4.I2.i1.p1">
<p class="ltx_p" id="A4.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A4.I2.i1.p1.1.1">Web Infos:</span> <span class="ltx_text ltx_font_typewriter" id="A4.I2.i1.p1.1.2">{web_infos}</span></p>
</div>
</li>
<li class="ltx_item" id="A4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A4.I2.i2.p1">
<p class="ltx_p" id="A4.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A4.I2.i2.p1.1.1">KG Infos:</span> <span class="ltx_text ltx_font_typewriter" id="A4.I2.i2.p1.1.2">{kg_infos}</span></p>
</div>
</li>
</ul>
<p class="ltx_p" id="A4.SSx4.p2.1.4"><span class="ltx_text ltx_font_bold" id="A4.SSx4.p2.1.4.1"># Answer:</span></p>
</blockquote>
</div>
<div class="ltx_para" id="A4.SSx4.p3">
<p class="ltx_p" id="A4.SSx4.p3.1">This prompt serves as a controlled test of the model’s dependency on external references, eliminating any influence from internal knowledge or complex reasoning processes. By focusing purely on the given references, the experiment assesses the model’s ability to generate accurate and concise responses within these constraints.</p>
</div>
</section>
<section class="ltx_subsection" id="A4.SSx5">
<h3 class="ltx_title ltx_title_subsection">Prompt with Chain of Thought (CoT) and Use of Internal Knowledge</h3>
<div class="ltx_para" id="A4.SSx5.p1">
<p class="ltx_p" id="A4.SSx5.p1.1">In this section, we introduce a prompt designed to enhance the reasoning capabilities of the model by explicitly incorporating Chain of Thought (CoT) reasoning. This prompt guides the model to think step by step, ensuring a more structured and transparent approach to problem-solving. By dividing the response into a thought process and a final answer, this prompt aims to improve the accuracy and relevance of the model’s output.</p>
</div>
<div class="ltx_para" id="A4.SSx5.p2">
<blockquote class="ltx_quote" id="A4.SSx5.p2.1">
<p class="ltx_p" id="A4.SSx5.p2.1.1">You are given a <span class="ltx_text ltx_font_bold" id="A4.SSx5.p2.1.1.1">Question</span>, <span class="ltx_text ltx_font_bold" id="A4.SSx5.p2.1.1.2">References</span>, and the time when it was asked in the Pacific Time Zone (PT), referred to as ‘Query Time’. The query time is formatted as “mm/dd/yyyy, hh:mm:ss PT”. The references may or may not help answer the question, please think step by step, then provide the final answer.</p>
<p class="ltx_p" id="A4.SSx5.p2.1.2">Please follow these guidelines when formulating your answer:</p>
<ol class="ltx_enumerate" id="A4.I3">
<li class="ltx_item" id="A4.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="A4.I3.i1.p1">
<p class="ltx_p" id="A4.I3.i1.p1.1">If the question contains a false premise or assumption, answer <span class="ltx_text ltx_font_bold" id="A4.I3.i1.p1.1.1">“invalid question”</span>.</p>
</div>
</li>
<li class="ltx_item" id="A4.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="A4.I3.i2.p1">
<p class="ltx_p" id="A4.I3.i2.p1.1">If you are uncertain or don’t know the answer, respond with <span class="ltx_text ltx_font_bold" id="A4.I3.i2.p1.1.1">“I don’t know”</span>.</p>
</div>
</li>
<li class="ltx_item" id="A4.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="A4.I3.i3.p1">
<p class="ltx_p" id="A4.I3.i3.p1.1">Your final answer should be <span class="ltx_text ltx_font_bold" id="A4.I3.i3.p1.1.1">short and concise</span>, using as few words as possible.</p>
</div>
</li>
<li class="ltx_item" id="A4.I3.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="A4.I3.i4.p1">
<p class="ltx_p" id="A4.I3.i4.p1.1">Your output format needs to meet the requirements: First, start with <span class="ltx_text ltx_font_bold" id="A4.I3.i4.p1.1.1">Thought:</span> and then output the thought process regarding the user’s question. After you finish thinking, you <span class="ltx_text ltx_font_bold" id="A4.I3.i4.p1.1.2">MUST</span> reply with the final answer on the last line, starting with <span class="ltx_text ltx_font_bold" id="A4.I3.i4.p1.1.3">Final Answer:</span> and using as few words as possible.</p>
</div>
</li>
</ol>
<p class="ltx_p" id="A4.SSx5.p2.1.3"><span class="ltx_text ltx_font_bold" id="A4.SSx5.p2.1.3.1"># Question:</span> <span class="ltx_text ltx_font_typewriter" id="A4.SSx5.p2.1.3.2">{query}</span>
<br class="ltx_break"/>
<span class="ltx_text ltx_font_bold" id="A4.SSx5.p2.1.3.3"># Query Time:</span> <span class="ltx_text ltx_font_typewriter" id="A4.SSx5.p2.1.3.4">{query_time}</span>
<br class="ltx_break"/>
<span class="ltx_text ltx_font_bold" id="A4.SSx5.p2.1.3.5"># References:</span> <span class="ltx_text ltx_font_typewriter" id="A4.SSx5.p2.1.3.6">{references}</span>
<br class="ltx_break"/>
<span class="ltx_text ltx_font_bold" id="A4.SSx5.p2.1.3.7"># Thought:</span></p>
</blockquote>
</div>
<div class="ltx_para" id="A4.SSx5.p3">
<p class="ltx_p" id="A4.SSx5.p3.1">In contrast to the first prompt, which did not employ Chain of Thought (CoT) reasoning, this prompt explicitly directs the model to engage in a step-by-step reasoning process. By instructing the model to think through the problem in a structured manner, this prompt facilitates a more deliberate and transparent approach to problem-solving.</p>
</div>
<div class="ltx_para" id="A4.SSx5.p4">
<p class="ltx_p" id="A4.SSx5.p4.1">The two-section format—comprising the thought process (Thought) and the final answer (Final Answer)—is key to this methodology. The model is encouraged to carefully consider the provided references, evaluate their relevance, and articulate a clear, concise conclusion.</p>
</div>
<div class="ltx_para" id="A4.SSx5.p5">
<p class="ltx_p" id="A4.SSx5.p5.1">This approach aims to enhance the model’s ability to handle complex queries by breaking them down into manageable steps. The incorporation of CoT is expected to lead to more accurate and contextually appropriate responses. Additionally, by emphasizing brevity in the final answer, the prompt ensures that the model’s response remains both concise and informative, aligning with the goal of delivering clear and precise answers.</p>
</div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Sep  3 03:10:03 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
