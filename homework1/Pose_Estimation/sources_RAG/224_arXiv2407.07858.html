<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>FACTS About Building Retrieval Augmented Generation-based Chatbots</title>
<!--Generated on Wed Jul 10 17:07:04 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2407.07858v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#S1" title="In FACTS About Building Retrieval Augmented Generation-based Chatbots"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#S2" title="In FACTS About Building Retrieval Augmented Generation-based Chatbots"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Case Study</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#S3" title="In FACTS About Building Retrieval Augmented Generation-based Chatbots"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Ensuring Freshness of Enterprise Data in LLM-powered Chatbots (F)</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#S3.SS1" title="In 3. Ensuring Freshness of Enterprise Data in LLM-powered Chatbots (F) ‣ FACTS About Building Retrieval Augmented Generation-based Chatbots"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Learnings</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#S4" title="In FACTS About Building Retrieval Augmented Generation-based Chatbots"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Building Flexible Architectures for generative AI chatbots (A)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#S5" title="In FACTS About Building Retrieval Augmented Generation-based Chatbots"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Cost Economics of Chatbot deployments (C)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#S6" title="In FACTS About Building Retrieval Augmented Generation-based Chatbots"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Testing RAG-based Chatbots (T)</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#S6.SS1" title="In 6. Testing RAG-based Chatbots (T) ‣ FACTS About Building Retrieval Augmented Generation-based Chatbots"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Learnings</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#S7" title="In FACTS About Building Retrieval Augmented Generation-based Chatbots"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Securing RAG-based Chatbots (S)</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#S7.SS1" title="In 7. Securing RAG-based Chatbots (S) ‣ FACTS About Building Retrieval Augmented Generation-based Chatbots"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Learnings</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#S8" title="In FACTS About Building Retrieval Augmented Generation-based Chatbots"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Related work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#S9" title="In FACTS About Building Retrieval Augmented Generation-based Chatbots"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9 </span>Conclusions</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">FACTS About Building Retrieval Augmented Generation-based Chatbots</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rama Akkiraju, Anbang Xu, Deepak Bora, Tan Yu, Lu An, Vishal Seth, Aaditya Shukla, Pritam Gundecha, Hridhay Mehta, Ashwin Jha, Prithvi Raj, Abhinav Balasubramanian, Murali Maram, Guru Muthusamy, Shivakesh Reddy Annepally, Sidney Knowles, Min Du, Nick Burnett, Sean Javiya, Ashok Marannan, Mamta Kumari, Surbhi Jha, Ethan Dereszenski, Anupam Chakraborty, Subhash Ranjan, Amina Terfai, Anoop Surya, Tracey Mercer, Vinodh Kumar Thanigachalam, Tamar Bar, Sanjana Krishnan, Jasmine Jaksic, Nave Algarici, Jacob Liberman, Joey Conway, Sonu Nayyar and Justin Boitano
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">NVIDIA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:rakkiraju,%20anbangx,%20dbora@nvidia.com">rakkiraju, anbangx, dbora@nvidia.com</a>
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id2.id1">Enterprise chatbots, powered by generative AI, are rapidly emerging as the most explored initial applications of this technology in the industry, aimed at enhancing employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), Langchain/Llamaindex types of LLM orchestration frameworks serve as key technological components in building generative-AI based chatbots. However, building successful enterprise chatbots is not easy. They require meticulous engineering of RAG pipelines. This includes fine-tuning semantic embeddings and LLMs, extracting relevant documents from vector databases, rephrasing queries, reranking results, designing effective prompts, honoring document access controls, providing concise responses, including pertinent references, safeguarding personal information, and building agents to orchestrate all these activities. In this paper, we present a framework for building effective RAG-based chatbots based on our first-hand experience of building three chatbots at NVIDIA: chatbots for IT and HR benefits, company financial earnings, and general enterprise content. Our contributions in this paper are three-fold. First, we introduce our FACTS framework for building enterprise-grade RAG-based chatbots that address the challenges mentioned. FACTS mnemonic refers to the five dimensions that RAG-based chatbots must get right - namely content <span class="ltx_text ltx_framed ltx_framed_underline" id="id2.id1.1">f</span>reshness (F), <span class="ltx_text ltx_framed ltx_framed_underline" id="id2.id1.2">a</span>rchitectures(A), <span class="ltx_text ltx_framed ltx_framed_underline" id="id2.id1.3">c</span>ost economics of LLMs (C), <span class="ltx_text ltx_framed ltx_framed_underline" id="id2.id1.4">t</span>esting (T), and <span class="ltx_text ltx_framed ltx_framed_underline" id="id2.id1.5">s</span>ecurity (S). Second, we present fifteen control points of RAG pipelines and techniques for optimizing chatbots’ performance at each stage. Finally, we present empirical results from our enterprise data on the accuracy-latency tradeoffs between large LLMs vs small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.</p>
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>978-1-4503-XXXX-X/18/06</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Chatbots are increasingly becoming an extension of search tools in companies for finding relevant information. Whether it’s HR benefits, IT help, sales queries, or engineering issues, enterprise chatbots are now go-to productivity tools. Before the debut of OpenAI’s Chat-GPT <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#bib.bib2" title="">achiam2023gpt, </a>)</cite> in November 2022, companies relied on internally developed chatbots based on dialog flows. Such bots required extensive training for intent understanding and meticulous orchestration for response generation and yet could only provide extractive answers at best. These early bots, built on dialog management systems paired with information retrieval and question answering (IRQA) solutions were fragile and limited in capability. While previous generation language models and GPT models existed, they lacked the accuracy, robustness, and reliability needed for broad enterprise use <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#bib.bib5" title="">galitsky2019developing, </a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Chat-GPT’s release, the emergence of vector databases, and the wide-spread use of retrieval augmented generation (RAGs) <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#bib.bib8" title="">lewis2020retrieval, </a>)</cite> marked the beginning of a new era in Chatbot domain. Now, LLMs can understand user intents with simple prompts in natural language, eliminating the need for complex intent variant training, synthesize enterprise content coherently, thereby empowering chatbots with conversational capability beyond scripted intent recognition. While LLMs bring their generative capabilities to construct coherent, factual, and logical responses to user queries, vector database-powered information retrieval (IR) systems augment LLMs ability to retrieve fresh content. Tools like LangChain <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#bib.bib1" title="">Langchain, </a>)</cite> and Llamaindex <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#bib.bib9" title="">Liu_LlamaIndex_2022, </a>)</cite> facilitate chatbot construction, and orchestration of complex workflows including memory, agents, prompt templates, and overall flow. Together, vector-search based IR systems, LLMs, and LangChain-like frameworks form core components of a RAG pipeline and are powering generative AI chatbots in post Chat-GPT era.</p>
</div>
<figure class="ltx_table" id="S1.T1">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1. </span> A summary of the three chatbots and the current state of development. </figcaption>
<table class="ltx_tabular ltx_align_middle" id="S1.T1.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S1.T1.3.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.3.1.1.1"><span class="ltx_text" id="S1.T1.3.1.1.1.1" style="font-size:90%;">Chatbot</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.3.1.1.2"><span class="ltx_text" id="S1.T1.3.1.1.2.1" style="font-size:90%;">Domain</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.3.1.1.3"><span class="ltx_text" id="S1.T1.3.1.1.3.1" style="font-size:90%;">Data Sources</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.3.1.1.4"><span class="ltx_text" id="S1.T1.3.1.1.4.1" style="font-size:90%;">Data Types</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.3.1.1.5"><span class="ltx_text" id="S1.T1.3.1.1.5.1" style="font-size:90%;">Access Control</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.3.1.1.6"><span class="ltx_text" id="S1.T1.3.1.1.6.1" style="font-size:90%;">Sample Queries</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.3.1.1.7"><span class="ltx_text" id="S1.T1.3.1.1.7.1" style="font-size:90%;">State</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.3.2.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S1.T1.3.2.2.1">
<span class="ltx_inline-block" id="S1.T1.3.2.2.1.1">
<span class="ltx_p" id="S1.T1.3.2.2.1.1.1"><span class="ltx_text" id="S1.T1.3.2.2.1.1.1.1" style="font-size:90%;">NVInfo</span></span>
<span class="ltx_p" id="S1.T1.3.2.2.1.1.2"><span class="ltx_text" id="S1.T1.3.2.2.1.1.2.1" style="font-size:90%;">Bot</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S1.T1.3.2.2.2">
<span class="ltx_inline-block" id="S1.T1.3.2.2.2.1">
<span class="ltx_p" id="S1.T1.3.2.2.2.1.1"><span class="ltx_text" id="S1.T1.3.2.2.2.1.1.1" style="font-size:90%;">Enterprise Internal</span></span>
<span class="ltx_p" id="S1.T1.3.2.2.2.1.2"><span class="ltx_text" id="S1.T1.3.2.2.2.1.2.1" style="font-size:90%;">Knowledge</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S1.T1.3.2.2.3">
<span class="ltx_inline-block" id="S1.T1.3.2.2.3.1">
<span class="ltx_p" id="S1.T1.3.2.2.3.1.1"><span class="ltx_text" id="S1.T1.3.2.2.3.1.1.1" style="font-size:90%;">SharePoint, GoogleDrive, Slack</span></span>
<span class="ltx_p" id="S1.T1.3.2.2.3.1.2"><span class="ltx_text" id="S1.T1.3.2.2.3.1.2.1" style="font-size:90%;">Confluence, ServiceNow, Jira </span><em class="ltx_emph ltx_font_italic" id="S1.T1.3.2.2.3.1.2.2" style="font-size:90%;">etc.</em></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S1.T1.3.2.2.4">
<span class="ltx_inline-block" id="S1.T1.3.2.2.4.1">
<span class="ltx_p" id="S1.T1.3.2.2.4.1.1"><span class="ltx_text" id="S1.T1.3.2.2.4.1.1.1" style="font-size:90%;">Docs, HTML</span></span>
<span class="ltx_p" id="S1.T1.3.2.2.4.1.2"><span class="ltx_text" id="S1.T1.3.2.2.4.1.2.1" style="font-size:90%;">PDFs, Slides</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S1.T1.3.2.2.5"><span class="ltx_text" id="S1.T1.3.2.2.5.1" style="font-size:90%;">Yes</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S1.T1.3.2.2.6">
<span class="ltx_inline-block" id="S1.T1.3.2.2.6.1">
<span class="ltx_p" id="S1.T1.3.2.2.6.1.1"><span class="ltx_text" id="S1.T1.3.2.2.6.1.1.1" style="font-size:90%;">Can I park overnight</span></span>
<span class="ltx_p" id="S1.T1.3.2.2.6.1.2"><span class="ltx_text" id="S1.T1.3.2.2.6.1.2.1" style="font-size:90%;">in HQ parking lots?</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S1.T1.3.2.2.7">
<span class="ltx_inline-block" id="S1.T1.3.2.2.7.1">
<span class="ltx_p" id="S1.T1.3.2.2.7.1.1"><span class="ltx_text" id="S1.T1.3.2.2.7.1.1.1" style="font-size:90%;">Early Access</span></span>
<span class="ltx_p" id="S1.T1.3.2.2.7.1.2"><span class="ltx_text" id="S1.T1.3.2.2.7.1.2.1" style="font-size:90%;">Testing</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.3.3.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S1.T1.3.3.3.1">
<span class="ltx_inline-block" id="S1.T1.3.3.3.1.1">
<span class="ltx_p" id="S1.T1.3.3.3.1.1.1"><span class="ltx_text" id="S1.T1.3.3.3.1.1.1.1" style="font-size:90%;">NVHelp</span></span>
<span class="ltx_p" id="S1.T1.3.3.3.1.1.2"><span class="ltx_text" id="S1.T1.3.3.3.1.1.2.1" style="font-size:90%;">Bot</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S1.T1.3.3.3.2">
<span class="ltx_inline-block" id="S1.T1.3.3.3.2.1">
<span class="ltx_p" id="S1.T1.3.3.3.2.1.1"><span class="ltx_text" id="S1.T1.3.3.3.2.1.1.1" style="font-size:90%;">IT Help</span></span>
<span class="ltx_p" id="S1.T1.3.3.3.2.1.2"><span class="ltx_text" id="S1.T1.3.3.3.2.1.2.1" style="font-size:90%;">HR Benefits</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S1.T1.3.3.3.3">
<span class="ltx_inline-block" id="S1.T1.3.3.3.3.1">
<span class="ltx_p" id="S1.T1.3.3.3.3.1.1"><span class="ltx_text" id="S1.T1.3.3.3.3.1.1.1" style="font-size:90%;">Knowledge Articles for ITHelp</span></span>
<span class="ltx_p" id="S1.T1.3.3.3.3.1.2"><span class="ltx_text" id="S1.T1.3.3.3.3.1.2.1" style="font-size:90%;">HR benefits pages</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S1.T1.3.3.3.4">
<span class="ltx_inline-block" id="S1.T1.3.3.3.4.1">
<span class="ltx_p" id="S1.T1.3.3.3.4.1.1"><span class="ltx_text" id="S1.T1.3.3.3.4.1.1.1" style="font-size:90%;">Text, PDFs</span></span>
<span class="ltx_p" id="S1.T1.3.3.3.4.1.2"><span class="ltx_text" id="S1.T1.3.3.3.4.1.2.1" style="font-size:90%;">Docs</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S1.T1.3.3.3.5"><span class="ltx_text" id="S1.T1.3.3.3.5.1" style="font-size:90%;">Yes</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S1.T1.3.3.3.6">
<span class="ltx_inline-block" id="S1.T1.3.3.3.6.1">
<span class="ltx_p" id="S1.T1.3.3.3.6.1.1"><span class="ltx_text" id="S1.T1.3.3.3.6.1.1.1" style="font-size:90%;">How to enroll in Employee</span></span>
<span class="ltx_p" id="S1.T1.3.3.3.6.1.2"><span class="ltx_text" id="S1.T1.3.3.3.6.1.2.1" style="font-size:90%;">Stock Purchase plan?</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S1.T1.3.3.3.7"><span class="ltx_text" id="S1.T1.3.3.3.7.1" style="font-size:90%;">Production</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.3.4.4">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt" id="S1.T1.3.4.4.1">
<span class="ltx_inline-block" id="S1.T1.3.4.4.1.1">
<span class="ltx_p" id="S1.T1.3.4.4.1.1.1"><span class="ltx_text" id="S1.T1.3.4.4.1.1.1.1" style="font-size:90%;">Scout</span></span>
<span class="ltx_p" id="S1.T1.3.4.4.1.1.2"><span class="ltx_text" id="S1.T1.3.4.4.1.1.2.1" style="font-size:90%;">Bot</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt" id="S1.T1.3.4.4.2"><span class="ltx_text" id="S1.T1.3.4.4.2.1" style="font-size:90%;">Financial Earnings</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt" id="S1.T1.3.4.4.3">
<span class="ltx_inline-block" id="S1.T1.3.4.4.3.1">
<span class="ltx_p" id="S1.T1.3.4.4.3.1.1"><span class="ltx_text" id="S1.T1.3.4.4.3.1.1.1" style="font-size:90%;">Company news, blogs, SEC filings</span></span>
<span class="ltx_p" id="S1.T1.3.4.4.3.1.2"><span class="ltx_text" id="S1.T1.3.4.4.3.1.2.1" style="font-size:90%;">Earnings-related interviews</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt" id="S1.T1.3.4.4.4">
<span class="ltx_inline-block" id="S1.T1.3.4.4.4.1">
<span class="ltx_p" id="S1.T1.3.4.4.4.1.1"><span class="ltx_text" id="S1.T1.3.4.4.4.1.1.1" style="font-size:90%;">HTML, PDFs</span></span>
<span class="ltx_p" id="S1.T1.3.4.4.4.1.2"><span class="ltx_text" id="S1.T1.3.4.4.4.1.2.1" style="font-size:90%;">Docs</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt" id="S1.T1.3.4.4.5"><span class="ltx_text" id="S1.T1.3.4.4.5.1" style="font-size:90%;">No</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt" id="S1.T1.3.4.4.6">
<span class="ltx_inline-block" id="S1.T1.3.4.4.6.1">
<span class="ltx_p" id="S1.T1.3.4.4.6.1.1"><span class="ltx_text" id="S1.T1.3.4.4.6.1.1.1" style="font-size:90%;">What are NVIDIA revenues</span></span>
<span class="ltx_p" id="S1.T1.3.4.4.6.1.2"><span class="ltx_text" id="S1.T1.3.4.4.6.1.2.1" style="font-size:90%;">for the past 3 years?</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_tt" id="S1.T1.3.4.4.7"><span class="ltx_text" id="S1.T1.3.4.4.7.1" style="font-size:90%;">Production</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">At NVIDIA, our main motivation was to improve our employee productivity by building enterprise chatbots. Our initial enthusiasm quickly met with the reality of addressing numerous challenges. We learned that crafting a successful enterprise chatbot, even in post Chat-GPT era, while promising, is not easy. The process demands meticulous engineering of RAG pipelines, fine-tuning LLMs, and engineering prompts, ensuring relevancy and accuracy of enterprise knowledge, honoring document access control permissions, providing concise responses, including pertinent references, and safeguarding personal information. All of these require careful design, skillful execution, and thorough evaluation, demanding many iterations. Additionally, maintaining user engagement while optimizing for speed and cost-efficiency is essential. Through our journey, we learned that getting an enterprise conversational virtual assistant right is akin to achieving a perfect symphony where every note carries significance!</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this paper, we share our experiences and strategies in building effective, secure, and cost-efficient chatbots. We answer the following questions from a practitioner perspective:</p>
</div>
<div class="ltx_para" id="S1.p5">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S1.I1.i1.p1.1.1">What are the key challenges to consider when building and deploying enterprise-grade generative AI-based chatbots?</span> We present our findings from trying to deliver <span class="ltx_text ltx_font_italic ltx_framed ltx_framed_underline" id="S1.I1.i1.p1.1.2">f</span>resh content (F) with flexible <span class="ltx_text ltx_font_italic ltx_framed ltx_framed_underline" id="S1.I1.i1.p1.1.3">a</span>rchitectures (A) that are <span class="ltx_text ltx_font_italic ltx_framed ltx_framed_underline" id="S1.I1.i1.p1.1.4">c</span>ost-efficient (C), <span class="ltx_text ltx_font_italic ltx_framed ltx_framed_underline" id="S1.I1.i1.p1.1.5">t</span>ested well (T), and <span class="ltx_text ltx_font_italic ltx_framed ltx_framed_underline" id="S1.I1.i1.p1.1.6">s</span>ecure (S) - (FACTS).</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S1.I1.i2.p1.1.1">How to achieve user acceptable levels of quality with RAG systems in building chatbots?</span> We present the fifteen control points of RAG pipelines and techniques for optimizing each control point and the overall RAG pipeline.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Case Study</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Our company’s content landscape includes both authoritative knowledge and unauthoritative content. Authoritative content encompasses IT help articles, HR resources in platforms like ServiceNow, and project documentation on Confluence, SharePoint, Google Drive, and engineering tools like NVBugs and GitHub. Employee-generated content complements these sources on platforms such as Slack and MS Teams. In this paper, we present three bots that we have built at NVIDIA using RAGs and LLMs. These bots are briefly introduced below. All three bots are built on our in-house built generative-AI chatbot platform called NVBot platform. Some of the queries that our bots are capable of answering are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#S1.T1" title="Table 1 ‣ 1. Introduction ‣ FACTS About Building Retrieval Augmented Generation-based Chatbots"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_para" id="S2.p2">
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i1.p1.1.1">NVInfo Bot</span> answers questions about enterprise content (approx. 500M documents of size ¿ 7 TB), complementing intranet search. It manages diverse data formats and enforces document access controls. The tech stack includes LangChain, a vendor vector database for retrieval and to handle document access controls, LLM model (multiple LLM models can be selected), and a custom web-UI.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i2.p1.1.1">NVHelp Bot</span> Bot focuses on IT help and HR benefits (approx. 2K multi-modal documents containing text, tables, images, pdfs, and html pages), using a similar tech stack to NVInfo bot with a smaller data volume.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i3.p1.1.1">Scout Bot</span> handles questions about financial earnings from public sources, managing structured and unstructured data (approx. 4K multi-modal documents containing text, tables, pdfs, and html pages). The tech stack includes an Open source Vector DB, LangChain, Ragas evaluation, selectable LLM models, and a custom web-UI.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">In the remainder of the paper, we present our FACTS framework that summarizes the challenges experienced and the learnings gained in building the aforementioned three chatbots. We first start with the challenge of dealing with delivering fresh enterprise content in each of the chatbots.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="315" id="S2.F1.g1" src="extracted/5723350/figures/RAG_arch_new.jpg" width="479"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Control Points in a typical RAG pipeline when building Chatbots.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Ensuring Freshness of Enterprise Data in LLM-powered Chatbots (F)</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Ensuring the freshness of enterprise data in LLM-powered chatbots presents several challenges. Foundation models, although powerful, often fall short as they lack domain-specific and enterprise-specific knowledge. Once trained, these models are essentially frozen in time and may hallucinate, providing undesired or inaccurate information when used on enterprise content that they are not trained on.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">Retrieval Augmented Generation (RAG) is a process where relevant information is retrieved from vector databases through semantic matching and then fed to LLMs for response generation. In a RAG pipeline, vector databases and LLMs collaboratively ensure the delivery of up-to-date enterprise knowledge. However, RAG pipelines have many control points, each of which when not tuned well can lead to lower accuracy, hallucinations, and irrelevant responses by Chatbots. Additionally, document access control permissions complicate the search and retrieval process, requiring careful management to ensure data security and relevance. Furthermore, multi-modal content necessitates the use of multi-modal retrievers to handle structured, unstructured, and semi-structured data, including presentations, diagrams, videos, and meeting recordings. Addressing these challenges is critical for maintaining the accuracy and reliability of enterprise chatbots. Inspired by <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#bib.bib3" title="">barnett2024seven, </a>)</cite>, we identify fifteen control points of RAG from our case studies visualized in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#S2.F1" title="Figure 1 ‣ 2. Case Study ‣ FACTS About Building Retrieval Augmented Generation-based Chatbots"><span class="ltx_text ltx_ref_tag">1</span></a>. Each control point is labeled with a number. In the remainder of this section, we present our insights and learnings for addressing RAG control points.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Learnings</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">In figure <a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#S3.F4" title="Figure 4 ‣ 3.1. Learnings ‣ 3. Ensuring Freshness of Enterprise Data in LLM-powered Chatbots (F) ‣ FACTS About Building Retrieval Augmented Generation-based Chatbots"><span class="ltx_text ltx_ref_tag">4</span></a>, we present a summary description of the fifteen control points of RAG pipelines, challenges associated with each control point, and our suggested approaches for optimizing each control point. Each control point is labeled as RAG-C[num] and RAG-Op[num] for RAG and RAGOps flows, respectively. Below, we present a few key learnings and insights to manage the fresh enterprise content.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">Metadata Enrichment, Chunking, Query Rephrasal, Query Reranking</span>: We noticed that metadata enrichment, chunking, query rephrasal and query re-ranking stages of RAG pipeline have the most impact on the quality of Chatbot responses. LLM response generation quality is highly dependent on retrieval relevancy. Retrieval relevancy is, in turn, highly dependent on document metadata enrichment, chunking, and query rephrasal. We implemented grid search-based auto-ML capabilities to find the right configurations of chunk token-sizes, experimented with various prompt variations, and explored different chunk reranking strategies to find optimal settings for each. While we have made significant improvements in retrieval relevancy and answer quality and accuracy, we believe, we still have more work to do to optimize the full pipeline.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.1">Hybrid Search</span>: We noticed that Vector databases are not so good at handling matching entities (e.g., people names, places, company names etc.). Using a combination of Lexical search (e.g., elastic search) and vector search provided better retrieval relevancy and more coverage. Setting up an infrastructure that supports hybrid search capabilities, which combines the strengths of both lexical and vector-based searches, can enhance the accuracy and speed of the retrieval process.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="90" id="S3.F2.g1" src="extracted/5723350/figures/Complex_agent_arch.png" width="269"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Agent architecture for handling complex queries</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.1">Agentic Architectures</span>: Questions such as ‘compare the revenue of NVIDIA from Q1 through Q4 of FY2024 and provide an analytical commentary on the key contributing factors that led to the changes in revenues during this time’ require complex agents that are capable of query decomposition and orchestration. Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#S3.F2" title="Figure 2 ‣ 3.1. Learnings ‣ 3. Ensuring Freshness of Enterprise Data in LLM-powered Chatbots (F) ‣ FACTS About Building Retrieval Augmented Generation-based Chatbots"><span class="ltx_text ltx_ref_tag">2</span></a> shows one mechanism we had implemented to deal with such questions in Scout bot. From our experience of building the three bots, we have realized that IR systems and LLMs are insufficient for answering complex queries. Complex agents and multi-agent architectures are needed to handle complex queries.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p5.1.1">To Fine-tune LLMs or not?</span> A key decision is whether to fine-tune LLMs, balancing the use of foundational models with domain-specific customizations. One size doesn’t fit all when it comes to LLMs. Some use cases may work well with foundational models, while others require customization. When considering customization, several options are available, including prompt engineering, P-tuning, parameter-efficient fine-tuning (PEFT), and full fine-tuning (FT). Fine-tuning requires significant investment in data labeling, training, and evaluations, each of which can be time-consuming and costly. Automating testing and quality evaluation processes become critical to ensuring efficiency and accuracy when customizing LLMs. Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#S3.F3" title="Figure 3 ‣ 3.1. Learnings ‣ 3. Ensuring Freshness of Enterprise Data in LLM-powered Chatbots (F) ‣ FACTS About Building Retrieval Augmented Generation-based Chatbots"><span class="ltx_text ltx_ref_tag">3</span></a> shows the accuracy vs latency tradeoff evaluations we have done comparing OpenAI’s GPT-4 model with some of the open-source models on about 245 queries from NVHelp bot domain. Our results show that the Llama3-70B model excels in several aspects of answer quality while maintaining acceptable latency.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="172" id="S3.F3.g1" src="extracted/5723350/figures/NVHelp_metrics.png" width="269"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>NVHelp answer quality and latency metrics comparison among different models</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS1.p6">
<p class="ltx_p" id="S3.SS1.p6.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p6.1.1">Handling multi-modal data</span>: Enterprise data is multi-modal. Handling structured, unstructured, and multi-modal data is crucial for a versatile RAG pipeline. From our experience, if the structure of the document is consistent and known apriori (like those found in EDGAR databases for SEC filings data in financial earnings domain that Scout bot was handling), implementing section-level splitting, using the section titles and subheadings and incoporating those in the context of chunks improves retrieval relevancy. We also found solutions like Unstructured.io, which specialize in extracting and structuring content from PDFs, helpful in parsing and chunking unstructured documents with context.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p7">
<p class="ltx_p" id="S3.SS1.p7.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p7.1.1">RAGOps</span>: Effective health monitoring of RAG pipelines is essential once they are deployed. When answer quality is poor, a thorough error analysis is required to determine whether the issue lies in retrieval relevancy or LLM response generation. To debug retrieval relevancy, developers need detailed information on which chunks were stored in vector databases with their associated metadata, how queries were rephrased, which chunks were retrieved, and how those chunks were ranked. Similarly, if an LLM response is incorrect, it is crucial to review the final prompt used for answer generation. For issues with citations, developers must trace back to the original document links and their corresponding chunks. RAGOps/LLMOps and evaluation frameworks, such as Ragas, are critical for providing the necessary automation to enable rapid iteration during accuracy improvement cycles in RAG pipelines.</p>
</div>
<div class="ltx_para" id="S3.SS1.p8">
<p class="ltx_p" id="S3.SS1.p8.1">More details on each control point are presented in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#S3.F4" title="Figure 4 ‣ 3.1. Learnings ‣ 3. Ensuring Freshness of Enterprise Data in LLM-powered Chatbots (F) ‣ FACTS About Building Retrieval Augmented Generation-based Chatbots"><span class="ltx_text ltx_ref_tag">4</span></a>. In summary, while promising, implementing RAG systems for chatbots demands meticulous planning and continuous evaluation to ensure secure and accurate data retrieval.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="338" id="S3.F4.g1" src="extracted/5723350/figures/RAG_remediations.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>RAG control points, challenges, and remediations</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="234" id="S3.F5.g1" src="extracted/5723350/figures/multipart_query_scout.png" width="269"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>Scout Bot: Multi-part query</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="248" id="S3.F6.g1" src="extracted/5723350/figures/HSA_query_nvhelp.png" width="269"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>NVHelp Bot: Answering questions on HR benefits</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Building Flexible Architectures for generative AI chatbots (A)</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Keeping up with rapid progress in AI is like navigating a fast-moving river. Every aspect, from vector databases and embedding models to LLMs, agentic architectures, low-code/no-code platforms, RAG evaluation frameworks, and prompting techniques, is evolving rapidly. Concurrently, departments within companies are exploring generative AI by building their own chatbots and AI copilots.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">In this dynamic environment, building common, flexible, and adaptive platforms are crucial. At NVIDIA, our chatbot ecosystem has grown significantly, reflecting a trend likely seen in many companies. From building three initial chatbots, we realized the importance of a common platform to avoid duplicated efforts in security, guardrails, authentication, prompts, user interfaces, feedback mechanisms, usage reporting, monitoring, and evaluations.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">To address this, we developed the NVBot platform (Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#S4.F7" title="Figure 7 ‣ 4. Building Flexible Architectures for generative AI chatbots (A) ‣ FACTS About Building Retrieval Augmented Generation-based Chatbots"><span class="ltx_text ltx_ref_tag">7</span></a>), a modular platform with a pluggable architecture. It allows developers to select LLMs, vector databases, embedding models, agents, and RAG evaluation frameworks that best suit their use case. It also provides common components for essential features like security, guardrails, authentication, authorization, user experience, and monitoring. Additionally, the platform supports citizen development, allowing multiple teams to contribute their tested prompts, workflows, guardrails, and fine-tuned models for collective use.</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1">As our ecosystem of bots expanded, we faced a critical question: should organizations build many domain-specific bots, a single enterprise bot, or go with a hybrid approach? Domain-specific chatbots excel in tailored environments, while nterprise-wide chatbots act as generalists, providing a centralized knowledge base for all employees. Through our experience, we realized that there is no need to choose one over the other.</p>
</div>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p" id="S4.p5.1">Novel architectural patterns are emerging where enterprise-wide chatbots act as ‘switchboards’, directing inquiries to specialized bots tuned with domain-specific data. This multibot architecture allows for the concurrent development of specialized chatbots while providing users with a unified interface. Our NVBot platform supports the coexistence and orchestration of multiple chatbots within an enterprise. The debate over a single bot or multiple specialized bots is ongoing. We envision a landscape where domain-specific bots coexist with a centralized information bot, supported by ’copilots’—generative AI capabilities integrated into workplace environments like programming IDEs and collaboration tools. At NVIDIA, we’re actively exploring all three chatbot variations—domain-specific, enterprise-wide, and copilot as generative AI reshapes workplace efficiency and information accessibility.</p>
</div>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="302" id="S4.F7.g1" src="extracted/5723350/figures/NVBot_Platform.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7. </span>Architecture of NVBot platform upon which multiple chatbots are being built.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Cost Economics of Chatbot deployments (C)</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Understanding the cost economics of generative AI-based chatbots involves several critical factors. The high costs of major and commercial LLMs can be unsustainable, with expenses adding up significantly across multiple use cases. Additionally, unseen expenses often accumulate as teams test various LLMs to meet specific needs. Moreover, when using commercial LLM vendor APIs, securing sensitive enterprise data requires guardrails to detect and prevent sensitive data leakage, as well as gateways for audit and legally permitted learning. There are also cost versus latency trade-offs to consider, as large LLMs with long context lengths typically have slower response times, impacting overall efficiency.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text ltx_font_bold" id="S5.p2.1.1">Bigger Vs. Smaller Models</span>: Larger, commercial LLMs, smaller open source LLMs are increasingly becoming viable for many use cases, thereby offering cost effective alternatives to companies. As opensource models are catching up with larger, commercial models, they are increasingly offering close-comparable accuracy, as demonstrated in our NVHelp bot emperical evaluation in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#S3.F3" title="Figure 3 ‣ 3.1. Learnings ‣ 3. Ensuring Freshness of Enterprise Data in LLM-powered Chatbots (F) ‣ FACTS About Building Retrieval Augmented Generation-based Chatbots"><span class="ltx_text ltx_ref_tag">3</span></a>, and generally have better latency performance compared to larger models. Additionally, GPU optimization of inference models can further speed up processing times. Open-source models optimized with NVIDIA’s Tensor RT-LLM inference libraries, for instance, have shown faster performance than non-optimized models. These strategies help balance the need for cost-efficiency with maintaining high performance and security standards.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1"><span class="ltx_text ltx_font_bold" id="S5.p3.1.1">LLM Gateway</span>: If you must use a vendor LLM API, it is better to implement an internal company LLM Gateway for audit, subscription and cost management across the company. Implementing an internal company LLM Gateway can streamline LLM usage, subscriptions, and data tracking for security audits. This central hub simplifies management and ensures efficient resource allocation. At NVIDIA IT, we have implemented an LLM Gateway that logs the inbound and outbound payloads for audit purposes and this data is guarded with access control permissions. Our LLM Gateway helps manage the subscriptions and costs of LLM API invocations.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">In summary, developing a hybrid and balanced LLM strategy is essential for managing costs and enabling innovation. This involves using smaller and customized LLMs to manage expenses while allowing responsible exploration with large LLMs via an LLM Gateway. It’s crucial to measure and monitor ROI by keeping track of LLM subscriptions and costs, as well as assessing Gen-AI feature usage and productivity enhancements. Ensuring the security of sensitive enterprise data in cloud-based LLM usage requires implementing guardrails to prevent data leakage and building an LLM Gateway for audits and legally permitted learning. Finally, be aware of the trade-offs between cost, accuracy, and latency, customizing smaller LLMs to match the accuracy of larger models while noting that large LLMs with long context lengths tend to have longer response time.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Testing RAG-based Chatbots (T)</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Testing generative AI solutions can be a lengthy process due to the need for human response validation. LLMs are increasingly being employed using ‘LLM-as-a-judge’ approach. However, it is advisable to use caution when using LLMs as human proxy, as using LLMs as judges can lead to self-fulfilling prophecy type of scenarios reinforcing their inherent biases in evaluations as well.</p>
<ul class="ltx_itemize" id="S6.I1">
<li class="ltx_item" id="S6.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i1.p1">
<p class="ltx_p" id="S6.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I1.i1.p1.1.1">Security Testing</span>: Automating security testing is critical for maintaining development velocity without compromising safety. A strong security framework and regression test datasets ensure that the chatbot remains resilient to potential threats. We are collaborating with our internal RED teams in security to prepare a set of datasets that can be tested with each major iteration of the chatbot.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i2.p1">
<p class="ltx_p" id="S6.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I1.i2.p1.1.1">Prompt Change Testing</span>: Generative AI models can be highly sensitive to prompt changes. To maintain accuracy, full regression testing is needed with each prompt alteration.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i3.p1">
<p class="ltx_p" id="S6.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I1.i3.p1.1.1">Feedback Loops</span>: Incorporating feedback gathered and the RLHF cycle is pivotal for continuous improvement. It allows LLM models to refine both our solutions and Language Models over time, ensuring that the chatbot becomes increasingly proficient. However, if the chosen foundational models don’t offer customization, then it becomes difficult to align the models to human feedback. If the feedback is significant and comes in many areas, then model customization may be an option. As of now, we have begun gathering user feedback but haven’t built our continuous learning pipelines using RLHF yet. Having tools to make this automated is critical to pos-production life cycle management of these chatbots.</p>
</div>
</li>
</ul>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Learnings</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.p1.1.1">Plan for Long Test Cycles</span>: Effective testing of RAG-based chatbots requires anticipation of lengthy test cycles. Begin by focusing on automating tests and enhancing accuracy assessments to streamline this essential phase.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.p2.1.1">Build Representative Ground Truth Datasets</span>: It is crucial to construct comprehensive ground truth datasets that reflect full spectrum of targeted solution strengths. This ensures that the chatbot is tested against scenarios that it will encounter in actual use.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS1.p3">
<p class="ltx_p" id="S6.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.p3.1.1">Automate Evaluations</span>: While leveraging LLMs as evaluators can provide scalable testing options, remember that the quality of human evaluations is unmatched. Automated tools should be used where feasible to supplement but not replace human oversight.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS1.p4">
<p class="ltx_p" id="S6.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.p4.1.1">Incorporate Human Feedback and Continuous Learning</span>: Establish mechanisms that allow for human feedback and systematic error analysis. Prioritize iterative improvements based on this feedback to continually refine chatbot performance and adaptability.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Securing RAG-based Chatbots (S)</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">Building trust is paramount when deploying generative AI chatbots. To mitigate risks, guardrails for hallucinations, toxicity, fairness, transparency, and security are critical. Strong foundational models are increasingly getting better at these guardrails. However, there are still many possibilities of jail breaks, adversarial attacks, and other security issues. Apart from these security risks, generative AI-based chatbots are susceptible to derivative risks (explained below). Since our bots are all internal enterprise chatbots, our focus has been more on the enterprise content security and guardrailing for sensitive data. Below we summarize our learnings and insights for securing RAG-based chatbots based on our experience. Addressing these challenges is imperative to maintaining the integrity and security of RAG-based chatbots within corporate environments.</p>
</div>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1. </span>Learnings</h3>
<div class="ltx_para ltx_noindent" id="S7.SS1.p1">
<p class="ltx_p" id="S7.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S7.SS1.p1.1.1">Enterprise Content Access Control</span>: Enterprise documents are protected by access controls, requiring RAG-based chatbots to comply with Access Control Lists (ACLs) during response generation. To ensure this compliance, we specifically selected an IR product known for its capability to honor these document ACLs effectively.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS1.p2">
<p class="ltx_p" id="S7.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S7.SS1.p2.1.1">Derivative Risks with Generative AI</span>: Chatbots might generate responses that lack context from their original data sources, potentially leading to misinterpretations. Additionally, enhanced search methods could inadvertently elevate the risk of exposing sensitive data if enterprise content is inappropriately secured. As part of our NVInfo bot journey, we implemented sensitive data guardrails in addition to leveraging sensitive data filtering and classification capabilities provided by the vector search solution we used to automatically filter out sensitive data during the retrieval.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS1.p3">
<p class="ltx_p" id="S7.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S7.SS1.p3.1.1">Data Governance and Content Security</span>: Efficient knowledge access can increase sensitive data leakage risks. Thus, it’s essential to prioritize data governance before deployment to safeguard against unauthorized access and data breaches. At NVIDIA, we embarked on an enterprise content security initiative for document sensitivity classification and exclusion of sensitive content from chatbots.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS1.p4">
<p class="ltx_p" id="S7.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S7.SS1.p4.1.1">Enterprise Guardrailing</span>: Implementing guardrails that align generative AI responses with specific enterprise policies and rules is essential. These guardrails help mitigate risks by ensuring that Chatbot-generated content adheres to established norms and ethical guidelines, preventing potential legal and reputational damage. In NVInfo bot, we implemented many guardrails in LLM prompts initially. However, later realized that not all LLMs follow these prompts consistently. Therefore, we implemented these guardrails during pre and post processing of queries and responses respectively using Nemo Guardrails <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#bib.bib13" title="">rebedea2023nemo, </a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8. </span>Related work</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">Our work can be compared with RAG papers on various topics dealing with RAG quality along all the FACTS dimensions we presented (freshness, architecture, costs, testing and security). Due to lack of space, we contrast our work with selective works. Barnett <em class="ltx_emph ltx_font_italic" id="S8.p1.1.1">et. al.</em> <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#bib.bib3" title="">barnett2024seven, </a>)</cite> presented seven failure points when engineering RAG systems. In their work, they highlight the challenges of getting retrieval augmented generation right by presenting their findings from having built three chatbots. Wenqi Glantz <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#bib.bib6" title="">wq2024, </a>)</cite> elaborated 12 RAG pain points and presented solutions. We experienced similar challenges first-hand when building our chatbots. However, none of these works discuss the challenges with complex queries, testing, dealing with document security, and the need for flexible architectures. In our work, we not only build on failure/pain points of RAGs as mentioned above, but also present our 15 control points in RAG pipelines and offer specific solutions for each stage. Also, we extend our insights and present practical techniques for handling complex queries, testing, and security. We present a reference architecture for one of the implementations of agentic architectures for complex query handling, strategies for testing and evaluating subjective query responses, and raised awareness for dealing with document ACLs and security. Furthermore, we present a reference architecture for a flexible generative-AI based Chatbot platform.</p>
</div>
<div class="ltx_para" id="S8.p2">
<p class="ltx_p" id="S8.p2.1">ChipNemo <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#bib.bib10" title="">liu2023chipnemo, </a>)</cite> presents evidence for using a domain adapted language model for improving RAG’s performance on domain specific questions. They finetuned the e5-small-unsupervised model with 3,000 domain specific auto-generated samples. We tried fine-tuning e5-large embeddings model in Scout Bot. Our results did not demonstrate significant improvements. We are presently collecting high quality human-annotated data to repeat the experiments. This could be an important direction to explore in the future for our work. Another interesting technique was presented by Setty <em class="ltx_emph ltx_font_italic" id="S8.p2.1.1">et. al.</em> <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#bib.bib15" title="">setty2024improving, </a>)</cite>, in improving RAG performance using Hypothetical Document Embeddings (HYDE) technique. HyDE uses an LLM to generate a theoretical document when responding to a query and then does the similarity search with both the original question and hypothetical answer. This is a promising approach but might make the architecture complex.</p>
</div>
<div class="ltx_para" id="S8.p3">
<p class="ltx_p" id="S8.p3.1">Active Retrieval augmented generation (FLARE) <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#bib.bib7" title="">jiang2023active, </a>)</cite> iteratively synthesizes a hypothetical next sentence. If the generated sentence contains low-probability tokens, FLARE would use the sentence as the new query for retrieval and regenerate the sentence. Mialon <em class="ltx_emph ltx_font_italic" id="S8.p3.1.1">et al.</em> <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#bib.bib12" title="">mialon2023augmented, </a>)</cite> reviews works for advanced augmented generation methods in language model. Self-refine <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#bib.bib11" title="">madaan2024self, </a>)</cite> builds an agent to improve the initial answer of RAG through iterative feedback and refinement. ReAct <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#bib.bib16" title="">yao2022react, </a>)</cite> Agent is widely used for handling the complex queries in a recursive manner. On the RAG evaluation front, RAGAS <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#bib.bib4" title="">es2023ragas, </a>)</cite> and ARES <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#bib.bib14" title="">saad2023ares, </a>)</cite> utilize LLMs as judges and build automatic RAG benchmark to evaluate the RAG system. Zhu <em class="ltx_emph ltx_font_italic" id="S8.p3.1.2">et al.</em> <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2407.07858v1#bib.bib17" title="">zhu2023large, </a>)</cite> overview the intensive usages of LLM in a RAG pipeline including retriever, data generation, rewriter, and reader. We believe that our work provides a unique perspective on building secure enterprise-grade chatbots via our FACTS framework.</p>
</div>
</section>
<section class="ltx_section" id="S9">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9. </span>Conclusions</h2>
<div class="ltx_para" id="S9.p1">
<p class="ltx_p" id="S9.p1.1">In this paper, we presented our approach to developing effective RAG-based chatbots, highlighting our experiences of building three chatbots at NVIDIA. We outlined our FACTS framework, emphasizing the importance of content freshness (F), architecture (A), LLM cost (C) management, planning for testing (T), and security (S) in creating robust, secure, and enterprise-grade chatbots. We also identified and elaborated on fifteen critical control points within RAG pipelines, providing strategies to enhance chatbot performance at each stage. Furthermore, our empirical analysis reveals the trade-offs between accuracy and latency when comparing large and small LLMs. This paper offers a holistic perspective on the essential factors and practical solutions for building secure and efficient enterprise-grade chatbots, making a unique contribution to the field.
More work is needed in several areas to build effective RAG-based chatbots. This includes developing agentic architectures for handling complex, multi-part, and analytical queries; efficiently summarizing large volumes of frequently updated enterprise data; incorporating auto-ML capabilities to optimize various RAG control points automatically; and creating more robust evaluation frameworks for assessing subjective responses and conversations.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
Langchain.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">https://github.com/langchain-ai</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(2)</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib2.1.1">Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.</span>
</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib2.2.1">arXiv preprint arXiv:2303.08774</span> (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(3)</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib3.1.1">Barnett, S., Kurniawan, S., Thudumu, S., Brannelly, Z., and Abdelrazek, M.</span>
</span>
<span class="ltx_bibblock">Seven failure points when engineering a retrieval augmented generation system.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.2.1">arXiv preprint arXiv:2401.05856</span> (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(4)</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib4.1.1">Es, S., James, J., Espinosa-Anke, L., and Schockaert, S.</span>
</span>
<span class="ltx_bibblock">Ragas: Automated evaluation of retrieval augmented generation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.2.1">arXiv preprint arXiv:2309.15217</span> (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(5)</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib5.1.1">Galitsky, B.</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib5.2.1">Developing enterprise chatbots</span>.

</span>
<span class="ltx_bibblock">Springer, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(6)</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib6.1.1">Glantz, W.</span>
</span>
<span class="ltx_bibblock">12 rag pain points and proposed solutions.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(7)</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib7.1.1">Jiang, Z., Xu, F. F., Gao, L., Sun, Z., Liu, Q., Dwivedi-Yu, J., Yang, Y., Callan, J., and Neubig, G.</span>
</span>
<span class="ltx_bibblock">Active retrieval augmented generation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.2.1">arXiv preprint arXiv:2305.06983</span> (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(8)</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib8.1.1">Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.-t., Rocktäschel, T., et al.</span>
</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib8.2.1">Advances in Neural Information Processing Systems 33</span> (2020), 9459–9474.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(9)</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib9.2.1">Liu, J.</span>
</span>
<span class="ltx_bibblock">LlamaIndex.

</span>
<span class="ltx_bibblock"><math alttext="https://github.com/jerryjliu/llama_{i}ndex" class="ltx_math_unparsed" display="inline" id="bib.bib9.1.m1.1"><semantics id="bib.bib9.1.m1.1a"><mrow id="bib.bib9.1.m1.1b"><mi id="bib.bib9.1.m1.1.1">h</mi><mi id="bib.bib9.1.m1.1.2">t</mi><mi id="bib.bib9.1.m1.1.3">t</mi><mi id="bib.bib9.1.m1.1.4">p</mi><mi id="bib.bib9.1.m1.1.5">s</mi><mo id="bib.bib9.1.m1.1.6" lspace="0.278em" rspace="0em">:</mo><mo id="bib.bib9.1.m1.1.7" lspace="0em" rspace="0em">/</mo><mo id="bib.bib9.1.m1.1.8" lspace="0em">/</mo><mi id="bib.bib9.1.m1.1.9">g</mi><mi id="bib.bib9.1.m1.1.10">i</mi><mi id="bib.bib9.1.m1.1.11">t</mi><mi id="bib.bib9.1.m1.1.12">h</mi><mi id="bib.bib9.1.m1.1.13">u</mi><mi id="bib.bib9.1.m1.1.14">b</mi><mo id="bib.bib9.1.m1.1.15" lspace="0em" rspace="0.167em">.</mo><mi id="bib.bib9.1.m1.1.16">c</mi><mi id="bib.bib9.1.m1.1.17">o</mi><mi id="bib.bib9.1.m1.1.18">m</mi><mo id="bib.bib9.1.m1.1.19">/</mo><mi id="bib.bib9.1.m1.1.20">j</mi><mi id="bib.bib9.1.m1.1.21">e</mi><mi id="bib.bib9.1.m1.1.22">r</mi><mi id="bib.bib9.1.m1.1.23">r</mi><mi id="bib.bib9.1.m1.1.24">y</mi><mi id="bib.bib9.1.m1.1.25">j</mi><mi id="bib.bib9.1.m1.1.26">l</mi><mi id="bib.bib9.1.m1.1.27">i</mi><mi id="bib.bib9.1.m1.1.28">u</mi><mo id="bib.bib9.1.m1.1.29">/</mo><mi id="bib.bib9.1.m1.1.30">l</mi><mi id="bib.bib9.1.m1.1.31">l</mi><mi id="bib.bib9.1.m1.1.32">a</mi><mi id="bib.bib9.1.m1.1.33">m</mi><msub id="bib.bib9.1.m1.1.34"><mi id="bib.bib9.1.m1.1.34.2">a</mi><mi id="bib.bib9.1.m1.1.34.3">i</mi></msub><mi id="bib.bib9.1.m1.1.35">n</mi><mi id="bib.bib9.1.m1.1.36">d</mi><mi id="bib.bib9.1.m1.1.37">e</mi><mi id="bib.bib9.1.m1.1.38">x</mi></mrow><annotation encoding="application/x-tex" id="bib.bib9.1.m1.1c">https://github.com/jerryjliu/llama_{i}ndex</annotation><annotation encoding="application/x-llamapun" id="bib.bib9.1.m1.1d">italic_h italic_t italic_t italic_p italic_s : / / italic_g italic_i italic_t italic_h italic_u italic_b . italic_c italic_o italic_m / italic_j italic_e italic_r italic_r italic_y italic_j italic_l italic_i italic_u / italic_l italic_l italic_a italic_m italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_n italic_d italic_e italic_x</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="bib.bib9.3.1"></span> (2022).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(10)</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib10.1.1">Liu, M., Ene, T.-D., Kirby, R., Cheng, C., Pinckney, N., Liang, R., Alben, J., Anand, H., Banerjee, S., Bayraktaroglu, I., et al.</span>
</span>
<span class="ltx_bibblock">Chipnemo: Domain-adapted llms for chip design.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib10.2.1">arXiv preprint arXiv:2311.00176</span> (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(11)</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib11.1.1">Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., et al.</span>
</span>
<span class="ltx_bibblock">Self-refine: Iterative refinement with self-feedback.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.2.1">Advances in Neural Information Processing Systems 36</span> (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(12)</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib12.1.1">Mialon, G., Dessì, R., Lomeli, M., Nalmpantis, C., Pasunuru, R., Raileanu, R., Rozière, B., Schick, T., Dwivedi-Yu, J., Celikyilmaz, A., et al.</span>
</span>
<span class="ltx_bibblock">Augmented language models: a survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.2.1">arXiv preprint arXiv:2302.07842</span> (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(13)</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib13.1.1">Rebedea, T., Dinu, R., Sreedhar, M., Parisien, C., and Cohen, J.</span>
</span>
<span class="ltx_bibblock">Nemo guardrails: A toolkit for controllable and safe llm applications with programmable rails.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.2.1">arXiv preprint arXiv:2310.10501</span> (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(14)</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib14.1.1">Saad-Falcon, J., Khattab, O., Potts, C., and Zaharia, M.</span>
</span>
<span class="ltx_bibblock">Ares: An automated evaluation framework for retrieval-augmented generation systems.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.2.1">arXiv preprint arXiv:2311.09476</span> (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(15)</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib15.1.1">Setty, S., Jijo, K., Chung, E., and Vidra, N.</span>
</span>
<span class="ltx_bibblock">Improving retrieval for rag based question answering models on financial documents.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib15.2.1">arXiv preprint arXiv:2404.07221</span> (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(16)</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib16.1.1">Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y.</span>
</span>
<span class="ltx_bibblock">React: Synergizing reasoning and acting in language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib16.2.1">arXiv preprint arXiv:2210.03629</span> (2022).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(17)</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib17.1.1">Zhu, Y., Yuan, H., Wang, S., Liu, J., Liu, W., Deng, C., Dou, Z., and Wen, J.-R.</span>
</span>
<span class="ltx_bibblock">Large language models for information retrieval: A survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib17.2.1">arXiv preprint arXiv:2308.07107</span> (2023).

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Jul 10 17:07:04 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
