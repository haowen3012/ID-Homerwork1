<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft</title>
<!--Generated on Tue Jun 25 13:42:42 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2406.17553v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#S1" title="In Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#S2" title="In Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#S2.SS0.SSS0.Px1" title="In 2 Related Work ‣ Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_title">IGLU - Multi Turn Dataset:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#S2.SS0.SSS0.Px2" title="In 2 Related Work ‣ Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_title">LLMs for Instruction Translation to a Code Snippet:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#S2.SS0.SSS0.Px3" title="In 2 Related Work ‣ Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_title">LLMs for Minecraft Collaborative Building Task</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#S3" title="In Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Dataset</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#S3.SS1" title="In 3 Dataset ‣ Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Builder Action Transformation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#S4" title="In Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#S4.SS0.SSS0.Px1" title="In 4 Experimental Setup ‣ Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_title">Few-shot Prompting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#S4.SS0.SSS0.Px2" title="In 4 Experimental Setup ‣ Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_title">LLM Fine Tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#S4.SS0.SSS0.Px3" title="In 4 Experimental Setup ‣ Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_title">Evaluation metrics</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#S5" title="In Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results &amp; Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#S5.SS0.SSS0.Px1" title="In 5 Results &amp; Analysis ‣ Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_title">Spatial Prepositions:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#S5.SS0.SSS0.Px2" title="In 5 Results &amp; Analysis ‣ Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_title">Geometric and Real-World Shapes:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#S5.SS0.SSS0.Px3" title="In 5 Results &amp; Analysis ‣ Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_title">Anaphora:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#S5.SS0.SSS0.Px4" title="In 5 Results &amp; Analysis ‣ Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_title">Builder Mistakes:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#S5.SS0.SSS0.Px5" title="In 5 Results &amp; Analysis ‣ Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_title">Underspecified Instructions:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#S5.SS1" title="In 5 Results &amp; Analysis ‣ Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Qualitative Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#S6" title="In Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#A1" title="In Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Prompt Structure</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#A1.SS1" title="In Appendix A Prompt Structure ‣ Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Ablation Study</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#A1.SS2" title="In Appendix A Prompt Structure ‣ Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Model Variants</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#A2" title="In Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Fine tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#A3" title="In Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Detailed Error Analysis</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Retrieval-Augmented Code Generation for Situated
<br class="ltx_break"/>Action Generation: A Case Study on Minecraft</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chalamalasetti Kranti
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Computational Linguistics, Department of Linguistics 
<br class="ltx_break"/>University of Potsdam, Germany
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sherzod Hakimov
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Computational Linguistics, Department of Linguistics 
<br class="ltx_break"/>University of Potsdam, Germany
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">David Schlangen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Computational Linguistics, Department of Linguistics 
<br class="ltx_break"/>University of Potsdam, Germany
</span>
<span class="ltx_contact ltx_role_affiliation">German Research Center for Artificial Intelligence (DFKI), Berlin, Germany
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">In the Minecraft Collaborative Building Task, two players collaborate: an Architect (A) provides instructions to a Builder (B) to assemble a specified structure using 3D blocks. In this work, we investigate the use of large language models (LLMs) to predict the sequence of actions taken by the Builder. Leveraging LLMs’ in-context learning abilities, we use few-shot prompting techniques, that significantly improve performance over baseline methods. Additionally, we present a detailed analysis of the gaps in performance for future work.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.1">
<p class="ltx_p" id="p1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1.1">Retrieval-Augmented Code Generation for Situated
<br class="ltx_break"/>Action Generation: A Case Study on Minecraft</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.1.2" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.1.2.1" style="width:0.0pt;"></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Accurate action prediction is fundamental in developing interactive agents, enabling these systems to anticipate and effectively respond to user behaviors <cite class="ltx_cite ltx_citemacro_citep">(Roy and Reiter, <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib21" title="">2005</a>; Goodrich and Schultz, <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib6" title="">2007</a>)</cite>. Understanding language in dialogues is central to this process, as it involves interpreting instructions, intentions, and contextual cues to predict subsequent actions <cite class="ltx_cite ltx_citemacro_citep">(Chen and Mooney, <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib2" title="">2011</a>; Matuszek et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib16" title="">2012</a>; Schlangen, <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib23" title="">2023</a>)</cite>. Enhancing the precision of action prediction is particularly important in collaborative environments, where seamless interaction and coordination between agents and users are essential for achieving complex tasks <cite class="ltx_cite ltx_citemacro_citep">(Winograd, <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib33" title="">1971</a>; Thomaz and Breazeal, <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib28" title="">2008</a>; Tellex et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib27" title="">2020</a>; Ichter et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib9" title="">2022</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="242" id="S1.F1.g1" src="x1.png" width="415"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Illustration of the LLM interpreting block placement instructions. The initial world view is empty. The LLM receives instructions from User A and generates action predictions.</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Building on the importance of accurate action prediction and understanding language in dialogues, our research focuses on the Minecraft Collaborative Building Task <cite class="ltx_cite ltx_citemacro_citep">(Narayan-Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib18" title="">2019</a>)</cite>. This task requires close coordination between an Architect (A) and a Builder (B). In this scenario, the Architect provides instructions to the Builder to assemble a specified structure using 3D blocks, making it an ideal testbed for investigating the capabilities of advanced computational models in interpreting and predicting actions based on natural language instructions.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Given the complexities involved in understanding natural language and predicting actions accurately, prior works <cite class="ltx_cite ltx_citemacro_citep">(Jayannavar et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib10" title="">2020</a>; Shi et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib24" title="">2022</a>; Kiseleva et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib12" title="">2022</a>)</cite> proposed end-to-end neural models for modeling builder action prediction. However, achieving high accuracy for this task remains challenging. On the other hand, LLMs have demonstrated impressive performance in various natural language processing tasks, thanks to their ability to perform in-context learning <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib1" title="">2020</a>)</cite>. Few-shot prompting techniques, in particular, allow these models to generalize from a limited number of examples <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib14" title="">2023</a>; Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib32" title="">2022</a>; Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib34" title="">2023</a>)</cite>, making them well-suited for tasks requiring nuanced understanding and prediction of actions <cite class="ltx_cite ltx_citemacro_citep">(Xi et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib35" title="">2023</a>; Liang et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib13" title="">2023</a>; Singh et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib25" title="">2023</a>; Driess et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib5" title="">2023</a>; Vemprala et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib30" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this work, we explore the application of LLMs to predict the sequence of actions taken by the Builder (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_tag">1</span></a>). Taking advantage of the code-generation capabilities of LLMs, we model the action prediction task as a code-generation task.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">Minecraft Collaborative Building Task:</span> <cite class="ltx_cite ltx_citemacro_citet">Narayan-Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib18" title="">2019</a>)</cite> sourced human-to-human conversations for building structures in the Minecraft world. Building on this dataset <cite class="ltx_cite ltx_citemacro_citep">(Jayannavar et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib10" title="">2020</a>)</cite> modeled builder action sequences using an encoder-decoder architecture with dialogue history and world state representation. Further advancing this task <cite class="ltx_cite ltx_citemacro_citep">(Shi et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib24" title="">2022</a>)</cite> proposed a joint learning task for collaborative building and asking clarification questions. With a best F1-score of 0.21, this task remains unsolved.</p>
</div>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">IGLU - Multi Turn Dataset:</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1"><cite class="ltx_cite ltx_citemacro_citet">Mohanty et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib17" title="">2023</a>)</cite> features a human collaborating with an AI agent to build target structures, using 31 out of 150 original Minecraft building tasks. Since we focus on action generation for human-human interactions, this dataset falls out of scope for our evaluation.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">LLMs for Instruction Translation to a Code Snippet:</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">Several research works <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib3" title="">2021</a>; Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib8" title="">2022</a>; Zeng et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib36" title="">2023</a>; Zhao et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib37" title="">2023</a>)</cite> utilize LLMs for translating input natural language instructions to executable code snippets. These efforts span domains such as program synthesis <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib31" title="">2021</a>; Touvron et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib29" title="">2023</a>; Rozière et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib22" title="">2023</a>; Hou and Ji, <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib7" title="">2024</a>)</cite> and robot instruction generation <cite class="ltx_cite ltx_citemacro_citep">(Liang et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib13" title="">2023</a>; Singh et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib25" title="">2023</a>; Driess et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib5" title="">2023</a>; Vemprala et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib30" title="">2024</a>; Kim et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib11" title="">2024</a>)</cite>. This capability of LLMs motivates us to formulate the builder action prediction task as a code generation task.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">LLMs for Minecraft Collaborative Building Task</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p1.1"><cite class="ltx_cite ltx_citemacro_citet">Madge and Poesio (<a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib15" title="">2024</a>)</cite> use LLMs with text representation for action prediction, aligning closely with our proposal. However, we represent builder actions as code snippets and experiment on the Minecraft Dialogue dataset <cite class="ltx_cite ltx_citemacro_citep">(Narayan-Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib18" title="">2019</a>)</cite>, providing a detailed analysis of action prediction performance.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Dataset</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Minecraft is used for studying collaborative tasks in a 3D voxel grid, with agents following architect’s natural language instructions to build structures. The collaborative dialogue dataset <cite class="ltx_cite ltx_citemacro_citep">(Narayan-Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib18" title="">2019</a>)</cite> has conversations for 150 target structures with varying levels of abstractions and complexity,
grouped into 547 dialogue games, split into train (309 games, 3,792 turn-code pairs), development (101 games, 1,335 turn-code pairs) and test sets (137 games, 1,615 turn-code pairs). We used the <span class="ltx_text ltx_font_italic" id="S3.p1.1.1">test set</span> for our evaluation.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Builder Action Transformation</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Since the builder action prediction task is framed as a code-generation task, we convert the conversation from the format in the corpus (i.e., dialogue + action) into instruction + pseudo code. All the utterances of the builder and architect before each builder’s action are aggregated into a single instruction. Builder actions involving <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.1">puts down</span> are converted to <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.2">place()</span> function, and actions involving <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.3">picks up</span> are converted to <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.4">pick()</span> function. The following is a sample representation of this conversion.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>We will make the converted corpus available by the time of the conference.
</span></span></span></p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<span class="ltx_ERROR undefined" id="S3.SS1.p2.1">{dialogue}</span><span class="ltx_ERROR undefined" id="S3.SS1.p2.2">\speak</span>
<p class="ltx_p" id="S3.SS1.p2.3">Instruction One block away from the edge, place a green block
<span class="ltx_ERROR undefined" id="S3.SS1.p2.3.1">\speak</span>Builder Action Builder puts down a green block at X:0 Y:1 Z:4
<span class="ltx_ERROR undefined" id="S3.SS1.p2.3.2">\speak</span><span class="ltx_text ltx_font_bold" id="S3.SS1.p2.3.3">Code-Representation</span> <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p2.3.4">place(color=‘green’, x=0, y=1, z=4)</span>
<br class="ltx_break"/>
<span class="ltx_ERROR undefined" id="S3.SS1.p2.3.5">\speak</span>Instruction remove the middle block
<span class="ltx_ERROR undefined" id="S3.SS1.p2.3.6">\speak</span>Builder Action Builder picks up a red block at X:0 Y:2 Z:0
<span class="ltx_ERROR undefined" id="S3.SS1.p2.3.7">\speak</span><span class="ltx_text ltx_font_bold" id="S3.SS1.p2.3.8">Code-Representation</span>
<span class="ltx_text ltx_font_typewriter" id="S3.SS1.p2.3.9">pick(color=‘red’, x=0, y=2, z=0)</span></p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Setup</h2>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Few-shot Prompting</h4>
<div class="ltx_para" id="S4.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px1.p1.1">Following standard prompting approaches <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib1" title="">2020</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib14" title="">2023</a>; Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib32" title="">2022</a>; Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib34" title="">2023</a>)</cite>, we adopt few-shot prompting to probe LLMs.
Our prompt (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#A1.F3" title="Figure 3 ‣ Appendix A Prompt Structure ‣ Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_tag">3</span></a> in Appendix) includes environment information such as the voxel grid size, available colors for the blocks and the quantity of each color. This is followed by dynamically adapted in-context examples suitable to the current turn instruction from the training set.</p>
</div>
<div class="ltx_para" id="S4.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="S4.SS0.SSS0.Px1.p2.1">We use the pre-trained all-MiniLM-L6-v2 model from Sentence Transformers <cite class="ltx_cite ltx_citemacro_citep">(Reimers and Gurevych, <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib20" title="">2019</a>)</cite> to compute the similarity between the current turn instruction and all turns in the training set. The top three turns and their corresponding builder actions
are selected as in-context examples. For instance, if the current turn instruction is to “start with a column of 5 purple bricks”, the in-context examples might be “add two lines of purple bricks”
and “start with a column of 5 red bricks”.
We conduct an ablation study on the impact of the parts of the prompt (refer to
section <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#A1.SS1" title="A.1 Ablation Study ‣ Appendix A Prompt Structure ‣ Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_tag">A.1</span></a> in Appendix) for the proposed task. Finally we append the test instruction and then query instruction-tuned LLMs such as <span class="ltx_text ltx_font_italic" id="S4.SS0.SSS0.Px1.p2.1.1">GPT-4</span> (version o), <span class="ltx_text ltx_font_italic" id="S4.SS0.SSS0.Px1.p2.1.2">Llama-3-8b</span> and <span class="ltx_text ltx_font_italic" id="S4.SS0.SSS0.Px1.p2.1.3">Llama-3-70b</span> (see Section <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#A1.SS2" title="A.2 Model Variants ‣ Appendix A Prompt Structure ‣ Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_tag">A.2</span></a> in Appendix for model selection). All models are queried with temperature 0 and <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px1.p2.1.4">max_new_tokens</span> = 500.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">LLM Fine Tuning</h4>
<div class="ltx_para" id="S4.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p1.1">In addition to few-shot prompting, we explored fine-tuning the Llama-3-8b model on the Minecraft building task to improve its generation of builder action sequences. We use the training set for the fine-tuning experiments. Additional details about the fine-tuning process are available Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#A2" title="Appendix B Fine tuning ‣ Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Evaluation metrics</h4>
<div class="ltx_para" id="S4.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px3.p1.1">We follow the same evaluation strategy as <cite class="ltx_cite ltx_citemacro_citet">Jayannavar et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib10" title="">2020</a>)</cite>. Each dialogue turn prompts the LLM to generate action sequences (code) that provide information about a single block. The predicted sequences are compared against ground truth in the test split by checking block color, X, Y, Z coordinates, and command type (pick or place). This procedure is applied to all action sequences. We report the micro-averaged F1 on all dialogue turns.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results &amp; Analysis</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.3">The results compared to the baseline Builder Action Prediction (BAP) model by <cite class="ltx_cite ltx_citemacro_citet">Jayannavar et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib10" title="">2020</a>)</cite>, are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#S5.T1" title="Table 1 ‣ Spatial Prepositions: ‣ 5 Results &amp; Analysis ‣ Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_tag">1</span></a>. GPT-4 achieved the best result (<math alttext="0.39" class="ltx_Math" display="inline" id="S5.p1.1.m1.1"><semantics id="S5.p1.1.m1.1a"><mn id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml">0.39</mn><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><cn id="S5.p1.1.m1.1.1.cmml" type="float" xref="S5.p1.1.m1.1.1">0.39</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">0.39</annotation><annotation encoding="application/x-llamapun" id="S5.p1.1.m1.1d">0.39</annotation></semantics></math>) closely followed by Llama-3-70b (<math alttext="0.33" class="ltx_Math" display="inline" id="S5.p1.2.m2.1"><semantics id="S5.p1.2.m2.1a"><mn id="S5.p1.2.m2.1.1" xref="S5.p1.2.m2.1.1.cmml">0.33</mn><annotation-xml encoding="MathML-Content" id="S5.p1.2.m2.1b"><cn id="S5.p1.2.m2.1.1.cmml" type="float" xref="S5.p1.2.m2.1.1">0.33</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.m2.1c">0.33</annotation><annotation encoding="application/x-llamapun" id="S5.p1.2.m2.1d">0.33</annotation></semantics></math>). The fine-tuned version of Llama-3-8b showed a <math alttext="\sim 6\%" class="ltx_Math" display="inline" id="S5.p1.3.m3.1"><semantics id="S5.p1.3.m3.1a"><mrow id="S5.p1.3.m3.1.1" xref="S5.p1.3.m3.1.1.cmml"><mi id="S5.p1.3.m3.1.1.2" xref="S5.p1.3.m3.1.1.2.cmml"></mi><mo id="S5.p1.3.m3.1.1.1" xref="S5.p1.3.m3.1.1.1.cmml">∼</mo><mrow id="S5.p1.3.m3.1.1.3" xref="S5.p1.3.m3.1.1.3.cmml"><mn id="S5.p1.3.m3.1.1.3.2" xref="S5.p1.3.m3.1.1.3.2.cmml">6</mn><mo id="S5.p1.3.m3.1.1.3.1" xref="S5.p1.3.m3.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.3.m3.1b"><apply id="S5.p1.3.m3.1.1.cmml" xref="S5.p1.3.m3.1.1"><csymbol cd="latexml" id="S5.p1.3.m3.1.1.1.cmml" xref="S5.p1.3.m3.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S5.p1.3.m3.1.1.2.cmml" xref="S5.p1.3.m3.1.1.2">absent</csymbol><apply id="S5.p1.3.m3.1.1.3.cmml" xref="S5.p1.3.m3.1.1.3"><csymbol cd="latexml" id="S5.p1.3.m3.1.1.3.1.cmml" xref="S5.p1.3.m3.1.1.3.1">percent</csymbol><cn id="S5.p1.3.m3.1.1.3.2.cmml" type="integer" xref="S5.p1.3.m3.1.1.3.2">6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.3.m3.1c">\sim 6\%</annotation><annotation encoding="application/x-llamapun" id="S5.p1.3.m3.1d">∼ 6 %</annotation></semantics></math> improvement over the vanilla version. Even though GPT-4 significantly outperforms the fine-tuned baseline, the upper bound for this task remains low. To understand this, we analysed the dialogues and identified references to spatial relations, real-world/geometric shapes, and anaphora. We then show how the GPT-4’s performance in these categories. The performance of other models is discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#A3" title="Appendix C Detailed Error Analysis ‣ Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_tag">C</span></a> in the Appendix. Additionally, we identified two more factors complicating the interpretation of architect utterances, which may further impact action prediction.</p>
</div>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Spatial Prepositions:</h4>
<div class="ltx_para" id="S5.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px1.p1.3">These are cases where the architect’s utterances include prepositions that refer to a specific position in the grid, e.g. “put another blue block on <span class="ltx_text ltx_font_italic" id="S5.SS0.SSS0.Px1.p1.3.1">top</span> of it” or “two blocks on the ground, and one above on the <span class="ltx_text ltx_font_italic" id="S5.SS0.SSS0.Px1.p1.3.2">left</span>”. Using Stanza <cite class="ltx_cite ltx_citemacro_citep">(Qi et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib19" title="">2020</a>)</cite> and spaCy <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://spacy.io/" title="">https://spacy.io/</a></span></span></span> we extract phrases with the Part-of-Speech (POS) tags: adverb (ADP), noun (NN), and preposition (IN), then manually filter out non-spatial phrases resulting in
<math alttext="135" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px1.p1.1.m1.1"><semantics id="S5.SS0.SSS0.Px1.p1.1.m1.1a"><mn id="S5.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">135</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px1.p1.1.m1.1b"><cn id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" type="integer" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1">135</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px1.p1.1.m1.1c">135</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px1.p1.1.m1.1d">135</annotation></semantics></math> words. Examples include “left”, “right”, “top”, “bottom”, “down”, “front“, “back”, “towards”, “between”, “behind”, “opposite”, “parallel”, and “inverse”. In the test set, <math alttext="75.42" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px1.p1.2.m2.1"><semantics id="S5.SS0.SSS0.Px1.p1.2.m2.1a"><mn id="S5.SS0.SSS0.Px1.p1.2.m2.1.1" xref="S5.SS0.SSS0.Px1.p1.2.m2.1.1.cmml">75.42</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px1.p1.2.m2.1b"><cn id="S5.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" type="float" xref="S5.SS0.SSS0.Px1.p1.2.m2.1.1">75.42</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px1.p1.2.m2.1c">75.42</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px1.p1.2.m2.1d">75.42</annotation></semantics></math>% of the utterances include a spatial preposition, but the model correctly generates code for only <math alttext="26.03" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px1.p1.3.m3.1"><semantics id="S5.SS0.SSS0.Px1.p1.3.m3.1a"><mn id="S5.SS0.SSS0.Px1.p1.3.m3.1.1" xref="S5.SS0.SSS0.Px1.p1.3.m3.1.1.cmml">26.03</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px1.p1.3.m3.1b"><cn id="S5.SS0.SSS0.Px1.p1.3.m3.1.1.cmml" type="float" xref="S5.SS0.SSS0.Px1.p1.3.m3.1.1">26.03</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px1.p1.3.m3.1c">26.03</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px1.p1.3.m3.1d">26.03</annotation></semantics></math>% of them, highlighting significant challenges in interpreting spatial prepositions and a need for improvement.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.2.1">F1</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T1.1.2.1.1">GPT-4</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.2.1.2"><span class="ltx_text ltx_font_bold" id="S5.T1.1.2.1.2.1">0.39</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.3.2.1">Llama-3-70b</th>
<td class="ltx_td ltx_align_center" id="S5.T1.1.3.2.2">0.33</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.4.3.1">Llama-3-8b</th>
<td class="ltx_td ltx_align_center" id="S5.T1.1.4.3.2">0.18</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.5.4.1">Llama-3-8b (fine-tuned)</th>
<td class="ltx_td ltx_align_center" id="S5.T1.1.5.4.2">0.19</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="S5.T1.1.6.5.1">BAP (fine-tuned)</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T1.1.6.5.2">0.21</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Micro-Average F1-score for the builder action prediction task. The BAP (fine-tuned) model results are reported for H2 in game history and with 4x data augmentation <cite class="ltx_cite ltx_citemacro_citep">(Jayannavar et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib10" title="">2020</a>)</cite></figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Geometric and Real-World Shapes:</h4>
<div class="ltx_para" id="S5.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px2.p1.2">Architect’s utterances often include noun phrases referring to geometric shapes or real-world objects. Using the same procedure as above, we extracted 148 relevant nouns, including “trident”, “chair”, “pitchfork”, “circle”, and “rectangle”. In the test set, <math alttext="29.85" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px2.p1.1.m1.1"><semantics id="S5.SS0.SSS0.Px2.p1.1.m1.1a"><mn id="S5.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S5.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">29.85</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.1.m1.1b"><cn id="S5.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" type="float" xref="S5.SS0.SSS0.Px2.p1.1.m1.1.1">29.85</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.1.m1.1c">29.85</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px2.p1.1.m1.1d">29.85</annotation></semantics></math>% of utterances include one of these shapes, but the model correctly generated code for only <math alttext="18.26" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px2.p1.2.m2.1"><semantics id="S5.SS0.SSS0.Px2.p1.2.m2.1a"><mn id="S5.SS0.SSS0.Px2.p1.2.m2.1.1" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1.cmml">18.26</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.2.m2.1b"><cn id="S5.SS0.SSS0.Px2.p1.2.m2.1.1.cmml" type="float" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1">18.26</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.2.m2.1c">18.26</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px2.p1.2.m2.1d">18.26</annotation></semantics></math>% of them. This indicates that while LLMs have extensive general knowledge, they struggle with code generation for shape references, necessitating further examination to address these challenges.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Anaphora:</h4>
<div class="ltx_para" id="S5.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px3.p1.3">Architect’s utterances often include pronouns referring to previous concepts, e.g. “put another block next to <span class="ltx_text ltx_font_italic" id="S5.SS0.SSS0.Px3.p1.3.1">it</span>”. Using the same procedure as above, we extracted and manually filtered these pronouns, identifying <math alttext="16" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px3.p1.1.m1.1"><semantics id="S5.SS0.SSS0.Px3.p1.1.m1.1a"><mn id="S5.SS0.SSS0.Px3.p1.1.m1.1.1" xref="S5.SS0.SSS0.Px3.p1.1.m1.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px3.p1.1.m1.1b"><cn id="S5.SS0.SSS0.Px3.p1.1.m1.1.1.cmml" type="integer" xref="S5.SS0.SSS0.Px3.p1.1.m1.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px3.p1.1.m1.1c">16</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px3.p1.1.m1.1d">16</annotation></semantics></math> words such as “that”, “this”, “those”, “it”. <math alttext="46.81" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px3.p1.2.m2.1"><semantics id="S5.SS0.SSS0.Px3.p1.2.m2.1a"><mn id="S5.SS0.SSS0.Px3.p1.2.m2.1.1" xref="S5.SS0.SSS0.Px3.p1.2.m2.1.1.cmml">46.81</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px3.p1.2.m2.1b"><cn id="S5.SS0.SSS0.Px3.p1.2.m2.1.1.cmml" type="float" xref="S5.SS0.SSS0.Px3.p1.2.m2.1.1">46.81</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px3.p1.2.m2.1c">46.81</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px3.p1.2.m2.1d">46.81</annotation></semantics></math>% of utterances in the test set include one of these pronouns but the model generated correct code only for <math alttext="25.53" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px3.p1.3.m3.1"><semantics id="S5.SS0.SSS0.Px3.p1.3.m3.1a"><mn id="S5.SS0.SSS0.Px3.p1.3.m3.1.1" xref="S5.SS0.SSS0.Px3.p1.3.m3.1.1.cmml">25.53</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px3.p1.3.m3.1b"><cn id="S5.SS0.SSS0.Px3.p1.3.m3.1.1.cmml" type="float" xref="S5.SS0.SSS0.Px3.p1.3.m3.1.1">25.53</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px3.p1.3.m3.1c">25.53</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px3.p1.3.m3.1d">25.53</annotation></semantics></math>% of them. Unlike spatial and geometric references, not all anaphora utterances indicate a reference to building concepts. They can also include acknowledgments or other types of information.</p>
</div>
<figure class="ltx_figure" id="S5.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="394" id="S5.F2.g1" src="x2.png" width="700"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Voxel representations for sample turns that correspond to spatial preposition, geometric shape, and anaphora categories. Two samples for each category are given. Samples on the left side are generated correctly while samples on the right hand side have mistakes that are highlighted.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px4">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Builder Mistakes:</h4>
<div class="ltx_para" id="S5.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px4.p1.1">These occur when the builder places a block and later removes it, leaving spurious action sequences—one <span class="ltx_text ltx_font_italic" id="S5.SS0.SSS0.Px4.p1.1.1">place</span> and one <span class="ltx_text ltx_font_italic" id="S5.SS0.SSS0.Px4.p1.1.2">pick</span>—in the ground truth. E.g.“puts down a red block at X:-2 Y:0 Z: -1 followed by picks up a red block at X:-2 Y:0 Z:-1 (for a detailed example, please refer to Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#A3.F5" title="Figure 5 ‣ Appendix C Detailed Error Analysis ‣ Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_tag">5</span></a> in Appendix). Such cases cause evaluation mismatches. We filtered these by identifying <span class="ltx_text ltx_font_italic" id="S5.SS0.SSS0.Px4.p1.1.3">place</span> commands followed by <span class="ltx_text ltx_font_italic" id="S5.SS0.SSS0.Px4.p1.1.4">pick</span> commands at the same position and block color, finding that 23.3% of turns contained such mistakes. These inaccuracies in the ground truth lead to lower evaluation scores, as the model is penalized for errors that are not indicative of its true performance.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px5">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Underspecified Instructions:</h4>
<div class="ltx_para" id="S5.SS0.SSS0.Px5.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px5.p1.1">These are cases where the architect’s utterance is underspecified, meaning it may lack specific details such as colour or precise location, or it may have multiple possible interpretations.
To give an example, in the snippet shown in
Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#A3.F6" title="Figure 6 ‣ Appendix C Detailed Error Analysis ‣ Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_tag">6</span></a> (in Appendix), there is no explicit indication in the dialogue history to interpret the instruction in a particular way, and the builder is left to decide on their own. Cases like this highlight the challenge of achieving a 100% match with the ground truth.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Qualitative Analysis</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">In Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#S5.F2" title="Figure 2 ‣ Anaphora: ‣ 5 Results &amp; Analysis ‣ Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_tag">2</span></a>, we provide qualitative examples for GPT-4 that illustrate the scenarios for spatial prepositions, geometric shapes, and anaphora. We have included two samples for each scenario: one where the model response aligns with the ground truth (left), and another where it does not (right). The model is able to handle spatial references such as “opposite”, geometric shapes like “3x3”, and repetitions like “duplication”. However, it encounters challenges in accurately maintaining the exact count of blocks, translating words such as “close”, “ring” into actions and resolving what “it” refers to.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We
investigate prompting large language models to build complex structures for the Minecraft building task. Predicting suitable builder action sequences for a given natural language instruction is challenging as the large language models need to accurately interpret language abstractions, decode spatial co-references and reason about repetitions solely based on the in-context samples in the prompt. We compared multiple instruction-tuned LLMs, both closed and open-source. In addition to showcasing improved performance over baseline results, we also conduct an in-depth analysis of the generated responses across multiple dimensions. In the future, we plan to investigate model architectures that can address the shortcomings identified in the evaluated models.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitations</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">Like all other prompting approaches, our approach needs to be more robust to the usability of pre-trained large language models.

<span class="ltx_inline-enumerate" id="Sx1.I1">
<span class="ltx_inline-item" id="Sx1.I1.i1"><span class="ltx_tag ltx_tag_inline-item">(i)</span> <span class="ltx_text" id="Sx1.I1.i1.1">Not all grounded locations conform to the dimensions of the grid
</span></span>
<span class="ltx_inline-item" id="Sx1.I1.i2"><span class="ltx_tag ltx_tag_inline-item">(ii)</span> <span class="ltx_text" id="Sx1.I1.i2.1">Fails in interpreting instructions involving agent’s perspective such as “towards your right”, “behind you”, “away from you” etc.
</span></span>
<span class="ltx_inline-item" id="Sx1.I1.i3"><span class="ltx_tag ltx_tag_inline-item">(iii)</span> <span class="ltx_text" id="Sx1.I1.i3.1">Struggles in understanding abstractions in the dialogue
</span></span>
<span class="ltx_inline-item" id="Sx1.I1.i4"><span class="ltx_tag ltx_tag_inline-item">(iv)</span> <span class="ltx_text" id="Sx1.I1.i4.1">Since the study is carried out in a simulated world, extending it to real-world agents may lead to incorrect consequences.
</span></span>
</span>.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Ethics Statement</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">The research uses an open-source dataset (Minecraft dialogue corpus) and open-access, closed-API based pre-trained large language models. While these models are accessible, their usage is subject to legal restrictions as outlined in their respective terms of service and licensing agreements. Minecraft dialogue corpus does not include players’ personal, private information and does not contain any offensive conversations. However, the pre-trained LLMs, which inherit biases from their training data, may lead to code that favors certain styles and neglects others, hindering code portability. Another concern is the potential complexity of LLM-generated code, which can hinder end-user refinement and reuse. Moreover, it’s crucial to ensure LLM-generated responses are free from harmful code, as their direct execution could impact the entire system.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html" title="">Language models are few-shot learners</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen and Mooney (2011)</span>
<span class="ltx_bibblock">
David L. Chen and Raymond J. Mooney. 2011.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1609/AAAI.V25I1.7974" title="">Learning to interpret natural language navigation instructions from observations</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2011, San Francisco, California, USA, August 7-11, 2011</em>, pages 859–865. AAAI Press.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2021)</span>
<span class="ltx_bibblock">
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2107.03374" title="">Evaluating large language models trained on code</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">CoRR</em>, abs/2107.03374.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dettmers et al. (2023)</span>
<span class="ltx_bibblock">
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://papers.nips.cc/paper_files/paper/2023/hash/1feb87871436031bdc0f2beaa62a049b-Abstract-Conference.html" title="">Qlora: Efficient finetuning of quantized llms</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Driess et al. (2023)</span>
<span class="ltx_bibblock">
Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.mlr.press/v202/driess23a.html" title="">Palm-e: An embodied multimodal language model</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA</em>, volume 202 of <em class="ltx_emph ltx_font_italic" id="bib.bib5.2.2">Proceedings of Machine Learning Research</em>, pages 8469–8488. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goodrich and Schultz (2007)</span>
<span class="ltx_bibblock">
Michael A. Goodrich and Alan C. Schultz. 2007.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1561/1100000005" title="">Human-robot interaction: A survey</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Found. Trends Hum. Comput. Interact.</em>, 1(3):203–275.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hou and Ji (2024)</span>
<span class="ltx_bibblock">
Wenpin Hou and Zhicheng Ji. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2403.00894" title="">A systematic evaluation of large language models for generating programming code</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">CoRR</em>, abs/2403.00894.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2022)</span>
<span class="ltx_bibblock">
Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.mlr.press/v162/huang22a.html" title="">Language models as zero-shot planners: Extracting actionable knowledge for embodied agents</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA</em>, volume 162 of <em class="ltx_emph ltx_font_italic" id="bib.bib8.2.2">Proceedings of Machine Learning Research</em>, pages 9118–9147. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ichter et al. (2022)</span>
<span class="ltx_bibblock">
Brian Ichter, Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Dmitry Kalashnikov, Sergey Levine, Yao Lu, Carolina Parada, Kanishka Rao, Pierre Sermanet, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Mengyuan Yan, Noah Brown, Michael Ahn, Omar Cortes, Nicolas Sievers, Clayton Tan, Sichun Xu, Diego Reyes, Jarek Rettinghouse, Jornell Quiambao, Peter Pastor, Linda Luu, Kuang-Huei Lee, Yuheng Kuang, Sally Jesmonth, Nikhil J. Joshi, Kyle Jeffrey, Rosario Jauregui Ruano, Jasmine Hsu, Keerthana Gopalakrishnan, Byron David, Andy Zeng, and Chuyuan Kelly Fu. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.mlr.press/v205/ichter23a.html" title="">Do as I can, not as I say: Grounding language in robotic affordances</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Conference on Robot Learning, CoRL 2022, 14-18 December 2022, Auckland, New Zealand</em>, volume 205 of <em class="ltx_emph ltx_font_italic" id="bib.bib9.2.2">Proceedings of Machine Learning Research</em>, pages 287–318. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jayannavar et al. (2020)</span>
<span class="ltx_bibblock">
Prashant Jayannavar, Anjali Narayan-Chen, and Julia Hockenmaier. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2020.ACL-MAIN.232" title="">Learning to execute instructions in a minecraft dialogue</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020</em>, pages 2589–2602. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2024)</span>
<span class="ltx_bibblock">
Yeseung Kim, Dohyun Kim, Jieun Choi, Jisang Park, Nayoung Oh, and Daehyung Park. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2404.09228" title="">A survey on integration of large language models with intelligent robots</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">CoRR</em>, abs/2404.09228.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kiseleva et al. (2022)</span>
<span class="ltx_bibblock">
Julia Kiseleva, Ziming Li, Mohammad Aliannejadi, Shrestha Mohanty, Maartje ter Hoeve, Mikhail Burtsev, Alexey Skrynnik, Artem Zholus, Aleksandr I. Panov, Kavya Srinet, Arthur Szlam, Yuxuan Sun, Marc-Alexandre Côté, Katja Hofmann, Ahmed Awadallah, Linar Abdrazakov, Igor Churin, Putra Manggala, Kata Naszádi, Michiel van der Meer, and Taewoon Kim. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2205.02388" title="">Interactive grounded language understanding in a collaborative environment: IGLU 2021</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">CoRR</em>, abs/2205.02388.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al. (2023)</span>
<span class="ltx_bibblock">
Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/ICRA48891.2023.10160591" title="">Code as policies: Language model programs for embodied control</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">IEEE International Conference on Robotics and Automation, ICRA 2023, London, UK, May 29 - June 2, 2023</em>, pages 9493–9500. IEEE.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3560815" title="">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">ACM Comput. Surv.</em>, 55(9):195:1–195:35.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Madge and Poesio (2024)</span>
<span class="ltx_bibblock">
Chris Madge and Massimo Poesio. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2402.08392" title="">Large language models as minecraft agents</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">CoRR</em>, abs/2402.08392.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Matuszek et al. (2012)</span>
<span class="ltx_bibblock">
Cynthia Matuszek, Evan Herbst, Luke Zettlemoyer, and Dieter Fox. 2012.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1007/978-3-319-00065-7_28" title="">Learning to parse natural language commands to a robot control system</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Experimental Robotics - The 13th International Symposium on Experimental Robotics, ISER 2012, June 18-21, 2012, Québec City, Canada</em>, volume 88 of <em class="ltx_emph ltx_font_italic" id="bib.bib16.2.2">Springer Tracts in Advanced Robotics</em>, pages 403–415. Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mohanty et al. (2023)</span>
<span class="ltx_bibblock">
Shrestha Mohanty, Negar Arabzadeh, Julia Kiseleva, Artem Zholus, Milagro Teruel, Ahmed Awadallah, Yuxuan Sun, Kavya Srinet, and Arthur Szlam. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2305.10783" title="">Transforming human-centered AI collaboration: Redefining embodied agents capabilities through interactive grounded language instructions</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">CoRR</em>, abs/2305.10783.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Narayan-Chen et al. (2019)</span>
<span class="ltx_bibblock">
Anjali Narayan-Chen, Prashant Jayannavar, and Julia Hockenmaier. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/P19-1537" title="">Collaborative dialogue in minecraft</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers</em>, pages 5405–5415. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qi et al. (2020)</span>
<span class="ltx_bibblock">
Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher D. Manning. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2020.ACL-DEMOS.14" title="">Stanza: A python natural language processing toolkit for many human languages</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, ACL 2020, Online, July 5-10, 2020</em>, pages 101–108. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers and Gurevych (2019)</span>
<span class="ltx_bibblock">
Nils Reimers and Iryna Gurevych. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/D19-1410" title="">Sentence-bert: Sentence embeddings using siamese bert-networks</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019</em>, pages 3980–3990. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roy and Reiter (2005)</span>
<span class="ltx_bibblock">
Deb Roy and Ehud Reiter. 2005.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1016/J.ARTINT.2005.06.002" title="">Connecting language to the world</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Artif. Intell.</em>, 167(1-2):1–12.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rozière et al. (2023)</span>
<span class="ltx_bibblock">
Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton-Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2308.12950" title="">Code llama: Open foundation models for code</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">CoRR</em>, abs/2308.12950.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schlangen (2023)</span>
<span class="ltx_bibblock">
David Schlangen. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2023.FINDINGS-EMNLP.591" title="">On general language understanding</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023</em>, pages 8818–8825. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. (2022)</span>
<span class="ltx_bibblock">
Zhengxiang Shi, Yue Feng, and Aldo Lipani. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2022.FINDINGS-NAACL.158" title="">Learning to execute actions or ask clarification questions</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Findings of the Association for Computational Linguistics: NAACL 2022, Seattle, WA, United States, July 10-15, 2022</em>, pages 2060–2070. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et al. (2023)</span>
<span class="ltx_bibblock">
Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1007/S10514-023-10135-3" title="">Progprompt: program generation for situated robot task planning using large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Auton. Robots</em>, 47(8):999–1012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. (2023)</span>
<span class="ltx_bibblock">
Chan Hee Song, Brian M. Sadler, Jiaman Wu, Wei-Lun Chao, Clayton Washington, and Yu Su. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/ICCV51070.2023.00280" title="">Llm-planner: Few-shot grounded planning for embodied agents with large language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023</em>, pages 2986–2997. IEEE.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tellex et al. (2020)</span>
<span class="ltx_bibblock">
Stefanie Tellex, Nakul Gopalan, Hadas Kress-Gazit, and Cynthia Matuszek. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1146/ANNUREV-CONTROL-101119-071628" title="">Robots that use language</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Annu. Rev. Control. Robotics Auton. Syst.</em>, 3:25–55.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thomaz and Breazeal (2008)</span>
<span class="ltx_bibblock">
Andrea Lockerd Thomaz and Cynthia Breazeal. 2008.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1016/J.ARTINT.2007.09.009" title="">Teachable robots: Understanding human teaching behavior to build more effective robot learners</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Artif. Intell.</em>, 172(6-7):716–737.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2302.13971" title="">Llama: Open and efficient foundation language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">CoRR</em>, abs/2302.13971.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vemprala et al. (2024)</span>
<span class="ltx_bibblock">
Sai Vemprala, Rogerio Bonatti, Arthur Bucker, and Ashish Kapoor. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/ACCESS.2024.3387941" title="">Chatgpt for robotics: Design principles and model abilities</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">IEEE Access</em>, 12:55682–55696.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2021)</span>
<span class="ltx_bibblock">
Yue Wang, Weishi Wang, Shafiq R. Joty, and Steven C. H. Hoi. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2021.EMNLP-MAIN.685" title="">Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021</em>, pages 8696–8708. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html" title="">Chain-of-thought prompting elicits reasoning in large language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Winograd (1971)</span>
<span class="ltx_bibblock">
Terry Winograd. 1971.

</span>
<span class="ltx_bibblock">Procedures as a representation for data in a computer program for understanding natural language.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2023)</span>
<span class="ltx_bibblock">
Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, and Thomas A. Funkhouser. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1007/S10514-023-10139-Z" title="">Tidybot: personalized robot assistance with large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Auton. Robots</em>, 47(8):1087–1102.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xi et al. (2023)</span>
<span class="ltx_bibblock">
Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huan, and Tao Gui. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2309.07864" title="">The rise and potential of large language model based agents: A survey</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">CoRR</em>, abs/2309.07864.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al. (2023)</span>
<span class="ltx_bibblock">
Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Marcin Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael S. Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Pete Florence. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/pdf?id=G2Q2Mh3avow" title="">Socratic models: Composing zero-shot multimodal reasoning with language</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023</em>. OpenReview.net.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2023)</span>
<span class="ltx_bibblock">
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2303.18223" title="">A survey of large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">CoRR</em>, abs/2303.18223.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Prompt Structure</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">The proposed task aims to investigate the capability of LLMs in accurately predicting actions based on input instructions. To generate an accurate action sequence, the models must understand the environment, follow dialogue history, and interpret the input instruction within the current context.</p>
</div>
<div class="ltx_para" id="A1.p2">
<p class="ltx_p" id="A1.p2.1">To achieve this, we construct a multi-part prompt, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#A1.F3" title="Figure 3 ‣ Appendix A Prompt Structure ‣ Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_tag">3</span></a>, to query the LLM. This prompt includes several components: system information, environment information, task information, context information, and other guidelines. The system information specifies the expected behavior for the LLM. The environment information provides detailed information about the build region, its boundaries, and block properties. Specifically, the Minecraft dialogue corpus limits the build region to an 11x9x11 grid, and the builder is provided with an inventory of 120 blocks in six colors <math alttext="colour\in" class="ltx_Math" display="inline" id="A1.p2.1.m1.1"><semantics id="A1.p2.1.m1.1a"><mrow id="A1.p2.1.m1.1.1" xref="A1.p2.1.m1.1.1.cmml"><mrow id="A1.p2.1.m1.1.1.2" xref="A1.p2.1.m1.1.1.2.cmml"><mi id="A1.p2.1.m1.1.1.2.2" xref="A1.p2.1.m1.1.1.2.2.cmml">c</mi><mo id="A1.p2.1.m1.1.1.2.1" xref="A1.p2.1.m1.1.1.2.1.cmml">⁢</mo><mi id="A1.p2.1.m1.1.1.2.3" xref="A1.p2.1.m1.1.1.2.3.cmml">o</mi><mo id="A1.p2.1.m1.1.1.2.1a" xref="A1.p2.1.m1.1.1.2.1.cmml">⁢</mo><mi id="A1.p2.1.m1.1.1.2.4" xref="A1.p2.1.m1.1.1.2.4.cmml">l</mi><mo id="A1.p2.1.m1.1.1.2.1b" xref="A1.p2.1.m1.1.1.2.1.cmml">⁢</mo><mi id="A1.p2.1.m1.1.1.2.5" xref="A1.p2.1.m1.1.1.2.5.cmml">o</mi><mo id="A1.p2.1.m1.1.1.2.1c" xref="A1.p2.1.m1.1.1.2.1.cmml">⁢</mo><mi id="A1.p2.1.m1.1.1.2.6" xref="A1.p2.1.m1.1.1.2.6.cmml">u</mi><mo id="A1.p2.1.m1.1.1.2.1d" xref="A1.p2.1.m1.1.1.2.1.cmml">⁢</mo><mi id="A1.p2.1.m1.1.1.2.7" xref="A1.p2.1.m1.1.1.2.7.cmml">r</mi></mrow><mo id="A1.p2.1.m1.1.1.1" xref="A1.p2.1.m1.1.1.1.cmml">∈</mo><mi id="A1.p2.1.m1.1.1.3" xref="A1.p2.1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="A1.p2.1.m1.1b"><apply id="A1.p2.1.m1.1.1.cmml" xref="A1.p2.1.m1.1.1"><in id="A1.p2.1.m1.1.1.1.cmml" xref="A1.p2.1.m1.1.1.1"></in><apply id="A1.p2.1.m1.1.1.2.cmml" xref="A1.p2.1.m1.1.1.2"><times id="A1.p2.1.m1.1.1.2.1.cmml" xref="A1.p2.1.m1.1.1.2.1"></times><ci id="A1.p2.1.m1.1.1.2.2.cmml" xref="A1.p2.1.m1.1.1.2.2">𝑐</ci><ci id="A1.p2.1.m1.1.1.2.3.cmml" xref="A1.p2.1.m1.1.1.2.3">𝑜</ci><ci id="A1.p2.1.m1.1.1.2.4.cmml" xref="A1.p2.1.m1.1.1.2.4">𝑙</ci><ci id="A1.p2.1.m1.1.1.2.5.cmml" xref="A1.p2.1.m1.1.1.2.5">𝑜</ci><ci id="A1.p2.1.m1.1.1.2.6.cmml" xref="A1.p2.1.m1.1.1.2.6">𝑢</ci><ci id="A1.p2.1.m1.1.1.2.7.cmml" xref="A1.p2.1.m1.1.1.2.7">𝑟</ci></apply><csymbol cd="latexml" id="A1.p2.1.m1.1.1.3.cmml" xref="A1.p2.1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.1.m1.1c">colour\in</annotation><annotation encoding="application/x-llamapun" id="A1.p2.1.m1.1d">italic_c italic_o italic_l italic_o italic_u italic_r ∈</annotation></semantics></math> (<span class="ltx_text ltx_font_italic" id="A1.p2.1.1">red</span>, <span class="ltx_text ltx_font_italic" id="A1.p2.1.2">blue</span>, <span class="ltx_text ltx_font_italic" id="A1.p2.1.3">orange</span>, <span class="ltx_text ltx_font_italic" id="A1.p2.1.4">purple</span>, <span class="ltx_text ltx_font_italic" id="A1.p2.1.5">yellow</span>, <span class="ltx_text ltx_font_italic" id="A1.p2.1.6">green</span>). The task information outlines the format of the expected outcome.</p>
</div>
<div class="ltx_para" id="A1.p3">
<p class="ltx_p" id="A1.p3.1">Following this, the context information includes in-context examples relevant to the current test instruction. Adopting the approach recommended by <cite class="ltx_cite ltx_citemacro_citet">Song et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib26" title="">2023</a>)</cite>, we use adaptive prompts, dynamically retrieving examples from the training set that are similar to the current test instruction. Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#A1.F4" title="Figure 4 ‣ Appendix A Prompt Structure ‣ Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_tag">4</span></a> shows how the in-context examples change based on the test instruction. This structure offers detailed information for the LLM to generate action sequences for the input instructions within the given environment.</p>
</div>
<figure class="ltx_figure" id="A1.F3"><svg class="ltx_picture ltx_centering" height="804.51" id="A1.F3.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,804.51) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 798.6 C 0 801.86 2.64 804.51 5.91 804.51 L 594.09 804.51 C 597.36 804.51 600 801.86 600 798.6 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 798.6 C 1.97 800.78 3.73 802.54 5.91 802.54 L 594.09 802.54 C 596.27 802.54 598.03 800.78 598.03 798.6 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 7.29 13.78)"><foreignobject color="#000000" height="776.95" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="585.42">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A1.F3.pic1.1.1.1.1.1" style="width:423.1pt;">
<span class="ltx_p" id="A1.F3.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_smallcaps" id="A1.F3.pic1.1.1.1.1.1.1.1">template A.0.1
<br class="ltx_break"/>
<br class="ltx_break"/></span><span class="ltx_text ltx_font_bold" id="A1.F3.pic1.1.1.1.1.1.1.2">System Info</span><span class="ltx_text ltx_font_typewriter" id="A1.F3.pic1.1.1.1.1.1.1.3">
<br class="ltx_break"/>
<br class="ltx_break"/>You are an expert at interpreting natural language instructions and translating them into specific actions for placing colored blocks within a 3D grid.

<br class="ltx_break"/>
<br class="ltx_break"/></span><span class="ltx_text ltx_font_bold" id="A1.F3.pic1.1.1.1.1.1.1.4">Environment Info</span><span class="ltx_text ltx_font_typewriter" id="A1.F3.pic1.1.1.1.1.1.1.5">
<br class="ltx_break"/>
<br class="ltx_break"/>The environment is structured as an 11x9x11 grid, with each cell representing a single block. The grid is organized such that columns align along the X-axis and rows along the Z-axis, employing Python indexing for cell identification. The X and Z axes range from -5 to 5, encompassing length and width, while the Y-axis ascends from 1 to 9, indicating height. This grid, following an XZY orientation, places X and Z as horizontal dimensions and Y as the vertical dimension. The ground level is defined at Y=1, with higher Y-values signifying increased elevation, thereby representing the vertical aspect of the environment. There are 20 blocks for each of the following colors: blue, orange, red, green, yellow, and purple.

<br class="ltx_break"/>
<br class="ltx_break"/>Please import the following functions for object interactions:

<br class="ltx_break"/>pick(color,x,y,z): This function picks the object at the specified location.

<br class="ltx_break"/>place(color,x,y,z): This function places the object at the specified location.

<br class="ltx_break"/>
<br class="ltx_break"/>You should only use these specified functions and not create any hypothetical functions.

<br class="ltx_break"/>
<br class="ltx_break"/>Please use the pick function only to select the pieces that you have previously placed on the board. You can safely assume that the available blocks are automatically selected, and therefore, you can directly use the "place()" function without needing to call the "pick()" function first.

<br class="ltx_break"/>
<br class="ltx_break"/></span><span class="ltx_text ltx_font_bold" id="A1.F3.pic1.1.1.1.1.1.1.6">Task Info</span><span class="ltx_text ltx_font_typewriter" id="A1.F3.pic1.1.1.1.1.1.1.7">
<br class="ltx_break"/>
<br class="ltx_break"/>Your task is to interpret natural language instructions and generate the corresponding code representation. For each instruction labeled "Instruction" please respond with code under the label "Output" followed by a newline.

<br class="ltx_break"/>
<br class="ltx_break"/></span><span class="ltx_text ltx_font_bold" id="A1.F3.pic1.1.1.1.1.1.1.8">Context Info</span><span class="ltx_text ltx_font_typewriter" id="A1.F3.pic1.1.1.1.1.1.1.9">
<br class="ltx_break"/>
<br class="ltx_break"/>$INCONTEXT_SAMPLES

<br class="ltx_break"/>
<br class="ltx_break"/></span><span class="ltx_text ltx_font_bold" id="A1.F3.pic1.1.1.1.1.1.1.10">Other Info</span><span class="ltx_text ltx_font_typewriter" id="A1.F3.pic1.1.1.1.1.1.1.11">
<br class="ltx_break"/>
<br class="ltx_break"/>Do not generate any other text/explanations. Use python code to express the solution. Please keep the solutions simple and clear. Do not use loops, comments in the solution. Do not generate the instructions on your own. Stick to the given format. Think step by step.

<br class="ltx_break"/>
<br class="ltx_break"/>Let’s get started.

<br class="ltx_break"/>
<br class="ltx_break"/>$TEST_INSTRUCTION</span></span>
</span></foreignobject></g></g></svg>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Prompt template used for the action prediction task. The system information specifies system level behavior, the environment information indicates the environment details of the user-agent environment, the context information describes the in-context examples, task information indicates the specific response format to follow.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F4"><svg class="ltx_picture ltx_centering" height="670.83" id="A1.F4.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,670.83) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 664.92 C 0 668.18 2.64 670.83 5.91 670.83 L 594.09 670.83 C 597.36 670.83 600 668.18 600 664.92 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 664.92 C 1.97 667.09 3.73 668.86 5.91 668.86 L 594.09 668.86 C 596.27 668.86 598.03 667.09 598.03 664.92 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 7.29 13.78)"><foreignobject color="#000000" height="643.27" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="585.42">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A1.F4.pic1.1.1.1.1.1" style="width:423.1pt;">
<span class="ltx_p" id="A1.F4.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_smallcaps" id="A1.F4.pic1.1.1.1.1.1.1.1">template A.0.2
<br class="ltx_break"/>
<br class="ltx_break"/></span><span class="ltx_text ltx_font_bold" id="A1.F4.pic1.1.1.1.1.1.1.2">INCONTEXT_SAMPLES</span><span class="ltx_text ltx_font_typewriter" id="A1.F4.pic1.1.1.1.1.1.1.3">
<br class="ltx_break"/>
<br class="ltx_break"/>The INCONTEXT_SAMPLES varies for each turn. Below is an example

<br class="ltx_break"/>
<br class="ltx_break"/></span><span class="ltx_text ltx_font_bold" id="A1.F4.pic1.1.1.1.1.1.1.4">Instruction</span><span class="ltx_text ltx_font_typewriter" id="A1.F4.pic1.1.1.1.1.1.1.5">
<br class="ltx_break"/>
<br class="ltx_break"/>Mission has started.. This structure looks fun- nothing will touch the ground!. cool!. We’ll start with a row of 4 purple bricks, a layer above the ground

<br class="ltx_break"/>
<br class="ltx_break"/></span><span class="ltx_text ltx_font_bold" id="A1.F4.pic1.1.1.1.1.1.1.6">Output</span><span class="ltx_text ltx_font_typewriter" id="A1.F4.pic1.1.1.1.1.1.1.7">
<br class="ltx_break"/>
<br class="ltx_break"/>place(color=’purple’,x=1,y=1,z=1)</span></span>
<span class="ltx_p" id="A1.F4.pic1.1.1.1.1.1.2"><span class="ltx_text ltx_font_typewriter" id="A1.F4.pic1.1.1.1.1.1.2.1">place(color=’purple’,x=1,y=2,z=1)</span></span>
<span class="ltx_p" id="A1.F4.pic1.1.1.1.1.1.3"><span class="ltx_text ltx_font_typewriter" id="A1.F4.pic1.1.1.1.1.1.3.1">pick(color=’purple’,x=1,y=1,z=1)</span></span>
<span class="ltx_p" id="A1.F4.pic1.1.1.1.1.1.4"><span class="ltx_text ltx_font_typewriter" id="A1.F4.pic1.1.1.1.1.1.4.1">place(color=’purple’,x=0,y=2,z=1)</span></span>
<span class="ltx_p" id="A1.F4.pic1.1.1.1.1.1.5"><span class="ltx_text ltx_font_typewriter" id="A1.F4.pic1.1.1.1.1.1.5.1">place(color=’purple’,x=-1,y=2,z=1)</span></span>
<span class="ltx_p" id="A1.F4.pic1.1.1.1.1.1.6"><span class="ltx_text ltx_font_typewriter" id="A1.F4.pic1.1.1.1.1.1.6.1">place(color=’purple’,x=-2,y=2,z=1)

<br class="ltx_break"/>
<br class="ltx_break"/></span><span class="ltx_text ltx_font_bold" id="A1.F4.pic1.1.1.1.1.1.6.2">Instruction</span><span class="ltx_text ltx_font_typewriter" id="A1.F4.pic1.1.1.1.1.1.6.3">
<br class="ltx_break"/>
<br class="ltx_break"/>Mission has started.. we’ll start with two lines of purple bricks, parallel to one another. seperated by an empty space

<br class="ltx_break"/>
<br class="ltx_break"/></span><span class="ltx_text ltx_font_bold" id="A1.F4.pic1.1.1.1.1.1.6.4">Output</span><span class="ltx_text ltx_font_typewriter" id="A1.F4.pic1.1.1.1.1.1.6.5">
<br class="ltx_break"/>
<br class="ltx_break"/>place(color=’purple’,x=1,y=1,z=-2)</span></span>
<span class="ltx_p" id="A1.F4.pic1.1.1.1.1.1.7"><span class="ltx_text ltx_font_typewriter" id="A1.F4.pic1.1.1.1.1.1.7.1">pick(color=’purple’,x=1,y=1,z=-2)

<br class="ltx_break"/>
<br class="ltx_break"/></span><span class="ltx_text ltx_font_bold" id="A1.F4.pic1.1.1.1.1.1.7.2">Instruction</span><span class="ltx_text ltx_font_typewriter" id="A1.F4.pic1.1.1.1.1.1.7.3">
<br class="ltx_break"/>
<br class="ltx_break"/>Mission has started.. We’ll start with a column of 5 red bricks

<br class="ltx_break"/>
<br class="ltx_break"/></span><span class="ltx_text ltx_font_bold" id="A1.F4.pic1.1.1.1.1.1.7.4">Output</span><span class="ltx_text ltx_font_typewriter" id="A1.F4.pic1.1.1.1.1.1.7.5">
<br class="ltx_break"/>
<br class="ltx_break"/>place(color=’red’,x=0,y=1,z=1)</span></span>
<span class="ltx_p" id="A1.F4.pic1.1.1.1.1.1.8"><span class="ltx_text ltx_font_typewriter" id="A1.F4.pic1.1.1.1.1.1.8.1">place(color=’red’,x=0,y=2,z=1)</span></span>
<span class="ltx_p" id="A1.F4.pic1.1.1.1.1.1.9"><span class="ltx_text ltx_font_typewriter" id="A1.F4.pic1.1.1.1.1.1.9.1">place(color=’red’,x=0,y=3,z=1)</span></span>
<span class="ltx_p" id="A1.F4.pic1.1.1.1.1.1.10"><span class="ltx_text ltx_font_typewriter" id="A1.F4.pic1.1.1.1.1.1.10.1">place(color=’red’,x=0,y=4,z=1)</span></span>
<span class="ltx_p" id="A1.F4.pic1.1.1.1.1.1.11"><span class="ltx_text ltx_font_typewriter" id="A1.F4.pic1.1.1.1.1.1.11.1">place(color=’red’,x=0,y=5,z=1)

<br class="ltx_break"/>
<br class="ltx_break"/></span></span>
<span class="ltx_p" id="A1.F4.pic1.1.1.1.1.1.12"><span class="ltx_text ltx_font_bold" id="A1.F4.pic1.1.1.1.1.1.12.1">TEST_INSTRUCTION</span><span class="ltx_text ltx_font_typewriter" id="A1.F4.pic1.1.1.1.1.1.12.2">
<br class="ltx_break"/>
<br class="ltx_break"/>Mission has started.. hi again!. Hello! This structure will start with a column of 5 purple bricks. It’s all purple.</span></span>
</span></foreignobject></g></g></svg>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Retrieval of relevant in-context examples based on current test instruction</figcaption>
</figure>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Ablation Study</h3>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">We investigate how the building blocks of the prompt structure shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#A1.F3" title="Figure 3 ‣ Appendix A Prompt Structure ‣ Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_tag">3</span></a> impact overall task performance. Using the validation set of the Minecraft Dialogue dataset for our ablation study, we observe that the prompt structure with all components is optimal for the action prediction task and the LLMs in our experiments, as demonstrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#A3.T2" title="Table 2 ‣ Appendix C Detailed Error Analysis ‣ Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_tag">2</span></a>. Specifically, omitting in-context examples results in poor performance, while excluding environment information reduced the score slightly. Consequently, we use the prompt featuring all components with three in-context examples to ensure the best performance.</p>
</div>
<div class="ltx_para" id="A1.SS1.p2">
<p class="ltx_p" id="A1.SS1.p2.1">We also experimented with CodeLlama-34b <span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ai.meta.com/blog/code-llama-large-language-model-coding/" title="">https://ai.meta.com/blog/code-llama-large-language-model-coding/</a></span></span></span> model. However, the results were not sat-
isfactory because the generated response contained new instructions generated by model (hallucinations) and it was impossible to evaluate the closeness of the resulting generation to the ground truth, and hence this model is excluded from the experimental results.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Model Variants</h3>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">For this study, we utilized three advanced large language models: GPT-4-O, Llama-3-8b, and Llama-3-70b. These models are instruction-tuned, enabling them to generate code snippets based on input text instructions. GPT-4-o and Llama-3-70b were accessed via their respective APIs, incurring a cost of $12 for usage. For the Llama-3-8b model, experiments were conducted locally on a single A100 GPU with 80GB of memory, taking 10 hours in total.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Fine tuning</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">Since the out-of-the-box performance of the LLMs sets a lower bound for the action prediction task, we explore fine-tuning to see if it improves task performance. We use the training set of the Minecraft Dialogue corpus and fine-tune the Llama-3-8b model. This deliberate choice is made because it is the smallest open-source model we have used and has the lowest performance among all models. For the fine-tuning process, we use Q-LORA <cite class="ltx_cite ltx_citemacro_citep">(Dettmers et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#bib.bib4" title="">2023</a>)</cite> to reduce the memory footprint during fine-tuning by applying low-rank adaptations, allowing efficient fine-tuning on limited hardware.</p>
</div>
<div class="ltx_para" id="A2.p2">
<p class="ltx_p" id="A2.p2.1">We experimented with hyperparameter changes (number of epochs, learning rate) and finally chose the optimal performance setting, which includes 15 epochs and a learning rate of 0.0002. The model is configured using the Adam optimizer with a batch size of 32. The training process spans 15 epochs, with an early stopping condition based on evaluation loss to prevent over-fitting. Specifically, if the validation loss does not decrease for 5 consecutive epochs, the training is stopped. The validation set is used for evaluation during training to monitor the model’s performance and guide the early stopping mechanism.</p>
</div>
<div class="ltx_para" id="A2.p3">
<p class="ltx_p" id="A2.p3.1">This fine-tuned model is then used for testing on the test set and is indicated as the Llama-3-8b (fine-tuned) model in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.17553v1#S5.T1" title="Table 1 ‣ Spatial Prepositions: ‣ 5 Results &amp; Analysis ‣ Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft"><span class="ltx_text ltx_ref_tag">1</span></a>. Compared to the baseline (pre-trained) model, the fine-tuned model shows an enhancement of <math alttext="\sim 6\%" class="ltx_Math" display="inline" id="A2.p3.1.m1.1"><semantics id="A2.p3.1.m1.1a"><mrow id="A2.p3.1.m1.1.1" xref="A2.p3.1.m1.1.1.cmml"><mi id="A2.p3.1.m1.1.1.2" xref="A2.p3.1.m1.1.1.2.cmml"></mi><mo id="A2.p3.1.m1.1.1.1" xref="A2.p3.1.m1.1.1.1.cmml">∼</mo><mrow id="A2.p3.1.m1.1.1.3" xref="A2.p3.1.m1.1.1.3.cmml"><mn id="A2.p3.1.m1.1.1.3.2" xref="A2.p3.1.m1.1.1.3.2.cmml">6</mn><mo id="A2.p3.1.m1.1.1.3.1" xref="A2.p3.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.p3.1.m1.1b"><apply id="A2.p3.1.m1.1.1.cmml" xref="A2.p3.1.m1.1.1"><csymbol cd="latexml" id="A2.p3.1.m1.1.1.1.cmml" xref="A2.p3.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="A2.p3.1.m1.1.1.2.cmml" xref="A2.p3.1.m1.1.1.2">absent</csymbol><apply id="A2.p3.1.m1.1.1.3.cmml" xref="A2.p3.1.m1.1.1.3"><csymbol cd="latexml" id="A2.p3.1.m1.1.1.3.1.cmml" xref="A2.p3.1.m1.1.1.3.1">percent</csymbol><cn id="A2.p3.1.m1.1.1.3.2.cmml" type="integer" xref="A2.p3.1.m1.1.1.3.2">6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p3.1.m1.1c">\sim 6\%</annotation><annotation encoding="application/x-llamapun" id="A2.p3.1.m1.1d">∼ 6 %</annotation></semantics></math> in the F1-score. This improvement, although marginal, indicates a potential to improve the model’s ability for the action prediction task.</p>
</div>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Detailed Error Analysis</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.9">In this section, we delve into the performance of other models (Llama-3-8b and Llama-3-70b) across the specific categories of spatial relations, real-world/geometric shapes, and anaphora, which were identified as key factors influencing the overall performance. Out of <math alttext="75.42" class="ltx_Math" display="inline" id="A3.p1.1.m1.1"><semantics id="A3.p1.1.m1.1a"><mn id="A3.p1.1.m1.1.1" xref="A3.p1.1.m1.1.1.cmml">75.42</mn><annotation-xml encoding="MathML-Content" id="A3.p1.1.m1.1b"><cn id="A3.p1.1.m1.1.1.cmml" type="float" xref="A3.p1.1.m1.1.1">75.42</cn></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.1.m1.1c">75.42</annotation><annotation encoding="application/x-llamapun" id="A3.p1.1.m1.1d">75.42</annotation></semantics></math>% of the spatial preposition references, Llama-3-70b could predict correct action sequences for <math alttext="22.66" class="ltx_Math" display="inline" id="A3.p1.2.m2.1"><semantics id="A3.p1.2.m2.1a"><mn id="A3.p1.2.m2.1.1" xref="A3.p1.2.m2.1.1.cmml">22.66</mn><annotation-xml encoding="MathML-Content" id="A3.p1.2.m2.1b"><cn id="A3.p1.2.m2.1.1.cmml" type="float" xref="A3.p1.2.m2.1.1">22.66</cn></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.2.m2.1c">22.66</annotation><annotation encoding="application/x-llamapun" id="A3.p1.2.m2.1d">22.66</annotation></semantics></math>% of utterances. For utterances containing geometric and real-world shapes, the model correctly predicted <math alttext="14.11" class="ltx_Math" display="inline" id="A3.p1.3.m3.1"><semantics id="A3.p1.3.m3.1a"><mn id="A3.p1.3.m3.1.1" xref="A3.p1.3.m3.1.1.cmml">14.11</mn><annotation-xml encoding="MathML-Content" id="A3.p1.3.m3.1b"><cn id="A3.p1.3.m3.1.1.cmml" type="float" xref="A3.p1.3.m3.1.1">14.11</cn></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.3.m3.1c">14.11</annotation><annotation encoding="application/x-llamapun" id="A3.p1.3.m3.1d">14.11</annotation></semantics></math>% out of <math alttext="29.85" class="ltx_Math" display="inline" id="A3.p1.4.m4.1"><semantics id="A3.p1.4.m4.1a"><mn id="A3.p1.4.m4.1.1" xref="A3.p1.4.m4.1.1.cmml">29.85</mn><annotation-xml encoding="MathML-Content" id="A3.p1.4.m4.1b"><cn id="A3.p1.4.m4.1.1.cmml" type="float" xref="A3.p1.4.m4.1.1">29.85</cn></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.4.m4.1c">29.85</annotation><annotation encoding="application/x-llamapun" id="A3.p1.4.m4.1d">29.85</annotation></semantics></math>% of utterances. Anaphora resolution showed an accuracy of <math alttext="20.5" class="ltx_Math" display="inline" id="A3.p1.5.m5.1"><semantics id="A3.p1.5.m5.1a"><mn id="A3.p1.5.m5.1.1" xref="A3.p1.5.m5.1.1.cmml">20.5</mn><annotation-xml encoding="MathML-Content" id="A3.p1.5.m5.1b"><cn id="A3.p1.5.m5.1.1.cmml" type="float" xref="A3.p1.5.m5.1.1">20.5</cn></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.5.m5.1c">20.5</annotation><annotation encoding="application/x-llamapun" id="A3.p1.5.m5.1d">20.5</annotation></semantics></math>% correctly predicted out of <math alttext="46.81" class="ltx_Math" display="inline" id="A3.p1.6.m6.1"><semantics id="A3.p1.6.m6.1a"><mn id="A3.p1.6.m6.1.1" xref="A3.p1.6.m6.1.1.cmml">46.81</mn><annotation-xml encoding="MathML-Content" id="A3.p1.6.m6.1b"><cn id="A3.p1.6.m6.1.1.cmml" type="float" xref="A3.p1.6.m6.1.1">46.81</cn></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.6.m6.1c">46.81</annotation><annotation encoding="application/x-llamapun" id="A3.p1.6.m6.1d">46.81</annotation></semantics></math>% of utterances. Similarly, the Llama-3-8b model predicted <math alttext="9.44" class="ltx_Math" display="inline" id="A3.p1.7.m7.1"><semantics id="A3.p1.7.m7.1a"><mn id="A3.p1.7.m7.1.1" xref="A3.p1.7.m7.1.1.cmml">9.44</mn><annotation-xml encoding="MathML-Content" id="A3.p1.7.m7.1b"><cn id="A3.p1.7.m7.1.1.cmml" type="float" xref="A3.p1.7.m7.1.1">9.44</cn></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.7.m7.1c">9.44</annotation><annotation encoding="application/x-llamapun" id="A3.p1.7.m7.1d">9.44</annotation></semantics></math>% correctly for spatial prepositions, <math alttext="6.22" class="ltx_Math" display="inline" id="A3.p1.8.m8.1"><semantics id="A3.p1.8.m8.1a"><mn id="A3.p1.8.m8.1.1" xref="A3.p1.8.m8.1.1.cmml">6.22</mn><annotation-xml encoding="MathML-Content" id="A3.p1.8.m8.1b"><cn id="A3.p1.8.m8.1.1.cmml" type="float" xref="A3.p1.8.m8.1.1">6.22</cn></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.8.m8.1c">6.22</annotation><annotation encoding="application/x-llamapun" id="A3.p1.8.m8.1d">6.22</annotation></semantics></math>% for real-world and geometric shapes, and <math alttext="10.05" class="ltx_Math" display="inline" id="A3.p1.9.m9.1"><semantics id="A3.p1.9.m9.1a"><mn id="A3.p1.9.m9.1.1" xref="A3.p1.9.m9.1.1.cmml">10.05</mn><annotation-xml encoding="MathML-Content" id="A3.p1.9.m9.1b"><cn id="A3.p1.9.m9.1.1.cmml" type="float" xref="A3.p1.9.m9.1.1">10.05</cn></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.9.m9.1c">10.05</annotation><annotation encoding="application/x-llamapun" id="A3.p1.9.m9.1d">10.05</annotation></semantics></math>% for anaphora. These results highlight the challenges both models face in comprehending and predicting builder actions based on these linguistic elements.</p>
</div>
<figure class="ltx_table" id="A3.T2">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A3.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A3.T2.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A3.T2.1.1.1.1.1">Prompt</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A3.T2.1.1.1.2.1">F1</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.T2.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T2.1.2.1.1">System Info + Env Info + Task Info + Context Info (Zero Samples) + Other Info</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T2.1.2.1.2">0.15</td>
</tr>
<tr class="ltx_tr" id="A3.T2.1.3.2">
<td class="ltx_td ltx_align_center" id="A3.T2.1.3.2.1">System Info + Env Info + Task Info + Context Info (One Sample) + Other Info</td>
<td class="ltx_td ltx_align_center" id="A3.T2.1.3.2.2">0.17</td>
</tr>
<tr class="ltx_tr" id="A3.T2.1.4.3">
<td class="ltx_td ltx_align_center" id="A3.T2.1.4.3.1">System Info + Env Info + Task Info + Context Info (Two Samples) + Other Info</td>
<td class="ltx_td ltx_align_center" id="A3.T2.1.4.3.2">0.18</td>
</tr>
<tr class="ltx_tr" id="A3.T2.1.5.4">
<td class="ltx_td ltx_align_center" id="A3.T2.1.5.4.1">System Info + Env Info + Task Info + Context Info (Three Samples) + Other Info</td>
<td class="ltx_td ltx_align_center" id="A3.T2.1.5.4.2">0.18</td>
</tr>
<tr class="ltx_tr" id="A3.T2.1.6.5">
<td class="ltx_td ltx_align_center" id="A3.T2.1.6.5.1">System Info + Env Info + Task Info + Context Info (Four Samples) + Other Info</td>
<td class="ltx_td ltx_align_center" id="A3.T2.1.6.5.2">0.18</td>
</tr>
<tr class="ltx_tr" id="A3.T2.1.7.6">
<td class="ltx_td ltx_align_center" id="A3.T2.1.7.6.1">System Info + Env Info + Task Info + Context Info (Five Samples) + Other Info</td>
<td class="ltx_td ltx_align_center" id="A3.T2.1.7.6.2">0.18</td>
</tr>
<tr class="ltx_tr" id="A3.T2.1.8.7">
<td class="ltx_td ltx_align_center" id="A3.T2.1.8.7.1">Env Info + Task Info + Context Info (Three Samples) + Other Info</td>
<td class="ltx_td ltx_align_center" id="A3.T2.1.8.7.2">0.19</td>
</tr>
<tr class="ltx_tr" id="A3.T2.1.9.8">
<td class="ltx_td ltx_align_center" id="A3.T2.1.9.8.1">System Info + Task Info + Context Info (Three Samples) + Other Info</td>
<td class="ltx_td ltx_align_center" id="A3.T2.1.9.8.2">0.17</td>
</tr>
<tr class="ltx_tr" id="A3.T2.1.10.9">
<td class="ltx_td ltx_align_center" id="A3.T2.1.10.9.1">System Info + Env Info + Context Info (Three Samples) + Other Info</td>
<td class="ltx_td ltx_align_center" id="A3.T2.1.10.9.2">0.17</td>
</tr>
<tr class="ltx_tr" id="A3.T2.1.11.10">
<td class="ltx_td ltx_align_center" id="A3.T2.1.11.10.1">System Info + Env Info + Context Info (Three Samples)</td>
<td class="ltx_td ltx_align_center" id="A3.T2.1.11.10.2">0.17</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Ablation study for the number of in-context examples and components of the prompt structre on validation split of the Minecraft dataset using Llama-3-8b.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="388" id="A3.F5.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Excerpt of an utterance that contains the builder mistakes from the game-id: B29-A1-C151-1524078449685. The action sequence pairs where an item is first placed and later picked up is highlighted with the same colour.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="399" id="A3.F6.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Examples utterances from the architect where the given command is underspecified</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jun 25 13:42:42 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
