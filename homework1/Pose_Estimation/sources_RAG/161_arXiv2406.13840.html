<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>StackRAG Agent: Improving Developer Answers with Retrieval-Augmented Generation</title>
<!--Generated on Wed Jun 19 21:05:55 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Multiagent tool,  RAG-based tool,  LLM,  Stack Overflow
" lang="en" name="keywords"/>
<base href="/html/2406.13840v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.13840v1#S1" title="In StackRAG Agent: Improving Developer Answers with Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.13840v1#S2" title="In StackRAG Agent: Improving Developer Answers with Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Works</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.13840v1#S3" title="In StackRAG Agent: Improving Developer Answers with Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Architecture</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.13840v1#S4" title="In StackRAG Agent: Improving Developer Answers with Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Evaluation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.13840v1#S5" title="In StackRAG Agent: Improving Developer Answers with Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Limitations and Improvements</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.13840v1#S6" title="In StackRAG Agent: Improving Developer Answers with Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">StackRAG Agent: Improving Developer Answers with Retrieval-Augmented Generation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Davit Abrahamyan
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id1.1.id1">Department of Computer Science</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id2.2.id2">University of British Columbia
<br class="ltx_break"/></span>Kelowna, Canada 
<br class="ltx_break"/>0009-0004-4397-1699
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Fatemeh H. Fard
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id3.1.id1">Department of Computer Science</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id4.2.id2">University of British Columbia
<br class="ltx_break"/></span>Kelowna, Canada 
<br class="ltx_break"/>fatemeh.fard@ubc.ca
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id5.id1">Developers spend much time finding information that is relevant to their questions. Stack Overflow has been the leading resource, and with the advent of Large Language Models (LLMs), generative models such as ChatGPT are used frequently. However, there is a catch in using each one separately. Searching for answers is time-consuming and tedious, as shown by the many tools developed by researchers to address this issue. On the other, using LLMs is not reliable, as they might produce irrelevant or unreliable answers (i.e., hallucination). In this work, we present StackRAG, a retrieval-augmented Multiagent generation tool based on LLMs that combines the two worlds: aggregating the knowledge from SO to enhance the reliability of the generated answers. Initial evaluations show that the generated answers are correct, accurate, relevant, and useful. A description video can be found <a class="ltx_ref ltx_href" href="https://bit.ly/3xxmWAm" title="">here</a><span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Please note that as the tool requires API keys, we are not able to set up a live demo of StackRAG.</span></span></span>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Multiagent tool, RAG-based tool, LLM, Stack Overflow

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In the recent year of technological innovations, the groundbreaking improvements in Natural Language Processing, epitomized by Large Language Models (LLM) such as GPT and Llama have revolutionized the way people interact with technology. Developers increasingly rely on such tools to address various challenges faced during the software development process, including but not limited to code generation and bug detection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13840v1#bib.bib1" title="">1</a>]</cite>.
However, such models are limited by their static training data. As a result, they cannot keep up with the recent innovations and, thus, cannot provide up-to-date responses to the latest challenges <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13840v1#bib.bib2" title="">2</a>]</cite>. This is specifically more challenging for the field of software engineering, which has many changes as new APIs, libraries, or frameworks are introduced.
Moreover, the possibility of hallucinations poses a significant problem of their reliability in the software development process <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13840v1#bib.bib3" title="">3</a>]</cite>, where bugs can result in fatal consequences <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13840v1#bib.bib4" title="">4</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">On the other hand, using software engineering techniques is recommended to develop reliable LLM-based models/tools <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13840v1#bib.bib3" title="">3</a>]</cite>.
Thus, even though the emergence of LLMs has ultimately improved the software development process, the usage of widely known platforms such as Stack Overflow (SO) is still prevalent and indispensable. SO provides a medium for developers to access solutions to challenges faced by other developers, engage in meaningful discussions, and facilitate the exchange of expertise between developers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13840v1#bib.bib5" title="">5</a>]</cite>. By utilizing the collective knowledge available in SO, developers can stay up-to-date with the challenges that the latest advancements pose, relying on the expertise and knowledge of other developers.
However, searching for related posts to explore solutions is tedious and time-consuming <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13840v1#bib.bib6" title="">6</a>]</cite> and only relying on LLMs is not reliable <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13840v1#bib.bib3" title="">3</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Thus, in this paper, we propose a Retrieval Augmented Generation (RAG)-based Multiagent LLM tool called <span class="ltx_text ltx_font_bold" id="S1.p3.1.1">StackRAG</span>.
StackRAG utilizes the public knowledge of the developers’ community from SO and combines it with the linguistic abilities of GPT to provide a tool that answers developers’ queries reliablity and with up to date information; addressing the current challenges of LLM usage.
Our agent-LLM tool aims to provide developers with more grounded and accurate answers, which will result in increased efficiency of the software development process.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Though agent-based models have been developed for various other tasks (see Section <a class="ltx_ref" href="https://arxiv.org/html/2406.13840v1#S2" title="II Related Works ‣ StackRAG Agent: Improving Developer Answers with Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">II</span></a>), there is no study or tool that is similar to our work in the context of providing accurate and useful knowledge from SO, with LLM-based models.
Our initial evaluations show that compared to the base LLM, GPT 4, StackRAG provides more correct, accurate, relevant, and useful responses.
We open-source the tool<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/DavidAbrahamyan/StackRAG" title="">https://github.com/DavidAbrahamyan/StackRAG</a></span></span></span>.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related Works</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Mining Stack Overflow has a long history in software engineering research. SO has been studied for the information highlights where the authors developed a recommender system for content highlighting with formatting styles <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13840v1#bib.bib7" title="">7</a>]</cite>, API recommendation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13840v1#bib.bib8" title="">8</a>]</cite>, representation learning for various tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13840v1#bib.bib6" title="">6</a>]</cite>, tag recommendation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13840v1#bib.bib9" title="">9</a>]</cite>, recommending code snippets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13840v1#bib.bib10" title="">10</a>]</cite>, and finding question relatedness <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13840v1#bib.bib11" title="">11</a>]</cite>.
Our work aims to find related SO posts for a given query. In this sense, our study is more similar to the ‘relatedness’ studies.
However, StackRAG differs from all the relatedness and recommendation systems that are developed for SO in several aspects.
We use the Multiagent LLM-based paradigm, which makes the user’s process from searching to response generation seamless.
Our approach combines the knowledge source from Stack Overflow with the generation capability of language models. This innovative direction of incorporating agents in software engineering is a significant step forward in the field.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">The other related category of studies is the one that uses multiagent or agent-based paradigms. Codeagnet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13840v1#bib.bib12" title="">12</a>]</cite>, RepoAgent <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13840v1#bib.bib13" title="">13</a>]</cite>, and RepairAgent <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13840v1#bib.bib14" title="">14</a>]</cite> are examples of such research in the software engineering domain.
PaperQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13840v1#bib.bib15" title="">15</a>]</cite> is a similar work, a RAG-based agent capable of answering questions related to scientific papers.
These studies differ from our work not only in the application area and context used but also in the processes used to extract relevant posts.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="273" id="S2.F1.g1" src="extracted/5679485/Figures/Agent-Architecture.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Architecture Diagram of StackRAG and LLM-based agent designed to foster the software development process.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Architecture</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">StackRAG is a Multiagent LLM-based tool developed using the LangChain Agent framework <span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://js.langchain.com/v0.1/docs/modules/agents/</span></span></span>. StackRAG is designed to help developers find solutions to the problems they face by utilizing the capabilities of GPT and the knowledge available on Stack Overflow.
The overall architecture of StackRAG is provided in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.13840v1#S2.F1" title="Figure 1 ‣ II Related Works ‣ StackRAG Agent: Improving Developer Answers with Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">1</span></a>.
StackRAG is equipped with four components, each specialized in providing a specific type of functionality.
The components are the Keyword Extractor, Search and Storage component, Evidence Gatherer, and Answer Generator.
Except for the Search and Storage component, the other ones include agents handling their objectives and other processing.
A user can ask a question <math alttext="Q" class="ltx_Math" display="inline" id="S3.p1.1.m1.1"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S3.p1.1.m1.1d">italic_Q</annotation></semantics></math> from StackRAG and the central orchestrator agent is responsible for deciding which component to use.
StackRAG’s evidence-gathering process is comprehensive and meticulous. It uses keywords extracted from the question to locate relevant question-answer pairs from Stack Overflow. After a series of filtering and processing steps, the most pertinent question-answer pairs are gathered as evidence. If the collected evidence is deemed sufficient, the agent proceeds to answer the question. If not, the process restarts until the necessary amount of evidence is gathered, ensuring a thorough and accurate response.
GPT is the base language model used as an agent in all components.
We explain the details of each component below.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.p2.1.1">Keyword Extractor</span>.
This component is responsible to extract keywords based on user’s query. The extracted keywords identify the main concepts of the query and will be used in later stages to find related questions and answers from Stack Overflow.
The query is processed initially by question_complexity_checker agent to assess if it should be broken down into sub-questions.
The reason is that if the query is long or complicated, we might miss some important concepts if we extract keywords directly.
Therefore, we assess the query first by asking the agent, and based on the boolean value of its complexity evaluator, the output is a list of sub-questions or is sent to the keyword_extractor agent.
In the case of sub-questions, they are passed into keyword_extractor asynchronously to speed up the process.
The question_complexity_checker and keyword_extractor agents are using the following prompts to accomplish their tasks.
The list of keywords <math alttext="K={k_{1},k_{2},...,k_{n}}" class="ltx_Math" display="inline" id="S3.p2.1.m1.4"><semantics id="S3.p2.1.m1.4a"><mrow id="S3.p2.1.m1.4.4" xref="S3.p2.1.m1.4.4.cmml"><mi id="S3.p2.1.m1.4.4.5" xref="S3.p2.1.m1.4.4.5.cmml">K</mi><mo id="S3.p2.1.m1.4.4.4" xref="S3.p2.1.m1.4.4.4.cmml">=</mo><mrow id="S3.p2.1.m1.4.4.3.3" xref="S3.p2.1.m1.4.4.3.4.cmml"><msub id="S3.p2.1.m1.2.2.1.1.1" xref="S3.p2.1.m1.2.2.1.1.1.cmml"><mi id="S3.p2.1.m1.2.2.1.1.1.2" xref="S3.p2.1.m1.2.2.1.1.1.2.cmml">k</mi><mn id="S3.p2.1.m1.2.2.1.1.1.3" xref="S3.p2.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.p2.1.m1.4.4.3.3.4" xref="S3.p2.1.m1.4.4.3.4.cmml">,</mo><msub id="S3.p2.1.m1.3.3.2.2.2" xref="S3.p2.1.m1.3.3.2.2.2.cmml"><mi id="S3.p2.1.m1.3.3.2.2.2.2" xref="S3.p2.1.m1.3.3.2.2.2.2.cmml">k</mi><mn id="S3.p2.1.m1.3.3.2.2.2.3" xref="S3.p2.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.p2.1.m1.4.4.3.3.5" xref="S3.p2.1.m1.4.4.3.4.cmml">,</mo><mi id="S3.p2.1.m1.1.1" mathvariant="normal" xref="S3.p2.1.m1.1.1.cmml">…</mi><mo id="S3.p2.1.m1.4.4.3.3.6" xref="S3.p2.1.m1.4.4.3.4.cmml">,</mo><msub id="S3.p2.1.m1.4.4.3.3.3" xref="S3.p2.1.m1.4.4.3.3.3.cmml"><mi id="S3.p2.1.m1.4.4.3.3.3.2" xref="S3.p2.1.m1.4.4.3.3.3.2.cmml">k</mi><mi id="S3.p2.1.m1.4.4.3.3.3.3" xref="S3.p2.1.m1.4.4.3.3.3.3.cmml">n</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.4b"><apply id="S3.p2.1.m1.4.4.cmml" xref="S3.p2.1.m1.4.4"><eq id="S3.p2.1.m1.4.4.4.cmml" xref="S3.p2.1.m1.4.4.4"></eq><ci id="S3.p2.1.m1.4.4.5.cmml" xref="S3.p2.1.m1.4.4.5">𝐾</ci><list id="S3.p2.1.m1.4.4.3.4.cmml" xref="S3.p2.1.m1.4.4.3.3"><apply id="S3.p2.1.m1.2.2.1.1.1.cmml" xref="S3.p2.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.p2.1.m1.2.2.1.1.1.1.cmml" xref="S3.p2.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.p2.1.m1.2.2.1.1.1.2.cmml" xref="S3.p2.1.m1.2.2.1.1.1.2">𝑘</ci><cn id="S3.p2.1.m1.2.2.1.1.1.3.cmml" type="integer" xref="S3.p2.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S3.p2.1.m1.3.3.2.2.2.cmml" xref="S3.p2.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.p2.1.m1.3.3.2.2.2.1.cmml" xref="S3.p2.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S3.p2.1.m1.3.3.2.2.2.2.cmml" xref="S3.p2.1.m1.3.3.2.2.2.2">𝑘</ci><cn id="S3.p2.1.m1.3.3.2.2.2.3.cmml" type="integer" xref="S3.p2.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">…</ci><apply id="S3.p2.1.m1.4.4.3.3.3.cmml" xref="S3.p2.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.p2.1.m1.4.4.3.3.3.1.cmml" xref="S3.p2.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S3.p2.1.m1.4.4.3.3.3.2.cmml" xref="S3.p2.1.m1.4.4.3.3.3.2">𝑘</ci><ci id="S3.p2.1.m1.4.4.3.3.3.3.cmml" xref="S3.p2.1.m1.4.4.3.3.3.3">𝑛</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.4c">K={k_{1},k_{2},...,k_{n}}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.1.m1.4d">italic_K = italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_k start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> from this component is sent to the central agent for further processing.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p3">
<p class="ltx_p" id="S3.p3.1"><span class="ltx_text ltx_font_typewriter" id="S3.p3.1.1" style="background-color:#F2F2F2;">PROMPT (question_complexity_checker):</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="S3.p3.1.2">You are a part of RAG architecture that specializes in generating answers to user’s given query using Stack Overflow.
You are going to be provided the user question. Your task is to determine whether the question is complex enough to be divided into sub-questions.
If, in order to answer the question, different topics have to be covered, return TRUE, all in capital letters. If there are multiple simple questions, in the given question, again, return TRUE. Otherwise, if you think that the question is not complex and there is no need to divide it into sub-questions, return FALSE.
Do not provide explanations for your choice, t=output a single word, either TRUE or FALSE.
Question: question</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p4">
<p class="ltx_p" id="S3.p4.1"><span class="ltx_text ltx_font_typewriter" id="S3.p4.1.1" style="background-color:#F2F2F2;">PROMPT (keyword_extractor): </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="S3.p4.1.2">You are a question-to-query parser. You are given a technical question. You have to use the question to create a Python list of search queries that will be useful in conducting a search in Stack Overflow. Make every query in the list as short as possible. Having less words will produce better results. But make sure you do not omit important search terms and make the search query too general. It does not have to be a complete sentence. Every single query in the list MUST be less than 4 words. Output MUST be a Python list with every element enclosed with double quotes. Question: question</span></p>
</div>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.1"><span class="ltx_text ltx_font_bold" id="S3.p5.1.1">Search and Storage</span>.
This component is responsible for searching and retrieving questions and answers from Stack Overflow related to the user’s query.
We search for relevant questions and their answers separately, as this approach could diversify the related SO posts, and more relevant information could be extracted.
For this purpose, the list of extracted keywords <math alttext="K" class="ltx_Math" display="inline" id="S3.p5.1.m1.1"><semantics id="S3.p5.1.m1.1a"><mi id="S3.p5.1.m1.1.1" xref="S3.p5.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.p5.1.m1.1b"><ci id="S3.p5.1.m1.1.1.cmml" xref="S3.p5.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S3.p5.1.m1.1d">italic_K</annotation></semantics></math> are provided and a search is conducted in Stack Overflow, using the StackExchange API.
Initially, we designed our system to search for questions and corresponding answers asynchronously, which would have reduced the time spent gathering the answers. However, because StackExchange API heavily penalizes users who make more than 30 API calls per second, we decided to retrieve the results sequentially.</p>
</div>
<div class="ltx_para" id="S3.p6">
<p class="ltx_p" id="S3.p6.1">Given a list of keywords <math alttext="K" class="ltx_Math" display="inline" id="S3.p6.1.m1.1"><semantics id="S3.p6.1.m1.1a"><mi id="S3.p6.1.m1.1.1" xref="S3.p6.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.p6.1.m1.1b"><ci id="S3.p6.1.m1.1.1.cmml" xref="S3.p6.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S3.p6.1.m1.1d">italic_K</annotation></semantics></math>, we store the information of the retrieved questions, including the question’s <span class="ltx_text ltx_font_typewriter" id="S3.p6.1.1">ID</span>, <span class="ltx_text ltx_font_typewriter" id="S3.p6.1.2">LINK</span>, <span class="ltx_text ltx_font_typewriter" id="S3.p6.1.3">TITLE</span>, <span class="ltx_text ltx_font_typewriter" id="S3.p6.1.4">BODY</span>, <span class="ltx_text ltx_font_typewriter" id="S3.p6.1.5">CREATION DATE</span>, and <span class="ltx_text ltx_font_typewriter" id="S3.p6.1.6">ACCEPTED-ANSWER-ID</span> or <span class="ltx_text ltx_font_typewriter" id="S3.p6.1.7">NONE</span> if there is no accepted answer.
In order to find the relevant questions to the user’s query and eliminate irrelevant questions, we use <span class="ltx_text ltx_font_italic" id="S3.p6.1.8">BM-25</span> re-ranking algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13840v1#bib.bib16" title="">16</a>]</cite>. The Stack Overflow questions’ <span class="ltx_text ltx_font_typewriter" id="S3.p6.1.9">TITLE</span> and <span class="ltx_text ltx_font_typewriter" id="S3.p6.1.10">BODY</span> are fed to BM-25 as a single string, and the user’s question is given as the query. We use the top 50 questions from the output of BM-25. The top 50 are chosen based on our empirical experiments.
This step leaves us with the top 50 most relevant questions, which are divided into two lists: the ones that contain an accepted answer and the ones that do not.
The questions with accepted answers are used in the next other components to provide evidence for answer generation.
The questions with no accepted answer are stored for the final answer generation, which are provided as links in the generated answer. The rationale is that these questions or some of their replies could potentially be useful for users, or they might be interested in investigating those SO posts.</p>
</div>
<div class="ltx_para" id="S3.p7">
<p class="ltx_p" id="S3.p7.3">The question <span class="ltx_text ltx_font_typewriter" id="S3.p7.3.1">ID</span>’s of the ones having an accepted answer are then used to search for their corresponding answers on SO.
A question can have an accepted answer as well as many unaccepted answers, which are still relevant to the question and could provide some insight or useful information for the user’s query. Therefore, we collect the accepted answers and the top 2 unaccepted answers to store in our vector database.
For collecting the unaccepted answers, we use two criteria: the <math alttext="score" class="ltx_Math" display="inline" id="S3.p7.1.m1.1"><semantics id="S3.p7.1.m1.1a"><mrow id="S3.p7.1.m1.1.1" xref="S3.p7.1.m1.1.1.cmml"><mi id="S3.p7.1.m1.1.1.2" xref="S3.p7.1.m1.1.1.2.cmml">s</mi><mo id="S3.p7.1.m1.1.1.1" xref="S3.p7.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.p7.1.m1.1.1.3" xref="S3.p7.1.m1.1.1.3.cmml">c</mi><mo id="S3.p7.1.m1.1.1.1a" xref="S3.p7.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.p7.1.m1.1.1.4" xref="S3.p7.1.m1.1.1.4.cmml">o</mi><mo id="S3.p7.1.m1.1.1.1b" xref="S3.p7.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.p7.1.m1.1.1.5" xref="S3.p7.1.m1.1.1.5.cmml">r</mi><mo id="S3.p7.1.m1.1.1.1c" xref="S3.p7.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.p7.1.m1.1.1.6" xref="S3.p7.1.m1.1.1.6.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p7.1.m1.1b"><apply id="S3.p7.1.m1.1.1.cmml" xref="S3.p7.1.m1.1.1"><times id="S3.p7.1.m1.1.1.1.cmml" xref="S3.p7.1.m1.1.1.1"></times><ci id="S3.p7.1.m1.1.1.2.cmml" xref="S3.p7.1.m1.1.1.2">𝑠</ci><ci id="S3.p7.1.m1.1.1.3.cmml" xref="S3.p7.1.m1.1.1.3">𝑐</ci><ci id="S3.p7.1.m1.1.1.4.cmml" xref="S3.p7.1.m1.1.1.4">𝑜</ci><ci id="S3.p7.1.m1.1.1.5.cmml" xref="S3.p7.1.m1.1.1.5">𝑟</ci><ci id="S3.p7.1.m1.1.1.6.cmml" xref="S3.p7.1.m1.1.1.6">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.1.m1.1c">score</annotation><annotation encoding="application/x-llamapun" id="S3.p7.1.m1.1d">italic_s italic_c italic_o italic_r italic_e</annotation></semantics></math> that each answer has in SO, and the <math alttext="creationdate" class="ltx_Math" display="inline" id="S3.p7.2.m2.1"><semantics id="S3.p7.2.m2.1a"><mrow id="S3.p7.2.m2.1.1" xref="S3.p7.2.m2.1.1.cmml"><mi id="S3.p7.2.m2.1.1.2" xref="S3.p7.2.m2.1.1.2.cmml">c</mi><mo id="S3.p7.2.m2.1.1.1" xref="S3.p7.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.p7.2.m2.1.1.3" xref="S3.p7.2.m2.1.1.3.cmml">r</mi><mo id="S3.p7.2.m2.1.1.1a" xref="S3.p7.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.p7.2.m2.1.1.4" xref="S3.p7.2.m2.1.1.4.cmml">e</mi><mo id="S3.p7.2.m2.1.1.1b" xref="S3.p7.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.p7.2.m2.1.1.5" xref="S3.p7.2.m2.1.1.5.cmml">a</mi><mo id="S3.p7.2.m2.1.1.1c" xref="S3.p7.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.p7.2.m2.1.1.6" xref="S3.p7.2.m2.1.1.6.cmml">t</mi><mo id="S3.p7.2.m2.1.1.1d" xref="S3.p7.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.p7.2.m2.1.1.7" xref="S3.p7.2.m2.1.1.7.cmml">i</mi><mo id="S3.p7.2.m2.1.1.1e" xref="S3.p7.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.p7.2.m2.1.1.8" xref="S3.p7.2.m2.1.1.8.cmml">o</mi><mo id="S3.p7.2.m2.1.1.1f" xref="S3.p7.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.p7.2.m2.1.1.9" xref="S3.p7.2.m2.1.1.9.cmml">n</mi><mo id="S3.p7.2.m2.1.1.1g" xref="S3.p7.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.p7.2.m2.1.1.10" xref="S3.p7.2.m2.1.1.10.cmml">d</mi><mo id="S3.p7.2.m2.1.1.1h" xref="S3.p7.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.p7.2.m2.1.1.11" xref="S3.p7.2.m2.1.1.11.cmml">a</mi><mo id="S3.p7.2.m2.1.1.1i" xref="S3.p7.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.p7.2.m2.1.1.12" xref="S3.p7.2.m2.1.1.12.cmml">t</mi><mo id="S3.p7.2.m2.1.1.1j" xref="S3.p7.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.p7.2.m2.1.1.13" xref="S3.p7.2.m2.1.1.13.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p7.2.m2.1b"><apply id="S3.p7.2.m2.1.1.cmml" xref="S3.p7.2.m2.1.1"><times id="S3.p7.2.m2.1.1.1.cmml" xref="S3.p7.2.m2.1.1.1"></times><ci id="S3.p7.2.m2.1.1.2.cmml" xref="S3.p7.2.m2.1.1.2">𝑐</ci><ci id="S3.p7.2.m2.1.1.3.cmml" xref="S3.p7.2.m2.1.1.3">𝑟</ci><ci id="S3.p7.2.m2.1.1.4.cmml" xref="S3.p7.2.m2.1.1.4">𝑒</ci><ci id="S3.p7.2.m2.1.1.5.cmml" xref="S3.p7.2.m2.1.1.5">𝑎</ci><ci id="S3.p7.2.m2.1.1.6.cmml" xref="S3.p7.2.m2.1.1.6">𝑡</ci><ci id="S3.p7.2.m2.1.1.7.cmml" xref="S3.p7.2.m2.1.1.7">𝑖</ci><ci id="S3.p7.2.m2.1.1.8.cmml" xref="S3.p7.2.m2.1.1.8">𝑜</ci><ci id="S3.p7.2.m2.1.1.9.cmml" xref="S3.p7.2.m2.1.1.9">𝑛</ci><ci id="S3.p7.2.m2.1.1.10.cmml" xref="S3.p7.2.m2.1.1.10">𝑑</ci><ci id="S3.p7.2.m2.1.1.11.cmml" xref="S3.p7.2.m2.1.1.11">𝑎</ci><ci id="S3.p7.2.m2.1.1.12.cmml" xref="S3.p7.2.m2.1.1.12">𝑡</ci><ci id="S3.p7.2.m2.1.1.13.cmml" xref="S3.p7.2.m2.1.1.13">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.2.m2.1c">creationdate</annotation><annotation encoding="application/x-llamapun" id="S3.p7.2.m2.1d">italic_c italic_r italic_e italic_a italic_t italic_i italic_o italic_n italic_d italic_a italic_t italic_e</annotation></semantics></math> of the answer.
We choose answers with higher <math alttext="score" class="ltx_Math" display="inline" id="S3.p7.3.m3.1"><semantics id="S3.p7.3.m3.1a"><mrow id="S3.p7.3.m3.1.1" xref="S3.p7.3.m3.1.1.cmml"><mi id="S3.p7.3.m3.1.1.2" xref="S3.p7.3.m3.1.1.2.cmml">s</mi><mo id="S3.p7.3.m3.1.1.1" xref="S3.p7.3.m3.1.1.1.cmml">⁢</mo><mi id="S3.p7.3.m3.1.1.3" xref="S3.p7.3.m3.1.1.3.cmml">c</mi><mo id="S3.p7.3.m3.1.1.1a" xref="S3.p7.3.m3.1.1.1.cmml">⁢</mo><mi id="S3.p7.3.m3.1.1.4" xref="S3.p7.3.m3.1.1.4.cmml">o</mi><mo id="S3.p7.3.m3.1.1.1b" xref="S3.p7.3.m3.1.1.1.cmml">⁢</mo><mi id="S3.p7.3.m3.1.1.5" xref="S3.p7.3.m3.1.1.5.cmml">r</mi><mo id="S3.p7.3.m3.1.1.1c" xref="S3.p7.3.m3.1.1.1.cmml">⁢</mo><mi id="S3.p7.3.m3.1.1.6" xref="S3.p7.3.m3.1.1.6.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p7.3.m3.1b"><apply id="S3.p7.3.m3.1.1.cmml" xref="S3.p7.3.m3.1.1"><times id="S3.p7.3.m3.1.1.1.cmml" xref="S3.p7.3.m3.1.1.1"></times><ci id="S3.p7.3.m3.1.1.2.cmml" xref="S3.p7.3.m3.1.1.2">𝑠</ci><ci id="S3.p7.3.m3.1.1.3.cmml" xref="S3.p7.3.m3.1.1.3">𝑐</ci><ci id="S3.p7.3.m3.1.1.4.cmml" xref="S3.p7.3.m3.1.1.4">𝑜</ci><ci id="S3.p7.3.m3.1.1.5.cmml" xref="S3.p7.3.m3.1.1.5">𝑟</ci><ci id="S3.p7.3.m3.1.1.6.cmml" xref="S3.p7.3.m3.1.1.6">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.3.m3.1c">score</annotation><annotation encoding="application/x-llamapun" id="S3.p7.3.m3.1d">italic_s italic_c italic_o italic_r italic_e</annotation></semantics></math>s and the ones that are more recent.</p>
</div>
<div class="ltx_para" id="S3.p8">
<p class="ltx_p" id="S3.p8.1">We store the extracted questions and answers as vector embeddings using text-embedding-ada-3-small model developed by OpenAI<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://platform.openai.com/docs/models/embeddings</span></span></span>
The questions’ <span class="ltx_text ltx_font_typewriter" id="S3.p8.1.1">TILE</span> and <span class="ltx_text ltx_font_typewriter" id="S3.p8.1.2">BODY</span> are combined with their retrieved answers (i.e., accepted and top-2 unaccepted answers) to generate its embedding for storage in Pinecone Vector Database.
Vector embeddings are the vector representations of data in n-dimensional space. In order to avoid the necessity of constantly calculating the embedding vectors of the same answers that we retrieve from SO, we are using Pinecone Vector Database<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://www.pinecone.io/</span></span></span>. In vector databases, each piece of data is stored as a vector rather than as is. Such databases often offer searching functionalities to users, such as cosine similarity when searching for input.</p>
</div>
<div class="ltx_para" id="S3.p9">
<p class="ltx_p" id="S3.p9.1">Additionally, we store the <span class="ltx_text ltx_font_typewriter" id="S3.p9.1.1">ID</span>’s of the questions in Pinecone in a JSON file locally. This local JSON file is intended to eliminate potential redundant calls to StackExchange API if a similar question is asked by the user.
Therefore, with each new query, before searching in SO, we check in the JSON file to see if we have already retrieved the answer previously.
This process reduces the number of calls and, therefore, the tool’s overall query-response time.
After all, a message is returned to our agent indicating if the data has been successfully stored in the database. If successfully stored, the agent proceeds to the next step. Otherwise, it repeats the searching and storing process. If searching fails after multiple trials, the tool generates an output noting that no results are found.</p>
</div>
<div class="ltx_para" id="S3.p10">
<p class="ltx_p" id="S3.p10.2"><span class="ltx_text ltx_font_bold" id="S3.p10.2.1">Evidence Gatherer</span>.
The Evidence Gatherer component uses the data stored in the Pinecone database to gather the most relevant pieces of information, which is used to answer the user’s query.
For this purpose, first we compute the similar question-answer pairs that also include diverse set of information. Then, we calculate their relevancy to the user’s query.
The results of these two steps gives us <math alttext="n" class="ltx_Math" display="inline" id="S3.p10.1.m1.1"><semantics id="S3.p10.1.m1.1a"><mi id="S3.p10.1.m1.1.1" xref="S3.p10.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.p10.1.m1.1b"><ci id="S3.p10.1.m1.1.1.cmml" xref="S3.p10.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p10.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.p10.1.m1.1d">italic_n</annotation></semantics></math> question-answer pairs that is given to the agent as evidence.
The agent should identify whether such evidence is sufficient to answer the query or the whole process (including keyword extraction) should be repeated.
We use <math alttext="n=3" class="ltx_Math" display="inline" id="S3.p10.2.m2.1"><semantics id="S3.p10.2.m2.1a"><mrow id="S3.p10.2.m2.1.1" xref="S3.p10.2.m2.1.1.cmml"><mi id="S3.p10.2.m2.1.1.2" xref="S3.p10.2.m2.1.1.2.cmml">n</mi><mo id="S3.p10.2.m2.1.1.1" xref="S3.p10.2.m2.1.1.1.cmml">=</mo><mn id="S3.p10.2.m2.1.1.3" xref="S3.p10.2.m2.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p10.2.m2.1b"><apply id="S3.p10.2.m2.1.1.cmml" xref="S3.p10.2.m2.1.1"><eq id="S3.p10.2.m2.1.1.1.cmml" xref="S3.p10.2.m2.1.1.1"></eq><ci id="S3.p10.2.m2.1.1.2.cmml" xref="S3.p10.2.m2.1.1.2">𝑛</ci><cn id="S3.p10.2.m2.1.1.3.cmml" type="integer" xref="S3.p10.2.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p10.2.m2.1c">n=3</annotation><annotation encoding="application/x-llamapun" id="S3.p10.2.m2.1d">italic_n = 3</annotation></semantics></math> in our work, which is set empirically.
The details of these steps are explained below.</p>
</div>
<div class="ltx_para" id="S3.p11">
<p class="ltx_p" id="S3.p11.1">To find similar question-answer pairs, first, the embedding of the user’s query is generated using the text-embedding-ada-002 model.
This embedding is compared with the embedding of question-answer pairs stored in the Pinecone database using the cosine similarity metric.
We do not use a threshold for the similarity score as we are interested to find the top results not necessarily the most similar one above a specific percentage.
Based on the cosine similarity score, we save the top 30 results and re-rank them using Maximum Marginal Relevance (MMR) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.13840v1#bib.bib17" title="">17</a>]</cite>.
As there can be multiple questions on SO containing similar questions and answers, we use MMR to select SO posts that are most similar and optimize for diversity in the retrieved answers. This diversity in the extracted question-answer pairs will help the model generate more well-grounded answers.
After applying MMR, we keep the top 15 results that will be used as evidence.</p>
</div>
<div class="ltx_para" id="S3.p12">
<p class="ltx_p" id="S3.p12.8">After gathering the evidence, we use evidence_scorer agent to score each piece of evidence (i.e., each question-answer pair) based on their relevance to the user’s query. The evidence_scorer agent provides a score ranging from <math alttext="1" class="ltx_Math" display="inline" id="S3.p12.1.m1.1"><semantics id="S3.p12.1.m1.1a"><mn id="S3.p12.1.m1.1.1" xref="S3.p12.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.p12.1.m1.1b"><cn id="S3.p12.1.m1.1.1.cmml" type="integer" xref="S3.p12.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p12.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="S3.p12.1.m1.1d">1</annotation></semantics></math> to <math alttext="5" class="ltx_Math" display="inline" id="S3.p12.2.m2.1"><semantics id="S3.p12.2.m2.1a"><mn id="S3.p12.2.m2.1.1" xref="S3.p12.2.m2.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S3.p12.2.m2.1b"><cn id="S3.p12.2.m2.1.1.cmml" type="integer" xref="S3.p12.2.m2.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p12.2.m2.1c">5</annotation><annotation encoding="application/x-llamapun" id="S3.p12.2.m2.1d">5</annotation></semantics></math>, assessing how relevant the evidence is in order to answer the question. A score of <math alttext="5" class="ltx_Math" display="inline" id="S3.p12.3.m3.1"><semantics id="S3.p12.3.m3.1a"><mn id="S3.p12.3.m3.1.1" xref="S3.p12.3.m3.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S3.p12.3.m3.1b"><cn id="S3.p12.3.m3.1.1.cmml" type="integer" xref="S3.p12.3.m3.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p12.3.m3.1c">5</annotation><annotation encoding="application/x-llamapun" id="S3.p12.3.m3.1d">5</annotation></semantics></math> indicates high relevance, and <math alttext="1" class="ltx_Math" display="inline" id="S3.p12.4.m4.1"><semantics id="S3.p12.4.m4.1a"><mn id="S3.p12.4.m4.1.1" xref="S3.p12.4.m4.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.p12.4.m4.1b"><cn id="S3.p12.4.m4.1.1.cmml" type="integer" xref="S3.p12.4.m4.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p12.4.m4.1c">1</annotation><annotation encoding="application/x-llamapun" id="S3.p12.4.m4.1d">1</annotation></semantics></math> indicates low relevance. If the retrieved evidence is not relevant at all, the agent returns “not useful” for that piece of evidence. To speed the process, the scoring of each piece of evidence is done in parallel, in asynchronously with other tasks.
Finally, we choose the top 3 pieces of evidence and we provide it as a single full evidence, <math alttext="E" class="ltx_Math" display="inline" id="S3.p12.5.m5.1"><semantics id="S3.p12.5.m5.1a"><mi id="S3.p12.5.m5.1.1" xref="S3.p12.5.m5.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S3.p12.5.m5.1b"><ci id="S3.p12.5.m5.1.1.cmml" xref="S3.p12.5.m5.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p12.5.m5.1c">E</annotation><annotation encoding="application/x-llamapun" id="S3.p12.5.m5.1d">italic_E</annotation></semantics></math>, which is provided to the evidence_checker agent.
The evidence_checker agent returns a boolean value indicating if the gathered evidence <math alttext="E" class="ltx_Math" display="inline" id="S3.p12.6.m6.1"><semantics id="S3.p12.6.m6.1a"><mi id="S3.p12.6.m6.1.1" xref="S3.p12.6.m6.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S3.p12.6.m6.1b"><ci id="S3.p12.6.m6.1.1.cmml" xref="S3.p12.6.m6.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p12.6.m6.1c">E</annotation><annotation encoding="application/x-llamapun" id="S3.p12.6.m6.1d">italic_E</annotation></semantics></math> is sufficient in order to answer query <math alttext="Q" class="ltx_Math" display="inline" id="S3.p12.7.m7.1"><semantics id="S3.p12.7.m7.1a"><mi id="S3.p12.7.m7.1.1" xref="S3.p12.7.m7.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.p12.7.m7.1b"><ci id="S3.p12.7.m7.1.1.cmml" xref="S3.p12.7.m7.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p12.7.m7.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S3.p12.7.m7.1d">italic_Q</annotation></semantics></math>.
If the gathered Evidence <math alttext="E" class="ltx_Math" display="inline" id="S3.p12.8.m8.1"><semantics id="S3.p12.8.m8.1a"><mi id="S3.p12.8.m8.1.1" xref="S3.p12.8.m8.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S3.p12.8.m8.1b"><ci id="S3.p12.8.m8.1.1.cmml" xref="S3.p12.8.m8.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p12.8.m8.1c">E</annotation><annotation encoding="application/x-llamapun" id="S3.p12.8.m8.1d">italic_E</annotation></semantics></math> is enough, we proceed to generating the answer using Answer Generator agent. Otherwise, the tool should repeat the process from the beginning and proceeds to generate a new set of keywords.
The prompts used for evidence_scorer and evidence_checker agents are as follows.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p13">
<p class="ltx_p" id="S3.p13.1"><span class="ltx_text ltx_font_typewriter" id="S3.p13.1.1" style="background-color:#F2F2F2;">PROMPT (evidence_scorer): </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="S3.p13.1.2">
You are a part of RAG architecture that specializes in generating answers to user’s given query using Stack Overflow.
Provided the gathered evidence from Stack Overflow as well as the user’s given question, your task is to determine how useful the evidence is in order to answer the user question. The evidence includes a question and its corresponding answer from Stack Overflow. Rate the given evidence on the scale from 1 to 5, with 1 indicating not useful and 5 indicating really useful. If the evidence is not useful at all, return “not useful” all in lowercase. Only output either a number from 1-5 or “not useful” with no explanation.
Gathered Evidence:
</span>{<span class="ltx_text ltx_font_italic" id="S3.p13.1.3">evidence</span>}<span class="ltx_text ltx_font_italic" id="S3.p13.1.4">
User Question:
</span>{<span class="ltx_text ltx_font_italic" id="S3.p13.1.5">question</span>}<span class="ltx_text ltx_font_italic" id="S3.p13.1.6">
</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p14">
<p class="ltx_p" id="S3.p14.1"><span class="ltx_text ltx_font_typewriter" id="S3.p14.1.1" style="background-color:#F2F2F2;">PROMPT (evidence_checker): </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="S3.p14.1.2">You are a part of RAG architecture that specializes in generating answers to user’s given query using Stack Overflow.
Provided the gathered evidence from Stack Overflow as well as the user’s given question, your task is to determine whether you have enough evidence to answer the question or not.
Do not generate answer even if you have enough evidence. The evidence does not have to directly answer the question, but it has to provide the basis upon which you can form the answer. If no such evidence is provided, return “FALSE”, do not use your own knowledge to answer the question.
Your output must be a single word, either “TRUE” or “FALSE”. All letters must be capital, do not explain why you chose a specific answer, only output either “TRUE” or “FALSE”
Gathered Evidence: </span>{<span class="ltx_text ltx_font_italic" id="S3.p14.1.3">evidence</span>}<span class="ltx_text ltx_font_italic" id="S3.p14.1.4">
User Question: </span>{<span class="ltx_text ltx_font_italic" id="S3.p14.1.5">question</span>}</p>
</div>
<div class="ltx_para" id="S3.p15">
<p class="ltx_p" id="S3.p15.2"><span class="ltx_text ltx_font_bold" id="S3.p15.2.1">Answer Generator</span>.
The main responsibility of this component, which is composed of a single agent, is to generate an answer given the evidence <math alttext="E" class="ltx_Math" display="inline" id="S3.p15.1.m1.1"><semantics id="S3.p15.1.m1.1a"><mi id="S3.p15.1.m1.1.1" xref="S3.p15.1.m1.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S3.p15.1.m1.1b"><ci id="S3.p15.1.m1.1.1.cmml" xref="S3.p15.1.m1.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p15.1.m1.1c">E</annotation><annotation encoding="application/x-llamapun" id="S3.p15.1.m1.1d">italic_E</annotation></semantics></math> and user’s query <math alttext="Q" class="ltx_Math" display="inline" id="S3.p15.2.m2.1"><semantics id="S3.p15.2.m2.1a"><mi id="S3.p15.2.m2.1.1" xref="S3.p15.2.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.p15.2.m2.1b"><ci id="S3.p15.2.m2.1.1.cmml" xref="S3.p15.2.m2.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p15.2.m2.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S3.p15.2.m2.1d">italic_Q</annotation></semantics></math>. The agent is instructed to always provide the links to the questions that were used in the response, so that the user can access and use all the responses available on SO.
Additionally, we provided it with the links to the questions that did not have an accepted answer but were relevant to the question (Stored in the Search and Storage Tool after applying BM-25).
Empirically, we found that providing links to such questions is a helpful feature for the user, as they can track those links and find useful answers in the future.
The prompt for the answer_generator agent is:</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p16">
<p class="ltx_p" id="S3.p16.1"><span class="ltx_text ltx_font_typewriter" id="S3.p16.1.1" style="background-color:#F2F2F2;">PROMPT (answer_generator): </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="S3.p16.1.2">
You are a part of RAG architecture that specializes in generating answers to user’s given query using Stack Overflow.
You are the final piece of this architecture, your task is to construct the final answer based on the given question and the provided evidence.
Be as thorough as possible, if you write code, do not omit anything, write every single detail.
Indicate whether the answer that you used in generating the response was an accepted answer in Stack Overflow or not.
At the end of your answer, mention all the links of the answers that you used in the following format:
<br class="ltx_break"/>Links used:
<br class="ltx_break"/>- [Question Title] Link1
<br class="ltx_break"/>- [Question Title] Link2
<br class="ltx_break"/>- [Question Title] Link3
<br class="ltx_break"/>…
<br class="ltx_break"/>You will also be provided a list of questions which are unanswered but are relevant to the user query, include their links at the end in the following format:
<br class="ltx_break"/>Unanswered questions that you may find useful in the future:
<br class="ltx_break"/>- [Question Title] Link1
<br class="ltx_break"/>- [Question Title] Link2
<br class="ltx_break"/>- [Question Title] Link3
<br class="ltx_break"/>…
<br class="ltx_break"/>User Question: </span>{<span class="ltx_text ltx_font_italic" id="S3.p16.1.3">question</span>}<span class="ltx_text ltx_font_italic" id="S3.p16.1.4">
Gathered Evidence: </span>{<span class="ltx_text ltx_font_italic" id="S3.p16.1.5">evidence</span>}<span class="ltx_text ltx_font_italic" id="S3.p16.1.6">
Unanswered Question List: </span>{<span class="ltx_text ltx_font_italic" id="S3.p16.1.7">unanswered_question_list</span>}<span class="ltx_text ltx_font_italic" id="S3.p16.1.8">
</span></p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Evaluation</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">To evaluate StackRAG’s performance, we conducted a manual evaluation by three software developers with different titles: Machine Learning Engineers, NLP engineers, and full-stack developers.
These developers have more than five years of experience and often use ChatGPT to find answers to their development-related questions.
We selected three questions that developers have struggled with in their careers and generated answers using GPT-3.5, GPT-4, and StackRAG.
We asked developers to evaluate the generated answers based on the metrics discussed below.
The three challenged questions are
<span class="ltx_text ltx_font_italic" id="S4.p1.1.1">‘How to do horizontal scaling of web sockets?’</span>,
<span class="ltx_text ltx_font_italic" id="S4.p1.1.2">‘How to import from a parent directory in Python?’</span>, and
<span class="ltx_text ltx_font_italic" id="S4.p1.1.3">‘How to use function calling in openAI API?’</span>.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.2"><span class="ltx_text ltx_font_bold" id="S4.p2.2.1">Evaluation Metrics.</span>
We used four different evaluation metrics to assess the performance of our model. Each metric has a score between <math alttext="1" class="ltx_Math" display="inline" id="S4.p2.1.m1.1"><semantics id="S4.p2.1.m1.1a"><mn id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><cn id="S4.p2.1.m1.1.1.cmml" type="integer" xref="S4.p2.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="S4.p2.1.m1.1d">1</annotation></semantics></math> to <math alttext="5" class="ltx_Math" display="inline" id="S4.p2.2.m2.1"><semantics id="S4.p2.2.m2.1a"><mn id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><cn id="S4.p2.2.m2.1.1.cmml" type="integer" xref="S4.p2.2.m2.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">5</annotation><annotation encoding="application/x-llamapun" id="S4.p2.2.m2.1d">5</annotation></semantics></math>, one being the worst and 5 indicating the best.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1"><span class="ltx_text ltx_font_italic" id="S4.p3.1.1">Correctness (C)</span>: Is the response provided to the question correct?</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1"><span class="ltx_text ltx_font_italic" id="S4.p4.1.1">Accuracy (A)</span>: Does the response accurately explain the question’s solution?</p>
</div>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p" id="S4.p5.1"><span class="ltx_text ltx_font_italic" id="S4.p5.1.1">Relevance (R)</span>: Is the generated text related to the asked question in terms of the topic?</p>
</div>
<div class="ltx_para" id="S4.p6">
<p class="ltx_p" id="S4.p6.1"><span class="ltx_text ltx_font_italic" id="S4.p6.1.1">Usefulness (U)</span>: Does the response provide useful information to solve the question?</p>
</div>
<div class="ltx_para" id="S4.p7">
<p class="ltx_p" id="S4.p7.1"><span class="ltx_text ltx_font_bold" id="S4.p7.1.1">Results.</span>
Table <a class="ltx_ref" href="https://arxiv.org/html/2406.13840v1#S4.T1" title="TABLE I ‣ IV Evaluation ‣ StackRAG Agent: Improving Developer Answers with Retrieval-Augmented Generation"><span class="ltx_text ltx_ref_tag">I</span></a> shows the evaluation of StackRAG, GPT-3.5, GPT-4 models, reporting the average scores the developers gave to the generated answers of the models.
Even though StackRAG uses GPT-4 as the base model, it performs better than GPT-4 in all four metrics.
This indicates that the developers find the answers generated by StackRAG more relevant to the topic. In addition to correct and accurate answers, they found more useful information in the StackRAG’s responses.
When we discussed this with the developers, they valued the links in the generated response.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.1">Metric/Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.2.1">C</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.3.1">A</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.4.1">R</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.5.1">U</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.2.2.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.2.2.1.1">StackRAG</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.2.2.2">4.66</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.2.2.3">4.55</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.2.2.4">4.55</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.2.2.5">4.55</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.3.3.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.3.3.1.1">GPT-3.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.3.3.2">4.22</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.3.3.3">4.11</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.3.3.4">4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.3.3.5">3.77</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.4">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.4.4.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.4.4.1.1">GPT-4</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.4.4.2">4.22</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.4.4.3">4.11</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.4.4.4">4.22</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.4.4.5">4.11</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Evaluation of StackRAG and GPT models.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Limitations and Improvements</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">One of StackRAGl’s main limitations is that it takes more time than directly prompting GPT models, as it uses multiple agents and a few calls to the base LLM.
Another reason for the increased response time is the calls to SO API, which has a daily limit of 10000 API calls. We have tried to mitigate the time issue by adding more asynchronous processes to search for answers. However, SO heavily penalizes us for making multiple API calls in a short period, which has reduced our total number of calls per day even more.
We are working on techniques to replace some agents or reduce the LLM calls to decrease the response time.
However, note that we should “get used to delegating tasks to AI agents and potentially wait for a response” <span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.youtube.com/watch?v=sal78ACtGTc&amp;t=591s&amp;ab_channel=SequoiaCapital" title="">https://www.youtube.com/watch?v=sal78ACtGTc&amp;t=591s&amp;ab_channel=SequoiaCapital</a></span></span></span>. So we should be prepared to wait for longer times, as the overall workflow is made easier and reduces significant efforts from the developers. When StackRAG is used, instead of searching on a search engine and investigating through several links and answers, or instead of several iterations of the prompts with an LLM and validating the responses, StackRAG provides a comprehensive, reliable response with links at once.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">The new paradigm in software engineering is considered the usage of LLM-based agents. In this work, we present StackRAG, a Multiagent RAG-based tool that enhances the developers’ experience when searching for a query from Stack Overflow. StackRAG aims to generate reliable answers based on SO, instead of directly asking an LLM. The initial evaluations show the potential benefit of StackRAG when searching for related queries from SO, compared to GPT-family models. A future direction is to enhance the tool by adding search engines and other repositories to provide the answers.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
X. Hou, Y. Zhao, Y. Liu, Z. Yang, K. Wang, L. Li, X. Luo, D. Lo, J. Grundy, and H. Wang, “Large language models for software engineering: A systematic literature review,” 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
J. Kaddour, J. Harris, M. Mozes, H. Bradley, R. Raileanu, and R. McHardy, “Challenges and applications of large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2307.10169</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
A. Fan, B. Gokkaya, M. Harman, M. Lyubarskiy, S. Sengupta, S. Yoo, and J. M. Zhang, “Large language models for software engineering: Survey and open problems,” in <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">2023 IEEE/ACM International Conference on Software Engineering: Future of Software Engineering (ICSE-FoSE)</em>, 2023, pp. 31–53.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
“Be more familiar with our enemies and pave the way forward: A review of the roles bugs played in software failures,” <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Journal of Systems and Software</em>, vol. 133, pp. 68–94, 2017. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S0164121217301334" title="">https://www.sciencedirect.com/science/article/pii/S0164121217301334</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
A. Anderson, D. Huttenlocher, J. Kleinberg, and J. Leskovec, “Discovering value from community activity on focused question answering sites: a case study of stack overflow,” in <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, ser. KDD ’12.   New York, NY, USA: Association for Computing Machinery, 2012, p. 850–858. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/2339530.2339665" title="">https://doi.org/10.1145/2339530.2339665</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
J. He, X. Zhou, B. Xu, T. Zhang, K. Kim, Z. Yang, F. Thung, I. C. Irsan, and D. Lo, “Representation learning for stack overflow posts: How far are we?” <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">ACM Trans. Softw. Eng. Methodol.</em>, vol. 33, no. 3, mar 2024. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3635711" title="">https://doi.org/10.1145/3635711</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
S. S. Ahmed, S. Wang, Y. Tian, T.-H. P. Chen, and H. Zhang, “Studying and recommending information highlighting in stack overflow answers,” <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Information and Software Technology</em>, vol. 172, p. 107478, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
M. Wei, N. S. Harzevili, Y. Huang, J. Wang, and S. Wang, “Clear: contrastive learning for api recommendation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the 44th International Conference on Software Engineering</em>, 2022, pp. 376–387.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
J. He, B. Xu, Z. Yang, D. Han, C. Yang, and D. Lo, “Ptm4tag: sharpening tag recommendation of stack overflow posts with pre-trained models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension</em>, 2022, pp. 1–11.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Z. Gao, X. Xia, D. Lo, J. Grundy, X. Zhang, and Z. Xing, “I know what you are searching for: Code snippet recommendation from stack overflow posts,” <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">ACM Trans. Softw. Eng. Methodol.</em>, vol. 32, no. 3, apr 2023. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3550150" title="">https://doi.org/10.1145/3550150</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
J. Pei, Y. Wu, Z. Qin, Y. Cong, and J. Guan, “Attention-based model for predicting question relatedness on stack overflow,” in <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)</em>.   IEEE, 2021, pp. 97–107.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
D. Tang, Z. Chen, K. Kim, Y. Song, H. Tian, S. Ezzini, Y. Huang, J. Klein, and T. F. Bissyandé, “Codeagent: Collaborative agents for software engineering.” <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">CoRR</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Q. Luo, Y. Ye, S. Liang, Z. Zhang, Y. Qin, Y. Lu, Y. Wu, X. Cong, Y. Lin, Y. Zhang <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">et al.</em>, “Repoagent: An llm-powered open-source framework for repository-level code documentation generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib13.2.2">arXiv preprint arXiv:2402.16667</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
I. Bouzenia, P. Devanbu, and M. Pradel, “Repairagent: An autonomous, llm-based agent for program repair,” <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2403.17134</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
J. Lála, O. O’Donoghue, A. Shtedritski, S. Cox, S. G. Rodriques, and A. D. White, “Paperqa: Retrieval-augmented generative agent for scientific research,” <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">arXiv preprint arXiv:2312.07559</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford, “Okapi at trec-3.” 01 1994, pp. 0–.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
J. Carbonell and J. Goldstein, “The use of mmr, diversity-based reranking for reordering documents and producing summaries,” in <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, ser. SIGIR ’98.   New York, NY, USA: Association for Computing Machinery, 1998, p. 335–336. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/290941.291025" title="">https://doi.org/10.1145/290941.291025</a>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Jun 19 21:05:55 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
