<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval Augmented Generation Systems</title>
<!--Generated on Thu Jul 11 08:16:57 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Large language model,  Retrieval-augmented generation,  Model similarity" lang="en" name="keywords"/>
<base href="/html/2407.08275v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#S1" title="In Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Motivation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#S2" title="In Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#S3" title="In Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#S3.SS1" title="In 3. Methods â€£ Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Pair-wise Embedding Similarity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#S3.SS2" title="In 3. Methods â€£ Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Retrieval Similarity</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#S4" title="In Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experimental Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#S5" title="In Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#S5.SS1" title="In 5. Results â€£ Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Intra- and Inter-Family Clusters</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#S5.SS2" title="In 5. Results â€£ Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Open Source Alternatives to Proprietary Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#S6" title="In Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#S7" title="In Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval Augmented Generation Systems</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Laura Caspari
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:laura.caspari@uni-passau.de">laura.caspari@uni-passau.de</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0009-0002-6670-3211" title="ORCID identifier">0009-0002-6670-3211</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id3.1.id1">University of Passau</span><span class="ltx_text ltx_affiliation_city" id="id4.2.id2">Passau</span><span class="ltx_text ltx_affiliation_country" id="id5.3.id3">Germany</span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kanishka Ghosh Dastidar
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:kanishka.ghoshdastidar@uni-passau.de">kanishka.ghoshdastidar@uni-passau.de</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0003-4171-0597" title="ORCID identifier">0000-0003-4171-0597</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id6.1.id1">University of Passau</span><span class="ltx_text ltx_affiliation_city" id="id7.2.id2">Passau</span><span class="ltx_text ltx_affiliation_country" id="id8.3.id3">Germany</span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Saber Zerhoudi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:saber.zerhoudi@uni-passau.de">saber.zerhoudi@uni-passau.de</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0003-2259-0462" title="ORCID identifier">0000-0003-2259-0462</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id9.1.id1">University of Passau</span><span class="ltx_text ltx_affiliation_city" id="id10.2.id2">Passau</span><span class="ltx_text ltx_affiliation_country" id="id11.3.id3">Germany</span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jelena Mitrovic
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:jelena.mitrovic@uni-passau.de">jelena.mitrovic@uni-passau.de</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0003-3220-8749" title="ORCID identifier">0000-0003-3220-8749</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id12.1.id1">University of Passau</span><span class="ltx_text ltx_affiliation_city" id="id13.2.id2">Passau</span><span class="ltx_text ltx_affiliation_country" id="id14.3.id3">Germany</span>
</span></span></span>
<span class="ltx_author_before">Â andÂ </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Michael Granitzer
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:michael.granitzer@uni-passau.de">michael.granitzer@uni-passau.de</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0003-3566-5507" title="ORCID identifier">0000-0003-3566-5507</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id15.1.id1">University of Passau</span><span class="ltx_text ltx_affiliation_city" id="id16.2.id2">Passau</span><span class="ltx_text ltx_affiliation_country" id="id17.3.id3">Germany</span>
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id2.2">The choice of embedding model is a crucial step in the design of Retrieval Augmented Generation (RAG) systems. Given the sheer volume of available options, identifying clusters of similar models streamlines this model selection process. Relying solely on benchmark performance scores only allows for a weak assessment of model similarity. Thus, in this study, we evaluate the similarity of embedding models within the context of RAG systems. Our assessment is two-fold: We use Centered Kernel Alignment to compare embeddings on a pair-wise level. Additionally, as it is especially pertinent to RAG systems, we evaluate the similarity of retrieval results between these models using Jaccard and rank similarity. We compare different families of embedding models, including proprietary ones, across five datasets from the popular Benchmark Information Retrieval (BEIR). Through our experiments we identify clusters of models corresponding to model families, but interestingly, also some inter-family clusters. Furthermore, our analysis of top-<math alttext="k" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><mi id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><ci id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">italic_k</annotation></semantics></math> retrieval similarity reveals high-variance at low <math alttext="k" class="ltx_Math" display="inline" id="id2.2.m2.1"><semantics id="id2.2.m2.1a"><mi id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><ci id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="id2.2.m2.1d">italic_k</annotation></semantics></math> values. We also identify possible open-source alternatives to proprietary models, with Mistral exhibiting the highest similarity to OpenAI models.</p>
</div>
<div class="ltx_keywords">Large language model, Retrieval-augmented generation, Model similarity
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id1"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">ccs: </span>Information systemsÂ Evaluation of retrieval results</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id2"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">ccs: </span>Information systemsÂ Retrieval models and ranking</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id3"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">ccs: </span>Information systemsÂ Language models</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Motivation</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Retrieval-Augmented Generation (RAG) is an emerging paradigm that helps mitigate the problems of factual hallucination <cite class="ltx_cite ltx_citemacro_citep">(Ji etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib14" title="">2023</a>)</cite> and outdated training data <cite class="ltx_cite ltx_citemacro_citep">(Mousavi
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib28" title="">2024</a>)</cite> of large language models (LLMs) by providing these models with access to an external, non-parametric knowledge source (e.g. a document corpus). Central to the functioning of RAG frameworks is the retrieval step, wherein a small subset of candidate documents is retrieved from the document corpus, specific to the input query or prompt. This retrieval process, known as dense-retrieval, hinges on text embeddings. Typically, the generation of these embeddings is assigned to an LLM, for which there are several options due to the rapid evolution of the field. Consequently, selecting the most suitable embedding model from an array of available choices emerges as a critical aspect in the development of RAG systems. The information to guide this choice is currently primarily limited to architectural details (which are also on occasion scarce due to the prevalence of closed models) and performance benchmarks such as the Massive Text Embedding Benchmark (MTEB) <cite class="ltx_cite ltx_citemacro_citep">(Muennighoff etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib29" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">We posit that an analysis of the similarity of the embeddings generated by these models would significantly aid this model selection process. Given the large number of candidates and ever increasing scale of the models, a from-scratch empirical evaluation of the embedding quality of these LLMs on a particular task can incur significant costs. This challenge becomes especially pronounced when dealing with large-scale corpora comprising potentially millions of documents. While the relative performance scores of these models on benchmark datasets offer the simplified perspective of comparing a single scalar value on an array of downstream tasks, such a view of model similarity might overlook the nuances of the relative behaviour of the models <cite class="ltx_cite ltx_citemacro_citep">(Klabunde etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib16" title="">2023b</a>)</cite>. As an example, the absolute difference in precision@k between two retrieval systems only provides a weak indication of the overlap of retrieved results. We argue that identifying clusters of models with similar behaviour would allow practitioners to construct smaller, yet diverse candidate pools of models to evaluate. Beyond model selection, as highlighted by Klabunde et al., <cite class="ltx_cite ltx_citemacro_citep">(Klabunde etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib15" title="">2023a</a>)</cite>, such an analysis also facilitates the identification of common factors contributing to strong performance, easier model ensembling, and detection of potential instances of unauthorized model reuse.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this paper, we analyze different LLMs in terms of the similarities of the embeddings they generate. Our similarity analysis serves as an unsupervised evaluation framework for these embedding models, in contrast to performance benchmarks that require labelled data. We do this from a dual perspective - we directly compare the embeddings using representational similarity measures. Additionally, we evaluate model similarity specifically in terms of their functional impact on RAG systems i.e. we look at how similar the retrieved results are.
Our evaluation focuses on several prominent model families, to analyze similarities both within and across them. We also compare proprietary models (such as those by OpenAI or Cohere) to open-sourced ones in order to identify the most similar alternatives. Our experiments are carried out on five popular benchmark datasets to determine if similarities between models are influenced by the choice of data. Our code is available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/casparil/embedding-model-similarity" title="">https://github.com/casparil/embedding-model-similarity</a>.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Studies evaluating similarities of neural networks fall into two main categories: the first involves comparing activations of different models generated at any pair of layers for a specific input (representational similarity), while the second compares the model outputs (functional similarity). Raghu et al. <cite class="ltx_cite ltx_citemacro_citep">(Raghu etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib34" title="">2017</a>)</cite> and Morcos et al. <cite class="ltx_cite ltx_citemacro_citep">(Morcos
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib27" title="">2018</a>)</cite> propose measures building on Canonical Correlation Analysis (CCA) <cite class="ltx_cite ltx_citemacro_citep">(Hardoon
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib12" title="">2004</a>)</cite>, a statistical technique used to find the linear relationship between two sets of variables by maximizing their correlation. Such comparisons using CCA or variants thereof can be found in several works <cite class="ltx_cite ltx_citemacro_citep">(Ding
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib7" title="">2021</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Zullich etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib43" title="">2020</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Chen
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib5" title="">2024</a>)</cite>. Beyond CCA-based measures, other works have also explored computing correlations <cite class="ltx_cite ltx_citemacro_citep">(Li
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib22" title="">2016</a>)</cite> and the mutual information <cite class="ltx_cite ltx_citemacro_citep">(Li
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib21" title="">2015</a>)</cite> between neurons across networks. Kornblith et al. <cite class="ltx_cite ltx_citemacro_citep">(Kornblith
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib17" title="">2019</a>)</cite> propose Centered Kernel Alignment (CKA), which they show improves over several similarity measures in identifying corresponding layers of identical networks with different initializations. A diverse range of functional similarity evaluations have also been explored in the literature. A few examples include model-stitching <cite class="ltx_cite ltx_citemacro_citep">(Bansal
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib3" title="">2021</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Lenc and Vedaldi, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib19" title="">2015</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Balogh and
Jelasity, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib2" title="">2023</a>)</cite>, disagreement measures between output classes <cite class="ltx_cite ltx_citemacro_citep">(MilaniÂ Fard etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib26" title="">2016</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Xie
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib42" title="">2019</a>)</cite>, and quantifying the similarity between the class-wise output probabilities <cite class="ltx_cite ltx_citemacro_citep">(Li
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib23" title="">2021</a>)</cite>. We would point the reader to the survey by Klabunde et al. <cite class="ltx_cite ltx_citemacro_citep">(Klabunde etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib16" title="">2023b</a>)</cite> for a detailed overview of representational and functional similarity measures.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Recently, a few works have also focused on specifically evaluating the similarity of LLMs. While Wu et al. <cite class="ltx_cite ltx_citemacro_citep">(Wu etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib40" title="">2020</a>)</cite> evaluate language models along several perspectives, such as their representational and neuron-level similarities, their evaluation pre-dates the introduction of the recent wave of large scale models. Freestone and Santu <cite class="ltx_cite ltx_citemacro_citep">(Freestone and
Santu, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib10" title="">2024</a>)</cite> consider similarities of word embeddings, and evaluate if LLMs differ significantly to classical encoding models in terms of their representations. The works by Klabunde et al. <cite class="ltx_cite ltx_citemacro_citep">(Klabunde etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib15" title="">2023a</a>)</cite> and Brown et al. <cite class="ltx_cite ltx_citemacro_citep">(Brown
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib4" title="">2023</a>)</cite> are more recent, and evaluate the representational similarity of LLMs, with the latter also considering the similarities between models of different sizes in the same model family.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Much of the literature on evaluation of LLM embeddings focuses on their performance on downstream tasks, with benchmarks such as BEIR <cite class="ltx_cite ltx_citemacro_citep">(Thakur etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib36" title="">2021</a>)</cite> (for retrieval specifically) and MTEB <cite class="ltx_cite ltx_citemacro_citep">(Muennighoff etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib29" title="">2023</a>)</cite> providing a unified view of embedding quality across metrics and datasets. The metrics used here mostly include typical information retrieval metrics such as precision, recall, and mean reciprocal rank at certain cutoffs. Some works specifically evaluate the retrieval components in a RAG context, where they either use a dataset outside of those included in the benchmarks <cite class="ltx_cite ltx_citemacro_citep">(Finardi etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib9" title="">2024</a>)</cite> or where the evaluation encompasses other aspects of the retriever beyond the embedding model being used <cite class="ltx_cite ltx_citemacro_citep">(Sawarkar
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib35" title="">2024</a>)</cite>. Another approach, that does not rely on ground-truth labels, is given by the Retrieval Augmented Generation Assessment (RAGAS) framework, which uses an LLM to determine the ratio of sentences in the retrieved context that are relevant to the answer being generated <cite class="ltx_cite ltx_citemacro_citep">(Es
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib8" title="">2023</a>)</cite>. To the best of our knowledge, there are no works that evaluate the similarity of embedding models from a retrieval perspective.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>The datasets used for generating embeddings with their number of queries and corpus size.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S2.T1.1.1.1.1">Dataset Name</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.2">Queries</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.3">Corpus</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T1.1.2.1.1">TREC-COVID</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.2.1.2">50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.2.1.3">171k</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.1.3.2.1">NFCorpus</th>
<td class="ltx_td ltx_align_center" id="S2.T1.1.3.2.2">323</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.3.2.3">3.6k</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.1.4.3.1">FiQA-2018</th>
<td class="ltx_td ltx_align_center" id="S2.T1.1.4.3.2">648</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.4.3.3">57k</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.1.5.4.1">ArguAna</th>
<td class="ltx_td ltx_align_center" id="S2.T1.1.5.4.2">1406</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.5.4.3">8.67k</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S2.T1.1.6.5.1">SciFact</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.6.5.2">300</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.6.5.3">5k</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Methods</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We evaluate embedding model similarity using two approaches. The first directly compares the embeddings of text chunks generated by the models. The second approach is specific to the RAG context, where we evaluate the similarity of retrieved results for a given query. These approaches are discussed in detail in the following sections.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2. </span>We compare a diverse set of open source models from different families as well as proprietary models with varying performance on MTEB.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.1.1">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.1.2">Embedding dimension</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.1.3">Max. Tokens</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.1.4">MTEB Average</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.1.5">Open Source</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.2.1.1">SFR-Embedding-Mistral</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.2.1.2">4096</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.2.1.3">32768</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.2.1.4">67.56</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.2.1.5">âœ“</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.3.2">
<td class="ltx_td ltx_align_left" id="S3.T2.1.3.2.1">mxbai-embed-large-v1</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.3.2.2">1024</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.3.2.3">512</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.3.2.4">64.68</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.3.2.5">âœ“</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.4.3">
<td class="ltx_td ltx_align_left" id="S3.T2.1.4.3.1">UAE-Large-V1</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.4.3.2">1024</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.4.3.3">512</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.4.3.4">64.64</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.4.3.5">âœ“</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.5.4">
<td class="ltx_td ltx_align_left" id="S3.T2.1.5.4.1">text-embedding-3-large</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.5.4.2">3072</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.5.4.3">8191</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.5.4.4">64.59</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.5.4.5">âœ—</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.6.5">
<td class="ltx_td ltx_align_left" id="S3.T2.1.6.5.1">Cohere embed-english-v3.0</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.6.5.2">1024</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.6.5.3">512</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.6.5.4">64.47</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.6.5.5">âœ—</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.7.6">
<td class="ltx_td ltx_align_left" id="S3.T2.1.7.6.1">bge-large-en-v1.5</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.7.6.2">1024</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.7.6.3">512</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.7.6.4">64.23</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.7.6.5">âœ“</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.8.7">
<td class="ltx_td ltx_align_left" id="S3.T2.1.8.7.1">bge-base-en-v1.5</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.8.7.2">768</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.8.7.3">512</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.8.7.4">63.55</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.8.7.5">âœ“</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.9.8">
<td class="ltx_td ltx_align_left" id="S3.T2.1.9.8.1">gte-large</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.9.8.2">1024</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.9.8.3">512</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.9.8.4">63.13</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.9.8.5">âœ“</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.10.9">
<td class="ltx_td ltx_align_left" id="S3.T2.1.10.9.1">gte-base</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.10.9.2">768</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.10.9.3">512</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.10.9.4">62.39</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.10.9.5">âœ“</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.11.10">
<td class="ltx_td ltx_align_left" id="S3.T2.1.11.10.1">text-embedding-3-small</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.11.10.2">1536</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.11.10.3">8191</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.11.10.4">62.26</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.11.10.5">âœ—</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.12.11">
<td class="ltx_td ltx_align_left" id="S3.T2.1.12.11.1">e5-large-v2</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.12.11.2">1024</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.12.11.3">512</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.12.11.4">62.25</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.12.11.5">âœ“</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.13.12">
<td class="ltx_td ltx_align_left" id="S3.T2.1.13.12.1">bge-small-en-v1.5</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.13.12.2">384</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.13.12.3">512</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.13.12.4">62.17</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.13.12.5">âœ“</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.14.13">
<td class="ltx_td ltx_align_left" id="S3.T2.1.14.13.1">e5-base-v2</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.14.13.2">768</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.14.13.3">512</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.14.13.4">61.5</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.14.13.5">âœ“</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.15.14">
<td class="ltx_td ltx_align_left" id="S3.T2.1.15.14.1">gte-small</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.15.14.2">384</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.15.14.3">512</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.15.14.4">61.36</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.15.14.5">âœ“</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.16.15">
<td class="ltx_td ltx_align_left" id="S3.T2.1.16.15.1">e5-small-v2</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.16.15.2">384</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.16.15.3">512</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.16.15.4">59.93</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.16.15.5">âœ“</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.17.16">
<td class="ltx_td ltx_align_left" id="S3.T2.1.17.16.1">gtr-t5-large</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.17.16.2">768</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.17.16.3">512</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.17.16.4">58.28</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.17.16.5">âœ“</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.18.17">
<td class="ltx_td ltx_align_left" id="S3.T2.1.18.17.1">sentence-t5-large</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.18.17.2">768</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.18.17.3">512</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.18.17.4">57.06</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.18.17.5">âœ“</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.19.18">
<td class="ltx_td ltx_align_left" id="S3.T2.1.19.18.1">gtr-t5-base</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.19.18.2">768</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.19.18.3">512</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.19.18.4">56.19</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.19.18.5">âœ“</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.20.19">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T2.1.20.19.1">sentence-t5-base</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.20.19.2">768</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.20.19.3">512</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.20.19.4">55.27</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.20.19.5">âœ“</td>
</tr>
</tbody>
</table>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Pair-wise Embedding Similarity</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">There are several metrics defined in the literature that measure representational similarity <cite class="ltx_cite ltx_citemacro_citep">(Klabunde etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib16" title="">2023b</a>)</cite>. Many of these metrics require the representation spaces of the embeddings to be compared to be aligned and/or the dimensionality of the embeddings across the models to be identical. To avoid these constraints, we pick Centered Kernel Alignment (CKA) <cite class="ltx_cite ltx_citemacro_citep">(Kornblith
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib17" title="">2019</a>)</cite> with a linear kernel as our similarity measure.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">The measure computes similarity between two sets of embeddings in two steps. First, for a set of embeddings, the pair-wise similarity scores between all entries within this set are computed using the kernel function. Thus, row <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.1">k</span> of the resulting similarity matrix contains entries representing the similarity between embedding <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.2">k</span> and all other embeddings, including itself. Computing two such embedding similarity matrices for different models with the same number of embeddings then leads to two matrices E and Eâ€™ of matching dimensions. These are compared directly in the second step with the Hilbert-Schmidt Independence Criterion (HSIC) <cite class="ltx_cite ltx_citemacro_citep">(Gretton etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib11" title="">2005</a>)</cite> using the following formula:</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="CKA(E,E^{\prime})=\frac{HSIC(E,E^{\prime})}{\sqrt{HSIC(E,E)HSIC(E^{\prime},E^{%
\prime})}}" class="ltx_Math" display="block" id="S3.E1.m1.8"><semantics id="S3.E1.m1.8a"><mrow id="S3.E1.m1.8.8" xref="S3.E1.m1.8.8.cmml"><mrow id="S3.E1.m1.8.8.1" xref="S3.E1.m1.8.8.1.cmml"><mi id="S3.E1.m1.8.8.1.3" xref="S3.E1.m1.8.8.1.3.cmml">C</mi><mo id="S3.E1.m1.8.8.1.2" xref="S3.E1.m1.8.8.1.2.cmml">â¢</mo><mi id="S3.E1.m1.8.8.1.4" xref="S3.E1.m1.8.8.1.4.cmml">K</mi><mo id="S3.E1.m1.8.8.1.2a" xref="S3.E1.m1.8.8.1.2.cmml">â¢</mo><mi id="S3.E1.m1.8.8.1.5" xref="S3.E1.m1.8.8.1.5.cmml">A</mi><mo id="S3.E1.m1.8.8.1.2b" xref="S3.E1.m1.8.8.1.2.cmml">â¢</mo><mrow id="S3.E1.m1.8.8.1.1.1" xref="S3.E1.m1.8.8.1.1.2.cmml"><mo id="S3.E1.m1.8.8.1.1.1.2" stretchy="false" xref="S3.E1.m1.8.8.1.1.2.cmml">(</mo><mi id="S3.E1.m1.7.7" xref="S3.E1.m1.7.7.cmml">E</mi><mo id="S3.E1.m1.8.8.1.1.1.3" xref="S3.E1.m1.8.8.1.1.2.cmml">,</mo><msup id="S3.E1.m1.8.8.1.1.1.1" xref="S3.E1.m1.8.8.1.1.1.1.cmml"><mi id="S3.E1.m1.8.8.1.1.1.1.2" xref="S3.E1.m1.8.8.1.1.1.1.2.cmml">E</mi><mo id="S3.E1.m1.8.8.1.1.1.1.3" xref="S3.E1.m1.8.8.1.1.1.1.3.cmml">â€²</mo></msup><mo id="S3.E1.m1.8.8.1.1.1.4" stretchy="false" xref="S3.E1.m1.8.8.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.8.8.2" xref="S3.E1.m1.8.8.2.cmml">=</mo><mfrac id="S3.E1.m1.6.6" xref="S3.E1.m1.6.6.cmml"><mrow id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml"><mi id="S3.E1.m1.2.2.2.4" xref="S3.E1.m1.2.2.2.4.cmml">H</mi><mo id="S3.E1.m1.2.2.2.3" xref="S3.E1.m1.2.2.2.3.cmml">â¢</mo><mi id="S3.E1.m1.2.2.2.5" xref="S3.E1.m1.2.2.2.5.cmml">S</mi><mo id="S3.E1.m1.2.2.2.3a" xref="S3.E1.m1.2.2.2.3.cmml">â¢</mo><mi id="S3.E1.m1.2.2.2.6" xref="S3.E1.m1.2.2.2.6.cmml">I</mi><mo id="S3.E1.m1.2.2.2.3b" xref="S3.E1.m1.2.2.2.3.cmml">â¢</mo><mi id="S3.E1.m1.2.2.2.7" xref="S3.E1.m1.2.2.2.7.cmml">C</mi><mo id="S3.E1.m1.2.2.2.3c" xref="S3.E1.m1.2.2.2.3.cmml">â¢</mo><mrow id="S3.E1.m1.2.2.2.2.1" xref="S3.E1.m1.2.2.2.2.2.cmml"><mo id="S3.E1.m1.2.2.2.2.1.2" stretchy="false" xref="S3.E1.m1.2.2.2.2.2.cmml">(</mo><mi id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">E</mi><mo id="S3.E1.m1.2.2.2.2.1.3" xref="S3.E1.m1.2.2.2.2.2.cmml">,</mo><msup id="S3.E1.m1.2.2.2.2.1.1" xref="S3.E1.m1.2.2.2.2.1.1.cmml"><mi id="S3.E1.m1.2.2.2.2.1.1.2" xref="S3.E1.m1.2.2.2.2.1.1.2.cmml">E</mi><mo id="S3.E1.m1.2.2.2.2.1.1.3" xref="S3.E1.m1.2.2.2.2.1.1.3.cmml">â€²</mo></msup><mo id="S3.E1.m1.2.2.2.2.1.4" stretchy="false" xref="S3.E1.m1.2.2.2.2.2.cmml">)</mo></mrow></mrow><msqrt id="S3.E1.m1.6.6.6" xref="S3.E1.m1.6.6.6.cmml"><mrow id="S3.E1.m1.6.6.6.4.4" xref="S3.E1.m1.6.6.6.4.4.cmml"><mi id="S3.E1.m1.6.6.6.4.4.6" xref="S3.E1.m1.6.6.6.4.4.6.cmml">H</mi><mo id="S3.E1.m1.6.6.6.4.4.5" xref="S3.E1.m1.6.6.6.4.4.5.cmml">â¢</mo><mi id="S3.E1.m1.6.6.6.4.4.7" xref="S3.E1.m1.6.6.6.4.4.7.cmml">S</mi><mo id="S3.E1.m1.6.6.6.4.4.5a" xref="S3.E1.m1.6.6.6.4.4.5.cmml">â¢</mo><mi id="S3.E1.m1.6.6.6.4.4.8" xref="S3.E1.m1.6.6.6.4.4.8.cmml">I</mi><mo id="S3.E1.m1.6.6.6.4.4.5b" xref="S3.E1.m1.6.6.6.4.4.5.cmml">â¢</mo><mi id="S3.E1.m1.6.6.6.4.4.9" xref="S3.E1.m1.6.6.6.4.4.9.cmml">C</mi><mo id="S3.E1.m1.6.6.6.4.4.5c" xref="S3.E1.m1.6.6.6.4.4.5.cmml">â¢</mo><mrow id="S3.E1.m1.6.6.6.4.4.10.2" xref="S3.E1.m1.6.6.6.4.4.10.1.cmml"><mo id="S3.E1.m1.6.6.6.4.4.10.2.1" stretchy="false" xref="S3.E1.m1.6.6.6.4.4.10.1.cmml">(</mo><mi id="S3.E1.m1.3.3.3.1.1.1" xref="S3.E1.m1.3.3.3.1.1.1.cmml">E</mi><mo id="S3.E1.m1.6.6.6.4.4.10.2.2" xref="S3.E1.m1.6.6.6.4.4.10.1.cmml">,</mo><mi id="S3.E1.m1.4.4.4.2.2.2" xref="S3.E1.m1.4.4.4.2.2.2.cmml">E</mi><mo id="S3.E1.m1.6.6.6.4.4.10.2.3" stretchy="false" xref="S3.E1.m1.6.6.6.4.4.10.1.cmml">)</mo></mrow><mo id="S3.E1.m1.6.6.6.4.4.5d" xref="S3.E1.m1.6.6.6.4.4.5.cmml">â¢</mo><mi id="S3.E1.m1.6.6.6.4.4.11" xref="S3.E1.m1.6.6.6.4.4.11.cmml">H</mi><mo id="S3.E1.m1.6.6.6.4.4.5e" xref="S3.E1.m1.6.6.6.4.4.5.cmml">â¢</mo><mi id="S3.E1.m1.6.6.6.4.4.12" xref="S3.E1.m1.6.6.6.4.4.12.cmml">S</mi><mo id="S3.E1.m1.6.6.6.4.4.5f" xref="S3.E1.m1.6.6.6.4.4.5.cmml">â¢</mo><mi id="S3.E1.m1.6.6.6.4.4.13" xref="S3.E1.m1.6.6.6.4.4.13.cmml">I</mi><mo id="S3.E1.m1.6.6.6.4.4.5g" xref="S3.E1.m1.6.6.6.4.4.5.cmml">â¢</mo><mi id="S3.E1.m1.6.6.6.4.4.14" xref="S3.E1.m1.6.6.6.4.4.14.cmml">C</mi><mo id="S3.E1.m1.6.6.6.4.4.5h" xref="S3.E1.m1.6.6.6.4.4.5.cmml">â¢</mo><mrow id="S3.E1.m1.6.6.6.4.4.4.2" xref="S3.E1.m1.6.6.6.4.4.4.3.cmml"><mo id="S3.E1.m1.6.6.6.4.4.4.2.3" stretchy="false" xref="S3.E1.m1.6.6.6.4.4.4.3.cmml">(</mo><msup id="S3.E1.m1.5.5.5.3.3.3.1.1" xref="S3.E1.m1.5.5.5.3.3.3.1.1.cmml"><mi id="S3.E1.m1.5.5.5.3.3.3.1.1.2" xref="S3.E1.m1.5.5.5.3.3.3.1.1.2.cmml">E</mi><mo id="S3.E1.m1.5.5.5.3.3.3.1.1.3" xref="S3.E1.m1.5.5.5.3.3.3.1.1.3.cmml">â€²</mo></msup><mo id="S3.E1.m1.6.6.6.4.4.4.2.4" xref="S3.E1.m1.6.6.6.4.4.4.3.cmml">,</mo><msup id="S3.E1.m1.6.6.6.4.4.4.2.2" xref="S3.E1.m1.6.6.6.4.4.4.2.2.cmml"><mi id="S3.E1.m1.6.6.6.4.4.4.2.2.2" xref="S3.E1.m1.6.6.6.4.4.4.2.2.2.cmml">E</mi><mo id="S3.E1.m1.6.6.6.4.4.4.2.2.3" xref="S3.E1.m1.6.6.6.4.4.4.2.2.3.cmml">â€²</mo></msup><mo id="S3.E1.m1.6.6.6.4.4.4.2.5" stretchy="false" xref="S3.E1.m1.6.6.6.4.4.4.3.cmml">)</mo></mrow></mrow></msqrt></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.8b"><apply id="S3.E1.m1.8.8.cmml" xref="S3.E1.m1.8.8"><eq id="S3.E1.m1.8.8.2.cmml" xref="S3.E1.m1.8.8.2"></eq><apply id="S3.E1.m1.8.8.1.cmml" xref="S3.E1.m1.8.8.1"><times id="S3.E1.m1.8.8.1.2.cmml" xref="S3.E1.m1.8.8.1.2"></times><ci id="S3.E1.m1.8.8.1.3.cmml" xref="S3.E1.m1.8.8.1.3">ğ¶</ci><ci id="S3.E1.m1.8.8.1.4.cmml" xref="S3.E1.m1.8.8.1.4">ğ¾</ci><ci id="S3.E1.m1.8.8.1.5.cmml" xref="S3.E1.m1.8.8.1.5">ğ´</ci><interval closure="open" id="S3.E1.m1.8.8.1.1.2.cmml" xref="S3.E1.m1.8.8.1.1.1"><ci id="S3.E1.m1.7.7.cmml" xref="S3.E1.m1.7.7">ğ¸</ci><apply id="S3.E1.m1.8.8.1.1.1.1.cmml" xref="S3.E1.m1.8.8.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.8.8.1.1.1.1.1.cmml" xref="S3.E1.m1.8.8.1.1.1.1">superscript</csymbol><ci id="S3.E1.m1.8.8.1.1.1.1.2.cmml" xref="S3.E1.m1.8.8.1.1.1.1.2">ğ¸</ci><ci id="S3.E1.m1.8.8.1.1.1.1.3.cmml" xref="S3.E1.m1.8.8.1.1.1.1.3">â€²</ci></apply></interval></apply><apply id="S3.E1.m1.6.6.cmml" xref="S3.E1.m1.6.6"><divide id="S3.E1.m1.6.6.7.cmml" xref="S3.E1.m1.6.6"></divide><apply id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2"><times id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.3"></times><ci id="S3.E1.m1.2.2.2.4.cmml" xref="S3.E1.m1.2.2.2.4">ğ»</ci><ci id="S3.E1.m1.2.2.2.5.cmml" xref="S3.E1.m1.2.2.2.5">ğ‘†</ci><ci id="S3.E1.m1.2.2.2.6.cmml" xref="S3.E1.m1.2.2.2.6">ğ¼</ci><ci id="S3.E1.m1.2.2.2.7.cmml" xref="S3.E1.m1.2.2.2.7">ğ¶</ci><interval closure="open" id="S3.E1.m1.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.1"><ci id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">ğ¸</ci><apply id="S3.E1.m1.2.2.2.2.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1">superscript</csymbol><ci id="S3.E1.m1.2.2.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.2">ğ¸</ci><ci id="S3.E1.m1.2.2.2.2.1.1.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3">â€²</ci></apply></interval></apply><apply id="S3.E1.m1.6.6.6.cmml" xref="S3.E1.m1.6.6.6"><root id="S3.E1.m1.6.6.6a.cmml" xref="S3.E1.m1.6.6.6"></root><apply id="S3.E1.m1.6.6.6.4.4.cmml" xref="S3.E1.m1.6.6.6.4.4"><times id="S3.E1.m1.6.6.6.4.4.5.cmml" xref="S3.E1.m1.6.6.6.4.4.5"></times><ci id="S3.E1.m1.6.6.6.4.4.6.cmml" xref="S3.E1.m1.6.6.6.4.4.6">ğ»</ci><ci id="S3.E1.m1.6.6.6.4.4.7.cmml" xref="S3.E1.m1.6.6.6.4.4.7">ğ‘†</ci><ci id="S3.E1.m1.6.6.6.4.4.8.cmml" xref="S3.E1.m1.6.6.6.4.4.8">ğ¼</ci><ci id="S3.E1.m1.6.6.6.4.4.9.cmml" xref="S3.E1.m1.6.6.6.4.4.9">ğ¶</ci><interval closure="open" id="S3.E1.m1.6.6.6.4.4.10.1.cmml" xref="S3.E1.m1.6.6.6.4.4.10.2"><ci id="S3.E1.m1.3.3.3.1.1.1.cmml" xref="S3.E1.m1.3.3.3.1.1.1">ğ¸</ci><ci id="S3.E1.m1.4.4.4.2.2.2.cmml" xref="S3.E1.m1.4.4.4.2.2.2">ğ¸</ci></interval><ci id="S3.E1.m1.6.6.6.4.4.11.cmml" xref="S3.E1.m1.6.6.6.4.4.11">ğ»</ci><ci id="S3.E1.m1.6.6.6.4.4.12.cmml" xref="S3.E1.m1.6.6.6.4.4.12">ğ‘†</ci><ci id="S3.E1.m1.6.6.6.4.4.13.cmml" xref="S3.E1.m1.6.6.6.4.4.13">ğ¼</ci><ci id="S3.E1.m1.6.6.6.4.4.14.cmml" xref="S3.E1.m1.6.6.6.4.4.14">ğ¶</ci><interval closure="open" id="S3.E1.m1.6.6.6.4.4.4.3.cmml" xref="S3.E1.m1.6.6.6.4.4.4.2"><apply id="S3.E1.m1.5.5.5.3.3.3.1.1.cmml" xref="S3.E1.m1.5.5.5.3.3.3.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.5.3.3.3.1.1.1.cmml" xref="S3.E1.m1.5.5.5.3.3.3.1.1">superscript</csymbol><ci id="S3.E1.m1.5.5.5.3.3.3.1.1.2.cmml" xref="S3.E1.m1.5.5.5.3.3.3.1.1.2">ğ¸</ci><ci id="S3.E1.m1.5.5.5.3.3.3.1.1.3.cmml" xref="S3.E1.m1.5.5.5.3.3.3.1.1.3">â€²</ci></apply><apply id="S3.E1.m1.6.6.6.4.4.4.2.2.cmml" xref="S3.E1.m1.6.6.6.4.4.4.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.6.4.4.4.2.2.1.cmml" xref="S3.E1.m1.6.6.6.4.4.4.2.2">superscript</csymbol><ci id="S3.E1.m1.6.6.6.4.4.4.2.2.2.cmml" xref="S3.E1.m1.6.6.6.4.4.4.2.2.2">ğ¸</ci><ci id="S3.E1.m1.6.6.6.4.4.4.2.2.3.cmml" xref="S3.E1.m1.6.6.6.4.4.4.2.2.3">â€²</ci></apply></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.8c">CKA(E,E^{\prime})=\frac{HSIC(E,E^{\prime})}{\sqrt{HSIC(E,E)HSIC(E^{\prime},E^{%
\prime})}}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.8d">italic_C italic_K italic_A ( italic_E , italic_E start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) = divide start_ARG italic_H italic_S italic_I italic_C ( italic_E , italic_E start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) end_ARG start_ARG square-root start_ARG italic_H italic_S italic_I italic_C ( italic_E , italic_E ) italic_H italic_S italic_I italic_C ( italic_E start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT , italic_E start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) end_ARG end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">The resulting similarity scores are bounded in the interval [0, 1] with a score of 1 indicating equivalent representations. CKA assumes that representations are mean-centered.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Retrieval Similarity</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">While a pair-wise comparison of embeddings offers insights into the similarities of the representations learned by these models, it does not suffice to quantify the similarities in outcomes when these embedding models are deployed for specific tasks. Therefore, in context of RAG systems, we consider the similarity of retrieved text chunks for a given query, when different embedding models are used. As a first step, for a given dataset, we generate embeddings of queries and document chunks with each of the embedding models. We then retrieve the <math alttext="k" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_k</annotation></semantics></math> most similar embeddings in terms of the cosine similarity for a particular query. As these embeddings correspond to specific chunks of text, we derive the sets of retrieved chunks C and Câ€™ for a pair of models. To measure the similarity of these sets, we use the Jaccard similarity coefficient as follows:</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Jaccard(C,C^{\prime})=\frac{|C\cap C^{\prime}|}{|C\cup C^{\prime}|}" class="ltx_Math" display="block" id="S3.E2.m1.4"><semantics id="S3.E2.m1.4a"><mrow id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml"><mrow id="S3.E2.m1.4.4.1" xref="S3.E2.m1.4.4.1.cmml"><mi id="S3.E2.m1.4.4.1.3" xref="S3.E2.m1.4.4.1.3.cmml">J</mi><mo id="S3.E2.m1.4.4.1.2" xref="S3.E2.m1.4.4.1.2.cmml">â¢</mo><mi id="S3.E2.m1.4.4.1.4" xref="S3.E2.m1.4.4.1.4.cmml">a</mi><mo id="S3.E2.m1.4.4.1.2a" xref="S3.E2.m1.4.4.1.2.cmml">â¢</mo><mi id="S3.E2.m1.4.4.1.5" xref="S3.E2.m1.4.4.1.5.cmml">c</mi><mo id="S3.E2.m1.4.4.1.2b" xref="S3.E2.m1.4.4.1.2.cmml">â¢</mo><mi id="S3.E2.m1.4.4.1.6" xref="S3.E2.m1.4.4.1.6.cmml">c</mi><mo id="S3.E2.m1.4.4.1.2c" xref="S3.E2.m1.4.4.1.2.cmml">â¢</mo><mi id="S3.E2.m1.4.4.1.7" xref="S3.E2.m1.4.4.1.7.cmml">a</mi><mo id="S3.E2.m1.4.4.1.2d" xref="S3.E2.m1.4.4.1.2.cmml">â¢</mo><mi id="S3.E2.m1.4.4.1.8" xref="S3.E2.m1.4.4.1.8.cmml">r</mi><mo id="S3.E2.m1.4.4.1.2e" xref="S3.E2.m1.4.4.1.2.cmml">â¢</mo><mi id="S3.E2.m1.4.4.1.9" xref="S3.E2.m1.4.4.1.9.cmml">d</mi><mo id="S3.E2.m1.4.4.1.2f" xref="S3.E2.m1.4.4.1.2.cmml">â¢</mo><mrow id="S3.E2.m1.4.4.1.1.1" xref="S3.E2.m1.4.4.1.1.2.cmml"><mo id="S3.E2.m1.4.4.1.1.1.2" stretchy="false" xref="S3.E2.m1.4.4.1.1.2.cmml">(</mo><mi id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml">C</mi><mo id="S3.E2.m1.4.4.1.1.1.3" xref="S3.E2.m1.4.4.1.1.2.cmml">,</mo><msup id="S3.E2.m1.4.4.1.1.1.1" xref="S3.E2.m1.4.4.1.1.1.1.cmml"><mi id="S3.E2.m1.4.4.1.1.1.1.2" xref="S3.E2.m1.4.4.1.1.1.1.2.cmml">C</mi><mo id="S3.E2.m1.4.4.1.1.1.1.3" xref="S3.E2.m1.4.4.1.1.1.1.3.cmml">â€²</mo></msup><mo id="S3.E2.m1.4.4.1.1.1.4" stretchy="false" xref="S3.E2.m1.4.4.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.4.4.2" xref="S3.E2.m1.4.4.2.cmml">=</mo><mfrac id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml"><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.2.cmml"><mo id="S3.E2.m1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.1.1.1.2.1.cmml">|</mo><mrow id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.2.cmml">C</mi><mo id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml">âˆ©</mo><msup id="S3.E2.m1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.3.2.cmml">C</mi><mo id="S3.E2.m1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.3.3.cmml">â€²</mo></msup></mrow><mo id="S3.E2.m1.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.1.1.1.2.1.cmml">|</mo></mrow><mrow id="S3.E2.m1.2.2.2.1" xref="S3.E2.m1.2.2.2.2.cmml"><mo id="S3.E2.m1.2.2.2.1.2" stretchy="false" xref="S3.E2.m1.2.2.2.2.1.cmml">|</mo><mrow id="S3.E2.m1.2.2.2.1.1" xref="S3.E2.m1.2.2.2.1.1.cmml"><mi id="S3.E2.m1.2.2.2.1.1.2" xref="S3.E2.m1.2.2.2.1.1.2.cmml">C</mi><mo id="S3.E2.m1.2.2.2.1.1.1" xref="S3.E2.m1.2.2.2.1.1.1.cmml">âˆª</mo><msup id="S3.E2.m1.2.2.2.1.1.3" xref="S3.E2.m1.2.2.2.1.1.3.cmml"><mi id="S3.E2.m1.2.2.2.1.1.3.2" xref="S3.E2.m1.2.2.2.1.1.3.2.cmml">C</mi><mo id="S3.E2.m1.2.2.2.1.1.3.3" xref="S3.E2.m1.2.2.2.1.1.3.3.cmml">â€²</mo></msup></mrow><mo id="S3.E2.m1.2.2.2.1.3" stretchy="false" xref="S3.E2.m1.2.2.2.2.1.cmml">|</mo></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.4b"><apply id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4"><eq id="S3.E2.m1.4.4.2.cmml" xref="S3.E2.m1.4.4.2"></eq><apply id="S3.E2.m1.4.4.1.cmml" xref="S3.E2.m1.4.4.1"><times id="S3.E2.m1.4.4.1.2.cmml" xref="S3.E2.m1.4.4.1.2"></times><ci id="S3.E2.m1.4.4.1.3.cmml" xref="S3.E2.m1.4.4.1.3">ğ½</ci><ci id="S3.E2.m1.4.4.1.4.cmml" xref="S3.E2.m1.4.4.1.4">ğ‘</ci><ci id="S3.E2.m1.4.4.1.5.cmml" xref="S3.E2.m1.4.4.1.5">ğ‘</ci><ci id="S3.E2.m1.4.4.1.6.cmml" xref="S3.E2.m1.4.4.1.6">ğ‘</ci><ci id="S3.E2.m1.4.4.1.7.cmml" xref="S3.E2.m1.4.4.1.7">ğ‘</ci><ci id="S3.E2.m1.4.4.1.8.cmml" xref="S3.E2.m1.4.4.1.8">ğ‘Ÿ</ci><ci id="S3.E2.m1.4.4.1.9.cmml" xref="S3.E2.m1.4.4.1.9">ğ‘‘</ci><interval closure="open" id="S3.E2.m1.4.4.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.1"><ci id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3">ğ¶</ci><apply id="S3.E2.m1.4.4.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1">superscript</csymbol><ci id="S3.E2.m1.4.4.1.1.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.2">ğ¶</ci><ci id="S3.E2.m1.4.4.1.1.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.1.1.3">â€²</ci></apply></interval></apply><apply id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2"><divide id="S3.E2.m1.2.2.3.cmml" xref="S3.E2.m1.2.2"></divide><apply id="S3.E2.m1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1"><abs id="S3.E2.m1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2"></abs><apply id="S3.E2.m1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"><intersect id="S3.E2.m1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1"></intersect><ci id="S3.E2.m1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.2">ğ¶</ci><apply id="S3.E2.m1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.3.2">ğ¶</ci><ci id="S3.E2.m1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.3.3">â€²</ci></apply></apply></apply><apply id="S3.E2.m1.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.1"><abs id="S3.E2.m1.2.2.2.2.1.cmml" xref="S3.E2.m1.2.2.2.1.2"></abs><apply id="S3.E2.m1.2.2.2.1.1.cmml" xref="S3.E2.m1.2.2.2.1.1"><union id="S3.E2.m1.2.2.2.1.1.1.cmml" xref="S3.E2.m1.2.2.2.1.1.1"></union><ci id="S3.E2.m1.2.2.2.1.1.2.cmml" xref="S3.E2.m1.2.2.2.1.1.2">ğ¶</ci><apply id="S3.E2.m1.2.2.2.1.1.3.cmml" xref="S3.E2.m1.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.1.1.3.1.cmml" xref="S3.E2.m1.2.2.2.1.1.3">superscript</csymbol><ci id="S3.E2.m1.2.2.2.1.1.3.2.cmml" xref="S3.E2.m1.2.2.2.1.1.3.2">ğ¶</ci><ci id="S3.E2.m1.2.2.2.1.1.3.3.cmml" xref="S3.E2.m1.2.2.2.1.1.3.3">â€²</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.4c">Jaccard(C,C^{\prime})=\frac{|C\cap C^{\prime}|}{|C\cup C^{\prime}|}</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.4d">italic_J italic_a italic_c italic_c italic_a italic_r italic_d ( italic_C , italic_C start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) = divide start_ARG | italic_C âˆ© italic_C start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT | end_ARG start_ARG | italic_C âˆª italic_C start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT | end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.2">Here, <math alttext="|C\cap C^{\prime}|" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.1"><semantics id="S3.SS2.p3.1.m1.1a"><mrow id="S3.SS2.p3.1.m1.1.1.1" xref="S3.SS2.p3.1.m1.1.1.2.cmml"><mo id="S3.SS2.p3.1.m1.1.1.1.2" stretchy="false" xref="S3.SS2.p3.1.m1.1.1.2.1.cmml">|</mo><mrow id="S3.SS2.p3.1.m1.1.1.1.1" xref="S3.SS2.p3.1.m1.1.1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.1.1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.1.1.2.cmml">C</mi><mo id="S3.SS2.p3.1.m1.1.1.1.1.1" xref="S3.SS2.p3.1.m1.1.1.1.1.1.cmml">âˆ©</mo><msup id="S3.SS2.p3.1.m1.1.1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.1.1.3.cmml"><mi id="S3.SS2.p3.1.m1.1.1.1.1.3.2" xref="S3.SS2.p3.1.m1.1.1.1.1.3.2.cmml">C</mi><mo id="S3.SS2.p3.1.m1.1.1.1.1.3.3" xref="S3.SS2.p3.1.m1.1.1.1.1.3.3.cmml">â€²</mo></msup></mrow><mo id="S3.SS2.p3.1.m1.1.1.1.3" stretchy="false" xref="S3.SS2.p3.1.m1.1.1.2.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.1"><abs id="S3.SS2.p3.1.m1.1.1.2.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1.2"></abs><apply id="S3.SS2.p3.1.m1.1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1"><intersect id="S3.SS2.p3.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1.1"></intersect><ci id="S3.SS2.p3.1.m1.1.1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1.2">ğ¶</ci><apply id="S3.SS2.p3.1.m1.1.1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.1.1.3.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1.3">superscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.1.1.3.2.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1.3.2">ğ¶</ci><ci id="S3.SS2.p3.1.m1.1.1.1.1.3.3.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1.3.3">â€²</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">|C\cap C^{\prime}|</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.1d">| italic_C âˆ© italic_C start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT |</annotation></semantics></math> corresponds to the overlap in text chunks by counting how often the two models retrieved the same chunks. Similarly, we can compute the union <math alttext="|C\cup C^{\prime}|" class="ltx_Math" display="inline" id="S3.SS2.p3.2.m2.1"><semantics id="S3.SS2.p3.2.m2.1a"><mrow id="S3.SS2.p3.2.m2.1.1.1" xref="S3.SS2.p3.2.m2.1.1.2.cmml"><mo id="S3.SS2.p3.2.m2.1.1.1.2" stretchy="false" xref="S3.SS2.p3.2.m2.1.1.2.1.cmml">|</mo><mrow id="S3.SS2.p3.2.m2.1.1.1.1" xref="S3.SS2.p3.2.m2.1.1.1.1.cmml"><mi id="S3.SS2.p3.2.m2.1.1.1.1.2" xref="S3.SS2.p3.2.m2.1.1.1.1.2.cmml">C</mi><mo id="S3.SS2.p3.2.m2.1.1.1.1.1" xref="S3.SS2.p3.2.m2.1.1.1.1.1.cmml">âˆª</mo><msup id="S3.SS2.p3.2.m2.1.1.1.1.3" xref="S3.SS2.p3.2.m2.1.1.1.1.3.cmml"><mi id="S3.SS2.p3.2.m2.1.1.1.1.3.2" xref="S3.SS2.p3.2.m2.1.1.1.1.3.2.cmml">C</mi><mo id="S3.SS2.p3.2.m2.1.1.1.1.3.3" xref="S3.SS2.p3.2.m2.1.1.1.1.3.3.cmml">â€²</mo></msup></mrow><mo id="S3.SS2.p3.2.m2.1.1.1.3" stretchy="false" xref="S3.SS2.p3.2.m2.1.1.2.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.1"><abs id="S3.SS2.p3.2.m2.1.1.2.1.cmml" xref="S3.SS2.p3.2.m2.1.1.1.2"></abs><apply id="S3.SS2.p3.2.m2.1.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1.1.1"><union id="S3.SS2.p3.2.m2.1.1.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1.1.1.1"></union><ci id="S3.SS2.p3.2.m2.1.1.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.1.1.2">ğ¶</ci><apply id="S3.SS2.p3.2.m2.1.1.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.1.1.3.1.cmml" xref="S3.SS2.p3.2.m2.1.1.1.1.3">superscript</csymbol><ci id="S3.SS2.p3.2.m2.1.1.1.1.3.2.cmml" xref="S3.SS2.p3.2.m2.1.1.1.1.3.2">ğ¶</ci><ci id="S3.SS2.p3.2.m2.1.1.1.1.3.3.cmml" xref="S3.SS2.p3.2.m2.1.1.1.1.3.3">â€²</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">|C\cup C^{\prime}|</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.2.m2.1d">| italic_C âˆª italic_C start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT |</annotation></semantics></math>, which corresponds to all retrieved text chunks, counting chunks present in both sets only once. The resulting score is bounded in the interval [0, 1] with 1 indicating that both models retrieved the same set of text chunks.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.3">While Jaccard similarity computes the percentage to which two sets overlap, it ignores the order in the sets. Rank similarity <cite class="ltx_cite ltx_citemacro_citep">(Wang
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib37" title="">2022a</a>)</cite>, on the other hand, considers the order of common elements, with closer elements having a higher impact on the score. The measure assigns ranks to common text chunks according to their similarity to the query, i.e. <math alttext="r_{C}(j)=n" class="ltx_Math" display="inline" id="S3.SS2.p4.1.m1.1"><semantics id="S3.SS2.p4.1.m1.1a"><mrow id="S3.SS2.p4.1.m1.1.2" xref="S3.SS2.p4.1.m1.1.2.cmml"><mrow id="S3.SS2.p4.1.m1.1.2.2" xref="S3.SS2.p4.1.m1.1.2.2.cmml"><msub id="S3.SS2.p4.1.m1.1.2.2.2" xref="S3.SS2.p4.1.m1.1.2.2.2.cmml"><mi id="S3.SS2.p4.1.m1.1.2.2.2.2" xref="S3.SS2.p4.1.m1.1.2.2.2.2.cmml">r</mi><mi id="S3.SS2.p4.1.m1.1.2.2.2.3" xref="S3.SS2.p4.1.m1.1.2.2.2.3.cmml">C</mi></msub><mo id="S3.SS2.p4.1.m1.1.2.2.1" xref="S3.SS2.p4.1.m1.1.2.2.1.cmml">â¢</mo><mrow id="S3.SS2.p4.1.m1.1.2.2.3.2" xref="S3.SS2.p4.1.m1.1.2.2.cmml"><mo id="S3.SS2.p4.1.m1.1.2.2.3.2.1" stretchy="false" xref="S3.SS2.p4.1.m1.1.2.2.cmml">(</mo><mi id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml">j</mi><mo id="S3.SS2.p4.1.m1.1.2.2.3.2.2" stretchy="false" xref="S3.SS2.p4.1.m1.1.2.2.cmml">)</mo></mrow></mrow><mo id="S3.SS2.p4.1.m1.1.2.1" xref="S3.SS2.p4.1.m1.1.2.1.cmml">=</mo><mi id="S3.SS2.p4.1.m1.1.2.3" xref="S3.SS2.p4.1.m1.1.2.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><apply id="S3.SS2.p4.1.m1.1.2.cmml" xref="S3.SS2.p4.1.m1.1.2"><eq id="S3.SS2.p4.1.m1.1.2.1.cmml" xref="S3.SS2.p4.1.m1.1.2.1"></eq><apply id="S3.SS2.p4.1.m1.1.2.2.cmml" xref="S3.SS2.p4.1.m1.1.2.2"><times id="S3.SS2.p4.1.m1.1.2.2.1.cmml" xref="S3.SS2.p4.1.m1.1.2.2.1"></times><apply id="S3.SS2.p4.1.m1.1.2.2.2.cmml" xref="S3.SS2.p4.1.m1.1.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p4.1.m1.1.2.2.2.1.cmml" xref="S3.SS2.p4.1.m1.1.2.2.2">subscript</csymbol><ci id="S3.SS2.p4.1.m1.1.2.2.2.2.cmml" xref="S3.SS2.p4.1.m1.1.2.2.2.2">ğ‘Ÿ</ci><ci id="S3.SS2.p4.1.m1.1.2.2.2.3.cmml" xref="S3.SS2.p4.1.m1.1.2.2.2.3">ğ¶</ci></apply><ci id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">ğ‘—</ci></apply><ci id="S3.SS2.p4.1.m1.1.2.3.cmml" xref="S3.SS2.p4.1.m1.1.2.3">ğ‘›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">r_{C}(j)=n</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.1.m1.1d">italic_r start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT ( italic_j ) = italic_n</annotation></semantics></math> if chunk <math alttext="j" class="ltx_Math" display="inline" id="S3.SS2.p4.2.m2.1"><semantics id="S3.SS2.p4.2.m2.1a"><mi id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b"><ci id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1">ğ‘—</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">j</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.2.m2.1d">italic_j</annotation></semantics></math> was the top-<math alttext="n" class="ltx_Math" display="inline" id="S3.SS2.p4.3.m3.1"><semantics id="S3.SS2.p4.3.m3.1a"><mi id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.1b"><ci id="S3.SS2.p4.3.m3.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.3.m3.1d">italic_n</annotation></semantics></math> retrieved result for the query. Ranks are then compared using:</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(3)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Rank(r_{C}(j),r_{C^{\prime}}(j))=\frac{2}{(1+|r_{C}(j)-r_{C^{\prime}}(j)|)(r_{%
C}(j)+r_{C^{\prime}}(j))}" class="ltx_Math" display="block" id="S3.E3.m1.10"><semantics id="S3.E3.m1.10a"><mrow id="S3.E3.m1.10.10" xref="S3.E3.m1.10.10.cmml"><mrow id="S3.E3.m1.10.10.2" xref="S3.E3.m1.10.10.2.cmml"><mi id="S3.E3.m1.10.10.2.4" xref="S3.E3.m1.10.10.2.4.cmml">R</mi><mo id="S3.E3.m1.10.10.2.3" xref="S3.E3.m1.10.10.2.3.cmml">â¢</mo><mi id="S3.E3.m1.10.10.2.5" xref="S3.E3.m1.10.10.2.5.cmml">a</mi><mo id="S3.E3.m1.10.10.2.3a" xref="S3.E3.m1.10.10.2.3.cmml">â¢</mo><mi id="S3.E3.m1.10.10.2.6" xref="S3.E3.m1.10.10.2.6.cmml">n</mi><mo id="S3.E3.m1.10.10.2.3b" xref="S3.E3.m1.10.10.2.3.cmml">â¢</mo><mi id="S3.E3.m1.10.10.2.7" xref="S3.E3.m1.10.10.2.7.cmml">k</mi><mo id="S3.E3.m1.10.10.2.3c" xref="S3.E3.m1.10.10.2.3.cmml">â¢</mo><mrow id="S3.E3.m1.10.10.2.2.2" xref="S3.E3.m1.10.10.2.2.3.cmml"><mo id="S3.E3.m1.10.10.2.2.2.3" stretchy="false" xref="S3.E3.m1.10.10.2.2.3.cmml">(</mo><mrow id="S3.E3.m1.9.9.1.1.1.1" xref="S3.E3.m1.9.9.1.1.1.1.cmml"><msub id="S3.E3.m1.9.9.1.1.1.1.2" xref="S3.E3.m1.9.9.1.1.1.1.2.cmml"><mi id="S3.E3.m1.9.9.1.1.1.1.2.2" xref="S3.E3.m1.9.9.1.1.1.1.2.2.cmml">r</mi><mi id="S3.E3.m1.9.9.1.1.1.1.2.3" xref="S3.E3.m1.9.9.1.1.1.1.2.3.cmml">C</mi></msub><mo id="S3.E3.m1.9.9.1.1.1.1.1" xref="S3.E3.m1.9.9.1.1.1.1.1.cmml">â¢</mo><mrow id="S3.E3.m1.9.9.1.1.1.1.3.2" xref="S3.E3.m1.9.9.1.1.1.1.cmml"><mo id="S3.E3.m1.9.9.1.1.1.1.3.2.1" stretchy="false" xref="S3.E3.m1.9.9.1.1.1.1.cmml">(</mo><mi id="S3.E3.m1.7.7" xref="S3.E3.m1.7.7.cmml">j</mi><mo id="S3.E3.m1.9.9.1.1.1.1.3.2.2" stretchy="false" xref="S3.E3.m1.9.9.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.10.10.2.2.2.4" xref="S3.E3.m1.10.10.2.2.3.cmml">,</mo><mrow id="S3.E3.m1.10.10.2.2.2.2" xref="S3.E3.m1.10.10.2.2.2.2.cmml"><msub id="S3.E3.m1.10.10.2.2.2.2.2" xref="S3.E3.m1.10.10.2.2.2.2.2.cmml"><mi id="S3.E3.m1.10.10.2.2.2.2.2.2" xref="S3.E3.m1.10.10.2.2.2.2.2.2.cmml">r</mi><msup id="S3.E3.m1.10.10.2.2.2.2.2.3" xref="S3.E3.m1.10.10.2.2.2.2.2.3.cmml"><mi id="S3.E3.m1.10.10.2.2.2.2.2.3.2" xref="S3.E3.m1.10.10.2.2.2.2.2.3.2.cmml">C</mi><mo id="S3.E3.m1.10.10.2.2.2.2.2.3.3" xref="S3.E3.m1.10.10.2.2.2.2.2.3.3.cmml">â€²</mo></msup></msub><mo id="S3.E3.m1.10.10.2.2.2.2.1" xref="S3.E3.m1.10.10.2.2.2.2.1.cmml">â¢</mo><mrow id="S3.E3.m1.10.10.2.2.2.2.3.2" xref="S3.E3.m1.10.10.2.2.2.2.cmml"><mo id="S3.E3.m1.10.10.2.2.2.2.3.2.1" stretchy="false" xref="S3.E3.m1.10.10.2.2.2.2.cmml">(</mo><mi id="S3.E3.m1.8.8" xref="S3.E3.m1.8.8.cmml">j</mi><mo id="S3.E3.m1.10.10.2.2.2.2.3.2.2" stretchy="false" xref="S3.E3.m1.10.10.2.2.2.2.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.10.10.2.2.2.5" stretchy="false" xref="S3.E3.m1.10.10.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.10.10.3" xref="S3.E3.m1.10.10.3.cmml">=</mo><mfrac id="S3.E3.m1.6.6" xref="S3.E3.m1.6.6.cmml"><mn id="S3.E3.m1.6.6.8" xref="S3.E3.m1.6.6.8.cmml">2</mn><mrow id="S3.E3.m1.6.6.6" xref="S3.E3.m1.6.6.6.cmml"><mrow id="S3.E3.m1.5.5.5.5.1" xref="S3.E3.m1.5.5.5.5.1.1.cmml"><mo id="S3.E3.m1.5.5.5.5.1.2" stretchy="false" xref="S3.E3.m1.5.5.5.5.1.1.cmml">(</mo><mrow id="S3.E3.m1.5.5.5.5.1.1" xref="S3.E3.m1.5.5.5.5.1.1.cmml"><mn id="S3.E3.m1.5.5.5.5.1.1.3" xref="S3.E3.m1.5.5.5.5.1.1.3.cmml">1</mn><mo id="S3.E3.m1.5.5.5.5.1.1.2" xref="S3.E3.m1.5.5.5.5.1.1.2.cmml">+</mo><mrow id="S3.E3.m1.5.5.5.5.1.1.1.1" xref="S3.E3.m1.5.5.5.5.1.1.1.2.cmml"><mo id="S3.E3.m1.5.5.5.5.1.1.1.1.2" stretchy="false" xref="S3.E3.m1.5.5.5.5.1.1.1.2.1.cmml">|</mo><mrow id="S3.E3.m1.5.5.5.5.1.1.1.1.1" xref="S3.E3.m1.5.5.5.5.1.1.1.1.1.cmml"><mrow id="S3.E3.m1.5.5.5.5.1.1.1.1.1.2" xref="S3.E3.m1.5.5.5.5.1.1.1.1.1.2.cmml"><msub id="S3.E3.m1.5.5.5.5.1.1.1.1.1.2.2" xref="S3.E3.m1.5.5.5.5.1.1.1.1.1.2.2.cmml"><mi id="S3.E3.m1.5.5.5.5.1.1.1.1.1.2.2.2" xref="S3.E3.m1.5.5.5.5.1.1.1.1.1.2.2.2.cmml">r</mi><mi id="S3.E3.m1.5.5.5.5.1.1.1.1.1.2.2.3" xref="S3.E3.m1.5.5.5.5.1.1.1.1.1.2.2.3.cmml">C</mi></msub><mo id="S3.E3.m1.5.5.5.5.1.1.1.1.1.2.1" xref="S3.E3.m1.5.5.5.5.1.1.1.1.1.2.1.cmml">â¢</mo><mrow id="S3.E3.m1.5.5.5.5.1.1.1.1.1.2.3.2" xref="S3.E3.m1.5.5.5.5.1.1.1.1.1.2.cmml"><mo id="S3.E3.m1.5.5.5.5.1.1.1.1.1.2.3.2.1" stretchy="false" xref="S3.E3.m1.5.5.5.5.1.1.1.1.1.2.cmml">(</mo><mi id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml">j</mi><mo id="S3.E3.m1.5.5.5.5.1.1.1.1.1.2.3.2.2" stretchy="false" xref="S3.E3.m1.5.5.5.5.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.5.5.5.5.1.1.1.1.1.1" xref="S3.E3.m1.5.5.5.5.1.1.1.1.1.1.cmml">âˆ’</mo><mrow id="S3.E3.m1.5.5.5.5.1.1.1.1.1.3" xref="S3.E3.m1.5.5.5.5.1.1.1.1.1.3.cmml"><msub id="S3.E3.m1.5.5.5.5.1.1.1.1.1.3.2" xref="S3.E3.m1.5.5.5.5.1.1.1.1.1.3.2.cmml"><mi id="S3.E3.m1.5.5.5.5.1.1.1.1.1.3.2.2" xref="S3.E3.m1.5.5.5.5.1.1.1.1.1.3.2.2.cmml">r</mi><msup id="S3.E3.m1.5.5.5.5.1.1.1.1.1.3.2.3" xref="S3.E3.m1.5.5.5.5.1.1.1.1.1.3.2.3.cmml"><mi id="S3.E3.m1.5.5.5.5.1.1.1.1.1.3.2.3.2" xref="S3.E3.m1.5.5.5.5.1.1.1.1.1.3.2.3.2.cmml">C</mi><mo id="S3.E3.m1.5.5.5.5.1.1.1.1.1.3.2.3.3" xref="S3.E3.m1.5.5.5.5.1.1.1.1.1.3.2.3.3.cmml">â€²</mo></msup></msub><mo id="S3.E3.m1.5.5.5.5.1.1.1.1.1.3.1" xref="S3.E3.m1.5.5.5.5.1.1.1.1.1.3.1.cmml">â¢</mo><mrow id="S3.E3.m1.5.5.5.5.1.1.1.1.1.3.3.2" xref="S3.E3.m1.5.5.5.5.1.1.1.1.1.3.cmml"><mo id="S3.E3.m1.5.5.5.5.1.1.1.1.1.3.3.2.1" stretchy="false" xref="S3.E3.m1.5.5.5.5.1.1.1.1.1.3.cmml">(</mo><mi id="S3.E3.m1.2.2.2.2" xref="S3.E3.m1.2.2.2.2.cmml">j</mi><mo id="S3.E3.m1.5.5.5.5.1.1.1.1.1.3.3.2.2" stretchy="false" xref="S3.E3.m1.5.5.5.5.1.1.1.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E3.m1.5.5.5.5.1.1.1.1.3" stretchy="false" xref="S3.E3.m1.5.5.5.5.1.1.1.2.1.cmml">|</mo></mrow></mrow><mo id="S3.E3.m1.5.5.5.5.1.3" stretchy="false" xref="S3.E3.m1.5.5.5.5.1.1.cmml">)</mo></mrow><mo id="S3.E3.m1.6.6.6.7" xref="S3.E3.m1.6.6.6.7.cmml">â¢</mo><mrow id="S3.E3.m1.6.6.6.6.1" xref="S3.E3.m1.6.6.6.6.1.1.cmml"><mo id="S3.E3.m1.6.6.6.6.1.2" stretchy="false" xref="S3.E3.m1.6.6.6.6.1.1.cmml">(</mo><mrow id="S3.E3.m1.6.6.6.6.1.1" xref="S3.E3.m1.6.6.6.6.1.1.cmml"><mrow id="S3.E3.m1.6.6.6.6.1.1.2" xref="S3.E3.m1.6.6.6.6.1.1.2.cmml"><msub id="S3.E3.m1.6.6.6.6.1.1.2.2" xref="S3.E3.m1.6.6.6.6.1.1.2.2.cmml"><mi id="S3.E3.m1.6.6.6.6.1.1.2.2.2" xref="S3.E3.m1.6.6.6.6.1.1.2.2.2.cmml">r</mi><mi id="S3.E3.m1.6.6.6.6.1.1.2.2.3" xref="S3.E3.m1.6.6.6.6.1.1.2.2.3.cmml">C</mi></msub><mo id="S3.E3.m1.6.6.6.6.1.1.2.1" xref="S3.E3.m1.6.6.6.6.1.1.2.1.cmml">â¢</mo><mrow id="S3.E3.m1.6.6.6.6.1.1.2.3.2" xref="S3.E3.m1.6.6.6.6.1.1.2.cmml"><mo id="S3.E3.m1.6.6.6.6.1.1.2.3.2.1" stretchy="false" xref="S3.E3.m1.6.6.6.6.1.1.2.cmml">(</mo><mi id="S3.E3.m1.3.3.3.3" xref="S3.E3.m1.3.3.3.3.cmml">j</mi><mo id="S3.E3.m1.6.6.6.6.1.1.2.3.2.2" stretchy="false" xref="S3.E3.m1.6.6.6.6.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.6.6.6.6.1.1.1" xref="S3.E3.m1.6.6.6.6.1.1.1.cmml">+</mo><mrow id="S3.E3.m1.6.6.6.6.1.1.3" xref="S3.E3.m1.6.6.6.6.1.1.3.cmml"><msub id="S3.E3.m1.6.6.6.6.1.1.3.2" xref="S3.E3.m1.6.6.6.6.1.1.3.2.cmml"><mi id="S3.E3.m1.6.6.6.6.1.1.3.2.2" xref="S3.E3.m1.6.6.6.6.1.1.3.2.2.cmml">r</mi><msup id="S3.E3.m1.6.6.6.6.1.1.3.2.3" xref="S3.E3.m1.6.6.6.6.1.1.3.2.3.cmml"><mi id="S3.E3.m1.6.6.6.6.1.1.3.2.3.2" xref="S3.E3.m1.6.6.6.6.1.1.3.2.3.2.cmml">C</mi><mo id="S3.E3.m1.6.6.6.6.1.1.3.2.3.3" xref="S3.E3.m1.6.6.6.6.1.1.3.2.3.3.cmml">â€²</mo></msup></msub><mo id="S3.E3.m1.6.6.6.6.1.1.3.1" xref="S3.E3.m1.6.6.6.6.1.1.3.1.cmml">â¢</mo><mrow id="S3.E3.m1.6.6.6.6.1.1.3.3.2" xref="S3.E3.m1.6.6.6.6.1.1.3.cmml"><mo id="S3.E3.m1.6.6.6.6.1.1.3.3.2.1" stretchy="false" xref="S3.E3.m1.6.6.6.6.1.1.3.cmml">(</mo><mi id="S3.E3.m1.4.4.4.4" xref="S3.E3.m1.4.4.4.4.cmml">j</mi><mo id="S3.E3.m1.6.6.6.6.1.1.3.3.2.2" stretchy="false" xref="S3.E3.m1.6.6.6.6.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E3.m1.6.6.6.6.1.3" stretchy="false" xref="S3.E3.m1.6.6.6.6.1.1.cmml">)</mo></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.10b"><apply id="S3.E3.m1.10.10.cmml" xref="S3.E3.m1.10.10"><eq id="S3.E3.m1.10.10.3.cmml" xref="S3.E3.m1.10.10.3"></eq><apply id="S3.E3.m1.10.10.2.cmml" xref="S3.E3.m1.10.10.2"><times id="S3.E3.m1.10.10.2.3.cmml" xref="S3.E3.m1.10.10.2.3"></times><ci id="S3.E3.m1.10.10.2.4.cmml" xref="S3.E3.m1.10.10.2.4">ğ‘…</ci><ci id="S3.E3.m1.10.10.2.5.cmml" xref="S3.E3.m1.10.10.2.5">ğ‘</ci><ci id="S3.E3.m1.10.10.2.6.cmml" xref="S3.E3.m1.10.10.2.6">ğ‘›</ci><ci id="S3.E3.m1.10.10.2.7.cmml" xref="S3.E3.m1.10.10.2.7">ğ‘˜</ci><interval closure="open" id="S3.E3.m1.10.10.2.2.3.cmml" xref="S3.E3.m1.10.10.2.2.2"><apply id="S3.E3.m1.9.9.1.1.1.1.cmml" xref="S3.E3.m1.9.9.1.1.1.1"><times id="S3.E3.m1.9.9.1.1.1.1.1.cmml" xref="S3.E3.m1.9.9.1.1.1.1.1"></times><apply id="S3.E3.m1.9.9.1.1.1.1.2.cmml" xref="S3.E3.m1.9.9.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.9.9.1.1.1.1.2.1.cmml" xref="S3.E3.m1.9.9.1.1.1.1.2">subscript</csymbol><ci id="S3.E3.m1.9.9.1.1.1.1.2.2.cmml" xref="S3.E3.m1.9.9.1.1.1.1.2.2">ğ‘Ÿ</ci><ci id="S3.E3.m1.9.9.1.1.1.1.2.3.cmml" xref="S3.E3.m1.9.9.1.1.1.1.2.3">ğ¶</ci></apply><ci id="S3.E3.m1.7.7.cmml" xref="S3.E3.m1.7.7">ğ‘—</ci></apply><apply id="S3.E3.m1.10.10.2.2.2.2.cmml" xref="S3.E3.m1.10.10.2.2.2.2"><times id="S3.E3.m1.10.10.2.2.2.2.1.cmml" xref="S3.E3.m1.10.10.2.2.2.2.1"></times><apply id="S3.E3.m1.10.10.2.2.2.2.2.cmml" xref="S3.E3.m1.10.10.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.10.10.2.2.2.2.2.1.cmml" xref="S3.E3.m1.10.10.2.2.2.2.2">subscript</csymbol><ci id="S3.E3.m1.10.10.2.2.2.2.2.2.cmml" xref="S3.E3.m1.10.10.2.2.2.2.2.2">ğ‘Ÿ</ci><apply id="S3.E3.m1.10.10.2.2.2.2.2.3.cmml" xref="S3.E3.m1.10.10.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.E3.m1.10.10.2.2.2.2.2.3.1.cmml" xref="S3.E3.m1.10.10.2.2.2.2.2.3">superscript</csymbol><ci id="S3.E3.m1.10.10.2.2.2.2.2.3.2.cmml" xref="S3.E3.m1.10.10.2.2.2.2.2.3.2">ğ¶</ci><ci id="S3.E3.m1.10.10.2.2.2.2.2.3.3.cmml" xref="S3.E3.m1.10.10.2.2.2.2.2.3.3">â€²</ci></apply></apply><ci id="S3.E3.m1.8.8.cmml" xref="S3.E3.m1.8.8">ğ‘—</ci></apply></interval></apply><apply id="S3.E3.m1.6.6.cmml" xref="S3.E3.m1.6.6"><divide id="S3.E3.m1.6.6.7.cmml" xref="S3.E3.m1.6.6"></divide><cn id="S3.E3.m1.6.6.8.cmml" type="integer" xref="S3.E3.m1.6.6.8">2</cn><apply id="S3.E3.m1.6.6.6.cmml" xref="S3.E3.m1.6.6.6"><times id="S3.E3.m1.6.6.6.7.cmml" xref="S3.E3.m1.6.6.6.7"></times><apply id="S3.E3.m1.5.5.5.5.1.1.cmml" xref="S3.E3.m1.5.5.5.5.1"><plus id="S3.E3.m1.5.5.5.5.1.1.2.cmml" xref="S3.E3.m1.5.5.5.5.1.1.2"></plus><cn id="S3.E3.m1.5.5.5.5.1.1.3.cmml" type="integer" xref="S3.E3.m1.5.5.5.5.1.1.3">1</cn><apply id="S3.E3.m1.5.5.5.5.1.1.1.2.cmml" xref="S3.E3.m1.5.5.5.5.1.1.1.1"><abs id="S3.E3.m1.5.5.5.5.1.1.1.2.1.cmml" xref="S3.E3.m1.5.5.5.5.1.1.1.1.2"></abs><apply id="S3.E3.m1.5.5.5.5.1.1.1.1.1.cmml" xref="S3.E3.m1.5.5.5.5.1.1.1.1.1"><minus id="S3.E3.m1.5.5.5.5.1.1.1.1.1.1.cmml" xref="S3.E3.m1.5.5.5.5.1.1.1.1.1.1"></minus><apply id="S3.E3.m1.5.5.5.5.1.1.1.1.1.2.cmml" xref="S3.E3.m1.5.5.5.5.1.1.1.1.1.2"><times id="S3.E3.m1.5.5.5.5.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.5.5.5.5.1.1.1.1.1.2.1"></times><apply id="S3.E3.m1.5.5.5.5.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.5.5.5.5.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.5.5.1.1.1.1.1.2.2.1.cmml" xref="S3.E3.m1.5.5.5.5.1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.E3.m1.5.5.5.5.1.1.1.1.1.2.2.2.cmml" xref="S3.E3.m1.5.5.5.5.1.1.1.1.1.2.2.2">ğ‘Ÿ</ci><ci id="S3.E3.m1.5.5.5.5.1.1.1.1.1.2.2.3.cmml" xref="S3.E3.m1.5.5.5.5.1.1.1.1.1.2.2.3">ğ¶</ci></apply><ci id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1">ğ‘—</ci></apply><apply id="S3.E3.m1.5.5.5.5.1.1.1.1.1.3.cmml" xref="S3.E3.m1.5.5.5.5.1.1.1.1.1.3"><times id="S3.E3.m1.5.5.5.5.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.5.5.5.5.1.1.1.1.1.3.1"></times><apply id="S3.E3.m1.5.5.5.5.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.5.5.5.5.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.5.5.1.1.1.1.1.3.2.1.cmml" xref="S3.E3.m1.5.5.5.5.1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E3.m1.5.5.5.5.1.1.1.1.1.3.2.2.cmml" xref="S3.E3.m1.5.5.5.5.1.1.1.1.1.3.2.2">ğ‘Ÿ</ci><apply id="S3.E3.m1.5.5.5.5.1.1.1.1.1.3.2.3.cmml" xref="S3.E3.m1.5.5.5.5.1.1.1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.5.5.1.1.1.1.1.3.2.3.1.cmml" xref="S3.E3.m1.5.5.5.5.1.1.1.1.1.3.2.3">superscript</csymbol><ci id="S3.E3.m1.5.5.5.5.1.1.1.1.1.3.2.3.2.cmml" xref="S3.E3.m1.5.5.5.5.1.1.1.1.1.3.2.3.2">ğ¶</ci><ci id="S3.E3.m1.5.5.5.5.1.1.1.1.1.3.2.3.3.cmml" xref="S3.E3.m1.5.5.5.5.1.1.1.1.1.3.2.3.3">â€²</ci></apply></apply><ci id="S3.E3.m1.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2">ğ‘—</ci></apply></apply></apply></apply><apply id="S3.E3.m1.6.6.6.6.1.1.cmml" xref="S3.E3.m1.6.6.6.6.1"><plus id="S3.E3.m1.6.6.6.6.1.1.1.cmml" xref="S3.E3.m1.6.6.6.6.1.1.1"></plus><apply id="S3.E3.m1.6.6.6.6.1.1.2.cmml" xref="S3.E3.m1.6.6.6.6.1.1.2"><times id="S3.E3.m1.6.6.6.6.1.1.2.1.cmml" xref="S3.E3.m1.6.6.6.6.1.1.2.1"></times><apply id="S3.E3.m1.6.6.6.6.1.1.2.2.cmml" xref="S3.E3.m1.6.6.6.6.1.1.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.6.6.6.6.1.1.2.2.1.cmml" xref="S3.E3.m1.6.6.6.6.1.1.2.2">subscript</csymbol><ci id="S3.E3.m1.6.6.6.6.1.1.2.2.2.cmml" xref="S3.E3.m1.6.6.6.6.1.1.2.2.2">ğ‘Ÿ</ci><ci id="S3.E3.m1.6.6.6.6.1.1.2.2.3.cmml" xref="S3.E3.m1.6.6.6.6.1.1.2.2.3">ğ¶</ci></apply><ci id="S3.E3.m1.3.3.3.3.cmml" xref="S3.E3.m1.3.3.3.3">ğ‘—</ci></apply><apply id="S3.E3.m1.6.6.6.6.1.1.3.cmml" xref="S3.E3.m1.6.6.6.6.1.1.3"><times id="S3.E3.m1.6.6.6.6.1.1.3.1.cmml" xref="S3.E3.m1.6.6.6.6.1.1.3.1"></times><apply id="S3.E3.m1.6.6.6.6.1.1.3.2.cmml" xref="S3.E3.m1.6.6.6.6.1.1.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.6.6.6.6.1.1.3.2.1.cmml" xref="S3.E3.m1.6.6.6.6.1.1.3.2">subscript</csymbol><ci id="S3.E3.m1.6.6.6.6.1.1.3.2.2.cmml" xref="S3.E3.m1.6.6.6.6.1.1.3.2.2">ğ‘Ÿ</ci><apply id="S3.E3.m1.6.6.6.6.1.1.3.2.3.cmml" xref="S3.E3.m1.6.6.6.6.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E3.m1.6.6.6.6.1.1.3.2.3.1.cmml" xref="S3.E3.m1.6.6.6.6.1.1.3.2.3">superscript</csymbol><ci id="S3.E3.m1.6.6.6.6.1.1.3.2.3.2.cmml" xref="S3.E3.m1.6.6.6.6.1.1.3.2.3.2">ğ¶</ci><ci id="S3.E3.m1.6.6.6.6.1.1.3.2.3.3.cmml" xref="S3.E3.m1.6.6.6.6.1.1.3.2.3.3">â€²</ci></apply></apply><ci id="S3.E3.m1.4.4.4.4.cmml" xref="S3.E3.m1.4.4.4.4">ğ‘—</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.10c">Rank(r_{C}(j),r_{C^{\prime}}(j))=\frac{2}{(1+|r_{C}(j)-r_{C^{\prime}}(j)|)(r_{%
C}(j)+r_{C^{\prime}}(j))}</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.10d">italic_R italic_a italic_n italic_k ( italic_r start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT ( italic_j ) , italic_r start_POSTSUBSCRIPT italic_C start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_j ) ) = divide start_ARG 2 end_ARG start_ARG ( 1 + | italic_r start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT ( italic_j ) - italic_r start_POSTSUBSCRIPT italic_C start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_j ) | ) ( italic_r start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT ( italic_j ) + italic_r start_POSTSUBSCRIPT italic_C start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_j ) ) end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS2.p6">
<p class="ltx_p" id="S3.SS2.p6.1">With this, rank similarity for two sets of retrieved text chunks C, Câ€™ is calculated as:</p>
</div>
<div class="ltx_para" id="S3.SS2.p7">
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(4)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="RankSim(C,C^{\prime})=\frac{1}{H(|C\cap C^{\prime}|)}\sum_{j\in|C\cap C^{%
\prime}|}Rank(r_{C}(j),r_{C^{\prime}}(j))" class="ltx_Math" display="block" id="S3.E4.m1.8"><semantics id="S3.E4.m1.8a"><mrow id="S3.E4.m1.8.8" xref="S3.E4.m1.8.8.cmml"><mrow id="S3.E4.m1.6.6.1" xref="S3.E4.m1.6.6.1.cmml"><mi id="S3.E4.m1.6.6.1.3" xref="S3.E4.m1.6.6.1.3.cmml">R</mi><mo id="S3.E4.m1.6.6.1.2" xref="S3.E4.m1.6.6.1.2.cmml">â¢</mo><mi id="S3.E4.m1.6.6.1.4" xref="S3.E4.m1.6.6.1.4.cmml">a</mi><mo id="S3.E4.m1.6.6.1.2a" xref="S3.E4.m1.6.6.1.2.cmml">â¢</mo><mi id="S3.E4.m1.6.6.1.5" xref="S3.E4.m1.6.6.1.5.cmml">n</mi><mo id="S3.E4.m1.6.6.1.2b" xref="S3.E4.m1.6.6.1.2.cmml">â¢</mo><mi id="S3.E4.m1.6.6.1.6" xref="S3.E4.m1.6.6.1.6.cmml">k</mi><mo id="S3.E4.m1.6.6.1.2c" xref="S3.E4.m1.6.6.1.2.cmml">â¢</mo><mi id="S3.E4.m1.6.6.1.7" xref="S3.E4.m1.6.6.1.7.cmml">S</mi><mo id="S3.E4.m1.6.6.1.2d" xref="S3.E4.m1.6.6.1.2.cmml">â¢</mo><mi id="S3.E4.m1.6.6.1.8" xref="S3.E4.m1.6.6.1.8.cmml">i</mi><mo id="S3.E4.m1.6.6.1.2e" xref="S3.E4.m1.6.6.1.2.cmml">â¢</mo><mi id="S3.E4.m1.6.6.1.9" xref="S3.E4.m1.6.6.1.9.cmml">m</mi><mo id="S3.E4.m1.6.6.1.2f" xref="S3.E4.m1.6.6.1.2.cmml">â¢</mo><mrow id="S3.E4.m1.6.6.1.1.1" xref="S3.E4.m1.6.6.1.1.2.cmml"><mo id="S3.E4.m1.6.6.1.1.1.2" stretchy="false" xref="S3.E4.m1.6.6.1.1.2.cmml">(</mo><mi id="S3.E4.m1.3.3" xref="S3.E4.m1.3.3.cmml">C</mi><mo id="S3.E4.m1.6.6.1.1.1.3" xref="S3.E4.m1.6.6.1.1.2.cmml">,</mo><msup id="S3.E4.m1.6.6.1.1.1.1" xref="S3.E4.m1.6.6.1.1.1.1.cmml"><mi id="S3.E4.m1.6.6.1.1.1.1.2" xref="S3.E4.m1.6.6.1.1.1.1.2.cmml">C</mi><mo id="S3.E4.m1.6.6.1.1.1.1.3" xref="S3.E4.m1.6.6.1.1.1.1.3.cmml">â€²</mo></msup><mo id="S3.E4.m1.6.6.1.1.1.4" stretchy="false" xref="S3.E4.m1.6.6.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E4.m1.8.8.4" xref="S3.E4.m1.8.8.4.cmml">=</mo><mrow id="S3.E4.m1.8.8.3" xref="S3.E4.m1.8.8.3.cmml"><mfrac id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml"><mn id="S3.E4.m1.1.1.3" xref="S3.E4.m1.1.1.3.cmml">1</mn><mrow id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.3" xref="S3.E4.m1.1.1.1.3.cmml">H</mi><mo id="S3.E4.m1.1.1.1.2" xref="S3.E4.m1.1.1.1.2.cmml">â¢</mo><mrow id="S3.E4.m1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.cmml"><mo id="S3.E4.m1.1.1.1.1.1.2" stretchy="false" xref="S3.E4.m1.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.2.cmml"><mo id="S3.E4.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E4.m1.1.1.1.1.1.1.2.1.cmml">|</mo><mrow id="S3.E4.m1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2.cmml">C</mi><mo id="S3.E4.m1.1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml">âˆ©</mo><msup id="S3.E4.m1.1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.3.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.2.cmml">C</mi><mo id="S3.E4.m1.1.1.1.1.1.1.1.1.3.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.3.cmml">â€²</mo></msup></mrow><mo id="S3.E4.m1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E4.m1.1.1.1.1.1.1.2.1.cmml">|</mo></mrow><mo id="S3.E4.m1.1.1.1.1.1.3" stretchy="false" xref="S3.E4.m1.1.1.1.cmml">)</mo></mrow></mrow></mfrac><mo id="S3.E4.m1.8.8.3.3" xref="S3.E4.m1.8.8.3.3.cmml">â¢</mo><mrow id="S3.E4.m1.8.8.3.2" xref="S3.E4.m1.8.8.3.2.cmml"><munder id="S3.E4.m1.8.8.3.2.3" xref="S3.E4.m1.8.8.3.2.3.cmml"><mo id="S3.E4.m1.8.8.3.2.3.2" movablelimits="false" xref="S3.E4.m1.8.8.3.2.3.2.cmml">âˆ‘</mo><mrow id="S3.E4.m1.2.2.1" xref="S3.E4.m1.2.2.1.cmml"><mi id="S3.E4.m1.2.2.1.3" xref="S3.E4.m1.2.2.1.3.cmml">j</mi><mo id="S3.E4.m1.2.2.1.2" xref="S3.E4.m1.2.2.1.2.cmml">âˆˆ</mo><mrow id="S3.E4.m1.2.2.1.1.1" xref="S3.E4.m1.2.2.1.1.2.cmml"><mo id="S3.E4.m1.2.2.1.1.1.2" stretchy="false" xref="S3.E4.m1.2.2.1.1.2.1.cmml">|</mo><mrow id="S3.E4.m1.2.2.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.cmml"><mi id="S3.E4.m1.2.2.1.1.1.1.2" xref="S3.E4.m1.2.2.1.1.1.1.2.cmml">C</mi><mo id="S3.E4.m1.2.2.1.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.1.cmml">âˆ©</mo><msup id="S3.E4.m1.2.2.1.1.1.1.3" xref="S3.E4.m1.2.2.1.1.1.1.3.cmml"><mi id="S3.E4.m1.2.2.1.1.1.1.3.2" xref="S3.E4.m1.2.2.1.1.1.1.3.2.cmml">C</mi><mo id="S3.E4.m1.2.2.1.1.1.1.3.3" xref="S3.E4.m1.2.2.1.1.1.1.3.3.cmml">â€²</mo></msup></mrow><mo id="S3.E4.m1.2.2.1.1.1.3" stretchy="false" xref="S3.E4.m1.2.2.1.1.2.1.cmml">|</mo></mrow></mrow></munder><mrow id="S3.E4.m1.8.8.3.2.2" xref="S3.E4.m1.8.8.3.2.2.cmml"><mi id="S3.E4.m1.8.8.3.2.2.4" xref="S3.E4.m1.8.8.3.2.2.4.cmml">R</mi><mo id="S3.E4.m1.8.8.3.2.2.3" xref="S3.E4.m1.8.8.3.2.2.3.cmml">â¢</mo><mi id="S3.E4.m1.8.8.3.2.2.5" xref="S3.E4.m1.8.8.3.2.2.5.cmml">a</mi><mo id="S3.E4.m1.8.8.3.2.2.3a" xref="S3.E4.m1.8.8.3.2.2.3.cmml">â¢</mo><mi id="S3.E4.m1.8.8.3.2.2.6" xref="S3.E4.m1.8.8.3.2.2.6.cmml">n</mi><mo id="S3.E4.m1.8.8.3.2.2.3b" xref="S3.E4.m1.8.8.3.2.2.3.cmml">â¢</mo><mi id="S3.E4.m1.8.8.3.2.2.7" xref="S3.E4.m1.8.8.3.2.2.7.cmml">k</mi><mo id="S3.E4.m1.8.8.3.2.2.3c" xref="S3.E4.m1.8.8.3.2.2.3.cmml">â¢</mo><mrow id="S3.E4.m1.8.8.3.2.2.2.2" xref="S3.E4.m1.8.8.3.2.2.2.3.cmml"><mo id="S3.E4.m1.8.8.3.2.2.2.2.3" stretchy="false" xref="S3.E4.m1.8.8.3.2.2.2.3.cmml">(</mo><mrow id="S3.E4.m1.7.7.2.1.1.1.1.1" xref="S3.E4.m1.7.7.2.1.1.1.1.1.cmml"><msub id="S3.E4.m1.7.7.2.1.1.1.1.1.2" xref="S3.E4.m1.7.7.2.1.1.1.1.1.2.cmml"><mi id="S3.E4.m1.7.7.2.1.1.1.1.1.2.2" xref="S3.E4.m1.7.7.2.1.1.1.1.1.2.2.cmml">r</mi><mi id="S3.E4.m1.7.7.2.1.1.1.1.1.2.3" xref="S3.E4.m1.7.7.2.1.1.1.1.1.2.3.cmml">C</mi></msub><mo id="S3.E4.m1.7.7.2.1.1.1.1.1.1" xref="S3.E4.m1.7.7.2.1.1.1.1.1.1.cmml">â¢</mo><mrow id="S3.E4.m1.7.7.2.1.1.1.1.1.3.2" xref="S3.E4.m1.7.7.2.1.1.1.1.1.cmml"><mo id="S3.E4.m1.7.7.2.1.1.1.1.1.3.2.1" stretchy="false" xref="S3.E4.m1.7.7.2.1.1.1.1.1.cmml">(</mo><mi id="S3.E4.m1.4.4" xref="S3.E4.m1.4.4.cmml">j</mi><mo id="S3.E4.m1.7.7.2.1.1.1.1.1.3.2.2" stretchy="false" xref="S3.E4.m1.7.7.2.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E4.m1.8.8.3.2.2.2.2.4" xref="S3.E4.m1.8.8.3.2.2.2.3.cmml">,</mo><mrow id="S3.E4.m1.8.8.3.2.2.2.2.2" xref="S3.E4.m1.8.8.3.2.2.2.2.2.cmml"><msub id="S3.E4.m1.8.8.3.2.2.2.2.2.2" xref="S3.E4.m1.8.8.3.2.2.2.2.2.2.cmml"><mi id="S3.E4.m1.8.8.3.2.2.2.2.2.2.2" xref="S3.E4.m1.8.8.3.2.2.2.2.2.2.2.cmml">r</mi><msup id="S3.E4.m1.8.8.3.2.2.2.2.2.2.3" xref="S3.E4.m1.8.8.3.2.2.2.2.2.2.3.cmml"><mi id="S3.E4.m1.8.8.3.2.2.2.2.2.2.3.2" xref="S3.E4.m1.8.8.3.2.2.2.2.2.2.3.2.cmml">C</mi><mo id="S3.E4.m1.8.8.3.2.2.2.2.2.2.3.3" xref="S3.E4.m1.8.8.3.2.2.2.2.2.2.3.3.cmml">â€²</mo></msup></msub><mo id="S3.E4.m1.8.8.3.2.2.2.2.2.1" xref="S3.E4.m1.8.8.3.2.2.2.2.2.1.cmml">â¢</mo><mrow id="S3.E4.m1.8.8.3.2.2.2.2.2.3.2" xref="S3.E4.m1.8.8.3.2.2.2.2.2.cmml"><mo id="S3.E4.m1.8.8.3.2.2.2.2.2.3.2.1" stretchy="false" xref="S3.E4.m1.8.8.3.2.2.2.2.2.cmml">(</mo><mi id="S3.E4.m1.5.5" xref="S3.E4.m1.5.5.cmml">j</mi><mo id="S3.E4.m1.8.8.3.2.2.2.2.2.3.2.2" stretchy="false" xref="S3.E4.m1.8.8.3.2.2.2.2.2.cmml">)</mo></mrow></mrow><mo id="S3.E4.m1.8.8.3.2.2.2.2.5" stretchy="false" xref="S3.E4.m1.8.8.3.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.8b"><apply id="S3.E4.m1.8.8.cmml" xref="S3.E4.m1.8.8"><eq id="S3.E4.m1.8.8.4.cmml" xref="S3.E4.m1.8.8.4"></eq><apply id="S3.E4.m1.6.6.1.cmml" xref="S3.E4.m1.6.6.1"><times id="S3.E4.m1.6.6.1.2.cmml" xref="S3.E4.m1.6.6.1.2"></times><ci id="S3.E4.m1.6.6.1.3.cmml" xref="S3.E4.m1.6.6.1.3">ğ‘…</ci><ci id="S3.E4.m1.6.6.1.4.cmml" xref="S3.E4.m1.6.6.1.4">ğ‘</ci><ci id="S3.E4.m1.6.6.1.5.cmml" xref="S3.E4.m1.6.6.1.5">ğ‘›</ci><ci id="S3.E4.m1.6.6.1.6.cmml" xref="S3.E4.m1.6.6.1.6">ğ‘˜</ci><ci id="S3.E4.m1.6.6.1.7.cmml" xref="S3.E4.m1.6.6.1.7">ğ‘†</ci><ci id="S3.E4.m1.6.6.1.8.cmml" xref="S3.E4.m1.6.6.1.8">ğ‘–</ci><ci id="S3.E4.m1.6.6.1.9.cmml" xref="S3.E4.m1.6.6.1.9">ğ‘š</ci><interval closure="open" id="S3.E4.m1.6.6.1.1.2.cmml" xref="S3.E4.m1.6.6.1.1.1"><ci id="S3.E4.m1.3.3.cmml" xref="S3.E4.m1.3.3">ğ¶</ci><apply id="S3.E4.m1.6.6.1.1.1.1.cmml" xref="S3.E4.m1.6.6.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.1.1.1.1.cmml" xref="S3.E4.m1.6.6.1.1.1.1">superscript</csymbol><ci id="S3.E4.m1.6.6.1.1.1.1.2.cmml" xref="S3.E4.m1.6.6.1.1.1.1.2">ğ¶</ci><ci id="S3.E4.m1.6.6.1.1.1.1.3.cmml" xref="S3.E4.m1.6.6.1.1.1.1.3">â€²</ci></apply></interval></apply><apply id="S3.E4.m1.8.8.3.cmml" xref="S3.E4.m1.8.8.3"><times id="S3.E4.m1.8.8.3.3.cmml" xref="S3.E4.m1.8.8.3.3"></times><apply id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1"><divide id="S3.E4.m1.1.1.2.cmml" xref="S3.E4.m1.1.1"></divide><cn id="S3.E4.m1.1.1.3.cmml" type="integer" xref="S3.E4.m1.1.1.3">1</cn><apply id="S3.E4.m1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"><times id="S3.E4.m1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.2"></times><ci id="S3.E4.m1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.3">ğ»</ci><apply id="S3.E4.m1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1"><abs id="S3.E4.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.2"></abs><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1"><intersect id="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1"></intersect><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2">ğ¶</ci><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.2">ğ¶</ci><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.3">â€²</ci></apply></apply></apply></apply></apply><apply id="S3.E4.m1.8.8.3.2.cmml" xref="S3.E4.m1.8.8.3.2"><apply id="S3.E4.m1.8.8.3.2.3.cmml" xref="S3.E4.m1.8.8.3.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.8.8.3.2.3.1.cmml" xref="S3.E4.m1.8.8.3.2.3">subscript</csymbol><sum id="S3.E4.m1.8.8.3.2.3.2.cmml" xref="S3.E4.m1.8.8.3.2.3.2"></sum><apply id="S3.E4.m1.2.2.1.cmml" xref="S3.E4.m1.2.2.1"><in id="S3.E4.m1.2.2.1.2.cmml" xref="S3.E4.m1.2.2.1.2"></in><ci id="S3.E4.m1.2.2.1.3.cmml" xref="S3.E4.m1.2.2.1.3">ğ‘—</ci><apply id="S3.E4.m1.2.2.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.1"><abs id="S3.E4.m1.2.2.1.1.2.1.cmml" xref="S3.E4.m1.2.2.1.1.1.2"></abs><apply id="S3.E4.m1.2.2.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1"><intersect id="S3.E4.m1.2.2.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1"></intersect><ci id="S3.E4.m1.2.2.1.1.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.2">ğ¶</ci><apply id="S3.E4.m1.2.2.1.1.1.1.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.1.1.3.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.3">superscript</csymbol><ci id="S3.E4.m1.2.2.1.1.1.1.3.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.3.2">ğ¶</ci><ci id="S3.E4.m1.2.2.1.1.1.1.3.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.3.3">â€²</ci></apply></apply></apply></apply></apply><apply id="S3.E4.m1.8.8.3.2.2.cmml" xref="S3.E4.m1.8.8.3.2.2"><times id="S3.E4.m1.8.8.3.2.2.3.cmml" xref="S3.E4.m1.8.8.3.2.2.3"></times><ci id="S3.E4.m1.8.8.3.2.2.4.cmml" xref="S3.E4.m1.8.8.3.2.2.4">ğ‘…</ci><ci id="S3.E4.m1.8.8.3.2.2.5.cmml" xref="S3.E4.m1.8.8.3.2.2.5">ğ‘</ci><ci id="S3.E4.m1.8.8.3.2.2.6.cmml" xref="S3.E4.m1.8.8.3.2.2.6">ğ‘›</ci><ci id="S3.E4.m1.8.8.3.2.2.7.cmml" xref="S3.E4.m1.8.8.3.2.2.7">ğ‘˜</ci><interval closure="open" id="S3.E4.m1.8.8.3.2.2.2.3.cmml" xref="S3.E4.m1.8.8.3.2.2.2.2"><apply id="S3.E4.m1.7.7.2.1.1.1.1.1.cmml" xref="S3.E4.m1.7.7.2.1.1.1.1.1"><times id="S3.E4.m1.7.7.2.1.1.1.1.1.1.cmml" xref="S3.E4.m1.7.7.2.1.1.1.1.1.1"></times><apply id="S3.E4.m1.7.7.2.1.1.1.1.1.2.cmml" xref="S3.E4.m1.7.7.2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.7.7.2.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.7.7.2.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E4.m1.7.7.2.1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.7.7.2.1.1.1.1.1.2.2">ğ‘Ÿ</ci><ci id="S3.E4.m1.7.7.2.1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.7.7.2.1.1.1.1.1.2.3">ğ¶</ci></apply><ci id="S3.E4.m1.4.4.cmml" xref="S3.E4.m1.4.4">ğ‘—</ci></apply><apply id="S3.E4.m1.8.8.3.2.2.2.2.2.cmml" xref="S3.E4.m1.8.8.3.2.2.2.2.2"><times id="S3.E4.m1.8.8.3.2.2.2.2.2.1.cmml" xref="S3.E4.m1.8.8.3.2.2.2.2.2.1"></times><apply id="S3.E4.m1.8.8.3.2.2.2.2.2.2.cmml" xref="S3.E4.m1.8.8.3.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.8.8.3.2.2.2.2.2.2.1.cmml" xref="S3.E4.m1.8.8.3.2.2.2.2.2.2">subscript</csymbol><ci id="S3.E4.m1.8.8.3.2.2.2.2.2.2.2.cmml" xref="S3.E4.m1.8.8.3.2.2.2.2.2.2.2">ğ‘Ÿ</ci><apply id="S3.E4.m1.8.8.3.2.2.2.2.2.2.3.cmml" xref="S3.E4.m1.8.8.3.2.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.8.8.3.2.2.2.2.2.2.3.1.cmml" xref="S3.E4.m1.8.8.3.2.2.2.2.2.2.3">superscript</csymbol><ci id="S3.E4.m1.8.8.3.2.2.2.2.2.2.3.2.cmml" xref="S3.E4.m1.8.8.3.2.2.2.2.2.2.3.2">ğ¶</ci><ci id="S3.E4.m1.8.8.3.2.2.2.2.2.2.3.3.cmml" xref="S3.E4.m1.8.8.3.2.2.2.2.2.2.3.3">â€²</ci></apply></apply><ci id="S3.E4.m1.5.5.cmml" xref="S3.E4.m1.5.5">ğ‘—</ci></apply></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.8c">RankSim(C,C^{\prime})=\frac{1}{H(|C\cap C^{\prime}|)}\sum_{j\in|C\cap C^{%
\prime}|}Rank(r_{C}(j),r_{C^{\prime}}(j))</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.8d">italic_R italic_a italic_n italic_k italic_S italic_i italic_m ( italic_C , italic_C start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) = divide start_ARG 1 end_ARG start_ARG italic_H ( | italic_C âˆ© italic_C start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT | ) end_ARG âˆ‘ start_POSTSUBSCRIPT italic_j âˆˆ | italic_C âˆ© italic_C start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT | end_POSTSUBSCRIPT italic_R italic_a italic_n italic_k ( italic_r start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT ( italic_j ) , italic_r start_POSTSUBSCRIPT italic_C start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_j ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS2.p8">
<p class="ltx_p" id="S3.SS2.p8.1">with <math alttext="H(|C\cap C^{\prime}|)=\sum_{k=1}^{K=|C\cap C^{\prime}|}\frac{1}{k}" class="ltx_Math" display="inline" id="S3.SS2.p8.1.m1.2"><semantics id="S3.SS2.p8.1.m1.2a"><mrow id="S3.SS2.p8.1.m1.2.2" xref="S3.SS2.p8.1.m1.2.2.cmml"><mrow id="S3.SS2.p8.1.m1.2.2.1" xref="S3.SS2.p8.1.m1.2.2.1.cmml"><mi id="S3.SS2.p8.1.m1.2.2.1.3" xref="S3.SS2.p8.1.m1.2.2.1.3.cmml">H</mi><mo id="S3.SS2.p8.1.m1.2.2.1.2" xref="S3.SS2.p8.1.m1.2.2.1.2.cmml">â¢</mo><mrow id="S3.SS2.p8.1.m1.2.2.1.1.1" xref="S3.SS2.p8.1.m1.2.2.1.cmml"><mo id="S3.SS2.p8.1.m1.2.2.1.1.1.2" stretchy="false" xref="S3.SS2.p8.1.m1.2.2.1.cmml">(</mo><mrow id="S3.SS2.p8.1.m1.2.2.1.1.1.1.1" xref="S3.SS2.p8.1.m1.2.2.1.1.1.1.2.cmml"><mo id="S3.SS2.p8.1.m1.2.2.1.1.1.1.1.2" stretchy="false" xref="S3.SS2.p8.1.m1.2.2.1.1.1.1.2.1.cmml">|</mo><mrow id="S3.SS2.p8.1.m1.2.2.1.1.1.1.1.1" xref="S3.SS2.p8.1.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p8.1.m1.2.2.1.1.1.1.1.1.2" xref="S3.SS2.p8.1.m1.2.2.1.1.1.1.1.1.2.cmml">C</mi><mo id="S3.SS2.p8.1.m1.2.2.1.1.1.1.1.1.1" xref="S3.SS2.p8.1.m1.2.2.1.1.1.1.1.1.1.cmml">âˆ©</mo><msup id="S3.SS2.p8.1.m1.2.2.1.1.1.1.1.1.3" xref="S3.SS2.p8.1.m1.2.2.1.1.1.1.1.1.3.cmml"><mi id="S3.SS2.p8.1.m1.2.2.1.1.1.1.1.1.3.2" xref="S3.SS2.p8.1.m1.2.2.1.1.1.1.1.1.3.2.cmml">C</mi><mo id="S3.SS2.p8.1.m1.2.2.1.1.1.1.1.1.3.3" xref="S3.SS2.p8.1.m1.2.2.1.1.1.1.1.1.3.3.cmml">â€²</mo></msup></mrow><mo id="S3.SS2.p8.1.m1.2.2.1.1.1.1.1.3" stretchy="false" xref="S3.SS2.p8.1.m1.2.2.1.1.1.1.2.1.cmml">|</mo></mrow><mo id="S3.SS2.p8.1.m1.2.2.1.1.1.3" stretchy="false" xref="S3.SS2.p8.1.m1.2.2.1.cmml">)</mo></mrow></mrow><mo id="S3.SS2.p8.1.m1.2.2.2" rspace="0.111em" xref="S3.SS2.p8.1.m1.2.2.2.cmml">=</mo><mrow id="S3.SS2.p8.1.m1.2.2.3" xref="S3.SS2.p8.1.m1.2.2.3.cmml"><msubsup id="S3.SS2.p8.1.m1.2.2.3.1" xref="S3.SS2.p8.1.m1.2.2.3.1.cmml"><mo id="S3.SS2.p8.1.m1.2.2.3.1.2.2" xref="S3.SS2.p8.1.m1.2.2.3.1.2.2.cmml">âˆ‘</mo><mrow id="S3.SS2.p8.1.m1.2.2.3.1.2.3" xref="S3.SS2.p8.1.m1.2.2.3.1.2.3.cmml"><mi id="S3.SS2.p8.1.m1.2.2.3.1.2.3.2" xref="S3.SS2.p8.1.m1.2.2.3.1.2.3.2.cmml">k</mi><mo id="S3.SS2.p8.1.m1.2.2.3.1.2.3.1" xref="S3.SS2.p8.1.m1.2.2.3.1.2.3.1.cmml">=</mo><mn id="S3.SS2.p8.1.m1.2.2.3.1.2.3.3" xref="S3.SS2.p8.1.m1.2.2.3.1.2.3.3.cmml">1</mn></mrow><mrow id="S3.SS2.p8.1.m1.1.1.1" xref="S3.SS2.p8.1.m1.1.1.1.cmml"><mi id="S3.SS2.p8.1.m1.1.1.1.3" xref="S3.SS2.p8.1.m1.1.1.1.3.cmml">K</mi><mo id="S3.SS2.p8.1.m1.1.1.1.2" xref="S3.SS2.p8.1.m1.1.1.1.2.cmml">=</mo><mrow id="S3.SS2.p8.1.m1.1.1.1.1.1" xref="S3.SS2.p8.1.m1.1.1.1.1.2.cmml"><mo id="S3.SS2.p8.1.m1.1.1.1.1.1.2" stretchy="false" xref="S3.SS2.p8.1.m1.1.1.1.1.2.1.cmml">|</mo><mrow id="S3.SS2.p8.1.m1.1.1.1.1.1.1" xref="S3.SS2.p8.1.m1.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p8.1.m1.1.1.1.1.1.1.2" xref="S3.SS2.p8.1.m1.1.1.1.1.1.1.2.cmml">C</mi><mo id="S3.SS2.p8.1.m1.1.1.1.1.1.1.1" xref="S3.SS2.p8.1.m1.1.1.1.1.1.1.1.cmml">âˆ©</mo><msup id="S3.SS2.p8.1.m1.1.1.1.1.1.1.3" xref="S3.SS2.p8.1.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.SS2.p8.1.m1.1.1.1.1.1.1.3.2" xref="S3.SS2.p8.1.m1.1.1.1.1.1.1.3.2.cmml">C</mi><mo id="S3.SS2.p8.1.m1.1.1.1.1.1.1.3.3" xref="S3.SS2.p8.1.m1.1.1.1.1.1.1.3.3.cmml">â€²</mo></msup></mrow><mo id="S3.SS2.p8.1.m1.1.1.1.1.1.3" stretchy="false" xref="S3.SS2.p8.1.m1.1.1.1.1.2.1.cmml">|</mo></mrow></mrow></msubsup><mfrac id="S3.SS2.p8.1.m1.2.2.3.2" xref="S3.SS2.p8.1.m1.2.2.3.2.cmml"><mn id="S3.SS2.p8.1.m1.2.2.3.2.2" xref="S3.SS2.p8.1.m1.2.2.3.2.2.cmml">1</mn><mi id="S3.SS2.p8.1.m1.2.2.3.2.3" xref="S3.SS2.p8.1.m1.2.2.3.2.3.cmml">k</mi></mfrac></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.1.m1.2b"><apply id="S3.SS2.p8.1.m1.2.2.cmml" xref="S3.SS2.p8.1.m1.2.2"><eq id="S3.SS2.p8.1.m1.2.2.2.cmml" xref="S3.SS2.p8.1.m1.2.2.2"></eq><apply id="S3.SS2.p8.1.m1.2.2.1.cmml" xref="S3.SS2.p8.1.m1.2.2.1"><times id="S3.SS2.p8.1.m1.2.2.1.2.cmml" xref="S3.SS2.p8.1.m1.2.2.1.2"></times><ci id="S3.SS2.p8.1.m1.2.2.1.3.cmml" xref="S3.SS2.p8.1.m1.2.2.1.3">ğ»</ci><apply id="S3.SS2.p8.1.m1.2.2.1.1.1.1.2.cmml" xref="S3.SS2.p8.1.m1.2.2.1.1.1.1.1"><abs id="S3.SS2.p8.1.m1.2.2.1.1.1.1.2.1.cmml" xref="S3.SS2.p8.1.m1.2.2.1.1.1.1.1.2"></abs><apply id="S3.SS2.p8.1.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.SS2.p8.1.m1.2.2.1.1.1.1.1.1"><intersect id="S3.SS2.p8.1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p8.1.m1.2.2.1.1.1.1.1.1.1"></intersect><ci id="S3.SS2.p8.1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p8.1.m1.2.2.1.1.1.1.1.1.2">ğ¶</ci><apply id="S3.SS2.p8.1.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p8.1.m1.2.2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p8.1.m1.2.2.1.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p8.1.m1.2.2.1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.SS2.p8.1.m1.2.2.1.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p8.1.m1.2.2.1.1.1.1.1.1.3.2">ğ¶</ci><ci id="S3.SS2.p8.1.m1.2.2.1.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p8.1.m1.2.2.1.1.1.1.1.1.3.3">â€²</ci></apply></apply></apply></apply><apply id="S3.SS2.p8.1.m1.2.2.3.cmml" xref="S3.SS2.p8.1.m1.2.2.3"><apply id="S3.SS2.p8.1.m1.2.2.3.1.cmml" xref="S3.SS2.p8.1.m1.2.2.3.1"><csymbol cd="ambiguous" id="S3.SS2.p8.1.m1.2.2.3.1.1.cmml" xref="S3.SS2.p8.1.m1.2.2.3.1">superscript</csymbol><apply id="S3.SS2.p8.1.m1.2.2.3.1.2.cmml" xref="S3.SS2.p8.1.m1.2.2.3.1"><csymbol cd="ambiguous" id="S3.SS2.p8.1.m1.2.2.3.1.2.1.cmml" xref="S3.SS2.p8.1.m1.2.2.3.1">subscript</csymbol><sum id="S3.SS2.p8.1.m1.2.2.3.1.2.2.cmml" xref="S3.SS2.p8.1.m1.2.2.3.1.2.2"></sum><apply id="S3.SS2.p8.1.m1.2.2.3.1.2.3.cmml" xref="S3.SS2.p8.1.m1.2.2.3.1.2.3"><eq id="S3.SS2.p8.1.m1.2.2.3.1.2.3.1.cmml" xref="S3.SS2.p8.1.m1.2.2.3.1.2.3.1"></eq><ci id="S3.SS2.p8.1.m1.2.2.3.1.2.3.2.cmml" xref="S3.SS2.p8.1.m1.2.2.3.1.2.3.2">ğ‘˜</ci><cn id="S3.SS2.p8.1.m1.2.2.3.1.2.3.3.cmml" type="integer" xref="S3.SS2.p8.1.m1.2.2.3.1.2.3.3">1</cn></apply></apply><apply id="S3.SS2.p8.1.m1.1.1.1.cmml" xref="S3.SS2.p8.1.m1.1.1.1"><eq id="S3.SS2.p8.1.m1.1.1.1.2.cmml" xref="S3.SS2.p8.1.m1.1.1.1.2"></eq><ci id="S3.SS2.p8.1.m1.1.1.1.3.cmml" xref="S3.SS2.p8.1.m1.1.1.1.3">ğ¾</ci><apply id="S3.SS2.p8.1.m1.1.1.1.1.2.cmml" xref="S3.SS2.p8.1.m1.1.1.1.1.1"><abs id="S3.SS2.p8.1.m1.1.1.1.1.2.1.cmml" xref="S3.SS2.p8.1.m1.1.1.1.1.1.2"></abs><apply id="S3.SS2.p8.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS2.p8.1.m1.1.1.1.1.1.1"><intersect id="S3.SS2.p8.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p8.1.m1.1.1.1.1.1.1.1"></intersect><ci id="S3.SS2.p8.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p8.1.m1.1.1.1.1.1.1.2">ğ¶</ci><apply id="S3.SS2.p8.1.m1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p8.1.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p8.1.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p8.1.m1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.SS2.p8.1.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p8.1.m1.1.1.1.1.1.1.3.2">ğ¶</ci><ci id="S3.SS2.p8.1.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p8.1.m1.1.1.1.1.1.1.3.3">â€²</ci></apply></apply></apply></apply></apply><apply id="S3.SS2.p8.1.m1.2.2.3.2.cmml" xref="S3.SS2.p8.1.m1.2.2.3.2"><divide id="S3.SS2.p8.1.m1.2.2.3.2.1.cmml" xref="S3.SS2.p8.1.m1.2.2.3.2"></divide><cn id="S3.SS2.p8.1.m1.2.2.3.2.2.cmml" type="integer" xref="S3.SS2.p8.1.m1.2.2.3.2.2">1</cn><ci id="S3.SS2.p8.1.m1.2.2.3.2.3.cmml" xref="S3.SS2.p8.1.m1.2.2.3.2.3">ğ‘˜</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.1.m1.2c">H(|C\cap C^{\prime}|)=\sum_{k=1}^{K=|C\cap C^{\prime}|}\frac{1}{k}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p8.1.m1.2d">italic_H ( | italic_C âˆ© italic_C start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT | ) = âˆ‘ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K = | italic_C âˆ© italic_C start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT | end_POSTSUPERSCRIPT divide start_ARG 1 end_ARG start_ARG italic_k end_ARG</annotation></semantics></math> denoting the K-th harmonic number, normalizing the score. Like the other measures, rank similarity is bounded in the interval [0, 1] with 1 indicating that all ranks are identical.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Experimental Setup</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">The following paragraphs describe our choice of datasets and models, along with details of the implementation of our experiments.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">As we focus on the retrieval component of RAG systems, we select five publicly available datasets from the BEIR benchmark <cite class="ltx_cite ltx_citemacro_citep">(Thakur etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib36" title="">2021</a>)</cite>. As generating embeddings for large datasets is a time-intensive process, especially for a larger number of models, we opt for five of the smaller datasets from the benchmark. This approach allows us to compare embeddings generated by a variety of models while at the same time allowing us to evaluate embedding similarity accross datasets. An overview of the datasets is shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#S2.T1" title="Table 1 â€£ 2. Related Work â€£ Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">1</span></a>. For each dataset, we create embeddings by splitting documents into text chunks such that each chunk contains 256 tokens. The embedding vectors are stored with Chroma DB <cite class="ltx_cite ltx_citemacro_citep">(Inc., <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib13" title="">2024</a>)</cite>, an open source embedding database. For each vector, we additionally store information about the document and text chunk ids it encodes to be able to match embeddings generated by different models for evaluation.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">For model selection, we primarily use publicly available models from the MTEB leaderboard <cite class="ltx_cite ltx_citemacro_citep">(Muennighoff etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib29" title="">2023</a>)</cite>. We do not simply pick the best performing models on the leaderboard; instead, our choices are influenced by several factors. Firstly, we focus on analyzing similarities within and across model families and pick models belonging to the e5 <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib38" title="">2022b</a>)</cite>, t5 <cite class="ltx_cite ltx_citemacro_citep">(Ni etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib31" title="">2021b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib30" title="">a</a>)</cite>, bge <cite class="ltx_cite ltx_citemacro_citep">(Xiao
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib41" title="">2023</a>)</cite>, and gte <cite class="ltx_cite ltx_citemacro_citep">(Li
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib24" title="">2023</a>)</cite> families. Secondly, we recognize that it might be of interest to users to avoid pay-by-token policies of proprietary models by identifying similar open-source alternatives. Therefore, we pick high-performing proprietary models, two from OpenAI (text-embedding-3-large and -small) <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib32" title="">2024</a>)</cite> and one from Cohere (Cohere embed-english-v3.0) <cite class="ltx_cite ltx_citemacro_citep">(Cohere, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib6" title="">2024</a>)</cite>. We also compare the mxbai-embed-large-v1 (mxbai) <cite class="ltx_cite ltx_citemacro_citep">(Lee
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib18" title="">2024</a>)</cite> and UAE-Large-V1 (UAE) <cite class="ltx_cite ltx_citemacro_citep">(Li and Li, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib20" title="">2023</a>)</cite> models, that not only report very similar performances on MTEB, but also identical embedding dimensions, model size and memory usage. Finally, we include SFR-Embedding-Mistral (Mistral) <cite class="ltx_cite ltx_citemacro_citep">(Meng
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib25" title="">2024</a>)</cite> as the best-performing model on the leaderboard at the time of our experiments. A detailed overview of all selected models can be seen in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#S3.T2" title="Table 2 â€£ 3. Methods â€£ Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="830" id="S4.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Mean CKA similarity across all five datasets. Models tend to be most similar to models belonging to their own family, though some interesting inter-family patterns are visible as well.</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="825" id="S4.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Rank similarity over all <math alttext="k" class="ltx_Math" display="inline" id="S4.F2.4.m1.1"><semantics id="S4.F2.4.m1.1b"><mi id="S4.F2.4.m1.1.1" xref="S4.F2.4.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.F2.4.m1.1c"><ci id="S4.F2.4.m1.1.1.cmml" xref="S4.F2.4.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.4.m1.1d">k</annotation><annotation encoding="application/x-llamapun" id="S4.F2.4.m1.1e">italic_k</annotation></semantics></math> on NFCorpus, comparing gte-large to all other models. Scores are highest and vary most for small <math alttext="k" class="ltx_Math" display="inline" id="S4.F2.5.m2.1"><semantics id="S4.F2.5.m2.1b"><mi id="S4.F2.5.m2.1.1" xref="S4.F2.5.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.F2.5.m2.1c"><ci id="S4.F2.5.m2.1.1.cmml" xref="S4.F2.5.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.5.m2.1d">k</annotation><annotation encoding="application/x-llamapun" id="S4.F2.5.m2.1e">italic_k</annotation></semantics></math>, but then drop quickly before stabilizing for larger <math alttext="k" class="ltx_Math" display="inline" id="S4.F2.6.m3.1"><semantics id="S4.F2.6.m3.1b"><mi id="S4.F2.6.m3.1.1" xref="S4.F2.6.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.F2.6.m3.1c"><ci id="S4.F2.6.m3.1.1.cmml" xref="S4.F2.6.m3.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.6.m3.1d">k</annotation><annotation encoding="application/x-llamapun" id="S4.F2.6.m3.1e">italic_k</annotation></semantics></math>.</figcaption>
</figure>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.5">To compare embedding similarity across models and datasets, we employ different strategies depending on the similarity measure. We apply CKA by retrieving all embeddings created by a model, matching embeddings using their document and text chunk ids and then computing their similarity for each of the five datasets. For Jaccard and rank similarity, we use sklearnâ€™s NearestNeighbor class <cite class="ltx_cite ltx_citemacro_citep">(Pedregosa etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib33" title="">2011</a>)</cite> to determine the the top-<math alttext="k" class="ltx_Math" display="inline" id="S4.p4.1.m1.1"><semantics id="S4.p4.1.m1.1a"><mi id="S4.p4.1.m1.1.1" xref="S4.p4.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.p4.1.m1.1b"><ci id="S4.p4.1.m1.1.1.cmml" xref="S4.p4.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.p4.1.m1.1d">italic_k</annotation></semantics></math> retrieval results. We compute Jaccard and rank scores per dataset, averaging over 25 queries. For the NFCorpus dataset, we calculate retrieval similarity for all possible <math alttext="k" class="ltx_Math" display="inline" id="S4.p4.2.m2.1"><semantics id="S4.p4.2.m2.1a"><mi id="S4.p4.2.m2.1.1" xref="S4.p4.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.p4.2.m2.1b"><ci id="S4.p4.2.m2.1.1.cmml" xref="S4.p4.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.p4.2.m2.1d">italic_k</annotation></semantics></math>, i.e. using all embeddings generated for the dataset. As calculating similarity for each possible <math alttext="k" class="ltx_Math" display="inline" id="S4.p4.3.m3.1"><semantics id="S4.p4.3.m3.1a"><mi id="S4.p4.3.m3.1.1" xref="S4.p4.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.p4.3.m3.1b"><ci id="S4.p4.3.m3.1.1.cmml" xref="S4.p4.3.m3.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.p4.3.m3.1d">italic_k</annotation></semantics></math> is computationally expensive, we did not repeat this for the remaining datasets and chose a smaller <math alttext="k" class="ltx_Math" display="inline" id="S4.p4.4.m4.1"><semantics id="S4.p4.4.m4.1a"><mi id="S4.p4.4.m4.1.1" xref="S4.p4.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.p4.4.m4.1b"><ci id="S4.p4.4.m4.1.1.cmml" xref="S4.p4.4.m4.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.4.m4.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.p4.4.m4.1d">italic_k</annotation></semantics></math> value instead. Furthermore, as only a limited number of results are to be provided as context to the generative model, analyzing retrieval similarity at low <math alttext="k" class="ltx_Math" display="inline" id="S4.p4.5.m5.1"><semantics id="S4.p4.5.m5.1a"><mi id="S4.p4.5.m5.1.1" xref="S4.p4.5.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.p4.5.m5.1b"><ci id="S4.p4.5.m5.1.1.cmml" xref="S4.p4.5.m5.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.5.m5.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.p4.5.m5.1d">italic_k</annotation></semantics></math> values for e.g. top-10 is of most interest. As we are interested in identifying clusters of similar models, we also perform a hierarchical clustering on heatmap values using Seaborn <cite class="ltx_cite ltx_citemacro_citep">(Waskom, <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#bib.bib39" title="">2021</a>)</cite>. The following section describes the results of our evaluation for the different measures.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Results</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">To evaluate how similar embeddings generated by different models are, we will first consider model families, checking if their pairwise and top-k similarity scores are highest within their family. Subsequently, we will identify the open source models which are most similar to our chosen proprietary models.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Intra- and Inter-Family Clusters</h3>
<figure class="ltx_figure" id="S5.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F3.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="396" id="S5.F3.1.g1" src="x3.png" width="398"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F3.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="396" id="S5.F3.2.g1" src="x4.png" width="398"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Jaccard similarity over all <math alttext="k" class="ltx_Math" display="inline" id="S5.F3.5.m1.1"><semantics id="S5.F3.5.m1.1b"><mi id="S5.F3.5.m1.1.1" xref="S5.F3.5.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.F3.5.m1.1c"><ci id="S5.F3.5.m1.1.1.cmml" xref="S5.F3.5.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F3.5.m1.1d">k</annotation><annotation encoding="application/x-llamapun" id="S5.F3.5.m1.1e">italic_k</annotation></semantics></math> on NFCorpus, comparing bge-large (a) and gte-large (b) to all other models. While bge-large shows high similarity to UAE-Large-v1 and mxbai-embed-large-v1, scores for gte-large are clustered much closer. Jaccard similarity seems to be most unstable for small values of <math alttext="k" class="ltx_Math" display="inline" id="S5.F3.6.m2.1"><semantics id="S5.F3.6.m2.1b"><mi id="S5.F3.6.m2.1.1" xref="S5.F3.6.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.F3.6.m2.1c"><ci id="S5.F3.6.m2.1.1.cmml" xref="S5.F3.6.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F3.6.m2.1d">k</annotation><annotation encoding="application/x-llamapun" id="S5.F3.6.m2.1e">italic_k</annotation></semantics></math>, which would commonly be chosen for retrieval tasks.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Comparing embeddings directly with CKA shows high similarity across most of the models, albeit with some variance. These scores allow us to identify certain clusters of models. Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#S4.F1" title="Figure 1 â€£ 4. Experimental Setup â€£ Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">1</span></a> shows the pair-wise CKA scores of all models averaged across the five datasets. As expected, scores for most models are highest within their own family. This holds true for the gtr-t5, sentence-t5 and text-embedding-3 (OpenAI) models. Although the sentence-t5 and gtr-t5 models are closely related, they do not exhibit significantly higher similarity with each other compared to the remaining models.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">From an inter-family perspective, we observe high similarity between the bge and gte models. For some models in these two families, interestingly, the highest similarity scores rather correspond to inter-family counterparts with matching embedding dimensions than with models in the same family. Specifically, gte-small reports the highest similarity to bge-small and gte-base to bge-base. On the other hand, gte-large shows slightly higher similarity to bge-base than bge-large and thus to a model with a lower embedding dimension. Another inter-family cluster is formed by the three models with the highest CKA scores overall, namely UAE, mxbai and bge-large, whose scores suggest almost perfect embedding similarity. In fact, the similarity score of bge-large to these two models is much higher than to other bge models.</p>
</div>
<figure class="ltx_figure" id="S5.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F4.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="399" id="S5.F4.1.g1" src="x5.png" width="398"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F4.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="399" id="S5.F4.2.g1" src="x6.png" width="398"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>Jaccard (a) and rank similarity (b) for the top-10 retrieved text chunks averaged over 25 queries on NFCorpus. The clusters vary slightly depending on the measure, as do the scores. Models tend to be most similar to models from their own family. However, some inter-family clusters are visible as well.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.11">Shifting our attention to top-<math alttext="k" class="ltx_Math" display="inline" id="S5.SS1.p3.1.m1.1"><semantics id="S5.SS1.p3.1.m1.1a"><mi id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><ci id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.1.m1.1d">italic_k</annotation></semantics></math> retrieval similarity, clusters vary depending on the <math alttext="k" class="ltx_Math" display="inline" id="S5.SS1.p3.2.m2.1"><semantics id="S5.SS1.p3.2.m2.1a"><mi id="S5.SS1.p3.2.m2.1.1" xref="S5.SS1.p3.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.2.m2.1b"><ci id="S5.SS1.p3.2.m2.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.2.m2.1d">italic_k</annotation></semantics></math> value. Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#S5.F3" title="Figure 3 â€£ 5.1. Intra- and Inter-Family Clusters â€£ 5. Results â€£ Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates how Jaccard similarity evolves over <math alttext="k" class="ltx_Math" display="inline" id="S5.SS1.p3.3.m3.1"><semantics id="S5.SS1.p3.3.m3.1a"><mi id="S5.SS1.p3.3.m3.1.1" xref="S5.SS1.p3.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.3.m3.1b"><ci id="S5.SS1.p3.3.m3.1.1.cmml" xref="S5.SS1.p3.3.m3.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.3.m3.1d">italic_k</annotation></semantics></math> on NFCorpus. The first plot displays Jaccard scores between bge-large and all other models, while the second plot illustrates the scores for gte-large. For extremely low <math alttext="k" class="ltx_Math" display="inline" id="S5.SS1.p3.4.m4.1"><semantics id="S5.SS1.p3.4.m4.1a"><mi id="S5.SS1.p3.4.m4.1.1" xref="S5.SS1.p3.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.4.m4.1b"><ci id="S5.SS1.p3.4.m4.1.1.cmml" xref="S5.SS1.p3.4.m4.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.4.m4.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.4.m4.1d">italic_k</annotation></semantics></math>, we observe some peaks for nearly all models, followed by a noticeable drop in similarity. Of course, for larger <math alttext="k" class="ltx_Math" display="inline" id="S5.SS1.p3.5.m5.1"><semantics id="S5.SS1.p3.5.m5.1a"><mi id="S5.SS1.p3.5.m5.1.1" xref="S5.SS1.p3.5.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.5.m5.1b"><ci id="S5.SS1.p3.5.m5.1.1.cmml" xref="S5.SS1.p3.5.m5.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.5.m5.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.5.m5.1d">italic_k</annotation></semantics></math>, the scores converge to one. Re-affirming our earlier observations with the CKA metric, bge-large demonstrates high retrieval similarity with UAE and mxbai. Similarity to the remaining models is much lower, with the highest scores for bge-base and bge-small for larger <math alttext="k" class="ltx_Math" display="inline" id="S5.SS1.p3.6.m6.1"><semantics id="S5.SS1.p3.6.m6.1a"><mi id="S5.SS1.p3.6.m6.1.1" xref="S5.SS1.p3.6.m6.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.6.m6.1b"><ci id="S5.SS1.p3.6.m6.1.1.cmml" xref="S5.SS1.p3.6.m6.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.6.m6.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.6.m6.1d">italic_k</annotation></semantics></math>. However, especially for small <math alttext="k" class="ltx_Math" display="inline" id="S5.SS1.p3.7.m7.1"><semantics id="S5.SS1.p3.7.m7.1a"><mi id="S5.SS1.p3.7.m7.1.1" xref="S5.SS1.p3.7.m7.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.7.m7.1b"><ci id="S5.SS1.p3.7.m7.1.1.cmml" xref="S5.SS1.p3.7.m7.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.7.m7.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.7.m7.1d">italic_k</annotation></semantics></math>, there is high variance in similarity score, with models from other families, e.g. Mistral or gte-large sometimes achieving higher scores than the bge models. A similar pattern can also be observed in the second plot, where Jaccard similarity for gte-large is highest within its family for larger <math alttext="k" class="ltx_Math" display="inline" id="S5.SS1.p3.8.m8.1"><semantics id="S5.SS1.p3.8.m8.1a"><mi id="S5.SS1.p3.8.m8.1.1" xref="S5.SS1.p3.8.m8.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.8.m8.1b"><ci id="S5.SS1.p3.8.m8.1.1.cmml" xref="S5.SS1.p3.8.m8.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.8.m8.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.8.m8.1d">italic_k</annotation></semantics></math>, but models like mxbai or bge-base sometimes reporting higher similarity for small <math alttext="k" class="ltx_Math" display="inline" id="S5.SS1.p3.9.m9.1"><semantics id="S5.SS1.p3.9.m9.1a"><mi id="S5.SS1.p3.9.m9.1.1" xref="S5.SS1.p3.9.m9.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.9.m9.1b"><ci id="S5.SS1.p3.9.m9.1.1.cmml" xref="S5.SS1.p3.9.m9.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.9.m9.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.9.m9.1d">italic_k</annotation></semantics></math>. Therefore, the clusters we identified through our CKA analysis are only truly reflected in these plots for large values of <math alttext="k" class="ltx_Math" display="inline" id="S5.SS1.p3.10.m10.1"><semantics id="S5.SS1.p3.10.m10.1a"><mi id="S5.SS1.p3.10.m10.1.1" xref="S5.SS1.p3.10.m10.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.10.m10.1b"><ci id="S5.SS1.p3.10.m10.1.1.cmml" xref="S5.SS1.p3.10.m10.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.10.m10.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.10.m10.1d">italic_k</annotation></semantics></math>. This suggest that in real-world use cases, where the top-<math alttext="k" class="ltx_Math" display="inline" id="S5.SS1.p3.11.m11.1"><semantics id="S5.SS1.p3.11.m11.1a"><mi id="S5.SS1.p3.11.m11.1.1" xref="S5.SS1.p3.11.m11.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.11.m11.1b"><ci id="S5.SS1.p3.11.m11.1.1.cmml" xref="S5.SS1.p3.11.m11.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.11.m11.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.11.m11.1d">italic_k</annotation></semantics></math> are crucial, such representational similarity measures might not provide the full picture. The plots for other model families provide nearly identical insights as those in the second plot in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#S5.F3" title="Figure 3 â€£ 5.1. Intra- and Inter-Family Clusters â€£ 5. Results â€£ Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">3</span></a> and thus we do not present them for sake of brevity.</p>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.4">For rank similarity, scores peak for small <math alttext="k" class="ltx_Math" display="inline" id="S5.SS1.p4.1.m1.1"><semantics id="S5.SS1.p4.1.m1.1a"><mi id="S5.SS1.p4.1.m1.1.1" xref="S5.SS1.p4.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.1.m1.1b"><ci id="S5.SS1.p4.1.m1.1.1.cmml" xref="S5.SS1.p4.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p4.1.m1.1d">italic_k</annotation></semantics></math> and then quickly start to drop until they approach a low stable score for larger <math alttext="k" class="ltx_Math" display="inline" id="S5.SS1.p4.2.m2.1"><semantics id="S5.SS1.p4.2.m2.1a"><mi id="S5.SS1.p4.2.m2.1.1" xref="S5.SS1.p4.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.2.m2.1b"><ci id="S5.SS1.p4.2.m2.1.1.cmml" xref="S5.SS1.p4.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p4.2.m2.1d">italic_k</annotation></semantics></math> as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#S4.F2" title="Figure 2 â€£ 4. Experimental Setup â€£ Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">2</span></a> for gte-large. Once again, the bge/UAE/mxbai inter-family cluster shows the highest similarity. In contrast to Jaccard similarity, the clusters that could be observed for CKA do not always show for rank similarity. As can be seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#S4.F2" title="Figure 2 â€£ 4. Experimental Setup â€£ Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">2</span></a>, the model with the highest rank similarity to gte-large is mxbai, rather than another gte model. Even so, the previously observed clusters also tend to appear for rank similarity, though they vary more depending on the models and dataset. Generally, scores for nearly all models are rather small for larger <math alttext="k" class="ltx_Math" display="inline" id="S5.SS1.p4.3.m3.1"><semantics id="S5.SS1.p4.3.m3.1a"><mi id="S5.SS1.p4.3.m3.1.1" xref="S5.SS1.p4.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.3.m3.1b"><ci id="S5.SS1.p4.3.m3.1.1.cmml" xref="S5.SS1.p4.3.m3.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p4.3.m3.1d">italic_k</annotation></semantics></math>, indicating low rank similarity. For small <math alttext="k" class="ltx_Math" display="inline" id="S5.SS1.p4.4.m4.1"><semantics id="S5.SS1.p4.4.m4.1a"><mi id="S5.SS1.p4.4.m4.1.1" xref="S5.SS1.p4.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.4.m4.1b"><ci id="S5.SS1.p4.4.m4.1.1.cmml" xref="S5.SS1.p4.4.m4.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.4.m4.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p4.4.m4.1d">italic_k</annotation></semantics></math>, results vary more and differences between individual models are more pronounced.</p>
</div>
<div class="ltx_para" id="S5.SS1.p5">
<p class="ltx_p" id="S5.SS1.p5.3">As retrieval similarity at small <math alttext="k" class="ltx_Math" display="inline" id="S5.SS1.p5.1.m1.1"><semantics id="S5.SS1.p5.1.m1.1a"><mi id="S5.SS1.p5.1.m1.1.1" xref="S5.SS1.p5.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p5.1.m1.1b"><ci id="S5.SS1.p5.1.m1.1.1.cmml" xref="S5.SS1.p5.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p5.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p5.1.m1.1d">italic_k</annotation></semantics></math> is of most interest from a practical perspective, we take a closer look at top-10 Jaccard similarity. The heatmaps in Figures <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#S5.F4" title="Figure 4 â€£ 5.1. Intra- and Inter-Family Clusters â€£ 5. Results â€£ Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">4</span></a>-<a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#S6.F6" title="Figure 6 â€£ 6. Discussion â€£ Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">6</span></a> show the top-10 Jaccard similarity between models across datasets. A striking insight here is that even the most similar models only report a Jaccard similarity of higher than 0.6, with the majority less than 0.5. This is of great relevance to practitioners, as it would imply that even using embeddings from models that report high representational similarity scores may yield little overlap in retrieved text chunks. As earlier, the cluster of UAE/mxbai/bge-large is the most prominent one with the highest scores. Intra-family scores tend to be the highest for most models, i.e. t5 and OpenAI. Depending on the dataset, this also applies to gte and e5 models, although Jaccard similarity to models from other families is sometimes higher. We also note that for the two larger datasets FiQA-2018 and TREC-COVID, the similarity scores are generally substantially lower, as can be seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#S6.F6" title="Figure 6 â€£ 6. Discussion â€£ Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">6</span></a>. For the smaller datasets, Jaccard similarity is generally higher, though results differ depending on the data (see Figures <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#S5.F4" title="Figure 4 â€£ 5.1. Intra- and Inter-Family Clusters â€£ 5. Results â€£ Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">4</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#S5.F5" title="Figure 5 â€£ 5.2. Open Source Alternatives to Proprietary Models â€£ 5. Results â€£ Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">5</span></a>). Similar observations can be made for rank similarity, although the appearance of family clusters is more dependent on the dataset. Larger datasets also lead to lower scores. These results illustrate that while family clusters can still be perceived at small <math alttext="k" class="ltx_Math" display="inline" id="S5.SS1.p5.2.m2.1"><semantics id="S5.SS1.p5.2.m2.1a"><mi id="S5.SS1.p5.2.m2.1.1" xref="S5.SS1.p5.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p5.2.m2.1b"><ci id="S5.SS1.p5.2.m2.1.1.cmml" xref="S5.SS1.p5.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p5.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p5.2.m2.1d">italic_k</annotation></semantics></math>, they are not as prominent as they are for larger <math alttext="k" class="ltx_Math" display="inline" id="S5.SS1.p5.3.m3.1"><semantics id="S5.SS1.p5.3.m3.1a"><mi id="S5.SS1.p5.3.m3.1.1" xref="S5.SS1.p5.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p5.3.m3.1b"><ci id="S5.SS1.p5.3.m3.1.1.cmml" xref="S5.SS1.p5.3.m3.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p5.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p5.3.m3.1d">italic_k</annotation></semantics></math>. Furthermore, the top-10 retrieved results differ substantially for most models and datasets and their similarity might be dependent on the dataset itself.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Open Source Alternatives to Proprietary Models</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We explicitly included proprietary models in our analysis to check which open source models are the best - which in our case means the most similar - alternative. The CKA scores in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#S4.F1" title="Figure 1 â€£ 4. Experimental Setup â€£ Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">1</span></a> indicate that embeddings generated by OpenAIâ€™s models (text-embedding-3-large/-small) are highly similar to those generated by Mistral, while the Cohere model (embed-english-v3.0) demonstrates high similarity to e5-large-v2.</p>
</div>
<figure class="ltx_figure" id="S5.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F5.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="382" id="S5.F5.1.g1" src="x7.png" width="398"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F5.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="382" id="S5.F5.2.g1" src="x8.png" width="398"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>Jaccard similarity for the top-10 retrieved text chunks averaged over 25 queries on SciFact (a) and ArguAna (b). The UAE and mxbai models show high levels of similarity along with bge-large. The remaining models tend to show the highest similarity within their own family with the exception of the bge/gte inter-family cluster.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.4">These observations do not entirely extend to retrieval similarity, especially for Cohere. While Mistral is still the most similar model to OpenAIâ€™s for larger <math alttext="k" class="ltx_Math" display="inline" id="S5.SS2.p2.1.m1.1"><semantics id="S5.SS2.p2.1.m1.1a"><mi id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><ci id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.1.m1.1d">italic_k</annotation></semantics></math> across all datasets, there is no consistently most similar model for Cohere. Rather, the model varies depending on the dataset and measure - Jaccard and rank similarity - sometimes showing highest similarity to e5-large-v2, but sometimes also to other models like Mistral. Taking a closer look at top-10 similarity, Mistral still largely exhibits the highest similarity to the OpenAI models, especially to text-embedding-3-large. For text-embedding-3-small, scores on all datasets are rather close to those of other models. In absolute terms, however, retrieval similarity between Mistral and OpenAI models is only low to moderate. On smaller datasets, the highest Jaccard similarity to text-embedding-3-large only reaches about <math alttext="0.6" class="ltx_Math" display="inline" id="S5.SS2.p2.2.m2.1"><semantics id="S5.SS2.p2.2.m2.1a"><mn id="S5.SS2.p2.2.m2.1.1" xref="S5.SS2.p2.2.m2.1.1.cmml">0.6</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.2.m2.1b"><cn id="S5.SS2.p2.2.m2.1.1.cmml" type="float" xref="S5.SS2.p2.2.m2.1.1">0.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.2.m2.1c">0.6</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.2.m2.1d">0.6</annotation></semantics></math> (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#S5.F5" title="Figure 5 â€£ 5.2. Open Source Alternatives to Proprietary Models â€£ 5. Results â€£ Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">5</span></a>), while on TREC-COVID, the largest dataset, Jaccard similarity goes down to merely <math alttext="0.18" class="ltx_Math" display="inline" id="S5.SS2.p2.3.m3.1"><semantics id="S5.SS2.p2.3.m3.1a"><mn id="S5.SS2.p2.3.m3.1.1" xref="S5.SS2.p2.3.m3.1.1.cmml">0.18</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.3.m3.1b"><cn id="S5.SS2.p2.3.m3.1.1.cmml" type="float" xref="S5.SS2.p2.3.m3.1.1">0.18</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.3.m3.1c">0.18</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.3.m3.1d">0.18</annotation></semantics></math> (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#S6.F6" title="Figure 6 â€£ 6. Discussion â€£ Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">6</span></a>). For Cohereâ€™s model, the most similar model for top-10 Jaccard similarity is different for each dataset, with the highest scores of <math alttext="0.51" class="ltx_Math" display="inline" id="S5.SS2.p2.4.m4.1"><semantics id="S5.SS2.p2.4.m4.1a"><mn id="S5.SS2.p2.4.m4.1.1" xref="S5.SS2.p2.4.m4.1.1.cmml">0.51</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.4.m4.1b"><cn id="S5.SS2.p2.4.m4.1.1.cmml" type="float" xref="S5.SS2.p2.4.m4.1.1">0.51</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.4.m4.1c">0.51</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.4.m4.1d">0.51</annotation></semantics></math> occurring on ArguAna shwon in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.08275v1#S5.F5" title="Figure 5 â€£ 5.2. Open Source Alternatives to Proprietary Models â€£ 5. Results â€£ Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval Augmented Generation Systems"><span class="ltx_text ltx_ref_tag">5</span></a>. For all proprietary models, even the best retrieval similarity at top-10 still suggests that the embeddings that would be presented to an LLM can differ notably. Once again, we could also observe dataset-dependent variance in scores, with lower retrieval similarity on larger datasets.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Discussion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.5">While a pair-wise comparison of embeddings using CKA shows intra- and inter-family model clusters, retrieval similarity over different <math alttext="k" class="ltx_Math" display="inline" id="S6.p1.1.m1.1"><semantics id="S6.p1.1.m1.1a"><mi id="S6.p1.1.m1.1.1" xref="S6.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.p1.1.m1.1b"><ci id="S6.p1.1.m1.1.1.cmml" xref="S6.p1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S6.p1.1.m1.1d">italic_k</annotation></semantics></math> offers a more nuanced picture. Especially for small <math alttext="k" class="ltx_Math" display="inline" id="S6.p1.2.m2.1"><semantics id="S6.p1.2.m2.1a"><mi id="S6.p1.2.m2.1.1" xref="S6.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.p1.2.m2.1b"><ci id="S6.p1.2.m2.1.1.cmml" xref="S6.p1.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S6.p1.2.m2.1d">italic_k</annotation></semantics></math>, which are of most interest from a practical perspective, retrieval similarity varies. When comparing the top-10 retrieved text chunks, the low Jaccard similarity scores indicate little overlap in retrieved chunks, even when CKA scores are high. Especially for the two larger datasets FiQA-2018 and TREC-COVID, these scores are extremely low. As RAG systems usually operate on millions of embeddings, our datasets are comparatively small. Therefore, should a general trend of larger datasets leading to lower retrieval similarity exist, text chunks retrieved by different models in a regular use case might be nearly distinct for small <math alttext="k" class="ltx_Math" display="inline" id="S6.p1.3.m3.1"><semantics id="S6.p1.3.m3.1a"><mi id="S6.p1.3.m3.1.1" xref="S6.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.p1.3.m3.1b"><ci id="S6.p1.3.m3.1.1.cmml" xref="S6.p1.3.m3.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="S6.p1.3.m3.1d">italic_k</annotation></semantics></math>. Overall, our results suggest that even though embeddings seem rather similar when compared directly, retrieval performance can still vary substantially, is most unstable for <math alttext="k" class="ltx_Math" display="inline" id="S6.p1.4.m4.1"><semantics id="S6.p1.4.m4.1a"><mi id="S6.p1.4.m4.1.1" xref="S6.p1.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.p1.4.m4.1b"><ci id="S6.p1.4.m4.1.1.cmml" xref="S6.p1.4.m4.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.4.m4.1c">k</annotation><annotation encoding="application/x-llamapun" id="S6.p1.4.m4.1d">italic_k</annotation></semantics></math> values that are commonly used in RAG systems and also dataset-dependent. Retrieved text chunks at small <math alttext="k" class="ltx_Math" display="inline" id="S6.p1.5.m5.1"><semantics id="S6.p1.5.m5.1a"><mi id="S6.p1.5.m5.1.1" xref="S6.p1.5.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.p1.5.m5.1b"><ci id="S6.p1.5.m5.1.1.cmml" xref="S6.p1.5.m5.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.5.m5.1c">k</annotation><annotation encoding="application/x-llamapun" id="S6.p1.5.m5.1d">italic_k</annotation></semantics></math> show the least overlap, often leading to high differences in the data that would be presented to an LLM as additional context.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.3">Our analysis demonstrates that although models tend to be most similar to models from their own family, inter-family clusters exist. The most prominent of these clusters is formed by the models bge-large-en-v1.5, UAE-Large-V1 and mxbai-embed-large-v1, which demonstrate high similarity even for retrieval at low <math alttext="k" class="ltx_Math" display="inline" id="S6.p2.1.m1.1"><semantics id="S6.p2.1.m1.1a"><mi id="S6.p2.1.m1.1.1" xref="S6.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.p2.1.m1.1b"><ci id="S6.p2.1.m1.1.1.cmml" xref="S6.p2.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S6.p2.1.m1.1d">italic_k</annotation></semantics></math>. Nevertheless, the high variance of retrieval similarity of the remaining clusters for small <math alttext="k" class="ltx_Math" display="inline" id="S6.p2.2.m2.1"><semantics id="S6.p2.2.m2.1a"><mi id="S6.p2.2.m2.1.1" xref="S6.p2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.p2.2.m2.1b"><ci id="S6.p2.2.m2.1.1.cmml" xref="S6.p2.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S6.p2.2.m2.1d">italic_k</annotation></semantics></math> means that while the identified clusters might provide some measure of orientation when choosing an embedding model, the choice still remains a non-trivial task. Identifying suitable alternatives to proprietary models is likewise not as simple. While we were able to determine SFR-Embedding-Mistral as the model being most similar to OpenAIâ€™s embedding models, Jaccard similarity at top-10 for larger datasets shows a low overlap in retrieved text chunks. Furthermore, for Cohereâ€™s embedding model, we were unable to find a single most similar model, as this model varied across datasets for small <math alttext="k" class="ltx_Math" display="inline" id="S6.p2.3.m3.1"><semantics id="S6.p2.3.m3.1a"><mi id="S6.p2.3.m3.1.1" xref="S6.p2.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.p2.3.m3.1b"><ci id="S6.p2.3.m3.1.1.cmml" xref="S6.p2.3.m3.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="S6.p2.3.m3.1d">italic_k</annotation></semantics></math> values.</p>
</div>
<figure class="ltx_figure" id="S6.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S6.F6.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="382" id="S6.F6.1.g1" src="x9.png" width="398"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S6.F6.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="382" id="S6.F6.2.g1" src="x10.png" width="398"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>Jaccard similarity for the top-10 retrieved text chunks averaged over 25 queries on FiQA-2018 (a) and TREC-COVID (b). Most models seem to retrieve almost completely distinct text chunks. Only the bge/UAE/mxbai cluster still shows a notable level of similarity, while the scores of the remaining clusters indicate only moderate to low levels of similarity.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.4">In this paper we evaluated the similarity of embedding models on different datasets. Given the large number of available models, identifying clusters or families of models with similar embeddings can simplify the model selection process. While previous work on LLM similarity exists, to the best of the authorsâ€™ knowledge, it so far lacks a clear focus on embedding models specifically in the context of RAG. We therefore analyzed the similarity of embeddings generated by 19 different models using CKA for pairwise comparison as well as Jaccard and rank similarity to compare retrieval behavior at top-<math alttext="k" class="ltx_Math" display="inline" id="S7.p1.1.m1.1"><semantics id="S7.p1.1.m1.1a"><mi id="S7.p1.1.m1.1.1" xref="S7.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S7.p1.1.m1.1b"><ci id="S7.p1.1.m1.1.1.cmml" xref="S7.p1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S7.p1.1.m1.1d">italic_k</annotation></semantics></math> across five datasets. Comparing embeddings with CKA generally showed intra- and inter-family clusters across datasets. These clusters also appeared when evaluating top-<math alttext="k" class="ltx_Math" display="inline" id="S7.p1.2.m2.1"><semantics id="S7.p1.2.m2.1a"><mi id="S7.p1.2.m2.1.1" xref="S7.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S7.p1.2.m2.1b"><ci id="S7.p1.2.m2.1.1.cmml" xref="S7.p1.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S7.p1.2.m2.1d">italic_k</annotation></semantics></math> retrieval similarity with large <math alttext="k" class="ltx_Math" display="inline" id="S7.p1.3.m3.1"><semantics id="S7.p1.3.m3.1a"><mi id="S7.p1.3.m3.1.1" xref="S7.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S7.p1.3.m3.1b"><ci id="S7.p1.3.m3.1.1.cmml" xref="S7.p1.3.m3.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="S7.p1.3.m3.1d">italic_k</annotation></semantics></math> values. However, scores for low <math alttext="k" class="ltx_Math" display="inline" id="S7.p1.4.m4.1"><semantics id="S7.p1.4.m4.1a"><mi id="S7.p1.4.m4.1.1" xref="S7.p1.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S7.p1.4.m4.1b"><ci id="S7.p1.4.m4.1.1.cmml" xref="S7.p1.4.m4.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.4.m4.1c">k</annotation><annotation encoding="application/x-llamapun" id="S7.p1.4.m4.1d">italic_k</annotation></semantics></math> values, which would commonly be chosen in RAG systems, show high variance and much lower similarity, especially on larger datasets. Although we were able to identify some model clusters, our results suggest that choosing the optimal model remains a non-trivial task that requires careful consideration.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">Still, we argue that a better understanding of how similarly different embedding models behave is an important research topic that requires further attention. There are a plethora of real-world scenarios where RAG systems can potentially be deployed. One such scenario, for example, is to retrieve relevant web content in response to a query. As large corpora of such data are available in the form of Web ARChive (WARC) files, evaluating embedding model similarity on such large, uncleaned datasets would perhaps lead to a better estimation of model similarity for a realistic RAG use case. Additionally, as documents often need to be chunked into smaller parts to fit into the models, the effect of chunking strategies such as token-based or semantic chunking on embedding similarity could be explored. Furthermore, our evaluation focused on a small sample of similarity measures, with their own definition about which criteria make models similar. Introducing more measures with different perspectives could improve our understanding on which factors influence model similarity. Finally, our focus was on identifying clusters or families of models, which for our initial experiments led us to choosing families of embedding models with varying performance on MTEB. With the frequent appearance of new models on the leaderboard and the focus on good MTEB performance, it would be of interest to compare the best performing models on MTEB and check if their relative difference in performance correlates with how similar these models are.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
This work has received funding from the European Unionâ€™s Horizon Europe research and innovation program under grant agreement No 101070014 (OpenWebSearch.EU, <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.3030/101070014" title="">https://doi.org/10.3030/101070014</a>).

</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Balogh and
Jelasity (2023)</span>
<span class="ltx_bibblock">
AndrÃ¡s Balogh and
MÃ¡rk Jelasity. 2023.

</span>
<span class="ltx_bibblock">On the Functional Similarity of Robust and
Non-Robust Neural Representations. In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings
of the 40th International Conference on Machine Learning</em>
<em class="ltx_emph ltx_font_italic" id="bib.bib2.2.2">(Proceedings of Machine Learning Research,
Vol.Â 202)</em>, Andreas
Krause, Emma Brunskill, Kyunghyun Cho,
Barbara Engelhardt, Sivan Sabato, and
Jonathan Scarlett (Eds.). PMLR,
1614â€“1635.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.mlr.press/v202/balogh23a.html" title="">https://proceedings.mlr.press/v202/balogh23a.html</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bansal
etÂ al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yamini Bansal, Preetum
Nakkiran, and Boaz Barak.
2021.

</span>
<span class="ltx_bibblock">Revisiting Model Stitching to Compare Neural
Representations. In <em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">Advances in Neural Information
Processing Systems</em>, M.Â Ranzato,
A.Â Beygelzimer, Y.Â Dauphin,
P.S. Liang, and J.Â Wortman Vaughan
(Eds.), Vol.Â 34. Curran Associates,
Inc., 225â€“236.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2021/file/01ded4259d101feb739b06c399e9cd9c-Paper.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2021/file/01ded4259d101feb739b06c399e9cd9c-Paper.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown
etÂ al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Davis Brown, Charles
Godfrey, Nicholas Konz, Jonathan Tu,
and Henry Kvinge. 2023.

</span>
<span class="ltx_bibblock">Understanding the Inner Workings of Language Models
Through Representation Dissimilarity.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2310.14993Â [cs.LG]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen
etÂ al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Wei Chen, Zichen Miao,
and Qiang Qiu. 2024.

</span>
<span class="ltx_bibblock">Inner Product-based Neural Network Similarity.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">Advances in Neural Information Processing
Systems</em> 36 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cohere (2024)</span>
<span class="ltx_bibblock">
Cohere. 2024.

</span>
<span class="ltx_bibblock">Embeddings - Text Embeddings with Advanced Language
Models.

</span>
<span class="ltx_bibblock">Cohere Homepage.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://cohere.com/embeddings" title="">https://cohere.com/embeddings</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding
etÂ al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Frances Ding,
Jean-Stanislas Denain, and Jacob
Steinhardt. 2021.

</span>
<span class="ltx_bibblock">Grounding representation similarity through
statistical testing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">Advances in Neural Information Processing
Systems</em> 34 (2021),
1556â€“1568.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Es
etÂ al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shahul Es, Jithin James,
Luis Espinosa-Anke, and Steven
Schockaert. 2023.

</span>
<span class="ltx_bibblock">RAGAS: Automated Evaluation of Retrieval Augmented
Generation.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2309.15217Â [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Finardi etÂ al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Paulo Finardi, Leonardo
Avila, Rodrigo Castaldoni, Pedro Gengo,
Celio Larcher, Marcos Piau,
Pablo Costa, and Vinicius CaridÃ¡.
2024.

</span>
<span class="ltx_bibblock">The Chronicles of RAG: The Retriever, the Chunk and
the Generator.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2401.07883Â [cs.LG]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Freestone and
Santu (2024)</span>
<span class="ltx_bibblock">
Matthew Freestone and
Shubhra KantiÂ Karmaker Santu.
2024.

</span>
<span class="ltx_bibblock">Word Embeddings Revisited: Do LLMs Offer Something
New?

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2402.11094Â [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gretton etÂ al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2005)</span>
<span class="ltx_bibblock">
Arthur Gretton, Olivier
Bousquet, Alex Smola, and Bernhard
SchÃ¶lkopf. 2005.

</span>
<span class="ltx_bibblock">Measuring Statistical Dependence with
Hilbert-Schmidt Norms. In <em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">Algorithmic Learning
Theory</em>, Sanjay Jain,
HansÂ Ulrich Simon, and Etsuji Tomita
(Eds.). Springer Berlin Heidelberg,
Berlin, Heidelberg, 63â€“77.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hardoon
etÂ al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2004)</span>
<span class="ltx_bibblock">
DavidÂ R Hardoon, Sandor
Szedmak, and John Shawe-Taylor.
2004.

</span>
<span class="ltx_bibblock">Canonical correlation analysis: An overview with
application to learning methods.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">Neural computation</em> 16,
12 (2004), 2639â€“2664.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Inc. (2024)</span>
<span class="ltx_bibblock">
Chroma Inc.
2024.

</span>
<span class="ltx_bibblock">Chroma.

</span>
<span class="ltx_bibblock">Chroma Homepage.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://docs.trychroma.com/" title="">https://docs.trychroma.com/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji etÂ al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Ziwei Ji, Nayeon Lee,
Rita Frieske, Tiezheng Yu,
Dan Su, Yan Xu, Etsuko
Ishii, YeÂ Jin Bang, Andrea Madotto,
and Pascale Fung. 2023.

</span>
<span class="ltx_bibblock">Survey of hallucination in natural language
generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">Comput. Surveys</em> 55,
12 (2023), 1â€“38.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Klabunde etÂ al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Max Klabunde, MehdiÂ Ben
Amor, Michael Granitzer, and Florian
Lemmerich. 2023a.

</span>
<span class="ltx_bibblock">Towards Measuring Representational Similarity of
Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">arXiv preprint arXiv:2312.02730</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Klabunde etÂ al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Max Klabunde, Tobias
Schumacher, Markus Strohmaier, and
Florian Lemmerich. 2023b.

</span>
<span class="ltx_bibblock">Similarity of neural network models: A survey of
functional and representational measures.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">arXiv preprint arXiv:2305.06329</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kornblith
etÂ al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Simon Kornblith, Mohammad
Norouzi, Honglak Lee, and Geoffrey
Hinton. 2019.

</span>
<span class="ltx_bibblock">Similarity of Neural Network Representations
Revisited. In <em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">Proceedings of the 36th
International Conference on Machine Learning</em>
<em class="ltx_emph ltx_font_italic" id="bib.bib17.4.2">(Proceedings of Machine Learning Research,
Vol.Â 97)</em>, Kamalika
Chaudhuri and Ruslan Salakhutdinov (Eds.).
PMLR, 3519â€“3529.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.mlr.press/v97/kornblith19a.html" title="">https://proceedings.mlr.press/v97/kornblith19a.html</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee
etÂ al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Sean Lee, Amir Shakir,
Darius Koenig, and Julius Lipp.
2024.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">Open Source Strikes Bread - New Fluffy
Embeddings Model</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mixedbread.ai/blog/mxbai-embed-large-v1" title="">https://www.mixedbread.ai/blog/mxbai-embed-large-v1</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lenc and Vedaldi (2015)</span>
<span class="ltx_bibblock">
Karel Lenc and Andrea
Vedaldi. 2015.

</span>
<span class="ltx_bibblock">Understanding image representations by measuring
their equivariance and equivalence. In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings
of the IEEE conference on computer vision and pattern recognition</em>.
991â€“999.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Li (2023)</span>
<span class="ltx_bibblock">
Xianming Li and Jing
Li. 2023.

</span>
<span class="ltx_bibblock">AnglE-optimized Text Embeddings.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2309.12871</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li
etÂ al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Yixuan Li, Jason
Yosinski, Jeff Clune, Hod Lipson, and
John Hopcroft. 2015.

</span>
<span class="ltx_bibblock">Convergent Learning: Do different neural networks
learn the same representations?. In <em class="ltx_emph ltx_font_italic" id="bib.bib21.3.1">Proceedings of
the 1st International Workshop on Feature Extraction: Modern Questions and
Challenges at NIPS 2015</em> <em class="ltx_emph ltx_font_italic" id="bib.bib21.4.2">(Proceedings of Machine
Learning Research, Vol.Â 44)</em>,
Dmitry Storcheus, Afshin
Rostamizadeh, and Sanjiv Kumar (Eds.).
PMLR, Montreal, Canada,
196â€“212.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.mlr.press/v44/li15convergent.html" title="">https://proceedings.mlr.press/v44/li15convergent.html</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li
etÂ al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Yixuan Li, Jason
Yosinski, Jeff Clune, Hod Lipson, and
John Hopcroft. 2016.

</span>
<span class="ltx_bibblock">Convergent Learning: Do different neural networks
learn the same representations?

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1511.07543Â [cs.LG]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li
etÂ al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yuanchun Li, Ziqi Zhang,
Bingyan Liu, Ziyue Yang, and
Yunxin Liu. 2021.

</span>
<span class="ltx_bibblock">ModelDiff: testing-based DNN similarity comparison
for model reuse detection. In <em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">Proceedings of the
30th ACM SIGSOFT International Symposium on Software Testing and Analysis</em>
<em class="ltx_emph ltx_font_italic" id="bib.bib23.4.2">(ISSTA â€™21)</em>. ACM.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3460319.3464816" title="">https://doi.org/10.1145/3460319.3464816</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li
etÂ al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zehan Li, Xin Zhang,
Yanzhao Zhang, Dingkun Long,
Pengjun Xie, and Meishan Zhang.
2023.

</span>
<span class="ltx_bibblock">Towards general text embeddings with multi-stage
contrastive learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">arXiv preprint arXiv:2308.03281</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meng
etÂ al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Rui Meng, Ye Liu,
ShafiqÂ Rayhan Joty, Caiming Xiong,
Yinbo Zhou, and Semih Yavuz.
2024.

</span>
<span class="ltx_bibblock">SFR-Embedding-Mistral:Enhance Text Retrieval with
Transfer Learning.

</span>
<span class="ltx_bibblock">Salesforce AI Research Blog.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://blog.salesforceairesearch.com/sfr-embedded-mistral/" title="">https://blog.salesforceairesearch.com/sfr-embedded-mistral/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MilaniÂ Fard etÂ al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Mahdi MilaniÂ Fard, Quentin
Cormier, Kevin Canini, and Maya
Gupta. 2016.

</span>
<span class="ltx_bibblock">Launch and iterate: Reducing prediction churn.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">Advances in Neural Information Processing
Systems</em> 29 (2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Morcos
etÂ al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Ari Morcos, Maithra
Raghu, and Samy Bengio.
2018.

</span>
<span class="ltx_bibblock">Insights on representational similarity in neural
networks with canonical correlation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">Advances in neural information processing
systems</em> 31 (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mousavi
etÂ al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
SeyedÂ Mahed Mousavi,
Simone Alghisi, and Giuseppe Riccardi.
2024.

</span>
<span class="ltx_bibblock">Is Your LLM Outdated? Benchmarking LLMs &amp; Alignment
Algorithms for Time-Sensitive Knowledge.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2404.08700Â [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muennighoff etÂ al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Niklas Muennighoff,
Nouamane Tazi, LoÃ¯c Magne, and
Nils Reimers. 2023.

</span>
<span class="ltx_bibblock">MTEB: Massive Text Embedding Benchmark.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2210.07316Â [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ni etÂ al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2021a)</span>
<span class="ltx_bibblock">
Jianmo Ni, Chen Qu,
Jing Lu, Zhuyun Dai,
GustavoÂ HernÃ¡ndez Ãbrego, Ji Ma,
VincentÂ Y. Zhao, Yi Luan,
KeithÂ B. Hall, Ming-Wei Chang, and
Yinfei Yang. 2021a.

</span>
<span class="ltx_bibblock">Large Dual Encoders Are Generalizable Retrievers.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2112.07899Â [cs.IR]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ni etÂ al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2021b)</span>
<span class="ltx_bibblock">
Jianmo Ni,
GustavoÂ HernÃ¡ndez Ãbrego, Noah
Constant, Ji Ma, KeithÂ B. Hall,
Daniel Cer, and Yinfei Yang.
2021b.

</span>
<span class="ltx_bibblock">Sentence-T5: Scalable Sentence Encoders from
Pre-trained Text-to-Text Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2108.08877Â [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2024)</span>
<span class="ltx_bibblock">
OpenAI. 2024.

</span>
<span class="ltx_bibblock">New embedding models with lower pricing.

</span>
<span class="ltx_bibblock">OpenAI Blog.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/blog/new-embedding-models-and-api-updates" title="">https://openai.com/blog/new-embedding-models-and-api-updates</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pedregosa etÂ al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2011)</span>
<span class="ltx_bibblock">
F. Pedregosa, G.
Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel,
M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J.
Vanderplas, A. Passos, D. Cournapeau,
M. Brucher, M. Perrot, and
E. Duchesnay. 2011.

</span>
<span class="ltx_bibblock">Scikit-learn: Machine Learning in Python.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.3.1">Journal of Machine Learning Research</em>
12 (2011), 2825â€“2830.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raghu etÂ al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Maithra Raghu, Justin
Gilmer, Jason Yosinski, and Jascha
Sohl-Dickstein. 2017.

</span>
<span class="ltx_bibblock">Svcca: Singular vector canonical correlation
analysis for deep learning dynamics and interpretability.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">Advances in neural information processing
systems</em> 30 (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sawarkar
etÂ al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Kunal Sawarkar, Abhilasha
Mangal, and ShivamÂ Raj Solanki.
2024.

</span>
<span class="ltx_bibblock">Blended RAG: Improving RAG (Retriever-Augmented
Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2404.07220Â [cs.IR]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thakur etÂ al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Nandan Thakur, Nils
Reimers, Andreas RÃ¼cklÃ©, Abhishek
Srivastava, and Iryna Gurevych.
2021.

</span>
<span class="ltx_bibblock">BEIR: A Heterogenous Benchmark for Zero-shot
Evaluation of Information Retrieval Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2104.08663Â [cs.IR]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang
etÂ al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2022a)</span>
<span class="ltx_bibblock">
Chenxu Wang, Wei Rao,
Wenna Guo, Pinghui Wang,
Jun Liu, and Xiaohong Guan.
2022a.

</span>
<span class="ltx_bibblock">Towards Understanding the Instability of Network
Embedding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.3.1">IEEE Transactions on Knowledge and Data
Engineering</em> 34, 2
(2022), 927â€“941.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/TKDE.2020.2989512" title="">https://doi.org/10.1109/TKDE.2020.2989512</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al<span class="ltx_text" id="bib.bib38.2.2.1">.</span> (2022b)</span>
<span class="ltx_bibblock">
Liang Wang, Nan Yang,
Xiaolong Huang, Binxing Jiao,
Linjun Yang, Daxin Jiang,
Rangan Majumder, and Furu Wei.
2022b.

</span>
<span class="ltx_bibblock">Text Embeddings by Weakly-Supervised Contrastive
Pre-training.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.3.1">arXiv preprint arXiv:2212.03533</em>
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Waskom (2021)</span>
<span class="ltx_bibblock">
MichaelÂ L. Waskom.
2021.

</span>
<span class="ltx_bibblock">seaborn: statistical data visualization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Journal of Open Source Software</em>
6, 60 (2021),
3021.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.21105/joss.03021" title="">https://doi.org/10.21105/joss.03021</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu etÂ al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
JohnÂ M. Wu, Yonatan
Belinkov, Hassan Sajjad, Nadir Durrani,
Fahim Dalvi, and James Glass.
2020.

</span>
<span class="ltx_bibblock">Similarity Analysis of Contextual Word Representation
Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2005.01172Â [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao
etÂ al<span class="ltx_text" id="bib.bib41.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shitao Xiao, Zheng Liu,
Peitian Zhang, and Niklas Muennighoff.
2023.

</span>
<span class="ltx_bibblock">C-Pack: Packaged Resources To Advance General Chinese
Embedding.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2309.07597Â [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie
etÂ al<span class="ltx_text" id="bib.bib42.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Xiaofei Xie, Lei Ma,
Haijun Wang, Yuekang Li,
Yang Liu, and Xiaohong Li.
2019.

</span>
<span class="ltx_bibblock">Diffchaser: Detecting disagreements for deep neural
networks. International Joint Conferences on Artificial Intelligence
Organization.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zullich etÂ al<span class="ltx_text" id="bib.bib43.3.3.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Marco Zullich, Felice
Pellegrino, Eric Medvet, Alessio
Ansuini, etÂ al<span class="ltx_text" id="bib.bib43.4.1">.</span> 2020.

</span>
<span class="ltx_bibblock">On the similarity between hidden layers of pruned
and unpruned convolutional neural networks. In
<em class="ltx_emph ltx_font_italic" id="bib.bib43.5.1">Proceedings of the 9th International Conference on
Pattern Recognition Applications and Methods</em>. Scitepress,
52â€“59.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Jul 11 08:16:57 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
